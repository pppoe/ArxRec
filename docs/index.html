<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level\n  Recognition", "author": "Lianghui Zhu and Junwei Zhou and Yan Liu and Xin Hao and Wenyu Liu and Xinggang Wang", "abstract": "  Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.\n", "link": "http://arxiv.org/abs/2402.14812v1", "date": "2024-02-22", "relevancy": 2.7483, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeakSAM%3A%20Segment%20Anything%20Meets%20Weakly-supervised%20Instance-level%0A%20%20Recognition&entry.906535625=Lianghui%20Zhu%20and%20Junwei%20Zhou%20and%20Yan%20Liu%20and%20Xin%20Hao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Weakly%20supervised%20visual%20recognition%20using%20inexact%20supervision%20is%20a%20critical%0Ayet%20challenging%20learning%20problem.%20It%20significantly%20reduces%20human%20labeling%20costs%0Aand%20traditionally%20relies%20on%20multi-instance%20learning%20and%20pseudo-labeling.%20This%0Apaper%20introduces%20WeakSAM%20and%20solves%20the%20weakly-supervised%20object%20detection%0A%28WSOD%29%20and%20segmentation%20by%20utilizing%20the%20pre-learned%20world%20knowledge%20contained%0Ain%20a%20vision%20foundation%20model%2C%20i.e.%2C%20the%20Segment%20Anything%20Model%20%28SAM%29.%20WeakSAM%0Aaddresses%20two%20critical%20limitations%20in%20traditional%20WSOD%20retraining%2C%20i.e.%2C%20pseudo%0Aground%20truth%20%28PGT%29%20incompleteness%20and%20noisy%20PGT%20instances%2C%20through%20adaptive%20PGT%0Ageneration%20and%20Region%20of%20Interest%20%28RoI%29%20drop%20regularization.%20It%20also%20addresses%0Athe%20SAM%27s%20problems%20of%20requiring%20prompts%20and%20category%20unawareness%20for%20automatic%0Aobject%20detection%20and%20segmentation.%20Our%20results%20indicate%20that%20WeakSAM%0Asignificantly%20surpasses%20previous%20state-of-the-art%20methods%20in%20WSOD%20and%20WSIS%0Abenchmarks%20with%20large%20margins%2C%20i.e.%20average%20improvements%20of%207.4%25%20and%208.5%25%2C%0Arespectively.%20The%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/hustvl/WeakSAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14812v1&entry.124074799=Read"},
{"title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place\n  Recognition", "author": "Feng Lu and Lijun Zhang and Xiangyuan Lan and Shuting Dong and Yaowei Wang and Chun Yuan", "abstract": "  Recent studies show that vision models pre-trained in generic visual learning\ntasks with large-scale data can provide useful feature representations for a\nwide range of visual perception problems. However, few attempts have been made\nto exploit pre-trained foundation models in visual place recognition (VPR). Due\nto the inherent difference in training objectives and data between the tasks of\nmodel pre-training and VPR, how to bridge the gap and fully unleash the\ncapability of pre-trained models for VPR is still a key issue to address. To\nthis end, we propose a novel method to realize seamless adaptation of\npre-trained models for VPR. Specifically, to obtain both global and local\nfeatures that focus on salient landmarks for discriminating places, we design a\nhybrid adaptation method to achieve both global and local adaptation\nefficiently, in which only lightweight adapters are tuned without adjusting the\npre-trained model. Besides, to guide effective adaptation, we propose a mutual\nnearest neighbor local feature loss, which ensures proper dense local features\nare produced for local matching and avoids time-consuming spatial verification\nin re-ranking. Experimental results show that our method outperforms the\nstate-of-the-art methods with less training data and training time, and uses\nabout only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based\nspatial verification. It ranks 1st on the MSLS challenge leaderboard (at the\ntime of submission). The code is released at\nhttps://github.com/Lu-Feng/SelaVPR.\n", "link": "http://arxiv.org/abs/2402.14505v1", "date": "2024-02-22", "relevancy": 2.7326, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5373}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Feng%20Lu%20and%20Lijun%20Zhang%20and%20Xiangyuan%20Lan%20and%20Shuting%20Dong%20and%20Yaowei%20Wang%20and%20Chun%20Yuan&entry.1292438233=%20%20Recent%20studies%20show%20that%20vision%20models%20pre-trained%20in%20generic%20visual%20learning%0Atasks%20with%20large-scale%20data%20can%20provide%20useful%20feature%20representations%20for%20a%0Awide%20range%20of%20visual%20perception%20problems.%20However%2C%20few%20attempts%20have%20been%20made%0Ato%20exploit%20pre-trained%20foundation%20models%20in%20visual%20place%20recognition%20%28VPR%29.%20Due%0Ato%20the%20inherent%20difference%20in%20training%20objectives%20and%20data%20between%20the%20tasks%20of%0Amodel%20pre-training%20and%20VPR%2C%20how%20to%20bridge%20the%20gap%20and%20fully%20unleash%20the%0Acapability%20of%20pre-trained%20models%20for%20VPR%20is%20still%20a%20key%20issue%20to%20address.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20method%20to%20realize%20seamless%20adaptation%20of%0Apre-trained%20models%20for%20VPR.%20Specifically%2C%20to%20obtain%20both%20global%20and%20local%0Afeatures%20that%20focus%20on%20salient%20landmarks%20for%20discriminating%20places%2C%20we%20design%20a%0Ahybrid%20adaptation%20method%20to%20achieve%20both%20global%20and%20local%20adaptation%0Aefficiently%2C%20in%20which%20only%20lightweight%20adapters%20are%20tuned%20without%20adjusting%20the%0Apre-trained%20model.%20Besides%2C%20to%20guide%20effective%20adaptation%2C%20we%20propose%20a%20mutual%0Anearest%20neighbor%20local%20feature%20loss%2C%20which%20ensures%20proper%20dense%20local%20features%0Aare%20produced%20for%20local%20matching%20and%20avoids%20time-consuming%20spatial%20verification%0Ain%20re-ranking.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20with%20less%20training%20data%20and%20training%20time%2C%20and%20uses%0Aabout%20only%203%25%20retrieval%20runtime%20of%20the%20two-stage%20VPR%20methods%20with%20RANSAC-based%0Aspatial%20verification.%20It%20ranks%201st%20on%20the%20MSLS%20challenge%20leaderboard%20%28at%20the%0Atime%20of%20submission%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Lu-Feng/SelaVPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14505v1&entry.124074799=Read"},
{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "author": "Yixuan Ren and Yang Zhou and Jimei Yang and Jing Shi and Difan Liu and Feng Liu and Mingi Kwon and Abhinav Shrivastava", "abstract": "  Image customization has been extensively studied in text-to-image (T2I)\ndiffusion models, leading to impressive outcomes and applications. With the\nemergence of text-to-video (T2V) diffusion models, its temporal counterpart,\nmotion customization, has not yet been well investigated. To address the\nchallenge of one-shot motion customization, we propose Customize-A-Video that\nmodels the motion from a single reference video and adapting it to new subjects\nand scenes with both spatial and temporal varieties. It leverages low-rank\nadaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V\ndiffusion model for specific motion modeling from the reference videos. To\ndisentangle the spatial and temporal information during the training pipeline,\nwe introduce a novel concept of appearance absorbers that detach the original\nappearance from the single reference video prior to motion learning. Our\nproposed method can be easily extended to various downstream tasks, including\ncustom video generation and editing, video appearance customization, and\nmultiple motion combination, in a plug-and-play fashion. Our project page can\nbe found at https://anonymous-314.github.io.\n", "link": "http://arxiv.org/abs/2402.14780v1", "date": "2024-02-22", "relevancy": 2.6751, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 1.0}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7215}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customize-A-Video%3A%20One-Shot%20Motion%20Customization%20of%20Text-to-Video%0A%20%20Diffusion%20Models&entry.906535625=Yixuan%20Ren%20and%20Yang%20Zhou%20and%20Jimei%20Yang%20and%20Jing%20Shi%20and%20Difan%20Liu%20and%20Feng%20Liu%20and%20Mingi%20Kwon%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Image%20customization%20has%20been%20extensively%20studied%20in%20text-to-image%20%28T2I%29%0Adiffusion%20models%2C%20leading%20to%20impressive%20outcomes%20and%20applications.%20With%20the%0Aemergence%20of%20text-to-video%20%28T2V%29%20diffusion%20models%2C%20its%20temporal%20counterpart%2C%0Amotion%20customization%2C%20has%20not%20yet%20been%20well%20investigated.%20To%20address%20the%0Achallenge%20of%20one-shot%20motion%20customization%2C%20we%20propose%20Customize-A-Video%20that%0Amodels%20the%20motion%20from%20a%20single%20reference%20video%20and%20adapting%20it%20to%20new%20subjects%0Aand%20scenes%20with%20both%20spatial%20and%20temporal%20varieties.%20It%20leverages%20low-rank%0Aadaptation%20%28LoRA%29%20on%20temporal%20attention%20layers%20to%20tailor%20the%20pre-trained%20T2V%0Adiffusion%20model%20for%20specific%20motion%20modeling%20from%20the%20reference%20videos.%20To%0Adisentangle%20the%20spatial%20and%20temporal%20information%20during%20the%20training%20pipeline%2C%0Awe%20introduce%20a%20novel%20concept%20of%20appearance%20absorbers%20that%20detach%20the%20original%0Aappearance%20from%20the%20single%20reference%20video%20prior%20to%20motion%20learning.%20Our%0Aproposed%20method%20can%20be%20easily%20extended%20to%20various%20downstream%20tasks%2C%20including%0Acustom%20video%20generation%20and%20editing%2C%20video%20appearance%20customization%2C%20and%0Amultiple%20motion%20combination%2C%20in%20a%20plug-and-play%20fashion.%20Our%20project%20page%20can%0Abe%20found%20at%20https%3A//anonymous-314.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14780v1&entry.124074799=Read"},
{"title": "Attention-stacked Generative Adversarial Network (AS-GAN)-empowered\n  Sensor Data Augmentation for Online Monitoring of Manufacturing System", "author": "Yuxuan Li and Chenang Liu", "abstract": "  Machine learning (ML) has been extensively adopted for the online\nsensing-based monitoring in advanced manufacturing systems. However, the sensor\ndata collected under abnormal states are usually insufficient, leading to\nsignificant data imbalanced issue for supervised machine learning. A common\nsolution is to incorporate data augmentation techniques, i.e., augmenting the\navailable abnormal states data (i.e., minority samples) via synthetic\ngeneration. To generate the high-quality minority samples, it is vital to learn\nthe underlying distribution of the abnormal states data. In recent years, the\ngenerative adversarial network (GAN)-based approaches become popular to learn\ndata distribution as well as perform data augmentation. However, in practice,\nthe quality of generated samples from GAN-based data augmentation may vary\ndrastically. In addition, the sensor signals are collected sequentially by time\nfrom the manufacturing systems, which means sequential information is also very\nimportant in data augmentation. To address these limitations, inspired by the\nmulti-head attention mechanism, this paper proposed an attention-stacked GAN\n(AS-GAN) architecture for sensor data augmentation of online monitoring in\nmanufacturing system. It incorporates a new attention-stacked framework to\nstrengthen the generator in GAN with the capability of capturing sequential\ninformation, and thereby the developed attention-stacked framework greatly\nhelps to improve the quality of the generated sensor signals. Afterwards, the\ngenerated high-quality sensor signals for abnormal states could be applied to\ntrain classifiers more accurately, further improving the online monitoring\nperformance of manufacturing systems. The case study conducted in additive\nmanufacturing also successfully validated the effectiveness of the proposed\nAS-GAN.\n", "link": "http://arxiv.org/abs/2306.06268v2", "date": "2024-02-22", "relevancy": 2.6283, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5637}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.495}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-stacked%20Generative%20Adversarial%20Network%20%28AS-GAN%29-empowered%0A%20%20Sensor%20Data%20Augmentation%20for%20Online%20Monitoring%20of%20Manufacturing%20System&entry.906535625=Yuxuan%20Li%20and%20Chenang%20Liu&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20has%20been%20extensively%20adopted%20for%20the%20online%0Asensing-based%20monitoring%20in%20advanced%20manufacturing%20systems.%20However%2C%20the%20sensor%0Adata%20collected%20under%20abnormal%20states%20are%20usually%20insufficient%2C%20leading%20to%0Asignificant%20data%20imbalanced%20issue%20for%20supervised%20machine%20learning.%20A%20common%0Asolution%20is%20to%20incorporate%20data%20augmentation%20techniques%2C%20i.e.%2C%20augmenting%20the%0Aavailable%20abnormal%20states%20data%20%28i.e.%2C%20minority%20samples%29%20via%20synthetic%0Ageneration.%20To%20generate%20the%20high-quality%20minority%20samples%2C%20it%20is%20vital%20to%20learn%0Athe%20underlying%20distribution%20of%20the%20abnormal%20states%20data.%20In%20recent%20years%2C%20the%0Agenerative%20adversarial%20network%20%28GAN%29-based%20approaches%20become%20popular%20to%20learn%0Adata%20distribution%20as%20well%20as%20perform%20data%20augmentation.%20However%2C%20in%20practice%2C%0Athe%20quality%20of%20generated%20samples%20from%20GAN-based%20data%20augmentation%20may%20vary%0Adrastically.%20In%20addition%2C%20the%20sensor%20signals%20are%20collected%20sequentially%20by%20time%0Afrom%20the%20manufacturing%20systems%2C%20which%20means%20sequential%20information%20is%20also%20very%0Aimportant%20in%20data%20augmentation.%20To%20address%20these%20limitations%2C%20inspired%20by%20the%0Amulti-head%20attention%20mechanism%2C%20this%20paper%20proposed%20an%20attention-stacked%20GAN%0A%28AS-GAN%29%20architecture%20for%20sensor%20data%20augmentation%20of%20online%20monitoring%20in%0Amanufacturing%20system.%20It%20incorporates%20a%20new%20attention-stacked%20framework%20to%0Astrengthen%20the%20generator%20in%20GAN%20with%20the%20capability%20of%20capturing%20sequential%0Ainformation%2C%20and%20thereby%20the%20developed%20attention-stacked%20framework%20greatly%0Ahelps%20to%20improve%20the%20quality%20of%20the%20generated%20sensor%20signals.%20Afterwards%2C%20the%0Agenerated%20high-quality%20sensor%20signals%20for%20abnormal%20states%20could%20be%20applied%20to%0Atrain%20classifiers%20more%20accurately%2C%20further%20improving%20the%20online%20monitoring%0Aperformance%20of%20manufacturing%20systems.%20The%20case%20study%20conducted%20in%20additive%0Amanufacturing%20also%20successfully%20validated%20the%20effectiveness%20of%20the%20proposed%0AAS-GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06268v2&entry.124074799=Read"},
{"title": "Scalable Human-Machine Point Cloud Compression", "author": "Mateen Ulhaq and Ivan V. Baji\u0107", "abstract": "  Due to the limited computational capabilities of edge devices, deep learning\ninference can be quite expensive. One remedy is to compress and transmit point\ncloud data over the network for server-side processing. Unfortunately, this\napproach can be sensitive to network factors, including available bitrate.\nLuckily, the bitrate requirements can be reduced without sacrificing inference\naccuracy by using a machine task-specialized codec. In this paper, we present a\nscalable codec for point-cloud data that is specialized for the machine task of\nclassification, while also providing a mechanism for human viewing. In the\nproposed scalable codec, the \"base\" bitstream supports the machine task, and an\n\"enhancement\" bitstream may be used for better input reconstruction performance\nfor human viewing. We base our architecture on PointNet++, and test its\nefficacy on the ModelNet40 dataset. We show significant improvements over prior\nnon-specialized codecs.\n", "link": "http://arxiv.org/abs/2402.12532v2", "date": "2024-02-22", "relevancy": 2.5362, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5422}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4898}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Human-Machine%20Point%20Cloud%20Compression&entry.906535625=Mateen%20Ulhaq%20and%20Ivan%20V.%20Baji%C4%87&entry.1292438233=%20%20Due%20to%20the%20limited%20computational%20capabilities%20of%20edge%20devices%2C%20deep%20learning%0Ainference%20can%20be%20quite%20expensive.%20One%20remedy%20is%20to%20compress%20and%20transmit%20point%0Acloud%20data%20over%20the%20network%20for%20server-side%20processing.%20Unfortunately%2C%20this%0Aapproach%20can%20be%20sensitive%20to%20network%20factors%2C%20including%20available%20bitrate.%0ALuckily%2C%20the%20bitrate%20requirements%20can%20be%20reduced%20without%20sacrificing%20inference%0Aaccuracy%20by%20using%20a%20machine%20task-specialized%20codec.%20In%20this%20paper%2C%20we%20present%20a%0Ascalable%20codec%20for%20point-cloud%20data%20that%20is%20specialized%20for%20the%20machine%20task%20of%0Aclassification%2C%20while%20also%20providing%20a%20mechanism%20for%20human%20viewing.%20In%20the%0Aproposed%20scalable%20codec%2C%20the%20%22base%22%20bitstream%20supports%20the%20machine%20task%2C%20and%20an%0A%22enhancement%22%20bitstream%20may%20be%20used%20for%20better%20input%20reconstruction%20performance%0Afor%20human%20viewing.%20We%20base%20our%20architecture%20on%20PointNet%2B%2B%2C%20and%20test%20its%0Aefficacy%20on%20the%20ModelNet40%20dataset.%20We%20show%20significant%20improvements%20over%20prior%0Anon-specialized%20codecs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12532v2&entry.124074799=Read"},
{"title": "TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via\n  Direct Taylor-based Grid Optimization", "author": "Renyi Mao and Qingshan Xu and Peng Zheng and Ye Wang and Tieru Wu and Rui Ma", "abstract": "  Coordinate-based neural implicit representation or implicit fields have been\nwidely studied for 3D geometry representation or novel view synthesis.\nRecently, a series of efforts have been devoted to accelerating the speed and\nimproving the quality of the coordinate-based implicit field learning. Instead\nof learning heavy MLPs to predict the neural implicit values for the query\ncoordinates, neural voxels or grids combined with shallow MLPs have been\nproposed to achieve high-quality implicit field learning with reduced\noptimization time. On the other hand, lightweight field representations such as\nlinear grid have been proposed to further improve the learning speed. In this\npaper, we aim for both fast and high-quality implicit field learning, and\npropose TaylorGrid, a novel implicit field representation which can be\nefficiently computed via direct Taylor expansion optimization on 2D or 3D\ngrids. As a general representation, TaylorGrid can be adapted to different\nimplicit fields learning tasks such as SDF learning or NeRF. From extensive\nquantitative and qualitative comparisons, TaylorGrid achieves a balance between\nthe linear grid and neural voxels, showing its superiority in fast and\nhigh-quality implicit field learning.\n", "link": "http://arxiv.org/abs/2402.14415v1", "date": "2024-02-22", "relevancy": 2.5315, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5338}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5153}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4698}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaylorGrid%3A%20Towards%20Fast%20and%20High-Quality%20Implicit%20Field%20Learning%20via%0A%20%20Direct%20Taylor-based%20Grid%20Optimization&entry.906535625=Renyi%20Mao%20and%20Qingshan%20Xu%20and%20Peng%20Zheng%20and%20Ye%20Wang%20and%20Tieru%20Wu%20and%20Rui%20Ma&entry.1292438233=%20%20Coordinate-based%20neural%20implicit%20representation%20or%20implicit%20fields%20have%20been%0Awidely%20studied%20for%203D%20geometry%20representation%20or%20novel%20view%20synthesis.%0ARecently%2C%20a%20series%20of%20efforts%20have%20been%20devoted%20to%20accelerating%20the%20speed%20and%0Aimproving%20the%20quality%20of%20the%20coordinate-based%20implicit%20field%20learning.%20Instead%0Aof%20learning%20heavy%20MLPs%20to%20predict%20the%20neural%20implicit%20values%20for%20the%20query%0Acoordinates%2C%20neural%20voxels%20or%20grids%20combined%20with%20shallow%20MLPs%20have%20been%0Aproposed%20to%20achieve%20high-quality%20implicit%20field%20learning%20with%20reduced%0Aoptimization%20time.%20On%20the%20other%20hand%2C%20lightweight%20field%20representations%20such%20as%0Alinear%20grid%20have%20been%20proposed%20to%20further%20improve%20the%20learning%20speed.%20In%20this%0Apaper%2C%20we%20aim%20for%20both%20fast%20and%20high-quality%20implicit%20field%20learning%2C%20and%0Apropose%20TaylorGrid%2C%20a%20novel%20implicit%20field%20representation%20which%20can%20be%0Aefficiently%20computed%20via%20direct%20Taylor%20expansion%20optimization%20on%202D%20or%203D%0Agrids.%20As%20a%20general%20representation%2C%20TaylorGrid%20can%20be%20adapted%20to%20different%0Aimplicit%20fields%20learning%20tasks%20such%20as%20SDF%20learning%20or%20NeRF.%20From%20extensive%0Aquantitative%20and%20qualitative%20comparisons%2C%20TaylorGrid%20achieves%20a%20balance%20between%0Athe%20linear%20grid%20and%20neural%20voxels%2C%20showing%20its%20superiority%20in%20fast%20and%0Ahigh-quality%20implicit%20field%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14415v1&entry.124074799=Read"},
{"title": "Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher\n  Mixture Model", "author": "Jean-R\u00e9my Conti and Nathan Noiry and Vincent Despiegel and St\u00e9phane Gentric and St\u00e9phan Cl\u00e9men\u00e7on", "abstract": "  In spite of the high performance and reliability of deep learning algorithms\nin a wide range of everyday applications, many investigations tend to show that\na lot of models exhibit biases, discriminating against specific subgroups of\nthe population (e.g. gender, ethnicity). This urges the practitioner to develop\nfair systems with a uniform/comparable performance across sensitive groups. In\nthis work, we investigate the gender bias of deep Face Recognition networks. In\norder to measure this bias, we introduce two new metrics, $\\mathrm{BFAR}$ and\n$\\mathrm{BFRR}$, that better reflect the inherent deployment needs of Face\nRecognition systems. Motivated by geometric considerations, we mitigate gender\nbias through a new post-processing methodology which transforms the deep\nembeddings of a pre-trained model to give more representation power to\ndiscriminated subgroups. It consists in training a shallow neural network by\nminimizing a Fair von Mises-Fisher loss whose hyperparameters account for the\nintra-class variance of each gender. Interestingly, we empirically observe that\nthese hyperparameters are correlated with our fairness metrics. In fact,\nextensive numerical experiments on a variety of datasets show that a careful\nselection significantly reduces gender bias. The code used for the experiments\ncan be found at https://github.com/JRConti/EthicalModule_vMF.\n", "link": "http://arxiv.org/abs/2210.13664v3", "date": "2024-02-22", "relevancy": 2.4833, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5137}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5091}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4672}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Gender%20Bias%20in%20Face%20Recognition%20Using%20the%20von%20Mises-Fisher%0A%20%20Mixture%20Model&entry.906535625=Jean-R%C3%A9my%20Conti%20and%20Nathan%20Noiry%20and%20Vincent%20Despiegel%20and%20St%C3%A9phane%20Gentric%20and%20St%C3%A9phan%20Cl%C3%A9men%C3%A7on&entry.1292438233=%20%20In%20spite%20of%20the%20high%20performance%20and%20reliability%20of%20deep%20learning%20algorithms%0Ain%20a%20wide%20range%20of%20everyday%20applications%2C%20many%20investigations%20tend%20to%20show%20that%0Aa%20lot%20of%20models%20exhibit%20biases%2C%20discriminating%20against%20specific%20subgroups%20of%0Athe%20population%20%28e.g.%20gender%2C%20ethnicity%29.%20This%20urges%20the%20practitioner%20to%20develop%0Afair%20systems%20with%20a%20uniform/comparable%20performance%20across%20sensitive%20groups.%20In%0Athis%20work%2C%20we%20investigate%20the%20gender%20bias%20of%20deep%20Face%20Recognition%20networks.%20In%0Aorder%20to%20measure%20this%20bias%2C%20we%20introduce%20two%20new%20metrics%2C%20%24%5Cmathrm%7BBFAR%7D%24%20and%0A%24%5Cmathrm%7BBFRR%7D%24%2C%20that%20better%20reflect%20the%20inherent%20deployment%20needs%20of%20Face%0ARecognition%20systems.%20Motivated%20by%20geometric%20considerations%2C%20we%20mitigate%20gender%0Abias%20through%20a%20new%20post-processing%20methodology%20which%20transforms%20the%20deep%0Aembeddings%20of%20a%20pre-trained%20model%20to%20give%20more%20representation%20power%20to%0Adiscriminated%20subgroups.%20It%20consists%20in%20training%20a%20shallow%20neural%20network%20by%0Aminimizing%20a%20Fair%20von%20Mises-Fisher%20loss%20whose%20hyperparameters%20account%20for%20the%0Aintra-class%20variance%20of%20each%20gender.%20Interestingly%2C%20we%20empirically%20observe%20that%0Athese%20hyperparameters%20are%20correlated%20with%20our%20fairness%20metrics.%20In%20fact%2C%0Aextensive%20numerical%20experiments%20on%20a%20variety%20of%20datasets%20show%20that%20a%20careful%0Aselection%20significantly%20reduces%20gender%20bias.%20The%20code%20used%20for%20the%20experiments%0Acan%20be%20found%20at%20https%3A//github.com/JRConti/EthicalModule_vMF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.13664v3&entry.124074799=Read"},
{"title": "CCPA: Long-term Person Re-Identification via Contrastive Clothing and\n  Pose Augmentation", "author": "Vuong D. Nguyen and Shishir K. Shah", "abstract": "  Long-term Person Re-Identification (LRe-ID) aims at matching an individual\nacross cameras after a long period of time, presenting variations in clothing,\npose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and\nPose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body\nshape information which is cloth-invariant using a Relation Graph Attention\nNetwork. Training a robust LRe-ID model requires a wide range of clothing\nvariations and expensive cloth labeling, which is lacked in current LRe-ID\ndatasets. To address this, we perform clothing and pose transfer across\nidentities to generate images of more clothing variations and of different\npersons wearing similar clothing. The augmented batch of images serve as inputs\nto our proposed Fine-grained Contrastive Losses, which not only supervise the\nRe-ID model to learn discriminative person embeddings under long-term scenarios\nbut also ensure in-distribution data generation. Results on LRe-ID datasets\ndemonstrate the effectiveness of our CCPA framework.\n", "link": "http://arxiv.org/abs/2402.14454v1", "date": "2024-02-22", "relevancy": 2.477, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5246}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4799}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCPA%3A%20Long-term%20Person%20Re-Identification%20via%20Contrastive%20Clothing%20and%0A%20%20Pose%20Augmentation&entry.906535625=Vuong%20D.%20Nguyen%20and%20Shishir%20K.%20Shah&entry.1292438233=%20%20Long-term%20Person%20Re-Identification%20%28LRe-ID%29%20aims%20at%20matching%20an%20individual%0Aacross%20cameras%20after%20a%20long%20period%20of%20time%2C%20presenting%20variations%20in%20clothing%2C%0Apose%2C%20and%20viewpoint.%20In%20this%20work%2C%20we%20propose%20CCPA%3A%20Contrastive%20Clothing%20and%0APose%20Augmentation%20framework%20for%20LRe-ID.%20Beyond%20appearance%2C%20CCPA%20captures%20body%0Ashape%20information%20which%20is%20cloth-invariant%20using%20a%20Relation%20Graph%20Attention%0ANetwork.%20Training%20a%20robust%20LRe-ID%20model%20requires%20a%20wide%20range%20of%20clothing%0Avariations%20and%20expensive%20cloth%20labeling%2C%20which%20is%20lacked%20in%20current%20LRe-ID%0Adatasets.%20To%20address%20this%2C%20we%20perform%20clothing%20and%20pose%20transfer%20across%0Aidentities%20to%20generate%20images%20of%20more%20clothing%20variations%20and%20of%20different%0Apersons%20wearing%20similar%20clothing.%20The%20augmented%20batch%20of%20images%20serve%20as%20inputs%0Ato%20our%20proposed%20Fine-grained%20Contrastive%20Losses%2C%20which%20not%20only%20supervise%20the%0ARe-ID%20model%20to%20learn%20discriminative%20person%20embeddings%20under%20long-term%20scenarios%0Abut%20also%20ensure%20in-distribution%20data%20generation.%20Results%20on%20LRe-ID%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20CCPA%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14454v1&entry.124074799=Read"},
{"title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation", "author": "Kai Cheng and Xiaoxiao Long and Kaizhi Yang and Yao Yao and Wei Yin and Yuexin Ma and Wenping Wang and Xuejin Chen", "abstract": "  The advent of 3D Gaussian Splatting (3DGS) has recently brought about a\nrevolution in the field of neural rendering, facilitating high-quality\nrenderings at real-time speed. However, 3DGS heavily depends on the initialized\npoint cloud produced by Structure-from-Motion (SfM) techniques. When tackling\nwith large-scale scenes that unavoidably contain texture-less surfaces, the SfM\ntechniques always fail to produce enough points in these surfaces and cannot\nprovide good initialization for 3DGS. As a result, 3DGS suffers from difficult\noptimization and low-quality renderings. In this paper, inspired by classical\nmulti-view stereo (MVS) techniques, we propose GaussianPro, a novel method that\napplies a progressive propagation strategy to guide the densification of the 3D\nGaussians. Compared to the simple split and clone strategies used in 3DGS, our\nmethod leverages the priors of the existing reconstructed geometries of the\nscene and patch matching techniques to produce new Gaussians with accurate\npositions and orientations. Experiments on both large-scale and small-scale\nscenes validate the effectiveness of our method, where our method significantly\nsurpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in\nterms of PSNR.\n", "link": "http://arxiv.org/abs/2402.14650v1", "date": "2024-02-22", "relevancy": 2.4738, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5116}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianPro%3A%203D%20Gaussian%20Splatting%20with%20Progressive%20Propagation&entry.906535625=Kai%20Cheng%20and%20Xiaoxiao%20Long%20and%20Kaizhi%20Yang%20and%20Yao%20Yao%20and%20Wei%20Yin%20and%20Yuexin%20Ma%20and%20Wenping%20Wang%20and%20Xuejin%20Chen&entry.1292438233=%20%20The%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20brought%20about%20a%0Arevolution%20in%20the%20field%20of%20neural%20rendering%2C%20facilitating%20high-quality%0Arenderings%20at%20real-time%20speed.%20However%2C%203DGS%20heavily%20depends%20on%20the%20initialized%0Apoint%20cloud%20produced%20by%20Structure-from-Motion%20%28SfM%29%20techniques.%20When%20tackling%0Awith%20large-scale%20scenes%20that%20unavoidably%20contain%20texture-less%20surfaces%2C%20the%20SfM%0Atechniques%20always%20fail%20to%20produce%20enough%20points%20in%20these%20surfaces%20and%20cannot%0Aprovide%20good%20initialization%20for%203DGS.%20As%20a%20result%2C%203DGS%20suffers%20from%20difficult%0Aoptimization%20and%20low-quality%20renderings.%20In%20this%20paper%2C%20inspired%20by%20classical%0Amulti-view%20stereo%20%28MVS%29%20techniques%2C%20we%20propose%20GaussianPro%2C%20a%20novel%20method%20that%0Aapplies%20a%20progressive%20propagation%20strategy%20to%20guide%20the%20densification%20of%20the%203D%0AGaussians.%20Compared%20to%20the%20simple%20split%20and%20clone%20strategies%20used%20in%203DGS%2C%20our%0Amethod%20leverages%20the%20priors%20of%20the%20existing%20reconstructed%20geometries%20of%20the%0Ascene%20and%20patch%20matching%20techniques%20to%20produce%20new%20Gaussians%20with%20accurate%0Apositions%20and%20orientations.%20Experiments%20on%20both%20large-scale%20and%20small-scale%0Ascenes%20validate%20the%20effectiveness%20of%20our%20method%2C%20where%20our%20method%20significantly%0Asurpasses%203DGS%20on%20the%20Waymo%20dataset%2C%20exhibiting%20an%20improvement%20of%201.15dB%20in%0Aterms%20of%20PSNR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14650v1&entry.124074799=Read"},
{"title": "DP-SGD Without Clipping: The Lipschitz Neural Network Way", "author": "Louis Bethune and Thomas Massena and Thibaut Boissin and Yannick Prudent and Corentin Friedrich and Franck Mamalet and Aurelien Bellet and Mathieu Serrurier and David Vigouroux", "abstract": "  State-of-the-art approaches for training Differentially Private (DP) Deep\nNeural Networks (DNN) face difficulties to estimate tight bounds on the\nsensitivity of the network's layers, and instead rely on a process of\nper-sample gradient clipping. This clipping process not only biases the\ndirection of gradients but also proves costly both in memory consumption and in\ncomputation. To provide sensitivity bounds and bypass the drawbacks of the\nclipping process, we propose to rely on Lipschitz constrained networks. Our\ntheoretical analysis reveals an unexplored link between the Lipschitz constant\nwith respect to their input and the one with respect to their parameters. By\nbounding the Lipschitz constant of each layer with respect to its parameters,\nwe prove that we can train these networks with privacy guarantees. Our analysis\nnot only allows the computation of the aforementioned sensitivities at scale,\nbut also provides guidance on how to maximize the gradient-to-noise ratio for\nfixed privacy guarantees. The code has been released as a Python package\navailable at https://github.com/Algue-Rythme/lip-dp\n", "link": "http://arxiv.org/abs/2305.16202v2", "date": "2024-02-22", "relevancy": 2.4501, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4856}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4541}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-SGD%20Without%20Clipping%3A%20The%20Lipschitz%20Neural%20Network%20Way&entry.906535625=Louis%20Bethune%20and%20Thomas%20Massena%20and%20Thibaut%20Boissin%20and%20Yannick%20Prudent%20and%20Corentin%20Friedrich%20and%20Franck%20Mamalet%20and%20Aurelien%20Bellet%20and%20Mathieu%20Serrurier%20and%20David%20Vigouroux&entry.1292438233=%20%20State-of-the-art%20approaches%20for%20training%20Differentially%20Private%20%28DP%29%20Deep%0ANeural%20Networks%20%28DNN%29%20face%20difficulties%20to%20estimate%20tight%20bounds%20on%20the%0Asensitivity%20of%20the%20network%27s%20layers%2C%20and%20instead%20rely%20on%20a%20process%20of%0Aper-sample%20gradient%20clipping.%20This%20clipping%20process%20not%20only%20biases%20the%0Adirection%20of%20gradients%20but%20also%20proves%20costly%20both%20in%20memory%20consumption%20and%20in%0Acomputation.%20To%20provide%20sensitivity%20bounds%20and%20bypass%20the%20drawbacks%20of%20the%0Aclipping%20process%2C%20we%20propose%20to%20rely%20on%20Lipschitz%20constrained%20networks.%20Our%0Atheoretical%20analysis%20reveals%20an%20unexplored%20link%20between%20the%20Lipschitz%20constant%0Awith%20respect%20to%20their%20input%20and%20the%20one%20with%20respect%20to%20their%20parameters.%20By%0Abounding%20the%20Lipschitz%20constant%20of%20each%20layer%20with%20respect%20to%20its%20parameters%2C%0Awe%20prove%20that%20we%20can%20train%20these%20networks%20with%20privacy%20guarantees.%20Our%20analysis%0Anot%20only%20allows%20the%20computation%20of%20the%20aforementioned%20sensitivities%20at%20scale%2C%0Abut%20also%20provides%20guidance%20on%20how%20to%20maximize%20the%20gradient-to-noise%20ratio%20for%0Afixed%20privacy%20guarantees.%20The%20code%20has%20been%20released%20as%20a%20Python%20package%0Aavailable%20at%20https%3A//github.com/Algue-Rythme/lip-dp%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16202v2&entry.124074799=Read"},
{"title": "High-Speed Detector For Low-Powered Devices In Aerial Grasping", "author": "Ashish Kumar and Laxmidhar Behera", "abstract": "  Autonomous aerial harvesting is a highly complex problem because it requires\nnumerous interdisciplinary algorithms to be executed on mini low-powered\ncomputing devices. Object detection is one such algorithm that is\ncompute-hungry. In this context, we make the following contributions: (i) Fast\nFruit Detector (FFD), a resource-efficient, single-stage, and\npostprocessing-free object detector based on our novel latent object\nrepresentation (LOR) module, query assignment, and prediction strategy. FFD\nachieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded\ndevice while co-existing with other time-critical sub-systems such as control,\ngrasping, SLAM, a major achievement of this work. (ii) a method to generate\nvast amounts of training data without exhaustive manual labelling of fruit\nimages since they consist of a large number of instances, which increases the\nlabelling cost and time. (iii) an open-source fruit detection dataset having\nplenty of very small-sized instances that are difficult to detect. Our\nexhaustive evaluations on our and MinneApple dataset show that FFD, being only\na single-scale detector, is more accurate than many representative detectors,\ne.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale\nFaster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and\nmulti-scale YOLO-v8 by 0.3 while being considerably faster.\n", "link": "http://arxiv.org/abs/2402.14591v1", "date": "2024-02-22", "relevancy": 2.4445, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5005}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4892}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.477}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Speed%20Detector%20For%20Low-Powered%20Devices%20In%20Aerial%20Grasping&entry.906535625=Ashish%20Kumar%20and%20Laxmidhar%20Behera&entry.1292438233=%20%20Autonomous%20aerial%20harvesting%20is%20a%20highly%20complex%20problem%20because%20it%20requires%0Anumerous%20interdisciplinary%20algorithms%20to%20be%20executed%20on%20mini%20low-powered%0Acomputing%20devices.%20Object%20detection%20is%20one%20such%20algorithm%20that%20is%0Acompute-hungry.%20In%20this%20context%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20Fast%0AFruit%20Detector%20%28FFD%29%2C%20a%20resource-efficient%2C%20single-stage%2C%20and%0Apostprocessing-free%20object%20detector%20based%20on%20our%20novel%20latent%20object%0Arepresentation%20%28LOR%29%20module%2C%20query%20assignment%2C%20and%20prediction%20strategy.%20FFD%0Aachieves%20100FPS%40FP32%20precision%20on%20the%20latest%2010W%20NVIDIA%20Jetson-NX%20embedded%0Adevice%20while%20co-existing%20with%20other%20time-critical%20sub-systems%20such%20as%20control%2C%0Agrasping%2C%20SLAM%2C%20a%20major%20achievement%20of%20this%20work.%20%28ii%29%20a%20method%20to%20generate%0Avast%20amounts%20of%20training%20data%20without%20exhaustive%20manual%20labelling%20of%20fruit%0Aimages%20since%20they%20consist%20of%20a%20large%20number%20of%20instances%2C%20which%20increases%20the%0Alabelling%20cost%20and%20time.%20%28iii%29%20an%20open-source%20fruit%20detection%20dataset%20having%0Aplenty%20of%20very%20small-sized%20instances%20that%20are%20difficult%20to%20detect.%20Our%0Aexhaustive%20evaluations%20on%20our%20and%20MinneApple%20dataset%20show%20that%20FFD%2C%20being%20only%0Aa%20single-scale%20detector%2C%20is%20more%20accurate%20than%20many%20representative%20detectors%2C%0Ae.g.%20FFD%20is%20better%20than%20single-scale%20Faster-RCNN%20by%2010.7AP%2C%20multi-scale%0AFaster-RCNN%20by%202.3AP%2C%20and%20better%20than%20latest%20single-scale%20YOLO-v8%20by%208AP%20and%0Amulti-scale%20YOLO-v8%20by%200.3%20while%20being%20considerably%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14591v1&entry.124074799=Read"},
{"title": "How Transformers Learn Causal Structure with Gradient Descent", "author": "Eshaan Nichani and Alex Damian and Jason D. Lee", "abstract": "  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n", "link": "http://arxiv.org/abs/2402.14735v1", "date": "2024-02-22", "relevancy": 2.408, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4867}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4816}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4765}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Learn%20Causal%20Structure%20with%20Gradient%20Descent&entry.906535625=Eshaan%20Nichani%20and%20Alex%20Damian%20and%20Jason%20D.%20Lee&entry.1292438233=%20%20The%20incredible%20success%20of%20transformers%20on%20sequence%20modeling%20tasks%20can%20be%0Alargely%20attributed%20to%20the%20self-attention%20mechanism%2C%20which%20allows%20information%20to%0Abe%20transferred%20between%20different%20parts%20of%20a%20sequence.%20Self-attention%20allows%0Atransformers%20to%20encode%20causal%20structure%20which%20makes%20them%20particularly%20suitable%0Afor%20sequence%20modeling.%20However%2C%20the%20process%20by%20which%20transformers%20learn%20such%0Acausal%20structure%20via%20gradient-based%20training%20algorithms%20remains%20poorly%0Aunderstood.%20To%20better%20understand%20this%20process%2C%20we%20introduce%20an%20in-context%0Alearning%20task%20that%20requires%20learning%20latent%20causal%20structure.%20We%20prove%20that%0Agradient%20descent%20on%20a%20simplified%20two-layer%20transformer%20learns%20to%20solve%20this%0Atask%20by%20encoding%20the%20latent%20causal%20graph%20in%20the%20first%20attention%20layer.%20The%20key%0Ainsight%20of%20our%20proof%20is%20that%20the%20gradient%20of%20the%20attention%20matrix%20encodes%20the%0Amutual%20information%20between%20tokens.%20As%20a%20consequence%20of%20the%20data%20processing%0Ainequality%2C%20the%20largest%20entries%20of%20this%20gradient%20correspond%20to%20edges%20in%20the%0Alatent%20causal%20graph.%20As%20a%20special%20case%2C%20when%20the%20sequences%20are%20generated%20from%0Ain-context%20Markov%20chains%2C%20we%20prove%20that%20transformers%20learn%20an%20induction%20head%0A%28Olsson%20et%20al.%2C%202022%29.%20We%20confirm%20our%20theoretical%20findings%20by%20showing%20that%0Atransformers%20trained%20on%20our%20in-context%20learning%20task%20are%20able%20to%20recover%20a%20wide%0Avariety%20of%20causal%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14735v1&entry.124074799=Read"},
{"title": "Clifford-Steerable Convolutional Neural Networks", "author": "Maksim Zhdanov and David Ruhe and Maurice Weiler and Ana Lucic and Johannes Brandstetter and Patrick Forr\u00e9", "abstract": "  We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a\nnovel class of $\\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector\nfields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance,\n$\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\\'e-equivariance on\nMinkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit\nparametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group\nequivariant neural networks. We significantly and consistently outperform\nbaseline methods on fluid dynamics as well as relativistic electrodynamics\nforecasting tasks.\n", "link": "http://arxiv.org/abs/2402.14730v1", "date": "2024-02-22", "relevancy": 2.4004, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4766}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4731}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clifford-Steerable%20Convolutional%20Neural%20Networks&entry.906535625=Maksim%20Zhdanov%20and%20David%20Ruhe%20and%20Maurice%20Weiler%20and%20Ana%20Lucic%20and%20Johannes%20Brandstetter%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20We%20present%20Clifford-Steerable%20Convolutional%20Neural%20Networks%20%28CS-CNNs%29%2C%20a%0Anovel%20class%20of%20%24%5Cmathrm%7BE%7D%28p%2C%20q%29%24-equivariant%20CNNs.%20CS-CNNs%20process%20multivector%0Afields%20on%20pseudo-Euclidean%20spaces%20%24%5Cmathbb%7BR%7D%5E%7Bp%2Cq%7D%24.%20They%20cover%2C%20for%20instance%2C%0A%24%5Cmathrm%7BE%7D%283%29%24-equivariance%20on%20%24%5Cmathbb%7BR%7D%5E3%24%20and%20Poincar%5C%27e-equivariance%20on%0AMinkowski%20spacetime%20%24%5Cmathbb%7BR%7D%5E%7B1%2C3%7D%24.%20Our%20approach%20is%20based%20on%20an%20implicit%0Aparametrization%20of%20%24%5Cmathrm%7BO%7D%28p%2Cq%29%24-steerable%20kernels%20via%20Clifford%20group%0Aequivariant%20neural%20networks.%20We%20significantly%20and%20consistently%20outperform%0Abaseline%20methods%20on%20fluid%20dynamics%20as%20well%20as%20relativistic%20electrodynamics%0Aforecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14730v1&entry.124074799=Read"},
{"title": "SENet: A Spectral Filtering Approach to Represent Exemplars for Few-shot\n  Learning", "author": "Tao Zhang and Wu Huang", "abstract": "  Prototype is widely used to represent internal structure of category for\nfew-shot learning, which was proposed as a simple inductive bias to address the\nissue of overfitting. However, since prototype representation is normally\naveraged from individual samples, it can appropriately to represent some\nclasses but with underfitting to represent some others that can be batter\nrepresented by exemplars. To address this problem, in this work, we propose\nShrinkage Exemplar Networks (SENet) for few-shot classification. In SENet,\ncategories are represented by the embedding of samples that shrink towards\ntheir mean via spectral filtering. Furthermore, a shrinkage exemplar loss is\nproposed to replace the widely used cross entropy loss for capturing the\ninformation of individual shrinkage samples. Several experiments were conducted\non miniImageNet, tiered-ImageNet and CIFAR-FS datasets. The experimental\nresults demonstrate the effectiveness of our proposed method.\n", "link": "http://arxiv.org/abs/2305.18970v2", "date": "2024-02-22", "relevancy": 2.3877, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4939}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4704}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4684}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SENet%3A%20A%20Spectral%20Filtering%20Approach%20to%20Represent%20Exemplars%20for%20Few-shot%0A%20%20Learning&entry.906535625=Tao%20Zhang%20and%20Wu%20Huang&entry.1292438233=%20%20Prototype%20is%20widely%20used%20to%20represent%20internal%20structure%20of%20category%20for%0Afew-shot%20learning%2C%20which%20was%20proposed%20as%20a%20simple%20inductive%20bias%20to%20address%20the%0Aissue%20of%20overfitting.%20However%2C%20since%20prototype%20representation%20is%20normally%0Aaveraged%20from%20individual%20samples%2C%20it%20can%20appropriately%20to%20represent%20some%0Aclasses%20but%20with%20underfitting%20to%20represent%20some%20others%20that%20can%20be%20batter%0Arepresented%20by%20exemplars.%20To%20address%20this%20problem%2C%20in%20this%20work%2C%20we%20propose%0AShrinkage%20Exemplar%20Networks%20%28SENet%29%20for%20few-shot%20classification.%20In%20SENet%2C%0Acategories%20are%20represented%20by%20the%20embedding%20of%20samples%20that%20shrink%20towards%0Atheir%20mean%20via%20spectral%20filtering.%20Furthermore%2C%20a%20shrinkage%20exemplar%20loss%20is%0Aproposed%20to%20replace%20the%20widely%20used%20cross%20entropy%20loss%20for%20capturing%20the%0Ainformation%20of%20individual%20shrinkage%20samples.%20Several%20experiments%20were%20conducted%0Aon%20miniImageNet%2C%20tiered-ImageNet%20and%20CIFAR-FS%20datasets.%20The%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18970v2&entry.124074799=Read"},
{"title": "A Survey on Global LiDAR Localization: Challenges, Advances and Open\n  Problems", "author": "Huan Yin and Xuecheng Xu and Sha Lu and Xieyuanli Chen and Rong Xiong and Shaojie Shen and Cyrill Stachniss and Yue Wang", "abstract": "  Knowledge about the own pose is key for all mobile robot applications. Thus\npose estimation is part of the core functionalities of mobile robots. Over the\nlast two decades, LiDAR scanners have become the standard sensor for robot\nlocalization and mapping. This article aims to provide an overview of recent\nprogress and advancements in LiDAR-based global localization. We begin by\nformulating the problem and exploring the application scope. We then present a\nreview of the methodology, including recent advancements in several topics,\nsuch as maps, descriptor extraction, and cross-robot localization. The contents\nof the article are organized under three themes. The first theme concerns the\ncombination of global place retrieval and local pose estimation. The second\ntheme is upgrading single-shot measurements to sequential ones for sequential\nglobal localization. Finally, the third theme focuses on extending single-robot\nglobal localization to cross-robot localization in multi-robot systems. We\nconclude the survey with a discussion of open challenges and promising\ndirections in global LiDAR localization. To our best knowledge, this is the\nfirst comprehensive survey on global LiDAR localization for mobile robots.\n", "link": "http://arxiv.org/abs/2302.07433v4", "date": "2024-02-23", "relevancy": 2.3495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5784}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Global%20LiDAR%20Localization%3A%20Challenges%2C%20Advances%20and%20Open%0A%20%20Problems&entry.906535625=Huan%20Yin%20and%20Xuecheng%20Xu%20and%20Sha%20Lu%20and%20Xieyuanli%20Chen%20and%20Rong%20Xiong%20and%20Shaojie%20Shen%20and%20Cyrill%20Stachniss%20and%20Yue%20Wang&entry.1292438233=%20%20Knowledge%20about%20the%20own%20pose%20is%20key%20for%20all%20mobile%20robot%20applications.%20Thus%0Apose%20estimation%20is%20part%20of%20the%20core%20functionalities%20of%20mobile%20robots.%20Over%20the%0Alast%20two%20decades%2C%20LiDAR%20scanners%20have%20become%20the%20standard%20sensor%20for%20robot%0Alocalization%20and%20mapping.%20This%20article%20aims%20to%20provide%20an%20overview%20of%20recent%0Aprogress%20and%20advancements%20in%20LiDAR-based%20global%20localization.%20We%20begin%20by%0Aformulating%20the%20problem%20and%20exploring%20the%20application%20scope.%20We%20then%20present%20a%0Areview%20of%20the%20methodology%2C%20including%20recent%20advancements%20in%20several%20topics%2C%0Asuch%20as%20maps%2C%20descriptor%20extraction%2C%20and%20cross-robot%20localization.%20The%20contents%0Aof%20the%20article%20are%20organized%20under%20three%20themes.%20The%20first%20theme%20concerns%20the%0Acombination%20of%20global%20place%20retrieval%20and%20local%20pose%20estimation.%20The%20second%0Atheme%20is%20upgrading%20single-shot%20measurements%20to%20sequential%20ones%20for%20sequential%0Aglobal%20localization.%20Finally%2C%20the%20third%20theme%20focuses%20on%20extending%20single-robot%0Aglobal%20localization%20to%20cross-robot%20localization%20in%20multi-robot%20systems.%20We%0Aconclude%20the%20survey%20with%20a%20discussion%20of%20open%20challenges%20and%20promising%0Adirections%20in%20global%20LiDAR%20localization.%20To%20our%20best%20knowledge%2C%20this%20is%20the%0Afirst%20comprehensive%20survey%20on%20global%20LiDAR%20localization%20for%20mobile%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07433v4&entry.124074799=Read"},
{"title": "DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based\n  Graph Continual Learning", "author": "Seungyoon Choi and Wonjoong Kim and Sungwon Kim and Yeonjun In and Sein Kim and Chanyoung Park", "abstract": "  We investigate the replay buffer in rehearsal-based approaches for graph\ncontinual learning (GCL) methods. Existing rehearsal-based GCL methods select\nthe most representative nodes for each class and store them in a replay buffer\nfor later use in training subsequent tasks. However, we discovered that\nconsidering only the class representativeness of each replayed node makes the\nreplayed nodes to be concentrated around the center of each class, incurring a\npotential risk of overfitting to nodes residing in those regions, which\naggravates catastrophic forgetting. Moreover, as the rehearsal-based approach\nheavily relies on a few replayed nodes to retain knowledge obtained from\nprevious tasks, involving the replayed nodes that have irrelevant neighbors in\nthe model training may have a significant detrimental impact on model\nperformance. In this paper, we propose a GCL model named DSLR, specifically, we\ndevise a coverage-based diversity (CD) approach to consider both the class\nrepresentativeness and the diversity within each class of the replayed nodes.\nMoreover, we adopt graph structure learning (GSL) to ensure that the replayed\nnodes are connected to truly informative neighbors. Extensive experimental\nresults demonstrate the effectiveness and efficiency of DSLR. Our source code\nis available at https://github.com/seungyoon-Choi/DSLR_official.\n", "link": "http://arxiv.org/abs/2402.13711v3", "date": "2024-02-23", "relevancy": 2.3386, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4866}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4604}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4561}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSLR%3A%20Diversity%20Enhancement%20and%20Structure%20Learning%20for%20Rehearsal-based%0A%20%20Graph%20Continual%20Learning&entry.906535625=Seungyoon%20Choi%20and%20Wonjoong%20Kim%20and%20Sungwon%20Kim%20and%20Yeonjun%20In%20and%20Sein%20Kim%20and%20Chanyoung%20Park&entry.1292438233=%20%20We%20investigate%20the%20replay%20buffer%20in%20rehearsal-based%20approaches%20for%20graph%0Acontinual%20learning%20%28GCL%29%20methods.%20Existing%20rehearsal-based%20GCL%20methods%20select%0Athe%20most%20representative%20nodes%20for%20each%20class%20and%20store%20them%20in%20a%20replay%20buffer%0Afor%20later%20use%20in%20training%20subsequent%20tasks.%20However%2C%20we%20discovered%20that%0Aconsidering%20only%20the%20class%20representativeness%20of%20each%20replayed%20node%20makes%20the%0Areplayed%20nodes%20to%20be%20concentrated%20around%20the%20center%20of%20each%20class%2C%20incurring%20a%0Apotential%20risk%20of%20overfitting%20to%20nodes%20residing%20in%20those%20regions%2C%20which%0Aaggravates%20catastrophic%20forgetting.%20Moreover%2C%20as%20the%20rehearsal-based%20approach%0Aheavily%20relies%20on%20a%20few%20replayed%20nodes%20to%20retain%20knowledge%20obtained%20from%0Aprevious%20tasks%2C%20involving%20the%20replayed%20nodes%20that%20have%20irrelevant%20neighbors%20in%0Athe%20model%20training%20may%20have%20a%20significant%20detrimental%20impact%20on%20model%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20a%20GCL%20model%20named%20DSLR%2C%20specifically%2C%20we%0Adevise%20a%20coverage-based%20diversity%20%28CD%29%20approach%20to%20consider%20both%20the%20class%0Arepresentativeness%20and%20the%20diversity%20within%20each%20class%20of%20the%20replayed%20nodes.%0AMoreover%2C%20we%20adopt%20graph%20structure%20learning%20%28GSL%29%20to%20ensure%20that%20the%20replayed%0Anodes%20are%20connected%20to%20truly%20informative%20neighbors.%20Extensive%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20DSLR.%20Our%20source%20code%0Ais%20available%20at%20https%3A//github.com/seungyoon-Choi/DSLR_official.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13711v3&entry.124074799=Read"},
{"title": "Uncertainty-Aware Evaluation for Vision-Language Models", "author": "Vasily Kostumov and Bulat Nutfullin and Oleg Pilipenko and Eugene Ilyushin", "abstract": "  Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in\npopularity recently due to their impressive performance in several\nvision-language tasks. Current evaluation methods, however, overlook an\nessential component: uncertainty, which is crucial for a comprehensive\nassessment of VLMs. Addressing this oversight, we present a benchmark\nincorporating uncertainty quantification into evaluating VLMs.\n  Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question\nAnswering (VQA) task. We examine models on 5 datasets that evaluate various\nvision-language capabilities.\n  Using conformal prediction as an uncertainty estimation approach, we\ndemonstrate that the models' uncertainty is not aligned with their accuracy.\nSpecifically, we show that models with the highest accuracy may also have the\nhighest uncertainty, which confirms the importance of measuring it for VLMs.\nOur empirical findings also reveal a correlation between model uncertainty and\nits language model part.\n", "link": "http://arxiv.org/abs/2402.14418v1", "date": "2024-02-22", "relevancy": 2.3296, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6048}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Evaluation%20for%20Vision-Language%20Models&entry.906535625=Vasily%20Kostumov%20and%20Bulat%20Nutfullin%20and%20Oleg%20Pilipenko%20and%20Eugene%20Ilyushin&entry.1292438233=%20%20Vision-Language%20Models%20like%20GPT-4%2C%20LLaVA%2C%20and%20CogVLM%20have%20surged%20in%0Apopularity%20recently%20due%20to%20their%20impressive%20performance%20in%20several%0Avision-language%20tasks.%20Current%20evaluation%20methods%2C%20however%2C%20overlook%20an%0Aessential%20component%3A%20uncertainty%2C%20which%20is%20crucial%20for%20a%20comprehensive%0Aassessment%20of%20VLMs.%20Addressing%20this%20oversight%2C%20we%20present%20a%20benchmark%0Aincorporating%20uncertainty%20quantification%20into%20evaluating%20VLMs.%0A%20%20Our%20analysis%20spans%2020%2B%20VLMs%2C%20focusing%20on%20the%20multiple-choice%20Visual%20Question%0AAnswering%20%28VQA%29%20task.%20We%20examine%20models%20on%205%20datasets%20that%20evaluate%20various%0Avision-language%20capabilities.%0A%20%20Using%20conformal%20prediction%20as%20an%20uncertainty%20estimation%20approach%2C%20we%0Ademonstrate%20that%20the%20models%27%20uncertainty%20is%20not%20aligned%20with%20their%20accuracy.%0ASpecifically%2C%20we%20show%20that%20models%20with%20the%20highest%20accuracy%20may%20also%20have%20the%0Ahighest%20uncertainty%2C%20which%20confirms%20the%20importance%20of%20measuring%20it%20for%20VLMs.%0AOur%20empirical%20findings%20also%20reveal%20a%20correlation%20between%20model%20uncertainty%20and%0Aits%20language%20model%20part.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14418v1&entry.124074799=Read"},
{"title": "Testing Spacecraft Formation Flying with Crazyflie Drones as Satellite\n  Surrogates", "author": "Arturo de la Barcena and Collin Rhodes and John McCarroll and Marzia Cescon and Kerianne L. Hobbs", "abstract": "  As the space domain becomes increasingly congested, autonomy is proposed as\none approach to enable small numbers of human ground operators to manage large\nconstellations of satellites and tackle more complex missions such as on-orbit\nor in-space servicing, assembly, and manufacturing. One of the biggest\nchallenges in developing novel spacecraft autonomy is mechanisms to test and\nevaluate their performance. Testing spacecraft autonomy on-orbit can be high\nrisk and prohibitively expensive. An alternative method is to test autonomy\nterrestrially using satellite surrogates such as attitude test beds on air\nbearings or drones for translational motion visualization. Against this\nbackground, this work develops an approach to evaluate autonomous spacecraft\nbehavior using a surrogate platform, namely a micro-quadcopter drone developed\nby the Bitcraze team, the Crazyflie 2.1. The Crazyflie drones are increasingly\nbecoming ubiquitous in flight testing labs because they are affordable, open\nsource, readily available, and include expansion decks which allow for features\nsuch as positioning systems, distance and/or motion sensors, wireless charging,\nand AI capabilities. In this paper, models of Crazyflie drones are used to\nsimulate the relative motion dynamics of spacecraft under linearized\nClohessy-Wiltshire dynamics in elliptical natural motion trajectories, in\npre-generated docking trajectories, and via trajectories output by neural\nnetwork control systems.\n", "link": "http://arxiv.org/abs/2402.14750v1", "date": "2024-02-22", "relevancy": 2.3021, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4821}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4522}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4469}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20Spacecraft%20Formation%20Flying%20with%20Crazyflie%20Drones%20as%20Satellite%0A%20%20Surrogates&entry.906535625=Arturo%20de%20la%20Barcena%20and%20Collin%20Rhodes%20and%20John%20McCarroll%20and%20Marzia%20Cescon%20and%20Kerianne%20L.%20Hobbs&entry.1292438233=%20%20As%20the%20space%20domain%20becomes%20increasingly%20congested%2C%20autonomy%20is%20proposed%20as%0Aone%20approach%20to%20enable%20small%20numbers%20of%20human%20ground%20operators%20to%20manage%20large%0Aconstellations%20of%20satellites%20and%20tackle%20more%20complex%20missions%20such%20as%20on-orbit%0Aor%20in-space%20servicing%2C%20assembly%2C%20and%20manufacturing.%20One%20of%20the%20biggest%0Achallenges%20in%20developing%20novel%20spacecraft%20autonomy%20is%20mechanisms%20to%20test%20and%0Aevaluate%20their%20performance.%20Testing%20spacecraft%20autonomy%20on-orbit%20can%20be%20high%0Arisk%20and%20prohibitively%20expensive.%20An%20alternative%20method%20is%20to%20test%20autonomy%0Aterrestrially%20using%20satellite%20surrogates%20such%20as%20attitude%20test%20beds%20on%20air%0Abearings%20or%20drones%20for%20translational%20motion%20visualization.%20Against%20this%0Abackground%2C%20this%20work%20develops%20an%20approach%20to%20evaluate%20autonomous%20spacecraft%0Abehavior%20using%20a%20surrogate%20platform%2C%20namely%20a%20micro-quadcopter%20drone%20developed%0Aby%20the%20Bitcraze%20team%2C%20the%20Crazyflie%202.1.%20The%20Crazyflie%20drones%20are%20increasingly%0Abecoming%20ubiquitous%20in%20flight%20testing%20labs%20because%20they%20are%20affordable%2C%20open%0Asource%2C%20readily%20available%2C%20and%20include%20expansion%20decks%20which%20allow%20for%20features%0Asuch%20as%20positioning%20systems%2C%20distance%20and/or%20motion%20sensors%2C%20wireless%20charging%2C%0Aand%20AI%20capabilities.%20In%20this%20paper%2C%20models%20of%20Crazyflie%20drones%20are%20used%20to%0Asimulate%20the%20relative%20motion%20dynamics%20of%20spacecraft%20under%20linearized%0AClohessy-Wiltshire%20dynamics%20in%20elliptical%20natural%20motion%20trajectories%2C%20in%0Apre-generated%20docking%20trajectories%2C%20and%20via%20trajectories%20output%20by%20neural%0Anetwork%20control%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14750v1&entry.124074799=Read"},
{"title": "FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single\n  Annotated Example Object", "author": "Hanzhi Chen and Binbin Xu and Stefan Leutenegger", "abstract": "  We present FuncGrasp, a framework that can infer dense yet reliable grasp\nconfigurations for unseen objects using one annotated object and single-view\nRGB-D observation via categorical priors. Unlike previous works that only\ntransfer a set of grasp poses, FuncGrasp aims to transfer infinite\nconfigurations parameterized by an object-centric continuous grasp function\nacross varying instances. To ease the transfer process, we propose Neural\nSurface Grasping Fields (NSGF), an effective neural representation defined on\nthe surface to densely encode grasp configurations. Further, we exploit\nfunction-to-function transfer using sphere primitives to establish semantically\nmeaningful categorical correspondences, which are learned in an unsupervised\nfashion without any expert knowledge. We showcase the effectiveness through\nextensive experiments in both simulators and the real world. Remarkably, our\nframework significantly outperforms several strong baseline methods in terms of\ndensity and reliability for generated grasps.\n", "link": "http://arxiv.org/abs/2402.05644v2", "date": "2024-02-22", "relevancy": 2.3001, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4938}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FuncGrasp%3A%20Learning%20Object-Centric%20Neural%20Grasp%20Functions%20from%20Single%0A%20%20Annotated%20Example%20Object&entry.906535625=Hanzhi%20Chen%20and%20Binbin%20Xu%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20We%20present%20FuncGrasp%2C%20a%20framework%20that%20can%20infer%20dense%20yet%20reliable%20grasp%0Aconfigurations%20for%20unseen%20objects%20using%20one%20annotated%20object%20and%20single-view%0ARGB-D%20observation%20via%20categorical%20priors.%20Unlike%20previous%20works%20that%20only%0Atransfer%20a%20set%20of%20grasp%20poses%2C%20FuncGrasp%20aims%20to%20transfer%20infinite%0Aconfigurations%20parameterized%20by%20an%20object-centric%20continuous%20grasp%20function%0Aacross%20varying%20instances.%20To%20ease%20the%20transfer%20process%2C%20we%20propose%20Neural%0ASurface%20Grasping%20Fields%20%28NSGF%29%2C%20an%20effective%20neural%20representation%20defined%20on%0Athe%20surface%20to%20densely%20encode%20grasp%20configurations.%20Further%2C%20we%20exploit%0Afunction-to-function%20transfer%20using%20sphere%20primitives%20to%20establish%20semantically%0Ameaningful%20categorical%20correspondences%2C%20which%20are%20learned%20in%20an%20unsupervised%0Afashion%20without%20any%20expert%20knowledge.%20We%20showcase%20the%20effectiveness%20through%0Aextensive%20experiments%20in%20both%20simulators%20and%20the%20real%20world.%20Remarkably%2C%20our%0Aframework%20significantly%20outperforms%20several%20strong%20baseline%20methods%20in%20terms%20of%0Adensity%20and%20reliability%20for%20generated%20grasps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05644v2&entry.124074799=Read"},
{"title": "An Identity-Preserved Framework for Human Motion Transfer", "author": "Jingzhe Ma and Xiaoqing Zhang and Shiqi Yu", "abstract": "  Human motion transfer (HMT) aims to generate a video clip for the target\nsubject by imitating the source subject's motion. Although previous methods\nhave achieved good results in synthesizing good-quality videos, they lose sight\nof individualized motion information from the source and target motions, which\nis significant for the realism of the motion in the generated video. To address\nthis problem, we propose a novel identity-preserved HMT network, termed\n\\textit{IDPres}. This network is a skeleton-based approach that uniquely\nincorporates the target's individualized motion and skeleton information to\naugment identity representations. This integration significantly enhances the\nrealism of movements in the generated videos. Our method focuses on the\nfine-grained disentanglement and synthesis of motion. To improve the\nrepresentation learning capability in latent space and facilitate the training\nof \\textit{IDPres}, we introduce three training schemes. These schemes enable\n\\textit{IDPres} to concurrently disentangle different representations and\naccurately control them, ensuring the synthesis of ideal motions. To evaluate\nthe proportion of individualized motion information in the generated video, we\nare the first to introduce a new quantitative metric called Identity Score\n(\\textit{ID-Score}), motivated by the success of gait recognition methods in\ncapturing identity information. Moreover, we collect an identity-motion paired\ndataset, $Dancer101$, consisting of solo-dance videos of 101 subjects from the\npublic domain, providing a benchmark to prompt the development of HMT methods.\nExtensive experiments demonstrate that the proposed \\textit{IDPres} method\nsurpasses existing state-of-the-art techniques in terms of reconstruction\naccuracy, realistic motion, and identity preservation.\n", "link": "http://arxiv.org/abs/2204.06862v3", "date": "2024-02-22", "relevancy": 2.285, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6404}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5936}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4931}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Identity-Preserved%20Framework%20for%20Human%20Motion%20Transfer&entry.906535625=Jingzhe%20Ma%20and%20Xiaoqing%20Zhang%20and%20Shiqi%20Yu&entry.1292438233=%20%20Human%20motion%20transfer%20%28HMT%29%20aims%20to%20generate%20a%20video%20clip%20for%20the%20target%0Asubject%20by%20imitating%20the%20source%20subject%27s%20motion.%20Although%20previous%20methods%0Ahave%20achieved%20good%20results%20in%20synthesizing%20good-quality%20videos%2C%20they%20lose%20sight%0Aof%20individualized%20motion%20information%20from%20the%20source%20and%20target%20motions%2C%20which%0Ais%20significant%20for%20the%20realism%20of%20the%20motion%20in%20the%20generated%20video.%20To%20address%0Athis%20problem%2C%20we%20propose%20a%20novel%20identity-preserved%20HMT%20network%2C%20termed%0A%5Ctextit%7BIDPres%7D.%20This%20network%20is%20a%20skeleton-based%20approach%20that%20uniquely%0Aincorporates%20the%20target%27s%20individualized%20motion%20and%20skeleton%20information%20to%0Aaugment%20identity%20representations.%20This%20integration%20significantly%20enhances%20the%0Arealism%20of%20movements%20in%20the%20generated%20videos.%20Our%20method%20focuses%20on%20the%0Afine-grained%20disentanglement%20and%20synthesis%20of%20motion.%20To%20improve%20the%0Arepresentation%20learning%20capability%20in%20latent%20space%20and%20facilitate%20the%20training%0Aof%20%5Ctextit%7BIDPres%7D%2C%20we%20introduce%20three%20training%20schemes.%20These%20schemes%20enable%0A%5Ctextit%7BIDPres%7D%20to%20concurrently%20disentangle%20different%20representations%20and%0Aaccurately%20control%20them%2C%20ensuring%20the%20synthesis%20of%20ideal%20motions.%20To%20evaluate%0Athe%20proportion%20of%20individualized%20motion%20information%20in%20the%20generated%20video%2C%20we%0Aare%20the%20first%20to%20introduce%20a%20new%20quantitative%20metric%20called%20Identity%20Score%0A%28%5Ctextit%7BID-Score%7D%29%2C%20motivated%20by%20the%20success%20of%20gait%20recognition%20methods%20in%0Acapturing%20identity%20information.%20Moreover%2C%20we%20collect%20an%20identity-motion%20paired%0Adataset%2C%20%24Dancer101%24%2C%20consisting%20of%20solo-dance%20videos%20of%20101%20subjects%20from%20the%0Apublic%20domain%2C%20providing%20a%20benchmark%20to%20prompt%20the%20development%20of%20HMT%20methods.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20%5Ctextit%7BIDPres%7D%20method%0Asurpasses%20existing%20state-of-the-art%20techniques%20in%20terms%20of%20reconstruction%0Aaccuracy%2C%20realistic%20motion%2C%20and%20identity%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.06862v3&entry.124074799=Read"},
{"title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video\n  Synthesis", "author": "Willi Menapace and Aliaksandr Siarohin and Ivan Skorokhodov and Ekaterina Deyneka and Tsai-Shien Chen and Anil Kag and Yuwei Fang and Aleksei Stoliar and Elisa Ricci and Jian Ren and Sergey Tulyakov", "abstract": "  Contemporary models for generating images show remarkable quality and\nversatility. Swayed by these advantages, the research community repurposes them\nto generate videos. Since video content is highly redundant, we argue that\nnaively bringing advances of image models to the video generation domain\nreduces motion fidelity, visual quality and impairs scalability. In this work,\nwe build Snap Video, a video-first model that systematically addresses these\nchallenges. To do that, we first extend the EDM framework to take into account\nspatially and temporally redundant pixels and naturally support video\ngeneration. Second, we show that a U-Net - a workhorse behind image generation\n- scales poorly when generating videos, requiring significant computational\noverhead. Hence, we propose a new transformer-based architecture that trains\n3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us\nto efficiently train a text-to-video model with billions of parameters for the\nfirst time, reach state-of-the-art results on a number of benchmarks, and\ngenerate videos with substantially higher quality, temporal consistency, and\nmotion complexity. The user studies showed that our model was favored by a\nlarge margin over the most recent methods. See our website at\nhttps://snap-research.github.io/snapvideo/.\n", "link": "http://arxiv.org/abs/2402.14797v1", "date": "2024-02-22", "relevancy": 2.2832, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6054}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4978}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snap%20Video%3A%20Scaled%20Spatiotemporal%20Transformers%20for%20Text-to-Video%0A%20%20Synthesis&entry.906535625=Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Ivan%20Skorokhodov%20and%20Ekaterina%20Deyneka%20and%20Tsai-Shien%20Chen%20and%20Anil%20Kag%20and%20Yuwei%20Fang%20and%20Aleksei%20Stoliar%20and%20Elisa%20Ricci%20and%20Jian%20Ren%20and%20Sergey%20Tulyakov&entry.1292438233=%20%20Contemporary%20models%20for%20generating%20images%20show%20remarkable%20quality%20and%0Aversatility.%20Swayed%20by%20these%20advantages%2C%20the%20research%20community%20repurposes%20them%0Ato%20generate%20videos.%20Since%20video%20content%20is%20highly%20redundant%2C%20we%20argue%20that%0Anaively%20bringing%20advances%20of%20image%20models%20to%20the%20video%20generation%20domain%0Areduces%20motion%20fidelity%2C%20visual%20quality%20and%20impairs%20scalability.%20In%20this%20work%2C%0Awe%20build%20Snap%20Video%2C%20a%20video-first%20model%20that%20systematically%20addresses%20these%0Achallenges.%20To%20do%20that%2C%20we%20first%20extend%20the%20EDM%20framework%20to%20take%20into%20account%0Aspatially%20and%20temporally%20redundant%20pixels%20and%20naturally%20support%20video%0Ageneration.%20Second%2C%20we%20show%20that%20a%20U-Net%20-%20a%20workhorse%20behind%20image%20generation%0A-%20scales%20poorly%20when%20generating%20videos%2C%20requiring%20significant%20computational%0Aoverhead.%20Hence%2C%20we%20propose%20a%20new%20transformer-based%20architecture%20that%20trains%0A3.31%20times%20faster%20than%20U-Nets%20%28and%20is%20~4.5%20faster%20at%20inference%29.%20This%20allows%20us%0Ato%20efficiently%20train%20a%20text-to-video%20model%20with%20billions%20of%20parameters%20for%20the%0Afirst%20time%2C%20reach%20state-of-the-art%20results%20on%20a%20number%20of%20benchmarks%2C%20and%0Agenerate%20videos%20with%20substantially%20higher%20quality%2C%20temporal%20consistency%2C%20and%0Amotion%20complexity.%20The%20user%20studies%20showed%20that%20our%20model%20was%20favored%20by%20a%0Alarge%20margin%20over%20the%20most%20recent%20methods.%20See%20our%20website%20at%0Ahttps%3A//snap-research.github.io/snapvideo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14797v1&entry.124074799=Read"},
{"title": "A Survey on Socially Aware Robot Navigation: Taxonomy and Future\n  Challenges", "author": "Phani Teja Singamaneni and Pilar Bachiller-Burgos and Luis J. Manso and Ana\u00eds Garrell and Alberto Sanfeliu and Anne Spalanzani and Rachid Alami", "abstract": "  Socially aware robot navigation is gaining popularity with the increase in\ndelivery and assistive robots. The research is further fueled by a need for\nsocially aware navigation skills in autonomous vehicles to move safely and\nappropriately in spaces shared with humans. Although most of these are ground\nrobots, drones are also entering the field. In this paper, we present a\nliterature survey of the works on socially aware robot navigation in the past\n10 years. We propose four different faceted taxonomies to navigate the\nliterature and examine the field from four different perspectives. Through the\ntaxonomic review, we discuss the current research directions and the extending\nscope of applications in various domains. Further, we put forward a list of\ncurrent research opportunities and present a discussion on possible future\nchallenges that are likely to emerge in the field.\n", "link": "http://arxiv.org/abs/2311.06922v4", "date": "2024-02-22", "relevancy": 2.2467, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6176}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Socially%20Aware%20Robot%20Navigation%3A%20Taxonomy%20and%20Future%0A%20%20Challenges&entry.906535625=Phani%20Teja%20Singamaneni%20and%20Pilar%20Bachiller-Burgos%20and%20Luis%20J.%20Manso%20and%20Ana%C3%ADs%20Garrell%20and%20Alberto%20Sanfeliu%20and%20Anne%20Spalanzani%20and%20Rachid%20Alami&entry.1292438233=%20%20Socially%20aware%20robot%20navigation%20is%20gaining%20popularity%20with%20the%20increase%20in%0Adelivery%20and%20assistive%20robots.%20The%20research%20is%20further%20fueled%20by%20a%20need%20for%0Asocially%20aware%20navigation%20skills%20in%20autonomous%20vehicles%20to%20move%20safely%20and%0Aappropriately%20in%20spaces%20shared%20with%20humans.%20Although%20most%20of%20these%20are%20ground%0Arobots%2C%20drones%20are%20also%20entering%20the%20field.%20In%20this%20paper%2C%20we%20present%20a%0Aliterature%20survey%20of%20the%20works%20on%20socially%20aware%20robot%20navigation%20in%20the%20past%0A10%20years.%20We%20propose%20four%20different%20faceted%20taxonomies%20to%20navigate%20the%0Aliterature%20and%20examine%20the%20field%20from%20four%20different%20perspectives.%20Through%20the%0Ataxonomic%20review%2C%20we%20discuss%20the%20current%20research%20directions%20and%20the%20extending%0Ascope%20of%20applications%20in%20various%20domains.%20Further%2C%20we%20put%20forward%20a%20list%20of%0Acurrent%20research%20opportunities%20and%20present%20a%20discussion%20on%20possible%20future%0Achallenges%20that%20are%20likely%20to%20emerge%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06922v4&entry.124074799=Read"},
{"title": "Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers", "author": "Yasemin G\u00f6ksu and Antonio De Almeida Correia and Vignesh Prasad and Alap Kshirsagar and Dorothea Koert and Jan Peters and Georgia Chalvatzaki", "abstract": "  Bimanual handovers are crucial for transferring large, deformable or delicate\nobjects. This paper proposes a framework for generating kinematically\nconstrained human-like bimanual robot motions to ensure seamless and natural\nrobot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to\nreactively generate suitable response trajectories for a robot based on the\nobserved human partner's motion. The trajectories are adapted with task space\nconstraints to ensure accurate handovers. Results from a pilot study show that\nour approach is perceived as more human--like compared to a baseline Inverse\nKinematics approach.\n", "link": "http://arxiv.org/abs/2402.14525v1", "date": "2024-02-22", "relevancy": 2.2389, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5528}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5305}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinematically%20Constrained%20Human-like%20Bimanual%20Robot-to-Human%20Handovers&entry.906535625=Yasemin%20G%C3%B6ksu%20and%20Antonio%20De%20Almeida%20Correia%20and%20Vignesh%20Prasad%20and%20Alap%20Kshirsagar%20and%20Dorothea%20Koert%20and%20Jan%20Peters%20and%20Georgia%20Chalvatzaki&entry.1292438233=%20%20Bimanual%20handovers%20are%20crucial%20for%20transferring%20large%2C%20deformable%20or%20delicate%0Aobjects.%20This%20paper%20proposes%20a%20framework%20for%20generating%20kinematically%0Aconstrained%20human-like%20bimanual%20robot%20motions%20to%20ensure%20seamless%20and%20natural%0Arobot-to-human%20object%20handovers.%20We%20use%20a%20Hidden%20Semi-Markov%20Model%20%28HSMM%29%20to%0Areactively%20generate%20suitable%20response%20trajectories%20for%20a%20robot%20based%20on%20the%0Aobserved%20human%20partner%27s%20motion.%20The%20trajectories%20are%20adapted%20with%20task%20space%0Aconstraints%20to%20ensure%20accurate%20handovers.%20Results%20from%20a%20pilot%20study%20show%20that%0Aour%20approach%20is%20perceived%20as%20more%20human--like%20compared%20to%20a%20baseline%20Inverse%0AKinematics%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14525v1&entry.124074799=Read"},
{"title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction\n  Denoising via Denoising Diffusion", "author": "Xueyi Liu and Li Yi", "abstract": "  In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.\n", "link": "http://arxiv.org/abs/2402.14810v1", "date": "2024-02-22", "relevancy": 2.2166, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5892}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5381}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5255}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeneOH%20Diffusion%3A%20Towards%20Generalizable%20Hand-Object%20Interaction%0A%20%20Denoising%20via%20Denoising%20Diffusion&entry.906535625=Xueyi%20Liu%20and%20Li%20Yi&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20challenging%20problem%20of%20denoising%20hand-object%0Ainteractions%20%28HOI%29.%20Given%20an%20erroneous%20interaction%20sequence%2C%20the%20objective%20is%0Ato%20refine%20the%20incorrect%20hand%20trajectory%20to%20remove%20interaction%20artifacts%20for%20a%0Aperceptually%20realistic%20sequence.%20This%20challenge%20involves%20intricate%20interaction%0Anoise%2C%20including%20unnatural%20hand%20poses%20and%20incorrect%20hand-object%20relations%2C%0Aalongside%20the%20necessity%20for%20robust%20generalization%20to%20new%20interactions%20and%0Adiverse%20noise%20patterns.%20We%20tackle%20those%20challenges%20through%20a%20novel%20approach%2C%0AGeneOH%20Diffusion%2C%20incorporating%20two%20key%20designs%3A%20an%20innovative%20contact-centric%0AHOI%20representation%20named%20GeneOH%20and%20a%20new%20domain-generalizable%20denoising%0Ascheme.%20The%20contact-centric%20representation%20GeneOH%20informatively%20parameterizes%0Athe%20HOI%20process%2C%20facilitating%20enhanced%20generalization%20across%20various%20HOI%0Ascenarios.%20The%20new%20denoising%20scheme%20consists%20of%20a%20canonical%20denoising%20model%0Atrained%20to%20project%20noisy%20data%20samples%20from%20a%20whitened%20noise%20space%20to%20a%20clean%0Adata%20manifold%20and%20a%20%22denoising%20via%20diffusion%22%20strategy%20which%20can%20handle%20input%0Atrajectories%20with%20various%20noise%20patterns%20by%20first%20diffusing%20them%20to%20align%20with%0Athe%20whitened%20noise%20space%20and%20cleaning%20via%20the%20canonical%20denoiser.%20Extensive%0Aexperiments%20on%20four%20benchmarks%20with%20significant%20domain%20variations%20demonstrate%0Athe%20superior%20effectiveness%20of%20our%20method.%20GeneOH%20Diffusion%20also%20shows%20promise%0Afor%20various%20downstream%20applications.%20Project%20website%3A%0Ahttps%3A//meowuu7.github.io/GeneOH-Diffusion/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14810v1&entry.124074799=Read"},
{"title": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World\n  Dexterous Manipulation", "author": "Jun Wang and Yuzhe Qin and Kaiming Kuang and Yigit Korkmaz and Akhilan Gurumoorthy and Hao Su and Xiaolong Wang", "abstract": "  We introduce CyberDemo, a novel approach to robotic imitation learning that\nleverages simulated human demonstrations for real-world tasks. By incorporating\nextensive data augmentation in a simulated environment, CyberDemo outperforms\ntraditional in-domain real-world demonstrations when transferred to the real\nworld, handling diverse physical and visual conditions. Regardless of its\naffordability and convenience in data collection, CyberDemo outperforms\nbaseline methods in terms of success rates across various tasks and exhibits\ngeneralizability with previously unseen objects. For example, it can rotate\nnovel tetra-valve and penta-valve, despite human demonstrations only involving\ntri-valves. Our research demonstrates the significant potential of simulated\nhuman demonstrations for real-world dexterous manipulation tasks. More details\ncan be found at https://cyber-demo.github.io\n", "link": "http://arxiv.org/abs/2402.14795v1", "date": "2024-02-22", "relevancy": 2.2133, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5903}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.521}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CyberDemo%3A%20Augmenting%20Simulated%20Human%20Demonstration%20for%20Real-World%0A%20%20Dexterous%20Manipulation&entry.906535625=Jun%20Wang%20and%20Yuzhe%20Qin%20and%20Kaiming%20Kuang%20and%20Yigit%20Korkmaz%20and%20Akhilan%20Gurumoorthy%20and%20Hao%20Su%20and%20Xiaolong%20Wang&entry.1292438233=%20%20We%20introduce%20CyberDemo%2C%20a%20novel%20approach%20to%20robotic%20imitation%20learning%20that%0Aleverages%20simulated%20human%20demonstrations%20for%20real-world%20tasks.%20By%20incorporating%0Aextensive%20data%20augmentation%20in%20a%20simulated%20environment%2C%20CyberDemo%20outperforms%0Atraditional%20in-domain%20real-world%20demonstrations%20when%20transferred%20to%20the%20real%0Aworld%2C%20handling%20diverse%20physical%20and%20visual%20conditions.%20Regardless%20of%20its%0Aaffordability%20and%20convenience%20in%20data%20collection%2C%20CyberDemo%20outperforms%0Abaseline%20methods%20in%20terms%20of%20success%20rates%20across%20various%20tasks%20and%20exhibits%0Ageneralizability%20with%20previously%20unseen%20objects.%20For%20example%2C%20it%20can%20rotate%0Anovel%20tetra-valve%20and%20penta-valve%2C%20despite%20human%20demonstrations%20only%20involving%0Atri-valves.%20Our%20research%20demonstrates%20the%20significant%20potential%20of%20simulated%0Ahuman%20demonstrations%20for%20real-world%20dexterous%20manipulation%20tasks.%20More%20details%0Acan%20be%20found%20at%20https%3A//cyber-demo.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14795v1&entry.124074799=Read"},
{"title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised\n  Learning", "author": "Johnathan Xie and Yoonho Lee and Annie S. Chen and Chelsea Finn", "abstract": "  Self-supervised learning excels in learning representations from large\namounts of unlabeled data, demonstrating success across multiple data\nmodalities. Yet, extending self-supervised learning to new modalities is\nnon-trivial because the specifics of existing methods are tailored to each\ndomain, such as domain-specific augmentations which reflect the invariances in\nthe target task. While masked modeling is promising as a domain-agnostic\nframework for self-supervised learning because it does not rely on input\naugmentations, its mask sampling procedure remains domain-specific. We present\nSelf-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling\nmethod. SMA trains an attention based model using a masked modeling objective,\nby learning masks to sample without any domain-specific assumptions. We\nevaluate SMA on three self-supervised learning benchmarks in protein biology,\nchemical property prediction, and particle physics. We find SMA is capable of\nlearning representations without domain-specific knowledge and achieves\nstate-of-the-art performance on these three benchmarks.\n", "link": "http://arxiv.org/abs/2402.14789v1", "date": "2024-02-22", "relevancy": 2.2041, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.619}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4994}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Guided%20Masked%20Autoencoders%20for%20Domain-Agnostic%20Self-Supervised%0A%20%20Learning&entry.906535625=Johnathan%20Xie%20and%20Yoonho%20Lee%20and%20Annie%20S.%20Chen%20and%20Chelsea%20Finn&entry.1292438233=%20%20Self-supervised%20learning%20excels%20in%20learning%20representations%20from%20large%0Aamounts%20of%20unlabeled%20data%2C%20demonstrating%20success%20across%20multiple%20data%0Amodalities.%20Yet%2C%20extending%20self-supervised%20learning%20to%20new%20modalities%20is%0Anon-trivial%20because%20the%20specifics%20of%20existing%20methods%20are%20tailored%20to%20each%0Adomain%2C%20such%20as%20domain-specific%20augmentations%20which%20reflect%20the%20invariances%20in%0Athe%20target%20task.%20While%20masked%20modeling%20is%20promising%20as%20a%20domain-agnostic%0Aframework%20for%20self-supervised%20learning%20because%20it%20does%20not%20rely%20on%20input%0Aaugmentations%2C%20its%20mask%20sampling%20procedure%20remains%20domain-specific.%20We%20present%0ASelf-guided%20Masked%20Autoencoders%20%28SMA%29%2C%20a%20fully%20domain-agnostic%20masked%20modeling%0Amethod.%20SMA%20trains%20an%20attention%20based%20model%20using%20a%20masked%20modeling%20objective%2C%0Aby%20learning%20masks%20to%20sample%20without%20any%20domain-specific%20assumptions.%20We%0Aevaluate%20SMA%20on%20three%20self-supervised%20learning%20benchmarks%20in%20protein%20biology%2C%0Achemical%20property%20prediction%2C%20and%20particle%20physics.%20We%20find%20SMA%20is%20capable%20of%0Alearning%20representations%20without%20domain-specific%20knowledge%20and%20achieves%0Astate-of-the-art%20performance%20on%20these%20three%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14789v1&entry.124074799=Read"},
{"title": "Generative Invertible Quantum Neural Networks", "author": "Armand Rousselot and Michael Spannowsky", "abstract": "  Invertible Neural Networks (INN) have become established tools for the\nsimulation and generation of highly complex data. We propose a quantum-gate\nalgorithm for a Quantum Invertible Neural Network (QINN) and apply it to the\nLHC data of jet-associated production of a Z-boson that decays into leptons, a\nstandard candle process for particle collider precision measurements. We\ncompare the QINN's performance for different loss functions and training\nscenarios. For this task, we find that a hybrid QINN matches the performance of\na significantly larger purely classical INN in learning and generating complex\ndata.\n", "link": "http://arxiv.org/abs/2302.12906v3", "date": "2024-02-22", "relevancy": 2.1719, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4612}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4247}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4172}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Invertible%20Quantum%20Neural%20Networks&entry.906535625=Armand%20Rousselot%20and%20Michael%20Spannowsky&entry.1292438233=%20%20Invertible%20Neural%20Networks%20%28INN%29%20have%20become%20established%20tools%20for%20the%0Asimulation%20and%20generation%20of%20highly%20complex%20data.%20We%20propose%20a%20quantum-gate%0Aalgorithm%20for%20a%20Quantum%20Invertible%20Neural%20Network%20%28QINN%29%20and%20apply%20it%20to%20the%0ALHC%20data%20of%20jet-associated%20production%20of%20a%20Z-boson%20that%20decays%20into%20leptons%2C%20a%0Astandard%20candle%20process%20for%20particle%20collider%20precision%20measurements.%20We%0Acompare%20the%20QINN%27s%20performance%20for%20different%20loss%20functions%20and%20training%0Ascenarios.%20For%20this%20task%2C%20we%20find%20that%20a%20hybrid%20QINN%20matches%20the%20performance%20of%0Aa%20significantly%20larger%20purely%20classical%20INN%20in%20learning%20and%20generating%20complex%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.12906v3&entry.124074799=Read"},
{"title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "author": "Fengqing Jiang and Zhangchen Xu and Luyao Niu and Zhen Xiang and Bhaskar Ramasubramanian and Bo Li and Radha Poovendran", "abstract": "  Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs.\n", "link": "http://arxiv.org/abs/2402.11753v2", "date": "2024-02-22", "relevancy": 2.1493, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4268}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.415}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtPrompt%3A%20ASCII%20Art-based%20Jailbreak%20Attacks%20against%20Aligned%20LLMs&entry.906535625=Fengqing%20Jiang%20and%20Zhangchen%20Xu%20and%20Luyao%20Niu%20and%20Zhen%20Xiang%20and%20Bhaskar%20Ramasubramanian%20and%20Bo%20Li%20and%20Radha%20Poovendran&entry.1292438233=%20%20Safety%20is%20critical%20to%20the%20usage%20of%20large%20language%20models%20%28LLMs%29.%20Multiple%0Atechniques%20such%20as%20data%20filtering%20and%20supervised%20fine-tuning%20have%20been%0Adeveloped%20to%20strengthen%20LLM%20safety.%20However%2C%20currently%20known%20techniques%20presume%0Athat%20corpora%20used%20for%20safety%20alignment%20of%20LLMs%20are%20solely%20interpreted%20by%0Asemantics.%20This%20assumption%2C%20however%2C%20does%20not%20hold%20in%20real-world%20applications%2C%0Awhich%20leads%20to%20severe%20vulnerabilities%20in%20LLMs.%20For%20example%2C%20users%20of%20forums%0Aoften%20use%20ASCII%20art%2C%20a%20form%20of%20text-based%20art%2C%20to%20convey%20image%20information.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20ASCII%20art-based%20jailbreak%20attack%20and%20introduce%20a%0Acomprehensive%20benchmark%20Vision-in-Text%20Challenge%20%28ViTC%29%20to%20evaluate%20the%0Acapabilities%20of%20LLMs%20in%20recognizing%20prompts%20that%20cannot%20be%20solely%20interpreted%0Aby%20semantics.%20We%20show%20that%20five%20SOTA%20LLMs%20%28GPT-3.5%2C%20GPT-4%2C%20Gemini%2C%20Claude%2C%20and%0ALlama2%29%20struggle%20to%20recognize%20prompts%20provided%20in%20the%20form%20of%20ASCII%20art.%20Based%0Aon%20this%20observation%2C%20we%20develop%20the%20jailbreak%20attack%20ArtPrompt%2C%20which%20leverages%0Athe%20poor%20performance%20of%20LLMs%20in%20recognizing%20ASCII%20art%20to%20bypass%20safety%20measures%0Aand%20elicit%20undesired%20behaviors%20from%20LLMs.%20ArtPrompt%20only%20requires%20black-box%0Aaccess%20to%20the%20victim%20LLMs%2C%20making%20it%20a%20practical%20attack.%20We%20evaluate%20ArtPrompt%0Aon%20five%20SOTA%20LLMs%2C%20and%20show%20that%20ArtPrompt%20can%20effectively%20and%20efficiently%0Ainduce%20undesired%20behaviors%20from%20all%20five%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11753v2&entry.124074799=Read"},
{"title": "Reading Relevant Feature from Global Representation Memory for Visual\n  Object Tracking", "author": "Xinyu Zhou and Pinxue Guo and Lingyi Hong and Jinglun Li and Wei Zhang and Weifeng Ge and Wenqiang Zhang", "abstract": "  Reference features from a template or historical frames are crucial for\nvisual object tracking. Prior works utilize all features from a fixed template\nor memory for visual object tracking. However, due to the dynamic nature of\nvideos, the required reference historical information for different search\nregions at different time steps is also inconsistent. Therefore, using all\nfeatures in the template and memory can lead to redundancy and impair tracking\nperformance. To alleviate this issue, we propose a novel tracking paradigm,\nconsisting of a relevance attention mechanism and a global representation\nmemory, which can adaptively assist the search region in selecting the most\nrelevant historical information from reference features. Specifically, the\nproposed relevance attention mechanism in this work differs from previous\napproaches in that it can dynamically choose and build the optimal global\nrepresentation memory for the current frame by accessing cross-frame\ninformation globally. Moreover, it can flexibly read the relevant historical\ninformation from the constructed memory to reduce redundancy and counteract the\nnegative effects of harmful information. Extensive experiments validate the\neffectiveness of the proposed method, achieving competitive performance on five\nchallenging datasets with 71 FPS.\n", "link": "http://arxiv.org/abs/2402.14392v2", "date": "2024-02-23", "relevancy": 2.1399, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5636}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5273}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5094}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20Relevant%20Feature%20from%20Global%20Representation%20Memory%20for%20Visual%0A%20%20Object%20Tracking&entry.906535625=Xinyu%20Zhou%20and%20Pinxue%20Guo%20and%20Lingyi%20Hong%20and%20Jinglun%20Li%20and%20Wei%20Zhang%20and%20Weifeng%20Ge%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Reference%20features%20from%20a%20template%20or%20historical%20frames%20are%20crucial%20for%0Avisual%20object%20tracking.%20Prior%20works%20utilize%20all%20features%20from%20a%20fixed%20template%0Aor%20memory%20for%20visual%20object%20tracking.%20However%2C%20due%20to%20the%20dynamic%20nature%20of%0Avideos%2C%20the%20required%20reference%20historical%20information%20for%20different%20search%0Aregions%20at%20different%20time%20steps%20is%20also%20inconsistent.%20Therefore%2C%20using%20all%0Afeatures%20in%20the%20template%20and%20memory%20can%20lead%20to%20redundancy%20and%20impair%20tracking%0Aperformance.%20To%20alleviate%20this%20issue%2C%20we%20propose%20a%20novel%20tracking%20paradigm%2C%0Aconsisting%20of%20a%20relevance%20attention%20mechanism%20and%20a%20global%20representation%0Amemory%2C%20which%20can%20adaptively%20assist%20the%20search%20region%20in%20selecting%20the%20most%0Arelevant%20historical%20information%20from%20reference%20features.%20Specifically%2C%20the%0Aproposed%20relevance%20attention%20mechanism%20in%20this%20work%20differs%20from%20previous%0Aapproaches%20in%20that%20it%20can%20dynamically%20choose%20and%20build%20the%20optimal%20global%0Arepresentation%20memory%20for%20the%20current%20frame%20by%20accessing%20cross-frame%0Ainformation%20globally.%20Moreover%2C%20it%20can%20flexibly%20read%20the%20relevant%20historical%0Ainformation%20from%20the%20constructed%20memory%20to%20reduce%20redundancy%20and%20counteract%20the%0Anegative%20effects%20of%20harmful%20information.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20method%2C%20achieving%20competitive%20performance%20on%20five%0Achallenging%20datasets%20with%2071%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14392v2&entry.124074799=Read"},
{"title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real\n  and Simulation", "author": "Junting Chen and Yao Mu and Qiaojun Yu and Tianming Wei and Silang Wu and Zhecheng Yuan and Zhixuan Liang and Chao Yang and Kaipeng Zhang and Wenqi Shao and Yu Qiao and Huazhe Xu and Mingyu Ding and Ping Luo", "abstract": "  Rapid progress in high-level task planning and code generation for open-world\nrobot manipulation has been witnessed in Embodied AI. However, previous studies\nput much effort into general common sense reasoning and task planning\ncapabilities of large-scale language or multi-modal models, relatively little\neffort on ensuring the deployability of generated code on real robots, and\nother fundamental components of autonomous robot systems including robot\nperception, motion planning, and control. To bridge this ``ideal-to-real'' gap,\nthis paper presents \\textbf{RobotScript}, a platform for 1) a deployable robot\nmanipulation pipeline powered by code generation; and 2) a code generation\nbenchmark for robot manipulation tasks in free-form natural language. The\nRobotScript platform addresses this gap by emphasizing the unified interface\nwith both simulation and real robots, based on abstraction from the Robot\nOperating System (ROS), ensuring syntax compliance and simulation validation\nwith Gazebo. We demonstrate the adaptability of our code generation framework\nacross multiple robot embodiments, including the Franka and UR5 robot arms, and\nmultiple grippers. Additionally, our benchmark assesses reasoning abilities for\nphysical space and constraints, highlighting the differences between GPT-3.5,\nGPT-4, and Gemini in handling complex physical interactions. Finally, we\npresent a thorough evaluation on the whole system, exploring how each module in\nthe pipeline: code generation, perception, motion planning, and even object\ngeometric properties, impact the overall performance of the system.\n", "link": "http://arxiv.org/abs/2402.14623v1", "date": "2024-02-22", "relevancy": 2.1129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5571}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5385}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5064}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboScript%3A%20Code%20Generation%20for%20Free-Form%20Manipulation%20Tasks%20across%20Real%0A%20%20and%20Simulation&entry.906535625=Junting%20Chen%20and%20Yao%20Mu%20and%20Qiaojun%20Yu%20and%20Tianming%20Wei%20and%20Silang%20Wu%20and%20Zhecheng%20Yuan%20and%20Zhixuan%20Liang%20and%20Chao%20Yang%20and%20Kaipeng%20Zhang%20and%20Wenqi%20Shao%20and%20Yu%20Qiao%20and%20Huazhe%20Xu%20and%20Mingyu%20Ding%20and%20Ping%20Luo&entry.1292438233=%20%20Rapid%20progress%20in%20high-level%20task%20planning%20and%20code%20generation%20for%20open-world%0Arobot%20manipulation%20has%20been%20witnessed%20in%20Embodied%20AI.%20However%2C%20previous%20studies%0Aput%20much%20effort%20into%20general%20common%20sense%20reasoning%20and%20task%20planning%0Acapabilities%20of%20large-scale%20language%20or%20multi-modal%20models%2C%20relatively%20little%0Aeffort%20on%20ensuring%20the%20deployability%20of%20generated%20code%20on%20real%20robots%2C%20and%0Aother%20fundamental%20components%20of%20autonomous%20robot%20systems%20including%20robot%0Aperception%2C%20motion%20planning%2C%20and%20control.%20To%20bridge%20this%20%60%60ideal-to-real%27%27%20gap%2C%0Athis%20paper%20presents%20%5Ctextbf%7BRobotScript%7D%2C%20a%20platform%20for%201%29%20a%20deployable%20robot%0Amanipulation%20pipeline%20powered%20by%20code%20generation%3B%20and%202%29%20a%20code%20generation%0Abenchmark%20for%20robot%20manipulation%20tasks%20in%20free-form%20natural%20language.%20The%0ARobotScript%20platform%20addresses%20this%20gap%20by%20emphasizing%20the%20unified%20interface%0Awith%20both%20simulation%20and%20real%20robots%2C%20based%20on%20abstraction%20from%20the%20Robot%0AOperating%20System%20%28ROS%29%2C%20ensuring%20syntax%20compliance%20and%20simulation%20validation%0Awith%20Gazebo.%20We%20demonstrate%20the%20adaptability%20of%20our%20code%20generation%20framework%0Aacross%20multiple%20robot%20embodiments%2C%20including%20the%20Franka%20and%20UR5%20robot%20arms%2C%20and%0Amultiple%20grippers.%20Additionally%2C%20our%20benchmark%20assesses%20reasoning%20abilities%20for%0Aphysical%20space%20and%20constraints%2C%20highlighting%20the%20differences%20between%20GPT-3.5%2C%0AGPT-4%2C%20and%20Gemini%20in%20handling%20complex%20physical%20interactions.%20Finally%2C%20we%0Apresent%20a%20thorough%20evaluation%20on%20the%20whole%20system%2C%20exploring%20how%20each%20module%20in%0Athe%20pipeline%3A%20code%20generation%2C%20perception%2C%20motion%20planning%2C%20and%20even%20object%0Ageometric%20properties%2C%20impact%20the%20overall%20performance%20of%20the%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14623v1&entry.124074799=Read"},
{"title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with\n  4D Gaussian Splatting", "author": "Zeyu Yang and Hongye Yang and Zijie Pan and Li Zhang", "abstract": "  Reconstructing dynamic 3D scenes from 2D images and generating diverse views\nover time is challenging due to scene complexity and temporal dynamics. Despite\nadvancements in neural implicit models, limitations persist: (i) Inadequate\nScene Structure: Existing methods struggle to reveal the spatial and temporal\nstructure of dynamic scenes from directly learning the complex 6D plenoptic\nfunction. (ii) Scaling Deformation Modeling: Explicitly modeling scene element\ndeformation becomes impractical for complex dynamics. To address these issues,\nwe consider the spacetime as an entirety and propose to approximate the\nunderlying spatio-temporal 4D volume of a dynamic scene by optimizing a\ncollection of 4D primitives, with explicit geometry and appearance modeling.\nLearning to optimize the 4D primitives enables us to synthesize novel views at\nany desired time with our tailored rendering routine. Our model is conceptually\nsimple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that\ncan rotate arbitrarily in space and time, as well as view-dependent and\ntime-evolved appearance represented by the coefficient of 4D spherindrical\nharmonics. This approach offers simplicity, flexibility for variable-length\nvideo and end-to-end training, and efficient real-time rendering, making it\nsuitable for capturing complex dynamic scene motions. Experiments across\nvarious benchmarks, including monocular and multi-view scenarios, demonstrate\nour 4DGS model's superior visual quality and efficiency.\n", "link": "http://arxiv.org/abs/2310.10642v3", "date": "2024-02-22", "relevancy": 2.091, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5187}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5094}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Photorealistic%20Dynamic%20Scene%20Representation%20and%20Rendering%20with%0A%20%204D%20Gaussian%20Splatting&entry.906535625=Zeyu%20Yang%20and%20Hongye%20Yang%20and%20Zijie%20Pan%20and%20Li%20Zhang&entry.1292438233=%20%20Reconstructing%20dynamic%203D%20scenes%20from%202D%20images%20and%20generating%20diverse%20views%0Aover%20time%20is%20challenging%20due%20to%20scene%20complexity%20and%20temporal%20dynamics.%20Despite%0Aadvancements%20in%20neural%20implicit%20models%2C%20limitations%20persist%3A%20%28i%29%20Inadequate%0AScene%20Structure%3A%20Existing%20methods%20struggle%20to%20reveal%20the%20spatial%20and%20temporal%0Astructure%20of%20dynamic%20scenes%20from%20directly%20learning%20the%20complex%206D%20plenoptic%0Afunction.%20%28ii%29%20Scaling%20Deformation%20Modeling%3A%20Explicitly%20modeling%20scene%20element%0Adeformation%20becomes%20impractical%20for%20complex%20dynamics.%20To%20address%20these%20issues%2C%0Awe%20consider%20the%20spacetime%20as%20an%20entirety%20and%20propose%20to%20approximate%20the%0Aunderlying%20spatio-temporal%204D%20volume%20of%20a%20dynamic%20scene%20by%20optimizing%20a%0Acollection%20of%204D%20primitives%2C%20with%20explicit%20geometry%20and%20appearance%20modeling.%0ALearning%20to%20optimize%20the%204D%20primitives%20enables%20us%20to%20synthesize%20novel%20views%20at%0Aany%20desired%20time%20with%20our%20tailored%20rendering%20routine.%20Our%20model%20is%20conceptually%0Asimple%2C%20consisting%20of%20a%204D%20Gaussian%20parameterized%20by%20anisotropic%20ellipses%20that%0Acan%20rotate%20arbitrarily%20in%20space%20and%20time%2C%20as%20well%20as%20view-dependent%20and%0Atime-evolved%20appearance%20represented%20by%20the%20coefficient%20of%204D%20spherindrical%0Aharmonics.%20This%20approach%20offers%20simplicity%2C%20flexibility%20for%20variable-length%0Avideo%20and%20end-to-end%20training%2C%20and%20efficient%20real-time%20rendering%2C%20making%20it%0Asuitable%20for%20capturing%20complex%20dynamic%20scene%20motions.%20Experiments%20across%0Avarious%20benchmarks%2C%20including%20monocular%20and%20multi-view%20scenarios%2C%20demonstrate%0Aour%204DGS%20model%27s%20superior%20visual%20quality%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10642v3&entry.124074799=Read"},
{"title": "Transformable Gaussian Reward Function for Socially-Aware Navigation\n  with Deep Reinforcement Learning", "author": "Jinyeob Kim and Sumin Kang and Sungwoo Yang and Beomjoon Kim and Jargalbaatar Yura and Donghan Kim", "abstract": "  Robot navigation has transitioned from prioritizing obstacle avoidance to\nadopting socially aware navigation strategies that accommodate human presence.\nAs a result, the recognition of socially aware navigation within dynamic\nhuman-centric environments has gained prominence in the field of robotics.\nAlthough reinforcement learning technique has fostered the advancement of\nsocially aware navigation, defining appropriate reward functions, especially in\ncongested environments, has posed a significant challenge. These rewards,\ncrucial in guiding robot actions, demand intricate human-crafted design due to\ntheir complex nature and inability to be automatically set. The multitude of\nmanually designed rewards poses issues with hyperparameter redundancy,\nimbalance, and inadequate representation of unique object characteristics. To\naddress these challenges, we introduce a transformable gaussian reward function\n(TGRF). The TGRF significantly reduces the burden of hyperparameter tuning,\ndisplays adaptability across various reward functions, and demonstrates\naccelerated learning rates, particularly excelling in crowded environments\nutilizing deep reinforcement learning (DRL). We introduce and validate TGRF\nthrough sections highlighting its conceptual background, characteristics,\nexperiments, and real-world application, paving the way for a more effective\nand adaptable approach in robotics.The complete source code is available on\nhttps://github.com/JinnnK/TGRF\n", "link": "http://arxiv.org/abs/2402.14569v1", "date": "2024-02-22", "relevancy": 2.0764, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5198}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5149}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformable%20Gaussian%20Reward%20Function%20for%20Socially-Aware%20Navigation%0A%20%20with%20Deep%20Reinforcement%20Learning&entry.906535625=Jinyeob%20Kim%20and%20Sumin%20Kang%20and%20Sungwoo%20Yang%20and%20Beomjoon%20Kim%20and%20Jargalbaatar%20Yura%20and%20Donghan%20Kim&entry.1292438233=%20%20Robot%20navigation%20has%20transitioned%20from%20prioritizing%20obstacle%20avoidance%20to%0Aadopting%20socially%20aware%20navigation%20strategies%20that%20accommodate%20human%20presence.%0AAs%20a%20result%2C%20the%20recognition%20of%20socially%20aware%20navigation%20within%20dynamic%0Ahuman-centric%20environments%20has%20gained%20prominence%20in%20the%20field%20of%20robotics.%0AAlthough%20reinforcement%20learning%20technique%20has%20fostered%20the%20advancement%20of%0Asocially%20aware%20navigation%2C%20defining%20appropriate%20reward%20functions%2C%20especially%20in%0Acongested%20environments%2C%20has%20posed%20a%20significant%20challenge.%20These%20rewards%2C%0Acrucial%20in%20guiding%20robot%20actions%2C%20demand%20intricate%20human-crafted%20design%20due%20to%0Atheir%20complex%20nature%20and%20inability%20to%20be%20automatically%20set.%20The%20multitude%20of%0Amanually%20designed%20rewards%20poses%20issues%20with%20hyperparameter%20redundancy%2C%0Aimbalance%2C%20and%20inadequate%20representation%20of%20unique%20object%20characteristics.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20transformable%20gaussian%20reward%20function%0A%28TGRF%29.%20The%20TGRF%20significantly%20reduces%20the%20burden%20of%20hyperparameter%20tuning%2C%0Adisplays%20adaptability%20across%20various%20reward%20functions%2C%20and%20demonstrates%0Aaccelerated%20learning%20rates%2C%20particularly%20excelling%20in%20crowded%20environments%0Autilizing%20deep%20reinforcement%20learning%20%28DRL%29.%20We%20introduce%20and%20validate%20TGRF%0Athrough%20sections%20highlighting%20its%20conceptual%20background%2C%20characteristics%2C%0Aexperiments%2C%20and%20real-world%20application%2C%20paving%20the%20way%20for%20a%20more%20effective%0Aand%20adaptable%20approach%20in%20robotics.The%20complete%20source%20code%20is%20available%20on%0Ahttps%3A//github.com/JinnnK/TGRF%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14569v1&entry.124074799=Read"},
{"title": "CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for\n  Optimized Learning Fusion", "author": "Zijun Long and George Killick and Lipeng Zhuang and Gerardo Aragon-Camarasa and Zaiqiao Meng and Richard Mccreadie", "abstract": "  State-of-the-art pre-trained image models predominantly adopt a two-stage\napproach: initial unsupervised pre-training on large-scale datasets followed by\ntask-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been\ndemonstrated that CE can compromise model generalization and stability. While\nrecent works employing contrastive learning address some of these limitations\nby enhancing the quality of embeddings and producing better decision\nboundaries, they often overlook the importance of hard negative mining and rely\non resource intensive and slow training using large sample batches. To counter\nthese issues, we introduce a novel approach named CLCE, which integrates\nLabel-Aware Contrastive Learning with CE. Our approach not only maintains the\nstrengths of both loss functions but also leverages hard negative mining in a\nsynergistic way to enhance performance. Experimental results demonstrate that\nCLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks,\nachieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in\ntransfer learning settings with the BEiT-3 model. Importantly, our proposed\nCLCE approach effectively mitigates the dependency of contrastive learning on\nlarge batch sizes such as 4096 samples per batch, a limitation that has\npreviously constrained the application of contrastive learning in\nbudget-limited hardware environments.\n", "link": "http://arxiv.org/abs/2402.14551v1", "date": "2024-02-22", "relevancy": 2.0746, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5243}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.517}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLCE%3A%20An%20Approach%20to%20Refining%20Cross-Entropy%20and%20Contrastive%20Learning%20for%0A%20%20Optimized%20Learning%20Fusion&entry.906535625=Zijun%20Long%20and%20George%20Killick%20and%20Lipeng%20Zhuang%20and%20Gerardo%20Aragon-Camarasa%20and%20Zaiqiao%20Meng%20and%20Richard%20Mccreadie&entry.1292438233=%20%20State-of-the-art%20pre-trained%20image%20models%20predominantly%20adopt%20a%20two-stage%0Aapproach%3A%20initial%20unsupervised%20pre-training%20on%20large-scale%20datasets%20followed%20by%0Atask-specific%20fine-tuning%20using%20Cross-Entropy%20loss~%28CE%29.%20However%2C%20it%20has%20been%0Ademonstrated%20that%20CE%20can%20compromise%20model%20generalization%20and%20stability.%20While%0Arecent%20works%20employing%20contrastive%20learning%20address%20some%20of%20these%20limitations%0Aby%20enhancing%20the%20quality%20of%20embeddings%20and%20producing%20better%20decision%0Aboundaries%2C%20they%20often%20overlook%20the%20importance%20of%20hard%20negative%20mining%20and%20rely%0Aon%20resource%20intensive%20and%20slow%20training%20using%20large%20sample%20batches.%20To%20counter%0Athese%20issues%2C%20we%20introduce%20a%20novel%20approach%20named%20CLCE%2C%20which%20integrates%0ALabel-Aware%20Contrastive%20Learning%20with%20CE.%20Our%20approach%20not%20only%20maintains%20the%0Astrengths%20of%20both%20loss%20functions%20but%20also%20leverages%20hard%20negative%20mining%20in%20a%0Asynergistic%20way%20to%20enhance%20performance.%20Experimental%20results%20demonstrate%20that%0ACLCE%20significantly%20outperforms%20CE%20in%20Top-1%20accuracy%20across%20twelve%20benchmarks%2C%0Aachieving%20gains%20of%20up%20to%203.52%25%20in%20few-shot%20learning%20scenarios%20and%203.41%25%20in%0Atransfer%20learning%20settings%20with%20the%20BEiT-3%20model.%20Importantly%2C%20our%20proposed%0ACLCE%20approach%20effectively%20mitigates%20the%20dependency%20of%20contrastive%20learning%20on%0Alarge%20batch%20sizes%20such%20as%204096%20samples%20per%20batch%2C%20a%20limitation%20that%20has%0Apreviously%20constrained%20the%20application%20of%20contrastive%20learning%20in%0Abudget-limited%20hardware%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14551v1&entry.124074799=Read"},
{"title": "AQD: Towards Accurate Fully-Quantized Object Detection", "author": "Peng Chen and Jing Liu and Bohan Zhuang and Mingkui Tan and Chunhua Shen", "abstract": "  Network quantization allows inference to be conducted using low-precision\narithmetic for improved inference efficiency of deep neural networks on edge\ndevices. However, designing aggressively low-bit (e.g., 2-bit) quantization\nschemes on complex tasks, such as object detection, still remains challenging\nin terms of severe performance degradation and unverifiable efficiency on\ncommon hardware. In this paper, we propose an Accurate Quantized object\nDetection solution, termed AQD, to fully get rid of floating-point computation.\nTo this end, we target using fixed-point operations in all kinds of layers,\nincluding the convolutional layers, normalization layers, and skip connections,\nallowing the inference to be executed using integer-only arithmetic. To\ndemonstrate the improved latency-vs-accuracy trade-off, we apply the proposed\nmethods on RetinaNet and FCOS. In particular, experimental results on MS-COCO\ndataset show that our AQD achieves comparable or even better performance\ncompared with the full-precision counterpart under extremely low-bit schemes,\nwhich is of great practical value. Source code and models are available at:\nhttps://github.com/ziplab/QTool\n", "link": "http://arxiv.org/abs/2007.06919v5", "date": "2024-02-22", "relevancy": 2.0733, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5279}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5042}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AQD%3A%20Towards%20Accurate%20Fully-Quantized%20Object%20Detection&entry.906535625=Peng%20Chen%20and%20Jing%20Liu%20and%20Bohan%20Zhuang%20and%20Mingkui%20Tan%20and%20Chunhua%20Shen&entry.1292438233=%20%20Network%20quantization%20allows%20inference%20to%20be%20conducted%20using%20low-precision%0Aarithmetic%20for%20improved%20inference%20efficiency%20of%20deep%20neural%20networks%20on%20edge%0Adevices.%20However%2C%20designing%20aggressively%20low-bit%20%28e.g.%2C%202-bit%29%20quantization%0Aschemes%20on%20complex%20tasks%2C%20such%20as%20object%20detection%2C%20still%20remains%20challenging%0Ain%20terms%20of%20severe%20performance%20degradation%20and%20unverifiable%20efficiency%20on%0Acommon%20hardware.%20In%20this%20paper%2C%20we%20propose%20an%20Accurate%20Quantized%20object%0ADetection%20solution%2C%20termed%20AQD%2C%20to%20fully%20get%20rid%20of%20floating-point%20computation.%0ATo%20this%20end%2C%20we%20target%20using%20fixed-point%20operations%20in%20all%20kinds%20of%20layers%2C%0Aincluding%20the%20convolutional%20layers%2C%20normalization%20layers%2C%20and%20skip%20connections%2C%0Aallowing%20the%20inference%20to%20be%20executed%20using%20integer-only%20arithmetic.%20To%0Ademonstrate%20the%20improved%20latency-vs-accuracy%20trade-off%2C%20we%20apply%20the%20proposed%0Amethods%20on%20RetinaNet%20and%20FCOS.%20In%20particular%2C%20experimental%20results%20on%20MS-COCO%0Adataset%20show%20that%20our%20AQD%20achieves%20comparable%20or%20even%20better%20performance%0Acompared%20with%20the%20full-precision%20counterpart%20under%20extremely%20low-bit%20schemes%2C%0Awhich%20is%20of%20great%20practical%20value.%20Source%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/ziplab/QTool%0A&entry.1838667208=http%3A//arxiv.org/abs/2007.06919v5&entry.124074799=Read"},
{"title": "DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large\n  Language Models", "author": "Yuhang Cao and Pan Zhang and Xiaoyi Dong and Dahua Lin and Jiaqi Wang", "abstract": "  We present DualFocus, a novel framework for integrating macro and micro\nperspectives within multi-modal large language models (MLLMs) to enhance\nvision-language task performance. Current MLLMs typically singularly focus on\ninputs at a predefined resolution, resulting in deficiencies in detailed\nquestions involving local regions. We introduced a DualFocus mechanism where\nthe model concentrates on the image from a macro perspective, responses to the\nquestion, and identifies suitable sub-regions to zoom in for subsequent micro\nperspective analysis. Via the integration of answers from both macro and micro\nperspectives, the model is adept at addressing tasks that encompass global,\ndetailed, and combined considerations. To endows the DualFocus mechanism in\nMLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and\nadapted it to align with the training regimen of DualFocus. Through comparative\nstudies across different model sizes and benchmarks, we demonstrate DualFocus's\nsuperiority in balancing detailed examination with holistic insight,\nsignificantly reducing hallucination instances in MLLMs and improving their\nperformance in various vision-language tasks.\n", "link": "http://arxiv.org/abs/2402.14767v1", "date": "2024-02-22", "relevancy": 2.0594, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5289}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5005}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualFocus%3A%20Integrating%20Macro%20and%20Micro%20Perspectives%20in%20Multi-modal%20Large%0A%20%20Language%20Models&entry.906535625=Yuhang%20Cao%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20We%20present%20DualFocus%2C%20a%20novel%20framework%20for%20integrating%20macro%20and%20micro%0Aperspectives%20within%20multi-modal%20large%20language%20models%20%28MLLMs%29%20to%20enhance%0Avision-language%20task%20performance.%20Current%20MLLMs%20typically%20singularly%20focus%20on%0Ainputs%20at%20a%20predefined%20resolution%2C%20resulting%20in%20deficiencies%20in%20detailed%0Aquestions%20involving%20local%20regions.%20We%20introduced%20a%20DualFocus%20mechanism%20where%0Athe%20model%20concentrates%20on%20the%20image%20from%20a%20macro%20perspective%2C%20responses%20to%20the%0Aquestion%2C%20and%20identifies%20suitable%20sub-regions%20to%20zoom%20in%20for%20subsequent%20micro%0Aperspective%20analysis.%20Via%20the%20integration%20of%20answers%20from%20both%20macro%20and%20micro%0Aperspectives%2C%20the%20model%20is%20adept%20at%20addressing%20tasks%20that%20encompass%20global%2C%0Adetailed%2C%20and%20combined%20considerations.%20To%20endows%20the%20DualFocus%20mechanism%20in%0AMLLMs%2C%20we%20curated%20a%20tailored%20dataset%20derived%20from%20the%20Visual%20Genome%20%28VG%29%20and%0Aadapted%20it%20to%20align%20with%20the%20training%20regimen%20of%20DualFocus.%20Through%20comparative%0Astudies%20across%20different%20model%20sizes%20and%20benchmarks%2C%20we%20demonstrate%20DualFocus%27s%0Asuperiority%20in%20balancing%20detailed%20examination%20with%20holistic%20insight%2C%0Asignificantly%20reducing%20hallucination%20instances%20in%20MLLMs%20and%20improving%20their%0Aperformance%20in%20various%20vision-language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14767v1&entry.124074799=Read"},
{"title": "YOLO-World: Real-Time Open-Vocabulary Object Detection", "author": "Tianheng Cheng and Lin Song and Yixiao Ge and Wenyu Liu and Xinggang Wang and Ying Shan", "abstract": "  The You Only Look Once (YOLO) series of detectors have established themselves\nas efficient and practical tools. However, their reliance on predefined and\ntrained object categories limits their applicability in open scenarios.\nAddressing this limitation, we introduce YOLO-World, an innovative approach\nthat enhances YOLO with open-vocabulary detection capabilities through\nvision-language modeling and pre-training on large-scale datasets.\nSpecifically, we propose a new Re-parameterizable Vision-Language Path\nAggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate\nthe interaction between visual and linguistic information. Our method excels in\ndetecting a wide range of objects in a zero-shot manner with high efficiency.\nOn the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on\nV100, which outperforms many state-of-the-art methods in terms of both accuracy\nand speed. Furthermore, the fine-tuned YOLO-World achieves remarkable\nperformance on several downstream tasks, including object detection and\nopen-vocabulary instance segmentation.\n", "link": "http://arxiv.org/abs/2401.17270v3", "date": "2024-02-22", "relevancy": 2.051, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5071}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO-World%3A%20Real-Time%20Open-Vocabulary%20Object%20Detection&entry.906535625=Tianheng%20Cheng%20and%20Lin%20Song%20and%20Yixiao%20Ge%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Ying%20Shan&entry.1292438233=%20%20The%20You%20Only%20Look%20Once%20%28YOLO%29%20series%20of%20detectors%20have%20established%20themselves%0Aas%20efficient%20and%20practical%20tools.%20However%2C%20their%20reliance%20on%20predefined%20and%0Atrained%20object%20categories%20limits%20their%20applicability%20in%20open%20scenarios.%0AAddressing%20this%20limitation%2C%20we%20introduce%20YOLO-World%2C%20an%20innovative%20approach%0Athat%20enhances%20YOLO%20with%20open-vocabulary%20detection%20capabilities%20through%0Avision-language%20modeling%20and%20pre-training%20on%20large-scale%20datasets.%0ASpecifically%2C%20we%20propose%20a%20new%20Re-parameterizable%20Vision-Language%20Path%0AAggregation%20Network%20%28RepVL-PAN%29%20and%20region-text%20contrastive%20loss%20to%20facilitate%0Athe%20interaction%20between%20visual%20and%20linguistic%20information.%20Our%20method%20excels%20in%0Adetecting%20a%20wide%20range%20of%20objects%20in%20a%20zero-shot%20manner%20with%20high%20efficiency.%0AOn%20the%20challenging%20LVIS%20dataset%2C%20YOLO-World%20achieves%2035.4%20AP%20with%2052.0%20FPS%20on%0AV100%2C%20which%20outperforms%20many%20state-of-the-art%20methods%20in%20terms%20of%20both%20accuracy%0Aand%20speed.%20Furthermore%2C%20the%20fine-tuned%20YOLO-World%20achieves%20remarkable%0Aperformance%20on%20several%20downstream%20tasks%2C%20including%20object%20detection%20and%0Aopen-vocabulary%20instance%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17270v3&entry.124074799=Read"},
{"title": "A Collision-Aware Cable Grasping Method in Cluttered Environment", "author": "Lei Zhang and Kaixin Bai and Qiang Li and Zhaopeng Chen and Jianwei Zhang", "abstract": "  We introduce a Cable Grasping-Convolutional Neural Network designed to\nfacilitate robust cable grasping in cluttered environments. Utilizing physics\nsimulations, we generate an extensive dataset that mimics the intricacies of\ncable grasping, factoring in potential collisions between cables and robotic\ngrippers. We employ the Approximate Convex Decomposition technique to dissect\nthe non-convex cable model, with grasp quality autonomously labeled based on\nsimulated grasping attempts. The CG-CNN is refined using this simulated dataset\nand enhanced through domain randomization techniques. Subsequently, the trained\nmodel predicts grasp quality, guiding the optimal grasp pose to the robot\ncontroller for execution. Grasping efficacy is assessed across both synthetic\nand real-world settings. Given our model implicit collision sensitivity, we\nachieved commendable success rates of 92.3% for known cables and 88.4% for\nunknown cables, surpassing contemporary state-of-the-art approaches.\nSupplementary materials can be found at\nhttps://leizhang-public.github.io/cg-cnn/ .\n", "link": "http://arxiv.org/abs/2402.14498v1", "date": "2024-02-22", "relevancy": 2.04, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4709}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4662}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Collision-Aware%20Cable%20Grasping%20Method%20in%20Cluttered%20Environment&entry.906535625=Lei%20Zhang%20and%20Kaixin%20Bai%20and%20Qiang%20Li%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang&entry.1292438233=%20%20We%20introduce%20a%20Cable%20Grasping-Convolutional%20Neural%20Network%20designed%20to%0Afacilitate%20robust%20cable%20grasping%20in%20cluttered%20environments.%20Utilizing%20physics%0Asimulations%2C%20we%20generate%20an%20extensive%20dataset%20that%20mimics%20the%20intricacies%20of%0Acable%20grasping%2C%20factoring%20in%20potential%20collisions%20between%20cables%20and%20robotic%0Agrippers.%20We%20employ%20the%20Approximate%20Convex%20Decomposition%20technique%20to%20dissect%0Athe%20non-convex%20cable%20model%2C%20with%20grasp%20quality%20autonomously%20labeled%20based%20on%0Asimulated%20grasping%20attempts.%20The%20CG-CNN%20is%20refined%20using%20this%20simulated%20dataset%0Aand%20enhanced%20through%20domain%20randomization%20techniques.%20Subsequently%2C%20the%20trained%0Amodel%20predicts%20grasp%20quality%2C%20guiding%20the%20optimal%20grasp%20pose%20to%20the%20robot%0Acontroller%20for%20execution.%20Grasping%20efficacy%20is%20assessed%20across%20both%20synthetic%0Aand%20real-world%20settings.%20Given%20our%20model%20implicit%20collision%20sensitivity%2C%20we%0Aachieved%20commendable%20success%20rates%20of%2092.3%25%20for%20known%20cables%20and%2088.4%25%20for%0Aunknown%20cables%2C%20surpassing%20contemporary%20state-of-the-art%20approaches.%0ASupplementary%20materials%20can%20be%20found%20at%0Ahttps%3A//leizhang-public.github.io/cg-cnn/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14498v1&entry.124074799=Read"},
{"title": "DynGMA: a robust approach for learning stochastic differential equations\n  from data", "author": "Aiqing Zhu and Qianxiao Li", "abstract": "  Learning unknown stochastic differential equations (SDEs) from observed data\nis a significant and challenging task with applications in various fields.\nCurrent approaches often use neural networks to represent drift and diffusion\nfunctions, and construct likelihood-based loss by approximating the transition\ndensity to train these networks. However, these methods often rely on one-step\nstochastic numerical schemes, necessitating data with sufficiently high time\nresolution. In this paper, we introduce novel approximations to the transition\ndensity of the parameterized SDE: a Gaussian density approximation inspired by\nthe random perturbation theory of dynamical systems, and its extension, the\ndynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust\ndensity approximation, our method exhibits superior accuracy compared to\nbaseline methods in learning the fully unknown drift and diffusion functions\nand computing the invariant distribution from trajectory data. And it is\ncapable of handling trajectory data with low time resolution and variable, even\nuncontrollable, time step sizes, such as data generated from Gillespie's\nstochastic simulations. We then conduct several experiments across various\nscenarios to verify the advantages and robustness of the proposed method.\n", "link": "http://arxiv.org/abs/2402.14475v1", "date": "2024-02-22", "relevancy": 2.0372, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5261}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5063}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynGMA%3A%20a%20robust%20approach%20for%20learning%20stochastic%20differential%20equations%0A%20%20from%20data&entry.906535625=Aiqing%20Zhu%20and%20Qianxiao%20Li&entry.1292438233=%20%20Learning%20unknown%20stochastic%20differential%20equations%20%28SDEs%29%20from%20observed%20data%0Ais%20a%20significant%20and%20challenging%20task%20with%20applications%20in%20various%20fields.%0ACurrent%20approaches%20often%20use%20neural%20networks%20to%20represent%20drift%20and%20diffusion%0Afunctions%2C%20and%20construct%20likelihood-based%20loss%20by%20approximating%20the%20transition%0Adensity%20to%20train%20these%20networks.%20However%2C%20these%20methods%20often%20rely%20on%20one-step%0Astochastic%20numerical%20schemes%2C%20necessitating%20data%20with%20sufficiently%20high%20time%0Aresolution.%20In%20this%20paper%2C%20we%20introduce%20novel%20approximations%20to%20the%20transition%0Adensity%20of%20the%20parameterized%20SDE%3A%20a%20Gaussian%20density%20approximation%20inspired%20by%0Athe%20random%20perturbation%20theory%20of%20dynamical%20systems%2C%20and%20its%20extension%2C%20the%0Adynamical%20Gaussian%20mixture%20approximation%20%28DynGMA%29.%20Benefiting%20from%20the%20robust%0Adensity%20approximation%2C%20our%20method%20exhibits%20superior%20accuracy%20compared%20to%0Abaseline%20methods%20in%20learning%20the%20fully%20unknown%20drift%20and%20diffusion%20functions%0Aand%20computing%20the%20invariant%20distribution%20from%20trajectory%20data.%20And%20it%20is%0Acapable%20of%20handling%20trajectory%20data%20with%20low%20time%20resolution%20and%20variable%2C%20even%0Auncontrollable%2C%20time%20step%20sizes%2C%20such%20as%20data%20generated%20from%20Gillespie%27s%0Astochastic%20simulations.%20We%20then%20conduct%20several%20experiments%20across%20various%0Ascenarios%20to%20verify%20the%20advantages%20and%20robustness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14475v1&entry.124074799=Read"},
{"title": "Visual Hallucinations of Multi-modal Large Language Models", "author": "Wen Huang and Hongbin Liu and Minxin Guo and Neil Zhenqiang Gong", "abstract": "  Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines\nincorrect details about an image in visual question answering. Existing studies\nfind VH instances only in existing image datasets, which results in biased\nunderstanding of MLLMs' performance under VH due to limited diversity of such\nVH instances. In this work, we propose a tool called VHTest to generate a\ndiverse set of VH instances. Specifically, VHTest finds some initial VH\ninstances in existing image datasets (e.g., COCO), generates a text description\nfor each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to\ngenerate VH images based on the text descriptions. We collect a benchmark\ndataset with 1,200 VH instances in 8 VH modes using VHTest. We find that\nexisting MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a\nlarge fraction of the instances in our benchmark. Moreover, we find that\nfine-tuning an MLLM using our benchmark dataset reduces its likelihood to\nhallucinate without sacrificing its performance on other benchmarks. Our\nbenchmarks are publicly available: https://github.com/wenhuang2000/VHTest.\n", "link": "http://arxiv.org/abs/2402.14683v1", "date": "2024-02-22", "relevancy": 2.0295, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.524}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.512}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4889}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Hallucinations%20of%20Multi-modal%20Large%20Language%20Models&entry.906535625=Wen%20Huang%20and%20Hongbin%20Liu%20and%20Minxin%20Guo%20and%20Neil%20Zhenqiang%20Gong&entry.1292438233=%20%20Visual%20hallucination%20%28VH%29%20means%20that%20a%20multi-modal%20LLM%20%28MLLM%29%20imagines%0Aincorrect%20details%20about%20an%20image%20in%20visual%20question%20answering.%20Existing%20studies%0Afind%20VH%20instances%20only%20in%20existing%20image%20datasets%2C%20which%20results%20in%20biased%0Aunderstanding%20of%20MLLMs%27%20performance%20under%20VH%20due%20to%20limited%20diversity%20of%20such%0AVH%20instances.%20In%20this%20work%2C%20we%20propose%20a%20tool%20called%20VHTest%20to%20generate%20a%0Adiverse%20set%20of%20VH%20instances.%20Specifically%2C%20VHTest%20finds%20some%20initial%20VH%0Ainstances%20in%20existing%20image%20datasets%20%28e.g.%2C%20COCO%29%2C%20generates%20a%20text%20description%0Afor%20each%20VH%20mode%2C%20and%20uses%20a%20text-to-image%20generative%20model%20%28e.g.%2C%20DALL-E-3%29%20to%0Agenerate%20VH%20images%20based%20on%20the%20text%20descriptions.%20We%20collect%20a%20benchmark%0Adataset%20with%201%2C200%20VH%20instances%20in%208%20VH%20modes%20using%20VHTest.%20We%20find%20that%0Aexisting%20MLLMs%20such%20as%20GPT-4V%2C%20LLaVA-1.5%2C%20and%20MiniGPT-v2%20hallucinate%20for%20a%0Alarge%20fraction%20of%20the%20instances%20in%20our%20benchmark.%20Moreover%2C%20we%20find%20that%0Afine-tuning%20an%20MLLM%20using%20our%20benchmark%20dataset%20reduces%20its%20likelihood%20to%0Ahallucinate%20without%20sacrificing%20its%20performance%20on%20other%20benchmarks.%20Our%0Abenchmarks%20are%20publicly%20available%3A%20https%3A//github.com/wenhuang2000/VHTest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14683v1&entry.124074799=Read"},
{"title": "Large Language Models as Urban Residents: An LLM Agent Framework for\n  Personal Mobility Generation", "author": "Jiawei Wang and Renhe Jiang and Chuang Yang and Zengqing Wu and Makoto Onizuka and Ryosuke Shibasaki and Chuan Xiao", "abstract": "  This paper introduces a novel approach using Large Language Models (LLMs)\nintegrated into an agent framework for flexible and efficient personal mobility\ngeneration. LLMs overcome the limitations of previous models by efficiently\nprocessing semantic data and offering versatility in modeling various tasks.\nOur approach addresses the critical need to align LLMs with real-world urban\nmobility data, focusing on three research questions: aligning LLMs with rich\nactivity data, developing reliable activity generation strategies, and\nexploring LLM applications in urban mobility. The key technical contribution is\na novel LLM agent framework that accounts for individual activity patterns and\nmotivations, including a self-consistency approach to align LLMs with\nreal-world activity data and a retrieval-augmented strategy for interpretable\nactivity generation. In experimental studies, comprehensive validation is\nperformed using real-world data. This research marks the pioneering work of\ndesigning an LLM agent framework for activity generation based on real-world\nhuman activity data, offering a promising tool for urban mobility analysis.\n", "link": "http://arxiv.org/abs/2402.14744v1", "date": "2024-02-22", "relevancy": 2.0248, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4848}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Urban%20Residents%3A%20An%20LLM%20Agent%20Framework%20for%0A%20%20Personal%20Mobility%20Generation&entry.906535625=Jiawei%20Wang%20and%20Renhe%20Jiang%20and%20Chuang%20Yang%20and%20Zengqing%20Wu%20and%20Makoto%20Onizuka%20and%20Ryosuke%20Shibasaki%20and%20Chuan%20Xiao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20using%20Large%20Language%20Models%20%28LLMs%29%0Aintegrated%20into%20an%20agent%20framework%20for%20flexible%20and%20efficient%20personal%20mobility%0Ageneration.%20LLMs%20overcome%20the%20limitations%20of%20previous%20models%20by%20efficiently%0Aprocessing%20semantic%20data%20and%20offering%20versatility%20in%20modeling%20various%20tasks.%0AOur%20approach%20addresses%20the%20critical%20need%20to%20align%20LLMs%20with%20real-world%20urban%0Amobility%20data%2C%20focusing%20on%20three%20research%20questions%3A%20aligning%20LLMs%20with%20rich%0Aactivity%20data%2C%20developing%20reliable%20activity%20generation%20strategies%2C%20and%0Aexploring%20LLM%20applications%20in%20urban%20mobility.%20The%20key%20technical%20contribution%20is%0Aa%20novel%20LLM%20agent%20framework%20that%20accounts%20for%20individual%20activity%20patterns%20and%0Amotivations%2C%20including%20a%20self-consistency%20approach%20to%20align%20LLMs%20with%0Areal-world%20activity%20data%20and%20a%20retrieval-augmented%20strategy%20for%20interpretable%0Aactivity%20generation.%20In%20experimental%20studies%2C%20comprehensive%20validation%20is%0Aperformed%20using%20real-world%20data.%20This%20research%20marks%20the%20pioneering%20work%20of%0Adesigning%20an%20LLM%20agent%20framework%20for%20activity%20generation%20based%20on%20real-world%0Ahuman%20activity%20data%2C%20offering%20a%20promising%20tool%20for%20urban%20mobility%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14744v1&entry.124074799=Read"},
{"title": "Overcoming Dimensional Collapse in Self-supervised Contrastive Learning\n  for Medical Image Segmentation", "author": "Jamshid Hassanpour and Vinkle Srivastav and Didier Mutter and Nicolas Padoy", "abstract": "  Self-supervised learning (SSL) approaches have achieved great success when\nthe amount of labeled data is limited. Within SSL, models learn robust feature\nrepresentations by solving pretext tasks. One such pretext task is contrastive\nlearning, which involves forming pairs of similar and dissimilar input samples,\nguiding the model to distinguish between them. In this work, we investigate the\napplication of contrastive learning to the domain of medical image analysis.\nOur findings reveal that MoCo v2, a state-of-the-art contrastive learning\nmethod, encounters dimensional collapse when applied to medical images. This is\nattributed to the high degree of inter-image similarity shared between the\nmedical images. To address this, we propose two key contributions: local\nfeature learning and feature decorrelation. Local feature learning improves the\nability of the model to focus on the local regions of the image, while feature\ndecorrelation removes the linear dependence among the features. Our\nexperimental findings demonstrate that our contributions significantly enhance\nthe model's performance in the downstream task of medical segmentation, both in\nthe linear evaluation and full fine-tuning settings. This work illustrates the\nimportance of effectively adapting SSL techniques to the characteristics of\nmedical imaging tasks.\n", "link": "http://arxiv.org/abs/2402.14611v1", "date": "2024-02-22", "relevancy": 2.0203, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4985}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4871}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Dimensional%20Collapse%20in%20Self-supervised%20Contrastive%20Learning%0A%20%20for%20Medical%20Image%20Segmentation&entry.906535625=Jamshid%20Hassanpour%20and%20Vinkle%20Srivastav%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20approaches%20have%20achieved%20great%20success%20when%0Athe%20amount%20of%20labeled%20data%20is%20limited.%20Within%20SSL%2C%20models%20learn%20robust%20feature%0Arepresentations%20by%20solving%20pretext%20tasks.%20One%20such%20pretext%20task%20is%20contrastive%0Alearning%2C%20which%20involves%20forming%20pairs%20of%20similar%20and%20dissimilar%20input%20samples%2C%0Aguiding%20the%20model%20to%20distinguish%20between%20them.%20In%20this%20work%2C%20we%20investigate%20the%0Aapplication%20of%20contrastive%20learning%20to%20the%20domain%20of%20medical%20image%20analysis.%0AOur%20findings%20reveal%20that%20MoCo%20v2%2C%20a%20state-of-the-art%20contrastive%20learning%0Amethod%2C%20encounters%20dimensional%20collapse%20when%20applied%20to%20medical%20images.%20This%20is%0Aattributed%20to%20the%20high%20degree%20of%20inter-image%20similarity%20shared%20between%20the%0Amedical%20images.%20To%20address%20this%2C%20we%20propose%20two%20key%20contributions%3A%20local%0Afeature%20learning%20and%20feature%20decorrelation.%20Local%20feature%20learning%20improves%20the%0Aability%20of%20the%20model%20to%20focus%20on%20the%20local%20regions%20of%20the%20image%2C%20while%20feature%0Adecorrelation%20removes%20the%20linear%20dependence%20among%20the%20features.%20Our%0Aexperimental%20findings%20demonstrate%20that%20our%20contributions%20significantly%20enhance%0Athe%20model%27s%20performance%20in%20the%20downstream%20task%20of%20medical%20segmentation%2C%20both%20in%0Athe%20linear%20evaluation%20and%20full%20fine-tuning%20settings.%20This%20work%20illustrates%20the%0Aimportance%20of%20effectively%20adapting%20SSL%20techniques%20to%20the%20characteristics%20of%0Amedical%20imaging%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14611v1&entry.124074799=Read"},
{"title": "FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View\n  Synthesis", "author": "Yan Xing and Pan Wang and Ligang Liu and Daolun Li and Li Zhang", "abstract": "  We present a novel framework, called FrameNeRF, designed to apply\noff-the-shelf fast high-fidelity NeRF models with fast training speed and high\nrendering quality for few-shot novel view synthesis tasks. The training\nstability of fast high-fidelity models is typically constrained to dense views,\nmaking them unsuitable for few-shot novel view synthesis tasks. To address this\nlimitation, we utilize a regularization model as a data generator to produce\ndense views from sparse inputs, facilitating subsequent training of fast\nhigh-fidelity models. Since these dense views are pseudo ground truth generated\nby the regularization model, original sparse images are then used to fine-tune\nthe fast high-fidelity model. This process helps the model learn realistic\ndetails and correct artifacts introduced in earlier stages. By leveraging an\noff-the-shelf regularization model and a fast high-fidelity model, our approach\nachieves state-of-the-art performance across various benchmark datasets.\n", "link": "http://arxiv.org/abs/2402.14586v1", "date": "2024-02-22", "relevancy": 2.0177, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.518}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5173}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4861}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FrameNeRF%3A%20A%20Simple%20and%20Efficient%20Framework%20for%20Few-shot%20Novel%20View%0A%20%20Synthesis&entry.906535625=Yan%20Xing%20and%20Pan%20Wang%20and%20Ligang%20Liu%20and%20Daolun%20Li%20and%20Li%20Zhang&entry.1292438233=%20%20We%20present%20a%20novel%20framework%2C%20called%20FrameNeRF%2C%20designed%20to%20apply%0Aoff-the-shelf%20fast%20high-fidelity%20NeRF%20models%20with%20fast%20training%20speed%20and%20high%0Arendering%20quality%20for%20few-shot%20novel%20view%20synthesis%20tasks.%20The%20training%0Astability%20of%20fast%20high-fidelity%20models%20is%20typically%20constrained%20to%20dense%20views%2C%0Amaking%20them%20unsuitable%20for%20few-shot%20novel%20view%20synthesis%20tasks.%20To%20address%20this%0Alimitation%2C%20we%20utilize%20a%20regularization%20model%20as%20a%20data%20generator%20to%20produce%0Adense%20views%20from%20sparse%20inputs%2C%20facilitating%20subsequent%20training%20of%20fast%0Ahigh-fidelity%20models.%20Since%20these%20dense%20views%20are%20pseudo%20ground%20truth%20generated%0Aby%20the%20regularization%20model%2C%20original%20sparse%20images%20are%20then%20used%20to%20fine-tune%0Athe%20fast%20high-fidelity%20model.%20This%20process%20helps%20the%20model%20learn%20realistic%0Adetails%20and%20correct%20artifacts%20introduced%20in%20earlier%20stages.%20By%20leveraging%20an%0Aoff-the-shelf%20regularization%20model%20and%20a%20fast%20high-fidelity%20model%2C%20our%20approach%0Aachieves%20state-of-the-art%20performance%20across%20various%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14586v1&entry.124074799=Read"},
{"title": "Tug-of-War Between Knowledge: Exploring and Resolving Knowledge\n  Conflicts in Retrieval-Augmented Language Models", "author": "Zhuoran Jin and Pengfei Cao and Yubo Chen and Kang Liu and Xiaojian Jiang and Jiexin Xu and Qiuxia Li and Jun Zhao", "abstract": "  Retrieval-augmented language models (RALMs) have demonstrated significant\npotential in refining and expanding their internal memory by retrieving\nevidence from external sources. However, RALMs will inevitably encounter\nknowledge conflicts when integrating their internal memory with external\nsources. Knowledge conflicts can ensnare RALMs in a tug-of-war between\nknowledge, limiting their practical applicability. In this paper, we focus on\nexploring and resolving knowledge conflicts in RALMs. First, we present an\nevaluation framework for assessing knowledge conflicts across various\ndimensions. Then, we investigate the behavior and preference of RALMs from the\nfollowing two perspectives: (1) Conflicts between internal memory and external\nsources: We find that stronger RALMs emerge with the Dunning-Kruger effect,\npersistently favoring their faulty internal memory even when correct evidence\nis provided. Besides, RALMs exhibit an availability bias towards common\nknowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:\nWe reveal that RALMs follow the principle of majority rule, leaning towards\nplacing trust in evidence that appears more frequently. Moreover, we find that\nRALMs exhibit confirmation bias, and are more willing to choose evidence that\nis consistent with their internal memory. To solve the challenge of knowledge\nconflicts, we propose a method called Conflict-Disentangle Contrastive Decoding\n(CD2) to better calibrate the model's confidence. Experimental results\ndemonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.\n", "link": "http://arxiv.org/abs/2402.14409v1", "date": "2024-02-22", "relevancy": 2.0115, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5211}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5203}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4781}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tug-of-War%20Between%20Knowledge%3A%20Exploring%20and%20Resolving%20Knowledge%0A%20%20Conflicts%20in%20Retrieval-Augmented%20Language%20Models&entry.906535625=Zhuoran%20Jin%20and%20Pengfei%20Cao%20and%20Yubo%20Chen%20and%20Kang%20Liu%20and%20Xiaojian%20Jiang%20and%20Jiexin%20Xu%20and%20Qiuxia%20Li%20and%20Jun%20Zhao&entry.1292438233=%20%20Retrieval-augmented%20language%20models%20%28RALMs%29%20have%20demonstrated%20significant%0Apotential%20in%20refining%20and%20expanding%20their%20internal%20memory%20by%20retrieving%0Aevidence%20from%20external%20sources.%20However%2C%20RALMs%20will%20inevitably%20encounter%0Aknowledge%20conflicts%20when%20integrating%20their%20internal%20memory%20with%20external%0Asources.%20Knowledge%20conflicts%20can%20ensnare%20RALMs%20in%20a%20tug-of-war%20between%0Aknowledge%2C%20limiting%20their%20practical%20applicability.%20In%20this%20paper%2C%20we%20focus%20on%0Aexploring%20and%20resolving%20knowledge%20conflicts%20in%20RALMs.%20First%2C%20we%20present%20an%0Aevaluation%20framework%20for%20assessing%20knowledge%20conflicts%20across%20various%0Adimensions.%20Then%2C%20we%20investigate%20the%20behavior%20and%20preference%20of%20RALMs%20from%20the%0Afollowing%20two%20perspectives%3A%20%281%29%20Conflicts%20between%20internal%20memory%20and%20external%0Asources%3A%20We%20find%20that%20stronger%20RALMs%20emerge%20with%20the%20Dunning-Kruger%20effect%2C%0Apersistently%20favoring%20their%20faulty%20internal%20memory%20even%20when%20correct%20evidence%0Ais%20provided.%20Besides%2C%20RALMs%20exhibit%20an%20availability%20bias%20towards%20common%0Aknowledge%3B%20%282%29%20Conflicts%20between%20truthful%2C%20irrelevant%20and%20misleading%20evidence%3A%0AWe%20reveal%20that%20RALMs%20follow%20the%20principle%20of%20majority%20rule%2C%20leaning%20towards%0Aplacing%20trust%20in%20evidence%20that%20appears%20more%20frequently.%20Moreover%2C%20we%20find%20that%0ARALMs%20exhibit%20confirmation%20bias%2C%20and%20are%20more%20willing%20to%20choose%20evidence%20that%0Ais%20consistent%20with%20their%20internal%20memory.%20To%20solve%20the%20challenge%20of%20knowledge%0Aconflicts%2C%20we%20propose%20a%20method%20called%20Conflict-Disentangle%20Contrastive%20Decoding%0A%28CD2%29%20to%20better%20calibrate%20the%20model%27s%20confidence.%20Experimental%20results%0Ademonstrate%20that%20our%20CD2%20can%20effectively%20resolve%20knowledge%20conflicts%20in%20RALMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14409v1&entry.124074799=Read"},
{"title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond", "author": "Xinyu Wang and Hainiu Xu and Lin Gui and Yulan He", "abstract": "  Task embedding, a meta-learning technique that captures task-specific\ninformation, has become prevalent, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradientfree manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To unleash the\npower of task embedding in the era of LLMs, we propose a framework for unified\ntask embeddings (FUTE), harmonizing task embeddings from various models,\nincluding smaller language models and LLMs with varied prompts, within a single\nvector space. Such uniformity enables the comparison and analysis of\nsimilarities amongst different models, extending the scope and utility of\nexisting task embedding methods in addressing multi-model scenarios, whilst\nmaintaining their performance to be comparable to architecture-specific\nmethods.\n", "link": "http://arxiv.org/abs/2402.14522v1", "date": "2024-02-22", "relevancy": 2.0054, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5052}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4936}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unified%20Task%20Embeddings%20Across%20Multiple%20Models%3A%20Bridging%20the%20Gap%0A%20%20for%20Prompt-Based%20Large%20Language%20Models%20and%20Beyond&entry.906535625=Xinyu%20Wang%20and%20Hainiu%20Xu%20and%20Lin%20Gui%20and%20Yulan%20He&entry.1292438233=%20%20Task%20embedding%2C%20a%20meta-learning%20technique%20that%20captures%20task-specific%0Ainformation%2C%20has%20become%20prevalent%2C%20especially%20in%20areas%20such%20as%20multi-task%0Alearning%2C%20model%20editing%2C%20and%20interpretability.%20However%2C%20it%20faces%20challenges%0Awith%20the%20emergence%20of%20prompt-guided%20Large%20Language%20Models%20%28LLMs%29%20operating%20in%20a%0Agradientfree%20manner.%20Existing%20task%20embedding%20methods%20rely%20on%20fine-tuned%2C%0Atask-specific%20language%20models%2C%20which%20hinders%20the%20adaptability%20of%20task%0Aembeddings%20across%20diverse%20models%2C%20especially%20prompt-based%20LLMs.%20To%20unleash%20the%0Apower%20of%20task%20embedding%20in%20the%20era%20of%20LLMs%2C%20we%20propose%20a%20framework%20for%20unified%0Atask%20embeddings%20%28FUTE%29%2C%20harmonizing%20task%20embeddings%20from%20various%20models%2C%0Aincluding%20smaller%20language%20models%20and%20LLMs%20with%20varied%20prompts%2C%20within%20a%20single%0Avector%20space.%20Such%20uniformity%20enables%20the%20comparison%20and%20analysis%20of%0Asimilarities%20amongst%20different%20models%2C%20extending%20the%20scope%20and%20utility%20of%0Aexisting%20task%20embedding%20methods%20in%20addressing%20multi-model%20scenarios%2C%20whilst%0Amaintaining%20their%20performance%20to%20be%20comparable%20to%20architecture-specific%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14522v1&entry.124074799=Read"},
{"title": "Rethinking Invariance Regularization in Adversarial Training to Improve\n  Robustness-Accuracy Trade-off", "author": "Futa Waseda and Isao Echizen", "abstract": "  Although adversarial training has been the state-of-the-art approach to\ndefend against adversarial examples (AEs), they suffer from a\nrobustness-accuracy trade-off. In this work, we revisit representation-based\ninvariance regularization to learn discriminative yet adversarially invariant\nrepresentations, aiming to mitigate this trade-off. We empirically identify two\nkey issues hindering invariance regularization: (1) a \"gradient conflict\"\nbetween invariance loss and classification objectives, indicating the existence\nof \"collapsing solutions,\" and (2) the mixture distribution problem arising\nfrom diverged distributions of clean and adversarial inputs. To address these\nissues, we propose Asymmetrically Representation-regularized Adversarial\nTraining (AR-AT), which incorporates a stop-gradient operation and a pre-dictor\nin the invariance loss to avoid \"collapsing solutions,\" inspired by a recent\nnon-contrastive self-supervised learning approach, and a split-BatchNorm (BN)\nstructure to resolve the mixture distribution problem. Our method significantly\nimproves the robustness-accuracy trade-off by learning adversarially invariant\nrepresentations without sacrificing discriminative power. Furthermore, we\ndiscuss the relevance of our findings to knowledge-distillation-based defense\nmethods, contributing to a deeper understanding of their relative successes.\n", "link": "http://arxiv.org/abs/2402.14648v1", "date": "2024-02-22", "relevancy": 2.0002, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5213}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Invariance%20Regularization%20in%20Adversarial%20Training%20to%20Improve%0A%20%20Robustness-Accuracy%20Trade-off&entry.906535625=Futa%20Waseda%20and%20Isao%20Echizen&entry.1292438233=%20%20Although%20adversarial%20training%20has%20been%20the%20state-of-the-art%20approach%20to%0Adefend%20against%20adversarial%20examples%20%28AEs%29%2C%20they%20suffer%20from%20a%0Arobustness-accuracy%20trade-off.%20In%20this%20work%2C%20we%20revisit%20representation-based%0Ainvariance%20regularization%20to%20learn%20discriminative%20yet%20adversarially%20invariant%0Arepresentations%2C%20aiming%20to%20mitigate%20this%20trade-off.%20We%20empirically%20identify%20two%0Akey%20issues%20hindering%20invariance%20regularization%3A%20%281%29%20a%20%22gradient%20conflict%22%0Abetween%20invariance%20loss%20and%20classification%20objectives%2C%20indicating%20the%20existence%0Aof%20%22collapsing%20solutions%2C%22%20and%20%282%29%20the%20mixture%20distribution%20problem%20arising%0Afrom%20diverged%20distributions%20of%20clean%20and%20adversarial%20inputs.%20To%20address%20these%0Aissues%2C%20we%20propose%20Asymmetrically%20Representation-regularized%20Adversarial%0ATraining%20%28AR-AT%29%2C%20which%20incorporates%20a%20stop-gradient%20operation%20and%20a%20pre-dictor%0Ain%20the%20invariance%20loss%20to%20avoid%20%22collapsing%20solutions%2C%22%20inspired%20by%20a%20recent%0Anon-contrastive%20self-supervised%20learning%20approach%2C%20and%20a%20split-BatchNorm%20%28BN%29%0Astructure%20to%20resolve%20the%20mixture%20distribution%20problem.%20Our%20method%20significantly%0Aimproves%20the%20robustness-accuracy%20trade-off%20by%20learning%20adversarially%20invariant%0Arepresentations%20without%20sacrificing%20discriminative%20power.%20Furthermore%2C%20we%0Adiscuss%20the%20relevance%20of%20our%20findings%20to%20knowledge-distillation-based%20defense%0Amethods%2C%20contributing%20to%20a%20deeper%20understanding%20of%20their%20relative%20successes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14648v1&entry.124074799=Read"},
{"title": "Bayesian Off-Policy Evaluation and Learning for Large Action Spaces", "author": "Imad Aouali and Victor-Emmanuel Brunel and David Rohde and Anna Korba", "abstract": "  In interactive systems, actions are often correlated, presenting an\nopportunity for more sample-efficient off-policy evaluation (OPE) and learning\n(OPL) in large action spaces. We introduce a unified Bayesian framework to\ncapture these correlations through structured and informative priors. In this\nframework, we propose sDM, a generic Bayesian approach designed for OPE and\nOPL, grounded in both algorithmic and theoretical foundations. Notably, sDM\nleverages action correlations without compromising computational efficiency.\nMoreover, inspired by online Bayesian bandits, we introduce Bayesian metrics\nthat assess the average performance of algorithms across multiple problem\ninstances, deviating from the conventional worst-case assessments. We analyze\nsDM in OPE and OPL, highlighting the benefits of leveraging action\ncorrelations. Empirical evidence showcases the strong performance of sDM.\n", "link": "http://arxiv.org/abs/2402.14664v1", "date": "2024-02-22", "relevancy": 1.997, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5936}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4595}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Off-Policy%20Evaluation%20and%20Learning%20for%20Large%20Action%20Spaces&entry.906535625=Imad%20Aouali%20and%20Victor-Emmanuel%20Brunel%20and%20David%20Rohde%20and%20Anna%20Korba&entry.1292438233=%20%20In%20interactive%20systems%2C%20actions%20are%20often%20correlated%2C%20presenting%20an%0Aopportunity%20for%20more%20sample-efficient%20off-policy%20evaluation%20%28OPE%29%20and%20learning%0A%28OPL%29%20in%20large%20action%20spaces.%20We%20introduce%20a%20unified%20Bayesian%20framework%20to%0Acapture%20these%20correlations%20through%20structured%20and%20informative%20priors.%20In%20this%0Aframework%2C%20we%20propose%20sDM%2C%20a%20generic%20Bayesian%20approach%20designed%20for%20OPE%20and%0AOPL%2C%20grounded%20in%20both%20algorithmic%20and%20theoretical%20foundations.%20Notably%2C%20sDM%0Aleverages%20action%20correlations%20without%20compromising%20computational%20efficiency.%0AMoreover%2C%20inspired%20by%20online%20Bayesian%20bandits%2C%20we%20introduce%20Bayesian%20metrics%0Athat%20assess%20the%20average%20performance%20of%20algorithms%20across%20multiple%20problem%0Ainstances%2C%20deviating%20from%20the%20conventional%20worst-case%20assessments.%20We%20analyze%0AsDM%20in%20OPE%20and%20OPL%2C%20highlighting%20the%20benefits%20of%20leveraging%20action%0Acorrelations.%20Empirical%20evidence%20showcases%20the%20strong%20performance%20of%20sDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14664v1&entry.124074799=Read"},
{"title": "Modeling and Numerical Analysis of Kangaroo Lower Body based on\n  Constrained Dynamics of Hybrid Serial-Parallel Floating-Base Systems", "author": "Enrico Mingo Hoffman and Andrea Curti and Narcis Miguel and Sai Kishor Kothakota and Alberto Molina and Adria Roig and Luca Marchionni", "abstract": "  This paper presents the modeling and numerical analysis of the Kangaroo lower\nbody prototype, a novel bipedal humanoid robot developed and manufactured by\nPAL Robotics. Kangaroo features high-power linear electric actuators combined\nwith unique serial-parallel hybrid chains, which allow for the positioning of\nall the leg actuators near the base of the robot in order to improve the\noverall mass distribution. To model and analyze such complex nonlinear\nmechanisms, we employ a constrained formulation that is extended to account for\nfloating-base systems in contact with the environment. A comparison is made to\ndemonstrate the significant improvements achieved with TALOS, another humanoid\nbipedal robot designed by PAL Robotics, in terms of equivalent Cartesian\ninertia at the feet and centroidal angular momentum. Finally, the paper\nincludes numerical experiments conducted through simulation and preliminary\ntests performed on the actual Kangaroo platform.\n", "link": "http://arxiv.org/abs/2312.04161v2", "date": "2024-02-22", "relevancy": 1.9937, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5563}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4912}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4826}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20and%20Numerical%20Analysis%20of%20Kangaroo%20Lower%20Body%20based%20on%0A%20%20Constrained%20Dynamics%20of%20Hybrid%20Serial-Parallel%20Floating-Base%20Systems&entry.906535625=Enrico%20Mingo%20Hoffman%20and%20Andrea%20Curti%20and%20Narcis%20Miguel%20and%20Sai%20Kishor%20Kothakota%20and%20Alberto%20Molina%20and%20Adria%20Roig%20and%20Luca%20Marchionni&entry.1292438233=%20%20This%20paper%20presents%20the%20modeling%20and%20numerical%20analysis%20of%20the%20Kangaroo%20lower%0Abody%20prototype%2C%20a%20novel%20bipedal%20humanoid%20robot%20developed%20and%20manufactured%20by%0APAL%20Robotics.%20Kangaroo%20features%20high-power%20linear%20electric%20actuators%20combined%0Awith%20unique%20serial-parallel%20hybrid%20chains%2C%20which%20allow%20for%20the%20positioning%20of%0Aall%20the%20leg%20actuators%20near%20the%20base%20of%20the%20robot%20in%20order%20to%20improve%20the%0Aoverall%20mass%20distribution.%20To%20model%20and%20analyze%20such%20complex%20nonlinear%0Amechanisms%2C%20we%20employ%20a%20constrained%20formulation%20that%20is%20extended%20to%20account%20for%0Afloating-base%20systems%20in%20contact%20with%20the%20environment.%20A%20comparison%20is%20made%20to%0Ademonstrate%20the%20significant%20improvements%20achieved%20with%20TALOS%2C%20another%20humanoid%0Abipedal%20robot%20designed%20by%20PAL%20Robotics%2C%20in%20terms%20of%20equivalent%20Cartesian%0Ainertia%20at%20the%20feet%20and%20centroidal%20angular%20momentum.%20Finally%2C%20the%20paper%0Aincludes%20numerical%20experiments%20conducted%20through%20simulation%20and%20preliminary%0Atests%20performed%20on%20the%20actual%20Kangaroo%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04161v2&entry.124074799=Read"},
{"title": "ACE : Off-Policy Actor-Critic with Causality-Aware Entropy\n  Regularization", "author": "Tianying Ji and Yongyuan Liang and Yan Zeng and Yu Luo and Guowei Xu and Jiawei Guo and Ruijie Zheng and Furong Huang and Fuchun Sun and Huazhe Xu", "abstract": "  The varying significance of distinct primitive behaviors during the policy\nlearning process has been overlooked by prior model-free RL algorithms.\nLeveraging this insight, we explore the causal relationship between different\naction dimensions and rewards to evaluate the significance of various primitive\nbehaviors during training. We introduce a causality-aware entropy term that\neffectively identifies and prioritizes actions with high potential impacts for\nefficient exploration. Furthermore, to prevent excessive focus on specific\nprimitive behaviors, we analyze the gradient dormancy phenomenon and introduce\na dormancy-guided reset mechanism to further enhance the efficacy of our\nmethod. Our proposed algorithm, ACE: Off-policy Actor-critic with\nCausality-aware Entropy regularization, demonstrates a substantial performance\nadvantage across 29 diverse continuous control tasks spanning 7 domains\ncompared to model-free RL baselines, which underscores the effectiveness,\nversatility, and efficient sample efficiency of our approach. Benchmark results\nand videos are available at https://ace-rl.github.io/.\n", "link": "http://arxiv.org/abs/2402.14528v1", "date": "2024-02-22", "relevancy": 1.9917, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4791}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%20%3A%20Off-Policy%20Actor-Critic%20with%20Causality-Aware%20Entropy%0A%20%20Regularization&entry.906535625=Tianying%20Ji%20and%20Yongyuan%20Liang%20and%20Yan%20Zeng%20and%20Yu%20Luo%20and%20Guowei%20Xu%20and%20Jiawei%20Guo%20and%20Ruijie%20Zheng%20and%20Furong%20Huang%20and%20Fuchun%20Sun%20and%20Huazhe%20Xu&entry.1292438233=%20%20The%20varying%20significance%20of%20distinct%20primitive%20behaviors%20during%20the%20policy%0Alearning%20process%20has%20been%20overlooked%20by%20prior%20model-free%20RL%20algorithms.%0ALeveraging%20this%20insight%2C%20we%20explore%20the%20causal%20relationship%20between%20different%0Aaction%20dimensions%20and%20rewards%20to%20evaluate%20the%20significance%20of%20various%20primitive%0Abehaviors%20during%20training.%20We%20introduce%20a%20causality-aware%20entropy%20term%20that%0Aeffectively%20identifies%20and%20prioritizes%20actions%20with%20high%20potential%20impacts%20for%0Aefficient%20exploration.%20Furthermore%2C%20to%20prevent%20excessive%20focus%20on%20specific%0Aprimitive%20behaviors%2C%20we%20analyze%20the%20gradient%20dormancy%20phenomenon%20and%20introduce%0Aa%20dormancy-guided%20reset%20mechanism%20to%20further%20enhance%20the%20efficacy%20of%20our%0Amethod.%20Our%20proposed%20algorithm%2C%20ACE%3A%20Off-policy%20Actor-critic%20with%0ACausality-aware%20Entropy%20regularization%2C%20demonstrates%20a%20substantial%20performance%0Aadvantage%20across%2029%20diverse%20continuous%20control%20tasks%20spanning%207%20domains%0Acompared%20to%20model-free%20RL%20baselines%2C%20which%20underscores%20the%20effectiveness%2C%0Aversatility%2C%20and%20efficient%20sample%20efficiency%20of%20our%20approach.%20Benchmark%20results%0Aand%20videos%20are%20available%20at%20https%3A//ace-rl.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14528v1&entry.124074799=Read"},
{"title": "Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion", "author": "Mike Laszkiewicz and Jonas Ricker and Johannes Lederer and Asja Fischer", "abstract": "  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n", "link": "http://arxiv.org/abs/2306.06210v4", "date": "2024-02-22", "relevancy": 1.9889, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5035}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4893}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion&entry.906535625=Mike%20Laszkiewicz%20and%20Jonas%20Ricker%20and%20Johannes%20Lederer%20and%20Asja%20Fischer&entry.1292438233=%20%20Recent%20breakthroughs%20in%20generative%20modeling%20have%20sparked%20interest%20in%0Apractical%20single-model%20attribution.%20Such%20methods%20predict%20whether%20a%20sample%20was%0Agenerated%20by%20a%20specific%20generator%20or%20not%2C%20for%20instance%2C%20to%20prove%20intellectual%0Aproperty%20theft.%20However%2C%20previous%20works%20are%20either%20limited%20to%20the%20closed-world%0Asetting%20or%20require%20undesirable%20changes%20to%20the%20generative%20model.%20We%20address%0Athese%20shortcomings%20by%2C%20first%2C%20viewing%20single-model%20attribution%20through%20the%20lens%0Aof%20anomaly%20detection.%20Arising%20from%20this%20change%20of%20perspective%2C%20we%20propose%0AFLIPAD%2C%20a%20new%20approach%20for%20single-model%20attribution%20in%20the%20open-world%20setting%0Abased%20on%20final-layer%20inversion%20and%20anomaly%20detection.%20We%20show%20that%20the%20utilized%0Afinal-layer%20inversion%20can%20be%20reduced%20to%20a%20convex%20lasso%20optimization%20problem%2C%0Amaking%20our%20approach%20theoretically%20sound%20and%20computationally%20efficient.%20The%0Atheoretical%20findings%20are%20accompanied%20by%20an%20experimental%20study%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach%20and%20its%20flexibility%20to%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06210v4&entry.124074799=Read"},
{"title": "A Framework for Variational Inference of Lightweight Bayesian Neural\n  Networks with Heteroscedastic Uncertainties", "author": "David J. Schodt and Ryan Brown and Michael Merritt and Samuel Park and Delsin Menolascino and Mark A. Peot", "abstract": "  Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural\nNetwork (BNN) is vital to many applications. Often, heteroscedastic aleatoric\nuncertainties are learned as outputs of the BNN in addition to the predictive\nmeans, however doing so may necessitate adding more learnable parameters to the\nnetwork. In this work, we demonstrate that both the heteroscedastic aleatoric\nand epistemic variance can be embedded into the variances of learned BNN\nparameters, improving predictive performance for lightweight networks. By\ncomplementing this approach with a moment propagation approach to inference, we\nintroduce a relatively simple framework for sampling-free variational inference\nsuitable for lightweight BNNs.\n", "link": "http://arxiv.org/abs/2402.14532v1", "date": "2024-02-22", "relevancy": 1.9735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4804}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Variational%20Inference%20of%20Lightweight%20Bayesian%20Neural%0A%20%20Networks%20with%20Heteroscedastic%20Uncertainties&entry.906535625=David%20J.%20Schodt%20and%20Ryan%20Brown%20and%20Michael%20Merritt%20and%20Samuel%20Park%20and%20Delsin%20Menolascino%20and%20Mark%20A.%20Peot&entry.1292438233=%20%20Obtaining%20heteroscedastic%20predictive%20uncertainties%20from%20a%20Bayesian%20Neural%0ANetwork%20%28BNN%29%20is%20vital%20to%20many%20applications.%20Often%2C%20heteroscedastic%20aleatoric%0Auncertainties%20are%20learned%20as%20outputs%20of%20the%20BNN%20in%20addition%20to%20the%20predictive%0Ameans%2C%20however%20doing%20so%20may%20necessitate%20adding%20more%20learnable%20parameters%20to%20the%0Anetwork.%20In%20this%20work%2C%20we%20demonstrate%20that%20both%20the%20heteroscedastic%20aleatoric%0Aand%20epistemic%20variance%20can%20be%20embedded%20into%20the%20variances%20of%20learned%20BNN%0Aparameters%2C%20improving%20predictive%20performance%20for%20lightweight%20networks.%20By%0Acomplementing%20this%20approach%20with%20a%20moment%20propagation%20approach%20to%20inference%2C%20we%0Aintroduce%20a%20relatively%20simple%20framework%20for%20sampling-free%20variational%20inference%0Asuitable%20for%20lightweight%20BNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14532v1&entry.124074799=Read"},
{"title": "Transition State Clustering for Interaction Segmentation and Learning", "author": "Fabian Hahne and Vignesh Prasad and Alap Kshirsagar and Dorothea Koert and Ruth Maria Stock-Homburg and Jan Peters and Georgia Chalvatzaki", "abstract": "  Hidden Markov Models with an underlying Mixture of Gaussian structure have\nproven effective in learning Human-Robot Interactions from demonstrations for\nvarious interactive tasks via Gaussian Mixture Regression. However, a mismatch\noccurs when segmenting the interaction using only the observed state of the\nhuman compared to the joint state of the human and the robot. To enhance this\nunderlying segmentation and subsequently the predictive abilities of such\nGaussian Mixture-based approaches, we take a hierarchical approach by learning\nan additional mixture distribution on the states at the transition boundary.\nThis helps prevent misclassifications that usually occur in such states. We\nfind that our framework improves the performance of the underlying Gaussian\nMixture-based approach, which we evaluate on various interactive tasks such as\nhandshaking and fistbumps.\n", "link": "http://arxiv.org/abs/2402.14548v1", "date": "2024-02-22", "relevancy": 1.9702, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5038}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4645}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transition%20State%20Clustering%20for%20Interaction%20Segmentation%20and%20Learning&entry.906535625=Fabian%20Hahne%20and%20Vignesh%20Prasad%20and%20Alap%20Kshirsagar%20and%20Dorothea%20Koert%20and%20Ruth%20Maria%20Stock-Homburg%20and%20Jan%20Peters%20and%20Georgia%20Chalvatzaki&entry.1292438233=%20%20Hidden%20Markov%20Models%20with%20an%20underlying%20Mixture%20of%20Gaussian%20structure%20have%0Aproven%20effective%20in%20learning%20Human-Robot%20Interactions%20from%20demonstrations%20for%0Avarious%20interactive%20tasks%20via%20Gaussian%20Mixture%20Regression.%20However%2C%20a%20mismatch%0Aoccurs%20when%20segmenting%20the%20interaction%20using%20only%20the%20observed%20state%20of%20the%0Ahuman%20compared%20to%20the%20joint%20state%20of%20the%20human%20and%20the%20robot.%20To%20enhance%20this%0Aunderlying%20segmentation%20and%20subsequently%20the%20predictive%20abilities%20of%20such%0AGaussian%20Mixture-based%20approaches%2C%20we%20take%20a%20hierarchical%20approach%20by%20learning%0Aan%20additional%20mixture%20distribution%20on%20the%20states%20at%20the%20transition%20boundary.%0AThis%20helps%20prevent%20misclassifications%20that%20usually%20occur%20in%20such%20states.%20We%0Afind%20that%20our%20framework%20improves%20the%20performance%20of%20the%20underlying%20Gaussian%0AMixture-based%20approach%2C%20which%20we%20evaluate%20on%20various%20interactive%20tasks%20such%20as%0Ahandshaking%20and%20fistbumps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14548v1&entry.124074799=Read"},
{"title": "Robust Training of Federated Models with Extremely Label Deficiency", "author": "Yonggang Zhang and Zhiqin Yang and Xinmei Tian and Nannan Wang and Tongliang Liu and Bo Han", "abstract": "  Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm\nfor collaboratively training machine learning models using distributed data\nwith label deficiency. Advanced FSSL methods predominantly focus on training a\nsingle model on each client. However, this approach could lead to a discrepancy\nbetween the objective functions of labeled and unlabeled data, resulting in\ngradient conflicts. To alleviate gradient conflict, we propose a novel\ntwin-model paradigm, called Twin-sight, designed to enhance mutual guidance by\nproviding insights from different perspectives of labeled and unlabeled data.\nIn particular, Twin-sight concurrently trains a supervised model with a\nsupervised objective function while training an unsupervised model using an\nunsupervised objective function. To enhance the synergy between these two\nmodels, Twin-sight introduces a neighbourhood-preserving constraint, which\nencourages the preservation of the neighbourhood relationship among data\nfeatures extracted by both models. Our comprehensive experiments on four\nbenchmark datasets provide substantial evidence that Twin-sight can\nsignificantly outperform state-of-the-art methods across various experimental\nsettings, demonstrating the efficacy of the proposed Twin-sight.\n", "link": "http://arxiv.org/abs/2402.14430v1", "date": "2024-02-22", "relevancy": 1.9702, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5247}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4882}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4841}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Training%20of%20Federated%20Models%20with%20Extremely%20Label%20Deficiency&entry.906535625=Yonggang%20Zhang%20and%20Zhiqin%20Yang%20and%20Xinmei%20Tian%20and%20Nannan%20Wang%20and%20Tongliang%20Liu%20and%20Bo%20Han&entry.1292438233=%20%20Federated%20semi-supervised%20learning%20%28FSSL%29%20has%20emerged%20as%20a%20powerful%20paradigm%0Afor%20collaboratively%20training%20machine%20learning%20models%20using%20distributed%20data%0Awith%20label%20deficiency.%20Advanced%20FSSL%20methods%20predominantly%20focus%20on%20training%20a%0Asingle%20model%20on%20each%20client.%20However%2C%20this%20approach%20could%20lead%20to%20a%20discrepancy%0Abetween%20the%20objective%20functions%20of%20labeled%20and%20unlabeled%20data%2C%20resulting%20in%0Agradient%20conflicts.%20To%20alleviate%20gradient%20conflict%2C%20we%20propose%20a%20novel%0Atwin-model%20paradigm%2C%20called%20Twin-sight%2C%20designed%20to%20enhance%20mutual%20guidance%20by%0Aproviding%20insights%20from%20different%20perspectives%20of%20labeled%20and%20unlabeled%20data.%0AIn%20particular%2C%20Twin-sight%20concurrently%20trains%20a%20supervised%20model%20with%20a%0Asupervised%20objective%20function%20while%20training%20an%20unsupervised%20model%20using%20an%0Aunsupervised%20objective%20function.%20To%20enhance%20the%20synergy%20between%20these%20two%0Amodels%2C%20Twin-sight%20introduces%20a%20neighbourhood-preserving%20constraint%2C%20which%0Aencourages%20the%20preservation%20of%20the%20neighbourhood%20relationship%20among%20data%0Afeatures%20extracted%20by%20both%20models.%20Our%20comprehensive%20experiments%20on%20four%0Abenchmark%20datasets%20provide%20substantial%20evidence%20that%20Twin-sight%20can%0Asignificantly%20outperform%20state-of-the-art%20methods%20across%20various%20experimental%0Asettings%2C%20demonstrating%20the%20efficacy%20of%20the%20proposed%20Twin-sight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14430v1&entry.124074799=Read"},
{"title": "Deep hybrid model with satellite imagery: how to combine demand modeling\n  and computer vision for behavior analysis?", "author": "Qingyi Wang and Shenhao Wang and Yunhan Zheng and Hongzhou Lin and Xiaohu Zhang and Jinhua Zhao and Joan Walker", "abstract": "  Classical demand modeling analyzes travel behavior using only low-dimensional\nnumeric data (i.e. sociodemographics and travel attributes) but not\nhigh-dimensional urban imagery. However, travel behavior depends on the factors\nrepresented by both numeric data and urban imagery, thus necessitating a\nsynergetic framework to combine them. This study creates a theoretical\nframework of deep hybrid models with a crossing structure consisting of a\nmixing operator and a behavioral predictor, thus integrating the numeric and\nimagery data into a latent space. Empirically, this framework is applied to\nanalyze travel mode choice using the MyDailyTravel Survey from Chicago as the\nnumeric inputs and the satellite images as the imagery inputs. We found that\ndeep hybrid models outperform both the traditional demand models and the recent\ndeep learning in predicting the aggregate and disaggregate travel behavior with\nour supervision-as-mixing design. The latent space in deep hybrid models can be\ninterpreted, because it reveals meaningful spatial and social patterns. The\ndeep hybrid models can also generate new urban images that do not exist in\nreality and interpret them with economic theory, such as computing substitution\npatterns and social welfare changes. Overall, the deep hybrid models\ndemonstrate the complementarity between the low-dimensional numeric and\nhigh-dimensional imagery data and between the traditional demand modeling and\nrecent deep learning. It generalizes the latent classes and variables in\nclassical hybrid demand models to a latent space, and leverages the\ncomputational power of deep learning for imagery while retaining the economic\ninterpretability on the microeconomics foundation.\n", "link": "http://arxiv.org/abs/2303.04204v2", "date": "2024-02-22", "relevancy": 1.9646, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4999}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4678}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20hybrid%20model%20with%20satellite%20imagery%3A%20how%20to%20combine%20demand%20modeling%0A%20%20and%20computer%20vision%20for%20behavior%20analysis%3F&entry.906535625=Qingyi%20Wang%20and%20Shenhao%20Wang%20and%20Yunhan%20Zheng%20and%20Hongzhou%20Lin%20and%20Xiaohu%20Zhang%20and%20Jinhua%20Zhao%20and%20Joan%20Walker&entry.1292438233=%20%20Classical%20demand%20modeling%20analyzes%20travel%20behavior%20using%20only%20low-dimensional%0Anumeric%20data%20%28i.e.%20sociodemographics%20and%20travel%20attributes%29%20but%20not%0Ahigh-dimensional%20urban%20imagery.%20However%2C%20travel%20behavior%20depends%20on%20the%20factors%0Arepresented%20by%20both%20numeric%20data%20and%20urban%20imagery%2C%20thus%20necessitating%20a%0Asynergetic%20framework%20to%20combine%20them.%20This%20study%20creates%20a%20theoretical%0Aframework%20of%20deep%20hybrid%20models%20with%20a%20crossing%20structure%20consisting%20of%20a%0Amixing%20operator%20and%20a%20behavioral%20predictor%2C%20thus%20integrating%20the%20numeric%20and%0Aimagery%20data%20into%20a%20latent%20space.%20Empirically%2C%20this%20framework%20is%20applied%20to%0Aanalyze%20travel%20mode%20choice%20using%20the%20MyDailyTravel%20Survey%20from%20Chicago%20as%20the%0Anumeric%20inputs%20and%20the%20satellite%20images%20as%20the%20imagery%20inputs.%20We%20found%20that%0Adeep%20hybrid%20models%20outperform%20both%20the%20traditional%20demand%20models%20and%20the%20recent%0Adeep%20learning%20in%20predicting%20the%20aggregate%20and%20disaggregate%20travel%20behavior%20with%0Aour%20supervision-as-mixing%20design.%20The%20latent%20space%20in%20deep%20hybrid%20models%20can%20be%0Ainterpreted%2C%20because%20it%20reveals%20meaningful%20spatial%20and%20social%20patterns.%20The%0Adeep%20hybrid%20models%20can%20also%20generate%20new%20urban%20images%20that%20do%20not%20exist%20in%0Areality%20and%20interpret%20them%20with%20economic%20theory%2C%20such%20as%20computing%20substitution%0Apatterns%20and%20social%20welfare%20changes.%20Overall%2C%20the%20deep%20hybrid%20models%0Ademonstrate%20the%20complementarity%20between%20the%20low-dimensional%20numeric%20and%0Ahigh-dimensional%20imagery%20data%20and%20between%20the%20traditional%20demand%20modeling%20and%0Arecent%20deep%20learning.%20It%20generalizes%20the%20latent%20classes%20and%20variables%20in%0Aclassical%20hybrid%20demand%20models%20to%20a%20latent%20space%2C%20and%20leverages%20the%0Acomputational%20power%20of%20deep%20learning%20for%20imagery%20while%20retaining%20the%20economic%0Ainterpretability%20on%20the%20microeconomics%20foundation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.04204v2&entry.124074799=Read"},
{"title": "Uncertainty Quantification of Spatiotemporal Travel Demand with\n  Probabilistic Graph Neural Networks", "author": "Qingyi Wang and Shenhao Wang and Dingyi Zhuang and Haris Koutsopoulos and Jinhua Zhao", "abstract": "  Recent studies have significantly improved the prediction accuracy of travel\ndemand using graph neural networks. However, these studies largely ignored\nuncertainty that inevitably exists in travel demand prediction. To fill this\ngap, this study proposes a framework of probabilistic graph neural networks\n(Prob-GNN) to quantify the spatiotemporal uncertainty of travel demand. This\nProb-GNN framework is substantiated by deterministic and probabilistic\nassumptions, and empirically applied to the task of predicting the transit and\nridesharing demand in Chicago. We found that the probabilistic assumptions\n(e.g. distribution tail, support) have a greater impact on uncertainty\nprediction than the deterministic ones (e.g. deep modules, depth). Among the\nfamily of Prob-GNNs, the GNNs with truncated Gaussian and Laplace distributions\nachieve the highest performance in transit and ridesharing data. Even under\nsignificant domain shifts, Prob-GNNs can predict the ridership uncertainty in a\nstable manner, when the models are trained on pre-COVID data and tested across\nmultiple periods during and after the COVID-19 pandemic. Prob-GNNs also reveal\nthe spatiotemporal pattern of uncertainty, which is concentrated on the\nafternoon peak hours and the areas with large travel volumes. Overall, our\nfindings highlight the importance of incorporating randomness into deep\nlearning for spatiotemporal ridership prediction. Future research should\ncontinue to investigate versatile probabilistic assumptions to capture\nbehavioral randomness, and further develop methods to quantify uncertainty to\nbuild resilient cities.\n", "link": "http://arxiv.org/abs/2303.04040v2", "date": "2024-02-22", "relevancy": 1.964, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20of%20Spatiotemporal%20Travel%20Demand%20with%0A%20%20Probabilistic%20Graph%20Neural%20Networks&entry.906535625=Qingyi%20Wang%20and%20Shenhao%20Wang%20and%20Dingyi%20Zhuang%20and%20Haris%20Koutsopoulos%20and%20Jinhua%20Zhao&entry.1292438233=%20%20Recent%20studies%20have%20significantly%20improved%20the%20prediction%20accuracy%20of%20travel%0Ademand%20using%20graph%20neural%20networks.%20However%2C%20these%20studies%20largely%20ignored%0Auncertainty%20that%20inevitably%20exists%20in%20travel%20demand%20prediction.%20To%20fill%20this%0Agap%2C%20this%20study%20proposes%20a%20framework%20of%20probabilistic%20graph%20neural%20networks%0A%28Prob-GNN%29%20to%20quantify%20the%20spatiotemporal%20uncertainty%20of%20travel%20demand.%20This%0AProb-GNN%20framework%20is%20substantiated%20by%20deterministic%20and%20probabilistic%0Aassumptions%2C%20and%20empirically%20applied%20to%20the%20task%20of%20predicting%20the%20transit%20and%0Aridesharing%20demand%20in%20Chicago.%20We%20found%20that%20the%20probabilistic%20assumptions%0A%28e.g.%20distribution%20tail%2C%20support%29%20have%20a%20greater%20impact%20on%20uncertainty%0Aprediction%20than%20the%20deterministic%20ones%20%28e.g.%20deep%20modules%2C%20depth%29.%20Among%20the%0Afamily%20of%20Prob-GNNs%2C%20the%20GNNs%20with%20truncated%20Gaussian%20and%20Laplace%20distributions%0Aachieve%20the%20highest%20performance%20in%20transit%20and%20ridesharing%20data.%20Even%20under%0Asignificant%20domain%20shifts%2C%20Prob-GNNs%20can%20predict%20the%20ridership%20uncertainty%20in%20a%0Astable%20manner%2C%20when%20the%20models%20are%20trained%20on%20pre-COVID%20data%20and%20tested%20across%0Amultiple%20periods%20during%20and%20after%20the%20COVID-19%20pandemic.%20Prob-GNNs%20also%20reveal%0Athe%20spatiotemporal%20pattern%20of%20uncertainty%2C%20which%20is%20concentrated%20on%20the%0Aafternoon%20peak%20hours%20and%20the%20areas%20with%20large%20travel%20volumes.%20Overall%2C%20our%0Afindings%20highlight%20the%20importance%20of%20incorporating%20randomness%20into%20deep%0Alearning%20for%20spatiotemporal%20ridership%20prediction.%20Future%20research%20should%0Acontinue%20to%20investigate%20versatile%20probabilistic%20assumptions%20to%20capture%0Abehavioral%20randomness%2C%20and%20further%20develop%20methods%20to%20quantify%20uncertainty%20to%0Abuild%20resilient%20cities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.04040v2&entry.124074799=Read"},
{"title": "S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph\n  Generation in OR", "author": "Jialun Pei and Diandian Guo and Jingyang Zhang and Manxi Lin and Yueming Jin and Pheng-Ann Heng", "abstract": "  Scene graph generation (SGG) of surgical procedures is crucial in enhancing\nholistically cognitive intelligence in the operating room (OR). However,\nprevious works have primarily relied on the multi-stage learning that generates\nsemantic scene graphs dependent on intermediate processes with pose estimation\nand object detection, which may compromise model efficiency and efficacy, also\nimpose extra annotation burden. In this study, we introduce a novel\nsingle-stage bimodal transformer framework for SGG in the OR, termed\nS^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D\npoint clouds for SGG in an end-to-end manner. Concretely, our model embraces a\nView-Sync Transfusion scheme to encourage multi-view visual information\ninteraction. Concurrently, a Geometry-Visual Cohesion operation is designed to\nintegrate the synergic 2D semantic features into 3D point cloud features.\nMoreover, based on the augmented feature, we propose a novel relation-sensitive\ntransformer decoder that embeds dynamic entity-pair queries and relational\ntrait priors, which enables the direct prediction of entity-pair relations for\ngraph generation without intermediate steps. Extensive experiments have\nvalidated the superior SGG performance and lower computational cost of\nS^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3%\nPrecision increase and 24.2M reduction in model parameters. We further compared\nour method with generic single-stage SGG methods with broader metrics for a\ncomprehensive evaluation, with consistently better performance achieved. The\ncode will be made available.\n", "link": "http://arxiv.org/abs/2402.14461v1", "date": "2024-02-22", "relevancy": 1.9635, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4597}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%5E2Former-OR%3A%20Single-Stage%20Bimodal%20Transformer%20for%20Scene%20Graph%0A%20%20Generation%20in%20OR&entry.906535625=Jialun%20Pei%20and%20Diandian%20Guo%20and%20Jingyang%20Zhang%20and%20Manxi%20Lin%20and%20Yueming%20Jin%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20Scene%20graph%20generation%20%28SGG%29%20of%20surgical%20procedures%20is%20crucial%20in%20enhancing%0Aholistically%20cognitive%20intelligence%20in%20the%20operating%20room%20%28OR%29.%20However%2C%0Aprevious%20works%20have%20primarily%20relied%20on%20the%20multi-stage%20learning%20that%20generates%0Asemantic%20scene%20graphs%20dependent%20on%20intermediate%20processes%20with%20pose%20estimation%0Aand%20object%20detection%2C%20which%20may%20compromise%20model%20efficiency%20and%20efficacy%2C%20also%0Aimpose%20extra%20annotation%20burden.%20In%20this%20study%2C%20we%20introduce%20a%20novel%0Asingle-stage%20bimodal%20transformer%20framework%20for%20SGG%20in%20the%20OR%2C%20termed%0AS%5E2Former-OR%2C%20aimed%20to%20complementally%20leverage%20multi-view%202D%20scenes%20and%203D%0Apoint%20clouds%20for%20SGG%20in%20an%20end-to-end%20manner.%20Concretely%2C%20our%20model%20embraces%20a%0AView-Sync%20Transfusion%20scheme%20to%20encourage%20multi-view%20visual%20information%0Ainteraction.%20Concurrently%2C%20a%20Geometry-Visual%20Cohesion%20operation%20is%20designed%20to%0Aintegrate%20the%20synergic%202D%20semantic%20features%20into%203D%20point%20cloud%20features.%0AMoreover%2C%20based%20on%20the%20augmented%20feature%2C%20we%20propose%20a%20novel%20relation-sensitive%0Atransformer%20decoder%20that%20embeds%20dynamic%20entity-pair%20queries%20and%20relational%0Atrait%20priors%2C%20which%20enables%20the%20direct%20prediction%20of%20entity-pair%20relations%20for%0Agraph%20generation%20without%20intermediate%20steps.%20Extensive%20experiments%20have%0Avalidated%20the%20superior%20SGG%20performance%20and%20lower%20computational%20cost%20of%0AS%5E2Former-OR%20on%204D-OR%20benchmark%2C%20compared%20with%20current%20OR-SGG%20methods%2C%20e.g.%2C%203%25%0APrecision%20increase%20and%2024.2M%20reduction%20in%20model%20parameters.%20We%20further%20compared%0Aour%20method%20with%20generic%20single-stage%20SGG%20methods%20with%20broader%20metrics%20for%20a%0Acomprehensive%20evaluation%2C%20with%20consistently%20better%20performance%20achieved.%20The%0Acode%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14461v1&entry.124074799=Read"},
{"title": "Physics-informed deep-learning applications to experimental fluid\n  mechanics", "author": "Hamidreza Eivazi and Yuning Wang and Ricardo Vinuesa", "abstract": "  High-resolution reconstruction of flow-field data from low-resolution and\nnoisy measurements is of interest due to the prevalence of such problems in\nexperimental fluid mechanics, where the measurement data are in general sparse,\nincomplete and noisy. Deep-learning approaches have been shown suitable for\nsuch super-resolution tasks. However, a high number of high-resolution examples\nis needed, which may not be available for many cases. Moreover, the obtained\npredictions may lack in complying with the physical principles, e.g. mass and\nmomentum conservation. Physics-informed deep learning provides frameworks for\nintegrating data and physical laws for learning. In this study, we apply\nphysics-informed neural networks (PINNs) for super-resolution of flow-field\ndata both in time and space from a limited set of noisy measurements without\nhaving any high-resolution reference data. Our objective is to obtain a\ncontinuous solution of the problem, providing a physically-consistent\nprediction at any point in the solution domain. We demonstrate the\napplicability of PINNs for the super-resolution of flow-field data in time and\nspace through three canonical cases: Burgers' equation, two-dimensional vortex\nshedding behind a circular cylinder and the minimal turbulent channel flow. The\nrobustness of the models is also investigated by adding synthetic Gaussian\nnoise. Furthermore, we show the capabilities of PINNs to improve the resolution\nand reduce the noise in a real experimental dataset consisting of\nhot-wire-anemometry measurements. Our results show the adequate capabilities of\nPINNs in the context of data augmentation for experiments in fluid mechanics.\n", "link": "http://arxiv.org/abs/2203.15402v2", "date": "2024-02-22", "relevancy": 1.9583, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20deep-learning%20applications%20to%20experimental%20fluid%0A%20%20mechanics&entry.906535625=Hamidreza%20Eivazi%20and%20Yuning%20Wang%20and%20Ricardo%20Vinuesa&entry.1292438233=%20%20High-resolution%20reconstruction%20of%20flow-field%20data%20from%20low-resolution%20and%0Anoisy%20measurements%20is%20of%20interest%20due%20to%20the%20prevalence%20of%20such%20problems%20in%0Aexperimental%20fluid%20mechanics%2C%20where%20the%20measurement%20data%20are%20in%20general%20sparse%2C%0Aincomplete%20and%20noisy.%20Deep-learning%20approaches%20have%20been%20shown%20suitable%20for%0Asuch%20super-resolution%20tasks.%20However%2C%20a%20high%20number%20of%20high-resolution%20examples%0Ais%20needed%2C%20which%20may%20not%20be%20available%20for%20many%20cases.%20Moreover%2C%20the%20obtained%0Apredictions%20may%20lack%20in%20complying%20with%20the%20physical%20principles%2C%20e.g.%20mass%20and%0Amomentum%20conservation.%20Physics-informed%20deep%20learning%20provides%20frameworks%20for%0Aintegrating%20data%20and%20physical%20laws%20for%20learning.%20In%20this%20study%2C%20we%20apply%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20for%20super-resolution%20of%20flow-field%0Adata%20both%20in%20time%20and%20space%20from%20a%20limited%20set%20of%20noisy%20measurements%20without%0Ahaving%20any%20high-resolution%20reference%20data.%20Our%20objective%20is%20to%20obtain%20a%0Acontinuous%20solution%20of%20the%20problem%2C%20providing%20a%20physically-consistent%0Aprediction%20at%20any%20point%20in%20the%20solution%20domain.%20We%20demonstrate%20the%0Aapplicability%20of%20PINNs%20for%20the%20super-resolution%20of%20flow-field%20data%20in%20time%20and%0Aspace%20through%20three%20canonical%20cases%3A%20Burgers%27%20equation%2C%20two-dimensional%20vortex%0Ashedding%20behind%20a%20circular%20cylinder%20and%20the%20minimal%20turbulent%20channel%20flow.%20The%0Arobustness%20of%20the%20models%20is%20also%20investigated%20by%20adding%20synthetic%20Gaussian%0Anoise.%20Furthermore%2C%20we%20show%20the%20capabilities%20of%20PINNs%20to%20improve%20the%20resolution%0Aand%20reduce%20the%20noise%20in%20a%20real%20experimental%20dataset%20consisting%20of%0Ahot-wire-anemometry%20measurements.%20Our%20results%20show%20the%20adequate%20capabilities%20of%0APINNs%20in%20the%20context%20of%20data%20augmentation%20for%20experiments%20in%20fluid%20mechanics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.15402v2&entry.124074799=Read"},
{"title": "Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment", "author": "Chinmay Vilas Samak and Tanmay Vilas Samak", "abstract": "  Modeling and simulation of autonomous vehicles plays a crucial role in\nachieving enterprise-scale realization that aligns with technical, business and\nregulatory requirements. Contemporary trends in digital lifecycle treatment\nhave proven beneficial to support SBD as well as V&V of these complex systems.\nAlthough, the development of appropriate fidelity simulation models capable of\ncapturing the intricate real-world physics and graphics (real2sim), while\nenabling real-time interactivity for decision-making, has remained a challenge.\nNevertheless, recent advances in AI-based tools and workflows, such as online\ndeep-learning algorithms leveraging live-streaming data sources, offer the\ntantalizing potential for real-time system-identification and adaptive modeling\nto simulate vehicles, environments, as well as their interactions. This\ntransition from virtual prototypes to digital twins not only improves\nsimulation fidelity and real-time factor, but can also support the development\nof online adaption/augmentation techniques that can help bridge the gap between\nsimulation and reality (sim2real). In such a milieu, this work focuses on\ndeveloping autonomy-oriented digital twins of vehicles across different scales\nand configurations to help support the streamlined development and deployment\nof Autoware stack, using a unified real2sim2real toolchain. Particularly, the\ncore deliverable for this project was to integrate the Autoware stack with\nAutoDRIVE Ecosystem to demonstrate end-to-end task of map-based autonomous\nnavigation. This work discusses the development of vehicle and environment\ndigital twins using AutoDRIVE Ecosystem, along with various APIs and HMIs to\nconnect with the same, followed by a detailed section on AutoDRIVE-Autoware\nintegration. Furthermore, this study describes the first-ever off-road\ndeployment of the Autoware stack, expanding the ODD beyond on-road autonomous\nnavigation.\n", "link": "http://arxiv.org/abs/2402.14739v1", "date": "2024-02-22", "relevancy": 1.9573, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5305}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4559}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomy%20Oriented%20Digital%20Twins%20for%20Real2Sim2Real%20Autoware%20Deployment&entry.906535625=Chinmay%20Vilas%20Samak%20and%20Tanmay%20Vilas%20Samak&entry.1292438233=%20%20Modeling%20and%20simulation%20of%20autonomous%20vehicles%20plays%20a%20crucial%20role%20in%0Aachieving%20enterprise-scale%20realization%20that%20aligns%20with%20technical%2C%20business%20and%0Aregulatory%20requirements.%20Contemporary%20trends%20in%20digital%20lifecycle%20treatment%0Ahave%20proven%20beneficial%20to%20support%20SBD%20as%20well%20as%20V%26V%20of%20these%20complex%20systems.%0AAlthough%2C%20the%20development%20of%20appropriate%20fidelity%20simulation%20models%20capable%20of%0Acapturing%20the%20intricate%20real-world%20physics%20and%20graphics%20%28real2sim%29%2C%20while%0Aenabling%20real-time%20interactivity%20for%20decision-making%2C%20has%20remained%20a%20challenge.%0ANevertheless%2C%20recent%20advances%20in%20AI-based%20tools%20and%20workflows%2C%20such%20as%20online%0Adeep-learning%20algorithms%20leveraging%20live-streaming%20data%20sources%2C%20offer%20the%0Atantalizing%20potential%20for%20real-time%20system-identification%20and%20adaptive%20modeling%0Ato%20simulate%20vehicles%2C%20environments%2C%20as%20well%20as%20their%20interactions.%20This%0Atransition%20from%20virtual%20prototypes%20to%20digital%20twins%20not%20only%20improves%0Asimulation%20fidelity%20and%20real-time%20factor%2C%20but%20can%20also%20support%20the%20development%0Aof%20online%20adaption/augmentation%20techniques%20that%20can%20help%20bridge%20the%20gap%20between%0Asimulation%20and%20reality%20%28sim2real%29.%20In%20such%20a%20milieu%2C%20this%20work%20focuses%20on%0Adeveloping%20autonomy-oriented%20digital%20twins%20of%20vehicles%20across%20different%20scales%0Aand%20configurations%20to%20help%20support%20the%20streamlined%20development%20and%20deployment%0Aof%20Autoware%20stack%2C%20using%20a%20unified%20real2sim2real%20toolchain.%20Particularly%2C%20the%0Acore%20deliverable%20for%20this%20project%20was%20to%20integrate%20the%20Autoware%20stack%20with%0AAutoDRIVE%20Ecosystem%20to%20demonstrate%20end-to-end%20task%20of%20map-based%20autonomous%0Anavigation.%20This%20work%20discusses%20the%20development%20of%20vehicle%20and%20environment%0Adigital%20twins%20using%20AutoDRIVE%20Ecosystem%2C%20along%20with%20various%20APIs%20and%20HMIs%20to%0Aconnect%20with%20the%20same%2C%20followed%20by%20a%20detailed%20section%20on%20AutoDRIVE-Autoware%0Aintegration.%20Furthermore%2C%20this%20study%20describes%20the%20first-ever%20off-road%0Adeployment%20of%20the%20Autoware%20stack%2C%20expanding%20the%20ODD%20beyond%20on-road%20autonomous%0Anavigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14739v1&entry.124074799=Read"},
{"title": "Balanced Data Sampling for Language Model Training with Clustering", "author": "Yunfan Shao and Linyang Li and Zhaoye Fei and Hang Yan and Dahua Lin and Xipeng Qiu", "abstract": "  Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.\n", "link": "http://arxiv.org/abs/2402.14526v1", "date": "2024-02-22", "relevancy": 1.9562, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5978}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4123}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.411}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%20Data%20Sampling%20for%20Language%20Model%20Training%20with%20Clustering&entry.906535625=Yunfan%20Shao%20and%20Linyang%20Li%20and%20Zhaoye%20Fei%20and%20Hang%20Yan%20and%20Dahua%20Lin%20and%20Xipeng%20Qiu&entry.1292438233=%20%20Data%20plays%20a%20fundamental%20role%20in%20the%20training%20of%20Large%20Language%20Models%0A%28LLMs%29.%20While%20attention%20has%20been%20paid%20to%20the%20collection%20and%20composition%20of%0Adatasets%2C%20determining%20the%20data%20sampling%20strategy%20in%20training%20remains%20an%20open%0Aquestion.%20Most%20LLMs%20are%20trained%20with%20a%20simple%20strategy%2C%20random%20sampling.%0AHowever%2C%20this%20sampling%20strategy%20ignores%20the%20unbalanced%20nature%20of%20training%20data%0Adistribution%2C%20which%20can%20be%20sub-optimal.%20In%20this%20paper%2C%20we%20propose%20ClusterClip%0ASampling%20to%20balance%20the%20text%20distribution%20of%20training%20data%20for%20better%20model%0Atraining.%20Specifically%2C%20ClusterClip%20Sampling%20utilizes%20data%20clustering%20to%0Areflect%20the%20data%20distribution%20of%20the%20training%20set%20and%20balances%20the%20common%0Asamples%20and%20rare%20samples%20during%20training%20based%20on%20the%20cluster%20results.%20A%0Arepetition%20clip%20operation%20is%20introduced%20to%20mitigate%20the%20overfitting%20issue%20led%0Aby%20samples%20from%20certain%20clusters.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20ClusterClip%20Sampling%2C%20which%20outperforms%20random%20sampling%20and%0Aother%20cluster-based%20sampling%20variants%20under%20various%20training%20datasets%20and%20large%0Alanguage%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14526v1&entry.124074799=Read"},
{"title": "Analytical Verification of Performance of Deep Neural Network Based\n  Time-Synchronized Distribution System State Estimation", "author": "Behrouz Azimian and Shiva Moshtagh and Anamitra Pal and Shanshan Ma", "abstract": "  Recently, we demonstrated success of a time-synchronized state estimator\nusing deep neural networks (DNNs) for real-time unobservable distribution\nsystems. In this letter, we provide analytical bounds on the performance of\nthat state estimator as a function of perturbations in the input measurements.\nIt has already been shown that evaluating performance based on only the test\ndataset might not effectively indicate a trained DNN's ability to handle input\nperturbations. As such, we analytically verify robustness and trustworthiness\nof DNNs to input perturbations by treating them as mixed-integer linear\nprogramming (MILP) problems. The ability of batch normalization in addressing\nthe scalability limitations of the MILP formulation is also highlighted. The\nframework is validated by performing time-synchronized distribution system\nstate estimation for a modified IEEE 34-node system and a real-world large\ndistribution system, both of which are incompletely observed by micro-phasor\nmeasurement units.\n", "link": "http://arxiv.org/abs/2311.06973v4", "date": "2024-02-22", "relevancy": 1.9551, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4474}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analytical%20Verification%20of%20Performance%20of%20Deep%20Neural%20Network%20Based%0A%20%20Time-Synchronized%20Distribution%20System%20State%20Estimation&entry.906535625=Behrouz%20Azimian%20and%20Shiva%20Moshtagh%20and%20Anamitra%20Pal%20and%20Shanshan%20Ma&entry.1292438233=%20%20Recently%2C%20we%20demonstrated%20success%20of%20a%20time-synchronized%20state%20estimator%0Ausing%20deep%20neural%20networks%20%28DNNs%29%20for%20real-time%20unobservable%20distribution%0Asystems.%20In%20this%20letter%2C%20we%20provide%20analytical%20bounds%20on%20the%20performance%20of%0Athat%20state%20estimator%20as%20a%20function%20of%20perturbations%20in%20the%20input%20measurements.%0AIt%20has%20already%20been%20shown%20that%20evaluating%20performance%20based%20on%20only%20the%20test%0Adataset%20might%20not%20effectively%20indicate%20a%20trained%20DNN%27s%20ability%20to%20handle%20input%0Aperturbations.%20As%20such%2C%20we%20analytically%20verify%20robustness%20and%20trustworthiness%0Aof%20DNNs%20to%20input%20perturbations%20by%20treating%20them%20as%20mixed-integer%20linear%0Aprogramming%20%28MILP%29%20problems.%20The%20ability%20of%20batch%20normalization%20in%20addressing%0Athe%20scalability%20limitations%20of%20the%20MILP%20formulation%20is%20also%20highlighted.%20The%0Aframework%20is%20validated%20by%20performing%20time-synchronized%20distribution%20system%0Astate%20estimation%20for%20a%20modified%20IEEE%2034-node%20system%20and%20a%20real-world%20large%0Adistribution%20system%2C%20both%20of%20which%20are%20incompletely%20observed%20by%20micro-phasor%0Ameasurement%20units.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06973v4&entry.124074799=Read"},
{"title": "QIS : Interactive Segmentation via Quasi-Conformal Mappings", "author": "Han Zhang and Daoping Zhang and Lok Ming Lui", "abstract": "  Image segmentation plays a crucial role in extracting important objects of\ninterest from images, enabling various applications. While existing methods\nhave shown success in segmenting clean images, they often struggle to produce\naccurate segmentation results when dealing with degraded images, such as those\ncontaining noise or occlusions. To address this challenge, interactive\nsegmentation has emerged as a promising approach, allowing users to provide\nmeaningful input to guide the segmentation process. However, an important\nproblem in interactive segmentation lies in determining how to incorporate\nminimal yet meaningful user guidance into the segmentation model. In this\npaper, we propose the quasi-conformal interactive segmentation (QIS) model,\nwhich incorporates user input in the form of positive and negative clicks.\nUsers mark a few pixels belonging to the object region as positive clicks,\nindicating that the segmentation model should include a region around these\nclicks. Conversely, negative clicks are provided on pixels belonging to the\nbackground, instructing the model to exclude the region near these clicks from\nthe segmentation mask. Additionally, the segmentation mask is obtained by\ndeforming a template mask with the same topology as the object of interest\nusing an orientation-preserving quasiconformal mapping. This approach helps to\navoid topological errors in the segmentation results. We provide a thorough\nanalysis of the proposed model, including theoretical support for the ability\nof QIS to include or exclude regions of interest or disinterest based on the\nuser's indication. To evaluate the performance of QIS, we conduct experiments\non synthesized images, medical images, natural images and noisy natural images.\nThe results demonstrate the efficacy of our proposed method.\n", "link": "http://arxiv.org/abs/2402.14695v1", "date": "2024-02-22", "relevancy": 1.9327, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4918}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4701}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QIS%20%3A%20Interactive%20Segmentation%20via%20Quasi-Conformal%20Mappings&entry.906535625=Han%20Zhang%20and%20Daoping%20Zhang%20and%20Lok%20Ming%20Lui&entry.1292438233=%20%20Image%20segmentation%20plays%20a%20crucial%20role%20in%20extracting%20important%20objects%20of%0Ainterest%20from%20images%2C%20enabling%20various%20applications.%20While%20existing%20methods%0Ahave%20shown%20success%20in%20segmenting%20clean%20images%2C%20they%20often%20struggle%20to%20produce%0Aaccurate%20segmentation%20results%20when%20dealing%20with%20degraded%20images%2C%20such%20as%20those%0Acontaining%20noise%20or%20occlusions.%20To%20address%20this%20challenge%2C%20interactive%0Asegmentation%20has%20emerged%20as%20a%20promising%20approach%2C%20allowing%20users%20to%20provide%0Ameaningful%20input%20to%20guide%20the%20segmentation%20process.%20However%2C%20an%20important%0Aproblem%20in%20interactive%20segmentation%20lies%20in%20determining%20how%20to%20incorporate%0Aminimal%20yet%20meaningful%20user%20guidance%20into%20the%20segmentation%20model.%20In%20this%0Apaper%2C%20we%20propose%20the%20quasi-conformal%20interactive%20segmentation%20%28QIS%29%20model%2C%0Awhich%20incorporates%20user%20input%20in%20the%20form%20of%20positive%20and%20negative%20clicks.%0AUsers%20mark%20a%20few%20pixels%20belonging%20to%20the%20object%20region%20as%20positive%20clicks%2C%0Aindicating%20that%20the%20segmentation%20model%20should%20include%20a%20region%20around%20these%0Aclicks.%20Conversely%2C%20negative%20clicks%20are%20provided%20on%20pixels%20belonging%20to%20the%0Abackground%2C%20instructing%20the%20model%20to%20exclude%20the%20region%20near%20these%20clicks%20from%0Athe%20segmentation%20mask.%20Additionally%2C%20the%20segmentation%20mask%20is%20obtained%20by%0Adeforming%20a%20template%20mask%20with%20the%20same%20topology%20as%20the%20object%20of%20interest%0Ausing%20an%20orientation-preserving%20quasiconformal%20mapping.%20This%20approach%20helps%20to%0Aavoid%20topological%20errors%20in%20the%20segmentation%20results.%20We%20provide%20a%20thorough%0Aanalysis%20of%20the%20proposed%20model%2C%20including%20theoretical%20support%20for%20the%20ability%0Aof%20QIS%20to%20include%20or%20exclude%20regions%20of%20interest%20or%20disinterest%20based%20on%20the%0Auser%27s%20indication.%20To%20evaluate%20the%20performance%20of%20QIS%2C%20we%20conduct%20experiments%0Aon%20synthesized%20images%2C%20medical%20images%2C%20natural%20images%20and%20noisy%20natural%20images.%0AThe%20results%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14695v1&entry.124074799=Read"},
{"title": "Distributed Radiance Fields for Edge Video Compression and Metaverse\n  Integration in Autonomous Driving", "author": "Eugen \u0160lapak and Mat\u00fa\u0161 Dopiriak and Mohammad Abdullah Al Faruque and Juraj Gazda and Marco Levorato", "abstract": "  The metaverse is a virtual space that combines physical and digital elements,\ncreating immersive and connected digital worlds. For autonomous mobility, it\nenables new possibilities with edge computing and digital twins (DTs) that\noffer virtual prototyping, prediction, and more. DTs can be created with 3D\nscene reconstruction methods that capture the real world's geometry,\nappearance, and dynamics. However, sending data for real-time DT updates in the\nmetaverse, such as camera images and videos from connected autonomous vehicles\n(CAVs) to edge servers, can increase network congestion, costs, and latency,\naffecting metaverse services. Herein, a new method is proposed based on\ndistributed radiance fields (RFs), multi-access edge computing (MEC) network\nfor video compression and metaverse DT updates. RF-based encoder and decoder\nare used to create and restore representations of camera images. The method is\nevaluated on a dataset of camera images from the CARLA simulator. Data savings\nof up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFs\ninstead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR)\nand structural similarity index measure (SSIM) qualitative metrics for the\nreconstructed images. Possible uses and challenges for the metaverse and\nautonomous mobility are also discussed.\n", "link": "http://arxiv.org/abs/2402.14642v1", "date": "2024-02-22", "relevancy": 1.9317, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4932}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4791}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.467}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Radiance%20Fields%20for%20Edge%20Video%20Compression%20and%20Metaverse%0A%20%20Integration%20in%20Autonomous%20Driving&entry.906535625=Eugen%20%C5%A0lapak%20and%20Mat%C3%BA%C5%A1%20Dopiriak%20and%20Mohammad%20Abdullah%20Al%20Faruque%20and%20Juraj%20Gazda%20and%20Marco%20Levorato&entry.1292438233=%20%20The%20metaverse%20is%20a%20virtual%20space%20that%20combines%20physical%20and%20digital%20elements%2C%0Acreating%20immersive%20and%20connected%20digital%20worlds.%20For%20autonomous%20mobility%2C%20it%0Aenables%20new%20possibilities%20with%20edge%20computing%20and%20digital%20twins%20%28DTs%29%20that%0Aoffer%20virtual%20prototyping%2C%20prediction%2C%20and%20more.%20DTs%20can%20be%20created%20with%203D%0Ascene%20reconstruction%20methods%20that%20capture%20the%20real%20world%27s%20geometry%2C%0Aappearance%2C%20and%20dynamics.%20However%2C%20sending%20data%20for%20real-time%20DT%20updates%20in%20the%0Ametaverse%2C%20such%20as%20camera%20images%20and%20videos%20from%20connected%20autonomous%20vehicles%0A%28CAVs%29%20to%20edge%20servers%2C%20can%20increase%20network%20congestion%2C%20costs%2C%20and%20latency%2C%0Aaffecting%20metaverse%20services.%20Herein%2C%20a%20new%20method%20is%20proposed%20based%20on%0Adistributed%20radiance%20fields%20%28RFs%29%2C%20multi-access%20edge%20computing%20%28MEC%29%20network%0Afor%20video%20compression%20and%20metaverse%20DT%20updates.%20RF-based%20encoder%20and%20decoder%0Aare%20used%20to%20create%20and%20restore%20representations%20of%20camera%20images.%20The%20method%20is%0Aevaluated%20on%20a%20dataset%20of%20camera%20images%20from%20the%20CARLA%20simulator.%20Data%20savings%0Aof%20up%20to%2080%25%20were%20achieved%20for%20H.264%20I-frame%20-%20P-frame%20pairs%20by%20using%20RFs%0Ainstead%20of%20I-frames%2C%20while%20maintaining%20high%20peak%20signal-to-noise%20ratio%20%28PSNR%29%0Aand%20structural%20similarity%20index%20measure%20%28SSIM%29%20qualitative%20metrics%20for%20the%0Areconstructed%20images.%20Possible%20uses%20and%20challenges%20for%20the%20metaverse%20and%0Aautonomous%20mobility%20are%20also%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14642v1&entry.124074799=Read"},
{"title": "Everyone Can Be Picasso? A Computational Framework into the Myth of\n  Human versus AI Painting", "author": "Yilin Ye and Rong Huang and Kang Zhang and Wei Zeng", "abstract": "  The recent advances of AI technology, particularly in AI-Generated Content\n(AIGC), have enabled everyone to easily generate beautiful paintings with\nsimple text description. With the stunning quality of AI paintings, it is\nwidely questioned whether there still exists difference between human and AI\npaintings and whether human artists will be replaced by AI. To answer these\nquestions, we develop a computational framework combining neural latent space\nand aesthetics features with visual analytics to investigate the difference\nbetween human and AI paintings. First, with categorical comparison of human and\nAI painting collections, we find that AI artworks show distributional\ndifference from human artworks in both latent space and some aesthetic features\nlike strokes and sharpness, while in other aesthetic features like color and\ncomposition there is less difference. Second, with individual artist analysis\nof Picasso, we show human artists' strength in evolving new styles compared to\nAI. Our findings provide concrete evidence for the existing discrepancies\nbetween human and AI paintings and further suggest improvements of AI art with\nmore consideration of aesthetics and human artists' involvement.\n", "link": "http://arxiv.org/abs/2304.07999v2", "date": "2024-02-22", "relevancy": 1.9301, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4905}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.461}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Everyone%20Can%20Be%20Picasso%3F%20A%20Computational%20Framework%20into%20the%20Myth%20of%0A%20%20Human%20versus%20AI%20Painting&entry.906535625=Yilin%20Ye%20and%20Rong%20Huang%20and%20Kang%20Zhang%20and%20Wei%20Zeng&entry.1292438233=%20%20The%20recent%20advances%20of%20AI%20technology%2C%20particularly%20in%20AI-Generated%20Content%0A%28AIGC%29%2C%20have%20enabled%20everyone%20to%20easily%20generate%20beautiful%20paintings%20with%0Asimple%20text%20description.%20With%20the%20stunning%20quality%20of%20AI%20paintings%2C%20it%20is%0Awidely%20questioned%20whether%20there%20still%20exists%20difference%20between%20human%20and%20AI%0Apaintings%20and%20whether%20human%20artists%20will%20be%20replaced%20by%20AI.%20To%20answer%20these%0Aquestions%2C%20we%20develop%20a%20computational%20framework%20combining%20neural%20latent%20space%0Aand%20aesthetics%20features%20with%20visual%20analytics%20to%20investigate%20the%20difference%0Abetween%20human%20and%20AI%20paintings.%20First%2C%20with%20categorical%20comparison%20of%20human%20and%0AAI%20painting%20collections%2C%20we%20find%20that%20AI%20artworks%20show%20distributional%0Adifference%20from%20human%20artworks%20in%20both%20latent%20space%20and%20some%20aesthetic%20features%0Alike%20strokes%20and%20sharpness%2C%20while%20in%20other%20aesthetic%20features%20like%20color%20and%0Acomposition%20there%20is%20less%20difference.%20Second%2C%20with%20individual%20artist%20analysis%0Aof%20Picasso%2C%20we%20show%20human%20artists%27%20strength%20in%20evolving%20new%20styles%20compared%20to%0AAI.%20Our%20findings%20provide%20concrete%20evidence%20for%20the%20existing%20discrepancies%0Abetween%20human%20and%20AI%20paintings%20and%20further%20suggest%20improvements%20of%20AI%20art%20with%0Amore%20consideration%20of%20aesthetics%20and%20human%20artists%27%20involvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07999v2&entry.124074799=Read"},
{"title": "Deep vessel segmentation based on a new combination of vesselness\n  filters", "author": "Guillaume Garret and Antoine Vacavant and Carole Frindel", "abstract": "  Vascular segmentation represents a crucial clinical task, yet its automation\nremains challenging. Because of the recent strides in deep learning, vesselness\nfilters, which can significantly aid the learning process, have been\noverlooked. This study introduces an innovative filter fusion method crafted to\namplify the effectiveness of vessel segmentation models. Our investigation\nseeks to establish the merits of a filter-based learning approach through a\ncomparative analysis. Specifically, we contrast the performance of a U-Net\nmodel trained on CT images with an identical U-Net configuration trained on\nvesselness hyper-volumes using matching parameters. Our findings, based on two\nvascular datasets, highlight improved segmentations, especially for small\nvessels, when the model's learning is exposed to vessel-enhanced inputs.\n", "link": "http://arxiv.org/abs/2402.14509v1", "date": "2024-02-22", "relevancy": 1.9269, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4913}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4788}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4733}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20vessel%20segmentation%20based%20on%20a%20new%20combination%20of%20vesselness%0A%20%20filters&entry.906535625=Guillaume%20Garret%20and%20Antoine%20Vacavant%20and%20Carole%20Frindel&entry.1292438233=%20%20Vascular%20segmentation%20represents%20a%20crucial%20clinical%20task%2C%20yet%20its%20automation%0Aremains%20challenging.%20Because%20of%20the%20recent%20strides%20in%20deep%20learning%2C%20vesselness%0Afilters%2C%20which%20can%20significantly%20aid%20the%20learning%20process%2C%20have%20been%0Aoverlooked.%20This%20study%20introduces%20an%20innovative%20filter%20fusion%20method%20crafted%20to%0Aamplify%20the%20effectiveness%20of%20vessel%20segmentation%20models.%20Our%20investigation%0Aseeks%20to%20establish%20the%20merits%20of%20a%20filter-based%20learning%20approach%20through%20a%0Acomparative%20analysis.%20Specifically%2C%20we%20contrast%20the%20performance%20of%20a%20U-Net%0Amodel%20trained%20on%20CT%20images%20with%20an%20identical%20U-Net%20configuration%20trained%20on%0Avesselness%20hyper-volumes%20using%20matching%20parameters.%20Our%20findings%2C%20based%20on%20two%0Avascular%20datasets%2C%20highlight%20improved%20segmentations%2C%20especially%20for%20small%0Avessels%2C%20when%20the%20model%27s%20learning%20is%20exposed%20to%20vessel-enhanced%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14509v1&entry.124074799=Read"},
{"title": "Two-stage Cytopathological Image Synthesis for Augmenting Cervical\n  Abnormality Screening", "author": "Zhenrong Shen and Manman Fei and Xin Wang and Jiangdong Cai and Sheng Wang and Lichi Zhang and Qian Wang", "abstract": "  Automatic thin-prep cytologic test (TCT) screening can assist pathologists in\nfinding cervical abnormality towards accurate and efficient cervical cancer\ndiagnosis. Current automatic TCT screening systems mostly involve abnormal\ncervical cell detection, which generally requires large-scale and diverse\ntraining data with high-quality annotations to achieve promising performance.\nPathological image synthesis is naturally raised to minimize the efforts in\ndata collection and annotation. However, it is challenging to generate\nrealistic large-size cytopathological images while simultaneously synthesizing\nvisually plausible appearances for small-size abnormal cervical cells. In this\npaper, we propose a two-stage image synthesis framework to create synthetic\ndata for augmenting cervical abnormality screening. In the first Global Image\nGeneration stage, a Normal Image Generator is designed to generate\ncytopathological images full of normal cervical cells. In the second Local Cell\nEditing stage, normal cells are randomly selected from the generated images and\nthen are converted to different types of abnormal cells using the proposed\nAbnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell\nSynthesizer are built upon the pre-trained Stable Diffusion via\nparameter-efficient fine-tuning methods for customizing cytopathological image\ncontents and extending spatial layout controllability, respectively. Our\nexperiments demonstrate the synthetic image quality, diversity, and\ncontrollability of the proposed synthesis framework, and validate its data\naugmentation effectiveness in enhancing the performance of abnormal cervical\ncell detection.\n", "link": "http://arxiv.org/abs/2402.14707v1", "date": "2024-02-22", "relevancy": 1.9235, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5034}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4676}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4577}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-stage%20Cytopathological%20Image%20Synthesis%20for%20Augmenting%20Cervical%0A%20%20Abnormality%20Screening&entry.906535625=Zhenrong%20Shen%20and%20Manman%20Fei%20and%20Xin%20Wang%20and%20Jiangdong%20Cai%20and%20Sheng%20Wang%20and%20Lichi%20Zhang%20and%20Qian%20Wang&entry.1292438233=%20%20Automatic%20thin-prep%20cytologic%20test%20%28TCT%29%20screening%20can%20assist%20pathologists%20in%0Afinding%20cervical%20abnormality%20towards%20accurate%20and%20efficient%20cervical%20cancer%0Adiagnosis.%20Current%20automatic%20TCT%20screening%20systems%20mostly%20involve%20abnormal%0Acervical%20cell%20detection%2C%20which%20generally%20requires%20large-scale%20and%20diverse%0Atraining%20data%20with%20high-quality%20annotations%20to%20achieve%20promising%20performance.%0APathological%20image%20synthesis%20is%20naturally%20raised%20to%20minimize%20the%20efforts%20in%0Adata%20collection%20and%20annotation.%20However%2C%20it%20is%20challenging%20to%20generate%0Arealistic%20large-size%20cytopathological%20images%20while%20simultaneously%20synthesizing%0Avisually%20plausible%20appearances%20for%20small-size%20abnormal%20cervical%20cells.%20In%20this%0Apaper%2C%20we%20propose%20a%20two-stage%20image%20synthesis%20framework%20to%20create%20synthetic%0Adata%20for%20augmenting%20cervical%20abnormality%20screening.%20In%20the%20first%20Global%20Image%0AGeneration%20stage%2C%20a%20Normal%20Image%20Generator%20is%20designed%20to%20generate%0Acytopathological%20images%20full%20of%20normal%20cervical%20cells.%20In%20the%20second%20Local%20Cell%0AEditing%20stage%2C%20normal%20cells%20are%20randomly%20selected%20from%20the%20generated%20images%20and%0Athen%20are%20converted%20to%20different%20types%20of%20abnormal%20cells%20using%20the%20proposed%0AAbnormal%20Cell%20Synthesizer.%20Both%20Normal%20Image%20Generator%20and%20Abnormal%20Cell%0ASynthesizer%20are%20built%20upon%20the%20pre-trained%20Stable%20Diffusion%20via%0Aparameter-efficient%20fine-tuning%20methods%20for%20customizing%20cytopathological%20image%0Acontents%20and%20extending%20spatial%20layout%20controllability%2C%20respectively.%20Our%0Aexperiments%20demonstrate%20the%20synthetic%20image%20quality%2C%20diversity%2C%20and%0Acontrollability%20of%20the%20proposed%20synthesis%20framework%2C%20and%20validate%20its%20data%0Aaugmentation%20effectiveness%20in%20enhancing%20the%20performance%20of%20abnormal%20cervical%0Acell%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14707v1&entry.124074799=Read"},
{"title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for\n  Mixture-of-Experts Large Language Models", "author": "Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li", "abstract": "  A pivotal advancement in the progress of large language models (LLMs) is the\nemergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,\nMoE LLMs can achieve higher performance with fewer parameters, but it is still\nhard to deploy them due to their immense parameter sizes. Different from\nprevious weight pruning methods that rely on specifically designed hardware,\nthis paper mainly aims to enhance the deployment efficiency of MoE LLMs by\nintroducing plug-and-play expert-level sparsification techniques. Specifically,\nwe propose, for the first time to our best knowledge, post-training approaches\nfor task-agnostic and task-specific expert pruning and skipping of MoE LLMs,\ntailored to improve deployment efficiency while maintaining model performance\nacross a wide range of tasks. Extensive experiments show that our proposed\nmethods can simultaneously reduce model sizes and increase the inference speed,\nwhile maintaining satisfactory performance. Data and code will be available at\nhttps://github.com/Lucky-Lance/Expert_Sparsity.\n", "link": "http://arxiv.org/abs/2402.14800v1", "date": "2024-02-22", "relevancy": 1.92, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4901}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.463}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Experts%20are%20Equal%3A%20Efficient%20Expert%20Pruning%20and%20Skipping%20for%0A%20%20Mixture-of-Experts%20Large%20Language%20Models&entry.906535625=Xudong%20Lu%20and%20Qi%20Liu%20and%20Yuhui%20Xu%20and%20Aojun%20Zhou%20and%20Siyuan%20Huang%20and%20Bo%20Zhang%20and%20Junchi%20Yan%20and%20Hongsheng%20Li&entry.1292438233=%20%20A%20pivotal%20advancement%20in%20the%20progress%20of%20large%20language%20models%20%28LLMs%29%20is%20the%0Aemergence%20of%20the%20Mixture-of-Experts%20%28MoE%29%20LLMs.%20Compared%20to%20traditional%20LLMs%2C%0AMoE%20LLMs%20can%20achieve%20higher%20performance%20with%20fewer%20parameters%2C%20but%20it%20is%20still%0Ahard%20to%20deploy%20them%20due%20to%20their%20immense%20parameter%20sizes.%20Different%20from%0Aprevious%20weight%20pruning%20methods%20that%20rely%20on%20specifically%20designed%20hardware%2C%0Athis%20paper%20mainly%20aims%20to%20enhance%20the%20deployment%20efficiency%20of%20MoE%20LLMs%20by%0Aintroducing%20plug-and-play%20expert-level%20sparsification%20techniques.%20Specifically%2C%0Awe%20propose%2C%20for%20the%20first%20time%20to%20our%20best%20knowledge%2C%20post-training%20approaches%0Afor%20task-agnostic%20and%20task-specific%20expert%20pruning%20and%20skipping%20of%20MoE%20LLMs%2C%0Atailored%20to%20improve%20deployment%20efficiency%20while%20maintaining%20model%20performance%0Aacross%20a%20wide%20range%20of%20tasks.%20Extensive%20experiments%20show%20that%20our%20proposed%0Amethods%20can%20simultaneously%20reduce%20model%20sizes%20and%20increase%20the%20inference%20speed%2C%0Awhile%20maintaining%20satisfactory%20performance.%20Data%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Lucky-Lance/Expert_Sparsity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14800v1&entry.124074799=Read"},
{"title": "SHM-Traffic: DRL and Transfer learning based UAV Control for Structural\n  Health Monitoring of Bridges with Traffic", "author": "Divija Swetha Gadiraju and Saeed Eftekhar Azam and Deepak Khazanchi", "abstract": "  This work focuses on using advanced techniques for structural health\nmonitoring (SHM) for bridges with Traffic. We propose an approach using deep\nreinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV).\nOur approach conducts a concrete bridge deck survey while traffic is ongoing\nand detects cracks. The UAV performs the crack detection, and the location of\ncracks is initially unknown. We use two edge detection techniques. First, we\nuse canny edge detection for crack detection. We also use a Convolutional\nNeural Network (CNN) for crack detection and compare it with canny edge\ndetection. Transfer learning is applied using CNN with pre-trained weights\nobtained from a crack image dataset. This enables the model to adapt and\nimprove its performance in identifying and localizing cracks. Proximal Policy\nOptimization (PPO) is applied for UAV control and bridge surveys. The\nexperimentation across various scenarios is performed to evaluate the\nperformance of the proposed methodology. Key metrics such as task completion\ntime and reward convergence are observed to gauge the effectiveness of the\napproach. We observe that the Canny edge detector offers up to 40\\% lower task\ncompletion time, while the CNN excels in up to 12\\% better damage detection and\n1.8 times better rewards.\n", "link": "http://arxiv.org/abs/2402.14757v1", "date": "2024-02-22", "relevancy": 1.9177, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5064}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4551}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHM-Traffic%3A%20DRL%20and%20Transfer%20learning%20based%20UAV%20Control%20for%20Structural%0A%20%20Health%20Monitoring%20of%20Bridges%20with%20Traffic&entry.906535625=Divija%20Swetha%20Gadiraju%20and%20Saeed%20Eftekhar%20Azam%20and%20Deepak%20Khazanchi&entry.1292438233=%20%20This%20work%20focuses%20on%20using%20advanced%20techniques%20for%20structural%20health%0Amonitoring%20%28SHM%29%20for%20bridges%20with%20Traffic.%20We%20propose%20an%20approach%20using%20deep%0Areinforcement%20learning%20%28DRL%29-based%20control%20for%20Unmanned%20Aerial%20Vehicle%20%28UAV%29.%0AOur%20approach%20conducts%20a%20concrete%20bridge%20deck%20survey%20while%20traffic%20is%20ongoing%0Aand%20detects%20cracks.%20The%20UAV%20performs%20the%20crack%20detection%2C%20and%20the%20location%20of%0Acracks%20is%20initially%20unknown.%20We%20use%20two%20edge%20detection%20techniques.%20First%2C%20we%0Ause%20canny%20edge%20detection%20for%20crack%20detection.%20We%20also%20use%20a%20Convolutional%0ANeural%20Network%20%28CNN%29%20for%20crack%20detection%20and%20compare%20it%20with%20canny%20edge%0Adetection.%20Transfer%20learning%20is%20applied%20using%20CNN%20with%20pre-trained%20weights%0Aobtained%20from%20a%20crack%20image%20dataset.%20This%20enables%20the%20model%20to%20adapt%20and%0Aimprove%20its%20performance%20in%20identifying%20and%20localizing%20cracks.%20Proximal%20Policy%0AOptimization%20%28PPO%29%20is%20applied%20for%20UAV%20control%20and%20bridge%20surveys.%20The%0Aexperimentation%20across%20various%20scenarios%20is%20performed%20to%20evaluate%20the%0Aperformance%20of%20the%20proposed%20methodology.%20Key%20metrics%20such%20as%20task%20completion%0Atime%20and%20reward%20convergence%20are%20observed%20to%20gauge%20the%20effectiveness%20of%20the%0Aapproach.%20We%20observe%20that%20the%20Canny%20edge%20detector%20offers%20up%20to%2040%5C%25%20lower%20task%0Acompletion%20time%2C%20while%20the%20CNN%20excels%20in%20up%20to%2012%5C%25%20better%20damage%20detection%20and%0A1.8%20times%20better%20rewards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14757v1&entry.124074799=Read"},
{"title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models", "author": "Jiaheng Liu and Zhiqi Bai and Yuanxing Zhang and Chenchen Zhang and Yu Zhang and Ge Zhang and Jiakai Wang and Haoran Que and Yukang Chen and Wenbo Su and Tiezheng Ge and Jie Fu and Wenhu Chen and Bo Zheng", "abstract": "  Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.\n", "link": "http://arxiv.org/abs/2401.06951v3", "date": "2024-02-22", "relevancy": 1.9139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5043}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4794}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4673}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E%5E2-LLM%3A%20Efficient%20and%20Extreme%20Length%20Extension%20of%20Large%20Language%20Models&entry.906535625=Jiaheng%20Liu%20and%20Zhiqi%20Bai%20and%20Yuanxing%20Zhang%20and%20Chenchen%20Zhang%20and%20Yu%20Zhang%20and%20Ge%20Zhang%20and%20Jiakai%20Wang%20and%20Haoran%20Que%20and%20Yukang%20Chen%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Jie%20Fu%20and%20Wenhu%20Chen%20and%20Bo%20Zheng&entry.1292438233=%20%20Typically%2C%20training%20LLMs%20with%20long%20context%20sizes%20is%20computationally%0Aexpensive%2C%20requiring%20extensive%20training%20hours%20and%20GPU%20resources.%20Existing%0Along-context%20extension%20methods%20usually%20need%20additional%20training%20procedures%20to%0Asupport%20corresponding%20long-context%20windows%2C%20where%20the%20long-context%20training%0Adata%20%28e.g.%2C%2032k%29%20is%20needed%2C%20and%20high%20GPU%20training%20costs%20are%20assumed.%20To%20address%0Athe%20aforementioned%20issues%2C%20we%20propose%20an%20Efficient%20and%20Extreme%20length%20extension%0Amethod%20for%20Large%20Language%20Models%2C%20called%20E%202%20-LLM%2C%20with%20only%20one%20training%0Aprocedure%20and%20dramatically%20reduced%20computation%20cost%2C%20which%20also%20removes%20the%0Aneed%20to%20collect%20long-context%20data.%20Concretely%2C%20first%2C%20the%20training%20data%20of%20our%0AE%202%20-LLM%20only%20requires%20a%20short%20length%20%28e.g.%2C%204k%29%2C%20which%20reduces%20the%20tuning%20cost%0Agreatly.%20Second%2C%20the%20training%20procedure%20on%20the%20short%20training%20context%20window%20is%0Aperformed%20only%20once%20time%2C%20and%20we%20can%20support%20different%20evaluation%20context%0Awindows%20at%20inference.%20Third%2C%20in%20E%202%20-%20LLM%2C%20based%20on%20RoPE%20position%20embeddings%2C%0Awe%20introduce%20two%20different%20augmentation%20methods%20on%20the%20scale%20and%20position%20index%0Aparameters%20for%20different%20samples%20in%20training.%20It%20aims%20to%20make%20the%20model%20more%0Arobust%20to%20the%20different%20relative%20differences%20when%20directly%20interpolating%20the%0Aarbitrary%20context%20length%20at%20inference.%20Comprehensive%20experimental%20results%20on%0Amultiple%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20E%202%20-LLM%20on%0Achallenging%20long-context%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06951v3&entry.124074799=Read"},
{"title": "Learning Quadruped Locomotion Policies using Logical Rules", "author": "David DeFazio and Yohei Hayamizu and Shiqi Zhang", "abstract": "  Quadruped animals are capable of exhibiting a diverse range of locomotion\ngaits. While progress has been made in demonstrating such gaits on robots,\ncurrent methods rely on motion priors, dynamics models, or other forms of\nextensive manual efforts. People can use natural language to describe dance\nmoves. Could one use a formal language to specify quadruped gaits? To this end,\nwe aim to enable easy gait specification and efficient policy learning.\nLeveraging Reward Machines~(RMs) for high-level gait specification over foot\ncontacts, our approach is called RM-based Locomotion Learning~(RMLL), and\nsupports adjusting gait frequency at execution time. Gait specification is\nenabled through the use of a few logical rules per gait (e.g., alternate\nbetween moving front feet and back feet) and does not require labor-intensive\nmotion priors. Experimental results in simulation highlight the diversity of\nlearned gaits (including two novel gaits), their energy consumption and\nstability across different terrains, and the superior sample-efficiency when\ncompared to baselines. We also demonstrate these learned policies with a real\nquadruped robot. Video and supplementary materials:\nhttps://sites.google.com/view/rm-locomotion-learning/home\n", "link": "http://arxiv.org/abs/2107.10969v3", "date": "2024-02-22", "relevancy": 1.9109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5634}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4627}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4585}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Quadruped%20Locomotion%20Policies%20using%20Logical%20Rules&entry.906535625=David%20DeFazio%20and%20Yohei%20Hayamizu%20and%20Shiqi%20Zhang&entry.1292438233=%20%20Quadruped%20animals%20are%20capable%20of%20exhibiting%20a%20diverse%20range%20of%20locomotion%0Agaits.%20While%20progress%20has%20been%20made%20in%20demonstrating%20such%20gaits%20on%20robots%2C%0Acurrent%20methods%20rely%20on%20motion%20priors%2C%20dynamics%20models%2C%20or%20other%20forms%20of%0Aextensive%20manual%20efforts.%20People%20can%20use%20natural%20language%20to%20describe%20dance%0Amoves.%20Could%20one%20use%20a%20formal%20language%20to%20specify%20quadruped%20gaits%3F%20To%20this%20end%2C%0Awe%20aim%20to%20enable%20easy%20gait%20specification%20and%20efficient%20policy%20learning.%0ALeveraging%20Reward%20Machines~%28RMs%29%20for%20high-level%20gait%20specification%20over%20foot%0Acontacts%2C%20our%20approach%20is%20called%20RM-based%20Locomotion%20Learning~%28RMLL%29%2C%20and%0Asupports%20adjusting%20gait%20frequency%20at%20execution%20time.%20Gait%20specification%20is%0Aenabled%20through%20the%20use%20of%20a%20few%20logical%20rules%20per%20gait%20%28e.g.%2C%20alternate%0Abetween%20moving%20front%20feet%20and%20back%20feet%29%20and%20does%20not%20require%20labor-intensive%0Amotion%20priors.%20Experimental%20results%20in%20simulation%20highlight%20the%20diversity%20of%0Alearned%20gaits%20%28including%20two%20novel%20gaits%29%2C%20their%20energy%20consumption%20and%0Astability%20across%20different%20terrains%2C%20and%20the%20superior%20sample-efficiency%20when%0Acompared%20to%20baselines.%20We%20also%20demonstrate%20these%20learned%20policies%20with%20a%20real%0Aquadruped%20robot.%20Video%20and%20supplementary%20materials%3A%0Ahttps%3A//sites.google.com/view/rm-locomotion-learning/home%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.10969v3&entry.124074799=Read"},
{"title": "Self-supervised Visualisation of Medical Image Datasets", "author": "Ifeoma Veronica Nwabufo and Jan Niklas B\u00f6hm and Philipp Berens and Dmitry Kobak", "abstract": "  Self-supervised learning methods based on data augmentations, such as SimCLR,\nBYOL, or DINO, allow obtaining semantically meaningful representations of image\ndatasets and are widely used prior to supervised fine-tuning. A recent\nself-supervised learning method, $t$-SimCNE, uses contrastive learning to\ndirectly train a 2D representation suitable for visualisation. When applied to\nnatural image datasets, $t$-SimCNE yields 2D visualisations with semantically\nmeaningful clusters. In this work, we used $t$-SimCNE to visualise medical\nimage datasets, including examples from dermatology, histology, and blood\nmicroscopy. We found that increasing the set of data augmentations to include\narbitrary rotations improved the results in terms of class separability,\ncompared to data augmentations used for natural images. Our 2D representations\nshow medically relevant structures and can be used to aid data exploration and\nannotation, improving on common approaches for data visualisation.\n", "link": "http://arxiv.org/abs/2402.14566v1", "date": "2024-02-22", "relevancy": 1.9073, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4879}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4788}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4704}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Visualisation%20of%20Medical%20Image%20Datasets&entry.906535625=Ifeoma%20Veronica%20Nwabufo%20and%20Jan%20Niklas%20B%C3%B6hm%20and%20Philipp%20Berens%20and%20Dmitry%20Kobak&entry.1292438233=%20%20Self-supervised%20learning%20methods%20based%20on%20data%20augmentations%2C%20such%20as%20SimCLR%2C%0ABYOL%2C%20or%20DINO%2C%20allow%20obtaining%20semantically%20meaningful%20representations%20of%20image%0Adatasets%20and%20are%20widely%20used%20prior%20to%20supervised%20fine-tuning.%20A%20recent%0Aself-supervised%20learning%20method%2C%20%24t%24-SimCNE%2C%20uses%20contrastive%20learning%20to%0Adirectly%20train%20a%202D%20representation%20suitable%20for%20visualisation.%20When%20applied%20to%0Anatural%20image%20datasets%2C%20%24t%24-SimCNE%20yields%202D%20visualisations%20with%20semantically%0Ameaningful%20clusters.%20In%20this%20work%2C%20we%20used%20%24t%24-SimCNE%20to%20visualise%20medical%0Aimage%20datasets%2C%20including%20examples%20from%20dermatology%2C%20histology%2C%20and%20blood%0Amicroscopy.%20We%20found%20that%20increasing%20the%20set%20of%20data%20augmentations%20to%20include%0Aarbitrary%20rotations%20improved%20the%20results%20in%20terms%20of%20class%20separability%2C%0Acompared%20to%20data%20augmentations%20used%20for%20natural%20images.%20Our%202D%20representations%0Ashow%20medically%20relevant%20structures%20and%20can%20be%20used%20to%20aid%20data%20exploration%20and%0Aannotation%2C%20improving%20on%20common%20approaches%20for%20data%20visualisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14566v1&entry.124074799=Read"},
{"title": "Link Prediction under Heterophily: A Physics-Inspired Graph Neural\n  Network Approach", "author": "Andrea Giuseppe Di Francesco and Francesco Caso and Maria Sofia Bucarelli and Fabrizio Silvestri", "abstract": "  In the past years, Graph Neural Networks (GNNs) have become the `de facto'\nstandard in various deep learning domains, thanks to their flexibility in\nmodeling real-world phenomena represented as graphs. However, the\nmessage-passing mechanism of GNNs faces challenges in learnability and\nexpressivity, hindering high performance on heterophilic graphs, where adjacent\nnodes frequently have different labels. Most existing solutions addressing\nthese challenges are primarily confined to specific benchmarks focused on node\nclassification tasks. This narrow focus restricts the potential impact that\nlink prediction under heterophily could offer in several applications,\nincluding recommender systems. For example, in social networks, two users may\nbe connected for some latent reason, making it challenging to predict such\nconnections in advance. Physics-Inspired GNNs such as GRAFF provided a\nsignificant contribution to enhance node classification performance under\nheterophily, thanks to the adoption of physics biases in the message-passing.\nDrawing inspiration from these findings, we advocate that the methodology\nemployed by GRAFF can improve link prediction performance as well. To further\nexplore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link\nprediction. We evaluate its efficacy within a recent collection of heterophilic\ngraphs, establishing a new benchmark for link prediction under heterophily. Our\napproach surpasses previous methods, in most of the datasets, showcasing a\nstrong flexibility in different contexts, and achieving relative AUROC\nimprovements of up to 26.7%.\n", "link": "http://arxiv.org/abs/2402.14802v1", "date": "2024-02-22", "relevancy": 1.9051, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4812}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4657}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Prediction%20under%20Heterophily%3A%20A%20Physics-Inspired%20Graph%20Neural%0A%20%20Network%20Approach&entry.906535625=Andrea%20Giuseppe%20Di%20Francesco%20and%20Francesco%20Caso%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri&entry.1292438233=%20%20In%20the%20past%20years%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20%60de%20facto%27%0Astandard%20in%20various%20deep%20learning%20domains%2C%20thanks%20to%20their%20flexibility%20in%0Amodeling%20real-world%20phenomena%20represented%20as%20graphs.%20However%2C%20the%0Amessage-passing%20mechanism%20of%20GNNs%20faces%20challenges%20in%20learnability%20and%0Aexpressivity%2C%20hindering%20high%20performance%20on%20heterophilic%20graphs%2C%20where%20adjacent%0Anodes%20frequently%20have%20different%20labels.%20Most%20existing%20solutions%20addressing%0Athese%20challenges%20are%20primarily%20confined%20to%20specific%20benchmarks%20focused%20on%20node%0Aclassification%20tasks.%20This%20narrow%20focus%20restricts%20the%20potential%20impact%20that%0Alink%20prediction%20under%20heterophily%20could%20offer%20in%20several%20applications%2C%0Aincluding%20recommender%20systems.%20For%20example%2C%20in%20social%20networks%2C%20two%20users%20may%0Abe%20connected%20for%20some%20latent%20reason%2C%20making%20it%20challenging%20to%20predict%20such%0Aconnections%20in%20advance.%20Physics-Inspired%20GNNs%20such%20as%20GRAFF%20provided%20a%0Asignificant%20contribution%20to%20enhance%20node%20classification%20performance%20under%0Aheterophily%2C%20thanks%20to%20the%20adoption%20of%20physics%20biases%20in%20the%20message-passing.%0ADrawing%20inspiration%20from%20these%20findings%2C%20we%20advocate%20that%20the%20methodology%0Aemployed%20by%20GRAFF%20can%20improve%20link%20prediction%20performance%20as%20well.%20To%20further%0Aexplore%20this%20hypothesis%2C%20we%20introduce%20GRAFF-LP%2C%20an%20extension%20of%20GRAFF%20to%20link%0Aprediction.%20We%20evaluate%20its%20efficacy%20within%20a%20recent%20collection%20of%20heterophilic%0Agraphs%2C%20establishing%20a%20new%20benchmark%20for%20link%20prediction%20under%20heterophily.%20Our%0Aapproach%20surpasses%20previous%20methods%2C%20in%20most%20of%20the%20datasets%2C%20showcasing%20a%0Astrong%20flexibility%20in%20different%20contexts%2C%20and%20achieving%20relative%20AUROC%0Aimprovements%20of%20up%20to%2026.7%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14802v1&entry.124074799=Read"},
{"title": "Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using\n  Self-Supervised Learning", "author": "Daniel Capell\u00e1n-Mart\u00edn and Abhijeet Parida and Juan J. G\u00f3mez-Valverde and Ramon Sanchez-Jacob and Pooneh Roshanitabrizi and Marius G. Linguraru and Mar\u00eda J. Ledesma-Carbayo and Syed M. Anwar", "abstract": "  Tuberculosis (TB) remains a significant global health challenge, with\npediatric cases posing a major concern. The World Health Organization (WHO)\nadvocates for chest X-rays (CXRs) for TB screening. However, visual\ninterpretation by radiologists can be subjective, time-consuming and prone to\nerror, especially in pediatric TB. Artificial intelligence (AI)-driven\ncomputer-aided detection (CAD) tools, especially those utilizing deep learning,\nshow promise in enhancing lung disease detection. However, challenges include\ndata scarcity and lack of generalizability. In this context, we propose a novel\nself-supervised paradigm leveraging Vision Transformers (ViT) for improved TB\ndetection in CXR, enabling zero-shot pediatric TB detection. We demonstrate\nimprovements in TB detection performance ($\\sim$12.7% and $\\sim$13.4% top\nAUC/AUPR gains in adults and children, respectively) when conducting\nself-supervised pre-training when compared to fully-supervised (i.e., non\npre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR\nin adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB\ndetection. As a result, this work demonstrates that self-supervised learning on\nadult CXRs effectively extends to challenging downstream tasks such as\npediatric TB detection, where data are scarce.\n", "link": "http://arxiv.org/abs/2402.14741v1", "date": "2024-02-22", "relevancy": 1.8825, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4938}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4742}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.446}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Pediatric%20Tuberculosis%20Detection%20in%20Chest%20X-Rays%20using%0A%20%20Self-Supervised%20Learning&entry.906535625=Daniel%20Capell%C3%A1n-Mart%C3%ADn%20and%20Abhijeet%20Parida%20and%20Juan%20J.%20G%C3%B3mez-Valverde%20and%20Ramon%20Sanchez-Jacob%20and%20Pooneh%20Roshanitabrizi%20and%20Marius%20G.%20Linguraru%20and%20Mar%C3%ADa%20J.%20Ledesma-Carbayo%20and%20Syed%20M.%20Anwar&entry.1292438233=%20%20Tuberculosis%20%28TB%29%20remains%20a%20significant%20global%20health%20challenge%2C%20with%0Apediatric%20cases%20posing%20a%20major%20concern.%20The%20World%20Health%20Organization%20%28WHO%29%0Aadvocates%20for%20chest%20X-rays%20%28CXRs%29%20for%20TB%20screening.%20However%2C%20visual%0Ainterpretation%20by%20radiologists%20can%20be%20subjective%2C%20time-consuming%20and%20prone%20to%0Aerror%2C%20especially%20in%20pediatric%20TB.%20Artificial%20intelligence%20%28AI%29-driven%0Acomputer-aided%20detection%20%28CAD%29%20tools%2C%20especially%20those%20utilizing%20deep%20learning%2C%0Ashow%20promise%20in%20enhancing%20lung%20disease%20detection.%20However%2C%20challenges%20include%0Adata%20scarcity%20and%20lack%20of%20generalizability.%20In%20this%20context%2C%20we%20propose%20a%20novel%0Aself-supervised%20paradigm%20leveraging%20Vision%20Transformers%20%28ViT%29%20for%20improved%20TB%0Adetection%20in%20CXR%2C%20enabling%20zero-shot%20pediatric%20TB%20detection.%20We%20demonstrate%0Aimprovements%20in%20TB%20detection%20performance%20%28%24%5Csim%2412.7%25%20and%20%24%5Csim%2413.4%25%20top%0AAUC/AUPR%20gains%20in%20adults%20and%20children%2C%20respectively%29%20when%20conducting%0Aself-supervised%20pre-training%20when%20compared%20to%20fully-supervised%20%28i.e.%2C%20non%0Apre-trained%29%20ViT%20models%2C%20achieving%20top%20performances%20of%200.959%20AUC%20and%200.962%20AUPR%0Ain%20adult%20TB%20detection%2C%20and%200.697%20AUC%20and%200.607%20AUPR%20in%20zero-shot%20pediatric%20TB%0Adetection.%20As%20a%20result%2C%20this%20work%20demonstrates%20that%20self-supervised%20learning%20on%0Aadult%20CXRs%20effectively%20extends%20to%20challenging%20downstream%20tasks%20such%20as%0Apediatric%20TB%20detection%2C%20where%20data%20are%20scarce.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14741v1&entry.124074799=Read"},
{"title": "Quadruplet Loss For Improving the Robustness to Face Morphing Attacks", "author": "Iurii Medvedev and Nuno Gon\u00e7alves", "abstract": "  Recent advancements in deep learning have revolutionized technology and\nsecurity measures, necessitating robust identification methods. Biometric\napproaches, leveraging personalized characteristics, offer a promising\nsolution. However, Face Recognition Systems are vulnerable to sophisticated\nattacks, notably face morphing techniques, enabling the creation of fraudulent\ndocuments. In this study, we introduce a novel quadruplet loss function for\nincreasing the robustness of face recognition systems against morphing attacks.\nOur approach involves specific sampling of face image quadruplets, combined\nwith face morphs, for network training. Experimental results demonstrate the\nefficiency of our strategy in improving the robustness of face recognition\nnetworks against morphing attacks.\n", "link": "http://arxiv.org/abs/2402.14665v1", "date": "2024-02-22", "relevancy": 1.8814, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4791}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4636}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quadruplet%20Loss%20For%20Improving%20the%20Robustness%20to%20Face%20Morphing%20Attacks&entry.906535625=Iurii%20Medvedev%20and%20Nuno%20Gon%C3%A7alves&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20revolutionized%20technology%20and%0Asecurity%20measures%2C%20necessitating%20robust%20identification%20methods.%20Biometric%0Aapproaches%2C%20leveraging%20personalized%20characteristics%2C%20offer%20a%20promising%0Asolution.%20However%2C%20Face%20Recognition%20Systems%20are%20vulnerable%20to%20sophisticated%0Aattacks%2C%20notably%20face%20morphing%20techniques%2C%20enabling%20the%20creation%20of%20fraudulent%0Adocuments.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20quadruplet%20loss%20function%20for%0Aincreasing%20the%20robustness%20of%20face%20recognition%20systems%20against%20morphing%20attacks.%0AOur%20approach%20involves%20specific%20sampling%20of%20face%20image%20quadruplets%2C%20combined%0Awith%20face%20morphs%2C%20for%20network%20training.%20Experimental%20results%20demonstrate%20the%0Aefficiency%20of%20our%20strategy%20in%20improving%20the%20robustness%20of%20face%20recognition%0Anetworks%20against%20morphing%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14665v1&entry.124074799=Read"},
{"title": "AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer", "author": "Joonwoo Kwon and Sooyoung Kim and Yuewei Lin and Shinjae Yoo and Jiook Cha", "abstract": "  Neural style transfer (NST) has evolved significantly in recent years. Yet,\ndespite its rapid progress and advancement, existing NST methods either\nstruggle to transfer aesthetic information from a style effectively or suffer\nfrom high computational costs and inefficiencies in feature disentanglement due\nto using pre-trained models. This work proposes a lightweight but effective\nmodel, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose\nthe image via its frequencies to better disentangle aesthetic styles from the\nreference image while training the entire model in an end-to-end manner to\nexclude pre-trained models at inference completely. To improve the network's\nability to extract more distinct representations and further enhance the\nstylization quality, this work introduces a new aesthetic feature: contrastive\nloss. Extensive experiments and ablations show the approach not only\noutperforms recent NST methods in terms of stylization quality, but it also\nachieves faster inference. Codes are available at\nhttps://github.com/Sooyyoungg/AesFA.\n", "link": "http://arxiv.org/abs/2312.05928v3", "date": "2024-02-22", "relevancy": 1.8781, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4964}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.485}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4365}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AesFA%3A%20An%20Aesthetic%20Feature-Aware%20Arbitrary%20Neural%20Style%20Transfer&entry.906535625=Joonwoo%20Kwon%20and%20Sooyoung%20Kim%20and%20Yuewei%20Lin%20and%20Shinjae%20Yoo%20and%20Jiook%20Cha&entry.1292438233=%20%20Neural%20style%20transfer%20%28NST%29%20has%20evolved%20significantly%20in%20recent%20years.%20Yet%2C%0Adespite%20its%20rapid%20progress%20and%20advancement%2C%20existing%20NST%20methods%20either%0Astruggle%20to%20transfer%20aesthetic%20information%20from%20a%20style%20effectively%20or%20suffer%0Afrom%20high%20computational%20costs%20and%20inefficiencies%20in%20feature%20disentanglement%20due%0Ato%20using%20pre-trained%20models.%20This%20work%20proposes%20a%20lightweight%20but%20effective%0Amodel%2C%20AesFA%20--%20Aesthetic%20Feature-Aware%20NST.%20The%20primary%20idea%20is%20to%20decompose%0Athe%20image%20via%20its%20frequencies%20to%20better%20disentangle%20aesthetic%20styles%20from%20the%0Areference%20image%20while%20training%20the%20entire%20model%20in%20an%20end-to-end%20manner%20to%0Aexclude%20pre-trained%20models%20at%20inference%20completely.%20To%20improve%20the%20network%27s%0Aability%20to%20extract%20more%20distinct%20representations%20and%20further%20enhance%20the%0Astylization%20quality%2C%20this%20work%20introduces%20a%20new%20aesthetic%20feature%3A%20contrastive%0Aloss.%20Extensive%20experiments%20and%20ablations%20show%20the%20approach%20not%20only%0Aoutperforms%20recent%20NST%20methods%20in%20terms%20of%20stylization%20quality%2C%20but%20it%20also%0Aachieves%20faster%20inference.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Sooyyoungg/AesFA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05928v3&entry.124074799=Read"},
{"title": "Bandits with Abstention under Expert Advice", "author": "Stephen Pasteris and Alberto Rumi and Maximilian Thiessen and Shota Saito and Atsushi Miyauchi and Fabio Vitale and Mark Herbster", "abstract": "  We study the classic problem of prediction with expert advice under bandit\nfeedback. Our model assumes that one action, corresponding to the learner's\nabstention from play, has no reward or loss on every trial. We propose the CBA\nalgorithm, which exploits this assumption to obtain reward bounds that can\nsignificantly improve those of the classical Exp4 algorithm. We can view our\nproblem as the aggregation of confidence-rated predictors when the learner has\nthe option of abstention from play. Importantly, we are the first to achieve\nbounds on the expected cumulative reward for general confidence-rated\npredictors. In the special case of specialists we achieve a novel reward bound,\nsignificantly improving previous bounds of SpecialistExp (treating abstention\nas another action). As an example application, we discuss learning unions of\nballs in a finite metric space. In this contextual setting, we devise an\nefficient implementation of CBA, reducing the runtime from quadratic to almost\nlinear in the number of contexts. Preliminary experiments show that CBA\nimproves over existing bandit algorithms.\n", "link": "http://arxiv.org/abs/2402.14585v1", "date": "2024-02-22", "relevancy": 1.8743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4782}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4537}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandits%20with%20Abstention%20under%20Expert%20Advice&entry.906535625=Stephen%20Pasteris%20and%20Alberto%20Rumi%20and%20Maximilian%20Thiessen%20and%20Shota%20Saito%20and%20Atsushi%20Miyauchi%20and%20Fabio%20Vitale%20and%20Mark%20Herbster&entry.1292438233=%20%20We%20study%20the%20classic%20problem%20of%20prediction%20with%20expert%20advice%20under%20bandit%0Afeedback.%20Our%20model%20assumes%20that%20one%20action%2C%20corresponding%20to%20the%20learner%27s%0Aabstention%20from%20play%2C%20has%20no%20reward%20or%20loss%20on%20every%20trial.%20We%20propose%20the%20CBA%0Aalgorithm%2C%20which%20exploits%20this%20assumption%20to%20obtain%20reward%20bounds%20that%20can%0Asignificantly%20improve%20those%20of%20the%20classical%20Exp4%20algorithm.%20We%20can%20view%20our%0Aproblem%20as%20the%20aggregation%20of%20confidence-rated%20predictors%20when%20the%20learner%20has%0Athe%20option%20of%20abstention%20from%20play.%20Importantly%2C%20we%20are%20the%20first%20to%20achieve%0Abounds%20on%20the%20expected%20cumulative%20reward%20for%20general%20confidence-rated%0Apredictors.%20In%20the%20special%20case%20of%20specialists%20we%20achieve%20a%20novel%20reward%20bound%2C%0Asignificantly%20improving%20previous%20bounds%20of%20SpecialistExp%20%28treating%20abstention%0Aas%20another%20action%29.%20As%20an%20example%20application%2C%20we%20discuss%20learning%20unions%20of%0Aballs%20in%20a%20finite%20metric%20space.%20In%20this%20contextual%20setting%2C%20we%20devise%20an%0Aefficient%20implementation%20of%20CBA%2C%20reducing%20the%20runtime%20from%20quadratic%20to%20almost%0Alinear%20in%20the%20number%20of%20contexts.%20Preliminary%20experiments%20show%20that%20CBA%0Aimproves%20over%20existing%20bandit%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14585v1&entry.124074799=Read"},
{"title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language\n  Models", "author": "Kenneth Li and Samy Jelassi and Hugh Zhang and Sham Kakade and Martin Wattenberg and David Brandfonbrener", "abstract": "  We present an approach called Q-probing to adapt a pre-trained language model\nto maximize a task-specific reward function. At a high level, Q-probing sits\nbetween heavier approaches such as finetuning and lighter approaches such as\nfew shot prompting, but can also be combined with either. The idea is to learn\na simple linear function on a model's embedding space that can be used to\nreweight candidate completions. We theoretically show that this sampling\nprocedure is equivalent to a KL-constrained maximization of the Q-probe as the\nnumber of samples increases. To train the Q-probes we consider either reward\nmodeling or a class of novel direct policy learning objectives based on\nimportance weighted policy gradients. With this technique, we see gains in\ndomains with ground-truth rewards (code generation) as well as implicit rewards\ndefined by preference data, even outperforming finetuning in data-limited\nregimes. Moreover, a Q-probe can be trained on top of an API since it only\nassumes access to sampling and embeddings. Code:\nhttps://github.com/likenneth/q_probe .\n", "link": "http://arxiv.org/abs/2402.14688v1", "date": "2024-02-22", "relevancy": 1.8675, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4662}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4587}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Probe%3A%20A%20Lightweight%20Approach%20to%20Reward%20Maximization%20for%20Language%0A%20%20Models&entry.906535625=Kenneth%20Li%20and%20Samy%20Jelassi%20and%20Hugh%20Zhang%20and%20Sham%20Kakade%20and%20Martin%20Wattenberg%20and%20David%20Brandfonbrener&entry.1292438233=%20%20We%20present%20an%20approach%20called%20Q-probing%20to%20adapt%20a%20pre-trained%20language%20model%0Ato%20maximize%20a%20task-specific%20reward%20function.%20At%20a%20high%20level%2C%20Q-probing%20sits%0Abetween%20heavier%20approaches%20such%20as%20finetuning%20and%20lighter%20approaches%20such%20as%0Afew%20shot%20prompting%2C%20but%20can%20also%20be%20combined%20with%20either.%20The%20idea%20is%20to%20learn%0Aa%20simple%20linear%20function%20on%20a%20model%27s%20embedding%20space%20that%20can%20be%20used%20to%0Areweight%20candidate%20completions.%20We%20theoretically%20show%20that%20this%20sampling%0Aprocedure%20is%20equivalent%20to%20a%20KL-constrained%20maximization%20of%20the%20Q-probe%20as%20the%0Anumber%20of%20samples%20increases.%20To%20train%20the%20Q-probes%20we%20consider%20either%20reward%0Amodeling%20or%20a%20class%20of%20novel%20direct%20policy%20learning%20objectives%20based%20on%0Aimportance%20weighted%20policy%20gradients.%20With%20this%20technique%2C%20we%20see%20gains%20in%0Adomains%20with%20ground-truth%20rewards%20%28code%20generation%29%20as%20well%20as%20implicit%20rewards%0Adefined%20by%20preference%20data%2C%20even%20outperforming%20finetuning%20in%20data-limited%0Aregimes.%20Moreover%2C%20a%20Q-probe%20can%20be%20trained%20on%20top%20of%20an%20API%20since%20it%20only%0Aassumes%20access%20to%20sampling%20and%20embeddings.%20Code%3A%0Ahttps%3A//github.com/likenneth/q_probe%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14688v1&entry.124074799=Read"},
{"title": "Incorporating Expert Rules into Neural Networks in the Framework of\n  Concept-Based Learning", "author": "Andrei V. Konstantinov and Lev V. Utkin", "abstract": "  A problem of incorporating the expert rules into machine learning models for\nextending the concept-based learning is formulated in the paper. It is proposed\nhow to combine logical rules and neural networks predicting the concept\nprobabilities. The first idea behind the combination is to form constraints for\na joint probability distribution over all combinations of concept values to\nsatisfy the expert rules. The second idea is to represent a feasible set of\nprobability distributions in the form of a convex polytope and to use its\nvertices or faces. We provide several approaches for solving the stated problem\nand for training neural networks which guarantee that the output probabilities\nof concepts would not violate the expert rules. The solution of the problem can\nbe viewed as a way for combining the inductive and deductive learning. Expert\nrules are used in a broader sense when any logical function that connects\nconcepts and class labels or just concepts with each other can be regarded as a\nrule. This feature significantly expands the class of the proposed results.\nNumerical examples illustrate the approaches. The code of proposed algorithms\nis publicly available.\n", "link": "http://arxiv.org/abs/2402.14726v1", "date": "2024-02-22", "relevancy": 1.8666, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4447}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Expert%20Rules%20into%20Neural%20Networks%20in%20the%20Framework%20of%0A%20%20Concept-Based%20Learning&entry.906535625=Andrei%20V.%20Konstantinov%20and%20Lev%20V.%20Utkin&entry.1292438233=%20%20A%20problem%20of%20incorporating%20the%20expert%20rules%20into%20machine%20learning%20models%20for%0Aextending%20the%20concept-based%20learning%20is%20formulated%20in%20the%20paper.%20It%20is%20proposed%0Ahow%20to%20combine%20logical%20rules%20and%20neural%20networks%20predicting%20the%20concept%0Aprobabilities.%20The%20first%20idea%20behind%20the%20combination%20is%20to%20form%20constraints%20for%0Aa%20joint%20probability%20distribution%20over%20all%20combinations%20of%20concept%20values%20to%0Asatisfy%20the%20expert%20rules.%20The%20second%20idea%20is%20to%20represent%20a%20feasible%20set%20of%0Aprobability%20distributions%20in%20the%20form%20of%20a%20convex%20polytope%20and%20to%20use%20its%0Avertices%20or%20faces.%20We%20provide%20several%20approaches%20for%20solving%20the%20stated%20problem%0Aand%20for%20training%20neural%20networks%20which%20guarantee%20that%20the%20output%20probabilities%0Aof%20concepts%20would%20not%20violate%20the%20expert%20rules.%20The%20solution%20of%20the%20problem%20can%0Abe%20viewed%20as%20a%20way%20for%20combining%20the%20inductive%20and%20deductive%20learning.%20Expert%0Arules%20are%20used%20in%20a%20broader%20sense%20when%20any%20logical%20function%20that%20connects%0Aconcepts%20and%20class%20labels%20or%20just%20concepts%20with%20each%20other%20can%20be%20regarded%20as%20a%0Arule.%20This%20feature%20significantly%20expands%20the%20class%20of%20the%20proposed%20results.%0ANumerical%20examples%20illustrate%20the%20approaches.%20The%20code%20of%20proposed%20algorithms%0Ais%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14726v1&entry.124074799=Read"},
{"title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in\n  Closed-Source LLMs", "author": "Simone Balloccu and Patr\u00edcia Schmidtov\u00e1 and Mateusz Lango and Ond\u0159ej Du\u0161ek", "abstract": "  Natural Language Processing (NLP) research is increasingly focusing on the\nuse of Large Language Models (LLMs), with some of the most popular ones being\neither fully or partially closed-source. The lack of access to model details,\nespecially regarding training data, has repeatedly raised concerns about data\ncontamination among researchers. Several attempts have been made to address\nthis issue, but they are limited to anecdotal evidence and trial and error.\nAdditionally, they overlook the problem of \\emph{indirect} data leaking, where\nmodels are iteratively improved by using data coming from users. In this work,\nwe conduct the first systematic analysis of work using OpenAI's GPT-3.5 and\nGPT-4, the most prominently used LLMs today, in the context of data\ncontamination. By analysing 255 papers and considering OpenAI's data usage\npolicy, we extensively document the amount of data leaked to these models\nduring the first year after the model's release. We report that these models\nhave been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the\nsame time, we document a number of evaluation malpractices emerging in the\nreviewed papers, such as unfair or missing baseline comparisons and\nreproducibility issues. We release our results as a collaborative project on\nhttps://leak-llm.github.io/, where other researchers can contribute to our\nefforts.\n", "link": "http://arxiv.org/abs/2402.03927v2", "date": "2024-02-22", "relevancy": 1.8661, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4921}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4718}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.451}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leak%2C%20Cheat%2C%20Repeat%3A%20Data%20Contamination%20and%20Evaluation%20Malpractices%20in%0A%20%20Closed-Source%20LLMs&entry.906535625=Simone%20Balloccu%20and%20Patr%C3%ADcia%20Schmidtov%C3%A1%20and%20Mateusz%20Lango%20and%20Ond%C5%99ej%20Du%C5%A1ek&entry.1292438233=%20%20Natural%20Language%20Processing%20%28NLP%29%20research%20is%20increasingly%20focusing%20on%20the%0Ause%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20with%20some%20of%20the%20most%20popular%20ones%20being%0Aeither%20fully%20or%20partially%20closed-source.%20The%20lack%20of%20access%20to%20model%20details%2C%0Aespecially%20regarding%20training%20data%2C%20has%20repeatedly%20raised%20concerns%20about%20data%0Acontamination%20among%20researchers.%20Several%20attempts%20have%20been%20made%20to%20address%0Athis%20issue%2C%20but%20they%20are%20limited%20to%20anecdotal%20evidence%20and%20trial%20and%20error.%0AAdditionally%2C%20they%20overlook%20the%20problem%20of%20%5Cemph%7Bindirect%7D%20data%20leaking%2C%20where%0Amodels%20are%20iteratively%20improved%20by%20using%20data%20coming%20from%20users.%20In%20this%20work%2C%0Awe%20conduct%20the%20first%20systematic%20analysis%20of%20work%20using%20OpenAI%27s%20GPT-3.5%20and%0AGPT-4%2C%20the%20most%20prominently%20used%20LLMs%20today%2C%20in%20the%20context%20of%20data%0Acontamination.%20By%20analysing%20255%20papers%20and%20considering%20OpenAI%27s%20data%20usage%0Apolicy%2C%20we%20extensively%20document%20the%20amount%20of%20data%20leaked%20to%20these%20models%0Aduring%20the%20first%20year%20after%20the%20model%27s%20release.%20We%20report%20that%20these%20models%0Ahave%20been%20globally%20exposed%20to%20%24%5Csim%244.7M%20samples%20from%20263%20benchmarks.%20At%20the%0Asame%20time%2C%20we%20document%20a%20number%20of%20evaluation%20malpractices%20emerging%20in%20the%0Areviewed%20papers%2C%20such%20as%20unfair%20or%20missing%20baseline%20comparisons%20and%0Areproducibility%20issues.%20We%20release%20our%20results%20as%20a%20collaborative%20project%20on%0Ahttps%3A//leak-llm.github.io/%2C%20where%20other%20researchers%20can%20contribute%20to%20our%0Aefforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03927v2&entry.124074799=Read"},
{"title": "Mobiprox: Supporting Dynamic Approximate Computing on Mobiles", "author": "Matev\u017e Fabjan\u010di\u010d and Octavian Machidon and Hashim Sharif and Yifan Zhao and Sa\u0161a Misailovi\u0107 and Veljko Pejovi\u0107", "abstract": "  Runtime-tunable context-dependent network compression would make mobile deep\nlearning (DL) adaptable to often varying resource availability, input\n\"difficulty\", or user needs. The existing compression techniques significantly\nreduce the memory, processing, and energy tax of DL, yet, the resulting models\ntend to be permanently impaired, sacrificing the inference power for reduced\nresource usage. The existing tunable compression approaches, on the other hand,\nrequire expensive re-training, do not support arbitrary strategies for adapting\nthe compression and do not provide mobile-ready implementations.\n  In this paper we present Mobiprox, a framework enabling mobile DL with\nflexible precision. Mobiprox implements tunable approximations of tensor\noperations and enables runtime-adaptable approximation of individual network\nlayers. A profiler and a tuner included with Mobiprox identify the most\npromising neural network approximation configurations leading to the desired\ninference quality with the minimal use of resources. Furthermore, we develop\ncontrol strategies that depending on contextual factors, such as the input data\ndifficulty, dynamically adjust the approximation levels across a mobile DL\nmodel's layers. We implement Mobiprox in Android OS and through experiments in\ndiverse mobile domains, including human activity recognition and spoken keyword\ndetection, demonstrate that it can save up to 15% system-wide energy with a\nminimal impact on the inference accuracy.\n", "link": "http://arxiv.org/abs/2303.11291v2", "date": "2024-02-22", "relevancy": 1.8593, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4712}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4427}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobiprox%3A%20Supporting%20Dynamic%20Approximate%20Computing%20on%20Mobiles&entry.906535625=Matev%C5%BE%20Fabjan%C4%8Di%C4%8D%20and%20Octavian%20Machidon%20and%20Hashim%20Sharif%20and%20Yifan%20Zhao%20and%20Sa%C5%A1a%20Misailovi%C4%87%20and%20Veljko%20Pejovi%C4%87&entry.1292438233=%20%20Runtime-tunable%20context-dependent%20network%20compression%20would%20make%20mobile%20deep%0Alearning%20%28DL%29%20adaptable%20to%20often%20varying%20resource%20availability%2C%20input%0A%22difficulty%22%2C%20or%20user%20needs.%20The%20existing%20compression%20techniques%20significantly%0Areduce%20the%20memory%2C%20processing%2C%20and%20energy%20tax%20of%20DL%2C%20yet%2C%20the%20resulting%20models%0Atend%20to%20be%20permanently%20impaired%2C%20sacrificing%20the%20inference%20power%20for%20reduced%0Aresource%20usage.%20The%20existing%20tunable%20compression%20approaches%2C%20on%20the%20other%20hand%2C%0Arequire%20expensive%20re-training%2C%20do%20not%20support%20arbitrary%20strategies%20for%20adapting%0Athe%20compression%20and%20do%20not%20provide%20mobile-ready%20implementations.%0A%20%20In%20this%20paper%20we%20present%20Mobiprox%2C%20a%20framework%20enabling%20mobile%20DL%20with%0Aflexible%20precision.%20Mobiprox%20implements%20tunable%20approximations%20of%20tensor%0Aoperations%20and%20enables%20runtime-adaptable%20approximation%20of%20individual%20network%0Alayers.%20A%20profiler%20and%20a%20tuner%20included%20with%20Mobiprox%20identify%20the%20most%0Apromising%20neural%20network%20approximation%20configurations%20leading%20to%20the%20desired%0Ainference%20quality%20with%20the%20minimal%20use%20of%20resources.%20Furthermore%2C%20we%20develop%0Acontrol%20strategies%20that%20depending%20on%20contextual%20factors%2C%20such%20as%20the%20input%20data%0Adifficulty%2C%20dynamically%20adjust%20the%20approximation%20levels%20across%20a%20mobile%20DL%0Amodel%27s%20layers.%20We%20implement%20Mobiprox%20in%20Android%20OS%20and%20through%20experiments%20in%0Adiverse%20mobile%20domains%2C%20including%20human%20activity%20recognition%20and%20spoken%20keyword%0Adetection%2C%20demonstrate%20that%20it%20can%20save%20up%20to%2015%25%20system-wide%20energy%20with%20a%0Aminimal%20impact%20on%20the%20inference%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11291v2&entry.124074799=Read"},
{"title": "Tables as Images? Exploring the Strengths and Limitations of LLMs on\n  Multimodal Representations of Tabular Data", "author": "Naihao Deng and Zhenjie Sun and Ruiqi He and Aman Sikka and Yulong Chen and Lin Ma and Yue Zhang and Rada Mihalcea", "abstract": "  In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analysis extends across six benchmarks for table-related tasks\nsuch as question-answering and fact-checking. We introduce for the first time\nthe assessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the influence of representation and prompting on\nLLM performance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.\n", "link": "http://arxiv.org/abs/2402.12424v3", "date": "2024-02-23", "relevancy": 1.8577, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4854}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4614}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.459}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tables%20as%20Images%3F%20Exploring%20the%20Strengths%20and%20Limitations%20of%20LLMs%20on%0A%20%20Multimodal%20Representations%20of%20Tabular%20Data&entry.906535625=Naihao%20Deng%20and%20Zhenjie%20Sun%20and%20Ruiqi%20He%20and%20Aman%20Sikka%20and%20Yulong%20Chen%20and%20Lin%20Ma%20and%20Yue%20Zhang%20and%20Rada%20Mihalcea&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20effectiveness%20of%20various%20LLMs%20in%0Ainterpreting%20tabular%20data%20through%20different%20prompting%20strategies%20and%20data%0Aformats.%20Our%20analysis%20extends%20across%20six%20benchmarks%20for%20table-related%20tasks%0Asuch%20as%20question-answering%20and%20fact-checking.%20We%20introduce%20for%20the%20first%20time%0Athe%20assessment%20of%20LLMs%27%20performance%20on%20image-based%20table%20representations.%0ASpecifically%2C%20we%20compare%20five%20text-based%20and%20three%20image-based%20table%0Arepresentations%2C%20demonstrating%20the%20influence%20of%20representation%20and%20prompting%20on%0ALLM%20performance.%20Our%20study%20provides%20insights%20into%20the%20effective%20use%20of%20LLMs%20on%0Atable-related%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12424v3&entry.124074799=Read"},
{"title": "Bad Values but Good Behavior: Learning Highly Misspecified Bandits and\n  MDPs", "author": "Debangshu Banerjee and Aditya Gopalan", "abstract": "  Parametric, feature-based reward models are employed by a variety of\nalgorithms in decision-making settings such as bandits and Markov decision\nprocesses (MDPs). The typical assumption under which the algorithms are\nanalysed is realizability, i.e., that the true values of actions are perfectly\nexplained by some parametric model in the class. We are, however, interested in\nthe situation where the true values are (significantly) misspecified with\nrespect to the model class. For parameterized bandits, contextual bandits and\nMDPs, we identify structural conditions, depending on the problem instance and\nmodel class, under which basic algorithms such as $\\epsilon$-greedy, LinUCB and\nfitted Q-learning provably learn optimal policies under even highly\nmisspecified models. This is in contrast to existing worst-case results for,\nsay misspecified bandits, which show regret bounds that scale linearly with\ntime, and shows that there can be a nontrivially large set of bandit instances\nthat are robust to misspecification.\n", "link": "http://arxiv.org/abs/2310.09358v2", "date": "2024-02-22", "relevancy": 1.8557, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4607}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4507}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bad%20Values%20but%20Good%20Behavior%3A%20Learning%20Highly%20Misspecified%20Bandits%20and%0A%20%20MDPs&entry.906535625=Debangshu%20Banerjee%20and%20Aditya%20Gopalan&entry.1292438233=%20%20Parametric%2C%20feature-based%20reward%20models%20are%20employed%20by%20a%20variety%20of%0Aalgorithms%20in%20decision-making%20settings%20such%20as%20bandits%20and%20Markov%20decision%0Aprocesses%20%28MDPs%29.%20The%20typical%20assumption%20under%20which%20the%20algorithms%20are%0Aanalysed%20is%20realizability%2C%20i.e.%2C%20that%20the%20true%20values%20of%20actions%20are%20perfectly%0Aexplained%20by%20some%20parametric%20model%20in%20the%20class.%20We%20are%2C%20however%2C%20interested%20in%0Athe%20situation%20where%20the%20true%20values%20are%20%28significantly%29%20misspecified%20with%0Arespect%20to%20the%20model%20class.%20For%20parameterized%20bandits%2C%20contextual%20bandits%20and%0AMDPs%2C%20we%20identify%20structural%20conditions%2C%20depending%20on%20the%20problem%20instance%20and%0Amodel%20class%2C%20under%20which%20basic%20algorithms%20such%20as%20%24%5Cepsilon%24-greedy%2C%20LinUCB%20and%0Afitted%20Q-learning%20provably%20learn%20optimal%20policies%20under%20even%20highly%0Amisspecified%20models.%20This%20is%20in%20contrast%20to%20existing%20worst-case%20results%20for%2C%0Asay%20misspecified%20bandits%2C%20which%20show%20regret%20bounds%20that%20scale%20linearly%20with%0Atime%2C%20and%20shows%20that%20there%20can%20be%20a%20nontrivially%20large%20set%20of%20bandit%20instances%0Athat%20are%20robust%20to%20misspecification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09358v2&entry.124074799=Read"},
{"title": "Batch and match: black-box variational inference with a score-based\n  divergence", "author": "Diana Cai and Chirag Modi and Loucas Pillaud-Vivien and Charles C. Margossian and Robert M. Gower and David M. Blei and Lawrence K. Saul", "abstract": "  Most leading implementations of black-box variational inference (BBVI) are\nbased on optimizing a stochastic evidence lower bound (ELBO). But such\napproaches to BBVI often converge slowly due to the high variance of their\ngradient estimates. In this work, we propose batch and match (BaM), an\nalternative approach to BBVI based on a score-based divergence. Notably, this\nscore-based divergence can be optimized by a closed-form proximal update for\nGaussian variational families with full covariance matrices. We analyze the\nconvergence of BaM when the target distribution is Gaussian, and we prove that\nin the limit of infinite batch size the variational parameter updates converge\nexponentially quickly to the target mean and covariance. We also evaluate the\nperformance of BaM on Gaussian and non-Gaussian target distributions that arise\nfrom posterior inference in hierarchical and deep generative models. In these\nexperiments, we find that BaM typically converges in fewer (and sometimes\nsignificantly fewer) gradient evaluations than leading implementations of BBVI\nbased on ELBO maximization.\n", "link": "http://arxiv.org/abs/2402.14758v1", "date": "2024-02-22", "relevancy": 1.8514, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4391}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batch%20and%20match%3A%20black-box%20variational%20inference%20with%20a%20score-based%0A%20%20divergence&entry.906535625=Diana%20Cai%20and%20Chirag%20Modi%20and%20Loucas%20Pillaud-Vivien%20and%20Charles%20C.%20Margossian%20and%20Robert%20M.%20Gower%20and%20David%20M.%20Blei%20and%20Lawrence%20K.%20Saul&entry.1292438233=%20%20Most%20leading%20implementations%20of%20black-box%20variational%20inference%20%28BBVI%29%20are%0Abased%20on%20optimizing%20a%20stochastic%20evidence%20lower%20bound%20%28ELBO%29.%20But%20such%0Aapproaches%20to%20BBVI%20often%20converge%20slowly%20due%20to%20the%20high%20variance%20of%20their%0Agradient%20estimates.%20In%20this%20work%2C%20we%20propose%20batch%20and%20match%20%28BaM%29%2C%20an%0Aalternative%20approach%20to%20BBVI%20based%20on%20a%20score-based%20divergence.%20Notably%2C%20this%0Ascore-based%20divergence%20can%20be%20optimized%20by%20a%20closed-form%20proximal%20update%20for%0AGaussian%20variational%20families%20with%20full%20covariance%20matrices.%20We%20analyze%20the%0Aconvergence%20of%20BaM%20when%20the%20target%20distribution%20is%20Gaussian%2C%20and%20we%20prove%20that%0Ain%20the%20limit%20of%20infinite%20batch%20size%20the%20variational%20parameter%20updates%20converge%0Aexponentially%20quickly%20to%20the%20target%20mean%20and%20covariance.%20We%20also%20evaluate%20the%0Aperformance%20of%20BaM%20on%20Gaussian%20and%20non-Gaussian%20target%20distributions%20that%20arise%0Afrom%20posterior%20inference%20in%20hierarchical%20and%20deep%20generative%20models.%20In%20these%0Aexperiments%2C%20we%20find%20that%20BaM%20typically%20converges%20in%20fewer%20%28and%20sometimes%0Asignificantly%20fewer%29%20gradient%20evaluations%20than%20leading%20implementations%20of%20BBVI%0Abased%20on%20ELBO%20maximization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14758v1&entry.124074799=Read"},
{"title": "Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound", "author": "Reuben Adams and John Shawe-Taylor and Benjamin Guedj", "abstract": "  Current PAC-Bayes generalisation bounds are restricted to scalar metrics of\nperformance, such as the loss or error rate. However, one ideally wants more\ninformation-rich certificates that control the entire distribution of possible\noutcomes, such as the distribution of the test loss in regression, or the\nprobabilities of different mis classifications. We provide the first PAC-Bayes\nbound capable of providing such rich information by bounding the\nKullback-Leibler divergence between the empirical and true probabilities of a\nset of M error types, which can either be discretized loss values for\nregression, or the elements of the confusion matrix (or a partition thereof)\nfor classification. We transform our bound into a differentiable training\nobjective. Our bound is especially useful in cases where the severity of\ndifferent mis-classifications may change over time; existing PAC-Bayes bounds\ncan only bound a particular pre-decided weighting of the error types. In\ncontrast our bound implicitly controls all uncountably many weightings\nsimultaneously.\n", "link": "http://arxiv.org/abs/2202.05560v2", "date": "2024-02-22", "relevancy": 1.8487, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4788}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4702}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4476}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Multiple%20Errors%20Simultaneously%20with%20a%20PAC-Bayes%20Bound&entry.906535625=Reuben%20Adams%20and%20John%20Shawe-Taylor%20and%20Benjamin%20Guedj&entry.1292438233=%20%20Current%20PAC-Bayes%20generalisation%20bounds%20are%20restricted%20to%20scalar%20metrics%20of%0Aperformance%2C%20such%20as%20the%20loss%20or%20error%20rate.%20However%2C%20one%20ideally%20wants%20more%0Ainformation-rich%20certificates%20that%20control%20the%20entire%20distribution%20of%20possible%0Aoutcomes%2C%20such%20as%20the%20distribution%20of%20the%20test%20loss%20in%20regression%2C%20or%20the%0Aprobabilities%20of%20different%20mis%20classifications.%20We%20provide%20the%20first%20PAC-Bayes%0Abound%20capable%20of%20providing%20such%20rich%20information%20by%20bounding%20the%0AKullback-Leibler%20divergence%20between%20the%20empirical%20and%20true%20probabilities%20of%20a%0Aset%20of%20M%20error%20types%2C%20which%20can%20either%20be%20discretized%20loss%20values%20for%0Aregression%2C%20or%20the%20elements%20of%20the%20confusion%20matrix%20%28or%20a%20partition%20thereof%29%0Afor%20classification.%20We%20transform%20our%20bound%20into%20a%20differentiable%20training%0Aobjective.%20Our%20bound%20is%20especially%20useful%20in%20cases%20where%20the%20severity%20of%0Adifferent%20mis-classifications%20may%20change%20over%20time%3B%20existing%20PAC-Bayes%20bounds%0Acan%20only%20bound%20a%20particular%20pre-decided%20weighting%20of%20the%20error%20types.%20In%0Acontrast%20our%20bound%20implicitly%20controls%20all%20uncountably%20many%20weightings%0Asimultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.05560v2&entry.124074799=Read"},
{"title": "PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a\n  Diffusion Probabilistic Model", "author": "Yukiya Hono and Kei Hashimoto and Yoshihiko Nankaku and Keiichi Tokuda", "abstract": "  This paper presents a neural vocoder based on a denoising diffusion\nprobabilistic model (DDPM) incorporating explicit periodic signals as auxiliary\nconditioning signals. Recently, DDPM-based neural vocoders have gained\nprominence as non-autoregressive models that can generate high-quality\nwaveforms. The neural vocoders based on DDPM have the advantage of training\nwith a simple time-domain loss. In practical applications, such as singing\nvoice synthesis, there is a demand for neural vocoders to generate\nhigh-fidelity speech waveforms with flexible pitch control. However,\nconventional DDPM-based neural vocoders struggle to generate speech waveforms\nunder such conditions. Our proposed model aims to accurately capture the\nperiodic structure of speech waveforms by incorporating explicit periodic\nsignals. Experimental results show that our model improves sound quality and\nprovides better pitch control than conventional DDPM-based neural vocoders.\n", "link": "http://arxiv.org/abs/2402.14692v1", "date": "2024-02-22", "relevancy": 1.848, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4694}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4679}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4531}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PeriodGrad%3A%20Towards%20Pitch-Controllable%20Neural%20Vocoder%20Based%20on%20a%0A%20%20Diffusion%20Probabilistic%20Model&entry.906535625=Yukiya%20Hono%20and%20Kei%20Hashimoto%20and%20Yoshihiko%20Nankaku%20and%20Keiichi%20Tokuda&entry.1292438233=%20%20This%20paper%20presents%20a%20neural%20vocoder%20based%20on%20a%20denoising%20diffusion%0Aprobabilistic%20model%20%28DDPM%29%20incorporating%20explicit%20periodic%20signals%20as%20auxiliary%0Aconditioning%20signals.%20Recently%2C%20DDPM-based%20neural%20vocoders%20have%20gained%0Aprominence%20as%20non-autoregressive%20models%20that%20can%20generate%20high-quality%0Awaveforms.%20The%20neural%20vocoders%20based%20on%20DDPM%20have%20the%20advantage%20of%20training%0Awith%20a%20simple%20time-domain%20loss.%20In%20practical%20applications%2C%20such%20as%20singing%0Avoice%20synthesis%2C%20there%20is%20a%20demand%20for%20neural%20vocoders%20to%20generate%0Ahigh-fidelity%20speech%20waveforms%20with%20flexible%20pitch%20control.%20However%2C%0Aconventional%20DDPM-based%20neural%20vocoders%20struggle%20to%20generate%20speech%20waveforms%0Aunder%20such%20conditions.%20Our%20proposed%20model%20aims%20to%20accurately%20capture%20the%0Aperiodic%20structure%20of%20speech%20waveforms%20by%20incorporating%20explicit%20periodic%0Asignals.%20Experimental%20results%20show%20that%20our%20model%20improves%20sound%20quality%20and%0Aprovides%20better%20pitch%20control%20than%20conventional%20DDPM-based%20neural%20vocoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14692v1&entry.124074799=Read"},
{"title": "Finetuning Large Language Models for Vulnerability Detection", "author": "Alexey Shestov and Rodion Levichev and Ravil Mussabayev and Anton Cheshkov", "abstract": "  This paper presents the results of finetuning large language models (LLMs)\nfor the task of detecting vulnerabilities in source code. We leverage\nWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and\nadapt it for vulnerability detection through further finetuning. To accelerate\ntraining, we modify WizardCoder's training procedure, also we investigate\noptimal training regimes. For the imbalanced dataset with many more negative\nexamples than positive, we also explore different techniques to improve\nclassification performance. The finetuned WizardCoder model achieves\nimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerability\ndatasets over CodeBERT-like model, demonstrating the effectiveness of adapting\npretrained LLMs for vulnerability detection in source code. The key\ncontributions are finetuning the state-of-the-art code LLM, WizardCoder,\nincreasing its training speed without the performance harm, optimizing the\ntraining procedure and regimes, handling class imbalance, and improving\nperformance on difficult vulnerability detection datasets. This demonstrates\nthe potential for transfer learning by finetuning large pretrained language\nmodels for specialized source code analysis tasks.\n", "link": "http://arxiv.org/abs/2401.17010v2", "date": "2024-02-22", "relevancy": 1.8454, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4719}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4621}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4505}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finetuning%20Large%20Language%20Models%20for%20Vulnerability%20Detection&entry.906535625=Alexey%20Shestov%20and%20Rodion%20Levichev%20and%20Ravil%20Mussabayev%20and%20Anton%20Cheshkov&entry.1292438233=%20%20This%20paper%20presents%20the%20results%20of%20finetuning%20large%20language%20models%20%28LLMs%29%0Afor%20the%20task%20of%20detecting%20vulnerabilities%20in%20source%20code.%20We%20leverage%0AWizardCoder%2C%20a%20recent%20improvement%20of%20the%20state-of-the-art%20LLM%20StarCoder%2C%20and%0Aadapt%20it%20for%20vulnerability%20detection%20through%20further%20finetuning.%20To%20accelerate%0Atraining%2C%20we%20modify%20WizardCoder%27s%20training%20procedure%2C%20also%20we%20investigate%0Aoptimal%20training%20regimes.%20For%20the%20imbalanced%20dataset%20with%20many%20more%20negative%0Aexamples%20than%20positive%2C%20we%20also%20explore%20different%20techniques%20to%20improve%0Aclassification%20performance.%20The%20finetuned%20WizardCoder%20model%20achieves%0Aimprovement%20in%20ROC%20AUC%20and%20F1%20measures%20on%20balanced%20and%20imbalanced%20vulnerability%0Adatasets%20over%20CodeBERT-like%20model%2C%20demonstrating%20the%20effectiveness%20of%20adapting%0Apretrained%20LLMs%20for%20vulnerability%20detection%20in%20source%20code.%20The%20key%0Acontributions%20are%20finetuning%20the%20state-of-the-art%20code%20LLM%2C%20WizardCoder%2C%0Aincreasing%20its%20training%20speed%20without%20the%20performance%20harm%2C%20optimizing%20the%0Atraining%20procedure%20and%20regimes%2C%20handling%20class%20imbalance%2C%20and%20improving%0Aperformance%20on%20difficult%20vulnerability%20detection%20datasets.%20This%20demonstrates%0Athe%20potential%20for%20transfer%20learning%20by%20finetuning%20large%20pretrained%20language%0Amodels%20for%20specialized%20source%20code%20analysis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17010v2&entry.124074799=Read"},
{"title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control", "author": "Dimitri von R\u00fctte and Luca Biggio and Yannic Kilcher and Thomas Hofmann", "abstract": "  Generating music with deep neural networks has been an area of active\nresearch in recent years. While the quality of generated samples has been\nsteadily increasing, most methods are only able to exert minimal control over\nthe generated sequence, if any. We propose the self-supervised\ndescription-to-sequence task, which allows for fine-grained controllable\ngeneration on a global level. We do so by extracting high-level features about\nthe target sequence and learning the conditional distribution of sequences\ngiven the corresponding high-level description in a sequence-to-sequence\nmodelling setup. We train FIGARO (FIne-grained music Generation via\nAttention-based, RObust control) by applying description-to-sequence modelling\nto symbolic music. By combining learned high level features with domain\nknowledge, which acts as a strong inductive bias, the model achieves\nstate-of-the-art results in controllable symbolic music generation and\ngeneralizes well beyond the training distribution.\n", "link": "http://arxiv.org/abs/2201.10936v4", "date": "2024-02-22", "relevancy": 1.8412, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4732}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4241}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIGARO%3A%20Generating%20Symbolic%20Music%20with%20Fine-Grained%20Artistic%20Control&entry.906535625=Dimitri%20von%20R%C3%BCtte%20and%20Luca%20Biggio%20and%20Yannic%20Kilcher%20and%20Thomas%20Hofmann&entry.1292438233=%20%20Generating%20music%20with%20deep%20neural%20networks%20has%20been%20an%20area%20of%20active%0Aresearch%20in%20recent%20years.%20While%20the%20quality%20of%20generated%20samples%20has%20been%0Asteadily%20increasing%2C%20most%20methods%20are%20only%20able%20to%20exert%20minimal%20control%20over%0Athe%20generated%20sequence%2C%20if%20any.%20We%20propose%20the%20self-supervised%0Adescription-to-sequence%20task%2C%20which%20allows%20for%20fine-grained%20controllable%0Ageneration%20on%20a%20global%20level.%20We%20do%20so%20by%20extracting%20high-level%20features%20about%0Athe%20target%20sequence%20and%20learning%20the%20conditional%20distribution%20of%20sequences%0Agiven%20the%20corresponding%20high-level%20description%20in%20a%20sequence-to-sequence%0Amodelling%20setup.%20We%20train%20FIGARO%20%28FIne-grained%20music%20Generation%20via%0AAttention-based%2C%20RObust%20control%29%20by%20applying%20description-to-sequence%20modelling%0Ato%20symbolic%20music.%20By%20combining%20learned%20high%20level%20features%20with%20domain%0Aknowledge%2C%20which%20acts%20as%20a%20strong%20inductive%20bias%2C%20the%20model%20achieves%0Astate-of-the-art%20results%20in%20controllable%20symbolic%20music%20generation%20and%0Ageneralizes%20well%20beyond%20the%20training%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.10936v4&entry.124074799=Read"},
{"title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning", "author": "Chen Jia", "abstract": "  Preference learning (PL) with large language models (LLMs) aims to align the\nLLMs' generations with human preferences. Previous work on reinforcement\nlearning from human feedback (RLHF) has demonstrated promising results in\nin-distribution PL. However, due to the difficulty of obtaining human feedback,\ndiscretely training reward models for every encountered distribution is\nchallenging. Thus, out-of-distribution (OOD) PL is practically useful for\nenhancing the generalization ability of LLMs with limited preference feedback.\nThis work addresses OOD PL by optimizing a general reward model through a\nmeta-learning approach. During meta-training, a bilevel optimization algorithm\nis utilized to learn a reward model capable of guiding policy learning to align\nwith human preferences across various distributions. When encountering a test\ndistribution, the meta-test procedure conducts regularized policy optimization\nusing the learned reward model for PL. We theoretically demonstrate the\nconvergence rate of the bilevel optimization algorithm under reasonable\nassumptions. Additionally, we conduct experiments on two text generation tasks\nacross 20 held-out domains and outperform a variety of strong baselines across\nvarious evaluation metrics.\n", "link": "http://arxiv.org/abs/2402.14760v1", "date": "2024-02-22", "relevancy": 1.841, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4639}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4599}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4591}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Reward%20Modeling%20for%20Out-of-Distribution%20Preference%20Learning&entry.906535625=Chen%20Jia&entry.1292438233=%20%20Preference%20learning%20%28PL%29%20with%20large%20language%20models%20%28LLMs%29%20aims%20to%20align%20the%0ALLMs%27%20generations%20with%20human%20preferences.%20Previous%20work%20on%20reinforcement%0Alearning%20from%20human%20feedback%20%28RLHF%29%20has%20demonstrated%20promising%20results%20in%0Ain-distribution%20PL.%20However%2C%20due%20to%20the%20difficulty%20of%20obtaining%20human%20feedback%2C%0Adiscretely%20training%20reward%20models%20for%20every%20encountered%20distribution%20is%0Achallenging.%20Thus%2C%20out-of-distribution%20%28OOD%29%20PL%20is%20practically%20useful%20for%0Aenhancing%20the%20generalization%20ability%20of%20LLMs%20with%20limited%20preference%20feedback.%0AThis%20work%20addresses%20OOD%20PL%20by%20optimizing%20a%20general%20reward%20model%20through%20a%0Ameta-learning%20approach.%20During%20meta-training%2C%20a%20bilevel%20optimization%20algorithm%0Ais%20utilized%20to%20learn%20a%20reward%20model%20capable%20of%20guiding%20policy%20learning%20to%20align%0Awith%20human%20preferences%20across%20various%20distributions.%20When%20encountering%20a%20test%0Adistribution%2C%20the%20meta-test%20procedure%20conducts%20regularized%20policy%20optimization%0Ausing%20the%20learned%20reward%20model%20for%20PL.%20We%20theoretically%20demonstrate%20the%0Aconvergence%20rate%20of%20the%20bilevel%20optimization%20algorithm%20under%20reasonable%0Aassumptions.%20Additionally%2C%20we%20conduct%20experiments%20on%20two%20text%20generation%20tasks%0Aacross%2020%20held-out%20domains%20and%20outperform%20a%20variety%20of%20strong%20baselines%20across%0Avarious%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14760v1&entry.124074799=Read"},
{"title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph\n  Neural Networks", "author": "Yifan Duan and Guibin Zhang and Shilong Wang and Xiaojiang Peng and Wang Ziqi and Junyuan Mao and Hao Wu and Xinke Jiang and Kun Wang", "abstract": "  Credit card fraud poses a significant threat to the economy. While Graph\nNeural Network (GNN)-based fraud detection methods perform well, they often\noverlook the causal effect of a node's local structure on predictions. This\npaper introduces a novel method for credit card fraud detection, the\n\\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal\n\\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork\n(CaT-GNN), which leverages causal invariant learning to reveal inherent\ncorrelations within transaction data. By decomposing the problem into discovery\nand intervention phases, CaT-GNN identifies causal nodes within the transaction\ngraph and applies a causal mixup strategy to enhance the model's robustness and\ninterpretability. CaT-GNN consists of two key components: Causal-Inspector and\nCausal-Intervener. The Causal-Inspector utilizes attention weights in the\ntemporal attention mechanism to identify causal and environment nodes without\nintroducing additional parameters. Subsequently, the Causal-Intervener performs\na causal mixup enhancement on environment nodes based on the set of nodes.\nEvaluated on three datasets, including a private financial dataset and two\npublic datasets, CaT-GNN demonstrates superior performance over existing\nstate-of-the-art methods. Our findings highlight the potential of integrating\ncausal reasoning with graph neural networks to improve fraud detection\ncapabilities in financial transactions.\n", "link": "http://arxiv.org/abs/2402.14708v1", "date": "2024-02-22", "relevancy": 1.8378, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4802}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.453}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4235}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaT-GNN%3A%20Enhancing%20Credit%20Card%20Fraud%20Detection%20via%20Causal%20Temporal%20Graph%0A%20%20Neural%20Networks&entry.906535625=Yifan%20Duan%20and%20Guibin%20Zhang%20and%20Shilong%20Wang%20and%20Xiaojiang%20Peng%20and%20Wang%20Ziqi%20and%20Junyuan%20Mao%20and%20Hao%20Wu%20and%20Xinke%20Jiang%20and%20Kun%20Wang&entry.1292438233=%20%20Credit%20card%20fraud%20poses%20a%20significant%20threat%20to%20the%20economy.%20While%20Graph%0ANeural%20Network%20%28GNN%29-based%20fraud%20detection%20methods%20perform%20well%2C%20they%20often%0Aoverlook%20the%20causal%20effect%20of%20a%20node%27s%20local%20structure%20on%20predictions.%20This%0Apaper%20introduces%20a%20novel%20method%20for%20credit%20card%20fraud%20detection%2C%20the%0A%5Ctextbf%7B%5Cunderline%7BCa%7D%7Dusal%20%5Ctextbf%7B%5Cunderline%7BT%7D%7Demporal%0A%5Ctextbf%7B%5Cunderline%7BG%7D%7Draph%20%5Ctextbf%7B%5Cunderline%7BN%7D%7Deural%20%5Ctextbf%7BN%7Detwork%0A%28CaT-GNN%29%2C%20which%20leverages%20causal%20invariant%20learning%20to%20reveal%20inherent%0Acorrelations%20within%20transaction%20data.%20By%20decomposing%20the%20problem%20into%20discovery%0Aand%20intervention%20phases%2C%20CaT-GNN%20identifies%20causal%20nodes%20within%20the%20transaction%0Agraph%20and%20applies%20a%20causal%20mixup%20strategy%20to%20enhance%20the%20model%27s%20robustness%20and%0Ainterpretability.%20CaT-GNN%20consists%20of%20two%20key%20components%3A%20Causal-Inspector%20and%0ACausal-Intervener.%20The%20Causal-Inspector%20utilizes%20attention%20weights%20in%20the%0Atemporal%20attention%20mechanism%20to%20identify%20causal%20and%20environment%20nodes%20without%0Aintroducing%20additional%20parameters.%20Subsequently%2C%20the%20Causal-Intervener%20performs%0Aa%20causal%20mixup%20enhancement%20on%20environment%20nodes%20based%20on%20the%20set%20of%20nodes.%0AEvaluated%20on%20three%20datasets%2C%20including%20a%20private%20financial%20dataset%20and%20two%0Apublic%20datasets%2C%20CaT-GNN%20demonstrates%20superior%20performance%20over%20existing%0Astate-of-the-art%20methods.%20Our%20findings%20highlight%20the%20potential%20of%20integrating%0Acausal%20reasoning%20with%20graph%20neural%20networks%20to%20improve%20fraud%20detection%0Acapabilities%20in%20financial%20transactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14708v1&entry.124074799=Read"},
{"title": "Parallelized Midpoint Randomization for Langevin Monte Carlo", "author": "Lu Yu and Arnak Dalalyan", "abstract": "  We explore the sampling problem within the framework where parallel\nevaluations of the gradient of the log-density are feasible. Our investigation\nfocuses on target distributions characterized by smooth and strongly\nlog-concave densities. We revisit the parallelized randomized midpoint method\nand employ proof techniques recently developed for analyzing its purely\nsequential version. Leveraging these techniques, we derive upper bounds on the\nWasserstein distance between the sampling and target densities. These bounds\nquantify the runtime improvement achieved by utilizing parallel processing\nunits, which can be considerable.\n", "link": "http://arxiv.org/abs/2402.14434v2", "date": "2024-02-23", "relevancy": 1.8294, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4555}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelized%20Midpoint%20Randomization%20for%20Langevin%20Monte%20Carlo&entry.906535625=Lu%20Yu%20and%20Arnak%20Dalalyan&entry.1292438233=%20%20We%20explore%20the%20sampling%20problem%20within%20the%20framework%20where%20parallel%0Aevaluations%20of%20the%20gradient%20of%20the%20log-density%20are%20feasible.%20Our%20investigation%0Afocuses%20on%20target%20distributions%20characterized%20by%20smooth%20and%20strongly%0Alog-concave%20densities.%20We%20revisit%20the%20parallelized%20randomized%20midpoint%20method%0Aand%20employ%20proof%20techniques%20recently%20developed%20for%20analyzing%20its%20purely%0Asequential%20version.%20Leveraging%20these%20techniques%2C%20we%20derive%20upper%20bounds%20on%20the%0AWasserstein%20distance%20between%20the%20sampling%20and%20target%20densities.%20These%20bounds%0Aquantify%20the%20runtime%20improvement%20achieved%20by%20utilizing%20parallel%20processing%0Aunits%2C%20which%20can%20be%20considerable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14434v2&entry.124074799=Read"},
{"title": "Learning and Sustaining Shared Normative Systems via Bayesian Rule\n  Induction in Markov Games", "author": "Ninell Oldenburg and Tan Zhi-Xuan", "abstract": "  A universal feature of human societies is the adoption of systems of rules\nand norms in the service of cooperative ends. How can we build learning agents\nthat do the same, so that they may flexibly cooperate with the human\ninstitutions they are embedded in? We hypothesize that agents can achieve this\nby assuming there exists a shared set of norms that most others comply with\nwhile pursuing their individual desires, even if they do not know the exact\ncontent of those norms. By assuming shared norms, a newly introduced agent can\ninfer the norms of an existing population from observations of compliance and\nviolation. Furthermore, groups of agents can converge to a shared set of norms,\neven if they initially diverge in their beliefs about what the norms are. This\nin turn enables the stability of the normative system: since agents can\nbootstrap common knowledge of the norms, this leads the norms to be widely\nadhered to, enabling new entrants to rapidly learn those norms. We formalize\nthis framework in the context of Markov games and demonstrate its operation in\na multi-agent environment via approximately Bayesian rule induction of\nobligative and prohibitive norms. Using our approach, agents are able to\nrapidly learn and sustain a variety of cooperative institutions, including\nresource management norms and compensation for pro-social labor, promoting\ncollective welfare while still allowing agents to act in their own interests.\n", "link": "http://arxiv.org/abs/2402.13399v2", "date": "2024-02-22", "relevancy": 1.8272, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4921}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4692}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4303}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Sustaining%20Shared%20Normative%20Systems%20via%20Bayesian%20Rule%0A%20%20Induction%20in%20Markov%20Games&entry.906535625=Ninell%20Oldenburg%20and%20Tan%20Zhi-Xuan&entry.1292438233=%20%20A%20universal%20feature%20of%20human%20societies%20is%20the%20adoption%20of%20systems%20of%20rules%0Aand%20norms%20in%20the%20service%20of%20cooperative%20ends.%20How%20can%20we%20build%20learning%20agents%0Athat%20do%20the%20same%2C%20so%20that%20they%20may%20flexibly%20cooperate%20with%20the%20human%0Ainstitutions%20they%20are%20embedded%20in%3F%20We%20hypothesize%20that%20agents%20can%20achieve%20this%0Aby%20assuming%20there%20exists%20a%20shared%20set%20of%20norms%20that%20most%20others%20comply%20with%0Awhile%20pursuing%20their%20individual%20desires%2C%20even%20if%20they%20do%20not%20know%20the%20exact%0Acontent%20of%20those%20norms.%20By%20assuming%20shared%20norms%2C%20a%20newly%20introduced%20agent%20can%0Ainfer%20the%20norms%20of%20an%20existing%20population%20from%20observations%20of%20compliance%20and%0Aviolation.%20Furthermore%2C%20groups%20of%20agents%20can%20converge%20to%20a%20shared%20set%20of%20norms%2C%0Aeven%20if%20they%20initially%20diverge%20in%20their%20beliefs%20about%20what%20the%20norms%20are.%20This%0Ain%20turn%20enables%20the%20stability%20of%20the%20normative%20system%3A%20since%20agents%20can%0Abootstrap%20common%20knowledge%20of%20the%20norms%2C%20this%20leads%20the%20norms%20to%20be%20widely%0Aadhered%20to%2C%20enabling%20new%20entrants%20to%20rapidly%20learn%20those%20norms.%20We%20formalize%0Athis%20framework%20in%20the%20context%20of%20Markov%20games%20and%20demonstrate%20its%20operation%20in%0Aa%20multi-agent%20environment%20via%20approximately%20Bayesian%20rule%20induction%20of%0Aobligative%20and%20prohibitive%20norms.%20Using%20our%20approach%2C%20agents%20are%20able%20to%0Arapidly%20learn%20and%20sustain%20a%20variety%20of%20cooperative%20institutions%2C%20including%0Aresource%20management%20norms%20and%20compensation%20for%20pro-social%20labor%2C%20promoting%0Acollective%20welfare%20while%20still%20allowing%20agents%20to%20act%20in%20their%20own%20interests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13399v2&entry.124074799=Read"},
{"title": "Imbalanced Data Clustering using Equilibrium K-Means", "author": "Yudong He", "abstract": "  Imbalanced data, characterized by an unequal distribution of data points\nacross different clusters, poses a challenge for traditional hard and fuzzy\nclustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and\nfuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium\nK-means (EKM), a novel and simple K-means-type algorithm that alternates\nbetween just two steps, yielding significantly improved clustering results for\nimbalanced data by reducing the tendency of centroids to crowd together in the\ncenter of large clusters. We also present a unifying perspective for HKM, FKM,\nand EKM, showing they are essentially gradient descent algorithms with an\nexplicit relationship to Newton's method. EKM has the same time and space\ncomplexity as FKM but offers a clearer physical meaning for its membership\ndefinition. We illustrate the performance of EKM on two synthetic and ten real\ndatasets, comparing it to various clustering algorithms, including HKM, FKM,\nmaximum-entropy fuzzy clustering, two FKM variations designed for imbalanced\ndata, and the Gaussian mixture model. The results demonstrate that EKM performs\ncompetitively on balanced data while significantly outperforming other\ntechniques on imbalanced data. For high-dimensional data clustering, we\ndemonstrate that a more discriminative representation can be obtained by\nmapping high-dimensional data via deep neural networks into a low-dimensional,\nEKM-friendly space. Deep clustering with EKM improves clustering accuracy by\n35% on an imbalanced dataset derived from MNIST compared to deep clustering\nbased on HKM.\n", "link": "http://arxiv.org/abs/2402.14490v1", "date": "2024-02-22", "relevancy": 1.8269, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4419}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imbalanced%20Data%20Clustering%20using%20Equilibrium%20K-Means&entry.906535625=Yudong%20He&entry.1292438233=%20%20Imbalanced%20data%2C%20characterized%20by%20an%20unequal%20distribution%20of%20data%20points%0Aacross%20different%20clusters%2C%20poses%20a%20challenge%20for%20traditional%20hard%20and%20fuzzy%0Aclustering%20algorithms%2C%20such%20as%20hard%20K-means%20%28HKM%2C%20or%20Lloyd%27s%20algorithm%29%20and%0Afuzzy%20K-means%20%28FKM%2C%20or%20Bezdek%27s%20algorithm%29.%20This%20paper%20introduces%20equilibrium%0AK-means%20%28EKM%29%2C%20a%20novel%20and%20simple%20K-means-type%20algorithm%20that%20alternates%0Abetween%20just%20two%20steps%2C%20yielding%20significantly%20improved%20clustering%20results%20for%0Aimbalanced%20data%20by%20reducing%20the%20tendency%20of%20centroids%20to%20crowd%20together%20in%20the%0Acenter%20of%20large%20clusters.%20We%20also%20present%20a%20unifying%20perspective%20for%20HKM%2C%20FKM%2C%0Aand%20EKM%2C%20showing%20they%20are%20essentially%20gradient%20descent%20algorithms%20with%20an%0Aexplicit%20relationship%20to%20Newton%27s%20method.%20EKM%20has%20the%20same%20time%20and%20space%0Acomplexity%20as%20FKM%20but%20offers%20a%20clearer%20physical%20meaning%20for%20its%20membership%0Adefinition.%20We%20illustrate%20the%20performance%20of%20EKM%20on%20two%20synthetic%20and%20ten%20real%0Adatasets%2C%20comparing%20it%20to%20various%20clustering%20algorithms%2C%20including%20HKM%2C%20FKM%2C%0Amaximum-entropy%20fuzzy%20clustering%2C%20two%20FKM%20variations%20designed%20for%20imbalanced%0Adata%2C%20and%20the%20Gaussian%20mixture%20model.%20The%20results%20demonstrate%20that%20EKM%20performs%0Acompetitively%20on%20balanced%20data%20while%20significantly%20outperforming%20other%0Atechniques%20on%20imbalanced%20data.%20For%20high-dimensional%20data%20clustering%2C%20we%0Ademonstrate%20that%20a%20more%20discriminative%20representation%20can%20be%20obtained%20by%0Amapping%20high-dimensional%20data%20via%20deep%20neural%20networks%20into%20a%20low-dimensional%2C%0AEKM-friendly%20space.%20Deep%20clustering%20with%20EKM%20improves%20clustering%20accuracy%20by%0A35%25%20on%20an%20imbalanced%20dataset%20derived%20from%20MNIST%20compared%20to%20deep%20clustering%0Abased%20on%20HKM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14490v1&entry.124074799=Read"},
{"title": "Big data analytics to classify earthwork-related locations: A Chengdu\n  study", "author": "Lei Yu and Ke Han", "abstract": "  Air pollution has significantly intensified, leading to severe health\nconsequences worldwide. Earthwork-related locations (ERLs) constitute\nsignificant sources of urban dust pollution. The effective management of ERLs\nhas long posed challenges for governmental and environmental agencies,\nprimarily due to their classification under different regulatory authorities,\ninformation barriers, delays in data updating, and a lack of dust suppression\nmeasures for various sources of dust pollution. To address these challenges, we\nclassified urban dust pollution sources using dump truck trajectory, urban\npoint of interest (POI), and land cover data. We compared several prediction\nmodels and investigated the relationship between features and dust pollution\nsources using real data. The results demonstrate that high-accuracy\nclassification can be achieved with a limited number of features. This method\nwas successfully implemented in the system called Alpha MAPS in Chengdu to\nprovide decision support for urban pollution control.\n", "link": "http://arxiv.org/abs/2402.14698v1", "date": "2024-02-22", "relevancy": 1.3198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4371}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Big%20data%20analytics%20to%20classify%20earthwork-related%20locations%3A%20A%20Chengdu%0A%20%20study&entry.906535625=Lei%20Yu%20and%20Ke%20Han&entry.1292438233=%20%20Air%20pollution%20has%20significantly%20intensified%2C%20leading%20to%20severe%20health%0Aconsequences%20worldwide.%20Earthwork-related%20locations%20%28ERLs%29%20constitute%0Asignificant%20sources%20of%20urban%20dust%20pollution.%20The%20effective%20management%20of%20ERLs%0Ahas%20long%20posed%20challenges%20for%20governmental%20and%20environmental%20agencies%2C%0Aprimarily%20due%20to%20their%20classification%20under%20different%20regulatory%20authorities%2C%0Ainformation%20barriers%2C%20delays%20in%20data%20updating%2C%20and%20a%20lack%20of%20dust%20suppression%0Ameasures%20for%20various%20sources%20of%20dust%20pollution.%20To%20address%20these%20challenges%2C%20we%0Aclassified%20urban%20dust%20pollution%20sources%20using%20dump%20truck%20trajectory%2C%20urban%0Apoint%20of%20interest%20%28POI%29%2C%20and%20land%20cover%20data.%20We%20compared%20several%20prediction%0Amodels%20and%20investigated%20the%20relationship%20between%20features%20and%20dust%20pollution%0Asources%20using%20real%20data.%20The%20results%20demonstrate%20that%20high-accuracy%0Aclassification%20can%20be%20achieved%20with%20a%20limited%20number%20of%20features.%20This%20method%0Awas%20successfully%20implemented%20in%20the%20system%20called%20Alpha%20MAPS%20in%20Chengdu%20to%0Aprovide%20decision%20support%20for%20urban%20pollution%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14698v1&entry.124074799=Read"},
{"title": "Federated Learning on Transcriptomic Data: Model Quality and Performance\n  Trade-Offs", "author": "Anika Hannemann and Jan Ewald and Leo Seeger and Erik Buchmann", "abstract": "  Machine learning on large-scale genomic or transcriptomic data is important\nfor many novel health applications. For example, precision medicine tailors\nmedical treatments to patients on the basis of individual biomarkers, cellular\nand molecular states, etc. However, the data required is sensitive, voluminous,\nheterogeneous, and typically distributed across locations where dedicated\nmachine learning hardware is not available. Due to privacy and regulatory\nreasons, it is also problematic to aggregate all data at a trusted third\nparty.Federated learning is a promising solution to this dilemma, because it\nenables decentralized, collaborative machine learning without exchanging raw\ndata. In this paper, we perform comparative experiments with the federated\nlearning frameworks TensorFlow Federated and Flower. Our test case is the\ntraining of disease prognosis and cell type classification models. We train the\nmodels with distributed transcriptomic data, considering both data\nheterogeneity and architectural heterogeneity. We measure model quality,\nrobustness against privacy-enhancing noise, computational performance and\nresource overhead. Each of the federated learning frameworks has different\nstrengths. However, our experiments confirm that both frameworks can readily\nbuild models on transcriptomic data, without transferring personal raw data to\na third party with abundant computational resources.\n", "link": "http://arxiv.org/abs/2402.14527v1", "date": "2024-02-22", "relevancy": 1.7708, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4762}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4415}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4305}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20on%20Transcriptomic%20Data%3A%20Model%20Quality%20and%20Performance%0A%20%20Trade-Offs&entry.906535625=Anika%20Hannemann%20and%20Jan%20Ewald%20and%20Leo%20Seeger%20and%20Erik%20Buchmann&entry.1292438233=%20%20Machine%20learning%20on%20large-scale%20genomic%20or%20transcriptomic%20data%20is%20important%0Afor%20many%20novel%20health%20applications.%20For%20example%2C%20precision%20medicine%20tailors%0Amedical%20treatments%20to%20patients%20on%20the%20basis%20of%20individual%20biomarkers%2C%20cellular%0Aand%20molecular%20states%2C%20etc.%20However%2C%20the%20data%20required%20is%20sensitive%2C%20voluminous%2C%0Aheterogeneous%2C%20and%20typically%20distributed%20across%20locations%20where%20dedicated%0Amachine%20learning%20hardware%20is%20not%20available.%20Due%20to%20privacy%20and%20regulatory%0Areasons%2C%20it%20is%20also%20problematic%20to%20aggregate%20all%20data%20at%20a%20trusted%20third%0Aparty.Federated%20learning%20is%20a%20promising%20solution%20to%20this%20dilemma%2C%20because%20it%0Aenables%20decentralized%2C%20collaborative%20machine%20learning%20without%20exchanging%20raw%0Adata.%20In%20this%20paper%2C%20we%20perform%20comparative%20experiments%20with%20the%20federated%0Alearning%20frameworks%20TensorFlow%20Federated%20and%20Flower.%20Our%20test%20case%20is%20the%0Atraining%20of%20disease%20prognosis%20and%20cell%20type%20classification%20models.%20We%20train%20the%0Amodels%20with%20distributed%20transcriptomic%20data%2C%20considering%20both%20data%0Aheterogeneity%20and%20architectural%20heterogeneity.%20We%20measure%20model%20quality%2C%0Arobustness%20against%20privacy-enhancing%20noise%2C%20computational%20performance%20and%0Aresource%20overhead.%20Each%20of%20the%20federated%20learning%20frameworks%20has%20different%0Astrengths.%20However%2C%20our%20experiments%20confirm%20that%20both%20frameworks%20can%20readily%0Abuild%20models%20on%20transcriptomic%20data%2C%20without%20transferring%20personal%20raw%20data%20to%0Aa%20third%20party%20with%20abundant%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14527v1&entry.124074799=Read"},
{"title": "IEPile: Unearthing Large-Scale Schema-Based Information Extraction\n  Corpus", "author": "Honghao Gui and Hongbin Ye and Lin Yuan and Ningyu Zhang and Mengshu Sun and Lei Liang and Huajun Chen", "abstract": "  Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimental results on LLaMA and Baichuan demonstrate that using IEPile can\nenhance the performance of LLMs for IE, especially the zero-shot\ngeneralization. We open-source the resource and pre-trained models, hoping to\nprovide valuable support to the NLP community.\n", "link": "http://arxiv.org/abs/2402.14710v1", "date": "2024-02-22", "relevancy": 1.6329, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4152}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4126}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3995}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IEPile%3A%20Unearthing%20Large-Scale%20Schema-Based%20Information%20Extraction%0A%20%20Corpus&entry.906535625=Honghao%20Gui%20and%20Hongbin%20Ye%20and%20Lin%20Yuan%20and%20Ningyu%20Zhang%20and%20Mengshu%20Sun%20and%20Lei%20Liang%20and%20Huajun%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20potential%20across%20various%0Adomains%3B%20however%2C%20they%20exhibit%20a%20significant%20performance%20gap%20in%20Information%0AExtraction%20%28IE%29.%20Note%20that%20high-quality%20instruction%20data%20is%20the%20vital%20key%20for%0Aenhancing%20the%20specific%20capabilities%20of%20LLMs%2C%20while%20current%20IE%20datasets%20tend%20to%0Abe%20small%20in%20scale%2C%20fragmented%2C%20and%20lack%20standardized%20schema.%20To%20this%20end%2C%20we%0Aintroduce%20IEPile%2C%20a%20comprehensive%20bilingual%20%28English%20and%20Chinese%29%20IE%0Ainstruction%20corpus%2C%20which%20contains%20approximately%200.32B%20tokens.%20We%20construct%0AIEPile%20by%20collecting%20and%20cleaning%2033%20existing%20IE%20datasets%2C%20and%20introduce%0Aschema-based%20instruction%20generation%20to%20unearth%20a%20large-scale%20corpus.%0AExperimental%20results%20on%20LLaMA%20and%20Baichuan%20demonstrate%20that%20using%20IEPile%20can%0Aenhance%20the%20performance%20of%20LLMs%20for%20IE%2C%20especially%20the%20zero-shot%0Ageneralization.%20We%20open-source%20the%20resource%20and%20pre-trained%20models%2C%20hoping%20to%0Aprovide%20valuable%20support%20to%20the%20NLP%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14710v1&entry.124074799=Read"},
{"title": "Data Science with LLMs and Interpretable Models", "author": "Sebastian Bordt and Ben Lengerich and Harsha Nori and Rich Caruana", "abstract": "  Recent years have seen important advances in the building of interpretable\nmodels, machine learning models that are designed to be easily understood by\nhumans. In this work, we show that large language models (LLMs) are remarkably\ngood at working with interpretable models, too. In particular, we show that\nLLMs can describe, interpret, and debug Generalized Additive Models (GAMs).\nCombining the flexibility of LLMs with the breadth of statistical patterns\naccurately described by GAMs enables dataset summarization, question answering,\nand model critique. LLMs can also improve the interaction between domain\nexperts and interpretable models, and generate hypotheses about the underlying\nphenomenon. We release \\url{https://github.com/interpretml/TalkToEBM} as an\nopen-source LLM-GAM interface.\n", "link": "http://arxiv.org/abs/2402.14474v1", "date": "2024-02-22", "relevancy": 1.3657, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.454}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4441}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Science%20with%20LLMs%20and%20Interpretable%20Models&entry.906535625=Sebastian%20Bordt%20and%20Ben%20Lengerich%20and%20Harsha%20Nori%20and%20Rich%20Caruana&entry.1292438233=%20%20Recent%20years%20have%20seen%20important%20advances%20in%20the%20building%20of%20interpretable%0Amodels%2C%20machine%20learning%20models%20that%20are%20designed%20to%20be%20easily%20understood%20by%0Ahumans.%20In%20this%20work%2C%20we%20show%20that%20large%20language%20models%20%28LLMs%29%20are%20remarkably%0Agood%20at%20working%20with%20interpretable%20models%2C%20too.%20In%20particular%2C%20we%20show%20that%0ALLMs%20can%20describe%2C%20interpret%2C%20and%20debug%20Generalized%20Additive%20Models%20%28GAMs%29.%0ACombining%20the%20flexibility%20of%20LLMs%20with%20the%20breadth%20of%20statistical%20patterns%0Aaccurately%20described%20by%20GAMs%20enables%20dataset%20summarization%2C%20question%20answering%2C%0Aand%20model%20critique.%20LLMs%20can%20also%20improve%20the%20interaction%20between%20domain%0Aexperts%20and%20interpretable%20models%2C%20and%20generate%20hypotheses%20about%20the%20underlying%0Aphenomenon.%20We%20release%20%5Curl%7Bhttps%3A//github.com/interpretml/TalkToEBM%7D%20as%20an%0Aopen-source%20LLM-GAM%20interface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14474v1&entry.124074799=Read"},
{"title": "Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on\n  Mediterranean Sea Water", "author": "Celio Trois and Luciana Didonet Del Fabro and Vladimir A. Baulin", "abstract": "  Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that\nfosters biodiversity, stores carbon, releases oxygen, and provides habitat to\nnumerous sea organisms. Leveraging augmented research, we collected a\ncomprehensive dataset of 174 features compiled from diverse data sources.\nThrough machine learning analysis, we discovered the existence of a robust\ncorrelation between the exact location of P. oceanica and water biogeochemical\nproperties. The model's feature importance, showed that carbon-related\nvariables as net biomass production and downward surface mass flux of carbon\ndioxide have their values altered in the areas with P. oceanica, which in turn\ncan be used for indirect location of P. oceanica meadows. The study provides\nthe evidence of the plant's ability to exert a global impact on the environment\nand underscores the crucial role of this plant in sea ecosystems, emphasizing\nthe need for its conservation and management.\n", "link": "http://arxiv.org/abs/2402.14459v1", "date": "2024-02-22", "relevancy": 1.1115, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3794}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3657}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Reveals%20Large-scale%20Impact%20of%20Posidonia%20Oceanica%20on%0A%20%20Mediterranean%20Sea%20Water&entry.906535625=Celio%20Trois%20and%20Luciana%20Didonet%20Del%20Fabro%20and%20Vladimir%20A.%20Baulin&entry.1292438233=%20%20Posidonia%20oceanica%20is%20a%20protected%20endemic%20seagrass%20of%20Mediterranean%20sea%20that%0Afosters%20biodiversity%2C%20stores%20carbon%2C%20releases%20oxygen%2C%20and%20provides%20habitat%20to%0Anumerous%20sea%20organisms.%20Leveraging%20augmented%20research%2C%20we%20collected%20a%0Acomprehensive%20dataset%20of%20174%20features%20compiled%20from%20diverse%20data%20sources.%0AThrough%20machine%20learning%20analysis%2C%20we%20discovered%20the%20existence%20of%20a%20robust%0Acorrelation%20between%20the%20exact%20location%20of%20P.%20oceanica%20and%20water%20biogeochemical%0Aproperties.%20The%20model%27s%20feature%20importance%2C%20showed%20that%20carbon-related%0Avariables%20as%20net%20biomass%20production%20and%20downward%20surface%20mass%20flux%20of%20carbon%0Adioxide%20have%20their%20values%20altered%20in%20the%20areas%20with%20P.%20oceanica%2C%20which%20in%20turn%0Acan%20be%20used%20for%20indirect%20location%20of%20P.%20oceanica%20meadows.%20The%20study%20provides%0Athe%20evidence%20of%20the%20plant%27s%20ability%20to%20exert%20a%20global%20impact%20on%20the%20environment%0Aand%20underscores%20the%20crucial%20role%20of%20this%20plant%20in%20sea%20ecosystems%2C%20emphasizing%0Athe%20need%20for%20its%20conservation%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14459v1&entry.124074799=Read"},
{"title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues", "author": "Ge Bai and Jie Liu and Xingyuan Bu and Yancheng He and Jiaheng Liu and Zhanhui Zhou and Zhuoran Lin and Wenbo Su and Tiezheng Ge and Bo Zheng and Wanli Ouyang", "abstract": "  The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities.\n", "link": "http://arxiv.org/abs/2402.14762v1", "date": "2024-02-22", "relevancy": 1.2755, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4092}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues&entry.906535625=Ge%20Bai%20and%20Jie%20Liu%20and%20Xingyuan%20Bu%20and%20Yancheng%20He%20and%20Jiaheng%20Liu%20and%20Zhanhui%20Zhou%20and%20Zhuoran%20Lin%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng%20and%20Wanli%20Ouyang&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20drastically%20enhanced%20dialogue%0Asystems.%20However%2C%20comprehensively%20evaluating%20the%20dialogue%20abilities%20of%20LLMs%0Aremains%20a%20challenge.%20Previous%20benchmarks%20have%20primarily%20focused%20on%20single-turn%0Adialogues%20or%20provided%20coarse-grained%20and%20incomplete%20assessments%20of%20multi-turn%0Adialogues%2C%20overlooking%20the%20complexity%20and%20fine-grained%20nuances%20of%20real-life%0Adialogues.%20To%20address%20this%20issue%2C%20we%20introduce%20MT-Bench-101%2C%20specifically%0Adesigned%20to%20evaluate%20the%20fine-grained%20abilities%20of%20LLMs%20in%20multi-turn%0Adialogues.%20By%20conducting%20a%20detailed%20analysis%20of%20real%20multi-turn%20dialogue%20data%2C%0Awe%20construct%20a%20three-tier%20hierarchical%20ability%20taxonomy%20comprising%204208%20turns%0Aacross%201388%20multi-turn%20dialogues%20in%2013%20distinct%20tasks.%20We%20then%20evaluate%2021%0Apopular%20LLMs%20based%20on%20MT-Bench-101%2C%20conducting%20comprehensive%20analyses%20from%20both%0Aability%20and%20task%20perspectives%20and%20observing%20differing%20trends%20in%20LLMs%0Aperformance%20across%20dialogue%20turns%20within%20various%20tasks.%20Further%20analysis%0Aindicates%20that%20neither%20utilizing%20common%20alignment%20techniques%20nor%20chat-specific%0Adesigns%20has%20led%20to%20obvious%20enhancements%20in%20the%20multi-turn%20abilities%20of%20LLMs.%0AExtensive%20case%20studies%20suggest%20that%20our%20designed%20tasks%20accurately%20assess%20the%0Acorresponding%20multi-turn%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14762v1&entry.124074799=Read"},
{"title": "Difference Learning for Air Quality Forecasting Transport Emulation", "author": "Reed River Chen and Christopher Ribaudo and Jennifer Sleeman and Chace Ashcraft and Collin Kofroth and Marisa Hughes and Ivanka Stajner and Kevin Viner and Kai Wang", "abstract": "  Human health is negatively impacted by poor air quality including increased\nrisk for respiratory and cardiovascular disease. Due to a recent increase in\nextreme air quality events, both globally and locally in the United States,\nfiner resolution air quality forecasting guidance is needed to effectively\nadapt to these events. The National Oceanic and Atmospheric Administration\nprovides air quality forecasting guidance for the Continental United States.\nTheir air quality forecasting model is based on a 15 km spatial resolution;\nhowever, the goal is to reach a three km spatial resolution. This is currently\nnot feasible due in part to prohibitive computational requirements for modeling\nthe transport of chemical species. In this work, we describe a deep learning\ntransport emulator that is able to reduce computations while maintaining skill\ncomparable with the existing numerical model. We show how this method maintains\nskill in the presence of extreme air quality events, making it a potential\ncandidate for operational use. We also explore evaluating how well this model\nmaintains the physical properties of the modeled transport for a given set of\nspecies.\n", "link": "http://arxiv.org/abs/2402.14806v1", "date": "2024-02-22", "relevancy": 1.389, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Difference%20Learning%20for%20Air%20Quality%20Forecasting%20Transport%20Emulation&entry.906535625=Reed%20River%20Chen%20and%20Christopher%20Ribaudo%20and%20Jennifer%20Sleeman%20and%20Chace%20Ashcraft%20and%20Collin%20Kofroth%20and%20Marisa%20Hughes%20and%20Ivanka%20Stajner%20and%20Kevin%20Viner%20and%20Kai%20Wang&entry.1292438233=%20%20Human%20health%20is%20negatively%20impacted%20by%20poor%20air%20quality%20including%20increased%0Arisk%20for%20respiratory%20and%20cardiovascular%20disease.%20Due%20to%20a%20recent%20increase%20in%0Aextreme%20air%20quality%20events%2C%20both%20globally%20and%20locally%20in%20the%20United%20States%2C%0Afiner%20resolution%20air%20quality%20forecasting%20guidance%20is%20needed%20to%20effectively%0Aadapt%20to%20these%20events.%20The%20National%20Oceanic%20and%20Atmospheric%20Administration%0Aprovides%20air%20quality%20forecasting%20guidance%20for%20the%20Continental%20United%20States.%0ATheir%20air%20quality%20forecasting%20model%20is%20based%20on%20a%2015%20km%20spatial%20resolution%3B%0Ahowever%2C%20the%20goal%20is%20to%20reach%20a%20three%20km%20spatial%20resolution.%20This%20is%20currently%0Anot%20feasible%20due%20in%20part%20to%20prohibitive%20computational%20requirements%20for%20modeling%0Athe%20transport%20of%20chemical%20species.%20In%20this%20work%2C%20we%20describe%20a%20deep%20learning%0Atransport%20emulator%20that%20is%20able%20to%20reduce%20computations%20while%20maintaining%20skill%0Acomparable%20with%20the%20existing%20numerical%20model.%20We%20show%20how%20this%20method%20maintains%0Askill%20in%20the%20presence%20of%20extreme%20air%20quality%20events%2C%20making%20it%20a%20potential%0Acandidate%20for%20operational%20use.%20We%20also%20explore%20evaluating%20how%20well%20this%20model%0Amaintains%20the%20physical%20properties%20of%20the%20modeled%20transport%20for%20a%20given%20set%20of%0Aspecies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14806v1&entry.124074799=Read"},
{"title": "Exploring the Influence of Driving Context on Lateral Driving Style\n  Preferences: A Simulator-Based Study", "author": "Johann Haselberger and Maximilian B\u00f6hle and Bernhard Schick and Steffen M\u00fcller", "abstract": "  Technological advancements focus on developing comfortable and acceptable\ndriving characteristics in autonomous vehicles. Present driving functions\npredominantly possess predefined parameters, and there is no universally\naccepted driving style for autonomous vehicles. Although driving may be\ntechnically safe, the passenger might still feel insecure due to a mismatch in\ndriving styles between the human and the autonomous system. Incorporating\ndriving style preferences into automated vehicles enhances acceptance, reduces\nuncertainty, and poses the opportunity to expedite their adoption. Despite the\nincreased research focus on driving styles, there remains a need for\ncomprehensive studies investigating how variations in the driving context\nimpact the assessment of automated driving functions. Therefore, this work\nevaluates lateral driving style preferences for autonomous vehicles on rural\nroads, considering different weather and traffic situations. A controlled study\n(N = 32) was conducted with a variety of German participants utilizing a\nhigh-fidelity driving simulator. The subjects experienced four different\ndriving styles, including mimicking of their own driving behavior under two\nweather conditions. A notable preference for the more passive driving style\nbecame evident based on statistical analyses of participants' responses during\nand after the drives. A low curve-cutting gradient, moderate lateral and\nlongitudinal acceleration constraints, and a pronounced reaction to oncoming\ntraffic characterize this style. This study could not confirm the hypothesis\nthat subjects prefer to be driven by mimicking their own driving behavior.\nFurthermore, the study illustrated that weather conditions and oncoming traffic\nsubstantially influence the perceived comfort during autonomous rides.\n", "link": "http://arxiv.org/abs/2402.14432v1", "date": "2024-02-22", "relevancy": 1.7271, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3946}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Influence%20of%20Driving%20Context%20on%20Lateral%20Driving%20Style%0A%20%20Preferences%3A%20A%20Simulator-Based%20Study&entry.906535625=Johann%20Haselberger%20and%20Maximilian%20B%C3%B6hle%20and%20Bernhard%20Schick%20and%20Steffen%20M%C3%BCller&entry.1292438233=%20%20Technological%20advancements%20focus%20on%20developing%20comfortable%20and%20acceptable%0Adriving%20characteristics%20in%20autonomous%20vehicles.%20Present%20driving%20functions%0Apredominantly%20possess%20predefined%20parameters%2C%20and%20there%20is%20no%20universally%0Aaccepted%20driving%20style%20for%20autonomous%20vehicles.%20Although%20driving%20may%20be%0Atechnically%20safe%2C%20the%20passenger%20might%20still%20feel%20insecure%20due%20to%20a%20mismatch%20in%0Adriving%20styles%20between%20the%20human%20and%20the%20autonomous%20system.%20Incorporating%0Adriving%20style%20preferences%20into%20automated%20vehicles%20enhances%20acceptance%2C%20reduces%0Auncertainty%2C%20and%20poses%20the%20opportunity%20to%20expedite%20their%20adoption.%20Despite%20the%0Aincreased%20research%20focus%20on%20driving%20styles%2C%20there%20remains%20a%20need%20for%0Acomprehensive%20studies%20investigating%20how%20variations%20in%20the%20driving%20context%0Aimpact%20the%20assessment%20of%20automated%20driving%20functions.%20Therefore%2C%20this%20work%0Aevaluates%20lateral%20driving%20style%20preferences%20for%20autonomous%20vehicles%20on%20rural%0Aroads%2C%20considering%20different%20weather%20and%20traffic%20situations.%20A%20controlled%20study%0A%28N%20%3D%2032%29%20was%20conducted%20with%20a%20variety%20of%20German%20participants%20utilizing%20a%0Ahigh-fidelity%20driving%20simulator.%20The%20subjects%20experienced%20four%20different%0Adriving%20styles%2C%20including%20mimicking%20of%20their%20own%20driving%20behavior%20under%20two%0Aweather%20conditions.%20A%20notable%20preference%20for%20the%20more%20passive%20driving%20style%0Abecame%20evident%20based%20on%20statistical%20analyses%20of%20participants%27%20responses%20during%0Aand%20after%20the%20drives.%20A%20low%20curve-cutting%20gradient%2C%20moderate%20lateral%20and%0Alongitudinal%20acceleration%20constraints%2C%20and%20a%20pronounced%20reaction%20to%20oncoming%0Atraffic%20characterize%20this%20style.%20This%20study%20could%20not%20confirm%20the%20hypothesis%0Athat%20subjects%20prefer%20to%20be%20driven%20by%20mimicking%20their%20own%20driving%20behavior.%0AFurthermore%2C%20the%20study%20illustrated%20that%20weather%20conditions%20and%20oncoming%20traffic%0Asubstantially%20influence%20the%20perceived%20comfort%20during%20autonomous%20rides.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14432v1&entry.124074799=Read"},
{"title": "NLAS-multi: A Multilingual Corpus of Automatically Generated Natural\n  Language Argumentation Schemes", "author": "Ramon Ruiz-Dolz and Joaquin Taverner and John Lawrence and Chris Reed", "abstract": "  Some of the major limitations identified in the areas of argument mining,\nargument generation, and natural language argument analysis are related to the\ncomplexity of annotating argumentatively rich data, the limited size of these\ncorpora, and the constraints that represent the different languages and domains\nin which these data is annotated. To address these limitations, in this paper\nwe present the following contributions: (i) an effective methodology for the\nautomatic generation of natural language arguments in different topics and\nlanguages, (ii) the largest publicly available corpus of natural language\nargumentation schemes, and (iii) a set of solid baselines and fine-tuned models\nfor the automatic identification of argumentation schemes.\n", "link": "http://arxiv.org/abs/2402.14458v1", "date": "2024-02-22", "relevancy": 1.2517, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4424}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4052}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NLAS-multi%3A%20A%20Multilingual%20Corpus%20of%20Automatically%20Generated%20Natural%0A%20%20Language%20Argumentation%20Schemes&entry.906535625=Ramon%20Ruiz-Dolz%20and%20Joaquin%20Taverner%20and%20John%20Lawrence%20and%20Chris%20Reed&entry.1292438233=%20%20Some%20of%20the%20major%20limitations%20identified%20in%20the%20areas%20of%20argument%20mining%2C%0Aargument%20generation%2C%20and%20natural%20language%20argument%20analysis%20are%20related%20to%20the%0Acomplexity%20of%20annotating%20argumentatively%20rich%20data%2C%20the%20limited%20size%20of%20these%0Acorpora%2C%20and%20the%20constraints%20that%20represent%20the%20different%20languages%20and%20domains%0Ain%20which%20these%20data%20is%20annotated.%20To%20address%20these%20limitations%2C%20in%20this%20paper%0Awe%20present%20the%20following%20contributions%3A%20%28i%29%20an%20effective%20methodology%20for%20the%0Aautomatic%20generation%20of%20natural%20language%20arguments%20in%20different%20topics%20and%0Alanguages%2C%20%28ii%29%20the%20largest%20publicly%20available%20corpus%20of%20natural%20language%0Aargumentation%20schemes%2C%20and%20%28iii%29%20a%20set%20of%20solid%20baselines%20and%20fine-tuned%20models%0Afor%20the%20automatic%20identification%20of%20argumentation%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14458v1&entry.124074799=Read"},
{"title": "Adaptive time series forecasting with markovian variance switching", "author": "Baptiste Ab\u00e9l\u00e8s and Joseph de Vilmarest and Olivier Wintemberger", "abstract": "  Adaptive time series forecasting is essential for prediction under regime\nchanges. Several classical methods assume linear Gaussian state space model\n(LGSSM) with variances constant in time. However, there are many real-world\nprocesses that cannot be captured by such models. We consider a state-space\nmodel with Markov switching variances. Such dynamical systems are usually\nintractable because of their computational complexity increasing exponentially\nwith time; Variational Bayes (VB) techniques have been applied to this problem.\nIn this paper, we propose a new way of estimating variances based on online\nlearning theory; we adapt expert aggregation methods to learn the variances\nover time. We apply the proposed method to synthetic data and to the problem of\nelectricity load forecasting. We show that this method is robust to\nmisspecification and outperforms traditional expert aggregation.\n", "link": "http://arxiv.org/abs/2402.14684v1", "date": "2024-02-22", "relevancy": 1.3427, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4343}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4289}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20time%20series%20forecasting%20with%20markovian%20variance%20switching&entry.906535625=Baptiste%20Ab%C3%A9l%C3%A8s%20and%20Joseph%20de%20Vilmarest%20and%20Olivier%20Wintemberger&entry.1292438233=%20%20Adaptive%20time%20series%20forecasting%20is%20essential%20for%20prediction%20under%20regime%0Achanges.%20Several%20classical%20methods%20assume%20linear%20Gaussian%20state%20space%20model%0A%28LGSSM%29%20with%20variances%20constant%20in%20time.%20However%2C%20there%20are%20many%20real-world%0Aprocesses%20that%20cannot%20be%20captured%20by%20such%20models.%20We%20consider%20a%20state-space%0Amodel%20with%20Markov%20switching%20variances.%20Such%20dynamical%20systems%20are%20usually%0Aintractable%20because%20of%20their%20computational%20complexity%20increasing%20exponentially%0Awith%20time%3B%20Variational%20Bayes%20%28VB%29%20techniques%20have%20been%20applied%20to%20this%20problem.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20way%20of%20estimating%20variances%20based%20on%20online%0Alearning%20theory%3B%20we%20adapt%20expert%20aggregation%20methods%20to%20learn%20the%20variances%0Aover%20time.%20We%20apply%20the%20proposed%20method%20to%20synthetic%20data%20and%20to%20the%20problem%20of%0Aelectricity%20load%20forecasting.%20We%20show%20that%20this%20method%20is%20robust%20to%0Amisspecification%20and%20outperforms%20traditional%20expert%20aggregation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14684v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


