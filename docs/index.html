<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation", "author": "Huizhuo Yuan and Zixiang Chen and Kaixuan Ji and Quanquan Gu", "abstract": "  Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.\n", "link": "http://arxiv.org/abs/2402.10210v1", "date": "2024-02-15", "relevancy": 2.8083, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5523}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5064}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Play%20Fine-Tuning%20of%20Diffusion%20Models%20for%20Text-to-Image%20Generation&entry.906535625=Huizhuo%20Yuan%20and%20Zixiang%20Chen%20and%20Kaixuan%20Ji%20and%20Quanquan%20Gu&entry.1292438233=%20%20Fine-tuning%20Diffusion%20Models%20remains%20an%20underexplored%20frontier%20in%20generative%0Aartificial%20intelligence%20%28GenAI%29%2C%20especially%20when%20compared%20with%20the%20remarkable%0Aprogress%20made%20in%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29.%20While%20cutting-edge%0Adiffusion%20models%20such%20as%20Stable%20Diffusion%20%28SD%29%20and%20SDXL%20rely%20on%20supervised%0Afine-tuning%2C%20their%20performance%20inevitably%20plateaus%20after%20seeing%20a%20certain%0Avolume%20of%20data.%20Recently%2C%20reinforcement%20learning%20%28RL%29%20has%20been%20employed%20to%0Afine-tune%20diffusion%20models%20with%20human%20preference%20data%2C%20but%20it%20requires%20at%20least%0Atwo%20images%20%28%22winner%22%20and%20%22loser%22%20images%29%20for%20each%20text%20prompt.%20In%20this%20paper%2C%0Awe%20introduce%20an%20innovative%20technique%20called%20self-play%20fine-tuning%20for%20diffusion%0Amodels%20%28SPIN-Diffusion%29%2C%20where%20the%20diffusion%20model%20engages%20in%20competition%20with%0Aits%20earlier%20versions%2C%20facilitating%20an%20iterative%20self-improvement%20process.%20Our%0Aapproach%20offers%20an%20alternative%20to%20conventional%20supervised%20fine-tuning%20and%20RL%0Astrategies%2C%20significantly%20improving%20both%20model%20performance%20and%20alignment.%20Our%0Aexperiments%20on%20the%20Pick-a-Pic%20dataset%20reveal%20that%20SPIN-Diffusion%20outperforms%0Athe%20existing%20supervised%20fine-tuning%20method%20in%20aspects%20of%20human%20preference%0Aalignment%20and%20visual%20appeal%20right%20from%20its%20first%20iteration.%20By%20the%20second%0Aiteration%2C%20it%20exceeds%20the%20performance%20of%20RLHF-based%20methods%20across%20all%20metrics%2C%0Aachieving%20these%20results%20with%20less%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10210v1&entry.124074799=Read"},
{"title": "Textual Localization: Decomposing Multi-concept Images for\n  Subject-Driven Text-to-Image Generation", "author": "Junjie Shentu and Matthew Watson and Noura Al Moubayed", "abstract": "  Subject-driven text-to-image diffusion models empower users to tailor the\nmodel to new concepts absent in the pre-training dataset using a few sample\nimages. However, prevalent subject-driven models primarily rely on\nsingle-concept input images, facing challenges in specifying the target concept\nwhen dealing with multi-concept input images. To this end, we introduce a\ntextual localized text-to-image model (Texual Localization) to handle\nmulti-concept input images. During fine-tuning, our method incorporates a novel\ncross-attention guidance to decompose multiple concepts, establishing distinct\nconnections between the visual representation of the target concept and the\nidentifier token in the text prompt. Experimental results reveal that our\nmethod outperforms or performs comparably to the baseline models in terms of\nimage fidelity and image-text alignment on multi-concept input images. In\ncomparison to Custom Diffusion, our method with hard guidance achieves CLIP-I\nscores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85%\nhigher in single-concept and multi-concept generation, respectively. Notably,\nour method generates cross-attention maps consistent with the target concept in\nthe generated images, a capability absent in existing models.\n", "link": "http://arxiv.org/abs/2402.09966v1", "date": "2024-02-15", "relevancy": 2.7097, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5659}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5188}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textual%20Localization%3A%20Decomposing%20Multi-concept%20Images%20for%0A%20%20Subject-Driven%20Text-to-Image%20Generation&entry.906535625=Junjie%20Shentu%20and%20Matthew%20Watson%20and%20Noura%20Al%20Moubayed&entry.1292438233=%20%20Subject-driven%20text-to-image%20diffusion%20models%20empower%20users%20to%20tailor%20the%0Amodel%20to%20new%20concepts%20absent%20in%20the%20pre-training%20dataset%20using%20a%20few%20sample%0Aimages.%20However%2C%20prevalent%20subject-driven%20models%20primarily%20rely%20on%0Asingle-concept%20input%20images%2C%20facing%20challenges%20in%20specifying%20the%20target%20concept%0Awhen%20dealing%20with%20multi-concept%20input%20images.%20To%20this%20end%2C%20we%20introduce%20a%0Atextual%20localized%20text-to-image%20model%20%28Texual%20Localization%29%20to%20handle%0Amulti-concept%20input%20images.%20During%20fine-tuning%2C%20our%20method%20incorporates%20a%20novel%0Across-attention%20guidance%20to%20decompose%20multiple%20concepts%2C%20establishing%20distinct%0Aconnections%20between%20the%20visual%20representation%20of%20the%20target%20concept%20and%20the%0Aidentifier%20token%20in%20the%20text%20prompt.%20Experimental%20results%20reveal%20that%20our%0Amethod%20outperforms%20or%20performs%20comparably%20to%20the%20baseline%20models%20in%20terms%20of%0Aimage%20fidelity%20and%20image-text%20alignment%20on%20multi-concept%20input%20images.%20In%0Acomparison%20to%20Custom%20Diffusion%2C%20our%20method%20with%20hard%20guidance%20achieves%20CLIP-I%0Ascores%20that%20are%207.04%25%2C%208.13%25%20higher%20and%20CLIP-T%20scores%20that%20are%202.22%25%2C%205.85%25%0Ahigher%20in%20single-concept%20and%20multi-concept%20generation%2C%20respectively.%20Notably%2C%0Aour%20method%20generates%20cross-attention%20maps%20consistent%20with%20the%20target%20concept%20in%0Athe%20generated%20images%2C%20a%20capability%20absent%20in%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09966v1&entry.124074799=Read"},
{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "author": "Helbert Paat and Qing Lian and Weilong Yao and Tong Zhang", "abstract": "  Advancements in deep learning-based 3D object detection necessitate the\navailability of large-scale datasets. However, this requirement introduces the\nchallenge of manual annotation, which is often both burdensome and\ntime-consuming. To tackle this issue, the literature has seen the emergence of\nseveral weakly supervised frameworks for 3D object detection which can\nautomatically generate pseudo labels for unlabeled data. Nevertheless, these\ngenerated pseudo labels contain noise and are not as accurate as those labeled\nby humans. In this paper, we present the first approach that addresses the\ninherent ambiguities present in pseudo labels by introducing an Evidential Deep\nLearning (EDL) based uncertainty estimation framework. Specifically, we propose\nMEDL-U, an EDL framework based on MTrans, which not only generates pseudo\nlabels but also quantifies the associated uncertainties. However, applying EDL\nto 3D object detection presents three primary challenges: (1) relatively lower\npseudolabel quality in comparison to other autolabelers; (2) excessively high\nevidential uncertainty estimates; and (3) lack of clear interpretability and\neffective utilization of uncertainties for downstream tasks. We tackle these\nissues through the introduction of an uncertainty-aware IoU-based loss, an\nevidence-aware multi-task loss function, and the implementation of a\npost-processing stage for uncertainty refinement. Our experimental results\ndemonstrate that probabilistic detectors trained using the outputs of MEDL-U\nsurpass deterministic detectors trained using outputs from previous 3D\nannotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U\nachieves state-of-the-art results on the KITTI official test set compared to\nexisting 3D automatic annotators.\n", "link": "http://arxiv.org/abs/2309.09599v3", "date": "2024-02-15", "relevancy": 2.6795, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 1.0}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5899}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDL-U%3A%20Uncertainty-aware%203D%20Automatic%20Annotation%20based%20on%20Evidential%0A%20%20Deep%20Learning&entry.906535625=Helbert%20Paat%20and%20Qing%20Lian%20and%20Weilong%20Yao%20and%20Tong%20Zhang&entry.1292438233=%20%20Advancements%20in%20deep%20learning-based%203D%20object%20detection%20necessitate%20the%0Aavailability%20of%20large-scale%20datasets.%20However%2C%20this%20requirement%20introduces%20the%0Achallenge%20of%20manual%20annotation%2C%20which%20is%20often%20both%20burdensome%20and%0Atime-consuming.%20To%20tackle%20this%20issue%2C%20the%20literature%20has%20seen%20the%20emergence%20of%0Aseveral%20weakly%20supervised%20frameworks%20for%203D%20object%20detection%20which%20can%0Aautomatically%20generate%20pseudo%20labels%20for%20unlabeled%20data.%20Nevertheless%2C%20these%0Agenerated%20pseudo%20labels%20contain%20noise%20and%20are%20not%20as%20accurate%20as%20those%20labeled%0Aby%20humans.%20In%20this%20paper%2C%20we%20present%20the%20first%20approach%20that%20addresses%20the%0Ainherent%20ambiguities%20present%20in%20pseudo%20labels%20by%20introducing%20an%20Evidential%20Deep%0ALearning%20%28EDL%29%20based%20uncertainty%20estimation%20framework.%20Specifically%2C%20we%20propose%0AMEDL-U%2C%20an%20EDL%20framework%20based%20on%20MTrans%2C%20which%20not%20only%20generates%20pseudo%0Alabels%20but%20also%20quantifies%20the%20associated%20uncertainties.%20However%2C%20applying%20EDL%0Ato%203D%20object%20detection%20presents%20three%20primary%20challenges%3A%20%281%29%20relatively%20lower%0Apseudolabel%20quality%20in%20comparison%20to%20other%20autolabelers%3B%20%282%29%20excessively%20high%0Aevidential%20uncertainty%20estimates%3B%20and%20%283%29%20lack%20of%20clear%20interpretability%20and%0Aeffective%20utilization%20of%20uncertainties%20for%20downstream%20tasks.%20We%20tackle%20these%0Aissues%20through%20the%20introduction%20of%20an%20uncertainty-aware%20IoU-based%20loss%2C%20an%0Aevidence-aware%20multi-task%20loss%20function%2C%20and%20the%20implementation%20of%20a%0Apost-processing%20stage%20for%20uncertainty%20refinement.%20Our%20experimental%20results%0Ademonstrate%20that%20probabilistic%20detectors%20trained%20using%20the%20outputs%20of%20MEDL-U%0Asurpass%20deterministic%20detectors%20trained%20using%20outputs%20from%20previous%203D%0Aannotators%20on%20the%20KITTI%20val%20set%20for%20all%20difficulty%20levels.%20Moreover%2C%20MEDL-U%0Aachieves%20state-of-the-art%20results%20on%20the%20KITTI%20official%20test%20set%20compared%20to%0Aexisting%203D%20automatic%20annotators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09599v3&entry.124074799=Read"},
{"title": "InstructBooth: Instruction-following Personalized Text-to-Image\n  Generation", "author": "Daewon Chae and Nokyung Park and Jinkyu Kim and Kimin Lee", "abstract": "  Personalizing text-to-image models using a limited set of images for a\nspecific object has been explored in subject-specific image generation.\nHowever, existing methods often face challenges in aligning with text prompts\ndue to overfitting to the limited training images. In this work, we introduce\nInstructBooth, a novel method designed to enhance image-text alignment in\npersonalized text-to-image models without sacrificing the personalization\nability. Our approach first personalizes text-to-image models with a small\nnumber of subject-specific images using a unique identifier. After\npersonalization, we fine-tune personalized text-to-image models using\nreinforcement learning to maximize a reward that quantifies image-text\nalignment. Additionally, we propose complementary techniques to increase the\nsynergy between these two processes. Our method demonstrates superior\nimage-text alignment compared to existing baselines, while maintaining high\npersonalization ability. In human evaluations, InstructBooth outperforms them\nwhen considering all comprehensive factors. Our project page is at\nhttps://sites.google.com/view/instructbooth.\n", "link": "http://arxiv.org/abs/2312.03011v2", "date": "2024-02-15", "relevancy": 2.6263, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6061}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4894}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4803}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructBooth%3A%20Instruction-following%20Personalized%20Text-to-Image%0A%20%20Generation&entry.906535625=Daewon%20Chae%20and%20Nokyung%20Park%20and%20Jinkyu%20Kim%20and%20Kimin%20Lee&entry.1292438233=%20%20Personalizing%20text-to-image%20models%20using%20a%20limited%20set%20of%20images%20for%20a%0Aspecific%20object%20has%20been%20explored%20in%20subject-specific%20image%20generation.%0AHowever%2C%20existing%20methods%20often%20face%20challenges%20in%20aligning%20with%20text%20prompts%0Adue%20to%20overfitting%20to%20the%20limited%20training%20images.%20In%20this%20work%2C%20we%20introduce%0AInstructBooth%2C%20a%20novel%20method%20designed%20to%20enhance%20image-text%20alignment%20in%0Apersonalized%20text-to-image%20models%20without%20sacrificing%20the%20personalization%0Aability.%20Our%20approach%20first%20personalizes%20text-to-image%20models%20with%20a%20small%0Anumber%20of%20subject-specific%20images%20using%20a%20unique%20identifier.%20After%0Apersonalization%2C%20we%20fine-tune%20personalized%20text-to-image%20models%20using%0Areinforcement%20learning%20to%20maximize%20a%20reward%20that%20quantifies%20image-text%0Aalignment.%20Additionally%2C%20we%20propose%20complementary%20techniques%20to%20increase%20the%0Asynergy%20between%20these%20two%20processes.%20Our%20method%20demonstrates%20superior%0Aimage-text%20alignment%20compared%20to%20existing%20baselines%2C%20while%20maintaining%20high%0Apersonalization%20ability.%20In%20human%20evaluations%2C%20InstructBooth%20outperforms%20them%0Awhen%20considering%20all%20comprehensive%20factors.%20Our%20project%20page%20is%20at%0Ahttps%3A//sites.google.com/view/instructbooth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03011v2&entry.124074799=Read"},
{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "author": "Parker Ewen and Hao Chen and Yuzhen Chen and Anran Li and Anup Bagali and Gitesh Gunjal and Ram Vasudevan", "abstract": "  Robots must be able to understand their surroundings to perform complex tasks\nin challenging environments and many of these complex tasks require estimates\nof physical properties such as friction or weight. Estimating such properties\nusing learning is challenging due to the large amounts of labelled data\nrequired for training and the difficulty of updating these learned models\nonline at run time. To overcome these challenges, this paper introduces a\nnovel, multi-modal approach for representing semantic predictions and physical\nproperty estimates jointly in a probabilistic manner. By using conjugate pairs,\nthe proposed method enables closed-form Bayesian updates given visual and\ntactile measurements without requiring additional training data. The efficacy\nof the proposed algorithm is demonstrated through several hardware experiments.\nIn particular, this paper illustrates that by conditioning semantic\nclassifications on physical properties, the proposed method quantitatively\noutperforms state-of-the-art semantic classification methods that rely on\nvision alone. To further illustrate its utility, the proposed method is used in\nseveral applications including to represent affordance-based properties\nprobabilistically and a challenging terrain traversal task using a legged\nrobot. In the latter task, the proposed method represents the coefficient of\nfriction of the terrain probabilistically, which enables the use of an on-line\nrisk-aware planner that switches the legged robot from a dynamic gait to a\nstatic, stable gait when the expected value of the coefficient of friction\nfalls below a given threshold. Videos of these case studies as well as the\nopen-source C++ and ROS interface can be found at\nhttps://roahmlab.github.io/multimodal_mapping/.\n", "link": "http://arxiv.org/abs/2402.05872v3", "date": "2024-02-15", "relevancy": 2.6262, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 1.0}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.61}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction&entry.906535625=Parker%20Ewen%20and%20Hao%20Chen%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Anup%20Bagali%20and%20Gitesh%20Gunjal%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Robots%20must%20be%20able%20to%20understand%20their%20surroundings%20to%20perform%20complex%20tasks%0Ain%20challenging%20environments%20and%20many%20of%20these%20complex%20tasks%20require%20estimates%0Aof%20physical%20properties%20such%20as%20friction%20or%20weight.%20Estimating%20such%20properties%0Ausing%20learning%20is%20challenging%20due%20to%20the%20large%20amounts%20of%20labelled%20data%0Arequired%20for%20training%20and%20the%20difficulty%20of%20updating%20these%20learned%20models%0Aonline%20at%20run%20time.%20To%20overcome%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%2C%20multi-modal%20approach%20for%20representing%20semantic%20predictions%20and%20physical%0Aproperty%20estimates%20jointly%20in%20a%20probabilistic%20manner.%20By%20using%20conjugate%20pairs%2C%0Athe%20proposed%20method%20enables%20closed-form%20Bayesian%20updates%20given%20visual%20and%0Atactile%20measurements%20without%20requiring%20additional%20training%20data.%20The%20efficacy%0Aof%20the%20proposed%20algorithm%20is%20demonstrated%20through%20several%20hardware%20experiments.%0AIn%20particular%2C%20this%20paper%20illustrates%20that%20by%20conditioning%20semantic%0Aclassifications%20on%20physical%20properties%2C%20the%20proposed%20method%20quantitatively%0Aoutperforms%20state-of-the-art%20semantic%20classification%20methods%20that%20rely%20on%0Avision%20alone.%20To%20further%20illustrate%20its%20utility%2C%20the%20proposed%20method%20is%20used%20in%0Aseveral%20applications%20including%20to%20represent%20affordance-based%20properties%0Aprobabilistically%20and%20a%20challenging%20terrain%20traversal%20task%20using%20a%20legged%0Arobot.%20In%20the%20latter%20task%2C%20the%20proposed%20method%20represents%20the%20coefficient%20of%0Afriction%20of%20the%20terrain%20probabilistically%2C%20which%20enables%20the%20use%20of%20an%20on-line%0Arisk-aware%20planner%20that%20switches%20the%20legged%20robot%20from%20a%20dynamic%20gait%20to%20a%0Astatic%2C%20stable%20gait%20when%20the%20expected%20value%20of%20the%20coefficient%20of%20friction%0Afalls%20below%20a%20given%20threshold.%20Videos%20of%20these%20case%20studies%20as%20well%20as%20the%0Aopen-source%20C%2B%2B%20and%20ROS%20interface%20can%20be%20found%20at%0Ahttps%3A//roahmlab.github.io/multimodal_mapping/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05872v3&entry.124074799=Read"},
{"title": "Grounded Question-Answering in Long Egocentric Videos", "author": "Shangzhe Di and Weidi Xie", "abstract": "  Existing approaches to video understanding, mainly designed for short videos\nfrom a third-person perspective, are limited in their applicability in certain\nfields, such as robotics. In this paper, we delve into open-ended\nquestion-answering (QA) in long, egocentric videos, which allows individuals or\nrobots to inquire about their own past visual experiences. This task presents\nunique challenges, including the complexity of temporally grounding queries\nwithin extensive video content, the high resource demands for precise data\nannotation, and the inherent difficulty of evaluating open-ended answers due to\ntheir ambiguous nature. Our proposed approach tackles these challenges by (i)\nintegrating query grounding and answering within a unified model to reduce\nerror propagation; (ii) employing large language models for efficient and\nscalable data synthesis; and (iii) introducing a close-ended QA task for\nevaluation, to manage answer ambiguity. Extensive experiments demonstrate the\neffectiveness of our method, which also achieves state-of-the-art performance\non the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are available\nat https://github.com/Becomebright/GroundVQA.\n", "link": "http://arxiv.org/abs/2312.06505v3", "date": "2024-02-15", "relevancy": 2.5901, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5071}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5059}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20Question-Answering%20in%20Long%20Egocentric%20Videos&entry.906535625=Shangzhe%20Di%20and%20Weidi%20Xie&entry.1292438233=%20%20Existing%20approaches%20to%20video%20understanding%2C%20mainly%20designed%20for%20short%20videos%0Afrom%20a%20third-person%20perspective%2C%20are%20limited%20in%20their%20applicability%20in%20certain%0Afields%2C%20such%20as%20robotics.%20In%20this%20paper%2C%20we%20delve%20into%20open-ended%0Aquestion-answering%20%28QA%29%20in%20long%2C%20egocentric%20videos%2C%20which%20allows%20individuals%20or%0Arobots%20to%20inquire%20about%20their%20own%20past%20visual%20experiences.%20This%20task%20presents%0Aunique%20challenges%2C%20including%20the%20complexity%20of%20temporally%20grounding%20queries%0Awithin%20extensive%20video%20content%2C%20the%20high%20resource%20demands%20for%20precise%20data%0Aannotation%2C%20and%20the%20inherent%20difficulty%20of%20evaluating%20open-ended%20answers%20due%20to%0Atheir%20ambiguous%20nature.%20Our%20proposed%20approach%20tackles%20these%20challenges%20by%20%28i%29%0Aintegrating%20query%20grounding%20and%20answering%20within%20a%20unified%20model%20to%20reduce%0Aerror%20propagation%3B%20%28ii%29%20employing%20large%20language%20models%20for%20efficient%20and%0Ascalable%20data%20synthesis%3B%20and%20%28iii%29%20introducing%20a%20close-ended%20QA%20task%20for%0Aevaluation%2C%20to%20manage%20answer%20ambiguity.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20which%20also%20achieves%20state-of-the-art%20performance%0Aon%20the%20QAEgo4D%20and%20Ego4D-NLQ%20benchmarks.%20Code%2C%20data%2C%20and%20models%20are%20available%0Aat%20https%3A//github.com/Becomebright/GroundVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06505v3&entry.124074799=Read"},
{"title": "GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via\n  Reinforcement Learning", "author": "Chengcheng Yu and Jiapeng Zhu and Xiang Li", "abstract": "  Graph neural networks (GNNs) have recently demonstrated significant success.\nActive learning for GNNs aims to query the valuable samples from the unlabeled\ndata for annotation to maximize the GNNs' performance at a low cost. However,\nmost existing methods for reinforced active learning in GNNs may lead to a\nhighly imbalanced class distribution, especially in highly skewed class\nscenarios. This further adversely affects the classification performance. To\ntackle this issue, in this paper, we propose a novel reinforced class-balanced\nactive learning framework for GNNs, namely, GraphCBAL. It learns an optimal\npolicy to acquire class-balanced and informative nodes for annotation,\nmaximizing the performance of GNNs trained with selected labeled nodes.\nGraphCBAL designs class-balance-aware states, as well as a reward function that\nachieves trade-off between model performance and class balance. We further\nupgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to\nobtain a more class-balanced labeled set. Extensive experiments on multiple\ndatasets demonstrate the effectiveness of the proposed approaches, achieving\nsuperior performance over state-of-the-art baselines. In particular, our\nmethods can strike the balance between classification results and class\nbalance.\n", "link": "http://arxiv.org/abs/2402.10074v1", "date": "2024-02-15", "relevancy": 2.5887, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5367}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4703}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphCBAL%3A%20Class-Balanced%20Active%20Learning%20for%20Graph%20Neural%20Networks%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Chengcheng%20Yu%20and%20Jiapeng%20Zhu%20and%20Xiang%20Li&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20recently%20demonstrated%20significant%20success.%0AActive%20learning%20for%20GNNs%20aims%20to%20query%20the%20valuable%20samples%20from%20the%20unlabeled%0Adata%20for%20annotation%20to%20maximize%20the%20GNNs%27%20performance%20at%20a%20low%20cost.%20However%2C%0Amost%20existing%20methods%20for%20reinforced%20active%20learning%20in%20GNNs%20may%20lead%20to%20a%0Ahighly%20imbalanced%20class%20distribution%2C%20especially%20in%20highly%20skewed%20class%0Ascenarios.%20This%20further%20adversely%20affects%20the%20classification%20performance.%20To%0Atackle%20this%20issue%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20reinforced%20class-balanced%0Aactive%20learning%20framework%20for%20GNNs%2C%20namely%2C%20GraphCBAL.%20It%20learns%20an%20optimal%0Apolicy%20to%20acquire%20class-balanced%20and%20informative%20nodes%20for%20annotation%2C%0Amaximizing%20the%20performance%20of%20GNNs%20trained%20with%20selected%20labeled%20nodes.%0AGraphCBAL%20designs%20class-balance-aware%20states%2C%20as%20well%20as%20a%20reward%20function%20that%0Aachieves%20trade-off%20between%20model%20performance%20and%20class%20balance.%20We%20further%0Aupgrade%20GraphCBAL%20to%20GraphCBAL%2B%2B%20by%20introducing%20a%20punishment%20mechanism%20to%0Aobtain%20a%20more%20class-balanced%20labeled%20set.%20Extensive%20experiments%20on%20multiple%0Adatasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approaches%2C%20achieving%0Asuperior%20performance%20over%20state-of-the-art%20baselines.%20In%20particular%2C%20our%0Amethods%20can%20strike%20the%20balance%20between%20classification%20results%20and%20class%0Abalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10074v1&entry.124074799=Read"},
{"title": "Classification Diffusion Models", "author": "Shahar Yadin and Noam Elata and Tomer Michaeli", "abstract": "  A prominent family of methods for learning data distributions relies on\ndensity ratio estimation (DRE), where a model is trained to $\\textit{classify}$\nbetween data samples and samples from some reference distribution. These\ntechniques are successful in simple low-dimensional settings but fail to\nachieve good results on complex high-dimensional data, like images. A different\nfamily of methods for learning distributions is that of denoising diffusion\nmodels (DDMs), in which a model is trained to $\\textit{denoise}$ data samples.\nThese approaches achieve state-of-the-art results in image, video, and audio\ngeneration. In this work, we present $\\textit{Classification Diffusion Models}$\n(CDMs), a generative technique that adopts the denoising-based formalism of\nDDMs while making use of a classifier that predicts the amount of noise added\nto a clean signal, similarly to DRE methods. Our approach is based on the\nobservation that an MSE-optimal denoiser for white Gaussian noise can be\nexpressed in terms of the gradient of a cross-entropy-optimal classifier for\npredicting the noise level. As we illustrate, CDM achieves better denoising\nresults compared to DDM, and leads to at least comparable FID in image\ngeneration. CDM is also capable of highly efficient one-step exact likelihood\nestimation, achieving state-of-the-art results among methods that use a single\nstep. Code is available on the project's webpage in\nhttps://shaharYadin.github.io/CDM/ .\n", "link": "http://arxiv.org/abs/2402.10095v1", "date": "2024-02-15", "relevancy": 2.544, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5541}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4706}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Diffusion%20Models&entry.906535625=Shahar%20Yadin%20and%20Noam%20Elata%20and%20Tomer%20Michaeli&entry.1292438233=%20%20A%20prominent%20family%20of%20methods%20for%20learning%20data%20distributions%20relies%20on%0Adensity%20ratio%20estimation%20%28DRE%29%2C%20where%20a%20model%20is%20trained%20to%20%24%5Ctextit%7Bclassify%7D%24%0Abetween%20data%20samples%20and%20samples%20from%20some%20reference%20distribution.%20These%0Atechniques%20are%20successful%20in%20simple%20low-dimensional%20settings%20but%20fail%20to%0Aachieve%20good%20results%20on%20complex%20high-dimensional%20data%2C%20like%20images.%20A%20different%0Afamily%20of%20methods%20for%20learning%20distributions%20is%20that%20of%20denoising%20diffusion%0Amodels%20%28DDMs%29%2C%20in%20which%20a%20model%20is%20trained%20to%20%24%5Ctextit%7Bdenoise%7D%24%20data%20samples.%0AThese%20approaches%20achieve%20state-of-the-art%20results%20in%20image%2C%20video%2C%20and%20audio%0Ageneration.%20In%20this%20work%2C%20we%20present%20%24%5Ctextit%7BClassification%20Diffusion%20Models%7D%24%0A%28CDMs%29%2C%20a%20generative%20technique%20that%20adopts%20the%20denoising-based%20formalism%20of%0ADDMs%20while%20making%20use%20of%20a%20classifier%20that%20predicts%20the%20amount%20of%20noise%20added%0Ato%20a%20clean%20signal%2C%20similarly%20to%20DRE%20methods.%20Our%20approach%20is%20based%20on%20the%0Aobservation%20that%20an%20MSE-optimal%20denoiser%20for%20white%20Gaussian%20noise%20can%20be%0Aexpressed%20in%20terms%20of%20the%20gradient%20of%20a%20cross-entropy-optimal%20classifier%20for%0Apredicting%20the%20noise%20level.%20As%20we%20illustrate%2C%20CDM%20achieves%20better%20denoising%0Aresults%20compared%20to%20DDM%2C%20and%20leads%20to%20at%20least%20comparable%20FID%20in%20image%0Ageneration.%20CDM%20is%20also%20capable%20of%20highly%20efficient%20one-step%20exact%20likelihood%0Aestimation%2C%20achieving%20state-of-the-art%20results%20among%20methods%20that%20use%20a%20single%0Astep.%20Code%20is%20available%20on%20the%20project%27s%20webpage%20in%0Ahttps%3A//shaharYadin.github.io/CDM/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10095v1&entry.124074799=Read"},
{"title": "Moderating Model Marketplaces: Platform Governance Puzzles for AI\n  Intermediaries", "author": "Robert Gorwa and Michael Veale", "abstract": "  The AI development community is increasingly making use of hosting\nintermediaries such as Hugging Face provide easy access to user-uploaded models\nand training data. These model marketplaces lower technical deployment barriers\nfor hundreds of thousands of users, yet can be used in numerous potentially\nharmful and illegal ways. In this article, we explain ways in which AI systems,\nwhich can both `contain' content and be open-ended tools, present one of the\ntrickiest platform governance challenges seen to date. We provide case studies\nof several incidents across three illustrative platforms -- Hugging Face,\nGitHub and Civitai -- to examine how model marketplaces moderate models.\nBuilding on this analysis, we outline important (and yet nevertheless limited)\npractices that industry has been developing to respond to moderation demands:\nlicensing, access and use restrictions, automated content moderation, and open\npolicy development. While the policy challenge at hand is a considerable one,\nwe conclude with some ideas as to how platforms could better mobilize resources\nto act as a careful, fair, and proportionate regulatory access point.\n", "link": "http://arxiv.org/abs/2311.12573v2", "date": "2024-02-15", "relevancy": 1.2607, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4412}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4144}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.414}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moderating%20Model%20Marketplaces%3A%20Platform%20Governance%20Puzzles%20for%20AI%0A%20%20Intermediaries&entry.906535625=Robert%20Gorwa%20and%20Michael%20Veale&entry.1292438233=%20%20The%20AI%20development%20community%20is%20increasingly%20making%20use%20of%20hosting%0Aintermediaries%20such%20as%20Hugging%20Face%20provide%20easy%20access%20to%20user-uploaded%20models%0Aand%20training%20data.%20These%20model%20marketplaces%20lower%20technical%20deployment%20barriers%0Afor%20hundreds%20of%20thousands%20of%20users%2C%20yet%20can%20be%20used%20in%20numerous%20potentially%0Aharmful%20and%20illegal%20ways.%20In%20this%20article%2C%20we%20explain%20ways%20in%20which%20AI%20systems%2C%0Awhich%20can%20both%20%60contain%27%20content%20and%20be%20open-ended%20tools%2C%20present%20one%20of%20the%0Atrickiest%20platform%20governance%20challenges%20seen%20to%20date.%20We%20provide%20case%20studies%0Aof%20several%20incidents%20across%20three%20illustrative%20platforms%20--%20Hugging%20Face%2C%0AGitHub%20and%20Civitai%20--%20to%20examine%20how%20model%20marketplaces%20moderate%20models.%0ABuilding%20on%20this%20analysis%2C%20we%20outline%20important%20%28and%20yet%20nevertheless%20limited%29%0Apractices%20that%20industry%20has%20been%20developing%20to%20respond%20to%20moderation%20demands%3A%0Alicensing%2C%20access%20and%20use%20restrictions%2C%20automated%20content%20moderation%2C%20and%20open%0Apolicy%20development.%20While%20the%20policy%20challenge%20at%20hand%20is%20a%20considerable%20one%2C%0Awe%20conclude%20with%20some%20ideas%20as%20to%20how%20platforms%20could%20better%20mobilize%20resources%0Ato%20act%20as%20a%20careful%2C%20fair%2C%20and%20proportionate%20regulatory%20access%20point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12573v2&entry.124074799=Read"},
{"title": "Decision Theoretic Foundations for Experiments Evaluating Human\n  Decisions", "author": "Jessica Hullman and Alex Kale and Jason Hartline", "abstract": "  Decision-making with information displays is a key focus of research in areas\nlike explainable AI, human-AI teaming, and data visualization. However, what\nconstitutes a decision problem, and what is required for an experiment to be\ncapable of concluding that human decisions are flawed in some way, remain open\nto speculation. We present a widely applicable definition of a decision problem\nsynthesized from statistical decision theory and information economics. We\nargue that to attribute loss in human performance to forms of bias, an\nexperiment must provide participants with the information that a rational agent\nwould need to identify the normative decision. We evaluate the extent to which\nrecent evaluations of decision-making from the literature on AI-assisted\ndecisions achieve this criteria. We find that only 10 (26\\%) of 39 studies that\nclaim to identify biased behavior present participants with sufficient\ninformation to characterize their behavior as deviating from good\ndecision-making in at least one treatment condition. We motivate the value of\nstudying well-defined decision problems by describing a characterization of\nperformance losses they allow us to conceive. In contrast, the ambiguities of a\npoorly communicated decision problem preclude normative interpretation. We\nconclude with recommendations for practice.\n", "link": "http://arxiv.org/abs/2401.15106v2", "date": "2024-02-15", "relevancy": 1.2899, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4067}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3964}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision%20Theoretic%20Foundations%20for%20Experiments%20Evaluating%20Human%0A%20%20Decisions&entry.906535625=Jessica%20Hullman%20and%20Alex%20Kale%20and%20Jason%20Hartline&entry.1292438233=%20%20Decision-making%20with%20information%20displays%20is%20a%20key%20focus%20of%20research%20in%20areas%0Alike%20explainable%20AI%2C%20human-AI%20teaming%2C%20and%20data%20visualization.%20However%2C%20what%0Aconstitutes%20a%20decision%20problem%2C%20and%20what%20is%20required%20for%20an%20experiment%20to%20be%0Acapable%20of%20concluding%20that%20human%20decisions%20are%20flawed%20in%20some%20way%2C%20remain%20open%0Ato%20speculation.%20We%20present%20a%20widely%20applicable%20definition%20of%20a%20decision%20problem%0Asynthesized%20from%20statistical%20decision%20theory%20and%20information%20economics.%20We%0Aargue%20that%20to%20attribute%20loss%20in%20human%20performance%20to%20forms%20of%20bias%2C%20an%0Aexperiment%20must%20provide%20participants%20with%20the%20information%20that%20a%20rational%20agent%0Awould%20need%20to%20identify%20the%20normative%20decision.%20We%20evaluate%20the%20extent%20to%20which%0Arecent%20evaluations%20of%20decision-making%20from%20the%20literature%20on%20AI-assisted%0Adecisions%20achieve%20this%20criteria.%20We%20find%20that%20only%2010%20%2826%5C%25%29%20of%2039%20studies%20that%0Aclaim%20to%20identify%20biased%20behavior%20present%20participants%20with%20sufficient%0Ainformation%20to%20characterize%20their%20behavior%20as%20deviating%20from%20good%0Adecision-making%20in%20at%20least%20one%20treatment%20condition.%20We%20motivate%20the%20value%20of%0Astudying%20well-defined%20decision%20problems%20by%20describing%20a%20characterization%20of%0Aperformance%20losses%20they%20allow%20us%20to%20conceive.%20In%20contrast%2C%20the%20ambiguities%20of%20a%0Apoorly%20communicated%20decision%20problem%20preclude%20normative%20interpretation.%20We%0Aconclude%20with%20recommendations%20for%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15106v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


