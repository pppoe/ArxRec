<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames\n  in Autonomous Driving Environments", "author": "Xiuzhong Hu and Guangming Xiong and Zheng Zang and Peng Jia and Yuxuan Han and Junyi Ma", "abstract": "  Large-scale 3D scene reconstruction and novel view synthesis are vital for\nautonomous vehicles, especially utilizing temporally sparse LiDAR frames.\nHowever, conventional explicit representations remain a significant bottleneck\ntowards representing the reconstructed and synthetic scenes at unlimited\nresolution. Although the recently developed neural radiance fields (NeRF) have\nshown compelling results in implicit representations, the problem of\nlarge-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR\nframes remains unexplored. To bridge this gap, we propose a 3D scene\nreconstruction and novel view synthesis framework called parent-child neural\nradiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,\nthe framework implements hierarchical spatial partitioning and multi-level\nscene representation, including scene, segment, and point levels. The\nmulti-level scene representation enhances the efficient utilization of sparse\nLiDAR point cloud data and enables the rapid acquisition of an approximate\nvolumetric scene representation. With extensive experiments, PC-NeRF is proven\nto achieve high-precision novel LiDAR view synthesis and 3D reconstruction in\nlarge-scale scenes. Moreover, PC-NeRF can effectively handle situations with\nsparse LiDAR frames and demonstrate high deployment efficiency with limited\ntraining epochs. Our approach implementation and the pre-trained models are\navailable at https://github.com/biter0088/pc-nerf.\n", "link": "http://arxiv.org/abs/2402.09325v1", "date": "2024-02-14", "relevancy": 1.1135, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5614}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5499}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PC-NeRF%3A%20Parent-Child%20Neural%20Radiance%20Fields%20Using%20Sparse%20LiDAR%20Frames%0A%20%20in%20Autonomous%20Driving%20Environments&entry.906535625=Xiuzhong%20Hu%20and%20Guangming%20Xiong%20and%20Zheng%20Zang%20and%20Peng%20Jia%20and%20Yuxuan%20Han%20and%20Junyi%20Ma&entry.1292438233=%20%20Large-scale%203D%20scene%20reconstruction%20and%20novel%20view%20synthesis%20are%20vital%20for%0Aautonomous%20vehicles%2C%20especially%20utilizing%20temporally%20sparse%20LiDAR%20frames.%0AHowever%2C%20conventional%20explicit%20representations%20remain%20a%20significant%20bottleneck%0Atowards%20representing%20the%20reconstructed%20and%20synthetic%20scenes%20at%20unlimited%0Aresolution.%20Although%20the%20recently%20developed%20neural%20radiance%20fields%20%28NeRF%29%20have%0Ashown%20compelling%20results%20in%20implicit%20representations%2C%20the%20problem%20of%0Alarge-scale%203D%20scene%20reconstruction%20and%20novel%20view%20synthesis%20using%20sparse%20LiDAR%0Aframes%20remains%20unexplored.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%203D%20scene%0Areconstruction%20and%20novel%20view%20synthesis%20framework%20called%20parent-child%20neural%0Aradiance%20field%20%28PC-NeRF%29.%20Based%20on%20its%20two%20modules%2C%20parent%20NeRF%20and%20child%20NeRF%2C%0Athe%20framework%20implements%20hierarchical%20spatial%20partitioning%20and%20multi-level%0Ascene%20representation%2C%20including%20scene%2C%20segment%2C%20and%20point%20levels.%20The%0Amulti-level%20scene%20representation%20enhances%20the%20efficient%20utilization%20of%20sparse%0ALiDAR%20point%20cloud%20data%20and%20enables%20the%20rapid%20acquisition%20of%20an%20approximate%0Avolumetric%20scene%20representation.%20With%20extensive%20experiments%2C%20PC-NeRF%20is%20proven%0Ato%20achieve%20high-precision%20novel%20LiDAR%20view%20synthesis%20and%203D%20reconstruction%20in%0Alarge-scale%20scenes.%20Moreover%2C%20PC-NeRF%20can%20effectively%20handle%20situations%20with%0Asparse%20LiDAR%20frames%20and%20demonstrate%20high%20deployment%20efficiency%20with%20limited%0Atraining%20epochs.%20Our%20approach%20implementation%20and%20the%20pre-trained%20models%20are%0Aavailable%20at%20https%3A//github.com/biter0088/pc-nerf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09325v1&entry.124074799=Read"},
{"title": "Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D\n  Ultrasound Localization Microscopy", "author": "Brice Rauby and Paul Xing and Jonathan Por\u00e9e and Maxime Gasse and Jean Provost", "abstract": "  Ultrasound Localization Microscopy (ULM) is a non-invasive technique that\nallows for the imaging of micro-vessels in vivo, at depth and with a resolution\non the order of ten microns. ULM is based on the sub-resolution localization of\nindividual microbubbles injected in the bloodstream. Mapping the whole\nangioarchitecture requires the accumulation of microbubbles trajectories from\nthousands of frames, typically acquired over a few minutes. ULM acquisition\ntimes can be reduced by increasing the microbubble concentration, but requires\nmore advanced algorithms to detect them individually. Several deep learning\napproaches have been proposed for this task, but they remain limited to 2D\nimaging, in part due to the associated large memory requirements. Herein, we\npropose to use sparse tensor neural networks to reduce memory usage in 2D and\nto improve the scaling of the memory requirement for the extension of deep\nlearning architecture to 3D. We study several approaches to efficiently convert\nultrasound data into a sparse format and study the impact of the associated\nloss of information. When applied in 2D, the sparse formulation reduces the\nmemory requirements by a factor 2 at the cost of a small reduction of\nperformance when compared against dense networks. In 3D, the proposed approach\nreduces memory requirements by two order of magnitude while largely\noutperforming conventional ULM in high concentration settings. We show that\nSparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense\ndeep learning based method in 2D ULM i.e. the use of higher concentration in\nsilico and reduced acquisition time.\n", "link": "http://arxiv.org/abs/2402.09359v1", "date": "2024-02-14", "relevancy": 1.07, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5596}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4746}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Sparse%20Tensor%20Neural%20Networks%20Enables%20Deep%20Learning%20for%203D%0A%20%20Ultrasound%20Localization%20Microscopy&entry.906535625=Brice%20Rauby%20and%20Paul%20Xing%20and%20Jonathan%20Por%C3%A9e%20and%20Maxime%20Gasse%20and%20Jean%20Provost&entry.1292438233=%20%20Ultrasound%20Localization%20Microscopy%20%28ULM%29%20is%20a%20non-invasive%20technique%20that%0Aallows%20for%20the%20imaging%20of%20micro-vessels%20in%20vivo%2C%20at%20depth%20and%20with%20a%20resolution%0Aon%20the%20order%20of%20ten%20microns.%20ULM%20is%20based%20on%20the%20sub-resolution%20localization%20of%0Aindividual%20microbubbles%20injected%20in%20the%20bloodstream.%20Mapping%20the%20whole%0Aangioarchitecture%20requires%20the%20accumulation%20of%20microbubbles%20trajectories%20from%0Athousands%20of%20frames%2C%20typically%20acquired%20over%20a%20few%20minutes.%20ULM%20acquisition%0Atimes%20can%20be%20reduced%20by%20increasing%20the%20microbubble%20concentration%2C%20but%20requires%0Amore%20advanced%20algorithms%20to%20detect%20them%20individually.%20Several%20deep%20learning%0Aapproaches%20have%20been%20proposed%20for%20this%20task%2C%20but%20they%20remain%20limited%20to%202D%0Aimaging%2C%20in%20part%20due%20to%20the%20associated%20large%20memory%20requirements.%20Herein%2C%20we%0Apropose%20to%20use%20sparse%20tensor%20neural%20networks%20to%20reduce%20memory%20usage%20in%202D%20and%0Ato%20improve%20the%20scaling%20of%20the%20memory%20requirement%20for%20the%20extension%20of%20deep%0Alearning%20architecture%20to%203D.%20We%20study%20several%20approaches%20to%20efficiently%20convert%0Aultrasound%20data%20into%20a%20sparse%20format%20and%20study%20the%20impact%20of%20the%20associated%0Aloss%20of%20information.%20When%20applied%20in%202D%2C%20the%20sparse%20formulation%20reduces%20the%0Amemory%20requirements%20by%20a%20factor%202%20at%20the%20cost%20of%20a%20small%20reduction%20of%0Aperformance%20when%20compared%20against%20dense%20networks.%20In%203D%2C%20the%20proposed%20approach%0Areduces%20memory%20requirements%20by%20two%20order%20of%20magnitude%20while%20largely%0Aoutperforming%20conventional%20ULM%20in%20high%20concentration%20settings.%20We%20show%20that%0ASparse%20Tensor%20Neural%20Networks%20in%203D%20ULM%20allow%20for%20the%20same%20benefits%20as%20dense%0Adeep%20learning%20based%20method%20in%202D%20ULM%20i.e.%20the%20use%20of%20higher%20concentration%20in%0Asilico%20and%20reduced%20acquisition%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09359v1&entry.124074799=Read"},
{"title": "Intriguing properties of generative classifiers", "author": "Priyank Jaini and Kevin Clark and Robert Geirhos", "abstract": "  What is the best paradigm to recognize objects -- discriminative inference\n(fast but potentially prone to shortcut learning) or using a generative model\n(slow but potentially more robust)? We build on recent advances in generative\nmodeling that turn text-to-image models into classifiers. This allows us to\nstudy their behavior and to compare them against discriminative models and\nhuman psychophysical data. We report four intriguing emergent properties of\ngenerative classifiers: they show a record-breaking human-like shape bias (99%\nfor Imagen), near human-level out-of-distribution accuracy, state-of-the-art\nalignment with human classification errors, and they understand certain\nperceptual illusions. Our results indicate that while the current dominant\nparadigm for modeling human object recognition is discriminative inference,\nzero-shot generative models approximate human object recognition data\nsurprisingly well.\n", "link": "http://arxiv.org/abs/2309.16779v2", "date": "2024-02-14", "relevancy": 1.0695, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5692}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.548}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4871}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intriguing%20properties%20of%20generative%20classifiers&entry.906535625=Priyank%20Jaini%20and%20Kevin%20Clark%20and%20Robert%20Geirhos&entry.1292438233=%20%20What%20is%20the%20best%20paradigm%20to%20recognize%20objects%20--%20discriminative%20inference%0A%28fast%20but%20potentially%20prone%20to%20shortcut%20learning%29%20or%20using%20a%20generative%20model%0A%28slow%20but%20potentially%20more%20robust%29%3F%20We%20build%20on%20recent%20advances%20in%20generative%0Amodeling%20that%20turn%20text-to-image%20models%20into%20classifiers.%20This%20allows%20us%20to%0Astudy%20their%20behavior%20and%20to%20compare%20them%20against%20discriminative%20models%20and%0Ahuman%20psychophysical%20data.%20We%20report%20four%20intriguing%20emergent%20properties%20of%0Agenerative%20classifiers%3A%20they%20show%20a%20record-breaking%20human-like%20shape%20bias%20%2899%25%0Afor%20Imagen%29%2C%20near%20human-level%20out-of-distribution%20accuracy%2C%20state-of-the-art%0Aalignment%20with%20human%20classification%20errors%2C%20and%20they%20understand%20certain%0Aperceptual%20illusions.%20Our%20results%20indicate%20that%20while%20the%20current%20dominant%0Aparadigm%20for%20modeling%20human%20object%20recognition%20is%20discriminative%20inference%2C%0Azero-shot%20generative%20models%20approximate%20human%20object%20recognition%20data%0Asurprisingly%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16779v2&entry.124074799=Read"},
{"title": "Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic\n  Distance Enhances Open World Object Detection", "author": "Thang Doan and Xin Li and Sima Behpour and Wenbin He and Liang Gou and Liu Ren", "abstract": "  Open World Object Detection (OWOD) is a challenging and realistic task that\nextends beyond the scope of standard Object Detection task. It involves\ndetecting both known and unknown objects while integrating learned knowledge\nfor future tasks. However, the level of \"unknownness\" varies significantly\ndepending on the context. For example, a tree is typically considered part of\nthe background in a self-driving scene, but it may be significant in a\nhousehold context. We argue that this contextual information should already be\nembedded within the known classes. In other words, there should be a semantic\nor latent structure relationship between the known and unknown items to be\ndiscovered. Motivated by this observation, we propose Hyp-OW, a method that\nlearns and models hierarchical representation of known items through a\nSuperClass Regularizer. Leveraging this representation allows us to effectively\ndetect unknown objects using a similarity distance-based relabeling module.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nHyp-OW, achieving improvement in both known and unknown detection (up to 6\npercent). These findings are particularly pronounced in our newly designed\nbenchmark, where a strong hierarchical structure exists between known and\nunknown objects. Our code can be found at\nhttps://github.com/boschresearch/Hyp-OW\n", "link": "http://arxiv.org/abs/2306.14291v3", "date": "2024-02-14", "relevancy": 1.0526, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5404}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5282}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5102}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyp-OW%3A%20Exploiting%20Hierarchical%20Structure%20Learning%20with%20Hyperbolic%0A%20%20Distance%20Enhances%20Open%20World%20Object%20Detection&entry.906535625=Thang%20Doan%20and%20Xin%20Li%20and%20Sima%20Behpour%20and%20Wenbin%20He%20and%20Liang%20Gou%20and%20Liu%20Ren&entry.1292438233=%20%20Open%20World%20Object%20Detection%20%28OWOD%29%20is%20a%20challenging%20and%20realistic%20task%20that%0Aextends%20beyond%20the%20scope%20of%20standard%20Object%20Detection%20task.%20It%20involves%0Adetecting%20both%20known%20and%20unknown%20objects%20while%20integrating%20learned%20knowledge%0Afor%20future%20tasks.%20However%2C%20the%20level%20of%20%22unknownness%22%20varies%20significantly%0Adepending%20on%20the%20context.%20For%20example%2C%20a%20tree%20is%20typically%20considered%20part%20of%0Athe%20background%20in%20a%20self-driving%20scene%2C%20but%20it%20may%20be%20significant%20in%20a%0Ahousehold%20context.%20We%20argue%20that%20this%20contextual%20information%20should%20already%20be%0Aembedded%20within%20the%20known%20classes.%20In%20other%20words%2C%20there%20should%20be%20a%20semantic%0Aor%20latent%20structure%20relationship%20between%20the%20known%20and%20unknown%20items%20to%20be%0Adiscovered.%20Motivated%20by%20this%20observation%2C%20we%20propose%20Hyp-OW%2C%20a%20method%20that%0Alearns%20and%20models%20hierarchical%20representation%20of%20known%20items%20through%20a%0ASuperClass%20Regularizer.%20Leveraging%20this%20representation%20allows%20us%20to%20effectively%0Adetect%20unknown%20objects%20using%20a%20similarity%20distance-based%20relabeling%20module.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%0AHyp-OW%2C%20achieving%20improvement%20in%20both%20known%20and%20unknown%20detection%20%28up%20to%206%0Apercent%29.%20These%20findings%20are%20particularly%20pronounced%20in%20our%20newly%20designed%0Abenchmark%2C%20where%20a%20strong%20hierarchical%20structure%20exists%20between%20known%20and%0Aunknown%20objects.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/boschresearch/Hyp-OW%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14291v3&entry.124074799=Read"},
{"title": "Safe Distributed Control of Multi-Robot Systems with Communication\n  Delays", "author": "Luca Ballotta and Rajat Talak", "abstract": "  Safe operation of multi-robot systems is critical, especially in\ncommunication-degraded environments such as underwater for seabed mapping,\nunderground caves for navigation, and in extraterrestrial missions for assembly\nand construction. We address safety of networked autonomous systems where the\ninformation exchanged between robots incurs communication delays. We formalize\na notion of distributed control barrier function (CBF) for multi-robot systems,\na safety certificate amenable to a distributed implementation, which provides\nformal ground to using graph neural networks to learn safe distributed\ncontrollers. Further, we observe that learning a distributed controller\nignoring delays can severely degrade safety. Our main contribution is a\npredictor-based framework to train a safe distributed controller under\ncommunication delays, where the current state of nearby robots is predicted\nfrom received data and age-of-information. Numerical experiments on multi-robot\ncollision avoidance show that our predictor-based approach can significantly\nimprove the safety of a learned distributed controller under communication\ndelays\n", "link": "http://arxiv.org/abs/2402.09382v1", "date": "2024-02-14", "relevancy": 1.0405, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.574}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5045}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4822}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Distributed%20Control%20of%20Multi-Robot%20Systems%20with%20Communication%0A%20%20Delays&entry.906535625=Luca%20Ballotta%20and%20Rajat%20Talak&entry.1292438233=%20%20Safe%20operation%20of%20multi-robot%20systems%20is%20critical%2C%20especially%20in%0Acommunication-degraded%20environments%20such%20as%20underwater%20for%20seabed%20mapping%2C%0Aunderground%20caves%20for%20navigation%2C%20and%20in%20extraterrestrial%20missions%20for%20assembly%0Aand%20construction.%20We%20address%20safety%20of%20networked%20autonomous%20systems%20where%20the%0Ainformation%20exchanged%20between%20robots%20incurs%20communication%20delays.%20We%20formalize%0Aa%20notion%20of%20distributed%20control%20barrier%20function%20%28CBF%29%20for%20multi-robot%20systems%2C%0Aa%20safety%20certificate%20amenable%20to%20a%20distributed%20implementation%2C%20which%20provides%0Aformal%20ground%20to%20using%20graph%20neural%20networks%20to%20learn%20safe%20distributed%0Acontrollers.%20Further%2C%20we%20observe%20that%20learning%20a%20distributed%20controller%0Aignoring%20delays%20can%20severely%20degrade%20safety.%20Our%20main%20contribution%20is%20a%0Apredictor-based%20framework%20to%20train%20a%20safe%20distributed%20controller%20under%0Acommunication%20delays%2C%20where%20the%20current%20state%20of%20nearby%20robots%20is%20predicted%0Afrom%20received%20data%20and%20age-of-information.%20Numerical%20experiments%20on%20multi-robot%0Acollision%20avoidance%20show%20that%20our%20predictor-based%20approach%20can%20significantly%0Aimprove%20the%20safety%20of%20a%20learned%20distributed%20controller%20under%20communication%0Adelays%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09382v1&entry.124074799=Read"},
{"title": "AdvST: Revisiting Data Augmentations for Single Domain Generalization", "author": "Guangtao Zheng and Mengdi Huai and Aidong Zhang", "abstract": "  Single domain generalization (SDG) aims to train a robust model against\nunknown target domain shifts using data from a single source domain. Data\naugmentation has been proven an effective approach to SDG. However, the utility\nof standard augmentations, such as translate, or invert, has not been fully\nexploited in SDG; practically, these augmentations are used as a part of a data\npreprocessing procedure. Although it is intuitive to use many such\naugmentations to boost the robustness of a model to out-of-distribution domain\nshifts, we lack a principled approach to harvest the benefit brought from\nmultiple these augmentations. Here, we conceptualize standard data\naugmentations with learnable parameters as semantics transformations that can\nmanipulate certain semantics of a sample, such as the geometry or color of an\nimage. Then, we propose Adversarial learning with Semantics Transformations\n(AdvST) that augments the source domain data with semantics transformations and\nlearns a robust model with the augmented data. We theoretically show that AdvST\nessentially optimizes a distributionally robust optimization objective defined\non a set of semantics distributions induced by the parameters of semantics\ntransformations. We demonstrate that AdvST can produce samples that expand the\ncoverage on target domain data. Compared with the state-of-the-art methods,\nAdvST, despite being a simple method, is surprisingly competitive and achieves\nthe best average SDG performance on the Digits, PACS, and DomainNet datasets.\nOur code is available at https://github.com/gtzheng/AdvST.\n", "link": "http://arxiv.org/abs/2312.12720v2", "date": "2024-02-14", "relevancy": 1.0282, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5403}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5361}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4659}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdvST%3A%20Revisiting%20Data%20Augmentations%20for%20Single%20Domain%20Generalization&entry.906535625=Guangtao%20Zheng%20and%20Mengdi%20Huai%20and%20Aidong%20Zhang&entry.1292438233=%20%20Single%20domain%20generalization%20%28SDG%29%20aims%20to%20train%20a%20robust%20model%20against%0Aunknown%20target%20domain%20shifts%20using%20data%20from%20a%20single%20source%20domain.%20Data%0Aaugmentation%20has%20been%20proven%20an%20effective%20approach%20to%20SDG.%20However%2C%20the%20utility%0Aof%20standard%20augmentations%2C%20such%20as%20translate%2C%20or%20invert%2C%20has%20not%20been%20fully%0Aexploited%20in%20SDG%3B%20practically%2C%20these%20augmentations%20are%20used%20as%20a%20part%20of%20a%20data%0Apreprocessing%20procedure.%20Although%20it%20is%20intuitive%20to%20use%20many%20such%0Aaugmentations%20to%20boost%20the%20robustness%20of%20a%20model%20to%20out-of-distribution%20domain%0Ashifts%2C%20we%20lack%20a%20principled%20approach%20to%20harvest%20the%20benefit%20brought%20from%0Amultiple%20these%20augmentations.%20Here%2C%20we%20conceptualize%20standard%20data%0Aaugmentations%20with%20learnable%20parameters%20as%20semantics%20transformations%20that%20can%0Amanipulate%20certain%20semantics%20of%20a%20sample%2C%20such%20as%20the%20geometry%20or%20color%20of%20an%0Aimage.%20Then%2C%20we%20propose%20Adversarial%20learning%20with%20Semantics%20Transformations%0A%28AdvST%29%20that%20augments%20the%20source%20domain%20data%20with%20semantics%20transformations%20and%0Alearns%20a%20robust%20model%20with%20the%20augmented%20data.%20We%20theoretically%20show%20that%20AdvST%0Aessentially%20optimizes%20a%20distributionally%20robust%20optimization%20objective%20defined%0Aon%20a%20set%20of%20semantics%20distributions%20induced%20by%20the%20parameters%20of%20semantics%0Atransformations.%20We%20demonstrate%20that%20AdvST%20can%20produce%20samples%20that%20expand%20the%0Acoverage%20on%20target%20domain%20data.%20Compared%20with%20the%20state-of-the-art%20methods%2C%0AAdvST%2C%20despite%20being%20a%20simple%20method%2C%20is%20surprisingly%20competitive%20and%20achieves%0Athe%20best%20average%20SDG%20performance%20on%20the%20Digits%2C%20PACS%2C%20and%20DomainNet%20datasets.%0AOur%20code%20is%20available%20at%20https%3A//github.com/gtzheng/AdvST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12720v2&entry.124074799=Read"},
{"title": "Bad Students Make Great Teachers: Active Learning Accelerates\n  Large-Scale Visual Understanding", "author": "Talfan Evans and Shreya Pathak and Hamza Merzic and Jonathan Schwarz and Ryutaro Tanno and Olivier J. Henaff", "abstract": "  Power-law scaling indicates that large-scale training with uniform sampling\nis prohibitively slow. Active learning methods aim to increase data efficiency\nby prioritizing learning on the most relevant examples. Despite their appeal,\nthese methods have yet to be widely adopted since no one algorithm has been\nshown to a) generalize across models and tasks b) scale to large datasets and\nc) yield overall FLOP savings when accounting for the overhead of data\nselection. In this work we propose a method which satisfies these three\nproperties, leveraging small, cheap proxy models to estimate \"learnability\"\nscores for datapoints, which are used to prioritize data for the training of\nmuch larger models. As a result, our models require 46% and 51% fewer training\nupdates and up to 25% less total computation to reach the same performance as\nuniformly trained visual classifiers on JFT and multimodal models on ALIGN.\nFinally, we find our data-prioritization scheme to be complementary with recent\ndata-curation and learning objectives, yielding a new state-of-the-art in\nseveral multimodal transfer tasks.\n", "link": "http://arxiv.org/abs/2312.05328v3", "date": "2024-02-14", "relevancy": 1.0243, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5151}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5119}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5095}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bad%20Students%20Make%20Great%20Teachers%3A%20Active%20Learning%20Accelerates%0A%20%20Large-Scale%20Visual%20Understanding&entry.906535625=Talfan%20Evans%20and%20Shreya%20Pathak%20and%20Hamza%20Merzic%20and%20Jonathan%20Schwarz%20and%20Ryutaro%20Tanno%20and%20Olivier%20J.%20Henaff&entry.1292438233=%20%20Power-law%20scaling%20indicates%20that%20large-scale%20training%20with%20uniform%20sampling%0Ais%20prohibitively%20slow.%20Active%20learning%20methods%20aim%20to%20increase%20data%20efficiency%0Aby%20prioritizing%20learning%20on%20the%20most%20relevant%20examples.%20Despite%20their%20appeal%2C%0Athese%20methods%20have%20yet%20to%20be%20widely%20adopted%20since%20no%20one%20algorithm%20has%20been%0Ashown%20to%20a%29%20generalize%20across%20models%20and%20tasks%20b%29%20scale%20to%20large%20datasets%20and%0Ac%29%20yield%20overall%20FLOP%20savings%20when%20accounting%20for%20the%20overhead%20of%20data%0Aselection.%20In%20this%20work%20we%20propose%20a%20method%20which%20satisfies%20these%20three%0Aproperties%2C%20leveraging%20small%2C%20cheap%20proxy%20models%20to%20estimate%20%22learnability%22%0Ascores%20for%20datapoints%2C%20which%20are%20used%20to%20prioritize%20data%20for%20the%20training%20of%0Amuch%20larger%20models.%20As%20a%20result%2C%20our%20models%20require%2046%25%20and%2051%25%20fewer%20training%0Aupdates%20and%20up%20to%2025%25%20less%20total%20computation%20to%20reach%20the%20same%20performance%20as%0Auniformly%20trained%20visual%20classifiers%20on%20JFT%20and%20multimodal%20models%20on%20ALIGN.%0AFinally%2C%20we%20find%20our%20data-prioritization%20scheme%20to%20be%20complementary%20with%20recent%0Adata-curation%20and%20learning%20objectives%2C%20yielding%20a%20new%20state-of-the-art%20in%0Aseveral%20multimodal%20transfer%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05328v3&entry.124074799=Read"},
{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "author": "Ze Ma and Daquan Zhou and Chun-Hsiao Yeh and Xue-She Wang and Xiuyu Li and Huanrui Yang and Zhen Dong and Kurt Keutzer and Jiashi Feng", "abstract": "  Creating content for a specific identity (ID) has shown significant interest\nin the field of generative models. In the field of text-to-image generation\n(T2I), subject-driven content generation has achieved great progress with the\nID in the images controllable. However, extending it to video generation is not\nwell explored. In this work, we propose a simple yet effective subject identity\ncontrollable video generation framework, termed Video Custom Diffusion (VCD).\nWith a specified subject ID defined by a few images, VCD reinforces the\nidentity information extraction and injects frame-wise correlation at the\ninitialization stage for stable video outputs with identity preserved to a\nlarge extent. To achieve this, we propose three novel components that are\nessential for high-quality ID preservation: 1) an ID module trained with the\ncropped identity by prompt-to-segmentation to disentangle the ID information\nand the background noise for more accurate ID token learning; 2) a\ntext-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better\ninter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD\nmodules to deblur the face and upscale the video for higher resolution.\n  Despite its simplicity, we conducted extensive experiments to verify that VCD\nis able to generate stable and high-quality videos with better ID over the\nselected strong baselines. Besides, due to the transferability of the ID\nmodule, VCD is also working well with finetuned text-to-image models available\npublically, further improving its usability. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.\n", "link": "http://arxiv.org/abs/2402.09368v1", "date": "2024-02-14", "relevancy": 1.0108, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5196}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5157}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4809}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magic-Me%3A%20Identity-Specific%20Video%20Customized%20Diffusion&entry.906535625=Ze%20Ma%20and%20Daquan%20Zhou%20and%20Chun-Hsiao%20Yeh%20and%20Xue-She%20Wang%20and%20Xiuyu%20Li%20and%20Huanrui%20Yang%20and%20Zhen%20Dong%20and%20Kurt%20Keutzer%20and%20Jiashi%20Feng&entry.1292438233=%20%20Creating%20content%20for%20a%20specific%20identity%20%28ID%29%20has%20shown%20significant%20interest%0Ain%20the%20field%20of%20generative%20models.%20In%20the%20field%20of%20text-to-image%20generation%0A%28T2I%29%2C%20subject-driven%20content%20generation%20has%20achieved%20great%20progress%20with%20the%0AID%20in%20the%20images%20controllable.%20However%2C%20extending%20it%20to%20video%20generation%20is%20not%0Awell%20explored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%20subject%20identity%0Acontrollable%20video%20generation%20framework%2C%20termed%20Video%20Custom%20Diffusion%20%28VCD%29.%0AWith%20a%20specified%20subject%20ID%20defined%20by%20a%20few%20images%2C%20VCD%20reinforces%20the%0Aidentity%20information%20extraction%20and%20injects%20frame-wise%20correlation%20at%20the%0Ainitialization%20stage%20for%20stable%20video%20outputs%20with%20identity%20preserved%20to%20a%0Alarge%20extent.%20To%20achieve%20this%2C%20we%20propose%20three%20novel%20components%20that%20are%0Aessential%20for%20high-quality%20ID%20preservation%3A%201%29%20an%20ID%20module%20trained%20with%20the%0Acropped%20identity%20by%20prompt-to-segmentation%20to%20disentangle%20the%20ID%20information%0Aand%20the%20background%20noise%20for%20more%20accurate%20ID%20token%20learning%3B%202%29%20a%0Atext-to-video%20%28T2V%29%20VCD%20module%20with%203D%20Gaussian%20Noise%20Prior%20for%20better%0Ainter-frame%20consistency%20and%203%29%20video-to-video%20%28V2V%29%20Face%20VCD%20and%20Tiled%20VCD%0Amodules%20to%20deblur%20the%20face%20and%20upscale%20the%20video%20for%20higher%20resolution.%0A%20%20Despite%20its%20simplicity%2C%20we%20conducted%20extensive%20experiments%20to%20verify%20that%20VCD%0Ais%20able%20to%20generate%20stable%20and%20high-quality%20videos%20with%20better%20ID%20over%20the%0Aselected%20strong%20baselines.%20Besides%2C%20due%20to%20the%20transferability%20of%20the%20ID%0Amodule%2C%20VCD%20is%20also%20working%20well%20with%20finetuned%20text-to-image%20models%20available%0Apublically%2C%20further%20improving%20its%20usability.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Zhen-Dong/Magic-Me.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09368v1&entry.124074799=Read"},
{"title": "Pseudorandom Error-Correcting Codes", "author": "Miranda Christ and Sam Gunn", "abstract": "  We construct pseudorandom error-correcting codes (or simply pseudorandom\ncodes), which are error-correcting codes with the property that any polynomial\nnumber of codewords are pseudorandom to any computationally-bounded adversary.\nEfficient decoding of corrupted codewords is possible with the help of a\ndecoding key.\n  We build pseudorandom codes that are robust to substitution and deletion\nerrors, where pseudorandomness rests on standard cryptographic assumptions.\nSpecifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of\nLPN, or polynomial hardness of LPN and the planted XOR problem at low density.\n  As our primary application of pseudorandom codes, we present an undetectable\nwatermarking scheme for outputs of language models that is robust to cropping\nand a constant rate of random substitutions and deletions. The watermark is\nundetectable in the sense that any number of samples of watermarked text are\ncomputationally indistinguishable from text output by the original model. This\nis the first undetectable watermarking scheme that can tolerate a constant rate\nof errors.\n  Our second application is to steganography, where a secret message is hidden\nin innocent-looking content. We present a constant-rate stateless steganography\nscheme with robustness to a constant rate of substitutions. Ours is the first\nstateless steganography scheme with provable steganographic security and any\nrobustness to errors.\n", "link": "http://arxiv.org/abs/2402.09370v1", "date": "2024-02-14", "relevancy": 0.7165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3797}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3648}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3302}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudorandom%20Error-Correcting%20Codes&entry.906535625=Miranda%20Christ%20and%20Sam%20Gunn&entry.1292438233=%20%20We%20construct%20pseudorandom%20error-correcting%20codes%20%28or%20simply%20pseudorandom%0Acodes%29%2C%20which%20are%20error-correcting%20codes%20with%20the%20property%20that%20any%20polynomial%0Anumber%20of%20codewords%20are%20pseudorandom%20to%20any%20computationally-bounded%20adversary.%0AEfficient%20decoding%20of%20corrupted%20codewords%20is%20possible%20with%20the%20help%20of%20a%0Adecoding%20key.%0A%20%20We%20build%20pseudorandom%20codes%20that%20are%20robust%20to%20substitution%20and%20deletion%0Aerrors%2C%20where%20pseudorandomness%20rests%20on%20standard%20cryptographic%20assumptions.%0ASpecifically%2C%20pseudorandomness%20is%20based%20on%20either%20%242%5E%7BO%28%5Csqrt%7Bn%7D%29%7D%24-hardness%20of%0ALPN%2C%20or%20polynomial%20hardness%20of%20LPN%20and%20the%20planted%20XOR%20problem%20at%20low%20density.%0A%20%20As%20our%20primary%20application%20of%20pseudorandom%20codes%2C%20we%20present%20an%20undetectable%0Awatermarking%20scheme%20for%20outputs%20of%20language%20models%20that%20is%20robust%20to%20cropping%0Aand%20a%20constant%20rate%20of%20random%20substitutions%20and%20deletions.%20The%20watermark%20is%0Aundetectable%20in%20the%20sense%20that%20any%20number%20of%20samples%20of%20watermarked%20text%20are%0Acomputationally%20indistinguishable%20from%20text%20output%20by%20the%20original%20model.%20This%0Ais%20the%20first%20undetectable%20watermarking%20scheme%20that%20can%20tolerate%20a%20constant%20rate%0Aof%20errors.%0A%20%20Our%20second%20application%20is%20to%20steganography%2C%20where%20a%20secret%20message%20is%20hidden%0Ain%20innocent-looking%20content.%20We%20present%20a%20constant-rate%20stateless%20steganography%0Ascheme%20with%20robustness%20to%20a%20constant%20rate%20of%20substitutions.%20Ours%20is%20the%20first%0Astateless%20steganography%20scheme%20with%20provable%20steganographic%20security%20and%20any%0Arobustness%20to%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09370v1&entry.124074799=Read"},
{"title": "Conditional Generative Modeling for High-dimensional Marked Temporal\n  Point Processes", "author": "Zheng Dong and Zekai Fan and Shixiang Zhu", "abstract": "  Point processes offer a versatile framework for sequential event modeling.\nHowever, the computational challenges and constrained representational power of\nthe existing point process models have impeded their potential for wider\napplications. This limitation becomes especially pronounced when dealing with\nevent data that is associated with multi-dimensional or high-dimensional marks\nsuch as texts or images. To address this challenge, this study proposes a novel\nevent-generation framework for modeling point processes with high-dimensional\nmarks. We aim to capture the distribution of events without explicitly\nspecifying the conditional intensity or probability density function. Instead,\nwe use a conditional generator that takes the history of events as input and\ngenerates the high-quality subsequent event that is likely to occur given the\nprior observations. The proposed framework offers a host of benefits, including\nconsiderable representational power to capture intricate dynamics in multi- or\neven high-dimensional event space, as well as exceptional efficiency in\nlearning the model and generating samples. Our numerical results demonstrate\nsuperior performance compared to other state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2305.12569v3", "date": "2024-02-14", "relevancy": 0.9545, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4966}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4825}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4527}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Generative%20Modeling%20for%20High-dimensional%20Marked%20Temporal%0A%20%20Point%20Processes&entry.906535625=Zheng%20Dong%20and%20Zekai%20Fan%20and%20Shixiang%20Zhu&entry.1292438233=%20%20Point%20processes%20offer%20a%20versatile%20framework%20for%20sequential%20event%20modeling.%0AHowever%2C%20the%20computational%20challenges%20and%20constrained%20representational%20power%20of%0Athe%20existing%20point%20process%20models%20have%20impeded%20their%20potential%20for%20wider%0Aapplications.%20This%20limitation%20becomes%20especially%20pronounced%20when%20dealing%20with%0Aevent%20data%20that%20is%20associated%20with%20multi-dimensional%20or%20high-dimensional%20marks%0Asuch%20as%20texts%20or%20images.%20To%20address%20this%20challenge%2C%20this%20study%20proposes%20a%20novel%0Aevent-generation%20framework%20for%20modeling%20point%20processes%20with%20high-dimensional%0Amarks.%20We%20aim%20to%20capture%20the%20distribution%20of%20events%20without%20explicitly%0Aspecifying%20the%20conditional%20intensity%20or%20probability%20density%20function.%20Instead%2C%0Awe%20use%20a%20conditional%20generator%20that%20takes%20the%20history%20of%20events%20as%20input%20and%0Agenerates%20the%20high-quality%20subsequent%20event%20that%20is%20likely%20to%20occur%20given%20the%0Aprior%20observations.%20The%20proposed%20framework%20offers%20a%20host%20of%20benefits%2C%20including%0Aconsiderable%20representational%20power%20to%20capture%20intricate%20dynamics%20in%20multi-%20or%0Aeven%20high-dimensional%20event%20space%2C%20as%20well%20as%20exceptional%20efficiency%20in%0Alearning%20the%20model%20and%20generating%20samples.%20Our%20numerical%20results%20demonstrate%0Asuperior%20performance%20compared%20to%20other%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12569v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


