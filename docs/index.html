<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 1200px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D\n  Point Cloud Understanding", "author": "Hai-Tao Yu and Mofei Song", "abstract": "  In perception, multiple sensory information is integrated to map visual\ninformation from 2D views onto 3D objects, which is beneficial for\nunderstanding in 3D environments. But in terms of a single 2D view rendered\nfrom different angles, only limited partial information can be provided.The\nrichness and value of Multi-view 2D information can provide superior\nself-supervised signals for 3D objects. In this paper, we propose a novel\nself-supervised point cloud representation learning method, MM-Point, which is\ndriven by intra-modal and inter-modal similarity objectives. The core of\nMM-Point lies in the Multi-modal interaction and transmission between 3D\nobjects and multiple 2D views at the same time. In order to more effectively\nsimultaneously perform the consistent cross-modal objective of 2D multi-view\ninformation based on contrastive learning, we further propose Multi-MLP and\nMulti-level Augmentation strategies. Through carefully designed transformation\nstrategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point\ndemonstrates state-of-the-art (SOTA) performance in various downstream tasks.\nFor instance, it achieves a peak accuracy of 92.4% on the synthetic dataset\nModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,\ncomparable to fully supervised methods. Additionally, we demonstrate its\neffectiveness in tasks such as few-shot classification, 3D part segmentation\nand 3D semantic segmentation.\n", "link": "http://arxiv.org/abs/2402.10002v1", "date": "2024-02-15", "relevancy": 1.2266, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.664}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6109}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5649}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Point%3A%20Multi-View%20Information-Enhanced%20Multi-Modal%20Self-Supervised%203D%0A%20%20Point%20Cloud%20Understanding&entry.906535625=Hai-Tao%20Yu%20and%20Mofei%20Song&entry.1292438233=%20%20In%20perception%2C%20multiple%20sensory%20information%20is%20integrated%20to%20map%20visual%0Ainformation%20from%202D%20views%20onto%203D%20objects%2C%20which%20is%20beneficial%20for%0Aunderstanding%20in%203D%20environments.%20But%20in%20terms%20of%20a%20single%202D%20view%20rendered%0Afrom%20different%20angles%2C%20only%20limited%20partial%20information%20can%20be%20provided.The%0Arichness%20and%20value%20of%20Multi-view%202D%20information%20can%20provide%20superior%0Aself-supervised%20signals%20for%203D%20objects.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aself-supervised%20point%20cloud%20representation%20learning%20method%2C%20MM-Point%2C%20which%20is%0Adriven%20by%20intra-modal%20and%20inter-modal%20similarity%20objectives.%20The%20core%20of%0AMM-Point%20lies%20in%20the%20Multi-modal%20interaction%20and%20transmission%20between%203D%0Aobjects%20and%20multiple%202D%20views%20at%20the%20same%20time.%20In%20order%20to%20more%20effectively%0Asimultaneously%20perform%20the%20consistent%20cross-modal%20objective%20of%202D%20multi-view%0Ainformation%20based%20on%20contrastive%20learning%2C%20we%20further%20propose%20Multi-MLP%20and%0AMulti-level%20Augmentation%20strategies.%20Through%20carefully%20designed%20transformation%0Astrategies%2C%20we%20further%20learn%20Multi-level%20invariance%20in%202D%20Multi-views.%20MM-Point%0Ademonstrates%20state-of-the-art%20%28SOTA%29%20performance%20in%20various%20downstream%20tasks.%0AFor%20instance%2C%20it%20achieves%20a%20peak%20accuracy%20of%2092.4%25%20on%20the%20synthetic%20dataset%0AModelNet40%2C%20and%20a%20top%20accuracy%20of%2087.8%25%20on%20the%20real-world%20dataset%20ScanObjectNN%2C%0Acomparable%20to%20fully%20supervised%20methods.%20Additionally%2C%20we%20demonstrate%20its%0Aeffectiveness%20in%20tasks%20such%20as%20few-shot%20classification%2C%203D%20part%20segmentation%0Aand%203D%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10002v1&entry.124074799=Read"},
{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "author": "Helbert Paat and Qing Lian and Weilong Yao and Tong Zhang", "abstract": "  Advancements in deep learning-based 3D object detection necessitate the\navailability of large-scale datasets. However, this requirement introduces the\nchallenge of manual annotation, which is often both burdensome and\ntime-consuming. To tackle this issue, the literature has seen the emergence of\nseveral weakly supervised frameworks for 3D object detection which can\nautomatically generate pseudo labels for unlabeled data. Nevertheless, these\ngenerated pseudo labels contain noise and are not as accurate as those labeled\nby humans. In this paper, we present the first approach that addresses the\ninherent ambiguities present in pseudo labels by introducing an Evidential Deep\nLearning (EDL) based uncertainty estimation framework. Specifically, we propose\nMEDL-U, an EDL framework based on MTrans, which not only generates pseudo\nlabels but also quantifies the associated uncertainties. However, applying EDL\nto 3D object detection presents three primary challenges: (1) relatively lower\npseudolabel quality in comparison to other autolabelers; (2) excessively high\nevidential uncertainty estimates; and (3) lack of clear interpretability and\neffective utilization of uncertainties for downstream tasks. We tackle these\nissues through the introduction of an uncertainty-aware IoU-based loss, an\nevidence-aware multi-task loss function, and the implementation of a\npost-processing stage for uncertainty refinement. Our experimental results\ndemonstrate that probabilistic detectors trained using the outputs of MEDL-U\nsurpass deterministic detectors trained using outputs from previous 3D\nannotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U\nachieves state-of-the-art results on the KITTI official test set compared to\nexisting 3D automatic annotators.\n", "link": "http://arxiv.org/abs/2309.09599v3", "date": "2024-02-15", "relevancy": 1.1945, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5899}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5841}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDL-U%3A%20Uncertainty-aware%203D%20Automatic%20Annotation%20based%20on%20Evidential%0A%20%20Deep%20Learning&entry.906535625=Helbert%20Paat%20and%20Qing%20Lian%20and%20Weilong%20Yao%20and%20Tong%20Zhang&entry.1292438233=%20%20Advancements%20in%20deep%20learning-based%203D%20object%20detection%20necessitate%20the%0Aavailability%20of%20large-scale%20datasets.%20However%2C%20this%20requirement%20introduces%20the%0Achallenge%20of%20manual%20annotation%2C%20which%20is%20often%20both%20burdensome%20and%0Atime-consuming.%20To%20tackle%20this%20issue%2C%20the%20literature%20has%20seen%20the%20emergence%20of%0Aseveral%20weakly%20supervised%20frameworks%20for%203D%20object%20detection%20which%20can%0Aautomatically%20generate%20pseudo%20labels%20for%20unlabeled%20data.%20Nevertheless%2C%20these%0Agenerated%20pseudo%20labels%20contain%20noise%20and%20are%20not%20as%20accurate%20as%20those%20labeled%0Aby%20humans.%20In%20this%20paper%2C%20we%20present%20the%20first%20approach%20that%20addresses%20the%0Ainherent%20ambiguities%20present%20in%20pseudo%20labels%20by%20introducing%20an%20Evidential%20Deep%0ALearning%20%28EDL%29%20based%20uncertainty%20estimation%20framework.%20Specifically%2C%20we%20propose%0AMEDL-U%2C%20an%20EDL%20framework%20based%20on%20MTrans%2C%20which%20not%20only%20generates%20pseudo%0Alabels%20but%20also%20quantifies%20the%20associated%20uncertainties.%20However%2C%20applying%20EDL%0Ato%203D%20object%20detection%20presents%20three%20primary%20challenges%3A%20%281%29%20relatively%20lower%0Apseudolabel%20quality%20in%20comparison%20to%20other%20autolabelers%3B%20%282%29%20excessively%20high%0Aevidential%20uncertainty%20estimates%3B%20and%20%283%29%20lack%20of%20clear%20interpretability%20and%0Aeffective%20utilization%20of%20uncertainties%20for%20downstream%20tasks.%20We%20tackle%20these%0Aissues%20through%20the%20introduction%20of%20an%20uncertainty-aware%20IoU-based%20loss%2C%20an%0Aevidence-aware%20multi-task%20loss%20function%2C%20and%20the%20implementation%20of%20a%0Apost-processing%20stage%20for%20uncertainty%20refinement.%20Our%20experimental%20results%0Ademonstrate%20that%20probabilistic%20detectors%20trained%20using%20the%20outputs%20of%20MEDL-U%0Asurpass%20deterministic%20detectors%20trained%20using%20outputs%20from%20previous%203D%0Aannotators%20on%20the%20KITTI%20val%20set%20for%20all%20difficulty%20levels.%20Moreover%2C%20MEDL-U%0Aachieves%20state-of-the-art%20results%20on%20the%20KITTI%20official%20test%20set%20compared%20to%0Aexisting%203D%20automatic%20annotators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09599v3&entry.124074799=Read"},
{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "author": "Parker Ewen and Hao Chen and Yuzhen Chen and Anran Li and Anup Bagali and Gitesh Gunjal and Ram Vasudevan", "abstract": "  Robots must be able to understand their surroundings to perform complex tasks\nin challenging environments and many of these complex tasks require estimates\nof physical properties such as friction or weight. Estimating such properties\nusing learning is challenging due to the large amounts of labelled data\nrequired for training and the difficulty of updating these learned models\nonline at run time. To overcome these challenges, this paper introduces a\nnovel, multi-modal approach for representing semantic predictions and physical\nproperty estimates jointly in a probabilistic manner. By using conjugate pairs,\nthe proposed method enables closed-form Bayesian updates given visual and\ntactile measurements without requiring additional training data. The efficacy\nof the proposed algorithm is demonstrated through several hardware experiments.\nIn particular, this paper illustrates that by conditioning semantic\nclassifications on physical properties, the proposed method quantitatively\noutperforms state-of-the-art semantic classification methods that rely on\nvision alone. To further illustrate its utility, the proposed method is used in\nseveral applications including to represent affordance-based properties\nprobabilistically and a challenging terrain traversal task using a legged\nrobot. In the latter task, the proposed method represents the coefficient of\nfriction of the terrain probabilistically, which enables the use of an on-line\nrisk-aware planner that switches the legged robot from a dynamic gait to a\nstatic, stable gait when the expected value of the coefficient of friction\nfalls below a given threshold. Videos of these case studies as well as the\nopen-source C++ and ROS interface can be found at\nhttps://roahmlab.github.io/multimodal_mapping/.\n", "link": "http://arxiv.org/abs/2402.05872v3", "date": "2024-02-15", "relevancy": 1.1475, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.61}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5455}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%27ve%20Got%20to%20Feel%20It%20To%20Believe%20It%3A%20Multi-Modal%20Bayesian%20Inference%20for%0A%20%20Semantic%20and%20Property%20Prediction&entry.906535625=Parker%20Ewen%20and%20Hao%20Chen%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Anup%20Bagali%20and%20Gitesh%20Gunjal%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Robots%20must%20be%20able%20to%20understand%20their%20surroundings%20to%20perform%20complex%20tasks%0Ain%20challenging%20environments%20and%20many%20of%20these%20complex%20tasks%20require%20estimates%0Aof%20physical%20properties%20such%20as%20friction%20or%20weight.%20Estimating%20such%20properties%0Ausing%20learning%20is%20challenging%20due%20to%20the%20large%20amounts%20of%20labelled%20data%0Arequired%20for%20training%20and%20the%20difficulty%20of%20updating%20these%20learned%20models%0Aonline%20at%20run%20time.%20To%20overcome%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%2C%20multi-modal%20approach%20for%20representing%20semantic%20predictions%20and%20physical%0Aproperty%20estimates%20jointly%20in%20a%20probabilistic%20manner.%20By%20using%20conjugate%20pairs%2C%0Athe%20proposed%20method%20enables%20closed-form%20Bayesian%20updates%20given%20visual%20and%0Atactile%20measurements%20without%20requiring%20additional%20training%20data.%20The%20efficacy%0Aof%20the%20proposed%20algorithm%20is%20demonstrated%20through%20several%20hardware%20experiments.%0AIn%20particular%2C%20this%20paper%20illustrates%20that%20by%20conditioning%20semantic%0Aclassifications%20on%20physical%20properties%2C%20the%20proposed%20method%20quantitatively%0Aoutperforms%20state-of-the-art%20semantic%20classification%20methods%20that%20rely%20on%0Avision%20alone.%20To%20further%20illustrate%20its%20utility%2C%20the%20proposed%20method%20is%20used%20in%0Aseveral%20applications%20including%20to%20represent%20affordance-based%20properties%0Aprobabilistically%20and%20a%20challenging%20terrain%20traversal%20task%20using%20a%20legged%0Arobot.%20In%20the%20latter%20task%2C%20the%20proposed%20method%20represents%20the%20coefficient%20of%0Afriction%20of%20the%20terrain%20probabilistically%2C%20which%20enables%20the%20use%20of%20an%20on-line%0Arisk-aware%20planner%20that%20switches%20the%20legged%20robot%20from%20a%20dynamic%20gait%20to%20a%0Astatic%2C%20stable%20gait%20when%20the%20expected%20value%20of%20the%20coefficient%20of%20friction%0Afalls%20below%20a%20given%20threshold.%20Videos%20of%20these%20case%20studies%20as%20well%20as%20the%0Aopen-source%20C%2B%2B%20and%20ROS%20interface%20can%20be%20found%20at%0Ahttps%3A//roahmlab.github.io/multimodal_mapping/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05872v3&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Visual Robot Localization Using LED State\n  Prediction as a Pretext Task", "author": "Mirko Nava and Nicholas Carlotti and Luca Crupi and Daniele Palossi and Alessandro Giusti", "abstract": "  We propose a novel self-supervised approach for learning to visually localize\nrobots equipped with controllable LEDs. We rely on a few training samples\nlabeled with position ground truth and many training samples in which only the\nLED state is known, whose collection is cheap. We show that using LED state\nprediction as a pretext task significantly helps to learn the visual\nlocalization end task. The resulting model does not require knowledge of LED\nstates during inference. We instantiate the approach to visual relative\nlocalization of nano-quadrotors: experimental results show that using our\npretext task significantly improves localization accuracy (from 68.3% to 76.2%)\nand outperforms alternative strategies, such as a supervised baseline, model\npre-training, and an autoencoding pretext task. We deploy our model aboard a\n27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a\npeer nano-drone. Our approach, relying on position labels for only 300 images,\nyields a mean tracking error of 4.2 cm versus 11.9 cm of a supervised baseline\nmodel trained without our pretext task. Videos and code of the proposed\napproach are available at https://github.com/idsia-robotics/leds-as-pretext\n", "link": "http://arxiv.org/abs/2402.09886v1", "date": "2024-02-15", "relevancy": 1.1359, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5777}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5669}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5592}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Visual%20Robot%20Localization%20Using%20LED%20State%0A%20%20Prediction%20as%20a%20Pretext%20Task&entry.906535625=Mirko%20Nava%20and%20Nicholas%20Carlotti%20and%20Luca%20Crupi%20and%20Daniele%20Palossi%20and%20Alessandro%20Giusti&entry.1292438233=%20%20We%20propose%20a%20novel%20self-supervised%20approach%20for%20learning%20to%20visually%20localize%0Arobots%20equipped%20with%20controllable%20LEDs.%20We%20rely%20on%20a%20few%20training%20samples%0Alabeled%20with%20position%20ground%20truth%20and%20many%20training%20samples%20in%20which%20only%20the%0ALED%20state%20is%20known%2C%20whose%20collection%20is%20cheap.%20We%20show%20that%20using%20LED%20state%0Aprediction%20as%20a%20pretext%20task%20significantly%20helps%20to%20learn%20the%20visual%0Alocalization%20end%20task.%20The%20resulting%20model%20does%20not%20require%20knowledge%20of%20LED%0Astates%20during%20inference.%20We%20instantiate%20the%20approach%20to%20visual%20relative%0Alocalization%20of%20nano-quadrotors%3A%20experimental%20results%20show%20that%20using%20our%0Apretext%20task%20significantly%20improves%20localization%20accuracy%20%28from%2068.3%25%20to%2076.2%25%29%0Aand%20outperforms%20alternative%20strategies%2C%20such%20as%20a%20supervised%20baseline%2C%20model%0Apre-training%2C%20and%20an%20autoencoding%20pretext%20task.%20We%20deploy%20our%20model%20aboard%20a%0A27-g%20Crazyflie%20nano-drone%2C%20running%20at%2021%20fps%2C%20in%20a%20position-tracking%20task%20of%20a%0Apeer%20nano-drone.%20Our%20approach%2C%20relying%20on%20position%20labels%20for%20only%20300%20images%2C%0Ayields%20a%20mean%20tracking%20error%20of%204.2%20cm%20versus%2011.9%20cm%20of%20a%20supervised%20baseline%0Amodel%20trained%20without%20our%20pretext%20task.%20Videos%20and%20code%20of%20the%20proposed%0Aapproach%20are%20available%20at%20https%3A//github.com/idsia-robotics/leds-as-pretext%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09886v1&entry.124074799=Read"},
{"title": "Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using\n  diffusion models", "author": "Tianyi Ren and Abhishek Sharma and Juampablo Heras Rivera and Harshitha Rebala and Ethan Honey and Agamdeep Chopra and Jacob Ruzevick and Mehmet Kurt", "abstract": "  Identification of tumor margins is essential for surgical decision-making for\nglioblastoma patients and provides reliable assistance for neurosurgeons.\nDespite improvements in deep learning architectures for tumor segmentation over\nthe years, creating a fully autonomous system suitable for clinical floors\nremains a formidable challenge because the model predictions have not yet\nreached the desired level of accuracy and generalizability for clinical\napplications. Generative modeling techniques have seen significant improvements\nin recent times. Specifically, Generative Adversarial Networks (GANs) and\nDenoising-diffusion-based models (DDPMs) have been used to generate\nhigher-quality images with fewer artifacts and finer attributes. In this work,\nwe introduce a framework called Re-Diffinet for modeling the discrepancy\nbetween the outputs of a segmentation model like U-Net and the ground truth,\nusing DDPMs. By explicitly modeling the discrepancy, the results show an\naverage improvement of 0.55\\% in the Dice score and 16.28\\% in HD95 from\ncross-validation over 5-folds, compared to the state-of-the-art U-Net\nsegmentation model.\n", "link": "http://arxiv.org/abs/2402.07354v3", "date": "2024-02-15", "relevancy": 1.0819, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5435}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5407}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-DiffiNet%3A%20Modeling%20discrepancies%20loss%20in%20tumor%20segmentation%20using%0A%20%20diffusion%20models&entry.906535625=Tianyi%20Ren%20and%20Abhishek%20Sharma%20and%20Juampablo%20Heras%20Rivera%20and%20Harshitha%20Rebala%20and%20Ethan%20Honey%20and%20Agamdeep%20Chopra%20and%20Jacob%20Ruzevick%20and%20Mehmet%20Kurt&entry.1292438233=%20%20Identification%20of%20tumor%20margins%20is%20essential%20for%20surgical%20decision-making%20for%0Aglioblastoma%20patients%20and%20provides%20reliable%20assistance%20for%20neurosurgeons.%0ADespite%20improvements%20in%20deep%20learning%20architectures%20for%20tumor%20segmentation%20over%0Athe%20years%2C%20creating%20a%20fully%20autonomous%20system%20suitable%20for%20clinical%20floors%0Aremains%20a%20formidable%20challenge%20because%20the%20model%20predictions%20have%20not%20yet%0Areached%20the%20desired%20level%20of%20accuracy%20and%20generalizability%20for%20clinical%0Aapplications.%20Generative%20modeling%20techniques%20have%20seen%20significant%20improvements%0Ain%20recent%20times.%20Specifically%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%0ADenoising-diffusion-based%20models%20%28DDPMs%29%20have%20been%20used%20to%20generate%0Ahigher-quality%20images%20with%20fewer%20artifacts%20and%20finer%20attributes.%20In%20this%20work%2C%0Awe%20introduce%20a%20framework%20called%20Re-Diffinet%20for%20modeling%20the%20discrepancy%0Abetween%20the%20outputs%20of%20a%20segmentation%20model%20like%20U-Net%20and%20the%20ground%20truth%2C%0Ausing%20DDPMs.%20By%20explicitly%20modeling%20the%20discrepancy%2C%20the%20results%20show%20an%0Aaverage%20improvement%20of%200.55%5C%25%20in%20the%20Dice%20score%20and%2016.28%5C%25%20in%20HD95%20from%0Across-validation%20over%205-folds%2C%20compared%20to%20the%20state-of-the-art%20U-Net%0Asegmentation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07354v3&entry.124074799=Read"},
{"title": "Uncertainty Decomposition and Quantification for In-Context Learning of\n  Large Language Models", "author": "Chen Ling and Xujiang Zhao and Wei Cheng and Yanchi Liu and Yiyou Sun and Xuchao Zhang and Mika Oishi and Takao Osaki and Katsushi Matsuda and Jie Ji and Guangji Bai and Liang Zhao and Haifeng Chen", "abstract": "  In-context learning has emerged as a groundbreaking ability of Large Language\nModels (LLMs) and revolutionized various fields by providing a few\ntask-relevant demonstrations in the prompt. However, trustworthy issues with\nLLM's response, such as hallucination, have also been actively discussed.\nExisting works have been devoted to quantifying the uncertainty in LLM's\nresponse, but they often overlook the complex nature of LLMs and the uniqueness\nof in-context learning. In this work, we delve into the predictive uncertainty\nof LLMs associated with in-context learning, highlighting that such\nuncertainties may stem from both the provided demonstrations (aleatoric\nuncertainty) and ambiguities tied to the model's configurations (epistemic\nuncertainty). We propose a novel formulation and corresponding estimation\nmethod to quantify both types of uncertainties. The proposed method offers an\nunsupervised way to understand the prediction of in-context learning in a\nplug-and-play fashion. Extensive experiments are conducted to demonstrate the\neffectiveness of the decomposition. The code and data are available at:\n\\url{https://github.com/lingchen0331/UQ_ICL}.\n", "link": "http://arxiv.org/abs/2402.10189v1", "date": "2024-02-15", "relevancy": 1.0809, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5669}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5629}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4915}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Decomposition%20and%20Quantification%20for%20In-Context%20Learning%20of%0A%20%20Large%20Language%20Models&entry.906535625=Chen%20Ling%20and%20Xujiang%20Zhao%20and%20Wei%20Cheng%20and%20Yanchi%20Liu%20and%20Yiyou%20Sun%20and%20Xuchao%20Zhang%20and%20Mika%20Oishi%20and%20Takao%20Osaki%20and%20Katsushi%20Matsuda%20and%20Jie%20Ji%20and%20Guangji%20Bai%20and%20Liang%20Zhao%20and%20Haifeng%20Chen&entry.1292438233=%20%20In-context%20learning%20has%20emerged%20as%20a%20groundbreaking%20ability%20of%20Large%20Language%0AModels%20%28LLMs%29%20and%20revolutionized%20various%20fields%20by%20providing%20a%20few%0Atask-relevant%20demonstrations%20in%20the%20prompt.%20However%2C%20trustworthy%20issues%20with%0ALLM%27s%20response%2C%20such%20as%20hallucination%2C%20have%20also%20been%20actively%20discussed.%0AExisting%20works%20have%20been%20devoted%20to%20quantifying%20the%20uncertainty%20in%20LLM%27s%0Aresponse%2C%20but%20they%20often%20overlook%20the%20complex%20nature%20of%20LLMs%20and%20the%20uniqueness%0Aof%20in-context%20learning.%20In%20this%20work%2C%20we%20delve%20into%20the%20predictive%20uncertainty%0Aof%20LLMs%20associated%20with%20in-context%20learning%2C%20highlighting%20that%20such%0Auncertainties%20may%20stem%20from%20both%20the%20provided%20demonstrations%20%28aleatoric%0Auncertainty%29%20and%20ambiguities%20tied%20to%20the%20model%27s%20configurations%20%28epistemic%0Auncertainty%29.%20We%20propose%20a%20novel%20formulation%20and%20corresponding%20estimation%0Amethod%20to%20quantify%20both%20types%20of%20uncertainties.%20The%20proposed%20method%20offers%20an%0Aunsupervised%20way%20to%20understand%20the%20prediction%20of%20in-context%20learning%20in%20a%0Aplug-and-play%20fashion.%20Extensive%20experiments%20are%20conducted%20to%20demonstrate%20the%0Aeffectiveness%20of%20the%20decomposition.%20The%20code%20and%20data%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/lingchen0331/UQ_ICL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10189v1&entry.124074799=Read"},
{"title": "ROAM: Robust and Object-Aware Motion Generation Using Neural Pose\n  Descriptors", "author": "Wanyue Zhang and Rishabh Dabral and Thomas Leimk\u00fchler and Vladislav Golyanik and Marc Habermann and Christian Theobalt", "abstract": "  Existing automatic approaches for 3D virtual character motion synthesis\nsupporting scene interactions do not generalise well to new objects outside\ntraining distributions, even when trained on extensive motion capture datasets\nwith diverse objects and annotated interactions. This paper addresses this\nlimitation and shows that robustness and generalisation to novel scene objects\nin 3D object-aware character synthesis can be achieved by training a motion\nmodel with as few as one reference object. We leverage an implicit feature\nrepresentation trained on object-only datasets, which encodes an\nSE(3)-equivariant descriptor field around the object. Given an unseen object\nand a reference pose-object pair, we optimise for the object-aware pose that is\nclosest in the feature space to the reference pose. Finally, we use l-NSM,\ni.e., our motion generation model that is trained to seamlessly transition from\nlocomotion to object interaction with the proposed bidirectional pose blending\nscheme. Through comprehensive numerical comparisons to state-of-the-art methods\nand in a user study, we demonstrate substantial improvements in 3D virtual\ncharacter motion and interaction quality and robustness to scenarios with\nunseen objects. Our project page is available at\nhttps://vcai.mpi-inf.mpg.de/projects/ROAM/.\n", "link": "http://arxiv.org/abs/2308.12969v2", "date": "2024-02-15", "relevancy": 1.0767, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5356}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.526}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROAM%3A%20Robust%20and%20Object-Aware%20Motion%20Generation%20Using%20Neural%20Pose%0A%20%20Descriptors&entry.906535625=Wanyue%20Zhang%20and%20Rishabh%20Dabral%20and%20Thomas%20Leimk%C3%BChler%20and%20Vladislav%20Golyanik%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20Existing%20automatic%20approaches%20for%203D%20virtual%20character%20motion%20synthesis%0Asupporting%20scene%20interactions%20do%20not%20generalise%20well%20to%20new%20objects%20outside%0Atraining%20distributions%2C%20even%20when%20trained%20on%20extensive%20motion%20capture%20datasets%0Awith%20diverse%20objects%20and%20annotated%20interactions.%20This%20paper%20addresses%20this%0Alimitation%20and%20shows%20that%20robustness%20and%20generalisation%20to%20novel%20scene%20objects%0Ain%203D%20object-aware%20character%20synthesis%20can%20be%20achieved%20by%20training%20a%20motion%0Amodel%20with%20as%20few%20as%20one%20reference%20object.%20We%20leverage%20an%20implicit%20feature%0Arepresentation%20trained%20on%20object-only%20datasets%2C%20which%20encodes%20an%0ASE%283%29-equivariant%20descriptor%20field%20around%20the%20object.%20Given%20an%20unseen%20object%0Aand%20a%20reference%20pose-object%20pair%2C%20we%20optimise%20for%20the%20object-aware%20pose%20that%20is%0Aclosest%20in%20the%20feature%20space%20to%20the%20reference%20pose.%20Finally%2C%20we%20use%20l-NSM%2C%0Ai.e.%2C%20our%20motion%20generation%20model%20that%20is%20trained%20to%20seamlessly%20transition%20from%0Alocomotion%20to%20object%20interaction%20with%20the%20proposed%20bidirectional%20pose%20blending%0Ascheme.%20Through%20comprehensive%20numerical%20comparisons%20to%20state-of-the-art%20methods%0Aand%20in%20a%20user%20study%2C%20we%20demonstrate%20substantial%20improvements%20in%203D%20virtual%0Acharacter%20motion%20and%20interaction%20quality%20and%20robustness%20to%20scenarios%20with%0Aunseen%20objects.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/ROAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12969v2&entry.124074799=Read"},
{"title": "Enhancing the Hierarchical Environment Design via Generative Trajectory\n  Modeling", "author": "Dexun Li and Pradeep Varakantham", "abstract": "  Unsupervised Environment Design (UED) is a paradigm for automatically\ngenerating a curriculum of training environments, enabling agents trained in\nthese environments to develop general capabilities, i.e., achieving good\nzero-shot transfer performance. However, existing UED approaches focus\nprimarily on the random generation of environments for open-ended agent\ntraining. This is impractical in scenarios with limited resources, such as the\nconstraints on the number of generated environments. In this paper, we\nintroduce a hierarchical MDP framework for environment design under resource\nconstraints. It consists of an upper-level RL teacher agent that generates\nsuitable training environments for a lower-level student agent. The RL teacher\ncan leverage previously discovered environment structures and generate\nenvironments at the frontier of the student's capabilities by observing the\nstudent policy's representation. Moreover, to reduce the time-consuming\ncollection of experiences for the upper-level teacher, we utilize recent\nadvances in generative modeling to synthesize a trajectory dataset to train the\nteacher agent. Our proposed method significantly reduces the resource-intensive\ninteractions between agents and environments and empirical experiments across\nvarious domains demonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2310.00301v2", "date": "2024-02-15", "relevancy": 1.0674, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5816}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5432}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4762}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Hierarchical%20Environment%20Design%20via%20Generative%20Trajectory%0A%20%20Modeling&entry.906535625=Dexun%20Li%20and%20Pradeep%20Varakantham&entry.1292438233=%20%20Unsupervised%20Environment%20Design%20%28UED%29%20is%20a%20paradigm%20for%20automatically%0Agenerating%20a%20curriculum%20of%20training%20environments%2C%20enabling%20agents%20trained%20in%0Athese%20environments%20to%20develop%20general%20capabilities%2C%20i.e.%2C%20achieving%20good%0Azero-shot%20transfer%20performance.%20However%2C%20existing%20UED%20approaches%20focus%0Aprimarily%20on%20the%20random%20generation%20of%20environments%20for%20open-ended%20agent%0Atraining.%20This%20is%20impractical%20in%20scenarios%20with%20limited%20resources%2C%20such%20as%20the%0Aconstraints%20on%20the%20number%20of%20generated%20environments.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20hierarchical%20MDP%20framework%20for%20environment%20design%20under%20resource%0Aconstraints.%20It%20consists%20of%20an%20upper-level%20RL%20teacher%20agent%20that%20generates%0Asuitable%20training%20environments%20for%20a%20lower-level%20student%20agent.%20The%20RL%20teacher%0Acan%20leverage%20previously%20discovered%20environment%20structures%20and%20generate%0Aenvironments%20at%20the%20frontier%20of%20the%20student%27s%20capabilities%20by%20observing%20the%0Astudent%20policy%27s%20representation.%20Moreover%2C%20to%20reduce%20the%20time-consuming%0Acollection%20of%20experiences%20for%20the%20upper-level%20teacher%2C%20we%20utilize%20recent%0Aadvances%20in%20generative%20modeling%20to%20synthesize%20a%20trajectory%20dataset%20to%20train%20the%0Ateacher%20agent.%20Our%20proposed%20method%20significantly%20reduces%20the%20resource-intensive%0Ainteractions%20between%20agents%20and%20environments%20and%20empirical%20experiments%20across%0Avarious%20domains%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00301v2&entry.124074799=Read"},
{"title": "Neural Networks Asymptotic Behaviours for the Resolution of Inverse\n  Problems", "author": "Luigi Del Debbio and Manuel Naviglio and Francesco Tarantelli", "abstract": "  This paper presents a study of the effectiveness of Neural Network (NN)\ntechniques for deconvolution inverse problems relevant for applications in\nQuantum Field Theory, but also in more general contexts. We consider NN's\nasymptotic limits, corresponding to Gaussian Processes (GPs), where\nnon-linearities in the parameters of the NN can be neglected. Using these\nresulting GPs, we address the deconvolution inverse problem in the case of a\nquantum harmonic oscillator simulated through Monte Carlo techniques on a\nlattice. In this simple toy model, the results of the inversion can be compared\nwith the known analytical solution. Our findings indicate that solving the\ninverse problem with a NN yields less performing results than those obtained\nusing the GPs derived from NN's asymptotic limits. Furthermore, we observe the\ntrained NN's accuracy approaching that of GPs with increasing layer width.\nNotably, one of these GPs defies interpretation as a probabilistic model,\noffering a novel perspective compared to established methods in the literature.\nOur results suggest the need for detailed studies of the training dynamics in\nmore realistic set-ups.\n", "link": "http://arxiv.org/abs/2402.09338v2", "date": "2024-02-15", "relevancy": 0.8804, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4629}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4302}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4274}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20Asymptotic%20Behaviours%20for%20the%20Resolution%20of%20Inverse%0A%20%20Problems&entry.906535625=Luigi%20Del%20Debbio%20and%20Manuel%20Naviglio%20and%20Francesco%20Tarantelli&entry.1292438233=%20%20This%20paper%20presents%20a%20study%20of%20the%20effectiveness%20of%20Neural%20Network%20%28NN%29%0Atechniques%20for%20deconvolution%20inverse%20problems%20relevant%20for%20applications%20in%0AQuantum%20Field%20Theory%2C%20but%20also%20in%20more%20general%20contexts.%20We%20consider%20NN%27s%0Aasymptotic%20limits%2C%20corresponding%20to%20Gaussian%20Processes%20%28GPs%29%2C%20where%0Anon-linearities%20in%20the%20parameters%20of%20the%20NN%20can%20be%20neglected.%20Using%20these%0Aresulting%20GPs%2C%20we%20address%20the%20deconvolution%20inverse%20problem%20in%20the%20case%20of%20a%0Aquantum%20harmonic%20oscillator%20simulated%20through%20Monte%20Carlo%20techniques%20on%20a%0Alattice.%20In%20this%20simple%20toy%20model%2C%20the%20results%20of%20the%20inversion%20can%20be%20compared%0Awith%20the%20known%20analytical%20solution.%20Our%20findings%20indicate%20that%20solving%20the%0Ainverse%20problem%20with%20a%20NN%20yields%20less%20performing%20results%20than%20those%20obtained%0Ausing%20the%20GPs%20derived%20from%20NN%27s%20asymptotic%20limits.%20Furthermore%2C%20we%20observe%20the%0Atrained%20NN%27s%20accuracy%20approaching%20that%20of%20GPs%20with%20increasing%20layer%20width.%0ANotably%2C%20one%20of%20these%20GPs%20defies%20interpretation%20as%20a%20probabilistic%20model%2C%0Aoffering%20a%20novel%20perspective%20compared%20to%20established%20methods%20in%20the%20literature.%0AOur%20results%20suggest%20the%20need%20for%20detailed%20studies%20of%20the%20training%20dynamics%20in%0Amore%20realistic%20set-ups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09338v2&entry.124074799=Read"},
{"title": "Benchmarking federated strategies in Peer-to-Peer Federated learning for\n  biomedical data", "author": "Jose L. Salmeron and Irina Ar\u00e9valo and Antonio Ruiz-Celma", "abstract": "  The increasing requirements for data protection and privacy has attracted a\nhuge research interest on distributed artificial intelligence and specifically\non federated learning, an emerging machine learning approach that allows the\nconstruction of a model between several participants who hold their own private\ndata. In the initial proposal of federated learning the architecture was\ncentralised and the aggregation was done with federated averaging, meaning that\na central server will orchestrate the federation using the most straightforward\naveraging strategy. This research is focused on testing different federated\nstrategies in a peer-to-peer environment. The authors propose various\naggregation strategies for federated learning, including weighted averaging\naggregation, using different factors and strategies based on participant\ncontribution. The strategies are tested with varying data sizes to identify the\nmost robust ones. This research tests the strategies with several biomedical\ndatasets and the results of the experiments show that the accuracy-based\nweighted average outperforms the classical federated averaging method.\n", "link": "http://arxiv.org/abs/2402.10135v1", "date": "2024-02-15", "relevancy": 0.8674, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4583}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4381}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4047}], "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20federated%20strategies%20in%20Peer-to-Peer%20Federated%20learning%20for%0A%20%20biomedical%20data&entry.906535625=Jose%20L.%20Salmeron%20and%20Irina%20Ar%C3%A9valo%20and%20Antonio%20Ruiz-Celma&entry.1292438233=%20%20The%20increasing%20requirements%20for%20data%20protection%20and%20privacy%20has%20attracted%20a%0Ahuge%20research%20interest%20on%20distributed%20artificial%20intelligence%20and%20specifically%0Aon%20federated%20learning%2C%20an%20emerging%20machine%20learning%20approach%20that%20allows%20the%0Aconstruction%20of%20a%20model%20between%20several%20participants%20who%20hold%20their%20own%20private%0Adata.%20In%20the%20initial%20proposal%20of%20federated%20learning%20the%20architecture%20was%0Acentralised%20and%20the%20aggregation%20was%20done%20with%20federated%20averaging%2C%20meaning%20that%0Aa%20central%20server%20will%20orchestrate%20the%20federation%20using%20the%20most%20straightforward%0Aaveraging%20strategy.%20This%20research%20is%20focused%20on%20testing%20different%20federated%0Astrategies%20in%20a%20peer-to-peer%20environment.%20The%20authors%20propose%20various%0Aaggregation%20strategies%20for%20federated%20learning%2C%20including%20weighted%20averaging%0Aaggregation%2C%20using%20different%20factors%20and%20strategies%20based%20on%20participant%0Acontribution.%20The%20strategies%20are%20tested%20with%20varying%20data%20sizes%20to%20identify%20the%0Amost%20robust%20ones.%20This%20research%20tests%20the%20strategies%20with%20several%20biomedical%0Adatasets%20and%20the%20results%20of%20the%20experiments%20show%20that%20the%20accuracy-based%0Aweighted%20average%20outperforms%20the%20classical%20federated%20averaging%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10135v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


