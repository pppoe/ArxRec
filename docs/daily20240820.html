<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240819.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction\n  Model", "author": "Minghua Liu and Chong Zeng and Xinyue Wei and Ruoxi Shi and Linghao Chen and Chao Xu and Mengqi Zhang and Zhaoning Wang and Xiaoshuai Zhang and Isabella Liu and Hongzhi Wu and Hao Su", "abstract": "  Open-world 3D reconstruction models have recently garnered significant\nattention. However, without sufficient 3D inductive bias, existing methods\ntypically entail expensive training costs and struggle to extract high-quality\n3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction\nmodel that explicitly leverages 3D native structure, input guidance, and\ntraining supervision. Specifically, instead of using a triplane representation,\nwe store features in 3D sparse voxels and combine transformers with 3D\nconvolutions to leverage an explicit 3D structure and projective bias. In\naddition to sparse-view RGB input, we require the network to take input and\ngenerate corresponding normal maps. The input normal maps can be predicted by\n2D diffusion models, significantly aiding in the guidance and refinement of the\ngeometry's learning. Moreover, by combining Signed Distance Function (SDF)\nsupervision with surface rendering, we directly learn to generate high-quality\nmeshes without the need for complex multi-stage training processes. By\nincorporating these explicit 3D biases, MeshFormer can be trained efficiently\nand deliver high-quality textured meshes with fine-grained geometric details.\nIt can also be integrated with 2D diffusion models to enable fast\nsingle-image-to-3D and text-to-3D tasks. Project page:\nhttps://meshformer3d.github.io\n", "link": "http://arxiv.org/abs/2408.10198v1", "date": "2024-08-19", "relevancy": 3.4515, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7064}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshFormer%3A%20High-Quality%20Mesh%20Generation%20with%203D-Guided%20Reconstruction%0A%20%20Model&body=Title%3A%20MeshFormer%3A%20High-Quality%20Mesh%20Generation%20with%203D-Guided%20Reconstruction%0A%20%20Model%0AAuthor%3A%20Minghua%20Liu%20and%20Chong%20Zeng%20and%20Xinyue%20Wei%20and%20Ruoxi%20Shi%20and%20Linghao%20Chen%20and%20Chao%20Xu%20and%20Mengqi%20Zhang%20and%20Zhaoning%20Wang%20and%20Xiaoshuai%20Zhang%20and%20Isabella%20Liu%20and%20Hongzhi%20Wu%20and%20Hao%20Su%0AAbstract%3A%20%20%20Open-world%203D%20reconstruction%20models%20have%20recently%20garnered%20significant%0Aattention.%20However%2C%20without%20sufficient%203D%20inductive%20bias%2C%20existing%20methods%0Atypically%20entail%20expensive%20training%20costs%20and%20struggle%20to%20extract%20high-quality%0A3D%20meshes.%20In%20this%20work%2C%20we%20introduce%20MeshFormer%2C%20a%20sparse-view%20reconstruction%0Amodel%20that%20explicitly%20leverages%203D%20native%20structure%2C%20input%20guidance%2C%20and%0Atraining%20supervision.%20Specifically%2C%20instead%20of%20using%20a%20triplane%20representation%2C%0Awe%20store%20features%20in%203D%20sparse%20voxels%20and%20combine%20transformers%20with%203D%0Aconvolutions%20to%20leverage%20an%20explicit%203D%20structure%20and%20projective%20bias.%20In%0Aaddition%20to%20sparse-view%20RGB%20input%2C%20we%20require%20the%20network%20to%20take%20input%20and%0Agenerate%20corresponding%20normal%20maps.%20The%20input%20normal%20maps%20can%20be%20predicted%20by%0A2D%20diffusion%20models%2C%20significantly%20aiding%20in%20the%20guidance%20and%20refinement%20of%20the%0Ageometry%27s%20learning.%20Moreover%2C%20by%20combining%20Signed%20Distance%20Function%20%28SDF%29%0Asupervision%20with%20surface%20rendering%2C%20we%20directly%20learn%20to%20generate%20high-quality%0Ameshes%20without%20the%20need%20for%20complex%20multi-stage%20training%20processes.%20By%0Aincorporating%20these%20explicit%203D%20biases%2C%20MeshFormer%20can%20be%20trained%20efficiently%0Aand%20deliver%20high-quality%20textured%20meshes%20with%20fine-grained%20geometric%20details.%0AIt%20can%20also%20be%20integrated%20with%202D%20diffusion%20models%20to%20enable%20fast%0Asingle-image-to-3D%20and%20text-to-3D%20tasks.%20Project%20page%3A%0Ahttps%3A//meshformer3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshFormer%253A%2520High-Quality%2520Mesh%2520Generation%2520with%25203D-Guided%2520Reconstruction%250A%2520%2520Model%26entry.906535625%3DMinghua%2520Liu%2520and%2520Chong%2520Zeng%2520and%2520Xinyue%2520Wei%2520and%2520Ruoxi%2520Shi%2520and%2520Linghao%2520Chen%2520and%2520Chao%2520Xu%2520and%2520Mengqi%2520Zhang%2520and%2520Zhaoning%2520Wang%2520and%2520Xiaoshuai%2520Zhang%2520and%2520Isabella%2520Liu%2520and%2520Hongzhi%2520Wu%2520and%2520Hao%2520Su%26entry.1292438233%3D%2520%2520Open-world%25203D%2520reconstruction%2520models%2520have%2520recently%2520garnered%2520significant%250Aattention.%2520However%252C%2520without%2520sufficient%25203D%2520inductive%2520bias%252C%2520existing%2520methods%250Atypically%2520entail%2520expensive%2520training%2520costs%2520and%2520struggle%2520to%2520extract%2520high-quality%250A3D%2520meshes.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MeshFormer%252C%2520a%2520sparse-view%2520reconstruction%250Amodel%2520that%2520explicitly%2520leverages%25203D%2520native%2520structure%252C%2520input%2520guidance%252C%2520and%250Atraining%2520supervision.%2520Specifically%252C%2520instead%2520of%2520using%2520a%2520triplane%2520representation%252C%250Awe%2520store%2520features%2520in%25203D%2520sparse%2520voxels%2520and%2520combine%2520transformers%2520with%25203D%250Aconvolutions%2520to%2520leverage%2520an%2520explicit%25203D%2520structure%2520and%2520projective%2520bias.%2520In%250Aaddition%2520to%2520sparse-view%2520RGB%2520input%252C%2520we%2520require%2520the%2520network%2520to%2520take%2520input%2520and%250Agenerate%2520corresponding%2520normal%2520maps.%2520The%2520input%2520normal%2520maps%2520can%2520be%2520predicted%2520by%250A2D%2520diffusion%2520models%252C%2520significantly%2520aiding%2520in%2520the%2520guidance%2520and%2520refinement%2520of%2520the%250Ageometry%2527s%2520learning.%2520Moreover%252C%2520by%2520combining%2520Signed%2520Distance%2520Function%2520%2528SDF%2529%250Asupervision%2520with%2520surface%2520rendering%252C%2520we%2520directly%2520learn%2520to%2520generate%2520high-quality%250Ameshes%2520without%2520the%2520need%2520for%2520complex%2520multi-stage%2520training%2520processes.%2520By%250Aincorporating%2520these%2520explicit%25203D%2520biases%252C%2520MeshFormer%2520can%2520be%2520trained%2520efficiently%250Aand%2520deliver%2520high-quality%2520textured%2520meshes%2520with%2520fine-grained%2520geometric%2520details.%250AIt%2520can%2520also%2520be%2520integrated%2520with%25202D%2520diffusion%2520models%2520to%2520enable%2520fast%250Asingle-image-to-3D%2520and%2520text-to-3D%2520tasks.%2520Project%2520page%253A%250Ahttps%253A//meshformer3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshFormer%3A%20High-Quality%20Mesh%20Generation%20with%203D-Guided%20Reconstruction%0A%20%20Model&entry.906535625=Minghua%20Liu%20and%20Chong%20Zeng%20and%20Xinyue%20Wei%20and%20Ruoxi%20Shi%20and%20Linghao%20Chen%20and%20Chao%20Xu%20and%20Mengqi%20Zhang%20and%20Zhaoning%20Wang%20and%20Xiaoshuai%20Zhang%20and%20Isabella%20Liu%20and%20Hongzhi%20Wu%20and%20Hao%20Su&entry.1292438233=%20%20Open-world%203D%20reconstruction%20models%20have%20recently%20garnered%20significant%0Aattention.%20However%2C%20without%20sufficient%203D%20inductive%20bias%2C%20existing%20methods%0Atypically%20entail%20expensive%20training%20costs%20and%20struggle%20to%20extract%20high-quality%0A3D%20meshes.%20In%20this%20work%2C%20we%20introduce%20MeshFormer%2C%20a%20sparse-view%20reconstruction%0Amodel%20that%20explicitly%20leverages%203D%20native%20structure%2C%20input%20guidance%2C%20and%0Atraining%20supervision.%20Specifically%2C%20instead%20of%20using%20a%20triplane%20representation%2C%0Awe%20store%20features%20in%203D%20sparse%20voxels%20and%20combine%20transformers%20with%203D%0Aconvolutions%20to%20leverage%20an%20explicit%203D%20structure%20and%20projective%20bias.%20In%0Aaddition%20to%20sparse-view%20RGB%20input%2C%20we%20require%20the%20network%20to%20take%20input%20and%0Agenerate%20corresponding%20normal%20maps.%20The%20input%20normal%20maps%20can%20be%20predicted%20by%0A2D%20diffusion%20models%2C%20significantly%20aiding%20in%20the%20guidance%20and%20refinement%20of%20the%0Ageometry%27s%20learning.%20Moreover%2C%20by%20combining%20Signed%20Distance%20Function%20%28SDF%29%0Asupervision%20with%20surface%20rendering%2C%20we%20directly%20learn%20to%20generate%20high-quality%0Ameshes%20without%20the%20need%20for%20complex%20multi-stage%20training%20processes.%20By%0Aincorporating%20these%20explicit%203D%20biases%2C%20MeshFormer%20can%20be%20trained%20efficiently%0Aand%20deliver%20high-quality%20textured%20meshes%20with%20fine-grained%20geometric%20details.%0AIt%20can%20also%20be%20integrated%20with%202D%20diffusion%20models%20to%20enable%20fast%0Asingle-image-to-3D%20and%20text-to-3D%20tasks.%20Project%20page%3A%0Ahttps%3A//meshformer3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10198v1&entry.124074799=Read"},
{"title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse\n  Views", "author": "Chao Xu and Ang Li and Linghao Chen and Yulin Liu and Ruoxi Shi and Hao Su and Minghua Liu", "abstract": "  Open-world 3D generation has recently attracted considerable attention. While\nmany single-image-to-3D methods have yielded visually appealing outcomes, they\noften lack sufficient controllability and tend to produce hallucinated regions\nthat may not align with users' expectations. In this paper, we explore an\nimportant scenario in which the input consists of one or a few unposed 2D\nimages of a single object, with little or no overlap. We propose a novel\nmethod, SpaRP, to reconstruct a 3D textured mesh and estimate the relative\ncamera poses for these sparse-view images. SpaRP distills knowledge from 2D\ndiffusion models and finetunes them to implicitly deduce the 3D spatial\nrelationships between the sparse views. The diffusion model is trained to\njointly predict surrogate representations for camera poses and multi-view\nimages of the object under known poses, integrating all information from the\ninput sparse views. These predictions are then leveraged to accomplish 3D\nreconstruction and pose estimation, and the reconstructed 3D model can be used\nto further refine the camera poses of input views. Through extensive\nexperiments on three datasets, we demonstrate that our method not only\nsignificantly outperforms baseline methods in terms of 3D reconstruction\nquality and pose prediction accuracy but also exhibits strong efficiency. It\nrequires only about 20 seconds to produce a textured mesh and camera poses for\nthe input views. Project page: https://chaoxu.xyz/sparp.\n", "link": "http://arxiv.org/abs/2408.10195v1", "date": "2024-08-19", "relevancy": 3.3367, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6683}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6683}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaRP%3A%20Fast%203D%20Object%20Reconstruction%20and%20Pose%20Estimation%20from%20Sparse%0A%20%20Views&body=Title%3A%20SpaRP%3A%20Fast%203D%20Object%20Reconstruction%20and%20Pose%20Estimation%20from%20Sparse%0A%20%20Views%0AAuthor%3A%20Chao%20Xu%20and%20Ang%20Li%20and%20Linghao%20Chen%20and%20Yulin%20Liu%20and%20Ruoxi%20Shi%20and%20Hao%20Su%20and%20Minghua%20Liu%0AAbstract%3A%20%20%20Open-world%203D%20generation%20has%20recently%20attracted%20considerable%20attention.%20While%0Amany%20single-image-to-3D%20methods%20have%20yielded%20visually%20appealing%20outcomes%2C%20they%0Aoften%20lack%20sufficient%20controllability%20and%20tend%20to%20produce%20hallucinated%20regions%0Athat%20may%20not%20align%20with%20users%27%20expectations.%20In%20this%20paper%2C%20we%20explore%20an%0Aimportant%20scenario%20in%20which%20the%20input%20consists%20of%20one%20or%20a%20few%20unposed%202D%0Aimages%20of%20a%20single%20object%2C%20with%20little%20or%20no%20overlap.%20We%20propose%20a%20novel%0Amethod%2C%20SpaRP%2C%20to%20reconstruct%20a%203D%20textured%20mesh%20and%20estimate%20the%20relative%0Acamera%20poses%20for%20these%20sparse-view%20images.%20SpaRP%20distills%20knowledge%20from%202D%0Adiffusion%20models%20and%20finetunes%20them%20to%20implicitly%20deduce%20the%203D%20spatial%0Arelationships%20between%20the%20sparse%20views.%20The%20diffusion%20model%20is%20trained%20to%0Ajointly%20predict%20surrogate%20representations%20for%20camera%20poses%20and%20multi-view%0Aimages%20of%20the%20object%20under%20known%20poses%2C%20integrating%20all%20information%20from%20the%0Ainput%20sparse%20views.%20These%20predictions%20are%20then%20leveraged%20to%20accomplish%203D%0Areconstruction%20and%20pose%20estimation%2C%20and%20the%20reconstructed%203D%20model%20can%20be%20used%0Ato%20further%20refine%20the%20camera%20poses%20of%20input%20views.%20Through%20extensive%0Aexperiments%20on%20three%20datasets%2C%20we%20demonstrate%20that%20our%20method%20not%20only%0Asignificantly%20outperforms%20baseline%20methods%20in%20terms%20of%203D%20reconstruction%0Aquality%20and%20pose%20prediction%20accuracy%20but%20also%20exhibits%20strong%20efficiency.%20It%0Arequires%20only%20about%2020%20seconds%20to%20produce%20a%20textured%20mesh%20and%20camera%20poses%20for%0Athe%20input%20views.%20Project%20page%3A%20https%3A//chaoxu.xyz/sparp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaRP%253A%2520Fast%25203D%2520Object%2520Reconstruction%2520and%2520Pose%2520Estimation%2520from%2520Sparse%250A%2520%2520Views%26entry.906535625%3DChao%2520Xu%2520and%2520Ang%2520Li%2520and%2520Linghao%2520Chen%2520and%2520Yulin%2520Liu%2520and%2520Ruoxi%2520Shi%2520and%2520Hao%2520Su%2520and%2520Minghua%2520Liu%26entry.1292438233%3D%2520%2520Open-world%25203D%2520generation%2520has%2520recently%2520attracted%2520considerable%2520attention.%2520While%250Amany%2520single-image-to-3D%2520methods%2520have%2520yielded%2520visually%2520appealing%2520outcomes%252C%2520they%250Aoften%2520lack%2520sufficient%2520controllability%2520and%2520tend%2520to%2520produce%2520hallucinated%2520regions%250Athat%2520may%2520not%2520align%2520with%2520users%2527%2520expectations.%2520In%2520this%2520paper%252C%2520we%2520explore%2520an%250Aimportant%2520scenario%2520in%2520which%2520the%2520input%2520consists%2520of%2520one%2520or%2520a%2520few%2520unposed%25202D%250Aimages%2520of%2520a%2520single%2520object%252C%2520with%2520little%2520or%2520no%2520overlap.%2520We%2520propose%2520a%2520novel%250Amethod%252C%2520SpaRP%252C%2520to%2520reconstruct%2520a%25203D%2520textured%2520mesh%2520and%2520estimate%2520the%2520relative%250Acamera%2520poses%2520for%2520these%2520sparse-view%2520images.%2520SpaRP%2520distills%2520knowledge%2520from%25202D%250Adiffusion%2520models%2520and%2520finetunes%2520them%2520to%2520implicitly%2520deduce%2520the%25203D%2520spatial%250Arelationships%2520between%2520the%2520sparse%2520views.%2520The%2520diffusion%2520model%2520is%2520trained%2520to%250Ajointly%2520predict%2520surrogate%2520representations%2520for%2520camera%2520poses%2520and%2520multi-view%250Aimages%2520of%2520the%2520object%2520under%2520known%2520poses%252C%2520integrating%2520all%2520information%2520from%2520the%250Ainput%2520sparse%2520views.%2520These%2520predictions%2520are%2520then%2520leveraged%2520to%2520accomplish%25203D%250Areconstruction%2520and%2520pose%2520estimation%252C%2520and%2520the%2520reconstructed%25203D%2520model%2520can%2520be%2520used%250Ato%2520further%2520refine%2520the%2520camera%2520poses%2520of%2520input%2520views.%2520Through%2520extensive%250Aexperiments%2520on%2520three%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520not%2520only%250Asignificantly%2520outperforms%2520baseline%2520methods%2520in%2520terms%2520of%25203D%2520reconstruction%250Aquality%2520and%2520pose%2520prediction%2520accuracy%2520but%2520also%2520exhibits%2520strong%2520efficiency.%2520It%250Arequires%2520only%2520about%252020%2520seconds%2520to%2520produce%2520a%2520textured%2520mesh%2520and%2520camera%2520poses%2520for%250Athe%2520input%2520views.%2520Project%2520page%253A%2520https%253A//chaoxu.xyz/sparp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaRP%3A%20Fast%203D%20Object%20Reconstruction%20and%20Pose%20Estimation%20from%20Sparse%0A%20%20Views&entry.906535625=Chao%20Xu%20and%20Ang%20Li%20and%20Linghao%20Chen%20and%20Yulin%20Liu%20and%20Ruoxi%20Shi%20and%20Hao%20Su%20and%20Minghua%20Liu&entry.1292438233=%20%20Open-world%203D%20generation%20has%20recently%20attracted%20considerable%20attention.%20While%0Amany%20single-image-to-3D%20methods%20have%20yielded%20visually%20appealing%20outcomes%2C%20they%0Aoften%20lack%20sufficient%20controllability%20and%20tend%20to%20produce%20hallucinated%20regions%0Athat%20may%20not%20align%20with%20users%27%20expectations.%20In%20this%20paper%2C%20we%20explore%20an%0Aimportant%20scenario%20in%20which%20the%20input%20consists%20of%20one%20or%20a%20few%20unposed%202D%0Aimages%20of%20a%20single%20object%2C%20with%20little%20or%20no%20overlap.%20We%20propose%20a%20novel%0Amethod%2C%20SpaRP%2C%20to%20reconstruct%20a%203D%20textured%20mesh%20and%20estimate%20the%20relative%0Acamera%20poses%20for%20these%20sparse-view%20images.%20SpaRP%20distills%20knowledge%20from%202D%0Adiffusion%20models%20and%20finetunes%20them%20to%20implicitly%20deduce%20the%203D%20spatial%0Arelationships%20between%20the%20sparse%20views.%20The%20diffusion%20model%20is%20trained%20to%0Ajointly%20predict%20surrogate%20representations%20for%20camera%20poses%20and%20multi-view%0Aimages%20of%20the%20object%20under%20known%20poses%2C%20integrating%20all%20information%20from%20the%0Ainput%20sparse%20views.%20These%20predictions%20are%20then%20leveraged%20to%20accomplish%203D%0Areconstruction%20and%20pose%20estimation%2C%20and%20the%20reconstructed%203D%20model%20can%20be%20used%0Ato%20further%20refine%20the%20camera%20poses%20of%20input%20views.%20Through%20extensive%0Aexperiments%20on%20three%20datasets%2C%20we%20demonstrate%20that%20our%20method%20not%20only%0Asignificantly%20outperforms%20baseline%20methods%20in%20terms%20of%203D%20reconstruction%0Aquality%20and%20pose%20prediction%20accuracy%20but%20also%20exhibits%20strong%20efficiency.%20It%0Arequires%20only%20about%2020%20seconds%20to%20produce%20a%20textured%20mesh%20and%20camera%20poses%20for%0Athe%20input%20views.%20Project%20page%3A%20https%3A//chaoxu.xyz/sparp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10195v1&entry.124074799=Read"},
{"title": "LoopSplat: Loop Closure by Registering 3D Gaussian Splats", "author": "Liyuan Zhu and Yue Li and Erik Sandstr\u00f6m and Konrad Schindler and Iro Armeni", "abstract": "  Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats\n(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.\nHowever, existing 3DGS-based methods fail to address the global consistency of\nthe scene via loop closure and/or global bundle adjustment. To this end, we\npropose LoopSplat, which takes RGB-D images as input and performs dense mapping\nwith 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure\nonline and computes relative loop edge constraints between submaps directly via\n3DGS registration, leading to improvements in efficiency and accuracy over\ntraditional global-to-local point cloud registration. It uses a robust pose\ngraph optimization formulation and rigidly aligns the submaps to achieve global\nconsistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,\nScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,\nmapping, and rendering compared to existing methods for dense RGB-D SLAM. Code\nis available at \\href{https://loopsplat.github.io/}{loopsplat.github.io}.\n", "link": "http://arxiv.org/abs/2408.10154v1", "date": "2024-08-19", "relevancy": 3.2408, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7618}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5983}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoopSplat%3A%20Loop%20Closure%20by%20Registering%203D%20Gaussian%20Splats&body=Title%3A%20LoopSplat%3A%20Loop%20Closure%20by%20Registering%203D%20Gaussian%20Splats%0AAuthor%3A%20Liyuan%20Zhu%20and%20Yue%20Li%20and%20Erik%20Sandstr%C3%B6m%20and%20Konrad%20Schindler%20and%20Iro%20Armeni%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20based%20on%203D%20Gaussian%20Splats%0A%283DGS%29%20has%20recently%20shown%20promise%20towards%20more%20accurate%2C%20dense%203D%20scene%20maps.%0AHowever%2C%20existing%203DGS-based%20methods%20fail%20to%20address%20the%20global%20consistency%20of%0Athe%20scene%20via%20loop%20closure%20and/or%20global%20bundle%20adjustment.%20To%20this%20end%2C%20we%0Apropose%20LoopSplat%2C%20which%20takes%20RGB-D%20images%20as%20input%20and%20performs%20dense%20mapping%0Awith%203DGS%20submaps%20and%20frame-to-model%20tracking.%20LoopSplat%20triggers%20loop%20closure%0Aonline%20and%20computes%20relative%20loop%20edge%20constraints%20between%20submaps%20directly%20via%0A3DGS%20registration%2C%20leading%20to%20improvements%20in%20efficiency%20and%20accuracy%20over%0Atraditional%20global-to-local%20point%20cloud%20registration.%20It%20uses%20a%20robust%20pose%0Agraph%20optimization%20formulation%20and%20rigidly%20aligns%20the%20submaps%20to%20achieve%20global%0Aconsistency.%20Evaluation%20on%20the%20synthetic%20Replica%20and%20real-world%20TUM-RGBD%2C%0AScanNet%2C%20and%20ScanNet%2B%2B%20datasets%20demonstrates%20competitive%20or%20superior%20tracking%2C%0Amapping%2C%20and%20rendering%20compared%20to%20existing%20methods%20for%20dense%20RGB-D%20SLAM.%20Code%0Ais%20available%20at%20%5Chref%7Bhttps%3A//loopsplat.github.io/%7D%7Bloopsplat.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoopSplat%253A%2520Loop%2520Closure%2520by%2520Registering%25203D%2520Gaussian%2520Splats%26entry.906535625%3DLiyuan%2520Zhu%2520and%2520Yue%2520Li%2520and%2520Erik%2520Sandstr%25C3%25B6m%2520and%2520Konrad%2520Schindler%2520and%2520Iro%2520Armeni%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520based%2520on%25203D%2520Gaussian%2520Splats%250A%25283DGS%2529%2520has%2520recently%2520shown%2520promise%2520towards%2520more%2520accurate%252C%2520dense%25203D%2520scene%2520maps.%250AHowever%252C%2520existing%25203DGS-based%2520methods%2520fail%2520to%2520address%2520the%2520global%2520consistency%2520of%250Athe%2520scene%2520via%2520loop%2520closure%2520and/or%2520global%2520bundle%2520adjustment.%2520To%2520this%2520end%252C%2520we%250Apropose%2520LoopSplat%252C%2520which%2520takes%2520RGB-D%2520images%2520as%2520input%2520and%2520performs%2520dense%2520mapping%250Awith%25203DGS%2520submaps%2520and%2520frame-to-model%2520tracking.%2520LoopSplat%2520triggers%2520loop%2520closure%250Aonline%2520and%2520computes%2520relative%2520loop%2520edge%2520constraints%2520between%2520submaps%2520directly%2520via%250A3DGS%2520registration%252C%2520leading%2520to%2520improvements%2520in%2520efficiency%2520and%2520accuracy%2520over%250Atraditional%2520global-to-local%2520point%2520cloud%2520registration.%2520It%2520uses%2520a%2520robust%2520pose%250Agraph%2520optimization%2520formulation%2520and%2520rigidly%2520aligns%2520the%2520submaps%2520to%2520achieve%2520global%250Aconsistency.%2520Evaluation%2520on%2520the%2520synthetic%2520Replica%2520and%2520real-world%2520TUM-RGBD%252C%250AScanNet%252C%2520and%2520ScanNet%252B%252B%2520datasets%2520demonstrates%2520competitive%2520or%2520superior%2520tracking%252C%250Amapping%252C%2520and%2520rendering%2520compared%2520to%2520existing%2520methods%2520for%2520dense%2520RGB-D%2520SLAM.%2520Code%250Ais%2520available%2520at%2520%255Chref%257Bhttps%253A//loopsplat.github.io/%257D%257Bloopsplat.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoopSplat%3A%20Loop%20Closure%20by%20Registering%203D%20Gaussian%20Splats&entry.906535625=Liyuan%20Zhu%20and%20Yue%20Li%20and%20Erik%20Sandstr%C3%B6m%20and%20Konrad%20Schindler%20and%20Iro%20Armeni&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20based%20on%203D%20Gaussian%20Splats%0A%283DGS%29%20has%20recently%20shown%20promise%20towards%20more%20accurate%2C%20dense%203D%20scene%20maps.%0AHowever%2C%20existing%203DGS-based%20methods%20fail%20to%20address%20the%20global%20consistency%20of%0Athe%20scene%20via%20loop%20closure%20and/or%20global%20bundle%20adjustment.%20To%20this%20end%2C%20we%0Apropose%20LoopSplat%2C%20which%20takes%20RGB-D%20images%20as%20input%20and%20performs%20dense%20mapping%0Awith%203DGS%20submaps%20and%20frame-to-model%20tracking.%20LoopSplat%20triggers%20loop%20closure%0Aonline%20and%20computes%20relative%20loop%20edge%20constraints%20between%20submaps%20directly%20via%0A3DGS%20registration%2C%20leading%20to%20improvements%20in%20efficiency%20and%20accuracy%20over%0Atraditional%20global-to-local%20point%20cloud%20registration.%20It%20uses%20a%20robust%20pose%0Agraph%20optimization%20formulation%20and%20rigidly%20aligns%20the%20submaps%20to%20achieve%20global%0Aconsistency.%20Evaluation%20on%20the%20synthetic%20Replica%20and%20real-world%20TUM-RGBD%2C%0AScanNet%2C%20and%20ScanNet%2B%2B%20datasets%20demonstrates%20competitive%20or%20superior%20tracking%2C%0Amapping%2C%20and%20rendering%20compared%20to%20existing%20methods%20for%20dense%20RGB-D%20SLAM.%20Code%0Ais%20available%20at%20%5Chref%7Bhttps%3A//loopsplat.github.io/%7D%7Bloopsplat.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10154v1&entry.124074799=Read"},
{"title": "Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane\n  Representation", "author": "Minye Wu and Tinne Tuytelaars", "abstract": "  Recent advancements in photo-realistic novel view synthesis have been\nsignificantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit\nnature of 3DGS data entails considerable storage requirements, highlighting a\npressing need for more efficient data representations. To address this, we\npresent Implicit Gaussian Splatting (IGS), an innovative hybrid model that\nintegrates explicit point clouds with implicit feature embeddings through a\nmulti-level tri-plane architecture. This architecture features 2D feature grids\nat various resolutions across different levels, facilitating continuous spatial\ndomain representation and enhancing spatial correlations among Gaussian\nprimitives. Building upon this foundation, we introduce a level-based\nprogressive training scheme, which incorporates explicit spatial\nregularization. This method capitalizes on spatial correlations to enhance both\nthe rendering quality and the compactness of the IGS representation.\nFurthermore, we propose a novel compression pipeline tailored for both point\nclouds and 2D feature grids, considering the entropy variations across\ndifferent levels. Extensive experimental evaluations demonstrate that our\nalgorithm can deliver high-quality rendering using only a few MBs, effectively\nbalancing storage efficiency and rendering fidelity, and yielding results that\nare competitive with the state-of-the-art.\n", "link": "http://arxiv.org/abs/2408.10041v1", "date": "2024-08-19", "relevancy": 3.1887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7082}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6378}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Gaussian%20Splatting%20with%20Efficient%20Multi-Level%20Tri-Plane%0A%20%20Representation&body=Title%3A%20Implicit%20Gaussian%20Splatting%20with%20Efficient%20Multi-Level%20Tri-Plane%0A%20%20Representation%0AAuthor%3A%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20Recent%20advancements%20in%20photo-realistic%20novel%20view%20synthesis%20have%20been%0Asignificantly%20driven%20by%20Gaussian%20Splatting%20%283DGS%29.%20Nevertheless%2C%20the%20explicit%0Anature%20of%203DGS%20data%20entails%20considerable%20storage%20requirements%2C%20highlighting%20a%0Apressing%20need%20for%20more%20efficient%20data%20representations.%20To%20address%20this%2C%20we%0Apresent%20Implicit%20Gaussian%20Splatting%20%28IGS%29%2C%20an%20innovative%20hybrid%20model%20that%0Aintegrates%20explicit%20point%20clouds%20with%20implicit%20feature%20embeddings%20through%20a%0Amulti-level%20tri-plane%20architecture.%20This%20architecture%20features%202D%20feature%20grids%0Aat%20various%20resolutions%20across%20different%20levels%2C%20facilitating%20continuous%20spatial%0Adomain%20representation%20and%20enhancing%20spatial%20correlations%20among%20Gaussian%0Aprimitives.%20Building%20upon%20this%20foundation%2C%20we%20introduce%20a%20level-based%0Aprogressive%20training%20scheme%2C%20which%20incorporates%20explicit%20spatial%0Aregularization.%20This%20method%20capitalizes%20on%20spatial%20correlations%20to%20enhance%20both%0Athe%20rendering%20quality%20and%20the%20compactness%20of%20the%20IGS%20representation.%0AFurthermore%2C%20we%20propose%20a%20novel%20compression%20pipeline%20tailored%20for%20both%20point%0Aclouds%20and%202D%20feature%20grids%2C%20considering%20the%20entropy%20variations%20across%0Adifferent%20levels.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%0Aalgorithm%20can%20deliver%20high-quality%20rendering%20using%20only%20a%20few%20MBs%2C%20effectively%0Abalancing%20storage%20efficiency%20and%20rendering%20fidelity%2C%20and%20yielding%20results%20that%0Aare%20competitive%20with%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Gaussian%2520Splatting%2520with%2520Efficient%2520Multi-Level%2520Tri-Plane%250A%2520%2520Representation%26entry.906535625%3DMinye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520photo-realistic%2520novel%2520view%2520synthesis%2520have%2520been%250Asignificantly%2520driven%2520by%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520Nevertheless%252C%2520the%2520explicit%250Anature%2520of%25203DGS%2520data%2520entails%2520considerable%2520storage%2520requirements%252C%2520highlighting%2520a%250Apressing%2520need%2520for%2520more%2520efficient%2520data%2520representations.%2520To%2520address%2520this%252C%2520we%250Apresent%2520Implicit%2520Gaussian%2520Splatting%2520%2528IGS%2529%252C%2520an%2520innovative%2520hybrid%2520model%2520that%250Aintegrates%2520explicit%2520point%2520clouds%2520with%2520implicit%2520feature%2520embeddings%2520through%2520a%250Amulti-level%2520tri-plane%2520architecture.%2520This%2520architecture%2520features%25202D%2520feature%2520grids%250Aat%2520various%2520resolutions%2520across%2520different%2520levels%252C%2520facilitating%2520continuous%2520spatial%250Adomain%2520representation%2520and%2520enhancing%2520spatial%2520correlations%2520among%2520Gaussian%250Aprimitives.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520introduce%2520a%2520level-based%250Aprogressive%2520training%2520scheme%252C%2520which%2520incorporates%2520explicit%2520spatial%250Aregularization.%2520This%2520method%2520capitalizes%2520on%2520spatial%2520correlations%2520to%2520enhance%2520both%250Athe%2520rendering%2520quality%2520and%2520the%2520compactness%2520of%2520the%2520IGS%2520representation.%250AFurthermore%252C%2520we%2520propose%2520a%2520novel%2520compression%2520pipeline%2520tailored%2520for%2520both%2520point%250Aclouds%2520and%25202D%2520feature%2520grids%252C%2520considering%2520the%2520entropy%2520variations%2520across%250Adifferent%2520levels.%2520Extensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520our%250Aalgorithm%2520can%2520deliver%2520high-quality%2520rendering%2520using%2520only%2520a%2520few%2520MBs%252C%2520effectively%250Abalancing%2520storage%2520efficiency%2520and%2520rendering%2520fidelity%252C%2520and%2520yielding%2520results%2520that%250Aare%2520competitive%2520with%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Gaussian%20Splatting%20with%20Efficient%20Multi-Level%20Tri-Plane%0A%20%20Representation&entry.906535625=Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20Recent%20advancements%20in%20photo-realistic%20novel%20view%20synthesis%20have%20been%0Asignificantly%20driven%20by%20Gaussian%20Splatting%20%283DGS%29.%20Nevertheless%2C%20the%20explicit%0Anature%20of%203DGS%20data%20entails%20considerable%20storage%20requirements%2C%20highlighting%20a%0Apressing%20need%20for%20more%20efficient%20data%20representations.%20To%20address%20this%2C%20we%0Apresent%20Implicit%20Gaussian%20Splatting%20%28IGS%29%2C%20an%20innovative%20hybrid%20model%20that%0Aintegrates%20explicit%20point%20clouds%20with%20implicit%20feature%20embeddings%20through%20a%0Amulti-level%20tri-plane%20architecture.%20This%20architecture%20features%202D%20feature%20grids%0Aat%20various%20resolutions%20across%20different%20levels%2C%20facilitating%20continuous%20spatial%0Adomain%20representation%20and%20enhancing%20spatial%20correlations%20among%20Gaussian%0Aprimitives.%20Building%20upon%20this%20foundation%2C%20we%20introduce%20a%20level-based%0Aprogressive%20training%20scheme%2C%20which%20incorporates%20explicit%20spatial%0Aregularization.%20This%20method%20capitalizes%20on%20spatial%20correlations%20to%20enhance%20both%0Athe%20rendering%20quality%20and%20the%20compactness%20of%20the%20IGS%20representation.%0AFurthermore%2C%20we%20propose%20a%20novel%20compression%20pipeline%20tailored%20for%20both%20point%0Aclouds%20and%202D%20feature%20grids%2C%20considering%20the%20entropy%20variations%20across%0Adifferent%20levels.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%0Aalgorithm%20can%20deliver%20high-quality%20rendering%20using%20only%20a%20few%20MBs%2C%20effectively%0Abalancing%20storage%20efficiency%20and%20rendering%20fidelity%2C%20and%20yielding%20results%20that%0Aare%20competitive%20with%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10041v1&entry.124074799=Read"},
{"title": "P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders", "author": "Xuechao Chen and Ying Chen and Jialin Li and Qiang Nie and Yong Liu and Qixing Huang and Yang Li", "abstract": "  3D pre-training is crucial to 3D perception tasks. However, limited by the\ndifficulties in collecting clean 3D data, 3D pre-training consistently faced\ndata scaling challenges. Inspired by semi-supervised learning leveraging\nlimited labeled data and a large amount of unlabeled data, in this work, we\npropose a novel self-supervised pre-training framework utilizing the real 3D\ndata and the pseudo-3D data lifted from images by a large depth estimation\nmodel. Another challenge lies in the efficiency. Previous methods such as\nPoint-BERT and Point-MAE, employ k nearest neighbors to embed 3D tokens,\nrequiring quadratic time complexity. To efficiently pre-train on such a large\namount of data, we propose a linear-time-complexity token embedding strategy\nand a training-efficient 2D reconstruction target. Our method achieves\nstate-of-the-art performance in 3D classification and few-shot learning while\nmaintaining high pre-training and downstream fine-tuning efficiency.\n", "link": "http://arxiv.org/abs/2408.10007v1", "date": "2024-08-19", "relevancy": 3.0299, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6102}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Masked%20Autoencoders&body=Title%3A%20P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Masked%20Autoencoders%0AAuthor%3A%20Xuechao%20Chen%20and%20Ying%20Chen%20and%20Jialin%20Li%20and%20Qiang%20Nie%20and%20Yong%20Liu%20and%20Qixing%20Huang%20and%20Yang%20Li%0AAbstract%3A%20%20%203D%20pre-training%20is%20crucial%20to%203D%20perception%20tasks.%20However%2C%20limited%20by%20the%0Adifficulties%20in%20collecting%20clean%203D%20data%2C%203D%20pre-training%20consistently%20faced%0Adata%20scaling%20challenges.%20Inspired%20by%20semi-supervised%20learning%20leveraging%0Alimited%20labeled%20data%20and%20a%20large%20amount%20of%20unlabeled%20data%2C%20in%20this%20work%2C%20we%0Apropose%20a%20novel%20self-supervised%20pre-training%20framework%20utilizing%20the%20real%203D%0Adata%20and%20the%20pseudo-3D%20data%20lifted%20from%20images%20by%20a%20large%20depth%20estimation%0Amodel.%20Another%20challenge%20lies%20in%20the%20efficiency.%20Previous%20methods%20such%20as%0APoint-BERT%20and%20Point-MAE%2C%20employ%20k%20nearest%20neighbors%20to%20embed%203D%20tokens%2C%0Arequiring%20quadratic%20time%20complexity.%20To%20efficiently%20pre-train%20on%20such%20a%20large%0Aamount%20of%20data%2C%20we%20propose%20a%20linear-time-complexity%20token%20embedding%20strategy%0Aand%20a%20training-efficient%202D%20reconstruction%20target.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20in%203D%20classification%20and%20few-shot%20learning%20while%0Amaintaining%20high%20pre-training%20and%20downstream%20fine-tuning%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3P%253A%2520Pseudo-3D%2520Pre-training%2520for%2520Scaling%25203D%2520Masked%2520Autoencoders%26entry.906535625%3DXuechao%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Jialin%2520Li%2520and%2520Qiang%2520Nie%2520and%2520Yong%2520Liu%2520and%2520Qixing%2520Huang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%25203D%2520pre-training%2520is%2520crucial%2520to%25203D%2520perception%2520tasks.%2520However%252C%2520limited%2520by%2520the%250Adifficulties%2520in%2520collecting%2520clean%25203D%2520data%252C%25203D%2520pre-training%2520consistently%2520faced%250Adata%2520scaling%2520challenges.%2520Inspired%2520by%2520semi-supervised%2520learning%2520leveraging%250Alimited%2520labeled%2520data%2520and%2520a%2520large%2520amount%2520of%2520unlabeled%2520data%252C%2520in%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520self-supervised%2520pre-training%2520framework%2520utilizing%2520the%2520real%25203D%250Adata%2520and%2520the%2520pseudo-3D%2520data%2520lifted%2520from%2520images%2520by%2520a%2520large%2520depth%2520estimation%250Amodel.%2520Another%2520challenge%2520lies%2520in%2520the%2520efficiency.%2520Previous%2520methods%2520such%2520as%250APoint-BERT%2520and%2520Point-MAE%252C%2520employ%2520k%2520nearest%2520neighbors%2520to%2520embed%25203D%2520tokens%252C%250Arequiring%2520quadratic%2520time%2520complexity.%2520To%2520efficiently%2520pre-train%2520on%2520such%2520a%2520large%250Aamount%2520of%2520data%252C%2520we%2520propose%2520a%2520linear-time-complexity%2520token%2520embedding%2520strategy%250Aand%2520a%2520training-efficient%25202D%2520reconstruction%2520target.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520in%25203D%2520classification%2520and%2520few-shot%2520learning%2520while%250Amaintaining%2520high%2520pre-training%2520and%2520downstream%2520fine-tuning%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Masked%20Autoencoders&entry.906535625=Xuechao%20Chen%20and%20Ying%20Chen%20and%20Jialin%20Li%20and%20Qiang%20Nie%20and%20Yong%20Liu%20and%20Qixing%20Huang%20and%20Yang%20Li&entry.1292438233=%20%203D%20pre-training%20is%20crucial%20to%203D%20perception%20tasks.%20However%2C%20limited%20by%20the%0Adifficulties%20in%20collecting%20clean%203D%20data%2C%203D%20pre-training%20consistently%20faced%0Adata%20scaling%20challenges.%20Inspired%20by%20semi-supervised%20learning%20leveraging%0Alimited%20labeled%20data%20and%20a%20large%20amount%20of%20unlabeled%20data%2C%20in%20this%20work%2C%20we%0Apropose%20a%20novel%20self-supervised%20pre-training%20framework%20utilizing%20the%20real%203D%0Adata%20and%20the%20pseudo-3D%20data%20lifted%20from%20images%20by%20a%20large%20depth%20estimation%0Amodel.%20Another%20challenge%20lies%20in%20the%20efficiency.%20Previous%20methods%20such%20as%0APoint-BERT%20and%20Point-MAE%2C%20employ%20k%20nearest%20neighbors%20to%20embed%203D%20tokens%2C%0Arequiring%20quadratic%20time%20complexity.%20To%20efficiently%20pre-train%20on%20such%20a%20large%0Aamount%20of%20data%2C%20we%20propose%20a%20linear-time-complexity%20token%20embedding%20strategy%0Aand%20a%20training-efficient%202D%20reconstruction%20target.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20in%203D%20classification%20and%20few-shot%20learning%20while%0Amaintaining%20high%20pre-training%20and%20downstream%20fine-tuning%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10007v1&entry.124074799=Read"},
{"title": "DatasetNeRF: Efficient 3D-aware Data Factory with Generative Radiance\n  Fields", "author": "Yu Chi and Fangneng Zhan and Sibo Wu and Christian Theobalt and Adam Kortylewski", "abstract": "  Progress in 3D computer vision tasks demands a huge amount of data, yet\nannotating multi-view images with 3D-consistent annotations, or point clouds\nwith part segmentation is both time-consuming and challenging. This paper\nintroduces DatasetNeRF, a novel approach capable of generating infinite,\nhigh-quality 3D-consistent 2D annotations alongside 3D point cloud\nsegmentations, while utilizing minimal 2D human-labeled annotations.\nSpecifically, we leverage the strong semantic prior within a 3D generative\nmodel to train a semantic decoder, requiring only a handful of fine-grained\nlabeled samples. Once trained, the decoder efficiently generalizes across the\nlatent space, enabling the generation of infinite data. The generated data is\napplicable across various computer vision tasks, including video segmentation\nand 3D point cloud segmentation. Our approach not only surpasses baseline\nmodels in segmentation quality, achieving superior 3D consistency and\nsegmentation precision on individual images, but also demonstrates versatility\nby being applicable to both articulated and non-articulated generative models.\nFurthermore, we explore applications stemming from our approach, such as\n3D-aware semantic editing and 3D inversion.\n", "link": "http://arxiv.org/abs/2311.12063v3", "date": "2024-08-19", "relevancy": 3.0234, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6091}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6091}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DatasetNeRF%3A%20Efficient%203D-aware%20Data%20Factory%20with%20Generative%20Radiance%0A%20%20Fields&body=Title%3A%20DatasetNeRF%3A%20Efficient%203D-aware%20Data%20Factory%20with%20Generative%20Radiance%0A%20%20Fields%0AAuthor%3A%20Yu%20Chi%20and%20Fangneng%20Zhan%20and%20Sibo%20Wu%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski%0AAbstract%3A%20%20%20Progress%20in%203D%20computer%20vision%20tasks%20demands%20a%20huge%20amount%20of%20data%2C%20yet%0Aannotating%20multi-view%20images%20with%203D-consistent%20annotations%2C%20or%20point%20clouds%0Awith%20part%20segmentation%20is%20both%20time-consuming%20and%20challenging.%20This%20paper%0Aintroduces%20DatasetNeRF%2C%20a%20novel%20approach%20capable%20of%20generating%20infinite%2C%0Ahigh-quality%203D-consistent%202D%20annotations%20alongside%203D%20point%20cloud%0Asegmentations%2C%20while%20utilizing%20minimal%202D%20human-labeled%20annotations.%0ASpecifically%2C%20we%20leverage%20the%20strong%20semantic%20prior%20within%20a%203D%20generative%0Amodel%20to%20train%20a%20semantic%20decoder%2C%20requiring%20only%20a%20handful%20of%20fine-grained%0Alabeled%20samples.%20Once%20trained%2C%20the%20decoder%20efficiently%20generalizes%20across%20the%0Alatent%20space%2C%20enabling%20the%20generation%20of%20infinite%20data.%20The%20generated%20data%20is%0Aapplicable%20across%20various%20computer%20vision%20tasks%2C%20including%20video%20segmentation%0Aand%203D%20point%20cloud%20segmentation.%20Our%20approach%20not%20only%20surpasses%20baseline%0Amodels%20in%20segmentation%20quality%2C%20achieving%20superior%203D%20consistency%20and%0Asegmentation%20precision%20on%20individual%20images%2C%20but%20also%20demonstrates%20versatility%0Aby%20being%20applicable%20to%20both%20articulated%20and%20non-articulated%20generative%20models.%0AFurthermore%2C%20we%20explore%20applications%20stemming%20from%20our%20approach%2C%20such%20as%0A3D-aware%20semantic%20editing%20and%203D%20inversion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12063v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDatasetNeRF%253A%2520Efficient%25203D-aware%2520Data%2520Factory%2520with%2520Generative%2520Radiance%250A%2520%2520Fields%26entry.906535625%3DYu%2520Chi%2520and%2520Fangneng%2520Zhan%2520and%2520Sibo%2520Wu%2520and%2520Christian%2520Theobalt%2520and%2520Adam%2520Kortylewski%26entry.1292438233%3D%2520%2520Progress%2520in%25203D%2520computer%2520vision%2520tasks%2520demands%2520a%2520huge%2520amount%2520of%2520data%252C%2520yet%250Aannotating%2520multi-view%2520images%2520with%25203D-consistent%2520annotations%252C%2520or%2520point%2520clouds%250Awith%2520part%2520segmentation%2520is%2520both%2520time-consuming%2520and%2520challenging.%2520This%2520paper%250Aintroduces%2520DatasetNeRF%252C%2520a%2520novel%2520approach%2520capable%2520of%2520generating%2520infinite%252C%250Ahigh-quality%25203D-consistent%25202D%2520annotations%2520alongside%25203D%2520point%2520cloud%250Asegmentations%252C%2520while%2520utilizing%2520minimal%25202D%2520human-labeled%2520annotations.%250ASpecifically%252C%2520we%2520leverage%2520the%2520strong%2520semantic%2520prior%2520within%2520a%25203D%2520generative%250Amodel%2520to%2520train%2520a%2520semantic%2520decoder%252C%2520requiring%2520only%2520a%2520handful%2520of%2520fine-grained%250Alabeled%2520samples.%2520Once%2520trained%252C%2520the%2520decoder%2520efficiently%2520generalizes%2520across%2520the%250Alatent%2520space%252C%2520enabling%2520the%2520generation%2520of%2520infinite%2520data.%2520The%2520generated%2520data%2520is%250Aapplicable%2520across%2520various%2520computer%2520vision%2520tasks%252C%2520including%2520video%2520segmentation%250Aand%25203D%2520point%2520cloud%2520segmentation.%2520Our%2520approach%2520not%2520only%2520surpasses%2520baseline%250Amodels%2520in%2520segmentation%2520quality%252C%2520achieving%2520superior%25203D%2520consistency%2520and%250Asegmentation%2520precision%2520on%2520individual%2520images%252C%2520but%2520also%2520demonstrates%2520versatility%250Aby%2520being%2520applicable%2520to%2520both%2520articulated%2520and%2520non-articulated%2520generative%2520models.%250AFurthermore%252C%2520we%2520explore%2520applications%2520stemming%2520from%2520our%2520approach%252C%2520such%2520as%250A3D-aware%2520semantic%2520editing%2520and%25203D%2520inversion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12063v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DatasetNeRF%3A%20Efficient%203D-aware%20Data%20Factory%20with%20Generative%20Radiance%0A%20%20Fields&entry.906535625=Yu%20Chi%20and%20Fangneng%20Zhan%20and%20Sibo%20Wu%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski&entry.1292438233=%20%20Progress%20in%203D%20computer%20vision%20tasks%20demands%20a%20huge%20amount%20of%20data%2C%20yet%0Aannotating%20multi-view%20images%20with%203D-consistent%20annotations%2C%20or%20point%20clouds%0Awith%20part%20segmentation%20is%20both%20time-consuming%20and%20challenging.%20This%20paper%0Aintroduces%20DatasetNeRF%2C%20a%20novel%20approach%20capable%20of%20generating%20infinite%2C%0Ahigh-quality%203D-consistent%202D%20annotations%20alongside%203D%20point%20cloud%0Asegmentations%2C%20while%20utilizing%20minimal%202D%20human-labeled%20annotations.%0ASpecifically%2C%20we%20leverage%20the%20strong%20semantic%20prior%20within%20a%203D%20generative%0Amodel%20to%20train%20a%20semantic%20decoder%2C%20requiring%20only%20a%20handful%20of%20fine-grained%0Alabeled%20samples.%20Once%20trained%2C%20the%20decoder%20efficiently%20generalizes%20across%20the%0Alatent%20space%2C%20enabling%20the%20generation%20of%20infinite%20data.%20The%20generated%20data%20is%0Aapplicable%20across%20various%20computer%20vision%20tasks%2C%20including%20video%20segmentation%0Aand%203D%20point%20cloud%20segmentation.%20Our%20approach%20not%20only%20surpasses%20baseline%0Amodels%20in%20segmentation%20quality%2C%20achieving%20superior%203D%20consistency%20and%0Asegmentation%20precision%20on%20individual%20images%2C%20but%20also%20demonstrates%20versatility%0Aby%20being%20applicable%20to%20both%20articulated%20and%20non-articulated%20generative%20models.%0AFurthermore%2C%20we%20explore%20applications%20stemming%20from%20our%20approach%2C%20such%20as%0A3D-aware%20semantic%20editing%20and%203D%20inversion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12063v3&entry.124074799=Read"},
{"title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface\n  Reconstruction", "author": "Yifan Wang and Di Huang and Weicai Ye and Guofeng Zhang and Wanli Ouyang and Tong He", "abstract": "  Signed Distance Function (SDF)-based volume rendering has demonstrated\nsignificant capabilities in surface reconstruction. Although promising,\nSDF-based methods often fail to capture detailed geometric structures,\nresulting in visible defects. By comparing SDF-based volume rendering to\ndensity-based volume rendering, we identify two main factors within the\nSDF-based approach that degrade surface quality: SDF-to-density representation\nand geometric regularization. These factors introduce challenges that hinder\nthe optimization of the SDF field. To address these issues, we introduce\nNeuRodin, a novel two-stage neural surface reconstruction framework that not\nonly achieves high-fidelity surface reconstruction but also retains the\nflexible optimization characteristics of density-based methods. NeuRodin\nincorporates innovative strategies that facilitate transformation of arbitrary\ntopologies and reduce artifacts associated with density bias. Extensive\nevaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the\nsuperiority of NeuRodin, showing strong reconstruction capabilities for both\nindoor and outdoor environments using solely posed RGB captures. Project\nwebsite: https://open3dvlab.github.io/NeuRodin/\n", "link": "http://arxiv.org/abs/2408.10178v1", "date": "2024-08-19", "relevancy": 2.9708, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6046}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5947}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuRodin%3A%20A%20Two-stage%20Framework%20for%20High-Fidelity%20Neural%20Surface%0A%20%20Reconstruction&body=Title%3A%20NeuRodin%3A%20A%20Two-stage%20Framework%20for%20High-Fidelity%20Neural%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Yifan%20Wang%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Guofeng%20Zhang%20and%20Wanli%20Ouyang%20and%20Tong%20He%0AAbstract%3A%20%20%20Signed%20Distance%20Function%20%28SDF%29-based%20volume%20rendering%20has%20demonstrated%0Asignificant%20capabilities%20in%20surface%20reconstruction.%20Although%20promising%2C%0ASDF-based%20methods%20often%20fail%20to%20capture%20detailed%20geometric%20structures%2C%0Aresulting%20in%20visible%20defects.%20By%20comparing%20SDF-based%20volume%20rendering%20to%0Adensity-based%20volume%20rendering%2C%20we%20identify%20two%20main%20factors%20within%20the%0ASDF-based%20approach%20that%20degrade%20surface%20quality%3A%20SDF-to-density%20representation%0Aand%20geometric%20regularization.%20These%20factors%20introduce%20challenges%20that%20hinder%0Athe%20optimization%20of%20the%20SDF%20field.%20To%20address%20these%20issues%2C%20we%20introduce%0ANeuRodin%2C%20a%20novel%20two-stage%20neural%20surface%20reconstruction%20framework%20that%20not%0Aonly%20achieves%20high-fidelity%20surface%20reconstruction%20but%20also%20retains%20the%0Aflexible%20optimization%20characteristics%20of%20density-based%20methods.%20NeuRodin%0Aincorporates%20innovative%20strategies%20that%20facilitate%20transformation%20of%20arbitrary%0Atopologies%20and%20reduce%20artifacts%20associated%20with%20density%20bias.%20Extensive%0Aevaluations%20on%20the%20Tanks%20and%20Temples%20and%20ScanNet%2B%2B%20datasets%20demonstrate%20the%0Asuperiority%20of%20NeuRodin%2C%20showing%20strong%20reconstruction%20capabilities%20for%20both%0Aindoor%20and%20outdoor%20environments%20using%20solely%20posed%20RGB%20captures.%20Project%0Awebsite%3A%20https%3A//open3dvlab.github.io/NeuRodin/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuRodin%253A%2520A%2520Two-stage%2520Framework%2520for%2520High-Fidelity%2520Neural%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DYifan%2520Wang%2520and%2520Di%2520Huang%2520and%2520Weicai%2520Ye%2520and%2520Guofeng%2520Zhang%2520and%2520Wanli%2520Ouyang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520Signed%2520Distance%2520Function%2520%2528SDF%2529-based%2520volume%2520rendering%2520has%2520demonstrated%250Asignificant%2520capabilities%2520in%2520surface%2520reconstruction.%2520Although%2520promising%252C%250ASDF-based%2520methods%2520often%2520fail%2520to%2520capture%2520detailed%2520geometric%2520structures%252C%250Aresulting%2520in%2520visible%2520defects.%2520By%2520comparing%2520SDF-based%2520volume%2520rendering%2520to%250Adensity-based%2520volume%2520rendering%252C%2520we%2520identify%2520two%2520main%2520factors%2520within%2520the%250ASDF-based%2520approach%2520that%2520degrade%2520surface%2520quality%253A%2520SDF-to-density%2520representation%250Aand%2520geometric%2520regularization.%2520These%2520factors%2520introduce%2520challenges%2520that%2520hinder%250Athe%2520optimization%2520of%2520the%2520SDF%2520field.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250ANeuRodin%252C%2520a%2520novel%2520two-stage%2520neural%2520surface%2520reconstruction%2520framework%2520that%2520not%250Aonly%2520achieves%2520high-fidelity%2520surface%2520reconstruction%2520but%2520also%2520retains%2520the%250Aflexible%2520optimization%2520characteristics%2520of%2520density-based%2520methods.%2520NeuRodin%250Aincorporates%2520innovative%2520strategies%2520that%2520facilitate%2520transformation%2520of%2520arbitrary%250Atopologies%2520and%2520reduce%2520artifacts%2520associated%2520with%2520density%2520bias.%2520Extensive%250Aevaluations%2520on%2520the%2520Tanks%2520and%2520Temples%2520and%2520ScanNet%252B%252B%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520NeuRodin%252C%2520showing%2520strong%2520reconstruction%2520capabilities%2520for%2520both%250Aindoor%2520and%2520outdoor%2520environments%2520using%2520solely%2520posed%2520RGB%2520captures.%2520Project%250Awebsite%253A%2520https%253A//open3dvlab.github.io/NeuRodin/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuRodin%3A%20A%20Two-stage%20Framework%20for%20High-Fidelity%20Neural%20Surface%0A%20%20Reconstruction&entry.906535625=Yifan%20Wang%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Guofeng%20Zhang%20and%20Wanli%20Ouyang%20and%20Tong%20He&entry.1292438233=%20%20Signed%20Distance%20Function%20%28SDF%29-based%20volume%20rendering%20has%20demonstrated%0Asignificant%20capabilities%20in%20surface%20reconstruction.%20Although%20promising%2C%0ASDF-based%20methods%20often%20fail%20to%20capture%20detailed%20geometric%20structures%2C%0Aresulting%20in%20visible%20defects.%20By%20comparing%20SDF-based%20volume%20rendering%20to%0Adensity-based%20volume%20rendering%2C%20we%20identify%20two%20main%20factors%20within%20the%0ASDF-based%20approach%20that%20degrade%20surface%20quality%3A%20SDF-to-density%20representation%0Aand%20geometric%20regularization.%20These%20factors%20introduce%20challenges%20that%20hinder%0Athe%20optimization%20of%20the%20SDF%20field.%20To%20address%20these%20issues%2C%20we%20introduce%0ANeuRodin%2C%20a%20novel%20two-stage%20neural%20surface%20reconstruction%20framework%20that%20not%0Aonly%20achieves%20high-fidelity%20surface%20reconstruction%20but%20also%20retains%20the%0Aflexible%20optimization%20characteristics%20of%20density-based%20methods.%20NeuRodin%0Aincorporates%20innovative%20strategies%20that%20facilitate%20transformation%20of%20arbitrary%0Atopologies%20and%20reduce%20artifacts%20associated%20with%20density%20bias.%20Extensive%0Aevaluations%20on%20the%20Tanks%20and%20Temples%20and%20ScanNet%2B%2B%20datasets%20demonstrate%20the%0Asuperiority%20of%20NeuRodin%2C%20showing%20strong%20reconstruction%20capabilities%20for%20both%0Aindoor%20and%20outdoor%20environments%20using%20solely%20posed%20RGB%20captures.%20Project%0Awebsite%3A%20https%3A//open3dvlab.github.io/NeuRodin/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10178v1&entry.124074799=Read"},
{"title": "3D-Aware Instance Segmentation and Tracking in Egocentric Videos", "author": "Yash Bhalgat and Vadim Tschernezki and Iro Laina and Jo\u00e3o F. Henriques and Andrea Vedaldi and Andrew Zisserman", "abstract": "  Egocentric videos present unique challenges for 3D scene understanding due to\nrapid camera motion, frequent object occlusions, and limited object visibility.\nThis paper introduces a novel approach to instance segmentation and tracking in\nfirst-person video that leverages 3D awareness to overcome these obstacles. Our\nmethod integrates scene geometry, 3D object centroid tracking, and instance\nsegmentation to create a robust framework for analyzing dynamic egocentric\nscenes. By incorporating spatial and temporal cues, we achieve superior\nperformance compared to state-of-the-art 2D approaches. Extensive evaluations\non the challenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specifically,\nour method outperforms the next best performing approach by $7$ points in\nAssociation Accuracy (AssA) and $4.5$ points in IDF1 score, while reducing the\nnumber of ID switches by $73\\%$ to $80\\%$ across various object categories.\nLeveraging our tracked instance segmentations, we showcase downstream\napplications in 3D object reconstruction and amodal video object segmentation\nin these egocentric settings.\n", "link": "http://arxiv.org/abs/2408.09860v1", "date": "2024-08-19", "relevancy": 2.8149, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5925}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Aware%20Instance%20Segmentation%20and%20Tracking%20in%20Egocentric%20Videos&body=Title%3A%203D-Aware%20Instance%20Segmentation%20and%20Tracking%20in%20Egocentric%20Videos%0AAuthor%3A%20Yash%20Bhalgat%20and%20Vadim%20Tschernezki%20and%20Iro%20Laina%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Andrea%20Vedaldi%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Egocentric%20videos%20present%20unique%20challenges%20for%203D%20scene%20understanding%20due%20to%0Arapid%20camera%20motion%2C%20frequent%20object%20occlusions%2C%20and%20limited%20object%20visibility.%0AThis%20paper%20introduces%20a%20novel%20approach%20to%20instance%20segmentation%20and%20tracking%20in%0Afirst-person%20video%20that%20leverages%203D%20awareness%20to%20overcome%20these%20obstacles.%20Our%0Amethod%20integrates%20scene%20geometry%2C%203D%20object%20centroid%20tracking%2C%20and%20instance%0Asegmentation%20to%20create%20a%20robust%20framework%20for%20analyzing%20dynamic%20egocentric%0Ascenes.%20By%20incorporating%20spatial%20and%20temporal%20cues%2C%20we%20achieve%20superior%0Aperformance%20compared%20to%20state-of-the-art%202D%20approaches.%20Extensive%20evaluations%0Aon%20the%20challenging%20EPIC%20Fields%20dataset%20demonstrate%20significant%20improvements%0Aacross%20a%20range%20of%20tracking%20and%20segmentation%20consistency%20metrics.%20Specifically%2C%0Aour%20method%20outperforms%20the%20next%20best%20performing%20approach%20by%20%247%24%20points%20in%0AAssociation%20Accuracy%20%28AssA%29%20and%20%244.5%24%20points%20in%20IDF1%20score%2C%20while%20reducing%20the%0Anumber%20of%20ID%20switches%20by%20%2473%5C%25%24%20to%20%2480%5C%25%24%20across%20various%20object%20categories.%0ALeveraging%20our%20tracked%20instance%20segmentations%2C%20we%20showcase%20downstream%0Aapplications%20in%203D%20object%20reconstruction%20and%20amodal%20video%20object%20segmentation%0Ain%20these%20egocentric%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Aware%2520Instance%2520Segmentation%2520and%2520Tracking%2520in%2520Egocentric%2520Videos%26entry.906535625%3DYash%2520Bhalgat%2520and%2520Vadim%2520Tschernezki%2520and%2520Iro%2520Laina%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Andrea%2520Vedaldi%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Egocentric%2520videos%2520present%2520unique%2520challenges%2520for%25203D%2520scene%2520understanding%2520due%2520to%250Arapid%2520camera%2520motion%252C%2520frequent%2520object%2520occlusions%252C%2520and%2520limited%2520object%2520visibility.%250AThis%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520instance%2520segmentation%2520and%2520tracking%2520in%250Afirst-person%2520video%2520that%2520leverages%25203D%2520awareness%2520to%2520overcome%2520these%2520obstacles.%2520Our%250Amethod%2520integrates%2520scene%2520geometry%252C%25203D%2520object%2520centroid%2520tracking%252C%2520and%2520instance%250Asegmentation%2520to%2520create%2520a%2520robust%2520framework%2520for%2520analyzing%2520dynamic%2520egocentric%250Ascenes.%2520By%2520incorporating%2520spatial%2520and%2520temporal%2520cues%252C%2520we%2520achieve%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%25202D%2520approaches.%2520Extensive%2520evaluations%250Aon%2520the%2520challenging%2520EPIC%2520Fields%2520dataset%2520demonstrate%2520significant%2520improvements%250Aacross%2520a%2520range%2520of%2520tracking%2520and%2520segmentation%2520consistency%2520metrics.%2520Specifically%252C%250Aour%2520method%2520outperforms%2520the%2520next%2520best%2520performing%2520approach%2520by%2520%25247%2524%2520points%2520in%250AAssociation%2520Accuracy%2520%2528AssA%2529%2520and%2520%25244.5%2524%2520points%2520in%2520IDF1%2520score%252C%2520while%2520reducing%2520the%250Anumber%2520of%2520ID%2520switches%2520by%2520%252473%255C%2525%2524%2520to%2520%252480%255C%2525%2524%2520across%2520various%2520object%2520categories.%250ALeveraging%2520our%2520tracked%2520instance%2520segmentations%252C%2520we%2520showcase%2520downstream%250Aapplications%2520in%25203D%2520object%2520reconstruction%2520and%2520amodal%2520video%2520object%2520segmentation%250Ain%2520these%2520egocentric%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Aware%20Instance%20Segmentation%20and%20Tracking%20in%20Egocentric%20Videos&entry.906535625=Yash%20Bhalgat%20and%20Vadim%20Tschernezki%20and%20Iro%20Laina%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Andrea%20Vedaldi%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Egocentric%20videos%20present%20unique%20challenges%20for%203D%20scene%20understanding%20due%20to%0Arapid%20camera%20motion%2C%20frequent%20object%20occlusions%2C%20and%20limited%20object%20visibility.%0AThis%20paper%20introduces%20a%20novel%20approach%20to%20instance%20segmentation%20and%20tracking%20in%0Afirst-person%20video%20that%20leverages%203D%20awareness%20to%20overcome%20these%20obstacles.%20Our%0Amethod%20integrates%20scene%20geometry%2C%203D%20object%20centroid%20tracking%2C%20and%20instance%0Asegmentation%20to%20create%20a%20robust%20framework%20for%20analyzing%20dynamic%20egocentric%0Ascenes.%20By%20incorporating%20spatial%20and%20temporal%20cues%2C%20we%20achieve%20superior%0Aperformance%20compared%20to%20state-of-the-art%202D%20approaches.%20Extensive%20evaluations%0Aon%20the%20challenging%20EPIC%20Fields%20dataset%20demonstrate%20significant%20improvements%0Aacross%20a%20range%20of%20tracking%20and%20segmentation%20consistency%20metrics.%20Specifically%2C%0Aour%20method%20outperforms%20the%20next%20best%20performing%20approach%20by%20%247%24%20points%20in%0AAssociation%20Accuracy%20%28AssA%29%20and%20%244.5%24%20points%20in%20IDF1%20score%2C%20while%20reducing%20the%0Anumber%20of%20ID%20switches%20by%20%2473%5C%25%24%20to%20%2480%5C%25%24%20across%20various%20object%20categories.%0ALeveraging%20our%20tracked%20instance%20segmentations%2C%20we%20showcase%20downstream%0Aapplications%20in%203D%20object%20reconstruction%20and%20amodal%20video%20object%20segmentation%0Ain%20these%20egocentric%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09860v1&entry.124074799=Read"},
{"title": "Boosting Open-Domain Continual Learning via Leveraging Intra-domain\n  Category-aware Prototype", "author": "Yadong Lu and Shitian Zhao and Boxiang Yun and Dongsheng Jiang and Yin Li and Qingli Li and Yan Wang", "abstract": "  Despite recent progress in enhancing the efficacy of Open-Domain Continual\nLearning (ODCL) in Vision-Language Models (VLM), failing to (1) correctly\nidentify the Task-ID of a test image and (2) use only the category set\ncorresponding to the Task-ID, while preserving the knowledge related to each\ndomain, cannot address the two primary challenges of ODCL: forgetting old\nknowledge and maintaining zero-shot capabilities, as well as the confusions\ncaused by category-relatedness between domains. In this paper, we propose a\nsimple yet effective solution: leveraging intra-domain category-aware\nprototypes for ODCL in CLIP (DPeCLIP), where the prototype is the key to\nbridging the above two processes. Concretely, we propose a training-free\nTask-ID discriminator method, by utilizing prototypes as classifiers for\nidentifying Task-IDs. Furthermore, to maintain the knowledge corresponding to\neach domain, we incorporate intra-domain category-aware prototypes as domain\nprior prompts into the training process. Extensive experiments conducted on 11\ndifferent datasets demonstrate the effectiveness of our approach, achieving\n2.37% and 1.14% average improvement in class-incremental and task-incremental\nsettings, respectively.\n", "link": "http://arxiv.org/abs/2408.09984v1", "date": "2024-08-19", "relevancy": 2.7553, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Open-Domain%20Continual%20Learning%20via%20Leveraging%20Intra-domain%0A%20%20Category-aware%20Prototype&body=Title%3A%20Boosting%20Open-Domain%20Continual%20Learning%20via%20Leveraging%20Intra-domain%0A%20%20Category-aware%20Prototype%0AAuthor%3A%20Yadong%20Lu%20and%20Shitian%20Zhao%20and%20Boxiang%20Yun%20and%20Dongsheng%20Jiang%20and%20Yin%20Li%20and%20Qingli%20Li%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Despite%20recent%20progress%20in%20enhancing%20the%20efficacy%20of%20Open-Domain%20Continual%0ALearning%20%28ODCL%29%20in%20Vision-Language%20Models%20%28VLM%29%2C%20failing%20to%20%281%29%20correctly%0Aidentify%20the%20Task-ID%20of%20a%20test%20image%20and%20%282%29%20use%20only%20the%20category%20set%0Acorresponding%20to%20the%20Task-ID%2C%20while%20preserving%20the%20knowledge%20related%20to%20each%0Adomain%2C%20cannot%20address%20the%20two%20primary%20challenges%20of%20ODCL%3A%20forgetting%20old%0Aknowledge%20and%20maintaining%20zero-shot%20capabilities%2C%20as%20well%20as%20the%20confusions%0Acaused%20by%20category-relatedness%20between%20domains.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20effective%20solution%3A%20leveraging%20intra-domain%20category-aware%0Aprototypes%20for%20ODCL%20in%20CLIP%20%28DPeCLIP%29%2C%20where%20the%20prototype%20is%20the%20key%20to%0Abridging%20the%20above%20two%20processes.%20Concretely%2C%20we%20propose%20a%20training-free%0ATask-ID%20discriminator%20method%2C%20by%20utilizing%20prototypes%20as%20classifiers%20for%0Aidentifying%20Task-IDs.%20Furthermore%2C%20to%20maintain%20the%20knowledge%20corresponding%20to%0Aeach%20domain%2C%20we%20incorporate%20intra-domain%20category-aware%20prototypes%20as%20domain%0Aprior%20prompts%20into%20the%20training%20process.%20Extensive%20experiments%20conducted%20on%2011%0Adifferent%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0A2.37%25%20and%201.14%25%20average%20improvement%20in%20class-incremental%20and%20task-incremental%0Asettings%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Open-Domain%2520Continual%2520Learning%2520via%2520Leveraging%2520Intra-domain%250A%2520%2520Category-aware%2520Prototype%26entry.906535625%3DYadong%2520Lu%2520and%2520Shitian%2520Zhao%2520and%2520Boxiang%2520Yun%2520and%2520Dongsheng%2520Jiang%2520and%2520Yin%2520Li%2520and%2520Qingli%2520Li%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520progress%2520in%2520enhancing%2520the%2520efficacy%2520of%2520Open-Domain%2520Continual%250ALearning%2520%2528ODCL%2529%2520in%2520Vision-Language%2520Models%2520%2528VLM%2529%252C%2520failing%2520to%2520%25281%2529%2520correctly%250Aidentify%2520the%2520Task-ID%2520of%2520a%2520test%2520image%2520and%2520%25282%2529%2520use%2520only%2520the%2520category%2520set%250Acorresponding%2520to%2520the%2520Task-ID%252C%2520while%2520preserving%2520the%2520knowledge%2520related%2520to%2520each%250Adomain%252C%2520cannot%2520address%2520the%2520two%2520primary%2520challenges%2520of%2520ODCL%253A%2520forgetting%2520old%250Aknowledge%2520and%2520maintaining%2520zero-shot%2520capabilities%252C%2520as%2520well%2520as%2520the%2520confusions%250Acaused%2520by%2520category-relatedness%2520between%2520domains.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520solution%253A%2520leveraging%2520intra-domain%2520category-aware%250Aprototypes%2520for%2520ODCL%2520in%2520CLIP%2520%2528DPeCLIP%2529%252C%2520where%2520the%2520prototype%2520is%2520the%2520key%2520to%250Abridging%2520the%2520above%2520two%2520processes.%2520Concretely%252C%2520we%2520propose%2520a%2520training-free%250ATask-ID%2520discriminator%2520method%252C%2520by%2520utilizing%2520prototypes%2520as%2520classifiers%2520for%250Aidentifying%2520Task-IDs.%2520Furthermore%252C%2520to%2520maintain%2520the%2520knowledge%2520corresponding%2520to%250Aeach%2520domain%252C%2520we%2520incorporate%2520intra-domain%2520category-aware%2520prototypes%2520as%2520domain%250Aprior%2520prompts%2520into%2520the%2520training%2520process.%2520Extensive%2520experiments%2520conducted%2520on%252011%250Adifferent%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520achieving%250A2.37%2525%2520and%25201.14%2525%2520average%2520improvement%2520in%2520class-incremental%2520and%2520task-incremental%250Asettings%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Open-Domain%20Continual%20Learning%20via%20Leveraging%20Intra-domain%0A%20%20Category-aware%20Prototype&entry.906535625=Yadong%20Lu%20and%20Shitian%20Zhao%20and%20Boxiang%20Yun%20and%20Dongsheng%20Jiang%20and%20Yin%20Li%20and%20Qingli%20Li%20and%20Yan%20Wang&entry.1292438233=%20%20Despite%20recent%20progress%20in%20enhancing%20the%20efficacy%20of%20Open-Domain%20Continual%0ALearning%20%28ODCL%29%20in%20Vision-Language%20Models%20%28VLM%29%2C%20failing%20to%20%281%29%20correctly%0Aidentify%20the%20Task-ID%20of%20a%20test%20image%20and%20%282%29%20use%20only%20the%20category%20set%0Acorresponding%20to%20the%20Task-ID%2C%20while%20preserving%20the%20knowledge%20related%20to%20each%0Adomain%2C%20cannot%20address%20the%20two%20primary%20challenges%20of%20ODCL%3A%20forgetting%20old%0Aknowledge%20and%20maintaining%20zero-shot%20capabilities%2C%20as%20well%20as%20the%20confusions%0Acaused%20by%20category-relatedness%20between%20domains.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20effective%20solution%3A%20leveraging%20intra-domain%20category-aware%0Aprototypes%20for%20ODCL%20in%20CLIP%20%28DPeCLIP%29%2C%20where%20the%20prototype%20is%20the%20key%20to%0Abridging%20the%20above%20two%20processes.%20Concretely%2C%20we%20propose%20a%20training-free%0ATask-ID%20discriminator%20method%2C%20by%20utilizing%20prototypes%20as%20classifiers%20for%0Aidentifying%20Task-IDs.%20Furthermore%2C%20to%20maintain%20the%20knowledge%20corresponding%20to%0Aeach%20domain%2C%20we%20incorporate%20intra-domain%20category-aware%20prototypes%20as%20domain%0Aprior%20prompts%20into%20the%20training%20process.%20Extensive%20experiments%20conducted%20on%2011%0Adifferent%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0A2.37%25%20and%201.14%25%20average%20improvement%20in%20class-incremental%20and%20task-incremental%0Asettings%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09984v1&entry.124074799=Read"},
{"title": "DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery", "author": "Corentin Dumery and Aoxiang Fan and Ren Li and Nicolas Talabot and Pascal Fua", "abstract": "  Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D\nscenes from multiple images. However, NeRFs remain difficult to segment into\nsemantically meaningful regions. Previous approaches to 3D segmentation of\nNeRFs either require user interaction to isolate a single object, or they rely\non 2D semantic masks with a limited number of classes for supervision. As a\nconsequence, they generalize poorly to class-agnostic masks automatically\ngenerated in real scenes. This is attributable to the ambiguity arising from\nzero-shot segmentation, yielding inconsistent masks across views. In contrast,\nwe propose a method that is robust to inconsistent segmentations and\nsuccessfully decomposes the scene into a set of objects of any class. By\nintroducing a limited number of competing object slots against which masks are\nmatched, a meaningful object representation emerges that best explains the 2D\nsupervision and minimizes an additional regularization term. Our experiments\ndemonstrate the ability of our method to generate 3D panoptic segmentations on\ncomplex scenes, and extract high-quality 3D assets from NeRFs that can then be\nused in virtual 3D environments.\n", "link": "http://arxiv.org/abs/2408.09928v1", "date": "2024-08-19", "relevancy": 2.7129, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5424}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiscoNeRF%3A%20Class-Agnostic%20Object%20Field%20for%203D%20Object%20Discovery&body=Title%3A%20DiscoNeRF%3A%20Class-Agnostic%20Object%20Field%20for%203D%20Object%20Discovery%0AAuthor%3A%20Corentin%20Dumery%20and%20Aoxiang%20Fan%20and%20Ren%20Li%20and%20Nicolas%20Talabot%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20become%20a%20powerful%20tool%20for%20modeling%203D%0Ascenes%20from%20multiple%20images.%20However%2C%20NeRFs%20remain%20difficult%20to%20segment%20into%0Asemantically%20meaningful%20regions.%20Previous%20approaches%20to%203D%20segmentation%20of%0ANeRFs%20either%20require%20user%20interaction%20to%20isolate%20a%20single%20object%2C%20or%20they%20rely%0Aon%202D%20semantic%20masks%20with%20a%20limited%20number%20of%20classes%20for%20supervision.%20As%20a%0Aconsequence%2C%20they%20generalize%20poorly%20to%20class-agnostic%20masks%20automatically%0Agenerated%20in%20real%20scenes.%20This%20is%20attributable%20to%20the%20ambiguity%20arising%20from%0Azero-shot%20segmentation%2C%20yielding%20inconsistent%20masks%20across%20views.%20In%20contrast%2C%0Awe%20propose%20a%20method%20that%20is%20robust%20to%20inconsistent%20segmentations%20and%0Asuccessfully%20decomposes%20the%20scene%20into%20a%20set%20of%20objects%20of%20any%20class.%20By%0Aintroducing%20a%20limited%20number%20of%20competing%20object%20slots%20against%20which%20masks%20are%0Amatched%2C%20a%20meaningful%20object%20representation%20emerges%20that%20best%20explains%20the%202D%0Asupervision%20and%20minimizes%20an%20additional%20regularization%20term.%20Our%20experiments%0Ademonstrate%20the%20ability%20of%20our%20method%20to%20generate%203D%20panoptic%20segmentations%20on%0Acomplex%20scenes%2C%20and%20extract%20high-quality%203D%20assets%20from%20NeRFs%20that%20can%20then%20be%0Aused%20in%20virtual%203D%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscoNeRF%253A%2520Class-Agnostic%2520Object%2520Field%2520for%25203D%2520Object%2520Discovery%26entry.906535625%3DCorentin%2520Dumery%2520and%2520Aoxiang%2520Fan%2520and%2520Ren%2520Li%2520and%2520Nicolas%2520Talabot%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520become%2520a%2520powerful%2520tool%2520for%2520modeling%25203D%250Ascenes%2520from%2520multiple%2520images.%2520However%252C%2520NeRFs%2520remain%2520difficult%2520to%2520segment%2520into%250Asemantically%2520meaningful%2520regions.%2520Previous%2520approaches%2520to%25203D%2520segmentation%2520of%250ANeRFs%2520either%2520require%2520user%2520interaction%2520to%2520isolate%2520a%2520single%2520object%252C%2520or%2520they%2520rely%250Aon%25202D%2520semantic%2520masks%2520with%2520a%2520limited%2520number%2520of%2520classes%2520for%2520supervision.%2520As%2520a%250Aconsequence%252C%2520they%2520generalize%2520poorly%2520to%2520class-agnostic%2520masks%2520automatically%250Agenerated%2520in%2520real%2520scenes.%2520This%2520is%2520attributable%2520to%2520the%2520ambiguity%2520arising%2520from%250Azero-shot%2520segmentation%252C%2520yielding%2520inconsistent%2520masks%2520across%2520views.%2520In%2520contrast%252C%250Awe%2520propose%2520a%2520method%2520that%2520is%2520robust%2520to%2520inconsistent%2520segmentations%2520and%250Asuccessfully%2520decomposes%2520the%2520scene%2520into%2520a%2520set%2520of%2520objects%2520of%2520any%2520class.%2520By%250Aintroducing%2520a%2520limited%2520number%2520of%2520competing%2520object%2520slots%2520against%2520which%2520masks%2520are%250Amatched%252C%2520a%2520meaningful%2520object%2520representation%2520emerges%2520that%2520best%2520explains%2520the%25202D%250Asupervision%2520and%2520minimizes%2520an%2520additional%2520regularization%2520term.%2520Our%2520experiments%250Ademonstrate%2520the%2520ability%2520of%2520our%2520method%2520to%2520generate%25203D%2520panoptic%2520segmentations%2520on%250Acomplex%2520scenes%252C%2520and%2520extract%2520high-quality%25203D%2520assets%2520from%2520NeRFs%2520that%2520can%2520then%2520be%250Aused%2520in%2520virtual%25203D%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiscoNeRF%3A%20Class-Agnostic%20Object%20Field%20for%203D%20Object%20Discovery&entry.906535625=Corentin%20Dumery%20and%20Aoxiang%20Fan%20and%20Ren%20Li%20and%20Nicolas%20Talabot%20and%20Pascal%20Fua&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20become%20a%20powerful%20tool%20for%20modeling%203D%0Ascenes%20from%20multiple%20images.%20However%2C%20NeRFs%20remain%20difficult%20to%20segment%20into%0Asemantically%20meaningful%20regions.%20Previous%20approaches%20to%203D%20segmentation%20of%0ANeRFs%20either%20require%20user%20interaction%20to%20isolate%20a%20single%20object%2C%20or%20they%20rely%0Aon%202D%20semantic%20masks%20with%20a%20limited%20number%20of%20classes%20for%20supervision.%20As%20a%0Aconsequence%2C%20they%20generalize%20poorly%20to%20class-agnostic%20masks%20automatically%0Agenerated%20in%20real%20scenes.%20This%20is%20attributable%20to%20the%20ambiguity%20arising%20from%0Azero-shot%20segmentation%2C%20yielding%20inconsistent%20masks%20across%20views.%20In%20contrast%2C%0Awe%20propose%20a%20method%20that%20is%20robust%20to%20inconsistent%20segmentations%20and%0Asuccessfully%20decomposes%20the%20scene%20into%20a%20set%20of%20objects%20of%20any%20class.%20By%0Aintroducing%20a%20limited%20number%20of%20competing%20object%20slots%20against%20which%20masks%20are%0Amatched%2C%20a%20meaningful%20object%20representation%20emerges%20that%20best%20explains%20the%202D%0Asupervision%20and%20minimizes%20an%20additional%20regularization%20term.%20Our%20experiments%0Ademonstrate%20the%20ability%20of%20our%20method%20to%20generate%203D%20panoptic%20segmentations%20on%0Acomplex%20scenes%2C%20and%20extract%20high-quality%203D%20assets%20from%20NeRFs%20that%20can%20then%20be%0Aused%20in%20virtual%203D%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09928v1&entry.124074799=Read"},
{"title": "Structure-preserving Image Translation for Depth Estimation in\n  Colonoscopy Video", "author": "Shuxian Wang and Akshay Paruchuri and Zhaoxi Zhang and Sarah McGill and Roni Sengupta", "abstract": "  Monocular depth estimation in colonoscopy video aims to overcome the unusual\nlighting properties of the colonoscopic environment. One of the major\nchallenges in this area is the domain gap between annotated but unrealistic\nsynthetic data and unannotated but realistic clinical data. Previous attempts\nto bridge this domain gap directly target the depth estimation task itself. We\npropose a general pipeline of structure-preserving synthetic-to-real (sim2real)\nimage translation (producing a modified version of the input image) to retain\ndepth geometry through the translation process. This allows us to generate\nlarge quantities of realistic-looking synthetic images for supervised depth\nestimation with improved generalization to the clinical domain. We also propose\na dataset of hand-picked sequences from clinical colonoscopies to improve the\nimage translation process. We demonstrate the simultaneous realism of the\ntranslated images and preservation of depth maps via the performance of\ndownstream depth estimation on various datasets.\n", "link": "http://arxiv.org/abs/2408.10153v1", "date": "2024-08-19", "relevancy": 2.6737, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20Image%20Translation%20for%20Depth%20Estimation%20in%0A%20%20Colonoscopy%20Video&body=Title%3A%20Structure-preserving%20Image%20Translation%20for%20Depth%20Estimation%20in%0A%20%20Colonoscopy%20Video%0AAuthor%3A%20Shuxian%20Wang%20and%20Akshay%20Paruchuri%20and%20Zhaoxi%20Zhang%20and%20Sarah%20McGill%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20in%20colonoscopy%20video%20aims%20to%20overcome%20the%20unusual%0Alighting%20properties%20of%20the%20colonoscopic%20environment.%20One%20of%20the%20major%0Achallenges%20in%20this%20area%20is%20the%20domain%20gap%20between%20annotated%20but%20unrealistic%0Asynthetic%20data%20and%20unannotated%20but%20realistic%20clinical%20data.%20Previous%20attempts%0Ato%20bridge%20this%20domain%20gap%20directly%20target%20the%20depth%20estimation%20task%20itself.%20We%0Apropose%20a%20general%20pipeline%20of%20structure-preserving%20synthetic-to-real%20%28sim2real%29%0Aimage%20translation%20%28producing%20a%20modified%20version%20of%20the%20input%20image%29%20to%20retain%0Adepth%20geometry%20through%20the%20translation%20process.%20This%20allows%20us%20to%20generate%0Alarge%20quantities%20of%20realistic-looking%20synthetic%20images%20for%20supervised%20depth%0Aestimation%20with%20improved%20generalization%20to%20the%20clinical%20domain.%20We%20also%20propose%0Aa%20dataset%20of%20hand-picked%20sequences%20from%20clinical%20colonoscopies%20to%20improve%20the%0Aimage%20translation%20process.%20We%20demonstrate%20the%20simultaneous%20realism%20of%20the%0Atranslated%20images%20and%20preservation%20of%20depth%20maps%20via%20the%20performance%20of%0Adownstream%20depth%20estimation%20on%20various%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520Image%2520Translation%2520for%2520Depth%2520Estimation%2520in%250A%2520%2520Colonoscopy%2520Video%26entry.906535625%3DShuxian%2520Wang%2520and%2520Akshay%2520Paruchuri%2520and%2520Zhaoxi%2520Zhang%2520and%2520Sarah%2520McGill%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520in%2520colonoscopy%2520video%2520aims%2520to%2520overcome%2520the%2520unusual%250Alighting%2520properties%2520of%2520the%2520colonoscopic%2520environment.%2520One%2520of%2520the%2520major%250Achallenges%2520in%2520this%2520area%2520is%2520the%2520domain%2520gap%2520between%2520annotated%2520but%2520unrealistic%250Asynthetic%2520data%2520and%2520unannotated%2520but%2520realistic%2520clinical%2520data.%2520Previous%2520attempts%250Ato%2520bridge%2520this%2520domain%2520gap%2520directly%2520target%2520the%2520depth%2520estimation%2520task%2520itself.%2520We%250Apropose%2520a%2520general%2520pipeline%2520of%2520structure-preserving%2520synthetic-to-real%2520%2528sim2real%2529%250Aimage%2520translation%2520%2528producing%2520a%2520modified%2520version%2520of%2520the%2520input%2520image%2529%2520to%2520retain%250Adepth%2520geometry%2520through%2520the%2520translation%2520process.%2520This%2520allows%2520us%2520to%2520generate%250Alarge%2520quantities%2520of%2520realistic-looking%2520synthetic%2520images%2520for%2520supervised%2520depth%250Aestimation%2520with%2520improved%2520generalization%2520to%2520the%2520clinical%2520domain.%2520We%2520also%2520propose%250Aa%2520dataset%2520of%2520hand-picked%2520sequences%2520from%2520clinical%2520colonoscopies%2520to%2520improve%2520the%250Aimage%2520translation%2520process.%2520We%2520demonstrate%2520the%2520simultaneous%2520realism%2520of%2520the%250Atranslated%2520images%2520and%2520preservation%2520of%2520depth%2520maps%2520via%2520the%2520performance%2520of%250Adownstream%2520depth%2520estimation%2520on%2520various%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20Image%20Translation%20for%20Depth%20Estimation%20in%0A%20%20Colonoscopy%20Video&entry.906535625=Shuxian%20Wang%20and%20Akshay%20Paruchuri%20and%20Zhaoxi%20Zhang%20and%20Sarah%20McGill%20and%20Roni%20Sengupta&entry.1292438233=%20%20Monocular%20depth%20estimation%20in%20colonoscopy%20video%20aims%20to%20overcome%20the%20unusual%0Alighting%20properties%20of%20the%20colonoscopic%20environment.%20One%20of%20the%20major%0Achallenges%20in%20this%20area%20is%20the%20domain%20gap%20between%20annotated%20but%20unrealistic%0Asynthetic%20data%20and%20unannotated%20but%20realistic%20clinical%20data.%20Previous%20attempts%0Ato%20bridge%20this%20domain%20gap%20directly%20target%20the%20depth%20estimation%20task%20itself.%20We%0Apropose%20a%20general%20pipeline%20of%20structure-preserving%20synthetic-to-real%20%28sim2real%29%0Aimage%20translation%20%28producing%20a%20modified%20version%20of%20the%20input%20image%29%20to%20retain%0Adepth%20geometry%20through%20the%20translation%20process.%20This%20allows%20us%20to%20generate%0Alarge%20quantities%20of%20realistic-looking%20synthetic%20images%20for%20supervised%20depth%0Aestimation%20with%20improved%20generalization%20to%20the%20clinical%20domain.%20We%20also%20propose%0Aa%20dataset%20of%20hand-picked%20sequences%20from%20clinical%20colonoscopies%20to%20improve%20the%0Aimage%20translation%20process.%20We%20demonstrate%20the%20simultaneous%20realism%20of%20the%0Atranslated%20images%20and%20preservation%20of%20depth%20maps%20via%20the%20performance%20of%0Adownstream%20depth%20estimation%20on%20various%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10153v1&entry.124074799=Read"},
{"title": "SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for\n  Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition", "author": "Wiktor Mucha and Michael Wray and Martin Kampel", "abstract": "  Hand pose represents key information for action recognition in the egocentric\nperspective, where the user is interacting with objects. We propose to improve\negocentric 3D hand pose estimation based on RGB frames only by using\npseudo-depth images. Incorporating state-of-the-art single RGB image depth\nestimation techniques, we generate pseudo-depth representations of the frames\nand use distance knowledge to segment irrelevant parts of the scene. The\nresulting depth maps are then used as segmentation masks for the RGB frames.\nExperimental results on H2O Dataset confirm the high accuracy of the estimated\npose with our method in an action recognition task. The 3D hand pose, together\nwith information from object detection, is processed by a transformer-based\naction recognition network, resulting in an accuracy of 91.73%, outperforming\nall state-of-the-art methods. Estimations of 3D hand pose result in competitive\nperformance with existing methods with a mean pose error of 28.66 mm. This\nmethod opens up new possibilities for employing distance information in\negocentric 3D hand pose estimation without relying on depth sensors.\n", "link": "http://arxiv.org/abs/2408.10037v1", "date": "2024-08-19", "relevancy": 2.6578, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5407}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5327}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHARP%3A%20Segmentation%20of%20Hands%20and%20Arms%20by%20Range%20using%20Pseudo-Depth%20for%0A%20%20Enhanced%20Egocentric%203D%20Hand%20Pose%20Estimation%20and%20Action%20Recognition&body=Title%3A%20SHARP%3A%20Segmentation%20of%20Hands%20and%20Arms%20by%20Range%20using%20Pseudo-Depth%20for%0A%20%20Enhanced%20Egocentric%203D%20Hand%20Pose%20Estimation%20and%20Action%20Recognition%0AAuthor%3A%20Wiktor%20Mucha%20and%20Michael%20Wray%20and%20Martin%20Kampel%0AAbstract%3A%20%20%20Hand%20pose%20represents%20key%20information%20for%20action%20recognition%20in%20the%20egocentric%0Aperspective%2C%20where%20the%20user%20is%20interacting%20with%20objects.%20We%20propose%20to%20improve%0Aegocentric%203D%20hand%20pose%20estimation%20based%20on%20RGB%20frames%20only%20by%20using%0Apseudo-depth%20images.%20Incorporating%20state-of-the-art%20single%20RGB%20image%20depth%0Aestimation%20techniques%2C%20we%20generate%20pseudo-depth%20representations%20of%20the%20frames%0Aand%20use%20distance%20knowledge%20to%20segment%20irrelevant%20parts%20of%20the%20scene.%20The%0Aresulting%20depth%20maps%20are%20then%20used%20as%20segmentation%20masks%20for%20the%20RGB%20frames.%0AExperimental%20results%20on%20H2O%20Dataset%20confirm%20the%20high%20accuracy%20of%20the%20estimated%0Apose%20with%20our%20method%20in%20an%20action%20recognition%20task.%20The%203D%20hand%20pose%2C%20together%0Awith%20information%20from%20object%20detection%2C%20is%20processed%20by%20a%20transformer-based%0Aaction%20recognition%20network%2C%20resulting%20in%20an%20accuracy%20of%2091.73%25%2C%20outperforming%0Aall%20state-of-the-art%20methods.%20Estimations%20of%203D%20hand%20pose%20result%20in%20competitive%0Aperformance%20with%20existing%20methods%20with%20a%20mean%20pose%20error%20of%2028.66%20mm.%20This%0Amethod%20opens%20up%20new%20possibilities%20for%20employing%20distance%20information%20in%0Aegocentric%203D%20hand%20pose%20estimation%20without%20relying%20on%20depth%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHARP%253A%2520Segmentation%2520of%2520Hands%2520and%2520Arms%2520by%2520Range%2520using%2520Pseudo-Depth%2520for%250A%2520%2520Enhanced%2520Egocentric%25203D%2520Hand%2520Pose%2520Estimation%2520and%2520Action%2520Recognition%26entry.906535625%3DWiktor%2520Mucha%2520and%2520Michael%2520Wray%2520and%2520Martin%2520Kampel%26entry.1292438233%3D%2520%2520Hand%2520pose%2520represents%2520key%2520information%2520for%2520action%2520recognition%2520in%2520the%2520egocentric%250Aperspective%252C%2520where%2520the%2520user%2520is%2520interacting%2520with%2520objects.%2520We%2520propose%2520to%2520improve%250Aegocentric%25203D%2520hand%2520pose%2520estimation%2520based%2520on%2520RGB%2520frames%2520only%2520by%2520using%250Apseudo-depth%2520images.%2520Incorporating%2520state-of-the-art%2520single%2520RGB%2520image%2520depth%250Aestimation%2520techniques%252C%2520we%2520generate%2520pseudo-depth%2520representations%2520of%2520the%2520frames%250Aand%2520use%2520distance%2520knowledge%2520to%2520segment%2520irrelevant%2520parts%2520of%2520the%2520scene.%2520The%250Aresulting%2520depth%2520maps%2520are%2520then%2520used%2520as%2520segmentation%2520masks%2520for%2520the%2520RGB%2520frames.%250AExperimental%2520results%2520on%2520H2O%2520Dataset%2520confirm%2520the%2520high%2520accuracy%2520of%2520the%2520estimated%250Apose%2520with%2520our%2520method%2520in%2520an%2520action%2520recognition%2520task.%2520The%25203D%2520hand%2520pose%252C%2520together%250Awith%2520information%2520from%2520object%2520detection%252C%2520is%2520processed%2520by%2520a%2520transformer-based%250Aaction%2520recognition%2520network%252C%2520resulting%2520in%2520an%2520accuracy%2520of%252091.73%2525%252C%2520outperforming%250Aall%2520state-of-the-art%2520methods.%2520Estimations%2520of%25203D%2520hand%2520pose%2520result%2520in%2520competitive%250Aperformance%2520with%2520existing%2520methods%2520with%2520a%2520mean%2520pose%2520error%2520of%252028.66%2520mm.%2520This%250Amethod%2520opens%2520up%2520new%2520possibilities%2520for%2520employing%2520distance%2520information%2520in%250Aegocentric%25203D%2520hand%2520pose%2520estimation%2520without%2520relying%2520on%2520depth%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHARP%3A%20Segmentation%20of%20Hands%20and%20Arms%20by%20Range%20using%20Pseudo-Depth%20for%0A%20%20Enhanced%20Egocentric%203D%20Hand%20Pose%20Estimation%20and%20Action%20Recognition&entry.906535625=Wiktor%20Mucha%20and%20Michael%20Wray%20and%20Martin%20Kampel&entry.1292438233=%20%20Hand%20pose%20represents%20key%20information%20for%20action%20recognition%20in%20the%20egocentric%0Aperspective%2C%20where%20the%20user%20is%20interacting%20with%20objects.%20We%20propose%20to%20improve%0Aegocentric%203D%20hand%20pose%20estimation%20based%20on%20RGB%20frames%20only%20by%20using%0Apseudo-depth%20images.%20Incorporating%20state-of-the-art%20single%20RGB%20image%20depth%0Aestimation%20techniques%2C%20we%20generate%20pseudo-depth%20representations%20of%20the%20frames%0Aand%20use%20distance%20knowledge%20to%20segment%20irrelevant%20parts%20of%20the%20scene.%20The%0Aresulting%20depth%20maps%20are%20then%20used%20as%20segmentation%20masks%20for%20the%20RGB%20frames.%0AExperimental%20results%20on%20H2O%20Dataset%20confirm%20the%20high%20accuracy%20of%20the%20estimated%0Apose%20with%20our%20method%20in%20an%20action%20recognition%20task.%20The%203D%20hand%20pose%2C%20together%0Awith%20information%20from%20object%20detection%2C%20is%20processed%20by%20a%20transformer-based%0Aaction%20recognition%20network%2C%20resulting%20in%20an%20accuracy%20of%2091.73%25%2C%20outperforming%0Aall%20state-of-the-art%20methods.%20Estimations%20of%203D%20hand%20pose%20result%20in%20competitive%0Aperformance%20with%20existing%20methods%20with%20a%20mean%20pose%20error%20of%2028.66%20mm.%20This%0Amethod%20opens%20up%20new%20possibilities%20for%20employing%20distance%20information%20in%0Aegocentric%203D%20hand%20pose%20estimation%20without%20relying%20on%20depth%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10037v1&entry.124074799=Read"},
{"title": "Exploiting Fine-Grained Prototype Distribution for Boosting Unsupervised\n  Class Incremental Learning", "author": "Jiaming Liu and Hongyuan Liu and Zhili Qin and Wei Han and Yulu Fan and Qinli Yang and Junming Shao", "abstract": "  The dynamic nature of open-world scenarios has attracted more attention to\nclass incremental learning (CIL). However, existing CIL methods typically\npresume the availability of complete ground-truth labels throughout the\ntraining process, an assumption rarely met in practical applications.\nConsequently, this paper explores a more challenging problem of unsupervised\nclass incremental learning (UCIL). The essence of addressing this problem lies\nin effectively capturing comprehensive feature representations and discovering\nunknown novel classes. To achieve this, we first model the knowledge of class\ndistribution by exploiting fine-grained prototypes. Subsequently, a granularity\nalignment technique is introduced to enhance the unsupervised class discovery.\nAdditionally, we proposed a strategy to minimize overlap between novel and\nexisting classes, thereby preserving historical knowledge and mitigating the\nphenomenon of catastrophic forgetting. Extensive experiments on the five\ndatasets demonstrate that our approach significantly outperforms current\nstate-of-the-art methods, indicating the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2408.10046v1", "date": "2024-08-19", "relevancy": 2.6438, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5219}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Fine-Grained%20Prototype%20Distribution%20for%20Boosting%20Unsupervised%0A%20%20Class%20Incremental%20Learning&body=Title%3A%20Exploiting%20Fine-Grained%20Prototype%20Distribution%20for%20Boosting%20Unsupervised%0A%20%20Class%20Incremental%20Learning%0AAuthor%3A%20Jiaming%20Liu%20and%20Hongyuan%20Liu%20and%20Zhili%20Qin%20and%20Wei%20Han%20and%20Yulu%20Fan%20and%20Qinli%20Yang%20and%20Junming%20Shao%0AAbstract%3A%20%20%20The%20dynamic%20nature%20of%20open-world%20scenarios%20has%20attracted%20more%20attention%20to%0Aclass%20incremental%20learning%20%28CIL%29.%20However%2C%20existing%20CIL%20methods%20typically%0Apresume%20the%20availability%20of%20complete%20ground-truth%20labels%20throughout%20the%0Atraining%20process%2C%20an%20assumption%20rarely%20met%20in%20practical%20applications.%0AConsequently%2C%20this%20paper%20explores%20a%20more%20challenging%20problem%20of%20unsupervised%0Aclass%20incremental%20learning%20%28UCIL%29.%20The%20essence%20of%20addressing%20this%20problem%20lies%0Ain%20effectively%20capturing%20comprehensive%20feature%20representations%20and%20discovering%0Aunknown%20novel%20classes.%20To%20achieve%20this%2C%20we%20first%20model%20the%20knowledge%20of%20class%0Adistribution%20by%20exploiting%20fine-grained%20prototypes.%20Subsequently%2C%20a%20granularity%0Aalignment%20technique%20is%20introduced%20to%20enhance%20the%20unsupervised%20class%20discovery.%0AAdditionally%2C%20we%20proposed%20a%20strategy%20to%20minimize%20overlap%20between%20novel%20and%0Aexisting%20classes%2C%20thereby%20preserving%20historical%20knowledge%20and%20mitigating%20the%0Aphenomenon%20of%20catastrophic%20forgetting.%20Extensive%20experiments%20on%20the%20five%0Adatasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20current%0Astate-of-the-art%20methods%2C%20indicating%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Fine-Grained%2520Prototype%2520Distribution%2520for%2520Boosting%2520Unsupervised%250A%2520%2520Class%2520Incremental%2520Learning%26entry.906535625%3DJiaming%2520Liu%2520and%2520Hongyuan%2520Liu%2520and%2520Zhili%2520Qin%2520and%2520Wei%2520Han%2520and%2520Yulu%2520Fan%2520and%2520Qinli%2520Yang%2520and%2520Junming%2520Shao%26entry.1292438233%3D%2520%2520The%2520dynamic%2520nature%2520of%2520open-world%2520scenarios%2520has%2520attracted%2520more%2520attention%2520to%250Aclass%2520incremental%2520learning%2520%2528CIL%2529.%2520However%252C%2520existing%2520CIL%2520methods%2520typically%250Apresume%2520the%2520availability%2520of%2520complete%2520ground-truth%2520labels%2520throughout%2520the%250Atraining%2520process%252C%2520an%2520assumption%2520rarely%2520met%2520in%2520practical%2520applications.%250AConsequently%252C%2520this%2520paper%2520explores%2520a%2520more%2520challenging%2520problem%2520of%2520unsupervised%250Aclass%2520incremental%2520learning%2520%2528UCIL%2529.%2520The%2520essence%2520of%2520addressing%2520this%2520problem%2520lies%250Ain%2520effectively%2520capturing%2520comprehensive%2520feature%2520representations%2520and%2520discovering%250Aunknown%2520novel%2520classes.%2520To%2520achieve%2520this%252C%2520we%2520first%2520model%2520the%2520knowledge%2520of%2520class%250Adistribution%2520by%2520exploiting%2520fine-grained%2520prototypes.%2520Subsequently%252C%2520a%2520granularity%250Aalignment%2520technique%2520is%2520introduced%2520to%2520enhance%2520the%2520unsupervised%2520class%2520discovery.%250AAdditionally%252C%2520we%2520proposed%2520a%2520strategy%2520to%2520minimize%2520overlap%2520between%2520novel%2520and%250Aexisting%2520classes%252C%2520thereby%2520preserving%2520historical%2520knowledge%2520and%2520mitigating%2520the%250Aphenomenon%2520of%2520catastrophic%2520forgetting.%2520Extensive%2520experiments%2520on%2520the%2520five%250Adatasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520current%250Astate-of-the-art%2520methods%252C%2520indicating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Fine-Grained%20Prototype%20Distribution%20for%20Boosting%20Unsupervised%0A%20%20Class%20Incremental%20Learning&entry.906535625=Jiaming%20Liu%20and%20Hongyuan%20Liu%20and%20Zhili%20Qin%20and%20Wei%20Han%20and%20Yulu%20Fan%20and%20Qinli%20Yang%20and%20Junming%20Shao&entry.1292438233=%20%20The%20dynamic%20nature%20of%20open-world%20scenarios%20has%20attracted%20more%20attention%20to%0Aclass%20incremental%20learning%20%28CIL%29.%20However%2C%20existing%20CIL%20methods%20typically%0Apresume%20the%20availability%20of%20complete%20ground-truth%20labels%20throughout%20the%0Atraining%20process%2C%20an%20assumption%20rarely%20met%20in%20practical%20applications.%0AConsequently%2C%20this%20paper%20explores%20a%20more%20challenging%20problem%20of%20unsupervised%0Aclass%20incremental%20learning%20%28UCIL%29.%20The%20essence%20of%20addressing%20this%20problem%20lies%0Ain%20effectively%20capturing%20comprehensive%20feature%20representations%20and%20discovering%0Aunknown%20novel%20classes.%20To%20achieve%20this%2C%20we%20first%20model%20the%20knowledge%20of%20class%0Adistribution%20by%20exploiting%20fine-grained%20prototypes.%20Subsequently%2C%20a%20granularity%0Aalignment%20technique%20is%20introduced%20to%20enhance%20the%20unsupervised%20class%20discovery.%0AAdditionally%2C%20we%20proposed%20a%20strategy%20to%20minimize%20overlap%20between%20novel%20and%0Aexisting%20classes%2C%20thereby%20preserving%20historical%20knowledge%20and%20mitigating%20the%0Aphenomenon%20of%20catastrophic%20forgetting.%20Extensive%20experiments%20on%20the%20five%0Adatasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20current%0Astate-of-the-art%20methods%2C%20indicating%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10046v1&entry.124074799=Read"},
{"title": "Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel\n  Segmentation", "author": "Patryk Rygiel and Dieuwertje Alblas and Christoph Brune and Kak Khee Yeung and Jelmer M. Wolterink", "abstract": "  Personalized 3D vascular models can aid in a range of diagnostic, prognostic,\nand treatment-planning tasks relevant to cardiovascular disease management.\nDeep learning provides a means to obtain such models automatically from image\ndata. Ideally, a user should have control over the included region in the\nvascular model. Additionally, the model should be watertight and highly\naccurate. To this end, we propose a combination of a global controller\nleveraging voxel mask segmentations to provide boundary conditions for vessels\nof interest to a local, iterative vessel segmentation model. We introduce the\npreservation of scale- and rotational symmetries in the local segmentation\nmodel, leading to generalisation to vessels of unseen sizes and orientations.\nCombined with the global controller, this enables flexible 3D vascular model\nbuilding, without additional retraining. We demonstrate the potential of our\nmethod on a dataset containing abdominal aortic aneurysms (AAAs). Our method\nperforms on par with a state-of-the-art segmentation model in the segmentation\nof AAAs, iliac arteries, and renal arteries, while providing a watertight,\nsmooth surface representation. Moreover, we demonstrate that by adapting the\nglobal controller, we can easily extend vessel sections in the 3D model.\n", "link": "http://arxiv.org/abs/2403.15314v2", "date": "2024-08-19", "relevancy": 2.6106, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation&body=Title%3A%20Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation%0AAuthor%3A%20Patryk%20Rygiel%20and%20Dieuwertje%20Alblas%20and%20Christoph%20Brune%20and%20Kak%20Khee%20Yeung%20and%20Jelmer%20M.%20Wolterink%0AAbstract%3A%20%20%20Personalized%203D%20vascular%20models%20can%20aid%20in%20a%20range%20of%20diagnostic%2C%20prognostic%2C%0Aand%20treatment-planning%20tasks%20relevant%20to%20cardiovascular%20disease%20management.%0ADeep%20learning%20provides%20a%20means%20to%20obtain%20such%20models%20automatically%20from%20image%0Adata.%20Ideally%2C%20a%20user%20should%20have%20control%20over%20the%20included%20region%20in%20the%0Avascular%20model.%20Additionally%2C%20the%20model%20should%20be%20watertight%20and%20highly%0Aaccurate.%20To%20this%20end%2C%20we%20propose%20a%20combination%20of%20a%20global%20controller%0Aleveraging%20voxel%20mask%20segmentations%20to%20provide%20boundary%20conditions%20for%20vessels%0Aof%20interest%20to%20a%20local%2C%20iterative%20vessel%20segmentation%20model.%20We%20introduce%20the%0Apreservation%20of%20scale-%20and%20rotational%20symmetries%20in%20the%20local%20segmentation%0Amodel%2C%20leading%20to%20generalisation%20to%20vessels%20of%20unseen%20sizes%20and%20orientations.%0ACombined%20with%20the%20global%20controller%2C%20this%20enables%20flexible%203D%20vascular%20model%0Abuilding%2C%20without%20additional%20retraining.%20We%20demonstrate%20the%20potential%20of%20our%0Amethod%20on%20a%20dataset%20containing%20abdominal%20aortic%20aneurysms%20%28AAAs%29.%20Our%20method%0Aperforms%20on%20par%20with%20a%20state-of-the-art%20segmentation%20model%20in%20the%20segmentation%0Aof%20AAAs%2C%20iliac%20arteries%2C%20and%20renal%20arteries%2C%20while%20providing%20a%20watertight%2C%0Asmooth%20surface%20representation.%20Moreover%2C%20we%20demonstrate%20that%20by%20adapting%20the%0Aglobal%20controller%2C%20we%20can%20easily%20extend%20vessel%20sections%20in%20the%203D%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Control%2520for%2520Local%2520SO%25283%2529-Equivariant%2520Scale-Invariant%2520Vessel%250A%2520%2520Segmentation%26entry.906535625%3DPatryk%2520Rygiel%2520and%2520Dieuwertje%2520Alblas%2520and%2520Christoph%2520Brune%2520and%2520Kak%2520Khee%2520Yeung%2520and%2520Jelmer%2520M.%2520Wolterink%26entry.1292438233%3D%2520%2520Personalized%25203D%2520vascular%2520models%2520can%2520aid%2520in%2520a%2520range%2520of%2520diagnostic%252C%2520prognostic%252C%250Aand%2520treatment-planning%2520tasks%2520relevant%2520to%2520cardiovascular%2520disease%2520management.%250ADeep%2520learning%2520provides%2520a%2520means%2520to%2520obtain%2520such%2520models%2520automatically%2520from%2520image%250Adata.%2520Ideally%252C%2520a%2520user%2520should%2520have%2520control%2520over%2520the%2520included%2520region%2520in%2520the%250Avascular%2520model.%2520Additionally%252C%2520the%2520model%2520should%2520be%2520watertight%2520and%2520highly%250Aaccurate.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520combination%2520of%2520a%2520global%2520controller%250Aleveraging%2520voxel%2520mask%2520segmentations%2520to%2520provide%2520boundary%2520conditions%2520for%2520vessels%250Aof%2520interest%2520to%2520a%2520local%252C%2520iterative%2520vessel%2520segmentation%2520model.%2520We%2520introduce%2520the%250Apreservation%2520of%2520scale-%2520and%2520rotational%2520symmetries%2520in%2520the%2520local%2520segmentation%250Amodel%252C%2520leading%2520to%2520generalisation%2520to%2520vessels%2520of%2520unseen%2520sizes%2520and%2520orientations.%250ACombined%2520with%2520the%2520global%2520controller%252C%2520this%2520enables%2520flexible%25203D%2520vascular%2520model%250Abuilding%252C%2520without%2520additional%2520retraining.%2520We%2520demonstrate%2520the%2520potential%2520of%2520our%250Amethod%2520on%2520a%2520dataset%2520containing%2520abdominal%2520aortic%2520aneurysms%2520%2528AAAs%2529.%2520Our%2520method%250Aperforms%2520on%2520par%2520with%2520a%2520state-of-the-art%2520segmentation%2520model%2520in%2520the%2520segmentation%250Aof%2520AAAs%252C%2520iliac%2520arteries%252C%2520and%2520renal%2520arteries%252C%2520while%2520providing%2520a%2520watertight%252C%250Asmooth%2520surface%2520representation.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520by%2520adapting%2520the%250Aglobal%2520controller%252C%2520we%2520can%2520easily%2520extend%2520vessel%2520sections%2520in%2520the%25203D%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation&entry.906535625=Patryk%20Rygiel%20and%20Dieuwertje%20Alblas%20and%20Christoph%20Brune%20and%20Kak%20Khee%20Yeung%20and%20Jelmer%20M.%20Wolterink&entry.1292438233=%20%20Personalized%203D%20vascular%20models%20can%20aid%20in%20a%20range%20of%20diagnostic%2C%20prognostic%2C%0Aand%20treatment-planning%20tasks%20relevant%20to%20cardiovascular%20disease%20management.%0ADeep%20learning%20provides%20a%20means%20to%20obtain%20such%20models%20automatically%20from%20image%0Adata.%20Ideally%2C%20a%20user%20should%20have%20control%20over%20the%20included%20region%20in%20the%0Avascular%20model.%20Additionally%2C%20the%20model%20should%20be%20watertight%20and%20highly%0Aaccurate.%20To%20this%20end%2C%20we%20propose%20a%20combination%20of%20a%20global%20controller%0Aleveraging%20voxel%20mask%20segmentations%20to%20provide%20boundary%20conditions%20for%20vessels%0Aof%20interest%20to%20a%20local%2C%20iterative%20vessel%20segmentation%20model.%20We%20introduce%20the%0Apreservation%20of%20scale-%20and%20rotational%20symmetries%20in%20the%20local%20segmentation%0Amodel%2C%20leading%20to%20generalisation%20to%20vessels%20of%20unseen%20sizes%20and%20orientations.%0ACombined%20with%20the%20global%20controller%2C%20this%20enables%20flexible%203D%20vascular%20model%0Abuilding%2C%20without%20additional%20retraining.%20We%20demonstrate%20the%20potential%20of%20our%0Amethod%20on%20a%20dataset%20containing%20abdominal%20aortic%20aneurysms%20%28AAAs%29.%20Our%20method%0Aperforms%20on%20par%20with%20a%20state-of-the-art%20segmentation%20model%20in%20the%20segmentation%0Aof%20AAAs%2C%20iliac%20arteries%2C%20and%20renal%20arteries%2C%20while%20providing%20a%20watertight%2C%0Asmooth%20surface%20representation.%20Moreover%2C%20we%20demonstrate%20that%20by%20adapting%20the%0Aglobal%20controller%2C%20we%20can%20easily%20extend%20vessel%20sections%20in%20the%203D%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15314v2&entry.124074799=Read"},
{"title": "Factorized-Dreamer: Training A High-Quality Video Generator with Limited\n  and Low-Quality Data", "author": "Tao Yang and Yangming Shi and Yunwen Huang and Feng Chen and Yin Zheng and Lei Zhang", "abstract": "  Text-to-video (T2V) generation has gained significant attention due to its\nwide applications to video generation, editing, enhancement and translation,\n\\etc. However, high-quality (HQ) video synthesis is extremely challenging\nbecause of the diverse and complex motions existed in real world. Most existing\nworks struggle to address this problem by collecting large-scale HQ videos,\nwhich are inaccessible to the community. In this work, we show that publicly\navailable limited and low-quality (LQ) data are sufficient to train a HQ video\ngenerator without recaptioning or finetuning. We factorize the whole T2V\ngeneration process into two steps: generating an image conditioned on a highly\ndescriptive caption, and synthesizing the video conditioned on the generated\nimage and a concise caption of motion details. Specifically, we present\n\\emph{Factorized-Dreamer}, a factorized spatiotemporal framework with several\ncritical designs for T2V generation, including an adapter to combine text and\nimage embeddings, a pixel-aware cross attention module to capture pixel-level\nimage information, a T5 text encoder to better understand motion description,\nand a PredictNet to supervise optical flows. We further present a noise\nschedule, which plays a key role in ensuring the quality and stability of video\ngeneration. Our model lowers the requirements in detailed captions and HQ\nvideos, and can be directly trained on limited LQ datasets with noisy and brief\ncaptions such as WebVid-10M, largely alleviating the cost to collect\nlarge-scale HQ video-text pairs. Extensive experiments in a variety of T2V and\nimage-to-video generation tasks demonstrate the effectiveness of our proposed\nFactorized-Dreamer. Our source codes are available at\n\\url{https://github.com/yangxy/Factorized-Dreamer/}.\n", "link": "http://arxiv.org/abs/2408.10119v1", "date": "2024-08-19", "relevancy": 2.6019, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6688}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6499}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factorized-Dreamer%3A%20Training%20A%20High-Quality%20Video%20Generator%20with%20Limited%0A%20%20and%20Low-Quality%20Data&body=Title%3A%20Factorized-Dreamer%3A%20Training%20A%20High-Quality%20Video%20Generator%20with%20Limited%0A%20%20and%20Low-Quality%20Data%0AAuthor%3A%20Tao%20Yang%20and%20Yangming%20Shi%20and%20Yunwen%20Huang%20and%20Feng%20Chen%20and%20Yin%20Zheng%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generation%20has%20gained%20significant%20attention%20due%20to%20its%0Awide%20applications%20to%20video%20generation%2C%20editing%2C%20enhancement%20and%20translation%2C%0A%5Cetc.%20However%2C%20high-quality%20%28HQ%29%20video%20synthesis%20is%20extremely%20challenging%0Abecause%20of%20the%20diverse%20and%20complex%20motions%20existed%20in%20real%20world.%20Most%20existing%0Aworks%20struggle%20to%20address%20this%20problem%20by%20collecting%20large-scale%20HQ%20videos%2C%0Awhich%20are%20inaccessible%20to%20the%20community.%20In%20this%20work%2C%20we%20show%20that%20publicly%0Aavailable%20limited%20and%20low-quality%20%28LQ%29%20data%20are%20sufficient%20to%20train%20a%20HQ%20video%0Agenerator%20without%20recaptioning%20or%20finetuning.%20We%20factorize%20the%20whole%20T2V%0Ageneration%20process%20into%20two%20steps%3A%20generating%20an%20image%20conditioned%20on%20a%20highly%0Adescriptive%20caption%2C%20and%20synthesizing%20the%20video%20conditioned%20on%20the%20generated%0Aimage%20and%20a%20concise%20caption%20of%20motion%20details.%20Specifically%2C%20we%20present%0A%5Cemph%7BFactorized-Dreamer%7D%2C%20a%20factorized%20spatiotemporal%20framework%20with%20several%0Acritical%20designs%20for%20T2V%20generation%2C%20including%20an%20adapter%20to%20combine%20text%20and%0Aimage%20embeddings%2C%20a%20pixel-aware%20cross%20attention%20module%20to%20capture%20pixel-level%0Aimage%20information%2C%20a%20T5%20text%20encoder%20to%20better%20understand%20motion%20description%2C%0Aand%20a%20PredictNet%20to%20supervise%20optical%20flows.%20We%20further%20present%20a%20noise%0Aschedule%2C%20which%20plays%20a%20key%20role%20in%20ensuring%20the%20quality%20and%20stability%20of%20video%0Ageneration.%20Our%20model%20lowers%20the%20requirements%20in%20detailed%20captions%20and%20HQ%0Avideos%2C%20and%20can%20be%20directly%20trained%20on%20limited%20LQ%20datasets%20with%20noisy%20and%20brief%0Acaptions%20such%20as%20WebVid-10M%2C%20largely%20alleviating%20the%20cost%20to%20collect%0Alarge-scale%20HQ%20video-text%20pairs.%20Extensive%20experiments%20in%20a%20variety%20of%20T2V%20and%0Aimage-to-video%20generation%20tasks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0AFactorized-Dreamer.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/Factorized-Dreamer/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorized-Dreamer%253A%2520Training%2520A%2520High-Quality%2520Video%2520Generator%2520with%2520Limited%250A%2520%2520and%2520Low-Quality%2520Data%26entry.906535625%3DTao%2520Yang%2520and%2520Yangming%2520Shi%2520and%2520Yunwen%2520Huang%2520and%2520Feng%2520Chen%2520and%2520Yin%2520Zheng%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generation%2520has%2520gained%2520significant%2520attention%2520due%2520to%2520its%250Awide%2520applications%2520to%2520video%2520generation%252C%2520editing%252C%2520enhancement%2520and%2520translation%252C%250A%255Cetc.%2520However%252C%2520high-quality%2520%2528HQ%2529%2520video%2520synthesis%2520is%2520extremely%2520challenging%250Abecause%2520of%2520the%2520diverse%2520and%2520complex%2520motions%2520existed%2520in%2520real%2520world.%2520Most%2520existing%250Aworks%2520struggle%2520to%2520address%2520this%2520problem%2520by%2520collecting%2520large-scale%2520HQ%2520videos%252C%250Awhich%2520are%2520inaccessible%2520to%2520the%2520community.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520publicly%250Aavailable%2520limited%2520and%2520low-quality%2520%2528LQ%2529%2520data%2520are%2520sufficient%2520to%2520train%2520a%2520HQ%2520video%250Agenerator%2520without%2520recaptioning%2520or%2520finetuning.%2520We%2520factorize%2520the%2520whole%2520T2V%250Ageneration%2520process%2520into%2520two%2520steps%253A%2520generating%2520an%2520image%2520conditioned%2520on%2520a%2520highly%250Adescriptive%2520caption%252C%2520and%2520synthesizing%2520the%2520video%2520conditioned%2520on%2520the%2520generated%250Aimage%2520and%2520a%2520concise%2520caption%2520of%2520motion%2520details.%2520Specifically%252C%2520we%2520present%250A%255Cemph%257BFactorized-Dreamer%257D%252C%2520a%2520factorized%2520spatiotemporal%2520framework%2520with%2520several%250Acritical%2520designs%2520for%2520T2V%2520generation%252C%2520including%2520an%2520adapter%2520to%2520combine%2520text%2520and%250Aimage%2520embeddings%252C%2520a%2520pixel-aware%2520cross%2520attention%2520module%2520to%2520capture%2520pixel-level%250Aimage%2520information%252C%2520a%2520T5%2520text%2520encoder%2520to%2520better%2520understand%2520motion%2520description%252C%250Aand%2520a%2520PredictNet%2520to%2520supervise%2520optical%2520flows.%2520We%2520further%2520present%2520a%2520noise%250Aschedule%252C%2520which%2520plays%2520a%2520key%2520role%2520in%2520ensuring%2520the%2520quality%2520and%2520stability%2520of%2520video%250Ageneration.%2520Our%2520model%2520lowers%2520the%2520requirements%2520in%2520detailed%2520captions%2520and%2520HQ%250Avideos%252C%2520and%2520can%2520be%2520directly%2520trained%2520on%2520limited%2520LQ%2520datasets%2520with%2520noisy%2520and%2520brief%250Acaptions%2520such%2520as%2520WebVid-10M%252C%2520largely%2520alleviating%2520the%2520cost%2520to%2520collect%250Alarge-scale%2520HQ%2520video-text%2520pairs.%2520Extensive%2520experiments%2520in%2520a%2520variety%2520of%2520T2V%2520and%250Aimage-to-video%2520generation%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250AFactorized-Dreamer.%2520Our%2520source%2520codes%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/yangxy/Factorized-Dreamer/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorized-Dreamer%3A%20Training%20A%20High-Quality%20Video%20Generator%20with%20Limited%0A%20%20and%20Low-Quality%20Data&entry.906535625=Tao%20Yang%20and%20Yangming%20Shi%20and%20Yunwen%20Huang%20and%20Feng%20Chen%20and%20Yin%20Zheng%20and%20Lei%20Zhang&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generation%20has%20gained%20significant%20attention%20due%20to%20its%0Awide%20applications%20to%20video%20generation%2C%20editing%2C%20enhancement%20and%20translation%2C%0A%5Cetc.%20However%2C%20high-quality%20%28HQ%29%20video%20synthesis%20is%20extremely%20challenging%0Abecause%20of%20the%20diverse%20and%20complex%20motions%20existed%20in%20real%20world.%20Most%20existing%0Aworks%20struggle%20to%20address%20this%20problem%20by%20collecting%20large-scale%20HQ%20videos%2C%0Awhich%20are%20inaccessible%20to%20the%20community.%20In%20this%20work%2C%20we%20show%20that%20publicly%0Aavailable%20limited%20and%20low-quality%20%28LQ%29%20data%20are%20sufficient%20to%20train%20a%20HQ%20video%0Agenerator%20without%20recaptioning%20or%20finetuning.%20We%20factorize%20the%20whole%20T2V%0Ageneration%20process%20into%20two%20steps%3A%20generating%20an%20image%20conditioned%20on%20a%20highly%0Adescriptive%20caption%2C%20and%20synthesizing%20the%20video%20conditioned%20on%20the%20generated%0Aimage%20and%20a%20concise%20caption%20of%20motion%20details.%20Specifically%2C%20we%20present%0A%5Cemph%7BFactorized-Dreamer%7D%2C%20a%20factorized%20spatiotemporal%20framework%20with%20several%0Acritical%20designs%20for%20T2V%20generation%2C%20including%20an%20adapter%20to%20combine%20text%20and%0Aimage%20embeddings%2C%20a%20pixel-aware%20cross%20attention%20module%20to%20capture%20pixel-level%0Aimage%20information%2C%20a%20T5%20text%20encoder%20to%20better%20understand%20motion%20description%2C%0Aand%20a%20PredictNet%20to%20supervise%20optical%20flows.%20We%20further%20present%20a%20noise%0Aschedule%2C%20which%20plays%20a%20key%20role%20in%20ensuring%20the%20quality%20and%20stability%20of%20video%0Ageneration.%20Our%20model%20lowers%20the%20requirements%20in%20detailed%20captions%20and%20HQ%0Avideos%2C%20and%20can%20be%20directly%20trained%20on%20limited%20LQ%20datasets%20with%20noisy%20and%20brief%0Acaptions%20such%20as%20WebVid-10M%2C%20largely%20alleviating%20the%20cost%20to%20collect%0Alarge-scale%20HQ%20video-text%20pairs.%20Extensive%20experiments%20in%20a%20variety%20of%20T2V%20and%0Aimage-to-video%20generation%20tasks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0AFactorized-Dreamer.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/Factorized-Dreamer/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10119v1&entry.124074799=Read"},
{"title": "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models", "author": "Simone Caldarella and Massimiliano Mancini and Elisa Ricci and Rahaf Aljundi", "abstract": "  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n", "link": "http://arxiv.org/abs/2408.01228v2", "date": "2024-08-19", "relevancy": 2.5761, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.532}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5116}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models&body=Title%3A%20The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models%0AAuthor%3A%20Simone%20Caldarella%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20combine%20visual%20and%20textual%20understanding%2C%0Arendering%20them%20well-suited%20for%20diverse%20tasks%20like%20generating%20image%20captions%20and%0Aanswering%20visual%20questions%20across%20various%20domains.%20However%2C%20these%20capabilities%0Aare%20built%20upon%20training%20on%20large%20amount%20of%20uncurated%20data%20crawled%20from%20the%20web.%0AThe%20latter%20may%20include%20sensitive%20information%20that%20VLMs%20could%20memorize%20and%20leak%2C%0Araising%20significant%20privacy%20concerns.%20In%20this%20paper%2C%20we%20assess%20whether%20these%0Avulnerabilities%20exist%2C%20focusing%20on%20identity%20leakage.%20Our%20study%20leads%20to%20three%0Akey%20findings%3A%20%28i%29%20VLMs%20leak%20identity%20information%2C%20even%20when%20the%20vision-language%0Aalignment%20and%20the%20fine-tuning%20use%20anonymized%20data%3B%20%28ii%29%20context%20has%20little%0Ainfluence%20on%20identity%20leakage%3B%20%28iii%29%20simple%2C%20widely%20used%20anonymization%0Atechniques%2C%20like%20blurring%2C%20are%20not%20sufficient%20to%20address%20the%20problem.%20These%0Afindings%20underscore%20the%20urgent%20need%20for%20robust%20privacy%20protection%20strategies%0Awhen%20deploying%20VLMs.%20Ethical%20awareness%20and%20responsible%20development%20practices%0Aare%20essential%20to%20mitigate%20these%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Phantom%2520Menace%253A%2520Unmasking%2520Privacy%2520Leakages%2520in%2520Vision-Language%2520Models%26entry.906535625%3DSimone%2520Caldarella%2520and%2520Massimiliano%2520Mancini%2520and%2520Elisa%2520Ricci%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520combine%2520visual%2520and%2520textual%2520understanding%252C%250Arendering%2520them%2520well-suited%2520for%2520diverse%2520tasks%2520like%2520generating%2520image%2520captions%2520and%250Aanswering%2520visual%2520questions%2520across%2520various%2520domains.%2520However%252C%2520these%2520capabilities%250Aare%2520built%2520upon%2520training%2520on%2520large%2520amount%2520of%2520uncurated%2520data%2520crawled%2520from%2520the%2520web.%250AThe%2520latter%2520may%2520include%2520sensitive%2520information%2520that%2520VLMs%2520could%2520memorize%2520and%2520leak%252C%250Araising%2520significant%2520privacy%2520concerns.%2520In%2520this%2520paper%252C%2520we%2520assess%2520whether%2520these%250Avulnerabilities%2520exist%252C%2520focusing%2520on%2520identity%2520leakage.%2520Our%2520study%2520leads%2520to%2520three%250Akey%2520findings%253A%2520%2528i%2529%2520VLMs%2520leak%2520identity%2520information%252C%2520even%2520when%2520the%2520vision-language%250Aalignment%2520and%2520the%2520fine-tuning%2520use%2520anonymized%2520data%253B%2520%2528ii%2529%2520context%2520has%2520little%250Ainfluence%2520on%2520identity%2520leakage%253B%2520%2528iii%2529%2520simple%252C%2520widely%2520used%2520anonymization%250Atechniques%252C%2520like%2520blurring%252C%2520are%2520not%2520sufficient%2520to%2520address%2520the%2520problem.%2520These%250Afindings%2520underscore%2520the%2520urgent%2520need%2520for%2520robust%2520privacy%2520protection%2520strategies%250Awhen%2520deploying%2520VLMs.%2520Ethical%2520awareness%2520and%2520responsible%2520development%2520practices%250Aare%2520essential%2520to%2520mitigate%2520these%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models&entry.906535625=Simone%20Caldarella%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20combine%20visual%20and%20textual%20understanding%2C%0Arendering%20them%20well-suited%20for%20diverse%20tasks%20like%20generating%20image%20captions%20and%0Aanswering%20visual%20questions%20across%20various%20domains.%20However%2C%20these%20capabilities%0Aare%20built%20upon%20training%20on%20large%20amount%20of%20uncurated%20data%20crawled%20from%20the%20web.%0AThe%20latter%20may%20include%20sensitive%20information%20that%20VLMs%20could%20memorize%20and%20leak%2C%0Araising%20significant%20privacy%20concerns.%20In%20this%20paper%2C%20we%20assess%20whether%20these%0Avulnerabilities%20exist%2C%20focusing%20on%20identity%20leakage.%20Our%20study%20leads%20to%20three%0Akey%20findings%3A%20%28i%29%20VLMs%20leak%20identity%20information%2C%20even%20when%20the%20vision-language%0Aalignment%20and%20the%20fine-tuning%20use%20anonymized%20data%3B%20%28ii%29%20context%20has%20little%0Ainfluence%20on%20identity%20leakage%3B%20%28iii%29%20simple%2C%20widely%20used%20anonymization%0Atechniques%2C%20like%20blurring%2C%20are%20not%20sufficient%20to%20address%20the%20problem.%20These%0Afindings%20underscore%20the%20urgent%20need%20for%20robust%20privacy%20protection%20strategies%0Awhen%20deploying%20VLMs.%20Ethical%20awareness%20and%20responsible%20development%20practices%0Aare%20essential%20to%20mitigate%20these%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01228v2&entry.124074799=Read"},
{"title": "Uniting contrastive and generative learning for event sequences models", "author": "Aleksandr Yugay and Alexey Zaytsev", "abstract": "  High-quality representation of transactional sequences is vital for modern\nbanking applications, including risk management, churn prediction, and\npersonalized customer offers. Different tasks require distinct representation\nproperties: local tasks benefit from capturing the client's current state,\nwhile global tasks rely on general behavioral patterns. Previous research has\ndemonstrated that various self-supervised approaches yield representations that\nbetter capture either global or local qualities.\n  This study investigates the integration of two self-supervised learning\ntechniques - instance-wise contrastive learning and a generative approach based\non restoring masked events in latent space. The combined approach creates\nrepresentations that balance local and global transactional data\ncharacteristics. Experiments conducted on several public datasets, focusing on\nsequence classification and next-event type prediction, show that the\nintegrated method achieves superior performance compared to individual\napproaches and demonstrates synergistic effects. These findings suggest that\nthe proposed approach offers a robust framework for advancing event sequences\nrepresentation learning in the financial sector.\n", "link": "http://arxiv.org/abs/2408.09995v1", "date": "2024-08-19", "relevancy": 2.5731, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5222}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uniting%20contrastive%20and%20generative%20learning%20for%20event%20sequences%20models&body=Title%3A%20Uniting%20contrastive%20and%20generative%20learning%20for%20event%20sequences%20models%0AAuthor%3A%20Aleksandr%20Yugay%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20High-quality%20representation%20of%20transactional%20sequences%20is%20vital%20for%20modern%0Abanking%20applications%2C%20including%20risk%20management%2C%20churn%20prediction%2C%20and%0Apersonalized%20customer%20offers.%20Different%20tasks%20require%20distinct%20representation%0Aproperties%3A%20local%20tasks%20benefit%20from%20capturing%20the%20client%27s%20current%20state%2C%0Awhile%20global%20tasks%20rely%20on%20general%20behavioral%20patterns.%20Previous%20research%20has%0Ademonstrated%20that%20various%20self-supervised%20approaches%20yield%20representations%20that%0Abetter%20capture%20either%20global%20or%20local%20qualities.%0A%20%20This%20study%20investigates%20the%20integration%20of%20two%20self-supervised%20learning%0Atechniques%20-%20instance-wise%20contrastive%20learning%20and%20a%20generative%20approach%20based%0Aon%20restoring%20masked%20events%20in%20latent%20space.%20The%20combined%20approach%20creates%0Arepresentations%20that%20balance%20local%20and%20global%20transactional%20data%0Acharacteristics.%20Experiments%20conducted%20on%20several%20public%20datasets%2C%20focusing%20on%0Asequence%20classification%20and%20next-event%20type%20prediction%2C%20show%20that%20the%0Aintegrated%20method%20achieves%20superior%20performance%20compared%20to%20individual%0Aapproaches%20and%20demonstrates%20synergistic%20effects.%20These%20findings%20suggest%20that%0Athe%20proposed%20approach%20offers%20a%20robust%20framework%20for%20advancing%20event%20sequences%0Arepresentation%20learning%20in%20the%20financial%20sector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniting%2520contrastive%2520and%2520generative%2520learning%2520for%2520event%2520sequences%2520models%26entry.906535625%3DAleksandr%2520Yugay%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520High-quality%2520representation%2520of%2520transactional%2520sequences%2520is%2520vital%2520for%2520modern%250Abanking%2520applications%252C%2520including%2520risk%2520management%252C%2520churn%2520prediction%252C%2520and%250Apersonalized%2520customer%2520offers.%2520Different%2520tasks%2520require%2520distinct%2520representation%250Aproperties%253A%2520local%2520tasks%2520benefit%2520from%2520capturing%2520the%2520client%2527s%2520current%2520state%252C%250Awhile%2520global%2520tasks%2520rely%2520on%2520general%2520behavioral%2520patterns.%2520Previous%2520research%2520has%250Ademonstrated%2520that%2520various%2520self-supervised%2520approaches%2520yield%2520representations%2520that%250Abetter%2520capture%2520either%2520global%2520or%2520local%2520qualities.%250A%2520%2520This%2520study%2520investigates%2520the%2520integration%2520of%2520two%2520self-supervised%2520learning%250Atechniques%2520-%2520instance-wise%2520contrastive%2520learning%2520and%2520a%2520generative%2520approach%2520based%250Aon%2520restoring%2520masked%2520events%2520in%2520latent%2520space.%2520The%2520combined%2520approach%2520creates%250Arepresentations%2520that%2520balance%2520local%2520and%2520global%2520transactional%2520data%250Acharacteristics.%2520Experiments%2520conducted%2520on%2520several%2520public%2520datasets%252C%2520focusing%2520on%250Asequence%2520classification%2520and%2520next-event%2520type%2520prediction%252C%2520show%2520that%2520the%250Aintegrated%2520method%2520achieves%2520superior%2520performance%2520compared%2520to%2520individual%250Aapproaches%2520and%2520demonstrates%2520synergistic%2520effects.%2520These%2520findings%2520suggest%2520that%250Athe%2520proposed%2520approach%2520offers%2520a%2520robust%2520framework%2520for%2520advancing%2520event%2520sequences%250Arepresentation%2520learning%2520in%2520the%2520financial%2520sector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uniting%20contrastive%20and%20generative%20learning%20for%20event%20sequences%20models&entry.906535625=Aleksandr%20Yugay%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20High-quality%20representation%20of%20transactional%20sequences%20is%20vital%20for%20modern%0Abanking%20applications%2C%20including%20risk%20management%2C%20churn%20prediction%2C%20and%0Apersonalized%20customer%20offers.%20Different%20tasks%20require%20distinct%20representation%0Aproperties%3A%20local%20tasks%20benefit%20from%20capturing%20the%20client%27s%20current%20state%2C%0Awhile%20global%20tasks%20rely%20on%20general%20behavioral%20patterns.%20Previous%20research%20has%0Ademonstrated%20that%20various%20self-supervised%20approaches%20yield%20representations%20that%0Abetter%20capture%20either%20global%20or%20local%20qualities.%0A%20%20This%20study%20investigates%20the%20integration%20of%20two%20self-supervised%20learning%0Atechniques%20-%20instance-wise%20contrastive%20learning%20and%20a%20generative%20approach%20based%0Aon%20restoring%20masked%20events%20in%20latent%20space.%20The%20combined%20approach%20creates%0Arepresentations%20that%20balance%20local%20and%20global%20transactional%20data%0Acharacteristics.%20Experiments%20conducted%20on%20several%20public%20datasets%2C%20focusing%20on%0Asequence%20classification%20and%20next-event%20type%20prediction%2C%20show%20that%20the%0Aintegrated%20method%20achieves%20superior%20performance%20compared%20to%20individual%0Aapproaches%20and%20demonstrates%20synergistic%20effects.%20These%20findings%20suggest%20that%0Athe%20proposed%20approach%20offers%20a%20robust%20framework%20for%20advancing%20event%20sequences%0Arepresentation%20learning%20in%20the%20financial%20sector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09995v1&entry.124074799=Read"},
{"title": "Pose-GuideNet: Automatic Scanning Guidance for Fetal Head Ultrasound\n  from Pose Estimation", "author": "Qianhui Men and Xiaoqing Guo and Aris T. Papageorghiou and J. Alison Noble", "abstract": "  3D pose estimation from a 2D cross-sectional view enables healthcare\nprofessionals to navigate through the 3D space, and such techniques initiate\nautomatic guidance in many image-guided radiology applications. In this work,\nwe investigate how estimating 3D fetal pose from freehand 2D ultrasound\nscanning can guide a sonographer to locate a head standard plane. Fetal head\npose is estimated by the proposed Pose-GuideNet, a novel 2D/3D registration\napproach to align freehand 2D ultrasound to a 3D anatomical atlas without the\nacquisition of 3D ultrasound. To facilitate the 2D to 3D cross-dimensional\nprojection, we exploit the prior knowledge in the atlas to align the standard\nplane frame in a freehand scan. A semantic-aware contrastive-based approach is\nfurther proposed to align the frames that are off standard planes based on\ntheir anatomical similarity. In the experiment, we enhance the existing\nassessment of freehand image localization by comparing the transformation of\nits estimated pose towards standard plane with the corresponding probe motion,\nwhich reflects the actual view change in 3D anatomy. Extensive results on two\nclinical head biometry tasks show that Pose-GuideNet not only accurately\npredicts pose but also successfully predicts the direction of the fetal head.\nEvaluations with probe motions further demonstrate the feasibility of adopting\nPose-GuideNet for freehand ultrasound-assisted navigation in a sensor-free\nenvironment.\n", "link": "http://arxiv.org/abs/2408.09931v1", "date": "2024-08-19", "relevancy": 2.5641, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5132}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5132}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose-GuideNet%3A%20Automatic%20Scanning%20Guidance%20for%20Fetal%20Head%20Ultrasound%0A%20%20from%20Pose%20Estimation&body=Title%3A%20Pose-GuideNet%3A%20Automatic%20Scanning%20Guidance%20for%20Fetal%20Head%20Ultrasound%0A%20%20from%20Pose%20Estimation%0AAuthor%3A%20Qianhui%20Men%20and%20Xiaoqing%20Guo%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%203D%20pose%20estimation%20from%20a%202D%20cross-sectional%20view%20enables%20healthcare%0Aprofessionals%20to%20navigate%20through%20the%203D%20space%2C%20and%20such%20techniques%20initiate%0Aautomatic%20guidance%20in%20many%20image-guided%20radiology%20applications.%20In%20this%20work%2C%0Awe%20investigate%20how%20estimating%203D%20fetal%20pose%20from%20freehand%202D%20ultrasound%0Ascanning%20can%20guide%20a%20sonographer%20to%20locate%20a%20head%20standard%20plane.%20Fetal%20head%0Apose%20is%20estimated%20by%20the%20proposed%20Pose-GuideNet%2C%20a%20novel%202D/3D%20registration%0Aapproach%20to%20align%20freehand%202D%20ultrasound%20to%20a%203D%20anatomical%20atlas%20without%20the%0Aacquisition%20of%203D%20ultrasound.%20To%20facilitate%20the%202D%20to%203D%20cross-dimensional%0Aprojection%2C%20we%20exploit%20the%20prior%20knowledge%20in%20the%20atlas%20to%20align%20the%20standard%0Aplane%20frame%20in%20a%20freehand%20scan.%20A%20semantic-aware%20contrastive-based%20approach%20is%0Afurther%20proposed%20to%20align%20the%20frames%20that%20are%20off%20standard%20planes%20based%20on%0Atheir%20anatomical%20similarity.%20In%20the%20experiment%2C%20we%20enhance%20the%20existing%0Aassessment%20of%20freehand%20image%20localization%20by%20comparing%20the%20transformation%20of%0Aits%20estimated%20pose%20towards%20standard%20plane%20with%20the%20corresponding%20probe%20motion%2C%0Awhich%20reflects%20the%20actual%20view%20change%20in%203D%20anatomy.%20Extensive%20results%20on%20two%0Aclinical%20head%20biometry%20tasks%20show%20that%20Pose-GuideNet%20not%20only%20accurately%0Apredicts%20pose%20but%20also%20successfully%20predicts%20the%20direction%20of%20the%20fetal%20head.%0AEvaluations%20with%20probe%20motions%20further%20demonstrate%20the%20feasibility%20of%20adopting%0APose-GuideNet%20for%20freehand%20ultrasound-assisted%20navigation%20in%20a%20sensor-free%0Aenvironment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose-GuideNet%253A%2520Automatic%2520Scanning%2520Guidance%2520for%2520Fetal%2520Head%2520Ultrasound%250A%2520%2520from%2520Pose%2520Estimation%26entry.906535625%3DQianhui%2520Men%2520and%2520Xiaoqing%2520Guo%2520and%2520Aris%2520T.%2520Papageorghiou%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%25203D%2520pose%2520estimation%2520from%2520a%25202D%2520cross-sectional%2520view%2520enables%2520healthcare%250Aprofessionals%2520to%2520navigate%2520through%2520the%25203D%2520space%252C%2520and%2520such%2520techniques%2520initiate%250Aautomatic%2520guidance%2520in%2520many%2520image-guided%2520radiology%2520applications.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520how%2520estimating%25203D%2520fetal%2520pose%2520from%2520freehand%25202D%2520ultrasound%250Ascanning%2520can%2520guide%2520a%2520sonographer%2520to%2520locate%2520a%2520head%2520standard%2520plane.%2520Fetal%2520head%250Apose%2520is%2520estimated%2520by%2520the%2520proposed%2520Pose-GuideNet%252C%2520a%2520novel%25202D/3D%2520registration%250Aapproach%2520to%2520align%2520freehand%25202D%2520ultrasound%2520to%2520a%25203D%2520anatomical%2520atlas%2520without%2520the%250Aacquisition%2520of%25203D%2520ultrasound.%2520To%2520facilitate%2520the%25202D%2520to%25203D%2520cross-dimensional%250Aprojection%252C%2520we%2520exploit%2520the%2520prior%2520knowledge%2520in%2520the%2520atlas%2520to%2520align%2520the%2520standard%250Aplane%2520frame%2520in%2520a%2520freehand%2520scan.%2520A%2520semantic-aware%2520contrastive-based%2520approach%2520is%250Afurther%2520proposed%2520to%2520align%2520the%2520frames%2520that%2520are%2520off%2520standard%2520planes%2520based%2520on%250Atheir%2520anatomical%2520similarity.%2520In%2520the%2520experiment%252C%2520we%2520enhance%2520the%2520existing%250Aassessment%2520of%2520freehand%2520image%2520localization%2520by%2520comparing%2520the%2520transformation%2520of%250Aits%2520estimated%2520pose%2520towards%2520standard%2520plane%2520with%2520the%2520corresponding%2520probe%2520motion%252C%250Awhich%2520reflects%2520the%2520actual%2520view%2520change%2520in%25203D%2520anatomy.%2520Extensive%2520results%2520on%2520two%250Aclinical%2520head%2520biometry%2520tasks%2520show%2520that%2520Pose-GuideNet%2520not%2520only%2520accurately%250Apredicts%2520pose%2520but%2520also%2520successfully%2520predicts%2520the%2520direction%2520of%2520the%2520fetal%2520head.%250AEvaluations%2520with%2520probe%2520motions%2520further%2520demonstrate%2520the%2520feasibility%2520of%2520adopting%250APose-GuideNet%2520for%2520freehand%2520ultrasound-assisted%2520navigation%2520in%2520a%2520sensor-free%250Aenvironment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose-GuideNet%3A%20Automatic%20Scanning%20Guidance%20for%20Fetal%20Head%20Ultrasound%0A%20%20from%20Pose%20Estimation&entry.906535625=Qianhui%20Men%20and%20Xiaoqing%20Guo%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble&entry.1292438233=%20%203D%20pose%20estimation%20from%20a%202D%20cross-sectional%20view%20enables%20healthcare%0Aprofessionals%20to%20navigate%20through%20the%203D%20space%2C%20and%20such%20techniques%20initiate%0Aautomatic%20guidance%20in%20many%20image-guided%20radiology%20applications.%20In%20this%20work%2C%0Awe%20investigate%20how%20estimating%203D%20fetal%20pose%20from%20freehand%202D%20ultrasound%0Ascanning%20can%20guide%20a%20sonographer%20to%20locate%20a%20head%20standard%20plane.%20Fetal%20head%0Apose%20is%20estimated%20by%20the%20proposed%20Pose-GuideNet%2C%20a%20novel%202D/3D%20registration%0Aapproach%20to%20align%20freehand%202D%20ultrasound%20to%20a%203D%20anatomical%20atlas%20without%20the%0Aacquisition%20of%203D%20ultrasound.%20To%20facilitate%20the%202D%20to%203D%20cross-dimensional%0Aprojection%2C%20we%20exploit%20the%20prior%20knowledge%20in%20the%20atlas%20to%20align%20the%20standard%0Aplane%20frame%20in%20a%20freehand%20scan.%20A%20semantic-aware%20contrastive-based%20approach%20is%0Afurther%20proposed%20to%20align%20the%20frames%20that%20are%20off%20standard%20planes%20based%20on%0Atheir%20anatomical%20similarity.%20In%20the%20experiment%2C%20we%20enhance%20the%20existing%0Aassessment%20of%20freehand%20image%20localization%20by%20comparing%20the%20transformation%20of%0Aits%20estimated%20pose%20towards%20standard%20plane%20with%20the%20corresponding%20probe%20motion%2C%0Awhich%20reflects%20the%20actual%20view%20change%20in%203D%20anatomy.%20Extensive%20results%20on%20two%0Aclinical%20head%20biometry%20tasks%20show%20that%20Pose-GuideNet%20not%20only%20accurately%0Apredicts%20pose%20but%20also%20successfully%20predicts%20the%20direction%20of%20the%20fetal%20head.%0AEvaluations%20with%20probe%20motions%20further%20demonstrate%20the%20feasibility%20of%20adopting%0APose-GuideNet%20for%20freehand%20ultrasound-assisted%20navigation%20in%20a%20sensor-free%0Aenvironment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09931v1&entry.124074799=Read"},
{"title": "Criticality Leveraged Adversarial Training (CLAT) for Boosted\n  Performance via Parameter Efficiency", "author": "Bhavna Gopal and Huanrui Yang and Jingyang Zhang and Mark Horton and Yiran Chen", "abstract": "  Adversarial training enhances neural network robustness but suffers from a\ntendency to overfit and increased generalization errors on clean data. This\nwork introduces CLAT, an innovative approach that mitigates adversarial\noverfitting by introducing parameter efficiency into the adversarial training\nprocess, improving both clean accuracy and adversarial robustness. Instead of\ntuning the entire model, CLAT identifies and fine-tunes robustness-critical\nlayers - those predominantly learning non-robust features - while freezing the\nremaining model to enhance robustness. It employs dynamic critical layer\nselection to adapt to changes in layer criticality throughout the fine-tuning\nprocess. Empirically, CLAT can be applied on top of existing adversarial\ntraining methods, significantly reduces the number of trainable parameters by\napproximately 95%, and achieves more than a 2% improvement in adversarial\nrobustness compared to baseline methods.\n", "link": "http://arxiv.org/abs/2408.10204v1", "date": "2024-08-19", "relevancy": 2.5597, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Criticality%20Leveraged%20Adversarial%20Training%20%28CLAT%29%20for%20Boosted%0A%20%20Performance%20via%20Parameter%20Efficiency&body=Title%3A%20Criticality%20Leveraged%20Adversarial%20Training%20%28CLAT%29%20for%20Boosted%0A%20%20Performance%20via%20Parameter%20Efficiency%0AAuthor%3A%20Bhavna%20Gopal%20and%20Huanrui%20Yang%20and%20Jingyang%20Zhang%20and%20Mark%20Horton%20and%20Yiran%20Chen%0AAbstract%3A%20%20%20Adversarial%20training%20enhances%20neural%20network%20robustness%20but%20suffers%20from%20a%0Atendency%20to%20overfit%20and%20increased%20generalization%20errors%20on%20clean%20data.%20This%0Awork%20introduces%20CLAT%2C%20an%20innovative%20approach%20that%20mitigates%20adversarial%0Aoverfitting%20by%20introducing%20parameter%20efficiency%20into%20the%20adversarial%20training%0Aprocess%2C%20improving%20both%20clean%20accuracy%20and%20adversarial%20robustness.%20Instead%20of%0Atuning%20the%20entire%20model%2C%20CLAT%20identifies%20and%20fine-tunes%20robustness-critical%0Alayers%20-%20those%20predominantly%20learning%20non-robust%20features%20-%20while%20freezing%20the%0Aremaining%20model%20to%20enhance%20robustness.%20It%20employs%20dynamic%20critical%20layer%0Aselection%20to%20adapt%20to%20changes%20in%20layer%20criticality%20throughout%20the%20fine-tuning%0Aprocess.%20Empirically%2C%20CLAT%20can%20be%20applied%20on%20top%20of%20existing%20adversarial%0Atraining%20methods%2C%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20by%0Aapproximately%2095%25%2C%20and%20achieves%20more%20than%20a%202%25%20improvement%20in%20adversarial%0Arobustness%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCriticality%2520Leveraged%2520Adversarial%2520Training%2520%2528CLAT%2529%2520for%2520Boosted%250A%2520%2520Performance%2520via%2520Parameter%2520Efficiency%26entry.906535625%3DBhavna%2520Gopal%2520and%2520Huanrui%2520Yang%2520and%2520Jingyang%2520Zhang%2520and%2520Mark%2520Horton%2520and%2520Yiran%2520Chen%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520enhances%2520neural%2520network%2520robustness%2520but%2520suffers%2520from%2520a%250Atendency%2520to%2520overfit%2520and%2520increased%2520generalization%2520errors%2520on%2520clean%2520data.%2520This%250Awork%2520introduces%2520CLAT%252C%2520an%2520innovative%2520approach%2520that%2520mitigates%2520adversarial%250Aoverfitting%2520by%2520introducing%2520parameter%2520efficiency%2520into%2520the%2520adversarial%2520training%250Aprocess%252C%2520improving%2520both%2520clean%2520accuracy%2520and%2520adversarial%2520robustness.%2520Instead%2520of%250Atuning%2520the%2520entire%2520model%252C%2520CLAT%2520identifies%2520and%2520fine-tunes%2520robustness-critical%250Alayers%2520-%2520those%2520predominantly%2520learning%2520non-robust%2520features%2520-%2520while%2520freezing%2520the%250Aremaining%2520model%2520to%2520enhance%2520robustness.%2520It%2520employs%2520dynamic%2520critical%2520layer%250Aselection%2520to%2520adapt%2520to%2520changes%2520in%2520layer%2520criticality%2520throughout%2520the%2520fine-tuning%250Aprocess.%2520Empirically%252C%2520CLAT%2520can%2520be%2520applied%2520on%2520top%2520of%2520existing%2520adversarial%250Atraining%2520methods%252C%2520significantly%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520by%250Aapproximately%252095%2525%252C%2520and%2520achieves%2520more%2520than%2520a%25202%2525%2520improvement%2520in%2520adversarial%250Arobustness%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Criticality%20Leveraged%20Adversarial%20Training%20%28CLAT%29%20for%20Boosted%0A%20%20Performance%20via%20Parameter%20Efficiency&entry.906535625=Bhavna%20Gopal%20and%20Huanrui%20Yang%20and%20Jingyang%20Zhang%20and%20Mark%20Horton%20and%20Yiran%20Chen&entry.1292438233=%20%20Adversarial%20training%20enhances%20neural%20network%20robustness%20but%20suffers%20from%20a%0Atendency%20to%20overfit%20and%20increased%20generalization%20errors%20on%20clean%20data.%20This%0Awork%20introduces%20CLAT%2C%20an%20innovative%20approach%20that%20mitigates%20adversarial%0Aoverfitting%20by%20introducing%20parameter%20efficiency%20into%20the%20adversarial%20training%0Aprocess%2C%20improving%20both%20clean%20accuracy%20and%20adversarial%20robustness.%20Instead%20of%0Atuning%20the%20entire%20model%2C%20CLAT%20identifies%20and%20fine-tunes%20robustness-critical%0Alayers%20-%20those%20predominantly%20learning%20non-robust%20features%20-%20while%20freezing%20the%0Aremaining%20model%20to%20enhance%20robustness.%20It%20employs%20dynamic%20critical%20layer%0Aselection%20to%20adapt%20to%20changes%20in%20layer%20criticality%20throughout%20the%20fine-tuning%0Aprocess.%20Empirically%2C%20CLAT%20can%20be%20applied%20on%20top%20of%20existing%20adversarial%0Atraining%20methods%2C%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20by%0Aapproximately%2095%25%2C%20and%20achieves%20more%20than%20a%202%25%20improvement%20in%20adversarial%0Arobustness%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10204v1&entry.124074799=Read"},
{"title": "Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning\n  for Facial Wrinkle Detection", "author": "Ik Jun Moon and Junho Moon and Ikbeom Jang", "abstract": "  1. Research question: With the growing interest in skin diseases and skin\naesthetics, the ability to predict facial wrinkles is becoming increasingly\nimportant. This study aims to evaluate whether a computational model,\nconvolutional neural networks (CNN), can be trained for automated facial\nwrinkle segmentation. 2. Findings: Our study presents an effective technique\nfor integrating data from multiple annotators and illustrates that transfer\nlearning can enhance performance, resulting in dependable segmentation of\nfacial wrinkles. 3. Meaning: This approach automates intricate and\ntime-consuming tasks of wrinkle analysis with a deep learning framework. It\ncould be used to facilitate skin treatments and diagnostics.\n", "link": "http://arxiv.org/abs/2408.09952v1", "date": "2024-08-19", "relevancy": 2.5423, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Pretraining%20and%20Multi-Annotator%20Supervised%20Finetuning%0A%20%20for%20Facial%20Wrinkle%20Detection&body=Title%3A%20Weakly%20Supervised%20Pretraining%20and%20Multi-Annotator%20Supervised%20Finetuning%0A%20%20for%20Facial%20Wrinkle%20Detection%0AAuthor%3A%20Ik%20Jun%20Moon%20and%20Junho%20Moon%20and%20Ikbeom%20Jang%0AAbstract%3A%20%20%201.%20Research%20question%3A%20With%20the%20growing%20interest%20in%20skin%20diseases%20and%20skin%0Aaesthetics%2C%20the%20ability%20to%20predict%20facial%20wrinkles%20is%20becoming%20increasingly%0Aimportant.%20This%20study%20aims%20to%20evaluate%20whether%20a%20computational%20model%2C%0Aconvolutional%20neural%20networks%20%28CNN%29%2C%20can%20be%20trained%20for%20automated%20facial%0Awrinkle%20segmentation.%202.%20Findings%3A%20Our%20study%20presents%20an%20effective%20technique%0Afor%20integrating%20data%20from%20multiple%20annotators%20and%20illustrates%20that%20transfer%0Alearning%20can%20enhance%20performance%2C%20resulting%20in%20dependable%20segmentation%20of%0Afacial%20wrinkles.%203.%20Meaning%3A%20This%20approach%20automates%20intricate%20and%0Atime-consuming%20tasks%20of%20wrinkle%20analysis%20with%20a%20deep%20learning%20framework.%20It%0Acould%20be%20used%20to%20facilitate%20skin%20treatments%20and%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Pretraining%2520and%2520Multi-Annotator%2520Supervised%2520Finetuning%250A%2520%2520for%2520Facial%2520Wrinkle%2520Detection%26entry.906535625%3DIk%2520Jun%2520Moon%2520and%2520Junho%2520Moon%2520and%2520Ikbeom%2520Jang%26entry.1292438233%3D%2520%25201.%2520Research%2520question%253A%2520With%2520the%2520growing%2520interest%2520in%2520skin%2520diseases%2520and%2520skin%250Aaesthetics%252C%2520the%2520ability%2520to%2520predict%2520facial%2520wrinkles%2520is%2520becoming%2520increasingly%250Aimportant.%2520This%2520study%2520aims%2520to%2520evaluate%2520whether%2520a%2520computational%2520model%252C%250Aconvolutional%2520neural%2520networks%2520%2528CNN%2529%252C%2520can%2520be%2520trained%2520for%2520automated%2520facial%250Awrinkle%2520segmentation.%25202.%2520Findings%253A%2520Our%2520study%2520presents%2520an%2520effective%2520technique%250Afor%2520integrating%2520data%2520from%2520multiple%2520annotators%2520and%2520illustrates%2520that%2520transfer%250Alearning%2520can%2520enhance%2520performance%252C%2520resulting%2520in%2520dependable%2520segmentation%2520of%250Afacial%2520wrinkles.%25203.%2520Meaning%253A%2520This%2520approach%2520automates%2520intricate%2520and%250Atime-consuming%2520tasks%2520of%2520wrinkle%2520analysis%2520with%2520a%2520deep%2520learning%2520framework.%2520It%250Acould%2520be%2520used%2520to%2520facilitate%2520skin%2520treatments%2520and%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Pretraining%20and%20Multi-Annotator%20Supervised%20Finetuning%0A%20%20for%20Facial%20Wrinkle%20Detection&entry.906535625=Ik%20Jun%20Moon%20and%20Junho%20Moon%20and%20Ikbeom%20Jang&entry.1292438233=%20%201.%20Research%20question%3A%20With%20the%20growing%20interest%20in%20skin%20diseases%20and%20skin%0Aaesthetics%2C%20the%20ability%20to%20predict%20facial%20wrinkles%20is%20becoming%20increasingly%0Aimportant.%20This%20study%20aims%20to%20evaluate%20whether%20a%20computational%20model%2C%0Aconvolutional%20neural%20networks%20%28CNN%29%2C%20can%20be%20trained%20for%20automated%20facial%0Awrinkle%20segmentation.%202.%20Findings%3A%20Our%20study%20presents%20an%20effective%20technique%0Afor%20integrating%20data%20from%20multiple%20annotators%20and%20illustrates%20that%20transfer%0Alearning%20can%20enhance%20performance%2C%20resulting%20in%20dependable%20segmentation%20of%0Afacial%20wrinkles.%203.%20Meaning%3A%20This%20approach%20automates%20intricate%20and%0Atime-consuming%20tasks%20of%20wrinkle%20analysis%20with%20a%20deep%20learning%20framework.%20It%0Acould%20be%20used%20to%20facilitate%20skin%20treatments%20and%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09952v1&entry.124074799=Read"},
{"title": "Topology-preserving Adversarial Training for Alleviating Natural\n  Accuracy Degradation", "author": "Xiaoyue Mi and Fan Tang and Yepeng Weng and Danding Wang and Juan Cao and Sheng Tang and Peng Li and Yang Liu", "abstract": "  Despite the effectiveness in improving the robustness of neural networks,\nadversarial training has suffered from the natural accuracy degradation\nproblem, i.e., accuracy on natural samples has reduced significantly. In this\nstudy, we reveal that natural accuracy degradation is highly related to the\ndisruption of the natural sample topology in the representation space by\nquantitative and qualitative experiments. Based on this observation, we propose\nTopology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by\npreserving the topology structure of natural samples from a standard model\ntrained only on natural samples during adversarial training. As an additional\nregularization, our method can be combined with various popular adversarial\ntraining algorithms, taking advantage of both sides. Extensive experiments on\nCIFAR-10, CIFAR-100, and Tiny ImageNet show that our proposed method achieves\nconsistent and significant improvements over various strong baselines in most\ncases. Specifically, without additional data, TRAIN achieves up to 8.86%\nimprovement in natural accuracy and 6.33% improvement in robust accuracy.\n", "link": "http://arxiv.org/abs/2311.17607v2", "date": "2024-08-19", "relevancy": 2.5146, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5047}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-preserving%20Adversarial%20Training%20for%20Alleviating%20Natural%0A%20%20Accuracy%20Degradation&body=Title%3A%20Topology-preserving%20Adversarial%20Training%20for%20Alleviating%20Natural%0A%20%20Accuracy%20Degradation%0AAuthor%3A%20Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20Yepeng%20Weng%20and%20Danding%20Wang%20and%20Juan%20Cao%20and%20Sheng%20Tang%20and%20Peng%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Despite%20the%20effectiveness%20in%20improving%20the%20robustness%20of%20neural%20networks%2C%0Aadversarial%20training%20has%20suffered%20from%20the%20natural%20accuracy%20degradation%0Aproblem%2C%20i.e.%2C%20accuracy%20on%20natural%20samples%20has%20reduced%20significantly.%20In%20this%0Astudy%2C%20we%20reveal%20that%20natural%20accuracy%20degradation%20is%20highly%20related%20to%20the%0Adisruption%20of%20the%20natural%20sample%20topology%20in%20the%20representation%20space%20by%0Aquantitative%20and%20qualitative%20experiments.%20Based%20on%20this%20observation%2C%20we%20propose%0ATopology-pReserving%20Adversarial%20traINing%20%28TRAIN%29%20to%20alleviate%20the%20problem%20by%0Apreserving%20the%20topology%20structure%20of%20natural%20samples%20from%20a%20standard%20model%0Atrained%20only%20on%20natural%20samples%20during%20adversarial%20training.%20As%20an%20additional%0Aregularization%2C%20our%20method%20can%20be%20combined%20with%20various%20popular%20adversarial%0Atraining%20algorithms%2C%20taking%20advantage%20of%20both%20sides.%20Extensive%20experiments%20on%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20ImageNet%20show%20that%20our%20proposed%20method%20achieves%0Aconsistent%20and%20significant%20improvements%20over%20various%20strong%20baselines%20in%20most%0Acases.%20Specifically%2C%20without%20additional%20data%2C%20TRAIN%20achieves%20up%20to%208.86%25%0Aimprovement%20in%20natural%20accuracy%20and%206.33%25%20improvement%20in%20robust%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-preserving%2520Adversarial%2520Training%2520for%2520Alleviating%2520Natural%250A%2520%2520Accuracy%2520Degradation%26entry.906535625%3DXiaoyue%2520Mi%2520and%2520Fan%2520Tang%2520and%2520Yepeng%2520Weng%2520and%2520Danding%2520Wang%2520and%2520Juan%2520Cao%2520and%2520Sheng%2520Tang%2520and%2520Peng%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Despite%2520the%2520effectiveness%2520in%2520improving%2520the%2520robustness%2520of%2520neural%2520networks%252C%250Aadversarial%2520training%2520has%2520suffered%2520from%2520the%2520natural%2520accuracy%2520degradation%250Aproblem%252C%2520i.e.%252C%2520accuracy%2520on%2520natural%2520samples%2520has%2520reduced%2520significantly.%2520In%2520this%250Astudy%252C%2520we%2520reveal%2520that%2520natural%2520accuracy%2520degradation%2520is%2520highly%2520related%2520to%2520the%250Adisruption%2520of%2520the%2520natural%2520sample%2520topology%2520in%2520the%2520representation%2520space%2520by%250Aquantitative%2520and%2520qualitative%2520experiments.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%250ATopology-pReserving%2520Adversarial%2520traINing%2520%2528TRAIN%2529%2520to%2520alleviate%2520the%2520problem%2520by%250Apreserving%2520the%2520topology%2520structure%2520of%2520natural%2520samples%2520from%2520a%2520standard%2520model%250Atrained%2520only%2520on%2520natural%2520samples%2520during%2520adversarial%2520training.%2520As%2520an%2520additional%250Aregularization%252C%2520our%2520method%2520can%2520be%2520combined%2520with%2520various%2520popular%2520adversarial%250Atraining%2520algorithms%252C%2520taking%2520advantage%2520of%2520both%2520sides.%2520Extensive%2520experiments%2520on%250ACIFAR-10%252C%2520CIFAR-100%252C%2520and%2520Tiny%2520ImageNet%2520show%2520that%2520our%2520proposed%2520method%2520achieves%250Aconsistent%2520and%2520significant%2520improvements%2520over%2520various%2520strong%2520baselines%2520in%2520most%250Acases.%2520Specifically%252C%2520without%2520additional%2520data%252C%2520TRAIN%2520achieves%2520up%2520to%25208.86%2525%250Aimprovement%2520in%2520natural%2520accuracy%2520and%25206.33%2525%2520improvement%2520in%2520robust%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-preserving%20Adversarial%20Training%20for%20Alleviating%20Natural%0A%20%20Accuracy%20Degradation&entry.906535625=Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20Yepeng%20Weng%20and%20Danding%20Wang%20and%20Juan%20Cao%20and%20Sheng%20Tang%20and%20Peng%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20Despite%20the%20effectiveness%20in%20improving%20the%20robustness%20of%20neural%20networks%2C%0Aadversarial%20training%20has%20suffered%20from%20the%20natural%20accuracy%20degradation%0Aproblem%2C%20i.e.%2C%20accuracy%20on%20natural%20samples%20has%20reduced%20significantly.%20In%20this%0Astudy%2C%20we%20reveal%20that%20natural%20accuracy%20degradation%20is%20highly%20related%20to%20the%0Adisruption%20of%20the%20natural%20sample%20topology%20in%20the%20representation%20space%20by%0Aquantitative%20and%20qualitative%20experiments.%20Based%20on%20this%20observation%2C%20we%20propose%0ATopology-pReserving%20Adversarial%20traINing%20%28TRAIN%29%20to%20alleviate%20the%20problem%20by%0Apreserving%20the%20topology%20structure%20of%20natural%20samples%20from%20a%20standard%20model%0Atrained%20only%20on%20natural%20samples%20during%20adversarial%20training.%20As%20an%20additional%0Aregularization%2C%20our%20method%20can%20be%20combined%20with%20various%20popular%20adversarial%0Atraining%20algorithms%2C%20taking%20advantage%20of%20both%20sides.%20Extensive%20experiments%20on%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20ImageNet%20show%20that%20our%20proposed%20method%20achieves%0Aconsistent%20and%20significant%20improvements%20over%20various%20strong%20baselines%20in%20most%0Acases.%20Specifically%2C%20without%20additional%20data%2C%20TRAIN%20achieves%20up%20to%208.86%25%0Aimprovement%20in%20natural%20accuracy%20and%206.33%25%20improvement%20in%20robust%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17607v2&entry.124074799=Read"},
{"title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware\n  Attention Mechanism", "author": "Jiafeng Zhong and Bin Li and Jiangyan Yi", "abstract": "  The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.\n", "link": "http://arxiv.org/abs/2407.21611v2", "date": "2024-08-19", "relevancy": 2.4963, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism&body=Title%3A%20Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism%0AAuthor%3A%20Jiafeng%20Zhong%20and%20Bin%20Li%20and%20Jiangyan%20Yi%0AAbstract%3A%20%20%20The%20task%20of%20partially%20spoofed%20audio%20localization%20aims%20to%20accurately%20determine%0Aaudio%20authenticity%20at%20a%20frame%20level.%20Although%20some%20works%20have%20achieved%0Aencouraging%20results%2C%20utilizing%20boundary%20information%20within%20a%20single%20model%0Aremains%20an%20unexplored%20research%20topic.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%0Acalled%20Boundary-aware%20Attention%20Mechanism%20%28BAM%29.%20Specifically%2C%20it%20consists%20of%0Atwo%20core%20modules%3A%20Boundary%20Enhancement%20and%20Boundary%20Frame-wise%20Attention.%20The%0Aformer%20assembles%20the%20intra-frame%20and%20inter-frame%20information%20to%20extract%0Adiscriminative%20boundary%20features%20that%20are%20subsequently%20used%20for%20boundary%0Aposition%20detection%20and%20authenticity%20decision%2C%20while%20the%20latter%20leverages%0Aboundary%20prediction%20results%20to%20explicitly%20control%20the%20feature%20interaction%0Abetween%20frames%2C%20which%20achieves%20effective%20discrimination%20between%20real%20and%20fake%0Aframes.%20Experimental%20results%20on%20PartialSpoof%20database%20demonstrate%20our%20proposed%0Amethod%20achieves%20the%20best%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/media-sec-lab/BAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Partially%2520Spoofed%2520Audio%2520Localization%2520with%2520Boundary-aware%250A%2520%2520Attention%2520Mechanism%26entry.906535625%3DJiafeng%2520Zhong%2520and%2520Bin%2520Li%2520and%2520Jiangyan%2520Yi%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520partially%2520spoofed%2520audio%2520localization%2520aims%2520to%2520accurately%2520determine%250Aaudio%2520authenticity%2520at%2520a%2520frame%2520level.%2520Although%2520some%2520works%2520have%2520achieved%250Aencouraging%2520results%252C%2520utilizing%2520boundary%2520information%2520within%2520a%2520single%2520model%250Aremains%2520an%2520unexplored%2520research%2520topic.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%250Acalled%2520Boundary-aware%2520Attention%2520Mechanism%2520%2528BAM%2529.%2520Specifically%252C%2520it%2520consists%2520of%250Atwo%2520core%2520modules%253A%2520Boundary%2520Enhancement%2520and%2520Boundary%2520Frame-wise%2520Attention.%2520The%250Aformer%2520assembles%2520the%2520intra-frame%2520and%2520inter-frame%2520information%2520to%2520extract%250Adiscriminative%2520boundary%2520features%2520that%2520are%2520subsequently%2520used%2520for%2520boundary%250Aposition%2520detection%2520and%2520authenticity%2520decision%252C%2520while%2520the%2520latter%2520leverages%250Aboundary%2520prediction%2520results%2520to%2520explicitly%2520control%2520the%2520feature%2520interaction%250Abetween%2520frames%252C%2520which%2520achieves%2520effective%2520discrimination%2520between%2520real%2520and%2520fake%250Aframes.%2520Experimental%2520results%2520on%2520PartialSpoof%2520database%2520demonstrate%2520our%2520proposed%250Amethod%2520achieves%2520the%2520best%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/media-sec-lab/BAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism&entry.906535625=Jiafeng%20Zhong%20and%20Bin%20Li%20and%20Jiangyan%20Yi&entry.1292438233=%20%20The%20task%20of%20partially%20spoofed%20audio%20localization%20aims%20to%20accurately%20determine%0Aaudio%20authenticity%20at%20a%20frame%20level.%20Although%20some%20works%20have%20achieved%0Aencouraging%20results%2C%20utilizing%20boundary%20information%20within%20a%20single%20model%0Aremains%20an%20unexplored%20research%20topic.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%0Acalled%20Boundary-aware%20Attention%20Mechanism%20%28BAM%29.%20Specifically%2C%20it%20consists%20of%0Atwo%20core%20modules%3A%20Boundary%20Enhancement%20and%20Boundary%20Frame-wise%20Attention.%20The%0Aformer%20assembles%20the%20intra-frame%20and%20inter-frame%20information%20to%20extract%0Adiscriminative%20boundary%20features%20that%20are%20subsequently%20used%20for%20boundary%0Aposition%20detection%20and%20authenticity%20decision%2C%20while%20the%20latter%20leverages%0Aboundary%20prediction%20results%20to%20explicitly%20control%20the%20feature%20interaction%0Abetween%20frames%2C%20which%20achieves%20effective%20discrimination%20between%20real%20and%20fake%0Aframes.%20Experimental%20results%20on%20PartialSpoof%20database%20demonstrate%20our%20proposed%0Amethod%20achieves%20the%20best%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/media-sec-lab/BAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21611v2&entry.124074799=Read"},
{"title": "$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via\n  Geometry and Appearance Refinement", "author": "Haoyang Wang and Liming Liu and Quanlu Jia and Jiangkai Wu and Haodan Zhang and Peiheng Wang and Xinggong Zhang", "abstract": "  Mesh reconstruction based on Neural Radiance Fields (NeRF) is popular in a\nvariety of applications such as computer graphics, virtual reality, and medical\nimaging due to its efficiency in handling complex geometric structures and\nfacilitating real-time rendering. However, existing works often fail to capture\nfine geometric details accurately and struggle with optimizing rendering\nquality. To address these challenges, we propose a novel algorithm that\nprogressively generates and optimizes meshes from multi-view images. Our\napproach initiates with the training of a NeRF model to establish an initial\nSigned Distance Field (SDF) and a view-dependent appearance field.\nSubsequently, we iteratively refine the SDF through a differentiable mesh\nextraction method, continuously updating both the vertex positions and their\nconnectivity based on the loss from mesh differentiable rasterization, while\nalso optimizing the appearance representation. To further leverage\nhigh-fidelity and detail-rich representations from NeRF, we propose an\nonline-learning strategy based on Upper Confidence Bound (UCB) to enhance\nviewpoints by adaptively incorporating images rendered by the initial NeRF\nmodel into the training dataset. Through extensive experiments, we demonstrate\nthat our method delivers highly competitive and robust performance in both mesh\nrendering quality and geometric quality.\n", "link": "http://arxiv.org/abs/2408.10135v1", "date": "2024-08-19", "relevancy": 2.4785, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6728}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.603}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24R%5E2%24-Mesh%3A%20Reinforcement%20Learning%20Powered%20Mesh%20Reconstruction%20via%0A%20%20Geometry%20and%20Appearance%20Refinement&body=Title%3A%20%24R%5E2%24-Mesh%3A%20Reinforcement%20Learning%20Powered%20Mesh%20Reconstruction%20via%0A%20%20Geometry%20and%20Appearance%20Refinement%0AAuthor%3A%20Haoyang%20Wang%20and%20Liming%20Liu%20and%20Quanlu%20Jia%20and%20Jiangkai%20Wu%20and%20Haodan%20Zhang%20and%20Peiheng%20Wang%20and%20Xinggong%20Zhang%0AAbstract%3A%20%20%20Mesh%20reconstruction%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20is%20popular%20in%20a%0Avariety%20of%20applications%20such%20as%20computer%20graphics%2C%20virtual%20reality%2C%20and%20medical%0Aimaging%20due%20to%20its%20efficiency%20in%20handling%20complex%20geometric%20structures%20and%0Afacilitating%20real-time%20rendering.%20However%2C%20existing%20works%20often%20fail%20to%20capture%0Afine%20geometric%20details%20accurately%20and%20struggle%20with%20optimizing%20rendering%0Aquality.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20algorithm%20that%0Aprogressively%20generates%20and%20optimizes%20meshes%20from%20multi-view%20images.%20Our%0Aapproach%20initiates%20with%20the%20training%20of%20a%20NeRF%20model%20to%20establish%20an%20initial%0ASigned%20Distance%20Field%20%28SDF%29%20and%20a%20view-dependent%20appearance%20field.%0ASubsequently%2C%20we%20iteratively%20refine%20the%20SDF%20through%20a%20differentiable%20mesh%0Aextraction%20method%2C%20continuously%20updating%20both%20the%20vertex%20positions%20and%20their%0Aconnectivity%20based%20on%20the%20loss%20from%20mesh%20differentiable%20rasterization%2C%20while%0Aalso%20optimizing%20the%20appearance%20representation.%20To%20further%20leverage%0Ahigh-fidelity%20and%20detail-rich%20representations%20from%20NeRF%2C%20we%20propose%20an%0Aonline-learning%20strategy%20based%20on%20Upper%20Confidence%20Bound%20%28UCB%29%20to%20enhance%0Aviewpoints%20by%20adaptively%20incorporating%20images%20rendered%20by%20the%20initial%20NeRF%0Amodel%20into%20the%20training%20dataset.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20our%20method%20delivers%20highly%20competitive%20and%20robust%20performance%20in%20both%20mesh%0Arendering%20quality%20and%20geometric%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524R%255E2%2524-Mesh%253A%2520Reinforcement%2520Learning%2520Powered%2520Mesh%2520Reconstruction%2520via%250A%2520%2520Geometry%2520and%2520Appearance%2520Refinement%26entry.906535625%3DHaoyang%2520Wang%2520and%2520Liming%2520Liu%2520and%2520Quanlu%2520Jia%2520and%2520Jiangkai%2520Wu%2520and%2520Haodan%2520Zhang%2520and%2520Peiheng%2520Wang%2520and%2520Xinggong%2520Zhang%26entry.1292438233%3D%2520%2520Mesh%2520reconstruction%2520based%2520on%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520is%2520popular%2520in%2520a%250Avariety%2520of%2520applications%2520such%2520as%2520computer%2520graphics%252C%2520virtual%2520reality%252C%2520and%2520medical%250Aimaging%2520due%2520to%2520its%2520efficiency%2520in%2520handling%2520complex%2520geometric%2520structures%2520and%250Afacilitating%2520real-time%2520rendering.%2520However%252C%2520existing%2520works%2520often%2520fail%2520to%2520capture%250Afine%2520geometric%2520details%2520accurately%2520and%2520struggle%2520with%2520optimizing%2520rendering%250Aquality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520algorithm%2520that%250Aprogressively%2520generates%2520and%2520optimizes%2520meshes%2520from%2520multi-view%2520images.%2520Our%250Aapproach%2520initiates%2520with%2520the%2520training%2520of%2520a%2520NeRF%2520model%2520to%2520establish%2520an%2520initial%250ASigned%2520Distance%2520Field%2520%2528SDF%2529%2520and%2520a%2520view-dependent%2520appearance%2520field.%250ASubsequently%252C%2520we%2520iteratively%2520refine%2520the%2520SDF%2520through%2520a%2520differentiable%2520mesh%250Aextraction%2520method%252C%2520continuously%2520updating%2520both%2520the%2520vertex%2520positions%2520and%2520their%250Aconnectivity%2520based%2520on%2520the%2520loss%2520from%2520mesh%2520differentiable%2520rasterization%252C%2520while%250Aalso%2520optimizing%2520the%2520appearance%2520representation.%2520To%2520further%2520leverage%250Ahigh-fidelity%2520and%2520detail-rich%2520representations%2520from%2520NeRF%252C%2520we%2520propose%2520an%250Aonline-learning%2520strategy%2520based%2520on%2520Upper%2520Confidence%2520Bound%2520%2528UCB%2529%2520to%2520enhance%250Aviewpoints%2520by%2520adaptively%2520incorporating%2520images%2520rendered%2520by%2520the%2520initial%2520NeRF%250Amodel%2520into%2520the%2520training%2520dataset.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520our%2520method%2520delivers%2520highly%2520competitive%2520and%2520robust%2520performance%2520in%2520both%2520mesh%250Arendering%2520quality%2520and%2520geometric%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24R%5E2%24-Mesh%3A%20Reinforcement%20Learning%20Powered%20Mesh%20Reconstruction%20via%0A%20%20Geometry%20and%20Appearance%20Refinement&entry.906535625=Haoyang%20Wang%20and%20Liming%20Liu%20and%20Quanlu%20Jia%20and%20Jiangkai%20Wu%20and%20Haodan%20Zhang%20and%20Peiheng%20Wang%20and%20Xinggong%20Zhang&entry.1292438233=%20%20Mesh%20reconstruction%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20is%20popular%20in%20a%0Avariety%20of%20applications%20such%20as%20computer%20graphics%2C%20virtual%20reality%2C%20and%20medical%0Aimaging%20due%20to%20its%20efficiency%20in%20handling%20complex%20geometric%20structures%20and%0Afacilitating%20real-time%20rendering.%20However%2C%20existing%20works%20often%20fail%20to%20capture%0Afine%20geometric%20details%20accurately%20and%20struggle%20with%20optimizing%20rendering%0Aquality.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20algorithm%20that%0Aprogressively%20generates%20and%20optimizes%20meshes%20from%20multi-view%20images.%20Our%0Aapproach%20initiates%20with%20the%20training%20of%20a%20NeRF%20model%20to%20establish%20an%20initial%0ASigned%20Distance%20Field%20%28SDF%29%20and%20a%20view-dependent%20appearance%20field.%0ASubsequently%2C%20we%20iteratively%20refine%20the%20SDF%20through%20a%20differentiable%20mesh%0Aextraction%20method%2C%20continuously%20updating%20both%20the%20vertex%20positions%20and%20their%0Aconnectivity%20based%20on%20the%20loss%20from%20mesh%20differentiable%20rasterization%2C%20while%0Aalso%20optimizing%20the%20appearance%20representation.%20To%20further%20leverage%0Ahigh-fidelity%20and%20detail-rich%20representations%20from%20NeRF%2C%20we%20propose%20an%0Aonline-learning%20strategy%20based%20on%20Upper%20Confidence%20Bound%20%28UCB%29%20to%20enhance%0Aviewpoints%20by%20adaptively%20incorporating%20images%20rendered%20by%20the%20initial%20NeRF%0Amodel%20into%20the%20training%20dataset.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20our%20method%20delivers%20highly%20competitive%20and%20robust%20performance%20in%20both%20mesh%0Arendering%20quality%20and%20geometric%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10135v1&entry.124074799=Read"},
{"title": "Dynamic Resolution Guidance for Facial Expression Recognition", "author": "Songpan Wang and Xu Li and Tianxiang Jiang and Yuanlun Xie", "abstract": "  Facial expression recognition (FER) is vital for human-computer interaction\nand emotion analysis, yet recognizing expressions in low-resolution images\nremains challenging. This paper introduces a practical method called Dynamic\nResolution Guidance for Facial Expression Recognition (DRGFER) to effectively\nrecognize facial expressions in images with varying resolutions without\ncompromising FER model accuracy. Our framework comprises two main components:\nthe Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation\nFacial Expression Recognition Network (MRAFER). The RRN determines image\nresolution, outputs a binary vector, and the MRAFER assigns images to suitable\nfacial expression recognition networks based on resolution. We evaluated DRGFER\non widely-used datasets RAFDB and FERPlus, demonstrating that our method\nretains optimal model performance at each resolution and outperforms\nalternative resolution approaches. The proposed framework exhibits robustness\nagainst resolution variations and facial expressions, offering a promising\nsolution for real-world applications.\n", "link": "http://arxiv.org/abs/2404.06365v2", "date": "2024-08-19", "relevancy": 2.4717, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5023}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4946}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition&body=Title%3A%20Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition%0AAuthor%3A%20Songpan%20Wang%20and%20Xu%20Li%20and%20Tianxiang%20Jiang%20and%20Yuanlun%20Xie%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20%28FER%29%20is%20vital%20for%20human-computer%20interaction%0Aand%20emotion%20analysis%2C%20yet%20recognizing%20expressions%20in%20low-resolution%20images%0Aremains%20challenging.%20This%20paper%20introduces%20a%20practical%20method%20called%20Dynamic%0AResolution%20Guidance%20for%20Facial%20Expression%20Recognition%20%28DRGFER%29%20to%20effectively%0Arecognize%20facial%20expressions%20in%20images%20with%20varying%20resolutions%20without%0Acompromising%20FER%20model%20accuracy.%20Our%20framework%20comprises%20two%20main%20components%3A%0Athe%20Resolution%20Recognition%20Network%20%28RRN%29%20and%20the%20Multi-Resolution%20Adaptation%0AFacial%20Expression%20Recognition%20Network%20%28MRAFER%29.%20The%20RRN%20determines%20image%0Aresolution%2C%20outputs%20a%20binary%20vector%2C%20and%20the%20MRAFER%20assigns%20images%20to%20suitable%0Afacial%20expression%20recognition%20networks%20based%20on%20resolution.%20We%20evaluated%20DRGFER%0Aon%20widely-used%20datasets%20RAFDB%20and%20FERPlus%2C%20demonstrating%20that%20our%20method%0Aretains%20optimal%20model%20performance%20at%20each%20resolution%20and%20outperforms%0Aalternative%20resolution%20approaches.%20The%20proposed%20framework%20exhibits%20robustness%0Aagainst%20resolution%20variations%20and%20facial%20expressions%2C%20offering%20a%20promising%0Asolution%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06365v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Resolution%2520Guidance%2520for%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DSongpan%2520Wang%2520and%2520Xu%2520Li%2520and%2520Tianxiang%2520Jiang%2520and%2520Yuanlun%2520Xie%26entry.1292438233%3D%2520%2520Facial%2520expression%2520recognition%2520%2528FER%2529%2520is%2520vital%2520for%2520human-computer%2520interaction%250Aand%2520emotion%2520analysis%252C%2520yet%2520recognizing%2520expressions%2520in%2520low-resolution%2520images%250Aremains%2520challenging.%2520This%2520paper%2520introduces%2520a%2520practical%2520method%2520called%2520Dynamic%250AResolution%2520Guidance%2520for%2520Facial%2520Expression%2520Recognition%2520%2528DRGFER%2529%2520to%2520effectively%250Arecognize%2520facial%2520expressions%2520in%2520images%2520with%2520varying%2520resolutions%2520without%250Acompromising%2520FER%2520model%2520accuracy.%2520Our%2520framework%2520comprises%2520two%2520main%2520components%253A%250Athe%2520Resolution%2520Recognition%2520Network%2520%2528RRN%2529%2520and%2520the%2520Multi-Resolution%2520Adaptation%250AFacial%2520Expression%2520Recognition%2520Network%2520%2528MRAFER%2529.%2520The%2520RRN%2520determines%2520image%250Aresolution%252C%2520outputs%2520a%2520binary%2520vector%252C%2520and%2520the%2520MRAFER%2520assigns%2520images%2520to%2520suitable%250Afacial%2520expression%2520recognition%2520networks%2520based%2520on%2520resolution.%2520We%2520evaluated%2520DRGFER%250Aon%2520widely-used%2520datasets%2520RAFDB%2520and%2520FERPlus%252C%2520demonstrating%2520that%2520our%2520method%250Aretains%2520optimal%2520model%2520performance%2520at%2520each%2520resolution%2520and%2520outperforms%250Aalternative%2520resolution%2520approaches.%2520The%2520proposed%2520framework%2520exhibits%2520robustness%250Aagainst%2520resolution%2520variations%2520and%2520facial%2520expressions%252C%2520offering%2520a%2520promising%250Asolution%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06365v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition&entry.906535625=Songpan%20Wang%20and%20Xu%20Li%20and%20Tianxiang%20Jiang%20and%20Yuanlun%20Xie&entry.1292438233=%20%20Facial%20expression%20recognition%20%28FER%29%20is%20vital%20for%20human-computer%20interaction%0Aand%20emotion%20analysis%2C%20yet%20recognizing%20expressions%20in%20low-resolution%20images%0Aremains%20challenging.%20This%20paper%20introduces%20a%20practical%20method%20called%20Dynamic%0AResolution%20Guidance%20for%20Facial%20Expression%20Recognition%20%28DRGFER%29%20to%20effectively%0Arecognize%20facial%20expressions%20in%20images%20with%20varying%20resolutions%20without%0Acompromising%20FER%20model%20accuracy.%20Our%20framework%20comprises%20two%20main%20components%3A%0Athe%20Resolution%20Recognition%20Network%20%28RRN%29%20and%20the%20Multi-Resolution%20Adaptation%0AFacial%20Expression%20Recognition%20Network%20%28MRAFER%29.%20The%20RRN%20determines%20image%0Aresolution%2C%20outputs%20a%20binary%20vector%2C%20and%20the%20MRAFER%20assigns%20images%20to%20suitable%0Afacial%20expression%20recognition%20networks%20based%20on%20resolution.%20We%20evaluated%20DRGFER%0Aon%20widely-used%20datasets%20RAFDB%20and%20FERPlus%2C%20demonstrating%20that%20our%20method%0Aretains%20optimal%20model%20performance%20at%20each%20resolution%20and%20outperforms%0Aalternative%20resolution%20approaches.%20The%20proposed%20framework%20exhibits%20robustness%0Aagainst%20resolution%20variations%20and%20facial%20expressions%2C%20offering%20a%20promising%0Asolution%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06365v2&entry.124074799=Read"},
{"title": "GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent", "author": "Sascha Marton and Stefan L\u00fcdtke and Christian Bartelt and Heiner Stuckenschmidt", "abstract": "  Decision Trees (DTs) are commonly used for many machine learning tasks due to\ntheir high degree of interpretability. However, learning a DT from data is a\ndifficult optimization problem, as it is non-convex and non-differentiable.\nTherefore, common approaches learn DTs using a greedy growth algorithm that\nminimizes the impurity locally at each internal node. Unfortunately, this\ngreedy procedure can lead to inaccurate trees. In this paper, we present a\nnovel approach for learning hard, axis-aligned DTs with gradient descent. The\nproposed method uses backpropagation with a straight-through operator on a\ndense DT representation, to jointly optimize all tree parameters. Our approach\noutperforms existing methods on binary classification benchmarks and achieves\ncompetitive results for multi-class tasks. The method is available under:\nhttps://github.com/s-marton/GradTree\n", "link": "http://arxiv.org/abs/2305.03515v7", "date": "2024-08-19", "relevancy": 2.4528, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5113}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4842}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GradTree%3A%20Learning%20Axis-Aligned%20Decision%20Trees%20with%20Gradient%20Descent&body=Title%3A%20GradTree%3A%20Learning%20Axis-Aligned%20Decision%20Trees%20with%20Gradient%20Descent%0AAuthor%3A%20Sascha%20Marton%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt%0AAbstract%3A%20%20%20Decision%20Trees%20%28DTs%29%20are%20commonly%20used%20for%20many%20machine%20learning%20tasks%20due%20to%0Atheir%20high%20degree%20of%20interpretability.%20However%2C%20learning%20a%20DT%20from%20data%20is%20a%0Adifficult%20optimization%20problem%2C%20as%20it%20is%20non-convex%20and%20non-differentiable.%0ATherefore%2C%20common%20approaches%20learn%20DTs%20using%20a%20greedy%20growth%20algorithm%20that%0Aminimizes%20the%20impurity%20locally%20at%20each%20internal%20node.%20Unfortunately%2C%20this%0Agreedy%20procedure%20can%20lead%20to%20inaccurate%20trees.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20approach%20for%20learning%20hard%2C%20axis-aligned%20DTs%20with%20gradient%20descent.%20The%0Aproposed%20method%20uses%20backpropagation%20with%20a%20straight-through%20operator%20on%20a%0Adense%20DT%20representation%2C%20to%20jointly%20optimize%20all%20tree%20parameters.%20Our%20approach%0Aoutperforms%20existing%20methods%20on%20binary%20classification%20benchmarks%20and%20achieves%0Acompetitive%20results%20for%20multi-class%20tasks.%20The%20method%20is%20available%20under%3A%0Ahttps%3A//github.com/s-marton/GradTree%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.03515v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradTree%253A%2520Learning%2520Axis-Aligned%2520Decision%2520Trees%2520with%2520Gradient%2520Descent%26entry.906535625%3DSascha%2520Marton%2520and%2520Stefan%2520L%25C3%25BCdtke%2520and%2520Christian%2520Bartelt%2520and%2520Heiner%2520Stuckenschmidt%26entry.1292438233%3D%2520%2520Decision%2520Trees%2520%2528DTs%2529%2520are%2520commonly%2520used%2520for%2520many%2520machine%2520learning%2520tasks%2520due%2520to%250Atheir%2520high%2520degree%2520of%2520interpretability.%2520However%252C%2520learning%2520a%2520DT%2520from%2520data%2520is%2520a%250Adifficult%2520optimization%2520problem%252C%2520as%2520it%2520is%2520non-convex%2520and%2520non-differentiable.%250ATherefore%252C%2520common%2520approaches%2520learn%2520DTs%2520using%2520a%2520greedy%2520growth%2520algorithm%2520that%250Aminimizes%2520the%2520impurity%2520locally%2520at%2520each%2520internal%2520node.%2520Unfortunately%252C%2520this%250Agreedy%2520procedure%2520can%2520lead%2520to%2520inaccurate%2520trees.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Anovel%2520approach%2520for%2520learning%2520hard%252C%2520axis-aligned%2520DTs%2520with%2520gradient%2520descent.%2520The%250Aproposed%2520method%2520uses%2520backpropagation%2520with%2520a%2520straight-through%2520operator%2520on%2520a%250Adense%2520DT%2520representation%252C%2520to%2520jointly%2520optimize%2520all%2520tree%2520parameters.%2520Our%2520approach%250Aoutperforms%2520existing%2520methods%2520on%2520binary%2520classification%2520benchmarks%2520and%2520achieves%250Acompetitive%2520results%2520for%2520multi-class%2520tasks.%2520The%2520method%2520is%2520available%2520under%253A%250Ahttps%253A//github.com/s-marton/GradTree%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.03515v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GradTree%3A%20Learning%20Axis-Aligned%20Decision%20Trees%20with%20Gradient%20Descent&entry.906535625=Sascha%20Marton%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt&entry.1292438233=%20%20Decision%20Trees%20%28DTs%29%20are%20commonly%20used%20for%20many%20machine%20learning%20tasks%20due%20to%0Atheir%20high%20degree%20of%20interpretability.%20However%2C%20learning%20a%20DT%20from%20data%20is%20a%0Adifficult%20optimization%20problem%2C%20as%20it%20is%20non-convex%20and%20non-differentiable.%0ATherefore%2C%20common%20approaches%20learn%20DTs%20using%20a%20greedy%20growth%20algorithm%20that%0Aminimizes%20the%20impurity%20locally%20at%20each%20internal%20node.%20Unfortunately%2C%20this%0Agreedy%20procedure%20can%20lead%20to%20inaccurate%20trees.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20approach%20for%20learning%20hard%2C%20axis-aligned%20DTs%20with%20gradient%20descent.%20The%0Aproposed%20method%20uses%20backpropagation%20with%20a%20straight-through%20operator%20on%20a%0Adense%20DT%20representation%2C%20to%20jointly%20optimize%20all%20tree%20parameters.%20Our%20approach%0Aoutperforms%20existing%20methods%20on%20binary%20classification%20benchmarks%20and%20achieves%0Acompetitive%20results%20for%20multi-class%20tasks.%20The%20method%20is%20available%20under%3A%0Ahttps%3A//github.com/s-marton/GradTree%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03515v7&entry.124074799=Read"},
{"title": "NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented\n  Camera Pose Regressor and Uncertainty Quantification", "author": "Juyeop Han and Lukas Lao Beyer and Guilherme V. Cavalheiro and Sertac Karaman", "abstract": "  In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful\ntool for 3D reconstruction and novel view synthesis. However, the computational\ncost of NeRF rendering and degradation in quality due to the presence of\nartifacts pose significant challenges for its application in real-time and\nrobust robotic tasks, especially on embedded systems. This paper introduces a\nnovel framework that integrates NeRF-derived localization information with\nVisual-Inertial Odometry (VIO) to provide a robust solution for real-time\nrobotic navigation. By training an absolute pose regression network with\naugmented image data rendered from a NeRF and quantifying its uncertainty, our\napproach effectively counters positional drift and enhances system reliability.\nWe also establish a mathematically sound foundation for combining visual\ninertial navigation with camera localization neural networks, considering\nuncertainty under a Bayesian framework. Experimental validation in a\nphotorealistic simulation environment demonstrates significant improvements in\naccuracy compared to a conventional VIO approach.\n", "link": "http://arxiv.org/abs/2404.01400v2", "date": "2024-08-19", "relevancy": 2.4439, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6383}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVINS%3A%20Robust%20Visual%20Inertial%20Navigation%20Fused%20with%20NeRF-augmented%0A%20%20Camera%20Pose%20Regressor%20and%20Uncertainty%20Quantification&body=Title%3A%20NVINS%3A%20Robust%20Visual%20Inertial%20Navigation%20Fused%20with%20NeRF-augmented%0A%20%20Camera%20Pose%20Regressor%20and%20Uncertainty%20Quantification%0AAuthor%3A%20Juyeop%20Han%20and%20Lukas%20Lao%20Beyer%20and%20Guilherme%20V.%20Cavalheiro%20and%20Sertac%20Karaman%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20emerged%20as%20a%20powerful%0Atool%20for%203D%20reconstruction%20and%20novel%20view%20synthesis.%20However%2C%20the%20computational%0Acost%20of%20NeRF%20rendering%20and%20degradation%20in%20quality%20due%20to%20the%20presence%20of%0Aartifacts%20pose%20significant%20challenges%20for%20its%20application%20in%20real-time%20and%0Arobust%20robotic%20tasks%2C%20especially%20on%20embedded%20systems.%20This%20paper%20introduces%20a%0Anovel%20framework%20that%20integrates%20NeRF-derived%20localization%20information%20with%0AVisual-Inertial%20Odometry%20%28VIO%29%20to%20provide%20a%20robust%20solution%20for%20real-time%0Arobotic%20navigation.%20By%20training%20an%20absolute%20pose%20regression%20network%20with%0Aaugmented%20image%20data%20rendered%20from%20a%20NeRF%20and%20quantifying%20its%20uncertainty%2C%20our%0Aapproach%20effectively%20counters%20positional%20drift%20and%20enhances%20system%20reliability.%0AWe%20also%20establish%20a%20mathematically%20sound%20foundation%20for%20combining%20visual%0Ainertial%20navigation%20with%20camera%20localization%20neural%20networks%2C%20considering%0Auncertainty%20under%20a%20Bayesian%20framework.%20Experimental%20validation%20in%20a%0Aphotorealistic%20simulation%20environment%20demonstrates%20significant%20improvements%20in%0Aaccuracy%20compared%20to%20a%20conventional%20VIO%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVINS%253A%2520Robust%2520Visual%2520Inertial%2520Navigation%2520Fused%2520with%2520NeRF-augmented%250A%2520%2520Camera%2520Pose%2520Regressor%2520and%2520Uncertainty%2520Quantification%26entry.906535625%3DJuyeop%2520Han%2520and%2520Lukas%2520Lao%2520Beyer%2520and%2520Guilherme%2520V.%2520Cavalheiro%2520and%2520Sertac%2520Karaman%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520emerged%2520as%2520a%2520powerful%250Atool%2520for%25203D%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%2520However%252C%2520the%2520computational%250Acost%2520of%2520NeRF%2520rendering%2520and%2520degradation%2520in%2520quality%2520due%2520to%2520the%2520presence%2520of%250Aartifacts%2520pose%2520significant%2520challenges%2520for%2520its%2520application%2520in%2520real-time%2520and%250Arobust%2520robotic%2520tasks%252C%2520especially%2520on%2520embedded%2520systems.%2520This%2520paper%2520introduces%2520a%250Anovel%2520framework%2520that%2520integrates%2520NeRF-derived%2520localization%2520information%2520with%250AVisual-Inertial%2520Odometry%2520%2528VIO%2529%2520to%2520provide%2520a%2520robust%2520solution%2520for%2520real-time%250Arobotic%2520navigation.%2520By%2520training%2520an%2520absolute%2520pose%2520regression%2520network%2520with%250Aaugmented%2520image%2520data%2520rendered%2520from%2520a%2520NeRF%2520and%2520quantifying%2520its%2520uncertainty%252C%2520our%250Aapproach%2520effectively%2520counters%2520positional%2520drift%2520and%2520enhances%2520system%2520reliability.%250AWe%2520also%2520establish%2520a%2520mathematically%2520sound%2520foundation%2520for%2520combining%2520visual%250Ainertial%2520navigation%2520with%2520camera%2520localization%2520neural%2520networks%252C%2520considering%250Auncertainty%2520under%2520a%2520Bayesian%2520framework.%2520Experimental%2520validation%2520in%2520a%250Aphotorealistic%2520simulation%2520environment%2520demonstrates%2520significant%2520improvements%2520in%250Aaccuracy%2520compared%2520to%2520a%2520conventional%2520VIO%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVINS%3A%20Robust%20Visual%20Inertial%20Navigation%20Fused%20with%20NeRF-augmented%0A%20%20Camera%20Pose%20Regressor%20and%20Uncertainty%20Quantification&entry.906535625=Juyeop%20Han%20and%20Lukas%20Lao%20Beyer%20and%20Guilherme%20V.%20Cavalheiro%20and%20Sertac%20Karaman&entry.1292438233=%20%20In%20recent%20years%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20emerged%20as%20a%20powerful%0Atool%20for%203D%20reconstruction%20and%20novel%20view%20synthesis.%20However%2C%20the%20computational%0Acost%20of%20NeRF%20rendering%20and%20degradation%20in%20quality%20due%20to%20the%20presence%20of%0Aartifacts%20pose%20significant%20challenges%20for%20its%20application%20in%20real-time%20and%0Arobust%20robotic%20tasks%2C%20especially%20on%20embedded%20systems.%20This%20paper%20introduces%20a%0Anovel%20framework%20that%20integrates%20NeRF-derived%20localization%20information%20with%0AVisual-Inertial%20Odometry%20%28VIO%29%20to%20provide%20a%20robust%20solution%20for%20real-time%0Arobotic%20navigation.%20By%20training%20an%20absolute%20pose%20regression%20network%20with%0Aaugmented%20image%20data%20rendered%20from%20a%20NeRF%20and%20quantifying%20its%20uncertainty%2C%20our%0Aapproach%20effectively%20counters%20positional%20drift%20and%20enhances%20system%20reliability.%0AWe%20also%20establish%20a%20mathematically%20sound%20foundation%20for%20combining%20visual%0Ainertial%20navigation%20with%20camera%20localization%20neural%20networks%2C%20considering%0Auncertainty%20under%20a%20Bayesian%20framework.%20Experimental%20validation%20in%20a%0Aphotorealistic%20simulation%20environment%20demonstrates%20significant%20improvements%20in%0Aaccuracy%20compared%20to%20a%20conventional%20VIO%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01400v2&entry.124074799=Read"},
{"title": "Exploring Vacant Classes in Label-Skewed Federated Learning", "author": "Kuangpu Guo and Yuhe Ding and Jian Liang and Ran He and Zilei Wang and Tieniu Tan", "abstract": "  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Code is available in the supplementary material.\n", "link": "http://arxiv.org/abs/2401.02329v2", "date": "2024-08-19", "relevancy": 2.3982, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4909}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4873}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning&body=Title%3A%20Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning%0AAuthor%3A%20Kuangpu%20Guo%20and%20Yuhe%20Ding%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Label%20skews%2C%20characterized%20by%20disparities%20in%20local%20label%20distribution%20across%0Aclients%2C%20pose%20a%20significant%20challenge%20in%20federated%20learning.%20As%20minority%0Aclasses%20suffer%20from%20worse%20accuracy%20due%20to%20overfitting%20on%20local%20imbalanced%20data%2C%0Aprior%20methods%20often%20incorporate%20class-balanced%20learning%20techniques%20during%20local%0Atraining.%20Although%20these%20methods%20improve%20the%20mean%20accuracy%20across%20all%20classes%2C%0Awe%20observe%20that%20vacant%20classes-referring%20to%20categories%20absent%20from%20a%20client%27s%0Adata%20distribution-remain%20poorly%20recognized.%20Besides%2C%20there%20is%20still%20a%20gap%20in%0Athe%20accuracy%20of%20local%20models%20on%20minority%20classes%20compared%20to%20the%20global%20model.%0AThis%20paper%20introduces%20FedVLS%2C%20a%20novel%20approach%20to%20label-skewed%20federated%0Alearning%20that%20integrates%20both%20vacant-class%20distillation%20and%20logit%20suppression%0Asimultaneously.%20Specifically%2C%20vacant-class%20distillation%20leverages%20knowledge%0Adistillation%20during%20local%20training%20on%20each%20client%20to%20retain%20essential%0Ainformation%20related%20to%20vacant%20classes%20from%20the%20global%20model.%20Moreover%2C%20logit%0Asuppression%20directly%20penalizes%20network%20logits%20for%20non-label%20classes%2C%0Aeffectively%20addressing%20misclassifications%20in%20minority%20classes%20that%20may%20be%0Abiased%20toward%20majority%20classes.%20Extensive%20experiments%20validate%20the%20efficacy%20of%0AFedVLS%2C%20demonstrating%20superior%20performance%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20diverse%20datasets%20with%20varying%20degrees%20of%0Alabel%20skews.%20Code%20is%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Vacant%2520Classes%2520in%2520Label-Skewed%2520Federated%2520Learning%26entry.906535625%3DKuangpu%2520Guo%2520and%2520Yuhe%2520Ding%2520and%2520Jian%2520Liang%2520and%2520Ran%2520He%2520and%2520Zilei%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Label%2520skews%252C%2520characterized%2520by%2520disparities%2520in%2520local%2520label%2520distribution%2520across%250Aclients%252C%2520pose%2520a%2520significant%2520challenge%2520in%2520federated%2520learning.%2520As%2520minority%250Aclasses%2520suffer%2520from%2520worse%2520accuracy%2520due%2520to%2520overfitting%2520on%2520local%2520imbalanced%2520data%252C%250Aprior%2520methods%2520often%2520incorporate%2520class-balanced%2520learning%2520techniques%2520during%2520local%250Atraining.%2520Although%2520these%2520methods%2520improve%2520the%2520mean%2520accuracy%2520across%2520all%2520classes%252C%250Awe%2520observe%2520that%2520vacant%2520classes-referring%2520to%2520categories%2520absent%2520from%2520a%2520client%2527s%250Adata%2520distribution-remain%2520poorly%2520recognized.%2520Besides%252C%2520there%2520is%2520still%2520a%2520gap%2520in%250Athe%2520accuracy%2520of%2520local%2520models%2520on%2520minority%2520classes%2520compared%2520to%2520the%2520global%2520model.%250AThis%2520paper%2520introduces%2520FedVLS%252C%2520a%2520novel%2520approach%2520to%2520label-skewed%2520federated%250Alearning%2520that%2520integrates%2520both%2520vacant-class%2520distillation%2520and%2520logit%2520suppression%250Asimultaneously.%2520Specifically%252C%2520vacant-class%2520distillation%2520leverages%2520knowledge%250Adistillation%2520during%2520local%2520training%2520on%2520each%2520client%2520to%2520retain%2520essential%250Ainformation%2520related%2520to%2520vacant%2520classes%2520from%2520the%2520global%2520model.%2520Moreover%252C%2520logit%250Asuppression%2520directly%2520penalizes%2520network%2520logits%2520for%2520non-label%2520classes%252C%250Aeffectively%2520addressing%2520misclassifications%2520in%2520minority%2520classes%2520that%2520may%2520be%250Abiased%2520toward%2520majority%2520classes.%2520Extensive%2520experiments%2520validate%2520the%2520efficacy%2520of%250AFedVLS%252C%2520demonstrating%2520superior%2520performance%2520compared%2520to%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520across%2520diverse%2520datasets%2520with%2520varying%2520degrees%2520of%250Alabel%2520skews.%2520Code%2520is%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning&entry.906535625=Kuangpu%20Guo%20and%20Yuhe%20Ding%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20Label%20skews%2C%20characterized%20by%20disparities%20in%20local%20label%20distribution%20across%0Aclients%2C%20pose%20a%20significant%20challenge%20in%20federated%20learning.%20As%20minority%0Aclasses%20suffer%20from%20worse%20accuracy%20due%20to%20overfitting%20on%20local%20imbalanced%20data%2C%0Aprior%20methods%20often%20incorporate%20class-balanced%20learning%20techniques%20during%20local%0Atraining.%20Although%20these%20methods%20improve%20the%20mean%20accuracy%20across%20all%20classes%2C%0Awe%20observe%20that%20vacant%20classes-referring%20to%20categories%20absent%20from%20a%20client%27s%0Adata%20distribution-remain%20poorly%20recognized.%20Besides%2C%20there%20is%20still%20a%20gap%20in%0Athe%20accuracy%20of%20local%20models%20on%20minority%20classes%20compared%20to%20the%20global%20model.%0AThis%20paper%20introduces%20FedVLS%2C%20a%20novel%20approach%20to%20label-skewed%20federated%0Alearning%20that%20integrates%20both%20vacant-class%20distillation%20and%20logit%20suppression%0Asimultaneously.%20Specifically%2C%20vacant-class%20distillation%20leverages%20knowledge%0Adistillation%20during%20local%20training%20on%20each%20client%20to%20retain%20essential%0Ainformation%20related%20to%20vacant%20classes%20from%20the%20global%20model.%20Moreover%2C%20logit%0Asuppression%20directly%20penalizes%20network%20logits%20for%20non-label%20classes%2C%0Aeffectively%20addressing%20misclassifications%20in%20minority%20classes%20that%20may%20be%0Abiased%20toward%20majority%20classes.%20Extensive%20experiments%20validate%20the%20efficacy%20of%0AFedVLS%2C%20demonstrating%20superior%20performance%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20diverse%20datasets%20with%20varying%20degrees%20of%0Alabel%20skews.%20Code%20is%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02329v2&entry.124074799=Read"},
{"title": "Geometry Informed Tokenization of Molecules for Language Model\n  Generation", "author": "Xiner Li and Limei Wang and Youzhi Luo and Carl Edwards and Shurui Gui and Yuchao Lin and Heng Ji and Shuiwang Ji", "abstract": "  We consider molecule generation in 3D space using language models (LMs),\nwhich requires discrete tokenization of 3D molecular geometries. Although\ntokenization of molecular graphs exists, that for 3D geometries is largely\nunexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which\nconverts molecular geometries into $SE(3)$-invariant 1D discrete sequences.\nGeo2Seq consists of canonical labeling and invariant spherical representation\nsteps, which together maintain geometric and atomic fidelity in a format\nconducive to LMs. Our experiments show that, when coupled with Geo2Seq, various\nLMs excel in molecular geometry generation, especially in controlled generation\ntasks.\n", "link": "http://arxiv.org/abs/2408.10120v1", "date": "2024-08-19", "relevancy": 2.318, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4664}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4643}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Informed%20Tokenization%20of%20Molecules%20for%20Language%20Model%0A%20%20Generation&body=Title%3A%20Geometry%20Informed%20Tokenization%20of%20Molecules%20for%20Language%20Model%0A%20%20Generation%0AAuthor%3A%20Xiner%20Li%20and%20Limei%20Wang%20and%20Youzhi%20Luo%20and%20Carl%20Edwards%20and%20Shurui%20Gui%20and%20Yuchao%20Lin%20and%20Heng%20Ji%20and%20Shuiwang%20Ji%0AAbstract%3A%20%20%20We%20consider%20molecule%20generation%20in%203D%20space%20using%20language%20models%20%28LMs%29%2C%0Awhich%20requires%20discrete%20tokenization%20of%203D%20molecular%20geometries.%20Although%0Atokenization%20of%20molecular%20graphs%20exists%2C%20that%20for%203D%20geometries%20is%20largely%0Aunexplored.%20Here%2C%20we%20attempt%20to%20bridge%20this%20gap%20by%20proposing%20the%20Geo2Seq%2C%20which%0Aconverts%20molecular%20geometries%20into%20%24SE%283%29%24-invariant%201D%20discrete%20sequences.%0AGeo2Seq%20consists%20of%20canonical%20labeling%20and%20invariant%20spherical%20representation%0Asteps%2C%20which%20together%20maintain%20geometric%20and%20atomic%20fidelity%20in%20a%20format%0Aconducive%20to%20LMs.%20Our%20experiments%20show%20that%2C%20when%20coupled%20with%20Geo2Seq%2C%20various%0ALMs%20excel%20in%20molecular%20geometry%20generation%2C%20especially%20in%20controlled%20generation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Informed%2520Tokenization%2520of%2520Molecules%2520for%2520Language%2520Model%250A%2520%2520Generation%26entry.906535625%3DXiner%2520Li%2520and%2520Limei%2520Wang%2520and%2520Youzhi%2520Luo%2520and%2520Carl%2520Edwards%2520and%2520Shurui%2520Gui%2520and%2520Yuchao%2520Lin%2520and%2520Heng%2520Ji%2520and%2520Shuiwang%2520Ji%26entry.1292438233%3D%2520%2520We%2520consider%2520molecule%2520generation%2520in%25203D%2520space%2520using%2520language%2520models%2520%2528LMs%2529%252C%250Awhich%2520requires%2520discrete%2520tokenization%2520of%25203D%2520molecular%2520geometries.%2520Although%250Atokenization%2520of%2520molecular%2520graphs%2520exists%252C%2520that%2520for%25203D%2520geometries%2520is%2520largely%250Aunexplored.%2520Here%252C%2520we%2520attempt%2520to%2520bridge%2520this%2520gap%2520by%2520proposing%2520the%2520Geo2Seq%252C%2520which%250Aconverts%2520molecular%2520geometries%2520into%2520%2524SE%25283%2529%2524-invariant%25201D%2520discrete%2520sequences.%250AGeo2Seq%2520consists%2520of%2520canonical%2520labeling%2520and%2520invariant%2520spherical%2520representation%250Asteps%252C%2520which%2520together%2520maintain%2520geometric%2520and%2520atomic%2520fidelity%2520in%2520a%2520format%250Aconducive%2520to%2520LMs.%2520Our%2520experiments%2520show%2520that%252C%2520when%2520coupled%2520with%2520Geo2Seq%252C%2520various%250ALMs%2520excel%2520in%2520molecular%2520geometry%2520generation%252C%2520especially%2520in%2520controlled%2520generation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Informed%20Tokenization%20of%20Molecules%20for%20Language%20Model%0A%20%20Generation&entry.906535625=Xiner%20Li%20and%20Limei%20Wang%20and%20Youzhi%20Luo%20and%20Carl%20Edwards%20and%20Shurui%20Gui%20and%20Yuchao%20Lin%20and%20Heng%20Ji%20and%20Shuiwang%20Ji&entry.1292438233=%20%20We%20consider%20molecule%20generation%20in%203D%20space%20using%20language%20models%20%28LMs%29%2C%0Awhich%20requires%20discrete%20tokenization%20of%203D%20molecular%20geometries.%20Although%0Atokenization%20of%20molecular%20graphs%20exists%2C%20that%20for%203D%20geometries%20is%20largely%0Aunexplored.%20Here%2C%20we%20attempt%20to%20bridge%20this%20gap%20by%20proposing%20the%20Geo2Seq%2C%20which%0Aconverts%20molecular%20geometries%20into%20%24SE%283%29%24-invariant%201D%20discrete%20sequences.%0AGeo2Seq%20consists%20of%20canonical%20labeling%20and%20invariant%20spherical%20representation%0Asteps%2C%20which%20together%20maintain%20geometric%20and%20atomic%20fidelity%20in%20a%20format%0Aconducive%20to%20LMs.%20Our%20experiments%20show%20that%2C%20when%20coupled%20with%20Geo2Seq%2C%20various%0ALMs%20excel%20in%20molecular%20geometry%20generation%2C%20especially%20in%20controlled%20generation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10120v1&entry.124074799=Read"},
{"title": "Text-Conditioned Resampler For Long Form Video Understanding", "author": "Bruno Korbar and Yongqin Xian and Alessio Tonioni and Andrew Zisserman and Federico Tombari", "abstract": "  In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.\n", "link": "http://arxiv.org/abs/2312.11897v3", "date": "2024-08-19", "relevancy": 2.2783, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5958}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5745}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Conditioned%20Resampler%20For%20Long%20Form%20Video%20Understanding&body=Title%3A%20Text-Conditioned%20Resampler%20For%20Long%20Form%20Video%20Understanding%0AAuthor%3A%20Bruno%20Korbar%20and%20Yongqin%20Xian%20and%20Alessio%20Tonioni%20and%20Andrew%20Zisserman%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20a%20text-conditioned%20video%20resampler%20%28TCR%29%20module%20that%0Auses%20a%20pre-trained%20and%20frozen%20visual%20encoder%20and%20large%20language%20model%20%28LLM%29%20to%0Aprocess%20long%20video%20sequences%20for%20a%20task.%20TCR%20localises%20relevant%20visual%20features%0Afrom%20the%20video%20given%20a%20text%20condition%20and%20provides%20them%20to%20a%20LLM%20to%20generate%20a%0Atext%20response.%20Due%20to%20its%20lightweight%20design%20and%20use%20of%20cross-attention%2C%20TCR%0Acan%20process%20more%20than%20100%20frames%20at%20a%20time%20with%20plain%20attention%20and%20without%0Aoptimised%20implementations.%20We%20make%20the%20following%20contributions%3A%20%28i%29%20we%20design%20a%0Atransformer-based%20sampling%20architecture%20that%20can%20process%20long%20videos%0Aconditioned%20on%20a%20task%2C%20together%20with%20a%20training%20method%20that%20enables%20it%20to%0Abridge%20pre-trained%20visual%20and%20language%20models%3B%20%28ii%29%20we%20identify%20tasks%20that%0Acould%20benefit%20from%20longer%20video%20perception%3B%20and%20%28iii%29%20we%20empirically%20validate%0Aits%20efficacy%20on%20a%20wide%20variety%20of%20evaluation%20tasks%20including%20NextQA%2C%20EgoSchema%2C%0Aand%20the%20EGO4D-LTA%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Conditioned%2520Resampler%2520For%2520Long%2520Form%2520Video%2520Understanding%26entry.906535625%3DBruno%2520Korbar%2520and%2520Yongqin%2520Xian%2520and%2520Alessio%2520Tonioni%2520and%2520Andrew%2520Zisserman%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520a%2520text-conditioned%2520video%2520resampler%2520%2528TCR%2529%2520module%2520that%250Auses%2520a%2520pre-trained%2520and%2520frozen%2520visual%2520encoder%2520and%2520large%2520language%2520model%2520%2528LLM%2529%2520to%250Aprocess%2520long%2520video%2520sequences%2520for%2520a%2520task.%2520TCR%2520localises%2520relevant%2520visual%2520features%250Afrom%2520the%2520video%2520given%2520a%2520text%2520condition%2520and%2520provides%2520them%2520to%2520a%2520LLM%2520to%2520generate%2520a%250Atext%2520response.%2520Due%2520to%2520its%2520lightweight%2520design%2520and%2520use%2520of%2520cross-attention%252C%2520TCR%250Acan%2520process%2520more%2520than%2520100%2520frames%2520at%2520a%2520time%2520with%2520plain%2520attention%2520and%2520without%250Aoptimised%2520implementations.%2520We%2520make%2520the%2520following%2520contributions%253A%2520%2528i%2529%2520we%2520design%2520a%250Atransformer-based%2520sampling%2520architecture%2520that%2520can%2520process%2520long%2520videos%250Aconditioned%2520on%2520a%2520task%252C%2520together%2520with%2520a%2520training%2520method%2520that%2520enables%2520it%2520to%250Abridge%2520pre-trained%2520visual%2520and%2520language%2520models%253B%2520%2528ii%2529%2520we%2520identify%2520tasks%2520that%250Acould%2520benefit%2520from%2520longer%2520video%2520perception%253B%2520and%2520%2528iii%2529%2520we%2520empirically%2520validate%250Aits%2520efficacy%2520on%2520a%2520wide%2520variety%2520of%2520evaluation%2520tasks%2520including%2520NextQA%252C%2520EgoSchema%252C%250Aand%2520the%2520EGO4D-LTA%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Conditioned%20Resampler%20For%20Long%20Form%20Video%20Understanding&entry.906535625=Bruno%20Korbar%20and%20Yongqin%20Xian%20and%20Alessio%20Tonioni%20and%20Andrew%20Zisserman%20and%20Federico%20Tombari&entry.1292438233=%20%20In%20this%20paper%20we%20present%20a%20text-conditioned%20video%20resampler%20%28TCR%29%20module%20that%0Auses%20a%20pre-trained%20and%20frozen%20visual%20encoder%20and%20large%20language%20model%20%28LLM%29%20to%0Aprocess%20long%20video%20sequences%20for%20a%20task.%20TCR%20localises%20relevant%20visual%20features%0Afrom%20the%20video%20given%20a%20text%20condition%20and%20provides%20them%20to%20a%20LLM%20to%20generate%20a%0Atext%20response.%20Due%20to%20its%20lightweight%20design%20and%20use%20of%20cross-attention%2C%20TCR%0Acan%20process%20more%20than%20100%20frames%20at%20a%20time%20with%20plain%20attention%20and%20without%0Aoptimised%20implementations.%20We%20make%20the%20following%20contributions%3A%20%28i%29%20we%20design%20a%0Atransformer-based%20sampling%20architecture%20that%20can%20process%20long%20videos%0Aconditioned%20on%20a%20task%2C%20together%20with%20a%20training%20method%20that%20enables%20it%20to%0Abridge%20pre-trained%20visual%20and%20language%20models%3B%20%28ii%29%20we%20identify%20tasks%20that%0Acould%20benefit%20from%20longer%20video%20perception%3B%20and%20%28iii%29%20we%20empirically%20validate%0Aits%20efficacy%20on%20a%20wide%20variety%20of%20evaluation%20tasks%20including%20NextQA%2C%20EgoSchema%2C%0Aand%20the%20EGO4D-LTA%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11897v3&entry.124074799=Read"},
{"title": "A Unified Framework to Enforce, Discover, and Promote Symmetry in\n  Machine Learning", "author": "Samuel E. Otto and Nicholas Zolman and J. Nathan Kutz and Steven L. Brunton", "abstract": "  Symmetry is present throughout nature and continues to play an increasingly\ncentral role in physics and machine learning. Fundamental symmetries, such as\nPoincar\\'{e} invariance, allow physical laws discovered in laboratories on\nEarth to be extrapolated to the farthest reaches of the universe. Symmetry is\nessential to achieving this extrapolatory power in machine learning\napplications. For example, translation invariance in image classification\nallows models with fewer parameters, such as convolutional neural networks, to\nbe trained on smaller data sets and achieve state-of-the-art performance. In\nthis paper, we provide a unifying theoretical and methodological framework for\nincorporating symmetry into machine learning models in three ways: 1. enforcing\nknown symmetry when training a model; 2. discovering unknown symmetries of a\ngiven model or data set; and 3. promoting symmetry during training by learning\na model that breaks symmetries within a user-specified group of candidates when\nthere is sufficient evidence in the data. We show that these tasks can be cast\nwithin a common mathematical framework whose central object is the Lie\nderivative associated with fiber-linear Lie group actions on vector bundles. We\nextend and unify several existing results by showing that enforcing and\ndiscovering symmetry are linear-algebraic tasks that are dual with respect to\nthe bilinear structure of the Lie derivative. We also propose a novel way to\npromote symmetry by introducing a class of convex regularization functions\nbased on the Lie derivative and nuclear norm relaxation to penalize symmetry\nbreaking during training of machine learning models. We explain how these ideas\ncan be applied to a wide range of machine learning models including basis\nfunction regression, dynamical systems discovery, neural networks, and neural\noperators acting on fields.\n", "link": "http://arxiv.org/abs/2311.00212v2", "date": "2024-08-19", "relevancy": 2.2751, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4739}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20to%20Enforce%2C%20Discover%2C%20and%20Promote%20Symmetry%20in%0A%20%20Machine%20Learning&body=Title%3A%20A%20Unified%20Framework%20to%20Enforce%2C%20Discover%2C%20and%20Promote%20Symmetry%20in%0A%20%20Machine%20Learning%0AAuthor%3A%20Samuel%20E.%20Otto%20and%20Nicholas%20Zolman%20and%20J.%20Nathan%20Kutz%20and%20Steven%20L.%20Brunton%0AAbstract%3A%20%20%20Symmetry%20is%20present%20throughout%20nature%20and%20continues%20to%20play%20an%20increasingly%0Acentral%20role%20in%20physics%20and%20machine%20learning.%20Fundamental%20symmetries%2C%20such%20as%0APoincar%5C%27%7Be%7D%20invariance%2C%20allow%20physical%20laws%20discovered%20in%20laboratories%20on%0AEarth%20to%20be%20extrapolated%20to%20the%20farthest%20reaches%20of%20the%20universe.%20Symmetry%20is%0Aessential%20to%20achieving%20this%20extrapolatory%20power%20in%20machine%20learning%0Aapplications.%20For%20example%2C%20translation%20invariance%20in%20image%20classification%0Aallows%20models%20with%20fewer%20parameters%2C%20such%20as%20convolutional%20neural%20networks%2C%20to%0Abe%20trained%20on%20smaller%20data%20sets%20and%20achieve%20state-of-the-art%20performance.%20In%0Athis%20paper%2C%20we%20provide%20a%20unifying%20theoretical%20and%20methodological%20framework%20for%0Aincorporating%20symmetry%20into%20machine%20learning%20models%20in%20three%20ways%3A%201.%20enforcing%0Aknown%20symmetry%20when%20training%20a%20model%3B%202.%20discovering%20unknown%20symmetries%20of%20a%0Agiven%20model%20or%20data%20set%3B%20and%203.%20promoting%20symmetry%20during%20training%20by%20learning%0Aa%20model%20that%20breaks%20symmetries%20within%20a%20user-specified%20group%20of%20candidates%20when%0Athere%20is%20sufficient%20evidence%20in%20the%20data.%20We%20show%20that%20these%20tasks%20can%20be%20cast%0Awithin%20a%20common%20mathematical%20framework%20whose%20central%20object%20is%20the%20Lie%0Aderivative%20associated%20with%20fiber-linear%20Lie%20group%20actions%20on%20vector%20bundles.%20We%0Aextend%20and%20unify%20several%20existing%20results%20by%20showing%20that%20enforcing%20and%0Adiscovering%20symmetry%20are%20linear-algebraic%20tasks%20that%20are%20dual%20with%20respect%20to%0Athe%20bilinear%20structure%20of%20the%20Lie%20derivative.%20We%20also%20propose%20a%20novel%20way%20to%0Apromote%20symmetry%20by%20introducing%20a%20class%20of%20convex%20regularization%20functions%0Abased%20on%20the%20Lie%20derivative%20and%20nuclear%20norm%20relaxation%20to%20penalize%20symmetry%0Abreaking%20during%20training%20of%20machine%20learning%20models.%20We%20explain%20how%20these%20ideas%0Acan%20be%20applied%20to%20a%20wide%20range%20of%20machine%20learning%20models%20including%20basis%0Afunction%20regression%2C%20dynamical%20systems%20discovery%2C%20neural%20networks%2C%20and%20neural%0Aoperators%20acting%20on%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520to%2520Enforce%252C%2520Discover%252C%2520and%2520Promote%2520Symmetry%2520in%250A%2520%2520Machine%2520Learning%26entry.906535625%3DSamuel%2520E.%2520Otto%2520and%2520Nicholas%2520Zolman%2520and%2520J.%2520Nathan%2520Kutz%2520and%2520Steven%2520L.%2520Brunton%26entry.1292438233%3D%2520%2520Symmetry%2520is%2520present%2520throughout%2520nature%2520and%2520continues%2520to%2520play%2520an%2520increasingly%250Acentral%2520role%2520in%2520physics%2520and%2520machine%2520learning.%2520Fundamental%2520symmetries%252C%2520such%2520as%250APoincar%255C%2527%257Be%257D%2520invariance%252C%2520allow%2520physical%2520laws%2520discovered%2520in%2520laboratories%2520on%250AEarth%2520to%2520be%2520extrapolated%2520to%2520the%2520farthest%2520reaches%2520of%2520the%2520universe.%2520Symmetry%2520is%250Aessential%2520to%2520achieving%2520this%2520extrapolatory%2520power%2520in%2520machine%2520learning%250Aapplications.%2520For%2520example%252C%2520translation%2520invariance%2520in%2520image%2520classification%250Aallows%2520models%2520with%2520fewer%2520parameters%252C%2520such%2520as%2520convolutional%2520neural%2520networks%252C%2520to%250Abe%2520trained%2520on%2520smaller%2520data%2520sets%2520and%2520achieve%2520state-of-the-art%2520performance.%2520In%250Athis%2520paper%252C%2520we%2520provide%2520a%2520unifying%2520theoretical%2520and%2520methodological%2520framework%2520for%250Aincorporating%2520symmetry%2520into%2520machine%2520learning%2520models%2520in%2520three%2520ways%253A%25201.%2520enforcing%250Aknown%2520symmetry%2520when%2520training%2520a%2520model%253B%25202.%2520discovering%2520unknown%2520symmetries%2520of%2520a%250Agiven%2520model%2520or%2520data%2520set%253B%2520and%25203.%2520promoting%2520symmetry%2520during%2520training%2520by%2520learning%250Aa%2520model%2520that%2520breaks%2520symmetries%2520within%2520a%2520user-specified%2520group%2520of%2520candidates%2520when%250Athere%2520is%2520sufficient%2520evidence%2520in%2520the%2520data.%2520We%2520show%2520that%2520these%2520tasks%2520can%2520be%2520cast%250Awithin%2520a%2520common%2520mathematical%2520framework%2520whose%2520central%2520object%2520is%2520the%2520Lie%250Aderivative%2520associated%2520with%2520fiber-linear%2520Lie%2520group%2520actions%2520on%2520vector%2520bundles.%2520We%250Aextend%2520and%2520unify%2520several%2520existing%2520results%2520by%2520showing%2520that%2520enforcing%2520and%250Adiscovering%2520symmetry%2520are%2520linear-algebraic%2520tasks%2520that%2520are%2520dual%2520with%2520respect%2520to%250Athe%2520bilinear%2520structure%2520of%2520the%2520Lie%2520derivative.%2520We%2520also%2520propose%2520a%2520novel%2520way%2520to%250Apromote%2520symmetry%2520by%2520introducing%2520a%2520class%2520of%2520convex%2520regularization%2520functions%250Abased%2520on%2520the%2520Lie%2520derivative%2520and%2520nuclear%2520norm%2520relaxation%2520to%2520penalize%2520symmetry%250Abreaking%2520during%2520training%2520of%2520machine%2520learning%2520models.%2520We%2520explain%2520how%2520these%2520ideas%250Acan%2520be%2520applied%2520to%2520a%2520wide%2520range%2520of%2520machine%2520learning%2520models%2520including%2520basis%250Afunction%2520regression%252C%2520dynamical%2520systems%2520discovery%252C%2520neural%2520networks%252C%2520and%2520neural%250Aoperators%2520acting%2520on%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20to%20Enforce%2C%20Discover%2C%20and%20Promote%20Symmetry%20in%0A%20%20Machine%20Learning&entry.906535625=Samuel%20E.%20Otto%20and%20Nicholas%20Zolman%20and%20J.%20Nathan%20Kutz%20and%20Steven%20L.%20Brunton&entry.1292438233=%20%20Symmetry%20is%20present%20throughout%20nature%20and%20continues%20to%20play%20an%20increasingly%0Acentral%20role%20in%20physics%20and%20machine%20learning.%20Fundamental%20symmetries%2C%20such%20as%0APoincar%5C%27%7Be%7D%20invariance%2C%20allow%20physical%20laws%20discovered%20in%20laboratories%20on%0AEarth%20to%20be%20extrapolated%20to%20the%20farthest%20reaches%20of%20the%20universe.%20Symmetry%20is%0Aessential%20to%20achieving%20this%20extrapolatory%20power%20in%20machine%20learning%0Aapplications.%20For%20example%2C%20translation%20invariance%20in%20image%20classification%0Aallows%20models%20with%20fewer%20parameters%2C%20such%20as%20convolutional%20neural%20networks%2C%20to%0Abe%20trained%20on%20smaller%20data%20sets%20and%20achieve%20state-of-the-art%20performance.%20In%0Athis%20paper%2C%20we%20provide%20a%20unifying%20theoretical%20and%20methodological%20framework%20for%0Aincorporating%20symmetry%20into%20machine%20learning%20models%20in%20three%20ways%3A%201.%20enforcing%0Aknown%20symmetry%20when%20training%20a%20model%3B%202.%20discovering%20unknown%20symmetries%20of%20a%0Agiven%20model%20or%20data%20set%3B%20and%203.%20promoting%20symmetry%20during%20training%20by%20learning%0Aa%20model%20that%20breaks%20symmetries%20within%20a%20user-specified%20group%20of%20candidates%20when%0Athere%20is%20sufficient%20evidence%20in%20the%20data.%20We%20show%20that%20these%20tasks%20can%20be%20cast%0Awithin%20a%20common%20mathematical%20framework%20whose%20central%20object%20is%20the%20Lie%0Aderivative%20associated%20with%20fiber-linear%20Lie%20group%20actions%20on%20vector%20bundles.%20We%0Aextend%20and%20unify%20several%20existing%20results%20by%20showing%20that%20enforcing%20and%0Adiscovering%20symmetry%20are%20linear-algebraic%20tasks%20that%20are%20dual%20with%20respect%20to%0Athe%20bilinear%20structure%20of%20the%20Lie%20derivative.%20We%20also%20propose%20a%20novel%20way%20to%0Apromote%20symmetry%20by%20introducing%20a%20class%20of%20convex%20regularization%20functions%0Abased%20on%20the%20Lie%20derivative%20and%20nuclear%20norm%20relaxation%20to%20penalize%20symmetry%0Abreaking%20during%20training%20of%20machine%20learning%20models.%20We%20explain%20how%20these%20ideas%0Acan%20be%20applied%20to%20a%20wide%20range%20of%20machine%20learning%20models%20including%20basis%0Afunction%20regression%2C%20dynamical%20systems%20discovery%2C%20neural%20networks%2C%20and%20neural%0Aoperators%20acting%20on%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00212v2&entry.124074799=Read"},
{"title": "Differential Private Stochastic Optimization with Heavy-tailed Data:\n  Towards Optimal Rates", "author": "Puning Zhao and Jiafei Wu and Zhe Liu and Chong Wang and Rongfei Fan and Qingming Li", "abstract": "  We study convex optimization problems under differential privacy (DP). With\nheavy-tailed gradients, existing works achieve suboptimal rates. The main\nobstacle is that existing gradient estimators have suboptimal tail properties,\nresulting in a superfluous factor of $d$ in the union bound. In this paper, we\nexplore algorithms achieving optimal rates of DP optimization with heavy-tailed\ngradients. Our first method is a simple clipping approach. Under bounded $p$-th\norder moments of gradients, with $n$ samples, it achieves\n$\\tilde{O}(\\sqrt{d/n}+\\sqrt{d}(\\sqrt{d}/n\\epsilon)^{1-1/p})$ population risk\nwith $\\epsilon\\leq 1/\\sqrt{d}$. We then propose an iterative updating method,\nwhich is more complex but achieves this rate for all $\\epsilon\\leq 1$. The\nresults significantly improve over existing methods. Such improvement relies on\na careful treatment of the tail behavior of gradient estimators. Our results\nmatch the minimax lower bound in \\cite{kamath2022improved}, indicating that the\ntheoretical limit of stochastic convex optimization under DP is achievable.\n", "link": "http://arxiv.org/abs/2408.09891v1", "date": "2024-08-19", "relevancy": 2.2592, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4561}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Private%20Stochastic%20Optimization%20with%20Heavy-tailed%20Data%3A%0A%20%20Towards%20Optimal%20Rates&body=Title%3A%20Differential%20Private%20Stochastic%20Optimization%20with%20Heavy-tailed%20Data%3A%0A%20%20Towards%20Optimal%20Rates%0AAuthor%3A%20Puning%20Zhao%20and%20Jiafei%20Wu%20and%20Zhe%20Liu%20and%20Chong%20Wang%20and%20Rongfei%20Fan%20and%20Qingming%20Li%0AAbstract%3A%20%20%20We%20study%20convex%20optimization%20problems%20under%20differential%20privacy%20%28DP%29.%20With%0Aheavy-tailed%20gradients%2C%20existing%20works%20achieve%20suboptimal%20rates.%20The%20main%0Aobstacle%20is%20that%20existing%20gradient%20estimators%20have%20suboptimal%20tail%20properties%2C%0Aresulting%20in%20a%20superfluous%20factor%20of%20%24d%24%20in%20the%20union%20bound.%20In%20this%20paper%2C%20we%0Aexplore%20algorithms%20achieving%20optimal%20rates%20of%20DP%20optimization%20with%20heavy-tailed%0Agradients.%20Our%20first%20method%20is%20a%20simple%20clipping%20approach.%20Under%20bounded%20%24p%24-th%0Aorder%20moments%20of%20gradients%2C%20with%20%24n%24%20samples%2C%20it%20achieves%0A%24%5Ctilde%7BO%7D%28%5Csqrt%7Bd/n%7D%2B%5Csqrt%7Bd%7D%28%5Csqrt%7Bd%7D/n%5Cepsilon%29%5E%7B1-1/p%7D%29%24%20population%20risk%0Awith%20%24%5Cepsilon%5Cleq%201/%5Csqrt%7Bd%7D%24.%20We%20then%20propose%20an%20iterative%20updating%20method%2C%0Awhich%20is%20more%20complex%20but%20achieves%20this%20rate%20for%20all%20%24%5Cepsilon%5Cleq%201%24.%20The%0Aresults%20significantly%20improve%20over%20existing%20methods.%20Such%20improvement%20relies%20on%0Aa%20careful%20treatment%20of%20the%20tail%20behavior%20of%20gradient%20estimators.%20Our%20results%0Amatch%20the%20minimax%20lower%20bound%20in%20%5Ccite%7Bkamath2022improved%7D%2C%20indicating%20that%20the%0Atheoretical%20limit%20of%20stochastic%20convex%20optimization%20under%20DP%20is%20achievable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Private%2520Stochastic%2520Optimization%2520with%2520Heavy-tailed%2520Data%253A%250A%2520%2520Towards%2520Optimal%2520Rates%26entry.906535625%3DPuning%2520Zhao%2520and%2520Jiafei%2520Wu%2520and%2520Zhe%2520Liu%2520and%2520Chong%2520Wang%2520and%2520Rongfei%2520Fan%2520and%2520Qingming%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520convex%2520optimization%2520problems%2520under%2520differential%2520privacy%2520%2528DP%2529.%2520With%250Aheavy-tailed%2520gradients%252C%2520existing%2520works%2520achieve%2520suboptimal%2520rates.%2520The%2520main%250Aobstacle%2520is%2520that%2520existing%2520gradient%2520estimators%2520have%2520suboptimal%2520tail%2520properties%252C%250Aresulting%2520in%2520a%2520superfluous%2520factor%2520of%2520%2524d%2524%2520in%2520the%2520union%2520bound.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520algorithms%2520achieving%2520optimal%2520rates%2520of%2520DP%2520optimization%2520with%2520heavy-tailed%250Agradients.%2520Our%2520first%2520method%2520is%2520a%2520simple%2520clipping%2520approach.%2520Under%2520bounded%2520%2524p%2524-th%250Aorder%2520moments%2520of%2520gradients%252C%2520with%2520%2524n%2524%2520samples%252C%2520it%2520achieves%250A%2524%255Ctilde%257BO%257D%2528%255Csqrt%257Bd/n%257D%252B%255Csqrt%257Bd%257D%2528%255Csqrt%257Bd%257D/n%255Cepsilon%2529%255E%257B1-1/p%257D%2529%2524%2520population%2520risk%250Awith%2520%2524%255Cepsilon%255Cleq%25201/%255Csqrt%257Bd%257D%2524.%2520We%2520then%2520propose%2520an%2520iterative%2520updating%2520method%252C%250Awhich%2520is%2520more%2520complex%2520but%2520achieves%2520this%2520rate%2520for%2520all%2520%2524%255Cepsilon%255Cleq%25201%2524.%2520The%250Aresults%2520significantly%2520improve%2520over%2520existing%2520methods.%2520Such%2520improvement%2520relies%2520on%250Aa%2520careful%2520treatment%2520of%2520the%2520tail%2520behavior%2520of%2520gradient%2520estimators.%2520Our%2520results%250Amatch%2520the%2520minimax%2520lower%2520bound%2520in%2520%255Ccite%257Bkamath2022improved%257D%252C%2520indicating%2520that%2520the%250Atheoretical%2520limit%2520of%2520stochastic%2520convex%2520optimization%2520under%2520DP%2520is%2520achievable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Private%20Stochastic%20Optimization%20with%20Heavy-tailed%20Data%3A%0A%20%20Towards%20Optimal%20Rates&entry.906535625=Puning%20Zhao%20and%20Jiafei%20Wu%20and%20Zhe%20Liu%20and%20Chong%20Wang%20and%20Rongfei%20Fan%20and%20Qingming%20Li&entry.1292438233=%20%20We%20study%20convex%20optimization%20problems%20under%20differential%20privacy%20%28DP%29.%20With%0Aheavy-tailed%20gradients%2C%20existing%20works%20achieve%20suboptimal%20rates.%20The%20main%0Aobstacle%20is%20that%20existing%20gradient%20estimators%20have%20suboptimal%20tail%20properties%2C%0Aresulting%20in%20a%20superfluous%20factor%20of%20%24d%24%20in%20the%20union%20bound.%20In%20this%20paper%2C%20we%0Aexplore%20algorithms%20achieving%20optimal%20rates%20of%20DP%20optimization%20with%20heavy-tailed%0Agradients.%20Our%20first%20method%20is%20a%20simple%20clipping%20approach.%20Under%20bounded%20%24p%24-th%0Aorder%20moments%20of%20gradients%2C%20with%20%24n%24%20samples%2C%20it%20achieves%0A%24%5Ctilde%7BO%7D%28%5Csqrt%7Bd/n%7D%2B%5Csqrt%7Bd%7D%28%5Csqrt%7Bd%7D/n%5Cepsilon%29%5E%7B1-1/p%7D%29%24%20population%20risk%0Awith%20%24%5Cepsilon%5Cleq%201/%5Csqrt%7Bd%7D%24.%20We%20then%20propose%20an%20iterative%20updating%20method%2C%0Awhich%20is%20more%20complex%20but%20achieves%20this%20rate%20for%20all%20%24%5Cepsilon%5Cleq%201%24.%20The%0Aresults%20significantly%20improve%20over%20existing%20methods.%20Such%20improvement%20relies%20on%0Aa%20careful%20treatment%20of%20the%20tail%20behavior%20of%20gradient%20estimators.%20Our%20results%0Amatch%20the%20minimax%20lower%20bound%20in%20%5Ccite%7Bkamath2022improved%7D%2C%20indicating%20that%20the%0Atheoretical%20limit%20of%20stochastic%20convex%20optimization%20under%20DP%20is%20achievable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09891v1&entry.124074799=Read"},
{"title": "Localization in Dynamic Planar Environments Using Few Distance\n  Measurements", "author": "Michael M. Bilevich and Shahar Guini and Dan Halperin", "abstract": "  We present a method for determining the unknown location of a sensor placed\nin a known 2D environment in the presence of unknown dynamic obstacles, using\nonly few distance measurements. We present guarantees on the quality of the\nlocalization, which are robust under mild assumptions on the density of the\nunknown/dynamic obstacles in the known environment. We demonstrate the\neffectiveness of our method in simulated experiments for different environments\nand varying dynamic-obstacle density. Our open source software is available at\nhttps://github.com/TAU-CGL/vb-fdml2-public.\n", "link": "http://arxiv.org/abs/2407.03219v2", "date": "2024-08-19", "relevancy": 2.2513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5784}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5602}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements&body=Title%3A%20Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements%0AAuthor%3A%20Michael%20M.%20Bilevich%20and%20Shahar%20Guini%20and%20Dan%20Halperin%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20determining%20the%20unknown%20location%20of%20a%20sensor%20placed%0Ain%20a%20known%202D%20environment%20in%20the%20presence%20of%20unknown%20dynamic%20obstacles%2C%20using%0Aonly%20few%20distance%20measurements.%20We%20present%20guarantees%20on%20the%20quality%20of%20the%0Alocalization%2C%20which%20are%20robust%20under%20mild%20assumptions%20on%20the%20density%20of%20the%0Aunknown/dynamic%20obstacles%20in%20the%20known%20environment.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20simulated%20experiments%20for%20different%20environments%0Aand%20varying%20dynamic-obstacle%20density.%20Our%20open%20source%20software%20is%20available%20at%0Ahttps%3A//github.com/TAU-CGL/vb-fdml2-public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%2520in%2520Dynamic%2520Planar%2520Environments%2520Using%2520Few%2520Distance%250A%2520%2520Measurements%26entry.906535625%3DMichael%2520M.%2520Bilevich%2520and%2520Shahar%2520Guini%2520and%2520Dan%2520Halperin%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520determining%2520the%2520unknown%2520location%2520of%2520a%2520sensor%2520placed%250Ain%2520a%2520known%25202D%2520environment%2520in%2520the%2520presence%2520of%2520unknown%2520dynamic%2520obstacles%252C%2520using%250Aonly%2520few%2520distance%2520measurements.%2520We%2520present%2520guarantees%2520on%2520the%2520quality%2520of%2520the%250Alocalization%252C%2520which%2520are%2520robust%2520under%2520mild%2520assumptions%2520on%2520the%2520density%2520of%2520the%250Aunknown/dynamic%2520obstacles%2520in%2520the%2520known%2520environment.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520simulated%2520experiments%2520for%2520different%2520environments%250Aand%2520varying%2520dynamic-obstacle%2520density.%2520Our%2520open%2520source%2520software%2520is%2520available%2520at%250Ahttps%253A//github.com/TAU-CGL/vb-fdml2-public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements&entry.906535625=Michael%20M.%20Bilevich%20and%20Shahar%20Guini%20and%20Dan%20Halperin&entry.1292438233=%20%20We%20present%20a%20method%20for%20determining%20the%20unknown%20location%20of%20a%20sensor%20placed%0Ain%20a%20known%202D%20environment%20in%20the%20presence%20of%20unknown%20dynamic%20obstacles%2C%20using%0Aonly%20few%20distance%20measurements.%20We%20present%20guarantees%20on%20the%20quality%20of%20the%0Alocalization%2C%20which%20are%20robust%20under%20mild%20assumptions%20on%20the%20density%20of%20the%0Aunknown/dynamic%20obstacles%20in%20the%20known%20environment.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20simulated%20experiments%20for%20different%20environments%0Aand%20varying%20dynamic-obstacle%20density.%20Our%20open%20source%20software%20is%20available%20at%0Ahttps%3A//github.com/TAU-CGL/vb-fdml2-public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03219v2&entry.124074799=Read"},
{"title": "OccMamba: Semantic Occupancy Prediction with State Space Models", "author": "Heng Li and Yuenan Hou and Xiaohan Xing and Xiao Sun and Yanyong Zhang", "abstract": "  Training deep learning models for semantic occupancy prediction is\nchallenging due to factors such as a large number of occupancy cells, severe\nocclusion, limited visual cues, complicated driving scenarios, etc. Recent\nmethods often adopt transformer-based architectures given their strong\ncapability in learning input-conditioned weights and long-range relationships.\nHowever, transformer-based networks are notorious for their quadratic\ncomputation complexity, seriously undermining their efficacy and deployment in\nsemantic occupancy prediction. Inspired by the global modeling and linear\ncomputation complexity of the Mamba architecture, we present the first\nMamba-based network for semantic occupancy prediction, termed OccMamba.\nHowever, directly applying the Mamba architecture to the occupancy prediction\ntask yields unsatisfactory performance due to the inherent domain gap between\nthe linguistic and 3D domains. To relieve this problem, we present a simple yet\neffective 3D-to-1D reordering operation, i.e., height-prioritized 2D Hilbert\nexpansion. It can maximally retain the spatial structure of point clouds as\nwell as facilitate the processing of Mamba blocks. Our OccMamba achieves\nstate-of-the-art performance on three prevalent occupancy prediction\nbenchmarks, including OpenOccupancy, SemanticKITTI and SemanticPOSS. Notably,\non OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ\nby 3.1% IoU and 3.2% mIoU, respectively. Codes will be released upon\npublication.\n", "link": "http://arxiv.org/abs/2408.09859v1", "date": "2024-08-19", "relevancy": 2.2161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6238}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccMamba%3A%20Semantic%20Occupancy%20Prediction%20with%20State%20Space%20Models&body=Title%3A%20OccMamba%3A%20Semantic%20Occupancy%20Prediction%20with%20State%20Space%20Models%0AAuthor%3A%20Heng%20Li%20and%20Yuenan%20Hou%20and%20Xiaohan%20Xing%20and%20Xiao%20Sun%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20Training%20deep%20learning%20models%20for%20semantic%20occupancy%20prediction%20is%0Achallenging%20due%20to%20factors%20such%20as%20a%20large%20number%20of%20occupancy%20cells%2C%20severe%0Aocclusion%2C%20limited%20visual%20cues%2C%20complicated%20driving%20scenarios%2C%20etc.%20Recent%0Amethods%20often%20adopt%20transformer-based%20architectures%20given%20their%20strong%0Acapability%20in%20learning%20input-conditioned%20weights%20and%20long-range%20relationships.%0AHowever%2C%20transformer-based%20networks%20are%20notorious%20for%20their%20quadratic%0Acomputation%20complexity%2C%20seriously%20undermining%20their%20efficacy%20and%20deployment%20in%0Asemantic%20occupancy%20prediction.%20Inspired%20by%20the%20global%20modeling%20and%20linear%0Acomputation%20complexity%20of%20the%20Mamba%20architecture%2C%20we%20present%20the%20first%0AMamba-based%20network%20for%20semantic%20occupancy%20prediction%2C%20termed%20OccMamba.%0AHowever%2C%20directly%20applying%20the%20Mamba%20architecture%20to%20the%20occupancy%20prediction%0Atask%20yields%20unsatisfactory%20performance%20due%20to%20the%20inherent%20domain%20gap%20between%0Athe%20linguistic%20and%203D%20domains.%20To%20relieve%20this%20problem%2C%20we%20present%20a%20simple%20yet%0Aeffective%203D-to-1D%20reordering%20operation%2C%20i.e.%2C%20height-prioritized%202D%20Hilbert%0Aexpansion.%20It%20can%20maximally%20retain%20the%20spatial%20structure%20of%20point%20clouds%20as%0Awell%20as%20facilitate%20the%20processing%20of%20Mamba%20blocks.%20Our%20OccMamba%20achieves%0Astate-of-the-art%20performance%20on%20three%20prevalent%20occupancy%20prediction%0Abenchmarks%2C%20including%20OpenOccupancy%2C%20SemanticKITTI%20and%20SemanticPOSS.%20Notably%2C%0Aon%20OpenOccupancy%2C%20our%20OccMamba%20outperforms%20the%20previous%20state-of-the-art%20Co-Occ%0Aby%203.1%25%20IoU%20and%203.2%25%20mIoU%2C%20respectively.%20Codes%20will%20be%20released%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccMamba%253A%2520Semantic%2520Occupancy%2520Prediction%2520with%2520State%2520Space%2520Models%26entry.906535625%3DHeng%2520Li%2520and%2520Yuenan%2520Hou%2520and%2520Xiaohan%2520Xing%2520and%2520Xiao%2520Sun%2520and%2520Yanyong%2520Zhang%26entry.1292438233%3D%2520%2520Training%2520deep%2520learning%2520models%2520for%2520semantic%2520occupancy%2520prediction%2520is%250Achallenging%2520due%2520to%2520factors%2520such%2520as%2520a%2520large%2520number%2520of%2520occupancy%2520cells%252C%2520severe%250Aocclusion%252C%2520limited%2520visual%2520cues%252C%2520complicated%2520driving%2520scenarios%252C%2520etc.%2520Recent%250Amethods%2520often%2520adopt%2520transformer-based%2520architectures%2520given%2520their%2520strong%250Acapability%2520in%2520learning%2520input-conditioned%2520weights%2520and%2520long-range%2520relationships.%250AHowever%252C%2520transformer-based%2520networks%2520are%2520notorious%2520for%2520their%2520quadratic%250Acomputation%2520complexity%252C%2520seriously%2520undermining%2520their%2520efficacy%2520and%2520deployment%2520in%250Asemantic%2520occupancy%2520prediction.%2520Inspired%2520by%2520the%2520global%2520modeling%2520and%2520linear%250Acomputation%2520complexity%2520of%2520the%2520Mamba%2520architecture%252C%2520we%2520present%2520the%2520first%250AMamba-based%2520network%2520for%2520semantic%2520occupancy%2520prediction%252C%2520termed%2520OccMamba.%250AHowever%252C%2520directly%2520applying%2520the%2520Mamba%2520architecture%2520to%2520the%2520occupancy%2520prediction%250Atask%2520yields%2520unsatisfactory%2520performance%2520due%2520to%2520the%2520inherent%2520domain%2520gap%2520between%250Athe%2520linguistic%2520and%25203D%2520domains.%2520To%2520relieve%2520this%2520problem%252C%2520we%2520present%2520a%2520simple%2520yet%250Aeffective%25203D-to-1D%2520reordering%2520operation%252C%2520i.e.%252C%2520height-prioritized%25202D%2520Hilbert%250Aexpansion.%2520It%2520can%2520maximally%2520retain%2520the%2520spatial%2520structure%2520of%2520point%2520clouds%2520as%250Awell%2520as%2520facilitate%2520the%2520processing%2520of%2520Mamba%2520blocks.%2520Our%2520OccMamba%2520achieves%250Astate-of-the-art%2520performance%2520on%2520three%2520prevalent%2520occupancy%2520prediction%250Abenchmarks%252C%2520including%2520OpenOccupancy%252C%2520SemanticKITTI%2520and%2520SemanticPOSS.%2520Notably%252C%250Aon%2520OpenOccupancy%252C%2520our%2520OccMamba%2520outperforms%2520the%2520previous%2520state-of-the-art%2520Co-Occ%250Aby%25203.1%2525%2520IoU%2520and%25203.2%2525%2520mIoU%252C%2520respectively.%2520Codes%2520will%2520be%2520released%2520upon%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccMamba%3A%20Semantic%20Occupancy%20Prediction%20with%20State%20Space%20Models&entry.906535625=Heng%20Li%20and%20Yuenan%20Hou%20and%20Xiaohan%20Xing%20and%20Xiao%20Sun%20and%20Yanyong%20Zhang&entry.1292438233=%20%20Training%20deep%20learning%20models%20for%20semantic%20occupancy%20prediction%20is%0Achallenging%20due%20to%20factors%20such%20as%20a%20large%20number%20of%20occupancy%20cells%2C%20severe%0Aocclusion%2C%20limited%20visual%20cues%2C%20complicated%20driving%20scenarios%2C%20etc.%20Recent%0Amethods%20often%20adopt%20transformer-based%20architectures%20given%20their%20strong%0Acapability%20in%20learning%20input-conditioned%20weights%20and%20long-range%20relationships.%0AHowever%2C%20transformer-based%20networks%20are%20notorious%20for%20their%20quadratic%0Acomputation%20complexity%2C%20seriously%20undermining%20their%20efficacy%20and%20deployment%20in%0Asemantic%20occupancy%20prediction.%20Inspired%20by%20the%20global%20modeling%20and%20linear%0Acomputation%20complexity%20of%20the%20Mamba%20architecture%2C%20we%20present%20the%20first%0AMamba-based%20network%20for%20semantic%20occupancy%20prediction%2C%20termed%20OccMamba.%0AHowever%2C%20directly%20applying%20the%20Mamba%20architecture%20to%20the%20occupancy%20prediction%0Atask%20yields%20unsatisfactory%20performance%20due%20to%20the%20inherent%20domain%20gap%20between%0Athe%20linguistic%20and%203D%20domains.%20To%20relieve%20this%20problem%2C%20we%20present%20a%20simple%20yet%0Aeffective%203D-to-1D%20reordering%20operation%2C%20i.e.%2C%20height-prioritized%202D%20Hilbert%0Aexpansion.%20It%20can%20maximally%20retain%20the%20spatial%20structure%20of%20point%20clouds%20as%0Awell%20as%20facilitate%20the%20processing%20of%20Mamba%20blocks.%20Our%20OccMamba%20achieves%0Astate-of-the-art%20performance%20on%20three%20prevalent%20occupancy%20prediction%0Abenchmarks%2C%20including%20OpenOccupancy%2C%20SemanticKITTI%20and%20SemanticPOSS.%20Notably%2C%0Aon%20OpenOccupancy%2C%20our%20OccMamba%20outperforms%20the%20previous%20state-of-the-art%20Co-Occ%0Aby%203.1%25%20IoU%20and%203.2%25%20mIoU%2C%20respectively.%20Codes%20will%20be%20released%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09859v1&entry.124074799=Read"},
{"title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos", "author": "Fuzhao Xue and Yukang Chen and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Ethan He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han", "abstract": "  Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)\nsystem that enables long-context training and inference, enabling 2M context\nlength training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster\nthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in\ntext-only settings. Moreover, it seamlessly integrates with Hugging Face\nTransformers. For model training, we propose a five-stage pipeline comprising\nalignment, pre-training, context extension, and long-short joint supervised\nfine-tuning. Regarding datasets, we meticulously construct large-scale visual\nlanguage pre-training datasets and long video instruction-following datasets to\nsupport our multi-stage training process. The full-stack solution extends the\nfeasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and\nimproves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%\naccuracy in 1400-frames video (274k context length) needle in a haystack.\nLongVILA-8B also demonstrates a consistent improvement in performance on long\nvideos within the VideoMME benchmark as the video frames increase.\n", "link": "http://arxiv.org/abs/2408.10188v1", "date": "2024-08-19", "relevancy": 2.2063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5662}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5468}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&body=Title%3A%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos%0AAuthor%3A%20Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models.%20We%0Aintroduce%20LongVILA%2C%20a%20full-stack%20solution%20for%20long-context%20vision-language%0Amodels%2C%20including%20system%2C%20model%20training%2C%20and%20dataset%20development.%20On%20the%0Asystem%20side%2C%20we%20introduce%20the%20first%20Multi-Modal%20Sequence%20Parallelism%20%28MM-SP%29%0Asystem%20that%20enables%20long-context%20training%20and%20inference%2C%20enabling%202M%20context%0Alength%20training%20on%20256%20GPUs.%20MM-SP%20is%20also%20efficient%2C%20being%202.1x%20-%205.7x%20faster%0Athan%20Ring-Style%20Sequence%20Parallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron-LM%20in%0Atext-only%20settings.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%20For%20model%20training%2C%20we%20propose%20a%20five-stage%20pipeline%20comprising%0Aalignment%2C%20pre-training%2C%20context%20extension%2C%20and%20long-short%20joint%20supervised%0Afine-tuning.%20Regarding%20datasets%2C%20we%20meticulously%20construct%20large-scale%20visual%0Alanguage%20pre-training%20datasets%20and%20long%20video%20instruction-following%20datasets%20to%0Asupport%20our%20multi-stage%20training%20process.%20The%20full-stack%20solution%20extends%20the%0Afeasible%20frame%20number%20of%20VILA%20by%20a%20factor%20of%20128%20%28from%208%20to%201024%20frames%29%20and%0Aimproves%20long%20video%20captioning%20score%20from%202.00%20to%203.26%20%281.6x%29%2C%20achieving%2099.5%25%0Aaccuracy%20in%201400-frames%20video%20%28274k%20context%20length%29%20needle%20in%20a%20haystack.%0ALongVILA-8B%20also%20demonstrates%20a%20consistent%20improvement%20in%20performance%20on%20long%0Avideos%20within%20the%20VideoMME%20benchmark%20as%20the%20video%20frames%20increase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVILA%253A%2520Scaling%2520Long-Context%2520Visual%2520Language%2520Models%2520for%2520Long%2520Videos%26entry.906535625%3DFuzhao%2520Xue%2520and%2520Yukang%2520Chen%2520and%2520Dacheng%2520Li%2520and%2520Qinghao%2520Hu%2520and%2520Ligeng%2520Zhu%2520and%2520Xiuyu%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhijian%2520Liu%2520and%2520Ethan%2520He%2520and%2520Hongxu%2520Yin%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Long-context%2520capability%2520is%2520critical%2520for%2520multi-modal%2520foundation%2520models.%2520We%250Aintroduce%2520LongVILA%252C%2520a%2520full-stack%2520solution%2520for%2520long-context%2520vision-language%250Amodels%252C%2520including%2520system%252C%2520model%2520training%252C%2520and%2520dataset%2520development.%2520On%2520the%250Asystem%2520side%252C%2520we%2520introduce%2520the%2520first%2520Multi-Modal%2520Sequence%2520Parallelism%2520%2528MM-SP%2529%250Asystem%2520that%2520enables%2520long-context%2520training%2520and%2520inference%252C%2520enabling%25202M%2520context%250Alength%2520training%2520on%2520256%2520GPUs.%2520MM-SP%2520is%2520also%2520efficient%252C%2520being%25202.1x%2520-%25205.7x%2520faster%250Athan%2520Ring-Style%2520Sequence%2520Parallelism%2520and%25201.1x%2520-%25201.4x%2520faster%2520than%2520Megatron-LM%2520in%250Atext-only%2520settings.%2520Moreover%252C%2520it%2520seamlessly%2520integrates%2520with%2520Hugging%2520Face%250ATransformers.%2520For%2520model%2520training%252C%2520we%2520propose%2520a%2520five-stage%2520pipeline%2520comprising%250Aalignment%252C%2520pre-training%252C%2520context%2520extension%252C%2520and%2520long-short%2520joint%2520supervised%250Afine-tuning.%2520Regarding%2520datasets%252C%2520we%2520meticulously%2520construct%2520large-scale%2520visual%250Alanguage%2520pre-training%2520datasets%2520and%2520long%2520video%2520instruction-following%2520datasets%2520to%250Asupport%2520our%2520multi-stage%2520training%2520process.%2520The%2520full-stack%2520solution%2520extends%2520the%250Afeasible%2520frame%2520number%2520of%2520VILA%2520by%2520a%2520factor%2520of%2520128%2520%2528from%25208%2520to%25201024%2520frames%2529%2520and%250Aimproves%2520long%2520video%2520captioning%2520score%2520from%25202.00%2520to%25203.26%2520%25281.6x%2529%252C%2520achieving%252099.5%2525%250Aaccuracy%2520in%25201400-frames%2520video%2520%2528274k%2520context%2520length%2529%2520needle%2520in%2520a%2520haystack.%250ALongVILA-8B%2520also%2520demonstrates%2520a%2520consistent%2520improvement%2520in%2520performance%2520on%2520long%250Avideos%2520within%2520the%2520VideoMME%2520benchmark%2520as%2520the%2520video%2520frames%2520increase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&entry.906535625=Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models.%20We%0Aintroduce%20LongVILA%2C%20a%20full-stack%20solution%20for%20long-context%20vision-language%0Amodels%2C%20including%20system%2C%20model%20training%2C%20and%20dataset%20development.%20On%20the%0Asystem%20side%2C%20we%20introduce%20the%20first%20Multi-Modal%20Sequence%20Parallelism%20%28MM-SP%29%0Asystem%20that%20enables%20long-context%20training%20and%20inference%2C%20enabling%202M%20context%0Alength%20training%20on%20256%20GPUs.%20MM-SP%20is%20also%20efficient%2C%20being%202.1x%20-%205.7x%20faster%0Athan%20Ring-Style%20Sequence%20Parallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron-LM%20in%0Atext-only%20settings.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%20For%20model%20training%2C%20we%20propose%20a%20five-stage%20pipeline%20comprising%0Aalignment%2C%20pre-training%2C%20context%20extension%2C%20and%20long-short%20joint%20supervised%0Afine-tuning.%20Regarding%20datasets%2C%20we%20meticulously%20construct%20large-scale%20visual%0Alanguage%20pre-training%20datasets%20and%20long%20video%20instruction-following%20datasets%20to%0Asupport%20our%20multi-stage%20training%20process.%20The%20full-stack%20solution%20extends%20the%0Afeasible%20frame%20number%20of%20VILA%20by%20a%20factor%20of%20128%20%28from%208%20to%201024%20frames%29%20and%0Aimproves%20long%20video%20captioning%20score%20from%202.00%20to%203.26%20%281.6x%29%2C%20achieving%2099.5%25%0Aaccuracy%20in%201400-frames%20video%20%28274k%20context%20length%29%20needle%20in%20a%20haystack.%0ALongVILA-8B%20also%20demonstrates%20a%20consistent%20improvement%20in%20performance%20on%20long%0Avideos%20within%20the%20VideoMME%20benchmark%20as%20the%20video%20frames%20increase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10188v1&entry.124074799=Read"},
{"title": "Electron-nucleus cross sections from transfer learning", "author": "Krzysztof M. Graczyk and Beata E. Kowal and Artur M. Ankowski and Rwik Dharmapal Banerjee and Jose Luis Bonilla and Hemant Prasad and Jan T. Sobczyk", "abstract": "  Transfer learning (TL) allows a deep neural network (DNN) trained on one type\nof data to be adapted for new problems with limited information. We propose to\nuse the TL technique in physics. The DNN learns the physics of one process, and\nafter fine-tuning, it makes predictions for related processes. We consider the\nDNNs, trained on inclusive electron-carbon scattering data, and show that after\nfine-tuning, they accurately predict cross sections for electron interactions\nwith nuclear targets ranging from lithium to iron. The method works even when\nthe DNN is fine-tuned on a small dataset.\n", "link": "http://arxiv.org/abs/2408.09936v1", "date": "2024-08-19", "relevancy": 2.2061, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4641}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4307}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Electron-nucleus%20cross%20sections%20from%20transfer%20learning&body=Title%3A%20Electron-nucleus%20cross%20sections%20from%20transfer%20learning%0AAuthor%3A%20Krzysztof%20M.%20Graczyk%20and%20Beata%20E.%20Kowal%20and%20Artur%20M.%20Ankowski%20and%20Rwik%20Dharmapal%20Banerjee%20and%20Jose%20Luis%20Bonilla%20and%20Hemant%20Prasad%20and%20Jan%20T.%20Sobczyk%0AAbstract%3A%20%20%20Transfer%20learning%20%28TL%29%20allows%20a%20deep%20neural%20network%20%28DNN%29%20trained%20on%20one%20type%0Aof%20data%20to%20be%20adapted%20for%20new%20problems%20with%20limited%20information.%20We%20propose%20to%0Ause%20the%20TL%20technique%20in%20physics.%20The%20DNN%20learns%20the%20physics%20of%20one%20process%2C%20and%0Aafter%20fine-tuning%2C%20it%20makes%20predictions%20for%20related%20processes.%20We%20consider%20the%0ADNNs%2C%20trained%20on%20inclusive%20electron-carbon%20scattering%20data%2C%20and%20show%20that%20after%0Afine-tuning%2C%20they%20accurately%20predict%20cross%20sections%20for%20electron%20interactions%0Awith%20nuclear%20targets%20ranging%20from%20lithium%20to%20iron.%20The%20method%20works%20even%20when%0Athe%20DNN%20is%20fine-tuned%20on%20a%20small%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElectron-nucleus%2520cross%2520sections%2520from%2520transfer%2520learning%26entry.906535625%3DKrzysztof%2520M.%2520Graczyk%2520and%2520Beata%2520E.%2520Kowal%2520and%2520Artur%2520M.%2520Ankowski%2520and%2520Rwik%2520Dharmapal%2520Banerjee%2520and%2520Jose%2520Luis%2520Bonilla%2520and%2520Hemant%2520Prasad%2520and%2520Jan%2520T.%2520Sobczyk%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520%2528TL%2529%2520allows%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520trained%2520on%2520one%2520type%250Aof%2520data%2520to%2520be%2520adapted%2520for%2520new%2520problems%2520with%2520limited%2520information.%2520We%2520propose%2520to%250Ause%2520the%2520TL%2520technique%2520in%2520physics.%2520The%2520DNN%2520learns%2520the%2520physics%2520of%2520one%2520process%252C%2520and%250Aafter%2520fine-tuning%252C%2520it%2520makes%2520predictions%2520for%2520related%2520processes.%2520We%2520consider%2520the%250ADNNs%252C%2520trained%2520on%2520inclusive%2520electron-carbon%2520scattering%2520data%252C%2520and%2520show%2520that%2520after%250Afine-tuning%252C%2520they%2520accurately%2520predict%2520cross%2520sections%2520for%2520electron%2520interactions%250Awith%2520nuclear%2520targets%2520ranging%2520from%2520lithium%2520to%2520iron.%2520The%2520method%2520works%2520even%2520when%250Athe%2520DNN%2520is%2520fine-tuned%2520on%2520a%2520small%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Electron-nucleus%20cross%20sections%20from%20transfer%20learning&entry.906535625=Krzysztof%20M.%20Graczyk%20and%20Beata%20E.%20Kowal%20and%20Artur%20M.%20Ankowski%20and%20Rwik%20Dharmapal%20Banerjee%20and%20Jose%20Luis%20Bonilla%20and%20Hemant%20Prasad%20and%20Jan%20T.%20Sobczyk&entry.1292438233=%20%20Transfer%20learning%20%28TL%29%20allows%20a%20deep%20neural%20network%20%28DNN%29%20trained%20on%20one%20type%0Aof%20data%20to%20be%20adapted%20for%20new%20problems%20with%20limited%20information.%20We%20propose%20to%0Ause%20the%20TL%20technique%20in%20physics.%20The%20DNN%20learns%20the%20physics%20of%20one%20process%2C%20and%0Aafter%20fine-tuning%2C%20it%20makes%20predictions%20for%20related%20processes.%20We%20consider%20the%0ADNNs%2C%20trained%20on%20inclusive%20electron-carbon%20scattering%20data%2C%20and%20show%20that%20after%0Afine-tuning%2C%20they%20accurately%20predict%20cross%20sections%20for%20electron%20interactions%0Awith%20nuclear%20targets%20ranging%20from%20lithium%20to%20iron.%20The%20method%20works%20even%20when%0Athe%20DNN%20is%20fine-tuned%20on%20a%20small%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09936v1&entry.124074799=Read"},
{"title": "ML-CrAIST: Multi-scale Low-high Frequency Information-based Cross black\n  Attention with Image Super-resolving Transformer", "author": "Alik Pramanick and Utsav Bheda and Arijit Sur", "abstract": "  Recently, transformers have captured significant interest in the area of\nsingle-image super-resolution tasks, demonstrating substantial gains in\nperformance. Current models heavily depend on the network's extensive ability\nto extract high-level semantic details from images while overlooking the\neffective utilization of multi-scale image details and intermediate information\nwithin the network. Furthermore, it has been observed that high-frequency areas\nin images present significant complexity for super-resolution compared to\nlow-frequency areas. This work proposes a transformer-based super-resolution\narchitecture called ML-CrAIST that addresses this gap by utilizing low-high\nfrequency information in multiple scales. Unlike most of the previous work\n(either spatial or channel), we operate spatial and channel self-attention,\nwhich concurrently model pixel interaction from both spatial and channel\ndimensions, exploiting the inherent correlations across spatial and channel\naxis. Further, we devise a cross-attention block for super-resolution, which\nexplores the correlations between low and high-frequency information.\nQuantitative and qualitative assessments indicate that our proposed ML-CrAIST\nsurpasses state-of-the-art super-resolution methods (e.g., 0.15 dB gain\n@Manga109 $\\times$4). Code is available on:\nhttps://github.com/Alik033/ML-CrAIST.\n", "link": "http://arxiv.org/abs/2408.09940v1", "date": "2024-08-19", "relevancy": 2.2024, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6166}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ML-CrAIST%3A%20Multi-scale%20Low-high%20Frequency%20Information-based%20Cross%20black%0A%20%20Attention%20with%20Image%20Super-resolving%20Transformer&body=Title%3A%20ML-CrAIST%3A%20Multi-scale%20Low-high%20Frequency%20Information-based%20Cross%20black%0A%20%20Attention%20with%20Image%20Super-resolving%20Transformer%0AAuthor%3A%20Alik%20Pramanick%20and%20Utsav%20Bheda%20and%20Arijit%20Sur%0AAbstract%3A%20%20%20Recently%2C%20transformers%20have%20captured%20significant%20interest%20in%20the%20area%20of%0Asingle-image%20super-resolution%20tasks%2C%20demonstrating%20substantial%20gains%20in%0Aperformance.%20Current%20models%20heavily%20depend%20on%20the%20network%27s%20extensive%20ability%0Ato%20extract%20high-level%20semantic%20details%20from%20images%20while%20overlooking%20the%0Aeffective%20utilization%20of%20multi-scale%20image%20details%20and%20intermediate%20information%0Awithin%20the%20network.%20Furthermore%2C%20it%20has%20been%20observed%20that%20high-frequency%20areas%0Ain%20images%20present%20significant%20complexity%20for%20super-resolution%20compared%20to%0Alow-frequency%20areas.%20This%20work%20proposes%20a%20transformer-based%20super-resolution%0Aarchitecture%20called%20ML-CrAIST%20that%20addresses%20this%20gap%20by%20utilizing%20low-high%0Afrequency%20information%20in%20multiple%20scales.%20Unlike%20most%20of%20the%20previous%20work%0A%28either%20spatial%20or%20channel%29%2C%20we%20operate%20spatial%20and%20channel%20self-attention%2C%0Awhich%20concurrently%20model%20pixel%20interaction%20from%20both%20spatial%20and%20channel%0Adimensions%2C%20exploiting%20the%20inherent%20correlations%20across%20spatial%20and%20channel%0Aaxis.%20Further%2C%20we%20devise%20a%20cross-attention%20block%20for%20super-resolution%2C%20which%0Aexplores%20the%20correlations%20between%20low%20and%20high-frequency%20information.%0AQuantitative%20and%20qualitative%20assessments%20indicate%20that%20our%20proposed%20ML-CrAIST%0Asurpasses%20state-of-the-art%20super-resolution%20methods%20%28e.g.%2C%200.15%20dB%20gain%0A%40Manga109%20%24%5Ctimes%244%29.%20Code%20is%20available%20on%3A%0Ahttps%3A//github.com/Alik033/ML-CrAIST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DML-CrAIST%253A%2520Multi-scale%2520Low-high%2520Frequency%2520Information-based%2520Cross%2520black%250A%2520%2520Attention%2520with%2520Image%2520Super-resolving%2520Transformer%26entry.906535625%3DAlik%2520Pramanick%2520and%2520Utsav%2520Bheda%2520and%2520Arijit%2520Sur%26entry.1292438233%3D%2520%2520Recently%252C%2520transformers%2520have%2520captured%2520significant%2520interest%2520in%2520the%2520area%2520of%250Asingle-image%2520super-resolution%2520tasks%252C%2520demonstrating%2520substantial%2520gains%2520in%250Aperformance.%2520Current%2520models%2520heavily%2520depend%2520on%2520the%2520network%2527s%2520extensive%2520ability%250Ato%2520extract%2520high-level%2520semantic%2520details%2520from%2520images%2520while%2520overlooking%2520the%250Aeffective%2520utilization%2520of%2520multi-scale%2520image%2520details%2520and%2520intermediate%2520information%250Awithin%2520the%2520network.%2520Furthermore%252C%2520it%2520has%2520been%2520observed%2520that%2520high-frequency%2520areas%250Ain%2520images%2520present%2520significant%2520complexity%2520for%2520super-resolution%2520compared%2520to%250Alow-frequency%2520areas.%2520This%2520work%2520proposes%2520a%2520transformer-based%2520super-resolution%250Aarchitecture%2520called%2520ML-CrAIST%2520that%2520addresses%2520this%2520gap%2520by%2520utilizing%2520low-high%250Afrequency%2520information%2520in%2520multiple%2520scales.%2520Unlike%2520most%2520of%2520the%2520previous%2520work%250A%2528either%2520spatial%2520or%2520channel%2529%252C%2520we%2520operate%2520spatial%2520and%2520channel%2520self-attention%252C%250Awhich%2520concurrently%2520model%2520pixel%2520interaction%2520from%2520both%2520spatial%2520and%2520channel%250Adimensions%252C%2520exploiting%2520the%2520inherent%2520correlations%2520across%2520spatial%2520and%2520channel%250Aaxis.%2520Further%252C%2520we%2520devise%2520a%2520cross-attention%2520block%2520for%2520super-resolution%252C%2520which%250Aexplores%2520the%2520correlations%2520between%2520low%2520and%2520high-frequency%2520information.%250AQuantitative%2520and%2520qualitative%2520assessments%2520indicate%2520that%2520our%2520proposed%2520ML-CrAIST%250Asurpasses%2520state-of-the-art%2520super-resolution%2520methods%2520%2528e.g.%252C%25200.15%2520dB%2520gain%250A%2540Manga109%2520%2524%255Ctimes%25244%2529.%2520Code%2520is%2520available%2520on%253A%250Ahttps%253A//github.com/Alik033/ML-CrAIST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ML-CrAIST%3A%20Multi-scale%20Low-high%20Frequency%20Information-based%20Cross%20black%0A%20%20Attention%20with%20Image%20Super-resolving%20Transformer&entry.906535625=Alik%20Pramanick%20and%20Utsav%20Bheda%20and%20Arijit%20Sur&entry.1292438233=%20%20Recently%2C%20transformers%20have%20captured%20significant%20interest%20in%20the%20area%20of%0Asingle-image%20super-resolution%20tasks%2C%20demonstrating%20substantial%20gains%20in%0Aperformance.%20Current%20models%20heavily%20depend%20on%20the%20network%27s%20extensive%20ability%0Ato%20extract%20high-level%20semantic%20details%20from%20images%20while%20overlooking%20the%0Aeffective%20utilization%20of%20multi-scale%20image%20details%20and%20intermediate%20information%0Awithin%20the%20network.%20Furthermore%2C%20it%20has%20been%20observed%20that%20high-frequency%20areas%0Ain%20images%20present%20significant%20complexity%20for%20super-resolution%20compared%20to%0Alow-frequency%20areas.%20This%20work%20proposes%20a%20transformer-based%20super-resolution%0Aarchitecture%20called%20ML-CrAIST%20that%20addresses%20this%20gap%20by%20utilizing%20low-high%0Afrequency%20information%20in%20multiple%20scales.%20Unlike%20most%20of%20the%20previous%20work%0A%28either%20spatial%20or%20channel%29%2C%20we%20operate%20spatial%20and%20channel%20self-attention%2C%0Awhich%20concurrently%20model%20pixel%20interaction%20from%20both%20spatial%20and%20channel%0Adimensions%2C%20exploiting%20the%20inherent%20correlations%20across%20spatial%20and%20channel%0Aaxis.%20Further%2C%20we%20devise%20a%20cross-attention%20block%20for%20super-resolution%2C%20which%0Aexplores%20the%20correlations%20between%20low%20and%20high-frequency%20information.%0AQuantitative%20and%20qualitative%20assessments%20indicate%20that%20our%20proposed%20ML-CrAIST%0Asurpasses%20state-of-the-art%20super-resolution%20methods%20%28e.g.%2C%200.15%20dB%20gain%0A%40Manga109%20%24%5Ctimes%244%29.%20Code%20is%20available%20on%3A%0Ahttps%3A//github.com/Alik033/ML-CrAIST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09940v1&entry.124074799=Read"},
{"title": "HyperSurf: Quadruped Robot Leg Capable of Surface Recognition with GRU\n  and Real-to-Sim Transferring", "author": "Sergei Satsevich and Yaroslav Savotin and Danil Belov and Elizaveta Pestova and Artem Erhov and Batyr Khabibullin and Artem Bazhenov and Vyacheslav Kovalev and Aleksey Fedoseev and Dzmitry Tsetserukou", "abstract": "  This paper introduces a system of data collection acceleration and\nreal-to-sim transferring for surface recognition on a quadruped robot. The\nsystem features a mechanical single-leg setup capable of stepping on various\neasily interchangeable surfaces. Additionally, it incorporates a GRU-based\nSurface Recognition System, inspired by the system detailed in the Dog-Surf\npaper. This setup facilitates the expansion of dataset collection for model\ntraining, enabling data acquisition from hard-to-reach surfaces in laboratory\nconditions. Furthermore, it opens avenues for transferring surface properties\nfrom reality to simulation, thereby allowing the training of optimal gaits for\nlegged robots in simulation environments using a pre-prepared library of\ndigital twins of surfaces. Moreover, enhancements have been made to the\nGRU-based Surface Recognition System, allowing for the integration of data from\nboth the quadruped robot and the single-leg setup. The dataset and code have\nbeen made publicly available.\n", "link": "http://arxiv.org/abs/2407.15622v2", "date": "2024-08-19", "relevancy": 2.1837, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5532}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring&body=Title%3A%20HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring%0AAuthor%3A%20Sergei%20Satsevich%20and%20Yaroslav%20Savotin%20and%20Danil%20Belov%20and%20Elizaveta%20Pestova%20and%20Artem%20Erhov%20and%20Batyr%20Khabibullin%20and%20Artem%20Bazhenov%20and%20Vyacheslav%20Kovalev%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20system%20of%20data%20collection%20acceleration%20and%0Areal-to-sim%20transferring%20for%20surface%20recognition%20on%20a%20quadruped%20robot.%20The%0Asystem%20features%20a%20mechanical%20single-leg%20setup%20capable%20of%20stepping%20on%20various%0Aeasily%20interchangeable%20surfaces.%20Additionally%2C%20it%20incorporates%20a%20GRU-based%0ASurface%20Recognition%20System%2C%20inspired%20by%20the%20system%20detailed%20in%20the%20Dog-Surf%0Apaper.%20This%20setup%20facilitates%20the%20expansion%20of%20dataset%20collection%20for%20model%0Atraining%2C%20enabling%20data%20acquisition%20from%20hard-to-reach%20surfaces%20in%20laboratory%0Aconditions.%20Furthermore%2C%20it%20opens%20avenues%20for%20transferring%20surface%20properties%0Afrom%20reality%20to%20simulation%2C%20thereby%20allowing%20the%20training%20of%20optimal%20gaits%20for%0Alegged%20robots%20in%20simulation%20environments%20using%20a%20pre-prepared%20library%20of%0Adigital%20twins%20of%20surfaces.%20Moreover%2C%20enhancements%20have%20been%20made%20to%20the%0AGRU-based%20Surface%20Recognition%20System%2C%20allowing%20for%20the%20integration%20of%20data%20from%0Aboth%20the%20quadruped%20robot%20and%20the%20single-leg%20setup.%20The%20dataset%20and%20code%20have%0Abeen%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperSurf%253A%2520Quadruped%2520Robot%2520Leg%2520Capable%2520of%2520Surface%2520Recognition%2520with%2520GRU%250A%2520%2520and%2520Real-to-Sim%2520Transferring%26entry.906535625%3DSergei%2520Satsevich%2520and%2520Yaroslav%2520Savotin%2520and%2520Danil%2520Belov%2520and%2520Elizaveta%2520Pestova%2520and%2520Artem%2520Erhov%2520and%2520Batyr%2520Khabibullin%2520and%2520Artem%2520Bazhenov%2520and%2520Vyacheslav%2520Kovalev%2520and%2520Aleksey%2520Fedoseev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520system%2520of%2520data%2520collection%2520acceleration%2520and%250Areal-to-sim%2520transferring%2520for%2520surface%2520recognition%2520on%2520a%2520quadruped%2520robot.%2520The%250Asystem%2520features%2520a%2520mechanical%2520single-leg%2520setup%2520capable%2520of%2520stepping%2520on%2520various%250Aeasily%2520interchangeable%2520surfaces.%2520Additionally%252C%2520it%2520incorporates%2520a%2520GRU-based%250ASurface%2520Recognition%2520System%252C%2520inspired%2520by%2520the%2520system%2520detailed%2520in%2520the%2520Dog-Surf%250Apaper.%2520This%2520setup%2520facilitates%2520the%2520expansion%2520of%2520dataset%2520collection%2520for%2520model%250Atraining%252C%2520enabling%2520data%2520acquisition%2520from%2520hard-to-reach%2520surfaces%2520in%2520laboratory%250Aconditions.%2520Furthermore%252C%2520it%2520opens%2520avenues%2520for%2520transferring%2520surface%2520properties%250Afrom%2520reality%2520to%2520simulation%252C%2520thereby%2520allowing%2520the%2520training%2520of%2520optimal%2520gaits%2520for%250Alegged%2520robots%2520in%2520simulation%2520environments%2520using%2520a%2520pre-prepared%2520library%2520of%250Adigital%2520twins%2520of%2520surfaces.%2520Moreover%252C%2520enhancements%2520have%2520been%2520made%2520to%2520the%250AGRU-based%2520Surface%2520Recognition%2520System%252C%2520allowing%2520for%2520the%2520integration%2520of%2520data%2520from%250Aboth%2520the%2520quadruped%2520robot%2520and%2520the%2520single-leg%2520setup.%2520The%2520dataset%2520and%2520code%2520have%250Abeen%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring&entry.906535625=Sergei%20Satsevich%20and%20Yaroslav%20Savotin%20and%20Danil%20Belov%20and%20Elizaveta%20Pestova%20and%20Artem%20Erhov%20and%20Batyr%20Khabibullin%20and%20Artem%20Bazhenov%20and%20Vyacheslav%20Kovalev%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20introduces%20a%20system%20of%20data%20collection%20acceleration%20and%0Areal-to-sim%20transferring%20for%20surface%20recognition%20on%20a%20quadruped%20robot.%20The%0Asystem%20features%20a%20mechanical%20single-leg%20setup%20capable%20of%20stepping%20on%20various%0Aeasily%20interchangeable%20surfaces.%20Additionally%2C%20it%20incorporates%20a%20GRU-based%0ASurface%20Recognition%20System%2C%20inspired%20by%20the%20system%20detailed%20in%20the%20Dog-Surf%0Apaper.%20This%20setup%20facilitates%20the%20expansion%20of%20dataset%20collection%20for%20model%0Atraining%2C%20enabling%20data%20acquisition%20from%20hard-to-reach%20surfaces%20in%20laboratory%0Aconditions.%20Furthermore%2C%20it%20opens%20avenues%20for%20transferring%20surface%20properties%0Afrom%20reality%20to%20simulation%2C%20thereby%20allowing%20the%20training%20of%20optimal%20gaits%20for%0Alegged%20robots%20in%20simulation%20environments%20using%20a%20pre-prepared%20library%20of%0Adigital%20twins%20of%20surfaces.%20Moreover%2C%20enhancements%20have%20been%20made%20to%20the%0AGRU-based%20Surface%20Recognition%20System%2C%20allowing%20for%20the%20integration%20of%20data%20from%0Aboth%20the%20quadruped%20robot%20and%20the%20single-leg%20setup.%20The%20dataset%20and%20code%20have%0Abeen%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15622v2&entry.124074799=Read"},
{"title": "Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit\n  Adjustment", "author": "Zhanzhong Pang and Fadime Sener and Shrinivas Ramasubramanian and Angela Yao", "abstract": "  Procedural activity videos often exhibit a long-tailed action distribution\ndue to varying action frequencies and durations. However, state-of-the-art\ntemporal action segmentation methods overlook the long tail and fail to\nrecognize tail actions. Existing long-tail methods make class-independent\nassumptions and struggle to identify tail classes when applied to temporal\nsegmentation frameworks. This work proposes a novel group-wise temporal logit\nadjustment~(G-TLA) framework that combines a group-wise softmax formulation\nwhile leveraging activity information and action ordering for logit adjustment.\nThe proposed framework significantly improves in segmenting tail actions\nwithout any performance loss on head actions.\n", "link": "http://arxiv.org/abs/2408.09919v1", "date": "2024-08-19", "relevancy": 2.1489, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5551}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5248}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tail%20Temporal%20Action%20Segmentation%20with%20Group-wise%20Temporal%20Logit%0A%20%20Adjustment&body=Title%3A%20Long-Tail%20Temporal%20Action%20Segmentation%20with%20Group-wise%20Temporal%20Logit%0A%20%20Adjustment%0AAuthor%3A%20Zhanzhong%20Pang%20and%20Fadime%20Sener%20and%20Shrinivas%20Ramasubramanian%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Procedural%20activity%20videos%20often%20exhibit%20a%20long-tailed%20action%20distribution%0Adue%20to%20varying%20action%20frequencies%20and%20durations.%20However%2C%20state-of-the-art%0Atemporal%20action%20segmentation%20methods%20overlook%20the%20long%20tail%20and%20fail%20to%0Arecognize%20tail%20actions.%20Existing%20long-tail%20methods%20make%20class-independent%0Aassumptions%20and%20struggle%20to%20identify%20tail%20classes%20when%20applied%20to%20temporal%0Asegmentation%20frameworks.%20This%20work%20proposes%20a%20novel%20group-wise%20temporal%20logit%0Aadjustment~%28G-TLA%29%20framework%20that%20combines%20a%20group-wise%20softmax%20formulation%0Awhile%20leveraging%20activity%20information%20and%20action%20ordering%20for%20logit%20adjustment.%0AThe%20proposed%20framework%20significantly%20improves%20in%20segmenting%20tail%20actions%0Awithout%20any%20performance%20loss%20on%20head%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tail%2520Temporal%2520Action%2520Segmentation%2520with%2520Group-wise%2520Temporal%2520Logit%250A%2520%2520Adjustment%26entry.906535625%3DZhanzhong%2520Pang%2520and%2520Fadime%2520Sener%2520and%2520Shrinivas%2520Ramasubramanian%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520Procedural%2520activity%2520videos%2520often%2520exhibit%2520a%2520long-tailed%2520action%2520distribution%250Adue%2520to%2520varying%2520action%2520frequencies%2520and%2520durations.%2520However%252C%2520state-of-the-art%250Atemporal%2520action%2520segmentation%2520methods%2520overlook%2520the%2520long%2520tail%2520and%2520fail%2520to%250Arecognize%2520tail%2520actions.%2520Existing%2520long-tail%2520methods%2520make%2520class-independent%250Aassumptions%2520and%2520struggle%2520to%2520identify%2520tail%2520classes%2520when%2520applied%2520to%2520temporal%250Asegmentation%2520frameworks.%2520This%2520work%2520proposes%2520a%2520novel%2520group-wise%2520temporal%2520logit%250Aadjustment~%2528G-TLA%2529%2520framework%2520that%2520combines%2520a%2520group-wise%2520softmax%2520formulation%250Awhile%2520leveraging%2520activity%2520information%2520and%2520action%2520ordering%2520for%2520logit%2520adjustment.%250AThe%2520proposed%2520framework%2520significantly%2520improves%2520in%2520segmenting%2520tail%2520actions%250Awithout%2520any%2520performance%2520loss%2520on%2520head%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tail%20Temporal%20Action%20Segmentation%20with%20Group-wise%20Temporal%20Logit%0A%20%20Adjustment&entry.906535625=Zhanzhong%20Pang%20and%20Fadime%20Sener%20and%20Shrinivas%20Ramasubramanian%20and%20Angela%20Yao&entry.1292438233=%20%20Procedural%20activity%20videos%20often%20exhibit%20a%20long-tailed%20action%20distribution%0Adue%20to%20varying%20action%20frequencies%20and%20durations.%20However%2C%20state-of-the-art%0Atemporal%20action%20segmentation%20methods%20overlook%20the%20long%20tail%20and%20fail%20to%0Arecognize%20tail%20actions.%20Existing%20long-tail%20methods%20make%20class-independent%0Aassumptions%20and%20struggle%20to%20identify%20tail%20classes%20when%20applied%20to%20temporal%0Asegmentation%20frameworks.%20This%20work%20proposes%20a%20novel%20group-wise%20temporal%20logit%0Aadjustment~%28G-TLA%29%20framework%20that%20combines%20a%20group-wise%20softmax%20formulation%0Awhile%20leveraging%20activity%20information%20and%20action%20ordering%20for%20logit%20adjustment.%0AThe%20proposed%20framework%20significantly%20improves%20in%20segmenting%20tail%20actions%0Awithout%20any%20performance%20loss%20on%20head%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09919v1&entry.124074799=Read"},
{"title": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics", "author": "Jiaqi Yue and Jiancheng Zhao and Chunhui Zhao", "abstract": "  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n", "link": "http://arxiv.org/abs/2403.14362v4", "date": "2024-08-19", "relevancy": 2.1474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5365}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics&body=Title%3A%20Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics%0AAuthor%3A%20Jiaqi%20Yue%20and%20Jiancheng%20Zhao%20and%20Chunhui%20Zhao%0AAbstract%3A%20%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20focuses%20on%20recognizing%20seen%20and%20unseen%0Aclasses%20against%20domain%20shift%20problem%20%28DSP%29%20where%20data%20of%20unseen%20classes%20may%20be%0Amisclassified%20as%20seen%20classes.%20However%2C%20existing%20GZSL%20is%20still%20limited%20to%20seen%0Adomains.%20In%20the%20current%20work%2C%20we%20pioneer%20cross-domain%20GZSL%20%28CDGZSL%29%20which%0Aaddresses%20GZSL%20towards%20unseen%20domains.%20Different%20from%20existing%20GZSL%20methods%0Awhich%20alleviate%20DSP%20by%20generating%20features%20of%20unseen%20classes%20with%20semantics%2C%0ACDGZSL%20needs%20to%20construct%20a%20common%20feature%20space%20across%20domains%20and%20acquire%20the%0Acorresponding%20intrinsic%20semantics%20shared%20among%20domains%20to%20transfer%20from%20seen%20to%0Aunseen%20domains.%20Considering%20the%20information%20asymmetry%20problem%20caused%20by%0Aredundant%20class%20semantics%20annotated%20with%20large%20language%20models%20%28LLMs%29%2C%20we%0Apresent%20Meta%20Domain%20Alignment%20Semantic%20Refinement%20%28MDASR%29.%20Technically%2C%20MDASR%0Aconsists%20of%20two%20parts%3A%20Inter-class%20Similarity%20Alignment%20%28ISA%29%2C%20which%20eliminates%0Athe%20non-intrinsic%20semantics%20not%20shared%20across%20all%20domains%20under%20the%20guidance%20of%0Ainter-class%20feature%20relationships%2C%20and%20Unseen-class%20Meta%20Generation%20%28UMG%29%2C%0Awhich%20preserves%20intrinsic%20semantics%20to%20maintain%20connectivity%20between%20seen%20and%0Aunseen%20classes%20by%20simulating%20feature%20generation.%20MDASR%20effectively%20aligns%20the%0Aredundant%20semantic%20space%20with%20the%20common%20feature%20space%2C%20mitigating%20the%0Ainformation%20asymmetry%20in%20CDGZSL.%20The%20effectiveness%20of%20MDASR%20is%20demonstrated%20on%0Athe%20Office-Home%20and%20Mini-DomainNet%2C%20and%20we%20have%20shared%20the%20LLM-based%20semantics%0Afor%20these%20datasets%20as%20the%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14362v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520but%2520Better%253A%2520Enabling%2520Generalized%2520Zero-shot%2520Learning%2520Towards%2520Unseen%250A%2520%2520Domains%2520by%2520Intrinsic%2520Learning%2520from%2520Redundant%2520LLM%2520Semantics%26entry.906535625%3DJiaqi%2520Yue%2520and%2520Jiancheng%2520Zhao%2520and%2520Chunhui%2520Zhao%26entry.1292438233%3D%2520%2520Generalized%2520zero-shot%2520learning%2520%2528GZSL%2529%2520focuses%2520on%2520recognizing%2520seen%2520and%2520unseen%250Aclasses%2520against%2520domain%2520shift%2520problem%2520%2528DSP%2529%2520where%2520data%2520of%2520unseen%2520classes%2520may%2520be%250Amisclassified%2520as%2520seen%2520classes.%2520However%252C%2520existing%2520GZSL%2520is%2520still%2520limited%2520to%2520seen%250Adomains.%2520In%2520the%2520current%2520work%252C%2520we%2520pioneer%2520cross-domain%2520GZSL%2520%2528CDGZSL%2529%2520which%250Aaddresses%2520GZSL%2520towards%2520unseen%2520domains.%2520Different%2520from%2520existing%2520GZSL%2520methods%250Awhich%2520alleviate%2520DSP%2520by%2520generating%2520features%2520of%2520unseen%2520classes%2520with%2520semantics%252C%250ACDGZSL%2520needs%2520to%2520construct%2520a%2520common%2520feature%2520space%2520across%2520domains%2520and%2520acquire%2520the%250Acorresponding%2520intrinsic%2520semantics%2520shared%2520among%2520domains%2520to%2520transfer%2520from%2520seen%2520to%250Aunseen%2520domains.%2520Considering%2520the%2520information%2520asymmetry%2520problem%2520caused%2520by%250Aredundant%2520class%2520semantics%2520annotated%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%250Apresent%2520Meta%2520Domain%2520Alignment%2520Semantic%2520Refinement%2520%2528MDASR%2529.%2520Technically%252C%2520MDASR%250Aconsists%2520of%2520two%2520parts%253A%2520Inter-class%2520Similarity%2520Alignment%2520%2528ISA%2529%252C%2520which%2520eliminates%250Athe%2520non-intrinsic%2520semantics%2520not%2520shared%2520across%2520all%2520domains%2520under%2520the%2520guidance%2520of%250Ainter-class%2520feature%2520relationships%252C%2520and%2520Unseen-class%2520Meta%2520Generation%2520%2528UMG%2529%252C%250Awhich%2520preserves%2520intrinsic%2520semantics%2520to%2520maintain%2520connectivity%2520between%2520seen%2520and%250Aunseen%2520classes%2520by%2520simulating%2520feature%2520generation.%2520MDASR%2520effectively%2520aligns%2520the%250Aredundant%2520semantic%2520space%2520with%2520the%2520common%2520feature%2520space%252C%2520mitigating%2520the%250Ainformation%2520asymmetry%2520in%2520CDGZSL.%2520The%2520effectiveness%2520of%2520MDASR%2520is%2520demonstrated%2520on%250Athe%2520Office-Home%2520and%2520Mini-DomainNet%252C%2520and%2520we%2520have%2520shared%2520the%2520LLM-based%2520semantics%250Afor%2520these%2520datasets%2520as%2520the%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14362v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics&entry.906535625=Jiaqi%20Yue%20and%20Jiancheng%20Zhao%20and%20Chunhui%20Zhao&entry.1292438233=%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20focuses%20on%20recognizing%20seen%20and%20unseen%0Aclasses%20against%20domain%20shift%20problem%20%28DSP%29%20where%20data%20of%20unseen%20classes%20may%20be%0Amisclassified%20as%20seen%20classes.%20However%2C%20existing%20GZSL%20is%20still%20limited%20to%20seen%0Adomains.%20In%20the%20current%20work%2C%20we%20pioneer%20cross-domain%20GZSL%20%28CDGZSL%29%20which%0Aaddresses%20GZSL%20towards%20unseen%20domains.%20Different%20from%20existing%20GZSL%20methods%0Awhich%20alleviate%20DSP%20by%20generating%20features%20of%20unseen%20classes%20with%20semantics%2C%0ACDGZSL%20needs%20to%20construct%20a%20common%20feature%20space%20across%20domains%20and%20acquire%20the%0Acorresponding%20intrinsic%20semantics%20shared%20among%20domains%20to%20transfer%20from%20seen%20to%0Aunseen%20domains.%20Considering%20the%20information%20asymmetry%20problem%20caused%20by%0Aredundant%20class%20semantics%20annotated%20with%20large%20language%20models%20%28LLMs%29%2C%20we%0Apresent%20Meta%20Domain%20Alignment%20Semantic%20Refinement%20%28MDASR%29.%20Technically%2C%20MDASR%0Aconsists%20of%20two%20parts%3A%20Inter-class%20Similarity%20Alignment%20%28ISA%29%2C%20which%20eliminates%0Athe%20non-intrinsic%20semantics%20not%20shared%20across%20all%20domains%20under%20the%20guidance%20of%0Ainter-class%20feature%20relationships%2C%20and%20Unseen-class%20Meta%20Generation%20%28UMG%29%2C%0Awhich%20preserves%20intrinsic%20semantics%20to%20maintain%20connectivity%20between%20seen%20and%0Aunseen%20classes%20by%20simulating%20feature%20generation.%20MDASR%20effectively%20aligns%20the%0Aredundant%20semantic%20space%20with%20the%20common%20feature%20space%2C%20mitigating%20the%0Ainformation%20asymmetry%20in%20CDGZSL.%20The%20effectiveness%20of%20MDASR%20is%20demonstrated%20on%0Athe%20Office-Home%20and%20Mini-DomainNet%2C%20and%20we%20have%20shared%20the%20LLM-based%20semantics%0Afor%20these%20datasets%20as%20the%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14362v4&entry.124074799=Read"},
{"title": "AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment\n  for Improved Feature Integration", "author": "Hong Su", "abstract": "  In very deep neural networks, gradients can become extremely small during\nbackpropagation, making it challenging to train the early layers. ResNet\n(Residual Network) addresses this issue by enabling gradients to flow directly\nthrough the network via skip connections, facilitating the training of much\ndeeper networks. However, in these skip connections, the input ipd is directly\nadded to the transformed data tfd, treating ipd and tfd equally, without\nadapting to different scenarios. In this paper, we propose AdaResNet\n(Auto-Adapting Residual Network), which automatically adjusts the ratio between\nipd and tfd based on the training data. We introduce a variable,\nweight}_{tfd}^{ipd, to represent this ratio. This variable is dynamically\nadjusted during backpropagation, allowing it to adapt to the training data\nrather than remaining fixed. Experimental results demonstrate that AdaResNet\nachieves a maximum accuracy improvement of over 50\\% compared to traditional\nResNet.\n", "link": "http://arxiv.org/abs/2408.09958v1", "date": "2024-08-19", "relevancy": 2.1471, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5786}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5167}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaResNet%3A%20Enhancing%20Residual%20Networks%20with%20Dynamic%20Weight%20Adjustment%0A%20%20for%20Improved%20Feature%20Integration&body=Title%3A%20AdaResNet%3A%20Enhancing%20Residual%20Networks%20with%20Dynamic%20Weight%20Adjustment%0A%20%20for%20Improved%20Feature%20Integration%0AAuthor%3A%20Hong%20Su%0AAbstract%3A%20%20%20In%20very%20deep%20neural%20networks%2C%20gradients%20can%20become%20extremely%20small%20during%0Abackpropagation%2C%20making%20it%20challenging%20to%20train%20the%20early%20layers.%20ResNet%0A%28Residual%20Network%29%20addresses%20this%20issue%20by%20enabling%20gradients%20to%20flow%20directly%0Athrough%20the%20network%20via%20skip%20connections%2C%20facilitating%20the%20training%20of%20much%0Adeeper%20networks.%20However%2C%20in%20these%20skip%20connections%2C%20the%20input%20ipd%20is%20directly%0Aadded%20to%20the%20transformed%20data%20tfd%2C%20treating%20ipd%20and%20tfd%20equally%2C%20without%0Aadapting%20to%20different%20scenarios.%20In%20this%20paper%2C%20we%20propose%20AdaResNet%0A%28Auto-Adapting%20Residual%20Network%29%2C%20which%20automatically%20adjusts%20the%20ratio%20between%0Aipd%20and%20tfd%20based%20on%20the%20training%20data.%20We%20introduce%20a%20variable%2C%0Aweight%7D_%7Btfd%7D%5E%7Bipd%2C%20to%20represent%20this%20ratio.%20This%20variable%20is%20dynamically%0Aadjusted%20during%20backpropagation%2C%20allowing%20it%20to%20adapt%20to%20the%20training%20data%0Arather%20than%20remaining%20fixed.%20Experimental%20results%20demonstrate%20that%20AdaResNet%0Aachieves%20a%20maximum%20accuracy%20improvement%20of%20over%2050%5C%25%20compared%20to%20traditional%0AResNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaResNet%253A%2520Enhancing%2520Residual%2520Networks%2520with%2520Dynamic%2520Weight%2520Adjustment%250A%2520%2520for%2520Improved%2520Feature%2520Integration%26entry.906535625%3DHong%2520Su%26entry.1292438233%3D%2520%2520In%2520very%2520deep%2520neural%2520networks%252C%2520gradients%2520can%2520become%2520extremely%2520small%2520during%250Abackpropagation%252C%2520making%2520it%2520challenging%2520to%2520train%2520the%2520early%2520layers.%2520ResNet%250A%2528Residual%2520Network%2529%2520addresses%2520this%2520issue%2520by%2520enabling%2520gradients%2520to%2520flow%2520directly%250Athrough%2520the%2520network%2520via%2520skip%2520connections%252C%2520facilitating%2520the%2520training%2520of%2520much%250Adeeper%2520networks.%2520However%252C%2520in%2520these%2520skip%2520connections%252C%2520the%2520input%2520ipd%2520is%2520directly%250Aadded%2520to%2520the%2520transformed%2520data%2520tfd%252C%2520treating%2520ipd%2520and%2520tfd%2520equally%252C%2520without%250Aadapting%2520to%2520different%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AdaResNet%250A%2528Auto-Adapting%2520Residual%2520Network%2529%252C%2520which%2520automatically%2520adjusts%2520the%2520ratio%2520between%250Aipd%2520and%2520tfd%2520based%2520on%2520the%2520training%2520data.%2520We%2520introduce%2520a%2520variable%252C%250Aweight%257D_%257Btfd%257D%255E%257Bipd%252C%2520to%2520represent%2520this%2520ratio.%2520This%2520variable%2520is%2520dynamically%250Aadjusted%2520during%2520backpropagation%252C%2520allowing%2520it%2520to%2520adapt%2520to%2520the%2520training%2520data%250Arather%2520than%2520remaining%2520fixed.%2520Experimental%2520results%2520demonstrate%2520that%2520AdaResNet%250Aachieves%2520a%2520maximum%2520accuracy%2520improvement%2520of%2520over%252050%255C%2525%2520compared%2520to%2520traditional%250AResNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaResNet%3A%20Enhancing%20Residual%20Networks%20with%20Dynamic%20Weight%20Adjustment%0A%20%20for%20Improved%20Feature%20Integration&entry.906535625=Hong%20Su&entry.1292438233=%20%20In%20very%20deep%20neural%20networks%2C%20gradients%20can%20become%20extremely%20small%20during%0Abackpropagation%2C%20making%20it%20challenging%20to%20train%20the%20early%20layers.%20ResNet%0A%28Residual%20Network%29%20addresses%20this%20issue%20by%20enabling%20gradients%20to%20flow%20directly%0Athrough%20the%20network%20via%20skip%20connections%2C%20facilitating%20the%20training%20of%20much%0Adeeper%20networks.%20However%2C%20in%20these%20skip%20connections%2C%20the%20input%20ipd%20is%20directly%0Aadded%20to%20the%20transformed%20data%20tfd%2C%20treating%20ipd%20and%20tfd%20equally%2C%20without%0Aadapting%20to%20different%20scenarios.%20In%20this%20paper%2C%20we%20propose%20AdaResNet%0A%28Auto-Adapting%20Residual%20Network%29%2C%20which%20automatically%20adjusts%20the%20ratio%20between%0Aipd%20and%20tfd%20based%20on%20the%20training%20data.%20We%20introduce%20a%20variable%2C%0Aweight%7D_%7Btfd%7D%5E%7Bipd%2C%20to%20represent%20this%20ratio.%20This%20variable%20is%20dynamically%0Aadjusted%20during%20backpropagation%2C%20allowing%20it%20to%20adapt%20to%20the%20training%20data%0Arather%20than%20remaining%20fixed.%20Experimental%20results%20demonstrate%20that%20AdaResNet%0Aachieves%20a%20maximum%20accuracy%20improvement%20of%20over%2050%5C%25%20compared%20to%20traditional%0AResNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09958v1&entry.124074799=Read"},
{"title": "Caption-Driven Explorations: Aligning Image and Text Embeddings through\n  Human-Inspired Foveated Vision", "author": "Dario Zanca and Andrea Zugarini and Simon Dietz and Thomas R. Altstidl and Mark A. Turban Ndjeuha and Leo Schwinn and Bjoern Eskofier", "abstract": "  Understanding human attention is crucial for vision science and AI. While\nmany models exist for free-viewing, less is known about task-driven image\nexploration. To address this, we introduce CapMIT1003, a dataset with captions\nand click-contingent image explorations, to study human attention during the\ncaptioning task. We also present NevaClip, a zero-shot method for predicting\nvisual scanpaths by combining CLIP models with NeVA algorithms. NevaClip\ngenerates fixations to align the representations of foveated visual stimuli and\ncaptions. The simulated scanpaths outperform existing human attention models in\nplausibility for captioning and free-viewing tasks. This research enhances the\nunderstanding of human attention and advances scanpath prediction models.\n", "link": "http://arxiv.org/abs/2408.09948v1", "date": "2024-08-19", "relevancy": 2.1468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Caption-Driven%20Explorations%3A%20Aligning%20Image%20and%20Text%20Embeddings%20through%0A%20%20Human-Inspired%20Foveated%20Vision&body=Title%3A%20Caption-Driven%20Explorations%3A%20Aligning%20Image%20and%20Text%20Embeddings%20through%0A%20%20Human-Inspired%20Foveated%20Vision%0AAuthor%3A%20Dario%20Zanca%20and%20Andrea%20Zugarini%20and%20Simon%20Dietz%20and%20Thomas%20R.%20Altstidl%20and%20Mark%20A.%20Turban%20Ndjeuha%20and%20Leo%20Schwinn%20and%20Bjoern%20Eskofier%0AAbstract%3A%20%20%20Understanding%20human%20attention%20is%20crucial%20for%20vision%20science%20and%20AI.%20While%0Amany%20models%20exist%20for%20free-viewing%2C%20less%20is%20known%20about%20task-driven%20image%0Aexploration.%20To%20address%20this%2C%20we%20introduce%20CapMIT1003%2C%20a%20dataset%20with%20captions%0Aand%20click-contingent%20image%20explorations%2C%20to%20study%20human%20attention%20during%20the%0Acaptioning%20task.%20We%20also%20present%20NevaClip%2C%20a%20zero-shot%20method%20for%20predicting%0Avisual%20scanpaths%20by%20combining%20CLIP%20models%20with%20NeVA%20algorithms.%20NevaClip%0Agenerates%20fixations%20to%20align%20the%20representations%20of%20foveated%20visual%20stimuli%20and%0Acaptions.%20The%20simulated%20scanpaths%20outperform%20existing%20human%20attention%20models%20in%0Aplausibility%20for%20captioning%20and%20free-viewing%20tasks.%20This%20research%20enhances%20the%0Aunderstanding%20of%20human%20attention%20and%20advances%20scanpath%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaption-Driven%2520Explorations%253A%2520Aligning%2520Image%2520and%2520Text%2520Embeddings%2520through%250A%2520%2520Human-Inspired%2520Foveated%2520Vision%26entry.906535625%3DDario%2520Zanca%2520and%2520Andrea%2520Zugarini%2520and%2520Simon%2520Dietz%2520and%2520Thomas%2520R.%2520Altstidl%2520and%2520Mark%2520A.%2520Turban%2520Ndjeuha%2520and%2520Leo%2520Schwinn%2520and%2520Bjoern%2520Eskofier%26entry.1292438233%3D%2520%2520Understanding%2520human%2520attention%2520is%2520crucial%2520for%2520vision%2520science%2520and%2520AI.%2520While%250Amany%2520models%2520exist%2520for%2520free-viewing%252C%2520less%2520is%2520known%2520about%2520task-driven%2520image%250Aexploration.%2520To%2520address%2520this%252C%2520we%2520introduce%2520CapMIT1003%252C%2520a%2520dataset%2520with%2520captions%250Aand%2520click-contingent%2520image%2520explorations%252C%2520to%2520study%2520human%2520attention%2520during%2520the%250Acaptioning%2520task.%2520We%2520also%2520present%2520NevaClip%252C%2520a%2520zero-shot%2520method%2520for%2520predicting%250Avisual%2520scanpaths%2520by%2520combining%2520CLIP%2520models%2520with%2520NeVA%2520algorithms.%2520NevaClip%250Agenerates%2520fixations%2520to%2520align%2520the%2520representations%2520of%2520foveated%2520visual%2520stimuli%2520and%250Acaptions.%2520The%2520simulated%2520scanpaths%2520outperform%2520existing%2520human%2520attention%2520models%2520in%250Aplausibility%2520for%2520captioning%2520and%2520free-viewing%2520tasks.%2520This%2520research%2520enhances%2520the%250Aunderstanding%2520of%2520human%2520attention%2520and%2520advances%2520scanpath%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Caption-Driven%20Explorations%3A%20Aligning%20Image%20and%20Text%20Embeddings%20through%0A%20%20Human-Inspired%20Foveated%20Vision&entry.906535625=Dario%20Zanca%20and%20Andrea%20Zugarini%20and%20Simon%20Dietz%20and%20Thomas%20R.%20Altstidl%20and%20Mark%20A.%20Turban%20Ndjeuha%20and%20Leo%20Schwinn%20and%20Bjoern%20Eskofier&entry.1292438233=%20%20Understanding%20human%20attention%20is%20crucial%20for%20vision%20science%20and%20AI.%20While%0Amany%20models%20exist%20for%20free-viewing%2C%20less%20is%20known%20about%20task-driven%20image%0Aexploration.%20To%20address%20this%2C%20we%20introduce%20CapMIT1003%2C%20a%20dataset%20with%20captions%0Aand%20click-contingent%20image%20explorations%2C%20to%20study%20human%20attention%20during%20the%0Acaptioning%20task.%20We%20also%20present%20NevaClip%2C%20a%20zero-shot%20method%20for%20predicting%0Avisual%20scanpaths%20by%20combining%20CLIP%20models%20with%20NeVA%20algorithms.%20NevaClip%0Agenerates%20fixations%20to%20align%20the%20representations%20of%20foveated%20visual%20stimuli%20and%0Acaptions.%20The%20simulated%20scanpaths%20outperform%20existing%20human%20attention%20models%20in%0Aplausibility%20for%20captioning%20and%20free-viewing%20tasks.%20This%20research%20enhances%20the%0Aunderstanding%20of%20human%20attention%20and%20advances%20scanpath%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09948v1&entry.124074799=Read"},
{"title": "A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual\n  Deepfake Detection", "author": "Kyungbok Lee and You Zhang and Zhiyao Duan", "abstract": "  This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand Unsynchronized videos). The experimental results demonstrate that our\napproach surpasses the previous models by a large margin. Furthermore, our\nproposed framework offers interpretability, indicating which modality the model\nidentifies as more likely to be fake. The source code is released at\nhttps://github.com/bok-bok/MSOC.\n", "link": "http://arxiv.org/abs/2406.14176v3", "date": "2024-08-19", "relevancy": 2.1428, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5427}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&body=Title%3A%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection%0AAuthor%3A%20Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideos%20%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20Unsynchronized%20videos%29.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20surpasses%20the%20previous%20models%20by%20a%20large%20margin.%20Furthermore%2C%20our%0Aproposed%20framework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20more%20likely%20to%20be%20fake.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/bok-bok/MSOC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14176v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stream%2520Fusion%2520Approach%2520with%2520One-Class%2520Learning%2520for%2520Audio-Visual%250A%2520%2520Deepfake%2520Detection%26entry.906535625%3DKyungbok%2520Lee%2520and%2520You%2520Zhang%2520and%2520Zhiyao%2520Duan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520developing%2520a%2520robust%2520audio-visual%250Adeepfake%2520detection%2520model.%2520In%2520practical%2520use%2520cases%252C%2520new%2520generation%2520algorithms%2520are%250Acontinually%2520emerging%252C%2520and%2520these%2520algorithms%2520are%2520not%2520encountered%2520during%2520the%250Adevelopment%2520of%2520detection%2520methods.%2520This%2520calls%2520for%2520the%2520generalization%2520ability%2520of%250Athe%2520method.%2520Additionally%252C%2520to%2520ensure%2520the%2520credibility%2520of%2520detection%2520methods%252C%2520it%2520is%250Abeneficial%2520for%2520the%2520model%2520to%2520interpret%2520which%2520cues%2520from%2520the%2520video%2520indicate%2520it%2520is%250Afake.%2520Motivated%2520by%2520these%2520considerations%252C%2520we%2520then%2520propose%2520a%2520multi-stream%2520fusion%250Aapproach%2520with%2520one-class%2520learning%2520as%2520a%2520representation-level%2520regularization%250Atechnique.%2520We%2520study%2520the%2520generalization%2520problem%2520of%2520audio-visual%2520deepfake%250Adetection%2520by%2520creating%2520a%2520new%2520benchmark%2520by%2520extending%2520and%2520re-splitting%2520the%250Aexisting%2520FakeAVCeleb%2520dataset.%2520The%2520benchmark%2520contains%2520four%2520categories%2520of%2520fake%250Avideos%2520%2528Real%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Real%2520Visual%252C%250Aand%2520Unsynchronized%2520videos%2529.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520surpasses%2520the%2520previous%2520models%2520by%2520a%2520large%2520margin.%2520Furthermore%252C%2520our%250Aproposed%2520framework%2520offers%2520interpretability%252C%2520indicating%2520which%2520modality%2520the%2520model%250Aidentifies%2520as%2520more%2520likely%2520to%2520be%2520fake.%2520The%2520source%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/bok-bok/MSOC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14176v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&entry.906535625=Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideos%20%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20Unsynchronized%20videos%29.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20surpasses%20the%20previous%20models%20by%20a%20large%20margin.%20Furthermore%2C%20our%0Aproposed%20framework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20more%20likely%20to%20be%20fake.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/bok-bok/MSOC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14176v3&entry.124074799=Read"},
{"title": "Harnessing Multi-resolution and Multi-scale Attention for Underwater\n  Image Restoration", "author": "Alik Pramanick and Arijit Sur and V. Vijaya Saradhi", "abstract": "  Underwater imagery is often compromised by factors such as color distortion\nand low contrast, posing challenges for high-level vision tasks. Recent\nunderwater image restoration (UIR) methods either analyze the input image at\nfull resolution, resulting in spatial richness but contextual weakness, or\nprogressively from high to low resolution, yielding reliable semantic\ninformation but reduced spatial accuracy. Here, we propose a lightweight\nmulti-stage network called Lit-Net that focuses on multi-resolution and\nmulti-scale image analysis for restoring underwater images while retaining\noriginal resolution during the first stage, refining features in the second,\nand focusing on reconstruction in the final stage. Our novel encoder block\nutilizes parallel $1\\times1$ convolution layers to capture local information\nand speed up operations. Further, we incorporate a modified weighted color\nchannel-specific $l_1$ loss ($cl_1$) function to recover color and detail\ninformation. Extensive experimentations on publicly available datasets suggest\nour model's superiority over recent state-of-the-art methods, with significant\nimprovement in qualitative and quantitative measures, such as $29.477$ dB PSNR\n($1.92\\%$ improvement) and $0.851$ SSIM ($2.87\\%$ improvement) on the EUVP\ndataset. The contributions of Lit-Net offer a more robust approach to\nunderwater image enhancement and super-resolution, which is of considerable\nimportance for underwater autonomous vehicles and surveillance. The code is\navailable at: https://github.com/Alik033/Lit-Net.\n", "link": "http://arxiv.org/abs/2408.09912v1", "date": "2024-08-19", "relevancy": 2.1416, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5429}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Multi-resolution%20and%20Multi-scale%20Attention%20for%20Underwater%0A%20%20Image%20Restoration&body=Title%3A%20Harnessing%20Multi-resolution%20and%20Multi-scale%20Attention%20for%20Underwater%0A%20%20Image%20Restoration%0AAuthor%3A%20Alik%20Pramanick%20and%20Arijit%20Sur%20and%20V.%20Vijaya%20Saradhi%0AAbstract%3A%20%20%20Underwater%20imagery%20is%20often%20compromised%20by%20factors%20such%20as%20color%20distortion%0Aand%20low%20contrast%2C%20posing%20challenges%20for%20high-level%20vision%20tasks.%20Recent%0Aunderwater%20image%20restoration%20%28UIR%29%20methods%20either%20analyze%20the%20input%20image%20at%0Afull%20resolution%2C%20resulting%20in%20spatial%20richness%20but%20contextual%20weakness%2C%20or%0Aprogressively%20from%20high%20to%20low%20resolution%2C%20yielding%20reliable%20semantic%0Ainformation%20but%20reduced%20spatial%20accuracy.%20Here%2C%20we%20propose%20a%20lightweight%0Amulti-stage%20network%20called%20Lit-Net%20that%20focuses%20on%20multi-resolution%20and%0Amulti-scale%20image%20analysis%20for%20restoring%20underwater%20images%20while%20retaining%0Aoriginal%20resolution%20during%20the%20first%20stage%2C%20refining%20features%20in%20the%20second%2C%0Aand%20focusing%20on%20reconstruction%20in%20the%20final%20stage.%20Our%20novel%20encoder%20block%0Autilizes%20parallel%20%241%5Ctimes1%24%20convolution%20layers%20to%20capture%20local%20information%0Aand%20speed%20up%20operations.%20Further%2C%20we%20incorporate%20a%20modified%20weighted%20color%0Achannel-specific%20%24l_1%24%20loss%20%28%24cl_1%24%29%20function%20to%20recover%20color%20and%20detail%0Ainformation.%20Extensive%20experimentations%20on%20publicly%20available%20datasets%20suggest%0Aour%20model%27s%20superiority%20over%20recent%20state-of-the-art%20methods%2C%20with%20significant%0Aimprovement%20in%20qualitative%20and%20quantitative%20measures%2C%20such%20as%20%2429.477%24%20dB%20PSNR%0A%28%241.92%5C%25%24%20improvement%29%20and%20%240.851%24%20SSIM%20%28%242.87%5C%25%24%20improvement%29%20on%20the%20EUVP%0Adataset.%20The%20contributions%20of%20Lit-Net%20offer%20a%20more%20robust%20approach%20to%0Aunderwater%20image%20enhancement%20and%20super-resolution%2C%20which%20is%20of%20considerable%0Aimportance%20for%20underwater%20autonomous%20vehicles%20and%20surveillance.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/Alik033/Lit-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Multi-resolution%2520and%2520Multi-scale%2520Attention%2520for%2520Underwater%250A%2520%2520Image%2520Restoration%26entry.906535625%3DAlik%2520Pramanick%2520and%2520Arijit%2520Sur%2520and%2520V.%2520Vijaya%2520Saradhi%26entry.1292438233%3D%2520%2520Underwater%2520imagery%2520is%2520often%2520compromised%2520by%2520factors%2520such%2520as%2520color%2520distortion%250Aand%2520low%2520contrast%252C%2520posing%2520challenges%2520for%2520high-level%2520vision%2520tasks.%2520Recent%250Aunderwater%2520image%2520restoration%2520%2528UIR%2529%2520methods%2520either%2520analyze%2520the%2520input%2520image%2520at%250Afull%2520resolution%252C%2520resulting%2520in%2520spatial%2520richness%2520but%2520contextual%2520weakness%252C%2520or%250Aprogressively%2520from%2520high%2520to%2520low%2520resolution%252C%2520yielding%2520reliable%2520semantic%250Ainformation%2520but%2520reduced%2520spatial%2520accuracy.%2520Here%252C%2520we%2520propose%2520a%2520lightweight%250Amulti-stage%2520network%2520called%2520Lit-Net%2520that%2520focuses%2520on%2520multi-resolution%2520and%250Amulti-scale%2520image%2520analysis%2520for%2520restoring%2520underwater%2520images%2520while%2520retaining%250Aoriginal%2520resolution%2520during%2520the%2520first%2520stage%252C%2520refining%2520features%2520in%2520the%2520second%252C%250Aand%2520focusing%2520on%2520reconstruction%2520in%2520the%2520final%2520stage.%2520Our%2520novel%2520encoder%2520block%250Autilizes%2520parallel%2520%25241%255Ctimes1%2524%2520convolution%2520layers%2520to%2520capture%2520local%2520information%250Aand%2520speed%2520up%2520operations.%2520Further%252C%2520we%2520incorporate%2520a%2520modified%2520weighted%2520color%250Achannel-specific%2520%2524l_1%2524%2520loss%2520%2528%2524cl_1%2524%2529%2520function%2520to%2520recover%2520color%2520and%2520detail%250Ainformation.%2520Extensive%2520experimentations%2520on%2520publicly%2520available%2520datasets%2520suggest%250Aour%2520model%2527s%2520superiority%2520over%2520recent%2520state-of-the-art%2520methods%252C%2520with%2520significant%250Aimprovement%2520in%2520qualitative%2520and%2520quantitative%2520measures%252C%2520such%2520as%2520%252429.477%2524%2520dB%2520PSNR%250A%2528%25241.92%255C%2525%2524%2520improvement%2529%2520and%2520%25240.851%2524%2520SSIM%2520%2528%25242.87%255C%2525%2524%2520improvement%2529%2520on%2520the%2520EUVP%250Adataset.%2520The%2520contributions%2520of%2520Lit-Net%2520offer%2520a%2520more%2520robust%2520approach%2520to%250Aunderwater%2520image%2520enhancement%2520and%2520super-resolution%252C%2520which%2520is%2520of%2520considerable%250Aimportance%2520for%2520underwater%2520autonomous%2520vehicles%2520and%2520surveillance.%2520The%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/Alik033/Lit-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Multi-resolution%20and%20Multi-scale%20Attention%20for%20Underwater%0A%20%20Image%20Restoration&entry.906535625=Alik%20Pramanick%20and%20Arijit%20Sur%20and%20V.%20Vijaya%20Saradhi&entry.1292438233=%20%20Underwater%20imagery%20is%20often%20compromised%20by%20factors%20such%20as%20color%20distortion%0Aand%20low%20contrast%2C%20posing%20challenges%20for%20high-level%20vision%20tasks.%20Recent%0Aunderwater%20image%20restoration%20%28UIR%29%20methods%20either%20analyze%20the%20input%20image%20at%0Afull%20resolution%2C%20resulting%20in%20spatial%20richness%20but%20contextual%20weakness%2C%20or%0Aprogressively%20from%20high%20to%20low%20resolution%2C%20yielding%20reliable%20semantic%0Ainformation%20but%20reduced%20spatial%20accuracy.%20Here%2C%20we%20propose%20a%20lightweight%0Amulti-stage%20network%20called%20Lit-Net%20that%20focuses%20on%20multi-resolution%20and%0Amulti-scale%20image%20analysis%20for%20restoring%20underwater%20images%20while%20retaining%0Aoriginal%20resolution%20during%20the%20first%20stage%2C%20refining%20features%20in%20the%20second%2C%0Aand%20focusing%20on%20reconstruction%20in%20the%20final%20stage.%20Our%20novel%20encoder%20block%0Autilizes%20parallel%20%241%5Ctimes1%24%20convolution%20layers%20to%20capture%20local%20information%0Aand%20speed%20up%20operations.%20Further%2C%20we%20incorporate%20a%20modified%20weighted%20color%0Achannel-specific%20%24l_1%24%20loss%20%28%24cl_1%24%29%20function%20to%20recover%20color%20and%20detail%0Ainformation.%20Extensive%20experimentations%20on%20publicly%20available%20datasets%20suggest%0Aour%20model%27s%20superiority%20over%20recent%20state-of-the-art%20methods%2C%20with%20significant%0Aimprovement%20in%20qualitative%20and%20quantitative%20measures%2C%20such%20as%20%2429.477%24%20dB%20PSNR%0A%28%241.92%5C%25%24%20improvement%29%20and%20%240.851%24%20SSIM%20%28%242.87%5C%25%24%20improvement%29%20on%20the%20EUVP%0Adataset.%20The%20contributions%20of%20Lit-Net%20offer%20a%20more%20robust%20approach%20to%0Aunderwater%20image%20enhancement%20and%20super-resolution%2C%20which%20is%20of%20considerable%0Aimportance%20for%20underwater%20autonomous%20vehicles%20and%20surveillance.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/Alik033/Lit-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09912v1&entry.124074799=Read"},
{"title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From\n  Pre-Trained Foundation Models", "author": "Anke Tang and Li Shen and Yong Luo and Shuai Xie and Han Hu and Lefei Zhang and Bo Du and Dacheng Tao", "abstract": "  Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneralization tasks, using full fine-tuning and LoRA fine-tuning, and we apply\nour method to large language models (CLIP models, Flan-T5 models, and\nMistral-7B models), highlighting the adaptability and scalability of SMILE.\nCode is available at https://github.com/tanganke/fusion_bench\n", "link": "http://arxiv.org/abs/2408.10174v1", "date": "2024-08-19", "relevancy": 2.1355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMILE%3A%20Zero-Shot%20Sparse%20Mixture%20of%20Low-Rank%20Experts%20Construction%20From%0A%20%20Pre-Trained%20Foundation%20Models&body=Title%3A%20SMILE%3A%20Zero-Shot%20Sparse%20Mixture%20of%20Low-Rank%20Experts%20Construction%20From%0A%20%20Pre-Trained%20Foundation%20Models%0AAuthor%3A%20Anke%20Tang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Shuai%20Xie%20and%20Han%20Hu%20and%20Lefei%20Zhang%20and%20Bo%20Du%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Deep%20model%20training%20on%20extensive%20datasets%20is%20increasingly%20becoming%0Acost-prohibitive%2C%20prompting%20the%20widespread%20adoption%20of%20deep%20model%20fusion%0Atechniques%20to%20leverage%20knowledge%20from%20pre-existing%20models.%20From%20simple%20weight%0Aaveraging%20to%20more%20sophisticated%20methods%20like%20AdaMerging%2C%20model%20fusion%0Aeffectively%20improves%20model%20performance%20and%20accelerates%20the%20development%20of%20new%0Amodels.%20However%2C%20potential%20interference%20between%20parameters%20of%20individual%20models%0Aand%20the%20lack%20of%20interpretability%20in%20the%20fusion%20progress%20remain%20significant%0Achallenges.%20Existing%20methods%20often%20try%20to%20resolve%20the%20parameter%20interference%0Aissue%20by%20evaluating%20attributes%20of%20parameters%2C%20such%20as%20their%20magnitude%20or%20sign%2C%0Aor%20by%20parameter%20pruning.%20In%20this%20study%2C%20we%20begin%20by%20examining%20the%20fine-tuning%0Aof%20linear%20layers%20through%20the%20lens%20of%20subspace%20analysis%20and%20explicitly%20define%0Aparameter%20interference%20as%20an%20optimization%20problem%20to%20shed%20light%20on%20this%0Asubject.%20Subsequently%2C%20we%20introduce%20an%20innovative%20approach%20to%20model%20fusion%0Acalled%20zero-shot%20Sparse%20MIxture%20of%20Low-rank%20Experts%20%28SMILE%29%20construction%2C%20which%0Aallows%20for%20the%20upscaling%20of%20source%20models%20into%20an%20MoE%20model%20without%20extra%20data%0Aor%20further%20training.%20Our%20approach%20relies%20on%20the%20observation%20that%20fine-tuning%0Amostly%20keeps%20the%20important%20parts%20from%20the%20pre-training%2C%20but%20it%20uses%20less%0Asignificant%20or%20unused%20areas%20to%20adapt%20to%20new%20tasks.%20Also%2C%20the%20issue%20of%20parameter%0Ainterference%2C%20which%20is%20intrinsically%20intractable%20in%20the%20original%20parameter%0Aspace%2C%20can%20be%20managed%20by%20expanding%20the%20dimensions.%20We%20conduct%20extensive%0Aexperiments%20across%20diverse%20scenarios%2C%20such%20as%20image%20classification%20and%20text%0Ageneralization%20tasks%2C%20using%20full%20fine-tuning%20and%20LoRA%20fine-tuning%2C%20and%20we%20apply%0Aour%20method%20to%20large%20language%20models%20%28CLIP%20models%2C%20Flan-T5%20models%2C%20and%0AMistral-7B%20models%29%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20SMILE.%0ACode%20is%20available%20at%20https%3A//github.com/tanganke/fusion_bench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMILE%253A%2520Zero-Shot%2520Sparse%2520Mixture%2520of%2520Low-Rank%2520Experts%2520Construction%2520From%250A%2520%2520Pre-Trained%2520Foundation%2520Models%26entry.906535625%3DAnke%2520Tang%2520and%2520Li%2520Shen%2520and%2520Yong%2520Luo%2520and%2520Shuai%2520Xie%2520and%2520Han%2520Hu%2520and%2520Lefei%2520Zhang%2520and%2520Bo%2520Du%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Deep%2520model%2520training%2520on%2520extensive%2520datasets%2520is%2520increasingly%2520becoming%250Acost-prohibitive%252C%2520prompting%2520the%2520widespread%2520adoption%2520of%2520deep%2520model%2520fusion%250Atechniques%2520to%2520leverage%2520knowledge%2520from%2520pre-existing%2520models.%2520From%2520simple%2520weight%250Aaveraging%2520to%2520more%2520sophisticated%2520methods%2520like%2520AdaMerging%252C%2520model%2520fusion%250Aeffectively%2520improves%2520model%2520performance%2520and%2520accelerates%2520the%2520development%2520of%2520new%250Amodels.%2520However%252C%2520potential%2520interference%2520between%2520parameters%2520of%2520individual%2520models%250Aand%2520the%2520lack%2520of%2520interpretability%2520in%2520the%2520fusion%2520progress%2520remain%2520significant%250Achallenges.%2520Existing%2520methods%2520often%2520try%2520to%2520resolve%2520the%2520parameter%2520interference%250Aissue%2520by%2520evaluating%2520attributes%2520of%2520parameters%252C%2520such%2520as%2520their%2520magnitude%2520or%2520sign%252C%250Aor%2520by%2520parameter%2520pruning.%2520In%2520this%2520study%252C%2520we%2520begin%2520by%2520examining%2520the%2520fine-tuning%250Aof%2520linear%2520layers%2520through%2520the%2520lens%2520of%2520subspace%2520analysis%2520and%2520explicitly%2520define%250Aparameter%2520interference%2520as%2520an%2520optimization%2520problem%2520to%2520shed%2520light%2520on%2520this%250Asubject.%2520Subsequently%252C%2520we%2520introduce%2520an%2520innovative%2520approach%2520to%2520model%2520fusion%250Acalled%2520zero-shot%2520Sparse%2520MIxture%2520of%2520Low-rank%2520Experts%2520%2528SMILE%2529%2520construction%252C%2520which%250Aallows%2520for%2520the%2520upscaling%2520of%2520source%2520models%2520into%2520an%2520MoE%2520model%2520without%2520extra%2520data%250Aor%2520further%2520training.%2520Our%2520approach%2520relies%2520on%2520the%2520observation%2520that%2520fine-tuning%250Amostly%2520keeps%2520the%2520important%2520parts%2520from%2520the%2520pre-training%252C%2520but%2520it%2520uses%2520less%250Asignificant%2520or%2520unused%2520areas%2520to%2520adapt%2520to%2520new%2520tasks.%2520Also%252C%2520the%2520issue%2520of%2520parameter%250Ainterference%252C%2520which%2520is%2520intrinsically%2520intractable%2520in%2520the%2520original%2520parameter%250Aspace%252C%2520can%2520be%2520managed%2520by%2520expanding%2520the%2520dimensions.%2520We%2520conduct%2520extensive%250Aexperiments%2520across%2520diverse%2520scenarios%252C%2520such%2520as%2520image%2520classification%2520and%2520text%250Ageneralization%2520tasks%252C%2520using%2520full%2520fine-tuning%2520and%2520LoRA%2520fine-tuning%252C%2520and%2520we%2520apply%250Aour%2520method%2520to%2520large%2520language%2520models%2520%2528CLIP%2520models%252C%2520Flan-T5%2520models%252C%2520and%250AMistral-7B%2520models%2529%252C%2520highlighting%2520the%2520adaptability%2520and%2520scalability%2520of%2520SMILE.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/tanganke/fusion_bench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMILE%3A%20Zero-Shot%20Sparse%20Mixture%20of%20Low-Rank%20Experts%20Construction%20From%0A%20%20Pre-Trained%20Foundation%20Models&entry.906535625=Anke%20Tang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Shuai%20Xie%20and%20Han%20Hu%20and%20Lefei%20Zhang%20and%20Bo%20Du%20and%20Dacheng%20Tao&entry.1292438233=%20%20Deep%20model%20training%20on%20extensive%20datasets%20is%20increasingly%20becoming%0Acost-prohibitive%2C%20prompting%20the%20widespread%20adoption%20of%20deep%20model%20fusion%0Atechniques%20to%20leverage%20knowledge%20from%20pre-existing%20models.%20From%20simple%20weight%0Aaveraging%20to%20more%20sophisticated%20methods%20like%20AdaMerging%2C%20model%20fusion%0Aeffectively%20improves%20model%20performance%20and%20accelerates%20the%20development%20of%20new%0Amodels.%20However%2C%20potential%20interference%20between%20parameters%20of%20individual%20models%0Aand%20the%20lack%20of%20interpretability%20in%20the%20fusion%20progress%20remain%20significant%0Achallenges.%20Existing%20methods%20often%20try%20to%20resolve%20the%20parameter%20interference%0Aissue%20by%20evaluating%20attributes%20of%20parameters%2C%20such%20as%20their%20magnitude%20or%20sign%2C%0Aor%20by%20parameter%20pruning.%20In%20this%20study%2C%20we%20begin%20by%20examining%20the%20fine-tuning%0Aof%20linear%20layers%20through%20the%20lens%20of%20subspace%20analysis%20and%20explicitly%20define%0Aparameter%20interference%20as%20an%20optimization%20problem%20to%20shed%20light%20on%20this%0Asubject.%20Subsequently%2C%20we%20introduce%20an%20innovative%20approach%20to%20model%20fusion%0Acalled%20zero-shot%20Sparse%20MIxture%20of%20Low-rank%20Experts%20%28SMILE%29%20construction%2C%20which%0Aallows%20for%20the%20upscaling%20of%20source%20models%20into%20an%20MoE%20model%20without%20extra%20data%0Aor%20further%20training.%20Our%20approach%20relies%20on%20the%20observation%20that%20fine-tuning%0Amostly%20keeps%20the%20important%20parts%20from%20the%20pre-training%2C%20but%20it%20uses%20less%0Asignificant%20or%20unused%20areas%20to%20adapt%20to%20new%20tasks.%20Also%2C%20the%20issue%20of%20parameter%0Ainterference%2C%20which%20is%20intrinsically%20intractable%20in%20the%20original%20parameter%0Aspace%2C%20can%20be%20managed%20by%20expanding%20the%20dimensions.%20We%20conduct%20extensive%0Aexperiments%20across%20diverse%20scenarios%2C%20such%20as%20image%20classification%20and%20text%0Ageneralization%20tasks%2C%20using%20full%20fine-tuning%20and%20LoRA%20fine-tuning%2C%20and%20we%20apply%0Aour%20method%20to%20large%20language%20models%20%28CLIP%20models%2C%20Flan-T5%20models%2C%20and%0AMistral-7B%20models%29%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20SMILE.%0ACode%20is%20available%20at%20https%3A//github.com/tanganke/fusion_bench%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10174v1&entry.124074799=Read"},
{"title": "Docling Technical Report", "author": "Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Nikolaos Livathinos and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Fabian Lindlbauer and Kasper Dinkla and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar", "abstract": "  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n", "link": "http://arxiv.org/abs/2408.09869v1", "date": "2024-08-19", "relevancy": 2.1317, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Docling%20Technical%20Report&body=Title%3A%20Docling%20Technical%20Report%0AAuthor%3A%20Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar%0AAbstract%3A%20%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocling%2520Technical%2520Report%26entry.906535625%3DChristoph%2520Auer%2520and%2520Maksym%2520Lysak%2520and%2520Ahmed%2520Nassar%2520and%2520Michele%2520Dolfi%2520and%2520Nikolaos%2520Livathinos%2520and%2520Panos%2520Vagenas%2520and%2520Cesar%2520Berrospi%2520Ramis%2520and%2520Matteo%2520Omenetti%2520and%2520Fabian%2520Lindlbauer%2520and%2520Kasper%2520Dinkla%2520and%2520Valery%2520Weber%2520and%2520Lucas%2520Morin%2520and%2520Ingmar%2520Meijer%2520and%2520Viktor%2520Kuropiatnyk%2520and%2520Peter%2520W.%2520J.%2520Staar%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520introduces%2520Docling%252C%2520an%2520easy%2520to%2520use%252C%2520self-contained%252C%250AMIT-licensed%2520open-source%2520package%2520for%2520PDF%2520document%2520conversion.%2520It%2520is%2520powered%2520by%250Astate-of-the-art%2520specialized%2520AI%2520models%2520for%2520layout%2520analysis%2520%2528DocLayNet%2529%2520and%250Atable%2520structure%2520recognition%2520%2528TableFormer%2529%252C%2520and%2520runs%2520efficiently%2520on%2520commodity%250Ahardware%2520in%2520a%2520small%2520resource%2520budget.%2520The%2520code%2520interface%2520allows%2520for%2520easy%250Aextensibility%2520and%2520addition%2520of%2520new%2520features%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Docling%20Technical%20Report&entry.906535625=Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar&entry.1292438233=%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09869v1&entry.124074799=Read"},
{"title": "Robust spectral clustering with rank statistics", "author": "Joshua Cape and Xianshi Yu and Jonquil Z. Liao", "abstract": "  This paper analyzes the statistical performance of a robust spectral\nclustering method for latent structure recovery in noisy data matrices. We\nconsider eigenvector-based clustering applied to a matrix of nonparametric rank\nstatistics that is derived entrywise from the raw, original data matrix. This\napproach is robust in the sense that, unlike traditional spectral clustering\nprocedures, it can provably recover population-level latent block structure\neven when the observed data matrix includes heavy-tailed entries and has a\nheterogeneous variance profile.\n  Our main theoretical contributions are threefold and hold under flexible data\ngenerating conditions. First, we establish that robust spectral clustering with\nrank statistics can consistently recover latent block structure, viewed as\ncommunities of nodes in a graph, in the sense that unobserved community\nmemberships for all but a vanishing fraction of nodes are correctly recovered\nwith high probability when the data matrix is large. Second, we refine the\nformer result and further establish that, under certain conditions, the\ncommunity membership of any individual, specified node of interest can be\nasymptotically exactly recovered with probability tending to one in the\nlarge-data limit. Third, we establish asymptotic normality results associated\nwith the truncated eigenstructure of matrices whose entries are rank\nstatistics, made possible by synthesizing contemporary entrywise matrix\nperturbation analysis with the classical nonparametric theory of so-called\nsimple linear rank statistics. Collectively, these results demonstrate the\nstatistical utility of rank-based data transformations when paired with\nspectral techniques for dimensionality reduction. Additionally, for a dataset\nof human connectomes, our approach yields parsimonious dimensionality reduction\nand improved recovery of ground-truth neuroanatomical cluster structure.\n", "link": "http://arxiv.org/abs/2408.10136v1", "date": "2024-08-19", "relevancy": 2.1276, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4452}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.418}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20spectral%20clustering%20with%20rank%20statistics&body=Title%3A%20Robust%20spectral%20clustering%20with%20rank%20statistics%0AAuthor%3A%20Joshua%20Cape%20and%20Xianshi%20Yu%20and%20Jonquil%20Z.%20Liao%0AAbstract%3A%20%20%20This%20paper%20analyzes%20the%20statistical%20performance%20of%20a%20robust%20spectral%0Aclustering%20method%20for%20latent%20structure%20recovery%20in%20noisy%20data%20matrices.%20We%0Aconsider%20eigenvector-based%20clustering%20applied%20to%20a%20matrix%20of%20nonparametric%20rank%0Astatistics%20that%20is%20derived%20entrywise%20from%20the%20raw%2C%20original%20data%20matrix.%20This%0Aapproach%20is%20robust%20in%20the%20sense%20that%2C%20unlike%20traditional%20spectral%20clustering%0Aprocedures%2C%20it%20can%20provably%20recover%20population-level%20latent%20block%20structure%0Aeven%20when%20the%20observed%20data%20matrix%20includes%20heavy-tailed%20entries%20and%20has%20a%0Aheterogeneous%20variance%20profile.%0A%20%20Our%20main%20theoretical%20contributions%20are%20threefold%20and%20hold%20under%20flexible%20data%0Agenerating%20conditions.%20First%2C%20we%20establish%20that%20robust%20spectral%20clustering%20with%0Arank%20statistics%20can%20consistently%20recover%20latent%20block%20structure%2C%20viewed%20as%0Acommunities%20of%20nodes%20in%20a%20graph%2C%20in%20the%20sense%20that%20unobserved%20community%0Amemberships%20for%20all%20but%20a%20vanishing%20fraction%20of%20nodes%20are%20correctly%20recovered%0Awith%20high%20probability%20when%20the%20data%20matrix%20is%20large.%20Second%2C%20we%20refine%20the%0Aformer%20result%20and%20further%20establish%20that%2C%20under%20certain%20conditions%2C%20the%0Acommunity%20membership%20of%20any%20individual%2C%20specified%20node%20of%20interest%20can%20be%0Aasymptotically%20exactly%20recovered%20with%20probability%20tending%20to%20one%20in%20the%0Alarge-data%20limit.%20Third%2C%20we%20establish%20asymptotic%20normality%20results%20associated%0Awith%20the%20truncated%20eigenstructure%20of%20matrices%20whose%20entries%20are%20rank%0Astatistics%2C%20made%20possible%20by%20synthesizing%20contemporary%20entrywise%20matrix%0Aperturbation%20analysis%20with%20the%20classical%20nonparametric%20theory%20of%20so-called%0Asimple%20linear%20rank%20statistics.%20Collectively%2C%20these%20results%20demonstrate%20the%0Astatistical%20utility%20of%20rank-based%20data%20transformations%20when%20paired%20with%0Aspectral%20techniques%20for%20dimensionality%20reduction.%20Additionally%2C%20for%20a%20dataset%0Aof%20human%20connectomes%2C%20our%20approach%20yields%20parsimonious%20dimensionality%20reduction%0Aand%20improved%20recovery%20of%20ground-truth%20neuroanatomical%20cluster%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520spectral%2520clustering%2520with%2520rank%2520statistics%26entry.906535625%3DJoshua%2520Cape%2520and%2520Xianshi%2520Yu%2520and%2520Jonquil%2520Z.%2520Liao%26entry.1292438233%3D%2520%2520This%2520paper%2520analyzes%2520the%2520statistical%2520performance%2520of%2520a%2520robust%2520spectral%250Aclustering%2520method%2520for%2520latent%2520structure%2520recovery%2520in%2520noisy%2520data%2520matrices.%2520We%250Aconsider%2520eigenvector-based%2520clustering%2520applied%2520to%2520a%2520matrix%2520of%2520nonparametric%2520rank%250Astatistics%2520that%2520is%2520derived%2520entrywise%2520from%2520the%2520raw%252C%2520original%2520data%2520matrix.%2520This%250Aapproach%2520is%2520robust%2520in%2520the%2520sense%2520that%252C%2520unlike%2520traditional%2520spectral%2520clustering%250Aprocedures%252C%2520it%2520can%2520provably%2520recover%2520population-level%2520latent%2520block%2520structure%250Aeven%2520when%2520the%2520observed%2520data%2520matrix%2520includes%2520heavy-tailed%2520entries%2520and%2520has%2520a%250Aheterogeneous%2520variance%2520profile.%250A%2520%2520Our%2520main%2520theoretical%2520contributions%2520are%2520threefold%2520and%2520hold%2520under%2520flexible%2520data%250Agenerating%2520conditions.%2520First%252C%2520we%2520establish%2520that%2520robust%2520spectral%2520clustering%2520with%250Arank%2520statistics%2520can%2520consistently%2520recover%2520latent%2520block%2520structure%252C%2520viewed%2520as%250Acommunities%2520of%2520nodes%2520in%2520a%2520graph%252C%2520in%2520the%2520sense%2520that%2520unobserved%2520community%250Amemberships%2520for%2520all%2520but%2520a%2520vanishing%2520fraction%2520of%2520nodes%2520are%2520correctly%2520recovered%250Awith%2520high%2520probability%2520when%2520the%2520data%2520matrix%2520is%2520large.%2520Second%252C%2520we%2520refine%2520the%250Aformer%2520result%2520and%2520further%2520establish%2520that%252C%2520under%2520certain%2520conditions%252C%2520the%250Acommunity%2520membership%2520of%2520any%2520individual%252C%2520specified%2520node%2520of%2520interest%2520can%2520be%250Aasymptotically%2520exactly%2520recovered%2520with%2520probability%2520tending%2520to%2520one%2520in%2520the%250Alarge-data%2520limit.%2520Third%252C%2520we%2520establish%2520asymptotic%2520normality%2520results%2520associated%250Awith%2520the%2520truncated%2520eigenstructure%2520of%2520matrices%2520whose%2520entries%2520are%2520rank%250Astatistics%252C%2520made%2520possible%2520by%2520synthesizing%2520contemporary%2520entrywise%2520matrix%250Aperturbation%2520analysis%2520with%2520the%2520classical%2520nonparametric%2520theory%2520of%2520so-called%250Asimple%2520linear%2520rank%2520statistics.%2520Collectively%252C%2520these%2520results%2520demonstrate%2520the%250Astatistical%2520utility%2520of%2520rank-based%2520data%2520transformations%2520when%2520paired%2520with%250Aspectral%2520techniques%2520for%2520dimensionality%2520reduction.%2520Additionally%252C%2520for%2520a%2520dataset%250Aof%2520human%2520connectomes%252C%2520our%2520approach%2520yields%2520parsimonious%2520dimensionality%2520reduction%250Aand%2520improved%2520recovery%2520of%2520ground-truth%2520neuroanatomical%2520cluster%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20spectral%20clustering%20with%20rank%20statistics&entry.906535625=Joshua%20Cape%20and%20Xianshi%20Yu%20and%20Jonquil%20Z.%20Liao&entry.1292438233=%20%20This%20paper%20analyzes%20the%20statistical%20performance%20of%20a%20robust%20spectral%0Aclustering%20method%20for%20latent%20structure%20recovery%20in%20noisy%20data%20matrices.%20We%0Aconsider%20eigenvector-based%20clustering%20applied%20to%20a%20matrix%20of%20nonparametric%20rank%0Astatistics%20that%20is%20derived%20entrywise%20from%20the%20raw%2C%20original%20data%20matrix.%20This%0Aapproach%20is%20robust%20in%20the%20sense%20that%2C%20unlike%20traditional%20spectral%20clustering%0Aprocedures%2C%20it%20can%20provably%20recover%20population-level%20latent%20block%20structure%0Aeven%20when%20the%20observed%20data%20matrix%20includes%20heavy-tailed%20entries%20and%20has%20a%0Aheterogeneous%20variance%20profile.%0A%20%20Our%20main%20theoretical%20contributions%20are%20threefold%20and%20hold%20under%20flexible%20data%0Agenerating%20conditions.%20First%2C%20we%20establish%20that%20robust%20spectral%20clustering%20with%0Arank%20statistics%20can%20consistently%20recover%20latent%20block%20structure%2C%20viewed%20as%0Acommunities%20of%20nodes%20in%20a%20graph%2C%20in%20the%20sense%20that%20unobserved%20community%0Amemberships%20for%20all%20but%20a%20vanishing%20fraction%20of%20nodes%20are%20correctly%20recovered%0Awith%20high%20probability%20when%20the%20data%20matrix%20is%20large.%20Second%2C%20we%20refine%20the%0Aformer%20result%20and%20further%20establish%20that%2C%20under%20certain%20conditions%2C%20the%0Acommunity%20membership%20of%20any%20individual%2C%20specified%20node%20of%20interest%20can%20be%0Aasymptotically%20exactly%20recovered%20with%20probability%20tending%20to%20one%20in%20the%0Alarge-data%20limit.%20Third%2C%20we%20establish%20asymptotic%20normality%20results%20associated%0Awith%20the%20truncated%20eigenstructure%20of%20matrices%20whose%20entries%20are%20rank%0Astatistics%2C%20made%20possible%20by%20synthesizing%20contemporary%20entrywise%20matrix%0Aperturbation%20analysis%20with%20the%20classical%20nonparametric%20theory%20of%20so-called%0Asimple%20linear%20rank%20statistics.%20Collectively%2C%20these%20results%20demonstrate%20the%0Astatistical%20utility%20of%20rank-based%20data%20transformations%20when%20paired%20with%0Aspectral%20techniques%20for%20dimensionality%20reduction.%20Additionally%2C%20for%20a%20dataset%0Aof%20human%20connectomes%2C%20our%20approach%20yields%20parsimonious%20dimensionality%20reduction%0Aand%20improved%20recovery%20of%20ground-truth%20neuroanatomical%20cluster%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10136v1&entry.124074799=Read"},
{"title": "Sparse Global Matching for Video Frame Interpolation with Large Motion", "author": "Chunxu Liu and Guozhen Zhang and Rui Zhao and Limin Wang", "abstract": "  Large motion poses a critical challenge in Video Frame Interpolation (VFI)\ntask. Existing methods are often constrained by limited receptive fields,\nresulting in sub-optimal performance when handling scenarios with large motion.\nIn this paper, we introduce a new pipeline for VFI, which can effectively\nintegrate global-level information to alleviate issues associated with large\nmotion. Specifically, we first estimate a pair of initial intermediate flows\nusing a high-resolution feature map for extracting local details. Then, we\nincorporate a sparse global matching branch to compensate for flow estimation,\nwhich consists of identifying flaws in initial flows and generating sparse flow\ncompensation with a global receptive field. Finally, we adaptively merge the\ninitial flow estimation with global flow compensation, yielding a more accurate\nintermediate flow. To evaluate the effectiveness of our method in handling\nlarge motion, we carefully curate a more challenging subset from commonly used\nbenchmarks. Our method demonstrates the state-of-the-art performance on these\nVFI subsets with large motion.\n", "link": "http://arxiv.org/abs/2404.06913v3", "date": "2024-08-19", "relevancy": 2.1227, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5456}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5401}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&body=Title%3A%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion%0AAuthor%3A%20Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06913v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Global%2520Matching%2520for%2520Video%2520Frame%2520Interpolation%2520with%2520Large%2520Motion%26entry.906535625%3DChunxu%2520Liu%2520and%2520Guozhen%2520Zhang%2520and%2520Rui%2520Zhao%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Large%2520motion%2520poses%2520a%2520critical%2520challenge%2520in%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%250Atask.%2520Existing%2520methods%2520are%2520often%2520constrained%2520by%2520limited%2520receptive%2520fields%252C%250Aresulting%2520in%2520sub-optimal%2520performance%2520when%2520handling%2520scenarios%2520with%2520large%2520motion.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520pipeline%2520for%2520VFI%252C%2520which%2520can%2520effectively%250Aintegrate%2520global-level%2520information%2520to%2520alleviate%2520issues%2520associated%2520with%2520large%250Amotion.%2520Specifically%252C%2520we%2520first%2520estimate%2520a%2520pair%2520of%2520initial%2520intermediate%2520flows%250Ausing%2520a%2520high-resolution%2520feature%2520map%2520for%2520extracting%2520local%2520details.%2520Then%252C%2520we%250Aincorporate%2520a%2520sparse%2520global%2520matching%2520branch%2520to%2520compensate%2520for%2520flow%2520estimation%252C%250Awhich%2520consists%2520of%2520identifying%2520flaws%2520in%2520initial%2520flows%2520and%2520generating%2520sparse%2520flow%250Acompensation%2520with%2520a%2520global%2520receptive%2520field.%2520Finally%252C%2520we%2520adaptively%2520merge%2520the%250Ainitial%2520flow%2520estimation%2520with%2520global%2520flow%2520compensation%252C%2520yielding%2520a%2520more%2520accurate%250Aintermediate%2520flow.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520handling%250Alarge%2520motion%252C%2520we%2520carefully%2520curate%2520a%2520more%2520challenging%2520subset%2520from%2520commonly%2520used%250Abenchmarks.%2520Our%2520method%2520demonstrates%2520the%2520state-of-the-art%2520performance%2520on%2520these%250AVFI%2520subsets%2520with%2520large%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06913v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&entry.906535625=Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang&entry.1292438233=%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06913v3&entry.124074799=Read"},
{"title": "Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional\n  Images", "author": "Wei Zhou and Zhou Wang", "abstract": "  Depth perception plays an essential role in the viewer experience for\nimmersive virtual reality (VR) visual environments. However, previous research\ninvestigations in the depth quality of 3D/stereoscopic images are rather\nlimited, and in particular, are largely lacking for 3D viewing of 360-degree\nomnidirectional content. In this work, we make one of the first attempts to\ndevelop an objective quality assessment model named depth quality index (DQI)\nfor efficient no-reference (NR) depth quality assessment of stereoscopic\nomnidirectional images. Motivated by the perceptual characteristics of the\nhuman visual system (HVS), the proposed DQI is built upon multi-color-channel,\nadaptive viewport selection, and interocular discrepancy features. Experimental\nresults demonstrate that the proposed method outperforms state-of-the-art image\nquality assessment (IQA) and depth quality assessment (DQA) approaches in\npredicting the perceptual depth quality when tested using both single-viewport\nand omnidirectional stereoscopic image databases. Furthermore, we demonstrate\nthat combining the proposed depth quality model with existing IQA methods\nsignificantly boosts the performance in predicting the overall quality of 3D\nomnidirectional images.\n", "link": "http://arxiv.org/abs/2408.10134v1", "date": "2024-08-19", "relevancy": 2.1207, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptual%20Depth%20Quality%20Assessment%20of%20Stereoscopic%20Omnidirectional%0A%20%20Images&body=Title%3A%20Perceptual%20Depth%20Quality%20Assessment%20of%20Stereoscopic%20Omnidirectional%0A%20%20Images%0AAuthor%3A%20Wei%20Zhou%20and%20Zhou%20Wang%0AAbstract%3A%20%20%20Depth%20perception%20plays%20an%20essential%20role%20in%20the%20viewer%20experience%20for%0Aimmersive%20virtual%20reality%20%28VR%29%20visual%20environments.%20However%2C%20previous%20research%0Ainvestigations%20in%20the%20depth%20quality%20of%203D/stereoscopic%20images%20are%20rather%0Alimited%2C%20and%20in%20particular%2C%20are%20largely%20lacking%20for%203D%20viewing%20of%20360-degree%0Aomnidirectional%20content.%20In%20this%20work%2C%20we%20make%20one%20of%20the%20first%20attempts%20to%0Adevelop%20an%20objective%20quality%20assessment%20model%20named%20depth%20quality%20index%20%28DQI%29%0Afor%20efficient%20no-reference%20%28NR%29%20depth%20quality%20assessment%20of%20stereoscopic%0Aomnidirectional%20images.%20Motivated%20by%20the%20perceptual%20characteristics%20of%20the%0Ahuman%20visual%20system%20%28HVS%29%2C%20the%20proposed%20DQI%20is%20built%20upon%20multi-color-channel%2C%0Aadaptive%20viewport%20selection%2C%20and%20interocular%20discrepancy%20features.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%20image%0Aquality%20assessment%20%28IQA%29%20and%20depth%20quality%20assessment%20%28DQA%29%20approaches%20in%0Apredicting%20the%20perceptual%20depth%20quality%20when%20tested%20using%20both%20single-viewport%0Aand%20omnidirectional%20stereoscopic%20image%20databases.%20Furthermore%2C%20we%20demonstrate%0Athat%20combining%20the%20proposed%20depth%20quality%20model%20with%20existing%20IQA%20methods%0Asignificantly%20boosts%20the%20performance%20in%20predicting%20the%20overall%20quality%20of%203D%0Aomnidirectional%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptual%2520Depth%2520Quality%2520Assessment%2520of%2520Stereoscopic%2520Omnidirectional%250A%2520%2520Images%26entry.906535625%3DWei%2520Zhou%2520and%2520Zhou%2520Wang%26entry.1292438233%3D%2520%2520Depth%2520perception%2520plays%2520an%2520essential%2520role%2520in%2520the%2520viewer%2520experience%2520for%250Aimmersive%2520virtual%2520reality%2520%2528VR%2529%2520visual%2520environments.%2520However%252C%2520previous%2520research%250Ainvestigations%2520in%2520the%2520depth%2520quality%2520of%25203D/stereoscopic%2520images%2520are%2520rather%250Alimited%252C%2520and%2520in%2520particular%252C%2520are%2520largely%2520lacking%2520for%25203D%2520viewing%2520of%2520360-degree%250Aomnidirectional%2520content.%2520In%2520this%2520work%252C%2520we%2520make%2520one%2520of%2520the%2520first%2520attempts%2520to%250Adevelop%2520an%2520objective%2520quality%2520assessment%2520model%2520named%2520depth%2520quality%2520index%2520%2528DQI%2529%250Afor%2520efficient%2520no-reference%2520%2528NR%2529%2520depth%2520quality%2520assessment%2520of%2520stereoscopic%250Aomnidirectional%2520images.%2520Motivated%2520by%2520the%2520perceptual%2520characteristics%2520of%2520the%250Ahuman%2520visual%2520system%2520%2528HVS%2529%252C%2520the%2520proposed%2520DQI%2520is%2520built%2520upon%2520multi-color-channel%252C%250Aadaptive%2520viewport%2520selection%252C%2520and%2520interocular%2520discrepancy%2520features.%2520Experimental%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520image%250Aquality%2520assessment%2520%2528IQA%2529%2520and%2520depth%2520quality%2520assessment%2520%2528DQA%2529%2520approaches%2520in%250Apredicting%2520the%2520perceptual%2520depth%2520quality%2520when%2520tested%2520using%2520both%2520single-viewport%250Aand%2520omnidirectional%2520stereoscopic%2520image%2520databases.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520combining%2520the%2520proposed%2520depth%2520quality%2520model%2520with%2520existing%2520IQA%2520methods%250Asignificantly%2520boosts%2520the%2520performance%2520in%2520predicting%2520the%2520overall%2520quality%2520of%25203D%250Aomnidirectional%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptual%20Depth%20Quality%20Assessment%20of%20Stereoscopic%20Omnidirectional%0A%20%20Images&entry.906535625=Wei%20Zhou%20and%20Zhou%20Wang&entry.1292438233=%20%20Depth%20perception%20plays%20an%20essential%20role%20in%20the%20viewer%20experience%20for%0Aimmersive%20virtual%20reality%20%28VR%29%20visual%20environments.%20However%2C%20previous%20research%0Ainvestigations%20in%20the%20depth%20quality%20of%203D/stereoscopic%20images%20are%20rather%0Alimited%2C%20and%20in%20particular%2C%20are%20largely%20lacking%20for%203D%20viewing%20of%20360-degree%0Aomnidirectional%20content.%20In%20this%20work%2C%20we%20make%20one%20of%20the%20first%20attempts%20to%0Adevelop%20an%20objective%20quality%20assessment%20model%20named%20depth%20quality%20index%20%28DQI%29%0Afor%20efficient%20no-reference%20%28NR%29%20depth%20quality%20assessment%20of%20stereoscopic%0Aomnidirectional%20images.%20Motivated%20by%20the%20perceptual%20characteristics%20of%20the%0Ahuman%20visual%20system%20%28HVS%29%2C%20the%20proposed%20DQI%20is%20built%20upon%20multi-color-channel%2C%0Aadaptive%20viewport%20selection%2C%20and%20interocular%20discrepancy%20features.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%20image%0Aquality%20assessment%20%28IQA%29%20and%20depth%20quality%20assessment%20%28DQA%29%20approaches%20in%0Apredicting%20the%20perceptual%20depth%20quality%20when%20tested%20using%20both%20single-viewport%0Aand%20omnidirectional%20stereoscopic%20image%20databases.%20Furthermore%2C%20we%20demonstrate%0Athat%20combining%20the%20proposed%20depth%20quality%20model%20with%20existing%20IQA%20methods%0Asignificantly%20boosts%20the%20performance%20in%20predicting%20the%20overall%20quality%20of%203D%0Aomnidirectional%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10134v1&entry.124074799=Read"},
{"title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic\n  Models", "author": "Aviv Bick and Kevin Y. Li and Eric P. Xing and J. Zico Kolter and Albert Gu", "abstract": "  Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.\n", "link": "http://arxiv.org/abs/2408.10189v1", "date": "2024-08-19", "relevancy": 2.1047, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5583}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20to%20SSMs%3A%20Distilling%20Quadratic%20Knowledge%20to%20Subquadratic%0A%20%20Models&body=Title%3A%20Transformers%20to%20SSMs%3A%20Distilling%20Quadratic%20Knowledge%20to%20Subquadratic%0A%20%20Models%0AAuthor%3A%20Aviv%20Bick%20and%20Kevin%20Y.%20Li%20and%20Eric%20P.%20Xing%20and%20J.%20Zico%20Kolter%20and%20Albert%20Gu%0AAbstract%3A%20%20%20Transformer%20architectures%20have%20become%20a%20dominant%20paradigm%20for%20domains%20like%0Alanguage%20modeling%20but%20suffer%20in%20many%20inference%20settings%20due%20to%20their%0Aquadratic-time%20self-attention.%20Recently%20proposed%20subquadratic%20architectures%2C%0Asuch%20as%20Mamba%2C%20have%20shown%20promise%2C%20but%20have%20been%20pretrained%20with%20substantially%0Aless%20computational%20resources%20than%20the%20strongest%20Transformer%20models.%20In%20this%0Awork%2C%20we%20present%20a%20method%20that%20is%20able%20to%20distill%20a%20pretrained%20Transformer%0Aarchitecture%20into%20alternative%20architectures%20such%20as%20state%20space%20models%20%28SSMs%29.%0AThe%20key%20idea%20to%20our%20approach%20is%20that%20we%20can%20view%20both%20Transformers%20and%20SSMs%20as%0Aapplying%20different%20forms%20of%20mixing%20matrices%20over%20the%20token%20sequences.%20We%20can%0Athus%20progressively%20distill%20the%20Transformer%20architecture%20by%20matching%20different%0Adegrees%20of%20granularity%20in%20the%20SSM%3A%20first%20matching%20the%20mixing%20matrices%0Athemselves%2C%20then%20the%20hidden%20units%20at%20each%20block%2C%20and%20finally%20the%20end-to-end%0Apredictions.%20Our%20method%2C%20called%20MOHAWK%2C%20is%20able%20to%20distill%20a%20Mamba-2%20variant%0Abased%20on%20the%20Phi-1.5%20architecture%20%28Phi-Mamba%29%20using%20only%203B%20tokens%20and%20a%20hybrid%0Aversion%20%28Hybrid%20Phi-Mamba%29%20using%205B%20tokens.%20Despite%20using%20less%20than%201%25%20of%20the%0Atraining%20data%20typically%20used%20to%20train%20models%20from%20scratch%2C%20Phi-Mamba%20boasts%0Asubstantially%20stronger%20performance%20compared%20to%20all%20past%20open-source%0Anon-Transformer%20models.%20MOHAWK%20allows%20models%20like%20SSMs%20to%20leverage%0Acomputational%20resources%20invested%20in%20training%20Transformer-based%20architectures%2C%0Ahighlighting%20a%20new%20avenue%20for%20building%20such%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520to%2520SSMs%253A%2520Distilling%2520Quadratic%2520Knowledge%2520to%2520Subquadratic%250A%2520%2520Models%26entry.906535625%3DAviv%2520Bick%2520and%2520Kevin%2520Y.%2520Li%2520and%2520Eric%2520P.%2520Xing%2520and%2520J.%2520Zico%2520Kolter%2520and%2520Albert%2520Gu%26entry.1292438233%3D%2520%2520Transformer%2520architectures%2520have%2520become%2520a%2520dominant%2520paradigm%2520for%2520domains%2520like%250Alanguage%2520modeling%2520but%2520suffer%2520in%2520many%2520inference%2520settings%2520due%2520to%2520their%250Aquadratic-time%2520self-attention.%2520Recently%2520proposed%2520subquadratic%2520architectures%252C%250Asuch%2520as%2520Mamba%252C%2520have%2520shown%2520promise%252C%2520but%2520have%2520been%2520pretrained%2520with%2520substantially%250Aless%2520computational%2520resources%2520than%2520the%2520strongest%2520Transformer%2520models.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520method%2520that%2520is%2520able%2520to%2520distill%2520a%2520pretrained%2520Transformer%250Aarchitecture%2520into%2520alternative%2520architectures%2520such%2520as%2520state%2520space%2520models%2520%2528SSMs%2529.%250AThe%2520key%2520idea%2520to%2520our%2520approach%2520is%2520that%2520we%2520can%2520view%2520both%2520Transformers%2520and%2520SSMs%2520as%250Aapplying%2520different%2520forms%2520of%2520mixing%2520matrices%2520over%2520the%2520token%2520sequences.%2520We%2520can%250Athus%2520progressively%2520distill%2520the%2520Transformer%2520architecture%2520by%2520matching%2520different%250Adegrees%2520of%2520granularity%2520in%2520the%2520SSM%253A%2520first%2520matching%2520the%2520mixing%2520matrices%250Athemselves%252C%2520then%2520the%2520hidden%2520units%2520at%2520each%2520block%252C%2520and%2520finally%2520the%2520end-to-end%250Apredictions.%2520Our%2520method%252C%2520called%2520MOHAWK%252C%2520is%2520able%2520to%2520distill%2520a%2520Mamba-2%2520variant%250Abased%2520on%2520the%2520Phi-1.5%2520architecture%2520%2528Phi-Mamba%2529%2520using%2520only%25203B%2520tokens%2520and%2520a%2520hybrid%250Aversion%2520%2528Hybrid%2520Phi-Mamba%2529%2520using%25205B%2520tokens.%2520Despite%2520using%2520less%2520than%25201%2525%2520of%2520the%250Atraining%2520data%2520typically%2520used%2520to%2520train%2520models%2520from%2520scratch%252C%2520Phi-Mamba%2520boasts%250Asubstantially%2520stronger%2520performance%2520compared%2520to%2520all%2520past%2520open-source%250Anon-Transformer%2520models.%2520MOHAWK%2520allows%2520models%2520like%2520SSMs%2520to%2520leverage%250Acomputational%2520resources%2520invested%2520in%2520training%2520Transformer-based%2520architectures%252C%250Ahighlighting%2520a%2520new%2520avenue%2520for%2520building%2520such%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20to%20SSMs%3A%20Distilling%20Quadratic%20Knowledge%20to%20Subquadratic%0A%20%20Models&entry.906535625=Aviv%20Bick%20and%20Kevin%20Y.%20Li%20and%20Eric%20P.%20Xing%20and%20J.%20Zico%20Kolter%20and%20Albert%20Gu&entry.1292438233=%20%20Transformer%20architectures%20have%20become%20a%20dominant%20paradigm%20for%20domains%20like%0Alanguage%20modeling%20but%20suffer%20in%20many%20inference%20settings%20due%20to%20their%0Aquadratic-time%20self-attention.%20Recently%20proposed%20subquadratic%20architectures%2C%0Asuch%20as%20Mamba%2C%20have%20shown%20promise%2C%20but%20have%20been%20pretrained%20with%20substantially%0Aless%20computational%20resources%20than%20the%20strongest%20Transformer%20models.%20In%20this%0Awork%2C%20we%20present%20a%20method%20that%20is%20able%20to%20distill%20a%20pretrained%20Transformer%0Aarchitecture%20into%20alternative%20architectures%20such%20as%20state%20space%20models%20%28SSMs%29.%0AThe%20key%20idea%20to%20our%20approach%20is%20that%20we%20can%20view%20both%20Transformers%20and%20SSMs%20as%0Aapplying%20different%20forms%20of%20mixing%20matrices%20over%20the%20token%20sequences.%20We%20can%0Athus%20progressively%20distill%20the%20Transformer%20architecture%20by%20matching%20different%0Adegrees%20of%20granularity%20in%20the%20SSM%3A%20first%20matching%20the%20mixing%20matrices%0Athemselves%2C%20then%20the%20hidden%20units%20at%20each%20block%2C%20and%20finally%20the%20end-to-end%0Apredictions.%20Our%20method%2C%20called%20MOHAWK%2C%20is%20able%20to%20distill%20a%20Mamba-2%20variant%0Abased%20on%20the%20Phi-1.5%20architecture%20%28Phi-Mamba%29%20using%20only%203B%20tokens%20and%20a%20hybrid%0Aversion%20%28Hybrid%20Phi-Mamba%29%20using%205B%20tokens.%20Despite%20using%20less%20than%201%25%20of%20the%0Atraining%20data%20typically%20used%20to%20train%20models%20from%20scratch%2C%20Phi-Mamba%20boasts%0Asubstantially%20stronger%20performance%20compared%20to%20all%20past%20open-source%0Anon-Transformer%20models.%20MOHAWK%20allows%20models%20like%20SSMs%20to%20leverage%0Acomputational%20resources%20invested%20in%20training%20Transformer-based%20architectures%2C%0Ahighlighting%20a%20new%20avenue%20for%20building%20such%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10189v1&entry.124074799=Read"},
{"title": "Spatial-Frequency Dual Progressive Attention Network For Medical Image\n  Segmentation", "author": "Zhenhuan Zhou and Along He and Yanlin Wu and Rui Yao and Xueshuo Xie and Tao Li", "abstract": "  In medical images, various types of lesions often manifest significant\ndifferences in their shape and texture. Accurate medical image segmentation\ndemands deep learning models with robust capabilities in multi-scale and\nboundary feature learning. However, previous networks still have limitations in\naddressing the above issues. Firstly, previous networks simultaneously fuse\nmulti-level features or employ deep supervision to enhance multi-scale\nlearning. However, this may lead to feature redundancy and excessive\ncomputational overhead, which is not conducive to network training and clinical\ndeployment. Secondly, the majority of medical image segmentation networks\nexclusively learn features in the spatial domain, disregarding the abundant\nglobal information in the frequency domain. This results in a bias towards\nlow-frequency components, neglecting crucial high-frequency information. To\naddress these problems, we introduce SF-UNet, a spatial-frequency dual-domain\nattention network. It comprises two main components: the Multi-scale\nProgressive Channel Attention (MPCA) block, which progressively extract\nmulti-scale features across adjacent encoder layers, and the lightweight\nFrequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling\nconcurrent learning of texture and boundary features from both spatial and\nfrequency domains. We validate the effectiveness of the proposed SF-UNet on\nthree public datasets. Experimental results show that compared to previous\nstate-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves\nthe best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC\nand IOU. Codes will be released at https://github.com/nkicsl/SF-UNet.\n", "link": "http://arxiv.org/abs/2406.07952v2", "date": "2024-08-19", "relevancy": 2.0991, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Frequency%20Dual%20Progressive%20Attention%20Network%20For%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Spatial-Frequency%20Dual%20Progressive%20Attention%20Network%20For%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Zhenhuan%20Zhou%20and%20Along%20He%20and%20Yanlin%20Wu%20and%20Rui%20Yao%20and%20Xueshuo%20Xie%20and%20Tao%20Li%0AAbstract%3A%20%20%20In%20medical%20images%2C%20various%20types%20of%20lesions%20often%20manifest%20significant%0Adifferences%20in%20their%20shape%20and%20texture.%20Accurate%20medical%20image%20segmentation%0Ademands%20deep%20learning%20models%20with%20robust%20capabilities%20in%20multi-scale%20and%0Aboundary%20feature%20learning.%20However%2C%20previous%20networks%20still%20have%20limitations%20in%0Aaddressing%20the%20above%20issues.%20Firstly%2C%20previous%20networks%20simultaneously%20fuse%0Amulti-level%20features%20or%20employ%20deep%20supervision%20to%20enhance%20multi-scale%0Alearning.%20However%2C%20this%20may%20lead%20to%20feature%20redundancy%20and%20excessive%0Acomputational%20overhead%2C%20which%20is%20not%20conducive%20to%20network%20training%20and%20clinical%0Adeployment.%20Secondly%2C%20the%20majority%20of%20medical%20image%20segmentation%20networks%0Aexclusively%20learn%20features%20in%20the%20spatial%20domain%2C%20disregarding%20the%20abundant%0Aglobal%20information%20in%20the%20frequency%20domain.%20This%20results%20in%20a%20bias%20towards%0Alow-frequency%20components%2C%20neglecting%20crucial%20high-frequency%20information.%20To%0Aaddress%20these%20problems%2C%20we%20introduce%20SF-UNet%2C%20a%20spatial-frequency%20dual-domain%0Aattention%20network.%20It%20comprises%20two%20main%20components%3A%20the%20Multi-scale%0AProgressive%20Channel%20Attention%20%28MPCA%29%20block%2C%20which%20progressively%20extract%0Amulti-scale%20features%20across%20adjacent%20encoder%20layers%2C%20and%20the%20lightweight%0AFrequency-Spatial%20Attention%20%28FSA%29%20block%2C%20with%20only%200.05M%20parameters%2C%20enabling%0Aconcurrent%20learning%20of%20texture%20and%20boundary%20features%20from%20both%20spatial%20and%0Afrequency%20domains.%20We%20validate%20the%20effectiveness%20of%20the%20proposed%20SF-UNet%20on%0Athree%20public%20datasets.%20Experimental%20results%20show%20that%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20medical%20image%20segmentation%20networks%2C%20SF-UNet%20achieves%0Athe%20best%20performance%2C%20and%20achieves%20up%20to%209.4%5C%25%20and%2010.78%5C%25%20improvement%20in%20DSC%0Aand%20IOU.%20Codes%20will%20be%20released%20at%20https%3A//github.com/nkicsl/SF-UNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Frequency%2520Dual%2520Progressive%2520Attention%2520Network%2520For%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DZhenhuan%2520Zhou%2520and%2520Along%2520He%2520and%2520Yanlin%2520Wu%2520and%2520Rui%2520Yao%2520and%2520Xueshuo%2520Xie%2520and%2520Tao%2520Li%26entry.1292438233%3D%2520%2520In%2520medical%2520images%252C%2520various%2520types%2520of%2520lesions%2520often%2520manifest%2520significant%250Adifferences%2520in%2520their%2520shape%2520and%2520texture.%2520Accurate%2520medical%2520image%2520segmentation%250Ademands%2520deep%2520learning%2520models%2520with%2520robust%2520capabilities%2520in%2520multi-scale%2520and%250Aboundary%2520feature%2520learning.%2520However%252C%2520previous%2520networks%2520still%2520have%2520limitations%2520in%250Aaddressing%2520the%2520above%2520issues.%2520Firstly%252C%2520previous%2520networks%2520simultaneously%2520fuse%250Amulti-level%2520features%2520or%2520employ%2520deep%2520supervision%2520to%2520enhance%2520multi-scale%250Alearning.%2520However%252C%2520this%2520may%2520lead%2520to%2520feature%2520redundancy%2520and%2520excessive%250Acomputational%2520overhead%252C%2520which%2520is%2520not%2520conducive%2520to%2520network%2520training%2520and%2520clinical%250Adeployment.%2520Secondly%252C%2520the%2520majority%2520of%2520medical%2520image%2520segmentation%2520networks%250Aexclusively%2520learn%2520features%2520in%2520the%2520spatial%2520domain%252C%2520disregarding%2520the%2520abundant%250Aglobal%2520information%2520in%2520the%2520frequency%2520domain.%2520This%2520results%2520in%2520a%2520bias%2520towards%250Alow-frequency%2520components%252C%2520neglecting%2520crucial%2520high-frequency%2520information.%2520To%250Aaddress%2520these%2520problems%252C%2520we%2520introduce%2520SF-UNet%252C%2520a%2520spatial-frequency%2520dual-domain%250Aattention%2520network.%2520It%2520comprises%2520two%2520main%2520components%253A%2520the%2520Multi-scale%250AProgressive%2520Channel%2520Attention%2520%2528MPCA%2529%2520block%252C%2520which%2520progressively%2520extract%250Amulti-scale%2520features%2520across%2520adjacent%2520encoder%2520layers%252C%2520and%2520the%2520lightweight%250AFrequency-Spatial%2520Attention%2520%2528FSA%2529%2520block%252C%2520with%2520only%25200.05M%2520parameters%252C%2520enabling%250Aconcurrent%2520learning%2520of%2520texture%2520and%2520boundary%2520features%2520from%2520both%2520spatial%2520and%250Afrequency%2520domains.%2520We%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520SF-UNet%2520on%250Athree%2520public%2520datasets.%2520Experimental%2520results%2520show%2520that%2520compared%2520to%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520medical%2520image%2520segmentation%2520networks%252C%2520SF-UNet%2520achieves%250Athe%2520best%2520performance%252C%2520and%2520achieves%2520up%2520to%25209.4%255C%2525%2520and%252010.78%255C%2525%2520improvement%2520in%2520DSC%250Aand%2520IOU.%2520Codes%2520will%2520be%2520released%2520at%2520https%253A//github.com/nkicsl/SF-UNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Frequency%20Dual%20Progressive%20Attention%20Network%20For%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Zhenhuan%20Zhou%20and%20Along%20He%20and%20Yanlin%20Wu%20and%20Rui%20Yao%20and%20Xueshuo%20Xie%20and%20Tao%20Li&entry.1292438233=%20%20In%20medical%20images%2C%20various%20types%20of%20lesions%20often%20manifest%20significant%0Adifferences%20in%20their%20shape%20and%20texture.%20Accurate%20medical%20image%20segmentation%0Ademands%20deep%20learning%20models%20with%20robust%20capabilities%20in%20multi-scale%20and%0Aboundary%20feature%20learning.%20However%2C%20previous%20networks%20still%20have%20limitations%20in%0Aaddressing%20the%20above%20issues.%20Firstly%2C%20previous%20networks%20simultaneously%20fuse%0Amulti-level%20features%20or%20employ%20deep%20supervision%20to%20enhance%20multi-scale%0Alearning.%20However%2C%20this%20may%20lead%20to%20feature%20redundancy%20and%20excessive%0Acomputational%20overhead%2C%20which%20is%20not%20conducive%20to%20network%20training%20and%20clinical%0Adeployment.%20Secondly%2C%20the%20majority%20of%20medical%20image%20segmentation%20networks%0Aexclusively%20learn%20features%20in%20the%20spatial%20domain%2C%20disregarding%20the%20abundant%0Aglobal%20information%20in%20the%20frequency%20domain.%20This%20results%20in%20a%20bias%20towards%0Alow-frequency%20components%2C%20neglecting%20crucial%20high-frequency%20information.%20To%0Aaddress%20these%20problems%2C%20we%20introduce%20SF-UNet%2C%20a%20spatial-frequency%20dual-domain%0Aattention%20network.%20It%20comprises%20two%20main%20components%3A%20the%20Multi-scale%0AProgressive%20Channel%20Attention%20%28MPCA%29%20block%2C%20which%20progressively%20extract%0Amulti-scale%20features%20across%20adjacent%20encoder%20layers%2C%20and%20the%20lightweight%0AFrequency-Spatial%20Attention%20%28FSA%29%20block%2C%20with%20only%200.05M%20parameters%2C%20enabling%0Aconcurrent%20learning%20of%20texture%20and%20boundary%20features%20from%20both%20spatial%20and%0Afrequency%20domains.%20We%20validate%20the%20effectiveness%20of%20the%20proposed%20SF-UNet%20on%0Athree%20public%20datasets.%20Experimental%20results%20show%20that%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20medical%20image%20segmentation%20networks%2C%20SF-UNet%20achieves%0Athe%20best%20performance%2C%20and%20achieves%20up%20to%209.4%5C%25%20and%2010.78%5C%25%20improvement%20in%20DSC%0Aand%20IOU.%20Codes%20will%20be%20released%20at%20https%3A//github.com/nkicsl/SF-UNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07952v2&entry.124074799=Read"},
{"title": "Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models", "author": "Tianyu Zhang and Yuxiang Ren and Chengbin Hou and Hairong Lv and Xuegong Zhang", "abstract": "  Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2408.10124v1", "date": "2024-08-19", "relevancy": 2.0883, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5177}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molecular%20Graph%20Representation%20Learning%20Integrating%20Large%20Language%0A%20%20Models%20with%20Domain-specific%20Small%20Models&body=Title%3A%20Molecular%20Graph%20Representation%20Learning%20Integrating%20Large%20Language%0A%20%20Models%20with%20Domain-specific%20Small%20Models%0AAuthor%3A%20Tianyu%20Zhang%20and%20Yuxiang%20Ren%20and%20Chengbin%20Hou%20and%20Hairong%20Lv%20and%20Xuegong%20Zhang%0AAbstract%3A%20%20%20Molecular%20property%20prediction%20is%20a%20crucial%20foundation%20for%20drug%20discovery.%20In%0Arecent%20years%2C%20pre-trained%20deep%20learning%20models%20have%20been%20widely%20applied%20to%20this%0Atask.%20Some%20approaches%20that%20incorporate%20prior%20biological%20domain%20knowledge%20into%0Athe%20pre-training%20framework%20have%20achieved%20impressive%20results.%20However%2C%20these%0Amethods%20heavily%20rely%20on%20biochemical%20experts%2C%20and%20retrieving%20and%20summarizing%0Avast%20amounts%20of%20domain%20knowledge%20literature%20is%20both%20time-consuming%20and%0Aexpensive.%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Aperformance%20in%20understanding%20and%20efficiently%20providing%20general%20knowledge.%0ANevertheless%2C%20they%20occasionally%20exhibit%20hallucinations%20and%20lack%20precision%20in%0Agenerating%20domain-specific%20knowledge.%20Conversely%2C%20Domain-specific%20Small%20Models%0A%28DSMs%29%20possess%20rich%20domain%20knowledge%20and%20can%20accurately%20calculate%20molecular%0Adomain-related%20metrics.%20However%2C%20due%20to%20their%20limited%20model%20size%20and%20singular%0Afunctionality%2C%20they%20lack%20the%20breadth%20of%20knowledge%20necessary%20for%20comprehensive%0Arepresentation%20learning.%20To%20leverage%20the%20advantages%20of%20both%20approaches%20in%0Amolecular%20property%20prediction%2C%20we%20propose%20a%20novel%20Molecular%20Graph%0Arepresentation%20learning%20framework%20that%20integrates%20Large%20language%20models%20and%0ADomain-specific%20small%20models%20%28MolGraph-LarDo%29.%20Technically%2C%20we%20design%20a%0Atwo-stage%20prompt%20strategy%20where%20DSMs%20are%20introduced%20to%20calibrate%20the%20knowledge%0Aprovided%20by%20LLMs%2C%20enhancing%20the%20accuracy%20of%20domain-specific%20information%20and%0Athus%20enabling%20LLMs%20to%20generate%20more%20precise%20textual%20descriptions%20for%20molecular%0Asamples.%20Subsequently%2C%20we%20employ%20a%20multi-modal%20alignment%20method%20to%20coordinate%0Avarious%20modalities%2C%20including%20molecular%20graphs%20and%20their%20corresponding%0Adescriptive%20texts%2C%20to%20guide%20the%20pre-training%20of%20molecular%20representations.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolecular%2520Graph%2520Representation%2520Learning%2520Integrating%2520Large%2520Language%250A%2520%2520Models%2520with%2520Domain-specific%2520Small%2520Models%26entry.906535625%3DTianyu%2520Zhang%2520and%2520Yuxiang%2520Ren%2520and%2520Chengbin%2520Hou%2520and%2520Hairong%2520Lv%2520and%2520Xuegong%2520Zhang%26entry.1292438233%3D%2520%2520Molecular%2520property%2520prediction%2520is%2520a%2520crucial%2520foundation%2520for%2520drug%2520discovery.%2520In%250Arecent%2520years%252C%2520pre-trained%2520deep%2520learning%2520models%2520have%2520been%2520widely%2520applied%2520to%2520this%250Atask.%2520Some%2520approaches%2520that%2520incorporate%2520prior%2520biological%2520domain%2520knowledge%2520into%250Athe%2520pre-training%2520framework%2520have%2520achieved%2520impressive%2520results.%2520However%252C%2520these%250Amethods%2520heavily%2520rely%2520on%2520biochemical%2520experts%252C%2520and%2520retrieving%2520and%2520summarizing%250Avast%2520amounts%2520of%2520domain%2520knowledge%2520literature%2520is%2520both%2520time-consuming%2520and%250Aexpensive.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%250Aperformance%2520in%2520understanding%2520and%2520efficiently%2520providing%2520general%2520knowledge.%250ANevertheless%252C%2520they%2520occasionally%2520exhibit%2520hallucinations%2520and%2520lack%2520precision%2520in%250Agenerating%2520domain-specific%2520knowledge.%2520Conversely%252C%2520Domain-specific%2520Small%2520Models%250A%2528DSMs%2529%2520possess%2520rich%2520domain%2520knowledge%2520and%2520can%2520accurately%2520calculate%2520molecular%250Adomain-related%2520metrics.%2520However%252C%2520due%2520to%2520their%2520limited%2520model%2520size%2520and%2520singular%250Afunctionality%252C%2520they%2520lack%2520the%2520breadth%2520of%2520knowledge%2520necessary%2520for%2520comprehensive%250Arepresentation%2520learning.%2520To%2520leverage%2520the%2520advantages%2520of%2520both%2520approaches%2520in%250Amolecular%2520property%2520prediction%252C%2520we%2520propose%2520a%2520novel%2520Molecular%2520Graph%250Arepresentation%2520learning%2520framework%2520that%2520integrates%2520Large%2520language%2520models%2520and%250ADomain-specific%2520small%2520models%2520%2528MolGraph-LarDo%2529.%2520Technically%252C%2520we%2520design%2520a%250Atwo-stage%2520prompt%2520strategy%2520where%2520DSMs%2520are%2520introduced%2520to%2520calibrate%2520the%2520knowledge%250Aprovided%2520by%2520LLMs%252C%2520enhancing%2520the%2520accuracy%2520of%2520domain-specific%2520information%2520and%250Athus%2520enabling%2520LLMs%2520to%2520generate%2520more%2520precise%2520textual%2520descriptions%2520for%2520molecular%250Asamples.%2520Subsequently%252C%2520we%2520employ%2520a%2520multi-modal%2520alignment%2520method%2520to%2520coordinate%250Avarious%2520modalities%252C%2520including%2520molecular%2520graphs%2520and%2520their%2520corresponding%250Adescriptive%2520texts%252C%2520to%2520guide%2520the%2520pre-training%2520of%2520molecular%2520representations.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molecular%20Graph%20Representation%20Learning%20Integrating%20Large%20Language%0A%20%20Models%20with%20Domain-specific%20Small%20Models&entry.906535625=Tianyu%20Zhang%20and%20Yuxiang%20Ren%20and%20Chengbin%20Hou%20and%20Hairong%20Lv%20and%20Xuegong%20Zhang&entry.1292438233=%20%20Molecular%20property%20prediction%20is%20a%20crucial%20foundation%20for%20drug%20discovery.%20In%0Arecent%20years%2C%20pre-trained%20deep%20learning%20models%20have%20been%20widely%20applied%20to%20this%0Atask.%20Some%20approaches%20that%20incorporate%20prior%20biological%20domain%20knowledge%20into%0Athe%20pre-training%20framework%20have%20achieved%20impressive%20results.%20However%2C%20these%0Amethods%20heavily%20rely%20on%20biochemical%20experts%2C%20and%20retrieving%20and%20summarizing%0Avast%20amounts%20of%20domain%20knowledge%20literature%20is%20both%20time-consuming%20and%0Aexpensive.%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Aperformance%20in%20understanding%20and%20efficiently%20providing%20general%20knowledge.%0ANevertheless%2C%20they%20occasionally%20exhibit%20hallucinations%20and%20lack%20precision%20in%0Agenerating%20domain-specific%20knowledge.%20Conversely%2C%20Domain-specific%20Small%20Models%0A%28DSMs%29%20possess%20rich%20domain%20knowledge%20and%20can%20accurately%20calculate%20molecular%0Adomain-related%20metrics.%20However%2C%20due%20to%20their%20limited%20model%20size%20and%20singular%0Afunctionality%2C%20they%20lack%20the%20breadth%20of%20knowledge%20necessary%20for%20comprehensive%0Arepresentation%20learning.%20To%20leverage%20the%20advantages%20of%20both%20approaches%20in%0Amolecular%20property%20prediction%2C%20we%20propose%20a%20novel%20Molecular%20Graph%0Arepresentation%20learning%20framework%20that%20integrates%20Large%20language%20models%20and%0ADomain-specific%20small%20models%20%28MolGraph-LarDo%29.%20Technically%2C%20we%20design%20a%0Atwo-stage%20prompt%20strategy%20where%20DSMs%20are%20introduced%20to%20calibrate%20the%20knowledge%0Aprovided%20by%20LLMs%2C%20enhancing%20the%20accuracy%20of%20domain-specific%20information%20and%0Athus%20enabling%20LLMs%20to%20generate%20more%20precise%20textual%20descriptions%20for%20molecular%0Asamples.%20Subsequently%2C%20we%20employ%20a%20multi-modal%20alignment%20method%20to%20coordinate%0Avarious%20modalities%2C%20including%20molecular%20graphs%20and%20their%20corresponding%0Adescriptive%20texts%2C%20to%20guide%20the%20pre-training%20of%20molecular%20representations.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10124v1&entry.124074799=Read"},
{"title": "CLIPCleaner: Cleaning Noisy Labels with CLIP", "author": "Chen Feng and Georgios Tzimiropoulos and Ioannis Patras", "abstract": "  Learning with Noisy labels (LNL) poses a significant challenge for the\nMachine Learning community. Some of the most widely used approaches that select\nas clean samples for which the model itself (the in-training model) has high\nconfidence, e.g., `small loss', can suffer from the so called\n`self-confirmation' bias. This bias arises because the in-training model, is at\nleast partially trained on the noisy labels. Furthermore, in the classification\ncase, an additional challenge arises because some of the label noise is between\nclasses that are visually very similar (`hard noise'). This paper addresses\nthese challenges by proposing a method (\\textit{CLIPCleaner}) that leverages\nCLIP, a powerful Vision-Language (VL) model for constructing a zero-shot\nclassifier for efficient, offline, clean sample selection. This has the\nadvantage that the sample selection is decoupled from the in-training model and\nthat the sample selection is aware of the semantic and visual similarities\nbetween the classes due to the way that CLIP is trained. We provide theoretical\njustifications and empirical evidence to demonstrate the advantages of CLIP for\nLNL compared to conventional pre-trained models. Compared to current methods\nthat combine iterative sample selection with various techniques,\n\\textit{CLIPCleaner} offers a simple, single-step approach that achieves\ncompetitive or superior performance on benchmark datasets. To the best of our\nknowledge, this is the first time a VL model has been used for sample selection\nto address the problem of Learning with Noisy Labels (LNL), highlighting their\npotential in the domain.\n", "link": "http://arxiv.org/abs/2408.10012v1", "date": "2024-08-19", "relevancy": 2.0836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5154}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP&body=Title%3A%20CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP%0AAuthor%3A%20Chen%20Feng%20and%20Georgios%20Tzimiropoulos%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20Learning%20with%20Noisy%20labels%20%28LNL%29%20poses%20a%20significant%20challenge%20for%20the%0AMachine%20Learning%20community.%20Some%20of%20the%20most%20widely%20used%20approaches%20that%20select%0Aas%20clean%20samples%20for%20which%20the%20model%20itself%20%28the%20in-training%20model%29%20has%20high%0Aconfidence%2C%20e.g.%2C%20%60small%20loss%27%2C%20can%20suffer%20from%20the%20so%20called%0A%60self-confirmation%27%20bias.%20This%20bias%20arises%20because%20the%20in-training%20model%2C%20is%20at%0Aleast%20partially%20trained%20on%20the%20noisy%20labels.%20Furthermore%2C%20in%20the%20classification%0Acase%2C%20an%20additional%20challenge%20arises%20because%20some%20of%20the%20label%20noise%20is%20between%0Aclasses%20that%20are%20visually%20very%20similar%20%28%60hard%20noise%27%29.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20a%20method%20%28%5Ctextit%7BCLIPCleaner%7D%29%20that%20leverages%0ACLIP%2C%20a%20powerful%20Vision-Language%20%28VL%29%20model%20for%20constructing%20a%20zero-shot%0Aclassifier%20for%20efficient%2C%20offline%2C%20clean%20sample%20selection.%20This%20has%20the%0Aadvantage%20that%20the%20sample%20selection%20is%20decoupled%20from%20the%20in-training%20model%20and%0Athat%20the%20sample%20selection%20is%20aware%20of%20the%20semantic%20and%20visual%20similarities%0Abetween%20the%20classes%20due%20to%20the%20way%20that%20CLIP%20is%20trained.%20We%20provide%20theoretical%0Ajustifications%20and%20empirical%20evidence%20to%20demonstrate%20the%20advantages%20of%20CLIP%20for%0ALNL%20compared%20to%20conventional%20pre-trained%20models.%20Compared%20to%20current%20methods%0Athat%20combine%20iterative%20sample%20selection%20with%20various%20techniques%2C%0A%5Ctextit%7BCLIPCleaner%7D%20offers%20a%20simple%2C%20single-step%20approach%20that%20achieves%0Acompetitive%20or%20superior%20performance%20on%20benchmark%20datasets.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20time%20a%20VL%20model%20has%20been%20used%20for%20sample%20selection%0Ato%20address%20the%20problem%20of%20Learning%20with%20Noisy%20Labels%20%28LNL%29%2C%20highlighting%20their%0Apotential%20in%20the%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPCleaner%253A%2520Cleaning%2520Noisy%2520Labels%2520with%2520CLIP%26entry.906535625%3DChen%2520Feng%2520and%2520Georgios%2520Tzimiropoulos%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520Learning%2520with%2520Noisy%2520labels%2520%2528LNL%2529%2520poses%2520a%2520significant%2520challenge%2520for%2520the%250AMachine%2520Learning%2520community.%2520Some%2520of%2520the%2520most%2520widely%2520used%2520approaches%2520that%2520select%250Aas%2520clean%2520samples%2520for%2520which%2520the%2520model%2520itself%2520%2528the%2520in-training%2520model%2529%2520has%2520high%250Aconfidence%252C%2520e.g.%252C%2520%2560small%2520loss%2527%252C%2520can%2520suffer%2520from%2520the%2520so%2520called%250A%2560self-confirmation%2527%2520bias.%2520This%2520bias%2520arises%2520because%2520the%2520in-training%2520model%252C%2520is%2520at%250Aleast%2520partially%2520trained%2520on%2520the%2520noisy%2520labels.%2520Furthermore%252C%2520in%2520the%2520classification%250Acase%252C%2520an%2520additional%2520challenge%2520arises%2520because%2520some%2520of%2520the%2520label%2520noise%2520is%2520between%250Aclasses%2520that%2520are%2520visually%2520very%2520similar%2520%2528%2560hard%2520noise%2527%2529.%2520This%2520paper%2520addresses%250Athese%2520challenges%2520by%2520proposing%2520a%2520method%2520%2528%255Ctextit%257BCLIPCleaner%257D%2529%2520that%2520leverages%250ACLIP%252C%2520a%2520powerful%2520Vision-Language%2520%2528VL%2529%2520model%2520for%2520constructing%2520a%2520zero-shot%250Aclassifier%2520for%2520efficient%252C%2520offline%252C%2520clean%2520sample%2520selection.%2520This%2520has%2520the%250Aadvantage%2520that%2520the%2520sample%2520selection%2520is%2520decoupled%2520from%2520the%2520in-training%2520model%2520and%250Athat%2520the%2520sample%2520selection%2520is%2520aware%2520of%2520the%2520semantic%2520and%2520visual%2520similarities%250Abetween%2520the%2520classes%2520due%2520to%2520the%2520way%2520that%2520CLIP%2520is%2520trained.%2520We%2520provide%2520theoretical%250Ajustifications%2520and%2520empirical%2520evidence%2520to%2520demonstrate%2520the%2520advantages%2520of%2520CLIP%2520for%250ALNL%2520compared%2520to%2520conventional%2520pre-trained%2520models.%2520Compared%2520to%2520current%2520methods%250Athat%2520combine%2520iterative%2520sample%2520selection%2520with%2520various%2520techniques%252C%250A%255Ctextit%257BCLIPCleaner%257D%2520offers%2520a%2520simple%252C%2520single-step%2520approach%2520that%2520achieves%250Acompetitive%2520or%2520superior%2520performance%2520on%2520benchmark%2520datasets.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520time%2520a%2520VL%2520model%2520has%2520been%2520used%2520for%2520sample%2520selection%250Ato%2520address%2520the%2520problem%2520of%2520Learning%2520with%2520Noisy%2520Labels%2520%2528LNL%2529%252C%2520highlighting%2520their%250Apotential%2520in%2520the%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP&entry.906535625=Chen%20Feng%20and%20Georgios%20Tzimiropoulos%20and%20Ioannis%20Patras&entry.1292438233=%20%20Learning%20with%20Noisy%20labels%20%28LNL%29%20poses%20a%20significant%20challenge%20for%20the%0AMachine%20Learning%20community.%20Some%20of%20the%20most%20widely%20used%20approaches%20that%20select%0Aas%20clean%20samples%20for%20which%20the%20model%20itself%20%28the%20in-training%20model%29%20has%20high%0Aconfidence%2C%20e.g.%2C%20%60small%20loss%27%2C%20can%20suffer%20from%20the%20so%20called%0A%60self-confirmation%27%20bias.%20This%20bias%20arises%20because%20the%20in-training%20model%2C%20is%20at%0Aleast%20partially%20trained%20on%20the%20noisy%20labels.%20Furthermore%2C%20in%20the%20classification%0Acase%2C%20an%20additional%20challenge%20arises%20because%20some%20of%20the%20label%20noise%20is%20between%0Aclasses%20that%20are%20visually%20very%20similar%20%28%60hard%20noise%27%29.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20a%20method%20%28%5Ctextit%7BCLIPCleaner%7D%29%20that%20leverages%0ACLIP%2C%20a%20powerful%20Vision-Language%20%28VL%29%20model%20for%20constructing%20a%20zero-shot%0Aclassifier%20for%20efficient%2C%20offline%2C%20clean%20sample%20selection.%20This%20has%20the%0Aadvantage%20that%20the%20sample%20selection%20is%20decoupled%20from%20the%20in-training%20model%20and%0Athat%20the%20sample%20selection%20is%20aware%20of%20the%20semantic%20and%20visual%20similarities%0Abetween%20the%20classes%20due%20to%20the%20way%20that%20CLIP%20is%20trained.%20We%20provide%20theoretical%0Ajustifications%20and%20empirical%20evidence%20to%20demonstrate%20the%20advantages%20of%20CLIP%20for%0ALNL%20compared%20to%20conventional%20pre-trained%20models.%20Compared%20to%20current%20methods%0Athat%20combine%20iterative%20sample%20selection%20with%20various%20techniques%2C%0A%5Ctextit%7BCLIPCleaner%7D%20offers%20a%20simple%2C%20single-step%20approach%20that%20achieves%0Acompetitive%20or%20superior%20performance%20on%20benchmark%20datasets.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20time%20a%20VL%20model%20has%20been%20used%20for%20sample%20selection%0Ato%20address%20the%20problem%20of%20Learning%20with%20Noisy%20Labels%20%28LNL%29%2C%20highlighting%20their%0Apotential%20in%20the%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10012v1&entry.124074799=Read"},
{"title": "UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track", "author": "Hao Fang and Feiyu Pan and Xiankai Lu and Wei Zhang and Runmin Cong", "abstract": "  Referring video object segmentation (RVOS) relies on natural language\nexpressions to segment target objects in video. In this year, LSVOS Challenge\nRVOS Track replaced the origin YouTube-RVOS benchmark with MeViS. MeViS focuses\non referring the target object in a video through its motion descriptions\ninstead of static attributes, posing a greater challenge to RVOS task. In this\nwork, we integrate strengths of that leading RVOS and VOS models to build up a\nsimple and effective pipeline for RVOS. Firstly, We finetune the\nstate-of-the-art RVOS model to obtain mask sequences that are correlated with\nlanguage descriptions. Secondly, based on a reliable and high-quality key\nframes, we leverage VOS model to enhance the quality and temporal consistency\nof the mask results. Finally, we further improve the performance of the RVOS\nmodel using semi-supervised learning. Our solution achieved 62.57 J&F on the\nMeViS test set and ranked 1st place for 6th LSVOS Challenge RVOS Track.\n", "link": "http://arxiv.org/abs/2408.10129v1", "date": "2024-08-19", "relevancy": 2.0761, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5209}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.52}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNINEXT-Cutie%3A%20The%201st%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track&body=Title%3A%20UNINEXT-Cutie%3A%20The%201st%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track%0AAuthor%3A%20Hao%20Fang%20and%20Feiyu%20Pan%20and%20Xiankai%20Lu%20and%20Wei%20Zhang%20and%20Runmin%20Cong%0AAbstract%3A%20%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20target%20objects%20in%20video.%20In%20this%20year%2C%20LSVOS%20Challenge%0ARVOS%20Track%20replaced%20the%20origin%20YouTube-RVOS%20benchmark%20with%20MeViS.%20MeViS%20focuses%0Aon%20referring%20the%20target%20object%20in%20a%20video%20through%20its%20motion%20descriptions%0Ainstead%20of%20static%20attributes%2C%20posing%20a%20greater%20challenge%20to%20RVOS%20task.%20In%20this%0Awork%2C%20we%20integrate%20strengths%20of%20that%20leading%20RVOS%20and%20VOS%20models%20to%20build%20up%20a%0Asimple%20and%20effective%20pipeline%20for%20RVOS.%20Firstly%2C%20We%20finetune%20the%0Astate-of-the-art%20RVOS%20model%20to%20obtain%20mask%20sequences%20that%20are%20correlated%20with%0Alanguage%20descriptions.%20Secondly%2C%20based%20on%20a%20reliable%20and%20high-quality%20key%0Aframes%2C%20we%20leverage%20VOS%20model%20to%20enhance%20the%20quality%20and%20temporal%20consistency%0Aof%20the%20mask%20results.%20Finally%2C%20we%20further%20improve%20the%20performance%20of%20the%20RVOS%0Amodel%20using%20semi-supervised%20learning.%20Our%20solution%20achieved%2062.57%20J%26F%20on%20the%0AMeViS%20test%20set%20and%20ranked%201st%20place%20for%206th%20LSVOS%20Challenge%20RVOS%20Track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNINEXT-Cutie%253A%2520The%25201st%2520Solution%2520for%2520LSVOS%2520Challenge%2520RVOS%2520Track%26entry.906535625%3DHao%2520Fang%2520and%2520Feiyu%2520Pan%2520and%2520Xiankai%2520Lu%2520and%2520Wei%2520Zhang%2520and%2520Runmin%2520Cong%26entry.1292438233%3D%2520%2520Referring%2520video%2520object%2520segmentation%2520%2528RVOS%2529%2520relies%2520on%2520natural%2520language%250Aexpressions%2520to%2520segment%2520target%2520objects%2520in%2520video.%2520In%2520this%2520year%252C%2520LSVOS%2520Challenge%250ARVOS%2520Track%2520replaced%2520the%2520origin%2520YouTube-RVOS%2520benchmark%2520with%2520MeViS.%2520MeViS%2520focuses%250Aon%2520referring%2520the%2520target%2520object%2520in%2520a%2520video%2520through%2520its%2520motion%2520descriptions%250Ainstead%2520of%2520static%2520attributes%252C%2520posing%2520a%2520greater%2520challenge%2520to%2520RVOS%2520task.%2520In%2520this%250Awork%252C%2520we%2520integrate%2520strengths%2520of%2520that%2520leading%2520RVOS%2520and%2520VOS%2520models%2520to%2520build%2520up%2520a%250Asimple%2520and%2520effective%2520pipeline%2520for%2520RVOS.%2520Firstly%252C%2520We%2520finetune%2520the%250Astate-of-the-art%2520RVOS%2520model%2520to%2520obtain%2520mask%2520sequences%2520that%2520are%2520correlated%2520with%250Alanguage%2520descriptions.%2520Secondly%252C%2520based%2520on%2520a%2520reliable%2520and%2520high-quality%2520key%250Aframes%252C%2520we%2520leverage%2520VOS%2520model%2520to%2520enhance%2520the%2520quality%2520and%2520temporal%2520consistency%250Aof%2520the%2520mask%2520results.%2520Finally%252C%2520we%2520further%2520improve%2520the%2520performance%2520of%2520the%2520RVOS%250Amodel%2520using%2520semi-supervised%2520learning.%2520Our%2520solution%2520achieved%252062.57%2520J%2526F%2520on%2520the%250AMeViS%2520test%2520set%2520and%2520ranked%25201st%2520place%2520for%25206th%2520LSVOS%2520Challenge%2520RVOS%2520Track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNINEXT-Cutie%3A%20The%201st%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track&entry.906535625=Hao%20Fang%20and%20Feiyu%20Pan%20and%20Xiankai%20Lu%20and%20Wei%20Zhang%20and%20Runmin%20Cong&entry.1292438233=%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20target%20objects%20in%20video.%20In%20this%20year%2C%20LSVOS%20Challenge%0ARVOS%20Track%20replaced%20the%20origin%20YouTube-RVOS%20benchmark%20with%20MeViS.%20MeViS%20focuses%0Aon%20referring%20the%20target%20object%20in%20a%20video%20through%20its%20motion%20descriptions%0Ainstead%20of%20static%20attributes%2C%20posing%20a%20greater%20challenge%20to%20RVOS%20task.%20In%20this%0Awork%2C%20we%20integrate%20strengths%20of%20that%20leading%20RVOS%20and%20VOS%20models%20to%20build%20up%20a%0Asimple%20and%20effective%20pipeline%20for%20RVOS.%20Firstly%2C%20We%20finetune%20the%0Astate-of-the-art%20RVOS%20model%20to%20obtain%20mask%20sequences%20that%20are%20correlated%20with%0Alanguage%20descriptions.%20Secondly%2C%20based%20on%20a%20reliable%20and%20high-quality%20key%0Aframes%2C%20we%20leverage%20VOS%20model%20to%20enhance%20the%20quality%20and%20temporal%20consistency%0Aof%20the%20mask%20results.%20Finally%2C%20we%20further%20improve%20the%20performance%20of%20the%20RVOS%0Amodel%20using%20semi-supervised%20learning.%20Our%20solution%20achieved%2062.57%20J%26F%20on%20the%0AMeViS%20test%20set%20and%20ranked%201st%20place%20for%206th%20LSVOS%20Challenge%20RVOS%20Track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10129v1&entry.124074799=Read"},
{"title": "An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side\n  Pre-trained Generator to Clients in Heterogeneous Federated Learning", "author": "Jianqing Zhang and Yang Liu and Yang Hua and Jian Cao", "abstract": "  Heterogeneous Federated Learning (HtFL) enables task-specific knowledge\nsharing among clients with different model architectures while preserving\nprivacy. Despite recent research progress, transferring knowledge in HtFL is\nstill difficult due to data and model heterogeneity. To tackle this, we\nintroduce a public pre-trained generator (e.g., StyleGAN or Stable Diffusion)\nas the bridge and propose a new upload-efficient knowledge transfer scheme\ncalled Federated Knowledge-Transfer-Loop (FedKTL). It can produce task-related\nprototypical image-vector pairs via the generator's inference on the server.\nWith these pairs, each client can transfer common knowledge from the generator\nto its local model through an additional supervised local task. We conduct\nextensive experiments on four datasets under two types of data heterogeneity\nwith 14 heterogeneous models, including CNNs and ViTs. Results show that our\nFedKTL surpasses seven state-of-the-art methods by up to 7.31%. Moreover, our\nknowledge transfer scheme is applicable in cloud-edge scenarios with only one\nedge client. Code: https://github.com/TsingZ0/FedKTL\n", "link": "http://arxiv.org/abs/2403.15760v2", "date": "2024-08-19", "relevancy": 2.0736, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5184}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Upload-Efficient%20Scheme%20for%20Transferring%20Knowledge%20From%20a%20Server-Side%0A%20%20Pre-trained%20Generator%20to%20Clients%20in%20Heterogeneous%20Federated%20Learning&body=Title%3A%20An%20Upload-Efficient%20Scheme%20for%20Transferring%20Knowledge%20From%20a%20Server-Side%0A%20%20Pre-trained%20Generator%20to%20Clients%20in%20Heterogeneous%20Federated%20Learning%0AAuthor%3A%20Jianqing%20Zhang%20and%20Yang%20Liu%20and%20Yang%20Hua%20and%20Jian%20Cao%0AAbstract%3A%20%20%20Heterogeneous%20Federated%20Learning%20%28HtFL%29%20enables%20task-specific%20knowledge%0Asharing%20among%20clients%20with%20different%20model%20architectures%20while%20preserving%0Aprivacy.%20Despite%20recent%20research%20progress%2C%20transferring%20knowledge%20in%20HtFL%20is%0Astill%20difficult%20due%20to%20data%20and%20model%20heterogeneity.%20To%20tackle%20this%2C%20we%0Aintroduce%20a%20public%20pre-trained%20generator%20%28e.g.%2C%20StyleGAN%20or%20Stable%20Diffusion%29%0Aas%20the%20bridge%20and%20propose%20a%20new%20upload-efficient%20knowledge%20transfer%20scheme%0Acalled%20Federated%20Knowledge-Transfer-Loop%20%28FedKTL%29.%20It%20can%20produce%20task-related%0Aprototypical%20image-vector%20pairs%20via%20the%20generator%27s%20inference%20on%20the%20server.%0AWith%20these%20pairs%2C%20each%20client%20can%20transfer%20common%20knowledge%20from%20the%20generator%0Ato%20its%20local%20model%20through%20an%20additional%20supervised%20local%20task.%20We%20conduct%0Aextensive%20experiments%20on%20four%20datasets%20under%20two%20types%20of%20data%20heterogeneity%0Awith%2014%20heterogeneous%20models%2C%20including%20CNNs%20and%20ViTs.%20Results%20show%20that%20our%0AFedKTL%20surpasses%20seven%20state-of-the-art%20methods%20by%20up%20to%207.31%25.%20Moreover%2C%20our%0Aknowledge%20transfer%20scheme%20is%20applicable%20in%20cloud-edge%20scenarios%20with%20only%20one%0Aedge%20client.%20Code%3A%20https%3A//github.com/TsingZ0/FedKTL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Upload-Efficient%2520Scheme%2520for%2520Transferring%2520Knowledge%2520From%2520a%2520Server-Side%250A%2520%2520Pre-trained%2520Generator%2520to%2520Clients%2520in%2520Heterogeneous%2520Federated%2520Learning%26entry.906535625%3DJianqing%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Hua%2520and%2520Jian%2520Cao%26entry.1292438233%3D%2520%2520Heterogeneous%2520Federated%2520Learning%2520%2528HtFL%2529%2520enables%2520task-specific%2520knowledge%250Asharing%2520among%2520clients%2520with%2520different%2520model%2520architectures%2520while%2520preserving%250Aprivacy.%2520Despite%2520recent%2520research%2520progress%252C%2520transferring%2520knowledge%2520in%2520HtFL%2520is%250Astill%2520difficult%2520due%2520to%2520data%2520and%2520model%2520heterogeneity.%2520To%2520tackle%2520this%252C%2520we%250Aintroduce%2520a%2520public%2520pre-trained%2520generator%2520%2528e.g.%252C%2520StyleGAN%2520or%2520Stable%2520Diffusion%2529%250Aas%2520the%2520bridge%2520and%2520propose%2520a%2520new%2520upload-efficient%2520knowledge%2520transfer%2520scheme%250Acalled%2520Federated%2520Knowledge-Transfer-Loop%2520%2528FedKTL%2529.%2520It%2520can%2520produce%2520task-related%250Aprototypical%2520image-vector%2520pairs%2520via%2520the%2520generator%2527s%2520inference%2520on%2520the%2520server.%250AWith%2520these%2520pairs%252C%2520each%2520client%2520can%2520transfer%2520common%2520knowledge%2520from%2520the%2520generator%250Ato%2520its%2520local%2520model%2520through%2520an%2520additional%2520supervised%2520local%2520task.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520four%2520datasets%2520under%2520two%2520types%2520of%2520data%2520heterogeneity%250Awith%252014%2520heterogeneous%2520models%252C%2520including%2520CNNs%2520and%2520ViTs.%2520Results%2520show%2520that%2520our%250AFedKTL%2520surpasses%2520seven%2520state-of-the-art%2520methods%2520by%2520up%2520to%25207.31%2525.%2520Moreover%252C%2520our%250Aknowledge%2520transfer%2520scheme%2520is%2520applicable%2520in%2520cloud-edge%2520scenarios%2520with%2520only%2520one%250Aedge%2520client.%2520Code%253A%2520https%253A//github.com/TsingZ0/FedKTL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Upload-Efficient%20Scheme%20for%20Transferring%20Knowledge%20From%20a%20Server-Side%0A%20%20Pre-trained%20Generator%20to%20Clients%20in%20Heterogeneous%20Federated%20Learning&entry.906535625=Jianqing%20Zhang%20and%20Yang%20Liu%20and%20Yang%20Hua%20and%20Jian%20Cao&entry.1292438233=%20%20Heterogeneous%20Federated%20Learning%20%28HtFL%29%20enables%20task-specific%20knowledge%0Asharing%20among%20clients%20with%20different%20model%20architectures%20while%20preserving%0Aprivacy.%20Despite%20recent%20research%20progress%2C%20transferring%20knowledge%20in%20HtFL%20is%0Astill%20difficult%20due%20to%20data%20and%20model%20heterogeneity.%20To%20tackle%20this%2C%20we%0Aintroduce%20a%20public%20pre-trained%20generator%20%28e.g.%2C%20StyleGAN%20or%20Stable%20Diffusion%29%0Aas%20the%20bridge%20and%20propose%20a%20new%20upload-efficient%20knowledge%20transfer%20scheme%0Acalled%20Federated%20Knowledge-Transfer-Loop%20%28FedKTL%29.%20It%20can%20produce%20task-related%0Aprototypical%20image-vector%20pairs%20via%20the%20generator%27s%20inference%20on%20the%20server.%0AWith%20these%20pairs%2C%20each%20client%20can%20transfer%20common%20knowledge%20from%20the%20generator%0Ato%20its%20local%20model%20through%20an%20additional%20supervised%20local%20task.%20We%20conduct%0Aextensive%20experiments%20on%20four%20datasets%20under%20two%20types%20of%20data%20heterogeneity%0Awith%2014%20heterogeneous%20models%2C%20including%20CNNs%20and%20ViTs.%20Results%20show%20that%20our%0AFedKTL%20surpasses%20seven%20state-of-the-art%20methods%20by%20up%20to%207.31%25.%20Moreover%2C%20our%0Aknowledge%20transfer%20scheme%20is%20applicable%20in%20cloud-edge%20scenarios%20with%20only%20one%0Aedge%20client.%20Code%3A%20https%3A//github.com/TsingZ0/FedKTL%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15760v2&entry.124074799=Read"},
{"title": "Envisioning Possibilities and Challenges of AI for Personalized Cancer\n  Care", "author": "Elaine Kong and  Kuo-Ting and  Huang and Aakash Gautam", "abstract": "  The use of Artificial Intelligence (AI) in healthcare, including in caring\nfor cancer survivors, has gained significant interest. However, gaps remain in\nour understanding of how such AI systems can provide care, especially for\nethnic and racial minority groups who continue to face care disparities.\nThrough interviews with six cancer survivors, we identify critical gaps in\ncurrent healthcare systems such as a lack of personalized care and insufficient\ncultural and linguistic accommodation. AI, when applied to care, was seen as a\nway to address these issues by enabling real-time, culturally aligned, and\nlinguistically appropriate interactions. We also uncovered concerns about the\nimplications of AI-driven personalization, such as data privacy, loss of human\ntouch in caregiving, and the risk of echo chambers that limit exposure to\ndiverse information. We conclude by discussing the trade-offs between\nAI-enhanced personalization and the need for structural changes in healthcare\nthat go beyond technological solutions, leading us to argue that we should\nbegin by asking, ``Why personalization?''\n", "link": "http://arxiv.org/abs/2408.10108v1", "date": "2024-08-19", "relevancy": 2.0708, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4509}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.396}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Envisioning%20Possibilities%20and%20Challenges%20of%20AI%20for%20Personalized%20Cancer%0A%20%20Care&body=Title%3A%20Envisioning%20Possibilities%20and%20Challenges%20of%20AI%20for%20Personalized%20Cancer%0A%20%20Care%0AAuthor%3A%20Elaine%20Kong%20and%20%20Kuo-Ting%20and%20%20Huang%20and%20Aakash%20Gautam%0AAbstract%3A%20%20%20The%20use%20of%20Artificial%20Intelligence%20%28AI%29%20in%20healthcare%2C%20including%20in%20caring%0Afor%20cancer%20survivors%2C%20has%20gained%20significant%20interest.%20However%2C%20gaps%20remain%20in%0Aour%20understanding%20of%20how%20such%20AI%20systems%20can%20provide%20care%2C%20especially%20for%0Aethnic%20and%20racial%20minority%20groups%20who%20continue%20to%20face%20care%20disparities.%0AThrough%20interviews%20with%20six%20cancer%20survivors%2C%20we%20identify%20critical%20gaps%20in%0Acurrent%20healthcare%20systems%20such%20as%20a%20lack%20of%20personalized%20care%20and%20insufficient%0Acultural%20and%20linguistic%20accommodation.%20AI%2C%20when%20applied%20to%20care%2C%20was%20seen%20as%20a%0Away%20to%20address%20these%20issues%20by%20enabling%20real-time%2C%20culturally%20aligned%2C%20and%0Alinguistically%20appropriate%20interactions.%20We%20also%20uncovered%20concerns%20about%20the%0Aimplications%20of%20AI-driven%20personalization%2C%20such%20as%20data%20privacy%2C%20loss%20of%20human%0Atouch%20in%20caregiving%2C%20and%20the%20risk%20of%20echo%20chambers%20that%20limit%20exposure%20to%0Adiverse%20information.%20We%20conclude%20by%20discussing%20the%20trade-offs%20between%0AAI-enhanced%20personalization%20and%20the%20need%20for%20structural%20changes%20in%20healthcare%0Athat%20go%20beyond%20technological%20solutions%2C%20leading%20us%20to%20argue%20that%20we%20should%0Abegin%20by%20asking%2C%20%60%60Why%20personalization%3F%27%27%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvisioning%2520Possibilities%2520and%2520Challenges%2520of%2520AI%2520for%2520Personalized%2520Cancer%250A%2520%2520Care%26entry.906535625%3DElaine%2520Kong%2520and%2520%2520Kuo-Ting%2520and%2520%2520Huang%2520and%2520Aakash%2520Gautam%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520in%2520healthcare%252C%2520including%2520in%2520caring%250Afor%2520cancer%2520survivors%252C%2520has%2520gained%2520significant%2520interest.%2520However%252C%2520gaps%2520remain%2520in%250Aour%2520understanding%2520of%2520how%2520such%2520AI%2520systems%2520can%2520provide%2520care%252C%2520especially%2520for%250Aethnic%2520and%2520racial%2520minority%2520groups%2520who%2520continue%2520to%2520face%2520care%2520disparities.%250AThrough%2520interviews%2520with%2520six%2520cancer%2520survivors%252C%2520we%2520identify%2520critical%2520gaps%2520in%250Acurrent%2520healthcare%2520systems%2520such%2520as%2520a%2520lack%2520of%2520personalized%2520care%2520and%2520insufficient%250Acultural%2520and%2520linguistic%2520accommodation.%2520AI%252C%2520when%2520applied%2520to%2520care%252C%2520was%2520seen%2520as%2520a%250Away%2520to%2520address%2520these%2520issues%2520by%2520enabling%2520real-time%252C%2520culturally%2520aligned%252C%2520and%250Alinguistically%2520appropriate%2520interactions.%2520We%2520also%2520uncovered%2520concerns%2520about%2520the%250Aimplications%2520of%2520AI-driven%2520personalization%252C%2520such%2520as%2520data%2520privacy%252C%2520loss%2520of%2520human%250Atouch%2520in%2520caregiving%252C%2520and%2520the%2520risk%2520of%2520echo%2520chambers%2520that%2520limit%2520exposure%2520to%250Adiverse%2520information.%2520We%2520conclude%2520by%2520discussing%2520the%2520trade-offs%2520between%250AAI-enhanced%2520personalization%2520and%2520the%2520need%2520for%2520structural%2520changes%2520in%2520healthcare%250Athat%2520go%2520beyond%2520technological%2520solutions%252C%2520leading%2520us%2520to%2520argue%2520that%2520we%2520should%250Abegin%2520by%2520asking%252C%2520%2560%2560Why%2520personalization%253F%2527%2527%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Envisioning%20Possibilities%20and%20Challenges%20of%20AI%20for%20Personalized%20Cancer%0A%20%20Care&entry.906535625=Elaine%20Kong%20and%20%20Kuo-Ting%20and%20%20Huang%20and%20Aakash%20Gautam&entry.1292438233=%20%20The%20use%20of%20Artificial%20Intelligence%20%28AI%29%20in%20healthcare%2C%20including%20in%20caring%0Afor%20cancer%20survivors%2C%20has%20gained%20significant%20interest.%20However%2C%20gaps%20remain%20in%0Aour%20understanding%20of%20how%20such%20AI%20systems%20can%20provide%20care%2C%20especially%20for%0Aethnic%20and%20racial%20minority%20groups%20who%20continue%20to%20face%20care%20disparities.%0AThrough%20interviews%20with%20six%20cancer%20survivors%2C%20we%20identify%20critical%20gaps%20in%0Acurrent%20healthcare%20systems%20such%20as%20a%20lack%20of%20personalized%20care%20and%20insufficient%0Acultural%20and%20linguistic%20accommodation.%20AI%2C%20when%20applied%20to%20care%2C%20was%20seen%20as%20a%0Away%20to%20address%20these%20issues%20by%20enabling%20real-time%2C%20culturally%20aligned%2C%20and%0Alinguistically%20appropriate%20interactions.%20We%20also%20uncovered%20concerns%20about%20the%0Aimplications%20of%20AI-driven%20personalization%2C%20such%20as%20data%20privacy%2C%20loss%20of%20human%0Atouch%20in%20caregiving%2C%20and%20the%20risk%20of%20echo%20chambers%20that%20limit%20exposure%20to%0Adiverse%20information.%20We%20conclude%20by%20discussing%20the%20trade-offs%20between%0AAI-enhanced%20personalization%20and%20the%20need%20for%20structural%20changes%20in%20healthcare%0Athat%20go%20beyond%20technological%20solutions%2C%20leading%20us%20to%20argue%20that%20we%20should%0Abegin%20by%20asking%2C%20%60%60Why%20personalization%3F%27%27%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10108v1&entry.124074799=Read"},
{"title": "Personalizing Reinforcement Learning from Human Feedback with\n  Variational Preference Learning", "author": "Sriyash Poddar and Yanming Wan and Hamish Ivison and Abhishek Gupta and Natasha Jaques", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment.\n", "link": "http://arxiv.org/abs/2408.10075v1", "date": "2024-08-19", "relevancy": 2.0663, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalizing%20Reinforcement%20Learning%20from%20Human%20Feedback%20with%0A%20%20Variational%20Preference%20Learning&body=Title%3A%20Personalizing%20Reinforcement%20Learning%20from%20Human%20Feedback%20with%0A%20%20Variational%20Preference%20Learning%0AAuthor%3A%20Sriyash%20Poddar%20and%20Yanming%20Wan%20and%20Hamish%20Ivison%20and%20Abhishek%20Gupta%20and%20Natasha%20Jaques%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20a%20powerful%20paradigm%20for%0Aaligning%20foundation%20models%20to%20human%20values%20and%20preferences.%20However%2C%20current%0ARLHF%20techniques%20cannot%20account%20for%20the%20naturally%20occurring%20differences%20in%0Aindividual%20human%20preferences%20across%20a%20diverse%20population.%20When%20these%0Adifferences%20arise%2C%20traditional%20RLHF%20frameworks%20simply%20average%20over%20them%2C%0Aleading%20to%20inaccurate%20rewards%20and%20poor%20performance%20for%20individual%20subgroups.%20To%0Aaddress%20the%20need%20for%20pluralistic%20alignment%2C%20we%20develop%20a%20class%20of%20multimodal%0ARLHF%20methods.%20Our%20proposed%20techniques%20are%20based%20on%20a%20latent%20variable%0Aformulation%20-%20inferring%20a%20novel%20user-specific%20latent%20and%20learning%20reward%20models%0Aand%20policies%20conditioned%20on%20this%20latent%20without%20additional%20user-specific%20data.%0AWhile%20conceptually%20simple%2C%20we%20show%20that%20in%20practice%2C%20this%20reward%20modeling%0Arequires%20careful%20algorithmic%20considerations%20around%20model%20architecture%20and%0Areward%20scaling.%20To%20empirically%20validate%20our%20proposed%20technique%2C%20we%20first%20show%0Athat%20it%20can%20provide%20a%20way%20to%20combat%20underspecification%20in%20simulated%20control%0Aproblems%2C%20inferring%20and%20optimizing%20user-specific%20reward%20functions.%20Next%2C%20we%0Aconduct%20experiments%20on%20pluralistic%20language%20datasets%20representing%20diverse%20user%0Apreferences%20and%20demonstrate%20improved%20reward%20function%20accuracy.%20We%20additionally%0Ashow%20the%20benefits%20of%20this%20probabilistic%20framework%20in%20terms%20of%20measuring%0Auncertainty%2C%20and%20actively%20learning%20user%20preferences.%20This%20work%20enables%20learning%0Afrom%20diverse%20populations%20of%20users%20with%20divergent%20preferences%2C%20an%20important%0Achallenge%20that%20naturally%20occurs%20in%20problems%20from%20robot%20learning%20to%20foundation%0Amodel%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalizing%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520with%250A%2520%2520Variational%2520Preference%2520Learning%26entry.906535625%3DSriyash%2520Poddar%2520and%2520Yanming%2520Wan%2520and%2520Hamish%2520Ivison%2520and%2520Abhishek%2520Gupta%2520and%2520Natasha%2520Jaques%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520a%2520powerful%2520paradigm%2520for%250Aaligning%2520foundation%2520models%2520to%2520human%2520values%2520and%2520preferences.%2520However%252C%2520current%250ARLHF%2520techniques%2520cannot%2520account%2520for%2520the%2520naturally%2520occurring%2520differences%2520in%250Aindividual%2520human%2520preferences%2520across%2520a%2520diverse%2520population.%2520When%2520these%250Adifferences%2520arise%252C%2520traditional%2520RLHF%2520frameworks%2520simply%2520average%2520over%2520them%252C%250Aleading%2520to%2520inaccurate%2520rewards%2520and%2520poor%2520performance%2520for%2520individual%2520subgroups.%2520To%250Aaddress%2520the%2520need%2520for%2520pluralistic%2520alignment%252C%2520we%2520develop%2520a%2520class%2520of%2520multimodal%250ARLHF%2520methods.%2520Our%2520proposed%2520techniques%2520are%2520based%2520on%2520a%2520latent%2520variable%250Aformulation%2520-%2520inferring%2520a%2520novel%2520user-specific%2520latent%2520and%2520learning%2520reward%2520models%250Aand%2520policies%2520conditioned%2520on%2520this%2520latent%2520without%2520additional%2520user-specific%2520data.%250AWhile%2520conceptually%2520simple%252C%2520we%2520show%2520that%2520in%2520practice%252C%2520this%2520reward%2520modeling%250Arequires%2520careful%2520algorithmic%2520considerations%2520around%2520model%2520architecture%2520and%250Areward%2520scaling.%2520To%2520empirically%2520validate%2520our%2520proposed%2520technique%252C%2520we%2520first%2520show%250Athat%2520it%2520can%2520provide%2520a%2520way%2520to%2520combat%2520underspecification%2520in%2520simulated%2520control%250Aproblems%252C%2520inferring%2520and%2520optimizing%2520user-specific%2520reward%2520functions.%2520Next%252C%2520we%250Aconduct%2520experiments%2520on%2520pluralistic%2520language%2520datasets%2520representing%2520diverse%2520user%250Apreferences%2520and%2520demonstrate%2520improved%2520reward%2520function%2520accuracy.%2520We%2520additionally%250Ashow%2520the%2520benefits%2520of%2520this%2520probabilistic%2520framework%2520in%2520terms%2520of%2520measuring%250Auncertainty%252C%2520and%2520actively%2520learning%2520user%2520preferences.%2520This%2520work%2520enables%2520learning%250Afrom%2520diverse%2520populations%2520of%2520users%2520with%2520divergent%2520preferences%252C%2520an%2520important%250Achallenge%2520that%2520naturally%2520occurs%2520in%2520problems%2520from%2520robot%2520learning%2520to%2520foundation%250Amodel%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalizing%20Reinforcement%20Learning%20from%20Human%20Feedback%20with%0A%20%20Variational%20Preference%20Learning&entry.906535625=Sriyash%20Poddar%20and%20Yanming%20Wan%20and%20Hamish%20Ivison%20and%20Abhishek%20Gupta%20and%20Natasha%20Jaques&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20a%20powerful%20paradigm%20for%0Aaligning%20foundation%20models%20to%20human%20values%20and%20preferences.%20However%2C%20current%0ARLHF%20techniques%20cannot%20account%20for%20the%20naturally%20occurring%20differences%20in%0Aindividual%20human%20preferences%20across%20a%20diverse%20population.%20When%20these%0Adifferences%20arise%2C%20traditional%20RLHF%20frameworks%20simply%20average%20over%20them%2C%0Aleading%20to%20inaccurate%20rewards%20and%20poor%20performance%20for%20individual%20subgroups.%20To%0Aaddress%20the%20need%20for%20pluralistic%20alignment%2C%20we%20develop%20a%20class%20of%20multimodal%0ARLHF%20methods.%20Our%20proposed%20techniques%20are%20based%20on%20a%20latent%20variable%0Aformulation%20-%20inferring%20a%20novel%20user-specific%20latent%20and%20learning%20reward%20models%0Aand%20policies%20conditioned%20on%20this%20latent%20without%20additional%20user-specific%20data.%0AWhile%20conceptually%20simple%2C%20we%20show%20that%20in%20practice%2C%20this%20reward%20modeling%0Arequires%20careful%20algorithmic%20considerations%20around%20model%20architecture%20and%0Areward%20scaling.%20To%20empirically%20validate%20our%20proposed%20technique%2C%20we%20first%20show%0Athat%20it%20can%20provide%20a%20way%20to%20combat%20underspecification%20in%20simulated%20control%0Aproblems%2C%20inferring%20and%20optimizing%20user-specific%20reward%20functions.%20Next%2C%20we%0Aconduct%20experiments%20on%20pluralistic%20language%20datasets%20representing%20diverse%20user%0Apreferences%20and%20demonstrate%20improved%20reward%20function%20accuracy.%20We%20additionally%0Ashow%20the%20benefits%20of%20this%20probabilistic%20framework%20in%20terms%20of%20measuring%0Auncertainty%2C%20and%20actively%20learning%20user%20preferences.%20This%20work%20enables%20learning%0Afrom%20diverse%20populations%20of%20users%20with%20divergent%20preferences%2C%20an%20important%0Achallenge%20that%20naturally%20occurs%20in%20problems%20from%20robot%20learning%20to%20foundation%0Amodel%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10075v1&entry.124074799=Read"},
{"title": "C${^2}$RL: Content and Context Representation Learning for Gloss-free\n  Sign Language Translation and Retrieval", "author": "Zhigang Chen and Benjia Zhou and Yiqing Huang and Jun Wan and Yibo Hu and Hailin Shi and Yanyan Liang and Zhen Lei and Du Zhang", "abstract": "  Sign Language Representation Learning (SLRL) is crucial for a range of sign\nlanguage-related downstream tasks such as Sign Language Translation (SLT) and\nSign Language Retrieval (SLRet). Recently, many gloss-based and gloss-free SLRL\nmethods have been proposed, showing promising performance. Among them, the\ngloss-free approach shows promise for strong scalability without relying on\ngloss annotations. However, it currently faces suboptimal solutions due to\nchallenges in encoding the intricate, context-sensitive characteristics of sign\nlanguage videos, mainly struggling to discern essential sign features using a\nnon-monotonic video-text alignment strategy. Therefore, we introduce an\ninnovative pretraining paradigm for gloss-free SLRL, called C${^2}$RL, in this\npaper. Specifically, rather than merely incorporating a non-monotonic semantic\nalignment of video and text to learn language-oriented sign features, we\nemphasize two pivotal aspects of SLRL: Implicit Content Learning (ICL) and\nExplicit Context Learning (ECL). ICL delves into the content of communication,\ncapturing the nuances, emphasis, timing, and rhythm of the signs. In contrast,\nECL focuses on understanding the contextual meaning of signs and converting\nthem into equivalent sentences. Despite its simplicity, extensive experiments\nconfirm that the joint optimization of ICL and ECL results in robust sign\nlanguage representation and significant performance gains in gloss-free SLT and\nSLRet tasks. Notably, C${^2}$RL improves the BLEU-4 score by +5.3 on P14T,\n+10.6 on CSL-daily, +6.2 on OpenASL, and +1.3 on How2Sign. It also boosts the\nR@1 score by +8.3 on P14T, +14.4 on CSL-daily, and +5.9 on How2Sign.\nAdditionally, we set a new baseline for the OpenASL dataset in the SLRet task.\n", "link": "http://arxiv.org/abs/2408.09949v1", "date": "2024-08-19", "relevancy": 2.0656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4977}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C%24%7B%5E2%7D%24RL%3A%20Content%20and%20Context%20Representation%20Learning%20for%20Gloss-free%0A%20%20Sign%20Language%20Translation%20and%20Retrieval&body=Title%3A%20C%24%7B%5E2%7D%24RL%3A%20Content%20and%20Context%20Representation%20Learning%20for%20Gloss-free%0A%20%20Sign%20Language%20Translation%20and%20Retrieval%0AAuthor%3A%20Zhigang%20Chen%20and%20Benjia%20Zhou%20and%20Yiqing%20Huang%20and%20Jun%20Wan%20and%20Yibo%20Hu%20and%20Hailin%20Shi%20and%20Yanyan%20Liang%20and%20Zhen%20Lei%20and%20Du%20Zhang%0AAbstract%3A%20%20%20Sign%20Language%20Representation%20Learning%20%28SLRL%29%20is%20crucial%20for%20a%20range%20of%20sign%0Alanguage-related%20downstream%20tasks%20such%20as%20Sign%20Language%20Translation%20%28SLT%29%20and%0ASign%20Language%20Retrieval%20%28SLRet%29.%20Recently%2C%20many%20gloss-based%20and%20gloss-free%20SLRL%0Amethods%20have%20been%20proposed%2C%20showing%20promising%20performance.%20Among%20them%2C%20the%0Agloss-free%20approach%20shows%20promise%20for%20strong%20scalability%20without%20relying%20on%0Agloss%20annotations.%20However%2C%20it%20currently%20faces%20suboptimal%20solutions%20due%20to%0Achallenges%20in%20encoding%20the%20intricate%2C%20context-sensitive%20characteristics%20of%20sign%0Alanguage%20videos%2C%20mainly%20struggling%20to%20discern%20essential%20sign%20features%20using%20a%0Anon-monotonic%20video-text%20alignment%20strategy.%20Therefore%2C%20we%20introduce%20an%0Ainnovative%20pretraining%20paradigm%20for%20gloss-free%20SLRL%2C%20called%20C%24%7B%5E2%7D%24RL%2C%20in%20this%0Apaper.%20Specifically%2C%20rather%20than%20merely%20incorporating%20a%20non-monotonic%20semantic%0Aalignment%20of%20video%20and%20text%20to%20learn%20language-oriented%20sign%20features%2C%20we%0Aemphasize%20two%20pivotal%20aspects%20of%20SLRL%3A%20Implicit%20Content%20Learning%20%28ICL%29%20and%0AExplicit%20Context%20Learning%20%28ECL%29.%20ICL%20delves%20into%20the%20content%20of%20communication%2C%0Acapturing%20the%20nuances%2C%20emphasis%2C%20timing%2C%20and%20rhythm%20of%20the%20signs.%20In%20contrast%2C%0AECL%20focuses%20on%20understanding%20the%20contextual%20meaning%20of%20signs%20and%20converting%0Athem%20into%20equivalent%20sentences.%20Despite%20its%20simplicity%2C%20extensive%20experiments%0Aconfirm%20that%20the%20joint%20optimization%20of%20ICL%20and%20ECL%20results%20in%20robust%20sign%0Alanguage%20representation%20and%20significant%20performance%20gains%20in%20gloss-free%20SLT%20and%0ASLRet%20tasks.%20Notably%2C%20C%24%7B%5E2%7D%24RL%20improves%20the%20BLEU-4%20score%20by%20%2B5.3%20on%20P14T%2C%0A%2B10.6%20on%20CSL-daily%2C%20%2B6.2%20on%20OpenASL%2C%20and%20%2B1.3%20on%20How2Sign.%20It%20also%20boosts%20the%0AR%401%20score%20by%20%2B8.3%20on%20P14T%2C%20%2B14.4%20on%20CSL-daily%2C%20and%20%2B5.9%20on%20How2Sign.%0AAdditionally%2C%20we%20set%20a%20new%20baseline%20for%20the%20OpenASL%20dataset%20in%20the%20SLRet%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC%2524%257B%255E2%257D%2524RL%253A%2520Content%2520and%2520Context%2520Representation%2520Learning%2520for%2520Gloss-free%250A%2520%2520Sign%2520Language%2520Translation%2520and%2520Retrieval%26entry.906535625%3DZhigang%2520Chen%2520and%2520Benjia%2520Zhou%2520and%2520Yiqing%2520Huang%2520and%2520Jun%2520Wan%2520and%2520Yibo%2520Hu%2520and%2520Hailin%2520Shi%2520and%2520Yanyan%2520Liang%2520and%2520Zhen%2520Lei%2520and%2520Du%2520Zhang%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Representation%2520Learning%2520%2528SLRL%2529%2520is%2520crucial%2520for%2520a%2520range%2520of%2520sign%250Alanguage-related%2520downstream%2520tasks%2520such%2520as%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520and%250ASign%2520Language%2520Retrieval%2520%2528SLRet%2529.%2520Recently%252C%2520many%2520gloss-based%2520and%2520gloss-free%2520SLRL%250Amethods%2520have%2520been%2520proposed%252C%2520showing%2520promising%2520performance.%2520Among%2520them%252C%2520the%250Agloss-free%2520approach%2520shows%2520promise%2520for%2520strong%2520scalability%2520without%2520relying%2520on%250Agloss%2520annotations.%2520However%252C%2520it%2520currently%2520faces%2520suboptimal%2520solutions%2520due%2520to%250Achallenges%2520in%2520encoding%2520the%2520intricate%252C%2520context-sensitive%2520characteristics%2520of%2520sign%250Alanguage%2520videos%252C%2520mainly%2520struggling%2520to%2520discern%2520essential%2520sign%2520features%2520using%2520a%250Anon-monotonic%2520video-text%2520alignment%2520strategy.%2520Therefore%252C%2520we%2520introduce%2520an%250Ainnovative%2520pretraining%2520paradigm%2520for%2520gloss-free%2520SLRL%252C%2520called%2520C%2524%257B%255E2%257D%2524RL%252C%2520in%2520this%250Apaper.%2520Specifically%252C%2520rather%2520than%2520merely%2520incorporating%2520a%2520non-monotonic%2520semantic%250Aalignment%2520of%2520video%2520and%2520text%2520to%2520learn%2520language-oriented%2520sign%2520features%252C%2520we%250Aemphasize%2520two%2520pivotal%2520aspects%2520of%2520SLRL%253A%2520Implicit%2520Content%2520Learning%2520%2528ICL%2529%2520and%250AExplicit%2520Context%2520Learning%2520%2528ECL%2529.%2520ICL%2520delves%2520into%2520the%2520content%2520of%2520communication%252C%250Acapturing%2520the%2520nuances%252C%2520emphasis%252C%2520timing%252C%2520and%2520rhythm%2520of%2520the%2520signs.%2520In%2520contrast%252C%250AECL%2520focuses%2520on%2520understanding%2520the%2520contextual%2520meaning%2520of%2520signs%2520and%2520converting%250Athem%2520into%2520equivalent%2520sentences.%2520Despite%2520its%2520simplicity%252C%2520extensive%2520experiments%250Aconfirm%2520that%2520the%2520joint%2520optimization%2520of%2520ICL%2520and%2520ECL%2520results%2520in%2520robust%2520sign%250Alanguage%2520representation%2520and%2520significant%2520performance%2520gains%2520in%2520gloss-free%2520SLT%2520and%250ASLRet%2520tasks.%2520Notably%252C%2520C%2524%257B%255E2%257D%2524RL%2520improves%2520the%2520BLEU-4%2520score%2520by%2520%252B5.3%2520on%2520P14T%252C%250A%252B10.6%2520on%2520CSL-daily%252C%2520%252B6.2%2520on%2520OpenASL%252C%2520and%2520%252B1.3%2520on%2520How2Sign.%2520It%2520also%2520boosts%2520the%250AR%25401%2520score%2520by%2520%252B8.3%2520on%2520P14T%252C%2520%252B14.4%2520on%2520CSL-daily%252C%2520and%2520%252B5.9%2520on%2520How2Sign.%250AAdditionally%252C%2520we%2520set%2520a%2520new%2520baseline%2520for%2520the%2520OpenASL%2520dataset%2520in%2520the%2520SLRet%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C%24%7B%5E2%7D%24RL%3A%20Content%20and%20Context%20Representation%20Learning%20for%20Gloss-free%0A%20%20Sign%20Language%20Translation%20and%20Retrieval&entry.906535625=Zhigang%20Chen%20and%20Benjia%20Zhou%20and%20Yiqing%20Huang%20and%20Jun%20Wan%20and%20Yibo%20Hu%20and%20Hailin%20Shi%20and%20Yanyan%20Liang%20and%20Zhen%20Lei%20and%20Du%20Zhang&entry.1292438233=%20%20Sign%20Language%20Representation%20Learning%20%28SLRL%29%20is%20crucial%20for%20a%20range%20of%20sign%0Alanguage-related%20downstream%20tasks%20such%20as%20Sign%20Language%20Translation%20%28SLT%29%20and%0ASign%20Language%20Retrieval%20%28SLRet%29.%20Recently%2C%20many%20gloss-based%20and%20gloss-free%20SLRL%0Amethods%20have%20been%20proposed%2C%20showing%20promising%20performance.%20Among%20them%2C%20the%0Agloss-free%20approach%20shows%20promise%20for%20strong%20scalability%20without%20relying%20on%0Agloss%20annotations.%20However%2C%20it%20currently%20faces%20suboptimal%20solutions%20due%20to%0Achallenges%20in%20encoding%20the%20intricate%2C%20context-sensitive%20characteristics%20of%20sign%0Alanguage%20videos%2C%20mainly%20struggling%20to%20discern%20essential%20sign%20features%20using%20a%0Anon-monotonic%20video-text%20alignment%20strategy.%20Therefore%2C%20we%20introduce%20an%0Ainnovative%20pretraining%20paradigm%20for%20gloss-free%20SLRL%2C%20called%20C%24%7B%5E2%7D%24RL%2C%20in%20this%0Apaper.%20Specifically%2C%20rather%20than%20merely%20incorporating%20a%20non-monotonic%20semantic%0Aalignment%20of%20video%20and%20text%20to%20learn%20language-oriented%20sign%20features%2C%20we%0Aemphasize%20two%20pivotal%20aspects%20of%20SLRL%3A%20Implicit%20Content%20Learning%20%28ICL%29%20and%0AExplicit%20Context%20Learning%20%28ECL%29.%20ICL%20delves%20into%20the%20content%20of%20communication%2C%0Acapturing%20the%20nuances%2C%20emphasis%2C%20timing%2C%20and%20rhythm%20of%20the%20signs.%20In%20contrast%2C%0AECL%20focuses%20on%20understanding%20the%20contextual%20meaning%20of%20signs%20and%20converting%0Athem%20into%20equivalent%20sentences.%20Despite%20its%20simplicity%2C%20extensive%20experiments%0Aconfirm%20that%20the%20joint%20optimization%20of%20ICL%20and%20ECL%20results%20in%20robust%20sign%0Alanguage%20representation%20and%20significant%20performance%20gains%20in%20gloss-free%20SLT%20and%0ASLRet%20tasks.%20Notably%2C%20C%24%7B%5E2%7D%24RL%20improves%20the%20BLEU-4%20score%20by%20%2B5.3%20on%20P14T%2C%0A%2B10.6%20on%20CSL-daily%2C%20%2B6.2%20on%20OpenASL%2C%20and%20%2B1.3%20on%20How2Sign.%20It%20also%20boosts%20the%0AR%401%20score%20by%20%2B8.3%20on%20P14T%2C%20%2B14.4%20on%20CSL-daily%2C%20and%20%2B5.9%20on%20How2Sign.%0AAdditionally%2C%20we%20set%20a%20new%20baseline%20for%20the%20OpenASL%20dataset%20in%20the%20SLRet%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09949v1&entry.124074799=Read"},
{"title": "Sliced Maximal Information Coefficient: A Training-Free Approach for\n  Image Quality Assessment Enhancement", "author": "Kang Xiao and Xu Wang and Yulin He and Baoliang Chen and Xuelin Shen", "abstract": "  Full-reference image quality assessment (FR-IQA) models generally operate by\nmeasuring the visual differences between a degraded image and its reference.\nHowever, existing FR-IQA models including both the classical ones (eg, PSNR and\nSSIM) and deep-learning based measures (eg, LPIPS and DISTS) still exhibit\nlimitations in capturing the full perception characteristics of the human\nvisual system (HVS). In this paper, instead of designing a new FR-IQA measure,\nwe aim to explore a generalized human visual attention estimation strategy to\nmimic the process of human quality rating and enhance existing IQA models. In\nparticular, we model human attention generation by measuring the statistical\ndependency between the degraded image and the reference image. The dependency\nis captured in a training-free manner by our proposed sliced maximal\ninformation coefficient and exhibits surprising generalization in different IQA\nmeasures. Experimental results verify the performance of existing IQA models\ncan be consistently improved when our attention module is incorporated. The\nsource code is available at https://github.com/KANGX99/SMIC.\n", "link": "http://arxiv.org/abs/2408.09920v1", "date": "2024-08-19", "relevancy": 2.0619, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5277}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5234}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sliced%20Maximal%20Information%20Coefficient%3A%20A%20Training-Free%20Approach%20for%0A%20%20Image%20Quality%20Assessment%20Enhancement&body=Title%3A%20Sliced%20Maximal%20Information%20Coefficient%3A%20A%20Training-Free%20Approach%20for%0A%20%20Image%20Quality%20Assessment%20Enhancement%0AAuthor%3A%20Kang%20Xiao%20and%20Xu%20Wang%20and%20Yulin%20He%20and%20Baoliang%20Chen%20and%20Xuelin%20Shen%0AAbstract%3A%20%20%20Full-reference%20image%20quality%20assessment%20%28FR-IQA%29%20models%20generally%20operate%20by%0Ameasuring%20the%20visual%20differences%20between%20a%20degraded%20image%20and%20its%20reference.%0AHowever%2C%20existing%20FR-IQA%20models%20including%20both%20the%20classical%20ones%20%28eg%2C%20PSNR%20and%0ASSIM%29%20and%20deep-learning%20based%20measures%20%28eg%2C%20LPIPS%20and%20DISTS%29%20still%20exhibit%0Alimitations%20in%20capturing%20the%20full%20perception%20characteristics%20of%20the%20human%0Avisual%20system%20%28HVS%29.%20In%20this%20paper%2C%20instead%20of%20designing%20a%20new%20FR-IQA%20measure%2C%0Awe%20aim%20to%20explore%20a%20generalized%20human%20visual%20attention%20estimation%20strategy%20to%0Amimic%20the%20process%20of%20human%20quality%20rating%20and%20enhance%20existing%20IQA%20models.%20In%0Aparticular%2C%20we%20model%20human%20attention%20generation%20by%20measuring%20the%20statistical%0Adependency%20between%20the%20degraded%20image%20and%20the%20reference%20image.%20The%20dependency%0Ais%20captured%20in%20a%20training-free%20manner%20by%20our%20proposed%20sliced%20maximal%0Ainformation%20coefficient%20and%20exhibits%20surprising%20generalization%20in%20different%20IQA%0Ameasures.%20Experimental%20results%20verify%20the%20performance%20of%20existing%20IQA%20models%0Acan%20be%20consistently%20improved%20when%20our%20attention%20module%20is%20incorporated.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/KANGX99/SMIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSliced%2520Maximal%2520Information%2520Coefficient%253A%2520A%2520Training-Free%2520Approach%2520for%250A%2520%2520Image%2520Quality%2520Assessment%2520Enhancement%26entry.906535625%3DKang%2520Xiao%2520and%2520Xu%2520Wang%2520and%2520Yulin%2520He%2520and%2520Baoliang%2520Chen%2520and%2520Xuelin%2520Shen%26entry.1292438233%3D%2520%2520Full-reference%2520image%2520quality%2520assessment%2520%2528FR-IQA%2529%2520models%2520generally%2520operate%2520by%250Ameasuring%2520the%2520visual%2520differences%2520between%2520a%2520degraded%2520image%2520and%2520its%2520reference.%250AHowever%252C%2520existing%2520FR-IQA%2520models%2520including%2520both%2520the%2520classical%2520ones%2520%2528eg%252C%2520PSNR%2520and%250ASSIM%2529%2520and%2520deep-learning%2520based%2520measures%2520%2528eg%252C%2520LPIPS%2520and%2520DISTS%2529%2520still%2520exhibit%250Alimitations%2520in%2520capturing%2520the%2520full%2520perception%2520characteristics%2520of%2520the%2520human%250Avisual%2520system%2520%2528HVS%2529.%2520In%2520this%2520paper%252C%2520instead%2520of%2520designing%2520a%2520new%2520FR-IQA%2520measure%252C%250Awe%2520aim%2520to%2520explore%2520a%2520generalized%2520human%2520visual%2520attention%2520estimation%2520strategy%2520to%250Amimic%2520the%2520process%2520of%2520human%2520quality%2520rating%2520and%2520enhance%2520existing%2520IQA%2520models.%2520In%250Aparticular%252C%2520we%2520model%2520human%2520attention%2520generation%2520by%2520measuring%2520the%2520statistical%250Adependency%2520between%2520the%2520degraded%2520image%2520and%2520the%2520reference%2520image.%2520The%2520dependency%250Ais%2520captured%2520in%2520a%2520training-free%2520manner%2520by%2520our%2520proposed%2520sliced%2520maximal%250Ainformation%2520coefficient%2520and%2520exhibits%2520surprising%2520generalization%2520in%2520different%2520IQA%250Ameasures.%2520Experimental%2520results%2520verify%2520the%2520performance%2520of%2520existing%2520IQA%2520models%250Acan%2520be%2520consistently%2520improved%2520when%2520our%2520attention%2520module%2520is%2520incorporated.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/KANGX99/SMIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sliced%20Maximal%20Information%20Coefficient%3A%20A%20Training-Free%20Approach%20for%0A%20%20Image%20Quality%20Assessment%20Enhancement&entry.906535625=Kang%20Xiao%20and%20Xu%20Wang%20and%20Yulin%20He%20and%20Baoliang%20Chen%20and%20Xuelin%20Shen&entry.1292438233=%20%20Full-reference%20image%20quality%20assessment%20%28FR-IQA%29%20models%20generally%20operate%20by%0Ameasuring%20the%20visual%20differences%20between%20a%20degraded%20image%20and%20its%20reference.%0AHowever%2C%20existing%20FR-IQA%20models%20including%20both%20the%20classical%20ones%20%28eg%2C%20PSNR%20and%0ASSIM%29%20and%20deep-learning%20based%20measures%20%28eg%2C%20LPIPS%20and%20DISTS%29%20still%20exhibit%0Alimitations%20in%20capturing%20the%20full%20perception%20characteristics%20of%20the%20human%0Avisual%20system%20%28HVS%29.%20In%20this%20paper%2C%20instead%20of%20designing%20a%20new%20FR-IQA%20measure%2C%0Awe%20aim%20to%20explore%20a%20generalized%20human%20visual%20attention%20estimation%20strategy%20to%0Amimic%20the%20process%20of%20human%20quality%20rating%20and%20enhance%20existing%20IQA%20models.%20In%0Aparticular%2C%20we%20model%20human%20attention%20generation%20by%20measuring%20the%20statistical%0Adependency%20between%20the%20degraded%20image%20and%20the%20reference%20image.%20The%20dependency%0Ais%20captured%20in%20a%20training-free%20manner%20by%20our%20proposed%20sliced%20maximal%0Ainformation%20coefficient%20and%20exhibits%20surprising%20generalization%20in%20different%20IQA%0Ameasures.%20Experimental%20results%20verify%20the%20performance%20of%20existing%20IQA%20models%0Acan%20be%20consistently%20improved%20when%20our%20attention%20module%20is%20incorporated.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/KANGX99/SMIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09920v1&entry.124074799=Read"},
{"title": "Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced\n  Feature Pyramid Network", "author": "Rasha Alshawi and Md Meftahul Ferdaus and Mahdi Abdelguerfi and Kendall Niles and Ken Pathak and Steve Sloan", "abstract": "  Imbalanced datasets are a significant challenge in real-world scenarios. They\nlead to models that underperform on underrepresented classes, which is a\ncritical issue in infrastructure inspection. This paper introduces the Enhanced\nFeature Pyramid Network (E-FPN), a deep learning model for the semantic\nsegmentation of culverts and sewer pipes within imbalanced datasets. The E-FPN\nincorporates architectural innovations like sparsely connected blocks and\ndepth-wise separable convolutions to improve feature extraction and handle\nobject variations. To address dataset imbalance, the model employs strategies\nlike class decomposition and data augmentation. Experimental results on the\nculvert-sewer defects dataset and a benchmark aerial semantic segmentation\ndrone dataset show that the E-FPN outperforms state-of-the-art methods,\nachieving an average Intersection over Union (IoU) improvement of 13.8% and\n27.2%, respectively. Additionally, class decomposition and data augmentation\ntogether boost the model's performance by approximately 6.9% IoU. The proposed\nE-FPN presents a promising solution for enhancing object segmentation in\nchallenging, multi-class real-world datasets, with potential applications\nextending beyond culvert-sewer defect detection.\n", "link": "http://arxiv.org/abs/2408.10181v1", "date": "2024-08-19", "relevancy": 2.0583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imbalance-Aware%20Culvert-Sewer%20Defect%20Segmentation%20Using%20an%20Enhanced%0A%20%20Feature%20Pyramid%20Network&body=Title%3A%20Imbalance-Aware%20Culvert-Sewer%20Defect%20Segmentation%20Using%20an%20Enhanced%0A%20%20Feature%20Pyramid%20Network%0AAuthor%3A%20Rasha%20Alshawi%20and%20Md%20Meftahul%20Ferdaus%20and%20Mahdi%20Abdelguerfi%20and%20Kendall%20Niles%20and%20Ken%20Pathak%20and%20Steve%20Sloan%0AAbstract%3A%20%20%20Imbalanced%20datasets%20are%20a%20significant%20challenge%20in%20real-world%20scenarios.%20They%0Alead%20to%20models%20that%20underperform%20on%20underrepresented%20classes%2C%20which%20is%20a%0Acritical%20issue%20in%20infrastructure%20inspection.%20This%20paper%20introduces%20the%20Enhanced%0AFeature%20Pyramid%20Network%20%28E-FPN%29%2C%20a%20deep%20learning%20model%20for%20the%20semantic%0Asegmentation%20of%20culverts%20and%20sewer%20pipes%20within%20imbalanced%20datasets.%20The%20E-FPN%0Aincorporates%20architectural%20innovations%20like%20sparsely%20connected%20blocks%20and%0Adepth-wise%20separable%20convolutions%20to%20improve%20feature%20extraction%20and%20handle%0Aobject%20variations.%20To%20address%20dataset%20imbalance%2C%20the%20model%20employs%20strategies%0Alike%20class%20decomposition%20and%20data%20augmentation.%20Experimental%20results%20on%20the%0Aculvert-sewer%20defects%20dataset%20and%20a%20benchmark%20aerial%20semantic%20segmentation%0Adrone%20dataset%20show%20that%20the%20E-FPN%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20an%20average%20Intersection%20over%20Union%20%28IoU%29%20improvement%20of%2013.8%25%20and%0A27.2%25%2C%20respectively.%20Additionally%2C%20class%20decomposition%20and%20data%20augmentation%0Atogether%20boost%20the%20model%27s%20performance%20by%20approximately%206.9%25%20IoU.%20The%20proposed%0AE-FPN%20presents%20a%20promising%20solution%20for%20enhancing%20object%20segmentation%20in%0Achallenging%2C%20multi-class%20real-world%20datasets%2C%20with%20potential%20applications%0Aextending%20beyond%20culvert-sewer%20defect%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImbalance-Aware%2520Culvert-Sewer%2520Defect%2520Segmentation%2520Using%2520an%2520Enhanced%250A%2520%2520Feature%2520Pyramid%2520Network%26entry.906535625%3DRasha%2520Alshawi%2520and%2520Md%2520Meftahul%2520Ferdaus%2520and%2520Mahdi%2520Abdelguerfi%2520and%2520Kendall%2520Niles%2520and%2520Ken%2520Pathak%2520and%2520Steve%2520Sloan%26entry.1292438233%3D%2520%2520Imbalanced%2520datasets%2520are%2520a%2520significant%2520challenge%2520in%2520real-world%2520scenarios.%2520They%250Alead%2520to%2520models%2520that%2520underperform%2520on%2520underrepresented%2520classes%252C%2520which%2520is%2520a%250Acritical%2520issue%2520in%2520infrastructure%2520inspection.%2520This%2520paper%2520introduces%2520the%2520Enhanced%250AFeature%2520Pyramid%2520Network%2520%2528E-FPN%2529%252C%2520a%2520deep%2520learning%2520model%2520for%2520the%2520semantic%250Asegmentation%2520of%2520culverts%2520and%2520sewer%2520pipes%2520within%2520imbalanced%2520datasets.%2520The%2520E-FPN%250Aincorporates%2520architectural%2520innovations%2520like%2520sparsely%2520connected%2520blocks%2520and%250Adepth-wise%2520separable%2520convolutions%2520to%2520improve%2520feature%2520extraction%2520and%2520handle%250Aobject%2520variations.%2520To%2520address%2520dataset%2520imbalance%252C%2520the%2520model%2520employs%2520strategies%250Alike%2520class%2520decomposition%2520and%2520data%2520augmentation.%2520Experimental%2520results%2520on%2520the%250Aculvert-sewer%2520defects%2520dataset%2520and%2520a%2520benchmark%2520aerial%2520semantic%2520segmentation%250Adrone%2520dataset%2520show%2520that%2520the%2520E-FPN%2520outperforms%2520state-of-the-art%2520methods%252C%250Aachieving%2520an%2520average%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520improvement%2520of%252013.8%2525%2520and%250A27.2%2525%252C%2520respectively.%2520Additionally%252C%2520class%2520decomposition%2520and%2520data%2520augmentation%250Atogether%2520boost%2520the%2520model%2527s%2520performance%2520by%2520approximately%25206.9%2525%2520IoU.%2520The%2520proposed%250AE-FPN%2520presents%2520a%2520promising%2520solution%2520for%2520enhancing%2520object%2520segmentation%2520in%250Achallenging%252C%2520multi-class%2520real-world%2520datasets%252C%2520with%2520potential%2520applications%250Aextending%2520beyond%2520culvert-sewer%2520defect%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imbalance-Aware%20Culvert-Sewer%20Defect%20Segmentation%20Using%20an%20Enhanced%0A%20%20Feature%20Pyramid%20Network&entry.906535625=Rasha%20Alshawi%20and%20Md%20Meftahul%20Ferdaus%20and%20Mahdi%20Abdelguerfi%20and%20Kendall%20Niles%20and%20Ken%20Pathak%20and%20Steve%20Sloan&entry.1292438233=%20%20Imbalanced%20datasets%20are%20a%20significant%20challenge%20in%20real-world%20scenarios.%20They%0Alead%20to%20models%20that%20underperform%20on%20underrepresented%20classes%2C%20which%20is%20a%0Acritical%20issue%20in%20infrastructure%20inspection.%20This%20paper%20introduces%20the%20Enhanced%0AFeature%20Pyramid%20Network%20%28E-FPN%29%2C%20a%20deep%20learning%20model%20for%20the%20semantic%0Asegmentation%20of%20culverts%20and%20sewer%20pipes%20within%20imbalanced%20datasets.%20The%20E-FPN%0Aincorporates%20architectural%20innovations%20like%20sparsely%20connected%20blocks%20and%0Adepth-wise%20separable%20convolutions%20to%20improve%20feature%20extraction%20and%20handle%0Aobject%20variations.%20To%20address%20dataset%20imbalance%2C%20the%20model%20employs%20strategies%0Alike%20class%20decomposition%20and%20data%20augmentation.%20Experimental%20results%20on%20the%0Aculvert-sewer%20defects%20dataset%20and%20a%20benchmark%20aerial%20semantic%20segmentation%0Adrone%20dataset%20show%20that%20the%20E-FPN%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20an%20average%20Intersection%20over%20Union%20%28IoU%29%20improvement%20of%2013.8%25%20and%0A27.2%25%2C%20respectively.%20Additionally%2C%20class%20decomposition%20and%20data%20augmentation%0Atogether%20boost%20the%20model%27s%20performance%20by%20approximately%206.9%25%20IoU.%20The%20proposed%0AE-FPN%20presents%20a%20promising%20solution%20for%20enhancing%20object%20segmentation%20in%0Achallenging%2C%20multi-class%20real-world%20datasets%2C%20with%20potential%20applications%0Aextending%20beyond%20culvert-sewer%20defect%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10181v1&entry.124074799=Read"},
{"title": "The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective", "author": "Renye Yan and Yaozhong Gan and You Wu and Ling Liang and Junliang Xing and Yimao Cai and Ru Huang", "abstract": "  The imbalance of exploration and exploitation has long been a significant\nchallenge in reinforcement learning. In policy optimization, excessive reliance\non exploration reduces learning efficiency, while over-dependence on\nexploitation might trap agents in local optima. This paper revisits the\nexploration-exploitation dilemma from the perspective of entropy by revealing\nthe relationship between entropy and the dynamic adaptive process of\nexploration and exploitation. Based on this theoretical insight, we establish\nan end-to-end adaptive framework called AdaZero, which automatically determines\nwhether to explore or to exploit as well as their balance of strength.\nExperiments show that AdaZero significantly outperforms baseline models across\nvarious Atari and MuJoCo environments with only a single setting. Especially in\nthe challenging environment of Montezuma, AdaZero boosts the final returns by\nup to fifteen times. Moreover, we conduct a series of visualization analyses to\nreveal the dynamics of our self-adaptive mechanism, demonstrating how entropy\nreflects and changes with respect to the agent's performance and adaptive\nprocess.\n", "link": "http://arxiv.org/abs/2408.09974v1", "date": "2024-08-19", "relevancy": 2.0583, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5831}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5066}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Exploration-Exploitation%20Dilemma%20Revisited%3A%20An%20Entropy%20Perspective&body=Title%3A%20The%20Exploration-Exploitation%20Dilemma%20Revisited%3A%20An%20Entropy%20Perspective%0AAuthor%3A%20Renye%20Yan%20and%20Yaozhong%20Gan%20and%20You%20Wu%20and%20Ling%20Liang%20and%20Junliang%20Xing%20and%20Yimao%20Cai%20and%20Ru%20Huang%0AAbstract%3A%20%20%20The%20imbalance%20of%20exploration%20and%20exploitation%20has%20long%20been%20a%20significant%0Achallenge%20in%20reinforcement%20learning.%20In%20policy%20optimization%2C%20excessive%20reliance%0Aon%20exploration%20reduces%20learning%20efficiency%2C%20while%20over-dependence%20on%0Aexploitation%20might%20trap%20agents%20in%20local%20optima.%20This%20paper%20revisits%20the%0Aexploration-exploitation%20dilemma%20from%20the%20perspective%20of%20entropy%20by%20revealing%0Athe%20relationship%20between%20entropy%20and%20the%20dynamic%20adaptive%20process%20of%0Aexploration%20and%20exploitation.%20Based%20on%20this%20theoretical%20insight%2C%20we%20establish%0Aan%20end-to-end%20adaptive%20framework%20called%20AdaZero%2C%20which%20automatically%20determines%0Awhether%20to%20explore%20or%20to%20exploit%20as%20well%20as%20their%20balance%20of%20strength.%0AExperiments%20show%20that%20AdaZero%20significantly%20outperforms%20baseline%20models%20across%0Avarious%20Atari%20and%20MuJoCo%20environments%20with%20only%20a%20single%20setting.%20Especially%20in%0Athe%20challenging%20environment%20of%20Montezuma%2C%20AdaZero%20boosts%20the%20final%20returns%20by%0Aup%20to%20fifteen%20times.%20Moreover%2C%20we%20conduct%20a%20series%20of%20visualization%20analyses%20to%0Areveal%20the%20dynamics%20of%20our%20self-adaptive%20mechanism%2C%20demonstrating%20how%20entropy%0Areflects%20and%20changes%20with%20respect%20to%20the%20agent%27s%20performance%20and%20adaptive%0Aprocess.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Exploration-Exploitation%2520Dilemma%2520Revisited%253A%2520An%2520Entropy%2520Perspective%26entry.906535625%3DRenye%2520Yan%2520and%2520Yaozhong%2520Gan%2520and%2520You%2520Wu%2520and%2520Ling%2520Liang%2520and%2520Junliang%2520Xing%2520and%2520Yimao%2520Cai%2520and%2520Ru%2520Huang%26entry.1292438233%3D%2520%2520The%2520imbalance%2520of%2520exploration%2520and%2520exploitation%2520has%2520long%2520been%2520a%2520significant%250Achallenge%2520in%2520reinforcement%2520learning.%2520In%2520policy%2520optimization%252C%2520excessive%2520reliance%250Aon%2520exploration%2520reduces%2520learning%2520efficiency%252C%2520while%2520over-dependence%2520on%250Aexploitation%2520might%2520trap%2520agents%2520in%2520local%2520optima.%2520This%2520paper%2520revisits%2520the%250Aexploration-exploitation%2520dilemma%2520from%2520the%2520perspective%2520of%2520entropy%2520by%2520revealing%250Athe%2520relationship%2520between%2520entropy%2520and%2520the%2520dynamic%2520adaptive%2520process%2520of%250Aexploration%2520and%2520exploitation.%2520Based%2520on%2520this%2520theoretical%2520insight%252C%2520we%2520establish%250Aan%2520end-to-end%2520adaptive%2520framework%2520called%2520AdaZero%252C%2520which%2520automatically%2520determines%250Awhether%2520to%2520explore%2520or%2520to%2520exploit%2520as%2520well%2520as%2520their%2520balance%2520of%2520strength.%250AExperiments%2520show%2520that%2520AdaZero%2520significantly%2520outperforms%2520baseline%2520models%2520across%250Avarious%2520Atari%2520and%2520MuJoCo%2520environments%2520with%2520only%2520a%2520single%2520setting.%2520Especially%2520in%250Athe%2520challenging%2520environment%2520of%2520Montezuma%252C%2520AdaZero%2520boosts%2520the%2520final%2520returns%2520by%250Aup%2520to%2520fifteen%2520times.%2520Moreover%252C%2520we%2520conduct%2520a%2520series%2520of%2520visualization%2520analyses%2520to%250Areveal%2520the%2520dynamics%2520of%2520our%2520self-adaptive%2520mechanism%252C%2520demonstrating%2520how%2520entropy%250Areflects%2520and%2520changes%2520with%2520respect%2520to%2520the%2520agent%2527s%2520performance%2520and%2520adaptive%250Aprocess.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Exploration-Exploitation%20Dilemma%20Revisited%3A%20An%20Entropy%20Perspective&entry.906535625=Renye%20Yan%20and%20Yaozhong%20Gan%20and%20You%20Wu%20and%20Ling%20Liang%20and%20Junliang%20Xing%20and%20Yimao%20Cai%20and%20Ru%20Huang&entry.1292438233=%20%20The%20imbalance%20of%20exploration%20and%20exploitation%20has%20long%20been%20a%20significant%0Achallenge%20in%20reinforcement%20learning.%20In%20policy%20optimization%2C%20excessive%20reliance%0Aon%20exploration%20reduces%20learning%20efficiency%2C%20while%20over-dependence%20on%0Aexploitation%20might%20trap%20agents%20in%20local%20optima.%20This%20paper%20revisits%20the%0Aexploration-exploitation%20dilemma%20from%20the%20perspective%20of%20entropy%20by%20revealing%0Athe%20relationship%20between%20entropy%20and%20the%20dynamic%20adaptive%20process%20of%0Aexploration%20and%20exploitation.%20Based%20on%20this%20theoretical%20insight%2C%20we%20establish%0Aan%20end-to-end%20adaptive%20framework%20called%20AdaZero%2C%20which%20automatically%20determines%0Awhether%20to%20explore%20or%20to%20exploit%20as%20well%20as%20their%20balance%20of%20strength.%0AExperiments%20show%20that%20AdaZero%20significantly%20outperforms%20baseline%20models%20across%0Avarious%20Atari%20and%20MuJoCo%20environments%20with%20only%20a%20single%20setting.%20Especially%20in%0Athe%20challenging%20environment%20of%20Montezuma%2C%20AdaZero%20boosts%20the%20final%20returns%20by%0Aup%20to%20fifteen%20times.%20Moreover%2C%20we%20conduct%20a%20series%20of%20visualization%20analyses%20to%0Areveal%20the%20dynamics%20of%20our%20self-adaptive%20mechanism%2C%20demonstrating%20how%20entropy%0Areflects%20and%20changes%20with%20respect%20to%20the%20agent%27s%20performance%20and%20adaptive%0Aprocess.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09974v1&entry.124074799=Read"},
{"title": "Preoperative Rotator Cuff Tear Prediction from Shoulder Radiographs\n  using a Convolutional Block Attention Module-Integrated Neural Network", "author": "Chris Hyunchul Jo and Jiwoong Yang and Byunghwan Jeon and Hackjoon Shim and Ikbeom Jang", "abstract": "  Research question: We test whether a plane shoulder radiograph can be used\ntogether with deep learning methods to identify patients with rotator cuff\ntears as opposed to using an MRI in standard of care. Findings: By integrating\nconvolutional block attention modules into a deep neural network, our model\ndemonstrates high accuracy in detecting patients with rotator cuff tears,\nachieving an average AUC of 0.889 and an accuracy of 0.831. Meaning: This study\nvalidates the efficacy of our deep learning model to accurately detect rotation\ncuff tears from radiographs, offering a viable pre-assessment or alternative to\nmore expensive imaging techniques such as MRI.\n", "link": "http://arxiv.org/abs/2408.09894v1", "date": "2024-08-19", "relevancy": 2.0446, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4117}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preoperative%20Rotator%20Cuff%20Tear%20Prediction%20from%20Shoulder%20Radiographs%0A%20%20using%20a%20Convolutional%20Block%20Attention%20Module-Integrated%20Neural%20Network&body=Title%3A%20Preoperative%20Rotator%20Cuff%20Tear%20Prediction%20from%20Shoulder%20Radiographs%0A%20%20using%20a%20Convolutional%20Block%20Attention%20Module-Integrated%20Neural%20Network%0AAuthor%3A%20Chris%20Hyunchul%20Jo%20and%20Jiwoong%20Yang%20and%20Byunghwan%20Jeon%20and%20Hackjoon%20Shim%20and%20Ikbeom%20Jang%0AAbstract%3A%20%20%20Research%20question%3A%20We%20test%20whether%20a%20plane%20shoulder%20radiograph%20can%20be%20used%0Atogether%20with%20deep%20learning%20methods%20to%20identify%20patients%20with%20rotator%20cuff%0Atears%20as%20opposed%20to%20using%20an%20MRI%20in%20standard%20of%20care.%20Findings%3A%20By%20integrating%0Aconvolutional%20block%20attention%20modules%20into%20a%20deep%20neural%20network%2C%20our%20model%0Ademonstrates%20high%20accuracy%20in%20detecting%20patients%20with%20rotator%20cuff%20tears%2C%0Aachieving%20an%20average%20AUC%20of%200.889%20and%20an%20accuracy%20of%200.831.%20Meaning%3A%20This%20study%0Avalidates%20the%20efficacy%20of%20our%20deep%20learning%20model%20to%20accurately%20detect%20rotation%0Acuff%20tears%20from%20radiographs%2C%20offering%20a%20viable%20pre-assessment%20or%20alternative%20to%0Amore%20expensive%20imaging%20techniques%20such%20as%20MRI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreoperative%2520Rotator%2520Cuff%2520Tear%2520Prediction%2520from%2520Shoulder%2520Radiographs%250A%2520%2520using%2520a%2520Convolutional%2520Block%2520Attention%2520Module-Integrated%2520Neural%2520Network%26entry.906535625%3DChris%2520Hyunchul%2520Jo%2520and%2520Jiwoong%2520Yang%2520and%2520Byunghwan%2520Jeon%2520and%2520Hackjoon%2520Shim%2520and%2520Ikbeom%2520Jang%26entry.1292438233%3D%2520%2520Research%2520question%253A%2520We%2520test%2520whether%2520a%2520plane%2520shoulder%2520radiograph%2520can%2520be%2520used%250Atogether%2520with%2520deep%2520learning%2520methods%2520to%2520identify%2520patients%2520with%2520rotator%2520cuff%250Atears%2520as%2520opposed%2520to%2520using%2520an%2520MRI%2520in%2520standard%2520of%2520care.%2520Findings%253A%2520By%2520integrating%250Aconvolutional%2520block%2520attention%2520modules%2520into%2520a%2520deep%2520neural%2520network%252C%2520our%2520model%250Ademonstrates%2520high%2520accuracy%2520in%2520detecting%2520patients%2520with%2520rotator%2520cuff%2520tears%252C%250Aachieving%2520an%2520average%2520AUC%2520of%25200.889%2520and%2520an%2520accuracy%2520of%25200.831.%2520Meaning%253A%2520This%2520study%250Avalidates%2520the%2520efficacy%2520of%2520our%2520deep%2520learning%2520model%2520to%2520accurately%2520detect%2520rotation%250Acuff%2520tears%2520from%2520radiographs%252C%2520offering%2520a%2520viable%2520pre-assessment%2520or%2520alternative%2520to%250Amore%2520expensive%2520imaging%2520techniques%2520such%2520as%2520MRI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preoperative%20Rotator%20Cuff%20Tear%20Prediction%20from%20Shoulder%20Radiographs%0A%20%20using%20a%20Convolutional%20Block%20Attention%20Module-Integrated%20Neural%20Network&entry.906535625=Chris%20Hyunchul%20Jo%20and%20Jiwoong%20Yang%20and%20Byunghwan%20Jeon%20and%20Hackjoon%20Shim%20and%20Ikbeom%20Jang&entry.1292438233=%20%20Research%20question%3A%20We%20test%20whether%20a%20plane%20shoulder%20radiograph%20can%20be%20used%0Atogether%20with%20deep%20learning%20methods%20to%20identify%20patients%20with%20rotator%20cuff%0Atears%20as%20opposed%20to%20using%20an%20MRI%20in%20standard%20of%20care.%20Findings%3A%20By%20integrating%0Aconvolutional%20block%20attention%20modules%20into%20a%20deep%20neural%20network%2C%20our%20model%0Ademonstrates%20high%20accuracy%20in%20detecting%20patients%20with%20rotator%20cuff%20tears%2C%0Aachieving%20an%20average%20AUC%20of%200.889%20and%20an%20accuracy%20of%200.831.%20Meaning%3A%20This%20study%0Avalidates%20the%20efficacy%20of%20our%20deep%20learning%20model%20to%20accurately%20detect%20rotation%0Acuff%20tears%20from%20radiographs%2C%20offering%20a%20viable%20pre-assessment%20or%20alternative%20to%0Amore%20expensive%20imaging%20techniques%20such%20as%20MRI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09894v1&entry.124074799=Read"},
{"title": "LNQ 2023 challenge: Benchmark of weakly-supervised techniques for\n  mediastinal lymph node quantification", "author": "Reuben Dorent and Roya Khajavi and Tagwa Idris and Erik Ziegler and Bhanusupriya Somarouthu and Heather Jacene and Ann LaCasce and Jonathan Deissler and Jan Ehrhardt and Sofija Engelson and Stefan M. Fischer and Yun Gu and Heinz Handels and Satoshi Kasai and Satoshi Kondo and Klaus Maier-Hein and Julia A. Schnabel and Guotai Wang and Litingyu Wang and Tassilo Wald and Guang-Zhong Yang and Hanxiao Zhang and Minghui Zhang and Steve Pieper and Gordon Harris and Ron Kikinis and Tina Kapur", "abstract": "  Accurate assessment of lymph node size in 3D CT scans is crucial for cancer\nstaging, therapeutic management, and monitoring treatment response. Existing\nstate-of-the-art segmentation frameworks in medical imaging often rely on fully\nannotated datasets. However, for lymph node segmentation, these datasets are\ntypically small due to the extensive time and expertise required to annotate\nthe numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which\nleverages incomplete or noisy annotations, has recently gained interest in the\nmedical imaging community as a potential solution. Despite the variety of\nweakly-supervised techniques proposed, most have been validated only on private\ndatasets or small publicly available datasets. To address this limitation, the\nMediastinal Lymph Node Quantification (LNQ) challenge was organized in\nconjunction with the 26th International Conference on Medical Image Computing\nand Computer Assisted Intervention (MICCAI 2023). This challenge aimed to\nadvance weakly-supervised segmentation methods by providing a new, partially\nannotated dataset and a robust evaluation framework. A total of 16 teams from 5\ncountries submitted predictions to the validation leaderboard, and 6 teams from\n3 countries participated in the evaluation phase. The results highlighted both\nthe potential and the current limitations of weakly-supervised approaches. On\none hand, weakly-supervised approaches obtained relatively good performance\nwith a median Dice score of $61.0\\%$. On the other hand, top-ranked teams, with\na median Dice score exceeding $70\\%$, boosted their performance by leveraging\nsmaller but fully annotated datasets to combine weak supervision and full\nsupervision. This highlights both the promise of weakly-supervised methods and\nthe ongoing need for high-quality, fully annotated data to achieve higher\nsegmentation performance.\n", "link": "http://arxiv.org/abs/2408.10069v1", "date": "2024-08-19", "relevancy": 2.0322, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification&body=Title%3A%20LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification%0AAuthor%3A%20Reuben%20Dorent%20and%20Roya%20Khajavi%20and%20Tagwa%20Idris%20and%20Erik%20Ziegler%20and%20Bhanusupriya%20Somarouthu%20and%20Heather%20Jacene%20and%20Ann%20LaCasce%20and%20Jonathan%20Deissler%20and%20Jan%20Ehrhardt%20and%20Sofija%20Engelson%20and%20Stefan%20M.%20Fischer%20and%20Yun%20Gu%20and%20Heinz%20Handels%20and%20Satoshi%20Kasai%20and%20Satoshi%20Kondo%20and%20Klaus%20Maier-Hein%20and%20Julia%20A.%20Schnabel%20and%20Guotai%20Wang%20and%20Litingyu%20Wang%20and%20Tassilo%20Wald%20and%20Guang-Zhong%20Yang%20and%20Hanxiao%20Zhang%20and%20Minghui%20Zhang%20and%20Steve%20Pieper%20and%20Gordon%20Harris%20and%20Ron%20Kikinis%20and%20Tina%20Kapur%0AAbstract%3A%20%20%20Accurate%20assessment%20of%20lymph%20node%20size%20in%203D%20CT%20scans%20is%20crucial%20for%20cancer%0Astaging%2C%20therapeutic%20management%2C%20and%20monitoring%20treatment%20response.%20Existing%0Astate-of-the-art%20segmentation%20frameworks%20in%20medical%20imaging%20often%20rely%20on%20fully%0Aannotated%20datasets.%20However%2C%20for%20lymph%20node%20segmentation%2C%20these%20datasets%20are%0Atypically%20small%20due%20to%20the%20extensive%20time%20and%20expertise%20required%20to%20annotate%0Athe%20numerous%20lymph%20nodes%20in%203D%20CT%20scans.%20Weakly-supervised%20learning%2C%20which%0Aleverages%20incomplete%20or%20noisy%20annotations%2C%20has%20recently%20gained%20interest%20in%20the%0Amedical%20imaging%20community%20as%20a%20potential%20solution.%20Despite%20the%20variety%20of%0Aweakly-supervised%20techniques%20proposed%2C%20most%20have%20been%20validated%20only%20on%20private%0Adatasets%20or%20small%20publicly%20available%20datasets.%20To%20address%20this%20limitation%2C%20the%0AMediastinal%20Lymph%20Node%20Quantification%20%28LNQ%29%20challenge%20was%20organized%20in%0Aconjunction%20with%20the%2026th%20International%20Conference%20on%20Medical%20Image%20Computing%0Aand%20Computer%20Assisted%20Intervention%20%28MICCAI%202023%29.%20This%20challenge%20aimed%20to%0Aadvance%20weakly-supervised%20segmentation%20methods%20by%20providing%20a%20new%2C%20partially%0Aannotated%20dataset%20and%20a%20robust%20evaluation%20framework.%20A%20total%20of%2016%20teams%20from%205%0Acountries%20submitted%20predictions%20to%20the%20validation%20leaderboard%2C%20and%206%20teams%20from%0A3%20countries%20participated%20in%20the%20evaluation%20phase.%20The%20results%20highlighted%20both%0Athe%20potential%20and%20the%20current%20limitations%20of%20weakly-supervised%20approaches.%20On%0Aone%20hand%2C%20weakly-supervised%20approaches%20obtained%20relatively%20good%20performance%0Awith%20a%20median%20Dice%20score%20of%20%2461.0%5C%25%24.%20On%20the%20other%20hand%2C%20top-ranked%20teams%2C%20with%0Aa%20median%20Dice%20score%20exceeding%20%2470%5C%25%24%2C%20boosted%20their%20performance%20by%20leveraging%0Asmaller%20but%20fully%20annotated%20datasets%20to%20combine%20weak%20supervision%20and%20full%0Asupervision.%20This%20highlights%20both%20the%20promise%20of%20weakly-supervised%20methods%20and%0Athe%20ongoing%20need%20for%20high-quality%2C%20fully%20annotated%20data%20to%20achieve%20higher%0Asegmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLNQ%25202023%2520challenge%253A%2520Benchmark%2520of%2520weakly-supervised%2520techniques%2520for%250A%2520%2520mediastinal%2520lymph%2520node%2520quantification%26entry.906535625%3DReuben%2520Dorent%2520and%2520Roya%2520Khajavi%2520and%2520Tagwa%2520Idris%2520and%2520Erik%2520Ziegler%2520and%2520Bhanusupriya%2520Somarouthu%2520and%2520Heather%2520Jacene%2520and%2520Ann%2520LaCasce%2520and%2520Jonathan%2520Deissler%2520and%2520Jan%2520Ehrhardt%2520and%2520Sofija%2520Engelson%2520and%2520Stefan%2520M.%2520Fischer%2520and%2520Yun%2520Gu%2520and%2520Heinz%2520Handels%2520and%2520Satoshi%2520Kasai%2520and%2520Satoshi%2520Kondo%2520and%2520Klaus%2520Maier-Hein%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Guotai%2520Wang%2520and%2520Litingyu%2520Wang%2520and%2520Tassilo%2520Wald%2520and%2520Guang-Zhong%2520Yang%2520and%2520Hanxiao%2520Zhang%2520and%2520Minghui%2520Zhang%2520and%2520Steve%2520Pieper%2520and%2520Gordon%2520Harris%2520and%2520Ron%2520Kikinis%2520and%2520Tina%2520Kapur%26entry.1292438233%3D%2520%2520Accurate%2520assessment%2520of%2520lymph%2520node%2520size%2520in%25203D%2520CT%2520scans%2520is%2520crucial%2520for%2520cancer%250Astaging%252C%2520therapeutic%2520management%252C%2520and%2520monitoring%2520treatment%2520response.%2520Existing%250Astate-of-the-art%2520segmentation%2520frameworks%2520in%2520medical%2520imaging%2520often%2520rely%2520on%2520fully%250Aannotated%2520datasets.%2520However%252C%2520for%2520lymph%2520node%2520segmentation%252C%2520these%2520datasets%2520are%250Atypically%2520small%2520due%2520to%2520the%2520extensive%2520time%2520and%2520expertise%2520required%2520to%2520annotate%250Athe%2520numerous%2520lymph%2520nodes%2520in%25203D%2520CT%2520scans.%2520Weakly-supervised%2520learning%252C%2520which%250Aleverages%2520incomplete%2520or%2520noisy%2520annotations%252C%2520has%2520recently%2520gained%2520interest%2520in%2520the%250Amedical%2520imaging%2520community%2520as%2520a%2520potential%2520solution.%2520Despite%2520the%2520variety%2520of%250Aweakly-supervised%2520techniques%2520proposed%252C%2520most%2520have%2520been%2520validated%2520only%2520on%2520private%250Adatasets%2520or%2520small%2520publicly%2520available%2520datasets.%2520To%2520address%2520this%2520limitation%252C%2520the%250AMediastinal%2520Lymph%2520Node%2520Quantification%2520%2528LNQ%2529%2520challenge%2520was%2520organized%2520in%250Aconjunction%2520with%2520the%252026th%2520International%2520Conference%2520on%2520Medical%2520Image%2520Computing%250Aand%2520Computer%2520Assisted%2520Intervention%2520%2528MICCAI%25202023%2529.%2520This%2520challenge%2520aimed%2520to%250Aadvance%2520weakly-supervised%2520segmentation%2520methods%2520by%2520providing%2520a%2520new%252C%2520partially%250Aannotated%2520dataset%2520and%2520a%2520robust%2520evaluation%2520framework.%2520A%2520total%2520of%252016%2520teams%2520from%25205%250Acountries%2520submitted%2520predictions%2520to%2520the%2520validation%2520leaderboard%252C%2520and%25206%2520teams%2520from%250A3%2520countries%2520participated%2520in%2520the%2520evaluation%2520phase.%2520The%2520results%2520highlighted%2520both%250Athe%2520potential%2520and%2520the%2520current%2520limitations%2520of%2520weakly-supervised%2520approaches.%2520On%250Aone%2520hand%252C%2520weakly-supervised%2520approaches%2520obtained%2520relatively%2520good%2520performance%250Awith%2520a%2520median%2520Dice%2520score%2520of%2520%252461.0%255C%2525%2524.%2520On%2520the%2520other%2520hand%252C%2520top-ranked%2520teams%252C%2520with%250Aa%2520median%2520Dice%2520score%2520exceeding%2520%252470%255C%2525%2524%252C%2520boosted%2520their%2520performance%2520by%2520leveraging%250Asmaller%2520but%2520fully%2520annotated%2520datasets%2520to%2520combine%2520weak%2520supervision%2520and%2520full%250Asupervision.%2520This%2520highlights%2520both%2520the%2520promise%2520of%2520weakly-supervised%2520methods%2520and%250Athe%2520ongoing%2520need%2520for%2520high-quality%252C%2520fully%2520annotated%2520data%2520to%2520achieve%2520higher%250Asegmentation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification&entry.906535625=Reuben%20Dorent%20and%20Roya%20Khajavi%20and%20Tagwa%20Idris%20and%20Erik%20Ziegler%20and%20Bhanusupriya%20Somarouthu%20and%20Heather%20Jacene%20and%20Ann%20LaCasce%20and%20Jonathan%20Deissler%20and%20Jan%20Ehrhardt%20and%20Sofija%20Engelson%20and%20Stefan%20M.%20Fischer%20and%20Yun%20Gu%20and%20Heinz%20Handels%20and%20Satoshi%20Kasai%20and%20Satoshi%20Kondo%20and%20Klaus%20Maier-Hein%20and%20Julia%20A.%20Schnabel%20and%20Guotai%20Wang%20and%20Litingyu%20Wang%20and%20Tassilo%20Wald%20and%20Guang-Zhong%20Yang%20and%20Hanxiao%20Zhang%20and%20Minghui%20Zhang%20and%20Steve%20Pieper%20and%20Gordon%20Harris%20and%20Ron%20Kikinis%20and%20Tina%20Kapur&entry.1292438233=%20%20Accurate%20assessment%20of%20lymph%20node%20size%20in%203D%20CT%20scans%20is%20crucial%20for%20cancer%0Astaging%2C%20therapeutic%20management%2C%20and%20monitoring%20treatment%20response.%20Existing%0Astate-of-the-art%20segmentation%20frameworks%20in%20medical%20imaging%20often%20rely%20on%20fully%0Aannotated%20datasets.%20However%2C%20for%20lymph%20node%20segmentation%2C%20these%20datasets%20are%0Atypically%20small%20due%20to%20the%20extensive%20time%20and%20expertise%20required%20to%20annotate%0Athe%20numerous%20lymph%20nodes%20in%203D%20CT%20scans.%20Weakly-supervised%20learning%2C%20which%0Aleverages%20incomplete%20or%20noisy%20annotations%2C%20has%20recently%20gained%20interest%20in%20the%0Amedical%20imaging%20community%20as%20a%20potential%20solution.%20Despite%20the%20variety%20of%0Aweakly-supervised%20techniques%20proposed%2C%20most%20have%20been%20validated%20only%20on%20private%0Adatasets%20or%20small%20publicly%20available%20datasets.%20To%20address%20this%20limitation%2C%20the%0AMediastinal%20Lymph%20Node%20Quantification%20%28LNQ%29%20challenge%20was%20organized%20in%0Aconjunction%20with%20the%2026th%20International%20Conference%20on%20Medical%20Image%20Computing%0Aand%20Computer%20Assisted%20Intervention%20%28MICCAI%202023%29.%20This%20challenge%20aimed%20to%0Aadvance%20weakly-supervised%20segmentation%20methods%20by%20providing%20a%20new%2C%20partially%0Aannotated%20dataset%20and%20a%20robust%20evaluation%20framework.%20A%20total%20of%2016%20teams%20from%205%0Acountries%20submitted%20predictions%20to%20the%20validation%20leaderboard%2C%20and%206%20teams%20from%0A3%20countries%20participated%20in%20the%20evaluation%20phase.%20The%20results%20highlighted%20both%0Athe%20potential%20and%20the%20current%20limitations%20of%20weakly-supervised%20approaches.%20On%0Aone%20hand%2C%20weakly-supervised%20approaches%20obtained%20relatively%20good%20performance%0Awith%20a%20median%20Dice%20score%20of%20%2461.0%5C%25%24.%20On%20the%20other%20hand%2C%20top-ranked%20teams%2C%20with%0Aa%20median%20Dice%20score%20exceeding%20%2470%5C%25%24%2C%20boosted%20their%20performance%20by%20leveraging%0Asmaller%20but%20fully%20annotated%20datasets%20to%20combine%20weak%20supervision%20and%20full%0Asupervision.%20This%20highlights%20both%20the%20promise%20of%20weakly-supervised%20methods%20and%0Athe%20ongoing%20need%20for%20high-quality%2C%20fully%20annotated%20data%20to%20achieve%20higher%0Asegmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10069v1&entry.124074799=Read"},
{"title": "In-Context Learning with Representations: Contextual Generalization of\n  Trained Transformers", "author": "Tong Yang and Yu Huang and Yingbin Liang and Yuejie Chi", "abstract": "  In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs.\n", "link": "http://arxiv.org/abs/2408.10147v1", "date": "2024-08-19", "relevancy": 2.0237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5136}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20with%20Representations%3A%20Contextual%20Generalization%20of%0A%20%20Trained%20Transformers&body=Title%3A%20In-Context%20Learning%20with%20Representations%3A%20Contextual%20Generalization%20of%0A%20%20Trained%20Transformers%0AAuthor%3A%20Tong%20Yang%20and%20Yu%20Huang%20and%20Yingbin%20Liang%20and%20Yuejie%20Chi%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20refers%20to%20a%20remarkable%20capability%20of%20pretrained%0Alarge%20language%20models%2C%20which%20can%20learn%20a%20new%20task%20given%20a%20few%20examples%20during%0Ainference.%20However%2C%20theoretical%20understanding%20of%20ICL%20is%20largely%20under-explored%2C%0Aparticularly%20whether%20transformers%20can%20be%20trained%20to%20generalize%20to%20unseen%0Aexamples%20in%20a%20prompt%2C%20which%20will%20require%20the%20model%20to%20acquire%20contextual%0Aknowledge%20of%20the%20prompt%20for%20generalization.%20This%20paper%20investigates%20the%0Atraining%20dynamics%20of%20transformers%20by%20gradient%20descent%20through%20the%20lens%20of%0Anon-linear%20regression%20tasks.%20The%20contextual%20generalization%20here%20can%20be%20attained%0Avia%20learning%20the%20template%20function%20for%20each%20task%20in-context%2C%20where%20all%20template%0Afunctions%20lie%20in%20a%20linear%20space%20with%20%24m%24%20basis%20functions.%20We%20analyze%20the%0Atraining%20dynamics%20of%20one-layer%20multi-head%20transformers%20to%20in-contextly%20predict%0Aunlabeled%20inputs%20given%20partially%20labeled%20prompts%2C%20where%20the%20labels%20contain%0AGaussian%20noise%20and%20the%20number%20of%20examples%20in%20each%20prompt%20are%20not%20sufficient%20to%0Adetermine%20the%20template.%20Under%20mild%20assumptions%2C%20we%20show%20that%20the%20training%20loss%0Afor%20a%20one-layer%20multi-head%20transformer%20converges%20linearly%20to%20a%20global%20minimum.%0AMoreover%2C%20the%20transformer%20effectively%20learns%20to%20perform%20ridge%20regression%20over%0Athe%20basis%20functions.%20To%20our%20knowledge%2C%20this%20study%20is%20the%20first%20provable%0Ademonstration%20that%20transformers%20can%20learn%20contextual%20%28i.e.%2C%20template%29%0Ainformation%20to%20generalize%20to%20both%20unseen%20examples%20and%20tasks%20when%20prompts%0Acontain%20only%20a%20small%20number%20of%20query-answer%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Learning%2520with%2520Representations%253A%2520Contextual%2520Generalization%2520of%250A%2520%2520Trained%2520Transformers%26entry.906535625%3DTong%2520Yang%2520and%2520Yu%2520Huang%2520and%2520Yingbin%2520Liang%2520and%2520Yuejie%2520Chi%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520refers%2520to%2520a%2520remarkable%2520capability%2520of%2520pretrained%250Alarge%2520language%2520models%252C%2520which%2520can%2520learn%2520a%2520new%2520task%2520given%2520a%2520few%2520examples%2520during%250Ainference.%2520However%252C%2520theoretical%2520understanding%2520of%2520ICL%2520is%2520largely%2520under-explored%252C%250Aparticularly%2520whether%2520transformers%2520can%2520be%2520trained%2520to%2520generalize%2520to%2520unseen%250Aexamples%2520in%2520a%2520prompt%252C%2520which%2520will%2520require%2520the%2520model%2520to%2520acquire%2520contextual%250Aknowledge%2520of%2520the%2520prompt%2520for%2520generalization.%2520This%2520paper%2520investigates%2520the%250Atraining%2520dynamics%2520of%2520transformers%2520by%2520gradient%2520descent%2520through%2520the%2520lens%2520of%250Anon-linear%2520regression%2520tasks.%2520The%2520contextual%2520generalization%2520here%2520can%2520be%2520attained%250Avia%2520learning%2520the%2520template%2520function%2520for%2520each%2520task%2520in-context%252C%2520where%2520all%2520template%250Afunctions%2520lie%2520in%2520a%2520linear%2520space%2520with%2520%2524m%2524%2520basis%2520functions.%2520We%2520analyze%2520the%250Atraining%2520dynamics%2520of%2520one-layer%2520multi-head%2520transformers%2520to%2520in-contextly%2520predict%250Aunlabeled%2520inputs%2520given%2520partially%2520labeled%2520prompts%252C%2520where%2520the%2520labels%2520contain%250AGaussian%2520noise%2520and%2520the%2520number%2520of%2520examples%2520in%2520each%2520prompt%2520are%2520not%2520sufficient%2520to%250Adetermine%2520the%2520template.%2520Under%2520mild%2520assumptions%252C%2520we%2520show%2520that%2520the%2520training%2520loss%250Afor%2520a%2520one-layer%2520multi-head%2520transformer%2520converges%2520linearly%2520to%2520a%2520global%2520minimum.%250AMoreover%252C%2520the%2520transformer%2520effectively%2520learns%2520to%2520perform%2520ridge%2520regression%2520over%250Athe%2520basis%2520functions.%2520To%2520our%2520knowledge%252C%2520this%2520study%2520is%2520the%2520first%2520provable%250Ademonstration%2520that%2520transformers%2520can%2520learn%2520contextual%2520%2528i.e.%252C%2520template%2529%250Ainformation%2520to%2520generalize%2520to%2520both%2520unseen%2520examples%2520and%2520tasks%2520when%2520prompts%250Acontain%2520only%2520a%2520small%2520number%2520of%2520query-answer%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20with%20Representations%3A%20Contextual%20Generalization%20of%0A%20%20Trained%20Transformers&entry.906535625=Tong%20Yang%20and%20Yu%20Huang%20and%20Yingbin%20Liang%20and%20Yuejie%20Chi&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20refers%20to%20a%20remarkable%20capability%20of%20pretrained%0Alarge%20language%20models%2C%20which%20can%20learn%20a%20new%20task%20given%20a%20few%20examples%20during%0Ainference.%20However%2C%20theoretical%20understanding%20of%20ICL%20is%20largely%20under-explored%2C%0Aparticularly%20whether%20transformers%20can%20be%20trained%20to%20generalize%20to%20unseen%0Aexamples%20in%20a%20prompt%2C%20which%20will%20require%20the%20model%20to%20acquire%20contextual%0Aknowledge%20of%20the%20prompt%20for%20generalization.%20This%20paper%20investigates%20the%0Atraining%20dynamics%20of%20transformers%20by%20gradient%20descent%20through%20the%20lens%20of%0Anon-linear%20regression%20tasks.%20The%20contextual%20generalization%20here%20can%20be%20attained%0Avia%20learning%20the%20template%20function%20for%20each%20task%20in-context%2C%20where%20all%20template%0Afunctions%20lie%20in%20a%20linear%20space%20with%20%24m%24%20basis%20functions.%20We%20analyze%20the%0Atraining%20dynamics%20of%20one-layer%20multi-head%20transformers%20to%20in-contextly%20predict%0Aunlabeled%20inputs%20given%20partially%20labeled%20prompts%2C%20where%20the%20labels%20contain%0AGaussian%20noise%20and%20the%20number%20of%20examples%20in%20each%20prompt%20are%20not%20sufficient%20to%0Adetermine%20the%20template.%20Under%20mild%20assumptions%2C%20we%20show%20that%20the%20training%20loss%0Afor%20a%20one-layer%20multi-head%20transformer%20converges%20linearly%20to%20a%20global%20minimum.%0AMoreover%2C%20the%20transformer%20effectively%20learns%20to%20perform%20ridge%20regression%20over%0Athe%20basis%20functions.%20To%20our%20knowledge%2C%20this%20study%20is%20the%20first%20provable%0Ademonstration%20that%20transformers%20can%20learn%20contextual%20%28i.e.%2C%20template%29%0Ainformation%20to%20generalize%20to%20both%20unseen%20examples%20and%20tasks%20when%20prompts%0Acontain%20only%20a%20small%20number%20of%20query-answer%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10147v1&entry.124074799=Read"},
{"title": "Towards Quantum Federated Learning", "author": "Chao Ren and Rudai Yan and Huihui Zhu and Han Yu and Minrui Xu and Yuan Shen and Yan Xu and Ming Xiao and Zhao Yang Dong and Mikael Skoglund and Dusit Niyato and Leong Chuan Kwek", "abstract": "  Quantum Federated Learning (QFL) is an emerging interdisciplinary field that\nmerges the principles of Quantum Computing (QC) and Federated Learning (FL),\nwith the goal of leveraging quantum technologies to enhance privacy, security,\nand efficiency in the learning process. Currently, there is no comprehensive\nsurvey for this interdisciplinary field. This review offers a thorough,\nholistic examination of QFL. We aim to provide a comprehensive understanding of\nthe principles, techniques, and emerging applications of QFL. We discuss the\ncurrent state of research in this rapidly evolving field, identify challenges\nand opportunities associated with integrating these technologies, and outline\nfuture directions and open research questions. We propose a unique taxonomy of\nQFL techniques, categorized according to their characteristics and the quantum\ntechniques employed. As the field of QFL continues to progress, we can\nanticipate further breakthroughs and applications across various industries,\ndriving innovation and addressing challenges related to data privacy, security,\nand resource optimization. This review serves as a first-of-its-kind\ncomprehensive guide for researchers and practitioners interested in\nunderstanding and advancing the field of QFL.\n", "link": "http://arxiv.org/abs/2306.09912v4", "date": "2024-08-19", "relevancy": 2.0203, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4186}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4015}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Quantum%20Federated%20Learning&body=Title%3A%20Towards%20Quantum%20Federated%20Learning%0AAuthor%3A%20Chao%20Ren%20and%20Rudai%20Yan%20and%20Huihui%20Zhu%20and%20Han%20Yu%20and%20Minrui%20Xu%20and%20Yuan%20Shen%20and%20Yan%20Xu%20and%20Ming%20Xiao%20and%20Zhao%20Yang%20Dong%20and%20Mikael%20Skoglund%20and%20Dusit%20Niyato%20and%20Leong%20Chuan%20Kwek%0AAbstract%3A%20%20%20Quantum%20Federated%20Learning%20%28QFL%29%20is%20an%20emerging%20interdisciplinary%20field%20that%0Amerges%20the%20principles%20of%20Quantum%20Computing%20%28QC%29%20and%20Federated%20Learning%20%28FL%29%2C%0Awith%20the%20goal%20of%20leveraging%20quantum%20technologies%20to%20enhance%20privacy%2C%20security%2C%0Aand%20efficiency%20in%20the%20learning%20process.%20Currently%2C%20there%20is%20no%20comprehensive%0Asurvey%20for%20this%20interdisciplinary%20field.%20This%20review%20offers%20a%20thorough%2C%0Aholistic%20examination%20of%20QFL.%20We%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%0Athe%20principles%2C%20techniques%2C%20and%20emerging%20applications%20of%20QFL.%20We%20discuss%20the%0Acurrent%20state%20of%20research%20in%20this%20rapidly%20evolving%20field%2C%20identify%20challenges%0Aand%20opportunities%20associated%20with%20integrating%20these%20technologies%2C%20and%20outline%0Afuture%20directions%20and%20open%20research%20questions.%20We%20propose%20a%20unique%20taxonomy%20of%0AQFL%20techniques%2C%20categorized%20according%20to%20their%20characteristics%20and%20the%20quantum%0Atechniques%20employed.%20As%20the%20field%20of%20QFL%20continues%20to%20progress%2C%20we%20can%0Aanticipate%20further%20breakthroughs%20and%20applications%20across%20various%20industries%2C%0Adriving%20innovation%20and%20addressing%20challenges%20related%20to%20data%20privacy%2C%20security%2C%0Aand%20resource%20optimization.%20This%20review%20serves%20as%20a%20first-of-its-kind%0Acomprehensive%20guide%20for%20researchers%20and%20practitioners%20interested%20in%0Aunderstanding%20and%20advancing%20the%20field%20of%20QFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09912v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Quantum%2520Federated%2520Learning%26entry.906535625%3DChao%2520Ren%2520and%2520Rudai%2520Yan%2520and%2520Huihui%2520Zhu%2520and%2520Han%2520Yu%2520and%2520Minrui%2520Xu%2520and%2520Yuan%2520Shen%2520and%2520Yan%2520Xu%2520and%2520Ming%2520Xiao%2520and%2520Zhao%2520Yang%2520Dong%2520and%2520Mikael%2520Skoglund%2520and%2520Dusit%2520Niyato%2520and%2520Leong%2520Chuan%2520Kwek%26entry.1292438233%3D%2520%2520Quantum%2520Federated%2520Learning%2520%2528QFL%2529%2520is%2520an%2520emerging%2520interdisciplinary%2520field%2520that%250Amerges%2520the%2520principles%2520of%2520Quantum%2520Computing%2520%2528QC%2529%2520and%2520Federated%2520Learning%2520%2528FL%2529%252C%250Awith%2520the%2520goal%2520of%2520leveraging%2520quantum%2520technologies%2520to%2520enhance%2520privacy%252C%2520security%252C%250Aand%2520efficiency%2520in%2520the%2520learning%2520process.%2520Currently%252C%2520there%2520is%2520no%2520comprehensive%250Asurvey%2520for%2520this%2520interdisciplinary%2520field.%2520This%2520review%2520offers%2520a%2520thorough%252C%250Aholistic%2520examination%2520of%2520QFL.%2520We%2520aim%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%250Athe%2520principles%252C%2520techniques%252C%2520and%2520emerging%2520applications%2520of%2520QFL.%2520We%2520discuss%2520the%250Acurrent%2520state%2520of%2520research%2520in%2520this%2520rapidly%2520evolving%2520field%252C%2520identify%2520challenges%250Aand%2520opportunities%2520associated%2520with%2520integrating%2520these%2520technologies%252C%2520and%2520outline%250Afuture%2520directions%2520and%2520open%2520research%2520questions.%2520We%2520propose%2520a%2520unique%2520taxonomy%2520of%250AQFL%2520techniques%252C%2520categorized%2520according%2520to%2520their%2520characteristics%2520and%2520the%2520quantum%250Atechniques%2520employed.%2520As%2520the%2520field%2520of%2520QFL%2520continues%2520to%2520progress%252C%2520we%2520can%250Aanticipate%2520further%2520breakthroughs%2520and%2520applications%2520across%2520various%2520industries%252C%250Adriving%2520innovation%2520and%2520addressing%2520challenges%2520related%2520to%2520data%2520privacy%252C%2520security%252C%250Aand%2520resource%2520optimization.%2520This%2520review%2520serves%2520as%2520a%2520first-of-its-kind%250Acomprehensive%2520guide%2520for%2520researchers%2520and%2520practitioners%2520interested%2520in%250Aunderstanding%2520and%2520advancing%2520the%2520field%2520of%2520QFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09912v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Quantum%20Federated%20Learning&entry.906535625=Chao%20Ren%20and%20Rudai%20Yan%20and%20Huihui%20Zhu%20and%20Han%20Yu%20and%20Minrui%20Xu%20and%20Yuan%20Shen%20and%20Yan%20Xu%20and%20Ming%20Xiao%20and%20Zhao%20Yang%20Dong%20and%20Mikael%20Skoglund%20and%20Dusit%20Niyato%20and%20Leong%20Chuan%20Kwek&entry.1292438233=%20%20Quantum%20Federated%20Learning%20%28QFL%29%20is%20an%20emerging%20interdisciplinary%20field%20that%0Amerges%20the%20principles%20of%20Quantum%20Computing%20%28QC%29%20and%20Federated%20Learning%20%28FL%29%2C%0Awith%20the%20goal%20of%20leveraging%20quantum%20technologies%20to%20enhance%20privacy%2C%20security%2C%0Aand%20efficiency%20in%20the%20learning%20process.%20Currently%2C%20there%20is%20no%20comprehensive%0Asurvey%20for%20this%20interdisciplinary%20field.%20This%20review%20offers%20a%20thorough%2C%0Aholistic%20examination%20of%20QFL.%20We%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%0Athe%20principles%2C%20techniques%2C%20and%20emerging%20applications%20of%20QFL.%20We%20discuss%20the%0Acurrent%20state%20of%20research%20in%20this%20rapidly%20evolving%20field%2C%20identify%20challenges%0Aand%20opportunities%20associated%20with%20integrating%20these%20technologies%2C%20and%20outline%0Afuture%20directions%20and%20open%20research%20questions.%20We%20propose%20a%20unique%20taxonomy%20of%0AQFL%20techniques%2C%20categorized%20according%20to%20their%20characteristics%20and%20the%20quantum%0Atechniques%20employed.%20As%20the%20field%20of%20QFL%20continues%20to%20progress%2C%20we%20can%0Aanticipate%20further%20breakthroughs%20and%20applications%20across%20various%20industries%2C%0Adriving%20innovation%20and%20addressing%20challenges%20related%20to%20data%20privacy%2C%20security%2C%0Aand%20resource%20optimization.%20This%20review%20serves%20as%20a%20first-of-its-kind%0Acomprehensive%20guide%20for%20researchers%20and%20practitioners%20interested%20in%0Aunderstanding%20and%20advancing%20the%20field%20of%20QFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09912v4&entry.124074799=Read"},
{"title": "Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with\n  Texture Map-Based Weak Supervision", "author": "Junho Moon and Haejun Chung and Ikbeom Jang", "abstract": "  Facial wrinkle detection plays a crucial role in cosmetic dermatology.\nPrecise manual segmentation of facial wrinkles is challenging and\ntime-consuming, with inherent subjectivity leading to inconsistent results\namong graders. To address this issue, we propose two solutions. First, we build\nand release the first public facial wrinkle dataset, `FFHQ-Wrinkle', an\nextension of the NVIDIA FFHQ dataset. This dataset includes 1,000 images with\nhuman labels and 50,000 images with automatically generated weak labels. This\ndataset can foster the research community to develop advanced wrinkle detection\nalgorithms. Second, we introduce a training strategy for U-Net-like\nencoder-decoder models to detect wrinkles across the face automatically. Our\nmethod employs a two-stage training strategy: texture map pretraining and\nfinetuning on human-labeled data. Initially, we pretrain models on a large\ndataset with weak labels (N=50k) or masked texture maps generated through\ncomputer vision techniques, without human intervention. Subsequently, we\nfinetune the models using human-labeled data (N=1k), which consists of manually\nlabeled wrinkle masks. During finetuning, the network inputs a combination of\nRGB and masked texture maps, comprising four channels. We effectively combine\nlabels from multiple annotators to minimize subjectivity in manual labeling.\nOur strategies demonstrate improved segmentation performance in facial wrinkle\nsegmentation both quantitatively and visually compared to existing pretraining\nmethods.\n", "link": "http://arxiv.org/abs/2408.10060v1", "date": "2024-08-19", "relevancy": 2.0189, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5128}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5043}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial%20Wrinkle%20Segmentation%20for%20Cosmetic%20Dermatology%3A%20Pretraining%20with%0A%20%20Texture%20Map-Based%20Weak%20Supervision&body=Title%3A%20Facial%20Wrinkle%20Segmentation%20for%20Cosmetic%20Dermatology%3A%20Pretraining%20with%0A%20%20Texture%20Map-Based%20Weak%20Supervision%0AAuthor%3A%20Junho%20Moon%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang%0AAbstract%3A%20%20%20Facial%20wrinkle%20detection%20plays%20a%20crucial%20role%20in%20cosmetic%20dermatology.%0APrecise%20manual%20segmentation%20of%20facial%20wrinkles%20is%20challenging%20and%0Atime-consuming%2C%20with%20inherent%20subjectivity%20leading%20to%20inconsistent%20results%0Aamong%20graders.%20To%20address%20this%20issue%2C%20we%20propose%20two%20solutions.%20First%2C%20we%20build%0Aand%20release%20the%20first%20public%20facial%20wrinkle%20dataset%2C%20%60FFHQ-Wrinkle%27%2C%20an%0Aextension%20of%20the%20NVIDIA%20FFHQ%20dataset.%20This%20dataset%20includes%201%2C000%20images%20with%0Ahuman%20labels%20and%2050%2C000%20images%20with%20automatically%20generated%20weak%20labels.%20This%0Adataset%20can%20foster%20the%20research%20community%20to%20develop%20advanced%20wrinkle%20detection%0Aalgorithms.%20Second%2C%20we%20introduce%20a%20training%20strategy%20for%20U-Net-like%0Aencoder-decoder%20models%20to%20detect%20wrinkles%20across%20the%20face%20automatically.%20Our%0Amethod%20employs%20a%20two-stage%20training%20strategy%3A%20texture%20map%20pretraining%20and%0Afinetuning%20on%20human-labeled%20data.%20Initially%2C%20we%20pretrain%20models%20on%20a%20large%0Adataset%20with%20weak%20labels%20%28N%3D50k%29%20or%20masked%20texture%20maps%20generated%20through%0Acomputer%20vision%20techniques%2C%20without%20human%20intervention.%20Subsequently%2C%20we%0Afinetune%20the%20models%20using%20human-labeled%20data%20%28N%3D1k%29%2C%20which%20consists%20of%20manually%0Alabeled%20wrinkle%20masks.%20During%20finetuning%2C%20the%20network%20inputs%20a%20combination%20of%0ARGB%20and%20masked%20texture%20maps%2C%20comprising%20four%20channels.%20We%20effectively%20combine%0Alabels%20from%20multiple%20annotators%20to%20minimize%20subjectivity%20in%20manual%20labeling.%0AOur%20strategies%20demonstrate%20improved%20segmentation%20performance%20in%20facial%20wrinkle%0Asegmentation%20both%20quantitatively%20and%20visually%20compared%20to%20existing%20pretraining%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial%2520Wrinkle%2520Segmentation%2520for%2520Cosmetic%2520Dermatology%253A%2520Pretraining%2520with%250A%2520%2520Texture%2520Map-Based%2520Weak%2520Supervision%26entry.906535625%3DJunho%2520Moon%2520and%2520Haejun%2520Chung%2520and%2520Ikbeom%2520Jang%26entry.1292438233%3D%2520%2520Facial%2520wrinkle%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520cosmetic%2520dermatology.%250APrecise%2520manual%2520segmentation%2520of%2520facial%2520wrinkles%2520is%2520challenging%2520and%250Atime-consuming%252C%2520with%2520inherent%2520subjectivity%2520leading%2520to%2520inconsistent%2520results%250Aamong%2520graders.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520two%2520solutions.%2520First%252C%2520we%2520build%250Aand%2520release%2520the%2520first%2520public%2520facial%2520wrinkle%2520dataset%252C%2520%2560FFHQ-Wrinkle%2527%252C%2520an%250Aextension%2520of%2520the%2520NVIDIA%2520FFHQ%2520dataset.%2520This%2520dataset%2520includes%25201%252C000%2520images%2520with%250Ahuman%2520labels%2520and%252050%252C000%2520images%2520with%2520automatically%2520generated%2520weak%2520labels.%2520This%250Adataset%2520can%2520foster%2520the%2520research%2520community%2520to%2520develop%2520advanced%2520wrinkle%2520detection%250Aalgorithms.%2520Second%252C%2520we%2520introduce%2520a%2520training%2520strategy%2520for%2520U-Net-like%250Aencoder-decoder%2520models%2520to%2520detect%2520wrinkles%2520across%2520the%2520face%2520automatically.%2520Our%250Amethod%2520employs%2520a%2520two-stage%2520training%2520strategy%253A%2520texture%2520map%2520pretraining%2520and%250Afinetuning%2520on%2520human-labeled%2520data.%2520Initially%252C%2520we%2520pretrain%2520models%2520on%2520a%2520large%250Adataset%2520with%2520weak%2520labels%2520%2528N%253D50k%2529%2520or%2520masked%2520texture%2520maps%2520generated%2520through%250Acomputer%2520vision%2520techniques%252C%2520without%2520human%2520intervention.%2520Subsequently%252C%2520we%250Afinetune%2520the%2520models%2520using%2520human-labeled%2520data%2520%2528N%253D1k%2529%252C%2520which%2520consists%2520of%2520manually%250Alabeled%2520wrinkle%2520masks.%2520During%2520finetuning%252C%2520the%2520network%2520inputs%2520a%2520combination%2520of%250ARGB%2520and%2520masked%2520texture%2520maps%252C%2520comprising%2520four%2520channels.%2520We%2520effectively%2520combine%250Alabels%2520from%2520multiple%2520annotators%2520to%2520minimize%2520subjectivity%2520in%2520manual%2520labeling.%250AOur%2520strategies%2520demonstrate%2520improved%2520segmentation%2520performance%2520in%2520facial%2520wrinkle%250Asegmentation%2520both%2520quantitatively%2520and%2520visually%2520compared%2520to%2520existing%2520pretraining%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial%20Wrinkle%20Segmentation%20for%20Cosmetic%20Dermatology%3A%20Pretraining%20with%0A%20%20Texture%20Map-Based%20Weak%20Supervision&entry.906535625=Junho%20Moon%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang&entry.1292438233=%20%20Facial%20wrinkle%20detection%20plays%20a%20crucial%20role%20in%20cosmetic%20dermatology.%0APrecise%20manual%20segmentation%20of%20facial%20wrinkles%20is%20challenging%20and%0Atime-consuming%2C%20with%20inherent%20subjectivity%20leading%20to%20inconsistent%20results%0Aamong%20graders.%20To%20address%20this%20issue%2C%20we%20propose%20two%20solutions.%20First%2C%20we%20build%0Aand%20release%20the%20first%20public%20facial%20wrinkle%20dataset%2C%20%60FFHQ-Wrinkle%27%2C%20an%0Aextension%20of%20the%20NVIDIA%20FFHQ%20dataset.%20This%20dataset%20includes%201%2C000%20images%20with%0Ahuman%20labels%20and%2050%2C000%20images%20with%20automatically%20generated%20weak%20labels.%20This%0Adataset%20can%20foster%20the%20research%20community%20to%20develop%20advanced%20wrinkle%20detection%0Aalgorithms.%20Second%2C%20we%20introduce%20a%20training%20strategy%20for%20U-Net-like%0Aencoder-decoder%20models%20to%20detect%20wrinkles%20across%20the%20face%20automatically.%20Our%0Amethod%20employs%20a%20two-stage%20training%20strategy%3A%20texture%20map%20pretraining%20and%0Afinetuning%20on%20human-labeled%20data.%20Initially%2C%20we%20pretrain%20models%20on%20a%20large%0Adataset%20with%20weak%20labels%20%28N%3D50k%29%20or%20masked%20texture%20maps%20generated%20through%0Acomputer%20vision%20techniques%2C%20without%20human%20intervention.%20Subsequently%2C%20we%0Afinetune%20the%20models%20using%20human-labeled%20data%20%28N%3D1k%29%2C%20which%20consists%20of%20manually%0Alabeled%20wrinkle%20masks.%20During%20finetuning%2C%20the%20network%20inputs%20a%20combination%20of%0ARGB%20and%20masked%20texture%20maps%2C%20comprising%20four%20channels.%20We%20effectively%20combine%0Alabels%20from%20multiple%20annotators%20to%20minimize%20subjectivity%20in%20manual%20labeling.%0AOur%20strategies%20demonstrate%20improved%20segmentation%20performance%20in%20facial%20wrinkle%0Asegmentation%20both%20quantitatively%20and%20visually%20compared%20to%20existing%20pretraining%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10060v1&entry.124074799=Read"},
{"title": "LCE: A Framework for Explainability of DNNs for Ultrasound Image Based\n  on Concept Discovery", "author": "Weiji Kong and Xun Gong and Juan Wang", "abstract": "  Explaining the decisions of Deep Neural Networks (DNNs) for medical images\nhas become increasingly important. Existing attribution methods have difficulty\nexplaining the meaning of pixels while existing concept-based methods are\nlimited by additional annotations or specific model structures that are\ndifficult to apply to ultrasound images. In this paper, we propose the Lesion\nConcept Explainer (LCE) framework, which combines attribution methods with\nconcept-based methods. We introduce the Segment Anything Model (SAM),\nfine-tuned on a large number of medical images, for concept discovery to enable\na meaningful explanation of ultrasound image DNNs. The proposed framework is\nevaluated in terms of both faithfulness and understandability. We point out\ndeficiencies in the popular faithfulness evaluation metrics and propose a new\nevaluation metric. Our evaluation of public and private breast ultrasound\ndatasets (BUSI and FG-US-B) shows that LCE performs well compared to\ncommonly-used explainability methods. Finally, we also validate that LCE can\nconsistently provide reliable explanations for more meaningful fine-grained\ndiagnostic tasks in breast ultrasound.\n", "link": "http://arxiv.org/abs/2408.09899v1", "date": "2024-08-19", "relevancy": 2.0171, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCE%3A%20A%20Framework%20for%20Explainability%20of%20DNNs%20for%20Ultrasound%20Image%20Based%0A%20%20on%20Concept%20Discovery&body=Title%3A%20LCE%3A%20A%20Framework%20for%20Explainability%20of%20DNNs%20for%20Ultrasound%20Image%20Based%0A%20%20on%20Concept%20Discovery%0AAuthor%3A%20Weiji%20Kong%20and%20Xun%20Gong%20and%20Juan%20Wang%0AAbstract%3A%20%20%20Explaining%20the%20decisions%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20for%20medical%20images%0Ahas%20become%20increasingly%20important.%20Existing%20attribution%20methods%20have%20difficulty%0Aexplaining%20the%20meaning%20of%20pixels%20while%20existing%20concept-based%20methods%20are%0Alimited%20by%20additional%20annotations%20or%20specific%20model%20structures%20that%20are%0Adifficult%20to%20apply%20to%20ultrasound%20images.%20In%20this%20paper%2C%20we%20propose%20the%20Lesion%0AConcept%20Explainer%20%28LCE%29%20framework%2C%20which%20combines%20attribution%20methods%20with%0Aconcept-based%20methods.%20We%20introduce%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%0Afine-tuned%20on%20a%20large%20number%20of%20medical%20images%2C%20for%20concept%20discovery%20to%20enable%0Aa%20meaningful%20explanation%20of%20ultrasound%20image%20DNNs.%20The%20proposed%20framework%20is%0Aevaluated%20in%20terms%20of%20both%20faithfulness%20and%20understandability.%20We%20point%20out%0Adeficiencies%20in%20the%20popular%20faithfulness%20evaluation%20metrics%20and%20propose%20a%20new%0Aevaluation%20metric.%20Our%20evaluation%20of%20public%20and%20private%20breast%20ultrasound%0Adatasets%20%28BUSI%20and%20FG-US-B%29%20shows%20that%20LCE%20performs%20well%20compared%20to%0Acommonly-used%20explainability%20methods.%20Finally%2C%20we%20also%20validate%20that%20LCE%20can%0Aconsistently%20provide%20reliable%20explanations%20for%20more%20meaningful%20fine-grained%0Adiagnostic%20tasks%20in%20breast%20ultrasound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCE%253A%2520A%2520Framework%2520for%2520Explainability%2520of%2520DNNs%2520for%2520Ultrasound%2520Image%2520Based%250A%2520%2520on%2520Concept%2520Discovery%26entry.906535625%3DWeiji%2520Kong%2520and%2520Xun%2520Gong%2520and%2520Juan%2520Wang%26entry.1292438233%3D%2520%2520Explaining%2520the%2520decisions%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520for%2520medical%2520images%250Ahas%2520become%2520increasingly%2520important.%2520Existing%2520attribution%2520methods%2520have%2520difficulty%250Aexplaining%2520the%2520meaning%2520of%2520pixels%2520while%2520existing%2520concept-based%2520methods%2520are%250Alimited%2520by%2520additional%2520annotations%2520or%2520specific%2520model%2520structures%2520that%2520are%250Adifficult%2520to%2520apply%2520to%2520ultrasound%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Lesion%250AConcept%2520Explainer%2520%2528LCE%2529%2520framework%252C%2520which%2520combines%2520attribution%2520methods%2520with%250Aconcept-based%2520methods.%2520We%2520introduce%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%250Afine-tuned%2520on%2520a%2520large%2520number%2520of%2520medical%2520images%252C%2520for%2520concept%2520discovery%2520to%2520enable%250Aa%2520meaningful%2520explanation%2520of%2520ultrasound%2520image%2520DNNs.%2520The%2520proposed%2520framework%2520is%250Aevaluated%2520in%2520terms%2520of%2520both%2520faithfulness%2520and%2520understandability.%2520We%2520point%2520out%250Adeficiencies%2520in%2520the%2520popular%2520faithfulness%2520evaluation%2520metrics%2520and%2520propose%2520a%2520new%250Aevaluation%2520metric.%2520Our%2520evaluation%2520of%2520public%2520and%2520private%2520breast%2520ultrasound%250Adatasets%2520%2528BUSI%2520and%2520FG-US-B%2529%2520shows%2520that%2520LCE%2520performs%2520well%2520compared%2520to%250Acommonly-used%2520explainability%2520methods.%2520Finally%252C%2520we%2520also%2520validate%2520that%2520LCE%2520can%250Aconsistently%2520provide%2520reliable%2520explanations%2520for%2520more%2520meaningful%2520fine-grained%250Adiagnostic%2520tasks%2520in%2520breast%2520ultrasound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCE%3A%20A%20Framework%20for%20Explainability%20of%20DNNs%20for%20Ultrasound%20Image%20Based%0A%20%20on%20Concept%20Discovery&entry.906535625=Weiji%20Kong%20and%20Xun%20Gong%20and%20Juan%20Wang&entry.1292438233=%20%20Explaining%20the%20decisions%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20for%20medical%20images%0Ahas%20become%20increasingly%20important.%20Existing%20attribution%20methods%20have%20difficulty%0Aexplaining%20the%20meaning%20of%20pixels%20while%20existing%20concept-based%20methods%20are%0Alimited%20by%20additional%20annotations%20or%20specific%20model%20structures%20that%20are%0Adifficult%20to%20apply%20to%20ultrasound%20images.%20In%20this%20paper%2C%20we%20propose%20the%20Lesion%0AConcept%20Explainer%20%28LCE%29%20framework%2C%20which%20combines%20attribution%20methods%20with%0Aconcept-based%20methods.%20We%20introduce%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%0Afine-tuned%20on%20a%20large%20number%20of%20medical%20images%2C%20for%20concept%20discovery%20to%20enable%0Aa%20meaningful%20explanation%20of%20ultrasound%20image%20DNNs.%20The%20proposed%20framework%20is%0Aevaluated%20in%20terms%20of%20both%20faithfulness%20and%20understandability.%20We%20point%20out%0Adeficiencies%20in%20the%20popular%20faithfulness%20evaluation%20metrics%20and%20propose%20a%20new%0Aevaluation%20metric.%20Our%20evaluation%20of%20public%20and%20private%20breast%20ultrasound%0Adatasets%20%28BUSI%20and%20FG-US-B%29%20shows%20that%20LCE%20performs%20well%20compared%20to%0Acommonly-used%20explainability%20methods.%20Finally%2C%20we%20also%20validate%20that%20LCE%20can%0Aconsistently%20provide%20reliable%20explanations%20for%20more%20meaningful%20fine-grained%0Adiagnostic%20tasks%20in%20breast%20ultrasound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09899v1&entry.124074799=Read"},
{"title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving", "author": "Mehdi Azarafza and Mojtaba Nayyeri and Charles Steinmetz and Steffen Staab and Achim Rettberg", "abstract": "  Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.\n", "link": "http://arxiv.org/abs/2402.13602v4", "date": "2024-08-19", "relevancy": 2.0088, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Reasoning%20Based%20on%20Large%20Language%20Models%20for%20Autonomous%20Car%0A%20%20Driving&body=Title%3A%20Hybrid%20Reasoning%20Based%20on%20Large%20Language%20Models%20for%20Autonomous%20Car%0A%20%20Driving%0AAuthor%3A%20Mehdi%20Azarafza%20and%20Mojtaba%20Nayyeri%20and%20Charles%20Steinmetz%20and%20Steffen%20Staab%20and%20Achim%20Rettberg%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20garnered%20significant%20attention%20for%20their%0Aability%20to%20understand%20text%20and%20images%2C%20generate%20human-like%20text%2C%20and%20perform%0Acomplex%20reasoning%20tasks.%20However%2C%20their%20ability%20to%20generalize%20this%20advanced%0Areasoning%20with%20a%20combination%20of%20natural%20language%20text%20for%20decision-making%20in%0Adynamic%20situations%20requires%20further%20exploration.%20In%20this%20study%2C%20we%20investigate%0Ahow%20well%20LLMs%20can%20adapt%20and%20apply%20a%20combination%20of%20arithmetic%20and%20common-sense%0Areasoning%2C%20particularly%20in%20autonomous%20driving%20scenarios.%20We%20hypothesize%20that%0ALLMs%20hybrid%20reasoning%20abilities%20can%20improve%20autonomous%20driving%20by%20enabling%20them%0Ato%20analyze%20detected%20object%20and%20sensor%20data%2C%20understand%20driving%20regulations%20and%0Aphysical%20laws%2C%20and%20offer%20additional%20context.%20This%20addresses%20complex%20scenarios%2C%0Alike%20decisions%20in%20low%20visibility%20%28due%20to%20weather%20conditions%29%2C%20where%20traditional%0Amethods%20might%20fall%20short.%20We%20evaluated%20Large%20Language%20Models%20%28LLMs%29%20based%20on%0Aaccuracy%20by%20comparing%20their%20answers%20with%20human-generated%20ground%20truth%20inside%0ACARLA.%20The%20results%20showed%20that%20when%20a%20combination%20of%20images%20%28detected%20objects%29%0Aand%20sensor%20data%20is%20fed%20into%20the%20LLM%2C%20it%20can%20offer%20precise%20information%20for%20brake%0Aand%20throttle%20control%20in%20autonomous%20vehicles%20across%20various%20weather%20conditions.%0AThis%20formulation%20and%20answers%20can%20assist%20in%20decision-making%20for%20auto-pilot%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13602v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Reasoning%2520Based%2520on%2520Large%2520Language%2520Models%2520for%2520Autonomous%2520Car%250A%2520%2520Driving%26entry.906535625%3DMehdi%2520Azarafza%2520and%2520Mojtaba%2520Nayyeri%2520and%2520Charles%2520Steinmetz%2520and%2520Steffen%2520Staab%2520and%2520Achim%2520Rettberg%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520garnered%2520significant%2520attention%2520for%2520their%250Aability%2520to%2520understand%2520text%2520and%2520images%252C%2520generate%2520human-like%2520text%252C%2520and%2520perform%250Acomplex%2520reasoning%2520tasks.%2520However%252C%2520their%2520ability%2520to%2520generalize%2520this%2520advanced%250Areasoning%2520with%2520a%2520combination%2520of%2520natural%2520language%2520text%2520for%2520decision-making%2520in%250Adynamic%2520situations%2520requires%2520further%2520exploration.%2520In%2520this%2520study%252C%2520we%2520investigate%250Ahow%2520well%2520LLMs%2520can%2520adapt%2520and%2520apply%2520a%2520combination%2520of%2520arithmetic%2520and%2520common-sense%250Areasoning%252C%2520particularly%2520in%2520autonomous%2520driving%2520scenarios.%2520We%2520hypothesize%2520that%250ALLMs%2520hybrid%2520reasoning%2520abilities%2520can%2520improve%2520autonomous%2520driving%2520by%2520enabling%2520them%250Ato%2520analyze%2520detected%2520object%2520and%2520sensor%2520data%252C%2520understand%2520driving%2520regulations%2520and%250Aphysical%2520laws%252C%2520and%2520offer%2520additional%2520context.%2520This%2520addresses%2520complex%2520scenarios%252C%250Alike%2520decisions%2520in%2520low%2520visibility%2520%2528due%2520to%2520weather%2520conditions%2529%252C%2520where%2520traditional%250Amethods%2520might%2520fall%2520short.%2520We%2520evaluated%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520based%2520on%250Aaccuracy%2520by%2520comparing%2520their%2520answers%2520with%2520human-generated%2520ground%2520truth%2520inside%250ACARLA.%2520The%2520results%2520showed%2520that%2520when%2520a%2520combination%2520of%2520images%2520%2528detected%2520objects%2529%250Aand%2520sensor%2520data%2520is%2520fed%2520into%2520the%2520LLM%252C%2520it%2520can%2520offer%2520precise%2520information%2520for%2520brake%250Aand%2520throttle%2520control%2520in%2520autonomous%2520vehicles%2520across%2520various%2520weather%2520conditions.%250AThis%2520formulation%2520and%2520answers%2520can%2520assist%2520in%2520decision-making%2520for%2520auto-pilot%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13602v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Reasoning%20Based%20on%20Large%20Language%20Models%20for%20Autonomous%20Car%0A%20%20Driving&entry.906535625=Mehdi%20Azarafza%20and%20Mojtaba%20Nayyeri%20and%20Charles%20Steinmetz%20and%20Steffen%20Staab%20and%20Achim%20Rettberg&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20garnered%20significant%20attention%20for%20their%0Aability%20to%20understand%20text%20and%20images%2C%20generate%20human-like%20text%2C%20and%20perform%0Acomplex%20reasoning%20tasks.%20However%2C%20their%20ability%20to%20generalize%20this%20advanced%0Areasoning%20with%20a%20combination%20of%20natural%20language%20text%20for%20decision-making%20in%0Adynamic%20situations%20requires%20further%20exploration.%20In%20this%20study%2C%20we%20investigate%0Ahow%20well%20LLMs%20can%20adapt%20and%20apply%20a%20combination%20of%20arithmetic%20and%20common-sense%0Areasoning%2C%20particularly%20in%20autonomous%20driving%20scenarios.%20We%20hypothesize%20that%0ALLMs%20hybrid%20reasoning%20abilities%20can%20improve%20autonomous%20driving%20by%20enabling%20them%0Ato%20analyze%20detected%20object%20and%20sensor%20data%2C%20understand%20driving%20regulations%20and%0Aphysical%20laws%2C%20and%20offer%20additional%20context.%20This%20addresses%20complex%20scenarios%2C%0Alike%20decisions%20in%20low%20visibility%20%28due%20to%20weather%20conditions%29%2C%20where%20traditional%0Amethods%20might%20fall%20short.%20We%20evaluated%20Large%20Language%20Models%20%28LLMs%29%20based%20on%0Aaccuracy%20by%20comparing%20their%20answers%20with%20human-generated%20ground%20truth%20inside%0ACARLA.%20The%20results%20showed%20that%20when%20a%20combination%20of%20images%20%28detected%20objects%29%0Aand%20sensor%20data%20is%20fed%20into%20the%20LLM%2C%20it%20can%20offer%20precise%20information%20for%20brake%0Aand%20throttle%20control%20in%20autonomous%20vehicles%20across%20various%20weather%20conditions.%0AThis%20formulation%20and%20answers%20can%20assist%20in%20decision-making%20for%20auto-pilot%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13602v4&entry.124074799=Read"},
{"title": "CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory\n  Prediction Models for Autonomous Driving", "author": "Changhe Chen and Mozhgan Pourkeshavarz and Amir Rasouli", "abstract": "  Benchmarking is a common method for evaluating trajectory prediction models\nfor autonomous driving. Existing benchmarks rely on datasets, which are biased\ntowards more common scenarios, such as cruising, and distance-based metrics\nthat are computed by averaging over all scenarios. Following such a regiment\nprovides a little insight into the properties of the models both in terms of\nhow well they can handle different scenarios and how admissible and diverse\ntheir outputs are. There exist a number of complementary metrics designed to\nmeasure the admissibility and diversity of trajectories, however, they suffer\nfrom biases, such as length of trajectories.\n  In this paper, we propose a new benChmarking paRadIgm for evaluaTing\ntrajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a\nmethod for extracting driving scenarios at varying levels of specificity\naccording to the structure of the roads, models' performance, and data\nproperties for fine-grained ranking of prediction models; 2) A set of new\nbias-free metrics for measuring diversity, by incorporating the characteristics\nof a given scenario, and admissibility, by considering the structure of roads\nand kinematic compliancy, motivated by real-world driving constraints. 3) Using\nthe proposed benchmark, we conduct extensive experimentation on a\nrepresentative set of the prediction models using the large scale Argoverse\ndataset. We show that the proposed benchmark can produce a more accurate\nranking of the models and serve as a means of characterizing their behavior. We\nfurther present ablation studies to highlight contributions of different\nelements that are used to compute the proposed metrics.\n", "link": "http://arxiv.org/abs/2310.07794v2", "date": "2024-08-19", "relevancy": 1.9938, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4928}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRITERIA%3A%20a%20New%20Benchmarking%20Paradigm%20for%20Evaluating%20Trajectory%0A%20%20Prediction%20Models%20for%20Autonomous%20Driving&body=Title%3A%20CRITERIA%3A%20a%20New%20Benchmarking%20Paradigm%20for%20Evaluating%20Trajectory%0A%20%20Prediction%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Changhe%20Chen%20and%20Mozhgan%20Pourkeshavarz%20and%20Amir%20Rasouli%0AAbstract%3A%20%20%20Benchmarking%20is%20a%20common%20method%20for%20evaluating%20trajectory%20prediction%20models%0Afor%20autonomous%20driving.%20Existing%20benchmarks%20rely%20on%20datasets%2C%20which%20are%20biased%0Atowards%20more%20common%20scenarios%2C%20such%20as%20cruising%2C%20and%20distance-based%20metrics%0Athat%20are%20computed%20by%20averaging%20over%20all%20scenarios.%20Following%20such%20a%20regiment%0Aprovides%20a%20little%20insight%20into%20the%20properties%20of%20the%20models%20both%20in%20terms%20of%0Ahow%20well%20they%20can%20handle%20different%20scenarios%20and%20how%20admissible%20and%20diverse%0Atheir%20outputs%20are.%20There%20exist%20a%20number%20of%20complementary%20metrics%20designed%20to%0Ameasure%20the%20admissibility%20and%20diversity%20of%20trajectories%2C%20however%2C%20they%20suffer%0Afrom%20biases%2C%20such%20as%20length%20of%20trajectories.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20benChmarking%20paRadIgm%20for%20evaluaTing%0AtrajEctoRy%20predIction%20Approaches%20%28CRITERIA%29.%20Particularly%2C%20we%20propose%201%29%20a%0Amethod%20for%20extracting%20driving%20scenarios%20at%20varying%20levels%20of%20specificity%0Aaccording%20to%20the%20structure%20of%20the%20roads%2C%20models%27%20performance%2C%20and%20data%0Aproperties%20for%20fine-grained%20ranking%20of%20prediction%20models%3B%202%29%20A%20set%20of%20new%0Abias-free%20metrics%20for%20measuring%20diversity%2C%20by%20incorporating%20the%20characteristics%0Aof%20a%20given%20scenario%2C%20and%20admissibility%2C%20by%20considering%20the%20structure%20of%20roads%0Aand%20kinematic%20compliancy%2C%20motivated%20by%20real-world%20driving%20constraints.%203%29%20Using%0Athe%20proposed%20benchmark%2C%20we%20conduct%20extensive%20experimentation%20on%20a%0Arepresentative%20set%20of%20the%20prediction%20models%20using%20the%20large%20scale%20Argoverse%0Adataset.%20We%20show%20that%20the%20proposed%20benchmark%20can%20produce%20a%20more%20accurate%0Aranking%20of%20the%20models%20and%20serve%20as%20a%20means%20of%20characterizing%20their%20behavior.%20We%0Afurther%20present%20ablation%20studies%20to%20highlight%20contributions%20of%20different%0Aelements%20that%20are%20used%20to%20compute%20the%20proposed%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRITERIA%253A%2520a%2520New%2520Benchmarking%2520Paradigm%2520for%2520Evaluating%2520Trajectory%250A%2520%2520Prediction%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DChanghe%2520Chen%2520and%2520Mozhgan%2520Pourkeshavarz%2520and%2520Amir%2520Rasouli%26entry.1292438233%3D%2520%2520Benchmarking%2520is%2520a%2520common%2520method%2520for%2520evaluating%2520trajectory%2520prediction%2520models%250Afor%2520autonomous%2520driving.%2520Existing%2520benchmarks%2520rely%2520on%2520datasets%252C%2520which%2520are%2520biased%250Atowards%2520more%2520common%2520scenarios%252C%2520such%2520as%2520cruising%252C%2520and%2520distance-based%2520metrics%250Athat%2520are%2520computed%2520by%2520averaging%2520over%2520all%2520scenarios.%2520Following%2520such%2520a%2520regiment%250Aprovides%2520a%2520little%2520insight%2520into%2520the%2520properties%2520of%2520the%2520models%2520both%2520in%2520terms%2520of%250Ahow%2520well%2520they%2520can%2520handle%2520different%2520scenarios%2520and%2520how%2520admissible%2520and%2520diverse%250Atheir%2520outputs%2520are.%2520There%2520exist%2520a%2520number%2520of%2520complementary%2520metrics%2520designed%2520to%250Ameasure%2520the%2520admissibility%2520and%2520diversity%2520of%2520trajectories%252C%2520however%252C%2520they%2520suffer%250Afrom%2520biases%252C%2520such%2520as%2520length%2520of%2520trajectories.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520benChmarking%2520paRadIgm%2520for%2520evaluaTing%250AtrajEctoRy%2520predIction%2520Approaches%2520%2528CRITERIA%2529.%2520Particularly%252C%2520we%2520propose%25201%2529%2520a%250Amethod%2520for%2520extracting%2520driving%2520scenarios%2520at%2520varying%2520levels%2520of%2520specificity%250Aaccording%2520to%2520the%2520structure%2520of%2520the%2520roads%252C%2520models%2527%2520performance%252C%2520and%2520data%250Aproperties%2520for%2520fine-grained%2520ranking%2520of%2520prediction%2520models%253B%25202%2529%2520A%2520set%2520of%2520new%250Abias-free%2520metrics%2520for%2520measuring%2520diversity%252C%2520by%2520incorporating%2520the%2520characteristics%250Aof%2520a%2520given%2520scenario%252C%2520and%2520admissibility%252C%2520by%2520considering%2520the%2520structure%2520of%2520roads%250Aand%2520kinematic%2520compliancy%252C%2520motivated%2520by%2520real-world%2520driving%2520constraints.%25203%2529%2520Using%250Athe%2520proposed%2520benchmark%252C%2520we%2520conduct%2520extensive%2520experimentation%2520on%2520a%250Arepresentative%2520set%2520of%2520the%2520prediction%2520models%2520using%2520the%2520large%2520scale%2520Argoverse%250Adataset.%2520We%2520show%2520that%2520the%2520proposed%2520benchmark%2520can%2520produce%2520a%2520more%2520accurate%250Aranking%2520of%2520the%2520models%2520and%2520serve%2520as%2520a%2520means%2520of%2520characterizing%2520their%2520behavior.%2520We%250Afurther%2520present%2520ablation%2520studies%2520to%2520highlight%2520contributions%2520of%2520different%250Aelements%2520that%2520are%2520used%2520to%2520compute%2520the%2520proposed%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRITERIA%3A%20a%20New%20Benchmarking%20Paradigm%20for%20Evaluating%20Trajectory%0A%20%20Prediction%20Models%20for%20Autonomous%20Driving&entry.906535625=Changhe%20Chen%20and%20Mozhgan%20Pourkeshavarz%20and%20Amir%20Rasouli&entry.1292438233=%20%20Benchmarking%20is%20a%20common%20method%20for%20evaluating%20trajectory%20prediction%20models%0Afor%20autonomous%20driving.%20Existing%20benchmarks%20rely%20on%20datasets%2C%20which%20are%20biased%0Atowards%20more%20common%20scenarios%2C%20such%20as%20cruising%2C%20and%20distance-based%20metrics%0Athat%20are%20computed%20by%20averaging%20over%20all%20scenarios.%20Following%20such%20a%20regiment%0Aprovides%20a%20little%20insight%20into%20the%20properties%20of%20the%20models%20both%20in%20terms%20of%0Ahow%20well%20they%20can%20handle%20different%20scenarios%20and%20how%20admissible%20and%20diverse%0Atheir%20outputs%20are.%20There%20exist%20a%20number%20of%20complementary%20metrics%20designed%20to%0Ameasure%20the%20admissibility%20and%20diversity%20of%20trajectories%2C%20however%2C%20they%20suffer%0Afrom%20biases%2C%20such%20as%20length%20of%20trajectories.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20benChmarking%20paRadIgm%20for%20evaluaTing%0AtrajEctoRy%20predIction%20Approaches%20%28CRITERIA%29.%20Particularly%2C%20we%20propose%201%29%20a%0Amethod%20for%20extracting%20driving%20scenarios%20at%20varying%20levels%20of%20specificity%0Aaccording%20to%20the%20structure%20of%20the%20roads%2C%20models%27%20performance%2C%20and%20data%0Aproperties%20for%20fine-grained%20ranking%20of%20prediction%20models%3B%202%29%20A%20set%20of%20new%0Abias-free%20metrics%20for%20measuring%20diversity%2C%20by%20incorporating%20the%20characteristics%0Aof%20a%20given%20scenario%2C%20and%20admissibility%2C%20by%20considering%20the%20structure%20of%20roads%0Aand%20kinematic%20compliancy%2C%20motivated%20by%20real-world%20driving%20constraints.%203%29%20Using%0Athe%20proposed%20benchmark%2C%20we%20conduct%20extensive%20experimentation%20on%20a%0Arepresentative%20set%20of%20the%20prediction%20models%20using%20the%20large%20scale%20Argoverse%0Adataset.%20We%20show%20that%20the%20proposed%20benchmark%20can%20produce%20a%20more%20accurate%0Aranking%20of%20the%20models%20and%20serve%20as%20a%20means%20of%20characterizing%20their%20behavior.%20We%0Afurther%20present%20ablation%20studies%20to%20highlight%20contributions%20of%20different%0Aelements%20that%20are%20used%20to%20compute%20the%20proposed%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07794v2&entry.124074799=Read"},
{"title": "Video Object Segmentation via SAM 2: The 4th Solution for LSVOS\n  Challenge VOS Track", "author": "Feiyu Pan and Hao Fang and Runmin Cong and Wei Zhang and Xiankai Lu", "abstract": "  Video Object Segmentation (VOS) task aims to segmenting a particular object\ninstance throughout the entire video sequence given only the object mask of the\nfirst frame. Recently, Segment Anything Model 2 (SAM 2) is proposed, which is a\nfoundation model towards solving promptable visual segmentation in images and\nvideos. SAM 2 builds a data engine, which improves model and data via user\ninteraction, to collect the largest video segmentation dataset to date. SAM 2\nis a simple transformer architecture with streaming memory for real-time video\nprocessing, which trained on the date provides strong performance across a wide\nrange of tasks. In this work, we evaluate the zero-shot performance of SAM 2 on\nthe more challenging VOS datasets MOSE and LVOS. Without fine-tuning on the\ntraining set, SAM 2 achieved 75.79 J&F on the test set and ranked 4th place for\n6th LSVOS Challenge VOS Track.\n", "link": "http://arxiv.org/abs/2408.10125v1", "date": "2024-08-19", "relevancy": 1.987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4978}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.497}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Object%20Segmentation%20via%20SAM%202%3A%20The%204th%20Solution%20for%20LSVOS%0A%20%20Challenge%20VOS%20Track&body=Title%3A%20Video%20Object%20Segmentation%20via%20SAM%202%3A%20The%204th%20Solution%20for%20LSVOS%0A%20%20Challenge%20VOS%20Track%0AAuthor%3A%20Feiyu%20Pan%20and%20Hao%20Fang%20and%20Runmin%20Cong%20and%20Wei%20Zhang%20and%20Xiankai%20Lu%0AAbstract%3A%20%20%20Video%20Object%20Segmentation%20%28VOS%29%20task%20aims%20to%20segmenting%20a%20particular%20object%0Ainstance%20throughout%20the%20entire%20video%20sequence%20given%20only%20the%20object%20mask%20of%20the%0Afirst%20frame.%20Recently%2C%20Segment%20Anything%20Model%202%20%28SAM%202%29%20is%20proposed%2C%20which%20is%20a%0Afoundation%20model%20towards%20solving%20promptable%20visual%20segmentation%20in%20images%20and%0Avideos.%20SAM%202%20builds%20a%20data%20engine%2C%20which%20improves%20model%20and%20data%20via%20user%0Ainteraction%2C%20to%20collect%20the%20largest%20video%20segmentation%20dataset%20to%20date.%20SAM%202%0Ais%20a%20simple%20transformer%20architecture%20with%20streaming%20memory%20for%20real-time%20video%0Aprocessing%2C%20which%20trained%20on%20the%20date%20provides%20strong%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20In%20this%20work%2C%20we%20evaluate%20the%20zero-shot%20performance%20of%20SAM%202%20on%0Athe%20more%20challenging%20VOS%20datasets%20MOSE%20and%20LVOS.%20Without%20fine-tuning%20on%20the%0Atraining%20set%2C%20SAM%202%20achieved%2075.79%20J%26F%20on%20the%20test%20set%20and%20ranked%204th%20place%20for%0A6th%20LSVOS%20Challenge%20VOS%20Track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Object%2520Segmentation%2520via%2520SAM%25202%253A%2520The%25204th%2520Solution%2520for%2520LSVOS%250A%2520%2520Challenge%2520VOS%2520Track%26entry.906535625%3DFeiyu%2520Pan%2520and%2520Hao%2520Fang%2520and%2520Runmin%2520Cong%2520and%2520Wei%2520Zhang%2520and%2520Xiankai%2520Lu%26entry.1292438233%3D%2520%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520task%2520aims%2520to%2520segmenting%2520a%2520particular%2520object%250Ainstance%2520throughout%2520the%2520entire%2520video%2520sequence%2520given%2520only%2520the%2520object%2520mask%2520of%2520the%250Afirst%2520frame.%2520Recently%252C%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%2520is%2520proposed%252C%2520which%2520is%2520a%250Afoundation%2520model%2520towards%2520solving%2520promptable%2520visual%2520segmentation%2520in%2520images%2520and%250Avideos.%2520SAM%25202%2520builds%2520a%2520data%2520engine%252C%2520which%2520improves%2520model%2520and%2520data%2520via%2520user%250Ainteraction%252C%2520to%2520collect%2520the%2520largest%2520video%2520segmentation%2520dataset%2520to%2520date.%2520SAM%25202%250Ais%2520a%2520simple%2520transformer%2520architecture%2520with%2520streaming%2520memory%2520for%2520real-time%2520video%250Aprocessing%252C%2520which%2520trained%2520on%2520the%2520date%2520provides%2520strong%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520tasks.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%2520zero-shot%2520performance%2520of%2520SAM%25202%2520on%250Athe%2520more%2520challenging%2520VOS%2520datasets%2520MOSE%2520and%2520LVOS.%2520Without%2520fine-tuning%2520on%2520the%250Atraining%2520set%252C%2520SAM%25202%2520achieved%252075.79%2520J%2526F%2520on%2520the%2520test%2520set%2520and%2520ranked%25204th%2520place%2520for%250A6th%2520LSVOS%2520Challenge%2520VOS%2520Track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Object%20Segmentation%20via%20SAM%202%3A%20The%204th%20Solution%20for%20LSVOS%0A%20%20Challenge%20VOS%20Track&entry.906535625=Feiyu%20Pan%20and%20Hao%20Fang%20and%20Runmin%20Cong%20and%20Wei%20Zhang%20and%20Xiankai%20Lu&entry.1292438233=%20%20Video%20Object%20Segmentation%20%28VOS%29%20task%20aims%20to%20segmenting%20a%20particular%20object%0Ainstance%20throughout%20the%20entire%20video%20sequence%20given%20only%20the%20object%20mask%20of%20the%0Afirst%20frame.%20Recently%2C%20Segment%20Anything%20Model%202%20%28SAM%202%29%20is%20proposed%2C%20which%20is%20a%0Afoundation%20model%20towards%20solving%20promptable%20visual%20segmentation%20in%20images%20and%0Avideos.%20SAM%202%20builds%20a%20data%20engine%2C%20which%20improves%20model%20and%20data%20via%20user%0Ainteraction%2C%20to%20collect%20the%20largest%20video%20segmentation%20dataset%20to%20date.%20SAM%202%0Ais%20a%20simple%20transformer%20architecture%20with%20streaming%20memory%20for%20real-time%20video%0Aprocessing%2C%20which%20trained%20on%20the%20date%20provides%20strong%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20In%20this%20work%2C%20we%20evaluate%20the%20zero-shot%20performance%20of%20SAM%202%20on%0Athe%20more%20challenging%20VOS%20datasets%20MOSE%20and%20LVOS.%20Without%20fine-tuning%20on%20the%0Atraining%20set%2C%20SAM%202%20achieved%2075.79%20J%26F%20on%20the%20test%20set%20and%20ranked%204th%20place%20for%0A6th%20LSVOS%20Challenge%20VOS%20Track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10125v1&entry.124074799=Read"},
{"title": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and\n  Competition", "author": "Tianwei Lin and Jiang Liu and Wenqiao Zhang and Zhaocheng Li and Yang Dai and Haoyuan Li and Zhelun Yu and Wanggui He and Juncheng Li and Hao Jiang and Siliang Tang and Yueting Zhuang", "abstract": "  While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multidimensional task scenarios.\nTo address this issue, one straightforward solution is to introduce\ntask-specific LoRA modules as domain experts, leveraging the modeling of\nmultiple experts' capabilities and thus enhancing the general capability of\nmulti-task learning. Despite promising, these additional components often add\ncomplexity to the training and inference process, contravening the efficient\ncharacterization of PEFT designed for. Considering this, we introduce an\ninnovative PEFT method, TeamLoRA, consisting of a collaboration and competition\nmodule for experts, and thus achieving the right balance of effectiveness and\nefficiency: (i) For collaboration, a novel knowledge-sharing and -organizing\nmechanism is devised to appropriately reduce the scale of matrix operations,\nthereby boosting the training and inference speed. (ii) For competition, we\npropose leveraging a game-theoretic interaction mechanism for experts,\nencouraging experts to transfer their domain-specific knowledge while facing\ndiverse downstream tasks, and thus enhancing the performance. By doing so,\nTeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration\nand competition, enabling a faster and more accurate PEFT paradigm for\nmulti-task learning. To validate the superiority of TeamLoRA, we curate a\ncomprehensive multi-task evaluation(CME) benchmark to thoroughly assess the\ncapability of multi-task learning. Experiments conducted on our CME and other\nbenchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project\nis available at https://github.com/Lin-Tianwei/TeamLoRA.\n", "link": "http://arxiv.org/abs/2408.09856v1", "date": "2024-08-19", "relevancy": 1.9838, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4878}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeamLoRA%3A%20Boosting%20Low-Rank%20Adaptation%20with%20Expert%20Collaboration%20and%0A%20%20Competition&body=Title%3A%20TeamLoRA%3A%20Boosting%20Low-Rank%20Adaptation%20with%20Expert%20Collaboration%20and%0A%20%20Competition%0AAuthor%3A%20Tianwei%20Lin%20and%20Jiang%20Liu%20and%20Wenqiao%20Zhang%20and%20Zhaocheng%20Li%20and%20Yang%20Dai%20and%20Haoyuan%20Li%20and%20Zhelun%20Yu%20and%20Wanggui%20He%20and%20Juncheng%20Li%20and%20Hao%20Jiang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20While%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20like%20LoRA%20have%0Aeffectively%20addressed%20GPU%20memory%20constraints%20during%20fine-tuning%2C%20their%0Aperformance%20often%20falls%20short%2C%20especially%20in%20multidimensional%20task%20scenarios.%0ATo%20address%20this%20issue%2C%20one%20straightforward%20solution%20is%20to%20introduce%0Atask-specific%20LoRA%20modules%20as%20domain%20experts%2C%20leveraging%20the%20modeling%20of%0Amultiple%20experts%27%20capabilities%20and%20thus%20enhancing%20the%20general%20capability%20of%0Amulti-task%20learning.%20Despite%20promising%2C%20these%20additional%20components%20often%20add%0Acomplexity%20to%20the%20training%20and%20inference%20process%2C%20contravening%20the%20efficient%0Acharacterization%20of%20PEFT%20designed%20for.%20Considering%20this%2C%20we%20introduce%20an%0Ainnovative%20PEFT%20method%2C%20TeamLoRA%2C%20consisting%20of%20a%20collaboration%20and%20competition%0Amodule%20for%20experts%2C%20and%20thus%20achieving%20the%20right%20balance%20of%20effectiveness%20and%0Aefficiency%3A%20%28i%29%20For%20collaboration%2C%20a%20novel%20knowledge-sharing%20and%20-organizing%0Amechanism%20is%20devised%20to%20appropriately%20reduce%20the%20scale%20of%20matrix%20operations%2C%0Athereby%20boosting%20the%20training%20and%20inference%20speed.%20%28ii%29%20For%20competition%2C%20we%0Apropose%20leveraging%20a%20game-theoretic%20interaction%20mechanism%20for%20experts%2C%0Aencouraging%20experts%20to%20transfer%20their%20domain-specific%20knowledge%20while%20facing%0Adiverse%20downstream%20tasks%2C%20and%20thus%20enhancing%20the%20performance.%20By%20doing%20so%2C%0ATeamLoRA%20elegantly%20connects%20the%20experts%20as%20a%20%22Team%22%20with%20internal%20collaboration%0Aand%20competition%2C%20enabling%20a%20faster%20and%20more%20accurate%20PEFT%20paradigm%20for%0Amulti-task%20learning.%20To%20validate%20the%20superiority%20of%20TeamLoRA%2C%20we%20curate%20a%0Acomprehensive%20multi-task%20evaluation%28CME%29%20benchmark%20to%20thoroughly%20assess%20the%0Acapability%20of%20multi-task%20learning.%20Experiments%20conducted%20on%20our%20CME%20and%20other%0Abenchmarks%20indicate%20the%20effectiveness%20and%20efficiency%20of%20TeamLoRA.%20Our%20project%0Ais%20available%20at%20https%3A//github.com/Lin-Tianwei/TeamLoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeamLoRA%253A%2520Boosting%2520Low-Rank%2520Adaptation%2520with%2520Expert%2520Collaboration%2520and%250A%2520%2520Competition%26entry.906535625%3DTianwei%2520Lin%2520and%2520Jiang%2520Liu%2520and%2520Wenqiao%2520Zhang%2520and%2520Zhaocheng%2520Li%2520and%2520Yang%2520Dai%2520and%2520Haoyuan%2520Li%2520and%2520Zhelun%2520Yu%2520and%2520Wanggui%2520He%2520and%2520Juncheng%2520Li%2520and%2520Hao%2520Jiang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520While%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520like%2520LoRA%2520have%250Aeffectively%2520addressed%2520GPU%2520memory%2520constraints%2520during%2520fine-tuning%252C%2520their%250Aperformance%2520often%2520falls%2520short%252C%2520especially%2520in%2520multidimensional%2520task%2520scenarios.%250ATo%2520address%2520this%2520issue%252C%2520one%2520straightforward%2520solution%2520is%2520to%2520introduce%250Atask-specific%2520LoRA%2520modules%2520as%2520domain%2520experts%252C%2520leveraging%2520the%2520modeling%2520of%250Amultiple%2520experts%2527%2520capabilities%2520and%2520thus%2520enhancing%2520the%2520general%2520capability%2520of%250Amulti-task%2520learning.%2520Despite%2520promising%252C%2520these%2520additional%2520components%2520often%2520add%250Acomplexity%2520to%2520the%2520training%2520and%2520inference%2520process%252C%2520contravening%2520the%2520efficient%250Acharacterization%2520of%2520PEFT%2520designed%2520for.%2520Considering%2520this%252C%2520we%2520introduce%2520an%250Ainnovative%2520PEFT%2520method%252C%2520TeamLoRA%252C%2520consisting%2520of%2520a%2520collaboration%2520and%2520competition%250Amodule%2520for%2520experts%252C%2520and%2520thus%2520achieving%2520the%2520right%2520balance%2520of%2520effectiveness%2520and%250Aefficiency%253A%2520%2528i%2529%2520For%2520collaboration%252C%2520a%2520novel%2520knowledge-sharing%2520and%2520-organizing%250Amechanism%2520is%2520devised%2520to%2520appropriately%2520reduce%2520the%2520scale%2520of%2520matrix%2520operations%252C%250Athereby%2520boosting%2520the%2520training%2520and%2520inference%2520speed.%2520%2528ii%2529%2520For%2520competition%252C%2520we%250Apropose%2520leveraging%2520a%2520game-theoretic%2520interaction%2520mechanism%2520for%2520experts%252C%250Aencouraging%2520experts%2520to%2520transfer%2520their%2520domain-specific%2520knowledge%2520while%2520facing%250Adiverse%2520downstream%2520tasks%252C%2520and%2520thus%2520enhancing%2520the%2520performance.%2520By%2520doing%2520so%252C%250ATeamLoRA%2520elegantly%2520connects%2520the%2520experts%2520as%2520a%2520%2522Team%2522%2520with%2520internal%2520collaboration%250Aand%2520competition%252C%2520enabling%2520a%2520faster%2520and%2520more%2520accurate%2520PEFT%2520paradigm%2520for%250Amulti-task%2520learning.%2520To%2520validate%2520the%2520superiority%2520of%2520TeamLoRA%252C%2520we%2520curate%2520a%250Acomprehensive%2520multi-task%2520evaluation%2528CME%2529%2520benchmark%2520to%2520thoroughly%2520assess%2520the%250Acapability%2520of%2520multi-task%2520learning.%2520Experiments%2520conducted%2520on%2520our%2520CME%2520and%2520other%250Abenchmarks%2520indicate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520TeamLoRA.%2520Our%2520project%250Ais%2520available%2520at%2520https%253A//github.com/Lin-Tianwei/TeamLoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeamLoRA%3A%20Boosting%20Low-Rank%20Adaptation%20with%20Expert%20Collaboration%20and%0A%20%20Competition&entry.906535625=Tianwei%20Lin%20and%20Jiang%20Liu%20and%20Wenqiao%20Zhang%20and%20Zhaocheng%20Li%20and%20Yang%20Dai%20and%20Haoyuan%20Li%20and%20Zhelun%20Yu%20and%20Wanggui%20He%20and%20Juncheng%20Li%20and%20Hao%20Jiang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20While%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20like%20LoRA%20have%0Aeffectively%20addressed%20GPU%20memory%20constraints%20during%20fine-tuning%2C%20their%0Aperformance%20often%20falls%20short%2C%20especially%20in%20multidimensional%20task%20scenarios.%0ATo%20address%20this%20issue%2C%20one%20straightforward%20solution%20is%20to%20introduce%0Atask-specific%20LoRA%20modules%20as%20domain%20experts%2C%20leveraging%20the%20modeling%20of%0Amultiple%20experts%27%20capabilities%20and%20thus%20enhancing%20the%20general%20capability%20of%0Amulti-task%20learning.%20Despite%20promising%2C%20these%20additional%20components%20often%20add%0Acomplexity%20to%20the%20training%20and%20inference%20process%2C%20contravening%20the%20efficient%0Acharacterization%20of%20PEFT%20designed%20for.%20Considering%20this%2C%20we%20introduce%20an%0Ainnovative%20PEFT%20method%2C%20TeamLoRA%2C%20consisting%20of%20a%20collaboration%20and%20competition%0Amodule%20for%20experts%2C%20and%20thus%20achieving%20the%20right%20balance%20of%20effectiveness%20and%0Aefficiency%3A%20%28i%29%20For%20collaboration%2C%20a%20novel%20knowledge-sharing%20and%20-organizing%0Amechanism%20is%20devised%20to%20appropriately%20reduce%20the%20scale%20of%20matrix%20operations%2C%0Athereby%20boosting%20the%20training%20and%20inference%20speed.%20%28ii%29%20For%20competition%2C%20we%0Apropose%20leveraging%20a%20game-theoretic%20interaction%20mechanism%20for%20experts%2C%0Aencouraging%20experts%20to%20transfer%20their%20domain-specific%20knowledge%20while%20facing%0Adiverse%20downstream%20tasks%2C%20and%20thus%20enhancing%20the%20performance.%20By%20doing%20so%2C%0ATeamLoRA%20elegantly%20connects%20the%20experts%20as%20a%20%22Team%22%20with%20internal%20collaboration%0Aand%20competition%2C%20enabling%20a%20faster%20and%20more%20accurate%20PEFT%20paradigm%20for%0Amulti-task%20learning.%20To%20validate%20the%20superiority%20of%20TeamLoRA%2C%20we%20curate%20a%0Acomprehensive%20multi-task%20evaluation%28CME%29%20benchmark%20to%20thoroughly%20assess%20the%0Acapability%20of%20multi-task%20learning.%20Experiments%20conducted%20on%20our%20CME%20and%20other%0Abenchmarks%20indicate%20the%20effectiveness%20and%20efficiency%20of%20TeamLoRA.%20Our%20project%0Ais%20available%20at%20https%3A//github.com/Lin-Tianwei/TeamLoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09856v1&entry.124074799=Read"},
{"title": "Convert and Speak: Zero-shot Accent Conversion with Minimum Supervision", "author": "Zhijun Jia and Huaying Xue and Xiulian Peng and Yan Lu", "abstract": "  Low resource of parallel data is the key challenge of accent conversion(AC)\nproblem in which both the pronunciation units and prosody pattern need to be\nconverted. We propose a two-stage generative framework \"convert-and-speak\" in\nwhich the conversion is only operated on the semantic token level and the\nspeech is synthesized conditioned on the converted semantic token with a speech\ngenerative model in target accent domain. The decoupling design enables the\n\"speaking\" module to use massive amount of target accent speech and relieves\nthe parallel data required for the \"conversion\" module. Conversion with the\nbridge of semantic token also relieves the requirement for the data with text\ntranscriptions and unlocks the usage of language pre-training technology to\nfurther efficiently reduce the need of parallel accent speech data. To reduce\nthe complexity and latency of \"speaking\", a single-stage AR generative model is\ndesigned to achieve good quality as well as lower computation cost. Experiments\non Indian-English to general American-English conversion show that the proposed\nframework achieves state-of-the-art performance in accent similarity, speech\nquality, and speaker maintenance with only 15 minutes of weakly parallel data\nwhich is not constrained to the same speaker. Extensive experimentation with\ndiverse accent types suggests that this framework possesses a high degree of\nadaptability, making it readily scalable to accommodate other accents with\nlow-resource data. Audio samples are available at\nhttps://www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.\n", "link": "http://arxiv.org/abs/2408.10096v1", "date": "2024-08-19", "relevancy": 1.9753, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5049}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4906}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convert%20and%20Speak%3A%20Zero-shot%20Accent%20Conversion%20with%20Minimum%20Supervision&body=Title%3A%20Convert%20and%20Speak%3A%20Zero-shot%20Accent%20Conversion%20with%20Minimum%20Supervision%0AAuthor%3A%20Zhijun%20Jia%20and%20Huaying%20Xue%20and%20Xiulian%20Peng%20and%20Yan%20Lu%0AAbstract%3A%20%20%20Low%20resource%20of%20parallel%20data%20is%20the%20key%20challenge%20of%20accent%20conversion%28AC%29%0Aproblem%20in%20which%20both%20the%20pronunciation%20units%20and%20prosody%20pattern%20need%20to%20be%0Aconverted.%20We%20propose%20a%20two-stage%20generative%20framework%20%22convert-and-speak%22%20in%0Awhich%20the%20conversion%20is%20only%20operated%20on%20the%20semantic%20token%20level%20and%20the%0Aspeech%20is%20synthesized%20conditioned%20on%20the%20converted%20semantic%20token%20with%20a%20speech%0Agenerative%20model%20in%20target%20accent%20domain.%20The%20decoupling%20design%20enables%20the%0A%22speaking%22%20module%20to%20use%20massive%20amount%20of%20target%20accent%20speech%20and%20relieves%0Athe%20parallel%20data%20required%20for%20the%20%22conversion%22%20module.%20Conversion%20with%20the%0Abridge%20of%20semantic%20token%20also%20relieves%20the%20requirement%20for%20the%20data%20with%20text%0Atranscriptions%20and%20unlocks%20the%20usage%20of%20language%20pre-training%20technology%20to%0Afurther%20efficiently%20reduce%20the%20need%20of%20parallel%20accent%20speech%20data.%20To%20reduce%0Athe%20complexity%20and%20latency%20of%20%22speaking%22%2C%20a%20single-stage%20AR%20generative%20model%20is%0Adesigned%20to%20achieve%20good%20quality%20as%20well%20as%20lower%20computation%20cost.%20Experiments%0Aon%20Indian-English%20to%20general%20American-English%20conversion%20show%20that%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20accent%20similarity%2C%20speech%0Aquality%2C%20and%20speaker%20maintenance%20with%20only%2015%20minutes%20of%20weakly%20parallel%20data%0Awhich%20is%20not%20constrained%20to%20the%20same%20speaker.%20Extensive%20experimentation%20with%0Adiverse%20accent%20types%20suggests%20that%20this%20framework%20possesses%20a%20high%20degree%20of%0Aadaptability%2C%20making%20it%20readily%20scalable%20to%20accommodate%20other%20accents%20with%0Alow-resource%20data.%20Audio%20samples%20are%20available%20at%0Ahttps%3A//www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvert%2520and%2520Speak%253A%2520Zero-shot%2520Accent%2520Conversion%2520with%2520Minimum%2520Supervision%26entry.906535625%3DZhijun%2520Jia%2520and%2520Huaying%2520Xue%2520and%2520Xiulian%2520Peng%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520Low%2520resource%2520of%2520parallel%2520data%2520is%2520the%2520key%2520challenge%2520of%2520accent%2520conversion%2528AC%2529%250Aproblem%2520in%2520which%2520both%2520the%2520pronunciation%2520units%2520and%2520prosody%2520pattern%2520need%2520to%2520be%250Aconverted.%2520We%2520propose%2520a%2520two-stage%2520generative%2520framework%2520%2522convert-and-speak%2522%2520in%250Awhich%2520the%2520conversion%2520is%2520only%2520operated%2520on%2520the%2520semantic%2520token%2520level%2520and%2520the%250Aspeech%2520is%2520synthesized%2520conditioned%2520on%2520the%2520converted%2520semantic%2520token%2520with%2520a%2520speech%250Agenerative%2520model%2520in%2520target%2520accent%2520domain.%2520The%2520decoupling%2520design%2520enables%2520the%250A%2522speaking%2522%2520module%2520to%2520use%2520massive%2520amount%2520of%2520target%2520accent%2520speech%2520and%2520relieves%250Athe%2520parallel%2520data%2520required%2520for%2520the%2520%2522conversion%2522%2520module.%2520Conversion%2520with%2520the%250Abridge%2520of%2520semantic%2520token%2520also%2520relieves%2520the%2520requirement%2520for%2520the%2520data%2520with%2520text%250Atranscriptions%2520and%2520unlocks%2520the%2520usage%2520of%2520language%2520pre-training%2520technology%2520to%250Afurther%2520efficiently%2520reduce%2520the%2520need%2520of%2520parallel%2520accent%2520speech%2520data.%2520To%2520reduce%250Athe%2520complexity%2520and%2520latency%2520of%2520%2522speaking%2522%252C%2520a%2520single-stage%2520AR%2520generative%2520model%2520is%250Adesigned%2520to%2520achieve%2520good%2520quality%2520as%2520well%2520as%2520lower%2520computation%2520cost.%2520Experiments%250Aon%2520Indian-English%2520to%2520general%2520American-English%2520conversion%2520show%2520that%2520the%2520proposed%250Aframework%2520achieves%2520state-of-the-art%2520performance%2520in%2520accent%2520similarity%252C%2520speech%250Aquality%252C%2520and%2520speaker%2520maintenance%2520with%2520only%252015%2520minutes%2520of%2520weakly%2520parallel%2520data%250Awhich%2520is%2520not%2520constrained%2520to%2520the%2520same%2520speaker.%2520Extensive%2520experimentation%2520with%250Adiverse%2520accent%2520types%2520suggests%2520that%2520this%2520framework%2520possesses%2520a%2520high%2520degree%2520of%250Aadaptability%252C%2520making%2520it%2520readily%2520scalable%2520to%2520accommodate%2520other%2520accents%2520with%250Alow-resource%2520data.%2520Audio%2520samples%2520are%2520available%2520at%250Ahttps%253A//www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convert%20and%20Speak%3A%20Zero-shot%20Accent%20Conversion%20with%20Minimum%20Supervision&entry.906535625=Zhijun%20Jia%20and%20Huaying%20Xue%20and%20Xiulian%20Peng%20and%20Yan%20Lu&entry.1292438233=%20%20Low%20resource%20of%20parallel%20data%20is%20the%20key%20challenge%20of%20accent%20conversion%28AC%29%0Aproblem%20in%20which%20both%20the%20pronunciation%20units%20and%20prosody%20pattern%20need%20to%20be%0Aconverted.%20We%20propose%20a%20two-stage%20generative%20framework%20%22convert-and-speak%22%20in%0Awhich%20the%20conversion%20is%20only%20operated%20on%20the%20semantic%20token%20level%20and%20the%0Aspeech%20is%20synthesized%20conditioned%20on%20the%20converted%20semantic%20token%20with%20a%20speech%0Agenerative%20model%20in%20target%20accent%20domain.%20The%20decoupling%20design%20enables%20the%0A%22speaking%22%20module%20to%20use%20massive%20amount%20of%20target%20accent%20speech%20and%20relieves%0Athe%20parallel%20data%20required%20for%20the%20%22conversion%22%20module.%20Conversion%20with%20the%0Abridge%20of%20semantic%20token%20also%20relieves%20the%20requirement%20for%20the%20data%20with%20text%0Atranscriptions%20and%20unlocks%20the%20usage%20of%20language%20pre-training%20technology%20to%0Afurther%20efficiently%20reduce%20the%20need%20of%20parallel%20accent%20speech%20data.%20To%20reduce%0Athe%20complexity%20and%20latency%20of%20%22speaking%22%2C%20a%20single-stage%20AR%20generative%20model%20is%0Adesigned%20to%20achieve%20good%20quality%20as%20well%20as%20lower%20computation%20cost.%20Experiments%0Aon%20Indian-English%20to%20general%20American-English%20conversion%20show%20that%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20accent%20similarity%2C%20speech%0Aquality%2C%20and%20speaker%20maintenance%20with%20only%2015%20minutes%20of%20weakly%20parallel%20data%0Awhich%20is%20not%20constrained%20to%20the%20same%20speaker.%20Extensive%20experimentation%20with%0Adiverse%20accent%20types%20suggests%20that%20this%20framework%20possesses%20a%20high%20degree%20of%0Aadaptability%2C%20making%20it%20readily%20scalable%20to%20accommodate%20other%20accents%20with%0Alow-resource%20data.%20Audio%20samples%20are%20available%20at%0Ahttps%3A//www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10096v1&entry.124074799=Read"},
{"title": "Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic\n  Bias in Facial Recognition", "author": "Rafael M. Mamede and Pedro C. Neto and Ana F. Sequeira", "abstract": "  This study investigates the effects of occlusions on the fairness of face\nrecognition systems, particularly focusing on demographic biases. Using the\nRacial Faces in the Wild (RFW) dataset and synthetically added realistic\nocclusions, we evaluate their effect on the performance of face recognition\nmodels trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We note\nincreases in the dispersion of FMR, FNMR, and accuracy alongside decreases in\nfairness according to Equilized Odds, Demographic Parity, STD of Accuracy, and\nFairness Discrepancy Rate. Additionally, we utilize a pixel attribution method\nto understand the importance of occlusions in model predictions, proposing a\nnew metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent to\nwhich occlusions affect model performance across different demographic groups.\nOur results indicate that occlusions exacerbate existing demographic biases,\nwith models placing higher importance on occlusions in an unequal fashion,\nparticularly affecting African individuals more severely.\n", "link": "http://arxiv.org/abs/2408.10175v1", "date": "2024-08-19", "relevancy": 1.9721, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Under%20Cover%3A%20Evaluating%20the%20Impact%20of%20Occlusions%20on%20Demographic%0A%20%20Bias%20in%20Facial%20Recognition&body=Title%3A%20Fairness%20Under%20Cover%3A%20Evaluating%20the%20Impact%20of%20Occlusions%20on%20Demographic%0A%20%20Bias%20in%20Facial%20Recognition%0AAuthor%3A%20Rafael%20M.%20Mamede%20and%20Pedro%20C.%20Neto%20and%20Ana%20F.%20Sequeira%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20effects%20of%20occlusions%20on%20the%20fairness%20of%20face%0Arecognition%20systems%2C%20particularly%20focusing%20on%20demographic%20biases.%20Using%20the%0ARacial%20Faces%20in%20the%20Wild%20%28RFW%29%20dataset%20and%20synthetically%20added%20realistic%0Aocclusions%2C%20we%20evaluate%20their%20effect%20on%20the%20performance%20of%20face%20recognition%0Amodels%20trained%20on%20the%20BUPT-Balanced%20and%20BUPT-GlobalFace%20datasets.%20We%20note%0Aincreases%20in%20the%20dispersion%20of%20FMR%2C%20FNMR%2C%20and%20accuracy%20alongside%20decreases%20in%0Afairness%20according%20to%20Equilized%20Odds%2C%20Demographic%20Parity%2C%20STD%20of%20Accuracy%2C%20and%0AFairness%20Discrepancy%20Rate.%20Additionally%2C%20we%20utilize%20a%20pixel%20attribution%20method%0Ato%20understand%20the%20importance%20of%20occlusions%20in%20model%20predictions%2C%20proposing%20a%0Anew%20metric%2C%20Face%20Occlusion%20Impact%20Ratio%20%28FOIR%29%2C%20that%20quantifies%20the%20extent%20to%0Awhich%20occlusions%20affect%20model%20performance%20across%20different%20demographic%20groups.%0AOur%20results%20indicate%20that%20occlusions%20exacerbate%20existing%20demographic%20biases%2C%0Awith%20models%20placing%20higher%20importance%20on%20occlusions%20in%20an%20unequal%20fashion%2C%0Aparticularly%20affecting%20African%20individuals%20more%20severely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Under%2520Cover%253A%2520Evaluating%2520the%2520Impact%2520of%2520Occlusions%2520on%2520Demographic%250A%2520%2520Bias%2520in%2520Facial%2520Recognition%26entry.906535625%3DRafael%2520M.%2520Mamede%2520and%2520Pedro%2520C.%2520Neto%2520and%2520Ana%2520F.%2520Sequeira%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520effects%2520of%2520occlusions%2520on%2520the%2520fairness%2520of%2520face%250Arecognition%2520systems%252C%2520particularly%2520focusing%2520on%2520demographic%2520biases.%2520Using%2520the%250ARacial%2520Faces%2520in%2520the%2520Wild%2520%2528RFW%2529%2520dataset%2520and%2520synthetically%2520added%2520realistic%250Aocclusions%252C%2520we%2520evaluate%2520their%2520effect%2520on%2520the%2520performance%2520of%2520face%2520recognition%250Amodels%2520trained%2520on%2520the%2520BUPT-Balanced%2520and%2520BUPT-GlobalFace%2520datasets.%2520We%2520note%250Aincreases%2520in%2520the%2520dispersion%2520of%2520FMR%252C%2520FNMR%252C%2520and%2520accuracy%2520alongside%2520decreases%2520in%250Afairness%2520according%2520to%2520Equilized%2520Odds%252C%2520Demographic%2520Parity%252C%2520STD%2520of%2520Accuracy%252C%2520and%250AFairness%2520Discrepancy%2520Rate.%2520Additionally%252C%2520we%2520utilize%2520a%2520pixel%2520attribution%2520method%250Ato%2520understand%2520the%2520importance%2520of%2520occlusions%2520in%2520model%2520predictions%252C%2520proposing%2520a%250Anew%2520metric%252C%2520Face%2520Occlusion%2520Impact%2520Ratio%2520%2528FOIR%2529%252C%2520that%2520quantifies%2520the%2520extent%2520to%250Awhich%2520occlusions%2520affect%2520model%2520performance%2520across%2520different%2520demographic%2520groups.%250AOur%2520results%2520indicate%2520that%2520occlusions%2520exacerbate%2520existing%2520demographic%2520biases%252C%250Awith%2520models%2520placing%2520higher%2520importance%2520on%2520occlusions%2520in%2520an%2520unequal%2520fashion%252C%250Aparticularly%2520affecting%2520African%2520individuals%2520more%2520severely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Under%20Cover%3A%20Evaluating%20the%20Impact%20of%20Occlusions%20on%20Demographic%0A%20%20Bias%20in%20Facial%20Recognition&entry.906535625=Rafael%20M.%20Mamede%20and%20Pedro%20C.%20Neto%20and%20Ana%20F.%20Sequeira&entry.1292438233=%20%20This%20study%20investigates%20the%20effects%20of%20occlusions%20on%20the%20fairness%20of%20face%0Arecognition%20systems%2C%20particularly%20focusing%20on%20demographic%20biases.%20Using%20the%0ARacial%20Faces%20in%20the%20Wild%20%28RFW%29%20dataset%20and%20synthetically%20added%20realistic%0Aocclusions%2C%20we%20evaluate%20their%20effect%20on%20the%20performance%20of%20face%20recognition%0Amodels%20trained%20on%20the%20BUPT-Balanced%20and%20BUPT-GlobalFace%20datasets.%20We%20note%0Aincreases%20in%20the%20dispersion%20of%20FMR%2C%20FNMR%2C%20and%20accuracy%20alongside%20decreases%20in%0Afairness%20according%20to%20Equilized%20Odds%2C%20Demographic%20Parity%2C%20STD%20of%20Accuracy%2C%20and%0AFairness%20Discrepancy%20Rate.%20Additionally%2C%20we%20utilize%20a%20pixel%20attribution%20method%0Ato%20understand%20the%20importance%20of%20occlusions%20in%20model%20predictions%2C%20proposing%20a%0Anew%20metric%2C%20Face%20Occlusion%20Impact%20Ratio%20%28FOIR%29%2C%20that%20quantifies%20the%20extent%20to%0Awhich%20occlusions%20affect%20model%20performance%20across%20different%20demographic%20groups.%0AOur%20results%20indicate%20that%20occlusions%20exacerbate%20existing%20demographic%20biases%2C%0Awith%20models%20placing%20higher%20importance%20on%20occlusions%20in%20an%20unequal%20fashion%2C%0Aparticularly%20affecting%20African%20individuals%20more%20severely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10175v1&entry.124074799=Read"},
{"title": "SZU-AFS Antispoofing System for the ASVspoof 5 Challenge", "author": "Yuxiong Xu and Jiafeng Zhong and Sengui Zheng and Zefeng Liu and Bin Li", "abstract": "  This paper presents the SZU-AFS anti-spoofing system, designed for Track 1 of\nthe ASVspoof 5 Challenge under open conditions. The system is built with four\nstages: selecting a baseline model, exploring effective data augmentation (DA)\nmethods for fine-tuning, applying a co-enhancement strategy based on gradient\nnorm aware minimization (GAM) for secondary fine-tuning, and fusing logits\nscores from the two best-performing fine-tuned models. The system utilizes the\nWav2Vec2 front-end feature extractor and the AASIST back-end classifier as the\nbaseline model. During model fine-tuning, three distinct DA policies have been\ninvestigated: single-DA, random-DA, and cascade-DA. Moreover, the employed\nGAM-based co-enhancement strategy, designed to fine-tune the augmented model at\nboth data and optimizer levels, helps the Adam optimizer find flatter minima,\nthereby boosting model generalization. Overall, the final fusion system\nachieves a minDCF of 0.115 and an EER of 4.04% on the evaluation set.\n", "link": "http://arxiv.org/abs/2408.09933v1", "date": "2024-08-19", "relevancy": 1.9619, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5081}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SZU-AFS%20Antispoofing%20System%20for%20the%20ASVspoof%205%20Challenge&body=Title%3A%20SZU-AFS%20Antispoofing%20System%20for%20the%20ASVspoof%205%20Challenge%0AAuthor%3A%20Yuxiong%20Xu%20and%20Jiafeng%20Zhong%20and%20Sengui%20Zheng%20and%20Zefeng%20Liu%20and%20Bin%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20SZU-AFS%20anti-spoofing%20system%2C%20designed%20for%20Track%201%20of%0Athe%20ASVspoof%205%20Challenge%20under%20open%20conditions.%20The%20system%20is%20built%20with%20four%0Astages%3A%20selecting%20a%20baseline%20model%2C%20exploring%20effective%20data%20augmentation%20%28DA%29%0Amethods%20for%20fine-tuning%2C%20applying%20a%20co-enhancement%20strategy%20based%20on%20gradient%0Anorm%20aware%20minimization%20%28GAM%29%20for%20secondary%20fine-tuning%2C%20and%20fusing%20logits%0Ascores%20from%20the%20two%20best-performing%20fine-tuned%20models.%20The%20system%20utilizes%20the%0AWav2Vec2%20front-end%20feature%20extractor%20and%20the%20AASIST%20back-end%20classifier%20as%20the%0Abaseline%20model.%20During%20model%20fine-tuning%2C%20three%20distinct%20DA%20policies%20have%20been%0Ainvestigated%3A%20single-DA%2C%20random-DA%2C%20and%20cascade-DA.%20Moreover%2C%20the%20employed%0AGAM-based%20co-enhancement%20strategy%2C%20designed%20to%20fine-tune%20the%20augmented%20model%20at%0Aboth%20data%20and%20optimizer%20levels%2C%20helps%20the%20Adam%20optimizer%20find%20flatter%20minima%2C%0Athereby%20boosting%20model%20generalization.%20Overall%2C%20the%20final%20fusion%20system%0Aachieves%20a%20minDCF%20of%200.115%20and%20an%20EER%20of%204.04%25%20on%20the%20evaluation%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSZU-AFS%2520Antispoofing%2520System%2520for%2520the%2520ASVspoof%25205%2520Challenge%26entry.906535625%3DYuxiong%2520Xu%2520and%2520Jiafeng%2520Zhong%2520and%2520Sengui%2520Zheng%2520and%2520Zefeng%2520Liu%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520SZU-AFS%2520anti-spoofing%2520system%252C%2520designed%2520for%2520Track%25201%2520of%250Athe%2520ASVspoof%25205%2520Challenge%2520under%2520open%2520conditions.%2520The%2520system%2520is%2520built%2520with%2520four%250Astages%253A%2520selecting%2520a%2520baseline%2520model%252C%2520exploring%2520effective%2520data%2520augmentation%2520%2528DA%2529%250Amethods%2520for%2520fine-tuning%252C%2520applying%2520a%2520co-enhancement%2520strategy%2520based%2520on%2520gradient%250Anorm%2520aware%2520minimization%2520%2528GAM%2529%2520for%2520secondary%2520fine-tuning%252C%2520and%2520fusing%2520logits%250Ascores%2520from%2520the%2520two%2520best-performing%2520fine-tuned%2520models.%2520The%2520system%2520utilizes%2520the%250AWav2Vec2%2520front-end%2520feature%2520extractor%2520and%2520the%2520AASIST%2520back-end%2520classifier%2520as%2520the%250Abaseline%2520model.%2520During%2520model%2520fine-tuning%252C%2520three%2520distinct%2520DA%2520policies%2520have%2520been%250Ainvestigated%253A%2520single-DA%252C%2520random-DA%252C%2520and%2520cascade-DA.%2520Moreover%252C%2520the%2520employed%250AGAM-based%2520co-enhancement%2520strategy%252C%2520designed%2520to%2520fine-tune%2520the%2520augmented%2520model%2520at%250Aboth%2520data%2520and%2520optimizer%2520levels%252C%2520helps%2520the%2520Adam%2520optimizer%2520find%2520flatter%2520minima%252C%250Athereby%2520boosting%2520model%2520generalization.%2520Overall%252C%2520the%2520final%2520fusion%2520system%250Aachieves%2520a%2520minDCF%2520of%25200.115%2520and%2520an%2520EER%2520of%25204.04%2525%2520on%2520the%2520evaluation%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SZU-AFS%20Antispoofing%20System%20for%20the%20ASVspoof%205%20Challenge&entry.906535625=Yuxiong%20Xu%20and%20Jiafeng%20Zhong%20and%20Sengui%20Zheng%20and%20Zefeng%20Liu%20and%20Bin%20Li&entry.1292438233=%20%20This%20paper%20presents%20the%20SZU-AFS%20anti-spoofing%20system%2C%20designed%20for%20Track%201%20of%0Athe%20ASVspoof%205%20Challenge%20under%20open%20conditions.%20The%20system%20is%20built%20with%20four%0Astages%3A%20selecting%20a%20baseline%20model%2C%20exploring%20effective%20data%20augmentation%20%28DA%29%0Amethods%20for%20fine-tuning%2C%20applying%20a%20co-enhancement%20strategy%20based%20on%20gradient%0Anorm%20aware%20minimization%20%28GAM%29%20for%20secondary%20fine-tuning%2C%20and%20fusing%20logits%0Ascores%20from%20the%20two%20best-performing%20fine-tuned%20models.%20The%20system%20utilizes%20the%0AWav2Vec2%20front-end%20feature%20extractor%20and%20the%20AASIST%20back-end%20classifier%20as%20the%0Abaseline%20model.%20During%20model%20fine-tuning%2C%20three%20distinct%20DA%20policies%20have%20been%0Ainvestigated%3A%20single-DA%2C%20random-DA%2C%20and%20cascade-DA.%20Moreover%2C%20the%20employed%0AGAM-based%20co-enhancement%20strategy%2C%20designed%20to%20fine-tune%20the%20augmented%20model%20at%0Aboth%20data%20and%20optimizer%20levels%2C%20helps%20the%20Adam%20optimizer%20find%20flatter%20minima%2C%0Athereby%20boosting%20model%20generalization.%20Overall%2C%20the%20final%20fusion%20system%0Aachieves%20a%20minDCF%20of%200.115%20and%20an%20EER%20of%204.04%25%20on%20the%20evaluation%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09933v1&entry.124074799=Read"},
{"title": "Unsupervised Machine Learning Hybrid Approach Integrating Linear\n  Programming in Loss Function: A Robust Optimization Technique", "author": "Andrew Kiruluta and Andreas Lemos", "abstract": "  This paper presents a novel hybrid approach that integrates linear\nprogramming (LP) within the loss function of an unsupervised machine learning\nmodel. By leveraging the strengths of both optimization techniques and machine\nlearning, this method introduces a robust framework for solving complex\noptimization problems where traditional methods may fall short. The proposed\napproach encapsulates the constraints and objectives of a linear programming\nproblem directly into the loss function, guiding the learning process to adhere\nto these constraints while optimizing the desired outcomes. This technique not\nonly preserves the interpretability of linear programming but also benefits\nfrom the flexibility and adaptability of machine learning, making it\nparticularly well-suited for unsupervised or semi-supervised learning\nscenarios.\n", "link": "http://arxiv.org/abs/2408.09967v1", "date": "2024-08-19", "relevancy": 1.9609, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique&body=Title%3A%20Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique%0AAuthor%3A%20Andrew%20Kiruluta%20and%20Andreas%20Lemos%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20hybrid%20approach%20that%20integrates%20linear%0Aprogramming%20%28LP%29%20within%20the%20loss%20function%20of%20an%20unsupervised%20machine%20learning%0Amodel.%20By%20leveraging%20the%20strengths%20of%20both%20optimization%20techniques%20and%20machine%0Alearning%2C%20this%20method%20introduces%20a%20robust%20framework%20for%20solving%20complex%0Aoptimization%20problems%20where%20traditional%20methods%20may%20fall%20short.%20The%20proposed%0Aapproach%20encapsulates%20the%20constraints%20and%20objectives%20of%20a%20linear%20programming%0Aproblem%20directly%20into%20the%20loss%20function%2C%20guiding%20the%20learning%20process%20to%20adhere%0Ato%20these%20constraints%20while%20optimizing%20the%20desired%20outcomes.%20This%20technique%20not%0Aonly%20preserves%20the%20interpretability%20of%20linear%20programming%20but%20also%20benefits%0Afrom%20the%20flexibility%20and%20adaptability%20of%20machine%20learning%2C%20making%20it%0Aparticularly%20well-suited%20for%20unsupervised%20or%20semi-supervised%20learning%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Machine%2520Learning%2520Hybrid%2520Approach%2520Integrating%2520Linear%250A%2520%2520Programming%2520in%2520Loss%2520Function%253A%2520A%2520Robust%2520Optimization%2520Technique%26entry.906535625%3DAndrew%2520Kiruluta%2520and%2520Andreas%2520Lemos%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520hybrid%2520approach%2520that%2520integrates%2520linear%250Aprogramming%2520%2528LP%2529%2520within%2520the%2520loss%2520function%2520of%2520an%2520unsupervised%2520machine%2520learning%250Amodel.%2520By%2520leveraging%2520the%2520strengths%2520of%2520both%2520optimization%2520techniques%2520and%2520machine%250Alearning%252C%2520this%2520method%2520introduces%2520a%2520robust%2520framework%2520for%2520solving%2520complex%250Aoptimization%2520problems%2520where%2520traditional%2520methods%2520may%2520fall%2520short.%2520The%2520proposed%250Aapproach%2520encapsulates%2520the%2520constraints%2520and%2520objectives%2520of%2520a%2520linear%2520programming%250Aproblem%2520directly%2520into%2520the%2520loss%2520function%252C%2520guiding%2520the%2520learning%2520process%2520to%2520adhere%250Ato%2520these%2520constraints%2520while%2520optimizing%2520the%2520desired%2520outcomes.%2520This%2520technique%2520not%250Aonly%2520preserves%2520the%2520interpretability%2520of%2520linear%2520programming%2520but%2520also%2520benefits%250Afrom%2520the%2520flexibility%2520and%2520adaptability%2520of%2520machine%2520learning%252C%2520making%2520it%250Aparticularly%2520well-suited%2520for%2520unsupervised%2520or%2520semi-supervised%2520learning%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique&entry.906535625=Andrew%20Kiruluta%20and%20Andreas%20Lemos&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20hybrid%20approach%20that%20integrates%20linear%0Aprogramming%20%28LP%29%20within%20the%20loss%20function%20of%20an%20unsupervised%20machine%20learning%0Amodel.%20By%20leveraging%20the%20strengths%20of%20both%20optimization%20techniques%20and%20machine%0Alearning%2C%20this%20method%20introduces%20a%20robust%20framework%20for%20solving%20complex%0Aoptimization%20problems%20where%20traditional%20methods%20may%20fall%20short.%20The%20proposed%0Aapproach%20encapsulates%20the%20constraints%20and%20objectives%20of%20a%20linear%20programming%0Aproblem%20directly%20into%20the%20loss%20function%2C%20guiding%20the%20learning%20process%20to%20adhere%0Ato%20these%20constraints%20while%20optimizing%20the%20desired%20outcomes.%20This%20technique%20not%0Aonly%20preserves%20the%20interpretability%20of%20linear%20programming%20but%20also%20benefits%0Afrom%20the%20flexibility%20and%20adaptability%20of%20machine%20learning%2C%20making%20it%0Aparticularly%20well-suited%20for%20unsupervised%20or%20semi-supervised%20learning%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09967v1&entry.124074799=Read"},
{"title": "Semantic Prototypes: Enhancing Transparency Without Black Boxes", "author": "Orfeas Menis-Mastromichalakis and Giorgos Filandrianos and Jason Liartis and Edmund Dervakos and Giorgos Stamou", "abstract": "  As machine learning (ML) models and datasets increase in complexity, the\ndemand for methods that enhance explainability and interpretability becomes\nparamount. Prototypes, by encapsulating essential characteristics within data,\noffer insights that enable tactical decision-making and enhance transparency.\nTraditional prototype methods often rely on sub-symbolic raw data and opaque\nlatent spaces, reducing explainability and increasing the risk of\nmisinterpretations. This paper presents a novel framework that utilizes\nsemantic descriptions to define prototypes and provide clear explanations,\neffectively addressing the shortcomings of conventional methods. Our approach\nleverages concept-based descriptions to cluster data on the semantic level,\nensuring that prototypes not only represent underlying properties intuitively\nbut are also straightforward to interpret. Our method simplifies the\ninterpretative process and effectively bridges the gap between complex data\nstructures and human cognitive processes, thereby enhancing transparency and\nfostering trust. Our approach outperforms existing widely-used prototype\nmethods in facilitating human understanding and informativeness, as validated\nthrough a user survey.\n", "link": "http://arxiv.org/abs/2407.15871v3", "date": "2024-08-19", "relevancy": 1.9461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes&body=Title%3A%20Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes%0AAuthor%3A%20Orfeas%20Menis-Mastromichalakis%20and%20Giorgos%20Filandrianos%20and%20Jason%20Liartis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20As%20machine%20learning%20%28ML%29%20models%20and%20datasets%20increase%20in%20complexity%2C%20the%0Ademand%20for%20methods%20that%20enhance%20explainability%20and%20interpretability%20becomes%0Aparamount.%20Prototypes%2C%20by%20encapsulating%20essential%20characteristics%20within%20data%2C%0Aoffer%20insights%20that%20enable%20tactical%20decision-making%20and%20enhance%20transparency.%0ATraditional%20prototype%20methods%20often%20rely%20on%20sub-symbolic%20raw%20data%20and%20opaque%0Alatent%20spaces%2C%20reducing%20explainability%20and%20increasing%20the%20risk%20of%0Amisinterpretations.%20This%20paper%20presents%20a%20novel%20framework%20that%20utilizes%0Asemantic%20descriptions%20to%20define%20prototypes%20and%20provide%20clear%20explanations%2C%0Aeffectively%20addressing%20the%20shortcomings%20of%20conventional%20methods.%20Our%20approach%0Aleverages%20concept-based%20descriptions%20to%20cluster%20data%20on%20the%20semantic%20level%2C%0Aensuring%20that%20prototypes%20not%20only%20represent%20underlying%20properties%20intuitively%0Abut%20are%20also%20straightforward%20to%20interpret.%20Our%20method%20simplifies%20the%0Ainterpretative%20process%20and%20effectively%20bridges%20the%20gap%20between%20complex%20data%0Astructures%20and%20human%20cognitive%20processes%2C%20thereby%20enhancing%20transparency%20and%0Afostering%20trust.%20Our%20approach%20outperforms%20existing%20widely-used%20prototype%0Amethods%20in%20facilitating%20human%20understanding%20and%20informativeness%2C%20as%20validated%0Athrough%20a%20user%20survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15871v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Prototypes%253A%2520Enhancing%2520Transparency%2520Without%2520Black%2520Boxes%26entry.906535625%3DOrfeas%2520Menis-Mastromichalakis%2520and%2520Giorgos%2520Filandrianos%2520and%2520Jason%2520Liartis%2520and%2520Edmund%2520Dervakos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520%2528ML%2529%2520models%2520and%2520datasets%2520increase%2520in%2520complexity%252C%2520the%250Ademand%2520for%2520methods%2520that%2520enhance%2520explainability%2520and%2520interpretability%2520becomes%250Aparamount.%2520Prototypes%252C%2520by%2520encapsulating%2520essential%2520characteristics%2520within%2520data%252C%250Aoffer%2520insights%2520that%2520enable%2520tactical%2520decision-making%2520and%2520enhance%2520transparency.%250ATraditional%2520prototype%2520methods%2520often%2520rely%2520on%2520sub-symbolic%2520raw%2520data%2520and%2520opaque%250Alatent%2520spaces%252C%2520reducing%2520explainability%2520and%2520increasing%2520the%2520risk%2520of%250Amisinterpretations.%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520that%2520utilizes%250Asemantic%2520descriptions%2520to%2520define%2520prototypes%2520and%2520provide%2520clear%2520explanations%252C%250Aeffectively%2520addressing%2520the%2520shortcomings%2520of%2520conventional%2520methods.%2520Our%2520approach%250Aleverages%2520concept-based%2520descriptions%2520to%2520cluster%2520data%2520on%2520the%2520semantic%2520level%252C%250Aensuring%2520that%2520prototypes%2520not%2520only%2520represent%2520underlying%2520properties%2520intuitively%250Abut%2520are%2520also%2520straightforward%2520to%2520interpret.%2520Our%2520method%2520simplifies%2520the%250Ainterpretative%2520process%2520and%2520effectively%2520bridges%2520the%2520gap%2520between%2520complex%2520data%250Astructures%2520and%2520human%2520cognitive%2520processes%252C%2520thereby%2520enhancing%2520transparency%2520and%250Afostering%2520trust.%2520Our%2520approach%2520outperforms%2520existing%2520widely-used%2520prototype%250Amethods%2520in%2520facilitating%2520human%2520understanding%2520and%2520informativeness%252C%2520as%2520validated%250Athrough%2520a%2520user%2520survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15871v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes&entry.906535625=Orfeas%20Menis-Mastromichalakis%20and%20Giorgos%20Filandrianos%20and%20Jason%20Liartis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20As%20machine%20learning%20%28ML%29%20models%20and%20datasets%20increase%20in%20complexity%2C%20the%0Ademand%20for%20methods%20that%20enhance%20explainability%20and%20interpretability%20becomes%0Aparamount.%20Prototypes%2C%20by%20encapsulating%20essential%20characteristics%20within%20data%2C%0Aoffer%20insights%20that%20enable%20tactical%20decision-making%20and%20enhance%20transparency.%0ATraditional%20prototype%20methods%20often%20rely%20on%20sub-symbolic%20raw%20data%20and%20opaque%0Alatent%20spaces%2C%20reducing%20explainability%20and%20increasing%20the%20risk%20of%0Amisinterpretations.%20This%20paper%20presents%20a%20novel%20framework%20that%20utilizes%0Asemantic%20descriptions%20to%20define%20prototypes%20and%20provide%20clear%20explanations%2C%0Aeffectively%20addressing%20the%20shortcomings%20of%20conventional%20methods.%20Our%20approach%0Aleverages%20concept-based%20descriptions%20to%20cluster%20data%20on%20the%20semantic%20level%2C%0Aensuring%20that%20prototypes%20not%20only%20represent%20underlying%20properties%20intuitively%0Abut%20are%20also%20straightforward%20to%20interpret.%20Our%20method%20simplifies%20the%0Ainterpretative%20process%20and%20effectively%20bridges%20the%20gap%20between%20complex%20data%0Astructures%20and%20human%20cognitive%20processes%2C%20thereby%20enhancing%20transparency%20and%0Afostering%20trust.%20Our%20approach%20outperforms%20existing%20widely-used%20prototype%0Amethods%20in%20facilitating%20human%20understanding%20and%20informativeness%2C%20as%20validated%0Athrough%20a%20user%20survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15871v3&entry.124074799=Read"},
{"title": "Data Augmentation of Contrastive Learning is Estimating\n  Positive-incentive Noise", "author": "Hongyuan Zhang and Yanchen Xu and Sida Huang and Xuelong Li", "abstract": "  Inspired by the idea of Positive-incentive Noise (Pi-Noise or $\\pi$-Noise)\nthat aims at learning the reliable noise beneficial to tasks, we scientifically\ninvestigate the connection between contrastive learning and $\\pi$-noise in this\npaper. By converting the contrastive loss to an auxiliary Gaussian distribution\nto quantitatively measure the difficulty of the specific contrastive model\nunder the information theory framework, we properly define the task entropy,\nthe core concept of $\\pi$-noise, of contrastive learning. It is further proved\nthat the predefined data augmentation in the standard contrastive learning\nparadigm can be regarded as a kind of point estimation of $\\pi$-noise. Inspired\nby the theoretical study, a framework that develops a $\\pi$-noise generator to\nlearn the beneficial noise (instead of estimation) as data augmentations for\ncontrast is proposed. The designed framework can be applied to diverse types of\ndata and is also completely compatible with the existing contrastive models.\nFrom the visualization, we surprisingly find that the proposed method\nsuccessfully learns effective augmentations.\n", "link": "http://arxiv.org/abs/2408.09929v1", "date": "2024-08-19", "relevancy": 1.9394, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4848}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20of%20Contrastive%20Learning%20is%20Estimating%0A%20%20Positive-incentive%20Noise&body=Title%3A%20Data%20Augmentation%20of%20Contrastive%20Learning%20is%20Estimating%0A%20%20Positive-incentive%20Noise%0AAuthor%3A%20Hongyuan%20Zhang%20and%20Yanchen%20Xu%20and%20Sida%20Huang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Inspired%20by%20the%20idea%20of%20Positive-incentive%20Noise%20%28Pi-Noise%20or%20%24%5Cpi%24-Noise%29%0Athat%20aims%20at%20learning%20the%20reliable%20noise%20beneficial%20to%20tasks%2C%20we%20scientifically%0Ainvestigate%20the%20connection%20between%20contrastive%20learning%20and%20%24%5Cpi%24-noise%20in%20this%0Apaper.%20By%20converting%20the%20contrastive%20loss%20to%20an%20auxiliary%20Gaussian%20distribution%0Ato%20quantitatively%20measure%20the%20difficulty%20of%20the%20specific%20contrastive%20model%0Aunder%20the%20information%20theory%20framework%2C%20we%20properly%20define%20the%20task%20entropy%2C%0Athe%20core%20concept%20of%20%24%5Cpi%24-noise%2C%20of%20contrastive%20learning.%20It%20is%20further%20proved%0Athat%20the%20predefined%20data%20augmentation%20in%20the%20standard%20contrastive%20learning%0Aparadigm%20can%20be%20regarded%20as%20a%20kind%20of%20point%20estimation%20of%20%24%5Cpi%24-noise.%20Inspired%0Aby%20the%20theoretical%20study%2C%20a%20framework%20that%20develops%20a%20%24%5Cpi%24-noise%20generator%20to%0Alearn%20the%20beneficial%20noise%20%28instead%20of%20estimation%29%20as%20data%20augmentations%20for%0Acontrast%20is%20proposed.%20The%20designed%20framework%20can%20be%20applied%20to%20diverse%20types%20of%0Adata%20and%20is%20also%20completely%20compatible%20with%20the%20existing%20contrastive%20models.%0AFrom%20the%20visualization%2C%20we%20surprisingly%20find%20that%20the%20proposed%20method%0Asuccessfully%20learns%20effective%20augmentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520of%2520Contrastive%2520Learning%2520is%2520Estimating%250A%2520%2520Positive-incentive%2520Noise%26entry.906535625%3DHongyuan%2520Zhang%2520and%2520Yanchen%2520Xu%2520and%2520Sida%2520Huang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520idea%2520of%2520Positive-incentive%2520Noise%2520%2528Pi-Noise%2520or%2520%2524%255Cpi%2524-Noise%2529%250Athat%2520aims%2520at%2520learning%2520the%2520reliable%2520noise%2520beneficial%2520to%2520tasks%252C%2520we%2520scientifically%250Ainvestigate%2520the%2520connection%2520between%2520contrastive%2520learning%2520and%2520%2524%255Cpi%2524-noise%2520in%2520this%250Apaper.%2520By%2520converting%2520the%2520contrastive%2520loss%2520to%2520an%2520auxiliary%2520Gaussian%2520distribution%250Ato%2520quantitatively%2520measure%2520the%2520difficulty%2520of%2520the%2520specific%2520contrastive%2520model%250Aunder%2520the%2520information%2520theory%2520framework%252C%2520we%2520properly%2520define%2520the%2520task%2520entropy%252C%250Athe%2520core%2520concept%2520of%2520%2524%255Cpi%2524-noise%252C%2520of%2520contrastive%2520learning.%2520It%2520is%2520further%2520proved%250Athat%2520the%2520predefined%2520data%2520augmentation%2520in%2520the%2520standard%2520contrastive%2520learning%250Aparadigm%2520can%2520be%2520regarded%2520as%2520a%2520kind%2520of%2520point%2520estimation%2520of%2520%2524%255Cpi%2524-noise.%2520Inspired%250Aby%2520the%2520theoretical%2520study%252C%2520a%2520framework%2520that%2520develops%2520a%2520%2524%255Cpi%2524-noise%2520generator%2520to%250Alearn%2520the%2520beneficial%2520noise%2520%2528instead%2520of%2520estimation%2529%2520as%2520data%2520augmentations%2520for%250Acontrast%2520is%2520proposed.%2520The%2520designed%2520framework%2520can%2520be%2520applied%2520to%2520diverse%2520types%2520of%250Adata%2520and%2520is%2520also%2520completely%2520compatible%2520with%2520the%2520existing%2520contrastive%2520models.%250AFrom%2520the%2520visualization%252C%2520we%2520surprisingly%2520find%2520that%2520the%2520proposed%2520method%250Asuccessfully%2520learns%2520effective%2520augmentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20of%20Contrastive%20Learning%20is%20Estimating%0A%20%20Positive-incentive%20Noise&entry.906535625=Hongyuan%20Zhang%20and%20Yanchen%20Xu%20and%20Sida%20Huang%20and%20Xuelong%20Li&entry.1292438233=%20%20Inspired%20by%20the%20idea%20of%20Positive-incentive%20Noise%20%28Pi-Noise%20or%20%24%5Cpi%24-Noise%29%0Athat%20aims%20at%20learning%20the%20reliable%20noise%20beneficial%20to%20tasks%2C%20we%20scientifically%0Ainvestigate%20the%20connection%20between%20contrastive%20learning%20and%20%24%5Cpi%24-noise%20in%20this%0Apaper.%20By%20converting%20the%20contrastive%20loss%20to%20an%20auxiliary%20Gaussian%20distribution%0Ato%20quantitatively%20measure%20the%20difficulty%20of%20the%20specific%20contrastive%20model%0Aunder%20the%20information%20theory%20framework%2C%20we%20properly%20define%20the%20task%20entropy%2C%0Athe%20core%20concept%20of%20%24%5Cpi%24-noise%2C%20of%20contrastive%20learning.%20It%20is%20further%20proved%0Athat%20the%20predefined%20data%20augmentation%20in%20the%20standard%20contrastive%20learning%0Aparadigm%20can%20be%20regarded%20as%20a%20kind%20of%20point%20estimation%20of%20%24%5Cpi%24-noise.%20Inspired%0Aby%20the%20theoretical%20study%2C%20a%20framework%20that%20develops%20a%20%24%5Cpi%24-noise%20generator%20to%0Alearn%20the%20beneficial%20noise%20%28instead%20of%20estimation%29%20as%20data%20augmentations%20for%0Acontrast%20is%20proposed.%20The%20designed%20framework%20can%20be%20applied%20to%20diverse%20types%20of%0Adata%20and%20is%20also%20completely%20compatible%20with%20the%20existing%20contrastive%20models.%0AFrom%20the%20visualization%2C%20we%20surprisingly%20find%20that%20the%20proposed%20method%0Asuccessfully%20learns%20effective%20augmentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09929v1&entry.124074799=Read"},
{"title": "Towards a Benchmark for Colorectal Cancer Segmentation in Endorectal\n  Ultrasound Videos: Dataset and Model Development", "author": "Yuncheng Jiang and Yiwen Hu and Zixun Zhang and Jun Wei and Chun-Mei Feng and Xuemei Tang and Xiang Wan and Yong Liu and Shuguang Cui and Zhen Li", "abstract": "  Endorectal ultrasound (ERUS) is an important imaging modality that provides\nhigh reliability for diagnosing the depth and boundary of invasion in\ncolorectal cancer. However, the lack of a large-scale ERUS dataset with\nhigh-quality annotations hinders the development of automatic ultrasound\ndiagnostics. In this paper, we collected and annotated the first benchmark\ndataset that covers diverse ERUS scenarios, i.e. colorectal cancer\nsegmentation, detection, and infiltration depth staging. Our ERUS-10K dataset\ncomprises 77 videos and 10,000 high-resolution annotated frames. Based on this\ndataset, we further introduce a benchmark model for colorectal cancer\nsegmentation, named the Adaptive Sparse-context TRansformer (ASTR). ASTR is\ndesigned based on three considerations: scanning mode discrepancy, temporal\ninformation, and low computational complexity. For generalizing to different\nscanning modes, the adaptive scanning-mode augmentation is proposed to convert\nbetween raw sector images and linear scan ones. For mining temporal\ninformation, the sparse-context transformer is incorporated to integrate\ninter-frame local and global features. For reducing computational complexity,\nthe sparse-context block is introduced to extract contextual features from\nauxiliary frames. Finally, on the benchmark dataset, the proposed ASTR model\nachieves a 77.6% Dice score in rectal cancer segmentation, largely\noutperforming previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.10067v1", "date": "2024-08-19", "relevancy": 1.9387, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.53}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Benchmark%20for%20Colorectal%20Cancer%20Segmentation%20in%20Endorectal%0A%20%20Ultrasound%20Videos%3A%20Dataset%20and%20Model%20Development&body=Title%3A%20Towards%20a%20Benchmark%20for%20Colorectal%20Cancer%20Segmentation%20in%20Endorectal%0A%20%20Ultrasound%20Videos%3A%20Dataset%20and%20Model%20Development%0AAuthor%3A%20Yuncheng%20Jiang%20and%20Yiwen%20Hu%20and%20Zixun%20Zhang%20and%20Jun%20Wei%20and%20Chun-Mei%20Feng%20and%20Xuemei%20Tang%20and%20Xiang%20Wan%20and%20Yong%20Liu%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20%20%20Endorectal%20ultrasound%20%28ERUS%29%20is%20an%20important%20imaging%20modality%20that%20provides%0Ahigh%20reliability%20for%20diagnosing%20the%20depth%20and%20boundary%20of%20invasion%20in%0Acolorectal%20cancer.%20However%2C%20the%20lack%20of%20a%20large-scale%20ERUS%20dataset%20with%0Ahigh-quality%20annotations%20hinders%20the%20development%20of%20automatic%20ultrasound%0Adiagnostics.%20In%20this%20paper%2C%20we%20collected%20and%20annotated%20the%20first%20benchmark%0Adataset%20that%20covers%20diverse%20ERUS%20scenarios%2C%20i.e.%20colorectal%20cancer%0Asegmentation%2C%20detection%2C%20and%20infiltration%20depth%20staging.%20Our%20ERUS-10K%20dataset%0Acomprises%2077%20videos%20and%2010%2C000%20high-resolution%20annotated%20frames.%20Based%20on%20this%0Adataset%2C%20we%20further%20introduce%20a%20benchmark%20model%20for%20colorectal%20cancer%0Asegmentation%2C%20named%20the%20Adaptive%20Sparse-context%20TRansformer%20%28ASTR%29.%20ASTR%20is%0Adesigned%20based%20on%20three%20considerations%3A%20scanning%20mode%20discrepancy%2C%20temporal%0Ainformation%2C%20and%20low%20computational%20complexity.%20For%20generalizing%20to%20different%0Ascanning%20modes%2C%20the%20adaptive%20scanning-mode%20augmentation%20is%20proposed%20to%20convert%0Abetween%20raw%20sector%20images%20and%20linear%20scan%20ones.%20For%20mining%20temporal%0Ainformation%2C%20the%20sparse-context%20transformer%20is%20incorporated%20to%20integrate%0Ainter-frame%20local%20and%20global%20features.%20For%20reducing%20computational%20complexity%2C%0Athe%20sparse-context%20block%20is%20introduced%20to%20extract%20contextual%20features%20from%0Aauxiliary%20frames.%20Finally%2C%20on%20the%20benchmark%20dataset%2C%20the%20proposed%20ASTR%20model%0Aachieves%20a%2077.6%25%20Dice%20score%20in%20rectal%20cancer%20segmentation%2C%20largely%0Aoutperforming%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Benchmark%2520for%2520Colorectal%2520Cancer%2520Segmentation%2520in%2520Endorectal%250A%2520%2520Ultrasound%2520Videos%253A%2520Dataset%2520and%2520Model%2520Development%26entry.906535625%3DYuncheng%2520Jiang%2520and%2520Yiwen%2520Hu%2520and%2520Zixun%2520Zhang%2520and%2520Jun%2520Wei%2520and%2520Chun-Mei%2520Feng%2520and%2520Xuemei%2520Tang%2520and%2520Xiang%2520Wan%2520and%2520Yong%2520Liu%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3D%2520%2520Endorectal%2520ultrasound%2520%2528ERUS%2529%2520is%2520an%2520important%2520imaging%2520modality%2520that%2520provides%250Ahigh%2520reliability%2520for%2520diagnosing%2520the%2520depth%2520and%2520boundary%2520of%2520invasion%2520in%250Acolorectal%2520cancer.%2520However%252C%2520the%2520lack%2520of%2520a%2520large-scale%2520ERUS%2520dataset%2520with%250Ahigh-quality%2520annotations%2520hinders%2520the%2520development%2520of%2520automatic%2520ultrasound%250Adiagnostics.%2520In%2520this%2520paper%252C%2520we%2520collected%2520and%2520annotated%2520the%2520first%2520benchmark%250Adataset%2520that%2520covers%2520diverse%2520ERUS%2520scenarios%252C%2520i.e.%2520colorectal%2520cancer%250Asegmentation%252C%2520detection%252C%2520and%2520infiltration%2520depth%2520staging.%2520Our%2520ERUS-10K%2520dataset%250Acomprises%252077%2520videos%2520and%252010%252C000%2520high-resolution%2520annotated%2520frames.%2520Based%2520on%2520this%250Adataset%252C%2520we%2520further%2520introduce%2520a%2520benchmark%2520model%2520for%2520colorectal%2520cancer%250Asegmentation%252C%2520named%2520the%2520Adaptive%2520Sparse-context%2520TRansformer%2520%2528ASTR%2529.%2520ASTR%2520is%250Adesigned%2520based%2520on%2520three%2520considerations%253A%2520scanning%2520mode%2520discrepancy%252C%2520temporal%250Ainformation%252C%2520and%2520low%2520computational%2520complexity.%2520For%2520generalizing%2520to%2520different%250Ascanning%2520modes%252C%2520the%2520adaptive%2520scanning-mode%2520augmentation%2520is%2520proposed%2520to%2520convert%250Abetween%2520raw%2520sector%2520images%2520and%2520linear%2520scan%2520ones.%2520For%2520mining%2520temporal%250Ainformation%252C%2520the%2520sparse-context%2520transformer%2520is%2520incorporated%2520to%2520integrate%250Ainter-frame%2520local%2520and%2520global%2520features.%2520For%2520reducing%2520computational%2520complexity%252C%250Athe%2520sparse-context%2520block%2520is%2520introduced%2520to%2520extract%2520contextual%2520features%2520from%250Aauxiliary%2520frames.%2520Finally%252C%2520on%2520the%2520benchmark%2520dataset%252C%2520the%2520proposed%2520ASTR%2520model%250Aachieves%2520a%252077.6%2525%2520Dice%2520score%2520in%2520rectal%2520cancer%2520segmentation%252C%2520largely%250Aoutperforming%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Benchmark%20for%20Colorectal%20Cancer%20Segmentation%20in%20Endorectal%0A%20%20Ultrasound%20Videos%3A%20Dataset%20and%20Model%20Development&entry.906535625=Yuncheng%20Jiang%20and%20Yiwen%20Hu%20and%20Zixun%20Zhang%20and%20Jun%20Wei%20and%20Chun-Mei%20Feng%20and%20Xuemei%20Tang%20and%20Xiang%20Wan%20and%20Yong%20Liu%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=%20%20Endorectal%20ultrasound%20%28ERUS%29%20is%20an%20important%20imaging%20modality%20that%20provides%0Ahigh%20reliability%20for%20diagnosing%20the%20depth%20and%20boundary%20of%20invasion%20in%0Acolorectal%20cancer.%20However%2C%20the%20lack%20of%20a%20large-scale%20ERUS%20dataset%20with%0Ahigh-quality%20annotations%20hinders%20the%20development%20of%20automatic%20ultrasound%0Adiagnostics.%20In%20this%20paper%2C%20we%20collected%20and%20annotated%20the%20first%20benchmark%0Adataset%20that%20covers%20diverse%20ERUS%20scenarios%2C%20i.e.%20colorectal%20cancer%0Asegmentation%2C%20detection%2C%20and%20infiltration%20depth%20staging.%20Our%20ERUS-10K%20dataset%0Acomprises%2077%20videos%20and%2010%2C000%20high-resolution%20annotated%20frames.%20Based%20on%20this%0Adataset%2C%20we%20further%20introduce%20a%20benchmark%20model%20for%20colorectal%20cancer%0Asegmentation%2C%20named%20the%20Adaptive%20Sparse-context%20TRansformer%20%28ASTR%29.%20ASTR%20is%0Adesigned%20based%20on%20three%20considerations%3A%20scanning%20mode%20discrepancy%2C%20temporal%0Ainformation%2C%20and%20low%20computational%20complexity.%20For%20generalizing%20to%20different%0Ascanning%20modes%2C%20the%20adaptive%20scanning-mode%20augmentation%20is%20proposed%20to%20convert%0Abetween%20raw%20sector%20images%20and%20linear%20scan%20ones.%20For%20mining%20temporal%0Ainformation%2C%20the%20sparse-context%20transformer%20is%20incorporated%20to%20integrate%0Ainter-frame%20local%20and%20global%20features.%20For%20reducing%20computational%20complexity%2C%0Athe%20sparse-context%20block%20is%20introduced%20to%20extract%20contextual%20features%20from%0Aauxiliary%20frames.%20Finally%2C%20on%20the%20benchmark%20dataset%2C%20the%20proposed%20ASTR%20model%0Aachieves%20a%2077.6%25%20Dice%20score%20in%20rectal%20cancer%20segmentation%2C%20largely%0Aoutperforming%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10067v1&entry.124074799=Read"},
{"title": "Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space\n  Constrained MDPs", "author": "Sergio Rozada and Dongsheng Ding and Antonio G. Marques and Alejandro Ribeiro", "abstract": "  We study the problem of computing deterministic optimal policies for\nconstrained Markov decision processes (MDPs) with continuous state and action\nspaces, which are widely encountered in constrained dynamical systems.\nDesigning deterministic policy gradient methods in continuous state and action\nspaces is particularly challenging due to the lack of enumerable state-action\npairs and the adoption of deterministic policies, hindering the application of\nexisting policy gradient methods for constrained MDPs. To this end, we develop\na deterministic policy gradient primal-dual method to find an optimal\ndeterministic policy with non-asymptotic convergence. Specifically, we leverage\nregularization of the Lagrangian of the constrained MDP to propose a\ndeterministic policy gradient primal-dual (D-PGPD) algorithm that updates the\ndeterministic policy via a quadratic-regularized gradient ascent step and the\ndual variable via a quadratic-regularized gradient descent step. We prove that\nthe primal-dual iterates of D-PGPD converge at a sub-linear rate to an optimal\nregularized primal-dual pair. We instantiate D-PGPD with function approximation\nand prove that the primal-dual iterates of D-PGPD converge at a sub-linear rate\nto an optimal regularized primal-dual pair, up to a function approximation\nerror. Furthermore, we demonstrate the effectiveness of our method in two\ncontinuous control problems: robot navigation and fluid control. To the best of\nour knowledge, this appears to be the first work that proposes a deterministic\npolicy search method for continuous-space constrained MDPs.\n", "link": "http://arxiv.org/abs/2408.10015v1", "date": "2024-08-19", "relevancy": 1.9239, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4816}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deterministic%20Policy%20Gradient%20Primal-Dual%20Methods%20for%20Continuous-Space%0A%20%20Constrained%20MDPs&body=Title%3A%20Deterministic%20Policy%20Gradient%20Primal-Dual%20Methods%20for%20Continuous-Space%0A%20%20Constrained%20MDPs%0AAuthor%3A%20Sergio%20Rozada%20and%20Dongsheng%20Ding%20and%20Antonio%20G.%20Marques%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20computing%20deterministic%20optimal%20policies%20for%0Aconstrained%20Markov%20decision%20processes%20%28MDPs%29%20with%20continuous%20state%20and%20action%0Aspaces%2C%20which%20are%20widely%20encountered%20in%20constrained%20dynamical%20systems.%0ADesigning%20deterministic%20policy%20gradient%20methods%20in%20continuous%20state%20and%20action%0Aspaces%20is%20particularly%20challenging%20due%20to%20the%20lack%20of%20enumerable%20state-action%0Apairs%20and%20the%20adoption%20of%20deterministic%20policies%2C%20hindering%20the%20application%20of%0Aexisting%20policy%20gradient%20methods%20for%20constrained%20MDPs.%20To%20this%20end%2C%20we%20develop%0Aa%20deterministic%20policy%20gradient%20primal-dual%20method%20to%20find%20an%20optimal%0Adeterministic%20policy%20with%20non-asymptotic%20convergence.%20Specifically%2C%20we%20leverage%0Aregularization%20of%20the%20Lagrangian%20of%20the%20constrained%20MDP%20to%20propose%20a%0Adeterministic%20policy%20gradient%20primal-dual%20%28D-PGPD%29%20algorithm%20that%20updates%20the%0Adeterministic%20policy%20via%20a%20quadratic-regularized%20gradient%20ascent%20step%20and%20the%0Adual%20variable%20via%20a%20quadratic-regularized%20gradient%20descent%20step.%20We%20prove%20that%0Athe%20primal-dual%20iterates%20of%20D-PGPD%20converge%20at%20a%20sub-linear%20rate%20to%20an%20optimal%0Aregularized%20primal-dual%20pair.%20We%20instantiate%20D-PGPD%20with%20function%20approximation%0Aand%20prove%20that%20the%20primal-dual%20iterates%20of%20D-PGPD%20converge%20at%20a%20sub-linear%20rate%0Ato%20an%20optimal%20regularized%20primal-dual%20pair%2C%20up%20to%20a%20function%20approximation%0Aerror.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20two%0Acontinuous%20control%20problems%3A%20robot%20navigation%20and%20fluid%20control.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20appears%20to%20be%20the%20first%20work%20that%20proposes%20a%20deterministic%0Apolicy%20search%20method%20for%20continuous-space%20constrained%20MDPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeterministic%2520Policy%2520Gradient%2520Primal-Dual%2520Methods%2520for%2520Continuous-Space%250A%2520%2520Constrained%2520MDPs%26entry.906535625%3DSergio%2520Rozada%2520and%2520Dongsheng%2520Ding%2520and%2520Antonio%2520G.%2520Marques%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520computing%2520deterministic%2520optimal%2520policies%2520for%250Aconstrained%2520Markov%2520decision%2520processes%2520%2528MDPs%2529%2520with%2520continuous%2520state%2520and%2520action%250Aspaces%252C%2520which%2520are%2520widely%2520encountered%2520in%2520constrained%2520dynamical%2520systems.%250ADesigning%2520deterministic%2520policy%2520gradient%2520methods%2520in%2520continuous%2520state%2520and%2520action%250Aspaces%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520enumerable%2520state-action%250Apairs%2520and%2520the%2520adoption%2520of%2520deterministic%2520policies%252C%2520hindering%2520the%2520application%2520of%250Aexisting%2520policy%2520gradient%2520methods%2520for%2520constrained%2520MDPs.%2520To%2520this%2520end%252C%2520we%2520develop%250Aa%2520deterministic%2520policy%2520gradient%2520primal-dual%2520method%2520to%2520find%2520an%2520optimal%250Adeterministic%2520policy%2520with%2520non-asymptotic%2520convergence.%2520Specifically%252C%2520we%2520leverage%250Aregularization%2520of%2520the%2520Lagrangian%2520of%2520the%2520constrained%2520MDP%2520to%2520propose%2520a%250Adeterministic%2520policy%2520gradient%2520primal-dual%2520%2528D-PGPD%2529%2520algorithm%2520that%2520updates%2520the%250Adeterministic%2520policy%2520via%2520a%2520quadratic-regularized%2520gradient%2520ascent%2520step%2520and%2520the%250Adual%2520variable%2520via%2520a%2520quadratic-regularized%2520gradient%2520descent%2520step.%2520We%2520prove%2520that%250Athe%2520primal-dual%2520iterates%2520of%2520D-PGPD%2520converge%2520at%2520a%2520sub-linear%2520rate%2520to%2520an%2520optimal%250Aregularized%2520primal-dual%2520pair.%2520We%2520instantiate%2520D-PGPD%2520with%2520function%2520approximation%250Aand%2520prove%2520that%2520the%2520primal-dual%2520iterates%2520of%2520D-PGPD%2520converge%2520at%2520a%2520sub-linear%2520rate%250Ato%2520an%2520optimal%2520regularized%2520primal-dual%2520pair%252C%2520up%2520to%2520a%2520function%2520approximation%250Aerror.%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520two%250Acontinuous%2520control%2520problems%253A%2520robot%2520navigation%2520and%2520fluid%2520control.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520appears%2520to%2520be%2520the%2520first%2520work%2520that%2520proposes%2520a%2520deterministic%250Apolicy%2520search%2520method%2520for%2520continuous-space%2520constrained%2520MDPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deterministic%20Policy%20Gradient%20Primal-Dual%20Methods%20for%20Continuous-Space%0A%20%20Constrained%20MDPs&entry.906535625=Sergio%20Rozada%20and%20Dongsheng%20Ding%20and%20Antonio%20G.%20Marques%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20We%20study%20the%20problem%20of%20computing%20deterministic%20optimal%20policies%20for%0Aconstrained%20Markov%20decision%20processes%20%28MDPs%29%20with%20continuous%20state%20and%20action%0Aspaces%2C%20which%20are%20widely%20encountered%20in%20constrained%20dynamical%20systems.%0ADesigning%20deterministic%20policy%20gradient%20methods%20in%20continuous%20state%20and%20action%0Aspaces%20is%20particularly%20challenging%20due%20to%20the%20lack%20of%20enumerable%20state-action%0Apairs%20and%20the%20adoption%20of%20deterministic%20policies%2C%20hindering%20the%20application%20of%0Aexisting%20policy%20gradient%20methods%20for%20constrained%20MDPs.%20To%20this%20end%2C%20we%20develop%0Aa%20deterministic%20policy%20gradient%20primal-dual%20method%20to%20find%20an%20optimal%0Adeterministic%20policy%20with%20non-asymptotic%20convergence.%20Specifically%2C%20we%20leverage%0Aregularization%20of%20the%20Lagrangian%20of%20the%20constrained%20MDP%20to%20propose%20a%0Adeterministic%20policy%20gradient%20primal-dual%20%28D-PGPD%29%20algorithm%20that%20updates%20the%0Adeterministic%20policy%20via%20a%20quadratic-regularized%20gradient%20ascent%20step%20and%20the%0Adual%20variable%20via%20a%20quadratic-regularized%20gradient%20descent%20step.%20We%20prove%20that%0Athe%20primal-dual%20iterates%20of%20D-PGPD%20converge%20at%20a%20sub-linear%20rate%20to%20an%20optimal%0Aregularized%20primal-dual%20pair.%20We%20instantiate%20D-PGPD%20with%20function%20approximation%0Aand%20prove%20that%20the%20primal-dual%20iterates%20of%20D-PGPD%20converge%20at%20a%20sub-linear%20rate%0Ato%20an%20optimal%20regularized%20primal-dual%20pair%2C%20up%20to%20a%20function%20approximation%0Aerror.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20two%0Acontinuous%20control%20problems%3A%20robot%20navigation%20and%20fluid%20control.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20appears%20to%20be%20the%20first%20work%20that%20proposes%20a%20deterministic%0Apolicy%20search%20method%20for%20continuous-space%20constrained%20MDPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10015v1&entry.124074799=Read"},
{"title": "Active Learning for Identifying Disaster-Related Tweets: A Comparison\n  with Keyword Filtering and Generic Fine-Tuning", "author": "David Hanny and Sebastian Schmidt and Bernd Resch", "abstract": "  Information from social media can provide essential information for emergency\nresponse during natural disasters in near real-time. However, it is difficult\nto identify the disaster-related posts among the large amounts of unstructured\ndata available. Previous methods often use keyword filtering, topic modelling\nor classification-based techniques to identify such posts. Active Learning (AL)\npresents a promising sub-field of Machine Learning (ML) that has not been used\nmuch in the field of text classification of social media content. This study\ntherefore investigates the potential of AL for identifying disaster-related\nTweets. We compare a keyword filtering approach, a RoBERTa model fine-tuned\nwith generic data from CrisisLex, a base RoBERTa model trained with AL and a\nfine-tuned RoBERTa model trained with AL regarding classification performance.\nFor testing, data from CrisisLex and manually labelled data from the 2021 flood\nin Germany and the 2023 Chile forest fires were considered. The results show\nthat generic fine-tuning combined with 10 rounds of AL outperformed all other\napproaches. Consequently, a broadly applicable model for the identification of\ndisaster-related Tweets could be trained with very little labelling effort. The\nmodel can be applied to use cases beyond this study and provides a useful tool\nfor further research in social media analysis.\n", "link": "http://arxiv.org/abs/2408.09914v1", "date": "2024-08-19", "relevancy": 1.9226, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4971}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20Identifying%20Disaster-Related%20Tweets%3A%20A%20Comparison%0A%20%20with%20Keyword%20Filtering%20and%20Generic%20Fine-Tuning&body=Title%3A%20Active%20Learning%20for%20Identifying%20Disaster-Related%20Tweets%3A%20A%20Comparison%0A%20%20with%20Keyword%20Filtering%20and%20Generic%20Fine-Tuning%0AAuthor%3A%20David%20Hanny%20and%20Sebastian%20Schmidt%20and%20Bernd%20Resch%0AAbstract%3A%20%20%20Information%20from%20social%20media%20can%20provide%20essential%20information%20for%20emergency%0Aresponse%20during%20natural%20disasters%20in%20near%20real-time.%20However%2C%20it%20is%20difficult%0Ato%20identify%20the%20disaster-related%20posts%20among%20the%20large%20amounts%20of%20unstructured%0Adata%20available.%20Previous%20methods%20often%20use%20keyword%20filtering%2C%20topic%20modelling%0Aor%20classification-based%20techniques%20to%20identify%20such%20posts.%20Active%20Learning%20%28AL%29%0Apresents%20a%20promising%20sub-field%20of%20Machine%20Learning%20%28ML%29%20that%20has%20not%20been%20used%0Amuch%20in%20the%20field%20of%20text%20classification%20of%20social%20media%20content.%20This%20study%0Atherefore%20investigates%20the%20potential%20of%20AL%20for%20identifying%20disaster-related%0ATweets.%20We%20compare%20a%20keyword%20filtering%20approach%2C%20a%20RoBERTa%20model%20fine-tuned%0Awith%20generic%20data%20from%20CrisisLex%2C%20a%20base%20RoBERTa%20model%20trained%20with%20AL%20and%20a%0Afine-tuned%20RoBERTa%20model%20trained%20with%20AL%20regarding%20classification%20performance.%0AFor%20testing%2C%20data%20from%20CrisisLex%20and%20manually%20labelled%20data%20from%20the%202021%20flood%0Ain%20Germany%20and%20the%202023%20Chile%20forest%20fires%20were%20considered.%20The%20results%20show%0Athat%20generic%20fine-tuning%20combined%20with%2010%20rounds%20of%20AL%20outperformed%20all%20other%0Aapproaches.%20Consequently%2C%20a%20broadly%20applicable%20model%20for%20the%20identification%20of%0Adisaster-related%20Tweets%20could%20be%20trained%20with%20very%20little%20labelling%20effort.%20The%0Amodel%20can%20be%20applied%20to%20use%20cases%20beyond%20this%20study%20and%20provides%20a%20useful%20tool%0Afor%20further%20research%20in%20social%20media%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520Identifying%2520Disaster-Related%2520Tweets%253A%2520A%2520Comparison%250A%2520%2520with%2520Keyword%2520Filtering%2520and%2520Generic%2520Fine-Tuning%26entry.906535625%3DDavid%2520Hanny%2520and%2520Sebastian%2520Schmidt%2520and%2520Bernd%2520Resch%26entry.1292438233%3D%2520%2520Information%2520from%2520social%2520media%2520can%2520provide%2520essential%2520information%2520for%2520emergency%250Aresponse%2520during%2520natural%2520disasters%2520in%2520near%2520real-time.%2520However%252C%2520it%2520is%2520difficult%250Ato%2520identify%2520the%2520disaster-related%2520posts%2520among%2520the%2520large%2520amounts%2520of%2520unstructured%250Adata%2520available.%2520Previous%2520methods%2520often%2520use%2520keyword%2520filtering%252C%2520topic%2520modelling%250Aor%2520classification-based%2520techniques%2520to%2520identify%2520such%2520posts.%2520Active%2520Learning%2520%2528AL%2529%250Apresents%2520a%2520promising%2520sub-field%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520that%2520has%2520not%2520been%2520used%250Amuch%2520in%2520the%2520field%2520of%2520text%2520classification%2520of%2520social%2520media%2520content.%2520This%2520study%250Atherefore%2520investigates%2520the%2520potential%2520of%2520AL%2520for%2520identifying%2520disaster-related%250ATweets.%2520We%2520compare%2520a%2520keyword%2520filtering%2520approach%252C%2520a%2520RoBERTa%2520model%2520fine-tuned%250Awith%2520generic%2520data%2520from%2520CrisisLex%252C%2520a%2520base%2520RoBERTa%2520model%2520trained%2520with%2520AL%2520and%2520a%250Afine-tuned%2520RoBERTa%2520model%2520trained%2520with%2520AL%2520regarding%2520classification%2520performance.%250AFor%2520testing%252C%2520data%2520from%2520CrisisLex%2520and%2520manually%2520labelled%2520data%2520from%2520the%25202021%2520flood%250Ain%2520Germany%2520and%2520the%25202023%2520Chile%2520forest%2520fires%2520were%2520considered.%2520The%2520results%2520show%250Athat%2520generic%2520fine-tuning%2520combined%2520with%252010%2520rounds%2520of%2520AL%2520outperformed%2520all%2520other%250Aapproaches.%2520Consequently%252C%2520a%2520broadly%2520applicable%2520model%2520for%2520the%2520identification%2520of%250Adisaster-related%2520Tweets%2520could%2520be%2520trained%2520with%2520very%2520little%2520labelling%2520effort.%2520The%250Amodel%2520can%2520be%2520applied%2520to%2520use%2520cases%2520beyond%2520this%2520study%2520and%2520provides%2520a%2520useful%2520tool%250Afor%2520further%2520research%2520in%2520social%2520media%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20Identifying%20Disaster-Related%20Tweets%3A%20A%20Comparison%0A%20%20with%20Keyword%20Filtering%20and%20Generic%20Fine-Tuning&entry.906535625=David%20Hanny%20and%20Sebastian%20Schmidt%20and%20Bernd%20Resch&entry.1292438233=%20%20Information%20from%20social%20media%20can%20provide%20essential%20information%20for%20emergency%0Aresponse%20during%20natural%20disasters%20in%20near%20real-time.%20However%2C%20it%20is%20difficult%0Ato%20identify%20the%20disaster-related%20posts%20among%20the%20large%20amounts%20of%20unstructured%0Adata%20available.%20Previous%20methods%20often%20use%20keyword%20filtering%2C%20topic%20modelling%0Aor%20classification-based%20techniques%20to%20identify%20such%20posts.%20Active%20Learning%20%28AL%29%0Apresents%20a%20promising%20sub-field%20of%20Machine%20Learning%20%28ML%29%20that%20has%20not%20been%20used%0Amuch%20in%20the%20field%20of%20text%20classification%20of%20social%20media%20content.%20This%20study%0Atherefore%20investigates%20the%20potential%20of%20AL%20for%20identifying%20disaster-related%0ATweets.%20We%20compare%20a%20keyword%20filtering%20approach%2C%20a%20RoBERTa%20model%20fine-tuned%0Awith%20generic%20data%20from%20CrisisLex%2C%20a%20base%20RoBERTa%20model%20trained%20with%20AL%20and%20a%0Afine-tuned%20RoBERTa%20model%20trained%20with%20AL%20regarding%20classification%20performance.%0AFor%20testing%2C%20data%20from%20CrisisLex%20and%20manually%20labelled%20data%20from%20the%202021%20flood%0Ain%20Germany%20and%20the%202023%20Chile%20forest%20fires%20were%20considered.%20The%20results%20show%0Athat%20generic%20fine-tuning%20combined%20with%2010%20rounds%20of%20AL%20outperformed%20all%20other%0Aapproaches.%20Consequently%2C%20a%20broadly%20applicable%20model%20for%20the%20identification%20of%0Adisaster-related%20Tweets%20could%20be%20trained%20with%20very%20little%20labelling%20effort.%20The%0Amodel%20can%20be%20applied%20to%20use%20cases%20beyond%20this%20study%20and%20provides%20a%20useful%20tool%0Afor%20further%20research%20in%20social%20media%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09914v1&entry.124074799=Read"},
{"title": "A new perspective on Bayesian Operational Modal Analysis", "author": "Brandon J. O'Connell and Max D. Champneys and Timothy J. Rogers", "abstract": "  In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies.\n", "link": "http://arxiv.org/abs/2408.08664v2", "date": "2024-08-19", "relevancy": 1.9171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4693}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis&body=Title%3A%20A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis%0AAuthor%3A%20Brandon%20J.%20O%27Connell%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers%0AAbstract%3A%20%20%20In%20the%20field%20of%20operational%20modal%20analysis%20%28OMA%29%2C%20obtained%20modal%20information%0Ais%20frequently%20used%20to%20assess%20the%20current%20state%20of%20aerospace%2C%20mechanical%2C%0Aoffshore%20and%20civil%20structures.%20However%2C%20the%20stochasticity%20of%20operational%0Asystems%20and%20the%20lack%20of%20forcing%20information%20can%20lead%20to%20inconsistent%20results.%0AQuantifying%20the%20uncertainty%20of%20the%20recovered%20modal%20parameters%20through%20OMA%20is%0Atherefore%20of%20significant%20value.%20In%20this%20article%2C%20a%20new%20perspective%20on%20Bayesian%0AOMA%20is%20proposed%3A%20a%20Bayesian%20stochastic%20subspace%20identification%20%28SSI%29%20algorithm.%0ADistinct%20from%20existing%20approaches%20to%20Bayesian%20OMA%2C%20a%20hierarchical%20probabilistic%0Amodel%20is%20embedded%20at%20the%20core%20of%20covariance-driven%20SSI.%20Through%20substitution%20of%0Acanonical%20correlation%20analysis%20with%20a%20Bayesian%20equivalent%2C%20posterior%0Adistributions%20over%20the%20modal%20properties%20are%20obtained.%20Two%20inference%20schemes%20are%0Apresented%20for%20the%20proposed%20Bayesian%20formulation%3A%20Markov%20Chain%20Monte%20Carlo%20and%0Avariational%20Bayes.%20Two%20case%20studies%20are%20then%20explored.%20The%20first%20is%20benchmark%0Astudy%20using%20data%20from%20a%20simulated%2C%20multi%20degree-of-freedom%2C%20linear%20system.%0AFollowing%20application%20of%20Bayesian%20SSI%2C%20it%20is%20shown%20that%20the%20same%20posterior%20is%0Atargeted%20and%20recovered%20by%20both%20inference%20schemes%2C%20with%20good%20agreement%20between%0Athe%20posterior%20mean%20and%20the%20conventional%20SSI%20result.%20The%20second%20study%20applies%0Athe%20variational%20form%20to%20data%20obtained%20from%20an%20in-service%20structure%3A%20The%20Z24%0Abridge.%20The%20results%20of%20this%20study%20are%20presented%20at%20single%20model%20orders%2C%20and%0Athen%20using%20a%20stabilisation%20diagram.%20The%20recovered%20posterior%20uncertainty%20is%0Apresented%20and%20compared%20to%20the%20classic%20SSI%20result.%20It%20is%20observed%20that%20the%0Aposterior%20distributions%20with%20mean%20values%20coinciding%20with%20the%20natural%0Afrequencies%20exhibit%20lower%20variance%20than%20values%20situated%20away%20from%20the%20natural%0Afrequencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520perspective%2520on%2520Bayesian%2520Operational%2520Modal%2520Analysis%26entry.906535625%3DBrandon%2520J.%2520O%2527Connell%2520and%2520Max%2520D.%2520Champneys%2520and%2520Timothy%2520J.%2520Rogers%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520operational%2520modal%2520analysis%2520%2528OMA%2529%252C%2520obtained%2520modal%2520information%250Ais%2520frequently%2520used%2520to%2520assess%2520the%2520current%2520state%2520of%2520aerospace%252C%2520mechanical%252C%250Aoffshore%2520and%2520civil%2520structures.%2520However%252C%2520the%2520stochasticity%2520of%2520operational%250Asystems%2520and%2520the%2520lack%2520of%2520forcing%2520information%2520can%2520lead%2520to%2520inconsistent%2520results.%250AQuantifying%2520the%2520uncertainty%2520of%2520the%2520recovered%2520modal%2520parameters%2520through%2520OMA%2520is%250Atherefore%2520of%2520significant%2520value.%2520In%2520this%2520article%252C%2520a%2520new%2520perspective%2520on%2520Bayesian%250AOMA%2520is%2520proposed%253A%2520a%2520Bayesian%2520stochastic%2520subspace%2520identification%2520%2528SSI%2529%2520algorithm.%250ADistinct%2520from%2520existing%2520approaches%2520to%2520Bayesian%2520OMA%252C%2520a%2520hierarchical%2520probabilistic%250Amodel%2520is%2520embedded%2520at%2520the%2520core%2520of%2520covariance-driven%2520SSI.%2520Through%2520substitution%2520of%250Acanonical%2520correlation%2520analysis%2520with%2520a%2520Bayesian%2520equivalent%252C%2520posterior%250Adistributions%2520over%2520the%2520modal%2520properties%2520are%2520obtained.%2520Two%2520inference%2520schemes%2520are%250Apresented%2520for%2520the%2520proposed%2520Bayesian%2520formulation%253A%2520Markov%2520Chain%2520Monte%2520Carlo%2520and%250Avariational%2520Bayes.%2520Two%2520case%2520studies%2520are%2520then%2520explored.%2520The%2520first%2520is%2520benchmark%250Astudy%2520using%2520data%2520from%2520a%2520simulated%252C%2520multi%2520degree-of-freedom%252C%2520linear%2520system.%250AFollowing%2520application%2520of%2520Bayesian%2520SSI%252C%2520it%2520is%2520shown%2520that%2520the%2520same%2520posterior%2520is%250Atargeted%2520and%2520recovered%2520by%2520both%2520inference%2520schemes%252C%2520with%2520good%2520agreement%2520between%250Athe%2520posterior%2520mean%2520and%2520the%2520conventional%2520SSI%2520result.%2520The%2520second%2520study%2520applies%250Athe%2520variational%2520form%2520to%2520data%2520obtained%2520from%2520an%2520in-service%2520structure%253A%2520The%2520Z24%250Abridge.%2520The%2520results%2520of%2520this%2520study%2520are%2520presented%2520at%2520single%2520model%2520orders%252C%2520and%250Athen%2520using%2520a%2520stabilisation%2520diagram.%2520The%2520recovered%2520posterior%2520uncertainty%2520is%250Apresented%2520and%2520compared%2520to%2520the%2520classic%2520SSI%2520result.%2520It%2520is%2520observed%2520that%2520the%250Aposterior%2520distributions%2520with%2520mean%2520values%2520coinciding%2520with%2520the%2520natural%250Afrequencies%2520exhibit%2520lower%2520variance%2520than%2520values%2520situated%2520away%2520from%2520the%2520natural%250Afrequencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis&entry.906535625=Brandon%20J.%20O%27Connell%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers&entry.1292438233=%20%20In%20the%20field%20of%20operational%20modal%20analysis%20%28OMA%29%2C%20obtained%20modal%20information%0Ais%20frequently%20used%20to%20assess%20the%20current%20state%20of%20aerospace%2C%20mechanical%2C%0Aoffshore%20and%20civil%20structures.%20However%2C%20the%20stochasticity%20of%20operational%0Asystems%20and%20the%20lack%20of%20forcing%20information%20can%20lead%20to%20inconsistent%20results.%0AQuantifying%20the%20uncertainty%20of%20the%20recovered%20modal%20parameters%20through%20OMA%20is%0Atherefore%20of%20significant%20value.%20In%20this%20article%2C%20a%20new%20perspective%20on%20Bayesian%0AOMA%20is%20proposed%3A%20a%20Bayesian%20stochastic%20subspace%20identification%20%28SSI%29%20algorithm.%0ADistinct%20from%20existing%20approaches%20to%20Bayesian%20OMA%2C%20a%20hierarchical%20probabilistic%0Amodel%20is%20embedded%20at%20the%20core%20of%20covariance-driven%20SSI.%20Through%20substitution%20of%0Acanonical%20correlation%20analysis%20with%20a%20Bayesian%20equivalent%2C%20posterior%0Adistributions%20over%20the%20modal%20properties%20are%20obtained.%20Two%20inference%20schemes%20are%0Apresented%20for%20the%20proposed%20Bayesian%20formulation%3A%20Markov%20Chain%20Monte%20Carlo%20and%0Avariational%20Bayes.%20Two%20case%20studies%20are%20then%20explored.%20The%20first%20is%20benchmark%0Astudy%20using%20data%20from%20a%20simulated%2C%20multi%20degree-of-freedom%2C%20linear%20system.%0AFollowing%20application%20of%20Bayesian%20SSI%2C%20it%20is%20shown%20that%20the%20same%20posterior%20is%0Atargeted%20and%20recovered%20by%20both%20inference%20schemes%2C%20with%20good%20agreement%20between%0Athe%20posterior%20mean%20and%20the%20conventional%20SSI%20result.%20The%20second%20study%20applies%0Athe%20variational%20form%20to%20data%20obtained%20from%20an%20in-service%20structure%3A%20The%20Z24%0Abridge.%20The%20results%20of%20this%20study%20are%20presented%20at%20single%20model%20orders%2C%20and%0Athen%20using%20a%20stabilisation%20diagram.%20The%20recovered%20posterior%20uncertainty%20is%0Apresented%20and%20compared%20to%20the%20classic%20SSI%20result.%20It%20is%20observed%20that%20the%0Aposterior%20distributions%20with%20mean%20values%20coinciding%20with%20the%20natural%0Afrequencies%20exhibit%20lower%20variance%20than%20values%20situated%20away%20from%20the%20natural%0Afrequencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08664v2&entry.124074799=Read"},
{"title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge", "author": "Ravi Raju and Swayambhoo Jain and Bo Li and Jonathan Li and Urmish Thakkar", "abstract": "  Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.\n", "link": "http://arxiv.org/abs/2408.08808v2", "date": "2024-08-19", "relevancy": 1.9081, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge&body=Title%3A%20Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge%0AAuthor%3A%20Ravi%20Raju%20and%20Swayambhoo%20Jain%20and%20Bo%20Li%20and%20Jonathan%20Li%20and%20Urmish%20Thakkar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20landscape%20of%20machine%0Alearning%2C%20yet%20current%20benchmarks%20often%20fall%20short%20in%20capturing%20the%20diverse%0Abehavior%20of%20these%20models%20in%20real-world%20applications.%20A%20benchmark%27s%20usefulness%0Ais%20determined%20by%20its%20ability%20to%20clearly%20differentiate%20between%20models%20of%20varying%0Acapabilities%20%28separability%29%20and%20closely%20align%20with%20human%20preferences.%20Existing%0Aframeworks%20like%20Alpaca-Eval%202.0%20LC%0A%5Ccite%7Bdubois2024lengthcontrolledalpacaevalsimpleway%7D%20and%20Arena-Hard%20v0.1%0A%5Ccite%7Bli2024crowdsourced%7D%20are%20limited%20by%20their%20focus%20on%20general-purpose%20queries%0Aand%20lack%20of%20diversity%20across%20domains%20such%20as%20law%2C%20medicine%2C%20and%20multilingual%0Acontexts.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0Adata%20pipeline%20that%20curates%20diverse%2C%20domain-specific%20evaluation%20sets%20tailored%0Afor%20LLM-as-a-Judge%20frameworks.%20Our%20approach%20leverages%20a%20combination%20of%20manual%0Acuration%2C%20semi-supervised%20learning%20to%20generate%20clusters%2C%20and%20stratified%0Asampling%20to%20ensure%20balanced%20representation%20across%20a%20wide%20range%20of%20domains%20and%0Alanguages.%20The%20resulting%20evaluation%20set%2C%20which%20includes%201573%20samples%20across%2014%0Acategories%2C%20demonstrates%20high%20separability%20%2884%5C%25%29%20across%20ten%20top-ranked%20models%2C%0Aand%20agreement%20%2884%5C%25%29%20with%20Chatbot%20Arena%20and%20%280.915%29%20Spearman%20correlation.%20The%0Aagreement%20values%20are%209%5C%25%20better%20than%20Arena%20Hard%20and%2020%5C%25%20better%20than%20AlpacaEval%0A2.0%20LC%2C%20while%20the%20Spearman%20coefficient%20is%200.7%20more%20than%20the%20next%20best%0Abenchmark%2C%20showcasing%20a%20significant%20improvement%20in%20the%20usefulness%20of%20the%0Abenchmark.%20We%20further%20provide%20an%20open-source%20evaluation%20tool%20that%20enables%0Afine-grained%20analysis%20of%20model%20performance%20across%20user-defined%20categories%2C%0Aoffering%20valuable%20insights%20for%20practitioners.%20This%20work%20contributes%20to%20the%0Aongoing%20effort%20to%20enhance%20the%20transparency%2C%20diversity%2C%20and%20effectiveness%20of%20LLM%0Aevaluation%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520Domain-Specific%2520Evaluation%2520Sets%2520for%2520LLM-as-a-judge%26entry.906535625%3DRavi%2520Raju%2520and%2520Swayambhoo%2520Jain%2520and%2520Bo%2520Li%2520and%2520Jonathan%2520Li%2520and%2520Urmish%2520Thakkar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520the%2520landscape%2520of%2520machine%250Alearning%252C%2520yet%2520current%2520benchmarks%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520diverse%250Abehavior%2520of%2520these%2520models%2520in%2520real-world%2520applications.%2520A%2520benchmark%2527s%2520usefulness%250Ais%2520determined%2520by%2520its%2520ability%2520to%2520clearly%2520differentiate%2520between%2520models%2520of%2520varying%250Acapabilities%2520%2528separability%2529%2520and%2520closely%2520align%2520with%2520human%2520preferences.%2520Existing%250Aframeworks%2520like%2520Alpaca-Eval%25202.0%2520LC%250A%255Ccite%257Bdubois2024lengthcontrolledalpacaevalsimpleway%257D%2520and%2520Arena-Hard%2520v0.1%250A%255Ccite%257Bli2024crowdsourced%257D%2520are%2520limited%2520by%2520their%2520focus%2520on%2520general-purpose%2520queries%250Aand%2520lack%2520of%2520diversity%2520across%2520domains%2520such%2520as%2520law%252C%2520medicine%252C%2520and%2520multilingual%250Acontexts.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520limitations%2520by%2520introducing%2520a%2520novel%250Adata%2520pipeline%2520that%2520curates%2520diverse%252C%2520domain-specific%2520evaluation%2520sets%2520tailored%250Afor%2520LLM-as-a-Judge%2520frameworks.%2520Our%2520approach%2520leverages%2520a%2520combination%2520of%2520manual%250Acuration%252C%2520semi-supervised%2520learning%2520to%2520generate%2520clusters%252C%2520and%2520stratified%250Asampling%2520to%2520ensure%2520balanced%2520representation%2520across%2520a%2520wide%2520range%2520of%2520domains%2520and%250Alanguages.%2520The%2520resulting%2520evaluation%2520set%252C%2520which%2520includes%25201573%2520samples%2520across%252014%250Acategories%252C%2520demonstrates%2520high%2520separability%2520%252884%255C%2525%2529%2520across%2520ten%2520top-ranked%2520models%252C%250Aand%2520agreement%2520%252884%255C%2525%2529%2520with%2520Chatbot%2520Arena%2520and%2520%25280.915%2529%2520Spearman%2520correlation.%2520The%250Aagreement%2520values%2520are%25209%255C%2525%2520better%2520than%2520Arena%2520Hard%2520and%252020%255C%2525%2520better%2520than%2520AlpacaEval%250A2.0%2520LC%252C%2520while%2520the%2520Spearman%2520coefficient%2520is%25200.7%2520more%2520than%2520the%2520next%2520best%250Abenchmark%252C%2520showcasing%2520a%2520significant%2520improvement%2520in%2520the%2520usefulness%2520of%2520the%250Abenchmark.%2520We%2520further%2520provide%2520an%2520open-source%2520evaluation%2520tool%2520that%2520enables%250Afine-grained%2520analysis%2520of%2520model%2520performance%2520across%2520user-defined%2520categories%252C%250Aoffering%2520valuable%2520insights%2520for%2520practitioners.%2520This%2520work%2520contributes%2520to%2520the%250Aongoing%2520effort%2520to%2520enhance%2520the%2520transparency%252C%2520diversity%252C%2520and%2520effectiveness%2520of%2520LLM%250Aevaluation%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge&entry.906535625=Ravi%20Raju%20and%20Swayambhoo%20Jain%20and%20Bo%20Li%20and%20Jonathan%20Li%20and%20Urmish%20Thakkar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20landscape%20of%20machine%0Alearning%2C%20yet%20current%20benchmarks%20often%20fall%20short%20in%20capturing%20the%20diverse%0Abehavior%20of%20these%20models%20in%20real-world%20applications.%20A%20benchmark%27s%20usefulness%0Ais%20determined%20by%20its%20ability%20to%20clearly%20differentiate%20between%20models%20of%20varying%0Acapabilities%20%28separability%29%20and%20closely%20align%20with%20human%20preferences.%20Existing%0Aframeworks%20like%20Alpaca-Eval%202.0%20LC%0A%5Ccite%7Bdubois2024lengthcontrolledalpacaevalsimpleway%7D%20and%20Arena-Hard%20v0.1%0A%5Ccite%7Bli2024crowdsourced%7D%20are%20limited%20by%20their%20focus%20on%20general-purpose%20queries%0Aand%20lack%20of%20diversity%20across%20domains%20such%20as%20law%2C%20medicine%2C%20and%20multilingual%0Acontexts.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0Adata%20pipeline%20that%20curates%20diverse%2C%20domain-specific%20evaluation%20sets%20tailored%0Afor%20LLM-as-a-Judge%20frameworks.%20Our%20approach%20leverages%20a%20combination%20of%20manual%0Acuration%2C%20semi-supervised%20learning%20to%20generate%20clusters%2C%20and%20stratified%0Asampling%20to%20ensure%20balanced%20representation%20across%20a%20wide%20range%20of%20domains%20and%0Alanguages.%20The%20resulting%20evaluation%20set%2C%20which%20includes%201573%20samples%20across%2014%0Acategories%2C%20demonstrates%20high%20separability%20%2884%5C%25%29%20across%20ten%20top-ranked%20models%2C%0Aand%20agreement%20%2884%5C%25%29%20with%20Chatbot%20Arena%20and%20%280.915%29%20Spearman%20correlation.%20The%0Aagreement%20values%20are%209%5C%25%20better%20than%20Arena%20Hard%20and%2020%5C%25%20better%20than%20AlpacaEval%0A2.0%20LC%2C%20while%20the%20Spearman%20coefficient%20is%200.7%20more%20than%20the%20next%20best%0Abenchmark%2C%20showcasing%20a%20significant%20improvement%20in%20the%20usefulness%20of%20the%0Abenchmark.%20We%20further%20provide%20an%20open-source%20evaluation%20tool%20that%20enables%0Afine-grained%20analysis%20of%20model%20performance%20across%20user-defined%20categories%2C%0Aoffering%20valuable%20insights%20for%20practitioners.%20This%20work%20contributes%20to%20the%0Aongoing%20effort%20to%20enhance%20the%20transparency%2C%20diversity%2C%20and%20effectiveness%20of%20LLM%0Aevaluation%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08808v2&entry.124074799=Read"},
{"title": "Understanding cyclists' perception of driverless vehicles through\n  eye-tracking and interviews", "author": "Siri Hegna Berge and Joost de Winter and Dimitra Dodou and Amir Pooyan Afghari and Eleonora Papadimitriou and Nagarjun Reddy and Yongqi Dong and Narayana Raju and Haneen Farah", "abstract": "  As automated vehicles (AVs) become increasingly popular, the question arises\nas to how cyclists will interact with such vehicles. This study investigated\n(1) whether cyclists spontaneously notice if a vehicle is driverless, (2) how\nwell they perform a driver-detection task when explicitly instructed, and (3)\nhow they carry out such tasks. Using a Wizard-of-Oz method, 37 participants\ncycled a designated route and encountered an AV multiple times in two\nexperimental sessions. In Session 1, participants cycled the route\nuninstructed, while in Session 2, they were instructed to verbally report\nwhether they detected the presence or absence of a driver. Additionally, we\nrecorded the participants' gaze behaviour with eye-tracking and their responses\nin post-session interviews. The interviews revealed that 30% of the cyclists\nspontaneously mentioned the absence of a driver (Session 1), and when\ninstructed (Session 2), they detected the absence and presence of the driver\nwith 93% accuracy. The eye-tracking data showed that cyclists looked more\nfrequently and longer at the vehicle in Session 2 compared to Session 1.\nFurthermore, participants exhibited intermittent sampling of the vehicle, and\nthey looked in front of the vehicle when it was far away and towards the\nwindshield region when it was closer. The post-session interviews also\nindicated that participants were curious, felt safe, and reported a need to\nreceive information about the AV's driving state. In conclusion, cyclists can\ndetect the absence of a driver in the AV, and this detection may influence\ntheir perceptions of safety. Further research is needed to explore these\nfindings in real-world traffic conditions.\n", "link": "http://arxiv.org/abs/2408.10064v1", "date": "2024-08-19", "relevancy": 1.9021, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4485}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20cyclists%27%20perception%20of%20driverless%20vehicles%20through%0A%20%20eye-tracking%20and%20interviews&body=Title%3A%20Understanding%20cyclists%27%20perception%20of%20driverless%20vehicles%20through%0A%20%20eye-tracking%20and%20interviews%0AAuthor%3A%20Siri%20Hegna%20Berge%20and%20Joost%20de%20Winter%20and%20Dimitra%20Dodou%20and%20Amir%20Pooyan%20Afghari%20and%20Eleonora%20Papadimitriou%20and%20Nagarjun%20Reddy%20and%20Yongqi%20Dong%20and%20Narayana%20Raju%20and%20Haneen%20Farah%0AAbstract%3A%20%20%20As%20automated%20vehicles%20%28AVs%29%20become%20increasingly%20popular%2C%20the%20question%20arises%0Aas%20to%20how%20cyclists%20will%20interact%20with%20such%20vehicles.%20This%20study%20investigated%0A%281%29%20whether%20cyclists%20spontaneously%20notice%20if%20a%20vehicle%20is%20driverless%2C%20%282%29%20how%0Awell%20they%20perform%20a%20driver-detection%20task%20when%20explicitly%20instructed%2C%20and%20%283%29%0Ahow%20they%20carry%20out%20such%20tasks.%20Using%20a%20Wizard-of-Oz%20method%2C%2037%20participants%0Acycled%20a%20designated%20route%20and%20encountered%20an%20AV%20multiple%20times%20in%20two%0Aexperimental%20sessions.%20In%20Session%201%2C%20participants%20cycled%20the%20route%0Auninstructed%2C%20while%20in%20Session%202%2C%20they%20were%20instructed%20to%20verbally%20report%0Awhether%20they%20detected%20the%20presence%20or%20absence%20of%20a%20driver.%20Additionally%2C%20we%0Arecorded%20the%20participants%27%20gaze%20behaviour%20with%20eye-tracking%20and%20their%20responses%0Ain%20post-session%20interviews.%20The%20interviews%20revealed%20that%2030%25%20of%20the%20cyclists%0Aspontaneously%20mentioned%20the%20absence%20of%20a%20driver%20%28Session%201%29%2C%20and%20when%0Ainstructed%20%28Session%202%29%2C%20they%20detected%20the%20absence%20and%20presence%20of%20the%20driver%0Awith%2093%25%20accuracy.%20The%20eye-tracking%20data%20showed%20that%20cyclists%20looked%20more%0Afrequently%20and%20longer%20at%20the%20vehicle%20in%20Session%202%20compared%20to%20Session%201.%0AFurthermore%2C%20participants%20exhibited%20intermittent%20sampling%20of%20the%20vehicle%2C%20and%0Athey%20looked%20in%20front%20of%20the%20vehicle%20when%20it%20was%20far%20away%20and%20towards%20the%0Awindshield%20region%20when%20it%20was%20closer.%20The%20post-session%20interviews%20also%0Aindicated%20that%20participants%20were%20curious%2C%20felt%20safe%2C%20and%20reported%20a%20need%20to%0Areceive%20information%20about%20the%20AV%27s%20driving%20state.%20In%20conclusion%2C%20cyclists%20can%0Adetect%20the%20absence%20of%20a%20driver%20in%20the%20AV%2C%20and%20this%20detection%20may%20influence%0Atheir%20perceptions%20of%20safety.%20Further%20research%20is%20needed%20to%20explore%20these%0Afindings%20in%20real-world%20traffic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520cyclists%2527%2520perception%2520of%2520driverless%2520vehicles%2520through%250A%2520%2520eye-tracking%2520and%2520interviews%26entry.906535625%3DSiri%2520Hegna%2520Berge%2520and%2520Joost%2520de%2520Winter%2520and%2520Dimitra%2520Dodou%2520and%2520Amir%2520Pooyan%2520Afghari%2520and%2520Eleonora%2520Papadimitriou%2520and%2520Nagarjun%2520Reddy%2520and%2520Yongqi%2520Dong%2520and%2520Narayana%2520Raju%2520and%2520Haneen%2520Farah%26entry.1292438233%3D%2520%2520As%2520automated%2520vehicles%2520%2528AVs%2529%2520become%2520increasingly%2520popular%252C%2520the%2520question%2520arises%250Aas%2520to%2520how%2520cyclists%2520will%2520interact%2520with%2520such%2520vehicles.%2520This%2520study%2520investigated%250A%25281%2529%2520whether%2520cyclists%2520spontaneously%2520notice%2520if%2520a%2520vehicle%2520is%2520driverless%252C%2520%25282%2529%2520how%250Awell%2520they%2520perform%2520a%2520driver-detection%2520task%2520when%2520explicitly%2520instructed%252C%2520and%2520%25283%2529%250Ahow%2520they%2520carry%2520out%2520such%2520tasks.%2520Using%2520a%2520Wizard-of-Oz%2520method%252C%252037%2520participants%250Acycled%2520a%2520designated%2520route%2520and%2520encountered%2520an%2520AV%2520multiple%2520times%2520in%2520two%250Aexperimental%2520sessions.%2520In%2520Session%25201%252C%2520participants%2520cycled%2520the%2520route%250Auninstructed%252C%2520while%2520in%2520Session%25202%252C%2520they%2520were%2520instructed%2520to%2520verbally%2520report%250Awhether%2520they%2520detected%2520the%2520presence%2520or%2520absence%2520of%2520a%2520driver.%2520Additionally%252C%2520we%250Arecorded%2520the%2520participants%2527%2520gaze%2520behaviour%2520with%2520eye-tracking%2520and%2520their%2520responses%250Ain%2520post-session%2520interviews.%2520The%2520interviews%2520revealed%2520that%252030%2525%2520of%2520the%2520cyclists%250Aspontaneously%2520mentioned%2520the%2520absence%2520of%2520a%2520driver%2520%2528Session%25201%2529%252C%2520and%2520when%250Ainstructed%2520%2528Session%25202%2529%252C%2520they%2520detected%2520the%2520absence%2520and%2520presence%2520of%2520the%2520driver%250Awith%252093%2525%2520accuracy.%2520The%2520eye-tracking%2520data%2520showed%2520that%2520cyclists%2520looked%2520more%250Afrequently%2520and%2520longer%2520at%2520the%2520vehicle%2520in%2520Session%25202%2520compared%2520to%2520Session%25201.%250AFurthermore%252C%2520participants%2520exhibited%2520intermittent%2520sampling%2520of%2520the%2520vehicle%252C%2520and%250Athey%2520looked%2520in%2520front%2520of%2520the%2520vehicle%2520when%2520it%2520was%2520far%2520away%2520and%2520towards%2520the%250Awindshield%2520region%2520when%2520it%2520was%2520closer.%2520The%2520post-session%2520interviews%2520also%250Aindicated%2520that%2520participants%2520were%2520curious%252C%2520felt%2520safe%252C%2520and%2520reported%2520a%2520need%2520to%250Areceive%2520information%2520about%2520the%2520AV%2527s%2520driving%2520state.%2520In%2520conclusion%252C%2520cyclists%2520can%250Adetect%2520the%2520absence%2520of%2520a%2520driver%2520in%2520the%2520AV%252C%2520and%2520this%2520detection%2520may%2520influence%250Atheir%2520perceptions%2520of%2520safety.%2520Further%2520research%2520is%2520needed%2520to%2520explore%2520these%250Afindings%2520in%2520real-world%2520traffic%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20cyclists%27%20perception%20of%20driverless%20vehicles%20through%0A%20%20eye-tracking%20and%20interviews&entry.906535625=Siri%20Hegna%20Berge%20and%20Joost%20de%20Winter%20and%20Dimitra%20Dodou%20and%20Amir%20Pooyan%20Afghari%20and%20Eleonora%20Papadimitriou%20and%20Nagarjun%20Reddy%20and%20Yongqi%20Dong%20and%20Narayana%20Raju%20and%20Haneen%20Farah&entry.1292438233=%20%20As%20automated%20vehicles%20%28AVs%29%20become%20increasingly%20popular%2C%20the%20question%20arises%0Aas%20to%20how%20cyclists%20will%20interact%20with%20such%20vehicles.%20This%20study%20investigated%0A%281%29%20whether%20cyclists%20spontaneously%20notice%20if%20a%20vehicle%20is%20driverless%2C%20%282%29%20how%0Awell%20they%20perform%20a%20driver-detection%20task%20when%20explicitly%20instructed%2C%20and%20%283%29%0Ahow%20they%20carry%20out%20such%20tasks.%20Using%20a%20Wizard-of-Oz%20method%2C%2037%20participants%0Acycled%20a%20designated%20route%20and%20encountered%20an%20AV%20multiple%20times%20in%20two%0Aexperimental%20sessions.%20In%20Session%201%2C%20participants%20cycled%20the%20route%0Auninstructed%2C%20while%20in%20Session%202%2C%20they%20were%20instructed%20to%20verbally%20report%0Awhether%20they%20detected%20the%20presence%20or%20absence%20of%20a%20driver.%20Additionally%2C%20we%0Arecorded%20the%20participants%27%20gaze%20behaviour%20with%20eye-tracking%20and%20their%20responses%0Ain%20post-session%20interviews.%20The%20interviews%20revealed%20that%2030%25%20of%20the%20cyclists%0Aspontaneously%20mentioned%20the%20absence%20of%20a%20driver%20%28Session%201%29%2C%20and%20when%0Ainstructed%20%28Session%202%29%2C%20they%20detected%20the%20absence%20and%20presence%20of%20the%20driver%0Awith%2093%25%20accuracy.%20The%20eye-tracking%20data%20showed%20that%20cyclists%20looked%20more%0Afrequently%20and%20longer%20at%20the%20vehicle%20in%20Session%202%20compared%20to%20Session%201.%0AFurthermore%2C%20participants%20exhibited%20intermittent%20sampling%20of%20the%20vehicle%2C%20and%0Athey%20looked%20in%20front%20of%20the%20vehicle%20when%20it%20was%20far%20away%20and%20towards%20the%0Awindshield%20region%20when%20it%20was%20closer.%20The%20post-session%20interviews%20also%0Aindicated%20that%20participants%20were%20curious%2C%20felt%20safe%2C%20and%20reported%20a%20need%20to%0Areceive%20information%20about%20the%20AV%27s%20driving%20state.%20In%20conclusion%2C%20cyclists%20can%0Adetect%20the%20absence%20of%20a%20driver%20in%20the%20AV%2C%20and%20this%20detection%20may%20influence%0Atheir%20perceptions%20of%20safety.%20Further%20research%20is%20needed%20to%20explore%20these%0Afindings%20in%20real-world%20traffic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10064v1&entry.124074799=Read"},
{"title": "Towards UAV-USV Collaboration in Harsh Maritime Conditions Including\n  Large Waves", "author": "Filip Nov\u00e1k and Tom\u00e1\u0161 B\u00e1\u010da and Ond\u0159ej Proch\u00e1zka and Martin Saska", "abstract": "  This paper introduces a system designed for tight collaboration between\nUnmanned Aerial Vehicles (UAVs) and Unmanned Surface Vehicles (USVs) in harsh\nmaritime conditions characterized by large waves. This onboard UAV system aims\nto enhance collaboration with USVs for following and landing tasks under such\nchallenging conditions. The main contribution of our system is the novel\nmathematical USV model, describing the movement of the USV in 6 degrees of\nfreedom on a wavy water surface, which is used to estimate and predict USV\nstates. The estimator fuses data from multiple global and onboard sensors,\nensuring accurate USV state estimation. The predictor computes future USV\nstates using the novel mathematical USV model and the last estimated states.\nThe estimated and predicted USV states are forwarded into a trajectory planner\nthat generates a UAV trajectory for following the USV or landing on its deck,\neven in harsh environmental conditions. The proposed approach was verified in\nnumerous simulations and deployed to the real world, where the UAV was able to\nfollow the USV and land on its deck repeatedly.\n", "link": "http://arxiv.org/abs/2408.10163v1", "date": "2024-08-19", "relevancy": 1.8949, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20UAV-USV%20Collaboration%20in%20Harsh%20Maritime%20Conditions%20Including%0A%20%20Large%20Waves&body=Title%3A%20Towards%20UAV-USV%20Collaboration%20in%20Harsh%20Maritime%20Conditions%20Including%0A%20%20Large%20Waves%0AAuthor%3A%20Filip%20Nov%C3%A1k%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Ond%C5%99ej%20Proch%C3%A1zka%20and%20Martin%20Saska%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20system%20designed%20for%20tight%20collaboration%20between%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20Unmanned%20Surface%20Vehicles%20%28USVs%29%20in%20harsh%0Amaritime%20conditions%20characterized%20by%20large%20waves.%20This%20onboard%20UAV%20system%20aims%0Ato%20enhance%20collaboration%20with%20USVs%20for%20following%20and%20landing%20tasks%20under%20such%0Achallenging%20conditions.%20The%20main%20contribution%20of%20our%20system%20is%20the%20novel%0Amathematical%20USV%20model%2C%20describing%20the%20movement%20of%20the%20USV%20in%206%20degrees%20of%0Afreedom%20on%20a%20wavy%20water%20surface%2C%20which%20is%20used%20to%20estimate%20and%20predict%20USV%0Astates.%20The%20estimator%20fuses%20data%20from%20multiple%20global%20and%20onboard%20sensors%2C%0Aensuring%20accurate%20USV%20state%20estimation.%20The%20predictor%20computes%20future%20USV%0Astates%20using%20the%20novel%20mathematical%20USV%20model%20and%20the%20last%20estimated%20states.%0AThe%20estimated%20and%20predicted%20USV%20states%20are%20forwarded%20into%20a%20trajectory%20planner%0Athat%20generates%20a%20UAV%20trajectory%20for%20following%20the%20USV%20or%20landing%20on%20its%20deck%2C%0Aeven%20in%20harsh%20environmental%20conditions.%20The%20proposed%20approach%20was%20verified%20in%0Anumerous%20simulations%20and%20deployed%20to%20the%20real%20world%2C%20where%20the%20UAV%20was%20able%20to%0Afollow%20the%20USV%20and%20land%20on%20its%20deck%20repeatedly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520UAV-USV%2520Collaboration%2520in%2520Harsh%2520Maritime%2520Conditions%2520Including%250A%2520%2520Large%2520Waves%26entry.906535625%3DFilip%2520Nov%25C3%25A1k%2520and%2520Tom%25C3%25A1%25C5%25A1%2520B%25C3%25A1%25C4%258Da%2520and%2520Ond%25C5%2599ej%2520Proch%25C3%25A1zka%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520system%2520designed%2520for%2520tight%2520collaboration%2520between%250AUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%2520Unmanned%2520Surface%2520Vehicles%2520%2528USVs%2529%2520in%2520harsh%250Amaritime%2520conditions%2520characterized%2520by%2520large%2520waves.%2520This%2520onboard%2520UAV%2520system%2520aims%250Ato%2520enhance%2520collaboration%2520with%2520USVs%2520for%2520following%2520and%2520landing%2520tasks%2520under%2520such%250Achallenging%2520conditions.%2520The%2520main%2520contribution%2520of%2520our%2520system%2520is%2520the%2520novel%250Amathematical%2520USV%2520model%252C%2520describing%2520the%2520movement%2520of%2520the%2520USV%2520in%25206%2520degrees%2520of%250Afreedom%2520on%2520a%2520wavy%2520water%2520surface%252C%2520which%2520is%2520used%2520to%2520estimate%2520and%2520predict%2520USV%250Astates.%2520The%2520estimator%2520fuses%2520data%2520from%2520multiple%2520global%2520and%2520onboard%2520sensors%252C%250Aensuring%2520accurate%2520USV%2520state%2520estimation.%2520The%2520predictor%2520computes%2520future%2520USV%250Astates%2520using%2520the%2520novel%2520mathematical%2520USV%2520model%2520and%2520the%2520last%2520estimated%2520states.%250AThe%2520estimated%2520and%2520predicted%2520USV%2520states%2520are%2520forwarded%2520into%2520a%2520trajectory%2520planner%250Athat%2520generates%2520a%2520UAV%2520trajectory%2520for%2520following%2520the%2520USV%2520or%2520landing%2520on%2520its%2520deck%252C%250Aeven%2520in%2520harsh%2520environmental%2520conditions.%2520The%2520proposed%2520approach%2520was%2520verified%2520in%250Anumerous%2520simulations%2520and%2520deployed%2520to%2520the%2520real%2520world%252C%2520where%2520the%2520UAV%2520was%2520able%2520to%250Afollow%2520the%2520USV%2520and%2520land%2520on%2520its%2520deck%2520repeatedly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20UAV-USV%20Collaboration%20in%20Harsh%20Maritime%20Conditions%20Including%0A%20%20Large%20Waves&entry.906535625=Filip%20Nov%C3%A1k%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Ond%C5%99ej%20Proch%C3%A1zka%20and%20Martin%20Saska&entry.1292438233=%20%20This%20paper%20introduces%20a%20system%20designed%20for%20tight%20collaboration%20between%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20Unmanned%20Surface%20Vehicles%20%28USVs%29%20in%20harsh%0Amaritime%20conditions%20characterized%20by%20large%20waves.%20This%20onboard%20UAV%20system%20aims%0Ato%20enhance%20collaboration%20with%20USVs%20for%20following%20and%20landing%20tasks%20under%20such%0Achallenging%20conditions.%20The%20main%20contribution%20of%20our%20system%20is%20the%20novel%0Amathematical%20USV%20model%2C%20describing%20the%20movement%20of%20the%20USV%20in%206%20degrees%20of%0Afreedom%20on%20a%20wavy%20water%20surface%2C%20which%20is%20used%20to%20estimate%20and%20predict%20USV%0Astates.%20The%20estimator%20fuses%20data%20from%20multiple%20global%20and%20onboard%20sensors%2C%0Aensuring%20accurate%20USV%20state%20estimation.%20The%20predictor%20computes%20future%20USV%0Astates%20using%20the%20novel%20mathematical%20USV%20model%20and%20the%20last%20estimated%20states.%0AThe%20estimated%20and%20predicted%20USV%20states%20are%20forwarded%20into%20a%20trajectory%20planner%0Athat%20generates%20a%20UAV%20trajectory%20for%20following%20the%20USV%20or%20landing%20on%20its%20deck%2C%0Aeven%20in%20harsh%20environmental%20conditions.%20The%20proposed%20approach%20was%20verified%20in%0Anumerous%20simulations%20and%20deployed%20to%20the%20real%20world%2C%20where%20the%20UAV%20was%20able%20to%0Afollow%20the%20USV%20and%20land%20on%20its%20deck%20repeatedly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10163v1&entry.124074799=Read"},
{"title": "MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation", "author": "Saif Anwar and Nathan Griffiths and Abhir Bhalerao and Thomas Popham", "abstract": "  Existing local Explainable AI (XAI) methods, such as LIME, select a region of\nthe input space in the vicinity of a given input instance, for which they\napproximate the behaviour of a model using a simpler and more interpretable\nsurrogate model. The size of this region is often controlled by a user-defined\nlocality hyperparameter. In this paper, we demonstrate the difficulties\nassociated with defining a suitable locality size to capture impactful model\nbehaviour, as well as the inadequacy of using a single locality size to explain\nall predictions. We propose a novel method, MASALA, for generating\nexplanations, which automatically determines the appropriate local region of\nimpactful model behaviour for each individual instance being explained. MASALA\napproximates the local behaviour used by a complex model to make a prediction\nby fitting a linear surrogate model to a set of points which experience similar\nmodel behaviour. These points are found by clustering the input space into\nregions of linear behavioural trends exhibited by the model. We compare the\nfidelity and consistency of explanations generated by our method with existing\nlocal XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDAS\ndatasets show that our method produces more faithful and consistent\nexplanations than existing methods, without the need to define any sensitive\nlocality hyperparameters.\n", "link": "http://arxiv.org/abs/2408.10085v1", "date": "2024-08-19", "relevancy": 1.8938, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASALA%3A%20Model-Agnostic%20Surrogate%20Explanations%20by%20Locality%20Adaptation&body=Title%3A%20MASALA%3A%20Model-Agnostic%20Surrogate%20Explanations%20by%20Locality%20Adaptation%0AAuthor%3A%20Saif%20Anwar%20and%20Nathan%20Griffiths%20and%20Abhir%20Bhalerao%20and%20Thomas%20Popham%0AAbstract%3A%20%20%20Existing%20local%20Explainable%20AI%20%28XAI%29%20methods%2C%20such%20as%20LIME%2C%20select%20a%20region%20of%0Athe%20input%20space%20in%20the%20vicinity%20of%20a%20given%20input%20instance%2C%20for%20which%20they%0Aapproximate%20the%20behaviour%20of%20a%20model%20using%20a%20simpler%20and%20more%20interpretable%0Asurrogate%20model.%20The%20size%20of%20this%20region%20is%20often%20controlled%20by%20a%20user-defined%0Alocality%20hyperparameter.%20In%20this%20paper%2C%20we%20demonstrate%20the%20difficulties%0Aassociated%20with%20defining%20a%20suitable%20locality%20size%20to%20capture%20impactful%20model%0Abehaviour%2C%20as%20well%20as%20the%20inadequacy%20of%20using%20a%20single%20locality%20size%20to%20explain%0Aall%20predictions.%20We%20propose%20a%20novel%20method%2C%20MASALA%2C%20for%20generating%0Aexplanations%2C%20which%20automatically%20determines%20the%20appropriate%20local%20region%20of%0Aimpactful%20model%20behaviour%20for%20each%20individual%20instance%20being%20explained.%20MASALA%0Aapproximates%20the%20local%20behaviour%20used%20by%20a%20complex%20model%20to%20make%20a%20prediction%0Aby%20fitting%20a%20linear%20surrogate%20model%20to%20a%20set%20of%20points%20which%20experience%20similar%0Amodel%20behaviour.%20These%20points%20are%20found%20by%20clustering%20the%20input%20space%20into%0Aregions%20of%20linear%20behavioural%20trends%20exhibited%20by%20the%20model.%20We%20compare%20the%0Afidelity%20and%20consistency%20of%20explanations%20generated%20by%20our%20method%20with%20existing%0Alocal%20XAI%20methods%2C%20namely%20LIME%20and%20CHILLI.%20Experiments%20on%20the%20PHM08%20and%20MIDAS%0Adatasets%20show%20that%20our%20method%20produces%20more%20faithful%20and%20consistent%0Aexplanations%20than%20existing%20methods%2C%20without%20the%20need%20to%20define%20any%20sensitive%0Alocality%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASALA%253A%2520Model-Agnostic%2520Surrogate%2520Explanations%2520by%2520Locality%2520Adaptation%26entry.906535625%3DSaif%2520Anwar%2520and%2520Nathan%2520Griffiths%2520and%2520Abhir%2520Bhalerao%2520and%2520Thomas%2520Popham%26entry.1292438233%3D%2520%2520Existing%2520local%2520Explainable%2520AI%2520%2528XAI%2529%2520methods%252C%2520such%2520as%2520LIME%252C%2520select%2520a%2520region%2520of%250Athe%2520input%2520space%2520in%2520the%2520vicinity%2520of%2520a%2520given%2520input%2520instance%252C%2520for%2520which%2520they%250Aapproximate%2520the%2520behaviour%2520of%2520a%2520model%2520using%2520a%2520simpler%2520and%2520more%2520interpretable%250Asurrogate%2520model.%2520The%2520size%2520of%2520this%2520region%2520is%2520often%2520controlled%2520by%2520a%2520user-defined%250Alocality%2520hyperparameter.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520the%2520difficulties%250Aassociated%2520with%2520defining%2520a%2520suitable%2520locality%2520size%2520to%2520capture%2520impactful%2520model%250Abehaviour%252C%2520as%2520well%2520as%2520the%2520inadequacy%2520of%2520using%2520a%2520single%2520locality%2520size%2520to%2520explain%250Aall%2520predictions.%2520We%2520propose%2520a%2520novel%2520method%252C%2520MASALA%252C%2520for%2520generating%250Aexplanations%252C%2520which%2520automatically%2520determines%2520the%2520appropriate%2520local%2520region%2520of%250Aimpactful%2520model%2520behaviour%2520for%2520each%2520individual%2520instance%2520being%2520explained.%2520MASALA%250Aapproximates%2520the%2520local%2520behaviour%2520used%2520by%2520a%2520complex%2520model%2520to%2520make%2520a%2520prediction%250Aby%2520fitting%2520a%2520linear%2520surrogate%2520model%2520to%2520a%2520set%2520of%2520points%2520which%2520experience%2520similar%250Amodel%2520behaviour.%2520These%2520points%2520are%2520found%2520by%2520clustering%2520the%2520input%2520space%2520into%250Aregions%2520of%2520linear%2520behavioural%2520trends%2520exhibited%2520by%2520the%2520model.%2520We%2520compare%2520the%250Afidelity%2520and%2520consistency%2520of%2520explanations%2520generated%2520by%2520our%2520method%2520with%2520existing%250Alocal%2520XAI%2520methods%252C%2520namely%2520LIME%2520and%2520CHILLI.%2520Experiments%2520on%2520the%2520PHM08%2520and%2520MIDAS%250Adatasets%2520show%2520that%2520our%2520method%2520produces%2520more%2520faithful%2520and%2520consistent%250Aexplanations%2520than%2520existing%2520methods%252C%2520without%2520the%2520need%2520to%2520define%2520any%2520sensitive%250Alocality%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASALA%3A%20Model-Agnostic%20Surrogate%20Explanations%20by%20Locality%20Adaptation&entry.906535625=Saif%20Anwar%20and%20Nathan%20Griffiths%20and%20Abhir%20Bhalerao%20and%20Thomas%20Popham&entry.1292438233=%20%20Existing%20local%20Explainable%20AI%20%28XAI%29%20methods%2C%20such%20as%20LIME%2C%20select%20a%20region%20of%0Athe%20input%20space%20in%20the%20vicinity%20of%20a%20given%20input%20instance%2C%20for%20which%20they%0Aapproximate%20the%20behaviour%20of%20a%20model%20using%20a%20simpler%20and%20more%20interpretable%0Asurrogate%20model.%20The%20size%20of%20this%20region%20is%20often%20controlled%20by%20a%20user-defined%0Alocality%20hyperparameter.%20In%20this%20paper%2C%20we%20demonstrate%20the%20difficulties%0Aassociated%20with%20defining%20a%20suitable%20locality%20size%20to%20capture%20impactful%20model%0Abehaviour%2C%20as%20well%20as%20the%20inadequacy%20of%20using%20a%20single%20locality%20size%20to%20explain%0Aall%20predictions.%20We%20propose%20a%20novel%20method%2C%20MASALA%2C%20for%20generating%0Aexplanations%2C%20which%20automatically%20determines%20the%20appropriate%20local%20region%20of%0Aimpactful%20model%20behaviour%20for%20each%20individual%20instance%20being%20explained.%20MASALA%0Aapproximates%20the%20local%20behaviour%20used%20by%20a%20complex%20model%20to%20make%20a%20prediction%0Aby%20fitting%20a%20linear%20surrogate%20model%20to%20a%20set%20of%20points%20which%20experience%20similar%0Amodel%20behaviour.%20These%20points%20are%20found%20by%20clustering%20the%20input%20space%20into%0Aregions%20of%20linear%20behavioural%20trends%20exhibited%20by%20the%20model.%20We%20compare%20the%0Afidelity%20and%20consistency%20of%20explanations%20generated%20by%20our%20method%20with%20existing%0Alocal%20XAI%20methods%2C%20namely%20LIME%20and%20CHILLI.%20Experiments%20on%20the%20PHM08%20and%20MIDAS%0Adatasets%20show%20that%20our%20method%20produces%20more%20faithful%20and%20consistent%0Aexplanations%20than%20existing%20methods%2C%20without%20the%20need%20to%20define%20any%20sensitive%0Alocality%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10085v1&entry.124074799=Read"},
{"title": "Structure Learning with Continuous Optimization: A Sober Look and Beyond", "author": "Ignavier Ng and Biwei Huang and Kun Zhang", "abstract": "  This paper investigates in which cases continuous optimization for directed\nacyclic graph (DAG) structure learning can and cannot perform well and why this\nhappens, and suggests possible directions to make the search procedure more\nreliable. Reisach et al. (2021) suggested that the remarkable performance of\nseveral continuous structure learning approaches is primarily driven by a high\nagreement between the order of increasing marginal variances and the\ntopological order, and demonstrated that these approaches do not perform well\nafter data standardization. We analyze this phenomenon for continuous\napproaches assuming equal and non-equal noise variances, and show that the\nstatement may not hold in either case by providing counterexamples,\njustifications, and possible alternative explanations. We further demonstrate\nthat nonconvexity may be a main concern especially for the non-equal noise\nvariances formulation, while recent advances in continuous structure learning\nfail to achieve improvement in this case. Our findings suggest that future\nworks should take into account the non-equal noise variances formulation to\nhandle more general settings and for a more comprehensive empirical evaluation.\nLastly, we provide insights into other aspects of the search procedure,\nincluding thresholding and sparsity, and show that they play an important role\nin the final solutions.\n", "link": "http://arxiv.org/abs/2304.02146v2", "date": "2024-08-19", "relevancy": 1.8899, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5061}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4486}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Learning%20with%20Continuous%20Optimization%3A%20A%20Sober%20Look%20and%20Beyond&body=Title%3A%20Structure%20Learning%20with%20Continuous%20Optimization%3A%20A%20Sober%20Look%20and%20Beyond%0AAuthor%3A%20Ignavier%20Ng%20and%20Biwei%20Huang%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20This%20paper%20investigates%20in%20which%20cases%20continuous%20optimization%20for%20directed%0Aacyclic%20graph%20%28DAG%29%20structure%20learning%20can%20and%20cannot%20perform%20well%20and%20why%20this%0Ahappens%2C%20and%20suggests%20possible%20directions%20to%20make%20the%20search%20procedure%20more%0Areliable.%20Reisach%20et%20al.%20%282021%29%20suggested%20that%20the%20remarkable%20performance%20of%0Aseveral%20continuous%20structure%20learning%20approaches%20is%20primarily%20driven%20by%20a%20high%0Aagreement%20between%20the%20order%20of%20increasing%20marginal%20variances%20and%20the%0Atopological%20order%2C%20and%20demonstrated%20that%20these%20approaches%20do%20not%20perform%20well%0Aafter%20data%20standardization.%20We%20analyze%20this%20phenomenon%20for%20continuous%0Aapproaches%20assuming%20equal%20and%20non-equal%20noise%20variances%2C%20and%20show%20that%20the%0Astatement%20may%20not%20hold%20in%20either%20case%20by%20providing%20counterexamples%2C%0Ajustifications%2C%20and%20possible%20alternative%20explanations.%20We%20further%20demonstrate%0Athat%20nonconvexity%20may%20be%20a%20main%20concern%20especially%20for%20the%20non-equal%20noise%0Avariances%20formulation%2C%20while%20recent%20advances%20in%20continuous%20structure%20learning%0Afail%20to%20achieve%20improvement%20in%20this%20case.%20Our%20findings%20suggest%20that%20future%0Aworks%20should%20take%20into%20account%20the%20non-equal%20noise%20variances%20formulation%20to%0Ahandle%20more%20general%20settings%20and%20for%20a%20more%20comprehensive%20empirical%20evaluation.%0ALastly%2C%20we%20provide%20insights%20into%20other%20aspects%20of%20the%20search%20procedure%2C%0Aincluding%20thresholding%20and%20sparsity%2C%20and%20show%20that%20they%20play%20an%20important%20role%0Ain%20the%20final%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Learning%2520with%2520Continuous%2520Optimization%253A%2520A%2520Sober%2520Look%2520and%2520Beyond%26entry.906535625%3DIgnavier%2520Ng%2520and%2520Biwei%2520Huang%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520in%2520which%2520cases%2520continuous%2520optimization%2520for%2520directed%250Aacyclic%2520graph%2520%2528DAG%2529%2520structure%2520learning%2520can%2520and%2520cannot%2520perform%2520well%2520and%2520why%2520this%250Ahappens%252C%2520and%2520suggests%2520possible%2520directions%2520to%2520make%2520the%2520search%2520procedure%2520more%250Areliable.%2520Reisach%2520et%2520al.%2520%25282021%2529%2520suggested%2520that%2520the%2520remarkable%2520performance%2520of%250Aseveral%2520continuous%2520structure%2520learning%2520approaches%2520is%2520primarily%2520driven%2520by%2520a%2520high%250Aagreement%2520between%2520the%2520order%2520of%2520increasing%2520marginal%2520variances%2520and%2520the%250Atopological%2520order%252C%2520and%2520demonstrated%2520that%2520these%2520approaches%2520do%2520not%2520perform%2520well%250Aafter%2520data%2520standardization.%2520We%2520analyze%2520this%2520phenomenon%2520for%2520continuous%250Aapproaches%2520assuming%2520equal%2520and%2520non-equal%2520noise%2520variances%252C%2520and%2520show%2520that%2520the%250Astatement%2520may%2520not%2520hold%2520in%2520either%2520case%2520by%2520providing%2520counterexamples%252C%250Ajustifications%252C%2520and%2520possible%2520alternative%2520explanations.%2520We%2520further%2520demonstrate%250Athat%2520nonconvexity%2520may%2520be%2520a%2520main%2520concern%2520especially%2520for%2520the%2520non-equal%2520noise%250Avariances%2520formulation%252C%2520while%2520recent%2520advances%2520in%2520continuous%2520structure%2520learning%250Afail%2520to%2520achieve%2520improvement%2520in%2520this%2520case.%2520Our%2520findings%2520suggest%2520that%2520future%250Aworks%2520should%2520take%2520into%2520account%2520the%2520non-equal%2520noise%2520variances%2520formulation%2520to%250Ahandle%2520more%2520general%2520settings%2520and%2520for%2520a%2520more%2520comprehensive%2520empirical%2520evaluation.%250ALastly%252C%2520we%2520provide%2520insights%2520into%2520other%2520aspects%2520of%2520the%2520search%2520procedure%252C%250Aincluding%2520thresholding%2520and%2520sparsity%252C%2520and%2520show%2520that%2520they%2520play%2520an%2520important%2520role%250Ain%2520the%2520final%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Learning%20with%20Continuous%20Optimization%3A%20A%20Sober%20Look%20and%20Beyond&entry.906535625=Ignavier%20Ng%20and%20Biwei%20Huang%20and%20Kun%20Zhang&entry.1292438233=%20%20This%20paper%20investigates%20in%20which%20cases%20continuous%20optimization%20for%20directed%0Aacyclic%20graph%20%28DAG%29%20structure%20learning%20can%20and%20cannot%20perform%20well%20and%20why%20this%0Ahappens%2C%20and%20suggests%20possible%20directions%20to%20make%20the%20search%20procedure%20more%0Areliable.%20Reisach%20et%20al.%20%282021%29%20suggested%20that%20the%20remarkable%20performance%20of%0Aseveral%20continuous%20structure%20learning%20approaches%20is%20primarily%20driven%20by%20a%20high%0Aagreement%20between%20the%20order%20of%20increasing%20marginal%20variances%20and%20the%0Atopological%20order%2C%20and%20demonstrated%20that%20these%20approaches%20do%20not%20perform%20well%0Aafter%20data%20standardization.%20We%20analyze%20this%20phenomenon%20for%20continuous%0Aapproaches%20assuming%20equal%20and%20non-equal%20noise%20variances%2C%20and%20show%20that%20the%0Astatement%20may%20not%20hold%20in%20either%20case%20by%20providing%20counterexamples%2C%0Ajustifications%2C%20and%20possible%20alternative%20explanations.%20We%20further%20demonstrate%0Athat%20nonconvexity%20may%20be%20a%20main%20concern%20especially%20for%20the%20non-equal%20noise%0Avariances%20formulation%2C%20while%20recent%20advances%20in%20continuous%20structure%20learning%0Afail%20to%20achieve%20improvement%20in%20this%20case.%20Our%20findings%20suggest%20that%20future%0Aworks%20should%20take%20into%20account%20the%20non-equal%20noise%20variances%20formulation%20to%0Ahandle%20more%20general%20settings%20and%20for%20a%20more%20comprehensive%20empirical%20evaluation.%0ALastly%2C%20we%20provide%20insights%20into%20other%20aspects%20of%20the%20search%20procedure%2C%0Aincluding%20thresholding%20and%20sparsity%2C%20and%20show%20that%20they%20play%20an%20important%20role%0Ain%20the%20final%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02146v2&entry.124074799=Read"},
{"title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation\n  using Large Language Models", "author": "Benjamin Feuer and Yurong Liu and Chinmay Hegde and Juliana Freire", "abstract": "  Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.\n", "link": "http://arxiv.org/abs/2310.18208v3", "date": "2024-08-19", "relevancy": 1.8841, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4972}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArcheType%3A%20A%20Novel%20Framework%20for%20Open-Source%20Column%20Type%20Annotation%0A%20%20using%20Large%20Language%20Models&body=Title%3A%20ArcheType%3A%20A%20Novel%20Framework%20for%20Open-Source%20Column%20Type%20Annotation%0A%20%20using%20Large%20Language%20Models%0AAuthor%3A%20Benjamin%20Feuer%20and%20Yurong%20Liu%20and%20Chinmay%20Hegde%20and%20Juliana%20Freire%0AAbstract%3A%20%20%20Existing%20deep-learning%20approaches%20to%20semantic%20column%20type%20annotation%20%28CTA%29%0Ahave%20important%20shortcomings%3A%20they%20rely%20on%20semantic%20types%20which%20are%20fixed%20at%0Atraining%20time%3B%20require%20a%20large%20number%20of%20training%20samples%20per%20type%20and%20incur%0Alarge%20run-time%20inference%20costs%3B%20and%20their%20performance%20can%20degrade%20when%0Aevaluated%20on%20novel%20datasets%2C%20even%20when%20types%20remain%20constant.%20Large%20language%0Amodels%20have%20exhibited%20strong%20zero-shot%20classification%20performance%20on%20a%20wide%0Arange%20of%20tasks%20and%20in%20this%20paper%20we%20explore%20their%20use%20for%20CTA.%20We%20introduce%0AArcheType%2C%20a%20simple%2C%20practical%20method%20for%20context%20sampling%2C%20prompt%0Aserialization%2C%20model%20querying%2C%20and%20label%20remapping%2C%20which%20enables%20large%0Alanguage%20models%20to%20solve%20CTA%20problems%20in%20a%20fully%20zero-shot%20manner.%20We%20ablate%0Aeach%20component%20of%20our%20method%20separately%2C%20and%20establish%20that%20improvements%20to%0Acontext%20sampling%20and%20label%20remapping%20provide%20the%20most%20consistent%20gains.%0AArcheType%20establishes%20a%20new%20state-of-the-art%20performance%20on%20zero-shot%20CTA%0Abenchmarks%20%28including%20three%20new%20domain-specific%20benchmarks%20which%20we%20release%0Aalong%20with%20this%20paper%29%2C%20and%20when%20used%20in%20conjunction%20with%20classical%20CTA%0Atechniques%2C%20it%20outperforms%20a%20SOTA%20DoDuo%20model%20on%20the%20fine-tuned%20SOTAB%0Abenchmark.%20Our%20code%20is%20available%20at%20https%3A//github.com/penfever/ArcheType.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18208v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArcheType%253A%2520A%2520Novel%2520Framework%2520for%2520Open-Source%2520Column%2520Type%2520Annotation%250A%2520%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DBenjamin%2520Feuer%2520and%2520Yurong%2520Liu%2520and%2520Chinmay%2520Hegde%2520and%2520Juliana%2520Freire%26entry.1292438233%3D%2520%2520Existing%2520deep-learning%2520approaches%2520to%2520semantic%2520column%2520type%2520annotation%2520%2528CTA%2529%250Ahave%2520important%2520shortcomings%253A%2520they%2520rely%2520on%2520semantic%2520types%2520which%2520are%2520fixed%2520at%250Atraining%2520time%253B%2520require%2520a%2520large%2520number%2520of%2520training%2520samples%2520per%2520type%2520and%2520incur%250Alarge%2520run-time%2520inference%2520costs%253B%2520and%2520their%2520performance%2520can%2520degrade%2520when%250Aevaluated%2520on%2520novel%2520datasets%252C%2520even%2520when%2520types%2520remain%2520constant.%2520Large%2520language%250Amodels%2520have%2520exhibited%2520strong%2520zero-shot%2520classification%2520performance%2520on%2520a%2520wide%250Arange%2520of%2520tasks%2520and%2520in%2520this%2520paper%2520we%2520explore%2520their%2520use%2520for%2520CTA.%2520We%2520introduce%250AArcheType%252C%2520a%2520simple%252C%2520practical%2520method%2520for%2520context%2520sampling%252C%2520prompt%250Aserialization%252C%2520model%2520querying%252C%2520and%2520label%2520remapping%252C%2520which%2520enables%2520large%250Alanguage%2520models%2520to%2520solve%2520CTA%2520problems%2520in%2520a%2520fully%2520zero-shot%2520manner.%2520We%2520ablate%250Aeach%2520component%2520of%2520our%2520method%2520separately%252C%2520and%2520establish%2520that%2520improvements%2520to%250Acontext%2520sampling%2520and%2520label%2520remapping%2520provide%2520the%2520most%2520consistent%2520gains.%250AArcheType%2520establishes%2520a%2520new%2520state-of-the-art%2520performance%2520on%2520zero-shot%2520CTA%250Abenchmarks%2520%2528including%2520three%2520new%2520domain-specific%2520benchmarks%2520which%2520we%2520release%250Aalong%2520with%2520this%2520paper%2529%252C%2520and%2520when%2520used%2520in%2520conjunction%2520with%2520classical%2520CTA%250Atechniques%252C%2520it%2520outperforms%2520a%2520SOTA%2520DoDuo%2520model%2520on%2520the%2520fine-tuned%2520SOTAB%250Abenchmark.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/penfever/ArcheType.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18208v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArcheType%3A%20A%20Novel%20Framework%20for%20Open-Source%20Column%20Type%20Annotation%0A%20%20using%20Large%20Language%20Models&entry.906535625=Benjamin%20Feuer%20and%20Yurong%20Liu%20and%20Chinmay%20Hegde%20and%20Juliana%20Freire&entry.1292438233=%20%20Existing%20deep-learning%20approaches%20to%20semantic%20column%20type%20annotation%20%28CTA%29%0Ahave%20important%20shortcomings%3A%20they%20rely%20on%20semantic%20types%20which%20are%20fixed%20at%0Atraining%20time%3B%20require%20a%20large%20number%20of%20training%20samples%20per%20type%20and%20incur%0Alarge%20run-time%20inference%20costs%3B%20and%20their%20performance%20can%20degrade%20when%0Aevaluated%20on%20novel%20datasets%2C%20even%20when%20types%20remain%20constant.%20Large%20language%0Amodels%20have%20exhibited%20strong%20zero-shot%20classification%20performance%20on%20a%20wide%0Arange%20of%20tasks%20and%20in%20this%20paper%20we%20explore%20their%20use%20for%20CTA.%20We%20introduce%0AArcheType%2C%20a%20simple%2C%20practical%20method%20for%20context%20sampling%2C%20prompt%0Aserialization%2C%20model%20querying%2C%20and%20label%20remapping%2C%20which%20enables%20large%0Alanguage%20models%20to%20solve%20CTA%20problems%20in%20a%20fully%20zero-shot%20manner.%20We%20ablate%0Aeach%20component%20of%20our%20method%20separately%2C%20and%20establish%20that%20improvements%20to%0Acontext%20sampling%20and%20label%20remapping%20provide%20the%20most%20consistent%20gains.%0AArcheType%20establishes%20a%20new%20state-of-the-art%20performance%20on%20zero-shot%20CTA%0Abenchmarks%20%28including%20three%20new%20domain-specific%20benchmarks%20which%20we%20release%0Aalong%20with%20this%20paper%29%2C%20and%20when%20used%20in%20conjunction%20with%20classical%20CTA%0Atechniques%2C%20it%20outperforms%20a%20SOTA%20DoDuo%20model%20on%20the%20fine-tuned%20SOTAB%0Abenchmark.%20Our%20code%20is%20available%20at%20https%3A//github.com/penfever/ArcheType.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18208v3&entry.124074799=Read"},
{"title": "Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation", "author": "Gen Li and Nikolaos Tsagkas and Jifei Song and Ruaridh Mon-Williams and Sethu Vijayakumar and Kun Shao and Laura Sevilla-Lara", "abstract": "  Affordance, defined as the potential actions that an object offers, is\ncrucial for robotic manipulation tasks. A deep understanding of affordance can\nlead to more intelligent AI systems. For example, such knowledge directs an\nagent to grasp a knife by the handle for cutting and by the blade when passing\nit to someone. In this paper, we present a streamlined affordance learning\nsystem that encompasses data collection, effective model training, and robot\ndeployment. First, we collect training data from egocentric videos in an\nautomatic manner. Different from previous methods that focus only on the object\ngraspable affordance and represent it as coarse heatmaps, we cover both\ngraspable (e.g., object handles) and functional affordances (e.g., knife\nblades, hammer heads) and extract data with precise segmentation masks. We then\npropose an effective model, termed Geometry-guided Affordance Transformer\n(GKT), to train on the collected data. GKT integrates an innovative Depth\nFeature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing\nthe model's understanding of affordances. To enable affordance-oriented\nmanipulation, we further introduce Aff-Grasp, a framework that combines GKT\nwith a grasp generation model. For comprehensive evaluation, we create an\naffordance evaluation dataset with pixel-wise annotations, and design\nreal-world tasks for robot experiments. The results show that GKT surpasses the\nstate-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of\n95.5% in affordance prediction and 77.1% in successful grasping among 179\ntrials, including evaluations with seen, unseen objects, and cluttered scenes.\n", "link": "http://arxiv.org/abs/2408.10123v1", "date": "2024-08-19", "relevancy": 1.8787, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6714}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6173}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Precise%20Affordances%20from%20Egocentric%20Videos%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20Learning%20Precise%20Affordances%20from%20Egocentric%20Videos%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Gen%20Li%20and%20Nikolaos%20Tsagkas%20and%20Jifei%20Song%20and%20Ruaridh%20Mon-Williams%20and%20Sethu%20Vijayakumar%20and%20Kun%20Shao%20and%20Laura%20Sevilla-Lara%0AAbstract%3A%20%20%20Affordance%2C%20defined%20as%20the%20potential%20actions%20that%20an%20object%20offers%2C%20is%0Acrucial%20for%20robotic%20manipulation%20tasks.%20A%20deep%20understanding%20of%20affordance%20can%0Alead%20to%20more%20intelligent%20AI%20systems.%20For%20example%2C%20such%20knowledge%20directs%20an%0Aagent%20to%20grasp%20a%20knife%20by%20the%20handle%20for%20cutting%20and%20by%20the%20blade%20when%20passing%0Ait%20to%20someone.%20In%20this%20paper%2C%20we%20present%20a%20streamlined%20affordance%20learning%0Asystem%20that%20encompasses%20data%20collection%2C%20effective%20model%20training%2C%20and%20robot%0Adeployment.%20First%2C%20we%20collect%20training%20data%20from%20egocentric%20videos%20in%20an%0Aautomatic%20manner.%20Different%20from%20previous%20methods%20that%20focus%20only%20on%20the%20object%0Agraspable%20affordance%20and%20represent%20it%20as%20coarse%20heatmaps%2C%20we%20cover%20both%0Agraspable%20%28e.g.%2C%20object%20handles%29%20and%20functional%20affordances%20%28e.g.%2C%20knife%0Ablades%2C%20hammer%20heads%29%20and%20extract%20data%20with%20precise%20segmentation%20masks.%20We%20then%0Apropose%20an%20effective%20model%2C%20termed%20Geometry-guided%20Affordance%20Transformer%0A%28GKT%29%2C%20to%20train%20on%20the%20collected%20data.%20GKT%20integrates%20an%20innovative%20Depth%0AFeature%20Injector%20%28DFI%29%20to%20incorporate%203D%20shape%20and%20geometric%20priors%2C%20enhancing%0Athe%20model%27s%20understanding%20of%20affordances.%20To%20enable%20affordance-oriented%0Amanipulation%2C%20we%20further%20introduce%20Aff-Grasp%2C%20a%20framework%20that%20combines%20GKT%0Awith%20a%20grasp%20generation%20model.%20For%20comprehensive%20evaluation%2C%20we%20create%20an%0Aaffordance%20evaluation%20dataset%20with%20pixel-wise%20annotations%2C%20and%20design%0Areal-world%20tasks%20for%20robot%20experiments.%20The%20results%20show%20that%20GKT%20surpasses%20the%0Astate-of-the-art%20by%2015.9%25%20in%20mIoU%2C%20and%20Aff-Grasp%20achieves%20high%20success%20rates%20of%0A95.5%25%20in%20affordance%20prediction%20and%2077.1%25%20in%20successful%20grasping%20among%20179%0Atrials%2C%20including%20evaluations%20with%20seen%2C%20unseen%20objects%2C%20and%20cluttered%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Precise%2520Affordances%2520from%2520Egocentric%2520Videos%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DGen%2520Li%2520and%2520Nikolaos%2520Tsagkas%2520and%2520Jifei%2520Song%2520and%2520Ruaridh%2520Mon-Williams%2520and%2520Sethu%2520Vijayakumar%2520and%2520Kun%2520Shao%2520and%2520Laura%2520Sevilla-Lara%26entry.1292438233%3D%2520%2520Affordance%252C%2520defined%2520as%2520the%2520potential%2520actions%2520that%2520an%2520object%2520offers%252C%2520is%250Acrucial%2520for%2520robotic%2520manipulation%2520tasks.%2520A%2520deep%2520understanding%2520of%2520affordance%2520can%250Alead%2520to%2520more%2520intelligent%2520AI%2520systems.%2520For%2520example%252C%2520such%2520knowledge%2520directs%2520an%250Aagent%2520to%2520grasp%2520a%2520knife%2520by%2520the%2520handle%2520for%2520cutting%2520and%2520by%2520the%2520blade%2520when%2520passing%250Ait%2520to%2520someone.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520streamlined%2520affordance%2520learning%250Asystem%2520that%2520encompasses%2520data%2520collection%252C%2520effective%2520model%2520training%252C%2520and%2520robot%250Adeployment.%2520First%252C%2520we%2520collect%2520training%2520data%2520from%2520egocentric%2520videos%2520in%2520an%250Aautomatic%2520manner.%2520Different%2520from%2520previous%2520methods%2520that%2520focus%2520only%2520on%2520the%2520object%250Agraspable%2520affordance%2520and%2520represent%2520it%2520as%2520coarse%2520heatmaps%252C%2520we%2520cover%2520both%250Agraspable%2520%2528e.g.%252C%2520object%2520handles%2529%2520and%2520functional%2520affordances%2520%2528e.g.%252C%2520knife%250Ablades%252C%2520hammer%2520heads%2529%2520and%2520extract%2520data%2520with%2520precise%2520segmentation%2520masks.%2520We%2520then%250Apropose%2520an%2520effective%2520model%252C%2520termed%2520Geometry-guided%2520Affordance%2520Transformer%250A%2528GKT%2529%252C%2520to%2520train%2520on%2520the%2520collected%2520data.%2520GKT%2520integrates%2520an%2520innovative%2520Depth%250AFeature%2520Injector%2520%2528DFI%2529%2520to%2520incorporate%25203D%2520shape%2520and%2520geometric%2520priors%252C%2520enhancing%250Athe%2520model%2527s%2520understanding%2520of%2520affordances.%2520To%2520enable%2520affordance-oriented%250Amanipulation%252C%2520we%2520further%2520introduce%2520Aff-Grasp%252C%2520a%2520framework%2520that%2520combines%2520GKT%250Awith%2520a%2520grasp%2520generation%2520model.%2520For%2520comprehensive%2520evaluation%252C%2520we%2520create%2520an%250Aaffordance%2520evaluation%2520dataset%2520with%2520pixel-wise%2520annotations%252C%2520and%2520design%250Areal-world%2520tasks%2520for%2520robot%2520experiments.%2520The%2520results%2520show%2520that%2520GKT%2520surpasses%2520the%250Astate-of-the-art%2520by%252015.9%2525%2520in%2520mIoU%252C%2520and%2520Aff-Grasp%2520achieves%2520high%2520success%2520rates%2520of%250A95.5%2525%2520in%2520affordance%2520prediction%2520and%252077.1%2525%2520in%2520successful%2520grasping%2520among%2520179%250Atrials%252C%2520including%2520evaluations%2520with%2520seen%252C%2520unseen%2520objects%252C%2520and%2520cluttered%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Precise%20Affordances%20from%20Egocentric%20Videos%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Gen%20Li%20and%20Nikolaos%20Tsagkas%20and%20Jifei%20Song%20and%20Ruaridh%20Mon-Williams%20and%20Sethu%20Vijayakumar%20and%20Kun%20Shao%20and%20Laura%20Sevilla-Lara&entry.1292438233=%20%20Affordance%2C%20defined%20as%20the%20potential%20actions%20that%20an%20object%20offers%2C%20is%0Acrucial%20for%20robotic%20manipulation%20tasks.%20A%20deep%20understanding%20of%20affordance%20can%0Alead%20to%20more%20intelligent%20AI%20systems.%20For%20example%2C%20such%20knowledge%20directs%20an%0Aagent%20to%20grasp%20a%20knife%20by%20the%20handle%20for%20cutting%20and%20by%20the%20blade%20when%20passing%0Ait%20to%20someone.%20In%20this%20paper%2C%20we%20present%20a%20streamlined%20affordance%20learning%0Asystem%20that%20encompasses%20data%20collection%2C%20effective%20model%20training%2C%20and%20robot%0Adeployment.%20First%2C%20we%20collect%20training%20data%20from%20egocentric%20videos%20in%20an%0Aautomatic%20manner.%20Different%20from%20previous%20methods%20that%20focus%20only%20on%20the%20object%0Agraspable%20affordance%20and%20represent%20it%20as%20coarse%20heatmaps%2C%20we%20cover%20both%0Agraspable%20%28e.g.%2C%20object%20handles%29%20and%20functional%20affordances%20%28e.g.%2C%20knife%0Ablades%2C%20hammer%20heads%29%20and%20extract%20data%20with%20precise%20segmentation%20masks.%20We%20then%0Apropose%20an%20effective%20model%2C%20termed%20Geometry-guided%20Affordance%20Transformer%0A%28GKT%29%2C%20to%20train%20on%20the%20collected%20data.%20GKT%20integrates%20an%20innovative%20Depth%0AFeature%20Injector%20%28DFI%29%20to%20incorporate%203D%20shape%20and%20geometric%20priors%2C%20enhancing%0Athe%20model%27s%20understanding%20of%20affordances.%20To%20enable%20affordance-oriented%0Amanipulation%2C%20we%20further%20introduce%20Aff-Grasp%2C%20a%20framework%20that%20combines%20GKT%0Awith%20a%20grasp%20generation%20model.%20For%20comprehensive%20evaluation%2C%20we%20create%20an%0Aaffordance%20evaluation%20dataset%20with%20pixel-wise%20annotations%2C%20and%20design%0Areal-world%20tasks%20for%20robot%20experiments.%20The%20results%20show%20that%20GKT%20surpasses%20the%0Astate-of-the-art%20by%2015.9%25%20in%20mIoU%2C%20and%20Aff-Grasp%20achieves%20high%20success%20rates%20of%0A95.5%25%20in%20affordance%20prediction%20and%2077.1%25%20in%20successful%20grasping%20among%20179%0Atrials%2C%20including%20evaluations%20with%20seen%2C%20unseen%20objects%2C%20and%20cluttered%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10123v1&entry.124074799=Read"},
{"title": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning", "author": "Partha Chakraborty and Mahmoud Alfadel and Meiyappan Nagappan", "abstract": "  Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement.\n", "link": "http://arxiv.org/abs/2407.17631v2", "date": "2024-08-19", "relevancy": 1.8738, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4837}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLAZE%3A%20Cross-Language%20and%20Cross-Project%20Bug%20Localization%20via%20Dynamic%0A%20%20Chunking%20and%20Hard%20Example%20Learning&body=Title%3A%20BLAZE%3A%20Cross-Language%20and%20Cross-Project%20Bug%20Localization%20via%20Dynamic%0A%20%20Chunking%20and%20Hard%20Example%20Learning%0AAuthor%3A%20Partha%20Chakraborty%20and%20Mahmoud%20Alfadel%20and%20Meiyappan%20Nagappan%0AAbstract%3A%20%20%20Software%20bugs%20require%20developers%20to%20exert%20significant%20effort%20to%20identify%20and%0Aresolve%20them%2C%20often%20consuming%20about%20one-third%20of%20their%20time.%20Bug%20localization%2C%0Athe%20process%20of%20pinpointing%20the%20exact%20source%20code%20files%20that%20need%20modification%2C%0Ais%20crucial%20in%20reducing%20this%20effort.%20Existing%20bug%20localization%20tools%2C%20typically%0Areliant%20on%20deep%20learning%20techniques%2C%20face%20limitations%20in%20cross-project%0Aapplicability%20and%20effectiveness%20in%20multi-language%20environments.%20Recent%0Aadvancements%20with%20Large%20Language%20Models%20%28LLMs%29%20offer%20detailed%20representations%0Afor%20bug%20localization.%20However%2C%20they%20encounter%20challenges%20with%20limited%20context%0Awindows%20and%20mapping%20accuracy.%20To%20address%20these%20issues%2C%20we%20propose%20BLAZE%2C%20an%0Aapproach%20that%20employs%20dynamic%20chunking%20and%20hard%20example%20learning.%20First%2C%20BLAZE%0Adynamically%20segments%20source%20code%20to%20minimize%20continuity%20loss.%20Then%2C%20BLAZE%0Afine-tunes%20a%20GPT-based%20model%20using%20challenging%20bug%20cases%2C%20in%20order%20to%20enhance%0Across-project%20and%20cross-language%20bug%20localization.%20To%20support%20the%20capability%20of%0ABLAZE%2C%20we%20create%20the%20BEETLEBOX%20dataset%2C%20which%20comprises%2026%2C321%20bugs%20from%2029%0Alarge%20and%20thriving%20open-source%20projects%20across%20five%20different%20programming%0Alanguages%20%28Java%2C%20C%2B%2B%2C%20Python%2C%20Go%2C%20and%20JavaScript%29.%20Our%20evaluations%20of%20BLAZE%20on%0Athree%20benchmark%20datasets%20BEETLEBOX%2C%20SWE-Bench%2C%20and%20Ye%20et%20al.%20demonstrate%0Asubstantial%20improvements%20compared%20to%20six%20state-of-the-art%20baselines.%0ASpecifically%2C%20BLAZE%20achieves%20up%20to%20an%20increase%20of%20120%25%20in%20Top%201%20accuracy%2C%20144%25%0Ain%20Mean%20Average%20Precision%20%28MAP%29%2C%20and%20100%25%20in%20Mean%20Reciprocal%20Rank%20%28MRR%29.%20An%0Aextensive%20ablation%20study%20confirms%20the%20contributions%20of%20our%20pipeline%20components%0Ato%20the%20overall%20performance%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLAZE%253A%2520Cross-Language%2520and%2520Cross-Project%2520Bug%2520Localization%2520via%2520Dynamic%250A%2520%2520Chunking%2520and%2520Hard%2520Example%2520Learning%26entry.906535625%3DPartha%2520Chakraborty%2520and%2520Mahmoud%2520Alfadel%2520and%2520Meiyappan%2520Nagappan%26entry.1292438233%3D%2520%2520Software%2520bugs%2520require%2520developers%2520to%2520exert%2520significant%2520effort%2520to%2520identify%2520and%250Aresolve%2520them%252C%2520often%2520consuming%2520about%2520one-third%2520of%2520their%2520time.%2520Bug%2520localization%252C%250Athe%2520process%2520of%2520pinpointing%2520the%2520exact%2520source%2520code%2520files%2520that%2520need%2520modification%252C%250Ais%2520crucial%2520in%2520reducing%2520this%2520effort.%2520Existing%2520bug%2520localization%2520tools%252C%2520typically%250Areliant%2520on%2520deep%2520learning%2520techniques%252C%2520face%2520limitations%2520in%2520cross-project%250Aapplicability%2520and%2520effectiveness%2520in%2520multi-language%2520environments.%2520Recent%250Aadvancements%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520detailed%2520representations%250Afor%2520bug%2520localization.%2520However%252C%2520they%2520encounter%2520challenges%2520with%2520limited%2520context%250Awindows%2520and%2520mapping%2520accuracy.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520BLAZE%252C%2520an%250Aapproach%2520that%2520employs%2520dynamic%2520chunking%2520and%2520hard%2520example%2520learning.%2520First%252C%2520BLAZE%250Adynamically%2520segments%2520source%2520code%2520to%2520minimize%2520continuity%2520loss.%2520Then%252C%2520BLAZE%250Afine-tunes%2520a%2520GPT-based%2520model%2520using%2520challenging%2520bug%2520cases%252C%2520in%2520order%2520to%2520enhance%250Across-project%2520and%2520cross-language%2520bug%2520localization.%2520To%2520support%2520the%2520capability%2520of%250ABLAZE%252C%2520we%2520create%2520the%2520BEETLEBOX%2520dataset%252C%2520which%2520comprises%252026%252C321%2520bugs%2520from%252029%250Alarge%2520and%2520thriving%2520open-source%2520projects%2520across%2520five%2520different%2520programming%250Alanguages%2520%2528Java%252C%2520C%252B%252B%252C%2520Python%252C%2520Go%252C%2520and%2520JavaScript%2529.%2520Our%2520evaluations%2520of%2520BLAZE%2520on%250Athree%2520benchmark%2520datasets%2520BEETLEBOX%252C%2520SWE-Bench%252C%2520and%2520Ye%2520et%2520al.%2520demonstrate%250Asubstantial%2520improvements%2520compared%2520to%2520six%2520state-of-the-art%2520baselines.%250ASpecifically%252C%2520BLAZE%2520achieves%2520up%2520to%2520an%2520increase%2520of%2520120%2525%2520in%2520Top%25201%2520accuracy%252C%2520144%2525%250Ain%2520Mean%2520Average%2520Precision%2520%2528MAP%2529%252C%2520and%2520100%2525%2520in%2520Mean%2520Reciprocal%2520Rank%2520%2528MRR%2529.%2520An%250Aextensive%2520ablation%2520study%2520confirms%2520the%2520contributions%2520of%2520our%2520pipeline%2520components%250Ato%2520the%2520overall%2520performance%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLAZE%3A%20Cross-Language%20and%20Cross-Project%20Bug%20Localization%20via%20Dynamic%0A%20%20Chunking%20and%20Hard%20Example%20Learning&entry.906535625=Partha%20Chakraborty%20and%20Mahmoud%20Alfadel%20and%20Meiyappan%20Nagappan&entry.1292438233=%20%20Software%20bugs%20require%20developers%20to%20exert%20significant%20effort%20to%20identify%20and%0Aresolve%20them%2C%20often%20consuming%20about%20one-third%20of%20their%20time.%20Bug%20localization%2C%0Athe%20process%20of%20pinpointing%20the%20exact%20source%20code%20files%20that%20need%20modification%2C%0Ais%20crucial%20in%20reducing%20this%20effort.%20Existing%20bug%20localization%20tools%2C%20typically%0Areliant%20on%20deep%20learning%20techniques%2C%20face%20limitations%20in%20cross-project%0Aapplicability%20and%20effectiveness%20in%20multi-language%20environments.%20Recent%0Aadvancements%20with%20Large%20Language%20Models%20%28LLMs%29%20offer%20detailed%20representations%0Afor%20bug%20localization.%20However%2C%20they%20encounter%20challenges%20with%20limited%20context%0Awindows%20and%20mapping%20accuracy.%20To%20address%20these%20issues%2C%20we%20propose%20BLAZE%2C%20an%0Aapproach%20that%20employs%20dynamic%20chunking%20and%20hard%20example%20learning.%20First%2C%20BLAZE%0Adynamically%20segments%20source%20code%20to%20minimize%20continuity%20loss.%20Then%2C%20BLAZE%0Afine-tunes%20a%20GPT-based%20model%20using%20challenging%20bug%20cases%2C%20in%20order%20to%20enhance%0Across-project%20and%20cross-language%20bug%20localization.%20To%20support%20the%20capability%20of%0ABLAZE%2C%20we%20create%20the%20BEETLEBOX%20dataset%2C%20which%20comprises%2026%2C321%20bugs%20from%2029%0Alarge%20and%20thriving%20open-source%20projects%20across%20five%20different%20programming%0Alanguages%20%28Java%2C%20C%2B%2B%2C%20Python%2C%20Go%2C%20and%20JavaScript%29.%20Our%20evaluations%20of%20BLAZE%20on%0Athree%20benchmark%20datasets%20BEETLEBOX%2C%20SWE-Bench%2C%20and%20Ye%20et%20al.%20demonstrate%0Asubstantial%20improvements%20compared%20to%20six%20state-of-the-art%20baselines.%0ASpecifically%2C%20BLAZE%20achieves%20up%20to%20an%20increase%20of%20120%25%20in%20Top%201%20accuracy%2C%20144%25%0Ain%20Mean%20Average%20Precision%20%28MAP%29%2C%20and%20100%25%20in%20Mean%20Reciprocal%20Rank%20%28MRR%29.%20An%0Aextensive%20ablation%20study%20confirms%20the%20contributions%20of%20our%20pipeline%20components%0Ato%20the%20overall%20performance%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17631v2&entry.124074799=Read"},
{"title": "MagicFace: Training-free Universal-Style Human Image Customized\n  Synthesis", "author": "Yibin Wang and Weizhong Zhang and Cheng Jin", "abstract": "  Current state-of-the-art methods for human image customized synthesis\ntypically require tedious training on large-scale datasets. In such cases, they\nare prone to overfitting and struggle to personalize individuals of unseen\nstyles. Moreover, these methods extensively focus on single-concept human image\nsynthesis and lack the flexibility needed for customizing individuals with\nmultiple given concepts, thereby impeding their broader practical application.\nTo this end, we propose MagicFace, a novel training-free method for\nuniversal-style human image personalized synthesis, enabling multi-concept\ncustomization by accurately integrating reference concept features into their\nlatent generated region at the pixel level. Specifically, MagicFace introduces\na coarse-to-fine generation pipeline, involving two sequential stages: semantic\nlayout construction and concept feature injection. This is achieved by our\nReference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA)\nmechanisms. In the first stage, RSA enables the latent image to query features\nfrom all reference concepts simultaneously, extracting the overall semantic\nunderstanding to facilitate the initial semantic layout establishment. In the\nsecond stage, we employ an attention-based semantic segmentation method to\npinpoint the latent generated regions of all concepts at each step. Following\nthis, RBA divides the pixels of the latent image into semantic groups, with\neach group querying fine-grained features from the corresponding reference\nconcept, which ensures precise attribute alignment and feature injection.\nThroughout the generation process, a weighted mask strategy is employed to\nensure the model focuses more on the reference concepts. Extensive experiments\ndemonstrate the superiority of MagicFace in both human-centric subject-to-image\nsynthesis and multi-concept human image customization.\n", "link": "http://arxiv.org/abs/2408.07433v3", "date": "2024-08-19", "relevancy": 1.8616, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5762}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&body=Title%3A%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis%0AAuthor%3A%20Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%0AAbstract%3A%20%20%20Current%20state-of-the-art%20methods%20for%20human%20image%20customized%20synthesis%0Atypically%20require%20tedious%20training%20on%20large-scale%20datasets.%20In%20such%20cases%2C%20they%0Aare%20prone%20to%20overfitting%20and%20struggle%20to%20personalize%20individuals%20of%20unseen%0Astyles.%20Moreover%2C%20these%20methods%20extensively%20focus%20on%20single-concept%20human%20image%0Asynthesis%20and%20lack%20the%20flexibility%20needed%20for%20customizing%20individuals%20with%0Amultiple%20given%20concepts%2C%20thereby%20impeding%20their%20broader%20practical%20application.%0ATo%20this%20end%2C%20we%20propose%20MagicFace%2C%20a%20novel%20training-free%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%2C%20enabling%20multi-concept%0Acustomization%20by%20accurately%20integrating%20reference%20concept%20features%20into%20their%0Alatent%20generated%20region%20at%20the%20pixel%20level.%20Specifically%2C%20MagicFace%20introduces%0Aa%20coarse-to-fine%20generation%20pipeline%2C%20involving%20two%20sequential%20stages%3A%20semantic%0Alayout%20construction%20and%20concept%20feature%20injection.%20This%20is%20achieved%20by%20our%0AReference-aware%20Self-Attention%20%28RSA%29%20and%20Region-grouped%20Blend%20Attention%20%28RBA%29%0Amechanisms.%20In%20the%20first%20stage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%0Afrom%20all%20reference%20concepts%20simultaneously%2C%20extracting%20the%20overall%20semantic%0Aunderstanding%20to%20facilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%0Asecond%20stage%2C%20we%20employ%20an%20attention-based%20semantic%20segmentation%20method%20to%0Apinpoint%20the%20latent%20generated%20regions%20of%20all%20concepts%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20the%20corresponding%20reference%0Aconcept%2C%20which%20ensures%20precise%20attribute%20alignment%20and%20feature%20injection.%0AThroughout%20the%20generation%20process%2C%20a%20weighted%20mask%20strategy%20is%20employed%20to%0Aensure%20the%20model%20focuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20MagicFace%20in%20both%20human-centric%20subject-to-image%0Asynthesis%20and%20multi-concept%20human%20image%20customization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07433v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicFace%253A%2520Training-free%2520Universal-Style%2520Human%2520Image%2520Customized%250A%2520%2520Synthesis%26entry.906535625%3DYibin%2520Wang%2520and%2520Weizhong%2520Zhang%2520and%2520Cheng%2520Jin%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520methods%2520for%2520human%2520image%2520customized%2520synthesis%250Atypically%2520require%2520tedious%2520training%2520on%2520large-scale%2520datasets.%2520In%2520such%2520cases%252C%2520they%250Aare%2520prone%2520to%2520overfitting%2520and%2520struggle%2520to%2520personalize%2520individuals%2520of%2520unseen%250Astyles.%2520Moreover%252C%2520these%2520methods%2520extensively%2520focus%2520on%2520single-concept%2520human%2520image%250Asynthesis%2520and%2520lack%2520the%2520flexibility%2520needed%2520for%2520customizing%2520individuals%2520with%250Amultiple%2520given%2520concepts%252C%2520thereby%2520impeding%2520their%2520broader%2520practical%2520application.%250ATo%2520this%2520end%252C%2520we%2520propose%2520MagicFace%252C%2520a%2520novel%2520training-free%2520method%2520for%250Auniversal-style%2520human%2520image%2520personalized%2520synthesis%252C%2520enabling%2520multi-concept%250Acustomization%2520by%2520accurately%2520integrating%2520reference%2520concept%2520features%2520into%2520their%250Alatent%2520generated%2520region%2520at%2520the%2520pixel%2520level.%2520Specifically%252C%2520MagicFace%2520introduces%250Aa%2520coarse-to-fine%2520generation%2520pipeline%252C%2520involving%2520two%2520sequential%2520stages%253A%2520semantic%250Alayout%2520construction%2520and%2520concept%2520feature%2520injection.%2520This%2520is%2520achieved%2520by%2520our%250AReference-aware%2520Self-Attention%2520%2528RSA%2529%2520and%2520Region-grouped%2520Blend%2520Attention%2520%2528RBA%2529%250Amechanisms.%2520In%2520the%2520first%2520stage%252C%2520RSA%2520enables%2520the%2520latent%2520image%2520to%2520query%2520features%250Afrom%2520all%2520reference%2520concepts%2520simultaneously%252C%2520extracting%2520the%2520overall%2520semantic%250Aunderstanding%2520to%2520facilitate%2520the%2520initial%2520semantic%2520layout%2520establishment.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520employ%2520an%2520attention-based%2520semantic%2520segmentation%2520method%2520to%250Apinpoint%2520the%2520latent%2520generated%2520regions%2520of%2520all%2520concepts%2520at%2520each%2520step.%2520Following%250Athis%252C%2520RBA%2520divides%2520the%2520pixels%2520of%2520the%2520latent%2520image%2520into%2520semantic%2520groups%252C%2520with%250Aeach%2520group%2520querying%2520fine-grained%2520features%2520from%2520the%2520corresponding%2520reference%250Aconcept%252C%2520which%2520ensures%2520precise%2520attribute%2520alignment%2520and%2520feature%2520injection.%250AThroughout%2520the%2520generation%2520process%252C%2520a%2520weighted%2520mask%2520strategy%2520is%2520employed%2520to%250Aensure%2520the%2520model%2520focuses%2520more%2520on%2520the%2520reference%2520concepts.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520MagicFace%2520in%2520both%2520human-centric%2520subject-to-image%250Asynthesis%2520and%2520multi-concept%2520human%2520image%2520customization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07433v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&entry.906535625=Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=%20%20Current%20state-of-the-art%20methods%20for%20human%20image%20customized%20synthesis%0Atypically%20require%20tedious%20training%20on%20large-scale%20datasets.%20In%20such%20cases%2C%20they%0Aare%20prone%20to%20overfitting%20and%20struggle%20to%20personalize%20individuals%20of%20unseen%0Astyles.%20Moreover%2C%20these%20methods%20extensively%20focus%20on%20single-concept%20human%20image%0Asynthesis%20and%20lack%20the%20flexibility%20needed%20for%20customizing%20individuals%20with%0Amultiple%20given%20concepts%2C%20thereby%20impeding%20their%20broader%20practical%20application.%0ATo%20this%20end%2C%20we%20propose%20MagicFace%2C%20a%20novel%20training-free%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%2C%20enabling%20multi-concept%0Acustomization%20by%20accurately%20integrating%20reference%20concept%20features%20into%20their%0Alatent%20generated%20region%20at%20the%20pixel%20level.%20Specifically%2C%20MagicFace%20introduces%0Aa%20coarse-to-fine%20generation%20pipeline%2C%20involving%20two%20sequential%20stages%3A%20semantic%0Alayout%20construction%20and%20concept%20feature%20injection.%20This%20is%20achieved%20by%20our%0AReference-aware%20Self-Attention%20%28RSA%29%20and%20Region-grouped%20Blend%20Attention%20%28RBA%29%0Amechanisms.%20In%20the%20first%20stage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%0Afrom%20all%20reference%20concepts%20simultaneously%2C%20extracting%20the%20overall%20semantic%0Aunderstanding%20to%20facilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%0Asecond%20stage%2C%20we%20employ%20an%20attention-based%20semantic%20segmentation%20method%20to%0Apinpoint%20the%20latent%20generated%20regions%20of%20all%20concepts%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20the%20corresponding%20reference%0Aconcept%2C%20which%20ensures%20precise%20attribute%20alignment%20and%20feature%20injection.%0AThroughout%20the%20generation%20process%2C%20a%20weighted%20mask%20strategy%20is%20employed%20to%0Aensure%20the%20model%20focuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20MagicFace%20in%20both%20human-centric%20subject-to-image%0Asynthesis%20and%20multi-concept%20human%20image%20customization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07433v3&entry.124074799=Read"},
{"title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations", "author": "Swapnaja Achintalwar and Adriana Alvarado Garcia and Ateret Anaby-Tavor and Ioana Baldini and Sara E. Berger and Bishwaranjan Bhattacharjee and Djallel Bouneffouf and Subhajit Chaudhury and Pin-Yu Chen and Lamogha Chiazor and Elizabeth M. Daly and Kirushikesh DB and Rog\u00e9rio Abreu de Paula and Pierre Dognin and Eitan Farchi and Soumya Ghosh and Michael Hind and Raya Horesh and George Kour and Ja Young Lee and Nishtha Madaan and Sameep Mehta and Erik Miehling and Keerthiram Murugesan and Manish Nagireddy and Inkit Padhi and David Piorkowski and Ambrish Rawat and Orna Raz and Prasanna Sattigeri and Hendrik Strobelt and Sarathkrishna Swaminathan and Christoph Tillmann and Aashka Trivedi and Kush R. Varshney and Dennis Wei and Shalisha Witherspooon and Marcel Zalmanovici", "abstract": "  Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope.\n", "link": "http://arxiv.org/abs/2403.06009v3", "date": "2024-08-19", "relevancy": 1.8597, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.533}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detectors%20for%20Safe%20and%20Reliable%20LLMs%3A%20Implementations%2C%20Uses%2C%20and%0A%20%20Limitations&body=Title%3A%20Detectors%20for%20Safe%20and%20Reliable%20LLMs%3A%20Implementations%2C%20Uses%2C%20and%0A%20%20Limitations%0AAuthor%3A%20Swapnaja%20Achintalwar%20and%20Adriana%20Alvarado%20Garcia%20and%20Ateret%20Anaby-Tavor%20and%20Ioana%20Baldini%20and%20Sara%20E.%20Berger%20and%20Bishwaranjan%20Bhattacharjee%20and%20Djallel%20Bouneffouf%20and%20Subhajit%20Chaudhury%20and%20Pin-Yu%20Chen%20and%20Lamogha%20Chiazor%20and%20Elizabeth%20M.%20Daly%20and%20Kirushikesh%20DB%20and%20Rog%C3%A9rio%20Abreu%20de%20Paula%20and%20Pierre%20Dognin%20and%20Eitan%20Farchi%20and%20Soumya%20Ghosh%20and%20Michael%20Hind%20and%20Raya%20Horesh%20and%20George%20Kour%20and%20Ja%20Young%20Lee%20and%20Nishtha%20Madaan%20and%20Sameep%20Mehta%20and%20Erik%20Miehling%20and%20Keerthiram%20Murugesan%20and%20Manish%20Nagireddy%20and%20Inkit%20Padhi%20and%20David%20Piorkowski%20and%20Ambrish%20Rawat%20and%20Orna%20Raz%20and%20Prasanna%20Sattigeri%20and%20Hendrik%20Strobelt%20and%20Sarathkrishna%20Swaminathan%20and%20Christoph%20Tillmann%20and%20Aashka%20Trivedi%20and%20Kush%20R.%20Varshney%20and%20Dennis%20Wei%20and%20Shalisha%20Witherspooon%20and%20Marcel%20Zalmanovici%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20a%20variety%20of%20risks%2C%20from%0Anon-faithful%20output%20to%20biased%20and%20toxic%20generations.%20Due%20to%20several%20limiting%0Afactors%20surrounding%20LLMs%20%28training%20cost%2C%20API%20access%2C%20data%20availability%2C%20etc.%29%2C%0Ait%20may%20not%20always%20be%20feasible%20to%20impose%20direct%20safety%20constraints%20on%20a%20deployed%0Amodel.%20Therefore%2C%20an%20efficient%20and%20reliable%20alternative%20is%20required.%20To%20this%0Aend%2C%20we%20present%20our%20ongoing%20efforts%20to%20create%20and%20deploy%20a%20library%20of%0Adetectors%3A%20compact%20and%20easy-to-build%20classification%20models%20that%20provide%20labels%0Afor%20various%20harms.%20In%20addition%20to%20the%20detectors%20themselves%2C%20we%20discuss%20a%20wide%0Arange%20of%20uses%20for%20these%20detector%20models%20-%20from%20acting%20as%20guardrails%20to%20enabling%0Aeffective%20AI%20governance.%20We%20also%20deep%20dive%20into%20inherent%20challenges%20in%20their%0Adevelopment%20and%20discuss%20future%20work%20aimed%20at%20making%20the%20detectors%20more%20reliable%0Aand%20broadening%20their%20scope.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetectors%2520for%2520Safe%2520and%2520Reliable%2520LLMs%253A%2520Implementations%252C%2520Uses%252C%2520and%250A%2520%2520Limitations%26entry.906535625%3DSwapnaja%2520Achintalwar%2520and%2520Adriana%2520Alvarado%2520Garcia%2520and%2520Ateret%2520Anaby-Tavor%2520and%2520Ioana%2520Baldini%2520and%2520Sara%2520E.%2520Berger%2520and%2520Bishwaranjan%2520Bhattacharjee%2520and%2520Djallel%2520Bouneffouf%2520and%2520Subhajit%2520Chaudhury%2520and%2520Pin-Yu%2520Chen%2520and%2520Lamogha%2520Chiazor%2520and%2520Elizabeth%2520M.%2520Daly%2520and%2520Kirushikesh%2520DB%2520and%2520Rog%25C3%25A9rio%2520Abreu%2520de%2520Paula%2520and%2520Pierre%2520Dognin%2520and%2520Eitan%2520Farchi%2520and%2520Soumya%2520Ghosh%2520and%2520Michael%2520Hind%2520and%2520Raya%2520Horesh%2520and%2520George%2520Kour%2520and%2520Ja%2520Young%2520Lee%2520and%2520Nishtha%2520Madaan%2520and%2520Sameep%2520Mehta%2520and%2520Erik%2520Miehling%2520and%2520Keerthiram%2520Murugesan%2520and%2520Manish%2520Nagireddy%2520and%2520Inkit%2520Padhi%2520and%2520David%2520Piorkowski%2520and%2520Ambrish%2520Rawat%2520and%2520Orna%2520Raz%2520and%2520Prasanna%2520Sattigeri%2520and%2520Hendrik%2520Strobelt%2520and%2520Sarathkrishna%2520Swaminathan%2520and%2520Christoph%2520Tillmann%2520and%2520Aashka%2520Trivedi%2520and%2520Kush%2520R.%2520Varshney%2520and%2520Dennis%2520Wei%2520and%2520Shalisha%2520Witherspooon%2520and%2520Marcel%2520Zalmanovici%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520susceptible%2520to%2520a%2520variety%2520of%2520risks%252C%2520from%250Anon-faithful%2520output%2520to%2520biased%2520and%2520toxic%2520generations.%2520Due%2520to%2520several%2520limiting%250Afactors%2520surrounding%2520LLMs%2520%2528training%2520cost%252C%2520API%2520access%252C%2520data%2520availability%252C%2520etc.%2529%252C%250Ait%2520may%2520not%2520always%2520be%2520feasible%2520to%2520impose%2520direct%2520safety%2520constraints%2520on%2520a%2520deployed%250Amodel.%2520Therefore%252C%2520an%2520efficient%2520and%2520reliable%2520alternative%2520is%2520required.%2520To%2520this%250Aend%252C%2520we%2520present%2520our%2520ongoing%2520efforts%2520to%2520create%2520and%2520deploy%2520a%2520library%2520of%250Adetectors%253A%2520compact%2520and%2520easy-to-build%2520classification%2520models%2520that%2520provide%2520labels%250Afor%2520various%2520harms.%2520In%2520addition%2520to%2520the%2520detectors%2520themselves%252C%2520we%2520discuss%2520a%2520wide%250Arange%2520of%2520uses%2520for%2520these%2520detector%2520models%2520-%2520from%2520acting%2520as%2520guardrails%2520to%2520enabling%250Aeffective%2520AI%2520governance.%2520We%2520also%2520deep%2520dive%2520into%2520inherent%2520challenges%2520in%2520their%250Adevelopment%2520and%2520discuss%2520future%2520work%2520aimed%2520at%2520making%2520the%2520detectors%2520more%2520reliable%250Aand%2520broadening%2520their%2520scope.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detectors%20for%20Safe%20and%20Reliable%20LLMs%3A%20Implementations%2C%20Uses%2C%20and%0A%20%20Limitations&entry.906535625=Swapnaja%20Achintalwar%20and%20Adriana%20Alvarado%20Garcia%20and%20Ateret%20Anaby-Tavor%20and%20Ioana%20Baldini%20and%20Sara%20E.%20Berger%20and%20Bishwaranjan%20Bhattacharjee%20and%20Djallel%20Bouneffouf%20and%20Subhajit%20Chaudhury%20and%20Pin-Yu%20Chen%20and%20Lamogha%20Chiazor%20and%20Elizabeth%20M.%20Daly%20and%20Kirushikesh%20DB%20and%20Rog%C3%A9rio%20Abreu%20de%20Paula%20and%20Pierre%20Dognin%20and%20Eitan%20Farchi%20and%20Soumya%20Ghosh%20and%20Michael%20Hind%20and%20Raya%20Horesh%20and%20George%20Kour%20and%20Ja%20Young%20Lee%20and%20Nishtha%20Madaan%20and%20Sameep%20Mehta%20and%20Erik%20Miehling%20and%20Keerthiram%20Murugesan%20and%20Manish%20Nagireddy%20and%20Inkit%20Padhi%20and%20David%20Piorkowski%20and%20Ambrish%20Rawat%20and%20Orna%20Raz%20and%20Prasanna%20Sattigeri%20and%20Hendrik%20Strobelt%20and%20Sarathkrishna%20Swaminathan%20and%20Christoph%20Tillmann%20and%20Aashka%20Trivedi%20and%20Kush%20R.%20Varshney%20and%20Dennis%20Wei%20and%20Shalisha%20Witherspooon%20and%20Marcel%20Zalmanovici&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20a%20variety%20of%20risks%2C%20from%0Anon-faithful%20output%20to%20biased%20and%20toxic%20generations.%20Due%20to%20several%20limiting%0Afactors%20surrounding%20LLMs%20%28training%20cost%2C%20API%20access%2C%20data%20availability%2C%20etc.%29%2C%0Ait%20may%20not%20always%20be%20feasible%20to%20impose%20direct%20safety%20constraints%20on%20a%20deployed%0Amodel.%20Therefore%2C%20an%20efficient%20and%20reliable%20alternative%20is%20required.%20To%20this%0Aend%2C%20we%20present%20our%20ongoing%20efforts%20to%20create%20and%20deploy%20a%20library%20of%0Adetectors%3A%20compact%20and%20easy-to-build%20classification%20models%20that%20provide%20labels%0Afor%20various%20harms.%20In%20addition%20to%20the%20detectors%20themselves%2C%20we%20discuss%20a%20wide%0Arange%20of%20uses%20for%20these%20detector%20models%20-%20from%20acting%20as%20guardrails%20to%20enabling%0Aeffective%20AI%20governance.%20We%20also%20deep%20dive%20into%20inherent%20challenges%20in%20their%0Adevelopment%20and%20discuss%20future%20work%20aimed%20at%20making%20the%20detectors%20more%20reliable%0Aand%20broadening%20their%20scope.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06009v3&entry.124074799=Read"},
{"title": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with\n  Large Language Models", "author": "Jiao Chen and Suyan Dai and Fangfang Chen and Zuohong Lv and Jianhua Tang", "abstract": "  Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive.\n", "link": "http://arxiv.org/abs/2408.09972v1", "date": "2024-08-19", "relevancy": 1.6218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Cloud%20Collaborative%20Motion%20Planning%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20Edge-Cloud%20Collaborative%20Motion%20Planning%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jiao%20Chen%20and%20Suyan%20Dai%20and%20Fangfang%20Chen%20and%20Zuohong%20Lv%20and%20Jianhua%20Tang%0AAbstract%3A%20%20%20Integrating%20large%20language%20models%20%28LLMs%29%20into%20autonomous%20driving%20enhances%0Apersonalization%20and%20adaptability%20in%20open-world%20scenarios.%20However%2C%20traditional%0Aedge%20computing%20models%20still%20face%20significant%20challenges%20in%20processing%20complex%0Adriving%20data%2C%20particularly%20regarding%20real-time%20performance%20and%20system%0Aefficiency.%20To%20address%20these%20challenges%2C%20this%20study%20introduces%20EC-Drive%2C%20a%0Anovel%20edge-cloud%20collaborative%20autonomous%20driving%20system%20with%20data%20drift%0Adetection%20capabilities.%20EC-Drive%20utilizes%20drift%20detection%20algorithms%20to%0Aselectively%20upload%20critical%20data%2C%20including%20new%20obstacles%20and%20traffic%20pattern%0Achanges%2C%20to%20the%20cloud%20for%20processing%20by%20GPT-4%2C%20while%20routine%20data%20is%0Aefficiently%20managed%20by%20smaller%20LLMs%20on%20edge%20devices.%20This%20approach%20not%20only%0Areduces%20inference%20latency%20but%20also%20improves%20system%20efficiency%20by%20optimizing%0Acommunication%20resource%20use.%20Experimental%20validation%20confirms%20the%20system%27s%0Arobust%20processing%20capabilities%20and%20practical%20applicability%20in%20real-world%0Adriving%20conditions%2C%20demonstrating%20the%20effectiveness%20of%20this%20edge-cloud%0Acollaboration%20framework.%20Our%20data%20and%20system%20demonstration%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/ec-drive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Cloud%2520Collaborative%2520Motion%2520Planning%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJiao%2520Chen%2520and%2520Suyan%2520Dai%2520and%2520Fangfang%2520Chen%2520and%2520Zuohong%2520Lv%2520and%2520Jianhua%2520Tang%26entry.1292438233%3D%2520%2520Integrating%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520autonomous%2520driving%2520enhances%250Apersonalization%2520and%2520adaptability%2520in%2520open-world%2520scenarios.%2520However%252C%2520traditional%250Aedge%2520computing%2520models%2520still%2520face%2520significant%2520challenges%2520in%2520processing%2520complex%250Adriving%2520data%252C%2520particularly%2520regarding%2520real-time%2520performance%2520and%2520system%250Aefficiency.%2520To%2520address%2520these%2520challenges%252C%2520this%2520study%2520introduces%2520EC-Drive%252C%2520a%250Anovel%2520edge-cloud%2520collaborative%2520autonomous%2520driving%2520system%2520with%2520data%2520drift%250Adetection%2520capabilities.%2520EC-Drive%2520utilizes%2520drift%2520detection%2520algorithms%2520to%250Aselectively%2520upload%2520critical%2520data%252C%2520including%2520new%2520obstacles%2520and%2520traffic%2520pattern%250Achanges%252C%2520to%2520the%2520cloud%2520for%2520processing%2520by%2520GPT-4%252C%2520while%2520routine%2520data%2520is%250Aefficiently%2520managed%2520by%2520smaller%2520LLMs%2520on%2520edge%2520devices.%2520This%2520approach%2520not%2520only%250Areduces%2520inference%2520latency%2520but%2520also%2520improves%2520system%2520efficiency%2520by%2520optimizing%250Acommunication%2520resource%2520use.%2520Experimental%2520validation%2520confirms%2520the%2520system%2527s%250Arobust%2520processing%2520capabilities%2520and%2520practical%2520applicability%2520in%2520real-world%250Adriving%2520conditions%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520this%2520edge-cloud%250Acollaboration%2520framework.%2520Our%2520data%2520and%2520system%2520demonstration%2520will%2520be%2520released%2520at%250Ahttps%253A//sites.google.com/view/ec-drive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Cloud%20Collaborative%20Motion%20Planning%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models&entry.906535625=Jiao%20Chen%20and%20Suyan%20Dai%20and%20Fangfang%20Chen%20and%20Zuohong%20Lv%20and%20Jianhua%20Tang&entry.1292438233=%20%20Integrating%20large%20language%20models%20%28LLMs%29%20into%20autonomous%20driving%20enhances%0Apersonalization%20and%20adaptability%20in%20open-world%20scenarios.%20However%2C%20traditional%0Aedge%20computing%20models%20still%20face%20significant%20challenges%20in%20processing%20complex%0Adriving%20data%2C%20particularly%20regarding%20real-time%20performance%20and%20system%0Aefficiency.%20To%20address%20these%20challenges%2C%20this%20study%20introduces%20EC-Drive%2C%20a%0Anovel%20edge-cloud%20collaborative%20autonomous%20driving%20system%20with%20data%20drift%0Adetection%20capabilities.%20EC-Drive%20utilizes%20drift%20detection%20algorithms%20to%0Aselectively%20upload%20critical%20data%2C%20including%20new%20obstacles%20and%20traffic%20pattern%0Achanges%2C%20to%20the%20cloud%20for%20processing%20by%20GPT-4%2C%20while%20routine%20data%20is%0Aefficiently%20managed%20by%20smaller%20LLMs%20on%20edge%20devices.%20This%20approach%20not%20only%0Areduces%20inference%20latency%20but%20also%20improves%20system%20efficiency%20by%20optimizing%0Acommunication%20resource%20use.%20Experimental%20validation%20confirms%20the%20system%27s%0Arobust%20processing%20capabilities%20and%20practical%20applicability%20in%20real-world%0Adriving%20conditions%2C%20demonstrating%20the%20effectiveness%20of%20this%20edge-cloud%0Acollaboration%20framework.%20Our%20data%20and%20system%20demonstration%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/ec-drive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09972v1&entry.124074799=Read"},
{"title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language", "author": "Manjil Karki and Pratik Shakya and Sandesh Acharya and Ravi Pandit and Dinesh Gothe", "abstract": "  Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.\n", "link": "http://arxiv.org/abs/2408.10128v1", "date": "2024-08-19", "relevancy": 1.7352, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4411}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language&body=Title%3A%20Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language%0AAuthor%3A%20Manjil%20Karki%20and%20Pratik%20Shakya%20and%20Sandesh%20Acharya%20and%20Ravi%20Pandit%20and%20Dinesh%20Gothe%0AAbstract%3A%20%20%20Voice%20cloning%20is%20a%20prominent%20feature%20in%20personalized%20speech%20interfaces.%20A%0Aneural%20vocal%20cloning%20system%20can%20mimic%20someone%27s%20voice%20using%20just%20a%20few%20audio%0Asamples.%20Both%20speaker%20encoding%20and%20speaker%20adaptation%20are%20topics%20of%20research%20in%0Athe%20field%20of%20voice%20cloning.%20Speaker%20adaptation%20relies%20on%20fine-tuning%20a%0Amulti-speaker%20generative%20model%2C%20which%20involves%20training%20a%20separate%20model%20to%0Ainfer%20a%20new%20speaker%20embedding%20used%20for%20speaker%20encoding.%20Both%20methods%20can%0Aachieve%20excellent%20performance%2C%20even%20with%20a%20small%20number%20of%20cloning%20audios%2C%20in%0Aterms%20of%20the%20speech%27s%20naturalness%20and%20similarity%20to%20the%20original%20speaker.%0ASpeaker%20encoding%20approaches%20are%20more%20appropriate%20for%20low-resource%20deployment%0Asince%20they%20require%20significantly%20less%20memory%20and%20have%20a%20faster%20cloning%20time%0Athan%20speaker%20adaption%2C%20which%20can%20offer%20slightly%20greater%20naturalness%20and%0Asimilarity.%20The%20main%20goal%20is%20to%20create%20a%20vocal%20cloning%20system%20that%20produces%0Aaudio%20output%20with%20a%20Nepali%20accent%20or%20that%20sounds%20like%20Nepali.%20For%20the%20further%0Aadvancement%20of%20TTS%2C%20the%20idea%20of%20transfer%20learning%20was%20effectively%20used%20to%0Aaddress%20several%20issues%20that%20were%20encountered%20in%20the%20development%20of%20this%20system%2C%0Aincluding%20the%20poor%20audio%20quality%20and%20the%20lack%20of%20available%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Voice%2520Cloning%2520for%2520Nepali%253A%2520Leveraging%2520Transfer%2520Learning%2520in%2520a%250A%2520%2520Low-Resource%2520Language%26entry.906535625%3DManjil%2520Karki%2520and%2520Pratik%2520Shakya%2520and%2520Sandesh%2520Acharya%2520and%2520Ravi%2520Pandit%2520and%2520Dinesh%2520Gothe%26entry.1292438233%3D%2520%2520Voice%2520cloning%2520is%2520a%2520prominent%2520feature%2520in%2520personalized%2520speech%2520interfaces.%2520A%250Aneural%2520vocal%2520cloning%2520system%2520can%2520mimic%2520someone%2527s%2520voice%2520using%2520just%2520a%2520few%2520audio%250Asamples.%2520Both%2520speaker%2520encoding%2520and%2520speaker%2520adaptation%2520are%2520topics%2520of%2520research%2520in%250Athe%2520field%2520of%2520voice%2520cloning.%2520Speaker%2520adaptation%2520relies%2520on%2520fine-tuning%2520a%250Amulti-speaker%2520generative%2520model%252C%2520which%2520involves%2520training%2520a%2520separate%2520model%2520to%250Ainfer%2520a%2520new%2520speaker%2520embedding%2520used%2520for%2520speaker%2520encoding.%2520Both%2520methods%2520can%250Aachieve%2520excellent%2520performance%252C%2520even%2520with%2520a%2520small%2520number%2520of%2520cloning%2520audios%252C%2520in%250Aterms%2520of%2520the%2520speech%2527s%2520naturalness%2520and%2520similarity%2520to%2520the%2520original%2520speaker.%250ASpeaker%2520encoding%2520approaches%2520are%2520more%2520appropriate%2520for%2520low-resource%2520deployment%250Asince%2520they%2520require%2520significantly%2520less%2520memory%2520and%2520have%2520a%2520faster%2520cloning%2520time%250Athan%2520speaker%2520adaption%252C%2520which%2520can%2520offer%2520slightly%2520greater%2520naturalness%2520and%250Asimilarity.%2520The%2520main%2520goal%2520is%2520to%2520create%2520a%2520vocal%2520cloning%2520system%2520that%2520produces%250Aaudio%2520output%2520with%2520a%2520Nepali%2520accent%2520or%2520that%2520sounds%2520like%2520Nepali.%2520For%2520the%2520further%250Aadvancement%2520of%2520TTS%252C%2520the%2520idea%2520of%2520transfer%2520learning%2520was%2520effectively%2520used%2520to%250Aaddress%2520several%2520issues%2520that%2520were%2520encountered%2520in%2520the%2520development%2520of%2520this%2520system%252C%250Aincluding%2520the%2520poor%2520audio%2520quality%2520and%2520the%2520lack%2520of%2520available%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language&entry.906535625=Manjil%20Karki%20and%20Pratik%20Shakya%20and%20Sandesh%20Acharya%20and%20Ravi%20Pandit%20and%20Dinesh%20Gothe&entry.1292438233=%20%20Voice%20cloning%20is%20a%20prominent%20feature%20in%20personalized%20speech%20interfaces.%20A%0Aneural%20vocal%20cloning%20system%20can%20mimic%20someone%27s%20voice%20using%20just%20a%20few%20audio%0Asamples.%20Both%20speaker%20encoding%20and%20speaker%20adaptation%20are%20topics%20of%20research%20in%0Athe%20field%20of%20voice%20cloning.%20Speaker%20adaptation%20relies%20on%20fine-tuning%20a%0Amulti-speaker%20generative%20model%2C%20which%20involves%20training%20a%20separate%20model%20to%0Ainfer%20a%20new%20speaker%20embedding%20used%20for%20speaker%20encoding.%20Both%20methods%20can%0Aachieve%20excellent%20performance%2C%20even%20with%20a%20small%20number%20of%20cloning%20audios%2C%20in%0Aterms%20of%20the%20speech%27s%20naturalness%20and%20similarity%20to%20the%20original%20speaker.%0ASpeaker%20encoding%20approaches%20are%20more%20appropriate%20for%20low-resource%20deployment%0Asince%20they%20require%20significantly%20less%20memory%20and%20have%20a%20faster%20cloning%20time%0Athan%20speaker%20adaption%2C%20which%20can%20offer%20slightly%20greater%20naturalness%20and%0Asimilarity.%20The%20main%20goal%20is%20to%20create%20a%20vocal%20cloning%20system%20that%20produces%0Aaudio%20output%20with%20a%20Nepali%20accent%20or%20that%20sounds%20like%20Nepali.%20For%20the%20further%0Aadvancement%20of%20TTS%2C%20the%20idea%20of%20transfer%20learning%20was%20effectively%20used%20to%0Aaddress%20several%20issues%20that%20were%20encountered%20in%20the%20development%20of%20this%20system%2C%0Aincluding%20the%20poor%20audio%20quality%20and%20the%20lack%20of%20available%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10128v1&entry.124074799=Read"},
{"title": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in\n  Explainable Recommendation", "author": "Ching-Wen Yang and Che Wei Chen and Kun-da Wu and Hao Xu and Jui-Feng Yao and Hung-Yu Kao", "abstract": "  Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance.\n", "link": "http://arxiv.org/abs/2408.09865v1", "date": "2024-08-19", "relevancy": 1.456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPLE%3A%20Enhancing%20Review%20Generation%20with%20Multi-Aspect%20Prompt%20LEarning%20in%0A%20%20Explainable%20Recommendation&body=Title%3A%20MAPLE%3A%20Enhancing%20Review%20Generation%20with%20Multi-Aspect%20Prompt%20LEarning%20in%0A%20%20Explainable%20Recommendation%0AAuthor%3A%20Ching-Wen%20Yang%20and%20Che%20Wei%20Chen%20and%20Kun-da%20Wu%20and%20Hao%20Xu%20and%20Jui-Feng%20Yao%20and%20Hung-Yu%20Kao%0AAbstract%3A%20%20%20Explainable%20Recommendation%20task%20is%20designed%20to%20receive%20a%20pair%20of%20user%20and%0Aitem%20and%20output%20explanations%20to%20justify%20why%20an%20item%20is%20recommended%20to%20a%20user.%0AMany%20models%20treat%20review-generation%20as%20a%20proxy%20of%20explainable%20recommendation.%0AAlthough%20they%20are%20able%20to%20generate%20fluent%20and%20grammatical%20sentences%2C%20they%0Asuffer%20from%20generality%20and%20hallucination%20issues.%20We%20propose%20a%20personalized%2C%0Aaspect-controlled%20model%20called%20Multi-Aspect%20Prompt%20LEarner%20%28MAPLE%29%2C%20in%20which%20it%0Aintegrates%20aspect%20category%20as%20another%20input%20dimension%20to%20facilitate%20the%0Amemorization%20of%20fine-grained%20aspect%20terms.%20Experiments%20on%20two%20real-world%20review%0Adatasets%20in%20restaurant%20domain%20show%20that%20MAPLE%20outperforms%20the%20baseline%0Areview-generation%20models%20in%20terms%20of%20text%20and%20feature%20diversity%20while%0Amaintaining%20excellent%20coherence%20and%20factual%20relevance.%20We%20further%20treat%20MAPLE%0Aas%20a%20retriever%20component%20in%20the%20retriever-reader%20framework%20and%20employ%20a%0ALarge-Language%20Model%20%28LLM%29%20as%20the%20reader%2C%20showing%20that%20MAPLE%27s%20explanation%0Aalong%20with%20the%20LLM%27s%20comprehension%20ability%20leads%20to%20enriched%20and%20personalized%0Aexplanation%20as%20a%20result.%20We%20will%20release%20the%20code%20and%20data%20in%20this%20http%20upon%0Aacceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPLE%253A%2520Enhancing%2520Review%2520Generation%2520with%2520Multi-Aspect%2520Prompt%2520LEarning%2520in%250A%2520%2520Explainable%2520Recommendation%26entry.906535625%3DChing-Wen%2520Yang%2520and%2520Che%2520Wei%2520Chen%2520and%2520Kun-da%2520Wu%2520and%2520Hao%2520Xu%2520and%2520Jui-Feng%2520Yao%2520and%2520Hung-Yu%2520Kao%26entry.1292438233%3D%2520%2520Explainable%2520Recommendation%2520task%2520is%2520designed%2520to%2520receive%2520a%2520pair%2520of%2520user%2520and%250Aitem%2520and%2520output%2520explanations%2520to%2520justify%2520why%2520an%2520item%2520is%2520recommended%2520to%2520a%2520user.%250AMany%2520models%2520treat%2520review-generation%2520as%2520a%2520proxy%2520of%2520explainable%2520recommendation.%250AAlthough%2520they%2520are%2520able%2520to%2520generate%2520fluent%2520and%2520grammatical%2520sentences%252C%2520they%250Asuffer%2520from%2520generality%2520and%2520hallucination%2520issues.%2520We%2520propose%2520a%2520personalized%252C%250Aaspect-controlled%2520model%2520called%2520Multi-Aspect%2520Prompt%2520LEarner%2520%2528MAPLE%2529%252C%2520in%2520which%2520it%250Aintegrates%2520aspect%2520category%2520as%2520another%2520input%2520dimension%2520to%2520facilitate%2520the%250Amemorization%2520of%2520fine-grained%2520aspect%2520terms.%2520Experiments%2520on%2520two%2520real-world%2520review%250Adatasets%2520in%2520restaurant%2520domain%2520show%2520that%2520MAPLE%2520outperforms%2520the%2520baseline%250Areview-generation%2520models%2520in%2520terms%2520of%2520text%2520and%2520feature%2520diversity%2520while%250Amaintaining%2520excellent%2520coherence%2520and%2520factual%2520relevance.%2520We%2520further%2520treat%2520MAPLE%250Aas%2520a%2520retriever%2520component%2520in%2520the%2520retriever-reader%2520framework%2520and%2520employ%2520a%250ALarge-Language%2520Model%2520%2528LLM%2529%2520as%2520the%2520reader%252C%2520showing%2520that%2520MAPLE%2527s%2520explanation%250Aalong%2520with%2520the%2520LLM%2527s%2520comprehension%2520ability%2520leads%2520to%2520enriched%2520and%2520personalized%250Aexplanation%2520as%2520a%2520result.%2520We%2520will%2520release%2520the%2520code%2520and%2520data%2520in%2520this%2520http%2520upon%250Aacceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPLE%3A%20Enhancing%20Review%20Generation%20with%20Multi-Aspect%20Prompt%20LEarning%20in%0A%20%20Explainable%20Recommendation&entry.906535625=Ching-Wen%20Yang%20and%20Che%20Wei%20Chen%20and%20Kun-da%20Wu%20and%20Hao%20Xu%20and%20Jui-Feng%20Yao%20and%20Hung-Yu%20Kao&entry.1292438233=%20%20Explainable%20Recommendation%20task%20is%20designed%20to%20receive%20a%20pair%20of%20user%20and%0Aitem%20and%20output%20explanations%20to%20justify%20why%20an%20item%20is%20recommended%20to%20a%20user.%0AMany%20models%20treat%20review-generation%20as%20a%20proxy%20of%20explainable%20recommendation.%0AAlthough%20they%20are%20able%20to%20generate%20fluent%20and%20grammatical%20sentences%2C%20they%0Asuffer%20from%20generality%20and%20hallucination%20issues.%20We%20propose%20a%20personalized%2C%0Aaspect-controlled%20model%20called%20Multi-Aspect%20Prompt%20LEarner%20%28MAPLE%29%2C%20in%20which%20it%0Aintegrates%20aspect%20category%20as%20another%20input%20dimension%20to%20facilitate%20the%0Amemorization%20of%20fine-grained%20aspect%20terms.%20Experiments%20on%20two%20real-world%20review%0Adatasets%20in%20restaurant%20domain%20show%20that%20MAPLE%20outperforms%20the%20baseline%0Areview-generation%20models%20in%20terms%20of%20text%20and%20feature%20diversity%20while%0Amaintaining%20excellent%20coherence%20and%20factual%20relevance.%20We%20further%20treat%20MAPLE%0Aas%20a%20retriever%20component%20in%20the%20retriever-reader%20framework%20and%20employ%20a%0ALarge-Language%20Model%20%28LLM%29%20as%20the%20reader%2C%20showing%20that%20MAPLE%27s%20explanation%0Aalong%20with%20the%20LLM%27s%20comprehension%20ability%20leads%20to%20enriched%20and%20personalized%0Aexplanation%20as%20a%20result.%20We%20will%20release%20the%20code%20and%20data%20in%20this%20http%20upon%0Aacceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09865v1&entry.124074799=Read"},
{"title": "Dynamic Label Injection for Imbalanced Industrial Defect Segmentation", "author": "Emanuele Caruso and Francesco Pelosin and Alessandro Simoni and Marco Boschetti", "abstract": "  In this work, we propose a simple yet effective method to tackle the problem\nof imbalanced multi-class semantic segmentation in deep learning systems. One\nof the key properties for a good training set is the balancing among the\nclasses. When the input distribution is heavily imbalanced in the number of\ninstances, the learning process could be hindered or difficult to carry on. To\nthis end, we propose a Dynamic Label Injection (DLI) algorithm to impose a\nuniform distribution in the input batch. Our algorithm computes the current\nbatch defect distribution and re-balances it by transferring defects using a\ncombination of Poisson-based seamless image cloning and cut-paste techniques. A\nthorough experimental section on the Magnetic Tiles dataset shows better\nresults of DLI compared to other balancing loss approaches also in the\nchallenging weakly-supervised setup. The code is available at\nhttps://github.com/covisionlab/dynamic-label-injection.git\n", "link": "http://arxiv.org/abs/2408.10031v1", "date": "2024-08-19", "relevancy": 1.5748, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Label%20Injection%20for%20Imbalanced%20Industrial%20Defect%20Segmentation&body=Title%3A%20Dynamic%20Label%20Injection%20for%20Imbalanced%20Industrial%20Defect%20Segmentation%0AAuthor%3A%20Emanuele%20Caruso%20and%20Francesco%20Pelosin%20and%20Alessandro%20Simoni%20and%20Marco%20Boschetti%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%20method%20to%20tackle%20the%20problem%0Aof%20imbalanced%20multi-class%20semantic%20segmentation%20in%20deep%20learning%20systems.%20One%0Aof%20the%20key%20properties%20for%20a%20good%20training%20set%20is%20the%20balancing%20among%20the%0Aclasses.%20When%20the%20input%20distribution%20is%20heavily%20imbalanced%20in%20the%20number%20of%0Ainstances%2C%20the%20learning%20process%20could%20be%20hindered%20or%20difficult%20to%20carry%20on.%20To%0Athis%20end%2C%20we%20propose%20a%20Dynamic%20Label%20Injection%20%28DLI%29%20algorithm%20to%20impose%20a%0Auniform%20distribution%20in%20the%20input%20batch.%20Our%20algorithm%20computes%20the%20current%0Abatch%20defect%20distribution%20and%20re-balances%20it%20by%20transferring%20defects%20using%20a%0Acombination%20of%20Poisson-based%20seamless%20image%20cloning%20and%20cut-paste%20techniques.%20A%0Athorough%20experimental%20section%20on%20the%20Magnetic%20Tiles%20dataset%20shows%20better%0Aresults%20of%20DLI%20compared%20to%20other%20balancing%20loss%20approaches%20also%20in%20the%0Achallenging%20weakly-supervised%20setup.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/covisionlab/dynamic-label-injection.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Label%2520Injection%2520for%2520Imbalanced%2520Industrial%2520Defect%2520Segmentation%26entry.906535625%3DEmanuele%2520Caruso%2520and%2520Francesco%2520Pelosin%2520and%2520Alessandro%2520Simoni%2520and%2520Marco%2520Boschetti%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%2520to%2520tackle%2520the%2520problem%250Aof%2520imbalanced%2520multi-class%2520semantic%2520segmentation%2520in%2520deep%2520learning%2520systems.%2520One%250Aof%2520the%2520key%2520properties%2520for%2520a%2520good%2520training%2520set%2520is%2520the%2520balancing%2520among%2520the%250Aclasses.%2520When%2520the%2520input%2520distribution%2520is%2520heavily%2520imbalanced%2520in%2520the%2520number%2520of%250Ainstances%252C%2520the%2520learning%2520process%2520could%2520be%2520hindered%2520or%2520difficult%2520to%2520carry%2520on.%2520To%250Athis%2520end%252C%2520we%2520propose%2520a%2520Dynamic%2520Label%2520Injection%2520%2528DLI%2529%2520algorithm%2520to%2520impose%2520a%250Auniform%2520distribution%2520in%2520the%2520input%2520batch.%2520Our%2520algorithm%2520computes%2520the%2520current%250Abatch%2520defect%2520distribution%2520and%2520re-balances%2520it%2520by%2520transferring%2520defects%2520using%2520a%250Acombination%2520of%2520Poisson-based%2520seamless%2520image%2520cloning%2520and%2520cut-paste%2520techniques.%2520A%250Athorough%2520experimental%2520section%2520on%2520the%2520Magnetic%2520Tiles%2520dataset%2520shows%2520better%250Aresults%2520of%2520DLI%2520compared%2520to%2520other%2520balancing%2520loss%2520approaches%2520also%2520in%2520the%250Achallenging%2520weakly-supervised%2520setup.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/covisionlab/dynamic-label-injection.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Label%20Injection%20for%20Imbalanced%20Industrial%20Defect%20Segmentation&entry.906535625=Emanuele%20Caruso%20and%20Francesco%20Pelosin%20and%20Alessandro%20Simoni%20and%20Marco%20Boschetti&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%20method%20to%20tackle%20the%20problem%0Aof%20imbalanced%20multi-class%20semantic%20segmentation%20in%20deep%20learning%20systems.%20One%0Aof%20the%20key%20properties%20for%20a%20good%20training%20set%20is%20the%20balancing%20among%20the%0Aclasses.%20When%20the%20input%20distribution%20is%20heavily%20imbalanced%20in%20the%20number%20of%0Ainstances%2C%20the%20learning%20process%20could%20be%20hindered%20or%20difficult%20to%20carry%20on.%20To%0Athis%20end%2C%20we%20propose%20a%20Dynamic%20Label%20Injection%20%28DLI%29%20algorithm%20to%20impose%20a%0Auniform%20distribution%20in%20the%20input%20batch.%20Our%20algorithm%20computes%20the%20current%0Abatch%20defect%20distribution%20and%20re-balances%20it%20by%20transferring%20defects%20using%20a%0Acombination%20of%20Poisson-based%20seamless%20image%20cloning%20and%20cut-paste%20techniques.%20A%0Athorough%20experimental%20section%20on%20the%20Magnetic%20Tiles%20dataset%20shows%20better%0Aresults%20of%20DLI%20compared%20to%20other%20balancing%20loss%20approaches%20also%20in%20the%0Achallenging%20weakly-supervised%20setup.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/covisionlab/dynamic-label-injection.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10031v1&entry.124074799=Read"},
{"title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples\n  in Constrained Access Environments", "author": "Heeyoung Lee and Hoyoon Byun and Changdae Oh and JinYeong Bak and Kyungwoo Song", "abstract": "  Accessing machine learning models through remote APIs has been gaining\nprevalence following the recent trend of scaling up model parameters for\nincreased performance. Even though these models exhibit remarkable ability,\ndetecting out-of-distribution (OOD) samples remains a crucial safety concern\nfor end users as these samples may induce unreliable outputs from the model. In\nthis work, we propose an OOD detection framework, MixDiff, that is applicable\neven when the model's parameters or its activations are not accessible to the\nend user. To bypass the access restriction, MixDiff applies an identical\ninput-level perturbation to a given target sample and a similar in-distribution\n(ID) sample, then compares the relative difference in the model outputs of\nthese two samples. MixDiff is model-agnostic and compatible with existing\noutput-based OOD detection methods. We provide theoretical analysis to\nillustrate MixDiff's effectiveness in discerning OOD samples that induce\noverconfident outputs from the model and empirically demonstrate that MixDiff\nconsistently enhances the OOD detection performance on various datasets in\nvision and text domains.\n", "link": "http://arxiv.org/abs/2408.10107v1", "date": "2024-08-19", "relevancy": 1.0144, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5042}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perturb-and-Compare%20Approach%20for%20Detecting%20Out-of-Distribution%20Samples%0A%20%20in%20Constrained%20Access%20Environments&body=Title%3A%20Perturb-and-Compare%20Approach%20for%20Detecting%20Out-of-Distribution%20Samples%0A%20%20in%20Constrained%20Access%20Environments%0AAuthor%3A%20Heeyoung%20Lee%20and%20Hoyoon%20Byun%20and%20Changdae%20Oh%20and%20JinYeong%20Bak%20and%20Kyungwoo%20Song%0AAbstract%3A%20%20%20Accessing%20machine%20learning%20models%20through%20remote%20APIs%20has%20been%20gaining%0Aprevalence%20following%20the%20recent%20trend%20of%20scaling%20up%20model%20parameters%20for%0Aincreased%20performance.%20Even%20though%20these%20models%20exhibit%20remarkable%20ability%2C%0Adetecting%20out-of-distribution%20%28OOD%29%20samples%20remains%20a%20crucial%20safety%20concern%0Afor%20end%20users%20as%20these%20samples%20may%20induce%20unreliable%20outputs%20from%20the%20model.%20In%0Athis%20work%2C%20we%20propose%20an%20OOD%20detection%20framework%2C%20MixDiff%2C%20that%20is%20applicable%0Aeven%20when%20the%20model%27s%20parameters%20or%20its%20activations%20are%20not%20accessible%20to%20the%0Aend%20user.%20To%20bypass%20the%20access%20restriction%2C%20MixDiff%20applies%20an%20identical%0Ainput-level%20perturbation%20to%20a%20given%20target%20sample%20and%20a%20similar%20in-distribution%0A%28ID%29%20sample%2C%20then%20compares%20the%20relative%20difference%20in%20the%20model%20outputs%20of%0Athese%20two%20samples.%20MixDiff%20is%20model-agnostic%20and%20compatible%20with%20existing%0Aoutput-based%20OOD%20detection%20methods.%20We%20provide%20theoretical%20analysis%20to%0Aillustrate%20MixDiff%27s%20effectiveness%20in%20discerning%20OOD%20samples%20that%20induce%0Aoverconfident%20outputs%20from%20the%20model%20and%20empirically%20demonstrate%20that%20MixDiff%0Aconsistently%20enhances%20the%20OOD%20detection%20performance%20on%20various%20datasets%20in%0Avision%20and%20text%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerturb-and-Compare%2520Approach%2520for%2520Detecting%2520Out-of-Distribution%2520Samples%250A%2520%2520in%2520Constrained%2520Access%2520Environments%26entry.906535625%3DHeeyoung%2520Lee%2520and%2520Hoyoon%2520Byun%2520and%2520Changdae%2520Oh%2520and%2520JinYeong%2520Bak%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3D%2520%2520Accessing%2520machine%2520learning%2520models%2520through%2520remote%2520APIs%2520has%2520been%2520gaining%250Aprevalence%2520following%2520the%2520recent%2520trend%2520of%2520scaling%2520up%2520model%2520parameters%2520for%250Aincreased%2520performance.%2520Even%2520though%2520these%2520models%2520exhibit%2520remarkable%2520ability%252C%250Adetecting%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520remains%2520a%2520crucial%2520safety%2520concern%250Afor%2520end%2520users%2520as%2520these%2520samples%2520may%2520induce%2520unreliable%2520outputs%2520from%2520the%2520model.%2520In%250Athis%2520work%252C%2520we%2520propose%2520an%2520OOD%2520detection%2520framework%252C%2520MixDiff%252C%2520that%2520is%2520applicable%250Aeven%2520when%2520the%2520model%2527s%2520parameters%2520or%2520its%2520activations%2520are%2520not%2520accessible%2520to%2520the%250Aend%2520user.%2520To%2520bypass%2520the%2520access%2520restriction%252C%2520MixDiff%2520applies%2520an%2520identical%250Ainput-level%2520perturbation%2520to%2520a%2520given%2520target%2520sample%2520and%2520a%2520similar%2520in-distribution%250A%2528ID%2529%2520sample%252C%2520then%2520compares%2520the%2520relative%2520difference%2520in%2520the%2520model%2520outputs%2520of%250Athese%2520two%2520samples.%2520MixDiff%2520is%2520model-agnostic%2520and%2520compatible%2520with%2520existing%250Aoutput-based%2520OOD%2520detection%2520methods.%2520We%2520provide%2520theoretical%2520analysis%2520to%250Aillustrate%2520MixDiff%2527s%2520effectiveness%2520in%2520discerning%2520OOD%2520samples%2520that%2520induce%250Aoverconfident%2520outputs%2520from%2520the%2520model%2520and%2520empirically%2520demonstrate%2520that%2520MixDiff%250Aconsistently%2520enhances%2520the%2520OOD%2520detection%2520performance%2520on%2520various%2520datasets%2520in%250Avision%2520and%2520text%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perturb-and-Compare%20Approach%20for%20Detecting%20Out-of-Distribution%20Samples%0A%20%20in%20Constrained%20Access%20Environments&entry.906535625=Heeyoung%20Lee%20and%20Hoyoon%20Byun%20and%20Changdae%20Oh%20and%20JinYeong%20Bak%20and%20Kyungwoo%20Song&entry.1292438233=%20%20Accessing%20machine%20learning%20models%20through%20remote%20APIs%20has%20been%20gaining%0Aprevalence%20following%20the%20recent%20trend%20of%20scaling%20up%20model%20parameters%20for%0Aincreased%20performance.%20Even%20though%20these%20models%20exhibit%20remarkable%20ability%2C%0Adetecting%20out-of-distribution%20%28OOD%29%20samples%20remains%20a%20crucial%20safety%20concern%0Afor%20end%20users%20as%20these%20samples%20may%20induce%20unreliable%20outputs%20from%20the%20model.%20In%0Athis%20work%2C%20we%20propose%20an%20OOD%20detection%20framework%2C%20MixDiff%2C%20that%20is%20applicable%0Aeven%20when%20the%20model%27s%20parameters%20or%20its%20activations%20are%20not%20accessible%20to%20the%0Aend%20user.%20To%20bypass%20the%20access%20restriction%2C%20MixDiff%20applies%20an%20identical%0Ainput-level%20perturbation%20to%20a%20given%20target%20sample%20and%20a%20similar%20in-distribution%0A%28ID%29%20sample%2C%20then%20compares%20the%20relative%20difference%20in%20the%20model%20outputs%20of%0Athese%20two%20samples.%20MixDiff%20is%20model-agnostic%20and%20compatible%20with%20existing%0Aoutput-based%20OOD%20detection%20methods.%20We%20provide%20theoretical%20analysis%20to%0Aillustrate%20MixDiff%27s%20effectiveness%20in%20discerning%20OOD%20samples%20that%20induce%0Aoverconfident%20outputs%20from%20the%20model%20and%20empirically%20demonstrate%20that%20MixDiff%0Aconsistently%20enhances%20the%20OOD%20detection%20performance%20on%20various%20datasets%20in%0Avision%20and%20text%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10107v1&entry.124074799=Read"},
{"title": "The curse of random quantum data", "author": "Kaining Zhang and Junyu Liu and Liu Liu and Liang Jiang and Min-Hsiu Hsieh and Dacheng Tao", "abstract": "  Quantum machine learning, which involves running machine learning algorithms\non quantum devices, may be one of the most significant flagship applications\nfor these devices. Unlike its classical counterparts, the role of data in\nquantum machine learning has not been fully understood. In this work, we\nquantify the performances of quantum machine learning in the landscape of\nquantum data. Provided that the encoding of quantum data is sufficiently\nrandom, the performance, we find that the training efficiency and\ngeneralization capabilities in quantum machine learning will be exponentially\nsuppressed with the increase in the number of qubits, which we call \"the curse\nof random quantum data\". Our findings apply to both the quantum kernel method\nand the large-width limit of quantum neural networks. Conversely, we highlight\nthat through meticulous design of quantum datasets, it is possible to avoid\nthese curses, thereby achieving efficient convergence and robust\ngeneralization. Our conclusions are corroborated by extensive numerical\nsimulations.\n", "link": "http://arxiv.org/abs/2408.09937v1", "date": "2024-08-19", "relevancy": 1.2602, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4192}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20curse%20of%20random%20quantum%20data&body=Title%3A%20The%20curse%20of%20random%20quantum%20data%0AAuthor%3A%20Kaining%20Zhang%20and%20Junyu%20Liu%20and%20Liu%20Liu%20and%20Liang%20Jiang%20and%20Min-Hsiu%20Hsieh%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Quantum%20machine%20learning%2C%20which%20involves%20running%20machine%20learning%20algorithms%0Aon%20quantum%20devices%2C%20may%20be%20one%20of%20the%20most%20significant%20flagship%20applications%0Afor%20these%20devices.%20Unlike%20its%20classical%20counterparts%2C%20the%20role%20of%20data%20in%0Aquantum%20machine%20learning%20has%20not%20been%20fully%20understood.%20In%20this%20work%2C%20we%0Aquantify%20the%20performances%20of%20quantum%20machine%20learning%20in%20the%20landscape%20of%0Aquantum%20data.%20Provided%20that%20the%20encoding%20of%20quantum%20data%20is%20sufficiently%0Arandom%2C%20the%20performance%2C%20we%20find%20that%20the%20training%20efficiency%20and%0Ageneralization%20capabilities%20in%20quantum%20machine%20learning%20will%20be%20exponentially%0Asuppressed%20with%20the%20increase%20in%20the%20number%20of%20qubits%2C%20which%20we%20call%20%22the%20curse%0Aof%20random%20quantum%20data%22.%20Our%20findings%20apply%20to%20both%20the%20quantum%20kernel%20method%0Aand%20the%20large-width%20limit%20of%20quantum%20neural%20networks.%20Conversely%2C%20we%20highlight%0Athat%20through%20meticulous%20design%20of%20quantum%20datasets%2C%20it%20is%20possible%20to%20avoid%0Athese%20curses%2C%20thereby%20achieving%20efficient%20convergence%20and%20robust%0Ageneralization.%20Our%20conclusions%20are%20corroborated%20by%20extensive%20numerical%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520curse%2520of%2520random%2520quantum%2520data%26entry.906535625%3DKaining%2520Zhang%2520and%2520Junyu%2520Liu%2520and%2520Liu%2520Liu%2520and%2520Liang%2520Jiang%2520and%2520Min-Hsiu%2520Hsieh%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Quantum%2520machine%2520learning%252C%2520which%2520involves%2520running%2520machine%2520learning%2520algorithms%250Aon%2520quantum%2520devices%252C%2520may%2520be%2520one%2520of%2520the%2520most%2520significant%2520flagship%2520applications%250Afor%2520these%2520devices.%2520Unlike%2520its%2520classical%2520counterparts%252C%2520the%2520role%2520of%2520data%2520in%250Aquantum%2520machine%2520learning%2520has%2520not%2520been%2520fully%2520understood.%2520In%2520this%2520work%252C%2520we%250Aquantify%2520the%2520performances%2520of%2520quantum%2520machine%2520learning%2520in%2520the%2520landscape%2520of%250Aquantum%2520data.%2520Provided%2520that%2520the%2520encoding%2520of%2520quantum%2520data%2520is%2520sufficiently%250Arandom%252C%2520the%2520performance%252C%2520we%2520find%2520that%2520the%2520training%2520efficiency%2520and%250Ageneralization%2520capabilities%2520in%2520quantum%2520machine%2520learning%2520will%2520be%2520exponentially%250Asuppressed%2520with%2520the%2520increase%2520in%2520the%2520number%2520of%2520qubits%252C%2520which%2520we%2520call%2520%2522the%2520curse%250Aof%2520random%2520quantum%2520data%2522.%2520Our%2520findings%2520apply%2520to%2520both%2520the%2520quantum%2520kernel%2520method%250Aand%2520the%2520large-width%2520limit%2520of%2520quantum%2520neural%2520networks.%2520Conversely%252C%2520we%2520highlight%250Athat%2520through%2520meticulous%2520design%2520of%2520quantum%2520datasets%252C%2520it%2520is%2520possible%2520to%2520avoid%250Athese%2520curses%252C%2520thereby%2520achieving%2520efficient%2520convergence%2520and%2520robust%250Ageneralization.%2520Our%2520conclusions%2520are%2520corroborated%2520by%2520extensive%2520numerical%250Asimulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20curse%20of%20random%20quantum%20data&entry.906535625=Kaining%20Zhang%20and%20Junyu%20Liu%20and%20Liu%20Liu%20and%20Liang%20Jiang%20and%20Min-Hsiu%20Hsieh%20and%20Dacheng%20Tao&entry.1292438233=%20%20Quantum%20machine%20learning%2C%20which%20involves%20running%20machine%20learning%20algorithms%0Aon%20quantum%20devices%2C%20may%20be%20one%20of%20the%20most%20significant%20flagship%20applications%0Afor%20these%20devices.%20Unlike%20its%20classical%20counterparts%2C%20the%20role%20of%20data%20in%0Aquantum%20machine%20learning%20has%20not%20been%20fully%20understood.%20In%20this%20work%2C%20we%0Aquantify%20the%20performances%20of%20quantum%20machine%20learning%20in%20the%20landscape%20of%0Aquantum%20data.%20Provided%20that%20the%20encoding%20of%20quantum%20data%20is%20sufficiently%0Arandom%2C%20the%20performance%2C%20we%20find%20that%20the%20training%20efficiency%20and%0Ageneralization%20capabilities%20in%20quantum%20machine%20learning%20will%20be%20exponentially%0Asuppressed%20with%20the%20increase%20in%20the%20number%20of%20qubits%2C%20which%20we%20call%20%22the%20curse%0Aof%20random%20quantum%20data%22.%20Our%20findings%20apply%20to%20both%20the%20quantum%20kernel%20method%0Aand%20the%20large-width%20limit%20of%20quantum%20neural%20networks.%20Conversely%2C%20we%20highlight%0Athat%20through%20meticulous%20design%20of%20quantum%20datasets%2C%20it%20is%20possible%20to%20avoid%0Athese%20curses%2C%20thereby%20achieving%20efficient%20convergence%20and%20robust%0Ageneralization.%20Our%20conclusions%20are%20corroborated%20by%20extensive%20numerical%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09937v1&entry.124074799=Read"},
{"title": "Multi-Scale Representation Learning for Image Restoration with\n  State-Space Model", "author": "Yuhong He and Long Peng and Qiaosi Yi and Chen Wu and Lu Wang", "abstract": "  Image restoration endeavors to reconstruct a high-quality, detail-rich image\nfrom a degraded counterpart, which is a pivotal process in photography and\nvarious computer vision systems. In real-world scenarios, different types of\ndegradation can cause the loss of image details at various scales and degrade\nimage contrast. Existing methods predominantly rely on CNN and Transformer to\ncapture multi-scale representations. However, these methods are often limited\nby the high computational complexity of Transformers and the constrained\nreceptive field of CNN, which hinder them from achieving superior performance\nand efficiency in image restoration. To address these challenges, we propose a\nnovel Multi-Scale State-Space Model-based (MS-Mamba) for efficient image\nrestoration that enhances the capacity for multi-scale representation learning\nthrough our proposed global and regional SSM modules. Additionally, an Adaptive\nGradient Block (AGB) and a Residual Fourier Block (RFB) are proposed to improve\nthe network's detail extraction capabilities by capturing gradients in various\ndirections and facilitating learning details in the frequency domain. Extensive\nexperiments on nine public benchmarks across four classic image restoration\ntasks, image deraining, dehazing, denoising, and low-light enhancement,\ndemonstrate that our proposed method achieves new state-of-the-art performance\nwhile maintaining low computational complexity. The source code will be\npublicly available.\n", "link": "http://arxiv.org/abs/2408.10145v1", "date": "2024-08-19", "relevancy": 1.6592, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5708}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5484}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Representation%20Learning%20for%20Image%20Restoration%20with%0A%20%20State-Space%20Model&body=Title%3A%20Multi-Scale%20Representation%20Learning%20for%20Image%20Restoration%20with%0A%20%20State-Space%20Model%0AAuthor%3A%20Yuhong%20He%20and%20Long%20Peng%20and%20Qiaosi%20Yi%20and%20Chen%20Wu%20and%20Lu%20Wang%0AAbstract%3A%20%20%20Image%20restoration%20endeavors%20to%20reconstruct%20a%20high-quality%2C%20detail-rich%20image%0Afrom%20a%20degraded%20counterpart%2C%20which%20is%20a%20pivotal%20process%20in%20photography%20and%0Avarious%20computer%20vision%20systems.%20In%20real-world%20scenarios%2C%20different%20types%20of%0Adegradation%20can%20cause%20the%20loss%20of%20image%20details%20at%20various%20scales%20and%20degrade%0Aimage%20contrast.%20Existing%20methods%20predominantly%20rely%20on%20CNN%20and%20Transformer%20to%0Acapture%20multi-scale%20representations.%20However%2C%20these%20methods%20are%20often%20limited%0Aby%20the%20high%20computational%20complexity%20of%20Transformers%20and%20the%20constrained%0Areceptive%20field%20of%20CNN%2C%20which%20hinder%20them%20from%20achieving%20superior%20performance%0Aand%20efficiency%20in%20image%20restoration.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20Multi-Scale%20State-Space%20Model-based%20%28MS-Mamba%29%20for%20efficient%20image%0Arestoration%20that%20enhances%20the%20capacity%20for%20multi-scale%20representation%20learning%0Athrough%20our%20proposed%20global%20and%20regional%20SSM%20modules.%20Additionally%2C%20an%20Adaptive%0AGradient%20Block%20%28AGB%29%20and%20a%20Residual%20Fourier%20Block%20%28RFB%29%20are%20proposed%20to%20improve%0Athe%20network%27s%20detail%20extraction%20capabilities%20by%20capturing%20gradients%20in%20various%0Adirections%20and%20facilitating%20learning%20details%20in%20the%20frequency%20domain.%20Extensive%0Aexperiments%20on%20nine%20public%20benchmarks%20across%20four%20classic%20image%20restoration%0Atasks%2C%20image%20deraining%2C%20dehazing%2C%20denoising%2C%20and%20low-light%20enhancement%2C%0Ademonstrate%20that%20our%20proposed%20method%20achieves%20new%20state-of-the-art%20performance%0Awhile%20maintaining%20low%20computational%20complexity.%20The%20source%20code%20will%20be%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Representation%2520Learning%2520for%2520Image%2520Restoration%2520with%250A%2520%2520State-Space%2520Model%26entry.906535625%3DYuhong%2520He%2520and%2520Long%2520Peng%2520and%2520Qiaosi%2520Yi%2520and%2520Chen%2520Wu%2520and%2520Lu%2520Wang%26entry.1292438233%3D%2520%2520Image%2520restoration%2520endeavors%2520to%2520reconstruct%2520a%2520high-quality%252C%2520detail-rich%2520image%250Afrom%2520a%2520degraded%2520counterpart%252C%2520which%2520is%2520a%2520pivotal%2520process%2520in%2520photography%2520and%250Avarious%2520computer%2520vision%2520systems.%2520In%2520real-world%2520scenarios%252C%2520different%2520types%2520of%250Adegradation%2520can%2520cause%2520the%2520loss%2520of%2520image%2520details%2520at%2520various%2520scales%2520and%2520degrade%250Aimage%2520contrast.%2520Existing%2520methods%2520predominantly%2520rely%2520on%2520CNN%2520and%2520Transformer%2520to%250Acapture%2520multi-scale%2520representations.%2520However%252C%2520these%2520methods%2520are%2520often%2520limited%250Aby%2520the%2520high%2520computational%2520complexity%2520of%2520Transformers%2520and%2520the%2520constrained%250Areceptive%2520field%2520of%2520CNN%252C%2520which%2520hinder%2520them%2520from%2520achieving%2520superior%2520performance%250Aand%2520efficiency%2520in%2520image%2520restoration.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520Multi-Scale%2520State-Space%2520Model-based%2520%2528MS-Mamba%2529%2520for%2520efficient%2520image%250Arestoration%2520that%2520enhances%2520the%2520capacity%2520for%2520multi-scale%2520representation%2520learning%250Athrough%2520our%2520proposed%2520global%2520and%2520regional%2520SSM%2520modules.%2520Additionally%252C%2520an%2520Adaptive%250AGradient%2520Block%2520%2528AGB%2529%2520and%2520a%2520Residual%2520Fourier%2520Block%2520%2528RFB%2529%2520are%2520proposed%2520to%2520improve%250Athe%2520network%2527s%2520detail%2520extraction%2520capabilities%2520by%2520capturing%2520gradients%2520in%2520various%250Adirections%2520and%2520facilitating%2520learning%2520details%2520in%2520the%2520frequency%2520domain.%2520Extensive%250Aexperiments%2520on%2520nine%2520public%2520benchmarks%2520across%2520four%2520classic%2520image%2520restoration%250Atasks%252C%2520image%2520deraining%252C%2520dehazing%252C%2520denoising%252C%2520and%2520low-light%2520enhancement%252C%250Ademonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520new%2520state-of-the-art%2520performance%250Awhile%2520maintaining%2520low%2520computational%2520complexity.%2520The%2520source%2520code%2520will%2520be%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Representation%20Learning%20for%20Image%20Restoration%20with%0A%20%20State-Space%20Model&entry.906535625=Yuhong%20He%20and%20Long%20Peng%20and%20Qiaosi%20Yi%20and%20Chen%20Wu%20and%20Lu%20Wang&entry.1292438233=%20%20Image%20restoration%20endeavors%20to%20reconstruct%20a%20high-quality%2C%20detail-rich%20image%0Afrom%20a%20degraded%20counterpart%2C%20which%20is%20a%20pivotal%20process%20in%20photography%20and%0Avarious%20computer%20vision%20systems.%20In%20real-world%20scenarios%2C%20different%20types%20of%0Adegradation%20can%20cause%20the%20loss%20of%20image%20details%20at%20various%20scales%20and%20degrade%0Aimage%20contrast.%20Existing%20methods%20predominantly%20rely%20on%20CNN%20and%20Transformer%20to%0Acapture%20multi-scale%20representations.%20However%2C%20these%20methods%20are%20often%20limited%0Aby%20the%20high%20computational%20complexity%20of%20Transformers%20and%20the%20constrained%0Areceptive%20field%20of%20CNN%2C%20which%20hinder%20them%20from%20achieving%20superior%20performance%0Aand%20efficiency%20in%20image%20restoration.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20Multi-Scale%20State-Space%20Model-based%20%28MS-Mamba%29%20for%20efficient%20image%0Arestoration%20that%20enhances%20the%20capacity%20for%20multi-scale%20representation%20learning%0Athrough%20our%20proposed%20global%20and%20regional%20SSM%20modules.%20Additionally%2C%20an%20Adaptive%0AGradient%20Block%20%28AGB%29%20and%20a%20Residual%20Fourier%20Block%20%28RFB%29%20are%20proposed%20to%20improve%0Athe%20network%27s%20detail%20extraction%20capabilities%20by%20capturing%20gradients%20in%20various%0Adirections%20and%20facilitating%20learning%20details%20in%20the%20frequency%20domain.%20Extensive%0Aexperiments%20on%20nine%20public%20benchmarks%20across%20four%20classic%20image%20restoration%0Atasks%2C%20image%20deraining%2C%20dehazing%2C%20denoising%2C%20and%20low-light%20enhancement%2C%0Ademonstrate%20that%20our%20proposed%20method%20achieves%20new%20state-of-the-art%20performance%0Awhile%20maintaining%20low%20computational%20complexity.%20The%20source%20code%20will%20be%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10145v1&entry.124074799=Read"},
{"title": "Interpreting Learned Feedback Patterns in Large Language Models", "author": "Luke Marks and Amir Abdullah and Clement Neo and Rauno Arike and David Krueger and Philip Torr and Fazl Barez", "abstract": "  Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs.\n", "link": "http://arxiv.org/abs/2310.08164v5", "date": "2024-08-19", "relevancy": 1.8462, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4675}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models&body=Title%3A%20Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models%0AAuthor%3A%20Luke%20Marks%20and%20Amir%20Abdullah%20and%20Clement%20Neo%20and%20Rauno%20Arike%20and%20David%20Krueger%20and%20Philip%20Torr%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20widely%20used%20to%20train%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20unclear%20whether%20LLMs%20accurately%0Alearn%20the%20underlying%20preferences%20in%20human%20feedback%20data.%20We%20coin%20the%20term%0A%5Ctextit%7BLearned%20Feedback%20Pattern%7D%20%28LFP%29%20for%20patterns%20in%20an%20LLM%27s%20activations%0Alearned%20during%20RLHF%20that%20improve%20its%20performance%20on%20the%20fine-tuning%20task.%20We%0Ahypothesize%20that%20LLMs%20with%20LFPs%20accurately%20aligned%20to%20the%20fine-tuning%20feedback%0Aexhibit%20consistent%20activation%20patterns%20for%20outputs%20that%20would%20have%20received%0Asimilar%20feedback%20during%20RLHF.%20To%20test%20this%2C%20we%20train%20probes%20to%20estimate%20the%0Afeedback%20signal%20implicit%20in%20the%20activations%20of%20a%20fine-tuned%20LLM.%20We%20then%0Acompare%20these%20estimates%20to%20the%20true%20feedback%2C%20measuring%20how%20accurate%20the%20LFPs%0Aare%20to%20the%20fine-tuning%20feedback.%20Our%20probes%20are%20trained%20on%20a%20condensed%2C%20sparse%0Aand%20interpretable%20representation%20of%20LLM%20activations%2C%20making%20it%20easier%20to%0Acorrelate%20features%20of%20the%20input%20with%20our%20probe%27s%20predictions.%20We%20validate%20our%0Aprobes%20by%20comparing%20the%20neural%20features%20they%20correlate%20with%20positive%20feedback%0Ainputs%20against%20the%20features%20GPT-4%20describes%20and%20classifies%20as%20related%20to%20LFPs.%0AUnderstanding%20LFPs%20can%20help%20minimize%20discrepancies%20between%20LLM%20behavior%20and%0Atraining%20objectives%2C%20which%20is%20essential%20for%20the%20safety%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08164v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Learned%2520Feedback%2520Patterns%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DLuke%2520Marks%2520and%2520Amir%2520Abdullah%2520and%2520Clement%2520Neo%2520and%2520Rauno%2520Arike%2520and%2520David%2520Krueger%2520and%2520Philip%2520Torr%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520is%2520widely%2520used%2520to%2520train%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520LLMs%2520accurately%250Alearn%2520the%2520underlying%2520preferences%2520in%2520human%2520feedback%2520data.%2520We%2520coin%2520the%2520term%250A%255Ctextit%257BLearned%2520Feedback%2520Pattern%257D%2520%2528LFP%2529%2520for%2520patterns%2520in%2520an%2520LLM%2527s%2520activations%250Alearned%2520during%2520RLHF%2520that%2520improve%2520its%2520performance%2520on%2520the%2520fine-tuning%2520task.%2520We%250Ahypothesize%2520that%2520LLMs%2520with%2520LFPs%2520accurately%2520aligned%2520to%2520the%2520fine-tuning%2520feedback%250Aexhibit%2520consistent%2520activation%2520patterns%2520for%2520outputs%2520that%2520would%2520have%2520received%250Asimilar%2520feedback%2520during%2520RLHF.%2520To%2520test%2520this%252C%2520we%2520train%2520probes%2520to%2520estimate%2520the%250Afeedback%2520signal%2520implicit%2520in%2520the%2520activations%2520of%2520a%2520fine-tuned%2520LLM.%2520We%2520then%250Acompare%2520these%2520estimates%2520to%2520the%2520true%2520feedback%252C%2520measuring%2520how%2520accurate%2520the%2520LFPs%250Aare%2520to%2520the%2520fine-tuning%2520feedback.%2520Our%2520probes%2520are%2520trained%2520on%2520a%2520condensed%252C%2520sparse%250Aand%2520interpretable%2520representation%2520of%2520LLM%2520activations%252C%2520making%2520it%2520easier%2520to%250Acorrelate%2520features%2520of%2520the%2520input%2520with%2520our%2520probe%2527s%2520predictions.%2520We%2520validate%2520our%250Aprobes%2520by%2520comparing%2520the%2520neural%2520features%2520they%2520correlate%2520with%2520positive%2520feedback%250Ainputs%2520against%2520the%2520features%2520GPT-4%2520describes%2520and%2520classifies%2520as%2520related%2520to%2520LFPs.%250AUnderstanding%2520LFPs%2520can%2520help%2520minimize%2520discrepancies%2520between%2520LLM%2520behavior%2520and%250Atraining%2520objectives%252C%2520which%2520is%2520essential%2520for%2520the%2520safety%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08164v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models&entry.906535625=Luke%20Marks%20and%20Amir%20Abdullah%20and%20Clement%20Neo%20and%20Rauno%20Arike%20and%20David%20Krueger%20and%20Philip%20Torr%20and%20Fazl%20Barez&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20widely%20used%20to%20train%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20unclear%20whether%20LLMs%20accurately%0Alearn%20the%20underlying%20preferences%20in%20human%20feedback%20data.%20We%20coin%20the%20term%0A%5Ctextit%7BLearned%20Feedback%20Pattern%7D%20%28LFP%29%20for%20patterns%20in%20an%20LLM%27s%20activations%0Alearned%20during%20RLHF%20that%20improve%20its%20performance%20on%20the%20fine-tuning%20task.%20We%0Ahypothesize%20that%20LLMs%20with%20LFPs%20accurately%20aligned%20to%20the%20fine-tuning%20feedback%0Aexhibit%20consistent%20activation%20patterns%20for%20outputs%20that%20would%20have%20received%0Asimilar%20feedback%20during%20RLHF.%20To%20test%20this%2C%20we%20train%20probes%20to%20estimate%20the%0Afeedback%20signal%20implicit%20in%20the%20activations%20of%20a%20fine-tuned%20LLM.%20We%20then%0Acompare%20these%20estimates%20to%20the%20true%20feedback%2C%20measuring%20how%20accurate%20the%20LFPs%0Aare%20to%20the%20fine-tuning%20feedback.%20Our%20probes%20are%20trained%20on%20a%20condensed%2C%20sparse%0Aand%20interpretable%20representation%20of%20LLM%20activations%2C%20making%20it%20easier%20to%0Acorrelate%20features%20of%20the%20input%20with%20our%20probe%27s%20predictions.%20We%20validate%20our%0Aprobes%20by%20comparing%20the%20neural%20features%20they%20correlate%20with%20positive%20feedback%0Ainputs%20against%20the%20features%20GPT-4%20describes%20and%20classifies%20as%20related%20to%20LFPs.%0AUnderstanding%20LFPs%20can%20help%20minimize%20discrepancies%20between%20LLM%20behavior%20and%0Atraining%20objectives%2C%20which%20is%20essential%20for%20the%20safety%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08164v5&entry.124074799=Read"},
{"title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil\n  Financial Time Series Regularities", "author": "Yuanjian Xu and Anxian Liu and Jianing Hao and Zhenzhuo Li and Shichang Meng and Guang Zhang", "abstract": "  Financial time series modeling is crucial for understanding and predicting\nmarket behaviors but faces challenges such as non-linearity, non-stationarity,\nand high noise levels. Traditional models struggle to capture complex patterns\ndue to these issues, compounded by limitations in computational resources and\nmodel capacity. Inspired by the success of large language models in NLP, we\nintroduce \\textbf{PLUTUS}, a \\textbf{P}re-trained \\textbf{L}arge\n\\textbf{U}nified \\textbf{T}ransformer-based model that \\textbf{U}nveils\nregularities in financial time \\textbf{S}eries. PLUTUS uses an invertible\nembedding module with contrastive learning and autoencoder techniques to create\nan approximate one-to-one mapping between raw data and patch embeddings.\nTimeFormer, an attention based architecture, forms the core of PLUTUS,\neffectively modeling high-noise time series. We incorporate a novel attention\nmechanisms to capture features across both variable and temporal dimensions.\nPLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,\ndesigned to thrive in noisy financial environments. To our knowledge, PLUTUS is\nthe first open-source, large-scale, pre-trained financial time series model\nwith over one billion parameters. It achieves state-of-the-art performance in\nvarious tasks, demonstrating strong transferability and establishing a robust\nfoundational model for finance. Our research provides technical guidance for\npre-training financial time series data, setting a new standard in the field.\n", "link": "http://arxiv.org/abs/2408.10111v1", "date": "2024-08-19", "relevancy": 1.8046, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5057}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLUTUS%3A%20A%20Well%20Pre-trained%20Large%20Unified%20Transformer%20can%20Unveil%0A%20%20Financial%20Time%20Series%20Regularities&body=Title%3A%20PLUTUS%3A%20A%20Well%20Pre-trained%20Large%20Unified%20Transformer%20can%20Unveil%0A%20%20Financial%20Time%20Series%20Regularities%0AAuthor%3A%20Yuanjian%20Xu%20and%20Anxian%20Liu%20and%20Jianing%20Hao%20and%20Zhenzhuo%20Li%20and%20Shichang%20Meng%20and%20Guang%20Zhang%0AAbstract%3A%20%20%20Financial%20time%20series%20modeling%20is%20crucial%20for%20understanding%20and%20predicting%0Amarket%20behaviors%20but%20faces%20challenges%20such%20as%20non-linearity%2C%20non-stationarity%2C%0Aand%20high%20noise%20levels.%20Traditional%20models%20struggle%20to%20capture%20complex%20patterns%0Adue%20to%20these%20issues%2C%20compounded%20by%20limitations%20in%20computational%20resources%20and%0Amodel%20capacity.%20Inspired%20by%20the%20success%20of%20large%20language%20models%20in%20NLP%2C%20we%0Aintroduce%20%5Ctextbf%7BPLUTUS%7D%2C%20a%20%5Ctextbf%7BP%7Dre-trained%20%5Ctextbf%7BL%7Darge%0A%5Ctextbf%7BU%7Dnified%20%5Ctextbf%7BT%7Dransformer-based%20model%20that%20%5Ctextbf%7BU%7Dnveils%0Aregularities%20in%20financial%20time%20%5Ctextbf%7BS%7Deries.%20PLUTUS%20uses%20an%20invertible%0Aembedding%20module%20with%20contrastive%20learning%20and%20autoencoder%20techniques%20to%20create%0Aan%20approximate%20one-to-one%20mapping%20between%20raw%20data%20and%20patch%20embeddings.%0ATimeFormer%2C%20an%20attention%20based%20architecture%2C%20forms%20the%20core%20of%20PLUTUS%2C%0Aeffectively%20modeling%20high-noise%20time%20series.%20We%20incorporate%20a%20novel%20attention%0Amechanisms%20to%20capture%20features%20across%20both%20variable%20and%20temporal%20dimensions.%0APLUTUS%20is%20pre-trained%20on%20an%20unprecedented%20dataset%20of%20100%20billion%20observations%2C%0Adesigned%20to%20thrive%20in%20noisy%20financial%20environments.%20To%20our%20knowledge%2C%20PLUTUS%20is%0Athe%20first%20open-source%2C%20large-scale%2C%20pre-trained%20financial%20time%20series%20model%0Awith%20over%20one%20billion%20parameters.%20It%20achieves%20state-of-the-art%20performance%20in%0Avarious%20tasks%2C%20demonstrating%20strong%20transferability%20and%20establishing%20a%20robust%0Afoundational%20model%20for%20finance.%20Our%20research%20provides%20technical%20guidance%20for%0Apre-training%20financial%20time%20series%20data%2C%20setting%20a%20new%20standard%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLUTUS%253A%2520A%2520Well%2520Pre-trained%2520Large%2520Unified%2520Transformer%2520can%2520Unveil%250A%2520%2520Financial%2520Time%2520Series%2520Regularities%26entry.906535625%3DYuanjian%2520Xu%2520and%2520Anxian%2520Liu%2520and%2520Jianing%2520Hao%2520and%2520Zhenzhuo%2520Li%2520and%2520Shichang%2520Meng%2520and%2520Guang%2520Zhang%26entry.1292438233%3D%2520%2520Financial%2520time%2520series%2520modeling%2520is%2520crucial%2520for%2520understanding%2520and%2520predicting%250Amarket%2520behaviors%2520but%2520faces%2520challenges%2520such%2520as%2520non-linearity%252C%2520non-stationarity%252C%250Aand%2520high%2520noise%2520levels.%2520Traditional%2520models%2520struggle%2520to%2520capture%2520complex%2520patterns%250Adue%2520to%2520these%2520issues%252C%2520compounded%2520by%2520limitations%2520in%2520computational%2520resources%2520and%250Amodel%2520capacity.%2520Inspired%2520by%2520the%2520success%2520of%2520large%2520language%2520models%2520in%2520NLP%252C%2520we%250Aintroduce%2520%255Ctextbf%257BPLUTUS%257D%252C%2520a%2520%255Ctextbf%257BP%257Dre-trained%2520%255Ctextbf%257BL%257Darge%250A%255Ctextbf%257BU%257Dnified%2520%255Ctextbf%257BT%257Dransformer-based%2520model%2520that%2520%255Ctextbf%257BU%257Dnveils%250Aregularities%2520in%2520financial%2520time%2520%255Ctextbf%257BS%257Deries.%2520PLUTUS%2520uses%2520an%2520invertible%250Aembedding%2520module%2520with%2520contrastive%2520learning%2520and%2520autoencoder%2520techniques%2520to%2520create%250Aan%2520approximate%2520one-to-one%2520mapping%2520between%2520raw%2520data%2520and%2520patch%2520embeddings.%250ATimeFormer%252C%2520an%2520attention%2520based%2520architecture%252C%2520forms%2520the%2520core%2520of%2520PLUTUS%252C%250Aeffectively%2520modeling%2520high-noise%2520time%2520series.%2520We%2520incorporate%2520a%2520novel%2520attention%250Amechanisms%2520to%2520capture%2520features%2520across%2520both%2520variable%2520and%2520temporal%2520dimensions.%250APLUTUS%2520is%2520pre-trained%2520on%2520an%2520unprecedented%2520dataset%2520of%2520100%2520billion%2520observations%252C%250Adesigned%2520to%2520thrive%2520in%2520noisy%2520financial%2520environments.%2520To%2520our%2520knowledge%252C%2520PLUTUS%2520is%250Athe%2520first%2520open-source%252C%2520large-scale%252C%2520pre-trained%2520financial%2520time%2520series%2520model%250Awith%2520over%2520one%2520billion%2520parameters.%2520It%2520achieves%2520state-of-the-art%2520performance%2520in%250Avarious%2520tasks%252C%2520demonstrating%2520strong%2520transferability%2520and%2520establishing%2520a%2520robust%250Afoundational%2520model%2520for%2520finance.%2520Our%2520research%2520provides%2520technical%2520guidance%2520for%250Apre-training%2520financial%2520time%2520series%2520data%252C%2520setting%2520a%2520new%2520standard%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLUTUS%3A%20A%20Well%20Pre-trained%20Large%20Unified%20Transformer%20can%20Unveil%0A%20%20Financial%20Time%20Series%20Regularities&entry.906535625=Yuanjian%20Xu%20and%20Anxian%20Liu%20and%20Jianing%20Hao%20and%20Zhenzhuo%20Li%20and%20Shichang%20Meng%20and%20Guang%20Zhang&entry.1292438233=%20%20Financial%20time%20series%20modeling%20is%20crucial%20for%20understanding%20and%20predicting%0Amarket%20behaviors%20but%20faces%20challenges%20such%20as%20non-linearity%2C%20non-stationarity%2C%0Aand%20high%20noise%20levels.%20Traditional%20models%20struggle%20to%20capture%20complex%20patterns%0Adue%20to%20these%20issues%2C%20compounded%20by%20limitations%20in%20computational%20resources%20and%0Amodel%20capacity.%20Inspired%20by%20the%20success%20of%20large%20language%20models%20in%20NLP%2C%20we%0Aintroduce%20%5Ctextbf%7BPLUTUS%7D%2C%20a%20%5Ctextbf%7BP%7Dre-trained%20%5Ctextbf%7BL%7Darge%0A%5Ctextbf%7BU%7Dnified%20%5Ctextbf%7BT%7Dransformer-based%20model%20that%20%5Ctextbf%7BU%7Dnveils%0Aregularities%20in%20financial%20time%20%5Ctextbf%7BS%7Deries.%20PLUTUS%20uses%20an%20invertible%0Aembedding%20module%20with%20contrastive%20learning%20and%20autoencoder%20techniques%20to%20create%0Aan%20approximate%20one-to-one%20mapping%20between%20raw%20data%20and%20patch%20embeddings.%0ATimeFormer%2C%20an%20attention%20based%20architecture%2C%20forms%20the%20core%20of%20PLUTUS%2C%0Aeffectively%20modeling%20high-noise%20time%20series.%20We%20incorporate%20a%20novel%20attention%0Amechanisms%20to%20capture%20features%20across%20both%20variable%20and%20temporal%20dimensions.%0APLUTUS%20is%20pre-trained%20on%20an%20unprecedented%20dataset%20of%20100%20billion%20observations%2C%0Adesigned%20to%20thrive%20in%20noisy%20financial%20environments.%20To%20our%20knowledge%2C%20PLUTUS%20is%0Athe%20first%20open-source%2C%20large-scale%2C%20pre-trained%20financial%20time%20series%20model%0Awith%20over%20one%20billion%20parameters.%20It%20achieves%20state-of-the-art%20performance%20in%0Avarious%20tasks%2C%20demonstrating%20strong%20transferability%20and%20establishing%20a%20robust%0Afoundational%20model%20for%20finance.%20Our%20research%20provides%20technical%20guidance%20for%0Apre-training%20financial%20time%20series%20data%2C%20setting%20a%20new%20standard%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10111v1&entry.124074799=Read"},
{"title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen\n  Vision-language Model", "author": "Junhui Yin and Xinyu Zhang and Lin Wu and Xiaojie Wang", "abstract": "  Current pre-trained vision-language models, such as CLIP, have demonstrated\nremarkable zero-shot generalization capabilities across various downstream\ntasks. However, their performance significantly degrades when test inputs\nexhibit different distributions. In this paper, we explore the concept of\ntest-time prompt tuning (TTPT), which facilitates the adaptation of the CLIP\nmodel to novel downstream tasks through a one-step unsupervised optimization\nthat involves only test samples. Inspired by in-context learning in natural\nlanguage processing (NLP), we propose In-Context Prompt Learning (InCPL) for\ntest-time visual recognition tasks, which empowers a pre-trained\nvision-language model with labeled examples as context information on\ndownstream task. Specifically, InCPL associates a new test sample with very few\nlabeled examples (sometimes just one) as context information, enabling reliable\nlabel estimation for the test sample and facilitating model adaptation. To\nachieve this, InCPL employs an efficient language-to-vision translator to\nexplore the textual prior information for visual prompt learning. Further, we\nintroduce a context-aware unsupervised loss to optimize visual prompts tailored\nto test samples. Finally, we design a cyclic learning strategy for visual and\ntextual prompts to ensure mutual synergy across different modalities. This\nenables a pre-trained, frozen CLIP model to adapt to any task using its learned\nadaptive prompt. Our method demonstrates superior performance and achieves\nstate-of-the-art results across various downstream datasets.\n", "link": "http://arxiv.org/abs/2403.06126v2", "date": "2024-08-19", "relevancy": 1.6342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5278}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-context%20Prompt%20Learning%20for%20Test-time%20Vision%20Recognition%20with%20Frozen%0A%20%20Vision-language%20Model&body=Title%3A%20In-context%20Prompt%20Learning%20for%20Test-time%20Vision%20Recognition%20with%20Frozen%0A%20%20Vision-language%20Model%0AAuthor%3A%20Junhui%20Yin%20and%20Xinyu%20Zhang%20and%20Lin%20Wu%20and%20Xiaojie%20Wang%0AAbstract%3A%20%20%20Current%20pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aremarkable%20zero-shot%20generalization%20capabilities%20across%20various%20downstream%0Atasks.%20However%2C%20their%20performance%20significantly%20degrades%20when%20test%20inputs%0Aexhibit%20different%20distributions.%20In%20this%20paper%2C%20we%20explore%20the%20concept%20of%0Atest-time%20prompt%20tuning%20%28TTPT%29%2C%20which%20facilitates%20the%20adaptation%20of%20the%20CLIP%0Amodel%20to%20novel%20downstream%20tasks%20through%20a%20one-step%20unsupervised%20optimization%0Athat%20involves%20only%20test%20samples.%20Inspired%20by%20in-context%20learning%20in%20natural%0Alanguage%20processing%20%28NLP%29%2C%20we%20propose%20In-Context%20Prompt%20Learning%20%28InCPL%29%20for%0Atest-time%20visual%20recognition%20tasks%2C%20which%20empowers%20a%20pre-trained%0Avision-language%20model%20with%20labeled%20examples%20as%20context%20information%20on%0Adownstream%20task.%20Specifically%2C%20InCPL%20associates%20a%20new%20test%20sample%20with%20very%20few%0Alabeled%20examples%20%28sometimes%20just%20one%29%20as%20context%20information%2C%20enabling%20reliable%0Alabel%20estimation%20for%20the%20test%20sample%20and%20facilitating%20model%20adaptation.%20To%0Aachieve%20this%2C%20InCPL%20employs%20an%20efficient%20language-to-vision%20translator%20to%0Aexplore%20the%20textual%20prior%20information%20for%20visual%20prompt%20learning.%20Further%2C%20we%0Aintroduce%20a%20context-aware%20unsupervised%20loss%20to%20optimize%20visual%20prompts%20tailored%0Ato%20test%20samples.%20Finally%2C%20we%20design%20a%20cyclic%20learning%20strategy%20for%20visual%20and%0Atextual%20prompts%20to%20ensure%20mutual%20synergy%20across%20different%20modalities.%20This%0Aenables%20a%20pre-trained%2C%20frozen%20CLIP%20model%20to%20adapt%20to%20any%20task%20using%20its%20learned%0Aadaptive%20prompt.%20Our%20method%20demonstrates%20superior%20performance%20and%20achieves%0Astate-of-the-art%20results%20across%20various%20downstream%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-context%2520Prompt%2520Learning%2520for%2520Test-time%2520Vision%2520Recognition%2520with%2520Frozen%250A%2520%2520Vision-language%2520Model%26entry.906535625%3DJunhui%2520Yin%2520and%2520Xinyu%2520Zhang%2520and%2520Lin%2520Wu%2520and%2520Xiaojie%2520Wang%26entry.1292438233%3D%2520%2520Current%2520pre-trained%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%250Aremarkable%2520zero-shot%2520generalization%2520capabilities%2520across%2520various%2520downstream%250Atasks.%2520However%252C%2520their%2520performance%2520significantly%2520degrades%2520when%2520test%2520inputs%250Aexhibit%2520different%2520distributions.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520concept%2520of%250Atest-time%2520prompt%2520tuning%2520%2528TTPT%2529%252C%2520which%2520facilitates%2520the%2520adaptation%2520of%2520the%2520CLIP%250Amodel%2520to%2520novel%2520downstream%2520tasks%2520through%2520a%2520one-step%2520unsupervised%2520optimization%250Athat%2520involves%2520only%2520test%2520samples.%2520Inspired%2520by%2520in-context%2520learning%2520in%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%252C%2520we%2520propose%2520In-Context%2520Prompt%2520Learning%2520%2528InCPL%2529%2520for%250Atest-time%2520visual%2520recognition%2520tasks%252C%2520which%2520empowers%2520a%2520pre-trained%250Avision-language%2520model%2520with%2520labeled%2520examples%2520as%2520context%2520information%2520on%250Adownstream%2520task.%2520Specifically%252C%2520InCPL%2520associates%2520a%2520new%2520test%2520sample%2520with%2520very%2520few%250Alabeled%2520examples%2520%2528sometimes%2520just%2520one%2529%2520as%2520context%2520information%252C%2520enabling%2520reliable%250Alabel%2520estimation%2520for%2520the%2520test%2520sample%2520and%2520facilitating%2520model%2520adaptation.%2520To%250Aachieve%2520this%252C%2520InCPL%2520employs%2520an%2520efficient%2520language-to-vision%2520translator%2520to%250Aexplore%2520the%2520textual%2520prior%2520information%2520for%2520visual%2520prompt%2520learning.%2520Further%252C%2520we%250Aintroduce%2520a%2520context-aware%2520unsupervised%2520loss%2520to%2520optimize%2520visual%2520prompts%2520tailored%250Ato%2520test%2520samples.%2520Finally%252C%2520we%2520design%2520a%2520cyclic%2520learning%2520strategy%2520for%2520visual%2520and%250Atextual%2520prompts%2520to%2520ensure%2520mutual%2520synergy%2520across%2520different%2520modalities.%2520This%250Aenables%2520a%2520pre-trained%252C%2520frozen%2520CLIP%2520model%2520to%2520adapt%2520to%2520any%2520task%2520using%2520its%2520learned%250Aadaptive%2520prompt.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520and%2520achieves%250Astate-of-the-art%2520results%2520across%2520various%2520downstream%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-context%20Prompt%20Learning%20for%20Test-time%20Vision%20Recognition%20with%20Frozen%0A%20%20Vision-language%20Model&entry.906535625=Junhui%20Yin%20and%20Xinyu%20Zhang%20and%20Lin%20Wu%20and%20Xiaojie%20Wang&entry.1292438233=%20%20Current%20pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aremarkable%20zero-shot%20generalization%20capabilities%20across%20various%20downstream%0Atasks.%20However%2C%20their%20performance%20significantly%20degrades%20when%20test%20inputs%0Aexhibit%20different%20distributions.%20In%20this%20paper%2C%20we%20explore%20the%20concept%20of%0Atest-time%20prompt%20tuning%20%28TTPT%29%2C%20which%20facilitates%20the%20adaptation%20of%20the%20CLIP%0Amodel%20to%20novel%20downstream%20tasks%20through%20a%20one-step%20unsupervised%20optimization%0Athat%20involves%20only%20test%20samples.%20Inspired%20by%20in-context%20learning%20in%20natural%0Alanguage%20processing%20%28NLP%29%2C%20we%20propose%20In-Context%20Prompt%20Learning%20%28InCPL%29%20for%0Atest-time%20visual%20recognition%20tasks%2C%20which%20empowers%20a%20pre-trained%0Avision-language%20model%20with%20labeled%20examples%20as%20context%20information%20on%0Adownstream%20task.%20Specifically%2C%20InCPL%20associates%20a%20new%20test%20sample%20with%20very%20few%0Alabeled%20examples%20%28sometimes%20just%20one%29%20as%20context%20information%2C%20enabling%20reliable%0Alabel%20estimation%20for%20the%20test%20sample%20and%20facilitating%20model%20adaptation.%20To%0Aachieve%20this%2C%20InCPL%20employs%20an%20efficient%20language-to-vision%20translator%20to%0Aexplore%20the%20textual%20prior%20information%20for%20visual%20prompt%20learning.%20Further%2C%20we%0Aintroduce%20a%20context-aware%20unsupervised%20loss%20to%20optimize%20visual%20prompts%20tailored%0Ato%20test%20samples.%20Finally%2C%20we%20design%20a%20cyclic%20learning%20strategy%20for%20visual%20and%0Atextual%20prompts%20to%20ensure%20mutual%20synergy%20across%20different%20modalities.%20This%0Aenables%20a%20pre-trained%2C%20frozen%20CLIP%20model%20to%20adapt%20to%20any%20task%20using%20its%20learned%0Aadaptive%20prompt.%20Our%20method%20demonstrates%20superior%20performance%20and%20achieves%0Astate-of-the-art%20results%20across%20various%20downstream%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06126v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


