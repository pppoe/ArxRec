<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250810.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MBA-SLAM: Motion Blur Aware Gaussian Splatting SLAM", "author": "Peng Wang and Lingzhe Zhao and Yin Zhang and Shiyu Zhao and Peidong Liu", "abstract": "  Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and\n3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in\nSimultaneous Localization and Mapping (SLAM) for photo-realistic rendering,\nparticularly when using high-quality video sequences as input. However,\nexisting methods struggle with motion-blurred frames, which are common in\nreal-world scenarios like low-light or long-exposure conditions. This often\nresults in a significant reduction in both camera localization accuracy and map\nreconstruction quality. To address this challenge, we propose a dense visual\ndeblur SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs and\nenhance image deblurring. Our approach integrates an efficient motion\nblur-aware tracker with either neural radiance fields or Gaussian Splatting\nbased mapper. By accurately modeling the physical image formation process of\nmotion-blurred images, our method simultaneously learns 3D scene representation\nand estimates the cameras' local trajectory during exposure time, enabling\nproactive compensation for motion blur caused by camera movement. In our\nexperiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art\nmethods in both camera localization and map reconstruction, showcasing superior\nperformance across a range of datasets, including synthetic and real datasets\nfeaturing sharp images as well as those affected by motion blur, highlighting\nthe versatility and robustness of our approach. Code is available at\nhttps://github.com/WU-CVGL/MBA-SLAM.\n", "link": "http://arxiv.org/abs/2411.08279v2", "date": "2025-08-08", "relevancy": 3.2997, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7217}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6434}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MBA-SLAM%3A%20Motion%20Blur%20Aware%20Gaussian%20Splatting%20SLAM&body=Title%3A%20MBA-SLAM%3A%20Motion%20Blur%20Aware%20Gaussian%20Splatting%20SLAM%0AAuthor%3A%20Peng%20Wang%20and%20Lingzhe%20Zhao%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Emerging%203D%20scene%20representations%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20demonstrated%20their%20effectiveness%20in%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20for%20photo-realistic%20rendering%2C%0Aparticularly%20when%20using%20high-quality%20video%20sequences%20as%20input.%20However%2C%0Aexisting%20methods%20struggle%20with%20motion-blurred%20frames%2C%20which%20are%20common%20in%0Areal-world%20scenarios%20like%20low-light%20or%20long-exposure%20conditions.%20This%20often%0Aresults%20in%20a%20significant%20reduction%20in%20both%20camera%20localization%20accuracy%20and%20map%0Areconstruction%20quality.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dense%20visual%0Adeblur%20SLAM%20pipeline%20%28i.e.%20MBA-SLAM%29%20to%20handle%20severe%20motion-blurred%20inputs%20and%0Aenhance%20image%20deblurring.%20Our%20approach%20integrates%20an%20efficient%20motion%0Ablur-aware%20tracker%20with%20either%20neural%20radiance%20fields%20or%20Gaussian%20Splatting%0Abased%20mapper.%20By%20accurately%20modeling%20the%20physical%20image%20formation%20process%20of%0Amotion-blurred%20images%2C%20our%20method%20simultaneously%20learns%203D%20scene%20representation%0Aand%20estimates%20the%20cameras%27%20local%20trajectory%20during%20exposure%20time%2C%20enabling%0Aproactive%20compensation%20for%20motion%20blur%20caused%20by%20camera%20movement.%20In%20our%0Aexperiments%2C%20we%20demonstrate%20that%20MBA-SLAM%20surpasses%20previous%20state-of-the-art%0Amethods%20in%20both%20camera%20localization%20and%20map%20reconstruction%2C%20showcasing%20superior%0Aperformance%20across%20a%20range%20of%20datasets%2C%20including%20synthetic%20and%20real%20datasets%0Afeaturing%20sharp%20images%20as%20well%20as%20those%20affected%20by%20motion%20blur%2C%20highlighting%0Athe%20versatility%20and%20robustness%20of%20our%20approach.%20Code%20is%20available%20at%0Ahttps%3A//github.com/WU-CVGL/MBA-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMBA-SLAM%253A%2520Motion%2520Blur%2520Aware%2520Gaussian%2520Splatting%2520SLAM%26entry.906535625%3DPeng%2520Wang%2520and%2520Lingzhe%2520Zhao%2520and%2520Yin%2520Zhang%2520and%2520Shiyu%2520Zhao%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Emerging%25203D%2520scene%2520representations%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520have%2520demonstrated%2520their%2520effectiveness%2520in%250ASimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520for%2520photo-realistic%2520rendering%252C%250Aparticularly%2520when%2520using%2520high-quality%2520video%2520sequences%2520as%2520input.%2520However%252C%250Aexisting%2520methods%2520struggle%2520with%2520motion-blurred%2520frames%252C%2520which%2520are%2520common%2520in%250Areal-world%2520scenarios%2520like%2520low-light%2520or%2520long-exposure%2520conditions.%2520This%2520often%250Aresults%2520in%2520a%2520significant%2520reduction%2520in%2520both%2520camera%2520localization%2520accuracy%2520and%2520map%250Areconstruction%2520quality.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520dense%2520visual%250Adeblur%2520SLAM%2520pipeline%2520%2528i.e.%2520MBA-SLAM%2529%2520to%2520handle%2520severe%2520motion-blurred%2520inputs%2520and%250Aenhance%2520image%2520deblurring.%2520Our%2520approach%2520integrates%2520an%2520efficient%2520motion%250Ablur-aware%2520tracker%2520with%2520either%2520neural%2520radiance%2520fields%2520or%2520Gaussian%2520Splatting%250Abased%2520mapper.%2520By%2520accurately%2520modeling%2520the%2520physical%2520image%2520formation%2520process%2520of%250Amotion-blurred%2520images%252C%2520our%2520method%2520simultaneously%2520learns%25203D%2520scene%2520representation%250Aand%2520estimates%2520the%2520cameras%2527%2520local%2520trajectory%2520during%2520exposure%2520time%252C%2520enabling%250Aproactive%2520compensation%2520for%2520motion%2520blur%2520caused%2520by%2520camera%2520movement.%2520In%2520our%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520MBA-SLAM%2520surpasses%2520previous%2520state-of-the-art%250Amethods%2520in%2520both%2520camera%2520localization%2520and%2520map%2520reconstruction%252C%2520showcasing%2520superior%250Aperformance%2520across%2520a%2520range%2520of%2520datasets%252C%2520including%2520synthetic%2520and%2520real%2520datasets%250Afeaturing%2520sharp%2520images%2520as%2520well%2520as%2520those%2520affected%2520by%2520motion%2520blur%252C%2520highlighting%250Athe%2520versatility%2520and%2520robustness%2520of%2520our%2520approach.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/WU-CVGL/MBA-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MBA-SLAM%3A%20Motion%20Blur%20Aware%20Gaussian%20Splatting%20SLAM&entry.906535625=Peng%20Wang%20and%20Lingzhe%20Zhao%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu&entry.1292438233=%20%20Emerging%203D%20scene%20representations%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20demonstrated%20their%20effectiveness%20in%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20for%20photo-realistic%20rendering%2C%0Aparticularly%20when%20using%20high-quality%20video%20sequences%20as%20input.%20However%2C%0Aexisting%20methods%20struggle%20with%20motion-blurred%20frames%2C%20which%20are%20common%20in%0Areal-world%20scenarios%20like%20low-light%20or%20long-exposure%20conditions.%20This%20often%0Aresults%20in%20a%20significant%20reduction%20in%20both%20camera%20localization%20accuracy%20and%20map%0Areconstruction%20quality.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dense%20visual%0Adeblur%20SLAM%20pipeline%20%28i.e.%20MBA-SLAM%29%20to%20handle%20severe%20motion-blurred%20inputs%20and%0Aenhance%20image%20deblurring.%20Our%20approach%20integrates%20an%20efficient%20motion%0Ablur-aware%20tracker%20with%20either%20neural%20radiance%20fields%20or%20Gaussian%20Splatting%0Abased%20mapper.%20By%20accurately%20modeling%20the%20physical%20image%20formation%20process%20of%0Amotion-blurred%20images%2C%20our%20method%20simultaneously%20learns%203D%20scene%20representation%0Aand%20estimates%20the%20cameras%27%20local%20trajectory%20during%20exposure%20time%2C%20enabling%0Aproactive%20compensation%20for%20motion%20blur%20caused%20by%20camera%20movement.%20In%20our%0Aexperiments%2C%20we%20demonstrate%20that%20MBA-SLAM%20surpasses%20previous%20state-of-the-art%0Amethods%20in%20both%20camera%20localization%20and%20map%20reconstruction%2C%20showcasing%20superior%0Aperformance%20across%20a%20range%20of%20datasets%2C%20including%20synthetic%20and%20real%20datasets%0Afeaturing%20sharp%20images%20as%20well%20as%20those%20affected%20by%20motion%20blur%2C%20highlighting%0Athe%20versatility%20and%20robustness%20of%20our%20approach.%20Code%20is%20available%20at%0Ahttps%3A//github.com/WU-CVGL/MBA-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08279v2&entry.124074799=Read"},
{"title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video\n  Diffusion Distillation", "author": "Wenbin Teng and Gonglin Chen and Haiwei Chen and Yajie Zhao", "abstract": "  Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage.\n", "link": "http://arxiv.org/abs/2508.06392v1", "date": "2025-08-08", "relevancy": 3.148, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6298}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FVGen%3A%20Accelerating%20Novel-View%20Synthesis%20with%20Adversarial%20Video%0A%20%20Diffusion%20Distillation&body=Title%3A%20FVGen%3A%20Accelerating%20Novel-View%20Synthesis%20with%20Adversarial%20Video%0A%20%20Diffusion%20Distillation%0AAuthor%3A%20Wenbin%20Teng%20and%20Gonglin%20Chen%20and%20Haiwei%20Chen%20and%20Yajie%20Zhao%0AAbstract%3A%20%20%20Recent%20progress%20in%203D%20reconstruction%20has%20enabled%20realistic%203D%20models%20from%0Adense%20image%20captures%2C%20yet%20challenges%20persist%20with%20sparse%20views%2C%20often%20leading%0Ato%20artifacts%20in%20unseen%20areas.%20Recent%20works%20leverage%20Video%20Diffusion%20Models%0A%28VDMs%29%20to%20generate%20dense%20observations%2C%20filling%20the%20gaps%20when%20only%20sparse%20views%0Aare%20available%20for%203D%20reconstruction%20tasks.%20A%20significant%20limitation%20of%20these%0Amethods%20is%20their%20slow%20sampling%20speed%20when%20using%20VDMs.%20In%20this%20paper%2C%20we%20present%0AFVGen%2C%20a%20novel%20framework%20that%20addresses%20this%20challenge%20by%20enabling%20fast%20novel%0Aview%20synthesis%20using%20VDMs%20in%20as%20few%20as%20four%20sampling%20steps.%20We%20propose%20a%20novel%0Avideo%20diffusion%20model%20distillation%20method%20that%20distills%20a%20multi-step%20denoising%0Ateacher%20model%20into%20a%20few-step%20denoising%20student%20model%20using%20Generative%0AAdversarial%20Networks%20%28GANs%29%20and%20softened%20reverse%20KL-divergence%20minimization.%0AExtensive%20experiments%20on%20real-world%20datasets%20show%20that%2C%20compared%20to%20previous%0Aworks%2C%20our%20framework%20generates%20the%20same%20number%20of%20novel%20views%20with%20similar%20%28or%0Aeven%20better%29%20visual%20quality%20while%20reducing%20sampling%20time%20by%20more%20than%2090%25.%0AFVGen%20significantly%20improves%20time%20efficiency%20for%20downstream%20reconstruction%0Atasks%2C%20particularly%20when%20working%20with%20sparse%20input%20views%20%28more%20than%202%29%20where%0Apre-trained%20VDMs%20need%20to%20be%20run%20multiple%20times%20to%20achieve%20better%20spatial%0Acoverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFVGen%253A%2520Accelerating%2520Novel-View%2520Synthesis%2520with%2520Adversarial%2520Video%250A%2520%2520Diffusion%2520Distillation%26entry.906535625%3DWenbin%2520Teng%2520and%2520Gonglin%2520Chen%2520and%2520Haiwei%2520Chen%2520and%2520Yajie%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%25203D%2520reconstruction%2520has%2520enabled%2520realistic%25203D%2520models%2520from%250Adense%2520image%2520captures%252C%2520yet%2520challenges%2520persist%2520with%2520sparse%2520views%252C%2520often%2520leading%250Ato%2520artifacts%2520in%2520unseen%2520areas.%2520Recent%2520works%2520leverage%2520Video%2520Diffusion%2520Models%250A%2528VDMs%2529%2520to%2520generate%2520dense%2520observations%252C%2520filling%2520the%2520gaps%2520when%2520only%2520sparse%2520views%250Aare%2520available%2520for%25203D%2520reconstruction%2520tasks.%2520A%2520significant%2520limitation%2520of%2520these%250Amethods%2520is%2520their%2520slow%2520sampling%2520speed%2520when%2520using%2520VDMs.%2520In%2520this%2520paper%252C%2520we%2520present%250AFVGen%252C%2520a%2520novel%2520framework%2520that%2520addresses%2520this%2520challenge%2520by%2520enabling%2520fast%2520novel%250Aview%2520synthesis%2520using%2520VDMs%2520in%2520as%2520few%2520as%2520four%2520sampling%2520steps.%2520We%2520propose%2520a%2520novel%250Avideo%2520diffusion%2520model%2520distillation%2520method%2520that%2520distills%2520a%2520multi-step%2520denoising%250Ateacher%2520model%2520into%2520a%2520few-step%2520denoising%2520student%2520model%2520using%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%2520and%2520softened%2520reverse%2520KL-divergence%2520minimization.%250AExtensive%2520experiments%2520on%2520real-world%2520datasets%2520show%2520that%252C%2520compared%2520to%2520previous%250Aworks%252C%2520our%2520framework%2520generates%2520the%2520same%2520number%2520of%2520novel%2520views%2520with%2520similar%2520%2528or%250Aeven%2520better%2529%2520visual%2520quality%2520while%2520reducing%2520sampling%2520time%2520by%2520more%2520than%252090%2525.%250AFVGen%2520significantly%2520improves%2520time%2520efficiency%2520for%2520downstream%2520reconstruction%250Atasks%252C%2520particularly%2520when%2520working%2520with%2520sparse%2520input%2520views%2520%2528more%2520than%25202%2529%2520where%250Apre-trained%2520VDMs%2520need%2520to%2520be%2520run%2520multiple%2520times%2520to%2520achieve%2520better%2520spatial%250Acoverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FVGen%3A%20Accelerating%20Novel-View%20Synthesis%20with%20Adversarial%20Video%0A%20%20Diffusion%20Distillation&entry.906535625=Wenbin%20Teng%20and%20Gonglin%20Chen%20and%20Haiwei%20Chen%20and%20Yajie%20Zhao&entry.1292438233=%20%20Recent%20progress%20in%203D%20reconstruction%20has%20enabled%20realistic%203D%20models%20from%0Adense%20image%20captures%2C%20yet%20challenges%20persist%20with%20sparse%20views%2C%20often%20leading%0Ato%20artifacts%20in%20unseen%20areas.%20Recent%20works%20leverage%20Video%20Diffusion%20Models%0A%28VDMs%29%20to%20generate%20dense%20observations%2C%20filling%20the%20gaps%20when%20only%20sparse%20views%0Aare%20available%20for%203D%20reconstruction%20tasks.%20A%20significant%20limitation%20of%20these%0Amethods%20is%20their%20slow%20sampling%20speed%20when%20using%20VDMs.%20In%20this%20paper%2C%20we%20present%0AFVGen%2C%20a%20novel%20framework%20that%20addresses%20this%20challenge%20by%20enabling%20fast%20novel%0Aview%20synthesis%20using%20VDMs%20in%20as%20few%20as%20four%20sampling%20steps.%20We%20propose%20a%20novel%0Avideo%20diffusion%20model%20distillation%20method%20that%20distills%20a%20multi-step%20denoising%0Ateacher%20model%20into%20a%20few-step%20denoising%20student%20model%20using%20Generative%0AAdversarial%20Networks%20%28GANs%29%20and%20softened%20reverse%20KL-divergence%20minimization.%0AExtensive%20experiments%20on%20real-world%20datasets%20show%20that%2C%20compared%20to%20previous%0Aworks%2C%20our%20framework%20generates%20the%20same%20number%20of%20novel%20views%20with%20similar%20%28or%0Aeven%20better%29%20visual%20quality%20while%20reducing%20sampling%20time%20by%20more%20than%2090%25.%0AFVGen%20significantly%20improves%20time%20efficiency%20for%20downstream%20reconstruction%0Atasks%2C%20particularly%20when%20working%20with%20sparse%20input%20views%20%28more%20than%202%29%20where%0Apre-trained%20VDMs%20need%20to%20be%20run%20multiple%20times%20to%20achieve%20better%20spatial%0Acoverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06392v1&entry.124074799=Read"},
{"title": "Your other Left! Vision-Language Models Fail to Identify Relative\n  Positions in Medical Images", "author": "Daniel Wolf and Heiko Hillenhagen and Billurvan Taskin and Alex B\u00e4uerle and Meinrad Beer and Michael G\u00f6tz and Timo Ropinski", "abstract": "  Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.\n", "link": "http://arxiv.org/abs/2508.00549v2", "date": "2025-08-08", "relevancy": 2.9679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20other%20Left%21%20Vision-Language%20Models%20Fail%20to%20Identify%20Relative%0A%20%20Positions%20in%20Medical%20Images&body=Title%3A%20Your%20other%20Left%21%20Vision-Language%20Models%20Fail%20to%20Identify%20Relative%0A%20%20Positions%20in%20Medical%20Images%0AAuthor%3A%20Daniel%20Wolf%20and%20Heiko%20Hillenhagen%20and%20Billurvan%20Taskin%20and%20Alex%20B%C3%A4uerle%20and%20Meinrad%20Beer%20and%20Michael%20G%C3%B6tz%20and%20Timo%20Ropinski%0AAbstract%3A%20%20%20Clinical%20decision-making%20relies%20heavily%20on%20understanding%20relative%20positions%0Aof%20anatomical%20structures%20and%20anomalies.%20Therefore%2C%20for%20Vision-Language%20Models%0A%28VLMs%29%20to%20be%20applicable%20in%20clinical%20practice%2C%20the%20ability%20to%20accurately%0Adetermine%20relative%20positions%20on%20medical%20images%20is%20a%20fundamental%20prerequisite.%0ADespite%20its%20importance%2C%20this%20capability%20remains%20highly%20underexplored.%20To%0Aaddress%20this%20gap%2C%20we%20evaluate%20the%20ability%20of%20state-of-the-art%20VLMs%2C%20GPT-4o%2C%0ALlama3.2%2C%20Pixtral%2C%20and%20JanusPro%2C%20and%20find%20that%20all%20models%20fail%20at%20this%0Afundamental%20task.%20Inspired%20by%20successful%20approaches%20in%20computer%20vision%2C%20we%0Ainvestigate%20whether%20visual%20prompts%2C%20such%20as%20alphanumeric%20or%20colored%20markers%0Aplaced%20on%20anatomical%20structures%2C%20can%20enhance%20performance.%20While%20these%20markers%0Aprovide%20moderate%20improvements%2C%20results%20remain%20significantly%20lower%20on%20medical%0Aimages%20compared%20to%20observations%20made%20on%20natural%20images.%20Our%20evaluations%20suggest%0Athat%2C%20in%20medical%20imaging%2C%20VLMs%20rely%20more%20on%20prior%20anatomical%20knowledge%20than%20on%0Aactual%20image%20content%20for%20answering%20relative%20position%20questions%2C%20often%20leading%0Ato%20incorrect%20conclusions.%20To%20facilitate%20further%20research%20in%20this%20area%2C%20we%0Aintroduce%20the%20MIRP%20%2C%20Medical%20Imaging%20Relative%20Positioning%2C%20benchmark%20dataset%2C%0Adesigned%20to%20systematically%20evaluate%20the%20capability%20to%20identify%20relative%0Apositions%20in%20medical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520other%2520Left%2521%2520Vision-Language%2520Models%2520Fail%2520to%2520Identify%2520Relative%250A%2520%2520Positions%2520in%2520Medical%2520Images%26entry.906535625%3DDaniel%2520Wolf%2520and%2520Heiko%2520Hillenhagen%2520and%2520Billurvan%2520Taskin%2520and%2520Alex%2520B%25C3%25A4uerle%2520and%2520Meinrad%2520Beer%2520and%2520Michael%2520G%25C3%25B6tz%2520and%2520Timo%2520Ropinski%26entry.1292438233%3D%2520%2520Clinical%2520decision-making%2520relies%2520heavily%2520on%2520understanding%2520relative%2520positions%250Aof%2520anatomical%2520structures%2520and%2520anomalies.%2520Therefore%252C%2520for%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520to%2520be%2520applicable%2520in%2520clinical%2520practice%252C%2520the%2520ability%2520to%2520accurately%250Adetermine%2520relative%2520positions%2520on%2520medical%2520images%2520is%2520a%2520fundamental%2520prerequisite.%250ADespite%2520its%2520importance%252C%2520this%2520capability%2520remains%2520highly%2520underexplored.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520evaluate%2520the%2520ability%2520of%2520state-of-the-art%2520VLMs%252C%2520GPT-4o%252C%250ALlama3.2%252C%2520Pixtral%252C%2520and%2520JanusPro%252C%2520and%2520find%2520that%2520all%2520models%2520fail%2520at%2520this%250Afundamental%2520task.%2520Inspired%2520by%2520successful%2520approaches%2520in%2520computer%2520vision%252C%2520we%250Ainvestigate%2520whether%2520visual%2520prompts%252C%2520such%2520as%2520alphanumeric%2520or%2520colored%2520markers%250Aplaced%2520on%2520anatomical%2520structures%252C%2520can%2520enhance%2520performance.%2520While%2520these%2520markers%250Aprovide%2520moderate%2520improvements%252C%2520results%2520remain%2520significantly%2520lower%2520on%2520medical%250Aimages%2520compared%2520to%2520observations%2520made%2520on%2520natural%2520images.%2520Our%2520evaluations%2520suggest%250Athat%252C%2520in%2520medical%2520imaging%252C%2520VLMs%2520rely%2520more%2520on%2520prior%2520anatomical%2520knowledge%2520than%2520on%250Aactual%2520image%2520content%2520for%2520answering%2520relative%2520position%2520questions%252C%2520often%2520leading%250Ato%2520incorrect%2520conclusions.%2520To%2520facilitate%2520further%2520research%2520in%2520this%2520area%252C%2520we%250Aintroduce%2520the%2520MIRP%2520%252C%2520Medical%2520Imaging%2520Relative%2520Positioning%252C%2520benchmark%2520dataset%252C%250Adesigned%2520to%2520systematically%2520evaluate%2520the%2520capability%2520to%2520identify%2520relative%250Apositions%2520in%2520medical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20other%20Left%21%20Vision-Language%20Models%20Fail%20to%20Identify%20Relative%0A%20%20Positions%20in%20Medical%20Images&entry.906535625=Daniel%20Wolf%20and%20Heiko%20Hillenhagen%20and%20Billurvan%20Taskin%20and%20Alex%20B%C3%A4uerle%20and%20Meinrad%20Beer%20and%20Michael%20G%C3%B6tz%20and%20Timo%20Ropinski&entry.1292438233=%20%20Clinical%20decision-making%20relies%20heavily%20on%20understanding%20relative%20positions%0Aof%20anatomical%20structures%20and%20anomalies.%20Therefore%2C%20for%20Vision-Language%20Models%0A%28VLMs%29%20to%20be%20applicable%20in%20clinical%20practice%2C%20the%20ability%20to%20accurately%0Adetermine%20relative%20positions%20on%20medical%20images%20is%20a%20fundamental%20prerequisite.%0ADespite%20its%20importance%2C%20this%20capability%20remains%20highly%20underexplored.%20To%0Aaddress%20this%20gap%2C%20we%20evaluate%20the%20ability%20of%20state-of-the-art%20VLMs%2C%20GPT-4o%2C%0ALlama3.2%2C%20Pixtral%2C%20and%20JanusPro%2C%20and%20find%20that%20all%20models%20fail%20at%20this%0Afundamental%20task.%20Inspired%20by%20successful%20approaches%20in%20computer%20vision%2C%20we%0Ainvestigate%20whether%20visual%20prompts%2C%20such%20as%20alphanumeric%20or%20colored%20markers%0Aplaced%20on%20anatomical%20structures%2C%20can%20enhance%20performance.%20While%20these%20markers%0Aprovide%20moderate%20improvements%2C%20results%20remain%20significantly%20lower%20on%20medical%0Aimages%20compared%20to%20observations%20made%20on%20natural%20images.%20Our%20evaluations%20suggest%0Athat%2C%20in%20medical%20imaging%2C%20VLMs%20rely%20more%20on%20prior%20anatomical%20knowledge%20than%20on%0Aactual%20image%20content%20for%20answering%20relative%20position%20questions%2C%20often%20leading%0Ato%20incorrect%20conclusions.%20To%20facilitate%20further%20research%20in%20this%20area%2C%20we%0Aintroduce%20the%20MIRP%20%2C%20Medical%20Imaging%20Relative%20Positioning%2C%20benchmark%20dataset%2C%0Adesigned%20to%20systematically%20evaluate%20the%20capability%20to%20identify%20relative%0Apositions%20in%20medical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00549v2&entry.124074799=Read"},
{"title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning", "author": "Zhangquan Chen and Ruihui Zhao and Chuwei Luo and Mingze Sun and Xinlei Yu and Yangyang Kang and Ruqi Huang", "abstract": "  Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2508.06259v1", "date": "2025-08-08", "relevancy": 2.8493, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIFThinker%3A%20Spatially-Aware%20Image%20Focus%20for%20Visual%20Reasoning&body=Title%3A%20SIFThinker%3A%20Spatially-Aware%20Image%20Focus%20for%20Visual%20Reasoning%0AAuthor%3A%20Zhangquan%20Chen%20and%20Ruihui%20Zhao%20and%20Chuwei%20Luo%20and%20Mingze%20Sun%20and%20Xinlei%20Yu%20and%20Yangyang%20Kang%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20Current%20multimodal%20large%20language%20models%20%28MLLMs%29%20still%20face%20significant%0Achallenges%20in%20complex%20visual%20tasks%20%28e.g.%2C%20spatial%20understanding%2C%20fine-grained%0Aperception%29.%20Prior%20methods%20have%20tried%20to%20incorporate%20visual%20reasoning%2C%20however%2C%0Athey%20fail%20to%20leverage%20attention%20correction%20with%20spatial%20cues%20to%20iteratively%0Arefine%20their%20focus%20on%20prompt-relevant%20regions.%20In%20this%20paper%2C%20we%20introduce%0ASIFThinker%2C%20a%20spatially-aware%20%22think-with-images%22%20framework%20that%20mimics%20human%0Avisual%20perception.%20Specifically%2C%20SIFThinker%20enables%20attention%20correcting%20and%0Aimage%20region%20focusing%20by%20interleaving%20depth-enhanced%20bounding%20boxes%20and%20natural%0Alanguage.%20Our%20contributions%20are%20twofold%3A%20First%2C%20we%20introduce%20a%0Areverse-expansion-forward-inference%20strategy%20that%20facilitates%20the%20generation%20of%0Ainterleaved%20image-text%20chains%20of%20thought%20for%20process-level%20supervision%2C%20which%0Ain%20turn%20leads%20to%20the%20construction%20of%20the%20SIF-50K%20dataset.%20Besides%2C%20we%20propose%0AGRPO-SIF%2C%20a%20reinforced%20training%20paradigm%20that%20integrates%20depth-informed%20visual%0Agrounding%20into%20a%20unified%20reasoning%20pipeline%2C%20teaching%20the%20model%20to%20dynamically%0Acorrect%20and%20focus%20on%20prompt-relevant%20regions.%20Extensive%20experiments%20demonstrate%0Athat%20SIFThinker%20outperforms%20state-of-the-art%20methods%20in%20spatial%20understanding%0Aand%20fine-grained%20visual%20perception%2C%20while%20maintaining%20strong%20general%0Acapabilities%2C%20highlighting%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIFThinker%253A%2520Spatially-Aware%2520Image%2520Focus%2520for%2520Visual%2520Reasoning%26entry.906535625%3DZhangquan%2520Chen%2520and%2520Ruihui%2520Zhao%2520and%2520Chuwei%2520Luo%2520and%2520Mingze%2520Sun%2520and%2520Xinlei%2520Yu%2520and%2520Yangyang%2520Kang%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520still%2520face%2520significant%250Achallenges%2520in%2520complex%2520visual%2520tasks%2520%2528e.g.%252C%2520spatial%2520understanding%252C%2520fine-grained%250Aperception%2529.%2520Prior%2520methods%2520have%2520tried%2520to%2520incorporate%2520visual%2520reasoning%252C%2520however%252C%250Athey%2520fail%2520to%2520leverage%2520attention%2520correction%2520with%2520spatial%2520cues%2520to%2520iteratively%250Arefine%2520their%2520focus%2520on%2520prompt-relevant%2520regions.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASIFThinker%252C%2520a%2520spatially-aware%2520%2522think-with-images%2522%2520framework%2520that%2520mimics%2520human%250Avisual%2520perception.%2520Specifically%252C%2520SIFThinker%2520enables%2520attention%2520correcting%2520and%250Aimage%2520region%2520focusing%2520by%2520interleaving%2520depth-enhanced%2520bounding%2520boxes%2520and%2520natural%250Alanguage.%2520Our%2520contributions%2520are%2520twofold%253A%2520First%252C%2520we%2520introduce%2520a%250Areverse-expansion-forward-inference%2520strategy%2520that%2520facilitates%2520the%2520generation%2520of%250Ainterleaved%2520image-text%2520chains%2520of%2520thought%2520for%2520process-level%2520supervision%252C%2520which%250Ain%2520turn%2520leads%2520to%2520the%2520construction%2520of%2520the%2520SIF-50K%2520dataset.%2520Besides%252C%2520we%2520propose%250AGRPO-SIF%252C%2520a%2520reinforced%2520training%2520paradigm%2520that%2520integrates%2520depth-informed%2520visual%250Agrounding%2520into%2520a%2520unified%2520reasoning%2520pipeline%252C%2520teaching%2520the%2520model%2520to%2520dynamically%250Acorrect%2520and%2520focus%2520on%2520prompt-relevant%2520regions.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520SIFThinker%2520outperforms%2520state-of-the-art%2520methods%2520in%2520spatial%2520understanding%250Aand%2520fine-grained%2520visual%2520perception%252C%2520while%2520maintaining%2520strong%2520general%250Acapabilities%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIFThinker%3A%20Spatially-Aware%20Image%20Focus%20for%20Visual%20Reasoning&entry.906535625=Zhangquan%20Chen%20and%20Ruihui%20Zhao%20and%20Chuwei%20Luo%20and%20Mingze%20Sun%20and%20Xinlei%20Yu%20and%20Yangyang%20Kang%20and%20Ruqi%20Huang&entry.1292438233=%20%20Current%20multimodal%20large%20language%20models%20%28MLLMs%29%20still%20face%20significant%0Achallenges%20in%20complex%20visual%20tasks%20%28e.g.%2C%20spatial%20understanding%2C%20fine-grained%0Aperception%29.%20Prior%20methods%20have%20tried%20to%20incorporate%20visual%20reasoning%2C%20however%2C%0Athey%20fail%20to%20leverage%20attention%20correction%20with%20spatial%20cues%20to%20iteratively%0Arefine%20their%20focus%20on%20prompt-relevant%20regions.%20In%20this%20paper%2C%20we%20introduce%0ASIFThinker%2C%20a%20spatially-aware%20%22think-with-images%22%20framework%20that%20mimics%20human%0Avisual%20perception.%20Specifically%2C%20SIFThinker%20enables%20attention%20correcting%20and%0Aimage%20region%20focusing%20by%20interleaving%20depth-enhanced%20bounding%20boxes%20and%20natural%0Alanguage.%20Our%20contributions%20are%20twofold%3A%20First%2C%20we%20introduce%20a%0Areverse-expansion-forward-inference%20strategy%20that%20facilitates%20the%20generation%20of%0Ainterleaved%20image-text%20chains%20of%20thought%20for%20process-level%20supervision%2C%20which%0Ain%20turn%20leads%20to%20the%20construction%20of%20the%20SIF-50K%20dataset.%20Besides%2C%20we%20propose%0AGRPO-SIF%2C%20a%20reinforced%20training%20paradigm%20that%20integrates%20depth-informed%20visual%0Agrounding%20into%20a%20unified%20reasoning%20pipeline%2C%20teaching%20the%20model%20to%20dynamically%0Acorrect%20and%20focus%20on%20prompt-relevant%20regions.%20Extensive%20experiments%20demonstrate%0Athat%20SIFThinker%20outperforms%20state-of-the-art%20methods%20in%20spatial%20understanding%0Aand%20fine-grained%20visual%20perception%2C%20while%20maintaining%20strong%20general%0Acapabilities%2C%20highlighting%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06259v1&entry.124074799=Read"},
{"title": "Can Multimodal Large Language Models Understand Spatial Relations?", "author": "Jingping Liu and Ziyan Liu and Zhedong Cen and Yan Zhou and Yinan Zou and Weiyan Zhang and Haiyun Jiang and Tong Ruan", "abstract": "  Spatial relation reasoning is a crucial task for multimodal large language\nmodels (MLLMs) to understand the objective world. However, current benchmarks\nhave issues like relying on bounding boxes, ignoring perspective substitutions,\nor allowing questions to be answered using only the model's prior knowledge\nwithout image understanding. To address these issues, we introduce SpatialMQA,\na human-annotated spatial relation reasoning benchmark based on COCO2017, which\nenables MLLMs to focus more on understanding images in the objective world. To\nensure data quality, we design a well-tailored annotation procedure, resulting\nin SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of\nclosed- and open-source MLLMs are implemented and the results indicate that the\ncurrent state-of-the-art MLLM achieves only 48.14% accuracy, far below the\nhuman-level accuracy of 98.40%. Extensive experimental analyses are also\nconducted, suggesting the future research directions. The benchmark and codes\nare available at https://github.com/ziyan-xiaoyu/SpatialMQA.git.\n", "link": "http://arxiv.org/abs/2505.19015v2", "date": "2025-08-08", "relevancy": 2.8379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Multimodal%20Large%20Language%20Models%20Understand%20Spatial%20Relations%3F&body=Title%3A%20Can%20Multimodal%20Large%20Language%20Models%20Understand%20Spatial%20Relations%3F%0AAuthor%3A%20Jingping%20Liu%20and%20Ziyan%20Liu%20and%20Zhedong%20Cen%20and%20Yan%20Zhou%20and%20Yinan%20Zou%20and%20Weiyan%20Zhang%20and%20Haiyun%20Jiang%20and%20Tong%20Ruan%0AAbstract%3A%20%20%20Spatial%20relation%20reasoning%20is%20a%20crucial%20task%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20understand%20the%20objective%20world.%20However%2C%20current%20benchmarks%0Ahave%20issues%20like%20relying%20on%20bounding%20boxes%2C%20ignoring%20perspective%20substitutions%2C%0Aor%20allowing%20questions%20to%20be%20answered%20using%20only%20the%20model%27s%20prior%20knowledge%0Awithout%20image%20understanding.%20To%20address%20these%20issues%2C%20we%20introduce%20SpatialMQA%2C%0Aa%20human-annotated%20spatial%20relation%20reasoning%20benchmark%20based%20on%20COCO2017%2C%20which%0Aenables%20MLLMs%20to%20focus%20more%20on%20understanding%20images%20in%20the%20objective%20world.%20To%0Aensure%20data%20quality%2C%20we%20design%20a%20well-tailored%20annotation%20procedure%2C%20resulting%0Ain%20SpatialMQA%20consisting%20of%205%2C392%20samples.%20Based%20on%20this%20benchmark%2C%20a%20series%20of%0Aclosed-%20and%20open-source%20MLLMs%20are%20implemented%20and%20the%20results%20indicate%20that%20the%0Acurrent%20state-of-the-art%20MLLM%20achieves%20only%2048.14%25%20accuracy%2C%20far%20below%20the%0Ahuman-level%20accuracy%20of%2098.40%25.%20Extensive%20experimental%20analyses%20are%20also%0Aconducted%2C%20suggesting%20the%20future%20research%20directions.%20The%20benchmark%20and%20codes%0Aare%20available%20at%20https%3A//github.com/ziyan-xiaoyu/SpatialMQA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Multimodal%2520Large%2520Language%2520Models%2520Understand%2520Spatial%2520Relations%253F%26entry.906535625%3DJingping%2520Liu%2520and%2520Ziyan%2520Liu%2520and%2520Zhedong%2520Cen%2520and%2520Yan%2520Zhou%2520and%2520Yinan%2520Zou%2520and%2520Weiyan%2520Zhang%2520and%2520Haiyun%2520Jiang%2520and%2520Tong%2520Ruan%26entry.1292438233%3D%2520%2520Spatial%2520relation%2520reasoning%2520is%2520a%2520crucial%2520task%2520for%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%2520to%2520understand%2520the%2520objective%2520world.%2520However%252C%2520current%2520benchmarks%250Ahave%2520issues%2520like%2520relying%2520on%2520bounding%2520boxes%252C%2520ignoring%2520perspective%2520substitutions%252C%250Aor%2520allowing%2520questions%2520to%2520be%2520answered%2520using%2520only%2520the%2520model%2527s%2520prior%2520knowledge%250Awithout%2520image%2520understanding.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520SpatialMQA%252C%250Aa%2520human-annotated%2520spatial%2520relation%2520reasoning%2520benchmark%2520based%2520on%2520COCO2017%252C%2520which%250Aenables%2520MLLMs%2520to%2520focus%2520more%2520on%2520understanding%2520images%2520in%2520the%2520objective%2520world.%2520To%250Aensure%2520data%2520quality%252C%2520we%2520design%2520a%2520well-tailored%2520annotation%2520procedure%252C%2520resulting%250Ain%2520SpatialMQA%2520consisting%2520of%25205%252C392%2520samples.%2520Based%2520on%2520this%2520benchmark%252C%2520a%2520series%2520of%250Aclosed-%2520and%2520open-source%2520MLLMs%2520are%2520implemented%2520and%2520the%2520results%2520indicate%2520that%2520the%250Acurrent%2520state-of-the-art%2520MLLM%2520achieves%2520only%252048.14%2525%2520accuracy%252C%2520far%2520below%2520the%250Ahuman-level%2520accuracy%2520of%252098.40%2525.%2520Extensive%2520experimental%2520analyses%2520are%2520also%250Aconducted%252C%2520suggesting%2520the%2520future%2520research%2520directions.%2520The%2520benchmark%2520and%2520codes%250Aare%2520available%2520at%2520https%253A//github.com/ziyan-xiaoyu/SpatialMQA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Multimodal%20Large%20Language%20Models%20Understand%20Spatial%20Relations%3F&entry.906535625=Jingping%20Liu%20and%20Ziyan%20Liu%20and%20Zhedong%20Cen%20and%20Yan%20Zhou%20and%20Yinan%20Zou%20and%20Weiyan%20Zhang%20and%20Haiyun%20Jiang%20and%20Tong%20Ruan&entry.1292438233=%20%20Spatial%20relation%20reasoning%20is%20a%20crucial%20task%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20understand%20the%20objective%20world.%20However%2C%20current%20benchmarks%0Ahave%20issues%20like%20relying%20on%20bounding%20boxes%2C%20ignoring%20perspective%20substitutions%2C%0Aor%20allowing%20questions%20to%20be%20answered%20using%20only%20the%20model%27s%20prior%20knowledge%0Awithout%20image%20understanding.%20To%20address%20these%20issues%2C%20we%20introduce%20SpatialMQA%2C%0Aa%20human-annotated%20spatial%20relation%20reasoning%20benchmark%20based%20on%20COCO2017%2C%20which%0Aenables%20MLLMs%20to%20focus%20more%20on%20understanding%20images%20in%20the%20objective%20world.%20To%0Aensure%20data%20quality%2C%20we%20design%20a%20well-tailored%20annotation%20procedure%2C%20resulting%0Ain%20SpatialMQA%20consisting%20of%205%2C392%20samples.%20Based%20on%20this%20benchmark%2C%20a%20series%20of%0Aclosed-%20and%20open-source%20MLLMs%20are%20implemented%20and%20the%20results%20indicate%20that%20the%0Acurrent%20state-of-the-art%20MLLM%20achieves%20only%2048.14%25%20accuracy%2C%20far%20below%20the%0Ahuman-level%20accuracy%20of%2098.40%25.%20Extensive%20experimental%20analyses%20are%20also%0Aconducted%2C%20suggesting%20the%20future%20research%20directions.%20The%20benchmark%20and%20codes%0Aare%20available%20at%20https%3A//github.com/ziyan-xiaoyu/SpatialMQA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19015v2&entry.124074799=Read"},
{"title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic\n  Alignment", "author": "Shengzhu Yang and Jiawei Du and Shuai Lu and Weihang Zhang and Ningli Wang and Huiqi Li", "abstract": "  Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.\n", "link": "http://arxiv.org/abs/2508.06434v1", "date": "2025-08-08", "relevancy": 2.8237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6475}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment&body=Title%3A%20CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment%0AAuthor%3A%20Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Ningli%20Wang%20and%20Huiqi%20Li%0AAbstract%3A%20%20%20Large-scale%20natural%20image-text%20datasets%2C%20especially%20those%20automatically%0Acollected%20from%20the%20web%2C%20often%20suffer%20from%20loose%20semantic%20alignment%20due%20to%20weak%0Asupervision%2C%20while%20medical%20datasets%20tend%20to%20have%20high%20cross-modal%20correlation%0Abut%20low%20content%20diversity.%20These%20properties%20pose%20a%20common%20challenge%20for%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%3A%20they%20hinder%20the%20model%27s%20ability%0Ato%20learn%20robust%20and%20generalizable%20representations.%20In%20this%20work%2C%20we%20propose%0ACLIPin%2C%20a%20unified%20non-contrastive%20plug-in%20that%20can%20be%20seamlessly%20integrated%0Ainto%20CLIP-style%20architectures%20to%20improve%20multimodal%20semantic%20alignment%2C%0Aproviding%20stronger%20supervision%20and%20enhancing%20alignment%20robustness.%20Furthermore%2C%0Atwo%20shared%20pre-projectors%20are%20designed%20for%20image%20and%20text%20modalities%0Arespectively%20to%20facilitate%20the%20integration%20of%20contrastive%20and%20non-contrastive%0Alearning%20in%20a%20parameter-compromise%20manner.%20Extensive%20experiments%20on%20diverse%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20and%20generality%20of%20CLIPin%20as%20a%0Aplug-and-play%20component%20compatible%20with%20various%20contrastive%20frameworks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/T6Yang/CLIPin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPin%253A%2520A%2520Non-contrastive%2520Plug-in%2520to%2520CLIP%2520for%2520Multimodal%2520Semantic%250A%2520%2520Alignment%26entry.906535625%3DShengzhu%2520Yang%2520and%2520Jiawei%2520Du%2520and%2520Shuai%2520Lu%2520and%2520Weihang%2520Zhang%2520and%2520Ningli%2520Wang%2520and%2520Huiqi%2520Li%26entry.1292438233%3D%2520%2520Large-scale%2520natural%2520image-text%2520datasets%252C%2520especially%2520those%2520automatically%250Acollected%2520from%2520the%2520web%252C%2520often%2520suffer%2520from%2520loose%2520semantic%2520alignment%2520due%2520to%2520weak%250Asupervision%252C%2520while%2520medical%2520datasets%2520tend%2520to%2520have%2520high%2520cross-modal%2520correlation%250Abut%2520low%2520content%2520diversity.%2520These%2520properties%2520pose%2520a%2520common%2520challenge%2520for%250Acontrastive%2520language-image%2520pretraining%2520%2528CLIP%2529%253A%2520they%2520hinder%2520the%2520model%2527s%2520ability%250Ato%2520learn%2520robust%2520and%2520generalizable%2520representations.%2520In%2520this%2520work%252C%2520we%2520propose%250ACLIPin%252C%2520a%2520unified%2520non-contrastive%2520plug-in%2520that%2520can%2520be%2520seamlessly%2520integrated%250Ainto%2520CLIP-style%2520architectures%2520to%2520improve%2520multimodal%2520semantic%2520alignment%252C%250Aproviding%2520stronger%2520supervision%2520and%2520enhancing%2520alignment%2520robustness.%2520Furthermore%252C%250Atwo%2520shared%2520pre-projectors%2520are%2520designed%2520for%2520image%2520and%2520text%2520modalities%250Arespectively%2520to%2520facilitate%2520the%2520integration%2520of%2520contrastive%2520and%2520non-contrastive%250Alearning%2520in%2520a%2520parameter-compromise%2520manner.%2520Extensive%2520experiments%2520on%2520diverse%250Adownstream%2520tasks%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520CLIPin%2520as%2520a%250Aplug-and-play%2520component%2520compatible%2520with%2520various%2520contrastive%2520frameworks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/T6Yang/CLIPin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment&entry.906535625=Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Ningli%20Wang%20and%20Huiqi%20Li&entry.1292438233=%20%20Large-scale%20natural%20image-text%20datasets%2C%20especially%20those%20automatically%0Acollected%20from%20the%20web%2C%20often%20suffer%20from%20loose%20semantic%20alignment%20due%20to%20weak%0Asupervision%2C%20while%20medical%20datasets%20tend%20to%20have%20high%20cross-modal%20correlation%0Abut%20low%20content%20diversity.%20These%20properties%20pose%20a%20common%20challenge%20for%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%3A%20they%20hinder%20the%20model%27s%20ability%0Ato%20learn%20robust%20and%20generalizable%20representations.%20In%20this%20work%2C%20we%20propose%0ACLIPin%2C%20a%20unified%20non-contrastive%20plug-in%20that%20can%20be%20seamlessly%20integrated%0Ainto%20CLIP-style%20architectures%20to%20improve%20multimodal%20semantic%20alignment%2C%0Aproviding%20stronger%20supervision%20and%20enhancing%20alignment%20robustness.%20Furthermore%2C%0Atwo%20shared%20pre-projectors%20are%20designed%20for%20image%20and%20text%20modalities%0Arespectively%20to%20facilitate%20the%20integration%20of%20contrastive%20and%20non-contrastive%0Alearning%20in%20a%20parameter-compromise%20manner.%20Extensive%20experiments%20on%20diverse%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20and%20generality%20of%20CLIPin%20as%20a%0Aplug-and-play%20component%20compatible%20with%20various%20contrastive%20frameworks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/T6Yang/CLIPin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06434v1&entry.124074799=Read"},
{"title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach\n  to Contextual Memory Integration", "author": "George Applegarth and Christian Weatherstone and Maximilian Hollingsworth and Henry Middlebrook and Marcus Irvin", "abstract": "  Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling.\n", "link": "http://arxiv.org/abs/2502.10699v2", "date": "2025-08-08", "relevancy": 2.8169, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Synaptic%20Resonance%20in%20Large%20Language%20Models%3A%20A%20Novel%20Approach%0A%20%20to%20Contextual%20Memory%20Integration&body=Title%3A%20Exploring%20Synaptic%20Resonance%20in%20Large%20Language%20Models%3A%20A%20Novel%20Approach%0A%20%20to%20Contextual%20Memory%20Integration%0AAuthor%3A%20George%20Applegarth%20and%20Christian%20Weatherstone%20and%20Maximilian%20Hollingsworth%20and%20Henry%20Middlebrook%20and%20Marcus%20Irvin%0AAbstract%3A%20%20%20Contextual%20memory%20integration%20remains%20a%20high%20challenge%20in%20the%20development%20of%0Alanguage%20models%2C%20particularly%20in%20tasks%20that%20require%20maintaining%20coherence%20over%0Aextended%20sequences.%20Traditional%20approaches%2C%20such%20as%20self-attention%20mechanisms%0Aand%20memory-augmented%20architectures%2C%20often%20prioritize%20short-term%20dependencies%2C%0Aleading%20to%20fragmentation%20and%20inconsistency%20in%20long-range%20contextual%0Aunderstanding.%20Inspired%20by%20principles%20of%20synaptic%20plasticity%20observed%20in%0Abiological%20neural%20systems%2C%20a%20novel%20mechanism%2C%20Synaptic%20Resonance%2C%20is%20introduced%0Ato%20dynamically%20reinforce%20relevant%20memory%20pathways%20during%20training%20and%0Ainference.%20Unlike%20static%20memory%20representations%2C%20this%20mechanism%20continuously%0Aadjusts%20synaptic%20weight%20matrices%20based%20on%20contextual%20relevance%2C%20allowing%20for%0Aimproved%20information%20retention%20without%20excessive%20computational%20overhead.%0AEvaluations%20conducted%20on%20an%20open-source%20language%20model%20demonstrate%20reductions%0Ain%20perplexity%2C%20enhancements%20in%20contextual%20coherence%2C%20and%20increased%20robustness%0Aagainst%20input%20noise%2C%20highlighting%20the%20effectiveness%20of%20reinforcement-driven%0Amemory%20modulation.%20Comparative%20analysis%20against%20baseline%20models%20further%20reveals%0Athat%20the%20proposed%20approach%20achieves%20higher%20memory%20retention%20efficiency%20while%0Amaintaining%20computational%20feasibility.%20The%20architectural%20modifications%0Aintegrate%20seamlessly%20into%20existing%20transformer-based%20frameworks%2C%20ensuring%0Astable%20convergence%20and%20efficient%20inference%20without%20sacrificing%20scalability.%0AApplications%20benefiting%20from%20improved%20long-term%20contextual%20consistency%2C%20such%20as%0Adialogue%20systems%20and%20document%20summarization%2C%20stand%20to%20gain%20from%20this%20approach.%0AEmpirical%20findings%20suggest%20that%20dynamically%20reinforced%20memory%20pathways%20offer%20a%0Apromising%20alternative%20to%20conventional%20memory%20mechanisms%2C%20addressing%0Alongstanding%20limitations%20in%20extended%20sequence%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Synaptic%2520Resonance%2520in%2520Large%2520Language%2520Models%253A%2520A%2520Novel%2520Approach%250A%2520%2520to%2520Contextual%2520Memory%2520Integration%26entry.906535625%3DGeorge%2520Applegarth%2520and%2520Christian%2520Weatherstone%2520and%2520Maximilian%2520Hollingsworth%2520and%2520Henry%2520Middlebrook%2520and%2520Marcus%2520Irvin%26entry.1292438233%3D%2520%2520Contextual%2520memory%2520integration%2520remains%2520a%2520high%2520challenge%2520in%2520the%2520development%2520of%250Alanguage%2520models%252C%2520particularly%2520in%2520tasks%2520that%2520require%2520maintaining%2520coherence%2520over%250Aextended%2520sequences.%2520Traditional%2520approaches%252C%2520such%2520as%2520self-attention%2520mechanisms%250Aand%2520memory-augmented%2520architectures%252C%2520often%2520prioritize%2520short-term%2520dependencies%252C%250Aleading%2520to%2520fragmentation%2520and%2520inconsistency%2520in%2520long-range%2520contextual%250Aunderstanding.%2520Inspired%2520by%2520principles%2520of%2520synaptic%2520plasticity%2520observed%2520in%250Abiological%2520neural%2520systems%252C%2520a%2520novel%2520mechanism%252C%2520Synaptic%2520Resonance%252C%2520is%2520introduced%250Ato%2520dynamically%2520reinforce%2520relevant%2520memory%2520pathways%2520during%2520training%2520and%250Ainference.%2520Unlike%2520static%2520memory%2520representations%252C%2520this%2520mechanism%2520continuously%250Aadjusts%2520synaptic%2520weight%2520matrices%2520based%2520on%2520contextual%2520relevance%252C%2520allowing%2520for%250Aimproved%2520information%2520retention%2520without%2520excessive%2520computational%2520overhead.%250AEvaluations%2520conducted%2520on%2520an%2520open-source%2520language%2520model%2520demonstrate%2520reductions%250Ain%2520perplexity%252C%2520enhancements%2520in%2520contextual%2520coherence%252C%2520and%2520increased%2520robustness%250Aagainst%2520input%2520noise%252C%2520highlighting%2520the%2520effectiveness%2520of%2520reinforcement-driven%250Amemory%2520modulation.%2520Comparative%2520analysis%2520against%2520baseline%2520models%2520further%2520reveals%250Athat%2520the%2520proposed%2520approach%2520achieves%2520higher%2520memory%2520retention%2520efficiency%2520while%250Amaintaining%2520computational%2520feasibility.%2520The%2520architectural%2520modifications%250Aintegrate%2520seamlessly%2520into%2520existing%2520transformer-based%2520frameworks%252C%2520ensuring%250Astable%2520convergence%2520and%2520efficient%2520inference%2520without%2520sacrificing%2520scalability.%250AApplications%2520benefiting%2520from%2520improved%2520long-term%2520contextual%2520consistency%252C%2520such%2520as%250Adialogue%2520systems%2520and%2520document%2520summarization%252C%2520stand%2520to%2520gain%2520from%2520this%2520approach.%250AEmpirical%2520findings%2520suggest%2520that%2520dynamically%2520reinforced%2520memory%2520pathways%2520offer%2520a%250Apromising%2520alternative%2520to%2520conventional%2520memory%2520mechanisms%252C%2520addressing%250Alongstanding%2520limitations%2520in%2520extended%2520sequence%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Synaptic%20Resonance%20in%20Large%20Language%20Models%3A%20A%20Novel%20Approach%0A%20%20to%20Contextual%20Memory%20Integration&entry.906535625=George%20Applegarth%20and%20Christian%20Weatherstone%20and%20Maximilian%20Hollingsworth%20and%20Henry%20Middlebrook%20and%20Marcus%20Irvin&entry.1292438233=%20%20Contextual%20memory%20integration%20remains%20a%20high%20challenge%20in%20the%20development%20of%0Alanguage%20models%2C%20particularly%20in%20tasks%20that%20require%20maintaining%20coherence%20over%0Aextended%20sequences.%20Traditional%20approaches%2C%20such%20as%20self-attention%20mechanisms%0Aand%20memory-augmented%20architectures%2C%20often%20prioritize%20short-term%20dependencies%2C%0Aleading%20to%20fragmentation%20and%20inconsistency%20in%20long-range%20contextual%0Aunderstanding.%20Inspired%20by%20principles%20of%20synaptic%20plasticity%20observed%20in%0Abiological%20neural%20systems%2C%20a%20novel%20mechanism%2C%20Synaptic%20Resonance%2C%20is%20introduced%0Ato%20dynamically%20reinforce%20relevant%20memory%20pathways%20during%20training%20and%0Ainference.%20Unlike%20static%20memory%20representations%2C%20this%20mechanism%20continuously%0Aadjusts%20synaptic%20weight%20matrices%20based%20on%20contextual%20relevance%2C%20allowing%20for%0Aimproved%20information%20retention%20without%20excessive%20computational%20overhead.%0AEvaluations%20conducted%20on%20an%20open-source%20language%20model%20demonstrate%20reductions%0Ain%20perplexity%2C%20enhancements%20in%20contextual%20coherence%2C%20and%20increased%20robustness%0Aagainst%20input%20noise%2C%20highlighting%20the%20effectiveness%20of%20reinforcement-driven%0Amemory%20modulation.%20Comparative%20analysis%20against%20baseline%20models%20further%20reveals%0Athat%20the%20proposed%20approach%20achieves%20higher%20memory%20retention%20efficiency%20while%0Amaintaining%20computational%20feasibility.%20The%20architectural%20modifications%0Aintegrate%20seamlessly%20into%20existing%20transformer-based%20frameworks%2C%20ensuring%0Astable%20convergence%20and%20efficient%20inference%20without%20sacrificing%20scalability.%0AApplications%20benefiting%20from%20improved%20long-term%20contextual%20consistency%2C%20such%20as%0Adialogue%20systems%20and%20document%20summarization%2C%20stand%20to%20gain%20from%20this%20approach.%0AEmpirical%20findings%20suggest%20that%20dynamically%20reinforced%20memory%20pathways%20offer%20a%0Apromising%20alternative%20to%20conventional%20memory%20mechanisms%2C%20addressing%0Alongstanding%20limitations%20in%20extended%20sequence%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10699v2&entry.124074799=Read"},
{"title": "Autonomous Structural Memory Manipulation for Large Language Models\n  Using Hierarchical Embedding Augmentation", "author": "Derek Yotheringhay and Alistair Kirkland and Humphrey Kirkbride and Josiah Whitesteeple", "abstract": "  Transformative innovations in model architectures have introduced\nhierarchical embedding augmentation as a means to redefine the representation\nof tokens through multi-level semantic structures, offering enhanced\nadaptability to complex linguistic inputs. Autonomous structural memory\nmanipulation further advances this paradigm through dynamic memory reallocation\nmechanisms that prioritize critical contextual features while suppressing less\nrelevant information, enabling scalable and efficient performance across\ndiverse tasks. Experimental results reveal substantial improvements in\ncomputational efficiency, with marked reductions in processing overhead for\nlonger input sequences, achieved through memory reorganization strategies that\nadapt to evolving contextual requirements. Hierarchical embeddings not only\nimproved contextual alignment but also facilitated task generalization by\ncapturing relationships at varying semantic granularities, ensuring coherence\nacross layers without introducing significant computational redundancies.\nComparative analysis against baseline models demonstrated unique advantages in\naccuracy, efficiency, and interpretability, particularly in tasks requiring\ncomplex contextual understanding or domain-specific adaptability. The ability\nto dynamically adjust token representations and memory configurations\ncontributed to the model's robustness under varied and unpredictable input\nconditions. Applications benefiting from these advancements include\nmulti-domain generalization, interactive systems, and scenarios involving\nreal-time decision-making, where traditional static memory architectures often\nface limitations. The proposed methodology combines advanced embedding and\nmemory management strategies into a cohesive framework that addresses\nscalability challenges while preserving task-specific relevance.\n", "link": "http://arxiv.org/abs/2501.14119v2", "date": "2025-08-08", "relevancy": 2.7864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Structural%20Memory%20Manipulation%20for%20Large%20Language%20Models%0A%20%20Using%20Hierarchical%20Embedding%20Augmentation&body=Title%3A%20Autonomous%20Structural%20Memory%20Manipulation%20for%20Large%20Language%20Models%0A%20%20Using%20Hierarchical%20Embedding%20Augmentation%0AAuthor%3A%20Derek%20Yotheringhay%20and%20Alistair%20Kirkland%20and%20Humphrey%20Kirkbride%20and%20Josiah%20Whitesteeple%0AAbstract%3A%20%20%20Transformative%20innovations%20in%20model%20architectures%20have%20introduced%0Ahierarchical%20embedding%20augmentation%20as%20a%20means%20to%20redefine%20the%20representation%0Aof%20tokens%20through%20multi-level%20semantic%20structures%2C%20offering%20enhanced%0Aadaptability%20to%20complex%20linguistic%20inputs.%20Autonomous%20structural%20memory%0Amanipulation%20further%20advances%20this%20paradigm%20through%20dynamic%20memory%20reallocation%0Amechanisms%20that%20prioritize%20critical%20contextual%20features%20while%20suppressing%20less%0Arelevant%20information%2C%20enabling%20scalable%20and%20efficient%20performance%20across%0Adiverse%20tasks.%20Experimental%20results%20reveal%20substantial%20improvements%20in%0Acomputational%20efficiency%2C%20with%20marked%20reductions%20in%20processing%20overhead%20for%0Alonger%20input%20sequences%2C%20achieved%20through%20memory%20reorganization%20strategies%20that%0Aadapt%20to%20evolving%20contextual%20requirements.%20Hierarchical%20embeddings%20not%20only%0Aimproved%20contextual%20alignment%20but%20also%20facilitated%20task%20generalization%20by%0Acapturing%20relationships%20at%20varying%20semantic%20granularities%2C%20ensuring%20coherence%0Aacross%20layers%20without%20introducing%20significant%20computational%20redundancies.%0AComparative%20analysis%20against%20baseline%20models%20demonstrated%20unique%20advantages%20in%0Aaccuracy%2C%20efficiency%2C%20and%20interpretability%2C%20particularly%20in%20tasks%20requiring%0Acomplex%20contextual%20understanding%20or%20domain-specific%20adaptability.%20The%20ability%0Ato%20dynamically%20adjust%20token%20representations%20and%20memory%20configurations%0Acontributed%20to%20the%20model%27s%20robustness%20under%20varied%20and%20unpredictable%20input%0Aconditions.%20Applications%20benefiting%20from%20these%20advancements%20include%0Amulti-domain%20generalization%2C%20interactive%20systems%2C%20and%20scenarios%20involving%0Areal-time%20decision-making%2C%20where%20traditional%20static%20memory%20architectures%20often%0Aface%20limitations.%20The%20proposed%20methodology%20combines%20advanced%20embedding%20and%0Amemory%20management%20strategies%20into%20a%20cohesive%20framework%20that%20addresses%0Ascalability%20challenges%20while%20preserving%20task-specific%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Structural%2520Memory%2520Manipulation%2520for%2520Large%2520Language%2520Models%250A%2520%2520Using%2520Hierarchical%2520Embedding%2520Augmentation%26entry.906535625%3DDerek%2520Yotheringhay%2520and%2520Alistair%2520Kirkland%2520and%2520Humphrey%2520Kirkbride%2520and%2520Josiah%2520Whitesteeple%26entry.1292438233%3D%2520%2520Transformative%2520innovations%2520in%2520model%2520architectures%2520have%2520introduced%250Ahierarchical%2520embedding%2520augmentation%2520as%2520a%2520means%2520to%2520redefine%2520the%2520representation%250Aof%2520tokens%2520through%2520multi-level%2520semantic%2520structures%252C%2520offering%2520enhanced%250Aadaptability%2520to%2520complex%2520linguistic%2520inputs.%2520Autonomous%2520structural%2520memory%250Amanipulation%2520further%2520advances%2520this%2520paradigm%2520through%2520dynamic%2520memory%2520reallocation%250Amechanisms%2520that%2520prioritize%2520critical%2520contextual%2520features%2520while%2520suppressing%2520less%250Arelevant%2520information%252C%2520enabling%2520scalable%2520and%2520efficient%2520performance%2520across%250Adiverse%2520tasks.%2520Experimental%2520results%2520reveal%2520substantial%2520improvements%2520in%250Acomputational%2520efficiency%252C%2520with%2520marked%2520reductions%2520in%2520processing%2520overhead%2520for%250Alonger%2520input%2520sequences%252C%2520achieved%2520through%2520memory%2520reorganization%2520strategies%2520that%250Aadapt%2520to%2520evolving%2520contextual%2520requirements.%2520Hierarchical%2520embeddings%2520not%2520only%250Aimproved%2520contextual%2520alignment%2520but%2520also%2520facilitated%2520task%2520generalization%2520by%250Acapturing%2520relationships%2520at%2520varying%2520semantic%2520granularities%252C%2520ensuring%2520coherence%250Aacross%2520layers%2520without%2520introducing%2520significant%2520computational%2520redundancies.%250AComparative%2520analysis%2520against%2520baseline%2520models%2520demonstrated%2520unique%2520advantages%2520in%250Aaccuracy%252C%2520efficiency%252C%2520and%2520interpretability%252C%2520particularly%2520in%2520tasks%2520requiring%250Acomplex%2520contextual%2520understanding%2520or%2520domain-specific%2520adaptability.%2520The%2520ability%250Ato%2520dynamically%2520adjust%2520token%2520representations%2520and%2520memory%2520configurations%250Acontributed%2520to%2520the%2520model%2527s%2520robustness%2520under%2520varied%2520and%2520unpredictable%2520input%250Aconditions.%2520Applications%2520benefiting%2520from%2520these%2520advancements%2520include%250Amulti-domain%2520generalization%252C%2520interactive%2520systems%252C%2520and%2520scenarios%2520involving%250Areal-time%2520decision-making%252C%2520where%2520traditional%2520static%2520memory%2520architectures%2520often%250Aface%2520limitations.%2520The%2520proposed%2520methodology%2520combines%2520advanced%2520embedding%2520and%250Amemory%2520management%2520strategies%2520into%2520a%2520cohesive%2520framework%2520that%2520addresses%250Ascalability%2520challenges%2520while%2520preserving%2520task-specific%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Structural%20Memory%20Manipulation%20for%20Large%20Language%20Models%0A%20%20Using%20Hierarchical%20Embedding%20Augmentation&entry.906535625=Derek%20Yotheringhay%20and%20Alistair%20Kirkland%20and%20Humphrey%20Kirkbride%20and%20Josiah%20Whitesteeple&entry.1292438233=%20%20Transformative%20innovations%20in%20model%20architectures%20have%20introduced%0Ahierarchical%20embedding%20augmentation%20as%20a%20means%20to%20redefine%20the%20representation%0Aof%20tokens%20through%20multi-level%20semantic%20structures%2C%20offering%20enhanced%0Aadaptability%20to%20complex%20linguistic%20inputs.%20Autonomous%20structural%20memory%0Amanipulation%20further%20advances%20this%20paradigm%20through%20dynamic%20memory%20reallocation%0Amechanisms%20that%20prioritize%20critical%20contextual%20features%20while%20suppressing%20less%0Arelevant%20information%2C%20enabling%20scalable%20and%20efficient%20performance%20across%0Adiverse%20tasks.%20Experimental%20results%20reveal%20substantial%20improvements%20in%0Acomputational%20efficiency%2C%20with%20marked%20reductions%20in%20processing%20overhead%20for%0Alonger%20input%20sequences%2C%20achieved%20through%20memory%20reorganization%20strategies%20that%0Aadapt%20to%20evolving%20contextual%20requirements.%20Hierarchical%20embeddings%20not%20only%0Aimproved%20contextual%20alignment%20but%20also%20facilitated%20task%20generalization%20by%0Acapturing%20relationships%20at%20varying%20semantic%20granularities%2C%20ensuring%20coherence%0Aacross%20layers%20without%20introducing%20significant%20computational%20redundancies.%0AComparative%20analysis%20against%20baseline%20models%20demonstrated%20unique%20advantages%20in%0Aaccuracy%2C%20efficiency%2C%20and%20interpretability%2C%20particularly%20in%20tasks%20requiring%0Acomplex%20contextual%20understanding%20or%20domain-specific%20adaptability.%20The%20ability%0Ato%20dynamically%20adjust%20token%20representations%20and%20memory%20configurations%0Acontributed%20to%20the%20model%27s%20robustness%20under%20varied%20and%20unpredictable%20input%0Aconditions.%20Applications%20benefiting%20from%20these%20advancements%20include%0Amulti-domain%20generalization%2C%20interactive%20systems%2C%20and%20scenarios%20involving%0Areal-time%20decision-making%2C%20where%20traditional%20static%20memory%20architectures%20often%0Aface%20limitations.%20The%20proposed%20methodology%20combines%20advanced%20embedding%20and%0Amemory%20management%20strategies%20into%20a%20cohesive%20framework%20that%20addresses%0Ascalability%20challenges%20while%20preserving%20task-specific%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14119v2&entry.124074799=Read"},
{"title": "TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network\n  for Lipreading", "author": "Byung Hoon Lee and Wooseok Shin and Sung Won Han", "abstract": "  The word-level lipreading approach typically employs a two-stage framework\nwith separate frontend and backend architectures to model dynamic lip\nmovements. Each component has been extensively studied, and in the backend\narchitecture, temporal convolutional networks (TCNs) have been widely adopted\nin state-of-the-art methods. Recently, dense skip connections have been\nintroduced in TCNs to mitigate the limited density of the receptive field,\nthereby improving the modeling of complex temporal representations. However,\ntheir performance remains constrained owing to potential information loss\nregarding the continuous nature of lip movements, caused by blind spots in the\nreceptive field. To address this limitation, we propose TD3Net, a temporal\ndensely connected multi-dilated convolutional network that combines dense skip\nconnections and multi-dilated temporal convolutions as the backend\narchitecture. TD3Net covers a wide and dense receptive field without blind\nspots by applying different dilation factors to skip-connected features.\nExperimental results on a word-level lipreading task using two large publicly\navailable datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that\nthe proposed method achieves performance comparable to state-of-the-art\nmethods. It achieved higher accuracy with fewer parameters and lower\nfloating-point operations compared to existing TCN-based backend architectures.\nMoreover, visualization results suggest that our approach effectively utilizes\ndiverse temporal features while preserving temporal continuity, presenting\nnotable advantages in lipreading systems. The code is available at our GitHub\nrepository:\nhttps://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading\n", "link": "http://arxiv.org/abs/2506.16073v2", "date": "2025-08-08", "relevancy": 2.7833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.602}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD3Net%3A%20A%20Temporal%20Densely%20Connected%20Multi-Dilated%20Convolutional%20Network%0A%20%20for%20Lipreading&body=Title%3A%20TD3Net%3A%20A%20Temporal%20Densely%20Connected%20Multi-Dilated%20Convolutional%20Network%0A%20%20for%20Lipreading%0AAuthor%3A%20Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han%0AAbstract%3A%20%20%20The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%0Awith%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%0Amovements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%0Aarchitecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%0Ain%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%0Aintroduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%0Athereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%0Atheir%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%0Aregarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%0Areceptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%0Adensely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%0Aconnections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%0Aarchitecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%0Aspots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%0AExperimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%0Aavailable%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%0Athe%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%0Afloating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%0AMoreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%0Adiverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%0Anotable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%0Arepository%3A%0Ahttps%3A//github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD3Net%253A%2520A%2520Temporal%2520Densely%2520Connected%2520Multi-Dilated%2520Convolutional%2520Network%250A%2520%2520for%2520Lipreading%26entry.906535625%3DByung%2520Hoon%2520Lee%2520and%2520Wooseok%2520Shin%2520and%2520Sung%2520Won%2520Han%26entry.1292438233%3D%2520%2520The%2520word-level%2520lipreading%2520approach%2520typically%2520employs%2520a%2520two-stage%2520framework%250Awith%2520separate%2520frontend%2520and%2520backend%2520architectures%2520to%2520model%2520dynamic%2520lip%250Amovements.%2520Each%2520component%2520has%2520been%2520extensively%2520studied%252C%2520and%2520in%2520the%2520backend%250Aarchitecture%252C%2520temporal%2520convolutional%2520networks%2520%2528TCNs%2529%2520have%2520been%2520widely%2520adopted%250Ain%2520state-of-the-art%2520methods.%2520Recently%252C%2520dense%2520skip%2520connections%2520have%2520been%250Aintroduced%2520in%2520TCNs%2520to%2520mitigate%2520the%2520limited%2520density%2520of%2520the%2520receptive%2520field%252C%250Athereby%2520improving%2520the%2520modeling%2520of%2520complex%2520temporal%2520representations.%2520However%252C%250Atheir%2520performance%2520remains%2520constrained%2520owing%2520to%2520potential%2520information%2520loss%250Aregarding%2520the%2520continuous%2520nature%2520of%2520lip%2520movements%252C%2520caused%2520by%2520blind%2520spots%2520in%2520the%250Areceptive%2520field.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520TD3Net%252C%2520a%2520temporal%250Adensely%2520connected%2520multi-dilated%2520convolutional%2520network%2520that%2520combines%2520dense%2520skip%250Aconnections%2520and%2520multi-dilated%2520temporal%2520convolutions%2520as%2520the%2520backend%250Aarchitecture.%2520TD3Net%2520covers%2520a%2520wide%2520and%2520dense%2520receptive%2520field%2520without%2520blind%250Aspots%2520by%2520applying%2520different%2520dilation%2520factors%2520to%2520skip-connected%2520features.%250AExperimental%2520results%2520on%2520a%2520word-level%2520lipreading%2520task%2520using%2520two%2520large%2520publicly%250Aavailable%2520datasets%252C%2520Lip%2520Reading%2520in%2520the%2520Wild%2520%2528LRW%2529%2520and%2520LRW-1000%252C%2520indicate%2520that%250Athe%2520proposed%2520method%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%250Amethods.%2520It%2520achieved%2520higher%2520accuracy%2520with%2520fewer%2520parameters%2520and%2520lower%250Afloating-point%2520operations%2520compared%2520to%2520existing%2520TCN-based%2520backend%2520architectures.%250AMoreover%252C%2520visualization%2520results%2520suggest%2520that%2520our%2520approach%2520effectively%2520utilizes%250Adiverse%2520temporal%2520features%2520while%2520preserving%2520temporal%2520continuity%252C%2520presenting%250Anotable%2520advantages%2520in%2520lipreading%2520systems.%2520The%2520code%2520is%2520available%2520at%2520our%2520GitHub%250Arepository%253A%250Ahttps%253A//github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD3Net%3A%20A%20Temporal%20Densely%20Connected%20Multi-Dilated%20Convolutional%20Network%0A%20%20for%20Lipreading&entry.906535625=Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han&entry.1292438233=%20%20The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%0Awith%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%0Amovements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%0Aarchitecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%0Ain%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%0Aintroduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%0Athereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%0Atheir%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%0Aregarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%0Areceptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%0Adensely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%0Aconnections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%0Aarchitecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%0Aspots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%0AExperimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%0Aavailable%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%0Athe%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%0Afloating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%0AMoreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%0Adiverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%0Anotable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%0Arepository%3A%0Ahttps%3A//github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16073v2&entry.124074799=Read"},
{"title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context\n  Learning via Representation Engineering", "author": "Yanshu Li and Yi Cao and Hongyang He and Qisen Cheng and Xiang Fu and Xi Xiao and Tianyang Wang and Ruixiang Tang", "abstract": "  Multimodal in-context learning (ICL) equips Large Vision-language Models\n(LVLMs) with the ability to adapt to new tasks via multiple user-provided\ndemonstrations, without requiring any model parameter updates. However, its\neffectiveness is constrained by the token-intensive nature of multimodal inputs\nand the complexity of cross-modal few-shot reasoning, which together hinder\nLVLMs from extracting useful patterns from demonstrations. To address these\nchallenges, we propose \\textbf{M$^2$IV}, a novel representation engineering\napproach that replaces explicit token-level demonstrations with a set of\nlearnable Multimodal In-context Vectors directly injected into the residual\nstreams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA)\nand multi-layer perceptrons (MLP) in the ICL process, we design a training\nstrategy that enables M$^2$IV to perform fine-grained semantic distillation and\nrobust cross-modal representation learning. M$^2$IV not only improves\nperformance across diverse tasks and LVLMs but also significantly reduces token\noverhead, enabling graceful scaling to many-shot scenarios. To further enhance\nusability, we introduce \\textbf{VLibrary}, a repository that stores trained\nM$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer\npre-trained LVLMs in a customized manner that meets diverse requirements.\nExtensive experiments demonstrate that M$^2$IV consistently outperforms vanilla\nICL and prior representation engineering baselines, achieving an average\naccuracy gain of 3.74\\% with substantial improvements in overall efficiency.\n", "link": "http://arxiv.org/abs/2504.04633v2", "date": "2025-08-08", "relevancy": 2.7822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering&body=Title%3A%20M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering%0AAuthor%3A%20Yanshu%20Li%20and%20Yi%20Cao%20and%20Hongyang%20He%20and%20Qisen%20Cheng%20and%20Xiang%20Fu%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Multimodal%20in-context%20learning%20%28ICL%29%20equips%20Large%20Vision-language%20Models%0A%28LVLMs%29%20with%20the%20ability%20to%20adapt%20to%20new%20tasks%20via%20multiple%20user-provided%0Ademonstrations%2C%20without%20requiring%20any%20model%20parameter%20updates.%20However%2C%20its%0Aeffectiveness%20is%20constrained%20by%20the%20token-intensive%20nature%20of%20multimodal%20inputs%0Aand%20the%20complexity%20of%20cross-modal%20few-shot%20reasoning%2C%20which%20together%20hinder%0ALVLMs%20from%20extracting%20useful%20patterns%20from%20demonstrations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextbf%7BM%24%5E2%24IV%7D%2C%20a%20novel%20representation%20engineering%0Aapproach%20that%20replaces%20explicit%20token-level%20demonstrations%20with%20a%20set%20of%0Alearnable%20Multimodal%20In-context%20Vectors%20directly%20injected%20into%20the%20residual%0Astreams%20of%20LVLMs.%20By%20analyzing%20the%20distinct%20roles%20of%20multi-head%20attention%20%28MHA%29%0Aand%20multi-layer%20perceptrons%20%28MLP%29%20in%20the%20ICL%20process%2C%20we%20design%20a%20training%0Astrategy%20that%20enables%20M%24%5E2%24IV%20to%20perform%20fine-grained%20semantic%20distillation%20and%0Arobust%20cross-modal%20representation%20learning.%20M%24%5E2%24IV%20not%20only%20improves%0Aperformance%20across%20diverse%20tasks%20and%20LVLMs%20but%20also%20significantly%20reduces%20token%0Aoverhead%2C%20enabling%20graceful%20scaling%20to%20many-shot%20scenarios.%20To%20further%20enhance%0Ausability%2C%20we%20introduce%20%5Ctextbf%7BVLibrary%7D%2C%20a%20repository%20that%20stores%20trained%0AM%24%5E2%24IVs%20for%20flexible%20retrieval%20and%20injection.%20With%20VLibrary%2C%20users%20can%20steer%0Apre-trained%20LVLMs%20in%20a%20customized%20manner%20that%20meets%20diverse%20requirements.%0AExtensive%20experiments%20demonstrate%20that%20M%24%5E2%24IV%20consistently%20outperforms%20vanilla%0AICL%20and%20prior%20representation%20engineering%20baselines%2C%20achieving%20an%20average%0Aaccuracy%20gain%20of%203.74%5C%25%20with%20substantial%20improvements%20in%20overall%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E2%2524IV%253A%2520Towards%2520Efficient%2520and%2520Fine-grained%2520Multimodal%2520In-Context%250A%2520%2520Learning%2520via%2520Representation%2520Engineering%26entry.906535625%3DYanshu%2520Li%2520and%2520Yi%2520Cao%2520and%2520Hongyang%2520He%2520and%2520Qisen%2520Cheng%2520and%2520Xiang%2520Fu%2520and%2520Xi%2520Xiao%2520and%2520Tianyang%2520Wang%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Multimodal%2520in-context%2520learning%2520%2528ICL%2529%2520equips%2520Large%2520Vision-language%2520Models%250A%2528LVLMs%2529%2520with%2520the%2520ability%2520to%2520adapt%2520to%2520new%2520tasks%2520via%2520multiple%2520user-provided%250Ademonstrations%252C%2520without%2520requiring%2520any%2520model%2520parameter%2520updates.%2520However%252C%2520its%250Aeffectiveness%2520is%2520constrained%2520by%2520the%2520token-intensive%2520nature%2520of%2520multimodal%2520inputs%250Aand%2520the%2520complexity%2520of%2520cross-modal%2520few-shot%2520reasoning%252C%2520which%2520together%2520hinder%250ALVLMs%2520from%2520extracting%2520useful%2520patterns%2520from%2520demonstrations.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520%255Ctextbf%257BM%2524%255E2%2524IV%257D%252C%2520a%2520novel%2520representation%2520engineering%250Aapproach%2520that%2520replaces%2520explicit%2520token-level%2520demonstrations%2520with%2520a%2520set%2520of%250Alearnable%2520Multimodal%2520In-context%2520Vectors%2520directly%2520injected%2520into%2520the%2520residual%250Astreams%2520of%2520LVLMs.%2520By%2520analyzing%2520the%2520distinct%2520roles%2520of%2520multi-head%2520attention%2520%2528MHA%2529%250Aand%2520multi-layer%2520perceptrons%2520%2528MLP%2529%2520in%2520the%2520ICL%2520process%252C%2520we%2520design%2520a%2520training%250Astrategy%2520that%2520enables%2520M%2524%255E2%2524IV%2520to%2520perform%2520fine-grained%2520semantic%2520distillation%2520and%250Arobust%2520cross-modal%2520representation%2520learning.%2520M%2524%255E2%2524IV%2520not%2520only%2520improves%250Aperformance%2520across%2520diverse%2520tasks%2520and%2520LVLMs%2520but%2520also%2520significantly%2520reduces%2520token%250Aoverhead%252C%2520enabling%2520graceful%2520scaling%2520to%2520many-shot%2520scenarios.%2520To%2520further%2520enhance%250Ausability%252C%2520we%2520introduce%2520%255Ctextbf%257BVLibrary%257D%252C%2520a%2520repository%2520that%2520stores%2520trained%250AM%2524%255E2%2524IVs%2520for%2520flexible%2520retrieval%2520and%2520injection.%2520With%2520VLibrary%252C%2520users%2520can%2520steer%250Apre-trained%2520LVLMs%2520in%2520a%2520customized%2520manner%2520that%2520meets%2520diverse%2520requirements.%250AExtensive%2520experiments%2520demonstrate%2520that%2520M%2524%255E2%2524IV%2520consistently%2520outperforms%2520vanilla%250AICL%2520and%2520prior%2520representation%2520engineering%2520baselines%252C%2520achieving%2520an%2520average%250Aaccuracy%2520gain%2520of%25203.74%255C%2525%2520with%2520substantial%2520improvements%2520in%2520overall%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering&entry.906535625=Yanshu%20Li%20and%20Yi%20Cao%20and%20Hongyang%20He%20and%20Qisen%20Cheng%20and%20Xiang%20Fu%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Multimodal%20in-context%20learning%20%28ICL%29%20equips%20Large%20Vision-language%20Models%0A%28LVLMs%29%20with%20the%20ability%20to%20adapt%20to%20new%20tasks%20via%20multiple%20user-provided%0Ademonstrations%2C%20without%20requiring%20any%20model%20parameter%20updates.%20However%2C%20its%0Aeffectiveness%20is%20constrained%20by%20the%20token-intensive%20nature%20of%20multimodal%20inputs%0Aand%20the%20complexity%20of%20cross-modal%20few-shot%20reasoning%2C%20which%20together%20hinder%0ALVLMs%20from%20extracting%20useful%20patterns%20from%20demonstrations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextbf%7BM%24%5E2%24IV%7D%2C%20a%20novel%20representation%20engineering%0Aapproach%20that%20replaces%20explicit%20token-level%20demonstrations%20with%20a%20set%20of%0Alearnable%20Multimodal%20In-context%20Vectors%20directly%20injected%20into%20the%20residual%0Astreams%20of%20LVLMs.%20By%20analyzing%20the%20distinct%20roles%20of%20multi-head%20attention%20%28MHA%29%0Aand%20multi-layer%20perceptrons%20%28MLP%29%20in%20the%20ICL%20process%2C%20we%20design%20a%20training%0Astrategy%20that%20enables%20M%24%5E2%24IV%20to%20perform%20fine-grained%20semantic%20distillation%20and%0Arobust%20cross-modal%20representation%20learning.%20M%24%5E2%24IV%20not%20only%20improves%0Aperformance%20across%20diverse%20tasks%20and%20LVLMs%20but%20also%20significantly%20reduces%20token%0Aoverhead%2C%20enabling%20graceful%20scaling%20to%20many-shot%20scenarios.%20To%20further%20enhance%0Ausability%2C%20we%20introduce%20%5Ctextbf%7BVLibrary%7D%2C%20a%20repository%20that%20stores%20trained%0AM%24%5E2%24IVs%20for%20flexible%20retrieval%20and%20injection.%20With%20VLibrary%2C%20users%20can%20steer%0Apre-trained%20LVLMs%20in%20a%20customized%20manner%20that%20meets%20diverse%20requirements.%0AExtensive%20experiments%20demonstrate%20that%20M%24%5E2%24IV%20consistently%20outperforms%20vanilla%0AICL%20and%20prior%20representation%20engineering%20baselines%2C%20achieving%20an%20average%0Aaccuracy%20gain%20of%203.74%5C%25%20with%20substantial%20improvements%20in%20overall%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04633v2&entry.124074799=Read"},
{"title": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension", "author": "Colin Sisate and Alistair Goldfinch and Vincent Waterstone and Sebastian Kingsley and Mariana Blackthorn", "abstract": "  Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies.\n", "link": "http://arxiv.org/abs/2502.00048v2", "date": "2025-08-08", "relevancy": 2.7516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextually%20Entangled%20Gradient%20Mapping%20for%20Optimized%20LLM%20Comprehension&body=Title%3A%20Contextually%20Entangled%20Gradient%20Mapping%20for%20Optimized%20LLM%20Comprehension%0AAuthor%3A%20Colin%20Sisate%20and%20Alistair%20Goldfinch%20and%20Vincent%20Waterstone%20and%20Sebastian%20Kingsley%20and%20Mariana%20Blackthorn%0AAbstract%3A%20%20%20Contextually%20Entangled%20Gradient%20Mapping%20%28CEGM%29%20introduces%20a%20new%20approach%20to%0Agradient%20optimization%2C%20redefining%20the%20relationship%20between%20contextual%0Aembeddings%20and%20gradient%20updates%20to%20enhance%20semantic%20coherence%20and%20reasoning%0Acapabilities%20in%20neural%20architectures.%20By%20treating%20gradients%20as%20dynamic%20carriers%0Aof%20contextual%20dependencies%20rather%20than%20isolated%20numerical%20entities%2C%20the%0Aproposed%20methodology%20bridges%20critical%20gaps%20in%20existing%20optimization%20strategies.%0AThe%20integration%20of%20entangled%20gradient%20dynamics%20into%20a%20loss%20regularization%0Aframework%20demonstrated%20significant%20improvements%20in%20tasks%20involving%20long-form%0Areasoning%2C%20contextual%20retention%2C%20and%20adaptability%20to%20unseen%20domains.%0AExperimental%20evaluations%20showed%20that%20the%20CEGM-enhanced%20model%20consistently%0Aoutperformed%20baseline%20approaches%2C%20achieving%20higher%20accuracy%20in%20token-level%0Apredictions%20and%20greater%20resilience%20to%20noisy%20inputs.%20Practical%20implementations%0Ainvolved%20modifications%20to%20training%20pipelines%2C%20introducing%20entanglement%20layers%0Aand%20dynamic%20coefficient%20adjustments%20that%20seamlessly%20align%20with%20existing%0Aarchitectures.%20Results%20further%20highlighted%20reductions%20in%20semantic%20drift%20during%0Asequential%20transformations%20and%20improvements%20in%20embedding%20coherence%20across%0Aparaphrased%20sentences%2C%20showing%20the%20robustness%20and%20versatility%20of%20the%20proposed%0Amethodology.%20The%20findings%20demonstrate%20the%20broader%20implications%20of%20gradient%0Aentanglement%20for%20both%20theoretical%20advancements%20and%20practical%20applications%20in%0Aoptimization%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextually%2520Entangled%2520Gradient%2520Mapping%2520for%2520Optimized%2520LLM%2520Comprehension%26entry.906535625%3DColin%2520Sisate%2520and%2520Alistair%2520Goldfinch%2520and%2520Vincent%2520Waterstone%2520and%2520Sebastian%2520Kingsley%2520and%2520Mariana%2520Blackthorn%26entry.1292438233%3D%2520%2520Contextually%2520Entangled%2520Gradient%2520Mapping%2520%2528CEGM%2529%2520introduces%2520a%2520new%2520approach%2520to%250Agradient%2520optimization%252C%2520redefining%2520the%2520relationship%2520between%2520contextual%250Aembeddings%2520and%2520gradient%2520updates%2520to%2520enhance%2520semantic%2520coherence%2520and%2520reasoning%250Acapabilities%2520in%2520neural%2520architectures.%2520By%2520treating%2520gradients%2520as%2520dynamic%2520carriers%250Aof%2520contextual%2520dependencies%2520rather%2520than%2520isolated%2520numerical%2520entities%252C%2520the%250Aproposed%2520methodology%2520bridges%2520critical%2520gaps%2520in%2520existing%2520optimization%2520strategies.%250AThe%2520integration%2520of%2520entangled%2520gradient%2520dynamics%2520into%2520a%2520loss%2520regularization%250Aframework%2520demonstrated%2520significant%2520improvements%2520in%2520tasks%2520involving%2520long-form%250Areasoning%252C%2520contextual%2520retention%252C%2520and%2520adaptability%2520to%2520unseen%2520domains.%250AExperimental%2520evaluations%2520showed%2520that%2520the%2520CEGM-enhanced%2520model%2520consistently%250Aoutperformed%2520baseline%2520approaches%252C%2520achieving%2520higher%2520accuracy%2520in%2520token-level%250Apredictions%2520and%2520greater%2520resilience%2520to%2520noisy%2520inputs.%2520Practical%2520implementations%250Ainvolved%2520modifications%2520to%2520training%2520pipelines%252C%2520introducing%2520entanglement%2520layers%250Aand%2520dynamic%2520coefficient%2520adjustments%2520that%2520seamlessly%2520align%2520with%2520existing%250Aarchitectures.%2520Results%2520further%2520highlighted%2520reductions%2520in%2520semantic%2520drift%2520during%250Asequential%2520transformations%2520and%2520improvements%2520in%2520embedding%2520coherence%2520across%250Aparaphrased%2520sentences%252C%2520showing%2520the%2520robustness%2520and%2520versatility%2520of%2520the%2520proposed%250Amethodology.%2520The%2520findings%2520demonstrate%2520the%2520broader%2520implications%2520of%2520gradient%250Aentanglement%2520for%2520both%2520theoretical%2520advancements%2520and%2520practical%2520applications%2520in%250Aoptimization%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextually%20Entangled%20Gradient%20Mapping%20for%20Optimized%20LLM%20Comprehension&entry.906535625=Colin%20Sisate%20and%20Alistair%20Goldfinch%20and%20Vincent%20Waterstone%20and%20Sebastian%20Kingsley%20and%20Mariana%20Blackthorn&entry.1292438233=%20%20Contextually%20Entangled%20Gradient%20Mapping%20%28CEGM%29%20introduces%20a%20new%20approach%20to%0Agradient%20optimization%2C%20redefining%20the%20relationship%20between%20contextual%0Aembeddings%20and%20gradient%20updates%20to%20enhance%20semantic%20coherence%20and%20reasoning%0Acapabilities%20in%20neural%20architectures.%20By%20treating%20gradients%20as%20dynamic%20carriers%0Aof%20contextual%20dependencies%20rather%20than%20isolated%20numerical%20entities%2C%20the%0Aproposed%20methodology%20bridges%20critical%20gaps%20in%20existing%20optimization%20strategies.%0AThe%20integration%20of%20entangled%20gradient%20dynamics%20into%20a%20loss%20regularization%0Aframework%20demonstrated%20significant%20improvements%20in%20tasks%20involving%20long-form%0Areasoning%2C%20contextual%20retention%2C%20and%20adaptability%20to%20unseen%20domains.%0AExperimental%20evaluations%20showed%20that%20the%20CEGM-enhanced%20model%20consistently%0Aoutperformed%20baseline%20approaches%2C%20achieving%20higher%20accuracy%20in%20token-level%0Apredictions%20and%20greater%20resilience%20to%20noisy%20inputs.%20Practical%20implementations%0Ainvolved%20modifications%20to%20training%20pipelines%2C%20introducing%20entanglement%20layers%0Aand%20dynamic%20coefficient%20adjustments%20that%20seamlessly%20align%20with%20existing%0Aarchitectures.%20Results%20further%20highlighted%20reductions%20in%20semantic%20drift%20during%0Asequential%20transformations%20and%20improvements%20in%20embedding%20coherence%20across%0Aparaphrased%20sentences%2C%20showing%20the%20robustness%20and%20versatility%20of%20the%20proposed%0Amethodology.%20The%20findings%20demonstrate%20the%20broader%20implications%20of%20gradient%0Aentanglement%20for%20both%20theoretical%20advancements%20and%20practical%20applications%20in%0Aoptimization%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00048v2&entry.124074799=Read"},
{"title": "Neural Contextual Reinforcement Framework for Logical Structure Language\n  Generation", "author": "Marcus Irvin and William Cooper and Edward Hughes and Jessica Morgan and Christopher Hamilton", "abstract": "  The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment.\n", "link": "http://arxiv.org/abs/2501.11417v2", "date": "2025-08-08", "relevancy": 2.7344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Contextual%20Reinforcement%20Framework%20for%20Logical%20Structure%20Language%0A%20%20Generation&body=Title%3A%20Neural%20Contextual%20Reinforcement%20Framework%20for%20Logical%20Structure%20Language%0A%20%20Generation%0AAuthor%3A%20Marcus%20Irvin%20and%20William%20Cooper%20and%20Edward%20Hughes%20and%20Jessica%20Morgan%20and%20Christopher%20Hamilton%0AAbstract%3A%20%20%20The%20Neural%20Contextual%20Reinforcement%20Framework%20introduces%20an%20innovative%0Aapproach%20to%20enhancing%20the%20logical%20coherence%20and%20structural%20consistency%20of%20text%0Agenerated%20by%20large%20language%20models.%20Leveraging%20reinforcement%20learning%0Aprinciples%2C%20the%20framework%20integrates%20custom%20reward%20functions%20and%20dynamic%0Acontext%20alignment%20mechanisms%20to%20address%20challenges%20inherent%20in%20maintaining%0Along-range%20dependencies%20across%20extended%20sequences.%20The%20architecture%0Aincorporates%20multi-head%20attention%20layers%20and%20hierarchical%20encoding%20modules%2C%0Aenabling%20the%20model%20to%20produce%20outputs%20that%20align%20closely%20with%20human%0Aexpectations%20of%20logical%20structure%20and%20semantic%20flow.%20Quantitative%20evaluations%0Aacross%20diverse%20datasets%20demonstrate%20substantial%20improvements%20in%20coherence%0Ametrics%2C%20perplexity%20reduction%2C%20and%20semantic%20alignment%2C%20showcasing%20the%0Aframework%27s%20ability%20to%20outperform%20baseline%20models%20in%20both%20general%20and%0Adomain-specific%20tasks.%20Qualitative%20analyses%20further%20highlight%20the%20framework%27s%0Acapacity%20to%20generate%20text%20with%20improved%20narrative%20clarity%20and%20reduced%0Aredundancy%2C%20reflecting%20its%20effectiveness%20in%20balancing%20fluency%20with%20structural%0Aprecision.%20In%20addition%20to%20its%20performance%20gains%2C%20the%20framework%20exhibits%0Arobustness%20in%20handling%20noisy%20input%20data%20and%20scalability%20across%20varying%20model%0Asizes%2C%20reinforcing%20its%20versatility%20in%20practical%20applications.%20Experimental%0Aresults%20reveal%20that%20optimal%20context%20window%20sizes%20significantly%20influence%0Acoherence%20outcomes%2C%20showing%20the%20importance%20of%20architectural%20flexibility%20in%0Aadapting%20to%20diverse%20linguistic%20structures.%20Cross-lingual%20performance%0Aevaluations%20affirm%20the%20framework%27s%20adaptability%20to%20multiple%20languages%2C%0Aextending%20its%20utility%20beyond%20monolingual%20contexts.%20Resource%20efficiency%20analyses%0Aindicate%20a%20reduction%20in%20computational%20overhead%20compared%20to%20traditional%0Aapproaches%2C%20emphasizing%20the%20practicality%20of%20the%20framework%20for%20large-scale%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Contextual%2520Reinforcement%2520Framework%2520for%2520Logical%2520Structure%2520Language%250A%2520%2520Generation%26entry.906535625%3DMarcus%2520Irvin%2520and%2520William%2520Cooper%2520and%2520Edward%2520Hughes%2520and%2520Jessica%2520Morgan%2520and%2520Christopher%2520Hamilton%26entry.1292438233%3D%2520%2520The%2520Neural%2520Contextual%2520Reinforcement%2520Framework%2520introduces%2520an%2520innovative%250Aapproach%2520to%2520enhancing%2520the%2520logical%2520coherence%2520and%2520structural%2520consistency%2520of%2520text%250Agenerated%2520by%2520large%2520language%2520models.%2520Leveraging%2520reinforcement%2520learning%250Aprinciples%252C%2520the%2520framework%2520integrates%2520custom%2520reward%2520functions%2520and%2520dynamic%250Acontext%2520alignment%2520mechanisms%2520to%2520address%2520challenges%2520inherent%2520in%2520maintaining%250Along-range%2520dependencies%2520across%2520extended%2520sequences.%2520The%2520architecture%250Aincorporates%2520multi-head%2520attention%2520layers%2520and%2520hierarchical%2520encoding%2520modules%252C%250Aenabling%2520the%2520model%2520to%2520produce%2520outputs%2520that%2520align%2520closely%2520with%2520human%250Aexpectations%2520of%2520logical%2520structure%2520and%2520semantic%2520flow.%2520Quantitative%2520evaluations%250Aacross%2520diverse%2520datasets%2520demonstrate%2520substantial%2520improvements%2520in%2520coherence%250Ametrics%252C%2520perplexity%2520reduction%252C%2520and%2520semantic%2520alignment%252C%2520showcasing%2520the%250Aframework%2527s%2520ability%2520to%2520outperform%2520baseline%2520models%2520in%2520both%2520general%2520and%250Adomain-specific%2520tasks.%2520Qualitative%2520analyses%2520further%2520highlight%2520the%2520framework%2527s%250Acapacity%2520to%2520generate%2520text%2520with%2520improved%2520narrative%2520clarity%2520and%2520reduced%250Aredundancy%252C%2520reflecting%2520its%2520effectiveness%2520in%2520balancing%2520fluency%2520with%2520structural%250Aprecision.%2520In%2520addition%2520to%2520its%2520performance%2520gains%252C%2520the%2520framework%2520exhibits%250Arobustness%2520in%2520handling%2520noisy%2520input%2520data%2520and%2520scalability%2520across%2520varying%2520model%250Asizes%252C%2520reinforcing%2520its%2520versatility%2520in%2520practical%2520applications.%2520Experimental%250Aresults%2520reveal%2520that%2520optimal%2520context%2520window%2520sizes%2520significantly%2520influence%250Acoherence%2520outcomes%252C%2520showing%2520the%2520importance%2520of%2520architectural%2520flexibility%2520in%250Aadapting%2520to%2520diverse%2520linguistic%2520structures.%2520Cross-lingual%2520performance%250Aevaluations%2520affirm%2520the%2520framework%2527s%2520adaptability%2520to%2520multiple%2520languages%252C%250Aextending%2520its%2520utility%2520beyond%2520monolingual%2520contexts.%2520Resource%2520efficiency%2520analyses%250Aindicate%2520a%2520reduction%2520in%2520computational%2520overhead%2520compared%2520to%2520traditional%250Aapproaches%252C%2520emphasizing%2520the%2520practicality%2520of%2520the%2520framework%2520for%2520large-scale%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Contextual%20Reinforcement%20Framework%20for%20Logical%20Structure%20Language%0A%20%20Generation&entry.906535625=Marcus%20Irvin%20and%20William%20Cooper%20and%20Edward%20Hughes%20and%20Jessica%20Morgan%20and%20Christopher%20Hamilton&entry.1292438233=%20%20The%20Neural%20Contextual%20Reinforcement%20Framework%20introduces%20an%20innovative%0Aapproach%20to%20enhancing%20the%20logical%20coherence%20and%20structural%20consistency%20of%20text%0Agenerated%20by%20large%20language%20models.%20Leveraging%20reinforcement%20learning%0Aprinciples%2C%20the%20framework%20integrates%20custom%20reward%20functions%20and%20dynamic%0Acontext%20alignment%20mechanisms%20to%20address%20challenges%20inherent%20in%20maintaining%0Along-range%20dependencies%20across%20extended%20sequences.%20The%20architecture%0Aincorporates%20multi-head%20attention%20layers%20and%20hierarchical%20encoding%20modules%2C%0Aenabling%20the%20model%20to%20produce%20outputs%20that%20align%20closely%20with%20human%0Aexpectations%20of%20logical%20structure%20and%20semantic%20flow.%20Quantitative%20evaluations%0Aacross%20diverse%20datasets%20demonstrate%20substantial%20improvements%20in%20coherence%0Ametrics%2C%20perplexity%20reduction%2C%20and%20semantic%20alignment%2C%20showcasing%20the%0Aframework%27s%20ability%20to%20outperform%20baseline%20models%20in%20both%20general%20and%0Adomain-specific%20tasks.%20Qualitative%20analyses%20further%20highlight%20the%20framework%27s%0Acapacity%20to%20generate%20text%20with%20improved%20narrative%20clarity%20and%20reduced%0Aredundancy%2C%20reflecting%20its%20effectiveness%20in%20balancing%20fluency%20with%20structural%0Aprecision.%20In%20addition%20to%20its%20performance%20gains%2C%20the%20framework%20exhibits%0Arobustness%20in%20handling%20noisy%20input%20data%20and%20scalability%20across%20varying%20model%0Asizes%2C%20reinforcing%20its%20versatility%20in%20practical%20applications.%20Experimental%0Aresults%20reveal%20that%20optimal%20context%20window%20sizes%20significantly%20influence%0Acoherence%20outcomes%2C%20showing%20the%20importance%20of%20architectural%20flexibility%20in%0Aadapting%20to%20diverse%20linguistic%20structures.%20Cross-lingual%20performance%0Aevaluations%20affirm%20the%20framework%27s%20adaptability%20to%20multiple%20languages%2C%0Aextending%20its%20utility%20beyond%20monolingual%20contexts.%20Resource%20efficiency%20analyses%0Aindicate%20a%20reduction%20in%20computational%20overhead%20compared%20to%20traditional%0Aapproaches%2C%20emphasizing%20the%20practicality%20of%20the%20framework%20for%20large-scale%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11417v2&entry.124074799=Read"},
{"title": "Contextual Reinforcement in Multimodal Token Compression for Large\n  Language Models", "author": "Naderdel Piero and Zacharias Cromwell and Nathaniel Wainwright and Matthias Nethercott", "abstract": "  Effective token compression remains a critical challenge for scaling models\nto handle increasingly complex and diverse datasets. A novel mechanism based on\ncontextual reinforcement is introduced, dynamically adjusting token importance\nthrough interdependencies and semantic relevance. This approach enables\nsubstantial reductions in token usage while preserving the quality and\ncoherence of information representation. Incorporating graph-based algorithms\nand adaptive weighting, the method captures subtle contextual relationships\nacross textual and multimodal data, ensuring robust alignment and performance\nin downstream tasks. Evaluations across varied domains reveal significant\nimprovements in accuracy and semantic retention, particularly for tasks\nrequiring detailed cross-modal interactions. Memory usage analyses demonstrate\nimproved computational efficiency, with minimal overhead despite the additional\nreinforcement processes. Performance gains are further validated through error\ndistribution analyses, showing reduced semantic loss and syntactic\ninconsistencies compared to baseline models. The modular architecture ensures\ncompatibility with a wide range of open-source frameworks, facilitating\nscalable implementation for real-world applications. These findings highlight\nthe potential of contextual reinforcement in redefining token management\nstrategies and advancing large-scale model design.\n", "link": "http://arxiv.org/abs/2501.16658v2", "date": "2025-08-08", "relevancy": 2.7297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Reinforcement%20in%20Multimodal%20Token%20Compression%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20Contextual%20Reinforcement%20in%20Multimodal%20Token%20Compression%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Naderdel%20Piero%20and%20Zacharias%20Cromwell%20and%20Nathaniel%20Wainwright%20and%20Matthias%20Nethercott%0AAbstract%3A%20%20%20Effective%20token%20compression%20remains%20a%20critical%20challenge%20for%20scaling%20models%0Ato%20handle%20increasingly%20complex%20and%20diverse%20datasets.%20A%20novel%20mechanism%20based%20on%0Acontextual%20reinforcement%20is%20introduced%2C%20dynamically%20adjusting%20token%20importance%0Athrough%20interdependencies%20and%20semantic%20relevance.%20This%20approach%20enables%0Asubstantial%20reductions%20in%20token%20usage%20while%20preserving%20the%20quality%20and%0Acoherence%20of%20information%20representation.%20Incorporating%20graph-based%20algorithms%0Aand%20adaptive%20weighting%2C%20the%20method%20captures%20subtle%20contextual%20relationships%0Aacross%20textual%20and%20multimodal%20data%2C%20ensuring%20robust%20alignment%20and%20performance%0Ain%20downstream%20tasks.%20Evaluations%20across%20varied%20domains%20reveal%20significant%0Aimprovements%20in%20accuracy%20and%20semantic%20retention%2C%20particularly%20for%20tasks%0Arequiring%20detailed%20cross-modal%20interactions.%20Memory%20usage%20analyses%20demonstrate%0Aimproved%20computational%20efficiency%2C%20with%20minimal%20overhead%20despite%20the%20additional%0Areinforcement%20processes.%20Performance%20gains%20are%20further%20validated%20through%20error%0Adistribution%20analyses%2C%20showing%20reduced%20semantic%20loss%20and%20syntactic%0Ainconsistencies%20compared%20to%20baseline%20models.%20The%20modular%20architecture%20ensures%0Acompatibility%20with%20a%20wide%20range%20of%20open-source%20frameworks%2C%20facilitating%0Ascalable%20implementation%20for%20real-world%20applications.%20These%20findings%20highlight%0Athe%20potential%20of%20contextual%20reinforcement%20in%20redefining%20token%20management%0Astrategies%20and%20advancing%20large-scale%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Reinforcement%2520in%2520Multimodal%2520Token%2520Compression%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DNaderdel%2520Piero%2520and%2520Zacharias%2520Cromwell%2520and%2520Nathaniel%2520Wainwright%2520and%2520Matthias%2520Nethercott%26entry.1292438233%3D%2520%2520Effective%2520token%2520compression%2520remains%2520a%2520critical%2520challenge%2520for%2520scaling%2520models%250Ato%2520handle%2520increasingly%2520complex%2520and%2520diverse%2520datasets.%2520A%2520novel%2520mechanism%2520based%2520on%250Acontextual%2520reinforcement%2520is%2520introduced%252C%2520dynamically%2520adjusting%2520token%2520importance%250Athrough%2520interdependencies%2520and%2520semantic%2520relevance.%2520This%2520approach%2520enables%250Asubstantial%2520reductions%2520in%2520token%2520usage%2520while%2520preserving%2520the%2520quality%2520and%250Acoherence%2520of%2520information%2520representation.%2520Incorporating%2520graph-based%2520algorithms%250Aand%2520adaptive%2520weighting%252C%2520the%2520method%2520captures%2520subtle%2520contextual%2520relationships%250Aacross%2520textual%2520and%2520multimodal%2520data%252C%2520ensuring%2520robust%2520alignment%2520and%2520performance%250Ain%2520downstream%2520tasks.%2520Evaluations%2520across%2520varied%2520domains%2520reveal%2520significant%250Aimprovements%2520in%2520accuracy%2520and%2520semantic%2520retention%252C%2520particularly%2520for%2520tasks%250Arequiring%2520detailed%2520cross-modal%2520interactions.%2520Memory%2520usage%2520analyses%2520demonstrate%250Aimproved%2520computational%2520efficiency%252C%2520with%2520minimal%2520overhead%2520despite%2520the%2520additional%250Areinforcement%2520processes.%2520Performance%2520gains%2520are%2520further%2520validated%2520through%2520error%250Adistribution%2520analyses%252C%2520showing%2520reduced%2520semantic%2520loss%2520and%2520syntactic%250Ainconsistencies%2520compared%2520to%2520baseline%2520models.%2520The%2520modular%2520architecture%2520ensures%250Acompatibility%2520with%2520a%2520wide%2520range%2520of%2520open-source%2520frameworks%252C%2520facilitating%250Ascalable%2520implementation%2520for%2520real-world%2520applications.%2520These%2520findings%2520highlight%250Athe%2520potential%2520of%2520contextual%2520reinforcement%2520in%2520redefining%2520token%2520management%250Astrategies%2520and%2520advancing%2520large-scale%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Reinforcement%20in%20Multimodal%20Token%20Compression%20for%20Large%0A%20%20Language%20Models&entry.906535625=Naderdel%20Piero%20and%20Zacharias%20Cromwell%20and%20Nathaniel%20Wainwright%20and%20Matthias%20Nethercott&entry.1292438233=%20%20Effective%20token%20compression%20remains%20a%20critical%20challenge%20for%20scaling%20models%0Ato%20handle%20increasingly%20complex%20and%20diverse%20datasets.%20A%20novel%20mechanism%20based%20on%0Acontextual%20reinforcement%20is%20introduced%2C%20dynamically%20adjusting%20token%20importance%0Athrough%20interdependencies%20and%20semantic%20relevance.%20This%20approach%20enables%0Asubstantial%20reductions%20in%20token%20usage%20while%20preserving%20the%20quality%20and%0Acoherence%20of%20information%20representation.%20Incorporating%20graph-based%20algorithms%0Aand%20adaptive%20weighting%2C%20the%20method%20captures%20subtle%20contextual%20relationships%0Aacross%20textual%20and%20multimodal%20data%2C%20ensuring%20robust%20alignment%20and%20performance%0Ain%20downstream%20tasks.%20Evaluations%20across%20varied%20domains%20reveal%20significant%0Aimprovements%20in%20accuracy%20and%20semantic%20retention%2C%20particularly%20for%20tasks%0Arequiring%20detailed%20cross-modal%20interactions.%20Memory%20usage%20analyses%20demonstrate%0Aimproved%20computational%20efficiency%2C%20with%20minimal%20overhead%20despite%20the%20additional%0Areinforcement%20processes.%20Performance%20gains%20are%20further%20validated%20through%20error%0Adistribution%20analyses%2C%20showing%20reduced%20semantic%20loss%20and%20syntactic%0Ainconsistencies%20compared%20to%20baseline%20models.%20The%20modular%20architecture%20ensures%0Acompatibility%20with%20a%20wide%20range%20of%20open-source%20frameworks%2C%20facilitating%0Ascalable%20implementation%20for%20real-world%20applications.%20These%20findings%20highlight%0Athe%20potential%20of%20contextual%20reinforcement%20in%20redefining%20token%20management%0Astrategies%20and%20advancing%20large-scale%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16658v2&entry.124074799=Read"},
{"title": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment", "author": "Qinghua Yao and Xiangrui Xu and Zhize Li", "abstract": "  Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference.\n", "link": "http://arxiv.org/abs/2508.05568v2", "date": "2025-08-08", "relevancy": 2.6884, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment&body=Title%3A%20X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment%0AAuthor%3A%20Qinghua%20Yao%20and%20Xiangrui%20Xu%20and%20Zhize%20Li%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20learning%20by%0Aintegrating%20disjoint%20feature%20subsets%20from%20multiple%20clients/parties.%20However%2C%0AVFL%20typically%20faces%20two%20key%20challenges%3A%20i%29%20the%20requirement%20for%20perfectly%0Aaligned%20data%20samples%20across%20all%20clients%20%28missing%20features%20are%20not%20allowed%29%3B%20ii%29%0Athe%20requirement%20for%20joint%20collaborative%20inference/prediction%20involving%20all%0Aclients%20%28it%20does%20not%20support%20locally%20independent%20inference%20on%20a%20single%20client%29.%0ATo%20address%20these%20challenges%2C%20we%20propose%20X-VFL%2C%20a%20new%20VFL%20framework%20designed%20to%0Adeal%20with%20the%20non-aligned%20data%20samples%20with%20%28partially%29%20missing%20features%20and%20to%0Asupport%20locally%20independent%20inference%20of%20new%20data%20samples%20for%20each%20client.%20In%0Aparticular%2C%20we%20design%20two%20novel%20modules%20in%20X-VFL%3A%20Cross%20Completion%20%28XCom%29%20and%0ADecision%20Subspace%20Alignment%20%28DS-Align%29.%20XCom%20can%20complete/reconstruct%20missing%0Afeatures%20for%20non-aligned%20data%20samples%20by%20leveraging%20information%20from%20other%0Aclients.%20DS-Align%20aligns%20local%20features%20with%20completed%20and%20global%20features%0Aacross%20all%20clients%20within%20the%20decision%20subspace%2C%20thus%20enabling%20locally%0Aindependent%20inference%20at%20each%20client.%20Moreover%2C%20we%20provide%20convergence%20theorems%0Afor%20different%20algorithms%20used%20in%20training%20X-VFL%2C%20showing%20an%20%24O%281/%5Csqrt%7BT%7D%29%24%0Aconvergence%20rate%20for%20SGD-type%20algorithms%20and%20an%20%24O%281/T%29%24%20rate%20for%20PAGE-type%0Aalgorithms%2C%20where%20%24T%24%20denotes%20the%20number%20of%20training%20update%20steps.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20X-VFL%20significantly%0Aoutperforms%20existing%20methods%2C%20e.g.%2C%20achieving%20a%2015%25%20improvement%20in%20accuracy%20on%0Athe%20image%20CIFAR-10%20dataset%20and%20a%2043%25%20improvement%20on%20the%20medical%20MIMIC-III%0Adataset.%20These%20results%20validate%20the%20practical%20effectiveness%20and%20superiority%20of%0AX-VFL%2C%20particularly%20in%20scenarios%20involving%20partially%20missing%20features%20and%0Alocally%20independent%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-VFL%253A%2520A%2520New%2520Vertical%2520Federated%2520Learning%2520Framework%2520with%2520Cross%2520Completion%250A%2520%2520and%2520Decision%2520Subspace%2520Alignment%26entry.906535625%3DQinghua%2520Yao%2520and%2520Xiangrui%2520Xu%2520and%2520Zhize%2520Li%26entry.1292438233%3D%2520%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520enables%2520collaborative%2520learning%2520by%250Aintegrating%2520disjoint%2520feature%2520subsets%2520from%2520multiple%2520clients/parties.%2520However%252C%250AVFL%2520typically%2520faces%2520two%2520key%2520challenges%253A%2520i%2529%2520the%2520requirement%2520for%2520perfectly%250Aaligned%2520data%2520samples%2520across%2520all%2520clients%2520%2528missing%2520features%2520are%2520not%2520allowed%2529%253B%2520ii%2529%250Athe%2520requirement%2520for%2520joint%2520collaborative%2520inference/prediction%2520involving%2520all%250Aclients%2520%2528it%2520does%2520not%2520support%2520locally%2520independent%2520inference%2520on%2520a%2520single%2520client%2529.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520X-VFL%252C%2520a%2520new%2520VFL%2520framework%2520designed%2520to%250Adeal%2520with%2520the%2520non-aligned%2520data%2520samples%2520with%2520%2528partially%2529%2520missing%2520features%2520and%2520to%250Asupport%2520locally%2520independent%2520inference%2520of%2520new%2520data%2520samples%2520for%2520each%2520client.%2520In%250Aparticular%252C%2520we%2520design%2520two%2520novel%2520modules%2520in%2520X-VFL%253A%2520Cross%2520Completion%2520%2528XCom%2529%2520and%250ADecision%2520Subspace%2520Alignment%2520%2528DS-Align%2529.%2520XCom%2520can%2520complete/reconstruct%2520missing%250Afeatures%2520for%2520non-aligned%2520data%2520samples%2520by%2520leveraging%2520information%2520from%2520other%250Aclients.%2520DS-Align%2520aligns%2520local%2520features%2520with%2520completed%2520and%2520global%2520features%250Aacross%2520all%2520clients%2520within%2520the%2520decision%2520subspace%252C%2520thus%2520enabling%2520locally%250Aindependent%2520inference%2520at%2520each%2520client.%2520Moreover%252C%2520we%2520provide%2520convergence%2520theorems%250Afor%2520different%2520algorithms%2520used%2520in%2520training%2520X-VFL%252C%2520showing%2520an%2520%2524O%25281/%255Csqrt%257BT%257D%2529%2524%250Aconvergence%2520rate%2520for%2520SGD-type%2520algorithms%2520and%2520an%2520%2524O%25281/T%2529%2524%2520rate%2520for%2520PAGE-type%250Aalgorithms%252C%2520where%2520%2524T%2524%2520denotes%2520the%2520number%2520of%2520training%2520update%2520steps.%2520Extensive%250Aexperiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520X-VFL%2520significantly%250Aoutperforms%2520existing%2520methods%252C%2520e.g.%252C%2520achieving%2520a%252015%2525%2520improvement%2520in%2520accuracy%2520on%250Athe%2520image%2520CIFAR-10%2520dataset%2520and%2520a%252043%2525%2520improvement%2520on%2520the%2520medical%2520MIMIC-III%250Adataset.%2520These%2520results%2520validate%2520the%2520practical%2520effectiveness%2520and%2520superiority%2520of%250AX-VFL%252C%2520particularly%2520in%2520scenarios%2520involving%2520partially%2520missing%2520features%2520and%250Alocally%2520independent%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment&entry.906535625=Qinghua%20Yao%20and%20Xiangrui%20Xu%20and%20Zhize%20Li&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20learning%20by%0Aintegrating%20disjoint%20feature%20subsets%20from%20multiple%20clients/parties.%20However%2C%0AVFL%20typically%20faces%20two%20key%20challenges%3A%20i%29%20the%20requirement%20for%20perfectly%0Aaligned%20data%20samples%20across%20all%20clients%20%28missing%20features%20are%20not%20allowed%29%3B%20ii%29%0Athe%20requirement%20for%20joint%20collaborative%20inference/prediction%20involving%20all%0Aclients%20%28it%20does%20not%20support%20locally%20independent%20inference%20on%20a%20single%20client%29.%0ATo%20address%20these%20challenges%2C%20we%20propose%20X-VFL%2C%20a%20new%20VFL%20framework%20designed%20to%0Adeal%20with%20the%20non-aligned%20data%20samples%20with%20%28partially%29%20missing%20features%20and%20to%0Asupport%20locally%20independent%20inference%20of%20new%20data%20samples%20for%20each%20client.%20In%0Aparticular%2C%20we%20design%20two%20novel%20modules%20in%20X-VFL%3A%20Cross%20Completion%20%28XCom%29%20and%0ADecision%20Subspace%20Alignment%20%28DS-Align%29.%20XCom%20can%20complete/reconstruct%20missing%0Afeatures%20for%20non-aligned%20data%20samples%20by%20leveraging%20information%20from%20other%0Aclients.%20DS-Align%20aligns%20local%20features%20with%20completed%20and%20global%20features%0Aacross%20all%20clients%20within%20the%20decision%20subspace%2C%20thus%20enabling%20locally%0Aindependent%20inference%20at%20each%20client.%20Moreover%2C%20we%20provide%20convergence%20theorems%0Afor%20different%20algorithms%20used%20in%20training%20X-VFL%2C%20showing%20an%20%24O%281/%5Csqrt%7BT%7D%29%24%0Aconvergence%20rate%20for%20SGD-type%20algorithms%20and%20an%20%24O%281/T%29%24%20rate%20for%20PAGE-type%0Aalgorithms%2C%20where%20%24T%24%20denotes%20the%20number%20of%20training%20update%20steps.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20X-VFL%20significantly%0Aoutperforms%20existing%20methods%2C%20e.g.%2C%20achieving%20a%2015%25%20improvement%20in%20accuracy%20on%0Athe%20image%20CIFAR-10%20dataset%20and%20a%2043%25%20improvement%20on%20the%20medical%20MIMIC-III%0Adataset.%20These%20results%20validate%20the%20practical%20effectiveness%20and%20superiority%20of%0AX-VFL%2C%20particularly%20in%20scenarios%20involving%20partially%20missing%20features%20and%0Alocally%20independent%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05568v2&entry.124074799=Read"},
{"title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for\n  Continual Visual Instruction Tuning", "author": "Chang Che and Ziqi Wang and Pengwan Yang and Qi Wang and Hui Ma and Zenglin Shi", "abstract": "  Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.\n", "link": "http://arxiv.org/abs/2508.06202v1", "date": "2025-08-08", "relevancy": 2.6253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20in%20LoRA%3A%20Towards%20Parameter-Efficient%20Architecture%20Expansion%20for%0A%20%20Continual%20Visual%20Instruction%20Tuning&body=Title%3A%20LoRA%20in%20LoRA%3A%20Towards%20Parameter-Efficient%20Architecture%20Expansion%20for%0A%20%20Continual%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Chang%20Che%20and%20Ziqi%20Wang%20and%20Pengwan%20Yang%20and%20Qi%20Wang%20and%20Hui%20Ma%20and%20Zenglin%20Shi%0AAbstract%3A%20%20%20Continual%20Visual%20Instruction%20Tuning%20%28CVIT%29%20enables%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20incrementally%20learn%20new%20tasks%20over%20time.%20However%2C%20this%0Aprocess%20is%20challenged%20by%20catastrophic%20forgetting%2C%20where%20performance%20on%0Apreviously%20learned%20tasks%20deteriorates%20as%20the%20model%20adapts%20to%20new%20ones.%20A%20common%0Aapproach%20to%20mitigate%20forgetting%20is%20architecture%20expansion%2C%20which%20introduces%0Atask-specific%20modules%20to%20prevent%20interference.%20Yet%2C%20existing%20methods%20often%0Aexpand%20entire%20layers%20for%20each%20task%2C%20leading%20to%20significant%20parameter%20overhead%0Aand%20poor%20scalability.%20To%20overcome%20these%20issues%2C%20we%20introduce%20LoRA%20in%20LoRA%0A%28LiLoRA%29%2C%20a%20highly%20efficient%20architecture%20expansion%20method%20tailored%20for%20CVIT%20in%0AMLLMs.%20LiLoRA%20shares%20the%20LoRA%20matrix%20A%20across%20tasks%20to%20reduce%20redundancy%2C%0Aapplies%20an%20additional%20low-rank%20decomposition%20to%20matrix%20B%20to%20minimize%0Atask-specific%20parameters%2C%20and%20incorporates%20a%20cosine-regularized%20stability%20loss%0Ato%20preserve%20consistency%20in%20shared%20representations%20over%20time.%20Extensive%0Aexperiments%20on%20a%20diverse%20CVIT%20benchmark%20show%20that%20LiLoRA%20consistently%20achieves%0Asuperior%20performance%20in%20sequential%20task%20learning%20while%20significantly%20improving%0Aparameter%20efficiency%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520in%2520LoRA%253A%2520Towards%2520Parameter-Efficient%2520Architecture%2520Expansion%2520for%250A%2520%2520Continual%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DChang%2520Che%2520and%2520Ziqi%2520Wang%2520and%2520Pengwan%2520Yang%2520and%2520Qi%2520Wang%2520and%2520Hui%2520Ma%2520and%2520Zenglin%2520Shi%26entry.1292438233%3D%2520%2520Continual%2520Visual%2520Instruction%2520Tuning%2520%2528CVIT%2529%2520enables%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520to%2520incrementally%2520learn%2520new%2520tasks%2520over%2520time.%2520However%252C%2520this%250Aprocess%2520is%2520challenged%2520by%2520catastrophic%2520forgetting%252C%2520where%2520performance%2520on%250Apreviously%2520learned%2520tasks%2520deteriorates%2520as%2520the%2520model%2520adapts%2520to%2520new%2520ones.%2520A%2520common%250Aapproach%2520to%2520mitigate%2520forgetting%2520is%2520architecture%2520expansion%252C%2520which%2520introduces%250Atask-specific%2520modules%2520to%2520prevent%2520interference.%2520Yet%252C%2520existing%2520methods%2520often%250Aexpand%2520entire%2520layers%2520for%2520each%2520task%252C%2520leading%2520to%2520significant%2520parameter%2520overhead%250Aand%2520poor%2520scalability.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520LoRA%2520in%2520LoRA%250A%2528LiLoRA%2529%252C%2520a%2520highly%2520efficient%2520architecture%2520expansion%2520method%2520tailored%2520for%2520CVIT%2520in%250AMLLMs.%2520LiLoRA%2520shares%2520the%2520LoRA%2520matrix%2520A%2520across%2520tasks%2520to%2520reduce%2520redundancy%252C%250Aapplies%2520an%2520additional%2520low-rank%2520decomposition%2520to%2520matrix%2520B%2520to%2520minimize%250Atask-specific%2520parameters%252C%2520and%2520incorporates%2520a%2520cosine-regularized%2520stability%2520loss%250Ato%2520preserve%2520consistency%2520in%2520shared%2520representations%2520over%2520time.%2520Extensive%250Aexperiments%2520on%2520a%2520diverse%2520CVIT%2520benchmark%2520show%2520that%2520LiLoRA%2520consistently%2520achieves%250Asuperior%2520performance%2520in%2520sequential%2520task%2520learning%2520while%2520significantly%2520improving%250Aparameter%2520efficiency%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20in%20LoRA%3A%20Towards%20Parameter-Efficient%20Architecture%20Expansion%20for%0A%20%20Continual%20Visual%20Instruction%20Tuning&entry.906535625=Chang%20Che%20and%20Ziqi%20Wang%20and%20Pengwan%20Yang%20and%20Qi%20Wang%20and%20Hui%20Ma%20and%20Zenglin%20Shi&entry.1292438233=%20%20Continual%20Visual%20Instruction%20Tuning%20%28CVIT%29%20enables%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20incrementally%20learn%20new%20tasks%20over%20time.%20However%2C%20this%0Aprocess%20is%20challenged%20by%20catastrophic%20forgetting%2C%20where%20performance%20on%0Apreviously%20learned%20tasks%20deteriorates%20as%20the%20model%20adapts%20to%20new%20ones.%20A%20common%0Aapproach%20to%20mitigate%20forgetting%20is%20architecture%20expansion%2C%20which%20introduces%0Atask-specific%20modules%20to%20prevent%20interference.%20Yet%2C%20existing%20methods%20often%0Aexpand%20entire%20layers%20for%20each%20task%2C%20leading%20to%20significant%20parameter%20overhead%0Aand%20poor%20scalability.%20To%20overcome%20these%20issues%2C%20we%20introduce%20LoRA%20in%20LoRA%0A%28LiLoRA%29%2C%20a%20highly%20efficient%20architecture%20expansion%20method%20tailored%20for%20CVIT%20in%0AMLLMs.%20LiLoRA%20shares%20the%20LoRA%20matrix%20A%20across%20tasks%20to%20reduce%20redundancy%2C%0Aapplies%20an%20additional%20low-rank%20decomposition%20to%20matrix%20B%20to%20minimize%0Atask-specific%20parameters%2C%20and%20incorporates%20a%20cosine-regularized%20stability%20loss%0Ato%20preserve%20consistency%20in%20shared%20representations%20over%20time.%20Extensive%0Aexperiments%20on%20a%20diverse%20CVIT%20benchmark%20show%20that%20LiLoRA%20consistently%20achieves%0Asuperior%20performance%20in%20sequential%20task%20learning%20while%20significantly%20improving%0Aparameter%20efficiency%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06202v1&entry.124074799=Read"},
{"title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship\n  Classification", "author": "Ch Muhammad Awais and Marco Reggiannini and Davide Moroni and Oktay Karakus", "abstract": "  SAR ship classification faces the challenge of long-tailed datasets, which\ncomplicates the classification of underrepresented classes. Oversampling\nmethods have proven effective in addressing class imbalance in optical data. In\nthis paper, we evaluated the effect of oversampling in the feature space for\nSAR ship classification. We propose two novel algorithms inspired by the\nMajor-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two\npublic datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three\nstate-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.\nAdditionally, we also analyzed the impact of oversampling methods on different\nclass sizes. The results demonstrated the effectiveness of our novel methods\nover the original M2m and baselines, with an average F1-score increase of 8.82%\nfor FuSARShip and 4.44% for OpenSARShip.\n", "link": "http://arxiv.org/abs/2508.06420v1", "date": "2025-08-08", "relevancy": 2.6224, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Space%20Oversampling%20for%20Addressing%20Class%20Imbalance%20in%20SAR%20Ship%0A%20%20Classification&body=Title%3A%20Feature-Space%20Oversampling%20for%20Addressing%20Class%20Imbalance%20in%20SAR%20Ship%0A%20%20Classification%0AAuthor%3A%20Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus%0AAbstract%3A%20%20%20SAR%20ship%20classification%20faces%20the%20challenge%20of%20long-tailed%20datasets%2C%20which%0Acomplicates%20the%20classification%20of%20underrepresented%20classes.%20Oversampling%0Amethods%20have%20proven%20effective%20in%20addressing%20class%20imbalance%20in%20optical%20data.%20In%0Athis%20paper%2C%20we%20evaluated%20the%20effect%20of%20oversampling%20in%20the%20feature%20space%20for%0ASAR%20ship%20classification.%20We%20propose%20two%20novel%20algorithms%20inspired%20by%20the%0AMajor-to-minor%20%28M2m%29%20method%20M2m%24_f%24%2C%20M2m%24_u%24.%20The%20algorithms%20are%20tested%20on%20two%0Apublic%20datasets%2C%20OpenSARShip%20%286%20classes%29%20and%20FuSARShip%20%289%20classes%29%2C%20using%20three%0Astate-of-the-art%20models%20as%20feature%20extractors%3A%20ViT%2C%20VGG16%2C%20and%20ResNet50.%0AAdditionally%2C%20we%20also%20analyzed%20the%20impact%20of%20oversampling%20methods%20on%20different%0Aclass%20sizes.%20The%20results%20demonstrated%20the%20effectiveness%20of%20our%20novel%20methods%0Aover%20the%20original%20M2m%20and%20baselines%2C%20with%20an%20average%20F1-score%20increase%20of%208.82%25%0Afor%20FuSARShip%20and%204.44%25%20for%20OpenSARShip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Space%2520Oversampling%2520for%2520Addressing%2520Class%2520Imbalance%2520in%2520SAR%2520Ship%250A%2520%2520Classification%26entry.906535625%3DCh%2520Muhammad%2520Awais%2520and%2520Marco%2520Reggiannini%2520and%2520Davide%2520Moroni%2520and%2520Oktay%2520Karakus%26entry.1292438233%3D%2520%2520SAR%2520ship%2520classification%2520faces%2520the%2520challenge%2520of%2520long-tailed%2520datasets%252C%2520which%250Acomplicates%2520the%2520classification%2520of%2520underrepresented%2520classes.%2520Oversampling%250Amethods%2520have%2520proven%2520effective%2520in%2520addressing%2520class%2520imbalance%2520in%2520optical%2520data.%2520In%250Athis%2520paper%252C%2520we%2520evaluated%2520the%2520effect%2520of%2520oversampling%2520in%2520the%2520feature%2520space%2520for%250ASAR%2520ship%2520classification.%2520We%2520propose%2520two%2520novel%2520algorithms%2520inspired%2520by%2520the%250AMajor-to-minor%2520%2528M2m%2529%2520method%2520M2m%2524_f%2524%252C%2520M2m%2524_u%2524.%2520The%2520algorithms%2520are%2520tested%2520on%2520two%250Apublic%2520datasets%252C%2520OpenSARShip%2520%25286%2520classes%2529%2520and%2520FuSARShip%2520%25289%2520classes%2529%252C%2520using%2520three%250Astate-of-the-art%2520models%2520as%2520feature%2520extractors%253A%2520ViT%252C%2520VGG16%252C%2520and%2520ResNet50.%250AAdditionally%252C%2520we%2520also%2520analyzed%2520the%2520impact%2520of%2520oversampling%2520methods%2520on%2520different%250Aclass%2520sizes.%2520The%2520results%2520demonstrated%2520the%2520effectiveness%2520of%2520our%2520novel%2520methods%250Aover%2520the%2520original%2520M2m%2520and%2520baselines%252C%2520with%2520an%2520average%2520F1-score%2520increase%2520of%25208.82%2525%250Afor%2520FuSARShip%2520and%25204.44%2525%2520for%2520OpenSARShip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Space%20Oversampling%20for%20Addressing%20Class%20Imbalance%20in%20SAR%20Ship%0A%20%20Classification&entry.906535625=Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus&entry.1292438233=%20%20SAR%20ship%20classification%20faces%20the%20challenge%20of%20long-tailed%20datasets%2C%20which%0Acomplicates%20the%20classification%20of%20underrepresented%20classes.%20Oversampling%0Amethods%20have%20proven%20effective%20in%20addressing%20class%20imbalance%20in%20optical%20data.%20In%0Athis%20paper%2C%20we%20evaluated%20the%20effect%20of%20oversampling%20in%20the%20feature%20space%20for%0ASAR%20ship%20classification.%20We%20propose%20two%20novel%20algorithms%20inspired%20by%20the%0AMajor-to-minor%20%28M2m%29%20method%20M2m%24_f%24%2C%20M2m%24_u%24.%20The%20algorithms%20are%20tested%20on%20two%0Apublic%20datasets%2C%20OpenSARShip%20%286%20classes%29%20and%20FuSARShip%20%289%20classes%29%2C%20using%20three%0Astate-of-the-art%20models%20as%20feature%20extractors%3A%20ViT%2C%20VGG16%2C%20and%20ResNet50.%0AAdditionally%2C%20we%20also%20analyzed%20the%20impact%20of%20oversampling%20methods%20on%20different%0Aclass%20sizes.%20The%20results%20demonstrated%20the%20effectiveness%20of%20our%20novel%20methods%0Aover%20the%20original%20M2m%20and%20baselines%2C%20with%20an%20average%20F1-score%20increase%20of%208.82%25%0Afor%20FuSARShip%20and%204.44%25%20for%20OpenSARShip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06420v1&entry.124074799=Read"},
{"title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with\n  Multimodal Large Language Models", "author": "Han Yin and Yafeng Chen and Chong Deng and Luyao Cheng and Hui Wang and Chao-Hong Tan and Qian Chen and Wen Wang and Xiangang Li", "abstract": "  The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke\nwhen and what\" within an audio clip, which is a crucial task in various\nreal-world multi-speaker scenarios such as meeting transcription and dialogue\nsystems. Existing SDR systems typically adopt a cascaded framework, combining\nmultiple modules such as speaker diarization (SD) and automatic speech\nrecognition (ASR). The cascaded systems suffer from several limitations, such\nas error propagation, difficulty in handling overlapping speech, and lack of\njoint optimization for exploring the synergy between SD and ASR tasks. To\naddress these limitations, we introduce SpeakerLM, a unified multimodal large\nlanguage model for SDR that jointly performs SD and ASR in an end-to-end\nmanner. Moreover, to facilitate diverse real-world scenarios, we incorporate a\nflexible speaker registration mechanism into SpeakerLM, enabling SDR under\ndifferent speaker registration settings. SpeakerLM is progressively developed\nwith a multi-stage training strategy on large-scale real data. Extensive\nexperiments show that SpeakerLM demonstrates strong data scaling capability and\ngeneralizability, outperforming state-of-the-art cascaded baselines on both\nin-domain and out-of-domain public SDR benchmarks. Furthermore, experimental\nresults show that the proposed speaker registration mechanism effectively\nensures robust SDR performance of SpeakerLM across diverse speaker registration\nconditions and varying numbers of registered speakers.\n", "link": "http://arxiv.org/abs/2508.06372v1", "date": "2025-08-08", "relevancy": 2.5967, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpeakerLM%3A%20End-to-End%20Versatile%20Speaker%20Diarization%20and%20Recognition%20with%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20SpeakerLM%3A%20End-to-End%20Versatile%20Speaker%20Diarization%20and%20Recognition%20with%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Han%20Yin%20and%20Yafeng%20Chen%20and%20Chong%20Deng%20and%20Luyao%20Cheng%20and%20Hui%20Wang%20and%20Chao-Hong%20Tan%20and%20Qian%20Chen%20and%20Wen%20Wang%20and%20Xiangang%20Li%0AAbstract%3A%20%20%20The%20Speaker%20Diarization%20and%20Recognition%20%28SDR%29%20task%20aims%20to%20predict%20%22who%20spoke%0Awhen%20and%20what%22%20within%20an%20audio%20clip%2C%20which%20is%20a%20crucial%20task%20in%20various%0Areal-world%20multi-speaker%20scenarios%20such%20as%20meeting%20transcription%20and%20dialogue%0Asystems.%20Existing%20SDR%20systems%20typically%20adopt%20a%20cascaded%20framework%2C%20combining%0Amultiple%20modules%20such%20as%20speaker%20diarization%20%28SD%29%20and%20automatic%20speech%0Arecognition%20%28ASR%29.%20The%20cascaded%20systems%20suffer%20from%20several%20limitations%2C%20such%0Aas%20error%20propagation%2C%20difficulty%20in%20handling%20overlapping%20speech%2C%20and%20lack%20of%0Ajoint%20optimization%20for%20exploring%20the%20synergy%20between%20SD%20and%20ASR%20tasks.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SpeakerLM%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20for%20SDR%20that%20jointly%20performs%20SD%20and%20ASR%20in%20an%20end-to-end%0Amanner.%20Moreover%2C%20to%20facilitate%20diverse%20real-world%20scenarios%2C%20we%20incorporate%20a%0Aflexible%20speaker%20registration%20mechanism%20into%20SpeakerLM%2C%20enabling%20SDR%20under%0Adifferent%20speaker%20registration%20settings.%20SpeakerLM%20is%20progressively%20developed%0Awith%20a%20multi-stage%20training%20strategy%20on%20large-scale%20real%20data.%20Extensive%0Aexperiments%20show%20that%20SpeakerLM%20demonstrates%20strong%20data%20scaling%20capability%20and%0Ageneralizability%2C%20outperforming%20state-of-the-art%20cascaded%20baselines%20on%20both%0Ain-domain%20and%20out-of-domain%20public%20SDR%20benchmarks.%20Furthermore%2C%20experimental%0Aresults%20show%20that%20the%20proposed%20speaker%20registration%20mechanism%20effectively%0Aensures%20robust%20SDR%20performance%20of%20SpeakerLM%20across%20diverse%20speaker%20registration%0Aconditions%20and%20varying%20numbers%20of%20registered%20speakers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeakerLM%253A%2520End-to-End%2520Versatile%2520Speaker%2520Diarization%2520and%2520Recognition%2520with%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DHan%2520Yin%2520and%2520Yafeng%2520Chen%2520and%2520Chong%2520Deng%2520and%2520Luyao%2520Cheng%2520and%2520Hui%2520Wang%2520and%2520Chao-Hong%2520Tan%2520and%2520Qian%2520Chen%2520and%2520Wen%2520Wang%2520and%2520Xiangang%2520Li%26entry.1292438233%3D%2520%2520The%2520Speaker%2520Diarization%2520and%2520Recognition%2520%2528SDR%2529%2520task%2520aims%2520to%2520predict%2520%2522who%2520spoke%250Awhen%2520and%2520what%2522%2520within%2520an%2520audio%2520clip%252C%2520which%2520is%2520a%2520crucial%2520task%2520in%2520various%250Areal-world%2520multi-speaker%2520scenarios%2520such%2520as%2520meeting%2520transcription%2520and%2520dialogue%250Asystems.%2520Existing%2520SDR%2520systems%2520typically%2520adopt%2520a%2520cascaded%2520framework%252C%2520combining%250Amultiple%2520modules%2520such%2520as%2520speaker%2520diarization%2520%2528SD%2529%2520and%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529.%2520The%2520cascaded%2520systems%2520suffer%2520from%2520several%2520limitations%252C%2520such%250Aas%2520error%2520propagation%252C%2520difficulty%2520in%2520handling%2520overlapping%2520speech%252C%2520and%2520lack%2520of%250Ajoint%2520optimization%2520for%2520exploring%2520the%2520synergy%2520between%2520SD%2520and%2520ASR%2520tasks.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520SpeakerLM%252C%2520a%2520unified%2520multimodal%2520large%250Alanguage%2520model%2520for%2520SDR%2520that%2520jointly%2520performs%2520SD%2520and%2520ASR%2520in%2520an%2520end-to-end%250Amanner.%2520Moreover%252C%2520to%2520facilitate%2520diverse%2520real-world%2520scenarios%252C%2520we%2520incorporate%2520a%250Aflexible%2520speaker%2520registration%2520mechanism%2520into%2520SpeakerLM%252C%2520enabling%2520SDR%2520under%250Adifferent%2520speaker%2520registration%2520settings.%2520SpeakerLM%2520is%2520progressively%2520developed%250Awith%2520a%2520multi-stage%2520training%2520strategy%2520on%2520large-scale%2520real%2520data.%2520Extensive%250Aexperiments%2520show%2520that%2520SpeakerLM%2520demonstrates%2520strong%2520data%2520scaling%2520capability%2520and%250Ageneralizability%252C%2520outperforming%2520state-of-the-art%2520cascaded%2520baselines%2520on%2520both%250Ain-domain%2520and%2520out-of-domain%2520public%2520SDR%2520benchmarks.%2520Furthermore%252C%2520experimental%250Aresults%2520show%2520that%2520the%2520proposed%2520speaker%2520registration%2520mechanism%2520effectively%250Aensures%2520robust%2520SDR%2520performance%2520of%2520SpeakerLM%2520across%2520diverse%2520speaker%2520registration%250Aconditions%2520and%2520varying%2520numbers%2520of%2520registered%2520speakers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpeakerLM%3A%20End-to-End%20Versatile%20Speaker%20Diarization%20and%20Recognition%20with%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Han%20Yin%20and%20Yafeng%20Chen%20and%20Chong%20Deng%20and%20Luyao%20Cheng%20and%20Hui%20Wang%20and%20Chao-Hong%20Tan%20and%20Qian%20Chen%20and%20Wen%20Wang%20and%20Xiangang%20Li&entry.1292438233=%20%20The%20Speaker%20Diarization%20and%20Recognition%20%28SDR%29%20task%20aims%20to%20predict%20%22who%20spoke%0Awhen%20and%20what%22%20within%20an%20audio%20clip%2C%20which%20is%20a%20crucial%20task%20in%20various%0Areal-world%20multi-speaker%20scenarios%20such%20as%20meeting%20transcription%20and%20dialogue%0Asystems.%20Existing%20SDR%20systems%20typically%20adopt%20a%20cascaded%20framework%2C%20combining%0Amultiple%20modules%20such%20as%20speaker%20diarization%20%28SD%29%20and%20automatic%20speech%0Arecognition%20%28ASR%29.%20The%20cascaded%20systems%20suffer%20from%20several%20limitations%2C%20such%0Aas%20error%20propagation%2C%20difficulty%20in%20handling%20overlapping%20speech%2C%20and%20lack%20of%0Ajoint%20optimization%20for%20exploring%20the%20synergy%20between%20SD%20and%20ASR%20tasks.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SpeakerLM%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20for%20SDR%20that%20jointly%20performs%20SD%20and%20ASR%20in%20an%20end-to-end%0Amanner.%20Moreover%2C%20to%20facilitate%20diverse%20real-world%20scenarios%2C%20we%20incorporate%20a%0Aflexible%20speaker%20registration%20mechanism%20into%20SpeakerLM%2C%20enabling%20SDR%20under%0Adifferent%20speaker%20registration%20settings.%20SpeakerLM%20is%20progressively%20developed%0Awith%20a%20multi-stage%20training%20strategy%20on%20large-scale%20real%20data.%20Extensive%0Aexperiments%20show%20that%20SpeakerLM%20demonstrates%20strong%20data%20scaling%20capability%20and%0Ageneralizability%2C%20outperforming%20state-of-the-art%20cascaded%20baselines%20on%20both%0Ain-domain%20and%20out-of-domain%20public%20SDR%20benchmarks.%20Furthermore%2C%20experimental%0Aresults%20show%20that%20the%20proposed%20speaker%20registration%20mechanism%20effectively%0Aensures%20robust%20SDR%20performance%20of%20SpeakerLM%20across%20diverse%20speaker%20registration%0Aconditions%20and%20varying%20numbers%20of%20registered%20speakers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06372v1&entry.124074799=Read"},
{"title": "Real-Time 3D Vision-Language Embedding Mapping", "author": "Christian Rauch and Bj\u00f6rn Ellensohn and Linus Nwankwo and Vedant Dave and Elmar Rueckert", "abstract": "  A metric-accurate semantic 3D representation is essential for many robotic\ntasks. This work proposes a simple, yet powerful, way to integrate the 2D\nembeddings of a Vision-Language Model in a metric-accurate 3D representation at\nreal-time. We combine a local embedding masking strategy, for a more distinct\nembedding distribution, with a confidence-weighted 3D integration for more\nreliable 3D embeddings. The resulting metric-accurate embedding representation\nis task-agnostic and can represent semantic concepts on a global multi-room, as\nwell as on a local object-level. This enables a variety of interactive robotic\napplications that require the localisation of objects-of-interest via natural\nlanguage. We evaluate our approach on a variety of real-world sequences and\ndemonstrate that these strategies achieve a more accurate object-of-interest\nlocalisation while improving the runtime performance in order to meet our\nreal-time constraints. We further demonstrate the versatility of our approach\nin a variety of interactive handheld, mobile robotics and manipulation tasks,\nrequiring only raw image data.\n", "link": "http://arxiv.org/abs/2508.06291v1", "date": "2025-08-08", "relevancy": 2.5924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%203D%20Vision-Language%20Embedding%20Mapping&body=Title%3A%20Real-Time%203D%20Vision-Language%20Embedding%20Mapping%0AAuthor%3A%20Christian%20Rauch%20and%20Bj%C3%B6rn%20Ellensohn%20and%20Linus%20Nwankwo%20and%20Vedant%20Dave%20and%20Elmar%20Rueckert%0AAbstract%3A%20%20%20A%20metric-accurate%20semantic%203D%20representation%20is%20essential%20for%20many%20robotic%0Atasks.%20This%20work%20proposes%20a%20simple%2C%20yet%20powerful%2C%20way%20to%20integrate%20the%202D%0Aembeddings%20of%20a%20Vision-Language%20Model%20in%20a%20metric-accurate%203D%20representation%20at%0Areal-time.%20We%20combine%20a%20local%20embedding%20masking%20strategy%2C%20for%20a%20more%20distinct%0Aembedding%20distribution%2C%20with%20a%20confidence-weighted%203D%20integration%20for%20more%0Areliable%203D%20embeddings.%20The%20resulting%20metric-accurate%20embedding%20representation%0Ais%20task-agnostic%20and%20can%20represent%20semantic%20concepts%20on%20a%20global%20multi-room%2C%20as%0Awell%20as%20on%20a%20local%20object-level.%20This%20enables%20a%20variety%20of%20interactive%20robotic%0Aapplications%20that%20require%20the%20localisation%20of%20objects-of-interest%20via%20natural%0Alanguage.%20We%20evaluate%20our%20approach%20on%20a%20variety%20of%20real-world%20sequences%20and%0Ademonstrate%20that%20these%20strategies%20achieve%20a%20more%20accurate%20object-of-interest%0Alocalisation%20while%20improving%20the%20runtime%20performance%20in%20order%20to%20meet%20our%0Areal-time%20constraints.%20We%20further%20demonstrate%20the%20versatility%20of%20our%20approach%0Ain%20a%20variety%20of%20interactive%20handheld%2C%20mobile%20robotics%20and%20manipulation%20tasks%2C%0Arequiring%20only%20raw%20image%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%25203D%2520Vision-Language%2520Embedding%2520Mapping%26entry.906535625%3DChristian%2520Rauch%2520and%2520Bj%25C3%25B6rn%2520Ellensohn%2520and%2520Linus%2520Nwankwo%2520and%2520Vedant%2520Dave%2520and%2520Elmar%2520Rueckert%26entry.1292438233%3D%2520%2520A%2520metric-accurate%2520semantic%25203D%2520representation%2520is%2520essential%2520for%2520many%2520robotic%250Atasks.%2520This%2520work%2520proposes%2520a%2520simple%252C%2520yet%2520powerful%252C%2520way%2520to%2520integrate%2520the%25202D%250Aembeddings%2520of%2520a%2520Vision-Language%2520Model%2520in%2520a%2520metric-accurate%25203D%2520representation%2520at%250Areal-time.%2520We%2520combine%2520a%2520local%2520embedding%2520masking%2520strategy%252C%2520for%2520a%2520more%2520distinct%250Aembedding%2520distribution%252C%2520with%2520a%2520confidence-weighted%25203D%2520integration%2520for%2520more%250Areliable%25203D%2520embeddings.%2520The%2520resulting%2520metric-accurate%2520embedding%2520representation%250Ais%2520task-agnostic%2520and%2520can%2520represent%2520semantic%2520concepts%2520on%2520a%2520global%2520multi-room%252C%2520as%250Awell%2520as%2520on%2520a%2520local%2520object-level.%2520This%2520enables%2520a%2520variety%2520of%2520interactive%2520robotic%250Aapplications%2520that%2520require%2520the%2520localisation%2520of%2520objects-of-interest%2520via%2520natural%250Alanguage.%2520We%2520evaluate%2520our%2520approach%2520on%2520a%2520variety%2520of%2520real-world%2520sequences%2520and%250Ademonstrate%2520that%2520these%2520strategies%2520achieve%2520a%2520more%2520accurate%2520object-of-interest%250Alocalisation%2520while%2520improving%2520the%2520runtime%2520performance%2520in%2520order%2520to%2520meet%2520our%250Areal-time%2520constraints.%2520We%2520further%2520demonstrate%2520the%2520versatility%2520of%2520our%2520approach%250Ain%2520a%2520variety%2520of%2520interactive%2520handheld%252C%2520mobile%2520robotics%2520and%2520manipulation%2520tasks%252C%250Arequiring%2520only%2520raw%2520image%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%203D%20Vision-Language%20Embedding%20Mapping&entry.906535625=Christian%20Rauch%20and%20Bj%C3%B6rn%20Ellensohn%20and%20Linus%20Nwankwo%20and%20Vedant%20Dave%20and%20Elmar%20Rueckert&entry.1292438233=%20%20A%20metric-accurate%20semantic%203D%20representation%20is%20essential%20for%20many%20robotic%0Atasks.%20This%20work%20proposes%20a%20simple%2C%20yet%20powerful%2C%20way%20to%20integrate%20the%202D%0Aembeddings%20of%20a%20Vision-Language%20Model%20in%20a%20metric-accurate%203D%20representation%20at%0Areal-time.%20We%20combine%20a%20local%20embedding%20masking%20strategy%2C%20for%20a%20more%20distinct%0Aembedding%20distribution%2C%20with%20a%20confidence-weighted%203D%20integration%20for%20more%0Areliable%203D%20embeddings.%20The%20resulting%20metric-accurate%20embedding%20representation%0Ais%20task-agnostic%20and%20can%20represent%20semantic%20concepts%20on%20a%20global%20multi-room%2C%20as%0Awell%20as%20on%20a%20local%20object-level.%20This%20enables%20a%20variety%20of%20interactive%20robotic%0Aapplications%20that%20require%20the%20localisation%20of%20objects-of-interest%20via%20natural%0Alanguage.%20We%20evaluate%20our%20approach%20on%20a%20variety%20of%20real-world%20sequences%20and%0Ademonstrate%20that%20these%20strategies%20achieve%20a%20more%20accurate%20object-of-interest%0Alocalisation%20while%20improving%20the%20runtime%20performance%20in%20order%20to%20meet%20our%0Areal-time%20constraints.%20We%20further%20demonstrate%20the%20versatility%20of%20our%20approach%0Ain%20a%20variety%20of%20interactive%20handheld%2C%20mobile%20robotics%20and%20manipulation%20tasks%2C%0Arequiring%20only%20raw%20image%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06291v1&entry.124074799=Read"},
{"title": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical\n  Reasoning in Language Models", "author": "Shubhra Mishra and Gabriel Poesia and Noah D. Goodman", "abstract": "  Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning.\n", "link": "http://arxiv.org/abs/2407.00900v2", "date": "2025-08-08", "relevancy": 2.5863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Next-Token%20to%20Mathematics%3A%20The%20Learning%20Dynamics%20of%20Mathematical%0A%20%20Reasoning%20in%20Language%20Models&body=Title%3A%20From%20Next-Token%20to%20Mathematics%3A%20The%20Learning%20Dynamics%20of%20Mathematical%0A%20%20Reasoning%20in%20Language%20Models%0AAuthor%3A%20Shubhra%20Mishra%20and%20Gabriel%20Poesia%20and%20Noah%20D.%20Goodman%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20solely%20trained%20on%20next-token%20prediction%20learn%20to%0Asolve%20a%20wide%20range%20of%20problems%20involving%20mathematical%20reasoning.%20But%20how%20does%0Athis%20ability%20evolve%20during%20training%3F%20We%20show%20the%20first%20analysis%20of%20how%0Amathematical%20reasoning%20abilities%20of%20several%20open-weight%20LLMs%20develop%20during%0Apre-training%20and%20post-training.%20To%20this%20end%2C%20we%20construct%20MathCAMPS%2C%20a%0Asynthetic%20dataset%20of%20novel%20mathematical%20reasoning%20problems%20grounded%20in%2044%0Afine-grained%20skills%20taken%20from%20the%20Common%20Core%20curriculum%20from%20K%20to%208th%20grades.%0AIn%20one%20experiment%2C%20we%20show%20that%20mathematical%20skills%20are%20learned%20during%0Apre-training%20in%20an%20order%20that%20measurably%20correlates%20with%20the%20human-designed%0Acurriculum%2C%20even%20though%20training%20data%20are%20randomly%20ordered.%20We%20also%20show%20a%0Adetailed%20analysis%20of%20which%20mathematical%20abilities%20benefit%20from%20instruction%0Atuning%2C%20a%20widely%20used%20post-training%20method%20and%2C%20in%20contrast%2C%20which%20skills%0Asuffer.%20Our%20work%20paves%20the%20way%20for%20an%20empirical%20understanding%20of%20LLM%20training%0Adynamics%20in%20relation%20to%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Next-Token%2520to%2520Mathematics%253A%2520The%2520Learning%2520Dynamics%2520of%2520Mathematical%250A%2520%2520Reasoning%2520in%2520Language%2520Models%26entry.906535625%3DShubhra%2520Mishra%2520and%2520Gabriel%2520Poesia%2520and%2520Noah%2520D.%2520Goodman%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520solely%2520trained%2520on%2520next-token%2520prediction%2520learn%2520to%250Asolve%2520a%2520wide%2520range%2520of%2520problems%2520involving%2520mathematical%2520reasoning.%2520But%2520how%2520does%250Athis%2520ability%2520evolve%2520during%2520training%253F%2520We%2520show%2520the%2520first%2520analysis%2520of%2520how%250Amathematical%2520reasoning%2520abilities%2520of%2520several%2520open-weight%2520LLMs%2520develop%2520during%250Apre-training%2520and%2520post-training.%2520To%2520this%2520end%252C%2520we%2520construct%2520MathCAMPS%252C%2520a%250Asynthetic%2520dataset%2520of%2520novel%2520mathematical%2520reasoning%2520problems%2520grounded%2520in%252044%250Afine-grained%2520skills%2520taken%2520from%2520the%2520Common%2520Core%2520curriculum%2520from%2520K%2520to%25208th%2520grades.%250AIn%2520one%2520experiment%252C%2520we%2520show%2520that%2520mathematical%2520skills%2520are%2520learned%2520during%250Apre-training%2520in%2520an%2520order%2520that%2520measurably%2520correlates%2520with%2520the%2520human-designed%250Acurriculum%252C%2520even%2520though%2520training%2520data%2520are%2520randomly%2520ordered.%2520We%2520also%2520show%2520a%250Adetailed%2520analysis%2520of%2520which%2520mathematical%2520abilities%2520benefit%2520from%2520instruction%250Atuning%252C%2520a%2520widely%2520used%2520post-training%2520method%2520and%252C%2520in%2520contrast%252C%2520which%2520skills%250Asuffer.%2520Our%2520work%2520paves%2520the%2520way%2520for%2520an%2520empirical%2520understanding%2520of%2520LLM%2520training%250Adynamics%2520in%2520relation%2520to%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Next-Token%20to%20Mathematics%3A%20The%20Learning%20Dynamics%20of%20Mathematical%0A%20%20Reasoning%20in%20Language%20Models&entry.906535625=Shubhra%20Mishra%20and%20Gabriel%20Poesia%20and%20Noah%20D.%20Goodman&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20solely%20trained%20on%20next-token%20prediction%20learn%20to%0Asolve%20a%20wide%20range%20of%20problems%20involving%20mathematical%20reasoning.%20But%20how%20does%0Athis%20ability%20evolve%20during%20training%3F%20We%20show%20the%20first%20analysis%20of%20how%0Amathematical%20reasoning%20abilities%20of%20several%20open-weight%20LLMs%20develop%20during%0Apre-training%20and%20post-training.%20To%20this%20end%2C%20we%20construct%20MathCAMPS%2C%20a%0Asynthetic%20dataset%20of%20novel%20mathematical%20reasoning%20problems%20grounded%20in%2044%0Afine-grained%20skills%20taken%20from%20the%20Common%20Core%20curriculum%20from%20K%20to%208th%20grades.%0AIn%20one%20experiment%2C%20we%20show%20that%20mathematical%20skills%20are%20learned%20during%0Apre-training%20in%20an%20order%20that%20measurably%20correlates%20with%20the%20human-designed%0Acurriculum%2C%20even%20though%20training%20data%20are%20randomly%20ordered.%20We%20also%20show%20a%0Adetailed%20analysis%20of%20which%20mathematical%20abilities%20benefit%20from%20instruction%0Atuning%2C%20a%20widely%20used%20post-training%20method%20and%2C%20in%20contrast%2C%20which%20skills%0Asuffer.%20Our%20work%20paves%20the%20way%20for%20an%20empirical%20understanding%20of%20LLM%20training%0Adynamics%20in%20relation%20to%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00900v2&entry.124074799=Read"},
{"title": "WildSAT: Learning Satellite Image Representations from Wildlife\n  Observations", "author": "Rangel Daroya and Elijah Cole and Oisin Mac Aodha and Grant Van Horn and Subhransu Maji", "abstract": "  Species distributions encode valuable ecological and environmental\ninformation, yet their potential for guiding representation learning in remote\nsensing remains underexplored. We introduce WildSAT, which pairs satellite\nimages with millions of geo-tagged wildlife observations readily-available on\ncitizen science platforms. WildSAT employs a contrastive learning approach that\njointly leverages satellite images, species occurrence maps, and textual\nhabitat descriptions to train or fine-tune models. This approach significantly\nimproves performance on diverse satellite image recognition tasks,\noutperforming both ImageNet-pretrained models and satellite-specific baselines.\nAdditionally, by aligning visual and textual information, WildSAT enables\nzero-shot retrieval, allowing users to search geographic locations based on\ntextual descriptions. WildSAT surpasses recent cross-modal learning methods,\nincluding approaches that align satellite images with ground imagery or\nwildlife photos, demonstrating the advantages of our approach. Finally, we\nanalyze the impact of key design choices and highlight the broad applicability\nof WildSAT to remote sensing and biodiversity monitoring.\n", "link": "http://arxiv.org/abs/2412.14428v2", "date": "2025-08-08", "relevancy": 2.5827, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildSAT%3A%20Learning%20Satellite%20Image%20Representations%20from%20Wildlife%0A%20%20Observations&body=Title%3A%20WildSAT%3A%20Learning%20Satellite%20Image%20Representations%20from%20Wildlife%0A%20%20Observations%0AAuthor%3A%20Rangel%20Daroya%20and%20Elijah%20Cole%20and%20Oisin%20Mac%20Aodha%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji%0AAbstract%3A%20%20%20Species%20distributions%20encode%20valuable%20ecological%20and%20environmental%0Ainformation%2C%20yet%20their%20potential%20for%20guiding%20representation%20learning%20in%20remote%0Asensing%20remains%20underexplored.%20We%20introduce%20WildSAT%2C%20which%20pairs%20satellite%0Aimages%20with%20millions%20of%20geo-tagged%20wildlife%20observations%20readily-available%20on%0Acitizen%20science%20platforms.%20WildSAT%20employs%20a%20contrastive%20learning%20approach%20that%0Ajointly%20leverages%20satellite%20images%2C%20species%20occurrence%20maps%2C%20and%20textual%0Ahabitat%20descriptions%20to%20train%20or%20fine-tune%20models.%20This%20approach%20significantly%0Aimproves%20performance%20on%20diverse%20satellite%20image%20recognition%20tasks%2C%0Aoutperforming%20both%20ImageNet-pretrained%20models%20and%20satellite-specific%20baselines.%0AAdditionally%2C%20by%20aligning%20visual%20and%20textual%20information%2C%20WildSAT%20enables%0Azero-shot%20retrieval%2C%20allowing%20users%20to%20search%20geographic%20locations%20based%20on%0Atextual%20descriptions.%20WildSAT%20surpasses%20recent%20cross-modal%20learning%20methods%2C%0Aincluding%20approaches%20that%20align%20satellite%20images%20with%20ground%20imagery%20or%0Awildlife%20photos%2C%20demonstrating%20the%20advantages%20of%20our%20approach.%20Finally%2C%20we%0Aanalyze%20the%20impact%20of%20key%20design%20choices%20and%20highlight%20the%20broad%20applicability%0Aof%20WildSAT%20to%20remote%20sensing%20and%20biodiversity%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildSAT%253A%2520Learning%2520Satellite%2520Image%2520Representations%2520from%2520Wildlife%250A%2520%2520Observations%26entry.906535625%3DRangel%2520Daroya%2520and%2520Elijah%2520Cole%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Grant%2520Van%2520Horn%2520and%2520Subhransu%2520Maji%26entry.1292438233%3D%2520%2520Species%2520distributions%2520encode%2520valuable%2520ecological%2520and%2520environmental%250Ainformation%252C%2520yet%2520their%2520potential%2520for%2520guiding%2520representation%2520learning%2520in%2520remote%250Asensing%2520remains%2520underexplored.%2520We%2520introduce%2520WildSAT%252C%2520which%2520pairs%2520satellite%250Aimages%2520with%2520millions%2520of%2520geo-tagged%2520wildlife%2520observations%2520readily-available%2520on%250Acitizen%2520science%2520platforms.%2520WildSAT%2520employs%2520a%2520contrastive%2520learning%2520approach%2520that%250Ajointly%2520leverages%2520satellite%2520images%252C%2520species%2520occurrence%2520maps%252C%2520and%2520textual%250Ahabitat%2520descriptions%2520to%2520train%2520or%2520fine-tune%2520models.%2520This%2520approach%2520significantly%250Aimproves%2520performance%2520on%2520diverse%2520satellite%2520image%2520recognition%2520tasks%252C%250Aoutperforming%2520both%2520ImageNet-pretrained%2520models%2520and%2520satellite-specific%2520baselines.%250AAdditionally%252C%2520by%2520aligning%2520visual%2520and%2520textual%2520information%252C%2520WildSAT%2520enables%250Azero-shot%2520retrieval%252C%2520allowing%2520users%2520to%2520search%2520geographic%2520locations%2520based%2520on%250Atextual%2520descriptions.%2520WildSAT%2520surpasses%2520recent%2520cross-modal%2520learning%2520methods%252C%250Aincluding%2520approaches%2520that%2520align%2520satellite%2520images%2520with%2520ground%2520imagery%2520or%250Awildlife%2520photos%252C%2520demonstrating%2520the%2520advantages%2520of%2520our%2520approach.%2520Finally%252C%2520we%250Aanalyze%2520the%2520impact%2520of%2520key%2520design%2520choices%2520and%2520highlight%2520the%2520broad%2520applicability%250Aof%2520WildSAT%2520to%2520remote%2520sensing%2520and%2520biodiversity%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildSAT%3A%20Learning%20Satellite%20Image%20Representations%20from%20Wildlife%0A%20%20Observations&entry.906535625=Rangel%20Daroya%20and%20Elijah%20Cole%20and%20Oisin%20Mac%20Aodha%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji&entry.1292438233=%20%20Species%20distributions%20encode%20valuable%20ecological%20and%20environmental%0Ainformation%2C%20yet%20their%20potential%20for%20guiding%20representation%20learning%20in%20remote%0Asensing%20remains%20underexplored.%20We%20introduce%20WildSAT%2C%20which%20pairs%20satellite%0Aimages%20with%20millions%20of%20geo-tagged%20wildlife%20observations%20readily-available%20on%0Acitizen%20science%20platforms.%20WildSAT%20employs%20a%20contrastive%20learning%20approach%20that%0Ajointly%20leverages%20satellite%20images%2C%20species%20occurrence%20maps%2C%20and%20textual%0Ahabitat%20descriptions%20to%20train%20or%20fine-tune%20models.%20This%20approach%20significantly%0Aimproves%20performance%20on%20diverse%20satellite%20image%20recognition%20tasks%2C%0Aoutperforming%20both%20ImageNet-pretrained%20models%20and%20satellite-specific%20baselines.%0AAdditionally%2C%20by%20aligning%20visual%20and%20textual%20information%2C%20WildSAT%20enables%0Azero-shot%20retrieval%2C%20allowing%20users%20to%20search%20geographic%20locations%20based%20on%0Atextual%20descriptions.%20WildSAT%20surpasses%20recent%20cross-modal%20learning%20methods%2C%0Aincluding%20approaches%20that%20align%20satellite%20images%20with%20ground%20imagery%20or%0Awildlife%20photos%2C%20demonstrating%20the%20advantages%20of%20our%20approach.%20Finally%2C%20we%0Aanalyze%20the%20impact%20of%20key%20design%20choices%20and%20highlight%20the%20broad%20applicability%0Aof%20WildSAT%20to%20remote%20sensing%20and%20biodiversity%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14428v2&entry.124074799=Read"},
{"title": "Learning the Topic, Not the Language: How LLMs Classify Online\n  Immigration Discourse Across Languages", "author": "Andrea Nasuto and Stefano Maria Iacus and Francisco Rowe and Devika Jain", "abstract": "  Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.\n", "link": "http://arxiv.org/abs/2508.06435v1", "date": "2025-08-08", "relevancy": 2.5707, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Topic%2C%20Not%20the%20Language%3A%20How%20LLMs%20Classify%20Online%0A%20%20Immigration%20Discourse%20Across%20Languages&body=Title%3A%20Learning%20the%20Topic%2C%20Not%20the%20Language%3A%20How%20LLMs%20Classify%20Online%0A%20%20Immigration%20Discourse%20Across%20Languages%0AAuthor%3A%20Andrea%20Nasuto%20and%20Stefano%20Maria%20Iacus%20and%20Francisco%20Rowe%20and%20Devika%20Jain%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20transforming%20social-science%20research%20by%0Aenabling%20scalable%2C%20precise%20analysis.%20Their%20adaptability%20raises%20the%20question%20of%0Awhether%20knowledge%20acquired%20through%20fine-tuning%20in%20a%20few%20languages%20can%20transfer%0Ato%20unseen%20languages%20that%20only%20appeared%20during%20pre-training.%20To%20examine%20this%2C%20we%0Afine-tune%20lightweight%20LLaMA%203.2-3B%20models%20on%20monolingual%2C%20bilingual%2C%20or%0Amultilingual%20data%20sets%20to%20classify%20immigration-related%20tweets%20from%20X/Twitter%0Aacross%2013%20languages%2C%20a%20domain%20characterised%20by%20polarised%2C%20culturally%20specific%0Adiscourse.%20We%20evaluate%20whether%20minimal%20language-specific%20fine-tuning%20enables%0Across-lingual%20topic%20detection%20and%20whether%20adding%20targeted%20languages%20corrects%0Apre-training%20biases.%20Results%20show%20that%20LLMs%20fine-tuned%20in%20one%20or%20two%20languages%0Acan%20reliably%20classify%20immigration-related%20content%20in%20unseen%20languages.%20However%2C%0Aidentifying%20whether%20a%20tweet%20expresses%20a%20pro-%20or%20anti-immigration%20stance%0Abenefits%20from%20multilingual%20fine-tuning.%20Pre-training%20bias%20favours%20dominant%0Alanguages%2C%20but%20even%20minimal%20exposure%20to%20under-represented%20languages%20during%0Afine-tuning%20%28as%20little%20as%20%249.62%5Ctimes10%5E%7B-11%7D%24%20of%20the%20original%20pre-training%0Atoken%20volume%29%20yields%20significant%20gains.%20These%20findings%20challenge%20the%20assumption%0Athat%20cross-lingual%20mastery%20requires%20extensive%20multilingual%20training%3A%20limited%0Alanguage%20coverage%20suffices%20for%20topic-level%20generalisation%2C%20and%20structural%0Abiases%20can%20be%20corrected%20with%20lightweight%20interventions.%20By%20releasing%0A4-bit-quantised%2C%20LoRA%20fine-tuned%20models%2C%20we%20provide%20an%20open-source%2C%0Areproducible%20alternative%20to%20proprietary%20LLMs%20that%20delivers%2035%20times%20faster%0Ainference%20at%20just%200.00000989%25%20of%20the%20dollar%20cost%20of%20the%20OpenAI%20GPT-4o%20model%2C%0Aenabling%20scalable%2C%20inclusive%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Topic%252C%2520Not%2520the%2520Language%253A%2520How%2520LLMs%2520Classify%2520Online%250A%2520%2520Immigration%2520Discourse%2520Across%2520Languages%26entry.906535625%3DAndrea%2520Nasuto%2520and%2520Stefano%2520Maria%2520Iacus%2520and%2520Francisco%2520Rowe%2520and%2520Devika%2520Jain%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520transforming%2520social-science%2520research%2520by%250Aenabling%2520scalable%252C%2520precise%2520analysis.%2520Their%2520adaptability%2520raises%2520the%2520question%2520of%250Awhether%2520knowledge%2520acquired%2520through%2520fine-tuning%2520in%2520a%2520few%2520languages%2520can%2520transfer%250Ato%2520unseen%2520languages%2520that%2520only%2520appeared%2520during%2520pre-training.%2520To%2520examine%2520this%252C%2520we%250Afine-tune%2520lightweight%2520LLaMA%25203.2-3B%2520models%2520on%2520monolingual%252C%2520bilingual%252C%2520or%250Amultilingual%2520data%2520sets%2520to%2520classify%2520immigration-related%2520tweets%2520from%2520X/Twitter%250Aacross%252013%2520languages%252C%2520a%2520domain%2520characterised%2520by%2520polarised%252C%2520culturally%2520specific%250Adiscourse.%2520We%2520evaluate%2520whether%2520minimal%2520language-specific%2520fine-tuning%2520enables%250Across-lingual%2520topic%2520detection%2520and%2520whether%2520adding%2520targeted%2520languages%2520corrects%250Apre-training%2520biases.%2520Results%2520show%2520that%2520LLMs%2520fine-tuned%2520in%2520one%2520or%2520two%2520languages%250Acan%2520reliably%2520classify%2520immigration-related%2520content%2520in%2520unseen%2520languages.%2520However%252C%250Aidentifying%2520whether%2520a%2520tweet%2520expresses%2520a%2520pro-%2520or%2520anti-immigration%2520stance%250Abenefits%2520from%2520multilingual%2520fine-tuning.%2520Pre-training%2520bias%2520favours%2520dominant%250Alanguages%252C%2520but%2520even%2520minimal%2520exposure%2520to%2520under-represented%2520languages%2520during%250Afine-tuning%2520%2528as%2520little%2520as%2520%25249.62%255Ctimes10%255E%257B-11%257D%2524%2520of%2520the%2520original%2520pre-training%250Atoken%2520volume%2529%2520yields%2520significant%2520gains.%2520These%2520findings%2520challenge%2520the%2520assumption%250Athat%2520cross-lingual%2520mastery%2520requires%2520extensive%2520multilingual%2520training%253A%2520limited%250Alanguage%2520coverage%2520suffices%2520for%2520topic-level%2520generalisation%252C%2520and%2520structural%250Abiases%2520can%2520be%2520corrected%2520with%2520lightweight%2520interventions.%2520By%2520releasing%250A4-bit-quantised%252C%2520LoRA%2520fine-tuned%2520models%252C%2520we%2520provide%2520an%2520open-source%252C%250Areproducible%2520alternative%2520to%2520proprietary%2520LLMs%2520that%2520delivers%252035%2520times%2520faster%250Ainference%2520at%2520just%25200.00000989%2525%2520of%2520the%2520dollar%2520cost%2520of%2520the%2520OpenAI%2520GPT-4o%2520model%252C%250Aenabling%2520scalable%252C%2520inclusive%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Topic%2C%20Not%20the%20Language%3A%20How%20LLMs%20Classify%20Online%0A%20%20Immigration%20Discourse%20Across%20Languages&entry.906535625=Andrea%20Nasuto%20and%20Stefano%20Maria%20Iacus%20and%20Francisco%20Rowe%20and%20Devika%20Jain&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20transforming%20social-science%20research%20by%0Aenabling%20scalable%2C%20precise%20analysis.%20Their%20adaptability%20raises%20the%20question%20of%0Awhether%20knowledge%20acquired%20through%20fine-tuning%20in%20a%20few%20languages%20can%20transfer%0Ato%20unseen%20languages%20that%20only%20appeared%20during%20pre-training.%20To%20examine%20this%2C%20we%0Afine-tune%20lightweight%20LLaMA%203.2-3B%20models%20on%20monolingual%2C%20bilingual%2C%20or%0Amultilingual%20data%20sets%20to%20classify%20immigration-related%20tweets%20from%20X/Twitter%0Aacross%2013%20languages%2C%20a%20domain%20characterised%20by%20polarised%2C%20culturally%20specific%0Adiscourse.%20We%20evaluate%20whether%20minimal%20language-specific%20fine-tuning%20enables%0Across-lingual%20topic%20detection%20and%20whether%20adding%20targeted%20languages%20corrects%0Apre-training%20biases.%20Results%20show%20that%20LLMs%20fine-tuned%20in%20one%20or%20two%20languages%0Acan%20reliably%20classify%20immigration-related%20content%20in%20unseen%20languages.%20However%2C%0Aidentifying%20whether%20a%20tweet%20expresses%20a%20pro-%20or%20anti-immigration%20stance%0Abenefits%20from%20multilingual%20fine-tuning.%20Pre-training%20bias%20favours%20dominant%0Alanguages%2C%20but%20even%20minimal%20exposure%20to%20under-represented%20languages%20during%0Afine-tuning%20%28as%20little%20as%20%249.62%5Ctimes10%5E%7B-11%7D%24%20of%20the%20original%20pre-training%0Atoken%20volume%29%20yields%20significant%20gains.%20These%20findings%20challenge%20the%20assumption%0Athat%20cross-lingual%20mastery%20requires%20extensive%20multilingual%20training%3A%20limited%0Alanguage%20coverage%20suffices%20for%20topic-level%20generalisation%2C%20and%20structural%0Abiases%20can%20be%20corrected%20with%20lightweight%20interventions.%20By%20releasing%0A4-bit-quantised%2C%20LoRA%20fine-tuned%20models%2C%20we%20provide%20an%20open-source%2C%0Areproducible%20alternative%20to%20proprietary%20LLMs%20that%20delivers%2035%20times%20faster%0Ainference%20at%20just%200.00000989%25%20of%20the%20dollar%20cost%20of%20the%20OpenAI%20GPT-4o%20model%2C%0Aenabling%20scalable%2C%20inclusive%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06435v1&entry.124074799=Read"},
{"title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface\n  Temperature Estimation via Spatio-Temporal Fusion", "author": "Sofiane Bouaziz and Adel Hafiane and Raphael Canals and Rachid Nedjai", "abstract": "  Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.\n", "link": "http://arxiv.org/abs/2508.06485v1", "date": "2025-08-08", "relevancy": 2.5689, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5365}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5163}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%0A%20%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion&body=Title%3A%20WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%0A%20%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion%0AAuthor%3A%20Sofiane%20Bouaziz%20and%20Adel%20Hafiane%20and%20Raphael%20Canals%20and%20Rachid%20Nedjai%0AAbstract%3A%20%20%20Urbanization%2C%20climate%20change%2C%20and%20agricultural%20stress%20are%20increasing%20the%0Ademand%20for%20precise%20and%20timely%20environmental%20monitoring.%20Land%20Surface%0ATemperature%20%28LST%29%20is%20a%20key%20variable%20in%20this%20context%20and%20is%20retrieved%20from%0Aremote%20sensing%20satellites.%20However%2C%20these%20systems%20face%20a%20trade-off%20between%0Aspatial%20and%20temporal%20resolution.%20While%20spatio-temporal%20fusion%20methods%20offer%0Apromising%20solutions%2C%20few%20have%20addressed%20the%20estimation%20of%20daily%20LST%20at%2010%20m%0Aresolution.%20In%20this%20study%2C%20we%20present%20WGAST%2C%20a%20Weakly-Supervised%20Generative%0ANetwork%20for%20Daily%2010%20m%20LST%20Estimation%20via%20Spatio-Temporal%20Fusion%20of%20Terra%0AMODIS%2C%20Landsat%208%2C%20and%20Sentinel-2.%20WGAST%20is%20the%20first%20end-to-end%20deep%20learning%0Aframework%20designed%20for%20this%20task.%20It%20adopts%20a%20conditional%20generative%0Aadversarial%20architecture%2C%20with%20a%20generator%20composed%20of%20four%20stages%3A%20feature%0Aextraction%2C%20fusion%2C%20LST%20reconstruction%2C%20and%20noise%20suppression.%20The%20first%20stage%0Aemploys%20a%20set%20of%20encoders%20to%20extract%20multi-level%20latent%20representations%20from%0Athe%20inputs%2C%20which%20are%20then%20fused%20in%20the%20second%20stage%20using%20cosine%20similarity%2C%0Anormalization%2C%20and%20temporal%20attention%20mechanisms.%20The%20third%20stage%20decodes%20the%0Afused%20features%20into%20high-resolution%20LST%2C%20followed%20by%20a%20Gaussian%20filter%20to%0Asuppress%20high-frequency%20noise.%20Training%20follows%20a%20weakly%20supervised%20strategy%0Abased%20on%20physical%20averaging%20principles%20and%20reinforced%20by%20a%20PatchGAN%0Adiscriminator.%20Experiments%20demonstrate%20that%20WGAST%20outperforms%20existing%20methods%0Ain%20both%20quantitative%20and%20qualitative%20evaluations.%20Compared%20to%20the%0Abest-performing%20baseline%2C%20on%20average%2C%20WGAST%20reduces%20RMSE%20by%2017.18%25%20and%20improves%0ASSIM%20by%2011.00%25.%20Furthermore%2C%20WGAST%20is%20robust%20to%20cloud-induced%20LST%20and%0Aeffectively%20captures%20fine-scale%20thermal%20patterns%2C%20as%20validated%20against%2033%0Aground-based%20sensors.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Sofianebouaziz1/WGAST.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWGAST%253A%2520Weakly-Supervised%2520Generative%2520Network%2520for%2520Daily%252010%2520m%2520Land%2520Surface%250A%2520%2520Temperature%2520Estimation%2520via%2520Spatio-Temporal%2520Fusion%26entry.906535625%3DSofiane%2520Bouaziz%2520and%2520Adel%2520Hafiane%2520and%2520Raphael%2520Canals%2520and%2520Rachid%2520Nedjai%26entry.1292438233%3D%2520%2520Urbanization%252C%2520climate%2520change%252C%2520and%2520agricultural%2520stress%2520are%2520increasing%2520the%250Ademand%2520for%2520precise%2520and%2520timely%2520environmental%2520monitoring.%2520Land%2520Surface%250ATemperature%2520%2528LST%2529%2520is%2520a%2520key%2520variable%2520in%2520this%2520context%2520and%2520is%2520retrieved%2520from%250Aremote%2520sensing%2520satellites.%2520However%252C%2520these%2520systems%2520face%2520a%2520trade-off%2520between%250Aspatial%2520and%2520temporal%2520resolution.%2520While%2520spatio-temporal%2520fusion%2520methods%2520offer%250Apromising%2520solutions%252C%2520few%2520have%2520addressed%2520the%2520estimation%2520of%2520daily%2520LST%2520at%252010%2520m%250Aresolution.%2520In%2520this%2520study%252C%2520we%2520present%2520WGAST%252C%2520a%2520Weakly-Supervised%2520Generative%250ANetwork%2520for%2520Daily%252010%2520m%2520LST%2520Estimation%2520via%2520Spatio-Temporal%2520Fusion%2520of%2520Terra%250AMODIS%252C%2520Landsat%25208%252C%2520and%2520Sentinel-2.%2520WGAST%2520is%2520the%2520first%2520end-to-end%2520deep%2520learning%250Aframework%2520designed%2520for%2520this%2520task.%2520It%2520adopts%2520a%2520conditional%2520generative%250Aadversarial%2520architecture%252C%2520with%2520a%2520generator%2520composed%2520of%2520four%2520stages%253A%2520feature%250Aextraction%252C%2520fusion%252C%2520LST%2520reconstruction%252C%2520and%2520noise%2520suppression.%2520The%2520first%2520stage%250Aemploys%2520a%2520set%2520of%2520encoders%2520to%2520extract%2520multi-level%2520latent%2520representations%2520from%250Athe%2520inputs%252C%2520which%2520are%2520then%2520fused%2520in%2520the%2520second%2520stage%2520using%2520cosine%2520similarity%252C%250Anormalization%252C%2520and%2520temporal%2520attention%2520mechanisms.%2520The%2520third%2520stage%2520decodes%2520the%250Afused%2520features%2520into%2520high-resolution%2520LST%252C%2520followed%2520by%2520a%2520Gaussian%2520filter%2520to%250Asuppress%2520high-frequency%2520noise.%2520Training%2520follows%2520a%2520weakly%2520supervised%2520strategy%250Abased%2520on%2520physical%2520averaging%2520principles%2520and%2520reinforced%2520by%2520a%2520PatchGAN%250Adiscriminator.%2520Experiments%2520demonstrate%2520that%2520WGAST%2520outperforms%2520existing%2520methods%250Ain%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%2520Compared%2520to%2520the%250Abest-performing%2520baseline%252C%2520on%2520average%252C%2520WGAST%2520reduces%2520RMSE%2520by%252017.18%2525%2520and%2520improves%250ASSIM%2520by%252011.00%2525.%2520Furthermore%252C%2520WGAST%2520is%2520robust%2520to%2520cloud-induced%2520LST%2520and%250Aeffectively%2520captures%2520fine-scale%2520thermal%2520patterns%252C%2520as%2520validated%2520against%252033%250Aground-based%2520sensors.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Sofianebouaziz1/WGAST.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%0A%20%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion&entry.906535625=Sofiane%20Bouaziz%20and%20Adel%20Hafiane%20and%20Raphael%20Canals%20and%20Rachid%20Nedjai&entry.1292438233=%20%20Urbanization%2C%20climate%20change%2C%20and%20agricultural%20stress%20are%20increasing%20the%0Ademand%20for%20precise%20and%20timely%20environmental%20monitoring.%20Land%20Surface%0ATemperature%20%28LST%29%20is%20a%20key%20variable%20in%20this%20context%20and%20is%20retrieved%20from%0Aremote%20sensing%20satellites.%20However%2C%20these%20systems%20face%20a%20trade-off%20between%0Aspatial%20and%20temporal%20resolution.%20While%20spatio-temporal%20fusion%20methods%20offer%0Apromising%20solutions%2C%20few%20have%20addressed%20the%20estimation%20of%20daily%20LST%20at%2010%20m%0Aresolution.%20In%20this%20study%2C%20we%20present%20WGAST%2C%20a%20Weakly-Supervised%20Generative%0ANetwork%20for%20Daily%2010%20m%20LST%20Estimation%20via%20Spatio-Temporal%20Fusion%20of%20Terra%0AMODIS%2C%20Landsat%208%2C%20and%20Sentinel-2.%20WGAST%20is%20the%20first%20end-to-end%20deep%20learning%0Aframework%20designed%20for%20this%20task.%20It%20adopts%20a%20conditional%20generative%0Aadversarial%20architecture%2C%20with%20a%20generator%20composed%20of%20four%20stages%3A%20feature%0Aextraction%2C%20fusion%2C%20LST%20reconstruction%2C%20and%20noise%20suppression.%20The%20first%20stage%0Aemploys%20a%20set%20of%20encoders%20to%20extract%20multi-level%20latent%20representations%20from%0Athe%20inputs%2C%20which%20are%20then%20fused%20in%20the%20second%20stage%20using%20cosine%20similarity%2C%0Anormalization%2C%20and%20temporal%20attention%20mechanisms.%20The%20third%20stage%20decodes%20the%0Afused%20features%20into%20high-resolution%20LST%2C%20followed%20by%20a%20Gaussian%20filter%20to%0Asuppress%20high-frequency%20noise.%20Training%20follows%20a%20weakly%20supervised%20strategy%0Abased%20on%20physical%20averaging%20principles%20and%20reinforced%20by%20a%20PatchGAN%0Adiscriminator.%20Experiments%20demonstrate%20that%20WGAST%20outperforms%20existing%20methods%0Ain%20both%20quantitative%20and%20qualitative%20evaluations.%20Compared%20to%20the%0Abest-performing%20baseline%2C%20on%20average%2C%20WGAST%20reduces%20RMSE%20by%2017.18%25%20and%20improves%0ASSIM%20by%2011.00%25.%20Furthermore%2C%20WGAST%20is%20robust%20to%20cloud-induced%20LST%20and%0Aeffectively%20captures%20fine-scale%20thermal%20patterns%2C%20as%20validated%20against%2033%0Aground-based%20sensors.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Sofianebouaziz1/WGAST.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06485v1&entry.124074799=Read"},
{"title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction", "author": "Gopika Sudhakaran and Hikaru Shindo and Patrick Schramowski and Simone Schaub-Meyer and Kristian Kersting and Stefan Roth", "abstract": "  Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.\n", "link": "http://arxiv.org/abs/2507.23543v2", "date": "2025-08-08", "relevancy": 2.5643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction&body=Title%3A%20ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction%0AAuthor%3A%20Gopika%20Sudhakaran%20and%20Hikaru%20Shindo%20and%20Patrick%20Schramowski%20and%20Simone%20Schaub-Meyer%20and%20Kristian%20Kersting%20and%20Stefan%20Roth%0AAbstract%3A%20%20%20Visual%20relation%20detection%20%28VRD%29%20is%20the%20task%20of%20identifying%20the%20relationships%0Abetween%20objects%20in%20a%20scene.%20VRD%20models%20trained%20solely%20on%20relation%20detection%0Adata%20struggle%20to%20generalize%20beyond%20the%20relations%20on%20which%20they%20are%20trained.%0AWhile%20prompt%20tuning%20has%20been%20used%20to%20adapt%20vision-language%20models%20%28VLMs%29%20for%0AVRD%2C%20it%20uses%20handcrafted%20prompts%20and%20struggles%20with%20novel%20or%20complex%20relations.%0AWe%20argue%20that%20instruction%20tuning%20offers%20a%20more%20effective%20solution%20by%0Afine-tuning%20VLMs%20on%20diverse%20instructional%20data.%20We%20thus%20introduce%20ART%2C%20an%0AAdaptive%20Relation%20Tuning%20framework%20that%20adapts%20VLMs%20for%20VRD%20through%20instruction%0Atuning%20and%20strategic%20instance%20selection.%20By%20converting%20VRD%20datasets%20into%20an%0Ainstruction%20tuning%20format%20and%20employing%20an%20adaptive%20sampling%20algorithm%2C%20ART%0Adirects%20the%20VLM%20to%20focus%20on%20informative%20relations%20while%20maintaining%0Ageneralizability.%20Specifically%2C%20we%20focus%20on%20the%20relation%20classification%2C%20where%0Asubject-object%20boxes%20are%20given%20and%20the%20model%20predicts%20the%20predicate%20between%0Athem.%20We%20tune%20on%20a%20held-in%20set%20and%20evaluate%20across%20multiple%20held-out%20datasets%0Aof%20varying%20complexity.%20Our%20approach%20strongly%20improves%20over%20its%20baselines%20and%0Acan%20infer%20unseen%20relation%20concepts%2C%20a%20capability%20absent%20in%20mainstream%20VRD%0Amethods.%20We%20demonstrate%20ART%27s%20practical%20value%20by%20using%20the%20predicted%20relations%0Afor%20segmenting%20complex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART%253A%2520Adaptive%2520Relation%2520Tuning%2520for%2520Generalized%2520Relation%2520Prediction%26entry.906535625%3DGopika%2520Sudhakaran%2520and%2520Hikaru%2520Shindo%2520and%2520Patrick%2520Schramowski%2520and%2520Simone%2520Schaub-Meyer%2520and%2520Kristian%2520Kersting%2520and%2520Stefan%2520Roth%26entry.1292438233%3D%2520%2520Visual%2520relation%2520detection%2520%2528VRD%2529%2520is%2520the%2520task%2520of%2520identifying%2520the%2520relationships%250Abetween%2520objects%2520in%2520a%2520scene.%2520VRD%2520models%2520trained%2520solely%2520on%2520relation%2520detection%250Adata%2520struggle%2520to%2520generalize%2520beyond%2520the%2520relations%2520on%2520which%2520they%2520are%2520trained.%250AWhile%2520prompt%2520tuning%2520has%2520been%2520used%2520to%2520adapt%2520vision-language%2520models%2520%2528VLMs%2529%2520for%250AVRD%252C%2520it%2520uses%2520handcrafted%2520prompts%2520and%2520struggles%2520with%2520novel%2520or%2520complex%2520relations.%250AWe%2520argue%2520that%2520instruction%2520tuning%2520offers%2520a%2520more%2520effective%2520solution%2520by%250Afine-tuning%2520VLMs%2520on%2520diverse%2520instructional%2520data.%2520We%2520thus%2520introduce%2520ART%252C%2520an%250AAdaptive%2520Relation%2520Tuning%2520framework%2520that%2520adapts%2520VLMs%2520for%2520VRD%2520through%2520instruction%250Atuning%2520and%2520strategic%2520instance%2520selection.%2520By%2520converting%2520VRD%2520datasets%2520into%2520an%250Ainstruction%2520tuning%2520format%2520and%2520employing%2520an%2520adaptive%2520sampling%2520algorithm%252C%2520ART%250Adirects%2520the%2520VLM%2520to%2520focus%2520on%2520informative%2520relations%2520while%2520maintaining%250Ageneralizability.%2520Specifically%252C%2520we%2520focus%2520on%2520the%2520relation%2520classification%252C%2520where%250Asubject-object%2520boxes%2520are%2520given%2520and%2520the%2520model%2520predicts%2520the%2520predicate%2520between%250Athem.%2520We%2520tune%2520on%2520a%2520held-in%2520set%2520and%2520evaluate%2520across%2520multiple%2520held-out%2520datasets%250Aof%2520varying%2520complexity.%2520Our%2520approach%2520strongly%2520improves%2520over%2520its%2520baselines%2520and%250Acan%2520infer%2520unseen%2520relation%2520concepts%252C%2520a%2520capability%2520absent%2520in%2520mainstream%2520VRD%250Amethods.%2520We%2520demonstrate%2520ART%2527s%2520practical%2520value%2520by%2520using%2520the%2520predicted%2520relations%250Afor%2520segmenting%2520complex%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction&entry.906535625=Gopika%20Sudhakaran%20and%20Hikaru%20Shindo%20and%20Patrick%20Schramowski%20and%20Simone%20Schaub-Meyer%20and%20Kristian%20Kersting%20and%20Stefan%20Roth&entry.1292438233=%20%20Visual%20relation%20detection%20%28VRD%29%20is%20the%20task%20of%20identifying%20the%20relationships%0Abetween%20objects%20in%20a%20scene.%20VRD%20models%20trained%20solely%20on%20relation%20detection%0Adata%20struggle%20to%20generalize%20beyond%20the%20relations%20on%20which%20they%20are%20trained.%0AWhile%20prompt%20tuning%20has%20been%20used%20to%20adapt%20vision-language%20models%20%28VLMs%29%20for%0AVRD%2C%20it%20uses%20handcrafted%20prompts%20and%20struggles%20with%20novel%20or%20complex%20relations.%0AWe%20argue%20that%20instruction%20tuning%20offers%20a%20more%20effective%20solution%20by%0Afine-tuning%20VLMs%20on%20diverse%20instructional%20data.%20We%20thus%20introduce%20ART%2C%20an%0AAdaptive%20Relation%20Tuning%20framework%20that%20adapts%20VLMs%20for%20VRD%20through%20instruction%0Atuning%20and%20strategic%20instance%20selection.%20By%20converting%20VRD%20datasets%20into%20an%0Ainstruction%20tuning%20format%20and%20employing%20an%20adaptive%20sampling%20algorithm%2C%20ART%0Adirects%20the%20VLM%20to%20focus%20on%20informative%20relations%20while%20maintaining%0Ageneralizability.%20Specifically%2C%20we%20focus%20on%20the%20relation%20classification%2C%20where%0Asubject-object%20boxes%20are%20given%20and%20the%20model%20predicts%20the%20predicate%20between%0Athem.%20We%20tune%20on%20a%20held-in%20set%20and%20evaluate%20across%20multiple%20held-out%20datasets%0Aof%20varying%20complexity.%20Our%20approach%20strongly%20improves%20over%20its%20baselines%20and%0Acan%20infer%20unseen%20relation%20concepts%2C%20a%20capability%20absent%20in%20mainstream%20VRD%0Amethods.%20We%20demonstrate%20ART%27s%20practical%20value%20by%20using%20the%20predicted%20relations%0Afor%20segmenting%20complex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23543v2&entry.124074799=Read"},
{"title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation", "author": "Xinyang Li and Gen Li and Zhihui Lin and Yichen Qian and GongXin Yao and Weinan Jia and Aowen Wang and Weihua Chen and Fan Wang", "abstract": "  Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of the virtual metaverse. Recently, diffusion\nmodels have become a popular generative technique in this field with their\nstrong generation capabilities. However, several challenges remain for\ndiffusion-based methods: 1) inefficient inference and visual artifacts caused\nby the implicit latent space of Variational Auto-Encoders (VAE), which\ncomplicates the diffusion process; 2) a lack of authentic facial expressions\nand head movements due to inadequate multi-modal information fusion. In this\npaper, MoDA handles these challenges by: 1) defining a joint parameter space\nthat bridges motion generation and neural rendering, and leveraging flow\nmatching to simplify diffusion learning; 2) introducing a multi-modal diffusion\narchitecture to model the interaction among noisy motion, audio, and auxiliary\nconditions, enhancing overall facial expressiveness. In addition, a\ncoarse-to-fine fusion strategy is employed to progressively integrate different\nmodalities, ensuring effective feature fusion. Experimental results demonstrate\nthat MoDA improves video diversity, realism, and efficiency, making it suitable\nfor real-world applications. Project Page:\nhttps://lixinyyang.github.io/MoDA.github.io/\n", "link": "http://arxiv.org/abs/2507.03256v3", "date": "2025-08-08", "relevancy": 2.5586, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6299}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDA%3A%20Multi-modal%20Diffusion%20Architecture%20for%20Talking%20Head%20Generation&body=Title%3A%20MoDA%3A%20Multi-modal%20Diffusion%20Architecture%20for%20Talking%20Head%20Generation%0AAuthor%3A%20Xinyang%20Li%20and%20Gen%20Li%20and%20Zhihui%20Lin%20and%20Yichen%20Qian%20and%20GongXin%20Yao%20and%20Weinan%20Jia%20and%20Aowen%20Wang%20and%20Weihua%20Chen%20and%20Fan%20Wang%0AAbstract%3A%20%20%20Talking%20head%20generation%20with%20arbitrary%20identities%20and%20speech%20audio%20remains%20a%0Acrucial%20problem%20in%20the%20realm%20of%20the%20virtual%20metaverse.%20Recently%2C%20diffusion%0Amodels%20have%20become%20a%20popular%20generative%20technique%20in%20this%20field%20with%20their%0Astrong%20generation%20capabilities.%20However%2C%20several%20challenges%20remain%20for%0Adiffusion-based%20methods%3A%201%29%20inefficient%20inference%20and%20visual%20artifacts%20caused%0Aby%20the%20implicit%20latent%20space%20of%20Variational%20Auto-Encoders%20%28VAE%29%2C%20which%0Acomplicates%20the%20diffusion%20process%3B%202%29%20a%20lack%20of%20authentic%20facial%20expressions%0Aand%20head%20movements%20due%20to%20inadequate%20multi-modal%20information%20fusion.%20In%20this%0Apaper%2C%20MoDA%20handles%20these%20challenges%20by%3A%201%29%20defining%20a%20joint%20parameter%20space%0Athat%20bridges%20motion%20generation%20and%20neural%20rendering%2C%20and%20leveraging%20flow%0Amatching%20to%20simplify%20diffusion%20learning%3B%202%29%20introducing%20a%20multi-modal%20diffusion%0Aarchitecture%20to%20model%20the%20interaction%20among%20noisy%20motion%2C%20audio%2C%20and%20auxiliary%0Aconditions%2C%20enhancing%20overall%20facial%20expressiveness.%20In%20addition%2C%20a%0Acoarse-to-fine%20fusion%20strategy%20is%20employed%20to%20progressively%20integrate%20different%0Amodalities%2C%20ensuring%20effective%20feature%20fusion.%20Experimental%20results%20demonstrate%0Athat%20MoDA%20improves%20video%20diversity%2C%20realism%2C%20and%20efficiency%2C%20making%20it%20suitable%0Afor%20real-world%20applications.%20Project%20Page%3A%0Ahttps%3A//lixinyyang.github.io/MoDA.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03256v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDA%253A%2520Multi-modal%2520Diffusion%2520Architecture%2520for%2520Talking%2520Head%2520Generation%26entry.906535625%3DXinyang%2520Li%2520and%2520Gen%2520Li%2520and%2520Zhihui%2520Lin%2520and%2520Yichen%2520Qian%2520and%2520GongXin%2520Yao%2520and%2520Weinan%2520Jia%2520and%2520Aowen%2520Wang%2520and%2520Weihua%2520Chen%2520and%2520Fan%2520Wang%26entry.1292438233%3D%2520%2520Talking%2520head%2520generation%2520with%2520arbitrary%2520identities%2520and%2520speech%2520audio%2520remains%2520a%250Acrucial%2520problem%2520in%2520the%2520realm%2520of%2520the%2520virtual%2520metaverse.%2520Recently%252C%2520diffusion%250Amodels%2520have%2520become%2520a%2520popular%2520generative%2520technique%2520in%2520this%2520field%2520with%2520their%250Astrong%2520generation%2520capabilities.%2520However%252C%2520several%2520challenges%2520remain%2520for%250Adiffusion-based%2520methods%253A%25201%2529%2520inefficient%2520inference%2520and%2520visual%2520artifacts%2520caused%250Aby%2520the%2520implicit%2520latent%2520space%2520of%2520Variational%2520Auto-Encoders%2520%2528VAE%2529%252C%2520which%250Acomplicates%2520the%2520diffusion%2520process%253B%25202%2529%2520a%2520lack%2520of%2520authentic%2520facial%2520expressions%250Aand%2520head%2520movements%2520due%2520to%2520inadequate%2520multi-modal%2520information%2520fusion.%2520In%2520this%250Apaper%252C%2520MoDA%2520handles%2520these%2520challenges%2520by%253A%25201%2529%2520defining%2520a%2520joint%2520parameter%2520space%250Athat%2520bridges%2520motion%2520generation%2520and%2520neural%2520rendering%252C%2520and%2520leveraging%2520flow%250Amatching%2520to%2520simplify%2520diffusion%2520learning%253B%25202%2529%2520introducing%2520a%2520multi-modal%2520diffusion%250Aarchitecture%2520to%2520model%2520the%2520interaction%2520among%2520noisy%2520motion%252C%2520audio%252C%2520and%2520auxiliary%250Aconditions%252C%2520enhancing%2520overall%2520facial%2520expressiveness.%2520In%2520addition%252C%2520a%250Acoarse-to-fine%2520fusion%2520strategy%2520is%2520employed%2520to%2520progressively%2520integrate%2520different%250Amodalities%252C%2520ensuring%2520effective%2520feature%2520fusion.%2520Experimental%2520results%2520demonstrate%250Athat%2520MoDA%2520improves%2520video%2520diversity%252C%2520realism%252C%2520and%2520efficiency%252C%2520making%2520it%2520suitable%250Afor%2520real-world%2520applications.%2520Project%2520Page%253A%250Ahttps%253A//lixinyyang.github.io/MoDA.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03256v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDA%3A%20Multi-modal%20Diffusion%20Architecture%20for%20Talking%20Head%20Generation&entry.906535625=Xinyang%20Li%20and%20Gen%20Li%20and%20Zhihui%20Lin%20and%20Yichen%20Qian%20and%20GongXin%20Yao%20and%20Weinan%20Jia%20and%20Aowen%20Wang%20and%20Weihua%20Chen%20and%20Fan%20Wang&entry.1292438233=%20%20Talking%20head%20generation%20with%20arbitrary%20identities%20and%20speech%20audio%20remains%20a%0Acrucial%20problem%20in%20the%20realm%20of%20the%20virtual%20metaverse.%20Recently%2C%20diffusion%0Amodels%20have%20become%20a%20popular%20generative%20technique%20in%20this%20field%20with%20their%0Astrong%20generation%20capabilities.%20However%2C%20several%20challenges%20remain%20for%0Adiffusion-based%20methods%3A%201%29%20inefficient%20inference%20and%20visual%20artifacts%20caused%0Aby%20the%20implicit%20latent%20space%20of%20Variational%20Auto-Encoders%20%28VAE%29%2C%20which%0Acomplicates%20the%20diffusion%20process%3B%202%29%20a%20lack%20of%20authentic%20facial%20expressions%0Aand%20head%20movements%20due%20to%20inadequate%20multi-modal%20information%20fusion.%20In%20this%0Apaper%2C%20MoDA%20handles%20these%20challenges%20by%3A%201%29%20defining%20a%20joint%20parameter%20space%0Athat%20bridges%20motion%20generation%20and%20neural%20rendering%2C%20and%20leveraging%20flow%0Amatching%20to%20simplify%20diffusion%20learning%3B%202%29%20introducing%20a%20multi-modal%20diffusion%0Aarchitecture%20to%20model%20the%20interaction%20among%20noisy%20motion%2C%20audio%2C%20and%20auxiliary%0Aconditions%2C%20enhancing%20overall%20facial%20expressiveness.%20In%20addition%2C%20a%0Acoarse-to-fine%20fusion%20strategy%20is%20employed%20to%20progressively%20integrate%20different%0Amodalities%2C%20ensuring%20effective%20feature%20fusion.%20Experimental%20results%20demonstrate%0Athat%20MoDA%20improves%20video%20diversity%2C%20realism%2C%20and%20efficiency%2C%20making%20it%20suitable%0Afor%20real-world%20applications.%20Project%20Page%3A%0Ahttps%3A//lixinyyang.github.io/MoDA.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03256v3&entry.124074799=Read"},
{"title": "XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur\n  MRI Segmentation", "author": "Byunghyun Ko and Anning Tian and Jeongkyu Lee", "abstract": "  Accurate segmentation of femur structures from Magnetic Resonance Imaging\n(MRI) is critical for orthopedic diagnosis and surgical planning but remains\nchallenging due to the limitations of existing 2D and 3D deep learning-based\nsegmentation approaches. In this study, we propose XAG-Net, a novel 2.5D\nU-Net-based architecture that incorporates pixel-wise cross-slice attention\n(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice\ncontextual modeling and intra-slice feature refinement. Unlike previous\nCSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent\nslices at each spatial location for fine-grained inter-slice modeling.\nExtensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and\n3D U-Net models in femur segmentation accuracy while maintaining computational\nefficiency. Ablation studies further validate the critical role of the CSA and\nAG modules, establishing XAG-Net as a promising framework for efficient and\naccurate femur MRI segmentation.\n", "link": "http://arxiv.org/abs/2508.06258v1", "date": "2025-08-08", "relevancy": 2.5567, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5175}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5095}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XAG-Net%3A%20A%20Cross-Slice%20Attention%20and%20Skip%20Gating%20Network%20for%202.5D%20Femur%0A%20%20MRI%20Segmentation&body=Title%3A%20XAG-Net%3A%20A%20Cross-Slice%20Attention%20and%20Skip%20Gating%20Network%20for%202.5D%20Femur%0A%20%20MRI%20Segmentation%0AAuthor%3A%20Byunghyun%20Ko%20and%20Anning%20Tian%20and%20Jeongkyu%20Lee%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20femur%20structures%20from%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%20is%20critical%20for%20orthopedic%20diagnosis%20and%20surgical%20planning%20but%20remains%0Achallenging%20due%20to%20the%20limitations%20of%20existing%202D%20and%203D%20deep%20learning-based%0Asegmentation%20approaches.%20In%20this%20study%2C%20we%20propose%20XAG-Net%2C%20a%20novel%202.5D%0AU-Net-based%20architecture%20that%20incorporates%20pixel-wise%20cross-slice%20attention%0A%28CSA%29%20and%20skip%20attention%20gating%20%28AG%29%20mechanisms%20to%20enhance%20inter-slice%0Acontextual%20modeling%20and%20intra-slice%20feature%20refinement.%20Unlike%20previous%0ACSA-based%20models%2C%20XAG-Net%20applies%20pixel-wise%20softmax%20attention%20across%20adjacent%0Aslices%20at%20each%20spatial%20location%20for%20fine-grained%20inter-slice%20modeling.%0AExtensive%20evaluations%20demonstrate%20that%20XAG-Net%20surpasses%20baseline%202D%2C%202.5D%2C%20and%0A3D%20U-Net%20models%20in%20femur%20segmentation%20accuracy%20while%20maintaining%20computational%0Aefficiency.%20Ablation%20studies%20further%20validate%20the%20critical%20role%20of%20the%20CSA%20and%0AAG%20modules%2C%20establishing%20XAG-Net%20as%20a%20promising%20framework%20for%20efficient%20and%0Aaccurate%20femur%20MRI%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXAG-Net%253A%2520A%2520Cross-Slice%2520Attention%2520and%2520Skip%2520Gating%2520Network%2520for%25202.5D%2520Femur%250A%2520%2520MRI%2520Segmentation%26entry.906535625%3DByunghyun%2520Ko%2520and%2520Anning%2520Tian%2520and%2520Jeongkyu%2520Lee%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520femur%2520structures%2520from%2520Magnetic%2520Resonance%2520Imaging%250A%2528MRI%2529%2520is%2520critical%2520for%2520orthopedic%2520diagnosis%2520and%2520surgical%2520planning%2520but%2520remains%250Achallenging%2520due%2520to%2520the%2520limitations%2520of%2520existing%25202D%2520and%25203D%2520deep%2520learning-based%250Asegmentation%2520approaches.%2520In%2520this%2520study%252C%2520we%2520propose%2520XAG-Net%252C%2520a%2520novel%25202.5D%250AU-Net-based%2520architecture%2520that%2520incorporates%2520pixel-wise%2520cross-slice%2520attention%250A%2528CSA%2529%2520and%2520skip%2520attention%2520gating%2520%2528AG%2529%2520mechanisms%2520to%2520enhance%2520inter-slice%250Acontextual%2520modeling%2520and%2520intra-slice%2520feature%2520refinement.%2520Unlike%2520previous%250ACSA-based%2520models%252C%2520XAG-Net%2520applies%2520pixel-wise%2520softmax%2520attention%2520across%2520adjacent%250Aslices%2520at%2520each%2520spatial%2520location%2520for%2520fine-grained%2520inter-slice%2520modeling.%250AExtensive%2520evaluations%2520demonstrate%2520that%2520XAG-Net%2520surpasses%2520baseline%25202D%252C%25202.5D%252C%2520and%250A3D%2520U-Net%2520models%2520in%2520femur%2520segmentation%2520accuracy%2520while%2520maintaining%2520computational%250Aefficiency.%2520Ablation%2520studies%2520further%2520validate%2520the%2520critical%2520role%2520of%2520the%2520CSA%2520and%250AAG%2520modules%252C%2520establishing%2520XAG-Net%2520as%2520a%2520promising%2520framework%2520for%2520efficient%2520and%250Aaccurate%2520femur%2520MRI%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XAG-Net%3A%20A%20Cross-Slice%20Attention%20and%20Skip%20Gating%20Network%20for%202.5D%20Femur%0A%20%20MRI%20Segmentation&entry.906535625=Byunghyun%20Ko%20and%20Anning%20Tian%20and%20Jeongkyu%20Lee&entry.1292438233=%20%20Accurate%20segmentation%20of%20femur%20structures%20from%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%20is%20critical%20for%20orthopedic%20diagnosis%20and%20surgical%20planning%20but%20remains%0Achallenging%20due%20to%20the%20limitations%20of%20existing%202D%20and%203D%20deep%20learning-based%0Asegmentation%20approaches.%20In%20this%20study%2C%20we%20propose%20XAG-Net%2C%20a%20novel%202.5D%0AU-Net-based%20architecture%20that%20incorporates%20pixel-wise%20cross-slice%20attention%0A%28CSA%29%20and%20skip%20attention%20gating%20%28AG%29%20mechanisms%20to%20enhance%20inter-slice%0Acontextual%20modeling%20and%20intra-slice%20feature%20refinement.%20Unlike%20previous%0ACSA-based%20models%2C%20XAG-Net%20applies%20pixel-wise%20softmax%20attention%20across%20adjacent%0Aslices%20at%20each%20spatial%20location%20for%20fine-grained%20inter-slice%20modeling.%0AExtensive%20evaluations%20demonstrate%20that%20XAG-Net%20surpasses%20baseline%202D%2C%202.5D%2C%20and%0A3D%20U-Net%20models%20in%20femur%20segmentation%20accuracy%20while%20maintaining%20computational%0Aefficiency.%20Ablation%20studies%20further%20validate%20the%20critical%20role%20of%20the%20CSA%20and%0AAG%20modules%2C%20establishing%20XAG-Net%20as%20a%20promising%20framework%20for%20efficient%20and%0Aaccurate%20femur%20MRI%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06258v1&entry.124074799=Read"},
{"title": "Topic Over Source: The Key to Effective Data Mixing for Language Models\n  Pre-training", "author": "Jiahui Peng and Xinlin Zhuang and Jiantao Qiu and Ren Ma and Jing Yu and He Zhu and Conghui He", "abstract": "  The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research.\n", "link": "http://arxiv.org/abs/2502.16802v3", "date": "2025-08-08", "relevancy": 2.5452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topic%20Over%20Source%3A%20The%20Key%20to%20Effective%20Data%20Mixing%20for%20Language%20Models%0A%20%20Pre-training&body=Title%3A%20Topic%20Over%20Source%3A%20The%20Key%20to%20Effective%20Data%20Mixing%20for%20Language%20Models%0A%20%20Pre-training%0AAuthor%3A%20Jiahui%20Peng%20and%20Xinlin%20Zhuang%20and%20Jiantao%20Qiu%20and%20Ren%20Ma%20and%20Jing%20Yu%20and%20He%20Zhu%20and%20Conghui%20He%0AAbstract%3A%20%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20is%20significantly%20affected%20by%0Athe%20quality%20and%20composition%20of%20their%20pre-training%20data%2C%20which%20is%20inherently%0Adiverse%2C%20spanning%20various%20languages%2C%20sources%2C%20and%20topics.%20Effectively%0Aintegrating%20these%20heterogeneous%20data%20groups%20is%20crucial%20for%20optimizing%20LLM%0Aperformance.%20Previous%20research%20has%20predominantly%20concentrated%20on%20source-based%0Adata%20mixing%2C%20often%20neglecting%20the%20nuanced%20topic-level%20characteristics%20of%20the%0Adata.%20To%20address%20this%20gap%2C%20we%20propose%20a%20topic-based%20data%20mixing%20strategy%20that%0Autilizes%20detailed%20topic%20labels%20generated%20through%20a%20multi-stage%20process%0Acombining%20unsupervised%20clustering%2C%20LLM-based%20summarization%2C%20and%20supervised%0Aclassifier%20training.%20With%20this%20strategy%2C%20we%20conduct%20the%20first%20comprehensive%0Acomparison%20of%20topic-based%20versus%20source-based%20partitioning%20across%20multiple%0Amixing%20strategies.%20We%20demonstrate%20that%20language%20models%20pretrained%20on%20data%20mixed%0Aby%20topics%20consistently%20outperform%20those%20trained%20on%20data%20mixed%20by%20sources%20across%0Amultiple%20methods%20including%20RegMix%2C%20DoReMi%2Ctemperature-based%20sampling%2C%20and%20a%0Amanual%20mixing%20method%20based%20on%20downstream%20task%20performance.%20Our%20theoretical%0Aanalysis%20reveals%20that%20topic-based%20data%20achieves%20significantly%20lower%20validation%0Aloss%20compared%20to%20source-based%20approaches%2C%20creating%20a%20better%20optimization%0Alandscape%20for%20model%20training.%20We%20will%20make%20our%20code%2C%20annotated%20datasets%2C%20and%0Atopic%20classification%20models%20publicly%20available%20to%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopic%2520Over%2520Source%253A%2520The%2520Key%2520to%2520Effective%2520Data%2520Mixing%2520for%2520Language%2520Models%250A%2520%2520Pre-training%26entry.906535625%3DJiahui%2520Peng%2520and%2520Xinlin%2520Zhuang%2520and%2520Jiantao%2520Qiu%2520and%2520Ren%2520Ma%2520and%2520Jing%2520Yu%2520and%2520He%2520Zhu%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520significantly%2520affected%2520by%250Athe%2520quality%2520and%2520composition%2520of%2520their%2520pre-training%2520data%252C%2520which%2520is%2520inherently%250Adiverse%252C%2520spanning%2520various%2520languages%252C%2520sources%252C%2520and%2520topics.%2520Effectively%250Aintegrating%2520these%2520heterogeneous%2520data%2520groups%2520is%2520crucial%2520for%2520optimizing%2520LLM%250Aperformance.%2520Previous%2520research%2520has%2520predominantly%2520concentrated%2520on%2520source-based%250Adata%2520mixing%252C%2520often%2520neglecting%2520the%2520nuanced%2520topic-level%2520characteristics%2520of%2520the%250Adata.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520topic-based%2520data%2520mixing%2520strategy%2520that%250Autilizes%2520detailed%2520topic%2520labels%2520generated%2520through%2520a%2520multi-stage%2520process%250Acombining%2520unsupervised%2520clustering%252C%2520LLM-based%2520summarization%252C%2520and%2520supervised%250Aclassifier%2520training.%2520With%2520this%2520strategy%252C%2520we%2520conduct%2520the%2520first%2520comprehensive%250Acomparison%2520of%2520topic-based%2520versus%2520source-based%2520partitioning%2520across%2520multiple%250Amixing%2520strategies.%2520We%2520demonstrate%2520that%2520language%2520models%2520pretrained%2520on%2520data%2520mixed%250Aby%2520topics%2520consistently%2520outperform%2520those%2520trained%2520on%2520data%2520mixed%2520by%2520sources%2520across%250Amultiple%2520methods%2520including%2520RegMix%252C%2520DoReMi%252Ctemperature-based%2520sampling%252C%2520and%2520a%250Amanual%2520mixing%2520method%2520based%2520on%2520downstream%2520task%2520performance.%2520Our%2520theoretical%250Aanalysis%2520reveals%2520that%2520topic-based%2520data%2520achieves%2520significantly%2520lower%2520validation%250Aloss%2520compared%2520to%2520source-based%2520approaches%252C%2520creating%2520a%2520better%2520optimization%250Alandscape%2520for%2520model%2520training.%2520We%2520will%2520make%2520our%2520code%252C%2520annotated%2520datasets%252C%2520and%250Atopic%2520classification%2520models%2520publicly%2520available%2520to%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic%20Over%20Source%3A%20The%20Key%20to%20Effective%20Data%20Mixing%20for%20Language%20Models%0A%20%20Pre-training&entry.906535625=Jiahui%20Peng%20and%20Xinlin%20Zhuang%20and%20Jiantao%20Qiu%20and%20Ren%20Ma%20and%20Jing%20Yu%20and%20He%20Zhu%20and%20Conghui%20He&entry.1292438233=%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20is%20significantly%20affected%20by%0Athe%20quality%20and%20composition%20of%20their%20pre-training%20data%2C%20which%20is%20inherently%0Adiverse%2C%20spanning%20various%20languages%2C%20sources%2C%20and%20topics.%20Effectively%0Aintegrating%20these%20heterogeneous%20data%20groups%20is%20crucial%20for%20optimizing%20LLM%0Aperformance.%20Previous%20research%20has%20predominantly%20concentrated%20on%20source-based%0Adata%20mixing%2C%20often%20neglecting%20the%20nuanced%20topic-level%20characteristics%20of%20the%0Adata.%20To%20address%20this%20gap%2C%20we%20propose%20a%20topic-based%20data%20mixing%20strategy%20that%0Autilizes%20detailed%20topic%20labels%20generated%20through%20a%20multi-stage%20process%0Acombining%20unsupervised%20clustering%2C%20LLM-based%20summarization%2C%20and%20supervised%0Aclassifier%20training.%20With%20this%20strategy%2C%20we%20conduct%20the%20first%20comprehensive%0Acomparison%20of%20topic-based%20versus%20source-based%20partitioning%20across%20multiple%0Amixing%20strategies.%20We%20demonstrate%20that%20language%20models%20pretrained%20on%20data%20mixed%0Aby%20topics%20consistently%20outperform%20those%20trained%20on%20data%20mixed%20by%20sources%20across%0Amultiple%20methods%20including%20RegMix%2C%20DoReMi%2Ctemperature-based%20sampling%2C%20and%20a%0Amanual%20mixing%20method%20based%20on%20downstream%20task%20performance.%20Our%20theoretical%0Aanalysis%20reveals%20that%20topic-based%20data%20achieves%20significantly%20lower%20validation%0Aloss%20compared%20to%20source-based%20approaches%2C%20creating%20a%20better%20optimization%0Alandscape%20for%20model%20training.%20We%20will%20make%20our%20code%2C%20annotated%20datasets%2C%20and%0Atopic%20classification%20models%20publicly%20available%20to%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16802v3&entry.124074799=Read"},
{"title": "SPARTA: Advancing Sparse Attention in Spiking Neural Networks via\n  Spike-Timing-Based Prioritization", "author": "Minsuk Jang and Changick Kim", "abstract": "  Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics\ninherent in spike-based processing, relying primarily on rate coding while\noverlooking precise timing information that provides rich computational cues.\nWe propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal\nAllocation), a framework that leverages heterogeneous neuron dynamics and\nspike-timing information to enable efficient sparse attention. SPARTA\nprioritizes tokens based on temporal cues, including firing patterns, spike\ntiming, and inter-spike intervals, achieving 65.4% sparsity through competitive\ngating. By selecting only the most salient tokens, SPARTA reduces attention\ncomplexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy.\nOur method achieves state-of-the-art performance on DVS-Gesture (98.78%) and\ncompetitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating\nthat exploiting spike timing dynamics improves both computational efficiency\nand accuracy.\n", "link": "http://arxiv.org/abs/2508.01646v2", "date": "2025-08-08", "relevancy": 2.5393, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4858}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARTA%3A%20Advancing%20Sparse%20Attention%20in%20Spiking%20Neural%20Networks%20via%0A%20%20Spike-Timing-Based%20Prioritization&body=Title%3A%20SPARTA%3A%20Advancing%20Sparse%20Attention%20in%20Spiking%20Neural%20Networks%20via%0A%20%20Spike-Timing-Based%20Prioritization%0AAuthor%3A%20Minsuk%20Jang%20and%20Changick%20Kim%0AAbstract%3A%20%20%20Current%20Spiking%20Neural%20Networks%20%28SNNs%29%20underutilize%20the%20temporal%20dynamics%0Ainherent%20in%20spike-based%20processing%2C%20relying%20primarily%20on%20rate%20coding%20while%0Aoverlooking%20precise%20timing%20information%20that%20provides%20rich%20computational%20cues.%0AWe%20propose%20SPARTA%20%28Spiking%20Priority%20Attention%20with%20Resource-Adaptive%20Temporal%0AAllocation%29%2C%20a%20framework%20that%20leverages%20heterogeneous%20neuron%20dynamics%20and%0Aspike-timing%20information%20to%20enable%20efficient%20sparse%20attention.%20SPARTA%0Aprioritizes%20tokens%20based%20on%20temporal%20cues%2C%20including%20firing%20patterns%2C%20spike%0Atiming%2C%20and%20inter-spike%20intervals%2C%20achieving%2065.4%25%20sparsity%20through%20competitive%0Agating.%20By%20selecting%20only%20the%20most%20salient%20tokens%2C%20SPARTA%20reduces%20attention%0Acomplexity%20from%20O%28N%5E2%29%20to%20O%28K%5E2%29%20with%20k%20%3C%3C%20n%2C%20while%20maintaining%20high%20accuracy.%0AOur%20method%20achieves%20state-of-the-art%20performance%20on%20DVS-Gesture%20%2898.78%25%29%20and%0Acompetitive%20results%20on%20CIFAR10-DVS%20%2883.06%25%29%20and%20CIFAR-10%20%2895.3%25%29%2C%20demonstrating%0Athat%20exploiting%20spike%20timing%20dynamics%20improves%20both%20computational%20efficiency%0Aand%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARTA%253A%2520Advancing%2520Sparse%2520Attention%2520in%2520Spiking%2520Neural%2520Networks%2520via%250A%2520%2520Spike-Timing-Based%2520Prioritization%26entry.906535625%3DMinsuk%2520Jang%2520and%2520Changick%2520Kim%26entry.1292438233%3D%2520%2520Current%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520underutilize%2520the%2520temporal%2520dynamics%250Ainherent%2520in%2520spike-based%2520processing%252C%2520relying%2520primarily%2520on%2520rate%2520coding%2520while%250Aoverlooking%2520precise%2520timing%2520information%2520that%2520provides%2520rich%2520computational%2520cues.%250AWe%2520propose%2520SPARTA%2520%2528Spiking%2520Priority%2520Attention%2520with%2520Resource-Adaptive%2520Temporal%250AAllocation%2529%252C%2520a%2520framework%2520that%2520leverages%2520heterogeneous%2520neuron%2520dynamics%2520and%250Aspike-timing%2520information%2520to%2520enable%2520efficient%2520sparse%2520attention.%2520SPARTA%250Aprioritizes%2520tokens%2520based%2520on%2520temporal%2520cues%252C%2520including%2520firing%2520patterns%252C%2520spike%250Atiming%252C%2520and%2520inter-spike%2520intervals%252C%2520achieving%252065.4%2525%2520sparsity%2520through%2520competitive%250Agating.%2520By%2520selecting%2520only%2520the%2520most%2520salient%2520tokens%252C%2520SPARTA%2520reduces%2520attention%250Acomplexity%2520from%2520O%2528N%255E2%2529%2520to%2520O%2528K%255E2%2529%2520with%2520k%2520%253C%253C%2520n%252C%2520while%2520maintaining%2520high%2520accuracy.%250AOur%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520DVS-Gesture%2520%252898.78%2525%2529%2520and%250Acompetitive%2520results%2520on%2520CIFAR10-DVS%2520%252883.06%2525%2529%2520and%2520CIFAR-10%2520%252895.3%2525%2529%252C%2520demonstrating%250Athat%2520exploiting%2520spike%2520timing%2520dynamics%2520improves%2520both%2520computational%2520efficiency%250Aand%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARTA%3A%20Advancing%20Sparse%20Attention%20in%20Spiking%20Neural%20Networks%20via%0A%20%20Spike-Timing-Based%20Prioritization&entry.906535625=Minsuk%20Jang%20and%20Changick%20Kim&entry.1292438233=%20%20Current%20Spiking%20Neural%20Networks%20%28SNNs%29%20underutilize%20the%20temporal%20dynamics%0Ainherent%20in%20spike-based%20processing%2C%20relying%20primarily%20on%20rate%20coding%20while%0Aoverlooking%20precise%20timing%20information%20that%20provides%20rich%20computational%20cues.%0AWe%20propose%20SPARTA%20%28Spiking%20Priority%20Attention%20with%20Resource-Adaptive%20Temporal%0AAllocation%29%2C%20a%20framework%20that%20leverages%20heterogeneous%20neuron%20dynamics%20and%0Aspike-timing%20information%20to%20enable%20efficient%20sparse%20attention.%20SPARTA%0Aprioritizes%20tokens%20based%20on%20temporal%20cues%2C%20including%20firing%20patterns%2C%20spike%0Atiming%2C%20and%20inter-spike%20intervals%2C%20achieving%2065.4%25%20sparsity%20through%20competitive%0Agating.%20By%20selecting%20only%20the%20most%20salient%20tokens%2C%20SPARTA%20reduces%20attention%0Acomplexity%20from%20O%28N%5E2%29%20to%20O%28K%5E2%29%20with%20k%20%3C%3C%20n%2C%20while%20maintaining%20high%20accuracy.%0AOur%20method%20achieves%20state-of-the-art%20performance%20on%20DVS-Gesture%20%2898.78%25%29%20and%0Acompetitive%20results%20on%20CIFAR10-DVS%20%2883.06%25%29%20and%20CIFAR-10%20%2895.3%25%29%2C%20demonstrating%0Athat%20exploiting%20spike%20timing%20dynamics%20improves%20both%20computational%20efficiency%0Aand%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01646v2&entry.124074799=Read"},
{"title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on\n  Long-Step Problems Requiring Auxiliary Lines", "author": "Yumeng Fu and Jiayin Zhu and Lingling Zhang and Bo Zhao and Shaoxuan Ma and Yushun Zhang and Yanrui Wu and Wenjun Wu", "abstract": "  Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.\n", "link": "http://arxiv.org/abs/2508.06226v1", "date": "2025-08-08", "relevancy": 2.5352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoLaux%3A%20A%20Benchmark%20for%20Evaluating%20MLLMs%27%20Geometry%20Performance%20on%0A%20%20Long-Step%20Problems%20Requiring%20Auxiliary%20Lines&body=Title%3A%20GeoLaux%3A%20A%20Benchmark%20for%20Evaluating%20MLLMs%27%20Geometry%20Performance%20on%0A%20%20Long-Step%20Problems%20Requiring%20Auxiliary%20Lines%0AAuthor%3A%20Yumeng%20Fu%20and%20Jiayin%20Zhu%20and%20Lingling%20Zhang%20and%20Bo%20Zhao%20and%20Shaoxuan%20Ma%20and%20Yushun%20Zhang%20and%20Yanrui%20Wu%20and%20Wenjun%20Wu%0AAbstract%3A%20%20%20Geometry%20problem%20solving%20%28GPS%29%20requires%20models%20to%20master%20diagram%0Acomprehension%2C%20logical%20reasoning%2C%20knowledge%20application%2C%20numerical%20computation%2C%0Aand%20auxiliary%20line%20construction.%20This%20presents%20a%20significant%20challenge%20for%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29.%20However%2C%20existing%20benchmarks%20for%0Aevaluating%20MLLM%20geometry%20skills%20overlook%20auxiliary%20line%20construction%20and%20lack%0Afine-grained%20process%20evaluation%2C%20making%20them%20insufficient%20for%20assessing%20MLLMs%27%0Along-step%20reasoning%20abilities.%20To%20bridge%20these%20gaps%2C%20we%20present%20the%20GeoLaux%0Abenchmark%2C%20comprising%202%2C186%20geometry%20problems%2C%20incorporating%20both%20calculation%0Aand%20proving%20questions.%20Notably%2C%20the%20problems%20require%20an%20average%20of%206.51%0Areasoning%20steps%2C%20with%20a%20maximum%20of%2024%20steps%2C%20and%2041.8%25%20of%20them%20need%20auxiliary%0Aline%20construction.%20Building%20on%20the%20dataset%2C%20we%20design%20a%20novel%20five-dimensional%0Aevaluation%20strategy%20assessing%20answer%20correctness%2C%20process%20correctness%2C%20process%0Aquality%2C%20auxiliary%20line%20impact%2C%20and%20error%20causes.%20Extensive%20experiments%20on%2013%0Aleading%20MLLMs%20%28including%20thinking%20models%20and%20non-thinking%20models%29%20yield%20three%0Apivotal%20findings%3A%20First%2C%20models%20exhibit%20substantial%20performance%20degradation%20in%0Aextended%20reasoning%20steps%20%28nine%20models%20demonstrate%20over%2050%25%20performance%20drop%29.%0ASecond%2C%20compared%20to%20calculation%20problems%2C%20MLLMs%20tend%20to%20take%20shortcuts%20when%0Asolving%20proving%20problems.%20Third%2C%20models%20lack%20auxiliary%20line%20awareness%2C%20and%0Aenhancing%20this%20capability%20proves%20particularly%20beneficial%20for%20overall%20geometry%0Areasoning%20improvement.%20These%20findings%20establish%20GeoLaux%20as%20both%20a%20benchmark%20for%0Aevaluating%20MLLMs%27%20long-step%20geometric%20reasoning%20with%20auxiliary%20lines%20and%20a%0Aguide%20for%20capability%20advancement.%20Our%20dataset%20and%20code%20are%20included%20in%0Asupplementary%20materials%20and%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoLaux%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520MLLMs%2527%2520Geometry%2520Performance%2520on%250A%2520%2520Long-Step%2520Problems%2520Requiring%2520Auxiliary%2520Lines%26entry.906535625%3DYumeng%2520Fu%2520and%2520Jiayin%2520Zhu%2520and%2520Lingling%2520Zhang%2520and%2520Bo%2520Zhao%2520and%2520Shaoxuan%2520Ma%2520and%2520Yushun%2520Zhang%2520and%2520Yanrui%2520Wu%2520and%2520Wenjun%2520Wu%26entry.1292438233%3D%2520%2520Geometry%2520problem%2520solving%2520%2528GPS%2529%2520requires%2520models%2520to%2520master%2520diagram%250Acomprehension%252C%2520logical%2520reasoning%252C%2520knowledge%2520application%252C%2520numerical%2520computation%252C%250Aand%2520auxiliary%2520line%2520construction.%2520This%2520presents%2520a%2520significant%2520challenge%2520for%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520However%252C%2520existing%2520benchmarks%2520for%250Aevaluating%2520MLLM%2520geometry%2520skills%2520overlook%2520auxiliary%2520line%2520construction%2520and%2520lack%250Afine-grained%2520process%2520evaluation%252C%2520making%2520them%2520insufficient%2520for%2520assessing%2520MLLMs%2527%250Along-step%2520reasoning%2520abilities.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520present%2520the%2520GeoLaux%250Abenchmark%252C%2520comprising%25202%252C186%2520geometry%2520problems%252C%2520incorporating%2520both%2520calculation%250Aand%2520proving%2520questions.%2520Notably%252C%2520the%2520problems%2520require%2520an%2520average%2520of%25206.51%250Areasoning%2520steps%252C%2520with%2520a%2520maximum%2520of%252024%2520steps%252C%2520and%252041.8%2525%2520of%2520them%2520need%2520auxiliary%250Aline%2520construction.%2520Building%2520on%2520the%2520dataset%252C%2520we%2520design%2520a%2520novel%2520five-dimensional%250Aevaluation%2520strategy%2520assessing%2520answer%2520correctness%252C%2520process%2520correctness%252C%2520process%250Aquality%252C%2520auxiliary%2520line%2520impact%252C%2520and%2520error%2520causes.%2520Extensive%2520experiments%2520on%252013%250Aleading%2520MLLMs%2520%2528including%2520thinking%2520models%2520and%2520non-thinking%2520models%2529%2520yield%2520three%250Apivotal%2520findings%253A%2520First%252C%2520models%2520exhibit%2520substantial%2520performance%2520degradation%2520in%250Aextended%2520reasoning%2520steps%2520%2528nine%2520models%2520demonstrate%2520over%252050%2525%2520performance%2520drop%2529.%250ASecond%252C%2520compared%2520to%2520calculation%2520problems%252C%2520MLLMs%2520tend%2520to%2520take%2520shortcuts%2520when%250Asolving%2520proving%2520problems.%2520Third%252C%2520models%2520lack%2520auxiliary%2520line%2520awareness%252C%2520and%250Aenhancing%2520this%2520capability%2520proves%2520particularly%2520beneficial%2520for%2520overall%2520geometry%250Areasoning%2520improvement.%2520These%2520findings%2520establish%2520GeoLaux%2520as%2520both%2520a%2520benchmark%2520for%250Aevaluating%2520MLLMs%2527%2520long-step%2520geometric%2520reasoning%2520with%2520auxiliary%2520lines%2520and%2520a%250Aguide%2520for%2520capability%2520advancement.%2520Our%2520dataset%2520and%2520code%2520are%2520included%2520in%250Asupplementary%2520materials%2520and%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoLaux%3A%20A%20Benchmark%20for%20Evaluating%20MLLMs%27%20Geometry%20Performance%20on%0A%20%20Long-Step%20Problems%20Requiring%20Auxiliary%20Lines&entry.906535625=Yumeng%20Fu%20and%20Jiayin%20Zhu%20and%20Lingling%20Zhang%20and%20Bo%20Zhao%20and%20Shaoxuan%20Ma%20and%20Yushun%20Zhang%20and%20Yanrui%20Wu%20and%20Wenjun%20Wu&entry.1292438233=%20%20Geometry%20problem%20solving%20%28GPS%29%20requires%20models%20to%20master%20diagram%0Acomprehension%2C%20logical%20reasoning%2C%20knowledge%20application%2C%20numerical%20computation%2C%0Aand%20auxiliary%20line%20construction.%20This%20presents%20a%20significant%20challenge%20for%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29.%20However%2C%20existing%20benchmarks%20for%0Aevaluating%20MLLM%20geometry%20skills%20overlook%20auxiliary%20line%20construction%20and%20lack%0Afine-grained%20process%20evaluation%2C%20making%20them%20insufficient%20for%20assessing%20MLLMs%27%0Along-step%20reasoning%20abilities.%20To%20bridge%20these%20gaps%2C%20we%20present%20the%20GeoLaux%0Abenchmark%2C%20comprising%202%2C186%20geometry%20problems%2C%20incorporating%20both%20calculation%0Aand%20proving%20questions.%20Notably%2C%20the%20problems%20require%20an%20average%20of%206.51%0Areasoning%20steps%2C%20with%20a%20maximum%20of%2024%20steps%2C%20and%2041.8%25%20of%20them%20need%20auxiliary%0Aline%20construction.%20Building%20on%20the%20dataset%2C%20we%20design%20a%20novel%20five-dimensional%0Aevaluation%20strategy%20assessing%20answer%20correctness%2C%20process%20correctness%2C%20process%0Aquality%2C%20auxiliary%20line%20impact%2C%20and%20error%20causes.%20Extensive%20experiments%20on%2013%0Aleading%20MLLMs%20%28including%20thinking%20models%20and%20non-thinking%20models%29%20yield%20three%0Apivotal%20findings%3A%20First%2C%20models%20exhibit%20substantial%20performance%20degradation%20in%0Aextended%20reasoning%20steps%20%28nine%20models%20demonstrate%20over%2050%25%20performance%20drop%29.%0ASecond%2C%20compared%20to%20calculation%20problems%2C%20MLLMs%20tend%20to%20take%20shortcuts%20when%0Asolving%20proving%20problems.%20Third%2C%20models%20lack%20auxiliary%20line%20awareness%2C%20and%0Aenhancing%20this%20capability%20proves%20particularly%20beneficial%20for%20overall%20geometry%0Areasoning%20improvement.%20These%20findings%20establish%20GeoLaux%20as%20both%20a%20benchmark%20for%0Aevaluating%20MLLMs%27%20long-step%20geometric%20reasoning%20with%20auxiliary%20lines%20and%20a%0Aguide%20for%20capability%20advancement.%20Our%20dataset%20and%20code%20are%20included%20in%0Asupplementary%20materials%20and%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06226v1&entry.124074799=Read"},
{"title": "SAR Strikes Back: A New Hope for RSVQA", "author": "Lucrezia Tosato and Flora Weissgerber and Laurent Wendling and Sylvain Lobry", "abstract": "  Remote Sensing Visual Question Answering (RSVQA) is a task that extracts\ninformation from satellite images to answer questions in natural language,\naiding image interpretation. While several methods exist for optical images\nwith varying spectral bands and resolutions, only recently have high-resolution\nSynthetic Aperture Radar (SAR) images been explored. SAR's ability to operate\nin all weather conditions and capture electromagnetic features makes it a\npromising modality, yet no study has compared SAR and optical imagery in RSVQA\nor proposed effective fusion strategies. This work investigates how to\nintegrate SAR data into RSVQA and how to best combine it with optical images.\nWe present a dataset that enables SAR-based RSVQA and explore two pipelines for\nthe task. The first is an end-to-end model, while the second is a two-stage\nframework: SAR information is first extracted and translated into text, which\nis then processed by a language model to produce the final answer. Our results\nshow that the two-stage model performs better, improving accuracy by nearly 10%\nover the end-to-end approach. We also evaluate fusion strategies for combining\nSAR and optical data. A decision-level fusion yields the best results, with an\nF1-micro score of 75.00%, F1-average of 81.21%, and overall accuracy of 75.49%\non the proposed dataset. SAR proves especially beneficial for questions related\nto specific land cover types, such as water areas, demonstrating its value as a\ncomplementary modality to optical imagery.\n", "link": "http://arxiv.org/abs/2501.08131v2", "date": "2025-08-08", "relevancy": 2.5293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA&body=Title%3A%20SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA%0AAuthor%3A%20Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry%0AAbstract%3A%20%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20is%20a%20task%20that%20extracts%0Ainformation%20from%20satellite%20images%20to%20answer%20questions%20in%20natural%20language%2C%0Aaiding%20image%20interpretation.%20While%20several%20methods%20exist%20for%20optical%20images%0Awith%20varying%20spectral%20bands%20and%20resolutions%2C%20only%20recently%20have%20high-resolution%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20images%20been%20explored.%20SAR%27s%20ability%20to%20operate%0Ain%20all%20weather%20conditions%20and%20capture%20electromagnetic%20features%20makes%20it%20a%0Apromising%20modality%2C%20yet%20no%20study%20has%20compared%20SAR%20and%20optical%20imagery%20in%20RSVQA%0Aor%20proposed%20effective%20fusion%20strategies.%20This%20work%20investigates%20how%20to%0Aintegrate%20SAR%20data%20into%20RSVQA%20and%20how%20to%20best%20combine%20it%20with%20optical%20images.%0AWe%20present%20a%20dataset%20that%20enables%20SAR-based%20RSVQA%20and%20explore%20two%20pipelines%20for%0Athe%20task.%20The%20first%20is%20an%20end-to-end%20model%2C%20while%20the%20second%20is%20a%20two-stage%0Aframework%3A%20SAR%20information%20is%20first%20extracted%20and%20translated%20into%20text%2C%20which%0Ais%20then%20processed%20by%20a%20language%20model%20to%20produce%20the%20final%20answer.%20Our%20results%0Ashow%20that%20the%20two-stage%20model%20performs%20better%2C%20improving%20accuracy%20by%20nearly%2010%25%0Aover%20the%20end-to-end%20approach.%20We%20also%20evaluate%20fusion%20strategies%20for%20combining%0ASAR%20and%20optical%20data.%20A%20decision-level%20fusion%20yields%20the%20best%20results%2C%20with%20an%0AF1-micro%20score%20of%2075.00%25%2C%20F1-average%20of%2081.21%25%2C%20and%20overall%20accuracy%20of%2075.49%25%0Aon%20the%20proposed%20dataset.%20SAR%20proves%20especially%20beneficial%20for%20questions%20related%0Ato%20specific%20land%20cover%20types%2C%20such%20as%20water%20areas%2C%20demonstrating%20its%20value%20as%20a%0Acomplementary%20modality%20to%20optical%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAR%2520Strikes%2520Back%253A%2520A%2520New%2520Hope%2520for%2520RSVQA%26entry.906535625%3DLucrezia%2520Tosato%2520and%2520Flora%2520Weissgerber%2520and%2520Laurent%2520Wendling%2520and%2520Sylvain%2520Lobry%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2520%2528RSVQA%2529%2520is%2520a%2520task%2520that%2520extracts%250Ainformation%2520from%2520satellite%2520images%2520to%2520answer%2520questions%2520in%2520natural%2520language%252C%250Aaiding%2520image%2520interpretation.%2520While%2520several%2520methods%2520exist%2520for%2520optical%2520images%250Awith%2520varying%2520spectral%2520bands%2520and%2520resolutions%252C%2520only%2520recently%2520have%2520high-resolution%250ASynthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520images%2520been%2520explored.%2520SAR%2527s%2520ability%2520to%2520operate%250Ain%2520all%2520weather%2520conditions%2520and%2520capture%2520electromagnetic%2520features%2520makes%2520it%2520a%250Apromising%2520modality%252C%2520yet%2520no%2520study%2520has%2520compared%2520SAR%2520and%2520optical%2520imagery%2520in%2520RSVQA%250Aor%2520proposed%2520effective%2520fusion%2520strategies.%2520This%2520work%2520investigates%2520how%2520to%250Aintegrate%2520SAR%2520data%2520into%2520RSVQA%2520and%2520how%2520to%2520best%2520combine%2520it%2520with%2520optical%2520images.%250AWe%2520present%2520a%2520dataset%2520that%2520enables%2520SAR-based%2520RSVQA%2520and%2520explore%2520two%2520pipelines%2520for%250Athe%2520task.%2520The%2520first%2520is%2520an%2520end-to-end%2520model%252C%2520while%2520the%2520second%2520is%2520a%2520two-stage%250Aframework%253A%2520SAR%2520information%2520is%2520first%2520extracted%2520and%2520translated%2520into%2520text%252C%2520which%250Ais%2520then%2520processed%2520by%2520a%2520language%2520model%2520to%2520produce%2520the%2520final%2520answer.%2520Our%2520results%250Ashow%2520that%2520the%2520two-stage%2520model%2520performs%2520better%252C%2520improving%2520accuracy%2520by%2520nearly%252010%2525%250Aover%2520the%2520end-to-end%2520approach.%2520We%2520also%2520evaluate%2520fusion%2520strategies%2520for%2520combining%250ASAR%2520and%2520optical%2520data.%2520A%2520decision-level%2520fusion%2520yields%2520the%2520best%2520results%252C%2520with%2520an%250AF1-micro%2520score%2520of%252075.00%2525%252C%2520F1-average%2520of%252081.21%2525%252C%2520and%2520overall%2520accuracy%2520of%252075.49%2525%250Aon%2520the%2520proposed%2520dataset.%2520SAR%2520proves%2520especially%2520beneficial%2520for%2520questions%2520related%250Ato%2520specific%2520land%2520cover%2520types%252C%2520such%2520as%2520water%2520areas%252C%2520demonstrating%2520its%2520value%2520as%2520a%250Acomplementary%2520modality%2520to%2520optical%2520imagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA&entry.906535625=Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry&entry.1292438233=%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20is%20a%20task%20that%20extracts%0Ainformation%20from%20satellite%20images%20to%20answer%20questions%20in%20natural%20language%2C%0Aaiding%20image%20interpretation.%20While%20several%20methods%20exist%20for%20optical%20images%0Awith%20varying%20spectral%20bands%20and%20resolutions%2C%20only%20recently%20have%20high-resolution%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20images%20been%20explored.%20SAR%27s%20ability%20to%20operate%0Ain%20all%20weather%20conditions%20and%20capture%20electromagnetic%20features%20makes%20it%20a%0Apromising%20modality%2C%20yet%20no%20study%20has%20compared%20SAR%20and%20optical%20imagery%20in%20RSVQA%0Aor%20proposed%20effective%20fusion%20strategies.%20This%20work%20investigates%20how%20to%0Aintegrate%20SAR%20data%20into%20RSVQA%20and%20how%20to%20best%20combine%20it%20with%20optical%20images.%0AWe%20present%20a%20dataset%20that%20enables%20SAR-based%20RSVQA%20and%20explore%20two%20pipelines%20for%0Athe%20task.%20The%20first%20is%20an%20end-to-end%20model%2C%20while%20the%20second%20is%20a%20two-stage%0Aframework%3A%20SAR%20information%20is%20first%20extracted%20and%20translated%20into%20text%2C%20which%0Ais%20then%20processed%20by%20a%20language%20model%20to%20produce%20the%20final%20answer.%20Our%20results%0Ashow%20that%20the%20two-stage%20model%20performs%20better%2C%20improving%20accuracy%20by%20nearly%2010%25%0Aover%20the%20end-to-end%20approach.%20We%20also%20evaluate%20fusion%20strategies%20for%20combining%0ASAR%20and%20optical%20data.%20A%20decision-level%20fusion%20yields%20the%20best%20results%2C%20with%20an%0AF1-micro%20score%20of%2075.00%25%2C%20F1-average%20of%2081.21%25%2C%20and%20overall%20accuracy%20of%2075.49%25%0Aon%20the%20proposed%20dataset.%20SAR%20proves%20especially%20beneficial%20for%20questions%20related%0Ato%20specific%20land%20cover%20types%2C%20such%20as%20water%20areas%2C%20demonstrating%20its%20value%20as%20a%0Acomplementary%20modality%20to%20optical%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08131v2&entry.124074799=Read"},
{"title": "Self-Steering Language Models", "author": "Gabriel Grand and Joshua B. Tenenbaum and Vikash K. Mansinghka and Alexander K. Lew and Jacob Andreas", "abstract": "  While test-time reasoning enables language models (LMs) to tackle complex\ntasks, searching or planning in natural language can be slow, costly, and\nerror-prone. But even when LMs struggle to emulate the precise reasoning steps\nneeded to solve a problem, they often excel at describing its abstract\nstructure--both how to verify solutions and how to search for them. This paper\nintroduces DisCIPL, a method for \"self-steering\" LMs where a Planner model\ngenerates a task-specific inference program that is executed by a population of\nFollower models. Our approach equips LMs with the ability to write recursive\nsearch procedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much\nlarger models, including GPT-4o and o1, on challenging constrained generation\ntasks. Our work opens up a design space of highly-parallelized Monte Carlo\ninference strategies that outperform standard best-of-N sampling, require no\nfinetuning, and can be implemented automatically by existing LMs.\n", "link": "http://arxiv.org/abs/2504.07081v2", "date": "2025-08-08", "relevancy": 2.521, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Steering%20Language%20Models&body=Title%3A%20Self-Steering%20Language%20Models%0AAuthor%3A%20Gabriel%20Grand%20and%20Joshua%20B.%20Tenenbaum%20and%20Vikash%20K.%20Mansinghka%20and%20Alexander%20K.%20Lew%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20While%20test-time%20reasoning%20enables%20language%20models%20%28LMs%29%20to%20tackle%20complex%0Atasks%2C%20searching%20or%20planning%20in%20natural%20language%20can%20be%20slow%2C%20costly%2C%20and%0Aerror-prone.%20But%20even%20when%20LMs%20struggle%20to%20emulate%20the%20precise%20reasoning%20steps%0Aneeded%20to%20solve%20a%20problem%2C%20they%20often%20excel%20at%20describing%20its%20abstract%0Astructure--both%20how%20to%20verify%20solutions%20and%20how%20to%20search%20for%20them.%20This%20paper%0Aintroduces%20DisCIPL%2C%20a%20method%20for%20%22self-steering%22%20LMs%20where%20a%20Planner%20model%0Agenerates%20a%20task-specific%20inference%20program%20that%20is%20executed%20by%20a%20population%20of%0AFollower%20models.%20Our%20approach%20equips%20LMs%20with%20the%20ability%20to%20write%20recursive%0Asearch%20procedures%20that%20guide%20LM%20inference%2C%20enabling%20new%20forms%20of%20verifiable%20and%0Aefficient%20reasoning.%20When%20instantiated%20with%20a%20small%20Follower%20%28e.g.%2C%0ALlama-3.2-1B%20or%20Qwen3-1.7B%29%2C%20DisCIPL%20matches%20%28and%20sometimes%20outperforms%29%20much%0Alarger%20models%2C%20including%20GPT-4o%20and%20o1%2C%20on%20challenging%20constrained%20generation%0Atasks.%20Our%20work%20opens%20up%20a%20design%20space%20of%20highly-parallelized%20Monte%20Carlo%0Ainference%20strategies%20that%20outperform%20standard%20best-of-N%20sampling%2C%20require%20no%0Afinetuning%2C%20and%20can%20be%20implemented%20automatically%20by%20existing%20LMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Steering%2520Language%2520Models%26entry.906535625%3DGabriel%2520Grand%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Vikash%2520K.%2520Mansinghka%2520and%2520Alexander%2520K.%2520Lew%2520and%2520Jacob%2520Andreas%26entry.1292438233%3D%2520%2520While%2520test-time%2520reasoning%2520enables%2520language%2520models%2520%2528LMs%2529%2520to%2520tackle%2520complex%250Atasks%252C%2520searching%2520or%2520planning%2520in%2520natural%2520language%2520can%2520be%2520slow%252C%2520costly%252C%2520and%250Aerror-prone.%2520But%2520even%2520when%2520LMs%2520struggle%2520to%2520emulate%2520the%2520precise%2520reasoning%2520steps%250Aneeded%2520to%2520solve%2520a%2520problem%252C%2520they%2520often%2520excel%2520at%2520describing%2520its%2520abstract%250Astructure--both%2520how%2520to%2520verify%2520solutions%2520and%2520how%2520to%2520search%2520for%2520them.%2520This%2520paper%250Aintroduces%2520DisCIPL%252C%2520a%2520method%2520for%2520%2522self-steering%2522%2520LMs%2520where%2520a%2520Planner%2520model%250Agenerates%2520a%2520task-specific%2520inference%2520program%2520that%2520is%2520executed%2520by%2520a%2520population%2520of%250AFollower%2520models.%2520Our%2520approach%2520equips%2520LMs%2520with%2520the%2520ability%2520to%2520write%2520recursive%250Asearch%2520procedures%2520that%2520guide%2520LM%2520inference%252C%2520enabling%2520new%2520forms%2520of%2520verifiable%2520and%250Aefficient%2520reasoning.%2520When%2520instantiated%2520with%2520a%2520small%2520Follower%2520%2528e.g.%252C%250ALlama-3.2-1B%2520or%2520Qwen3-1.7B%2529%252C%2520DisCIPL%2520matches%2520%2528and%2520sometimes%2520outperforms%2529%2520much%250Alarger%2520models%252C%2520including%2520GPT-4o%2520and%2520o1%252C%2520on%2520challenging%2520constrained%2520generation%250Atasks.%2520Our%2520work%2520opens%2520up%2520a%2520design%2520space%2520of%2520highly-parallelized%2520Monte%2520Carlo%250Ainference%2520strategies%2520that%2520outperform%2520standard%2520best-of-N%2520sampling%252C%2520require%2520no%250Afinetuning%252C%2520and%2520can%2520be%2520implemented%2520automatically%2520by%2520existing%2520LMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Steering%20Language%20Models&entry.906535625=Gabriel%20Grand%20and%20Joshua%20B.%20Tenenbaum%20and%20Vikash%20K.%20Mansinghka%20and%20Alexander%20K.%20Lew%20and%20Jacob%20Andreas&entry.1292438233=%20%20While%20test-time%20reasoning%20enables%20language%20models%20%28LMs%29%20to%20tackle%20complex%0Atasks%2C%20searching%20or%20planning%20in%20natural%20language%20can%20be%20slow%2C%20costly%2C%20and%0Aerror-prone.%20But%20even%20when%20LMs%20struggle%20to%20emulate%20the%20precise%20reasoning%20steps%0Aneeded%20to%20solve%20a%20problem%2C%20they%20often%20excel%20at%20describing%20its%20abstract%0Astructure--both%20how%20to%20verify%20solutions%20and%20how%20to%20search%20for%20them.%20This%20paper%0Aintroduces%20DisCIPL%2C%20a%20method%20for%20%22self-steering%22%20LMs%20where%20a%20Planner%20model%0Agenerates%20a%20task-specific%20inference%20program%20that%20is%20executed%20by%20a%20population%20of%0AFollower%20models.%20Our%20approach%20equips%20LMs%20with%20the%20ability%20to%20write%20recursive%0Asearch%20procedures%20that%20guide%20LM%20inference%2C%20enabling%20new%20forms%20of%20verifiable%20and%0Aefficient%20reasoning.%20When%20instantiated%20with%20a%20small%20Follower%20%28e.g.%2C%0ALlama-3.2-1B%20or%20Qwen3-1.7B%29%2C%20DisCIPL%20matches%20%28and%20sometimes%20outperforms%29%20much%0Alarger%20models%2C%20including%20GPT-4o%20and%20o1%2C%20on%20challenging%20constrained%20generation%0Atasks.%20Our%20work%20opens%20up%20a%20design%20space%20of%20highly-parallelized%20Monte%20Carlo%0Ainference%20strategies%20that%20outperform%20standard%20best-of-N%20sampling%2C%20require%20no%0Afinetuning%2C%20and%20can%20be%20implemented%20automatically%20by%20existing%20LMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07081v2&entry.124074799=Read"},
{"title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior\n  Across 15 Cities", "author": "Kieran Elrod and Katherine Flanigan and Mario Berg\u00e9s", "abstract": "  Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.\n", "link": "http://arxiv.org/abs/2508.06342v1", "date": "2025-08-08", "relevancy": 2.514, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4986}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Street%20View%20Sociability%3A%20Interpretable%20Analysis%20of%20Urban%20Social%20Behavior%0A%20%20Across%2015%20Cities&body=Title%3A%20Street%20View%20Sociability%3A%20Interpretable%20Analysis%20of%20Urban%20Social%20Behavior%0A%20%20Across%2015%20Cities%0AAuthor%3A%20Kieran%20Elrod%20and%20Katherine%20Flanigan%20and%20Mario%20Berg%C3%A9s%0AAbstract%3A%20%20%20Designing%20socially%20active%20streets%20has%20long%20been%20a%20goal%20of%20urban%20planning%2C%20yet%0Aexisting%20quantitative%20research%20largely%20measures%20pedestrian%20volume%20rather%20than%0Athe%20quality%20of%20social%20interactions.%20We%20hypothesize%20that%20street%20view%20imagery%20--%0Aan%20inexpensive%20data%20source%20with%20global%20coverage%20--%20contains%20latent%20social%0Ainformation%20that%20can%20be%20extracted%20and%20interpreted%20through%20established%20social%0Ascience%20theory.%20As%20a%20proof%20of%20concept%2C%20we%20analyzed%202%2C998%20street%20view%20images%0Afrom%2015%20cities%20using%20a%20multimodal%20large%20language%20model%20guided%20by%20Mehta%27s%0Ataxonomy%20of%20passive%2C%20fleeting%2C%20and%20enduring%20sociability%20--%20one%20illustrative%0Aexample%20of%20a%20theory%20grounded%20in%20urban%20design%20that%20could%20be%20substituted%20or%0Acomplemented%20by%20other%20sociological%20frameworks.%20We%20then%20used%20linear%20regression%0Amodels%2C%20controlling%20for%20factors%20like%20weather%2C%20time%20of%20day%2C%20and%20pedestrian%0Acounts%2C%20to%20test%20whether%20the%20inferred%20sociability%20measures%20correlate%20with%0Acity-level%20place%20attachment%20scores%20from%20the%20World%20Values%20Survey%20and%20with%0Aenvironmental%20predictors%20%28e.g.%2C%20green%2C%20sky%2C%20and%20water%20view%20indices%29%20derived%0Afrom%20individual%20street%20view%20images.%20Results%20aligned%20with%20long-standing%20urban%0Aplanning%20theory%3A%20the%20sky%20view%20index%20was%20associated%20with%20all%20three%20sociability%0Atypes%2C%20the%20green%20view%20index%20predicted%20enduring%20sociability%2C%20and%20place%0Aattachment%20was%20positively%20associated%20with%20fleeting%20sociability.%20These%20results%0Aprovide%20preliminary%20evidence%20that%20street%20view%20images%20can%20be%20used%20to%20infer%0Arelationships%20between%20specific%20types%20of%20social%20interactions%20and%20built%0Aenvironment%20variables.%20Further%20research%20could%20establish%20street%20view%20imagery%20as%0Aa%20scalable%2C%20privacy-preserving%20tool%20for%20studying%20urban%20sociability%2C%20enabling%0Across-cultural%20theory%20testing%20and%20evidence-based%20design%20of%20socially%20vibrant%0Acities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreet%2520View%2520Sociability%253A%2520Interpretable%2520Analysis%2520of%2520Urban%2520Social%2520Behavior%250A%2520%2520Across%252015%2520Cities%26entry.906535625%3DKieran%2520Elrod%2520and%2520Katherine%2520Flanigan%2520and%2520Mario%2520Berg%25C3%25A9s%26entry.1292438233%3D%2520%2520Designing%2520socially%2520active%2520streets%2520has%2520long%2520been%2520a%2520goal%2520of%2520urban%2520planning%252C%2520yet%250Aexisting%2520quantitative%2520research%2520largely%2520measures%2520pedestrian%2520volume%2520rather%2520than%250Athe%2520quality%2520of%2520social%2520interactions.%2520We%2520hypothesize%2520that%2520street%2520view%2520imagery%2520--%250Aan%2520inexpensive%2520data%2520source%2520with%2520global%2520coverage%2520--%2520contains%2520latent%2520social%250Ainformation%2520that%2520can%2520be%2520extracted%2520and%2520interpreted%2520through%2520established%2520social%250Ascience%2520theory.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520analyzed%25202%252C998%2520street%2520view%2520images%250Afrom%252015%2520cities%2520using%2520a%2520multimodal%2520large%2520language%2520model%2520guided%2520by%2520Mehta%2527s%250Ataxonomy%2520of%2520passive%252C%2520fleeting%252C%2520and%2520enduring%2520sociability%2520--%2520one%2520illustrative%250Aexample%2520of%2520a%2520theory%2520grounded%2520in%2520urban%2520design%2520that%2520could%2520be%2520substituted%2520or%250Acomplemented%2520by%2520other%2520sociological%2520frameworks.%2520We%2520then%2520used%2520linear%2520regression%250Amodels%252C%2520controlling%2520for%2520factors%2520like%2520weather%252C%2520time%2520of%2520day%252C%2520and%2520pedestrian%250Acounts%252C%2520to%2520test%2520whether%2520the%2520inferred%2520sociability%2520measures%2520correlate%2520with%250Acity-level%2520place%2520attachment%2520scores%2520from%2520the%2520World%2520Values%2520Survey%2520and%2520with%250Aenvironmental%2520predictors%2520%2528e.g.%252C%2520green%252C%2520sky%252C%2520and%2520water%2520view%2520indices%2529%2520derived%250Afrom%2520individual%2520street%2520view%2520images.%2520Results%2520aligned%2520with%2520long-standing%2520urban%250Aplanning%2520theory%253A%2520the%2520sky%2520view%2520index%2520was%2520associated%2520with%2520all%2520three%2520sociability%250Atypes%252C%2520the%2520green%2520view%2520index%2520predicted%2520enduring%2520sociability%252C%2520and%2520place%250Aattachment%2520was%2520positively%2520associated%2520with%2520fleeting%2520sociability.%2520These%2520results%250Aprovide%2520preliminary%2520evidence%2520that%2520street%2520view%2520images%2520can%2520be%2520used%2520to%2520infer%250Arelationships%2520between%2520specific%2520types%2520of%2520social%2520interactions%2520and%2520built%250Aenvironment%2520variables.%2520Further%2520research%2520could%2520establish%2520street%2520view%2520imagery%2520as%250Aa%2520scalable%252C%2520privacy-preserving%2520tool%2520for%2520studying%2520urban%2520sociability%252C%2520enabling%250Across-cultural%2520theory%2520testing%2520and%2520evidence-based%2520design%2520of%2520socially%2520vibrant%250Acities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Street%20View%20Sociability%3A%20Interpretable%20Analysis%20of%20Urban%20Social%20Behavior%0A%20%20Across%2015%20Cities&entry.906535625=Kieran%20Elrod%20and%20Katherine%20Flanigan%20and%20Mario%20Berg%C3%A9s&entry.1292438233=%20%20Designing%20socially%20active%20streets%20has%20long%20been%20a%20goal%20of%20urban%20planning%2C%20yet%0Aexisting%20quantitative%20research%20largely%20measures%20pedestrian%20volume%20rather%20than%0Athe%20quality%20of%20social%20interactions.%20We%20hypothesize%20that%20street%20view%20imagery%20--%0Aan%20inexpensive%20data%20source%20with%20global%20coverage%20--%20contains%20latent%20social%0Ainformation%20that%20can%20be%20extracted%20and%20interpreted%20through%20established%20social%0Ascience%20theory.%20As%20a%20proof%20of%20concept%2C%20we%20analyzed%202%2C998%20street%20view%20images%0Afrom%2015%20cities%20using%20a%20multimodal%20large%20language%20model%20guided%20by%20Mehta%27s%0Ataxonomy%20of%20passive%2C%20fleeting%2C%20and%20enduring%20sociability%20--%20one%20illustrative%0Aexample%20of%20a%20theory%20grounded%20in%20urban%20design%20that%20could%20be%20substituted%20or%0Acomplemented%20by%20other%20sociological%20frameworks.%20We%20then%20used%20linear%20regression%0Amodels%2C%20controlling%20for%20factors%20like%20weather%2C%20time%20of%20day%2C%20and%20pedestrian%0Acounts%2C%20to%20test%20whether%20the%20inferred%20sociability%20measures%20correlate%20with%0Acity-level%20place%20attachment%20scores%20from%20the%20World%20Values%20Survey%20and%20with%0Aenvironmental%20predictors%20%28e.g.%2C%20green%2C%20sky%2C%20and%20water%20view%20indices%29%20derived%0Afrom%20individual%20street%20view%20images.%20Results%20aligned%20with%20long-standing%20urban%0Aplanning%20theory%3A%20the%20sky%20view%20index%20was%20associated%20with%20all%20three%20sociability%0Atypes%2C%20the%20green%20view%20index%20predicted%20enduring%20sociability%2C%20and%20place%0Aattachment%20was%20positively%20associated%20with%20fleeting%20sociability.%20These%20results%0Aprovide%20preliminary%20evidence%20that%20street%20view%20images%20can%20be%20used%20to%20infer%0Arelationships%20between%20specific%20types%20of%20social%20interactions%20and%20built%0Aenvironment%20variables.%20Further%20research%20could%20establish%20street%20view%20imagery%20as%0Aa%20scalable%2C%20privacy-preserving%20tool%20for%20studying%20urban%20sociability%2C%20enabling%0Across-cultural%20theory%20testing%20and%20evidence-based%20design%20of%20socially%20vibrant%0Acities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06342v1&entry.124074799=Read"},
{"title": "Decompositional Reasoning for Graph Retrieval with Large Language Models", "author": "Valentin Six and Evan Dufraisse and Ga\u00ebl de Chalendar", "abstract": "  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n", "link": "http://arxiv.org/abs/2506.13380v2", "date": "2025-08-08", "relevancy": 2.5027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models&body=Title%3A%20Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models%0AAuthor%3A%20Valentin%20Six%20and%20Evan%20Dufraisse%20and%20Ga%C3%ABl%20de%20Chalendar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%2C%20but%20struggle%20with%0Amulti-hop%20reasoning%20and%20factual%20consistency%2C%20limiting%20their%20effectiveness%20on%0Aknowledge-intensive%20tasks%20like%20complex%20question%20answering%20%28QA%29.%20Linking%0AKnowledge%20Graphs%20%28KG%29%20and%20LLMs%20has%20shown%20promising%20results%2C%20but%20LLMs%20generally%0Alack%20the%20ability%20to%20reason%20efficiently%20over%20graph-structured%20information.%20To%0Atackle%20this%20problem%2C%20we%20propose%20a%20novel%20retrieval%20approach%20that%20integrates%0Atextual%20knowledge%20graphs%20into%20the%20LLM%20reasoning%20process%20via%20query%0Adecomposition.%20Our%20method%20decomposes%20complex%20questions%20into%20sub-questions%2C%0Aretrieves%20relevant%20textual%20subgraphs%2C%20and%20composes%20a%20question-specific%0Aknowledge%20graph%20to%20guide%20answer%20generation.%20For%20that%2C%20we%20use%20a%20weighted%0Asimilarity%20function%20that%20focuses%20on%20both%20the%20complex%20question%20and%20the%20generated%0Asubquestions%20to%20extract%20a%20relevant%20subgraph%2C%20which%20allows%20efficient%20and%20precise%0Aretrieval%20for%20complex%20questions%20and%20improves%20the%20performance%20of%20LLMs%20on%0Amulti-hop%20QA%20tasks.%20This%20structured%20reasoning%20pipeline%20enhances%20factual%0Agrounding%20and%20interpretability%20while%20leveraging%20the%20generative%20strengths%20of%0ALLMs.%20We%20evaluate%20our%20method%20on%20standard%20multi-hop%20QA%20benchmarks%20and%20show%20that%0Ait%20achieves%20comparable%20or%20superior%20performance%20to%20competitive%20existing%20methods%2C%0Ausing%20smaller%20models%20and%20fewer%20LLM%20calls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecompositional%2520Reasoning%2520for%2520Graph%2520Retrieval%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DValentin%2520Six%2520and%2520Evan%2520Dufraisse%2520and%2520Ga%25C3%25ABl%2520de%2520Chalendar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520many%2520NLP%2520tasks%252C%2520but%2520struggle%2520with%250Amulti-hop%2520reasoning%2520and%2520factual%2520consistency%252C%2520limiting%2520their%2520effectiveness%2520on%250Aknowledge-intensive%2520tasks%2520like%2520complex%2520question%2520answering%2520%2528QA%2529.%2520Linking%250AKnowledge%2520Graphs%2520%2528KG%2529%2520and%2520LLMs%2520has%2520shown%2520promising%2520results%252C%2520but%2520LLMs%2520generally%250Alack%2520the%2520ability%2520to%2520reason%2520efficiently%2520over%2520graph-structured%2520information.%2520To%250Atackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520retrieval%2520approach%2520that%2520integrates%250Atextual%2520knowledge%2520graphs%2520into%2520the%2520LLM%2520reasoning%2520process%2520via%2520query%250Adecomposition.%2520Our%2520method%2520decomposes%2520complex%2520questions%2520into%2520sub-questions%252C%250Aretrieves%2520relevant%2520textual%2520subgraphs%252C%2520and%2520composes%2520a%2520question-specific%250Aknowledge%2520graph%2520to%2520guide%2520answer%2520generation.%2520For%2520that%252C%2520we%2520use%2520a%2520weighted%250Asimilarity%2520function%2520that%2520focuses%2520on%2520both%2520the%2520complex%2520question%2520and%2520the%2520generated%250Asubquestions%2520to%2520extract%2520a%2520relevant%2520subgraph%252C%2520which%2520allows%2520efficient%2520and%2520precise%250Aretrieval%2520for%2520complex%2520questions%2520and%2520improves%2520the%2520performance%2520of%2520LLMs%2520on%250Amulti-hop%2520QA%2520tasks.%2520This%2520structured%2520reasoning%2520pipeline%2520enhances%2520factual%250Agrounding%2520and%2520interpretability%2520while%2520leveraging%2520the%2520generative%2520strengths%2520of%250ALLMs.%2520We%2520evaluate%2520our%2520method%2520on%2520standard%2520multi-hop%2520QA%2520benchmarks%2520and%2520show%2520that%250Ait%2520achieves%2520comparable%2520or%2520superior%2520performance%2520to%2520competitive%2520existing%2520methods%252C%250Ausing%2520smaller%2520models%2520and%2520fewer%2520LLM%2520calls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models&entry.906535625=Valentin%20Six%20and%20Evan%20Dufraisse%20and%20Ga%C3%ABl%20de%20Chalendar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%2C%20but%20struggle%20with%0Amulti-hop%20reasoning%20and%20factual%20consistency%2C%20limiting%20their%20effectiveness%20on%0Aknowledge-intensive%20tasks%20like%20complex%20question%20answering%20%28QA%29.%20Linking%0AKnowledge%20Graphs%20%28KG%29%20and%20LLMs%20has%20shown%20promising%20results%2C%20but%20LLMs%20generally%0Alack%20the%20ability%20to%20reason%20efficiently%20over%20graph-structured%20information.%20To%0Atackle%20this%20problem%2C%20we%20propose%20a%20novel%20retrieval%20approach%20that%20integrates%0Atextual%20knowledge%20graphs%20into%20the%20LLM%20reasoning%20process%20via%20query%0Adecomposition.%20Our%20method%20decomposes%20complex%20questions%20into%20sub-questions%2C%0Aretrieves%20relevant%20textual%20subgraphs%2C%20and%20composes%20a%20question-specific%0Aknowledge%20graph%20to%20guide%20answer%20generation.%20For%20that%2C%20we%20use%20a%20weighted%0Asimilarity%20function%20that%20focuses%20on%20both%20the%20complex%20question%20and%20the%20generated%0Asubquestions%20to%20extract%20a%20relevant%20subgraph%2C%20which%20allows%20efficient%20and%20precise%0Aretrieval%20for%20complex%20questions%20and%20improves%20the%20performance%20of%20LLMs%20on%0Amulti-hop%20QA%20tasks.%20This%20structured%20reasoning%20pipeline%20enhances%20factual%0Agrounding%20and%20interpretability%20while%20leveraging%20the%20generative%20strengths%20of%0ALLMs.%20We%20evaluate%20our%20method%20on%20standard%20multi-hop%20QA%20benchmarks%20and%20show%20that%0Ait%20achieves%20comparable%20or%20superior%20performance%20to%20competitive%20existing%20methods%2C%0Ausing%20smaller%20models%20and%20fewer%20LLM%20calls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13380v2&entry.124074799=Read"},
{"title": "Post-training for Efficient Communication via Convention Formation", "author": "Yilun Hua and Evan Wang and Yoav Artzi", "abstract": "  Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.\n", "link": "http://arxiv.org/abs/2508.06482v1", "date": "2025-08-08", "relevancy": 2.4684, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4997}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-training%20for%20Efficient%20Communication%20via%20Convention%20Formation&body=Title%3A%20Post-training%20for%20Efficient%20Communication%20via%20Convention%20Formation%0AAuthor%3A%20Yilun%20Hua%20and%20Evan%20Wang%20and%20Yoav%20Artzi%0AAbstract%3A%20%20%20Humans%20communicate%20with%20increasing%20efficiency%20in%20multi-turn%20interactions%2C%20by%0Aadapting%20their%20language%20and%20forming%20ad-hoc%20conventions.%20In%20contrast%2C%20prior%20work%0Ashows%20that%20LLMs%20do%20not%20naturally%20show%20this%20behavior.%20We%20develop%20a%20post-training%0Aprocess%20to%20develop%20this%20ability%20through%20targeted%20fine-tuning%20on%20heuristically%0Aidentified%20demonstrations%20of%20convention%20formation.%20We%20evaluate%20with%20two%20new%0Abenchmarks%20focused%20on%20this%20capability.%20First%2C%20we%20design%20a%20focused%2C%0Acognitively-motivated%20interaction%20benchmark%20that%20consistently%20elicits%20strong%0Aconvention%20formation%20trends%20in%20humans.%20Second%2C%20we%20create%20a%20new%0Adocument-grounded%20reference%20completion%20task%20that%20reflects%20in-the-wild%0Aconvention%20formation%20behavior.%20Our%20studies%20show%20significantly%20improved%0Aconvention%20formation%20abilities%20in%20post-trained%20LLMs%20across%20the%20two%20evaluation%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-training%2520for%2520Efficient%2520Communication%2520via%2520Convention%2520Formation%26entry.906535625%3DYilun%2520Hua%2520and%2520Evan%2520Wang%2520and%2520Yoav%2520Artzi%26entry.1292438233%3D%2520%2520Humans%2520communicate%2520with%2520increasing%2520efficiency%2520in%2520multi-turn%2520interactions%252C%2520by%250Aadapting%2520their%2520language%2520and%2520forming%2520ad-hoc%2520conventions.%2520In%2520contrast%252C%2520prior%2520work%250Ashows%2520that%2520LLMs%2520do%2520not%2520naturally%2520show%2520this%2520behavior.%2520We%2520develop%2520a%2520post-training%250Aprocess%2520to%2520develop%2520this%2520ability%2520through%2520targeted%2520fine-tuning%2520on%2520heuristically%250Aidentified%2520demonstrations%2520of%2520convention%2520formation.%2520We%2520evaluate%2520with%2520two%2520new%250Abenchmarks%2520focused%2520on%2520this%2520capability.%2520First%252C%2520we%2520design%2520a%2520focused%252C%250Acognitively-motivated%2520interaction%2520benchmark%2520that%2520consistently%2520elicits%2520strong%250Aconvention%2520formation%2520trends%2520in%2520humans.%2520Second%252C%2520we%2520create%2520a%2520new%250Adocument-grounded%2520reference%2520completion%2520task%2520that%2520reflects%2520in-the-wild%250Aconvention%2520formation%2520behavior.%2520Our%2520studies%2520show%2520significantly%2520improved%250Aconvention%2520formation%2520abilities%2520in%2520post-trained%2520LLMs%2520across%2520the%2520two%2520evaluation%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-training%20for%20Efficient%20Communication%20via%20Convention%20Formation&entry.906535625=Yilun%20Hua%20and%20Evan%20Wang%20and%20Yoav%20Artzi&entry.1292438233=%20%20Humans%20communicate%20with%20increasing%20efficiency%20in%20multi-turn%20interactions%2C%20by%0Aadapting%20their%20language%20and%20forming%20ad-hoc%20conventions.%20In%20contrast%2C%20prior%20work%0Ashows%20that%20LLMs%20do%20not%20naturally%20show%20this%20behavior.%20We%20develop%20a%20post-training%0Aprocess%20to%20develop%20this%20ability%20through%20targeted%20fine-tuning%20on%20heuristically%0Aidentified%20demonstrations%20of%20convention%20formation.%20We%20evaluate%20with%20two%20new%0Abenchmarks%20focused%20on%20this%20capability.%20First%2C%20we%20design%20a%20focused%2C%0Acognitively-motivated%20interaction%20benchmark%20that%20consistently%20elicits%20strong%0Aconvention%20formation%20trends%20in%20humans.%20Second%2C%20we%20create%20a%20new%0Adocument-grounded%20reference%20completion%20task%20that%20reflects%20in-the-wild%0Aconvention%20formation%20behavior.%20Our%20studies%20show%20significantly%20improved%0Aconvention%20formation%20abilities%20in%20post-trained%20LLMs%20across%20the%20two%20evaluation%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06482v1&entry.124074799=Read"},
{"title": "A Classification-Aware Super-Resolution Framework for Ship Targets in\n  SAR Imagery", "author": "Ch Muhammad Awais and Marco Reggiannini and Davide Moroni and Oktay Karakus", "abstract": "  High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.\n", "link": "http://arxiv.org/abs/2508.06407v1", "date": "2025-08-08", "relevancy": 2.465, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5019}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%0A%20%20SAR%20Imagery&body=Title%3A%20A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%0A%20%20SAR%20Imagery%0AAuthor%3A%20Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus%0AAbstract%3A%20%20%20High-resolution%20imagery%20plays%20a%20critical%20role%20in%20improving%20the%20performance%20of%0Avisual%20recognition%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%0AIn%20many%20domains%2C%20including%20remote%20sensing%20and%20surveillance%2C%20low-resolution%0Aimages%20can%20limit%20the%20accuracy%20of%20automated%20analysis.%20To%20address%20this%2C%0Asuper-resolution%20%28SR%29%20techniques%20have%20been%20widely%20adopted%20to%20attempt%20to%0Areconstruct%20high-resolution%20images%20from%20low-resolution%20inputs.%20Related%0Atraditional%20approaches%20focus%20solely%20on%20enhancing%20image%20quality%20based%20on%0Apixel-level%20metrics%2C%20leaving%20the%20relationship%20between%20super-resolved%20image%0Afidelity%20and%20downstream%20classification%20performance%20largely%20underexplored.%20This%0Araises%20a%20key%20question%3A%20can%20integrating%20classification%20objectives%20directly%20into%0Athe%20super-resolution%20process%20further%20improve%20classification%20accuracy%3F%20In%20this%0Apaper%2C%20we%20try%20to%20respond%20to%20this%20question%20by%20investigating%20the%20relationship%0Abetween%20super-resolution%20and%20classification%20through%20the%20deployment%20of%20a%0Aspecialised%20algorithmic%20strategy.%20We%20propose%20a%20novel%20methodology%20that%20increases%0Athe%20resolution%20of%20synthetic%20aperture%20radar%20imagery%20by%20optimising%20loss%20functions%0Athat%20account%20for%20both%20image%20quality%20and%20classification%20performance.%20Our%0Aapproach%20improves%20image%20quality%2C%20as%20measured%20by%20scientifically%20ascertained%0Aimage%20quality%20indicators%2C%20while%20also%20enhancing%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Classification-Aware%2520Super-Resolution%2520Framework%2520for%2520Ship%2520Targets%2520in%250A%2520%2520SAR%2520Imagery%26entry.906535625%3DCh%2520Muhammad%2520Awais%2520and%2520Marco%2520Reggiannini%2520and%2520Davide%2520Moroni%2520and%2520Oktay%2520Karakus%26entry.1292438233%3D%2520%2520High-resolution%2520imagery%2520plays%2520a%2520critical%2520role%2520in%2520improving%2520the%2520performance%2520of%250Avisual%2520recognition%2520tasks%2520such%2520as%2520classification%252C%2520detection%252C%2520and%2520segmentation.%250AIn%2520many%2520domains%252C%2520including%2520remote%2520sensing%2520and%2520surveillance%252C%2520low-resolution%250Aimages%2520can%2520limit%2520the%2520accuracy%2520of%2520automated%2520analysis.%2520To%2520address%2520this%252C%250Asuper-resolution%2520%2528SR%2529%2520techniques%2520have%2520been%2520widely%2520adopted%2520to%2520attempt%2520to%250Areconstruct%2520high-resolution%2520images%2520from%2520low-resolution%2520inputs.%2520Related%250Atraditional%2520approaches%2520focus%2520solely%2520on%2520enhancing%2520image%2520quality%2520based%2520on%250Apixel-level%2520metrics%252C%2520leaving%2520the%2520relationship%2520between%2520super-resolved%2520image%250Afidelity%2520and%2520downstream%2520classification%2520performance%2520largely%2520underexplored.%2520This%250Araises%2520a%2520key%2520question%253A%2520can%2520integrating%2520classification%2520objectives%2520directly%2520into%250Athe%2520super-resolution%2520process%2520further%2520improve%2520classification%2520accuracy%253F%2520In%2520this%250Apaper%252C%2520we%2520try%2520to%2520respond%2520to%2520this%2520question%2520by%2520investigating%2520the%2520relationship%250Abetween%2520super-resolution%2520and%2520classification%2520through%2520the%2520deployment%2520of%2520a%250Aspecialised%2520algorithmic%2520strategy.%2520We%2520propose%2520a%2520novel%2520methodology%2520that%2520increases%250Athe%2520resolution%2520of%2520synthetic%2520aperture%2520radar%2520imagery%2520by%2520optimising%2520loss%2520functions%250Athat%2520account%2520for%2520both%2520image%2520quality%2520and%2520classification%2520performance.%2520Our%250Aapproach%2520improves%2520image%2520quality%252C%2520as%2520measured%2520by%2520scientifically%2520ascertained%250Aimage%2520quality%2520indicators%252C%2520while%2520also%2520enhancing%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%0A%20%20SAR%20Imagery&entry.906535625=Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus&entry.1292438233=%20%20High-resolution%20imagery%20plays%20a%20critical%20role%20in%20improving%20the%20performance%20of%0Avisual%20recognition%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%0AIn%20many%20domains%2C%20including%20remote%20sensing%20and%20surveillance%2C%20low-resolution%0Aimages%20can%20limit%20the%20accuracy%20of%20automated%20analysis.%20To%20address%20this%2C%0Asuper-resolution%20%28SR%29%20techniques%20have%20been%20widely%20adopted%20to%20attempt%20to%0Areconstruct%20high-resolution%20images%20from%20low-resolution%20inputs.%20Related%0Atraditional%20approaches%20focus%20solely%20on%20enhancing%20image%20quality%20based%20on%0Apixel-level%20metrics%2C%20leaving%20the%20relationship%20between%20super-resolved%20image%0Afidelity%20and%20downstream%20classification%20performance%20largely%20underexplored.%20This%0Araises%20a%20key%20question%3A%20can%20integrating%20classification%20objectives%20directly%20into%0Athe%20super-resolution%20process%20further%20improve%20classification%20accuracy%3F%20In%20this%0Apaper%2C%20we%20try%20to%20respond%20to%20this%20question%20by%20investigating%20the%20relationship%0Abetween%20super-resolution%20and%20classification%20through%20the%20deployment%20of%20a%0Aspecialised%20algorithmic%20strategy.%20We%20propose%20a%20novel%20methodology%20that%20increases%0Athe%20resolution%20of%20synthetic%20aperture%20radar%20imagery%20by%20optimising%20loss%20functions%0Athat%20account%20for%20both%20image%20quality%20and%20classification%20performance.%20Our%0Aapproach%20improves%20image%20quality%2C%20as%20measured%20by%20scientifically%20ascertained%0Aimage%20quality%20indicators%2C%20while%20also%20enhancing%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06407v1&entry.124074799=Read"},
{"title": "Generative Video Bi-flow", "author": "Chen Liu and Tobias Ritschel", "abstract": "  We propose a novel generative video model to robustly learn temporal change\nas a neural Ordinary Differential Equation (ODE) flow with a bilinear objective\nwhich combines two aspects: The first is to map from the past into future video\nframes directly. Previous work has mapped the noise to new frames, a more\ncomputationally expensive process. Unfortunately, starting from the previous\nframe, instead of noise, is more prone to drifting errors. Hence, second, we\nadditionally learn how to remove the accumulated errors as the joint objective\nby adding noise during training. We demonstrate unconditional video generation\nin a streaming manner for various video datasets, all at competitive quality\ncompared to a conditional diffusion baseline but with higher speed, i.e., fewer\nODE solver steps.\n", "link": "http://arxiv.org/abs/2503.06364v2", "date": "2025-08-08", "relevancy": 2.4538, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6248}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6139}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Video%20Bi-flow&body=Title%3A%20Generative%20Video%20Bi-flow%0AAuthor%3A%20Chen%20Liu%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20generative%20video%20model%20to%20robustly%20learn%20temporal%20change%0Aas%20a%20neural%20Ordinary%20Differential%20Equation%20%28ODE%29%20flow%20with%20a%20bilinear%20objective%0Awhich%20combines%20two%20aspects%3A%20The%20first%20is%20to%20map%20from%20the%20past%20into%20future%20video%0Aframes%20directly.%20Previous%20work%20has%20mapped%20the%20noise%20to%20new%20frames%2C%20a%20more%0Acomputationally%20expensive%20process.%20Unfortunately%2C%20starting%20from%20the%20previous%0Aframe%2C%20instead%20of%20noise%2C%20is%20more%20prone%20to%20drifting%20errors.%20Hence%2C%20second%2C%20we%0Aadditionally%20learn%20how%20to%20remove%20the%20accumulated%20errors%20as%20the%20joint%20objective%0Aby%20adding%20noise%20during%20training.%20We%20demonstrate%20unconditional%20video%20generation%0Ain%20a%20streaming%20manner%20for%20various%20video%20datasets%2C%20all%20at%20competitive%20quality%0Acompared%20to%20a%20conditional%20diffusion%20baseline%20but%20with%20higher%20speed%2C%20i.e.%2C%20fewer%0AODE%20solver%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Video%2520Bi-flow%26entry.906535625%3DChen%2520Liu%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520generative%2520video%2520model%2520to%2520robustly%2520learn%2520temporal%2520change%250Aas%2520a%2520neural%2520Ordinary%2520Differential%2520Equation%2520%2528ODE%2529%2520flow%2520with%2520a%2520bilinear%2520objective%250Awhich%2520combines%2520two%2520aspects%253A%2520The%2520first%2520is%2520to%2520map%2520from%2520the%2520past%2520into%2520future%2520video%250Aframes%2520directly.%2520Previous%2520work%2520has%2520mapped%2520the%2520noise%2520to%2520new%2520frames%252C%2520a%2520more%250Acomputationally%2520expensive%2520process.%2520Unfortunately%252C%2520starting%2520from%2520the%2520previous%250Aframe%252C%2520instead%2520of%2520noise%252C%2520is%2520more%2520prone%2520to%2520drifting%2520errors.%2520Hence%252C%2520second%252C%2520we%250Aadditionally%2520learn%2520how%2520to%2520remove%2520the%2520accumulated%2520errors%2520as%2520the%2520joint%2520objective%250Aby%2520adding%2520noise%2520during%2520training.%2520We%2520demonstrate%2520unconditional%2520video%2520generation%250Ain%2520a%2520streaming%2520manner%2520for%2520various%2520video%2520datasets%252C%2520all%2520at%2520competitive%2520quality%250Acompared%2520to%2520a%2520conditional%2520diffusion%2520baseline%2520but%2520with%2520higher%2520speed%252C%2520i.e.%252C%2520fewer%250AODE%2520solver%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Video%20Bi-flow&entry.906535625=Chen%20Liu%20and%20Tobias%20Ritschel&entry.1292438233=%20%20We%20propose%20a%20novel%20generative%20video%20model%20to%20robustly%20learn%20temporal%20change%0Aas%20a%20neural%20Ordinary%20Differential%20Equation%20%28ODE%29%20flow%20with%20a%20bilinear%20objective%0Awhich%20combines%20two%20aspects%3A%20The%20first%20is%20to%20map%20from%20the%20past%20into%20future%20video%0Aframes%20directly.%20Previous%20work%20has%20mapped%20the%20noise%20to%20new%20frames%2C%20a%20more%0Acomputationally%20expensive%20process.%20Unfortunately%2C%20starting%20from%20the%20previous%0Aframe%2C%20instead%20of%20noise%2C%20is%20more%20prone%20to%20drifting%20errors.%20Hence%2C%20second%2C%20we%0Aadditionally%20learn%20how%20to%20remove%20the%20accumulated%20errors%20as%20the%20joint%20objective%0Aby%20adding%20noise%20during%20training.%20We%20demonstrate%20unconditional%20video%20generation%0Ain%20a%20streaming%20manner%20for%20various%20video%20datasets%2C%20all%20at%20competitive%20quality%0Acompared%20to%20a%20conditional%20diffusion%20baseline%20but%20with%20higher%20speed%2C%20i.e.%2C%20fewer%0AODE%20solver%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06364v2&entry.124074799=Read"},
{"title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular\n  Data", "author": "Ruiyu Zhang and Ce Zhao and Xin Zhao and Lin Nie and Wai-Fung Lam", "abstract": "  Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential.\n", "link": "http://arxiv.org/abs/2508.06347v1", "date": "2025-08-08", "relevancy": 2.4383, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Equation-VAE%3A%20Disentangled%20Latent%20Representations%20for%20Tabular%0A%20%20Data&body=Title%3A%20Structural%20Equation-VAE%3A%20Disentangled%20Latent%20Representations%20for%20Tabular%0A%20%20Data%0AAuthor%3A%20Ruiyu%20Zhang%20and%20Ce%20Zhao%20and%20Xin%20Zhao%20and%20Lin%20Nie%20and%20Wai-Fung%20Lam%0AAbstract%3A%20%20%20Learning%20interpretable%20latent%20representations%20from%20tabular%20data%20remains%20a%0Achallenge%20in%20deep%20generative%20modeling.%20We%20introduce%20SE-VAE%20%28Structural%0AEquation-Variational%20Autoencoder%29%2C%20a%20novel%20architecture%20that%20embeds%20measurement%0Astructure%20directly%20into%20the%20design%20of%20a%20variational%20autoencoder.%20Inspired%20by%0Astructural%20equation%20modeling%2C%20SE-VAE%20aligns%20latent%20subspaces%20with%20known%0Aindicator%20groupings%20and%20introduces%20a%20global%20nuisance%20latent%20to%20isolate%0Aconstruct-specific%20confounding%20variation.%20This%20modular%20architecture%20enables%0Adisentanglement%20through%20design%20rather%20than%20through%20statistical%20regularizers%0Aalone.%20We%20evaluate%20SE-VAE%20on%20a%20suite%20of%20simulated%20tabular%20datasets%20and%0Abenchmark%20its%20performance%20against%20a%20series%20of%20leading%20baselines%20using%20standard%0Adisentanglement%20metrics.%20SE-VAE%20consistently%20outperforms%20alternatives%20in%20factor%0Arecovery%2C%20interpretability%2C%20and%20robustness%20to%20nuisance%20variation.%20Ablation%0Aresults%20reveal%20that%20architectural%20structure%2C%20rather%20than%20regularization%0Astrength%2C%20is%20the%20key%20driver%20of%20performance.%20SE-VAE%20offers%20a%20principled%0Aframework%20for%20white-box%20generative%20modeling%20in%20scientific%20and%20social%20domains%0Awhere%20latent%20constructs%20are%20theory-driven%20and%20measurement%20validity%20is%0Aessential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Equation-VAE%253A%2520Disentangled%2520Latent%2520Representations%2520for%2520Tabular%250A%2520%2520Data%26entry.906535625%3DRuiyu%2520Zhang%2520and%2520Ce%2520Zhao%2520and%2520Xin%2520Zhao%2520and%2520Lin%2520Nie%2520and%2520Wai-Fung%2520Lam%26entry.1292438233%3D%2520%2520Learning%2520interpretable%2520latent%2520representations%2520from%2520tabular%2520data%2520remains%2520a%250Achallenge%2520in%2520deep%2520generative%2520modeling.%2520We%2520introduce%2520SE-VAE%2520%2528Structural%250AEquation-Variational%2520Autoencoder%2529%252C%2520a%2520novel%2520architecture%2520that%2520embeds%2520measurement%250Astructure%2520directly%2520into%2520the%2520design%2520of%2520a%2520variational%2520autoencoder.%2520Inspired%2520by%250Astructural%2520equation%2520modeling%252C%2520SE-VAE%2520aligns%2520latent%2520subspaces%2520with%2520known%250Aindicator%2520groupings%2520and%2520introduces%2520a%2520global%2520nuisance%2520latent%2520to%2520isolate%250Aconstruct-specific%2520confounding%2520variation.%2520This%2520modular%2520architecture%2520enables%250Adisentanglement%2520through%2520design%2520rather%2520than%2520through%2520statistical%2520regularizers%250Aalone.%2520We%2520evaluate%2520SE-VAE%2520on%2520a%2520suite%2520of%2520simulated%2520tabular%2520datasets%2520and%250Abenchmark%2520its%2520performance%2520against%2520a%2520series%2520of%2520leading%2520baselines%2520using%2520standard%250Adisentanglement%2520metrics.%2520SE-VAE%2520consistently%2520outperforms%2520alternatives%2520in%2520factor%250Arecovery%252C%2520interpretability%252C%2520and%2520robustness%2520to%2520nuisance%2520variation.%2520Ablation%250Aresults%2520reveal%2520that%2520architectural%2520structure%252C%2520rather%2520than%2520regularization%250Astrength%252C%2520is%2520the%2520key%2520driver%2520of%2520performance.%2520SE-VAE%2520offers%2520a%2520principled%250Aframework%2520for%2520white-box%2520generative%2520modeling%2520in%2520scientific%2520and%2520social%2520domains%250Awhere%2520latent%2520constructs%2520are%2520theory-driven%2520and%2520measurement%2520validity%2520is%250Aessential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Equation-VAE%3A%20Disentangled%20Latent%20Representations%20for%20Tabular%0A%20%20Data&entry.906535625=Ruiyu%20Zhang%20and%20Ce%20Zhao%20and%20Xin%20Zhao%20and%20Lin%20Nie%20and%20Wai-Fung%20Lam&entry.1292438233=%20%20Learning%20interpretable%20latent%20representations%20from%20tabular%20data%20remains%20a%0Achallenge%20in%20deep%20generative%20modeling.%20We%20introduce%20SE-VAE%20%28Structural%0AEquation-Variational%20Autoencoder%29%2C%20a%20novel%20architecture%20that%20embeds%20measurement%0Astructure%20directly%20into%20the%20design%20of%20a%20variational%20autoencoder.%20Inspired%20by%0Astructural%20equation%20modeling%2C%20SE-VAE%20aligns%20latent%20subspaces%20with%20known%0Aindicator%20groupings%20and%20introduces%20a%20global%20nuisance%20latent%20to%20isolate%0Aconstruct-specific%20confounding%20variation.%20This%20modular%20architecture%20enables%0Adisentanglement%20through%20design%20rather%20than%20through%20statistical%20regularizers%0Aalone.%20We%20evaluate%20SE-VAE%20on%20a%20suite%20of%20simulated%20tabular%20datasets%20and%0Abenchmark%20its%20performance%20against%20a%20series%20of%20leading%20baselines%20using%20standard%0Adisentanglement%20metrics.%20SE-VAE%20consistently%20outperforms%20alternatives%20in%20factor%0Arecovery%2C%20interpretability%2C%20and%20robustness%20to%20nuisance%20variation.%20Ablation%0Aresults%20reveal%20that%20architectural%20structure%2C%20rather%20than%20regularization%0Astrength%2C%20is%20the%20key%20driver%20of%20performance.%20SE-VAE%20offers%20a%20principled%0Aframework%20for%20white-box%20generative%20modeling%20in%20scientific%20and%20social%20domains%0Awhere%20latent%20constructs%20are%20theory-driven%20and%20measurement%20validity%20is%0Aessential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06347v1&entry.124074799=Read"},
{"title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation", "author": "Mattia Litrico and Mario Valerio Giuffrida and Sebastiano Battiato and Devis Tuia", "abstract": "  Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance.\n", "link": "http://arxiv.org/abs/2508.06452v1", "date": "2025-08-08", "relevancy": 2.3981, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6213}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRUST%3A%20Leveraging%20Text%20Robustness%20for%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20TRUST%3A%20Leveraging%20Text%20Robustness%20for%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Mattia%20Litrico%20and%20Mario%20Valerio%20Giuffrida%20and%20Sebastiano%20Battiato%20and%20Devis%20Tuia%0AAbstract%3A%20%20%20Recent%20unsupervised%20domain%20adaptation%20%28UDA%29%20methods%20have%20shown%20great%20success%0Ain%20addressing%20classical%20domain%20shifts%20%28e.g.%2C%20synthetic-to-real%29%2C%20but%20they%20still%0Asuffer%20under%20complex%20shifts%20%28e.g.%20geographical%20shift%29%2C%20where%20both%20the%0Abackground%20and%20object%20appearances%20differ%20significantly%20across%20domains.%20Prior%0Aworks%20showed%20that%20the%20language%20modality%20can%20help%20in%20the%20adaptation%20process%2C%0Aexhibiting%20more%20robustness%20to%20such%20complex%20shifts.%20In%20this%20paper%2C%20we%20introduce%0ATRUST%2C%20a%20novel%20UDA%20approach%20that%20exploits%20the%20robustness%20of%20the%20language%0Amodality%20to%20guide%20the%20adaptation%20of%20a%20vision%20model.%20TRUST%20generates%0Apseudo-labels%20for%20target%20samples%20from%20their%20captions%20and%20introduces%20a%20novel%0Auncertainty%20estimation%20strategy%20that%20uses%20normalised%20CLIP%20similarity%20scores%20to%0Aestimate%20the%20uncertainty%20of%20the%20generated%20pseudo-labels.%20Such%20estimated%0Auncertainty%20is%20then%20used%20to%20reweight%20the%20classification%20loss%2C%20mitigating%20the%0Aadverse%20effects%20of%20wrong%20pseudo-labels%20obtained%20from%20low-quality%20captions.%20To%0Afurther%20increase%20the%20robustness%20of%20the%20vision%20model%2C%20we%20propose%20a%20multimodal%0Asoft-contrastive%20learning%20loss%20that%20aligns%20the%20vision%20and%20language%20feature%0Aspaces%2C%20by%20leveraging%20captions%20to%20guide%20the%20contrastive%20training%20of%20the%20vision%0Amodel%20on%20target%20images.%20In%20our%20contrastive%20loss%2C%20each%20pair%20of%20images%20acts%20as%0Aboth%20a%20positive%20and%20a%20negative%20pair%20and%20their%20feature%20representations%20are%0Aattracted%20and%20repulsed%20with%20a%20strength%20proportional%20to%20the%20similarity%20of%20their%0Acaptions.%20This%20solution%20avoids%20the%20need%20for%20hardly%20determining%20positive%20and%0Anegative%20pairs%2C%20which%20is%20critical%20in%20the%20UDA%20setting.%20Our%20approach%20outperforms%0Aprevious%20methods%2C%20setting%20the%20new%20state-of-the-art%20on%20classical%20%28DomainNet%29%20and%0Acomplex%20%28GeoNet%29%20domain%20shifts.%20The%20code%20will%20be%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRUST%253A%2520Leveraging%2520Text%2520Robustness%2520for%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DMattia%2520Litrico%2520and%2520Mario%2520Valerio%2520Giuffrida%2520and%2520Sebastiano%2520Battiato%2520and%2520Devis%2520Tuia%26entry.1292438233%3D%2520%2520Recent%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520methods%2520have%2520shown%2520great%2520success%250Ain%2520addressing%2520classical%2520domain%2520shifts%2520%2528e.g.%252C%2520synthetic-to-real%2529%252C%2520but%2520they%2520still%250Asuffer%2520under%2520complex%2520shifts%2520%2528e.g.%2520geographical%2520shift%2529%252C%2520where%2520both%2520the%250Abackground%2520and%2520object%2520appearances%2520differ%2520significantly%2520across%2520domains.%2520Prior%250Aworks%2520showed%2520that%2520the%2520language%2520modality%2520can%2520help%2520in%2520the%2520adaptation%2520process%252C%250Aexhibiting%2520more%2520robustness%2520to%2520such%2520complex%2520shifts.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ATRUST%252C%2520a%2520novel%2520UDA%2520approach%2520that%2520exploits%2520the%2520robustness%2520of%2520the%2520language%250Amodality%2520to%2520guide%2520the%2520adaptation%2520of%2520a%2520vision%2520model.%2520TRUST%2520generates%250Apseudo-labels%2520for%2520target%2520samples%2520from%2520their%2520captions%2520and%2520introduces%2520a%2520novel%250Auncertainty%2520estimation%2520strategy%2520that%2520uses%2520normalised%2520CLIP%2520similarity%2520scores%2520to%250Aestimate%2520the%2520uncertainty%2520of%2520the%2520generated%2520pseudo-labels.%2520Such%2520estimated%250Auncertainty%2520is%2520then%2520used%2520to%2520reweight%2520the%2520classification%2520loss%252C%2520mitigating%2520the%250Aadverse%2520effects%2520of%2520wrong%2520pseudo-labels%2520obtained%2520from%2520low-quality%2520captions.%2520To%250Afurther%2520increase%2520the%2520robustness%2520of%2520the%2520vision%2520model%252C%2520we%2520propose%2520a%2520multimodal%250Asoft-contrastive%2520learning%2520loss%2520that%2520aligns%2520the%2520vision%2520and%2520language%2520feature%250Aspaces%252C%2520by%2520leveraging%2520captions%2520to%2520guide%2520the%2520contrastive%2520training%2520of%2520the%2520vision%250Amodel%2520on%2520target%2520images.%2520In%2520our%2520contrastive%2520loss%252C%2520each%2520pair%2520of%2520images%2520acts%2520as%250Aboth%2520a%2520positive%2520and%2520a%2520negative%2520pair%2520and%2520their%2520feature%2520representations%2520are%250Aattracted%2520and%2520repulsed%2520with%2520a%2520strength%2520proportional%2520to%2520the%2520similarity%2520of%2520their%250Acaptions.%2520This%2520solution%2520avoids%2520the%2520need%2520for%2520hardly%2520determining%2520positive%2520and%250Anegative%2520pairs%252C%2520which%2520is%2520critical%2520in%2520the%2520UDA%2520setting.%2520Our%2520approach%2520outperforms%250Aprevious%2520methods%252C%2520setting%2520the%2520new%2520state-of-the-art%2520on%2520classical%2520%2528DomainNet%2529%2520and%250Acomplex%2520%2528GeoNet%2529%2520domain%2520shifts.%2520The%2520code%2520will%2520be%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRUST%3A%20Leveraging%20Text%20Robustness%20for%20Unsupervised%20Domain%20Adaptation&entry.906535625=Mattia%20Litrico%20and%20Mario%20Valerio%20Giuffrida%20and%20Sebastiano%20Battiato%20and%20Devis%20Tuia&entry.1292438233=%20%20Recent%20unsupervised%20domain%20adaptation%20%28UDA%29%20methods%20have%20shown%20great%20success%0Ain%20addressing%20classical%20domain%20shifts%20%28e.g.%2C%20synthetic-to-real%29%2C%20but%20they%20still%0Asuffer%20under%20complex%20shifts%20%28e.g.%20geographical%20shift%29%2C%20where%20both%20the%0Abackground%20and%20object%20appearances%20differ%20significantly%20across%20domains.%20Prior%0Aworks%20showed%20that%20the%20language%20modality%20can%20help%20in%20the%20adaptation%20process%2C%0Aexhibiting%20more%20robustness%20to%20such%20complex%20shifts.%20In%20this%20paper%2C%20we%20introduce%0ATRUST%2C%20a%20novel%20UDA%20approach%20that%20exploits%20the%20robustness%20of%20the%20language%0Amodality%20to%20guide%20the%20adaptation%20of%20a%20vision%20model.%20TRUST%20generates%0Apseudo-labels%20for%20target%20samples%20from%20their%20captions%20and%20introduces%20a%20novel%0Auncertainty%20estimation%20strategy%20that%20uses%20normalised%20CLIP%20similarity%20scores%20to%0Aestimate%20the%20uncertainty%20of%20the%20generated%20pseudo-labels.%20Such%20estimated%0Auncertainty%20is%20then%20used%20to%20reweight%20the%20classification%20loss%2C%20mitigating%20the%0Aadverse%20effects%20of%20wrong%20pseudo-labels%20obtained%20from%20low-quality%20captions.%20To%0Afurther%20increase%20the%20robustness%20of%20the%20vision%20model%2C%20we%20propose%20a%20multimodal%0Asoft-contrastive%20learning%20loss%20that%20aligns%20the%20vision%20and%20language%20feature%0Aspaces%2C%20by%20leveraging%20captions%20to%20guide%20the%20contrastive%20training%20of%20the%20vision%0Amodel%20on%20target%20images.%20In%20our%20contrastive%20loss%2C%20each%20pair%20of%20images%20acts%20as%0Aboth%20a%20positive%20and%20a%20negative%20pair%20and%20their%20feature%20representations%20are%0Aattracted%20and%20repulsed%20with%20a%20strength%20proportional%20to%20the%20similarity%20of%20their%0Acaptions.%20This%20solution%20avoids%20the%20need%20for%20hardly%20determining%20positive%20and%0Anegative%20pairs%2C%20which%20is%20critical%20in%20the%20UDA%20setting.%20Our%20approach%20outperforms%0Aprevious%20methods%2C%20setting%20the%20new%20state-of-the-art%20on%20classical%20%28DomainNet%29%20and%0Acomplex%20%28GeoNet%29%20domain%20shifts.%20The%20code%20will%20be%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06452v1&entry.124074799=Read"},
{"title": "MotionSwap", "author": "Om Patil and Jinesh Modi and Suryabha Mukhopadhyay and Meghaditya Giri and Chhavi Malhotra", "abstract": "  Face swapping technology has gained significant attention in both academic\nresearch and commercial applications. This paper presents our implementation\nand enhancement of SimSwap, an efficient framework for high fidelity face\nswapping. We introduce several improvements to the original model, including\nthe integration of self and cross-attention mechanisms in the generator\narchitecture, dynamic loss weighting, and cosine annealing learning rate\nscheduling. These enhancements lead to significant improvements in identity\npreservation, attribute consistency, and overall visual quality.\n  Our experimental results, spanning 400,000 training iterations, demonstrate\nprogressive improvements in generator and discriminator performance. The\nenhanced model achieves better identity similarity, lower FID scores, and\nvisibly superior qualitative results compared to the baseline. Ablation studies\nconfirm the importance of each architectural and training improvement. We\nconclude by identifying key future directions, such as integrating StyleGAN3,\nimproving lip synchronization, incorporating 3D facial modeling, and\nintroducing temporal consistency for video-based applications.\n", "link": "http://arxiv.org/abs/2508.06430v1", "date": "2025-08-08", "relevancy": 2.3357, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5853}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionSwap&body=Title%3A%20MotionSwap%0AAuthor%3A%20Om%20Patil%20and%20Jinesh%20Modi%20and%20Suryabha%20Mukhopadhyay%20and%20Meghaditya%20Giri%20and%20Chhavi%20Malhotra%0AAbstract%3A%20%20%20Face%20swapping%20technology%20has%20gained%20significant%20attention%20in%20both%20academic%0Aresearch%20and%20commercial%20applications.%20This%20paper%20presents%20our%20implementation%0Aand%20enhancement%20of%20SimSwap%2C%20an%20efficient%20framework%20for%20high%20fidelity%20face%0Aswapping.%20We%20introduce%20several%20improvements%20to%20the%20original%20model%2C%20including%0Athe%20integration%20of%20self%20and%20cross-attention%20mechanisms%20in%20the%20generator%0Aarchitecture%2C%20dynamic%20loss%20weighting%2C%20and%20cosine%20annealing%20learning%20rate%0Ascheduling.%20These%20enhancements%20lead%20to%20significant%20improvements%20in%20identity%0Apreservation%2C%20attribute%20consistency%2C%20and%20overall%20visual%20quality.%0A%20%20Our%20experimental%20results%2C%20spanning%20400%2C000%20training%20iterations%2C%20demonstrate%0Aprogressive%20improvements%20in%20generator%20and%20discriminator%20performance.%20The%0Aenhanced%20model%20achieves%20better%20identity%20similarity%2C%20lower%20FID%20scores%2C%20and%0Avisibly%20superior%20qualitative%20results%20compared%20to%20the%20baseline.%20Ablation%20studies%0Aconfirm%20the%20importance%20of%20each%20architectural%20and%20training%20improvement.%20We%0Aconclude%20by%20identifying%20key%20future%20directions%2C%20such%20as%20integrating%20StyleGAN3%2C%0Aimproving%20lip%20synchronization%2C%20incorporating%203D%20facial%20modeling%2C%20and%0Aintroducing%20temporal%20consistency%20for%20video-based%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionSwap%26entry.906535625%3DOm%2520Patil%2520and%2520Jinesh%2520Modi%2520and%2520Suryabha%2520Mukhopadhyay%2520and%2520Meghaditya%2520Giri%2520and%2520Chhavi%2520Malhotra%26entry.1292438233%3D%2520%2520Face%2520swapping%2520technology%2520has%2520gained%2520significant%2520attention%2520in%2520both%2520academic%250Aresearch%2520and%2520commercial%2520applications.%2520This%2520paper%2520presents%2520our%2520implementation%250Aand%2520enhancement%2520of%2520SimSwap%252C%2520an%2520efficient%2520framework%2520for%2520high%2520fidelity%2520face%250Aswapping.%2520We%2520introduce%2520several%2520improvements%2520to%2520the%2520original%2520model%252C%2520including%250Athe%2520integration%2520of%2520self%2520and%2520cross-attention%2520mechanisms%2520in%2520the%2520generator%250Aarchitecture%252C%2520dynamic%2520loss%2520weighting%252C%2520and%2520cosine%2520annealing%2520learning%2520rate%250Ascheduling.%2520These%2520enhancements%2520lead%2520to%2520significant%2520improvements%2520in%2520identity%250Apreservation%252C%2520attribute%2520consistency%252C%2520and%2520overall%2520visual%2520quality.%250A%2520%2520Our%2520experimental%2520results%252C%2520spanning%2520400%252C000%2520training%2520iterations%252C%2520demonstrate%250Aprogressive%2520improvements%2520in%2520generator%2520and%2520discriminator%2520performance.%2520The%250Aenhanced%2520model%2520achieves%2520better%2520identity%2520similarity%252C%2520lower%2520FID%2520scores%252C%2520and%250Avisibly%2520superior%2520qualitative%2520results%2520compared%2520to%2520the%2520baseline.%2520Ablation%2520studies%250Aconfirm%2520the%2520importance%2520of%2520each%2520architectural%2520and%2520training%2520improvement.%2520We%250Aconclude%2520by%2520identifying%2520key%2520future%2520directions%252C%2520such%2520as%2520integrating%2520StyleGAN3%252C%250Aimproving%2520lip%2520synchronization%252C%2520incorporating%25203D%2520facial%2520modeling%252C%2520and%250Aintroducing%2520temporal%2520consistency%2520for%2520video-based%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionSwap&entry.906535625=Om%20Patil%20and%20Jinesh%20Modi%20and%20Suryabha%20Mukhopadhyay%20and%20Meghaditya%20Giri%20and%20Chhavi%20Malhotra&entry.1292438233=%20%20Face%20swapping%20technology%20has%20gained%20significant%20attention%20in%20both%20academic%0Aresearch%20and%20commercial%20applications.%20This%20paper%20presents%20our%20implementation%0Aand%20enhancement%20of%20SimSwap%2C%20an%20efficient%20framework%20for%20high%20fidelity%20face%0Aswapping.%20We%20introduce%20several%20improvements%20to%20the%20original%20model%2C%20including%0Athe%20integration%20of%20self%20and%20cross-attention%20mechanisms%20in%20the%20generator%0Aarchitecture%2C%20dynamic%20loss%20weighting%2C%20and%20cosine%20annealing%20learning%20rate%0Ascheduling.%20These%20enhancements%20lead%20to%20significant%20improvements%20in%20identity%0Apreservation%2C%20attribute%20consistency%2C%20and%20overall%20visual%20quality.%0A%20%20Our%20experimental%20results%2C%20spanning%20400%2C000%20training%20iterations%2C%20demonstrate%0Aprogressive%20improvements%20in%20generator%20and%20discriminator%20performance.%20The%0Aenhanced%20model%20achieves%20better%20identity%20similarity%2C%20lower%20FID%20scores%2C%20and%0Avisibly%20superior%20qualitative%20results%20compared%20to%20the%20baseline.%20Ablation%20studies%0Aconfirm%20the%20importance%20of%20each%20architectural%20and%20training%20improvement.%20We%0Aconclude%20by%20identifying%20key%20future%20directions%2C%20such%20as%20integrating%20StyleGAN3%2C%0Aimproving%20lip%20synchronization%2C%20incorporating%203D%20facial%20modeling%2C%20and%0Aintroducing%20temporal%20consistency%20for%20video-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06430v1&entry.124074799=Read"},
{"title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion", "author": "Yehonathan Litman and Fernando De la Torre and Shubham Tulsiani", "abstract": "  Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.\n", "link": "http://arxiv.org/abs/2508.06494v1", "date": "2025-08-08", "relevancy": 2.3232, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightSwitch%3A%20Multi-view%20Relighting%20with%20Material-guided%20Diffusion&body=Title%3A%20LightSwitch%3A%20Multi-view%20Relighting%20with%20Material-guided%20Diffusion%0AAuthor%3A%20Yehonathan%20Litman%20and%20Fernando%20De%20la%20Torre%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20Recent%20approaches%20for%203D%20relighting%20have%20shown%20promise%20in%20integrating%202D%0Aimage%20relighting%20generative%20priors%20to%20alter%20the%20appearance%20of%20a%203D%0Arepresentation%20while%20preserving%20the%20underlying%20structure.%20Nevertheless%2C%0Agenerative%20priors%20used%20for%202D%20relighting%20that%20directly%20relight%20from%20an%20input%0Aimage%20do%20not%20take%20advantage%20of%20intrinsic%20properties%20of%20the%20subject%20that%20can%20be%0Ainferred%20or%20cannot%20consider%20multi-view%20data%20at%20scale%2C%20leading%20to%20subpar%0Arelighting.%20In%20this%20paper%2C%20we%20propose%20Lightswitch%2C%20a%20novel%20finetuned%0Amaterial-relighting%20diffusion%20framework%20that%20efficiently%20relights%20an%20arbitrary%0Anumber%20of%20input%20images%20to%20a%20target%20lighting%20condition%20while%20incorporating%20cues%0Afrom%20inferred%20intrinsic%20properties.%20By%20using%20multi-view%20and%20material%0Ainformation%20cues%20together%20with%20a%20scalable%20denoising%20scheme%2C%20our%20method%0Aconsistently%20and%20efficiently%20relights%20dense%20multi-view%20data%20of%20objects%20with%0Adiverse%20material%20compositions.%20We%20show%20that%20our%202D%20relighting%20prediction%0Aquality%20exceeds%20previous%20state-of-the-art%20relighting%20priors%20that%20directly%0Arelight%20from%20images.%20We%20further%20demonstrate%20that%20LightSwitch%20matches%20or%0Aoutperforms%20state-of-the-art%20diffusion%20inverse%20rendering%20methods%20in%20relighting%0Asynthetic%20and%20real%20objects%20in%20as%20little%20as%202%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightSwitch%253A%2520Multi-view%2520Relighting%2520with%2520Material-guided%2520Diffusion%26entry.906535625%3DYehonathan%2520Litman%2520and%2520Fernando%2520De%2520la%2520Torre%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3D%2520%2520Recent%2520approaches%2520for%25203D%2520relighting%2520have%2520shown%2520promise%2520in%2520integrating%25202D%250Aimage%2520relighting%2520generative%2520priors%2520to%2520alter%2520the%2520appearance%2520of%2520a%25203D%250Arepresentation%2520while%2520preserving%2520the%2520underlying%2520structure.%2520Nevertheless%252C%250Agenerative%2520priors%2520used%2520for%25202D%2520relighting%2520that%2520directly%2520relight%2520from%2520an%2520input%250Aimage%2520do%2520not%2520take%2520advantage%2520of%2520intrinsic%2520properties%2520of%2520the%2520subject%2520that%2520can%2520be%250Ainferred%2520or%2520cannot%2520consider%2520multi-view%2520data%2520at%2520scale%252C%2520leading%2520to%2520subpar%250Arelighting.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Lightswitch%252C%2520a%2520novel%2520finetuned%250Amaterial-relighting%2520diffusion%2520framework%2520that%2520efficiently%2520relights%2520an%2520arbitrary%250Anumber%2520of%2520input%2520images%2520to%2520a%2520target%2520lighting%2520condition%2520while%2520incorporating%2520cues%250Afrom%2520inferred%2520intrinsic%2520properties.%2520By%2520using%2520multi-view%2520and%2520material%250Ainformation%2520cues%2520together%2520with%2520a%2520scalable%2520denoising%2520scheme%252C%2520our%2520method%250Aconsistently%2520and%2520efficiently%2520relights%2520dense%2520multi-view%2520data%2520of%2520objects%2520with%250Adiverse%2520material%2520compositions.%2520We%2520show%2520that%2520our%25202D%2520relighting%2520prediction%250Aquality%2520exceeds%2520previous%2520state-of-the-art%2520relighting%2520priors%2520that%2520directly%250Arelight%2520from%2520images.%2520We%2520further%2520demonstrate%2520that%2520LightSwitch%2520matches%2520or%250Aoutperforms%2520state-of-the-art%2520diffusion%2520inverse%2520rendering%2520methods%2520in%2520relighting%250Asynthetic%2520and%2520real%2520objects%2520in%2520as%2520little%2520as%25202%2520minutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightSwitch%3A%20Multi-view%20Relighting%20with%20Material-guided%20Diffusion&entry.906535625=Yehonathan%20Litman%20and%20Fernando%20De%20la%20Torre%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20Recent%20approaches%20for%203D%20relighting%20have%20shown%20promise%20in%20integrating%202D%0Aimage%20relighting%20generative%20priors%20to%20alter%20the%20appearance%20of%20a%203D%0Arepresentation%20while%20preserving%20the%20underlying%20structure.%20Nevertheless%2C%0Agenerative%20priors%20used%20for%202D%20relighting%20that%20directly%20relight%20from%20an%20input%0Aimage%20do%20not%20take%20advantage%20of%20intrinsic%20properties%20of%20the%20subject%20that%20can%20be%0Ainferred%20or%20cannot%20consider%20multi-view%20data%20at%20scale%2C%20leading%20to%20subpar%0Arelighting.%20In%20this%20paper%2C%20we%20propose%20Lightswitch%2C%20a%20novel%20finetuned%0Amaterial-relighting%20diffusion%20framework%20that%20efficiently%20relights%20an%20arbitrary%0Anumber%20of%20input%20images%20to%20a%20target%20lighting%20condition%20while%20incorporating%20cues%0Afrom%20inferred%20intrinsic%20properties.%20By%20using%20multi-view%20and%20material%0Ainformation%20cues%20together%20with%20a%20scalable%20denoising%20scheme%2C%20our%20method%0Aconsistently%20and%20efficiently%20relights%20dense%20multi-view%20data%20of%20objects%20with%0Adiverse%20material%20compositions.%20We%20show%20that%20our%202D%20relighting%20prediction%0Aquality%20exceeds%20previous%20state-of-the-art%20relighting%20priors%20that%20directly%0Arelight%20from%20images.%20We%20further%20demonstrate%20that%20LightSwitch%20matches%20or%0Aoutperforms%20state-of-the-art%20diffusion%20inverse%20rendering%20methods%20in%20relighting%0Asynthetic%20and%20real%20objects%20in%20as%20little%20as%202%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06494v1&entry.124074799=Read"},
{"title": "CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D\n  Medical Image Interpolation", "author": "Xia Li and Runzhao Yang and Xiangtai Li and Antony Lomax and Ye Zhang and Joachim Buhmann", "abstract": "  Motion information from 4D medical imaging offers critical insights into\ndynamic changes in patient anatomy for clinical assessments and radiotherapy\nplanning and, thereby, enhances the capabilities of 3D image analysis. However,\ninherent physical and technical constraints of imaging hardware often\nnecessitate a compromise between temporal resolution and image quality. Frame\ninterpolation emerges as a pivotal solution to this challenge. Previous methods\noften suffer from discretion when they estimate the intermediate motion and\nexecute the forward warping. In this study, we draw inspiration from fluid\nmechanics to propose a novel approach for continuously modeling patient\nanatomic motion using implicit neural representation. It ensures both spatial\nand temporal continuity, effectively bridging Eulerian and Lagrangian\nspecifications together to naturally facilitate continuous frame interpolation.\nOur experiments across multiple datasets underscore the method's superior\naccuracy and speed. Furthermore, as a case-specific optimization\n(training-free) approach, it circumvents the need for extensive datasets and\naddresses model generalization issues.\n", "link": "http://arxiv.org/abs/2405.15385v2", "date": "2025-08-08", "relevancy": 2.3205, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6321}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5491}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CPT-Interp%3A%20Continuous%20sPatial%20and%20Temporal%20Motion%20Modeling%20for%204D%0A%20%20Medical%20Image%20Interpolation&body=Title%3A%20CPT-Interp%3A%20Continuous%20sPatial%20and%20Temporal%20Motion%20Modeling%20for%204D%0A%20%20Medical%20Image%20Interpolation%0AAuthor%3A%20Xia%20Li%20and%20Runzhao%20Yang%20and%20Xiangtai%20Li%20and%20Antony%20Lomax%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann%0AAbstract%3A%20%20%20Motion%20information%20from%204D%20medical%20imaging%20offers%20critical%20insights%20into%0Adynamic%20changes%20in%20patient%20anatomy%20for%20clinical%20assessments%20and%20radiotherapy%0Aplanning%20and%2C%20thereby%2C%20enhances%20the%20capabilities%20of%203D%20image%20analysis.%20However%2C%0Ainherent%20physical%20and%20technical%20constraints%20of%20imaging%20hardware%20often%0Anecessitate%20a%20compromise%20between%20temporal%20resolution%20and%20image%20quality.%20Frame%0Ainterpolation%20emerges%20as%20a%20pivotal%20solution%20to%20this%20challenge.%20Previous%20methods%0Aoften%20suffer%20from%20discretion%20when%20they%20estimate%20the%20intermediate%20motion%20and%0Aexecute%20the%20forward%20warping.%20In%20this%20study%2C%20we%20draw%20inspiration%20from%20fluid%0Amechanics%20to%20propose%20a%20novel%20approach%20for%20continuously%20modeling%20patient%0Aanatomic%20motion%20using%20implicit%20neural%20representation.%20It%20ensures%20both%20spatial%0Aand%20temporal%20continuity%2C%20effectively%20bridging%20Eulerian%20and%20Lagrangian%0Aspecifications%20together%20to%20naturally%20facilitate%20continuous%20frame%20interpolation.%0AOur%20experiments%20across%20multiple%20datasets%20underscore%20the%20method%27s%20superior%0Aaccuracy%20and%20speed.%20Furthermore%2C%20as%20a%20case-specific%20optimization%0A%28training-free%29%20approach%2C%20it%20circumvents%20the%20need%20for%20extensive%20datasets%20and%0Aaddresses%20model%20generalization%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCPT-Interp%253A%2520Continuous%2520sPatial%2520and%2520Temporal%2520Motion%2520Modeling%2520for%25204D%250A%2520%2520Medical%2520Image%2520Interpolation%26entry.906535625%3DXia%2520Li%2520and%2520Runzhao%2520Yang%2520and%2520Xiangtai%2520Li%2520and%2520Antony%2520Lomax%2520and%2520Ye%2520Zhang%2520and%2520Joachim%2520Buhmann%26entry.1292438233%3D%2520%2520Motion%2520information%2520from%25204D%2520medical%2520imaging%2520offers%2520critical%2520insights%2520into%250Adynamic%2520changes%2520in%2520patient%2520anatomy%2520for%2520clinical%2520assessments%2520and%2520radiotherapy%250Aplanning%2520and%252C%2520thereby%252C%2520enhances%2520the%2520capabilities%2520of%25203D%2520image%2520analysis.%2520However%252C%250Ainherent%2520physical%2520and%2520technical%2520constraints%2520of%2520imaging%2520hardware%2520often%250Anecessitate%2520a%2520compromise%2520between%2520temporal%2520resolution%2520and%2520image%2520quality.%2520Frame%250Ainterpolation%2520emerges%2520as%2520a%2520pivotal%2520solution%2520to%2520this%2520challenge.%2520Previous%2520methods%250Aoften%2520suffer%2520from%2520discretion%2520when%2520they%2520estimate%2520the%2520intermediate%2520motion%2520and%250Aexecute%2520the%2520forward%2520warping.%2520In%2520this%2520study%252C%2520we%2520draw%2520inspiration%2520from%2520fluid%250Amechanics%2520to%2520propose%2520a%2520novel%2520approach%2520for%2520continuously%2520modeling%2520patient%250Aanatomic%2520motion%2520using%2520implicit%2520neural%2520representation.%2520It%2520ensures%2520both%2520spatial%250Aand%2520temporal%2520continuity%252C%2520effectively%2520bridging%2520Eulerian%2520and%2520Lagrangian%250Aspecifications%2520together%2520to%2520naturally%2520facilitate%2520continuous%2520frame%2520interpolation.%250AOur%2520experiments%2520across%2520multiple%2520datasets%2520underscore%2520the%2520method%2527s%2520superior%250Aaccuracy%2520and%2520speed.%2520Furthermore%252C%2520as%2520a%2520case-specific%2520optimization%250A%2528training-free%2529%2520approach%252C%2520it%2520circumvents%2520the%2520need%2520for%2520extensive%2520datasets%2520and%250Aaddresses%2520model%2520generalization%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPT-Interp%3A%20Continuous%20sPatial%20and%20Temporal%20Motion%20Modeling%20for%204D%0A%20%20Medical%20Image%20Interpolation&entry.906535625=Xia%20Li%20and%20Runzhao%20Yang%20and%20Xiangtai%20Li%20and%20Antony%20Lomax%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann&entry.1292438233=%20%20Motion%20information%20from%204D%20medical%20imaging%20offers%20critical%20insights%20into%0Adynamic%20changes%20in%20patient%20anatomy%20for%20clinical%20assessments%20and%20radiotherapy%0Aplanning%20and%2C%20thereby%2C%20enhances%20the%20capabilities%20of%203D%20image%20analysis.%20However%2C%0Ainherent%20physical%20and%20technical%20constraints%20of%20imaging%20hardware%20often%0Anecessitate%20a%20compromise%20between%20temporal%20resolution%20and%20image%20quality.%20Frame%0Ainterpolation%20emerges%20as%20a%20pivotal%20solution%20to%20this%20challenge.%20Previous%20methods%0Aoften%20suffer%20from%20discretion%20when%20they%20estimate%20the%20intermediate%20motion%20and%0Aexecute%20the%20forward%20warping.%20In%20this%20study%2C%20we%20draw%20inspiration%20from%20fluid%0Amechanics%20to%20propose%20a%20novel%20approach%20for%20continuously%20modeling%20patient%0Aanatomic%20motion%20using%20implicit%20neural%20representation.%20It%20ensures%20both%20spatial%0Aand%20temporal%20continuity%2C%20effectively%20bridging%20Eulerian%20and%20Lagrangian%0Aspecifications%20together%20to%20naturally%20facilitate%20continuous%20frame%20interpolation.%0AOur%20experiments%20across%20multiple%20datasets%20underscore%20the%20method%27s%20superior%0Aaccuracy%20and%20speed.%20Furthermore%2C%20as%20a%20case-specific%20optimization%0A%28training-free%29%20approach%2C%20it%20circumvents%20the%20need%20for%20extensive%20datasets%20and%0Aaddresses%20model%20generalization%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15385v2&entry.124074799=Read"},
{"title": "A Study of Gender Classification Techniques Based on Iris Images: A Deep\n  Survey and Analysis", "author": "Basna Mohammed Salih Hasan and Ramadhan J. Mstafa", "abstract": "  Gender classification is attractive in a range of applications, including\nsurveillance and monitoring, corporate profiling, and human-computer\ninteraction. Individuals' identities may be gleaned from information about\ntheir gender, which is a kind of soft biometric. Over the years, several\nmethods for determining a person's gender have been devised. Some of the most\nwell-known ones are based on physical characteristics like face, fingerprint,\npalmprint, DNA, ears, gait, and iris. On the other hand, facial features\naccount for the vast majority of gender classification methods. Also, the iris\nis a significant biometric trait because the iris, according to research,\nremains basically constant during an individual's life. Besides that, the iris\nis externally visible and is non-invasive to the user, which is important for\npractical applications. Furthermore, there are already high-quality methods for\nsegmenting and encoding iris images, and the current methods facilitate\nselecting and extracting attribute vectors from iris textures. This study\ndiscusses several approaches to determining gender. The previous works of\nliterature are briefly reviewed. Additionally, there are a variety of\nmethodologies for different steps of gender classification. This study provides\nresearchers with knowledge and analysis of the existing gender classification\napproaches. Also, it will assist researchers who are interested in this\nspecific area, as well as highlight the gaps and challenges in the field, and\nfinally provide suggestions and future paths for improvement.\n", "link": "http://arxiv.org/abs/2508.05246v2", "date": "2025-08-08", "relevancy": 2.3139, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4751}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4572}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis&body=Title%3A%20A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis%0AAuthor%3A%20Basna%20Mohammed%20Salih%20Hasan%20and%20Ramadhan%20J.%20Mstafa%0AAbstract%3A%20%20%20Gender%20classification%20is%20attractive%20in%20a%20range%20of%20applications%2C%20including%0Asurveillance%20and%20monitoring%2C%20corporate%20profiling%2C%20and%20human-computer%0Ainteraction.%20Individuals%27%20identities%20may%20be%20gleaned%20from%20information%20about%0Atheir%20gender%2C%20which%20is%20a%20kind%20of%20soft%20biometric.%20Over%20the%20years%2C%20several%0Amethods%20for%20determining%20a%20person%27s%20gender%20have%20been%20devised.%20Some%20of%20the%20most%0Awell-known%20ones%20are%20based%20on%20physical%20characteristics%20like%20face%2C%20fingerprint%2C%0Apalmprint%2C%20DNA%2C%20ears%2C%20gait%2C%20and%20iris.%20On%20the%20other%20hand%2C%20facial%20features%0Aaccount%20for%20the%20vast%20majority%20of%20gender%20classification%20methods.%20Also%2C%20the%20iris%0Ais%20a%20significant%20biometric%20trait%20because%20the%20iris%2C%20according%20to%20research%2C%0Aremains%20basically%20constant%20during%20an%20individual%27s%20life.%20Besides%20that%2C%20the%20iris%0Ais%20externally%20visible%20and%20is%20non-invasive%20to%20the%20user%2C%20which%20is%20important%20for%0Apractical%20applications.%20Furthermore%2C%20there%20are%20already%20high-quality%20methods%20for%0Asegmenting%20and%20encoding%20iris%20images%2C%20and%20the%20current%20methods%20facilitate%0Aselecting%20and%20extracting%20attribute%20vectors%20from%20iris%20textures.%20This%20study%0Adiscusses%20several%20approaches%20to%20determining%20gender.%20The%20previous%20works%20of%0Aliterature%20are%20briefly%20reviewed.%20Additionally%2C%20there%20are%20a%20variety%20of%0Amethodologies%20for%20different%20steps%20of%20gender%20classification.%20This%20study%20provides%0Aresearchers%20with%20knowledge%20and%20analysis%20of%20the%20existing%20gender%20classification%0Aapproaches.%20Also%2C%20it%20will%20assist%20researchers%20who%20are%20interested%20in%20this%0Aspecific%20area%2C%20as%20well%20as%20highlight%20the%20gaps%20and%20challenges%20in%20the%20field%2C%20and%0Afinally%20provide%20suggestions%20and%20future%20paths%20for%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05246v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Gender%2520Classification%2520Techniques%2520Based%2520on%2520Iris%2520Images%253A%2520A%2520Deep%250A%2520%2520Survey%2520and%2520Analysis%26entry.906535625%3DBasna%2520Mohammed%2520Salih%2520Hasan%2520and%2520Ramadhan%2520J.%2520Mstafa%26entry.1292438233%3D%2520%2520Gender%2520classification%2520is%2520attractive%2520in%2520a%2520range%2520of%2520applications%252C%2520including%250Asurveillance%2520and%2520monitoring%252C%2520corporate%2520profiling%252C%2520and%2520human-computer%250Ainteraction.%2520Individuals%2527%2520identities%2520may%2520be%2520gleaned%2520from%2520information%2520about%250Atheir%2520gender%252C%2520which%2520is%2520a%2520kind%2520of%2520soft%2520biometric.%2520Over%2520the%2520years%252C%2520several%250Amethods%2520for%2520determining%2520a%2520person%2527s%2520gender%2520have%2520been%2520devised.%2520Some%2520of%2520the%2520most%250Awell-known%2520ones%2520are%2520based%2520on%2520physical%2520characteristics%2520like%2520face%252C%2520fingerprint%252C%250Apalmprint%252C%2520DNA%252C%2520ears%252C%2520gait%252C%2520and%2520iris.%2520On%2520the%2520other%2520hand%252C%2520facial%2520features%250Aaccount%2520for%2520the%2520vast%2520majority%2520of%2520gender%2520classification%2520methods.%2520Also%252C%2520the%2520iris%250Ais%2520a%2520significant%2520biometric%2520trait%2520because%2520the%2520iris%252C%2520according%2520to%2520research%252C%250Aremains%2520basically%2520constant%2520during%2520an%2520individual%2527s%2520life.%2520Besides%2520that%252C%2520the%2520iris%250Ais%2520externally%2520visible%2520and%2520is%2520non-invasive%2520to%2520the%2520user%252C%2520which%2520is%2520important%2520for%250Apractical%2520applications.%2520Furthermore%252C%2520there%2520are%2520already%2520high-quality%2520methods%2520for%250Asegmenting%2520and%2520encoding%2520iris%2520images%252C%2520and%2520the%2520current%2520methods%2520facilitate%250Aselecting%2520and%2520extracting%2520attribute%2520vectors%2520from%2520iris%2520textures.%2520This%2520study%250Adiscusses%2520several%2520approaches%2520to%2520determining%2520gender.%2520The%2520previous%2520works%2520of%250Aliterature%2520are%2520briefly%2520reviewed.%2520Additionally%252C%2520there%2520are%2520a%2520variety%2520of%250Amethodologies%2520for%2520different%2520steps%2520of%2520gender%2520classification.%2520This%2520study%2520provides%250Aresearchers%2520with%2520knowledge%2520and%2520analysis%2520of%2520the%2520existing%2520gender%2520classification%250Aapproaches.%2520Also%252C%2520it%2520will%2520assist%2520researchers%2520who%2520are%2520interested%2520in%2520this%250Aspecific%2520area%252C%2520as%2520well%2520as%2520highlight%2520the%2520gaps%2520and%2520challenges%2520in%2520the%2520field%252C%2520and%250Afinally%2520provide%2520suggestions%2520and%2520future%2520paths%2520for%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05246v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis&entry.906535625=Basna%20Mohammed%20Salih%20Hasan%20and%20Ramadhan%20J.%20Mstafa&entry.1292438233=%20%20Gender%20classification%20is%20attractive%20in%20a%20range%20of%20applications%2C%20including%0Asurveillance%20and%20monitoring%2C%20corporate%20profiling%2C%20and%20human-computer%0Ainteraction.%20Individuals%27%20identities%20may%20be%20gleaned%20from%20information%20about%0Atheir%20gender%2C%20which%20is%20a%20kind%20of%20soft%20biometric.%20Over%20the%20years%2C%20several%0Amethods%20for%20determining%20a%20person%27s%20gender%20have%20been%20devised.%20Some%20of%20the%20most%0Awell-known%20ones%20are%20based%20on%20physical%20characteristics%20like%20face%2C%20fingerprint%2C%0Apalmprint%2C%20DNA%2C%20ears%2C%20gait%2C%20and%20iris.%20On%20the%20other%20hand%2C%20facial%20features%0Aaccount%20for%20the%20vast%20majority%20of%20gender%20classification%20methods.%20Also%2C%20the%20iris%0Ais%20a%20significant%20biometric%20trait%20because%20the%20iris%2C%20according%20to%20research%2C%0Aremains%20basically%20constant%20during%20an%20individual%27s%20life.%20Besides%20that%2C%20the%20iris%0Ais%20externally%20visible%20and%20is%20non-invasive%20to%20the%20user%2C%20which%20is%20important%20for%0Apractical%20applications.%20Furthermore%2C%20there%20are%20already%20high-quality%20methods%20for%0Asegmenting%20and%20encoding%20iris%20images%2C%20and%20the%20current%20methods%20facilitate%0Aselecting%20and%20extracting%20attribute%20vectors%20from%20iris%20textures.%20This%20study%0Adiscusses%20several%20approaches%20to%20determining%20gender.%20The%20previous%20works%20of%0Aliterature%20are%20briefly%20reviewed.%20Additionally%2C%20there%20are%20a%20variety%20of%0Amethodologies%20for%20different%20steps%20of%20gender%20classification.%20This%20study%20provides%0Aresearchers%20with%20knowledge%20and%20analysis%20of%20the%20existing%20gender%20classification%0Aapproaches.%20Also%2C%20it%20will%20assist%20researchers%20who%20are%20interested%20in%20this%0Aspecific%20area%2C%20as%20well%20as%20highlight%20the%20gaps%20and%20challenges%20in%20the%20field%2C%20and%0Afinally%20provide%20suggestions%20and%20future%20paths%20for%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05246v2&entry.124074799=Read"},
{"title": "Sample-efficient LLM Optimization with Reset Replay", "author": "Zichuan Liu and Jinyu Wang and Lei Song and Jiang Bian", "abstract": "  Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.\n", "link": "http://arxiv.org/abs/2508.06412v1", "date": "2025-08-08", "relevancy": 2.3117, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4602}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay&body=Title%3A%20Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay%0AAuthor%3A%20Zichuan%20Liu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20advancements%20in%20post-training%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20through%20Reinforcement%20Learning%20%28RL%29%20and%20preference%20optimization%0Amethods%2C%20are%20key%20drivers%20for%20enhancing%20their%20reasoning%20capabilities.%20However%2C%0Athese%20methods%20are%20often%20plagued%20by%20low%20sample%20efficiency%20and%20a%20susceptibility%0Ato%20primacy%20bias%2C%20where%20overfitting%20to%20initial%20experiences%20degrades%20policy%0Aquality%20and%20damages%20the%20learning%20process.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LLM%20optimization%20with%20Reset%20Replay%20%28LoRR%29%2C%20a%20general%20and%20powerful%0Aplugin%20designed%20to%20enhance%20sample%20efficiency%20in%20any%20preference-based%0Aoptimization%20framework.%20LoRR%20core%20mechanism%20enables%20training%20at%20a%20high%20replay%0Anumber%2C%20maximizing%20the%20utility%20of%20each%20collected%20data%20batch.%20To%20counteract%20the%0Arisk%20of%20overfitting%20inherent%20in%20high-replay%20training%2C%20LoRR%20incorporates%20a%0Aperiodic%20reset%20strategy%20with%20reusing%20initial%20data%2C%20which%20preserves%20network%0Aplasticity.%20Furthermore%2C%20it%20leverages%20a%20hybrid%20optimization%20objective%2C%0Acombining%20supervised%20fine-tuning%20%28SFT%29%20and%20preference-based%20losses%20to%20further%0Abolster%20data%20exploitation.%20Our%20extensive%20experiments%20demonstrate%20that%20LoRR%0Asignificantly%20boosts%20the%20performance%20of%20various%20preference%20optimization%20methods%0Aon%20both%20mathematical%20and%20general%20reasoning%20benchmarks.%20Notably%2C%20an%20iterative%0ADPO%20approach%20augmented%20with%20LoRR%20achieves%20comparable%20performance%20on%20challenging%0Amath%20tasks%2C%20outperforming%20some%20complex%20and%20computationally%20intensive%20RL-based%0Aalgorithms.%20These%20findings%20highlight%20that%20LoRR%20offers%20a%20practical%2C%0Asample-efficient%2C%20and%20highly%20effective%20paradigm%20for%20LLM%20finetuning%2C%20unlocking%0Agreater%20performance%20from%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-efficient%2520LLM%2520Optimization%2520with%2520Reset%2520Replay%26entry.906535625%3DZichuan%2520Liu%2520and%2520Jinyu%2520Wang%2520and%2520Lei%2520Song%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520post-training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Aparticularly%2520through%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520preference%2520optimization%250Amethods%252C%2520are%2520key%2520drivers%2520for%2520enhancing%2520their%2520reasoning%2520capabilities.%2520However%252C%250Athese%2520methods%2520are%2520often%2520plagued%2520by%2520low%2520sample%2520efficiency%2520and%2520a%2520susceptibility%250Ato%2520primacy%2520bias%252C%2520where%2520overfitting%2520to%2520initial%2520experiences%2520degrades%2520policy%250Aquality%2520and%2520damages%2520the%2520learning%2520process.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520LLM%2520optimization%2520with%2520Reset%2520Replay%2520%2528LoRR%2529%252C%2520a%2520general%2520and%2520powerful%250Aplugin%2520designed%2520to%2520enhance%2520sample%2520efficiency%2520in%2520any%2520preference-based%250Aoptimization%2520framework.%2520LoRR%2520core%2520mechanism%2520enables%2520training%2520at%2520a%2520high%2520replay%250Anumber%252C%2520maximizing%2520the%2520utility%2520of%2520each%2520collected%2520data%2520batch.%2520To%2520counteract%2520the%250Arisk%2520of%2520overfitting%2520inherent%2520in%2520high-replay%2520training%252C%2520LoRR%2520incorporates%2520a%250Aperiodic%2520reset%2520strategy%2520with%2520reusing%2520initial%2520data%252C%2520which%2520preserves%2520network%250Aplasticity.%2520Furthermore%252C%2520it%2520leverages%2520a%2520hybrid%2520optimization%2520objective%252C%250Acombining%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520preference-based%2520losses%2520to%2520further%250Abolster%2520data%2520exploitation.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520LoRR%250Asignificantly%2520boosts%2520the%2520performance%2520of%2520various%2520preference%2520optimization%2520methods%250Aon%2520both%2520mathematical%2520and%2520general%2520reasoning%2520benchmarks.%2520Notably%252C%2520an%2520iterative%250ADPO%2520approach%2520augmented%2520with%2520LoRR%2520achieves%2520comparable%2520performance%2520on%2520challenging%250Amath%2520tasks%252C%2520outperforming%2520some%2520complex%2520and%2520computationally%2520intensive%2520RL-based%250Aalgorithms.%2520These%2520findings%2520highlight%2520that%2520LoRR%2520offers%2520a%2520practical%252C%250Asample-efficient%252C%2520and%2520highly%2520effective%2520paradigm%2520for%2520LLM%2520finetuning%252C%2520unlocking%250Agreater%2520performance%2520from%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay&entry.906535625=Zichuan%20Liu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20advancements%20in%20post-training%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20through%20Reinforcement%20Learning%20%28RL%29%20and%20preference%20optimization%0Amethods%2C%20are%20key%20drivers%20for%20enhancing%20their%20reasoning%20capabilities.%20However%2C%0Athese%20methods%20are%20often%20plagued%20by%20low%20sample%20efficiency%20and%20a%20susceptibility%0Ato%20primacy%20bias%2C%20where%20overfitting%20to%20initial%20experiences%20degrades%20policy%0Aquality%20and%20damages%20the%20learning%20process.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LLM%20optimization%20with%20Reset%20Replay%20%28LoRR%29%2C%20a%20general%20and%20powerful%0Aplugin%20designed%20to%20enhance%20sample%20efficiency%20in%20any%20preference-based%0Aoptimization%20framework.%20LoRR%20core%20mechanism%20enables%20training%20at%20a%20high%20replay%0Anumber%2C%20maximizing%20the%20utility%20of%20each%20collected%20data%20batch.%20To%20counteract%20the%0Arisk%20of%20overfitting%20inherent%20in%20high-replay%20training%2C%20LoRR%20incorporates%20a%0Aperiodic%20reset%20strategy%20with%20reusing%20initial%20data%2C%20which%20preserves%20network%0Aplasticity.%20Furthermore%2C%20it%20leverages%20a%20hybrid%20optimization%20objective%2C%0Acombining%20supervised%20fine-tuning%20%28SFT%29%20and%20preference-based%20losses%20to%20further%0Abolster%20data%20exploitation.%20Our%20extensive%20experiments%20demonstrate%20that%20LoRR%0Asignificantly%20boosts%20the%20performance%20of%20various%20preference%20optimization%20methods%0Aon%20both%20mathematical%20and%20general%20reasoning%20benchmarks.%20Notably%2C%20an%20iterative%0ADPO%20approach%20augmented%20with%20LoRR%20achieves%20comparable%20performance%20on%20challenging%0Amath%20tasks%2C%20outperforming%20some%20complex%20and%20computationally%20intensive%20RL-based%0Aalgorithms.%20These%20findings%20highlight%20that%20LoRR%20offers%20a%20practical%2C%0Asample-efficient%2C%20and%20highly%20effective%20paradigm%20for%20LLM%20finetuning%2C%20unlocking%0Agreater%20performance%20from%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06412v1&entry.124074799=Read"},
{"title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual\n  Anomaly Detection", "author": "Zhaopeng Gu and Bingke Zhu and Guibo Zhu and Yingying Chen and Wei Ge and Ming Tang and Jinqiao Wang", "abstract": "  Anomaly detection is a critical task across numerous domains and modalities,\nyet existing methods are often highly specialized, limiting their\ngeneralizability. These specialized models, tailored for specific anomaly types\nlike textural defects or logical errors, typically exhibit limited performance\nwhen deployed outside their designated contexts. To overcome this limitation,\nwe propose AnomalyMoE, a novel and universal anomaly detection framework based\non a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the\ncomplex anomaly detection problem into three distinct semantic hierarchies:\nlocal structural anomalies, component-level semantic anomalies, and global\nlogical anomalies. AnomalyMoE correspondingly employs three dedicated expert\nnetworks at the patch, component, and global levels, and is specialized in\nreconstructing features and identifying deviations at its designated semantic\nlevel. This hierarchical design allows a single model to concurrently\nunderstand and detect a wide spectrum of anomalies. Furthermore, we introduce\nan Expert Information Repulsion (EIR) module to promote expert diversity and an\nExpert Selection Balancing (ESB) module to ensure the comprehensive utilization\nof all experts. Experiments on 8 challenging datasets spanning industrial\nimaging, 3D point clouds, medical imaging, video surveillance, and logical\nanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art\nperformance, significantly outperforming specialized methods in their\nrespective domains.\n", "link": "http://arxiv.org/abs/2508.06203v1", "date": "2025-08-08", "relevancy": 2.2896, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyMoE%3A%20Towards%20a%20Language-free%20Generalist%20Model%20for%20Unified%20Visual%0A%20%20Anomaly%20Detection&body=Title%3A%20AnomalyMoE%3A%20Towards%20a%20Language-free%20Generalist%20Model%20for%20Unified%20Visual%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Zhaopeng%20Gu%20and%20Bingke%20Zhu%20and%20Guibo%20Zhu%20and%20Yingying%20Chen%20and%20Wei%20Ge%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20a%20critical%20task%20across%20numerous%20domains%20and%20modalities%2C%0Ayet%20existing%20methods%20are%20often%20highly%20specialized%2C%20limiting%20their%0Ageneralizability.%20These%20specialized%20models%2C%20tailored%20for%20specific%20anomaly%20types%0Alike%20textural%20defects%20or%20logical%20errors%2C%20typically%20exhibit%20limited%20performance%0Awhen%20deployed%20outside%20their%20designated%20contexts.%20To%20overcome%20this%20limitation%2C%0Awe%20propose%20AnomalyMoE%2C%20a%20novel%20and%20universal%20anomaly%20detection%20framework%20based%0Aon%20a%20Mixture-of-Experts%20%28MoE%29%20architecture.%20Our%20key%20insight%20is%20to%20decompose%20the%0Acomplex%20anomaly%20detection%20problem%20into%20three%20distinct%20semantic%20hierarchies%3A%0Alocal%20structural%20anomalies%2C%20component-level%20semantic%20anomalies%2C%20and%20global%0Alogical%20anomalies.%20AnomalyMoE%20correspondingly%20employs%20three%20dedicated%20expert%0Anetworks%20at%20the%20patch%2C%20component%2C%20and%20global%20levels%2C%20and%20is%20specialized%20in%0Areconstructing%20features%20and%20identifying%20deviations%20at%20its%20designated%20semantic%0Alevel.%20This%20hierarchical%20design%20allows%20a%20single%20model%20to%20concurrently%0Aunderstand%20and%20detect%20a%20wide%20spectrum%20of%20anomalies.%20Furthermore%2C%20we%20introduce%0Aan%20Expert%20Information%20Repulsion%20%28EIR%29%20module%20to%20promote%20expert%20diversity%20and%20an%0AExpert%20Selection%20Balancing%20%28ESB%29%20module%20to%20ensure%20the%20comprehensive%20utilization%0Aof%20all%20experts.%20Experiments%20on%208%20challenging%20datasets%20spanning%20industrial%0Aimaging%2C%203D%20point%20clouds%2C%20medical%20imaging%2C%20video%20surveillance%2C%20and%20logical%0Aanomaly%20detection%20demonstrate%20that%20AnomalyMoE%20establishes%20new%20state-of-the-art%0Aperformance%2C%20significantly%20outperforming%20specialized%20methods%20in%20their%0Arespective%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyMoE%253A%2520Towards%2520a%2520Language-free%2520Generalist%2520Model%2520for%2520Unified%2520Visual%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DZhaopeng%2520Gu%2520and%2520Bingke%2520Zhu%2520and%2520Guibo%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Wei%2520Ge%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520a%2520critical%2520task%2520across%2520numerous%2520domains%2520and%2520modalities%252C%250Ayet%2520existing%2520methods%2520are%2520often%2520highly%2520specialized%252C%2520limiting%2520their%250Ageneralizability.%2520These%2520specialized%2520models%252C%2520tailored%2520for%2520specific%2520anomaly%2520types%250Alike%2520textural%2520defects%2520or%2520logical%2520errors%252C%2520typically%2520exhibit%2520limited%2520performance%250Awhen%2520deployed%2520outside%2520their%2520designated%2520contexts.%2520To%2520overcome%2520this%2520limitation%252C%250Awe%2520propose%2520AnomalyMoE%252C%2520a%2520novel%2520and%2520universal%2520anomaly%2520detection%2520framework%2520based%250Aon%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture.%2520Our%2520key%2520insight%2520is%2520to%2520decompose%2520the%250Acomplex%2520anomaly%2520detection%2520problem%2520into%2520three%2520distinct%2520semantic%2520hierarchies%253A%250Alocal%2520structural%2520anomalies%252C%2520component-level%2520semantic%2520anomalies%252C%2520and%2520global%250Alogical%2520anomalies.%2520AnomalyMoE%2520correspondingly%2520employs%2520three%2520dedicated%2520expert%250Anetworks%2520at%2520the%2520patch%252C%2520component%252C%2520and%2520global%2520levels%252C%2520and%2520is%2520specialized%2520in%250Areconstructing%2520features%2520and%2520identifying%2520deviations%2520at%2520its%2520designated%2520semantic%250Alevel.%2520This%2520hierarchical%2520design%2520allows%2520a%2520single%2520model%2520to%2520concurrently%250Aunderstand%2520and%2520detect%2520a%2520wide%2520spectrum%2520of%2520anomalies.%2520Furthermore%252C%2520we%2520introduce%250Aan%2520Expert%2520Information%2520Repulsion%2520%2528EIR%2529%2520module%2520to%2520promote%2520expert%2520diversity%2520and%2520an%250AExpert%2520Selection%2520Balancing%2520%2528ESB%2529%2520module%2520to%2520ensure%2520the%2520comprehensive%2520utilization%250Aof%2520all%2520experts.%2520Experiments%2520on%25208%2520challenging%2520datasets%2520spanning%2520industrial%250Aimaging%252C%25203D%2520point%2520clouds%252C%2520medical%2520imaging%252C%2520video%2520surveillance%252C%2520and%2520logical%250Aanomaly%2520detection%2520demonstrate%2520that%2520AnomalyMoE%2520establishes%2520new%2520state-of-the-art%250Aperformance%252C%2520significantly%2520outperforming%2520specialized%2520methods%2520in%2520their%250Arespective%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyMoE%3A%20Towards%20a%20Language-free%20Generalist%20Model%20for%20Unified%20Visual%0A%20%20Anomaly%20Detection&entry.906535625=Zhaopeng%20Gu%20and%20Bingke%20Zhu%20and%20Guibo%20Zhu%20and%20Yingying%20Chen%20and%20Wei%20Ge%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Anomaly%20detection%20is%20a%20critical%20task%20across%20numerous%20domains%20and%20modalities%2C%0Ayet%20existing%20methods%20are%20often%20highly%20specialized%2C%20limiting%20their%0Ageneralizability.%20These%20specialized%20models%2C%20tailored%20for%20specific%20anomaly%20types%0Alike%20textural%20defects%20or%20logical%20errors%2C%20typically%20exhibit%20limited%20performance%0Awhen%20deployed%20outside%20their%20designated%20contexts.%20To%20overcome%20this%20limitation%2C%0Awe%20propose%20AnomalyMoE%2C%20a%20novel%20and%20universal%20anomaly%20detection%20framework%20based%0Aon%20a%20Mixture-of-Experts%20%28MoE%29%20architecture.%20Our%20key%20insight%20is%20to%20decompose%20the%0Acomplex%20anomaly%20detection%20problem%20into%20three%20distinct%20semantic%20hierarchies%3A%0Alocal%20structural%20anomalies%2C%20component-level%20semantic%20anomalies%2C%20and%20global%0Alogical%20anomalies.%20AnomalyMoE%20correspondingly%20employs%20three%20dedicated%20expert%0Anetworks%20at%20the%20patch%2C%20component%2C%20and%20global%20levels%2C%20and%20is%20specialized%20in%0Areconstructing%20features%20and%20identifying%20deviations%20at%20its%20designated%20semantic%0Alevel.%20This%20hierarchical%20design%20allows%20a%20single%20model%20to%20concurrently%0Aunderstand%20and%20detect%20a%20wide%20spectrum%20of%20anomalies.%20Furthermore%2C%20we%20introduce%0Aan%20Expert%20Information%20Repulsion%20%28EIR%29%20module%20to%20promote%20expert%20diversity%20and%20an%0AExpert%20Selection%20Balancing%20%28ESB%29%20module%20to%20ensure%20the%20comprehensive%20utilization%0Aof%20all%20experts.%20Experiments%20on%208%20challenging%20datasets%20spanning%20industrial%0Aimaging%2C%203D%20point%20clouds%2C%20medical%20imaging%2C%20video%20surveillance%2C%20and%20logical%0Aanomaly%20detection%20demonstrate%20that%20AnomalyMoE%20establishes%20new%20state-of-the-art%0Aperformance%2C%20significantly%20outperforming%20specialized%20methods%20in%20their%0Arespective%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06203v1&entry.124074799=Read"},
{"title": "Reconsidering the Performance of GAE in Link Prediction", "author": "Weishuo Ma and Yanbo Wang and Xiyuan Wang and Muhan Zhang", "abstract": "  Recent advancements in graph neural networks (GNNs) for link prediction have\nintroduced sophisticated training techniques and model architectures. However,\nreliance on outdated baselines may exaggerate the benefits of these new\napproaches. To tackle this issue, we systematically explore Graph Autoencoders\n(GAEs) by applying model-agnostic tricks in recent methods and tuning\nhyperparameters. We find that a well-tuned GAE can match the performance of\nrecent sophisticated models while offering superior computational efficiency on\nwidely-used link prediction benchmarks. Our approach delivers substantial\nperformance gains on datasets where structural information dominates and\nfeature data is limited. Specifically, our GAE achieves a state-of-the-art\nHits@100 score of 78.41\\% on the ogbl-ppa dataset. Furthermore, we examine the\nimpact of various tricks to uncover the reasons behind our success and to guide\nthe design of future methods. Our study emphasizes the critical need to update\nbaselines for a more accurate assessment of progress in GNNs for link\nprediction. Our code is available at https://github.com/GraphPKU/Refined-GAE.\n", "link": "http://arxiv.org/abs/2411.03845v3", "date": "2025-08-08", "relevancy": 2.2883, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4806}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction&body=Title%3A%20Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction%0AAuthor%3A%20Weishuo%20Ma%20and%20Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20graph%20neural%20networks%20%28GNNs%29%20for%20link%20prediction%20have%0Aintroduced%20sophisticated%20training%20techniques%20and%20model%20architectures.%20However%2C%0Areliance%20on%20outdated%20baselines%20may%20exaggerate%20the%20benefits%20of%20these%20new%0Aapproaches.%20To%20tackle%20this%20issue%2C%20we%20systematically%20explore%20Graph%20Autoencoders%0A%28GAEs%29%20by%20applying%20model-agnostic%20tricks%20in%20recent%20methods%20and%20tuning%0Ahyperparameters.%20We%20find%20that%20a%20well-tuned%20GAE%20can%20match%20the%20performance%20of%0Arecent%20sophisticated%20models%20while%20offering%20superior%20computational%20efficiency%20on%0Awidely-used%20link%20prediction%20benchmarks.%20Our%20approach%20delivers%20substantial%0Aperformance%20gains%20on%20datasets%20where%20structural%20information%20dominates%20and%0Afeature%20data%20is%20limited.%20Specifically%2C%20our%20GAE%20achieves%20a%20state-of-the-art%0AHits%40100%20score%20of%2078.41%5C%25%20on%20the%20ogbl-ppa%20dataset.%20Furthermore%2C%20we%20examine%20the%0Aimpact%20of%20various%20tricks%20to%20uncover%20the%20reasons%20behind%20our%20success%20and%20to%20guide%0Athe%20design%20of%20future%20methods.%20Our%20study%20emphasizes%20the%20critical%20need%20to%20update%0Abaselines%20for%20a%20more%20accurate%20assessment%20of%20progress%20in%20GNNs%20for%20link%0Aprediction.%20Our%20code%20is%20available%20at%20https%3A//github.com/GraphPKU/Refined-GAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03845v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconsidering%2520the%2520Performance%2520of%2520GAE%2520in%2520Link%2520Prediction%26entry.906535625%3DWeishuo%2520Ma%2520and%2520Yanbo%2520Wang%2520and%2520Xiyuan%2520Wang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520link%2520prediction%2520have%250Aintroduced%2520sophisticated%2520training%2520techniques%2520and%2520model%2520architectures.%2520However%252C%250Areliance%2520on%2520outdated%2520baselines%2520may%2520exaggerate%2520the%2520benefits%2520of%2520these%2520new%250Aapproaches.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520systematically%2520explore%2520Graph%2520Autoencoders%250A%2528GAEs%2529%2520by%2520applying%2520model-agnostic%2520tricks%2520in%2520recent%2520methods%2520and%2520tuning%250Ahyperparameters.%2520We%2520find%2520that%2520a%2520well-tuned%2520GAE%2520can%2520match%2520the%2520performance%2520of%250Arecent%2520sophisticated%2520models%2520while%2520offering%2520superior%2520computational%2520efficiency%2520on%250Awidely-used%2520link%2520prediction%2520benchmarks.%2520Our%2520approach%2520delivers%2520substantial%250Aperformance%2520gains%2520on%2520datasets%2520where%2520structural%2520information%2520dominates%2520and%250Afeature%2520data%2520is%2520limited.%2520Specifically%252C%2520our%2520GAE%2520achieves%2520a%2520state-of-the-art%250AHits%2540100%2520score%2520of%252078.41%255C%2525%2520on%2520the%2520ogbl-ppa%2520dataset.%2520Furthermore%252C%2520we%2520examine%2520the%250Aimpact%2520of%2520various%2520tricks%2520to%2520uncover%2520the%2520reasons%2520behind%2520our%2520success%2520and%2520to%2520guide%250Athe%2520design%2520of%2520future%2520methods.%2520Our%2520study%2520emphasizes%2520the%2520critical%2520need%2520to%2520update%250Abaselines%2520for%2520a%2520more%2520accurate%2520assessment%2520of%2520progress%2520in%2520GNNs%2520for%2520link%250Aprediction.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/GraphPKU/Refined-GAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03845v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction&entry.906535625=Weishuo%20Ma%20and%20Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20graph%20neural%20networks%20%28GNNs%29%20for%20link%20prediction%20have%0Aintroduced%20sophisticated%20training%20techniques%20and%20model%20architectures.%20However%2C%0Areliance%20on%20outdated%20baselines%20may%20exaggerate%20the%20benefits%20of%20these%20new%0Aapproaches.%20To%20tackle%20this%20issue%2C%20we%20systematically%20explore%20Graph%20Autoencoders%0A%28GAEs%29%20by%20applying%20model-agnostic%20tricks%20in%20recent%20methods%20and%20tuning%0Ahyperparameters.%20We%20find%20that%20a%20well-tuned%20GAE%20can%20match%20the%20performance%20of%0Arecent%20sophisticated%20models%20while%20offering%20superior%20computational%20efficiency%20on%0Awidely-used%20link%20prediction%20benchmarks.%20Our%20approach%20delivers%20substantial%0Aperformance%20gains%20on%20datasets%20where%20structural%20information%20dominates%20and%0Afeature%20data%20is%20limited.%20Specifically%2C%20our%20GAE%20achieves%20a%20state-of-the-art%0AHits%40100%20score%20of%2078.41%5C%25%20on%20the%20ogbl-ppa%20dataset.%20Furthermore%2C%20we%20examine%20the%0Aimpact%20of%20various%20tricks%20to%20uncover%20the%20reasons%20behind%20our%20success%20and%20to%20guide%0Athe%20design%20of%20future%20methods.%20Our%20study%20emphasizes%20the%20critical%20need%20to%20update%0Abaselines%20for%20a%20more%20accurate%20assessment%20of%20progress%20in%20GNNs%20for%20link%0Aprediction.%20Our%20code%20is%20available%20at%20https%3A//github.com/GraphPKU/Refined-GAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03845v3&entry.124074799=Read"},
{"title": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions", "author": "YiZhou Li", "abstract": "  Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios.\n", "link": "http://arxiv.org/abs/2507.21761v2", "date": "2025-08-08", "relevancy": 2.2882, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5851}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOR-VIT%3A%20Efficient%20Vision%20Transformer%20with%20Mixture-of-Recursions&body=Title%3A%20MOR-VIT%3A%20Efficient%20Vision%20Transformer%20with%20Mixture-of-Recursions%0AAuthor%3A%20YiZhou%20Li%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20in%20image%0Arecognition%2C%20yet%20standard%20ViT%20architectures%20are%20hampered%20by%20substantial%0Aparameter%20redundancy%20and%20high%20computational%20cost%2C%20limiting%20their%20practical%0Adeployment.%20While%20recent%20efforts%20on%20efficient%20ViTs%20primarily%20focus%20on%20static%0Amodel%20compression%20or%20token-level%20sparsification%2C%20they%20remain%20constrained%20by%0Afixed%20computational%20depth%20for%20all%20tokens.%20In%20this%20work%2C%20we%20present%20MoR-ViT%2C%20a%0Anovel%20vision%20transformer%20framework%20that%2C%20for%20the%20first%20time%2C%20incorporates%20a%0Atoken-level%20dynamic%20recursion%20mechanism%20inspired%20by%20the%20Mixture-of-Recursions%0A%28MoR%29%20paradigm.%20This%20approach%20enables%20each%20token%20to%20adaptively%20determine%20its%0Aprocessing%20depth%2C%20yielding%20a%20flexible%20and%20input-dependent%20allocation%20of%0Acomputational%20resources.%20Extensive%20experiments%20on%20ImageNet-1K%20and%20transfer%0Abenchmarks%20demonstrate%20that%20MoR-ViT%20not%20only%20achieves%20state-of-the-art%20accuracy%0Awith%20up%20to%2070%25%20parameter%20reduction%20and%202.5x%20inference%20acceleration%2C%20but%20also%0Aoutperforms%20leading%20efficient%20ViT%20baselines%20such%20as%20DynamicViT%20and%20TinyViT%0Aunder%20comparable%20conditions.%20These%20results%20establish%20dynamic%20recursion%20as%20an%0Aeffective%20strategy%20for%20efficient%20vision%20transformers%20and%20open%20new%20avenues%20for%0Ascalable%20and%20deployable%20deep%20learning%20models%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOR-VIT%253A%2520Efficient%2520Vision%2520Transformer%2520with%2520Mixture-of-Recursions%26entry.906535625%3DYiZhou%2520Li%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520image%250Arecognition%252C%2520yet%2520standard%2520ViT%2520architectures%2520are%2520hampered%2520by%2520substantial%250Aparameter%2520redundancy%2520and%2520high%2520computational%2520cost%252C%2520limiting%2520their%2520practical%250Adeployment.%2520While%2520recent%2520efforts%2520on%2520efficient%2520ViTs%2520primarily%2520focus%2520on%2520static%250Amodel%2520compression%2520or%2520token-level%2520sparsification%252C%2520they%2520remain%2520constrained%2520by%250Afixed%2520computational%2520depth%2520for%2520all%2520tokens.%2520In%2520this%2520work%252C%2520we%2520present%2520MoR-ViT%252C%2520a%250Anovel%2520vision%2520transformer%2520framework%2520that%252C%2520for%2520the%2520first%2520time%252C%2520incorporates%2520a%250Atoken-level%2520dynamic%2520recursion%2520mechanism%2520inspired%2520by%2520the%2520Mixture-of-Recursions%250A%2528MoR%2529%2520paradigm.%2520This%2520approach%2520enables%2520each%2520token%2520to%2520adaptively%2520determine%2520its%250Aprocessing%2520depth%252C%2520yielding%2520a%2520flexible%2520and%2520input-dependent%2520allocation%2520of%250Acomputational%2520resources.%2520Extensive%2520experiments%2520on%2520ImageNet-1K%2520and%2520transfer%250Abenchmarks%2520demonstrate%2520that%2520MoR-ViT%2520not%2520only%2520achieves%2520state-of-the-art%2520accuracy%250Awith%2520up%2520to%252070%2525%2520parameter%2520reduction%2520and%25202.5x%2520inference%2520acceleration%252C%2520but%2520also%250Aoutperforms%2520leading%2520efficient%2520ViT%2520baselines%2520such%2520as%2520DynamicViT%2520and%2520TinyViT%250Aunder%2520comparable%2520conditions.%2520These%2520results%2520establish%2520dynamic%2520recursion%2520as%2520an%250Aeffective%2520strategy%2520for%2520efficient%2520vision%2520transformers%2520and%2520open%2520new%2520avenues%2520for%250Ascalable%2520and%2520deployable%2520deep%2520learning%2520models%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOR-VIT%3A%20Efficient%20Vision%20Transformer%20with%20Mixture-of-Recursions&entry.906535625=YiZhou%20Li&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20in%20image%0Arecognition%2C%20yet%20standard%20ViT%20architectures%20are%20hampered%20by%20substantial%0Aparameter%20redundancy%20and%20high%20computational%20cost%2C%20limiting%20their%20practical%0Adeployment.%20While%20recent%20efforts%20on%20efficient%20ViTs%20primarily%20focus%20on%20static%0Amodel%20compression%20or%20token-level%20sparsification%2C%20they%20remain%20constrained%20by%0Afixed%20computational%20depth%20for%20all%20tokens.%20In%20this%20work%2C%20we%20present%20MoR-ViT%2C%20a%0Anovel%20vision%20transformer%20framework%20that%2C%20for%20the%20first%20time%2C%20incorporates%20a%0Atoken-level%20dynamic%20recursion%20mechanism%20inspired%20by%20the%20Mixture-of-Recursions%0A%28MoR%29%20paradigm.%20This%20approach%20enables%20each%20token%20to%20adaptively%20determine%20its%0Aprocessing%20depth%2C%20yielding%20a%20flexible%20and%20input-dependent%20allocation%20of%0Acomputational%20resources.%20Extensive%20experiments%20on%20ImageNet-1K%20and%20transfer%0Abenchmarks%20demonstrate%20that%20MoR-ViT%20not%20only%20achieves%20state-of-the-art%20accuracy%0Awith%20up%20to%2070%25%20parameter%20reduction%20and%202.5x%20inference%20acceleration%2C%20but%20also%0Aoutperforms%20leading%20efficient%20ViT%20baselines%20such%20as%20DynamicViT%20and%20TinyViT%0Aunder%20comparable%20conditions.%20These%20results%20establish%20dynamic%20recursion%20as%20an%0Aeffective%20strategy%20for%20efficient%20vision%20transformers%20and%20open%20new%20avenues%20for%0Ascalable%20and%20deployable%20deep%20learning%20models%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21761v2&entry.124074799=Read"},
{"title": "A Systematic Literature Review of Retrieval-Augmented Generation:\n  Techniques, Metrics, and Challenges", "author": "Andrew Brown and Muhammad Roman and Barry Devereux", "abstract": "  This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.\n", "link": "http://arxiv.org/abs/2508.06401v1", "date": "2025-08-08", "relevancy": 2.2855, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4611}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4562}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&body=Title%3A%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges%0AAuthor%3A%20Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux%0AAbstract%3A%20%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Literature%2520Review%2520of%2520Retrieval-Augmented%2520Generation%253A%250A%2520%2520Techniques%252C%2520Metrics%252C%2520and%2520Challenges%26entry.906535625%3DAndrew%2520Brown%2520and%2520Muhammad%2520Roman%2520and%2520Barry%2520Devereux%26entry.1292438233%3D%2520%2520This%2520systematic%2520review%2520of%2520the%2520research%2520literature%2520on%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520provides%2520a%2520focused%2520analysis%2520of%2520the%2520most%2520highly%2520cited%2520studies%250Apublished%2520between%25202020%2520and%2520May%25202025.%2520A%2520total%2520of%2520128%2520articles%2520met%2520our%2520inclusion%250Acriteria.%2520The%2520records%2520were%2520retrieved%2520from%2520ACM%2520Digital%2520Library%252C%2520IEEE%2520Xplore%252C%250AScopus%252C%2520ScienceDirect%252C%2520and%2520the%2520Digital%2520Bibliography%2520and%2520Library%2520Project%2520%2528DBLP%2529.%250ARAG%2520couples%2520a%2520neural%2520retriever%2520with%2520a%2520generative%2520language%2520model%252C%2520grounding%250Aoutput%2520in%2520up-to-date%252C%2520non-parametric%2520memory%2520while%2520retaining%2520the%2520semantic%250Ageneralisation%2520stored%2520in%2520model%2520weights.%2520Guided%2520by%2520the%2520PRISMA%25202020%2520framework%252C%2520we%250A%2528i%2529%2520specify%2520explicit%2520inclusion%2520and%2520exclusion%2520criteria%2520based%2520on%2520citation%2520count%250Aand%2520research%2520questions%252C%2520%2528ii%2529%2520catalogue%2520datasets%252C%2520architectures%252C%2520and%2520evaluation%250Apractices%252C%2520and%2520%2528iii%2529%2520synthesise%2520empirical%2520evidence%2520on%2520the%2520effectiveness%2520and%250Alimitations%2520of%2520RAG.%2520To%2520mitigate%2520citation-lag%2520bias%252C%2520we%2520applied%2520a%2520lower%250Acitation-count%2520threshold%2520to%2520papers%2520published%2520in%25202025%2520so%2520that%2520emerging%250Abreakthroughs%2520with%2520naturally%2520fewer%2520citations%2520were%2520still%2520captured.%2520This%2520review%250Aclarifies%2520the%2520current%2520research%2520landscape%252C%2520highlights%2520methodological%2520gaps%252C%2520and%250Acharts%2520priority%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&entry.906535625=Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux&entry.1292438233=%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06401v1&entry.124074799=Read"},
{"title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach\n  to Weakly-Supervised Video Anomaly Detection", "author": "Giacomo D'Amicantonio and Snehashis Majhi and Quan Kong and Lorenzo Garattoni and Gianpiero Francesca and Fran\u00e7ois Bremond and Egor Bondarev", "abstract": "  Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.\n", "link": "http://arxiv.org/abs/2508.06318v1", "date": "2025-08-08", "relevancy": 2.2718, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5699}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5667}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Experts%20Guided%20by%20Gaussian%20Splatters%20Matters%3A%20A%20new%20Approach%0A%20%20to%20Weakly-Supervised%20Video%20Anomaly%20Detection&body=Title%3A%20Mixture%20of%20Experts%20Guided%20by%20Gaussian%20Splatters%20Matters%3A%20A%20new%20Approach%0A%20%20to%20Weakly-Supervised%20Video%20Anomaly%20Detection%0AAuthor%3A%20Giacomo%20D%27Amicantonio%20and%20Snehashis%20Majhi%20and%20Quan%20Kong%20and%20Lorenzo%20Garattoni%20and%20Gianpiero%20Francesca%20and%20Fran%C3%A7ois%20Bremond%20and%20Egor%20Bondarev%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20is%20a%20challenging%20task%20due%20to%20the%20variability%20of%0Aanomalous%20events%20and%20the%20limited%20availability%20of%20labeled%20data.%20Under%20the%0AWeakly-Supervised%20VAD%20%28WSVAD%29%20paradigm%2C%20only%20video-level%20labels%20are%20provided%0Aduring%20training%2C%20while%20predictions%20are%20made%20at%20the%20frame%20level.%20Although%0Astate-of-the-art%20models%20perform%20well%20on%20simple%20anomalies%20%28e.g.%2C%20explosions%29%2C%0Athey%20struggle%20with%20complex%20real-world%20events%20%28e.g.%2C%20shoplifting%29.%20This%0Adifficulty%20stems%20from%20two%20key%20issues%3A%20%281%29%20the%20inability%20of%20current%20models%20to%0Aaddress%20the%20diversity%20of%20anomaly%20types%2C%20as%20they%20process%20all%20categories%20with%20a%0Ashared%20model%2C%20overlooking%20category-specific%20features%3B%20and%20%282%29%20the%20weak%0Asupervision%20signal%2C%20which%20lacks%20precise%20temporal%20information%2C%20limiting%20the%0Aability%20to%20capture%20nuanced%20anomalous%20patterns%20blended%20with%20normal%20events.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Gaussian%20Splatting-guided%20Mixture%20of%0AExperts%20%28GS-MoE%29%2C%20a%20novel%20framework%20that%20employs%20a%20set%20of%20expert%20models%2C%20each%0Aspecialized%20in%20capturing%20specific%20anomaly%20types.%20These%20experts%20are%20guided%20by%20a%0Atemporal%20Gaussian%20splatting%20loss%2C%20enabling%20the%20model%20to%20leverage%20temporal%0Aconsistency%20and%20enhance%20weak%20supervision.%20The%20Gaussian%20splatting%20approach%0Aencourages%20a%20more%20precise%20and%20comprehensive%20representation%20of%20anomalies%20by%0Afocusing%20on%20temporal%20segments%20most%20likely%20to%20contain%20abnormal%20events.%20The%0Apredictions%20from%20these%20specialized%20experts%20are%20integrated%20through%20a%0Amixture-of-experts%20mechanism%20to%20model%20complex%20relationships%20across%20diverse%0Aanomaly%20patterns.%20Our%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20a%0A91.58%25%20AUC%20on%20the%20UCF-Crime%20dataset%2C%20and%20demonstrates%20superior%20results%20on%0AXD-Violence%20and%20MSAD%20datasets.%20By%20leveraging%20category-specific%20expertise%20and%0Atemporal%20guidance%2C%20GS-MoE%20sets%20a%20new%20benchmark%20for%20VAD%20under%20weak%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Experts%2520Guided%2520by%2520Gaussian%2520Splatters%2520Matters%253A%2520A%2520new%2520Approach%250A%2520%2520to%2520Weakly-Supervised%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DGiacomo%2520D%2527Amicantonio%2520and%2520Snehashis%2520Majhi%2520and%2520Quan%2520Kong%2520and%2520Lorenzo%2520Garattoni%2520and%2520Gianpiero%2520Francesca%2520and%2520Fran%25C3%25A7ois%2520Bremond%2520and%2520Egor%2520Bondarev%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520variability%2520of%250Aanomalous%2520events%2520and%2520the%2520limited%2520availability%2520of%2520labeled%2520data.%2520Under%2520the%250AWeakly-Supervised%2520VAD%2520%2528WSVAD%2529%2520paradigm%252C%2520only%2520video-level%2520labels%2520are%2520provided%250Aduring%2520training%252C%2520while%2520predictions%2520are%2520made%2520at%2520the%2520frame%2520level.%2520Although%250Astate-of-the-art%2520models%2520perform%2520well%2520on%2520simple%2520anomalies%2520%2528e.g.%252C%2520explosions%2529%252C%250Athey%2520struggle%2520with%2520complex%2520real-world%2520events%2520%2528e.g.%252C%2520shoplifting%2529.%2520This%250Adifficulty%2520stems%2520from%2520two%2520key%2520issues%253A%2520%25281%2529%2520the%2520inability%2520of%2520current%2520models%2520to%250Aaddress%2520the%2520diversity%2520of%2520anomaly%2520types%252C%2520as%2520they%2520process%2520all%2520categories%2520with%2520a%250Ashared%2520model%252C%2520overlooking%2520category-specific%2520features%253B%2520and%2520%25282%2529%2520the%2520weak%250Asupervision%2520signal%252C%2520which%2520lacks%2520precise%2520temporal%2520information%252C%2520limiting%2520the%250Aability%2520to%2520capture%2520nuanced%2520anomalous%2520patterns%2520blended%2520with%2520normal%2520events.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Gaussian%2520Splatting-guided%2520Mixture%2520of%250AExperts%2520%2528GS-MoE%2529%252C%2520a%2520novel%2520framework%2520that%2520employs%2520a%2520set%2520of%2520expert%2520models%252C%2520each%250Aspecialized%2520in%2520capturing%2520specific%2520anomaly%2520types.%2520These%2520experts%2520are%2520guided%2520by%2520a%250Atemporal%2520Gaussian%2520splatting%2520loss%252C%2520enabling%2520the%2520model%2520to%2520leverage%2520temporal%250Aconsistency%2520and%2520enhance%2520weak%2520supervision.%2520The%2520Gaussian%2520splatting%2520approach%250Aencourages%2520a%2520more%2520precise%2520and%2520comprehensive%2520representation%2520of%2520anomalies%2520by%250Afocusing%2520on%2520temporal%2520segments%2520most%2520likely%2520to%2520contain%2520abnormal%2520events.%2520The%250Apredictions%2520from%2520these%2520specialized%2520experts%2520are%2520integrated%2520through%2520a%250Amixture-of-experts%2520mechanism%2520to%2520model%2520complex%2520relationships%2520across%2520diverse%250Aanomaly%2520patterns.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520a%250A91.58%2525%2520AUC%2520on%2520the%2520UCF-Crime%2520dataset%252C%2520and%2520demonstrates%2520superior%2520results%2520on%250AXD-Violence%2520and%2520MSAD%2520datasets.%2520By%2520leveraging%2520category-specific%2520expertise%2520and%250Atemporal%2520guidance%252C%2520GS-MoE%2520sets%2520a%2520new%2520benchmark%2520for%2520VAD%2520under%2520weak%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Experts%20Guided%20by%20Gaussian%20Splatters%20Matters%3A%20A%20new%20Approach%0A%20%20to%20Weakly-Supervised%20Video%20Anomaly%20Detection&entry.906535625=Giacomo%20D%27Amicantonio%20and%20Snehashis%20Majhi%20and%20Quan%20Kong%20and%20Lorenzo%20Garattoni%20and%20Gianpiero%20Francesca%20and%20Fran%C3%A7ois%20Bremond%20and%20Egor%20Bondarev&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20is%20a%20challenging%20task%20due%20to%20the%20variability%20of%0Aanomalous%20events%20and%20the%20limited%20availability%20of%20labeled%20data.%20Under%20the%0AWeakly-Supervised%20VAD%20%28WSVAD%29%20paradigm%2C%20only%20video-level%20labels%20are%20provided%0Aduring%20training%2C%20while%20predictions%20are%20made%20at%20the%20frame%20level.%20Although%0Astate-of-the-art%20models%20perform%20well%20on%20simple%20anomalies%20%28e.g.%2C%20explosions%29%2C%0Athey%20struggle%20with%20complex%20real-world%20events%20%28e.g.%2C%20shoplifting%29.%20This%0Adifficulty%20stems%20from%20two%20key%20issues%3A%20%281%29%20the%20inability%20of%20current%20models%20to%0Aaddress%20the%20diversity%20of%20anomaly%20types%2C%20as%20they%20process%20all%20categories%20with%20a%0Ashared%20model%2C%20overlooking%20category-specific%20features%3B%20and%20%282%29%20the%20weak%0Asupervision%20signal%2C%20which%20lacks%20precise%20temporal%20information%2C%20limiting%20the%0Aability%20to%20capture%20nuanced%20anomalous%20patterns%20blended%20with%20normal%20events.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Gaussian%20Splatting-guided%20Mixture%20of%0AExperts%20%28GS-MoE%29%2C%20a%20novel%20framework%20that%20employs%20a%20set%20of%20expert%20models%2C%20each%0Aspecialized%20in%20capturing%20specific%20anomaly%20types.%20These%20experts%20are%20guided%20by%20a%0Atemporal%20Gaussian%20splatting%20loss%2C%20enabling%20the%20model%20to%20leverage%20temporal%0Aconsistency%20and%20enhance%20weak%20supervision.%20The%20Gaussian%20splatting%20approach%0Aencourages%20a%20more%20precise%20and%20comprehensive%20representation%20of%20anomalies%20by%0Afocusing%20on%20temporal%20segments%20most%20likely%20to%20contain%20abnormal%20events.%20The%0Apredictions%20from%20these%20specialized%20experts%20are%20integrated%20through%20a%0Amixture-of-experts%20mechanism%20to%20model%20complex%20relationships%20across%20diverse%0Aanomaly%20patterns.%20Our%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20a%0A91.58%25%20AUC%20on%20the%20UCF-Crime%20dataset%2C%20and%20demonstrates%20superior%20results%20on%0AXD-Violence%20and%20MSAD%20datasets.%20By%20leveraging%20category-specific%20expertise%20and%0Atemporal%20guidance%2C%20GS-MoE%20sets%20a%20new%20benchmark%20for%20VAD%20under%20weak%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06318v1&entry.124074799=Read"},
{"title": "MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for\n  Referring Video Object Segmentation", "author": "Fu Rong and Meng Lan and Qian Zhang and Lefei Zhang", "abstract": "  Referring video object segmentation (RVOS) aims to segment objects in a video\naccording to textual descriptions, which requires the integration of multimodal\ninformation and temporal dynamics perception. The Segment Anything Model 2 (SAM\n2) has shown great effectiveness across various video segmentation tasks.\nHowever, its application to offline RVOS is challenged by the translation of\nthe text into effective prompts and a lack of global context awareness. In this\npaper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these\nchallenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to\njointly encode video and textual features, generating semantically aligned\nvideo and text embeddings, along with multimodal class tokens. A mask prior\ngenerator utilizes the video embeddings and class tokens to create pseudo masks\nof target objects and global context. These masks are fed into the prompt\nencoder as dense prompts along with multimodal class tokens as sparse prompts\nto generate accurate prompts for SAM 2. To provide the online SAM 2 with a\nglobal view, we introduce a hierarchical global-historical aggregator, which\nallows SAM 2 to aggregate global and historical information of target objects\nat both pixel and object levels, enhancing the target representation and\ntemporal consistency. Extensive experiments on several RVOS benchmarks\ndemonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed\nmodules. The code is available at https://github.com/rongfu-dsb/MPG-SAM2.\n", "link": "http://arxiv.org/abs/2501.13667v5", "date": "2025-08-08", "relevancy": 2.2669, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPG-SAM%202%3A%20Adapting%20SAM%202%20with%20Mask%20Priors%20and%20Global%20Context%20for%0A%20%20Referring%20Video%20Object%20Segmentation&body=Title%3A%20MPG-SAM%202%3A%20Adapting%20SAM%202%20with%20Mask%20Priors%20and%20Global%20Context%20for%0A%20%20Referring%20Video%20Object%20Segmentation%0AAuthor%3A%20Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang%0AAbstract%3A%20%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20aims%20to%20segment%20objects%20in%20a%20video%0Aaccording%20to%20textual%20descriptions%2C%20which%20requires%20the%20integration%20of%20multimodal%0Ainformation%20and%20temporal%20dynamics%20perception.%20The%20Segment%20Anything%20Model%202%20%28SAM%0A2%29%20has%20shown%20great%20effectiveness%20across%20various%20video%20segmentation%20tasks.%0AHowever%2C%20its%20application%20to%20offline%20RVOS%20is%20challenged%20by%20the%20translation%20of%0Athe%20text%20into%20effective%20prompts%20and%20a%20lack%20of%20global%20context%20awareness.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20RVOS%20framework%2C%20termed%20MPG-SAM%202%2C%20to%20address%20these%0Achallenges.%20Specifically%2C%20MPG-SAM%202%20employs%20a%20unified%20multimodal%20encoder%20to%0Ajointly%20encode%20video%20and%20textual%20features%2C%20generating%20semantically%20aligned%0Avideo%20and%20text%20embeddings%2C%20along%20with%20multimodal%20class%20tokens.%20A%20mask%20prior%0Agenerator%20utilizes%20the%20video%20embeddings%20and%20class%20tokens%20to%20create%20pseudo%20masks%0Aof%20target%20objects%20and%20global%20context.%20These%20masks%20are%20fed%20into%20the%20prompt%0Aencoder%20as%20dense%20prompts%20along%20with%20multimodal%20class%20tokens%20as%20sparse%20prompts%0Ato%20generate%20accurate%20prompts%20for%20SAM%202.%20To%20provide%20the%20online%20SAM%202%20with%20a%0Aglobal%20view%2C%20we%20introduce%20a%20hierarchical%20global-historical%20aggregator%2C%20which%0Aallows%20SAM%202%20to%20aggregate%20global%20and%20historical%20information%20of%20target%20objects%0Aat%20both%20pixel%20and%20object%20levels%2C%20enhancing%20the%20target%20representation%20and%0Atemporal%20consistency.%20Extensive%20experiments%20on%20several%20RVOS%20benchmarks%0Ademonstrate%20the%20superiority%20of%20MPG-SAM%202%20and%20the%20effectiveness%20of%20our%20proposed%0Amodules.%20The%20code%20is%20available%20at%20https%3A//github.com/rongfu-dsb/MPG-SAM2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13667v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPG-SAM%25202%253A%2520Adapting%2520SAM%25202%2520with%2520Mask%2520Priors%2520and%2520Global%2520Context%2520for%250A%2520%2520Referring%2520Video%2520Object%2520Segmentation%26entry.906535625%3DFu%2520Rong%2520and%2520Meng%2520Lan%2520and%2520Qian%2520Zhang%2520and%2520Lefei%2520Zhang%26entry.1292438233%3D%2520%2520Referring%2520video%2520object%2520segmentation%2520%2528RVOS%2529%2520aims%2520to%2520segment%2520objects%2520in%2520a%2520video%250Aaccording%2520to%2520textual%2520descriptions%252C%2520which%2520requires%2520the%2520integration%2520of%2520multimodal%250Ainformation%2520and%2520temporal%2520dynamics%2520perception.%2520The%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%250A2%2529%2520has%2520shown%2520great%2520effectiveness%2520across%2520various%2520video%2520segmentation%2520tasks.%250AHowever%252C%2520its%2520application%2520to%2520offline%2520RVOS%2520is%2520challenged%2520by%2520the%2520translation%2520of%250Athe%2520text%2520into%2520effective%2520prompts%2520and%2520a%2520lack%2520of%2520global%2520context%2520awareness.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520RVOS%2520framework%252C%2520termed%2520MPG-SAM%25202%252C%2520to%2520address%2520these%250Achallenges.%2520Specifically%252C%2520MPG-SAM%25202%2520employs%2520a%2520unified%2520multimodal%2520encoder%2520to%250Ajointly%2520encode%2520video%2520and%2520textual%2520features%252C%2520generating%2520semantically%2520aligned%250Avideo%2520and%2520text%2520embeddings%252C%2520along%2520with%2520multimodal%2520class%2520tokens.%2520A%2520mask%2520prior%250Agenerator%2520utilizes%2520the%2520video%2520embeddings%2520and%2520class%2520tokens%2520to%2520create%2520pseudo%2520masks%250Aof%2520target%2520objects%2520and%2520global%2520context.%2520These%2520masks%2520are%2520fed%2520into%2520the%2520prompt%250Aencoder%2520as%2520dense%2520prompts%2520along%2520with%2520multimodal%2520class%2520tokens%2520as%2520sparse%2520prompts%250Ato%2520generate%2520accurate%2520prompts%2520for%2520SAM%25202.%2520To%2520provide%2520the%2520online%2520SAM%25202%2520with%2520a%250Aglobal%2520view%252C%2520we%2520introduce%2520a%2520hierarchical%2520global-historical%2520aggregator%252C%2520which%250Aallows%2520SAM%25202%2520to%2520aggregate%2520global%2520and%2520historical%2520information%2520of%2520target%2520objects%250Aat%2520both%2520pixel%2520and%2520object%2520levels%252C%2520enhancing%2520the%2520target%2520representation%2520and%250Atemporal%2520consistency.%2520Extensive%2520experiments%2520on%2520several%2520RVOS%2520benchmarks%250Ademonstrate%2520the%2520superiority%2520of%2520MPG-SAM%25202%2520and%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amodules.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/rongfu-dsb/MPG-SAM2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13667v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPG-SAM%202%3A%20Adapting%20SAM%202%20with%20Mask%20Priors%20and%20Global%20Context%20for%0A%20%20Referring%20Video%20Object%20Segmentation&entry.906535625=Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang&entry.1292438233=%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20aims%20to%20segment%20objects%20in%20a%20video%0Aaccording%20to%20textual%20descriptions%2C%20which%20requires%20the%20integration%20of%20multimodal%0Ainformation%20and%20temporal%20dynamics%20perception.%20The%20Segment%20Anything%20Model%202%20%28SAM%0A2%29%20has%20shown%20great%20effectiveness%20across%20various%20video%20segmentation%20tasks.%0AHowever%2C%20its%20application%20to%20offline%20RVOS%20is%20challenged%20by%20the%20translation%20of%0Athe%20text%20into%20effective%20prompts%20and%20a%20lack%20of%20global%20context%20awareness.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20RVOS%20framework%2C%20termed%20MPG-SAM%202%2C%20to%20address%20these%0Achallenges.%20Specifically%2C%20MPG-SAM%202%20employs%20a%20unified%20multimodal%20encoder%20to%0Ajointly%20encode%20video%20and%20textual%20features%2C%20generating%20semantically%20aligned%0Avideo%20and%20text%20embeddings%2C%20along%20with%20multimodal%20class%20tokens.%20A%20mask%20prior%0Agenerator%20utilizes%20the%20video%20embeddings%20and%20class%20tokens%20to%20create%20pseudo%20masks%0Aof%20target%20objects%20and%20global%20context.%20These%20masks%20are%20fed%20into%20the%20prompt%0Aencoder%20as%20dense%20prompts%20along%20with%20multimodal%20class%20tokens%20as%20sparse%20prompts%0Ato%20generate%20accurate%20prompts%20for%20SAM%202.%20To%20provide%20the%20online%20SAM%202%20with%20a%0Aglobal%20view%2C%20we%20introduce%20a%20hierarchical%20global-historical%20aggregator%2C%20which%0Aallows%20SAM%202%20to%20aggregate%20global%20and%20historical%20information%20of%20target%20objects%0Aat%20both%20pixel%20and%20object%20levels%2C%20enhancing%20the%20target%20representation%20and%0Atemporal%20consistency.%20Extensive%20experiments%20on%20several%20RVOS%20benchmarks%0Ademonstrate%20the%20superiority%20of%20MPG-SAM%202%20and%20the%20effectiveness%20of%20our%20proposed%0Amodules.%20The%20code%20is%20available%20at%20https%3A//github.com/rongfu-dsb/MPG-SAM2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13667v5&entry.124074799=Read"},
{"title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "author": "Yingxian Chen and Jiahui Liu and Ruifan Di and Yanwei Li and Chirui Chang and Shizhen Zhao and Wilton W. T. Fok and Xiaojuan Qi and Yik-Chung Wu", "abstract": "  Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.\n", "link": "http://arxiv.org/abs/2508.06350v1", "date": "2025-08-08", "relevancy": 2.2623, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Effective%20Tokens%20with%20Video%20Anomaly%20in%20Large%20Language%20Models&body=Title%3A%20Aligning%20Effective%20Tokens%20with%20Video%20Anomaly%20in%20Large%20Language%20Models%0AAuthor%3A%20Yingxian%20Chen%20and%20Jiahui%20Liu%20and%20Ruifan%20Di%20and%20Yanwei%20Li%20and%20Chirui%20Chang%20and%20Shizhen%20Zhao%20and%20Wilton%20W.%20T.%20Fok%20and%20Xiaojuan%20Qi%20and%20Yik-Chung%20Wu%0AAbstract%3A%20%20%20Understanding%20abnormal%20events%20in%20videos%20is%20a%20vital%20and%20challenging%20task%20that%0Ahas%20garnered%20significant%20attention%20in%20a%20wide%20range%20of%20applications.%20Although%0Acurrent%20video%20understanding%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20are%0Acapable%20of%20analyzing%20general%20videos%2C%20they%20often%20struggle%20to%20handle%20anomalies%0Adue%20to%20the%20spatial%20and%20temporal%20sparsity%20of%20abnormal%20events%2C%20where%20the%0Aredundant%20information%20always%20leads%20to%20suboptimal%20outcomes.%20To%20address%20these%0Achallenges%2C%20exploiting%20the%20representation%20and%20generalization%20capabilities%20of%0AVison%20Language%20Models%20%28VLMs%29%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20we%20propose%0AVA-GPT%2C%20a%20novel%20MLLM%20designed%20for%20summarizing%20and%20localizing%20abnormal%20events%20in%0Avarious%20videos.%20Our%20approach%20efficiently%20aligns%20effective%20tokens%20between%20visual%0Aencoders%20and%20LLMs%20through%20two%20key%20proposed%20modules%3A%20Spatial%20Effective%20Token%0ASelection%20%28SETS%29%20and%20Temporal%20Effective%20Token%20Generation%20%28TETG%29.%20These%20modules%0Aenable%20our%20model%20to%20effectively%20capture%20and%20analyze%20both%20spatial%20and%20temporal%0Ainformation%20associated%20with%20abnormal%20events%2C%20resulting%20in%20more%20accurate%0Aresponses%20and%20interactions.%20Furthermore%2C%20we%20construct%20an%20instruction-following%0Adataset%20specifically%20for%20fine-tuning%20video-anomaly-aware%20MLLMs%2C%20and%20introduce%20a%0Across-domain%20evaluation%20benchmark%20based%20on%20XD-Violence%20dataset.%20Our%20proposed%0Amethod%20outperforms%20existing%20state-of-the-art%20methods%20on%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Effective%2520Tokens%2520with%2520Video%2520Anomaly%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYingxian%2520Chen%2520and%2520Jiahui%2520Liu%2520and%2520Ruifan%2520Di%2520and%2520Yanwei%2520Li%2520and%2520Chirui%2520Chang%2520and%2520Shizhen%2520Zhao%2520and%2520Wilton%2520W.%2520T.%2520Fok%2520and%2520Xiaojuan%2520Qi%2520and%2520Yik-Chung%2520Wu%26entry.1292438233%3D%2520%2520Understanding%2520abnormal%2520events%2520in%2520videos%2520is%2520a%2520vital%2520and%2520challenging%2520task%2520that%250Ahas%2520garnered%2520significant%2520attention%2520in%2520a%2520wide%2520range%2520of%2520applications.%2520Although%250Acurrent%2520video%2520understanding%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%250Acapable%2520of%2520analyzing%2520general%2520videos%252C%2520they%2520often%2520struggle%2520to%2520handle%2520anomalies%250Adue%2520to%2520the%2520spatial%2520and%2520temporal%2520sparsity%2520of%2520abnormal%2520events%252C%2520where%2520the%250Aredundant%2520information%2520always%2520leads%2520to%2520suboptimal%2520outcomes.%2520To%2520address%2520these%250Achallenges%252C%2520exploiting%2520the%2520representation%2520and%2520generalization%2520capabilities%2520of%250AVison%2520Language%2520Models%2520%2528VLMs%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520we%2520propose%250AVA-GPT%252C%2520a%2520novel%2520MLLM%2520designed%2520for%2520summarizing%2520and%2520localizing%2520abnormal%2520events%2520in%250Avarious%2520videos.%2520Our%2520approach%2520efficiently%2520aligns%2520effective%2520tokens%2520between%2520visual%250Aencoders%2520and%2520LLMs%2520through%2520two%2520key%2520proposed%2520modules%253A%2520Spatial%2520Effective%2520Token%250ASelection%2520%2528SETS%2529%2520and%2520Temporal%2520Effective%2520Token%2520Generation%2520%2528TETG%2529.%2520These%2520modules%250Aenable%2520our%2520model%2520to%2520effectively%2520capture%2520and%2520analyze%2520both%2520spatial%2520and%2520temporal%250Ainformation%2520associated%2520with%2520abnormal%2520events%252C%2520resulting%2520in%2520more%2520accurate%250Aresponses%2520and%2520interactions.%2520Furthermore%252C%2520we%2520construct%2520an%2520instruction-following%250Adataset%2520specifically%2520for%2520fine-tuning%2520video-anomaly-aware%2520MLLMs%252C%2520and%2520introduce%2520a%250Across-domain%2520evaluation%2520benchmark%2520based%2520on%2520XD-Violence%2520dataset.%2520Our%2520proposed%250Amethod%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520on%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Effective%20Tokens%20with%20Video%20Anomaly%20in%20Large%20Language%20Models&entry.906535625=Yingxian%20Chen%20and%20Jiahui%20Liu%20and%20Ruifan%20Di%20and%20Yanwei%20Li%20and%20Chirui%20Chang%20and%20Shizhen%20Zhao%20and%20Wilton%20W.%20T.%20Fok%20and%20Xiaojuan%20Qi%20and%20Yik-Chung%20Wu&entry.1292438233=%20%20Understanding%20abnormal%20events%20in%20videos%20is%20a%20vital%20and%20challenging%20task%20that%0Ahas%20garnered%20significant%20attention%20in%20a%20wide%20range%20of%20applications.%20Although%0Acurrent%20video%20understanding%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20are%0Acapable%20of%20analyzing%20general%20videos%2C%20they%20often%20struggle%20to%20handle%20anomalies%0Adue%20to%20the%20spatial%20and%20temporal%20sparsity%20of%20abnormal%20events%2C%20where%20the%0Aredundant%20information%20always%20leads%20to%20suboptimal%20outcomes.%20To%20address%20these%0Achallenges%2C%20exploiting%20the%20representation%20and%20generalization%20capabilities%20of%0AVison%20Language%20Models%20%28VLMs%29%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20we%20propose%0AVA-GPT%2C%20a%20novel%20MLLM%20designed%20for%20summarizing%20and%20localizing%20abnormal%20events%20in%0Avarious%20videos.%20Our%20approach%20efficiently%20aligns%20effective%20tokens%20between%20visual%0Aencoders%20and%20LLMs%20through%20two%20key%20proposed%20modules%3A%20Spatial%20Effective%20Token%0ASelection%20%28SETS%29%20and%20Temporal%20Effective%20Token%20Generation%20%28TETG%29.%20These%20modules%0Aenable%20our%20model%20to%20effectively%20capture%20and%20analyze%20both%20spatial%20and%20temporal%0Ainformation%20associated%20with%20abnormal%20events%2C%20resulting%20in%20more%20accurate%0Aresponses%20and%20interactions.%20Furthermore%2C%20we%20construct%20an%20instruction-following%0Adataset%20specifically%20for%20fine-tuning%20video-anomaly-aware%20MLLMs%2C%20and%20introduce%20a%0Across-domain%20evaluation%20benchmark%20based%20on%20XD-Violence%20dataset.%20Our%20proposed%0Amethod%20outperforms%20existing%20state-of-the-art%20methods%20on%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06350v1&entry.124074799=Read"},
{"title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset", "author": "Ruiyan Wang and Lin Zuo and Zonghao Lin and Qiang Wang and Zhengxue Cheng and Rong Xie and Jun Ling and Li Song", "abstract": "  The Human-Object Interaction (HOI) task explores the dynamic interactions\nbetween humans and objects in physical environments, providing essential\nbiomechanical and cognitive-behavioral foundations for fields such as robotics,\nvirtual reality, and human-computer interaction. However, existing HOI data\nsets focus on details of affordance, often neglecting the influence of physical\nproperties of objects on human long-term motion. To bridge this gap, we\nintroduce the PA-HOI Motion Capture dataset, which highlights the impact of\nobjects' physical attributes on human motion dynamics, including human posture,\nmoving velocity, and other motion characteristics. The dataset comprises 562\nmotion sequences of human-object interactions, with each sequence performed by\nsubjects of different genders interacting with 35 3D objects that vary in size,\nshape, and weight. This dataset stands out by significantly extending the scope\nof existing ones for understanding how the physical attributes of different\nobjects influence human posture, speed, motion scale, and interacting\nstrategies. We further demonstrate the applicability of the PA-HOI dataset by\nintegrating it with existing motion generation methods, validating its capacity\nto transfer realistic physical awareness.\n", "link": "http://arxiv.org/abs/2508.06205v1", "date": "2025-08-08", "relevancy": 2.2404, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5746}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5621}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PA-HOI%3A%20A%20Physics-Aware%20Human%20and%20Object%20Interaction%20Dataset&body=Title%3A%20PA-HOI%3A%20A%20Physics-Aware%20Human%20and%20Object%20Interaction%20Dataset%0AAuthor%3A%20Ruiyan%20Wang%20and%20Lin%20Zuo%20and%20Zonghao%20Lin%20and%20Qiang%20Wang%20and%20Zhengxue%20Cheng%20and%20Rong%20Xie%20and%20Jun%20Ling%20and%20Li%20Song%0AAbstract%3A%20%20%20The%20Human-Object%20Interaction%20%28HOI%29%20task%20explores%20the%20dynamic%20interactions%0Abetween%20humans%20and%20objects%20in%20physical%20environments%2C%20providing%20essential%0Abiomechanical%20and%20cognitive-behavioral%20foundations%20for%20fields%20such%20as%20robotics%2C%0Avirtual%20reality%2C%20and%20human-computer%20interaction.%20However%2C%20existing%20HOI%20data%0Asets%20focus%20on%20details%20of%20affordance%2C%20often%20neglecting%20the%20influence%20of%20physical%0Aproperties%20of%20objects%20on%20human%20long-term%20motion.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20PA-HOI%20Motion%20Capture%20dataset%2C%20which%20highlights%20the%20impact%20of%0Aobjects%27%20physical%20attributes%20on%20human%20motion%20dynamics%2C%20including%20human%20posture%2C%0Amoving%20velocity%2C%20and%20other%20motion%20characteristics.%20The%20dataset%20comprises%20562%0Amotion%20sequences%20of%20human-object%20interactions%2C%20with%20each%20sequence%20performed%20by%0Asubjects%20of%20different%20genders%20interacting%20with%2035%203D%20objects%20that%20vary%20in%20size%2C%0Ashape%2C%20and%20weight.%20This%20dataset%20stands%20out%20by%20significantly%20extending%20the%20scope%0Aof%20existing%20ones%20for%20understanding%20how%20the%20physical%20attributes%20of%20different%0Aobjects%20influence%20human%20posture%2C%20speed%2C%20motion%20scale%2C%20and%20interacting%0Astrategies.%20We%20further%20demonstrate%20the%20applicability%20of%20the%20PA-HOI%20dataset%20by%0Aintegrating%20it%20with%20existing%20motion%20generation%20methods%2C%20validating%20its%20capacity%0Ato%20transfer%20realistic%20physical%20awareness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPA-HOI%253A%2520A%2520Physics-Aware%2520Human%2520and%2520Object%2520Interaction%2520Dataset%26entry.906535625%3DRuiyan%2520Wang%2520and%2520Lin%2520Zuo%2520and%2520Zonghao%2520Lin%2520and%2520Qiang%2520Wang%2520and%2520Zhengxue%2520Cheng%2520and%2520Rong%2520Xie%2520and%2520Jun%2520Ling%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520The%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520task%2520explores%2520the%2520dynamic%2520interactions%250Abetween%2520humans%2520and%2520objects%2520in%2520physical%2520environments%252C%2520providing%2520essential%250Abiomechanical%2520and%2520cognitive-behavioral%2520foundations%2520for%2520fields%2520such%2520as%2520robotics%252C%250Avirtual%2520reality%252C%2520and%2520human-computer%2520interaction.%2520However%252C%2520existing%2520HOI%2520data%250Asets%2520focus%2520on%2520details%2520of%2520affordance%252C%2520often%2520neglecting%2520the%2520influence%2520of%2520physical%250Aproperties%2520of%2520objects%2520on%2520human%2520long-term%2520motion.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520PA-HOI%2520Motion%2520Capture%2520dataset%252C%2520which%2520highlights%2520the%2520impact%2520of%250Aobjects%2527%2520physical%2520attributes%2520on%2520human%2520motion%2520dynamics%252C%2520including%2520human%2520posture%252C%250Amoving%2520velocity%252C%2520and%2520other%2520motion%2520characteristics.%2520The%2520dataset%2520comprises%2520562%250Amotion%2520sequences%2520of%2520human-object%2520interactions%252C%2520with%2520each%2520sequence%2520performed%2520by%250Asubjects%2520of%2520different%2520genders%2520interacting%2520with%252035%25203D%2520objects%2520that%2520vary%2520in%2520size%252C%250Ashape%252C%2520and%2520weight.%2520This%2520dataset%2520stands%2520out%2520by%2520significantly%2520extending%2520the%2520scope%250Aof%2520existing%2520ones%2520for%2520understanding%2520how%2520the%2520physical%2520attributes%2520of%2520different%250Aobjects%2520influence%2520human%2520posture%252C%2520speed%252C%2520motion%2520scale%252C%2520and%2520interacting%250Astrategies.%2520We%2520further%2520demonstrate%2520the%2520applicability%2520of%2520the%2520PA-HOI%2520dataset%2520by%250Aintegrating%2520it%2520with%2520existing%2520motion%2520generation%2520methods%252C%2520validating%2520its%2520capacity%250Ato%2520transfer%2520realistic%2520physical%2520awareness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PA-HOI%3A%20A%20Physics-Aware%20Human%20and%20Object%20Interaction%20Dataset&entry.906535625=Ruiyan%20Wang%20and%20Lin%20Zuo%20and%20Zonghao%20Lin%20and%20Qiang%20Wang%20and%20Zhengxue%20Cheng%20and%20Rong%20Xie%20and%20Jun%20Ling%20and%20Li%20Song&entry.1292438233=%20%20The%20Human-Object%20Interaction%20%28HOI%29%20task%20explores%20the%20dynamic%20interactions%0Abetween%20humans%20and%20objects%20in%20physical%20environments%2C%20providing%20essential%0Abiomechanical%20and%20cognitive-behavioral%20foundations%20for%20fields%20such%20as%20robotics%2C%0Avirtual%20reality%2C%20and%20human-computer%20interaction.%20However%2C%20existing%20HOI%20data%0Asets%20focus%20on%20details%20of%20affordance%2C%20often%20neglecting%20the%20influence%20of%20physical%0Aproperties%20of%20objects%20on%20human%20long-term%20motion.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20PA-HOI%20Motion%20Capture%20dataset%2C%20which%20highlights%20the%20impact%20of%0Aobjects%27%20physical%20attributes%20on%20human%20motion%20dynamics%2C%20including%20human%20posture%2C%0Amoving%20velocity%2C%20and%20other%20motion%20characteristics.%20The%20dataset%20comprises%20562%0Amotion%20sequences%20of%20human-object%20interactions%2C%20with%20each%20sequence%20performed%20by%0Asubjects%20of%20different%20genders%20interacting%20with%2035%203D%20objects%20that%20vary%20in%20size%2C%0Ashape%2C%20and%20weight.%20This%20dataset%20stands%20out%20by%20significantly%20extending%20the%20scope%0Aof%20existing%20ones%20for%20understanding%20how%20the%20physical%20attributes%20of%20different%0Aobjects%20influence%20human%20posture%2C%20speed%2C%20motion%20scale%2C%20and%20interacting%0Astrategies.%20We%20further%20demonstrate%20the%20applicability%20of%20the%20PA-HOI%20dataset%20by%0Aintegrating%20it%20with%20existing%20motion%20generation%20methods%2C%20validating%20its%20capacity%0Ato%20transfer%20realistic%20physical%20awareness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06205v1&entry.124074799=Read"},
{"title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in\n  Competitive Computer Games", "author": "Mille Mei Zhen Loo and Gert Luzkov and Paolo Burelli", "abstract": "  Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.\n", "link": "http://arxiv.org/abs/2508.06348v1", "date": "2025-08-08", "relevancy": 2.24, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4524}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4484}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AntiCheatPT%3A%20A%20Transformer-Based%20Approach%20to%20Cheat%20Detection%20in%0A%20%20Competitive%20Computer%20Games&body=Title%3A%20AntiCheatPT%3A%20A%20Transformer-Based%20Approach%20to%20Cheat%20Detection%20in%0A%20%20Competitive%20Computer%20Games%0AAuthor%3A%20Mille%20Mei%20Zhen%20Loo%20and%20Gert%20Luzkov%20and%20Paolo%20Burelli%0AAbstract%3A%20%20%20Cheating%20in%20online%20video%20games%20compromises%20the%20integrity%20of%20gaming%0Aexperiences.%20Anti-cheat%20systems%2C%20such%20as%20VAC%20%28Valve%20Anti-Cheat%29%2C%20face%0Asignificant%20challenges%20in%20keeping%20pace%20with%20evolving%20cheating%20methods%20without%0Aimposing%20invasive%20measures%20on%20users%27%20systems.%20This%20paper%20presents%0AAntiCheatPT%5C_256%2C%20a%20transformer-based%20machine%20learning%20model%20designed%20to%20detect%0Acheating%20behaviour%20in%20Counter-Strike%202%20using%20gameplay%20data.%20To%20support%20this%2C%20we%0Aintroduce%20and%20publicly%20release%20CS2CD%3A%20A%20labelled%20dataset%20of%20795%20matches.%20Using%0Athis%20dataset%2C%2090%2C707%20context%20windows%20were%20created%20and%20subsequently%20augmented%20to%0Aaddress%20class%20imbalance.%20The%20transformer%20model%2C%20trained%20on%20these%20windows%2C%0Aachieved%20an%20accuracy%20of%2089.17%5C%25%20and%20an%20AUC%20of%2093.36%5C%25%20on%20an%20unaugmented%20test%0Aset.%20This%20approach%20emphasizes%20reproducibility%20and%20real-world%20applicability%2C%0Aoffering%20a%20robust%20baseline%20for%20future%20research%20in%20data-driven%20cheat%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAntiCheatPT%253A%2520A%2520Transformer-Based%2520Approach%2520to%2520Cheat%2520Detection%2520in%250A%2520%2520Competitive%2520Computer%2520Games%26entry.906535625%3DMille%2520Mei%2520Zhen%2520Loo%2520and%2520Gert%2520Luzkov%2520and%2520Paolo%2520Burelli%26entry.1292438233%3D%2520%2520Cheating%2520in%2520online%2520video%2520games%2520compromises%2520the%2520integrity%2520of%2520gaming%250Aexperiences.%2520Anti-cheat%2520systems%252C%2520such%2520as%2520VAC%2520%2528Valve%2520Anti-Cheat%2529%252C%2520face%250Asignificant%2520challenges%2520in%2520keeping%2520pace%2520with%2520evolving%2520cheating%2520methods%2520without%250Aimposing%2520invasive%2520measures%2520on%2520users%2527%2520systems.%2520This%2520paper%2520presents%250AAntiCheatPT%255C_256%252C%2520a%2520transformer-based%2520machine%2520learning%2520model%2520designed%2520to%2520detect%250Acheating%2520behaviour%2520in%2520Counter-Strike%25202%2520using%2520gameplay%2520data.%2520To%2520support%2520this%252C%2520we%250Aintroduce%2520and%2520publicly%2520release%2520CS2CD%253A%2520A%2520labelled%2520dataset%2520of%2520795%2520matches.%2520Using%250Athis%2520dataset%252C%252090%252C707%2520context%2520windows%2520were%2520created%2520and%2520subsequently%2520augmented%2520to%250Aaddress%2520class%2520imbalance.%2520The%2520transformer%2520model%252C%2520trained%2520on%2520these%2520windows%252C%250Aachieved%2520an%2520accuracy%2520of%252089.17%255C%2525%2520and%2520an%2520AUC%2520of%252093.36%255C%2525%2520on%2520an%2520unaugmented%2520test%250Aset.%2520This%2520approach%2520emphasizes%2520reproducibility%2520and%2520real-world%2520applicability%252C%250Aoffering%2520a%2520robust%2520baseline%2520for%2520future%2520research%2520in%2520data-driven%2520cheat%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AntiCheatPT%3A%20A%20Transformer-Based%20Approach%20to%20Cheat%20Detection%20in%0A%20%20Competitive%20Computer%20Games&entry.906535625=Mille%20Mei%20Zhen%20Loo%20and%20Gert%20Luzkov%20and%20Paolo%20Burelli&entry.1292438233=%20%20Cheating%20in%20online%20video%20games%20compromises%20the%20integrity%20of%20gaming%0Aexperiences.%20Anti-cheat%20systems%2C%20such%20as%20VAC%20%28Valve%20Anti-Cheat%29%2C%20face%0Asignificant%20challenges%20in%20keeping%20pace%20with%20evolving%20cheating%20methods%20without%0Aimposing%20invasive%20measures%20on%20users%27%20systems.%20This%20paper%20presents%0AAntiCheatPT%5C_256%2C%20a%20transformer-based%20machine%20learning%20model%20designed%20to%20detect%0Acheating%20behaviour%20in%20Counter-Strike%202%20using%20gameplay%20data.%20To%20support%20this%2C%20we%0Aintroduce%20and%20publicly%20release%20CS2CD%3A%20A%20labelled%20dataset%20of%20795%20matches.%20Using%0Athis%20dataset%2C%2090%2C707%20context%20windows%20were%20created%20and%20subsequently%20augmented%20to%0Aaddress%20class%20imbalance.%20The%20transformer%20model%2C%20trained%20on%20these%20windows%2C%0Aachieved%20an%20accuracy%20of%2089.17%5C%25%20and%20an%20AUC%20of%2093.36%5C%25%20on%20an%20unaugmented%20test%0Aset.%20This%20approach%20emphasizes%20reproducibility%20and%20real-world%20applicability%2C%0Aoffering%20a%20robust%20baseline%20for%20future%20research%20in%20data-driven%20cheat%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06348v1&entry.124074799=Read"},
{"title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation", "author": "Guido Manni and Clemente Lauretti and Loredana Zollo and Paolo Soda", "abstract": "  Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.\n", "link": "http://arxiv.org/abs/2508.06429v1", "date": "2025-08-08", "relevancy": 2.2356, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5756}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.548}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARSE%20Data%2C%20Rich%20Results%3A%20Few-Shot%20Semi-Supervised%20Learning%20via%0A%20%20Class-Conditioned%20Image%20Translation&body=Title%3A%20SPARSE%20Data%2C%20Rich%20Results%3A%20Few-Shot%20Semi-Supervised%20Learning%20via%0A%20%20Class-Conditioned%20Image%20Translation%0AAuthor%3A%20Guido%20Manni%20and%20Clemente%20Lauretti%20and%20Loredana%20Zollo%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Deep%20learning%20has%20revolutionized%20medical%20imaging%2C%20but%20its%20effectiveness%20is%0Aseverely%20limited%20by%20insufficient%20labeled%20training%20data.%20This%20paper%20introduces%20a%0Anovel%20GAN-based%20semi-supervised%20learning%20framework%20specifically%20designed%20for%0Alow%20labeled-data%20regimes%2C%20evaluated%20across%20settings%20with%205%20to%2050%20labeled%0Asamples%20per%20class.%20Our%20approach%20integrates%20three%20specialized%20neural%20networks%20--%0Aa%20generator%20for%20class-conditioned%20image%20translation%2C%20a%20discriminator%20for%0Aauthenticity%20assessment%20and%20classification%2C%20and%20a%20dedicated%20classifier%20--%0Awithin%20a%20three-phase%20training%20framework.%20The%20method%20alternates%20between%0Asupervised%20training%20on%20limited%20labeled%20data%20and%20unsupervised%20learning%20that%0Aleverages%20abundant%20unlabeled%20images%20through%20image-to-image%20translation%20rather%0Athan%20generation%20from%20noise.%20We%20employ%20ensemble-based%20pseudo-labeling%20that%0Acombines%20confidence-weighted%20predictions%20from%20the%20discriminator%20and%20classifier%0Awith%20temporal%20consistency%20through%20exponential%20moving%20averaging%2C%20enabling%0Areliable%20label%20estimation%20for%20unlabeled%20data.%20Comprehensive%20evaluation%20across%0Aeleven%20MedMNIST%20datasets%20demonstrates%20that%20our%20approach%20achieves%20statistically%0Asignificant%20improvements%20over%20six%20state-of-the-art%20GAN-based%20semi-supervised%0Amethods%2C%20with%20particularly%20strong%20performance%20in%20the%20extreme%205-shot%20setting%0Awhere%20the%20scarcity%20of%20labeled%20data%20is%20most%20challenging.%20The%20framework%20maintains%0Aits%20superiority%20across%20all%20evaluated%20settings%20%285%2C%2010%2C%2020%2C%20and%2050%20shots%20per%0Aclass%29.%20Our%20approach%20offers%20a%20practical%20solution%20for%20medical%20imaging%0Aapplications%20where%20annotation%20costs%20are%20prohibitive%2C%20enabling%20robust%0Aclassification%20performance%20even%20with%20minimal%20labeled%20data.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GuidoManni/SPARSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARSE%2520Data%252C%2520Rich%2520Results%253A%2520Few-Shot%2520Semi-Supervised%2520Learning%2520via%250A%2520%2520Class-Conditioned%2520Image%2520Translation%26entry.906535625%3DGuido%2520Manni%2520and%2520Clemente%2520Lauretti%2520and%2520Loredana%2520Zollo%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520revolutionized%2520medical%2520imaging%252C%2520but%2520its%2520effectiveness%2520is%250Aseverely%2520limited%2520by%2520insufficient%2520labeled%2520training%2520data.%2520This%2520paper%2520introduces%2520a%250Anovel%2520GAN-based%2520semi-supervised%2520learning%2520framework%2520specifically%2520designed%2520for%250Alow%2520labeled-data%2520regimes%252C%2520evaluated%2520across%2520settings%2520with%25205%2520to%252050%2520labeled%250Asamples%2520per%2520class.%2520Our%2520approach%2520integrates%2520three%2520specialized%2520neural%2520networks%2520--%250Aa%2520generator%2520for%2520class-conditioned%2520image%2520translation%252C%2520a%2520discriminator%2520for%250Aauthenticity%2520assessment%2520and%2520classification%252C%2520and%2520a%2520dedicated%2520classifier%2520--%250Awithin%2520a%2520three-phase%2520training%2520framework.%2520The%2520method%2520alternates%2520between%250Asupervised%2520training%2520on%2520limited%2520labeled%2520data%2520and%2520unsupervised%2520learning%2520that%250Aleverages%2520abundant%2520unlabeled%2520images%2520through%2520image-to-image%2520translation%2520rather%250Athan%2520generation%2520from%2520noise.%2520We%2520employ%2520ensemble-based%2520pseudo-labeling%2520that%250Acombines%2520confidence-weighted%2520predictions%2520from%2520the%2520discriminator%2520and%2520classifier%250Awith%2520temporal%2520consistency%2520through%2520exponential%2520moving%2520averaging%252C%2520enabling%250Areliable%2520label%2520estimation%2520for%2520unlabeled%2520data.%2520Comprehensive%2520evaluation%2520across%250Aeleven%2520MedMNIST%2520datasets%2520demonstrates%2520that%2520our%2520approach%2520achieves%2520statistically%250Asignificant%2520improvements%2520over%2520six%2520state-of-the-art%2520GAN-based%2520semi-supervised%250Amethods%252C%2520with%2520particularly%2520strong%2520performance%2520in%2520the%2520extreme%25205-shot%2520setting%250Awhere%2520the%2520scarcity%2520of%2520labeled%2520data%2520is%2520most%2520challenging.%2520The%2520framework%2520maintains%250Aits%2520superiority%2520across%2520all%2520evaluated%2520settings%2520%25285%252C%252010%252C%252020%252C%2520and%252050%2520shots%2520per%250Aclass%2529.%2520Our%2520approach%2520offers%2520a%2520practical%2520solution%2520for%2520medical%2520imaging%250Aapplications%2520where%2520annotation%2520costs%2520are%2520prohibitive%252C%2520enabling%2520robust%250Aclassification%2520performance%2520even%2520with%2520minimal%2520labeled%2520data.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/GuidoManni/SPARSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARSE%20Data%2C%20Rich%20Results%3A%20Few-Shot%20Semi-Supervised%20Learning%20via%0A%20%20Class-Conditioned%20Image%20Translation&entry.906535625=Guido%20Manni%20and%20Clemente%20Lauretti%20and%20Loredana%20Zollo%20and%20Paolo%20Soda&entry.1292438233=%20%20Deep%20learning%20has%20revolutionized%20medical%20imaging%2C%20but%20its%20effectiveness%20is%0Aseverely%20limited%20by%20insufficient%20labeled%20training%20data.%20This%20paper%20introduces%20a%0Anovel%20GAN-based%20semi-supervised%20learning%20framework%20specifically%20designed%20for%0Alow%20labeled-data%20regimes%2C%20evaluated%20across%20settings%20with%205%20to%2050%20labeled%0Asamples%20per%20class.%20Our%20approach%20integrates%20three%20specialized%20neural%20networks%20--%0Aa%20generator%20for%20class-conditioned%20image%20translation%2C%20a%20discriminator%20for%0Aauthenticity%20assessment%20and%20classification%2C%20and%20a%20dedicated%20classifier%20--%0Awithin%20a%20three-phase%20training%20framework.%20The%20method%20alternates%20between%0Asupervised%20training%20on%20limited%20labeled%20data%20and%20unsupervised%20learning%20that%0Aleverages%20abundant%20unlabeled%20images%20through%20image-to-image%20translation%20rather%0Athan%20generation%20from%20noise.%20We%20employ%20ensemble-based%20pseudo-labeling%20that%0Acombines%20confidence-weighted%20predictions%20from%20the%20discriminator%20and%20classifier%0Awith%20temporal%20consistency%20through%20exponential%20moving%20averaging%2C%20enabling%0Areliable%20label%20estimation%20for%20unlabeled%20data.%20Comprehensive%20evaluation%20across%0Aeleven%20MedMNIST%20datasets%20demonstrates%20that%20our%20approach%20achieves%20statistically%0Asignificant%20improvements%20over%20six%20state-of-the-art%20GAN-based%20semi-supervised%0Amethods%2C%20with%20particularly%20strong%20performance%20in%20the%20extreme%205-shot%20setting%0Awhere%20the%20scarcity%20of%20labeled%20data%20is%20most%20challenging.%20The%20framework%20maintains%0Aits%20superiority%20across%20all%20evaluated%20settings%20%285%2C%2010%2C%2020%2C%20and%2050%20shots%20per%0Aclass%29.%20Our%20approach%20offers%20a%20practical%20solution%20for%20medical%20imaging%0Aapplications%20where%20annotation%20costs%20are%20prohibitive%2C%20enabling%20robust%0Aclassification%20performance%20even%20with%20minimal%20labeled%20data.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GuidoManni/SPARSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06429v1&entry.124074799=Read"},
{"title": "Learning to Initialize Trajectory Optimization for Vision-Based\n  Autonomous Flight in Unknown Environments", "author": "Yicheng Chen and Jinjie Li and Wenyuan Qin and Yongzhao Hua and Xiwang Dong and Qingdong Li", "abstract": "  Autonomous flight in unknown environments requires precise spatial and\ntemporal trajectory planning, often involving computationally expensive\nnonconvex optimization prone to local optima. To overcome these challenges, we\npresent the Neural-Enhanced Trajectory Planner (NEO-Planner), a novel approach\nthat leverages a Neural Network (NN) Planner to provide informed initial values\nfor trajectory optimization. The NN-Planner is trained on a dataset generated\nby an expert planner using batch sampling, capturing multimodal trajectory\nsolutions. It learns to predict spatial and temporal parameters for\ntrajectories directly from raw sensor observations. NEO-Planner starts\noptimization from these predictions, accelerating computation speed while\nmaintaining explainability. Furthermore, we introduce a robust online\nreplanning framework that accommodates planning latency for smooth trajectory\ntracking. Extensive simulations demonstrate that NEO-Planner reduces\noptimization iterations by 20%, leading to a 26% decrease in computation time\ncompared with pure optimization-based methods. It maintains trajectory quality\ncomparable to baseline approaches and generalizes well to unseen environments.\nReal-world experiments validate its effectiveness for autonomous drone\nnavigation in cluttered, unknown environments.\n", "link": "http://arxiv.org/abs/2309.10683v2", "date": "2025-08-08", "relevancy": 2.2163, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5625}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Initialize%20Trajectory%20Optimization%20for%20Vision-Based%0A%20%20Autonomous%20Flight%20in%20Unknown%20Environments&body=Title%3A%20Learning%20to%20Initialize%20Trajectory%20Optimization%20for%20Vision-Based%0A%20%20Autonomous%20Flight%20in%20Unknown%20Environments%0AAuthor%3A%20Yicheng%20Chen%20and%20Jinjie%20Li%20and%20Wenyuan%20Qin%20and%20Yongzhao%20Hua%20and%20Xiwang%20Dong%20and%20Qingdong%20Li%0AAbstract%3A%20%20%20Autonomous%20flight%20in%20unknown%20environments%20requires%20precise%20spatial%20and%0Atemporal%20trajectory%20planning%2C%20often%20involving%20computationally%20expensive%0Anonconvex%20optimization%20prone%20to%20local%20optima.%20To%20overcome%20these%20challenges%2C%20we%0Apresent%20the%20Neural-Enhanced%20Trajectory%20Planner%20%28NEO-Planner%29%2C%20a%20novel%20approach%0Athat%20leverages%20a%20Neural%20Network%20%28NN%29%20Planner%20to%20provide%20informed%20initial%20values%0Afor%20trajectory%20optimization.%20The%20NN-Planner%20is%20trained%20on%20a%20dataset%20generated%0Aby%20an%20expert%20planner%20using%20batch%20sampling%2C%20capturing%20multimodal%20trajectory%0Asolutions.%20It%20learns%20to%20predict%20spatial%20and%20temporal%20parameters%20for%0Atrajectories%20directly%20from%20raw%20sensor%20observations.%20NEO-Planner%20starts%0Aoptimization%20from%20these%20predictions%2C%20accelerating%20computation%20speed%20while%0Amaintaining%20explainability.%20Furthermore%2C%20we%20introduce%20a%20robust%20online%0Areplanning%20framework%20that%20accommodates%20planning%20latency%20for%20smooth%20trajectory%0Atracking.%20Extensive%20simulations%20demonstrate%20that%20NEO-Planner%20reduces%0Aoptimization%20iterations%20by%2020%25%2C%20leading%20to%20a%2026%25%20decrease%20in%20computation%20time%0Acompared%20with%20pure%20optimization-based%20methods.%20It%20maintains%20trajectory%20quality%0Acomparable%20to%20baseline%20approaches%20and%20generalizes%20well%20to%20unseen%20environments.%0AReal-world%20experiments%20validate%20its%20effectiveness%20for%20autonomous%20drone%0Anavigation%20in%20cluttered%2C%20unknown%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10683v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Initialize%2520Trajectory%2520Optimization%2520for%2520Vision-Based%250A%2520%2520Autonomous%2520Flight%2520in%2520Unknown%2520Environments%26entry.906535625%3DYicheng%2520Chen%2520and%2520Jinjie%2520Li%2520and%2520Wenyuan%2520Qin%2520and%2520Yongzhao%2520Hua%2520and%2520Xiwang%2520Dong%2520and%2520Qingdong%2520Li%26entry.1292438233%3D%2520%2520Autonomous%2520flight%2520in%2520unknown%2520environments%2520requires%2520precise%2520spatial%2520and%250Atemporal%2520trajectory%2520planning%252C%2520often%2520involving%2520computationally%2520expensive%250Anonconvex%2520optimization%2520prone%2520to%2520local%2520optima.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apresent%2520the%2520Neural-Enhanced%2520Trajectory%2520Planner%2520%2528NEO-Planner%2529%252C%2520a%2520novel%2520approach%250Athat%2520leverages%2520a%2520Neural%2520Network%2520%2528NN%2529%2520Planner%2520to%2520provide%2520informed%2520initial%2520values%250Afor%2520trajectory%2520optimization.%2520The%2520NN-Planner%2520is%2520trained%2520on%2520a%2520dataset%2520generated%250Aby%2520an%2520expert%2520planner%2520using%2520batch%2520sampling%252C%2520capturing%2520multimodal%2520trajectory%250Asolutions.%2520It%2520learns%2520to%2520predict%2520spatial%2520and%2520temporal%2520parameters%2520for%250Atrajectories%2520directly%2520from%2520raw%2520sensor%2520observations.%2520NEO-Planner%2520starts%250Aoptimization%2520from%2520these%2520predictions%252C%2520accelerating%2520computation%2520speed%2520while%250Amaintaining%2520explainability.%2520Furthermore%252C%2520we%2520introduce%2520a%2520robust%2520online%250Areplanning%2520framework%2520that%2520accommodates%2520planning%2520latency%2520for%2520smooth%2520trajectory%250Atracking.%2520Extensive%2520simulations%2520demonstrate%2520that%2520NEO-Planner%2520reduces%250Aoptimization%2520iterations%2520by%252020%2525%252C%2520leading%2520to%2520a%252026%2525%2520decrease%2520in%2520computation%2520time%250Acompared%2520with%2520pure%2520optimization-based%2520methods.%2520It%2520maintains%2520trajectory%2520quality%250Acomparable%2520to%2520baseline%2520approaches%2520and%2520generalizes%2520well%2520to%2520unseen%2520environments.%250AReal-world%2520experiments%2520validate%2520its%2520effectiveness%2520for%2520autonomous%2520drone%250Anavigation%2520in%2520cluttered%252C%2520unknown%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10683v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Initialize%20Trajectory%20Optimization%20for%20Vision-Based%0A%20%20Autonomous%20Flight%20in%20Unknown%20Environments&entry.906535625=Yicheng%20Chen%20and%20Jinjie%20Li%20and%20Wenyuan%20Qin%20and%20Yongzhao%20Hua%20and%20Xiwang%20Dong%20and%20Qingdong%20Li&entry.1292438233=%20%20Autonomous%20flight%20in%20unknown%20environments%20requires%20precise%20spatial%20and%0Atemporal%20trajectory%20planning%2C%20often%20involving%20computationally%20expensive%0Anonconvex%20optimization%20prone%20to%20local%20optima.%20To%20overcome%20these%20challenges%2C%20we%0Apresent%20the%20Neural-Enhanced%20Trajectory%20Planner%20%28NEO-Planner%29%2C%20a%20novel%20approach%0Athat%20leverages%20a%20Neural%20Network%20%28NN%29%20Planner%20to%20provide%20informed%20initial%20values%0Afor%20trajectory%20optimization.%20The%20NN-Planner%20is%20trained%20on%20a%20dataset%20generated%0Aby%20an%20expert%20planner%20using%20batch%20sampling%2C%20capturing%20multimodal%20trajectory%0Asolutions.%20It%20learns%20to%20predict%20spatial%20and%20temporal%20parameters%20for%0Atrajectories%20directly%20from%20raw%20sensor%20observations.%20NEO-Planner%20starts%0Aoptimization%20from%20these%20predictions%2C%20accelerating%20computation%20speed%20while%0Amaintaining%20explainability.%20Furthermore%2C%20we%20introduce%20a%20robust%20online%0Areplanning%20framework%20that%20accommodates%20planning%20latency%20for%20smooth%20trajectory%0Atracking.%20Extensive%20simulations%20demonstrate%20that%20NEO-Planner%20reduces%0Aoptimization%20iterations%20by%2020%25%2C%20leading%20to%20a%2026%25%20decrease%20in%20computation%20time%0Acompared%20with%20pure%20optimization-based%20methods.%20It%20maintains%20trajectory%20quality%0Acomparable%20to%20baseline%20approaches%20and%20generalizes%20well%20to%20unseen%20environments.%0AReal-world%20experiments%20validate%20its%20effectiveness%20for%20autonomous%20drone%0Anavigation%20in%20cluttered%2C%20unknown%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10683v2&entry.124074799=Read"},
{"title": "CDI: Blind Image Restoration Fidelity Evaluation based on Consistency\n  with Degraded Image", "author": "Xiaojun Tang and Jingru Wang and Guangwei Huang and Guannan Chen and Rui Zheng and Lian Huai and Yuyu Liu and Xingqun Jiang", "abstract": "  Recent advancements in Blind Image Restoration (BIR) methods, based on\nGenerative Adversarial Networks and Diffusion Models, have significantly\nimproved visual quality. However, they present significant challenges for Image\nQuality Assessment (IQA), as the existing Full-Reference IQA methods often rate\nimages with high perceptual quality poorly. In this paper, we reassess the\nSolution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and\npropose constructing a specific BIR IQA system. In stead of directly comparing\na restored image with a reference image, the BIR IQA evaluates fidelity by\ncalculating the Consistency with Degraded Image (CDI). Specifically, we propose\na wavelet domain Reference Guided CDI algorithm, which can acquire the\nconsistency with a degraded image for various types without requiring knowledge\nof degradation parameters. The supported degradation types include down\nsampling, blur, noise, JPEG and complex combined degradations etc. In addition,\nwe propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without\nreference images. Finally, in order to validate the rationality of CDI, we\ncreate a new Degraded Images Switch Display Comparison Dataset (DISDCD) for\nsubjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify\nthat CDI is markedly superior to common Full Reference IQA methods for BIR\nfidelity evaluation. The source code and the DISDCD dataset will be publicly\navailable shortly.\n", "link": "http://arxiv.org/abs/2501.14264v2", "date": "2025-08-08", "relevancy": 2.2163, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5811}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDI%3A%20Blind%20Image%20Restoration%20Fidelity%20Evaluation%20based%20on%20Consistency%0A%20%20with%20Degraded%20Image&body=Title%3A%20CDI%3A%20Blind%20Image%20Restoration%20Fidelity%20Evaluation%20based%20on%20Consistency%0A%20%20with%20Degraded%20Image%0AAuthor%3A%20Xiaojun%20Tang%20and%20Jingru%20Wang%20and%20Guangwei%20Huang%20and%20Guannan%20Chen%20and%20Rui%20Zheng%20and%20Lian%20Huai%20and%20Yuyu%20Liu%20and%20Xingqun%20Jiang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Blind%20Image%20Restoration%20%28BIR%29%20methods%2C%20based%20on%0AGenerative%20Adversarial%20Networks%20and%20Diffusion%20Models%2C%20have%20significantly%0Aimproved%20visual%20quality.%20However%2C%20they%20present%20significant%20challenges%20for%20Image%0AQuality%20Assessment%20%28IQA%29%2C%20as%20the%20existing%20Full-Reference%20IQA%20methods%20often%20rate%0Aimages%20with%20high%20perceptual%20quality%20poorly.%20In%20this%20paper%2C%20we%20reassess%20the%0ASolution%20Non-Uniqueness%20and%20Degradation%20Indeterminacy%20issues%20of%20BIR%2C%20and%0Apropose%20constructing%20a%20specific%20BIR%20IQA%20system.%20In%20stead%20of%20directly%20comparing%0Aa%20restored%20image%20with%20a%20reference%20image%2C%20the%20BIR%20IQA%20evaluates%20fidelity%20by%0Acalculating%20the%20Consistency%20with%20Degraded%20Image%20%28CDI%29.%20Specifically%2C%20we%20propose%0Aa%20wavelet%20domain%20Reference%20Guided%20CDI%20algorithm%2C%20which%20can%20acquire%20the%0Aconsistency%20with%20a%20degraded%20image%20for%20various%20types%20without%20requiring%20knowledge%0Aof%20degradation%20parameters.%20The%20supported%20degradation%20types%20include%20down%0Asampling%2C%20blur%2C%20noise%2C%20JPEG%20and%20complex%20combined%20degradations%20etc.%20In%20addition%2C%0Awe%20propose%20a%20Reference%20Agnostic%20CDI%2C%20enabling%20BIR%20fidelity%20evaluation%20without%0Areference%20images.%20Finally%2C%20in%20order%20to%20validate%20the%20rationality%20of%20CDI%2C%20we%0Acreate%20a%20new%20Degraded%20Images%20Switch%20Display%20Comparison%20Dataset%20%28DISDCD%29%20for%0Asubjective%20evaluation%20of%20BIR%20fidelity.%20Experiments%20conducted%20on%20DISDCD%20verify%0Athat%20CDI%20is%20markedly%20superior%20to%20common%20Full%20Reference%20IQA%20methods%20for%20BIR%0Afidelity%20evaluation.%20The%20source%20code%20and%20the%20DISDCD%20dataset%20will%20be%20publicly%0Aavailable%20shortly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDI%253A%2520Blind%2520Image%2520Restoration%2520Fidelity%2520Evaluation%2520based%2520on%2520Consistency%250A%2520%2520with%2520Degraded%2520Image%26entry.906535625%3DXiaojun%2520Tang%2520and%2520Jingru%2520Wang%2520and%2520Guangwei%2520Huang%2520and%2520Guannan%2520Chen%2520and%2520Rui%2520Zheng%2520and%2520Lian%2520Huai%2520and%2520Yuyu%2520Liu%2520and%2520Xingqun%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Blind%2520Image%2520Restoration%2520%2528BIR%2529%2520methods%252C%2520based%2520on%250AGenerative%2520Adversarial%2520Networks%2520and%2520Diffusion%2520Models%252C%2520have%2520significantly%250Aimproved%2520visual%2520quality.%2520However%252C%2520they%2520present%2520significant%2520challenges%2520for%2520Image%250AQuality%2520Assessment%2520%2528IQA%2529%252C%2520as%2520the%2520existing%2520Full-Reference%2520IQA%2520methods%2520often%2520rate%250Aimages%2520with%2520high%2520perceptual%2520quality%2520poorly.%2520In%2520this%2520paper%252C%2520we%2520reassess%2520the%250ASolution%2520Non-Uniqueness%2520and%2520Degradation%2520Indeterminacy%2520issues%2520of%2520BIR%252C%2520and%250Apropose%2520constructing%2520a%2520specific%2520BIR%2520IQA%2520system.%2520In%2520stead%2520of%2520directly%2520comparing%250Aa%2520restored%2520image%2520with%2520a%2520reference%2520image%252C%2520the%2520BIR%2520IQA%2520evaluates%2520fidelity%2520by%250Acalculating%2520the%2520Consistency%2520with%2520Degraded%2520Image%2520%2528CDI%2529.%2520Specifically%252C%2520we%2520propose%250Aa%2520wavelet%2520domain%2520Reference%2520Guided%2520CDI%2520algorithm%252C%2520which%2520can%2520acquire%2520the%250Aconsistency%2520with%2520a%2520degraded%2520image%2520for%2520various%2520types%2520without%2520requiring%2520knowledge%250Aof%2520degradation%2520parameters.%2520The%2520supported%2520degradation%2520types%2520include%2520down%250Asampling%252C%2520blur%252C%2520noise%252C%2520JPEG%2520and%2520complex%2520combined%2520degradations%2520etc.%2520In%2520addition%252C%250Awe%2520propose%2520a%2520Reference%2520Agnostic%2520CDI%252C%2520enabling%2520BIR%2520fidelity%2520evaluation%2520without%250Areference%2520images.%2520Finally%252C%2520in%2520order%2520to%2520validate%2520the%2520rationality%2520of%2520CDI%252C%2520we%250Acreate%2520a%2520new%2520Degraded%2520Images%2520Switch%2520Display%2520Comparison%2520Dataset%2520%2528DISDCD%2529%2520for%250Asubjective%2520evaluation%2520of%2520BIR%2520fidelity.%2520Experiments%2520conducted%2520on%2520DISDCD%2520verify%250Athat%2520CDI%2520is%2520markedly%2520superior%2520to%2520common%2520Full%2520Reference%2520IQA%2520methods%2520for%2520BIR%250Afidelity%2520evaluation.%2520The%2520source%2520code%2520and%2520the%2520DISDCD%2520dataset%2520will%2520be%2520publicly%250Aavailable%2520shortly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDI%3A%20Blind%20Image%20Restoration%20Fidelity%20Evaluation%20based%20on%20Consistency%0A%20%20with%20Degraded%20Image&entry.906535625=Xiaojun%20Tang%20and%20Jingru%20Wang%20and%20Guangwei%20Huang%20and%20Guannan%20Chen%20and%20Rui%20Zheng%20and%20Lian%20Huai%20and%20Yuyu%20Liu%20and%20Xingqun%20Jiang&entry.1292438233=%20%20Recent%20advancements%20in%20Blind%20Image%20Restoration%20%28BIR%29%20methods%2C%20based%20on%0AGenerative%20Adversarial%20Networks%20and%20Diffusion%20Models%2C%20have%20significantly%0Aimproved%20visual%20quality.%20However%2C%20they%20present%20significant%20challenges%20for%20Image%0AQuality%20Assessment%20%28IQA%29%2C%20as%20the%20existing%20Full-Reference%20IQA%20methods%20often%20rate%0Aimages%20with%20high%20perceptual%20quality%20poorly.%20In%20this%20paper%2C%20we%20reassess%20the%0ASolution%20Non-Uniqueness%20and%20Degradation%20Indeterminacy%20issues%20of%20BIR%2C%20and%0Apropose%20constructing%20a%20specific%20BIR%20IQA%20system.%20In%20stead%20of%20directly%20comparing%0Aa%20restored%20image%20with%20a%20reference%20image%2C%20the%20BIR%20IQA%20evaluates%20fidelity%20by%0Acalculating%20the%20Consistency%20with%20Degraded%20Image%20%28CDI%29.%20Specifically%2C%20we%20propose%0Aa%20wavelet%20domain%20Reference%20Guided%20CDI%20algorithm%2C%20which%20can%20acquire%20the%0Aconsistency%20with%20a%20degraded%20image%20for%20various%20types%20without%20requiring%20knowledge%0Aof%20degradation%20parameters.%20The%20supported%20degradation%20types%20include%20down%0Asampling%2C%20blur%2C%20noise%2C%20JPEG%20and%20complex%20combined%20degradations%20etc.%20In%20addition%2C%0Awe%20propose%20a%20Reference%20Agnostic%20CDI%2C%20enabling%20BIR%20fidelity%20evaluation%20without%0Areference%20images.%20Finally%2C%20in%20order%20to%20validate%20the%20rationality%20of%20CDI%2C%20we%0Acreate%20a%20new%20Degraded%20Images%20Switch%20Display%20Comparison%20Dataset%20%28DISDCD%29%20for%0Asubjective%20evaluation%20of%20BIR%20fidelity.%20Experiments%20conducted%20on%20DISDCD%20verify%0Athat%20CDI%20is%20markedly%20superior%20to%20common%20Full%20Reference%20IQA%20methods%20for%20BIR%0Afidelity%20evaluation.%20The%20source%20code%20and%20the%20DISDCD%20dataset%20will%20be%20publicly%0Aavailable%20shortly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14264v2&entry.124074799=Read"},
{"title": "Computer Vision-based Adaptive Control for Back Exoskeleton Performance\n  Optimization", "author": "Andrea Dal Prete and Seyram Ofori and Chan Yon Sin and Ashwin Narayan and Francesco Braghin and Marta Gandolla and Haoyong Yu", "abstract": "  Back exoskeletons can reduce musculoskeletal strain, but their effectiveness\ndepends on support modulation and adaptive control. This study addresses two\nchallenges: defining optimal support strategies and developing adaptive control\nbased on payload estimation. We introduce an optimization space based on muscle\nactivity reduction, perceived discomfort, and user preference, constructing\nfunctions to identify optimal strategies. Experiments with 12 subjects revealed\noptimal operating regions, highlighting the need for dynamic modulation. Based\non these insights, we developed a vision-based adaptive control pipeline that\nestimates payloads in real-time by enhancing exoskeleton contextual\nunderstanding, minimising latency and enabling support adaptation within the\ndefined optimisation space. Validation with 12 more subjects showed over 80%\naccuracy and improvements across all metrics. Compared to static control,\nadaptive modulation reduced peak back muscle activation by up to 23% while\npreserving user preference and minimising discomfort. These findings validate\nthe proposed framework and highlight the potential of intelligent,\ncontext-aware control in industrial exoskeletons.\n", "link": "http://arxiv.org/abs/2508.06207v1", "date": "2025-08-08", "relevancy": 2.2138, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6001}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer%20Vision-based%20Adaptive%20Control%20for%20Back%20Exoskeleton%20Performance%0A%20%20Optimization&body=Title%3A%20Computer%20Vision-based%20Adaptive%20Control%20for%20Back%20Exoskeleton%20Performance%0A%20%20Optimization%0AAuthor%3A%20Andrea%20Dal%20Prete%20and%20Seyram%20Ofori%20and%20Chan%20Yon%20Sin%20and%20Ashwin%20Narayan%20and%20Francesco%20Braghin%20and%20Marta%20Gandolla%20and%20Haoyong%20Yu%0AAbstract%3A%20%20%20Back%20exoskeletons%20can%20reduce%20musculoskeletal%20strain%2C%20but%20their%20effectiveness%0Adepends%20on%20support%20modulation%20and%20adaptive%20control.%20This%20study%20addresses%20two%0Achallenges%3A%20defining%20optimal%20support%20strategies%20and%20developing%20adaptive%20control%0Abased%20on%20payload%20estimation.%20We%20introduce%20an%20optimization%20space%20based%20on%20muscle%0Aactivity%20reduction%2C%20perceived%20discomfort%2C%20and%20user%20preference%2C%20constructing%0Afunctions%20to%20identify%20optimal%20strategies.%20Experiments%20with%2012%20subjects%20revealed%0Aoptimal%20operating%20regions%2C%20highlighting%20the%20need%20for%20dynamic%20modulation.%20Based%0Aon%20these%20insights%2C%20we%20developed%20a%20vision-based%20adaptive%20control%20pipeline%20that%0Aestimates%20payloads%20in%20real-time%20by%20enhancing%20exoskeleton%20contextual%0Aunderstanding%2C%20minimising%20latency%20and%20enabling%20support%20adaptation%20within%20the%0Adefined%20optimisation%20space.%20Validation%20with%2012%20more%20subjects%20showed%20over%2080%25%0Aaccuracy%20and%20improvements%20across%20all%20metrics.%20Compared%20to%20static%20control%2C%0Aadaptive%20modulation%20reduced%20peak%20back%20muscle%20activation%20by%20up%20to%2023%25%20while%0Apreserving%20user%20preference%20and%20minimising%20discomfort.%20These%20findings%20validate%0Athe%20proposed%20framework%20and%20highlight%20the%20potential%20of%20intelligent%2C%0Acontext-aware%20control%20in%20industrial%20exoskeletons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer%2520Vision-based%2520Adaptive%2520Control%2520for%2520Back%2520Exoskeleton%2520Performance%250A%2520%2520Optimization%26entry.906535625%3DAndrea%2520Dal%2520Prete%2520and%2520Seyram%2520Ofori%2520and%2520Chan%2520Yon%2520Sin%2520and%2520Ashwin%2520Narayan%2520and%2520Francesco%2520Braghin%2520and%2520Marta%2520Gandolla%2520and%2520Haoyong%2520Yu%26entry.1292438233%3D%2520%2520Back%2520exoskeletons%2520can%2520reduce%2520musculoskeletal%2520strain%252C%2520but%2520their%2520effectiveness%250Adepends%2520on%2520support%2520modulation%2520and%2520adaptive%2520control.%2520This%2520study%2520addresses%2520two%250Achallenges%253A%2520defining%2520optimal%2520support%2520strategies%2520and%2520developing%2520adaptive%2520control%250Abased%2520on%2520payload%2520estimation.%2520We%2520introduce%2520an%2520optimization%2520space%2520based%2520on%2520muscle%250Aactivity%2520reduction%252C%2520perceived%2520discomfort%252C%2520and%2520user%2520preference%252C%2520constructing%250Afunctions%2520to%2520identify%2520optimal%2520strategies.%2520Experiments%2520with%252012%2520subjects%2520revealed%250Aoptimal%2520operating%2520regions%252C%2520highlighting%2520the%2520need%2520for%2520dynamic%2520modulation.%2520Based%250Aon%2520these%2520insights%252C%2520we%2520developed%2520a%2520vision-based%2520adaptive%2520control%2520pipeline%2520that%250Aestimates%2520payloads%2520in%2520real-time%2520by%2520enhancing%2520exoskeleton%2520contextual%250Aunderstanding%252C%2520minimising%2520latency%2520and%2520enabling%2520support%2520adaptation%2520within%2520the%250Adefined%2520optimisation%2520space.%2520Validation%2520with%252012%2520more%2520subjects%2520showed%2520over%252080%2525%250Aaccuracy%2520and%2520improvements%2520across%2520all%2520metrics.%2520Compared%2520to%2520static%2520control%252C%250Aadaptive%2520modulation%2520reduced%2520peak%2520back%2520muscle%2520activation%2520by%2520up%2520to%252023%2525%2520while%250Apreserving%2520user%2520preference%2520and%2520minimising%2520discomfort.%2520These%2520findings%2520validate%250Athe%2520proposed%2520framework%2520and%2520highlight%2520the%2520potential%2520of%2520intelligent%252C%250Acontext-aware%2520control%2520in%2520industrial%2520exoskeletons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20Vision-based%20Adaptive%20Control%20for%20Back%20Exoskeleton%20Performance%0A%20%20Optimization&entry.906535625=Andrea%20Dal%20Prete%20and%20Seyram%20Ofori%20and%20Chan%20Yon%20Sin%20and%20Ashwin%20Narayan%20and%20Francesco%20Braghin%20and%20Marta%20Gandolla%20and%20Haoyong%20Yu&entry.1292438233=%20%20Back%20exoskeletons%20can%20reduce%20musculoskeletal%20strain%2C%20but%20their%20effectiveness%0Adepends%20on%20support%20modulation%20and%20adaptive%20control.%20This%20study%20addresses%20two%0Achallenges%3A%20defining%20optimal%20support%20strategies%20and%20developing%20adaptive%20control%0Abased%20on%20payload%20estimation.%20We%20introduce%20an%20optimization%20space%20based%20on%20muscle%0Aactivity%20reduction%2C%20perceived%20discomfort%2C%20and%20user%20preference%2C%20constructing%0Afunctions%20to%20identify%20optimal%20strategies.%20Experiments%20with%2012%20subjects%20revealed%0Aoptimal%20operating%20regions%2C%20highlighting%20the%20need%20for%20dynamic%20modulation.%20Based%0Aon%20these%20insights%2C%20we%20developed%20a%20vision-based%20adaptive%20control%20pipeline%20that%0Aestimates%20payloads%20in%20real-time%20by%20enhancing%20exoskeleton%20contextual%0Aunderstanding%2C%20minimising%20latency%20and%20enabling%20support%20adaptation%20within%20the%0Adefined%20optimisation%20space.%20Validation%20with%2012%20more%20subjects%20showed%20over%2080%25%0Aaccuracy%20and%20improvements%20across%20all%20metrics.%20Compared%20to%20static%20control%2C%0Aadaptive%20modulation%20reduced%20peak%20back%20muscle%20activation%20by%20up%20to%2023%25%20while%0Apreserving%20user%20preference%20and%20minimising%20discomfort.%20These%20findings%20validate%0Athe%20proposed%20framework%20and%20highlight%20the%20potential%20of%20intelligent%2C%0Acontext-aware%20control%20in%20industrial%20exoskeletons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06207v1&entry.124074799=Read"},
{"title": "DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online\n  Nodes", "author": "Mogens Henrik From and Jacob Nielsen and Lukas Galke and Peter Schneider-Kamp", "abstract": "  Training large neural network models requires extensive computational\nresources, often distributed across several nodes and accelerators. Recent\nfindings suggest that it may be sufficient to only exchange the fast moving\ncomponents of the gradients, while accumulating momentum locally (Decoupled\nMomentum, or DeMo). However, DeMo assumes that models fit on a single\naccelerator. We relax this assumption and introduce FlexDeMo, whereby nodes\nfully shard model parameters locally between different accelerators, while\ninter-node communication is reduced by synchronizing only fast-moving\ncomponents instead of the full gradients -- resulting in a hybrid sharded data\nparallel training strategy. We further introduce a framework, denoted as\nDeToNATION, that generalizes DeMo, FlexDeMo, and other popular distributed\ntraining schemes such as DiLoCo -- introducing new variations of replication\nschemes and challenging choices made in DeMo. Our results across language and\nvision domains show that FlexDeMo attains similar validation loss as hybrid\nsharded data parallel training employing AdamW and full gradient\nsynchronization, while being substantially faster. FlexDeMo is thus a promising\ndistributed training scheme for the largest machine learning models.\n", "link": "http://arxiv.org/abs/2502.06728v3", "date": "2025-08-08", "relevancy": 2.195, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.579}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeToNATION%3A%20Decoupled%20Torch%20Network-Aware%20Training%20on%20Interlinked%20Online%0A%20%20Nodes&body=Title%3A%20DeToNATION%3A%20Decoupled%20Torch%20Network-Aware%20Training%20on%20Interlinked%20Online%0A%20%20Nodes%0AAuthor%3A%20Mogens%20Henrik%20From%20and%20Jacob%20Nielsen%20and%20Lukas%20Galke%20and%20Peter%20Schneider-Kamp%0AAbstract%3A%20%20%20Training%20large%20neural%20network%20models%20requires%20extensive%20computational%0Aresources%2C%20often%20distributed%20across%20several%20nodes%20and%20accelerators.%20Recent%0Afindings%20suggest%20that%20it%20may%20be%20sufficient%20to%20only%20exchange%20the%20fast%20moving%0Acomponents%20of%20the%20gradients%2C%20while%20accumulating%20momentum%20locally%20%28Decoupled%0AMomentum%2C%20or%20DeMo%29.%20However%2C%20DeMo%20assumes%20that%20models%20fit%20on%20a%20single%0Aaccelerator.%20We%20relax%20this%20assumption%20and%20introduce%20FlexDeMo%2C%20whereby%20nodes%0Afully%20shard%20model%20parameters%20locally%20between%20different%20accelerators%2C%20while%0Ainter-node%20communication%20is%20reduced%20by%20synchronizing%20only%20fast-moving%0Acomponents%20instead%20of%20the%20full%20gradients%20--%20resulting%20in%20a%20hybrid%20sharded%20data%0Aparallel%20training%20strategy.%20We%20further%20introduce%20a%20framework%2C%20denoted%20as%0ADeToNATION%2C%20that%20generalizes%20DeMo%2C%20FlexDeMo%2C%20and%20other%20popular%20distributed%0Atraining%20schemes%20such%20as%20DiLoCo%20--%20introducing%20new%20variations%20of%20replication%0Aschemes%20and%20challenging%20choices%20made%20in%20DeMo.%20Our%20results%20across%20language%20and%0Avision%20domains%20show%20that%20FlexDeMo%20attains%20similar%20validation%20loss%20as%20hybrid%0Asharded%20data%20parallel%20training%20employing%20AdamW%20and%20full%20gradient%0Asynchronization%2C%20while%20being%20substantially%20faster.%20FlexDeMo%20is%20thus%20a%20promising%0Adistributed%20training%20scheme%20for%20the%20largest%20machine%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06728v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeToNATION%253A%2520Decoupled%2520Torch%2520Network-Aware%2520Training%2520on%2520Interlinked%2520Online%250A%2520%2520Nodes%26entry.906535625%3DMogens%2520Henrik%2520From%2520and%2520Jacob%2520Nielsen%2520and%2520Lukas%2520Galke%2520and%2520Peter%2520Schneider-Kamp%26entry.1292438233%3D%2520%2520Training%2520large%2520neural%2520network%2520models%2520requires%2520extensive%2520computational%250Aresources%252C%2520often%2520distributed%2520across%2520several%2520nodes%2520and%2520accelerators.%2520Recent%250Afindings%2520suggest%2520that%2520it%2520may%2520be%2520sufficient%2520to%2520only%2520exchange%2520the%2520fast%2520moving%250Acomponents%2520of%2520the%2520gradients%252C%2520while%2520accumulating%2520momentum%2520locally%2520%2528Decoupled%250AMomentum%252C%2520or%2520DeMo%2529.%2520However%252C%2520DeMo%2520assumes%2520that%2520models%2520fit%2520on%2520a%2520single%250Aaccelerator.%2520We%2520relax%2520this%2520assumption%2520and%2520introduce%2520FlexDeMo%252C%2520whereby%2520nodes%250Afully%2520shard%2520model%2520parameters%2520locally%2520between%2520different%2520accelerators%252C%2520while%250Ainter-node%2520communication%2520is%2520reduced%2520by%2520synchronizing%2520only%2520fast-moving%250Acomponents%2520instead%2520of%2520the%2520full%2520gradients%2520--%2520resulting%2520in%2520a%2520hybrid%2520sharded%2520data%250Aparallel%2520training%2520strategy.%2520We%2520further%2520introduce%2520a%2520framework%252C%2520denoted%2520as%250ADeToNATION%252C%2520that%2520generalizes%2520DeMo%252C%2520FlexDeMo%252C%2520and%2520other%2520popular%2520distributed%250Atraining%2520schemes%2520such%2520as%2520DiLoCo%2520--%2520introducing%2520new%2520variations%2520of%2520replication%250Aschemes%2520and%2520challenging%2520choices%2520made%2520in%2520DeMo.%2520Our%2520results%2520across%2520language%2520and%250Avision%2520domains%2520show%2520that%2520FlexDeMo%2520attains%2520similar%2520validation%2520loss%2520as%2520hybrid%250Asharded%2520data%2520parallel%2520training%2520employing%2520AdamW%2520and%2520full%2520gradient%250Asynchronization%252C%2520while%2520being%2520substantially%2520faster.%2520FlexDeMo%2520is%2520thus%2520a%2520promising%250Adistributed%2520training%2520scheme%2520for%2520the%2520largest%2520machine%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06728v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeToNATION%3A%20Decoupled%20Torch%20Network-Aware%20Training%20on%20Interlinked%20Online%0A%20%20Nodes&entry.906535625=Mogens%20Henrik%20From%20and%20Jacob%20Nielsen%20and%20Lukas%20Galke%20and%20Peter%20Schneider-Kamp&entry.1292438233=%20%20Training%20large%20neural%20network%20models%20requires%20extensive%20computational%0Aresources%2C%20often%20distributed%20across%20several%20nodes%20and%20accelerators.%20Recent%0Afindings%20suggest%20that%20it%20may%20be%20sufficient%20to%20only%20exchange%20the%20fast%20moving%0Acomponents%20of%20the%20gradients%2C%20while%20accumulating%20momentum%20locally%20%28Decoupled%0AMomentum%2C%20or%20DeMo%29.%20However%2C%20DeMo%20assumes%20that%20models%20fit%20on%20a%20single%0Aaccelerator.%20We%20relax%20this%20assumption%20and%20introduce%20FlexDeMo%2C%20whereby%20nodes%0Afully%20shard%20model%20parameters%20locally%20between%20different%20accelerators%2C%20while%0Ainter-node%20communication%20is%20reduced%20by%20synchronizing%20only%20fast-moving%0Acomponents%20instead%20of%20the%20full%20gradients%20--%20resulting%20in%20a%20hybrid%20sharded%20data%0Aparallel%20training%20strategy.%20We%20further%20introduce%20a%20framework%2C%20denoted%20as%0ADeToNATION%2C%20that%20generalizes%20DeMo%2C%20FlexDeMo%2C%20and%20other%20popular%20distributed%0Atraining%20schemes%20such%20as%20DiLoCo%20--%20introducing%20new%20variations%20of%20replication%0Aschemes%20and%20challenging%20choices%20made%20in%20DeMo.%20Our%20results%20across%20language%20and%0Avision%20domains%20show%20that%20FlexDeMo%20attains%20similar%20validation%20loss%20as%20hybrid%0Asharded%20data%20parallel%20training%20employing%20AdamW%20and%20full%20gradient%0Asynchronization%2C%20while%20being%20substantially%20faster.%20FlexDeMo%20is%20thus%20a%20promising%0Adistributed%20training%20scheme%20for%20the%20largest%20machine%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06728v3&entry.124074799=Read"},
{"title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "author": "Saad Ejaz and Marco Giberna and Muhammad Shaheer and Jose Andres Millan-Romera and Ali Tourani and Paul Kremer and Holger Voos and Jose Luis Sanchez-Lopez", "abstract": "  3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.\n", "link": "http://arxiv.org/abs/2508.06283v1", "date": "2025-08-08", "relevancy": 2.1812, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Situationally-aware%20Path%20Planning%20Exploiting%203D%20Scene%20Graphs&body=Title%3A%20Situationally-aware%20Path%20Planning%20Exploiting%203D%20Scene%20Graphs%0AAuthor%3A%20Saad%20Ejaz%20and%20Marco%20Giberna%20and%20Muhammad%20Shaheer%20and%20Jose%20Andres%20Millan-Romera%20and%20Ali%20Tourani%20and%20Paul%20Kremer%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez%0AAbstract%3A%20%20%203D%20Scene%20Graphs%20integrate%20both%20metric%20and%20semantic%20information%2C%20yet%20their%0Astructure%20remains%20underutilized%20for%20improving%20path%20planning%20efficiency%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20present%20S-Path%2C%20a%20situationally-aware%20path%0Aplanner%20that%20leverages%20the%20metric-semantic%20structure%20of%20indoor%203D%20Scene%20Graphs%0Ato%20significantly%20enhance%20planning%20efficiency.%20S-Path%20follows%20a%20two-stage%0Aprocess%3A%20it%20first%20performs%20a%20search%20over%20a%20semantic%20graph%20derived%20from%20the%0Ascene%20graph%20to%20yield%20a%20human-understandable%20high-level%20path.%20This%20also%0Aidentifies%20relevant%20regions%20for%20planning%2C%20which%20later%20allows%20the%20decomposition%0Aof%20the%20problem%20into%20smaller%2C%20independent%20subproblems%20that%20can%20be%20solved%20in%0Aparallel.%20We%20also%20introduce%20a%20replanning%20mechanism%20that%2C%20in%20the%20event%20of%20an%0Ainfeasible%20path%2C%20reuses%20information%20from%20previously%20solved%20subproblems%20to%0Aupdate%20semantic%20heuristics%20and%20prioritize%20reuse%20to%20further%20improve%20the%0Aefficiency%20of%20future%20planning%20attempts.%20Extensive%20experiments%20on%20both%0Areal-world%20and%20simulated%20environments%20show%20that%20S-Path%20achieves%20average%0Areductions%20of%205.7x%20in%20planning%20time%20while%20maintaining%20comparable%20path%0Aoptimality%20to%20classical%20sampling-based%20planners%20and%20surpassing%20them%20in%20complex%0Ascenarios%2C%20making%20it%20an%20efficient%20and%20interpretable%20path%20planner%20for%0Aenvironments%20represented%20by%20indoor%203D%20Scene%20Graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSituationally-aware%2520Path%2520Planning%2520Exploiting%25203D%2520Scene%2520Graphs%26entry.906535625%3DSaad%2520Ejaz%2520and%2520Marco%2520Giberna%2520and%2520Muhammad%2520Shaheer%2520and%2520Jose%2520Andres%2520Millan-Romera%2520and%2520Ali%2520Tourani%2520and%2520Paul%2520Kremer%2520and%2520Holger%2520Voos%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%26entry.1292438233%3D%2520%25203D%2520Scene%2520Graphs%2520integrate%2520both%2520metric%2520and%2520semantic%2520information%252C%2520yet%2520their%250Astructure%2520remains%2520underutilized%2520for%2520improving%2520path%2520planning%2520efficiency%2520and%250Ainterpretability.%2520In%2520this%2520work%252C%2520we%2520present%2520S-Path%252C%2520a%2520situationally-aware%2520path%250Aplanner%2520that%2520leverages%2520the%2520metric-semantic%2520structure%2520of%2520indoor%25203D%2520Scene%2520Graphs%250Ato%2520significantly%2520enhance%2520planning%2520efficiency.%2520S-Path%2520follows%2520a%2520two-stage%250Aprocess%253A%2520it%2520first%2520performs%2520a%2520search%2520over%2520a%2520semantic%2520graph%2520derived%2520from%2520the%250Ascene%2520graph%2520to%2520yield%2520a%2520human-understandable%2520high-level%2520path.%2520This%2520also%250Aidentifies%2520relevant%2520regions%2520for%2520planning%252C%2520which%2520later%2520allows%2520the%2520decomposition%250Aof%2520the%2520problem%2520into%2520smaller%252C%2520independent%2520subproblems%2520that%2520can%2520be%2520solved%2520in%250Aparallel.%2520We%2520also%2520introduce%2520a%2520replanning%2520mechanism%2520that%252C%2520in%2520the%2520event%2520of%2520an%250Ainfeasible%2520path%252C%2520reuses%2520information%2520from%2520previously%2520solved%2520subproblems%2520to%250Aupdate%2520semantic%2520heuristics%2520and%2520prioritize%2520reuse%2520to%2520further%2520improve%2520the%250Aefficiency%2520of%2520future%2520planning%2520attempts.%2520Extensive%2520experiments%2520on%2520both%250Areal-world%2520and%2520simulated%2520environments%2520show%2520that%2520S-Path%2520achieves%2520average%250Areductions%2520of%25205.7x%2520in%2520planning%2520time%2520while%2520maintaining%2520comparable%2520path%250Aoptimality%2520to%2520classical%2520sampling-based%2520planners%2520and%2520surpassing%2520them%2520in%2520complex%250Ascenarios%252C%2520making%2520it%2520an%2520efficient%2520and%2520interpretable%2520path%2520planner%2520for%250Aenvironments%2520represented%2520by%2520indoor%25203D%2520Scene%2520Graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Situationally-aware%20Path%20Planning%20Exploiting%203D%20Scene%20Graphs&entry.906535625=Saad%20Ejaz%20and%20Marco%20Giberna%20and%20Muhammad%20Shaheer%20and%20Jose%20Andres%20Millan-Romera%20and%20Ali%20Tourani%20and%20Paul%20Kremer%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%203D%20Scene%20Graphs%20integrate%20both%20metric%20and%20semantic%20information%2C%20yet%20their%0Astructure%20remains%20underutilized%20for%20improving%20path%20planning%20efficiency%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20present%20S-Path%2C%20a%20situationally-aware%20path%0Aplanner%20that%20leverages%20the%20metric-semantic%20structure%20of%20indoor%203D%20Scene%20Graphs%0Ato%20significantly%20enhance%20planning%20efficiency.%20S-Path%20follows%20a%20two-stage%0Aprocess%3A%20it%20first%20performs%20a%20search%20over%20a%20semantic%20graph%20derived%20from%20the%0Ascene%20graph%20to%20yield%20a%20human-understandable%20high-level%20path.%20This%20also%0Aidentifies%20relevant%20regions%20for%20planning%2C%20which%20later%20allows%20the%20decomposition%0Aof%20the%20problem%20into%20smaller%2C%20independent%20subproblems%20that%20can%20be%20solved%20in%0Aparallel.%20We%20also%20introduce%20a%20replanning%20mechanism%20that%2C%20in%20the%20event%20of%20an%0Ainfeasible%20path%2C%20reuses%20information%20from%20previously%20solved%20subproblems%20to%0Aupdate%20semantic%20heuristics%20and%20prioritize%20reuse%20to%20further%20improve%20the%0Aefficiency%20of%20future%20planning%20attempts.%20Extensive%20experiments%20on%20both%0Areal-world%20and%20simulated%20environments%20show%20that%20S-Path%20achieves%20average%0Areductions%20of%205.7x%20in%20planning%20time%20while%20maintaining%20comparable%20path%0Aoptimality%20to%20classical%20sampling-based%20planners%20and%20surpassing%20them%20in%20complex%0Ascenarios%2C%20making%20it%20an%20efficient%20and%20interpretable%20path%20planner%20for%0Aenvironments%20represented%20by%20indoor%203D%20Scene%20Graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06283v1&entry.124074799=Read"},
{"title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung\n  Nodule Malignancy Prediction", "author": "Luoting Zhuang and Seyed Mohammad Hossein Tabatabaei and Ramin Salehi-Rad and Linh M. Tran and Denise R. Aberle and Ashley E. Prosper and William Hsu", "abstract": "  Machine learning models have utilized semantic features, deep features, or\nboth to assess lung nodule malignancy. However, their reliance on manual\nannotation during inference, limited interpretability, and sensitivity to\nimaging variations hinder their application in real-world clinical settings.\nThus, this research aims to integrate semantic features derived from\nradiologists' assessments of nodules, guiding the model to learn clinically\nrelevant, robust, and explainable imaging features for predicting lung cancer.\nWe obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST)\nwith 1,246 nodules and semantic features. Additionally, the Lung Image Database\nConsortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for\nnodule characteristics. Three external datasets were obtained from UCLA Health,\nthe LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a\npretrained Contrastive Language-Image Pretraining (CLIP) model with a\nparameter-efficient fine-tuning approach to align imaging and semantic text\nfeatures and predict the one-year lung cancer diagnosis. Our model outperformed\nstate-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and\nAUPRC of 0.776. It also showed robust results in external datasets. Using CLIP,\nwe also obtained predictions on semantic features through zero-shot inference,\nsuch as nodule margin (AUROC: 0.812), nodule consistency (0.812), and pleural\nattachment (0.840). Our approach surpasses the SOTA models in predicting lung\ncancer across datasets collected from diverse clinical settings, providing\nexplainable outputs, aiding clinicians in comprehending the underlying meaning\nof model predictions. This approach also prevents the model from learning\nshortcuts and generalizes across clinical settings. The code is available at\nhttps://github.com/luotingzhuang/CLIP_nodule.\n", "link": "http://arxiv.org/abs/2504.21344v2", "date": "2025-08-08", "relevancy": 2.1684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Model-Based%20Semantic-Guided%20Imaging%20Biomarker%20for%20Lung%0A%20%20Nodule%20Malignancy%20Prediction&body=Title%3A%20Vision-Language%20Model-Based%20Semantic-Guided%20Imaging%20Biomarker%20for%20Lung%0A%20%20Nodule%20Malignancy%20Prediction%0AAuthor%3A%20Luoting%20Zhuang%20and%20Seyed%20Mohammad%20Hossein%20Tabatabaei%20and%20Ramin%20Salehi-Rad%20and%20Linh%20M.%20Tran%20and%20Denise%20R.%20Aberle%20and%20Ashley%20E.%20Prosper%20and%20William%20Hsu%0AAbstract%3A%20%20%20Machine%20learning%20models%20have%20utilized%20semantic%20features%2C%20deep%20features%2C%20or%0Aboth%20to%20assess%20lung%20nodule%20malignancy.%20However%2C%20their%20reliance%20on%20manual%0Aannotation%20during%20inference%2C%20limited%20interpretability%2C%20and%20sensitivity%20to%0Aimaging%20variations%20hinder%20their%20application%20in%20real-world%20clinical%20settings.%0AThus%2C%20this%20research%20aims%20to%20integrate%20semantic%20features%20derived%20from%0Aradiologists%27%20assessments%20of%20nodules%2C%20guiding%20the%20model%20to%20learn%20clinically%0Arelevant%2C%20robust%2C%20and%20explainable%20imaging%20features%20for%20predicting%20lung%20cancer.%0AWe%20obtained%20938%20low-dose%20CT%20scans%20from%20the%20National%20Lung%20Screening%20Trial%20%28NLST%29%0Awith%201%2C246%20nodules%20and%20semantic%20features.%20Additionally%2C%20the%20Lung%20Image%20Database%0AConsortium%20dataset%20contains%201%2C018%20CT%20scans%2C%20with%202%2C625%20lesions%20annotated%20for%0Anodule%20characteristics.%20Three%20external%20datasets%20were%20obtained%20from%20UCLA%20Health%2C%0Athe%20LUNGx%20Challenge%2C%20and%20the%20Duke%20Lung%20Cancer%20Screening.%20We%20fine-tuned%20a%0Apretrained%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20model%20with%20a%0Aparameter-efficient%20fine-tuning%20approach%20to%20align%20imaging%20and%20semantic%20text%0Afeatures%20and%20predict%20the%20one-year%20lung%20cancer%20diagnosis.%20Our%20model%20outperformed%0Astate-of-the-art%20%28SOTA%29%20models%20in%20the%20NLST%20test%20set%20with%20an%20AUROC%20of%200.901%20and%0AAUPRC%20of%200.776.%20It%20also%20showed%20robust%20results%20in%20external%20datasets.%20Using%20CLIP%2C%0Awe%20also%20obtained%20predictions%20on%20semantic%20features%20through%20zero-shot%20inference%2C%0Asuch%20as%20nodule%20margin%20%28AUROC%3A%200.812%29%2C%20nodule%20consistency%20%280.812%29%2C%20and%20pleural%0Aattachment%20%280.840%29.%20Our%20approach%20surpasses%20the%20SOTA%20models%20in%20predicting%20lung%0Acancer%20across%20datasets%20collected%20from%20diverse%20clinical%20settings%2C%20providing%0Aexplainable%20outputs%2C%20aiding%20clinicians%20in%20comprehending%20the%20underlying%20meaning%0Aof%20model%20predictions.%20This%20approach%20also%20prevents%20the%20model%20from%20learning%0Ashortcuts%20and%20generalizes%20across%20clinical%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/luotingzhuang/CLIP_nodule.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Model-Based%2520Semantic-Guided%2520Imaging%2520Biomarker%2520for%2520Lung%250A%2520%2520Nodule%2520Malignancy%2520Prediction%26entry.906535625%3DLuoting%2520Zhuang%2520and%2520Seyed%2520Mohammad%2520Hossein%2520Tabatabaei%2520and%2520Ramin%2520Salehi-Rad%2520and%2520Linh%2520M.%2520Tran%2520and%2520Denise%2520R.%2520Aberle%2520and%2520Ashley%2520E.%2520Prosper%2520and%2520William%2520Hsu%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520have%2520utilized%2520semantic%2520features%252C%2520deep%2520features%252C%2520or%250Aboth%2520to%2520assess%2520lung%2520nodule%2520malignancy.%2520However%252C%2520their%2520reliance%2520on%2520manual%250Aannotation%2520during%2520inference%252C%2520limited%2520interpretability%252C%2520and%2520sensitivity%2520to%250Aimaging%2520variations%2520hinder%2520their%2520application%2520in%2520real-world%2520clinical%2520settings.%250AThus%252C%2520this%2520research%2520aims%2520to%2520integrate%2520semantic%2520features%2520derived%2520from%250Aradiologists%2527%2520assessments%2520of%2520nodules%252C%2520guiding%2520the%2520model%2520to%2520learn%2520clinically%250Arelevant%252C%2520robust%252C%2520and%2520explainable%2520imaging%2520features%2520for%2520predicting%2520lung%2520cancer.%250AWe%2520obtained%2520938%2520low-dose%2520CT%2520scans%2520from%2520the%2520National%2520Lung%2520Screening%2520Trial%2520%2528NLST%2529%250Awith%25201%252C246%2520nodules%2520and%2520semantic%2520features.%2520Additionally%252C%2520the%2520Lung%2520Image%2520Database%250AConsortium%2520dataset%2520contains%25201%252C018%2520CT%2520scans%252C%2520with%25202%252C625%2520lesions%2520annotated%2520for%250Anodule%2520characteristics.%2520Three%2520external%2520datasets%2520were%2520obtained%2520from%2520UCLA%2520Health%252C%250Athe%2520LUNGx%2520Challenge%252C%2520and%2520the%2520Duke%2520Lung%2520Cancer%2520Screening.%2520We%2520fine-tuned%2520a%250Apretrained%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520model%2520with%2520a%250Aparameter-efficient%2520fine-tuning%2520approach%2520to%2520align%2520imaging%2520and%2520semantic%2520text%250Afeatures%2520and%2520predict%2520the%2520one-year%2520lung%2520cancer%2520diagnosis.%2520Our%2520model%2520outperformed%250Astate-of-the-art%2520%2528SOTA%2529%2520models%2520in%2520the%2520NLST%2520test%2520set%2520with%2520an%2520AUROC%2520of%25200.901%2520and%250AAUPRC%2520of%25200.776.%2520It%2520also%2520showed%2520robust%2520results%2520in%2520external%2520datasets.%2520Using%2520CLIP%252C%250Awe%2520also%2520obtained%2520predictions%2520on%2520semantic%2520features%2520through%2520zero-shot%2520inference%252C%250Asuch%2520as%2520nodule%2520margin%2520%2528AUROC%253A%25200.812%2529%252C%2520nodule%2520consistency%2520%25280.812%2529%252C%2520and%2520pleural%250Aattachment%2520%25280.840%2529.%2520Our%2520approach%2520surpasses%2520the%2520SOTA%2520models%2520in%2520predicting%2520lung%250Acancer%2520across%2520datasets%2520collected%2520from%2520diverse%2520clinical%2520settings%252C%2520providing%250Aexplainable%2520outputs%252C%2520aiding%2520clinicians%2520in%2520comprehending%2520the%2520underlying%2520meaning%250Aof%2520model%2520predictions.%2520This%2520approach%2520also%2520prevents%2520the%2520model%2520from%2520learning%250Ashortcuts%2520and%2520generalizes%2520across%2520clinical%2520settings.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/luotingzhuang/CLIP_nodule.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Model-Based%20Semantic-Guided%20Imaging%20Biomarker%20for%20Lung%0A%20%20Nodule%20Malignancy%20Prediction&entry.906535625=Luoting%20Zhuang%20and%20Seyed%20Mohammad%20Hossein%20Tabatabaei%20and%20Ramin%20Salehi-Rad%20and%20Linh%20M.%20Tran%20and%20Denise%20R.%20Aberle%20and%20Ashley%20E.%20Prosper%20and%20William%20Hsu&entry.1292438233=%20%20Machine%20learning%20models%20have%20utilized%20semantic%20features%2C%20deep%20features%2C%20or%0Aboth%20to%20assess%20lung%20nodule%20malignancy.%20However%2C%20their%20reliance%20on%20manual%0Aannotation%20during%20inference%2C%20limited%20interpretability%2C%20and%20sensitivity%20to%0Aimaging%20variations%20hinder%20their%20application%20in%20real-world%20clinical%20settings.%0AThus%2C%20this%20research%20aims%20to%20integrate%20semantic%20features%20derived%20from%0Aradiologists%27%20assessments%20of%20nodules%2C%20guiding%20the%20model%20to%20learn%20clinically%0Arelevant%2C%20robust%2C%20and%20explainable%20imaging%20features%20for%20predicting%20lung%20cancer.%0AWe%20obtained%20938%20low-dose%20CT%20scans%20from%20the%20National%20Lung%20Screening%20Trial%20%28NLST%29%0Awith%201%2C246%20nodules%20and%20semantic%20features.%20Additionally%2C%20the%20Lung%20Image%20Database%0AConsortium%20dataset%20contains%201%2C018%20CT%20scans%2C%20with%202%2C625%20lesions%20annotated%20for%0Anodule%20characteristics.%20Three%20external%20datasets%20were%20obtained%20from%20UCLA%20Health%2C%0Athe%20LUNGx%20Challenge%2C%20and%20the%20Duke%20Lung%20Cancer%20Screening.%20We%20fine-tuned%20a%0Apretrained%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20model%20with%20a%0Aparameter-efficient%20fine-tuning%20approach%20to%20align%20imaging%20and%20semantic%20text%0Afeatures%20and%20predict%20the%20one-year%20lung%20cancer%20diagnosis.%20Our%20model%20outperformed%0Astate-of-the-art%20%28SOTA%29%20models%20in%20the%20NLST%20test%20set%20with%20an%20AUROC%20of%200.901%20and%0AAUPRC%20of%200.776.%20It%20also%20showed%20robust%20results%20in%20external%20datasets.%20Using%20CLIP%2C%0Awe%20also%20obtained%20predictions%20on%20semantic%20features%20through%20zero-shot%20inference%2C%0Asuch%20as%20nodule%20margin%20%28AUROC%3A%200.812%29%2C%20nodule%20consistency%20%280.812%29%2C%20and%20pleural%0Aattachment%20%280.840%29.%20Our%20approach%20surpasses%20the%20SOTA%20models%20in%20predicting%20lung%0Acancer%20across%20datasets%20collected%20from%20diverse%20clinical%20settings%2C%20providing%0Aexplainable%20outputs%2C%20aiding%20clinicians%20in%20comprehending%20the%20underlying%20meaning%0Aof%20model%20predictions.%20This%20approach%20also%20prevents%20the%20model%20from%20learning%0Ashortcuts%20and%20generalizes%20across%20clinical%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/luotingzhuang/CLIP_nodule.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21344v2&entry.124074799=Read"},
{"title": "A Study on Regularization-Based Continual Learning Methods for Indic ASR", "author": "Gokul Adethya T and S. Jaya Nirmala", "abstract": "  Indias linguistic diversity poses significant challenges for developing\ninclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual\nmodels, which require simultaneous access to all language data, are impractical\ndue to the sequential arrival of data and privacy constraints. Continual\nLearning (CL) offers a solution by enabling models to learn new languages\nsequentially without catastrophically forgetting previously learned knowledge.\nThis paper investigates CL for ASR on Indian languages using a subset of the\nIndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,\ninitially pretrained on Hindi, which is then incrementally trained on eight\nadditional Indian languages, for a total sequence of nine languages. We\nevaluate three prominent regularization- and distillation-based CL strategies:\nElastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning\nwithout Forgetting (LwF), selected for their suitability in no-replay,\nprivacy-conscious scenarios. Performance is analyzed using Word Error Rate\n(WER) for both RNN-T and CTC paths on clean and noisy data, as well as\nknowledge retention via Backward Transfer. We also explore the impact of\nvarying the number of training epochs (1, 2, 5, and 10) per task. Results,\ncompared against naive fine-tuning, demonstrate CLs effectiveness in mitigating\nforgetting, making it a promising approach for scalable ASR in diverse Indian\nlanguages under realistic constraints. The code is available at:\nhttps://github.com/FrozenWolf-Cyber/Indic-CL-ASR\n", "link": "http://arxiv.org/abs/2508.06280v1", "date": "2025-08-08", "relevancy": 2.1607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20on%20Regularization-Based%20Continual%20Learning%20Methods%20for%20Indic%20ASR&body=Title%3A%20A%20Study%20on%20Regularization-Based%20Continual%20Learning%20Methods%20for%20Indic%20ASR%0AAuthor%3A%20Gokul%20Adethya%20T%20and%20S.%20Jaya%20Nirmala%0AAbstract%3A%20%20%20Indias%20linguistic%20diversity%20poses%20significant%20challenges%20for%20developing%0Ainclusive%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems.%20Traditional%20multilingual%0Amodels%2C%20which%20require%20simultaneous%20access%20to%20all%20language%20data%2C%20are%20impractical%0Adue%20to%20the%20sequential%20arrival%20of%20data%20and%20privacy%20constraints.%20Continual%0ALearning%20%28CL%29%20offers%20a%20solution%20by%20enabling%20models%20to%20learn%20new%20languages%0Asequentially%20without%20catastrophically%20forgetting%20previously%20learned%20knowledge.%0AThis%20paper%20investigates%20CL%20for%20ASR%20on%20Indian%20languages%20using%20a%20subset%20of%20the%0AIndicSUPERB%20benchmark.%20We%20employ%20a%20Conformer-based%20hybrid%20RNN-T/CTC%20model%2C%0Ainitially%20pretrained%20on%20Hindi%2C%20which%20is%20then%20incrementally%20trained%20on%20eight%0Aadditional%20Indian%20languages%2C%20for%20a%20total%20sequence%20of%20nine%20languages.%20We%0Aevaluate%20three%20prominent%20regularization-%20and%20distillation-based%20CL%20strategies%3A%0AElastic%20Weight%20Consolidation%20%28EWC%29%2C%20Memory%20Aware%20Synapses%20%28MAS%29%2C%20and%20Learning%0Awithout%20Forgetting%20%28LwF%29%2C%20selected%20for%20their%20suitability%20in%20no-replay%2C%0Aprivacy-conscious%20scenarios.%20Performance%20is%20analyzed%20using%20Word%20Error%20Rate%0A%28WER%29%20for%20both%20RNN-T%20and%20CTC%20paths%20on%20clean%20and%20noisy%20data%2C%20as%20well%20as%0Aknowledge%20retention%20via%20Backward%20Transfer.%20We%20also%20explore%20the%20impact%20of%0Avarying%20the%20number%20of%20training%20epochs%20%281%2C%202%2C%205%2C%20and%2010%29%20per%20task.%20Results%2C%0Acompared%20against%20naive%20fine-tuning%2C%20demonstrate%20CLs%20effectiveness%20in%20mitigating%0Aforgetting%2C%20making%20it%20a%20promising%20approach%20for%20scalable%20ASR%20in%20diverse%20Indian%0Alanguages%20under%20realistic%20constraints.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FrozenWolf-Cyber/Indic-CL-ASR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520on%2520Regularization-Based%2520Continual%2520Learning%2520Methods%2520for%2520Indic%2520ASR%26entry.906535625%3DGokul%2520Adethya%2520T%2520and%2520S.%2520Jaya%2520Nirmala%26entry.1292438233%3D%2520%2520Indias%2520linguistic%2520diversity%2520poses%2520significant%2520challenges%2520for%2520developing%250Ainclusive%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520systems.%2520Traditional%2520multilingual%250Amodels%252C%2520which%2520require%2520simultaneous%2520access%2520to%2520all%2520language%2520data%252C%2520are%2520impractical%250Adue%2520to%2520the%2520sequential%2520arrival%2520of%2520data%2520and%2520privacy%2520constraints.%2520Continual%250ALearning%2520%2528CL%2529%2520offers%2520a%2520solution%2520by%2520enabling%2520models%2520to%2520learn%2520new%2520languages%250Asequentially%2520without%2520catastrophically%2520forgetting%2520previously%2520learned%2520knowledge.%250AThis%2520paper%2520investigates%2520CL%2520for%2520ASR%2520on%2520Indian%2520languages%2520using%2520a%2520subset%2520of%2520the%250AIndicSUPERB%2520benchmark.%2520We%2520employ%2520a%2520Conformer-based%2520hybrid%2520RNN-T/CTC%2520model%252C%250Ainitially%2520pretrained%2520on%2520Hindi%252C%2520which%2520is%2520then%2520incrementally%2520trained%2520on%2520eight%250Aadditional%2520Indian%2520languages%252C%2520for%2520a%2520total%2520sequence%2520of%2520nine%2520languages.%2520We%250Aevaluate%2520three%2520prominent%2520regularization-%2520and%2520distillation-based%2520CL%2520strategies%253A%250AElastic%2520Weight%2520Consolidation%2520%2528EWC%2529%252C%2520Memory%2520Aware%2520Synapses%2520%2528MAS%2529%252C%2520and%2520Learning%250Awithout%2520Forgetting%2520%2528LwF%2529%252C%2520selected%2520for%2520their%2520suitability%2520in%2520no-replay%252C%250Aprivacy-conscious%2520scenarios.%2520Performance%2520is%2520analyzed%2520using%2520Word%2520Error%2520Rate%250A%2528WER%2529%2520for%2520both%2520RNN-T%2520and%2520CTC%2520paths%2520on%2520clean%2520and%2520noisy%2520data%252C%2520as%2520well%2520as%250Aknowledge%2520retention%2520via%2520Backward%2520Transfer.%2520We%2520also%2520explore%2520the%2520impact%2520of%250Avarying%2520the%2520number%2520of%2520training%2520epochs%2520%25281%252C%25202%252C%25205%252C%2520and%252010%2529%2520per%2520task.%2520Results%252C%250Acompared%2520against%2520naive%2520fine-tuning%252C%2520demonstrate%2520CLs%2520effectiveness%2520in%2520mitigating%250Aforgetting%252C%2520making%2520it%2520a%2520promising%2520approach%2520for%2520scalable%2520ASR%2520in%2520diverse%2520Indian%250Alanguages%2520under%2520realistic%2520constraints.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/FrozenWolf-Cyber/Indic-CL-ASR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20on%20Regularization-Based%20Continual%20Learning%20Methods%20for%20Indic%20ASR&entry.906535625=Gokul%20Adethya%20T%20and%20S.%20Jaya%20Nirmala&entry.1292438233=%20%20Indias%20linguistic%20diversity%20poses%20significant%20challenges%20for%20developing%0Ainclusive%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems.%20Traditional%20multilingual%0Amodels%2C%20which%20require%20simultaneous%20access%20to%20all%20language%20data%2C%20are%20impractical%0Adue%20to%20the%20sequential%20arrival%20of%20data%20and%20privacy%20constraints.%20Continual%0ALearning%20%28CL%29%20offers%20a%20solution%20by%20enabling%20models%20to%20learn%20new%20languages%0Asequentially%20without%20catastrophically%20forgetting%20previously%20learned%20knowledge.%0AThis%20paper%20investigates%20CL%20for%20ASR%20on%20Indian%20languages%20using%20a%20subset%20of%20the%0AIndicSUPERB%20benchmark.%20We%20employ%20a%20Conformer-based%20hybrid%20RNN-T/CTC%20model%2C%0Ainitially%20pretrained%20on%20Hindi%2C%20which%20is%20then%20incrementally%20trained%20on%20eight%0Aadditional%20Indian%20languages%2C%20for%20a%20total%20sequence%20of%20nine%20languages.%20We%0Aevaluate%20three%20prominent%20regularization-%20and%20distillation-based%20CL%20strategies%3A%0AElastic%20Weight%20Consolidation%20%28EWC%29%2C%20Memory%20Aware%20Synapses%20%28MAS%29%2C%20and%20Learning%0Awithout%20Forgetting%20%28LwF%29%2C%20selected%20for%20their%20suitability%20in%20no-replay%2C%0Aprivacy-conscious%20scenarios.%20Performance%20is%20analyzed%20using%20Word%20Error%20Rate%0A%28WER%29%20for%20both%20RNN-T%20and%20CTC%20paths%20on%20clean%20and%20noisy%20data%2C%20as%20well%20as%0Aknowledge%20retention%20via%20Backward%20Transfer.%20We%20also%20explore%20the%20impact%20of%0Avarying%20the%20number%20of%20training%20epochs%20%281%2C%202%2C%205%2C%20and%2010%29%20per%20task.%20Results%2C%0Acompared%20against%20naive%20fine-tuning%2C%20demonstrate%20CLs%20effectiveness%20in%20mitigating%0Aforgetting%2C%20making%20it%20a%20promising%20approach%20for%20scalable%20ASR%20in%20diverse%20Indian%0Alanguages%20under%20realistic%20constraints.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FrozenWolf-Cyber/Indic-CL-ASR%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06280v1&entry.124074799=Read"},
{"title": "Architectural Fusion Through Contextual Partitioning in Large Language\n  Models: A Novel Approach to Parameterized Knowledge Integration", "author": "Offa Kingsleigh and Alfred Abercrombie and David Woolstencroft and Beorhtric Meadowcroft and Marcus Irvin", "abstract": "  Contextual Partitioning introduces an innovative approach to enhancing the\narchitectural design of large-scale computational models through the dynamic\nsegmentation of parameters into context-aware regions. This methodology\nemphasizes the importance of task-specific specialization, achieved through\nadaptive parameter allocation mechanisms that align with the linguistic\nfeatures of input data. Experimental evaluations demonstrated substantial\nimprovements in accuracy, perplexity, and contextual coherence across a variety\nof linguistic tasks, highlighting the adaptability and scalability of the\nproposed framework. By reducing redundancy and enhancing computational\nefficiency, Contextual Partitioning not only streamlines model operations but\nalso expands the scope of applications for advanced language processing\nsystems. The approach operates autonomously, requiring no external fine-tuning,\nthereby addressing a significant limitation in conventional parameter\noptimization techniques. Empirical results demonstrate the effectiveness of\ngradient-driven segmentation, enabling models to dynamically recalibrate and\nspecialize in response to task-specific demands. Furthermore, resource\nutilization metrics reveal notable reductions in memory usage and training\ntimes, confirming the efficiency of the approach. Observations from qualitative\nanalyses illustrate improved contextual coherence and logical flow in generated\noutputs, reinforcing the practical value of this technique. The findings\ncollectively demonstrate the potential for Contextual Partitioning to redefine\nthe scalability and adaptability of computational language architectures in\ndiverse and complex domains.\n", "link": "http://arxiv.org/abs/2501.12901v2", "date": "2025-08-08", "relevancy": 2.1573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration&body=Title%3A%20Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration%0AAuthor%3A%20Offa%20Kingsleigh%20and%20Alfred%20Abercrombie%20and%20David%20Woolstencroft%20and%20Beorhtric%20Meadowcroft%20and%20Marcus%20Irvin%0AAbstract%3A%20%20%20Contextual%20Partitioning%20introduces%20an%20innovative%20approach%20to%20enhancing%20the%0Aarchitectural%20design%20of%20large-scale%20computational%20models%20through%20the%20dynamic%0Asegmentation%20of%20parameters%20into%20context-aware%20regions.%20This%20methodology%0Aemphasizes%20the%20importance%20of%20task-specific%20specialization%2C%20achieved%20through%0Aadaptive%20parameter%20allocation%20mechanisms%20that%20align%20with%20the%20linguistic%0Afeatures%20of%20input%20data.%20Experimental%20evaluations%20demonstrated%20substantial%0Aimprovements%20in%20accuracy%2C%20perplexity%2C%20and%20contextual%20coherence%20across%20a%20variety%0Aof%20linguistic%20tasks%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20the%0Aproposed%20framework.%20By%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency%2C%20Contextual%20Partitioning%20not%20only%20streamlines%20model%20operations%20but%0Aalso%20expands%20the%20scope%20of%20applications%20for%20advanced%20language%20processing%0Asystems.%20The%20approach%20operates%20autonomously%2C%20requiring%20no%20external%20fine-tuning%2C%0Athereby%20addressing%20a%20significant%20limitation%20in%20conventional%20parameter%0Aoptimization%20techniques.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0Agradient-driven%20segmentation%2C%20enabling%20models%20to%20dynamically%20recalibrate%20and%0Aspecialize%20in%20response%20to%20task-specific%20demands.%20Furthermore%2C%20resource%0Autilization%20metrics%20reveal%20notable%20reductions%20in%20memory%20usage%20and%20training%0Atimes%2C%20confirming%20the%20efficiency%20of%20the%20approach.%20Observations%20from%20qualitative%0Aanalyses%20illustrate%20improved%20contextual%20coherence%20and%20logical%20flow%20in%20generated%0Aoutputs%2C%20reinforcing%20the%20practical%20value%20of%20this%20technique.%20The%20findings%0Acollectively%20demonstrate%20the%20potential%20for%20Contextual%20Partitioning%20to%20redefine%0Athe%20scalability%20and%20adaptability%20of%20computational%20language%20architectures%20in%0Adiverse%20and%20complex%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectural%2520Fusion%2520Through%2520Contextual%2520Partitioning%2520in%2520Large%2520Language%250A%2520%2520Models%253A%2520A%2520Novel%2520Approach%2520to%2520Parameterized%2520Knowledge%2520Integration%26entry.906535625%3DOffa%2520Kingsleigh%2520and%2520Alfred%2520Abercrombie%2520and%2520David%2520Woolstencroft%2520and%2520Beorhtric%2520Meadowcroft%2520and%2520Marcus%2520Irvin%26entry.1292438233%3D%2520%2520Contextual%2520Partitioning%2520introduces%2520an%2520innovative%2520approach%2520to%2520enhancing%2520the%250Aarchitectural%2520design%2520of%2520large-scale%2520computational%2520models%2520through%2520the%2520dynamic%250Asegmentation%2520of%2520parameters%2520into%2520context-aware%2520regions.%2520This%2520methodology%250Aemphasizes%2520the%2520importance%2520of%2520task-specific%2520specialization%252C%2520achieved%2520through%250Aadaptive%2520parameter%2520allocation%2520mechanisms%2520that%2520align%2520with%2520the%2520linguistic%250Afeatures%2520of%2520input%2520data.%2520Experimental%2520evaluations%2520demonstrated%2520substantial%250Aimprovements%2520in%2520accuracy%252C%2520perplexity%252C%2520and%2520contextual%2520coherence%2520across%2520a%2520variety%250Aof%2520linguistic%2520tasks%252C%2520highlighting%2520the%2520adaptability%2520and%2520scalability%2520of%2520the%250Aproposed%2520framework.%2520By%2520reducing%2520redundancy%2520and%2520enhancing%2520computational%250Aefficiency%252C%2520Contextual%2520Partitioning%2520not%2520only%2520streamlines%2520model%2520operations%2520but%250Aalso%2520expands%2520the%2520scope%2520of%2520applications%2520for%2520advanced%2520language%2520processing%250Asystems.%2520The%2520approach%2520operates%2520autonomously%252C%2520requiring%2520no%2520external%2520fine-tuning%252C%250Athereby%2520addressing%2520a%2520significant%2520limitation%2520in%2520conventional%2520parameter%250Aoptimization%2520techniques.%2520Empirical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Agradient-driven%2520segmentation%252C%2520enabling%2520models%2520to%2520dynamically%2520recalibrate%2520and%250Aspecialize%2520in%2520response%2520to%2520task-specific%2520demands.%2520Furthermore%252C%2520resource%250Autilization%2520metrics%2520reveal%2520notable%2520reductions%2520in%2520memory%2520usage%2520and%2520training%250Atimes%252C%2520confirming%2520the%2520efficiency%2520of%2520the%2520approach.%2520Observations%2520from%2520qualitative%250Aanalyses%2520illustrate%2520improved%2520contextual%2520coherence%2520and%2520logical%2520flow%2520in%2520generated%250Aoutputs%252C%2520reinforcing%2520the%2520practical%2520value%2520of%2520this%2520technique.%2520The%2520findings%250Acollectively%2520demonstrate%2520the%2520potential%2520for%2520Contextual%2520Partitioning%2520to%2520redefine%250Athe%2520scalability%2520and%2520adaptability%2520of%2520computational%2520language%2520architectures%2520in%250Adiverse%2520and%2520complex%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration&entry.906535625=Offa%20Kingsleigh%20and%20Alfred%20Abercrombie%20and%20David%20Woolstencroft%20and%20Beorhtric%20Meadowcroft%20and%20Marcus%20Irvin&entry.1292438233=%20%20Contextual%20Partitioning%20introduces%20an%20innovative%20approach%20to%20enhancing%20the%0Aarchitectural%20design%20of%20large-scale%20computational%20models%20through%20the%20dynamic%0Asegmentation%20of%20parameters%20into%20context-aware%20regions.%20This%20methodology%0Aemphasizes%20the%20importance%20of%20task-specific%20specialization%2C%20achieved%20through%0Aadaptive%20parameter%20allocation%20mechanisms%20that%20align%20with%20the%20linguistic%0Afeatures%20of%20input%20data.%20Experimental%20evaluations%20demonstrated%20substantial%0Aimprovements%20in%20accuracy%2C%20perplexity%2C%20and%20contextual%20coherence%20across%20a%20variety%0Aof%20linguistic%20tasks%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20the%0Aproposed%20framework.%20By%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency%2C%20Contextual%20Partitioning%20not%20only%20streamlines%20model%20operations%20but%0Aalso%20expands%20the%20scope%20of%20applications%20for%20advanced%20language%20processing%0Asystems.%20The%20approach%20operates%20autonomously%2C%20requiring%20no%20external%20fine-tuning%2C%0Athereby%20addressing%20a%20significant%20limitation%20in%20conventional%20parameter%0Aoptimization%20techniques.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0Agradient-driven%20segmentation%2C%20enabling%20models%20to%20dynamically%20recalibrate%20and%0Aspecialize%20in%20response%20to%20task-specific%20demands.%20Furthermore%2C%20resource%0Autilization%20metrics%20reveal%20notable%20reductions%20in%20memory%20usage%20and%20training%0Atimes%2C%20confirming%20the%20efficiency%20of%20the%20approach.%20Observations%20from%20qualitative%0Aanalyses%20illustrate%20improved%20contextual%20coherence%20and%20logical%20flow%20in%20generated%0Aoutputs%2C%20reinforcing%20the%20practical%20value%20of%20this%20technique.%20The%20findings%0Acollectively%20demonstrate%20the%20potential%20for%20Contextual%20Partitioning%20to%20redefine%0Athe%20scalability%20and%20adaptability%20of%20computational%20language%20architectures%20in%0Adiverse%20and%20complex%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12901v2&entry.124074799=Read"},
{"title": "MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN\n  Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation", "author": "Xiaoxiang Han and Xinyu Li and Jiang Shang and Yiman Liu and Keyan Chen and Shugong Xu and Qiaohong Liu and Qi Zhang", "abstract": "  Segmenting anatomical structures and lesions from ultrasound images\ncontributes to disease assessment. Weakly supervised learning (WSL) based on\nsparse annotation has achieved encouraging performance and demonstrated the\npotential to reduce annotation costs. This study attempts to introduce\nscribble-based WSL into ultrasound image segmentation tasks. However,\nultrasound images often suffer from poor contrast and unclear edges, coupled\nwith insufficient supervison signals for edges, posing challenges to edge\nprediction. Uncertainty modeling has been proven to facilitate models in\ndealing with these issues. Nevertheless, existing uncertainty estimation\nparadigms are not robust enough and often filter out predictions near decision\nboundaries, resulting in unstable edge predictions. Therefore, we propose\nleveraging predictions near decision boundaries effectively. Specifically, we\nintroduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided\nConsistency strategy. This strategy utilizes high-evidence predictions, which\nare more likely to occur near high-density regions, to guide the optimization\nof low-evidence predictions that may appear near decision boundaries.\nFurthermore, the diverse sizes and locations of lesions in ultrasound images\npose a challenge for CNNs with local receptive fields, as they struggle to\nmodel global information. Therefore, we introduce Visual Mamba based on\nstructured state space sequence models, which achieves long-range dependency\nwith linear computational complexity, and we construct a novel hybrid CNN-Mamba\nframework. During training, the collaboration between the CNN branch and the\nMamba branch in the proposed framework draws inspiration from each other based\non the EGC strategy. Experiments demonstrate the competitiveness of the\nproposed method. Dataset and code will be available on\nhttps://github.com/GtLinyer/MambaEviScrib.\n", "link": "http://arxiv.org/abs/2409.19370v3", "date": "2025-08-08", "relevancy": 2.1548, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5818}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation&body=Title%3A%20MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation%0AAuthor%3A%20Xiaoxiang%20Han%20and%20Xinyu%20Li%20and%20Jiang%20Shang%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Shugong%20Xu%20and%20Qiaohong%20Liu%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Segmenting%20anatomical%20structures%20and%20lesions%20from%20ultrasound%20images%0Acontributes%20to%20disease%20assessment.%20Weakly%20supervised%20learning%20%28WSL%29%20based%20on%0Asparse%20annotation%20has%20achieved%20encouraging%20performance%20and%20demonstrated%20the%0Apotential%20to%20reduce%20annotation%20costs.%20This%20study%20attempts%20to%20introduce%0Ascribble-based%20WSL%20into%20ultrasound%20image%20segmentation%20tasks.%20However%2C%0Aultrasound%20images%20often%20suffer%20from%20poor%20contrast%20and%20unclear%20edges%2C%20coupled%0Awith%20insufficient%20supervison%20signals%20for%20edges%2C%20posing%20challenges%20to%20edge%0Aprediction.%20Uncertainty%20modeling%20has%20been%20proven%20to%20facilitate%20models%20in%0Adealing%20with%20these%20issues.%20Nevertheless%2C%20existing%20uncertainty%20estimation%0Aparadigms%20are%20not%20robust%20enough%20and%20often%20filter%20out%20predictions%20near%20decision%0Aboundaries%2C%20resulting%20in%20unstable%20edge%20predictions.%20Therefore%2C%20we%20propose%0Aleveraging%20predictions%20near%20decision%20boundaries%20effectively.%20Specifically%2C%20we%0Aintroduce%20Dempster-Shafer%20Theory%20%28DST%29%20of%20evidence%20to%20design%20an%20Evidence-Guided%0AConsistency%20strategy.%20This%20strategy%20utilizes%20high-evidence%20predictions%2C%20which%0Aare%20more%20likely%20to%20occur%20near%20high-density%20regions%2C%20to%20guide%20the%20optimization%0Aof%20low-evidence%20predictions%20that%20may%20appear%20near%20decision%20boundaries.%0AFurthermore%2C%20the%20diverse%20sizes%20and%20locations%20of%20lesions%20in%20ultrasound%20images%0Apose%20a%20challenge%20for%20CNNs%20with%20local%20receptive%20fields%2C%20as%20they%20struggle%20to%0Amodel%20global%20information.%20Therefore%2C%20we%20introduce%20Visual%20Mamba%20based%20on%0Astructured%20state%20space%20sequence%20models%2C%20which%20achieves%20long-range%20dependency%0Awith%20linear%20computational%20complexity%2C%20and%20we%20construct%20a%20novel%20hybrid%20CNN-Mamba%0Aframework.%20During%20training%2C%20the%20collaboration%20between%20the%20CNN%20branch%20and%20the%0AMamba%20branch%20in%20the%20proposed%20framework%20draws%20inspiration%20from%20each%20other%20based%0Aon%20the%20EGC%20strategy.%20Experiments%20demonstrate%20the%20competitiveness%20of%20the%0Aproposed%20method.%20Dataset%20and%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/GtLinyer/MambaEviScrib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19370v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaEviScrib%253A%2520Mamba%2520and%2520Evidence-Guided%2520Consistency%2520Enhance%2520CNN%250A%2520%2520Robustness%2520for%2520Scribble-Based%2520Weakly%2520Supervised%2520Ultrasound%2520Image%2520Segmentation%26entry.906535625%3DXiaoxiang%2520Han%2520and%2520Xinyu%2520Li%2520and%2520Jiang%2520Shang%2520and%2520Yiman%2520Liu%2520and%2520Keyan%2520Chen%2520and%2520Shugong%2520Xu%2520and%2520Qiaohong%2520Liu%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Segmenting%2520anatomical%2520structures%2520and%2520lesions%2520from%2520ultrasound%2520images%250Acontributes%2520to%2520disease%2520assessment.%2520Weakly%2520supervised%2520learning%2520%2528WSL%2529%2520based%2520on%250Asparse%2520annotation%2520has%2520achieved%2520encouraging%2520performance%2520and%2520demonstrated%2520the%250Apotential%2520to%2520reduce%2520annotation%2520costs.%2520This%2520study%2520attempts%2520to%2520introduce%250Ascribble-based%2520WSL%2520into%2520ultrasound%2520image%2520segmentation%2520tasks.%2520However%252C%250Aultrasound%2520images%2520often%2520suffer%2520from%2520poor%2520contrast%2520and%2520unclear%2520edges%252C%2520coupled%250Awith%2520insufficient%2520supervison%2520signals%2520for%2520edges%252C%2520posing%2520challenges%2520to%2520edge%250Aprediction.%2520Uncertainty%2520modeling%2520has%2520been%2520proven%2520to%2520facilitate%2520models%2520in%250Adealing%2520with%2520these%2520issues.%2520Nevertheless%252C%2520existing%2520uncertainty%2520estimation%250Aparadigms%2520are%2520not%2520robust%2520enough%2520and%2520often%2520filter%2520out%2520predictions%2520near%2520decision%250Aboundaries%252C%2520resulting%2520in%2520unstable%2520edge%2520predictions.%2520Therefore%252C%2520we%2520propose%250Aleveraging%2520predictions%2520near%2520decision%2520boundaries%2520effectively.%2520Specifically%252C%2520we%250Aintroduce%2520Dempster-Shafer%2520Theory%2520%2528DST%2529%2520of%2520evidence%2520to%2520design%2520an%2520Evidence-Guided%250AConsistency%2520strategy.%2520This%2520strategy%2520utilizes%2520high-evidence%2520predictions%252C%2520which%250Aare%2520more%2520likely%2520to%2520occur%2520near%2520high-density%2520regions%252C%2520to%2520guide%2520the%2520optimization%250Aof%2520low-evidence%2520predictions%2520that%2520may%2520appear%2520near%2520decision%2520boundaries.%250AFurthermore%252C%2520the%2520diverse%2520sizes%2520and%2520locations%2520of%2520lesions%2520in%2520ultrasound%2520images%250Apose%2520a%2520challenge%2520for%2520CNNs%2520with%2520local%2520receptive%2520fields%252C%2520as%2520they%2520struggle%2520to%250Amodel%2520global%2520information.%2520Therefore%252C%2520we%2520introduce%2520Visual%2520Mamba%2520based%2520on%250Astructured%2520state%2520space%2520sequence%2520models%252C%2520which%2520achieves%2520long-range%2520dependency%250Awith%2520linear%2520computational%2520complexity%252C%2520and%2520we%2520construct%2520a%2520novel%2520hybrid%2520CNN-Mamba%250Aframework.%2520During%2520training%252C%2520the%2520collaboration%2520between%2520the%2520CNN%2520branch%2520and%2520the%250AMamba%2520branch%2520in%2520the%2520proposed%2520framework%2520draws%2520inspiration%2520from%2520each%2520other%2520based%250Aon%2520the%2520EGC%2520strategy.%2520Experiments%2520demonstrate%2520the%2520competitiveness%2520of%2520the%250Aproposed%2520method.%2520Dataset%2520and%2520code%2520will%2520be%2520available%2520on%250Ahttps%253A//github.com/GtLinyer/MambaEviScrib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19370v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation&entry.906535625=Xiaoxiang%20Han%20and%20Xinyu%20Li%20and%20Jiang%20Shang%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Shugong%20Xu%20and%20Qiaohong%20Liu%20and%20Qi%20Zhang&entry.1292438233=%20%20Segmenting%20anatomical%20structures%20and%20lesions%20from%20ultrasound%20images%0Acontributes%20to%20disease%20assessment.%20Weakly%20supervised%20learning%20%28WSL%29%20based%20on%0Asparse%20annotation%20has%20achieved%20encouraging%20performance%20and%20demonstrated%20the%0Apotential%20to%20reduce%20annotation%20costs.%20This%20study%20attempts%20to%20introduce%0Ascribble-based%20WSL%20into%20ultrasound%20image%20segmentation%20tasks.%20However%2C%0Aultrasound%20images%20often%20suffer%20from%20poor%20contrast%20and%20unclear%20edges%2C%20coupled%0Awith%20insufficient%20supervison%20signals%20for%20edges%2C%20posing%20challenges%20to%20edge%0Aprediction.%20Uncertainty%20modeling%20has%20been%20proven%20to%20facilitate%20models%20in%0Adealing%20with%20these%20issues.%20Nevertheless%2C%20existing%20uncertainty%20estimation%0Aparadigms%20are%20not%20robust%20enough%20and%20often%20filter%20out%20predictions%20near%20decision%0Aboundaries%2C%20resulting%20in%20unstable%20edge%20predictions.%20Therefore%2C%20we%20propose%0Aleveraging%20predictions%20near%20decision%20boundaries%20effectively.%20Specifically%2C%20we%0Aintroduce%20Dempster-Shafer%20Theory%20%28DST%29%20of%20evidence%20to%20design%20an%20Evidence-Guided%0AConsistency%20strategy.%20This%20strategy%20utilizes%20high-evidence%20predictions%2C%20which%0Aare%20more%20likely%20to%20occur%20near%20high-density%20regions%2C%20to%20guide%20the%20optimization%0Aof%20low-evidence%20predictions%20that%20may%20appear%20near%20decision%20boundaries.%0AFurthermore%2C%20the%20diverse%20sizes%20and%20locations%20of%20lesions%20in%20ultrasound%20images%0Apose%20a%20challenge%20for%20CNNs%20with%20local%20receptive%20fields%2C%20as%20they%20struggle%20to%0Amodel%20global%20information.%20Therefore%2C%20we%20introduce%20Visual%20Mamba%20based%20on%0Astructured%20state%20space%20sequence%20models%2C%20which%20achieves%20long-range%20dependency%0Awith%20linear%20computational%20complexity%2C%20and%20we%20construct%20a%20novel%20hybrid%20CNN-Mamba%0Aframework.%20During%20training%2C%20the%20collaboration%20between%20the%20CNN%20branch%20and%20the%0AMamba%20branch%20in%20the%20proposed%20framework%20draws%20inspiration%20from%20each%20other%20based%0Aon%20the%20EGC%20strategy.%20Experiments%20demonstrate%20the%20competitiveness%20of%20the%0Aproposed%20method.%20Dataset%20and%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/GtLinyer/MambaEviScrib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19370v3&entry.124074799=Read"},
{"title": "A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and\n  Street Images", "author": "Wooyong Jung and Sola Kim and Dongwook Kim and Maryam Tabar and Dongwon Lee", "abstract": "  Homelessness in the United States has surged to levels unseen since the Great\nDepression. However, existing methods for monitoring it, such as point-in-time\n(PIT) counts, have limitations in terms of frequency, consistency, and spatial\ndetail. This study proposes a new approach using publicly available,\ncrowdsourced data, specifically 311 Service Calls and street-level imagery, to\ntrack and forecast homeless tent trends in San Francisco. Our predictive model\ncaptures fine-grained daily and neighborhood-level variations, uncovering\npatterns that traditional counts often overlook, such as rapid fluctuations\nduring the COVID-19 pandemic and spatial shifts in tent locations over time. By\nproviding more timely, localized, and cost-effective information, this approach\nserves as a valuable tool for guiding policy responses and evaluating\ninterventions aimed at reducing unsheltered homelessness.\n", "link": "http://arxiv.org/abs/2508.06409v1", "date": "2025-08-08", "relevancy": 2.1473, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images&body=Title%3A%20A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images%0AAuthor%3A%20Wooyong%20Jung%20and%20Sola%20Kim%20and%20Dongwook%20Kim%20and%20Maryam%20Tabar%20and%20Dongwon%20Lee%0AAbstract%3A%20%20%20Homelessness%20in%20the%20United%20States%20has%20surged%20to%20levels%20unseen%20since%20the%20Great%0ADepression.%20However%2C%20existing%20methods%20for%20monitoring%20it%2C%20such%20as%20point-in-time%0A%28PIT%29%20counts%2C%20have%20limitations%20in%20terms%20of%20frequency%2C%20consistency%2C%20and%20spatial%0Adetail.%20This%20study%20proposes%20a%20new%20approach%20using%20publicly%20available%2C%0Acrowdsourced%20data%2C%20specifically%20311%20Service%20Calls%20and%20street-level%20imagery%2C%20to%0Atrack%20and%20forecast%20homeless%20tent%20trends%20in%20San%20Francisco.%20Our%20predictive%20model%0Acaptures%20fine-grained%20daily%20and%20neighborhood-level%20variations%2C%20uncovering%0Apatterns%20that%20traditional%20counts%20often%20overlook%2C%20such%20as%20rapid%20fluctuations%0Aduring%20the%20COVID-19%20pandemic%20and%20spatial%20shifts%20in%20tent%20locations%20over%20time.%20By%0Aproviding%20more%20timely%2C%20localized%2C%20and%20cost-effective%20information%2C%20this%20approach%0Aserves%20as%20a%20valuable%20tool%20for%20guiding%20policy%20responses%20and%20evaluating%0Ainterventions%20aimed%20at%20reducing%20unsheltered%20homelessness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Lens%2520on%2520Homelessness%253A%2520Daily%2520Tent%2520Monitoring%2520with%2520311%2520Calls%2520and%250A%2520%2520Street%2520Images%26entry.906535625%3DWooyong%2520Jung%2520and%2520Sola%2520Kim%2520and%2520Dongwook%2520Kim%2520and%2520Maryam%2520Tabar%2520and%2520Dongwon%2520Lee%26entry.1292438233%3D%2520%2520Homelessness%2520in%2520the%2520United%2520States%2520has%2520surged%2520to%2520levels%2520unseen%2520since%2520the%2520Great%250ADepression.%2520However%252C%2520existing%2520methods%2520for%2520monitoring%2520it%252C%2520such%2520as%2520point-in-time%250A%2528PIT%2529%2520counts%252C%2520have%2520limitations%2520in%2520terms%2520of%2520frequency%252C%2520consistency%252C%2520and%2520spatial%250Adetail.%2520This%2520study%2520proposes%2520a%2520new%2520approach%2520using%2520publicly%2520available%252C%250Acrowdsourced%2520data%252C%2520specifically%2520311%2520Service%2520Calls%2520and%2520street-level%2520imagery%252C%2520to%250Atrack%2520and%2520forecast%2520homeless%2520tent%2520trends%2520in%2520San%2520Francisco.%2520Our%2520predictive%2520model%250Acaptures%2520fine-grained%2520daily%2520and%2520neighborhood-level%2520variations%252C%2520uncovering%250Apatterns%2520that%2520traditional%2520counts%2520often%2520overlook%252C%2520such%2520as%2520rapid%2520fluctuations%250Aduring%2520the%2520COVID-19%2520pandemic%2520and%2520spatial%2520shifts%2520in%2520tent%2520locations%2520over%2520time.%2520By%250Aproviding%2520more%2520timely%252C%2520localized%252C%2520and%2520cost-effective%2520information%252C%2520this%2520approach%250Aserves%2520as%2520a%2520valuable%2520tool%2520for%2520guiding%2520policy%2520responses%2520and%2520evaluating%250Ainterventions%2520aimed%2520at%2520reducing%2520unsheltered%2520homelessness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images&entry.906535625=Wooyong%20Jung%20and%20Sola%20Kim%20and%20Dongwook%20Kim%20and%20Maryam%20Tabar%20and%20Dongwon%20Lee&entry.1292438233=%20%20Homelessness%20in%20the%20United%20States%20has%20surged%20to%20levels%20unseen%20since%20the%20Great%0ADepression.%20However%2C%20existing%20methods%20for%20monitoring%20it%2C%20such%20as%20point-in-time%0A%28PIT%29%20counts%2C%20have%20limitations%20in%20terms%20of%20frequency%2C%20consistency%2C%20and%20spatial%0Adetail.%20This%20study%20proposes%20a%20new%20approach%20using%20publicly%20available%2C%0Acrowdsourced%20data%2C%20specifically%20311%20Service%20Calls%20and%20street-level%20imagery%2C%20to%0Atrack%20and%20forecast%20homeless%20tent%20trends%20in%20San%20Francisco.%20Our%20predictive%20model%0Acaptures%20fine-grained%20daily%20and%20neighborhood-level%20variations%2C%20uncovering%0Apatterns%20that%20traditional%20counts%20often%20overlook%2C%20such%20as%20rapid%20fluctuations%0Aduring%20the%20COVID-19%20pandemic%20and%20spatial%20shifts%20in%20tent%20locations%20over%20time.%20By%0Aproviding%20more%20timely%2C%20localized%2C%20and%20cost-effective%20information%2C%20this%20approach%0Aserves%20as%20a%20valuable%20tool%20for%20guiding%20policy%20responses%20and%20evaluating%0Ainterventions%20aimed%20at%20reducing%20unsheltered%20homelessness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06409v1&entry.124074799=Read"},
{"title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on\n  Infographic?", "author": "Keummin Ka and Junhyeong Park and Jahyun Jeon and Youngjae Yu", "abstract": "  Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.\n", "link": "http://arxiv.org/abs/2508.06220v1", "date": "2025-08-08", "relevancy": 2.1447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoCausalQA%3ACan%20Models%20Perform%20Non-explicit%20Causal%20Reasoning%20Based%20on%0A%20%20Infographic%3F&body=Title%3A%20InfoCausalQA%3ACan%20Models%20Perform%20Non-explicit%20Causal%20Reasoning%20Based%20on%0A%20%20Infographic%3F%0AAuthor%3A%20Keummin%20Ka%20and%20Junhyeong%20Park%20and%20Jahyun%20Jeon%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20perception%20and%20reasoning.%20However%2C%20the%20ability%20to%20perform%0Acausal%20inference%20--%20a%20core%20aspect%20of%20human%20cognition%20--%20remains%20underexplored%2C%0Aparticularly%20in%20multimodal%20settings.%20In%20this%20study%2C%20we%20introduce%20InfoCausalQA%2C%0Aa%20novel%20benchmark%20designed%20to%20evaluate%20causal%20reasoning%20grounded%20in%0Ainfographics%20that%20combine%20structured%20visual%20data%20with%20textual%20context.%20The%0Abenchmark%20comprises%20two%20tasks%3A%20Task%201%20focuses%20on%20quantitative%20causal%20reasoning%0Abased%20on%20inferred%20numerical%20trends%2C%20while%20Task%202%20targets%20semantic%20causal%0Areasoning%20involving%20five%20types%20of%20causal%20relations%3A%20cause%2C%20effect%2C%0Aintervention%2C%20counterfactual%2C%20and%20temporal.%20We%20manually%20collected%20494%0Ainfographic-text%20pairs%20from%20four%20public%20sources%20and%20used%20GPT-4o%20to%20generate%0A1%2C482%20high-quality%20multiple-choice%20QA%20pairs.%20These%20questions%20were%20then%0Acarefully%20revised%20by%20humans%20to%20ensure%20they%20cannot%20be%20answered%20based%20on%0Asurface-level%20cues%20alone%20but%20instead%20require%20genuine%20visual%20grounding.%20Our%0Aexperimental%20results%20reveal%20that%20current%20VLMs%20exhibit%20limited%20capability%20in%0Acomputational%20reasoning%20and%20even%20more%20pronounced%20limitations%20in%20semantic%20causal%0Areasoning.%20Their%20significantly%20lower%20performance%20compared%20to%20humans%20indicates%20a%0Asubstantial%20gap%20in%20leveraging%20infographic-based%20information%20for%20causal%0Ainference.%20Through%20InfoCausalQA%2C%20we%20highlight%20the%20need%20for%20advancing%20the%20causal%0Areasoning%20abilities%20of%20multimodal%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoCausalQA%253ACan%2520Models%2520Perform%2520Non-explicit%2520Causal%2520Reasoning%2520Based%2520on%250A%2520%2520Infographic%253F%26entry.906535625%3DKeummin%2520Ka%2520and%2520Junhyeong%2520Park%2520and%2520Jahyun%2520Jeon%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520in%2520perception%2520and%2520reasoning.%2520However%252C%2520the%2520ability%2520to%2520perform%250Acausal%2520inference%2520--%2520a%2520core%2520aspect%2520of%2520human%2520cognition%2520--%2520remains%2520underexplored%252C%250Aparticularly%2520in%2520multimodal%2520settings.%2520In%2520this%2520study%252C%2520we%2520introduce%2520InfoCausalQA%252C%250Aa%2520novel%2520benchmark%2520designed%2520to%2520evaluate%2520causal%2520reasoning%2520grounded%2520in%250Ainfographics%2520that%2520combine%2520structured%2520visual%2520data%2520with%2520textual%2520context.%2520The%250Abenchmark%2520comprises%2520two%2520tasks%253A%2520Task%25201%2520focuses%2520on%2520quantitative%2520causal%2520reasoning%250Abased%2520on%2520inferred%2520numerical%2520trends%252C%2520while%2520Task%25202%2520targets%2520semantic%2520causal%250Areasoning%2520involving%2520five%2520types%2520of%2520causal%2520relations%253A%2520cause%252C%2520effect%252C%250Aintervention%252C%2520counterfactual%252C%2520and%2520temporal.%2520We%2520manually%2520collected%2520494%250Ainfographic-text%2520pairs%2520from%2520four%2520public%2520sources%2520and%2520used%2520GPT-4o%2520to%2520generate%250A1%252C482%2520high-quality%2520multiple-choice%2520QA%2520pairs.%2520These%2520questions%2520were%2520then%250Acarefully%2520revised%2520by%2520humans%2520to%2520ensure%2520they%2520cannot%2520be%2520answered%2520based%2520on%250Asurface-level%2520cues%2520alone%2520but%2520instead%2520require%2520genuine%2520visual%2520grounding.%2520Our%250Aexperimental%2520results%2520reveal%2520that%2520current%2520VLMs%2520exhibit%2520limited%2520capability%2520in%250Acomputational%2520reasoning%2520and%2520even%2520more%2520pronounced%2520limitations%2520in%2520semantic%2520causal%250Areasoning.%2520Their%2520significantly%2520lower%2520performance%2520compared%2520to%2520humans%2520indicates%2520a%250Asubstantial%2520gap%2520in%2520leveraging%2520infographic-based%2520information%2520for%2520causal%250Ainference.%2520Through%2520InfoCausalQA%252C%2520we%2520highlight%2520the%2520need%2520for%2520advancing%2520the%2520causal%250Areasoning%2520abilities%2520of%2520multimodal%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoCausalQA%3ACan%20Models%20Perform%20Non-explicit%20Causal%20Reasoning%20Based%20on%0A%20%20Infographic%3F&entry.906535625=Keummin%20Ka%20and%20Junhyeong%20Park%20and%20Jahyun%20Jeon%20and%20Youngjae%20Yu&entry.1292438233=%20%20Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20perception%20and%20reasoning.%20However%2C%20the%20ability%20to%20perform%0Acausal%20inference%20--%20a%20core%20aspect%20of%20human%20cognition%20--%20remains%20underexplored%2C%0Aparticularly%20in%20multimodal%20settings.%20In%20this%20study%2C%20we%20introduce%20InfoCausalQA%2C%0Aa%20novel%20benchmark%20designed%20to%20evaluate%20causal%20reasoning%20grounded%20in%0Ainfographics%20that%20combine%20structured%20visual%20data%20with%20textual%20context.%20The%0Abenchmark%20comprises%20two%20tasks%3A%20Task%201%20focuses%20on%20quantitative%20causal%20reasoning%0Abased%20on%20inferred%20numerical%20trends%2C%20while%20Task%202%20targets%20semantic%20causal%0Areasoning%20involving%20five%20types%20of%20causal%20relations%3A%20cause%2C%20effect%2C%0Aintervention%2C%20counterfactual%2C%20and%20temporal.%20We%20manually%20collected%20494%0Ainfographic-text%20pairs%20from%20four%20public%20sources%20and%20used%20GPT-4o%20to%20generate%0A1%2C482%20high-quality%20multiple-choice%20QA%20pairs.%20These%20questions%20were%20then%0Acarefully%20revised%20by%20humans%20to%20ensure%20they%20cannot%20be%20answered%20based%20on%0Asurface-level%20cues%20alone%20but%20instead%20require%20genuine%20visual%20grounding.%20Our%0Aexperimental%20results%20reveal%20that%20current%20VLMs%20exhibit%20limited%20capability%20in%0Acomputational%20reasoning%20and%20even%20more%20pronounced%20limitations%20in%20semantic%20causal%0Areasoning.%20Their%20significantly%20lower%20performance%20compared%20to%20humans%20indicates%20a%0Asubstantial%20gap%20in%20leveraging%20infographic-based%20information%20for%20causal%0Ainference.%20Through%20InfoCausalQA%2C%20we%20highlight%20the%20need%20for%20advancing%20the%20causal%0Areasoning%20abilities%20of%20multimodal%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06220v1&entry.124074799=Read"},
{"title": "Optimal sampling for least-squares approximation", "author": "Ben Adcock", "abstract": "  Least-squares approximation is one of the most important methods for\nrecovering an unknown function from data. While in many applications the data\nis fixed, in many others there is substantial freedom to choose where to\nsample. In this paper, we review recent progress on near-optimal random\nsampling strategies for (weighted) least-squares approximation in arbitrary\nlinear spaces. We introduce the Christoffel function as a key quantity in the\nanalysis of (weighted) least-squares approximation from random samples, then\nshow how it can be used to construct a random sampling strategy, termed\nChristoffel sampling, that possesses near-optimal sample complexity: namely,\nthe number of samples scales log-linearly in the dimension of the approximation\nspace $n$. We discuss a series of variations, extensions and further topics,\nand throughout highlight connections to approximation theory, machine learning,\ninformation-based complexity and numerical linear algebra. Finally, motivated\nby various contemporary applications, we consider a generalization of the\nclassical setting where the samples need not be pointwise samples of a\nscalar-valued function, and the approximation space need not be linear. We show\nthat, even in this significantly more general setting, suitable generalizations\nof Christoffel function still determine the sample complexity. Consequently,\nthese can be used to design enhanced, Christoffel sampling strategies in a\nunified way for general recovery problems. This article is largely\nself-contained, and intended to be accessible to nonspecialists.\n", "link": "http://arxiv.org/abs/2409.02342v2", "date": "2025-08-08", "relevancy": 2.1393, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4506}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4165}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20sampling%20for%20least-squares%20approximation&body=Title%3A%20Optimal%20sampling%20for%20least-squares%20approximation%0AAuthor%3A%20Ben%20Adcock%0AAbstract%3A%20%20%20Least-squares%20approximation%20is%20one%20of%20the%20most%20important%20methods%20for%0Arecovering%20an%20unknown%20function%20from%20data.%20While%20in%20many%20applications%20the%20data%0Ais%20fixed%2C%20in%20many%20others%20there%20is%20substantial%20freedom%20to%20choose%20where%20to%0Asample.%20In%20this%20paper%2C%20we%20review%20recent%20progress%20on%20near-optimal%20random%0Asampling%20strategies%20for%20%28weighted%29%20least-squares%20approximation%20in%20arbitrary%0Alinear%20spaces.%20We%20introduce%20the%20Christoffel%20function%20as%20a%20key%20quantity%20in%20the%0Aanalysis%20of%20%28weighted%29%20least-squares%20approximation%20from%20random%20samples%2C%20then%0Ashow%20how%20it%20can%20be%20used%20to%20construct%20a%20random%20sampling%20strategy%2C%20termed%0AChristoffel%20sampling%2C%20that%20possesses%20near-optimal%20sample%20complexity%3A%20namely%2C%0Athe%20number%20of%20samples%20scales%20log-linearly%20in%20the%20dimension%20of%20the%20approximation%0Aspace%20%24n%24.%20We%20discuss%20a%20series%20of%20variations%2C%20extensions%20and%20further%20topics%2C%0Aand%20throughout%20highlight%20connections%20to%20approximation%20theory%2C%20machine%20learning%2C%0Ainformation-based%20complexity%20and%20numerical%20linear%20algebra.%20Finally%2C%20motivated%0Aby%20various%20contemporary%20applications%2C%20we%20consider%20a%20generalization%20of%20the%0Aclassical%20setting%20where%20the%20samples%20need%20not%20be%20pointwise%20samples%20of%20a%0Ascalar-valued%20function%2C%20and%20the%20approximation%20space%20need%20not%20be%20linear.%20We%20show%0Athat%2C%20even%20in%20this%20significantly%20more%20general%20setting%2C%20suitable%20generalizations%0Aof%20Christoffel%20function%20still%20determine%20the%20sample%20complexity.%20Consequently%2C%0Athese%20can%20be%20used%20to%20design%20enhanced%2C%20Christoffel%20sampling%20strategies%20in%20a%0Aunified%20way%20for%20general%20recovery%20problems.%20This%20article%20is%20largely%0Aself-contained%2C%20and%20intended%20to%20be%20accessible%20to%20nonspecialists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520sampling%2520for%2520least-squares%2520approximation%26entry.906535625%3DBen%2520Adcock%26entry.1292438233%3D%2520%2520Least-squares%2520approximation%2520is%2520one%2520of%2520the%2520most%2520important%2520methods%2520for%250Arecovering%2520an%2520unknown%2520function%2520from%2520data.%2520While%2520in%2520many%2520applications%2520the%2520data%250Ais%2520fixed%252C%2520in%2520many%2520others%2520there%2520is%2520substantial%2520freedom%2520to%2520choose%2520where%2520to%250Asample.%2520In%2520this%2520paper%252C%2520we%2520review%2520recent%2520progress%2520on%2520near-optimal%2520random%250Asampling%2520strategies%2520for%2520%2528weighted%2529%2520least-squares%2520approximation%2520in%2520arbitrary%250Alinear%2520spaces.%2520We%2520introduce%2520the%2520Christoffel%2520function%2520as%2520a%2520key%2520quantity%2520in%2520the%250Aanalysis%2520of%2520%2528weighted%2529%2520least-squares%2520approximation%2520from%2520random%2520samples%252C%2520then%250Ashow%2520how%2520it%2520can%2520be%2520used%2520to%2520construct%2520a%2520random%2520sampling%2520strategy%252C%2520termed%250AChristoffel%2520sampling%252C%2520that%2520possesses%2520near-optimal%2520sample%2520complexity%253A%2520namely%252C%250Athe%2520number%2520of%2520samples%2520scales%2520log-linearly%2520in%2520the%2520dimension%2520of%2520the%2520approximation%250Aspace%2520%2524n%2524.%2520We%2520discuss%2520a%2520series%2520of%2520variations%252C%2520extensions%2520and%2520further%2520topics%252C%250Aand%2520throughout%2520highlight%2520connections%2520to%2520approximation%2520theory%252C%2520machine%2520learning%252C%250Ainformation-based%2520complexity%2520and%2520numerical%2520linear%2520algebra.%2520Finally%252C%2520motivated%250Aby%2520various%2520contemporary%2520applications%252C%2520we%2520consider%2520a%2520generalization%2520of%2520the%250Aclassical%2520setting%2520where%2520the%2520samples%2520need%2520not%2520be%2520pointwise%2520samples%2520of%2520a%250Ascalar-valued%2520function%252C%2520and%2520the%2520approximation%2520space%2520need%2520not%2520be%2520linear.%2520We%2520show%250Athat%252C%2520even%2520in%2520this%2520significantly%2520more%2520general%2520setting%252C%2520suitable%2520generalizations%250Aof%2520Christoffel%2520function%2520still%2520determine%2520the%2520sample%2520complexity.%2520Consequently%252C%250Athese%2520can%2520be%2520used%2520to%2520design%2520enhanced%252C%2520Christoffel%2520sampling%2520strategies%2520in%2520a%250Aunified%2520way%2520for%2520general%2520recovery%2520problems.%2520This%2520article%2520is%2520largely%250Aself-contained%252C%2520and%2520intended%2520to%2520be%2520accessible%2520to%2520nonspecialists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20sampling%20for%20least-squares%20approximation&entry.906535625=Ben%20Adcock&entry.1292438233=%20%20Least-squares%20approximation%20is%20one%20of%20the%20most%20important%20methods%20for%0Arecovering%20an%20unknown%20function%20from%20data.%20While%20in%20many%20applications%20the%20data%0Ais%20fixed%2C%20in%20many%20others%20there%20is%20substantial%20freedom%20to%20choose%20where%20to%0Asample.%20In%20this%20paper%2C%20we%20review%20recent%20progress%20on%20near-optimal%20random%0Asampling%20strategies%20for%20%28weighted%29%20least-squares%20approximation%20in%20arbitrary%0Alinear%20spaces.%20We%20introduce%20the%20Christoffel%20function%20as%20a%20key%20quantity%20in%20the%0Aanalysis%20of%20%28weighted%29%20least-squares%20approximation%20from%20random%20samples%2C%20then%0Ashow%20how%20it%20can%20be%20used%20to%20construct%20a%20random%20sampling%20strategy%2C%20termed%0AChristoffel%20sampling%2C%20that%20possesses%20near-optimal%20sample%20complexity%3A%20namely%2C%0Athe%20number%20of%20samples%20scales%20log-linearly%20in%20the%20dimension%20of%20the%20approximation%0Aspace%20%24n%24.%20We%20discuss%20a%20series%20of%20variations%2C%20extensions%20and%20further%20topics%2C%0Aand%20throughout%20highlight%20connections%20to%20approximation%20theory%2C%20machine%20learning%2C%0Ainformation-based%20complexity%20and%20numerical%20linear%20algebra.%20Finally%2C%20motivated%0Aby%20various%20contemporary%20applications%2C%20we%20consider%20a%20generalization%20of%20the%0Aclassical%20setting%20where%20the%20samples%20need%20not%20be%20pointwise%20samples%20of%20a%0Ascalar-valued%20function%2C%20and%20the%20approximation%20space%20need%20not%20be%20linear.%20We%20show%0Athat%2C%20even%20in%20this%20significantly%20more%20general%20setting%2C%20suitable%20generalizations%0Aof%20Christoffel%20function%20still%20determine%20the%20sample%20complexity.%20Consequently%2C%0Athese%20can%20be%20used%20to%20design%20enhanced%2C%20Christoffel%20sampling%20strategies%20in%20a%0Aunified%20way%20for%20general%20recovery%20problems.%20This%20article%20is%20largely%0Aself-contained%2C%20and%20intended%20to%20be%20accessible%20to%20nonspecialists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02342v2&entry.124074799=Read"},
{"title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "author": "Muhammad Hassam Ejaz and Muhammad Bilal and Usman Habib and Muhammad Attique and Tae-Sun Chung", "abstract": "  Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.\n", "link": "http://arxiv.org/abs/2507.01494v3", "date": "2025-08-08", "relevancy": 2.1244, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4296}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4293}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review&body=Title%3A%20Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review%0AAuthor%3A%20Muhammad%20Hassam%20Ejaz%20and%20Muhammad%20Bilal%20and%20Usman%20Habib%20and%20Muhammad%20Attique%20and%20Tae-Sun%20Chung%0AAbstract%3A%20%20%20Insect%20pests%20continue%20to%20bring%20a%20serious%20threat%20to%20crop%20yields%20around%20the%0Aworld%2C%20and%20traditional%20methods%20for%20monitoring%20them%20are%20often%20slow%2C%20manual%2C%20and%0Adifficult%20to%20scale.%20In%20recent%20years%2C%20deep%20learning%20has%20emerged%20as%20a%20powerful%0Asolution%2C%20with%20techniques%20like%20convolutional%20neural%20networks%20%28CNNs%29%2C%20vision%0Atransformers%20%28ViTs%29%2C%20and%20hybrid%20models%20gaining%20popularity%20for%20automating%20pest%0Adetection.%20This%20review%20looks%20at%2037%20carefully%20selected%20studies%20published%20between%0A2018%20and%202025%2C%20all%20focused%20on%20AI-based%20pest%20classification.%20The%20selected%0Aresearch%20is%20organized%20by%20crop%20type%2C%20pest%20species%2C%20model%20architecture%2C%20dataset%0Ausage%2C%20and%20key%20technical%20challenges.%20The%20early%20studies%20relied%20heavily%20on%20CNNs%0Abut%20latest%20work%20is%20shifting%20toward%20hybrid%20and%20transformer-based%20models%20that%0Adeliver%20higher%20accuracy%20and%20better%20contextual%20understanding.%20Still%2C%20challenges%0Alike%20imbalanced%20datasets%2C%20difficulty%20in%20detecting%20small%20pests%2C%20limited%0Ageneralizability%2C%20and%20deployment%20on%20edge%20devices%20remain%20significant%20hurdles.%0AOverall%2C%20this%20review%20offers%20a%20structured%20overview%20of%20the%20field%2C%20highlights%0Auseful%20datasets%2C%20and%20outlines%20the%20key%20challenges%20and%20future%20directions%20for%0AAI-based%20pest%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01494v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrop%2520Pest%2520Classification%2520Using%2520Deep%2520Learning%2520Techniques%253A%2520A%2520Review%26entry.906535625%3DMuhammad%2520Hassam%2520Ejaz%2520and%2520Muhammad%2520Bilal%2520and%2520Usman%2520Habib%2520and%2520Muhammad%2520Attique%2520and%2520Tae-Sun%2520Chung%26entry.1292438233%3D%2520%2520Insect%2520pests%2520continue%2520to%2520bring%2520a%2520serious%2520threat%2520to%2520crop%2520yields%2520around%2520the%250Aworld%252C%2520and%2520traditional%2520methods%2520for%2520monitoring%2520them%2520are%2520often%2520slow%252C%2520manual%252C%2520and%250Adifficult%2520to%2520scale.%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%250Asolution%252C%2520with%2520techniques%2520like%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520vision%250Atransformers%2520%2528ViTs%2529%252C%2520and%2520hybrid%2520models%2520gaining%2520popularity%2520for%2520automating%2520pest%250Adetection.%2520This%2520review%2520looks%2520at%252037%2520carefully%2520selected%2520studies%2520published%2520between%250A2018%2520and%25202025%252C%2520all%2520focused%2520on%2520AI-based%2520pest%2520classification.%2520The%2520selected%250Aresearch%2520is%2520organized%2520by%2520crop%2520type%252C%2520pest%2520species%252C%2520model%2520architecture%252C%2520dataset%250Ausage%252C%2520and%2520key%2520technical%2520challenges.%2520The%2520early%2520studies%2520relied%2520heavily%2520on%2520CNNs%250Abut%2520latest%2520work%2520is%2520shifting%2520toward%2520hybrid%2520and%2520transformer-based%2520models%2520that%250Adeliver%2520higher%2520accuracy%2520and%2520better%2520contextual%2520understanding.%2520Still%252C%2520challenges%250Alike%2520imbalanced%2520datasets%252C%2520difficulty%2520in%2520detecting%2520small%2520pests%252C%2520limited%250Ageneralizability%252C%2520and%2520deployment%2520on%2520edge%2520devices%2520remain%2520significant%2520hurdles.%250AOverall%252C%2520this%2520review%2520offers%2520a%2520structured%2520overview%2520of%2520the%2520field%252C%2520highlights%250Auseful%2520datasets%252C%2520and%2520outlines%2520the%2520key%2520challenges%2520and%2520future%2520directions%2520for%250AAI-based%2520pest%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01494v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review&entry.906535625=Muhammad%20Hassam%20Ejaz%20and%20Muhammad%20Bilal%20and%20Usman%20Habib%20and%20Muhammad%20Attique%20and%20Tae-Sun%20Chung&entry.1292438233=%20%20Insect%20pests%20continue%20to%20bring%20a%20serious%20threat%20to%20crop%20yields%20around%20the%0Aworld%2C%20and%20traditional%20methods%20for%20monitoring%20them%20are%20often%20slow%2C%20manual%2C%20and%0Adifficult%20to%20scale.%20In%20recent%20years%2C%20deep%20learning%20has%20emerged%20as%20a%20powerful%0Asolution%2C%20with%20techniques%20like%20convolutional%20neural%20networks%20%28CNNs%29%2C%20vision%0Atransformers%20%28ViTs%29%2C%20and%20hybrid%20models%20gaining%20popularity%20for%20automating%20pest%0Adetection.%20This%20review%20looks%20at%2037%20carefully%20selected%20studies%20published%20between%0A2018%20and%202025%2C%20all%20focused%20on%20AI-based%20pest%20classification.%20The%20selected%0Aresearch%20is%20organized%20by%20crop%20type%2C%20pest%20species%2C%20model%20architecture%2C%20dataset%0Ausage%2C%20and%20key%20technical%20challenges.%20The%20early%20studies%20relied%20heavily%20on%20CNNs%0Abut%20latest%20work%20is%20shifting%20toward%20hybrid%20and%20transformer-based%20models%20that%0Adeliver%20higher%20accuracy%20and%20better%20contextual%20understanding.%20Still%2C%20challenges%0Alike%20imbalanced%20datasets%2C%20difficulty%20in%20detecting%20small%20pests%2C%20limited%0Ageneralizability%2C%20and%20deployment%20on%20edge%20devices%20remain%20significant%20hurdles.%0AOverall%2C%20this%20review%20offers%20a%20structured%20overview%20of%20the%20field%2C%20highlights%0Auseful%20datasets%2C%20and%20outlines%20the%20key%20challenges%20and%20future%20directions%20for%0AAI-based%20pest%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01494v3&entry.124074799=Read"},
{"title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "author": "Constantin Ruhdorfer and Matteo Bortoletto and Victor Oei and Anna Penzkofer and Andreas Bulling", "abstract": "  We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating.\n", "link": "http://arxiv.org/abs/2508.06336v1", "date": "2025-08-08", "relevancy": 2.1177, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5547}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5209}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Partner%20Design%20Enables%20Robust%20Ad-hoc%20Teamwork&body=Title%3A%20Unsupervised%20Partner%20Design%20Enables%20Robust%20Ad-hoc%20Teamwork%0AAuthor%3A%20Constantin%20Ruhdorfer%20and%20Matteo%20Bortoletto%20and%20Victor%20Oei%20and%20Anna%20Penzkofer%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20introduce%20Unsupervised%20Partner%20Design%20%28UPD%29%20-%20a%20population-free%2C%0Amulti-agent%20reinforcement%20learning%20framework%20for%20robust%20ad-hoc%20teamwork%20that%0Aadaptively%20generates%20training%20partners%20without%20requiring%20pretrained%20partners%20or%0Amanual%20parameter%20tuning.%20UPD%20constructs%20diverse%20partners%20by%20stochastically%0Amixing%20an%20ego%20agent%27s%20policy%20with%20biased%20random%20behaviours%20and%20scores%20them%0Ausing%20a%20variance-based%20learnability%20metric%20that%20prioritises%20partners%20near%20the%0Aego%20agent%27s%20current%20learning%20frontier.%20We%20show%20that%20UPD%20can%20be%20integrated%20with%0Aunsupervised%20environment%20design%2C%20resulting%20in%20the%20first%20method%20enabling%20fully%0Aunsupervised%20curricula%20over%20both%20level%20and%20partner%20distributions%20in%20a%0Acooperative%20setting.%20Through%20extensive%20evaluations%20on%20Overcooked-AI%20and%20the%0AOvercooked%20Generalisation%20Challenge%2C%20we%20demonstrate%20that%20this%20dynamic%20partner%0Acurriculum%20is%20highly%20effective%3A%20UPD%20consistently%20outperforms%20both%0Apopulation-based%20and%20population-free%20baselines%20as%20well%20as%20ablations.%20In%20a%20user%0Astudy%2C%20we%20further%20show%20that%20UPD%20achieves%20higher%20returns%20than%20all%20baselines%20and%0Awas%20perceived%20as%20significantly%20more%20adaptive%2C%20more%20human-like%2C%20a%20better%0Acollaborator%2C%20and%20less%20frustrating.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Partner%2520Design%2520Enables%2520Robust%2520Ad-hoc%2520Teamwork%26entry.906535625%3DConstantin%2520Ruhdorfer%2520and%2520Matteo%2520Bortoletto%2520and%2520Victor%2520Oei%2520and%2520Anna%2520Penzkofer%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520introduce%2520Unsupervised%2520Partner%2520Design%2520%2528UPD%2529%2520-%2520a%2520population-free%252C%250Amulti-agent%2520reinforcement%2520learning%2520framework%2520for%2520robust%2520ad-hoc%2520teamwork%2520that%250Aadaptively%2520generates%2520training%2520partners%2520without%2520requiring%2520pretrained%2520partners%2520or%250Amanual%2520parameter%2520tuning.%2520UPD%2520constructs%2520diverse%2520partners%2520by%2520stochastically%250Amixing%2520an%2520ego%2520agent%2527s%2520policy%2520with%2520biased%2520random%2520behaviours%2520and%2520scores%2520them%250Ausing%2520a%2520variance-based%2520learnability%2520metric%2520that%2520prioritises%2520partners%2520near%2520the%250Aego%2520agent%2527s%2520current%2520learning%2520frontier.%2520We%2520show%2520that%2520UPD%2520can%2520be%2520integrated%2520with%250Aunsupervised%2520environment%2520design%252C%2520resulting%2520in%2520the%2520first%2520method%2520enabling%2520fully%250Aunsupervised%2520curricula%2520over%2520both%2520level%2520and%2520partner%2520distributions%2520in%2520a%250Acooperative%2520setting.%2520Through%2520extensive%2520evaluations%2520on%2520Overcooked-AI%2520and%2520the%250AOvercooked%2520Generalisation%2520Challenge%252C%2520we%2520demonstrate%2520that%2520this%2520dynamic%2520partner%250Acurriculum%2520is%2520highly%2520effective%253A%2520UPD%2520consistently%2520outperforms%2520both%250Apopulation-based%2520and%2520population-free%2520baselines%2520as%2520well%2520as%2520ablations.%2520In%2520a%2520user%250Astudy%252C%2520we%2520further%2520show%2520that%2520UPD%2520achieves%2520higher%2520returns%2520than%2520all%2520baselines%2520and%250Awas%2520perceived%2520as%2520significantly%2520more%2520adaptive%252C%2520more%2520human-like%252C%2520a%2520better%250Acollaborator%252C%2520and%2520less%2520frustrating.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Partner%20Design%20Enables%20Robust%20Ad-hoc%20Teamwork&entry.906535625=Constantin%20Ruhdorfer%20and%20Matteo%20Bortoletto%20and%20Victor%20Oei%20and%20Anna%20Penzkofer%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20introduce%20Unsupervised%20Partner%20Design%20%28UPD%29%20-%20a%20population-free%2C%0Amulti-agent%20reinforcement%20learning%20framework%20for%20robust%20ad-hoc%20teamwork%20that%0Aadaptively%20generates%20training%20partners%20without%20requiring%20pretrained%20partners%20or%0Amanual%20parameter%20tuning.%20UPD%20constructs%20diverse%20partners%20by%20stochastically%0Amixing%20an%20ego%20agent%27s%20policy%20with%20biased%20random%20behaviours%20and%20scores%20them%0Ausing%20a%20variance-based%20learnability%20metric%20that%20prioritises%20partners%20near%20the%0Aego%20agent%27s%20current%20learning%20frontier.%20We%20show%20that%20UPD%20can%20be%20integrated%20with%0Aunsupervised%20environment%20design%2C%20resulting%20in%20the%20first%20method%20enabling%20fully%0Aunsupervised%20curricula%20over%20both%20level%20and%20partner%20distributions%20in%20a%0Acooperative%20setting.%20Through%20extensive%20evaluations%20on%20Overcooked-AI%20and%20the%0AOvercooked%20Generalisation%20Challenge%2C%20we%20demonstrate%20that%20this%20dynamic%20partner%0Acurriculum%20is%20highly%20effective%3A%20UPD%20consistently%20outperforms%20both%0Apopulation-based%20and%20population-free%20baselines%20as%20well%20as%20ablations.%20In%20a%20user%0Astudy%2C%20we%20further%20show%20that%20UPD%20achieves%20higher%20returns%20than%20all%20baselines%20and%0Awas%20perceived%20as%20significantly%20more%20adaptive%2C%20more%20human-like%2C%20a%20better%0Acollaborator%2C%20and%20less%20frustrating.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06336v1&entry.124074799=Read"},
{"title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution", "author": "Zailong Tian and Zhuoheng Han and Yanzhe Chen and Haozhe Xu and Xi Yang and richeng xuan and Hongfeng Wang and Lizi Liao", "abstract": "  Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.\n", "link": "http://arxiv.org/abs/2508.06225v1", "date": "2025-08-08", "relevancy": 2.1157, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5481}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&body=Title%3A%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution%0AAuthor%3A%20Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20richeng%20xuan%20and%20Hongfeng%20Wang%20and%20Lizi%20Liao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20%2A%2AOverconfidence%20Phenomenon%2A%2A%20in%0Acurrent%20LLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%0Aactual%20correctness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%0Aquantify%20this%20phenomenon%2C%20we%20introduce%20%2A%2ATH-Score%2A%2A%2C%20a%20novel%20metric%20measuring%0Aconfidence-accuracy%20alignment.%20Furthermore%2C%20we%20propose%20%2A%2ALLM-as-a-Fuser%2A%2A%2C%20an%0Aensemble%20framework%20that%20transforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20substantially%20improves%0Acalibration%20and%20enables%20adaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%0Aachieving%20superior%20reliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverconfidence%2520in%2520LLM-as-a-Judge%253A%2520Diagnosis%2520and%2520Confidence-Driven%250A%2520%2520Solution%26entry.906535625%3DZailong%2520Tian%2520and%2520Zhuoheng%2520Han%2520and%2520Yanzhe%2520Chen%2520and%2520Haozhe%2520Xu%2520and%2520Xi%2520Yang%2520and%2520richeng%2520xuan%2520and%2520Hongfeng%2520Wang%2520and%2520Lizi%2520Liao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520as%2520automated%2520judges%252C%2520where%250Apractical%2520value%2520depends%2520on%2520both%2520accuracy%2520and%2520trustworthy%252C%2520risk-aware%2520judgments.%250AExisting%2520approaches%2520predominantly%2520focus%2520on%2520accuracy%252C%2520overlooking%2520the%2520necessity%250Aof%2520well-calibrated%2520confidence%252C%2520which%2520is%2520vital%2520for%2520adaptive%2520and%2520reliable%250Aevaluation%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520advocate%2520a%2520shift%2520from%2520accuracy-centric%250Aevaluation%2520to%2520confidence-driven%252C%2520risk-aware%2520LLM-as-a-Judge%2520systems%252C%2520emphasizing%250Athe%2520necessity%2520of%2520well-calibrated%2520confidence%2520for%2520trustworthy%2520and%2520adaptive%250Aevaluation.%2520We%2520systematically%2520identify%2520the%2520%252A%252AOverconfidence%2520Phenomenon%252A%252A%2520in%250Acurrent%2520LLM-as-a-Judges%252C%2520where%2520predicted%2520confidence%2520significantly%2520overstates%250Aactual%2520correctness%252C%2520undermining%2520reliability%2520in%2520practical%2520deployment.%2520To%250Aquantify%2520this%2520phenomenon%252C%2520we%2520introduce%2520%252A%252ATH-Score%252A%252A%252C%2520a%2520novel%2520metric%2520measuring%250Aconfidence-accuracy%2520alignment.%2520Furthermore%252C%2520we%2520propose%2520%252A%252ALLM-as-a-Fuser%252A%252A%252C%2520an%250Aensemble%2520framework%2520that%2520transforms%2520LLMs%2520into%2520reliable%252C%2520risk-aware%2520evaluators.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520substantially%2520improves%250Acalibration%2520and%2520enables%2520adaptive%252C%2520confidence-driven%2520evaluation%2520pipelines%252C%250Aachieving%2520superior%2520reliability%2520and%2520accuracy%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&entry.906535625=Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20richeng%20xuan%20and%20Hongfeng%20Wang%20and%20Lizi%20Liao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20%2A%2AOverconfidence%20Phenomenon%2A%2A%20in%0Acurrent%20LLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%0Aactual%20correctness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%0Aquantify%20this%20phenomenon%2C%20we%20introduce%20%2A%2ATH-Score%2A%2A%2C%20a%20novel%20metric%20measuring%0Aconfidence-accuracy%20alignment.%20Furthermore%2C%20we%20propose%20%2A%2ALLM-as-a-Fuser%2A%2A%2C%20an%0Aensemble%20framework%20that%20transforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20substantially%20improves%0Acalibration%20and%20enables%20adaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%0Aachieving%20superior%20reliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06225v1&entry.124074799=Read"},
{"title": "Identity Increases Stability in Neural Cellular Automata", "author": "James Stovold", "abstract": "  Neural Cellular Automata (NCAs) offer a way to study the growth of\ntwo-dimensional artificial organisms from a single seed cell. From the outset,\nNCA-grown organisms have had issues with stability, their natural boundary\noften breaking down and exhibiting tumour-like growth or failing to maintain\nthe expected shape. In this paper, we present a method for improving the\nstability of NCA-grown organisms by introducing an 'identity' layer with simple\nconstraints during training.\n  Results show that NCAs grown in close proximity are more stable compared with\nthe original NCA model. Moreover, only a single identity value is required to\nachieve this increase in stability. We observe emergent movement from the\nstable organisms, with increasing prevalence for models with multiple identity\nvalues.\n  This work lays the foundation for further study of the interaction between\nNCA-grown organisms, paving the way for studying social interaction at a\ncellular level in artificial organisms.\n", "link": "http://arxiv.org/abs/2508.06389v1", "date": "2025-08-08", "relevancy": 2.1154, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4669}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.403}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity%20Increases%20Stability%20in%20Neural%20Cellular%20Automata&body=Title%3A%20Identity%20Increases%20Stability%20in%20Neural%20Cellular%20Automata%0AAuthor%3A%20James%20Stovold%0AAbstract%3A%20%20%20Neural%20Cellular%20Automata%20%28NCAs%29%20offer%20a%20way%20to%20study%20the%20growth%20of%0Atwo-dimensional%20artificial%20organisms%20from%20a%20single%20seed%20cell.%20From%20the%20outset%2C%0ANCA-grown%20organisms%20have%20had%20issues%20with%20stability%2C%20their%20natural%20boundary%0Aoften%20breaking%20down%20and%20exhibiting%20tumour-like%20growth%20or%20failing%20to%20maintain%0Athe%20expected%20shape.%20In%20this%20paper%2C%20we%20present%20a%20method%20for%20improving%20the%0Astability%20of%20NCA-grown%20organisms%20by%20introducing%20an%20%27identity%27%20layer%20with%20simple%0Aconstraints%20during%20training.%0A%20%20Results%20show%20that%20NCAs%20grown%20in%20close%20proximity%20are%20more%20stable%20compared%20with%0Athe%20original%20NCA%20model.%20Moreover%2C%20only%20a%20single%20identity%20value%20is%20required%20to%0Aachieve%20this%20increase%20in%20stability.%20We%20observe%20emergent%20movement%20from%20the%0Astable%20organisms%2C%20with%20increasing%20prevalence%20for%20models%20with%20multiple%20identity%0Avalues.%0A%20%20This%20work%20lays%20the%20foundation%20for%20further%20study%20of%20the%20interaction%20between%0ANCA-grown%20organisms%2C%20paving%20the%20way%20for%20studying%20social%20interaction%20at%20a%0Acellular%20level%20in%20artificial%20organisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity%2520Increases%2520Stability%2520in%2520Neural%2520Cellular%2520Automata%26entry.906535625%3DJames%2520Stovold%26entry.1292438233%3D%2520%2520Neural%2520Cellular%2520Automata%2520%2528NCAs%2529%2520offer%2520a%2520way%2520to%2520study%2520the%2520growth%2520of%250Atwo-dimensional%2520artificial%2520organisms%2520from%2520a%2520single%2520seed%2520cell.%2520From%2520the%2520outset%252C%250ANCA-grown%2520organisms%2520have%2520had%2520issues%2520with%2520stability%252C%2520their%2520natural%2520boundary%250Aoften%2520breaking%2520down%2520and%2520exhibiting%2520tumour-like%2520growth%2520or%2520failing%2520to%2520maintain%250Athe%2520expected%2520shape.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520for%2520improving%2520the%250Astability%2520of%2520NCA-grown%2520organisms%2520by%2520introducing%2520an%2520%2527identity%2527%2520layer%2520with%2520simple%250Aconstraints%2520during%2520training.%250A%2520%2520Results%2520show%2520that%2520NCAs%2520grown%2520in%2520close%2520proximity%2520are%2520more%2520stable%2520compared%2520with%250Athe%2520original%2520NCA%2520model.%2520Moreover%252C%2520only%2520a%2520single%2520identity%2520value%2520is%2520required%2520to%250Aachieve%2520this%2520increase%2520in%2520stability.%2520We%2520observe%2520emergent%2520movement%2520from%2520the%250Astable%2520organisms%252C%2520with%2520increasing%2520prevalence%2520for%2520models%2520with%2520multiple%2520identity%250Avalues.%250A%2520%2520This%2520work%2520lays%2520the%2520foundation%2520for%2520further%2520study%2520of%2520the%2520interaction%2520between%250ANCA-grown%2520organisms%252C%2520paving%2520the%2520way%2520for%2520studying%2520social%2520interaction%2520at%2520a%250Acellular%2520level%2520in%2520artificial%2520organisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Increases%20Stability%20in%20Neural%20Cellular%20Automata&entry.906535625=James%20Stovold&entry.1292438233=%20%20Neural%20Cellular%20Automata%20%28NCAs%29%20offer%20a%20way%20to%20study%20the%20growth%20of%0Atwo-dimensional%20artificial%20organisms%20from%20a%20single%20seed%20cell.%20From%20the%20outset%2C%0ANCA-grown%20organisms%20have%20had%20issues%20with%20stability%2C%20their%20natural%20boundary%0Aoften%20breaking%20down%20and%20exhibiting%20tumour-like%20growth%20or%20failing%20to%20maintain%0Athe%20expected%20shape.%20In%20this%20paper%2C%20we%20present%20a%20method%20for%20improving%20the%0Astability%20of%20NCA-grown%20organisms%20by%20introducing%20an%20%27identity%27%20layer%20with%20simple%0Aconstraints%20during%20training.%0A%20%20Results%20show%20that%20NCAs%20grown%20in%20close%20proximity%20are%20more%20stable%20compared%20with%0Athe%20original%20NCA%20model.%20Moreover%2C%20only%20a%20single%20identity%20value%20is%20required%20to%0Aachieve%20this%20increase%20in%20stability.%20We%20observe%20emergent%20movement%20from%20the%0Astable%20organisms%2C%20with%20increasing%20prevalence%20for%20models%20with%20multiple%20identity%0Avalues.%0A%20%20This%20work%20lays%20the%20foundation%20for%20further%20study%20of%20the%20interaction%20between%0ANCA-grown%20organisms%2C%20paving%20the%20way%20for%20studying%20social%20interaction%20at%20a%0Acellular%20level%20in%20artificial%20organisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06389v1&entry.124074799=Read"},
{"title": "Deepfake Detection that Generalizes Across Benchmarks", "author": "Andrii Yermakov and Jan Cech and Jiri Matas and Mario Fritz", "abstract": "  The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.\n", "link": "http://arxiv.org/abs/2508.06248v1", "date": "2025-08-08", "relevancy": 2.1128, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5275}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Detection%20that%20Generalizes%20Across%20Benchmarks&body=Title%3A%20Deepfake%20Detection%20that%20Generalizes%20Across%20Benchmarks%0AAuthor%3A%20Andrii%20Yermakov%20and%20Jan%20Cech%20and%20Jiri%20Matas%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20The%20generalization%20of%20deepfake%20detectors%20to%20unseen%20manipulation%20techniques%0Aremains%20a%20challenge%20for%20practical%20deployment.%20Although%20many%20approaches%20adapt%0Afoundation%20models%20by%20introducing%20significant%20architectural%20complexity%2C%20this%0Awork%20demonstrates%20that%20robust%20generalization%20is%20achievable%20through%20a%0Aparameter-efficient%20adaptation%20of%20a%20pre-trained%20CLIP%20vision%20encoder.%20The%0Aproposed%20method%2C%20LNCLIP-DF%2C%20fine-tunes%20only%20the%20Layer%20Normalization%20parameters%0A%280.03%25%20of%20the%20total%29%20and%20enhances%20generalization%20by%20enforcing%20a%20hyperspherical%0Afeature%20manifold%20using%20L2%20normalization%20and%20latent%20space%20augmentations.%0A%20%20We%20conducted%20an%20extensive%20evaluation%20on%2013%20benchmark%20datasets%20spanning%20from%0A2019%20to%202025.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20more%20complex%2C%20recent%20approaches%20in%20average%20cross-dataset%20AUROC.%0AOur%20analysis%20yields%20two%20primary%20findings%20for%20the%20field%3A%201%29%20training%20on%20paired%0Areal-fake%20data%20from%20the%20same%20source%20video%20is%20essential%20for%20mitigating%20shortcut%0Alearning%20and%20improving%20generalization%2C%20and%202%29%20detection%20difficulty%20on%20academic%0Adatasets%20has%20not%20strictly%20increased%20over%20time%2C%20with%20models%20trained%20on%20older%2C%0Adiverse%20datasets%20showing%20strong%20generalization%20capabilities.%0A%20%20This%20work%20delivers%20a%20computationally%20efficient%20and%20reproducible%20method%2C%0Aproving%20that%20state-of-the-art%20generalization%20is%20attainable%20by%20making%20targeted%2C%0Aminimal%20changes%20to%20a%20pre-trained%20CLIP%20model.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepfake%2520Detection%2520that%2520Generalizes%2520Across%2520Benchmarks%26entry.906535625%3DAndrii%2520Yermakov%2520and%2520Jan%2520Cech%2520and%2520Jiri%2520Matas%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520The%2520generalization%2520of%2520deepfake%2520detectors%2520to%2520unseen%2520manipulation%2520techniques%250Aremains%2520a%2520challenge%2520for%2520practical%2520deployment.%2520Although%2520many%2520approaches%2520adapt%250Afoundation%2520models%2520by%2520introducing%2520significant%2520architectural%2520complexity%252C%2520this%250Awork%2520demonstrates%2520that%2520robust%2520generalization%2520is%2520achievable%2520through%2520a%250Aparameter-efficient%2520adaptation%2520of%2520a%2520pre-trained%2520CLIP%2520vision%2520encoder.%2520The%250Aproposed%2520method%252C%2520LNCLIP-DF%252C%2520fine-tunes%2520only%2520the%2520Layer%2520Normalization%2520parameters%250A%25280.03%2525%2520of%2520the%2520total%2529%2520and%2520enhances%2520generalization%2520by%2520enforcing%2520a%2520hyperspherical%250Afeature%2520manifold%2520using%2520L2%2520normalization%2520and%2520latent%2520space%2520augmentations.%250A%2520%2520We%2520conducted%2520an%2520extensive%2520evaluation%2520on%252013%2520benchmark%2520datasets%2520spanning%2520from%250A2019%2520to%25202025.%2520The%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%252C%250Aoutperforming%2520more%2520complex%252C%2520recent%2520approaches%2520in%2520average%2520cross-dataset%2520AUROC.%250AOur%2520analysis%2520yields%2520two%2520primary%2520findings%2520for%2520the%2520field%253A%25201%2529%2520training%2520on%2520paired%250Areal-fake%2520data%2520from%2520the%2520same%2520source%2520video%2520is%2520essential%2520for%2520mitigating%2520shortcut%250Alearning%2520and%2520improving%2520generalization%252C%2520and%25202%2529%2520detection%2520difficulty%2520on%2520academic%250Adatasets%2520has%2520not%2520strictly%2520increased%2520over%2520time%252C%2520with%2520models%2520trained%2520on%2520older%252C%250Adiverse%2520datasets%2520showing%2520strong%2520generalization%2520capabilities.%250A%2520%2520This%2520work%2520delivers%2520a%2520computationally%2520efficient%2520and%2520reproducible%2520method%252C%250Aproving%2520that%2520state-of-the-art%2520generalization%2520is%2520attainable%2520by%2520making%2520targeted%252C%250Aminimal%2520changes%2520to%2520a%2520pre-trained%2520CLIP%2520model.%2520The%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Detection%20that%20Generalizes%20Across%20Benchmarks&entry.906535625=Andrii%20Yermakov%20and%20Jan%20Cech%20and%20Jiri%20Matas%20and%20Mario%20Fritz&entry.1292438233=%20%20The%20generalization%20of%20deepfake%20detectors%20to%20unseen%20manipulation%20techniques%0Aremains%20a%20challenge%20for%20practical%20deployment.%20Although%20many%20approaches%20adapt%0Afoundation%20models%20by%20introducing%20significant%20architectural%20complexity%2C%20this%0Awork%20demonstrates%20that%20robust%20generalization%20is%20achievable%20through%20a%0Aparameter-efficient%20adaptation%20of%20a%20pre-trained%20CLIP%20vision%20encoder.%20The%0Aproposed%20method%2C%20LNCLIP-DF%2C%20fine-tunes%20only%20the%20Layer%20Normalization%20parameters%0A%280.03%25%20of%20the%20total%29%20and%20enhances%20generalization%20by%20enforcing%20a%20hyperspherical%0Afeature%20manifold%20using%20L2%20normalization%20and%20latent%20space%20augmentations.%0A%20%20We%20conducted%20an%20extensive%20evaluation%20on%2013%20benchmark%20datasets%20spanning%20from%0A2019%20to%202025.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20more%20complex%2C%20recent%20approaches%20in%20average%20cross-dataset%20AUROC.%0AOur%20analysis%20yields%20two%20primary%20findings%20for%20the%20field%3A%201%29%20training%20on%20paired%0Areal-fake%20data%20from%20the%20same%20source%20video%20is%20essential%20for%20mitigating%20shortcut%0Alearning%20and%20improving%20generalization%2C%20and%202%29%20detection%20difficulty%20on%20academic%0Adatasets%20has%20not%20strictly%20increased%20over%20time%2C%20with%20models%20trained%20on%20older%2C%0Adiverse%20datasets%20showing%20strong%20generalization%20capabilities.%0A%20%20This%20work%20delivers%20a%20computationally%20efficient%20and%20reproducible%20method%2C%0Aproving%20that%20state-of-the-art%20generalization%20is%20attainable%20by%20making%20targeted%2C%0Aminimal%20changes%20to%20a%20pre-trained%20CLIP%20model.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06248v1&entry.124074799=Read"},
{"title": "Event2Vec: Processing neuromorphic events directly by representations in\n  vector space", "author": "Wei Fang and Priyadarshini Panda", "abstract": "  The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec.\n", "link": "http://arxiv.org/abs/2504.15371v2", "date": "2025-08-08", "relevancy": 2.085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event2Vec%3A%20Processing%20neuromorphic%20events%20directly%20by%20representations%20in%0A%20%20vector%20space&body=Title%3A%20Event2Vec%3A%20Processing%20neuromorphic%20events%20directly%20by%20representations%20in%0A%20%20vector%20space%0AAuthor%3A%20Wei%20Fang%20and%20Priyadarshini%20Panda%0AAbstract%3A%20%20%20The%20neuromorphic%20event%20cameras%20have%20overwhelming%20advantages%20in%20temporal%0Aresolution%2C%20power%20efficiency%2C%20and%20dynamic%20range%20compared%20to%20traditional%0Acameras.%20However%2C%20the%20event%20cameras%20output%20asynchronous%2C%20sparse%2C%20and%20irregular%0Aevents%2C%20which%20are%20not%20compatible%20with%20mainstream%20computer%20vision%20and%20deep%0Alearning%20methods.%20Various%20methods%20have%20been%20proposed%20to%20solve%20this%20issue%20but%20at%0Athe%20cost%20of%20long%20preprocessing%20procedures%2C%20losing%20temporal%20resolutions%2C%20or%0Abeing%20incompatible%20with%20massively%20parallel%20computation.%20Inspired%20by%20the%20great%0Asuccess%20of%20the%20word%20to%20vector%2C%20we%20summarize%20the%20similarities%20between%20words%20and%0Aevents%2C%20then%20propose%20the%20first%20event%20to%20vector%20%28event2vec%29%20representation.%20We%0Avalidate%20event2vec%20on%20classifying%20the%20ASL-DVS%20dataset%2C%20showing%20impressive%0Aparameter%20efficiency%2C%20accuracy%2C%20and%20speed%20than%20previous%20graph/image/voxel-based%0Arepresentations.%20Beyond%20task%20performance%2C%20the%20most%20attractive%20advantage%20of%0Aevent2vec%20is%20that%20it%20aligns%20events%20to%20the%20domain%20of%20natural%20language%0Aprocessing%2C%20showing%20the%20promising%20prospect%20of%20integrating%20events%20into%20large%0Alanguage%20and%20multimodal%20models.%20Our%20codes%2C%20models%2C%20and%20training%20logs%20are%0Aavailable%20at%20https%3A//github.com/fangwei123456/event2vec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent2Vec%253A%2520Processing%2520neuromorphic%2520events%2520directly%2520by%2520representations%2520in%250A%2520%2520vector%2520space%26entry.906535625%3DWei%2520Fang%2520and%2520Priyadarshini%2520Panda%26entry.1292438233%3D%2520%2520The%2520neuromorphic%2520event%2520cameras%2520have%2520overwhelming%2520advantages%2520in%2520temporal%250Aresolution%252C%2520power%2520efficiency%252C%2520and%2520dynamic%2520range%2520compared%2520to%2520traditional%250Acameras.%2520However%252C%2520the%2520event%2520cameras%2520output%2520asynchronous%252C%2520sparse%252C%2520and%2520irregular%250Aevents%252C%2520which%2520are%2520not%2520compatible%2520with%2520mainstream%2520computer%2520vision%2520and%2520deep%250Alearning%2520methods.%2520Various%2520methods%2520have%2520been%2520proposed%2520to%2520solve%2520this%2520issue%2520but%2520at%250Athe%2520cost%2520of%2520long%2520preprocessing%2520procedures%252C%2520losing%2520temporal%2520resolutions%252C%2520or%250Abeing%2520incompatible%2520with%2520massively%2520parallel%2520computation.%2520Inspired%2520by%2520the%2520great%250Asuccess%2520of%2520the%2520word%2520to%2520vector%252C%2520we%2520summarize%2520the%2520similarities%2520between%2520words%2520and%250Aevents%252C%2520then%2520propose%2520the%2520first%2520event%2520to%2520vector%2520%2528event2vec%2529%2520representation.%2520We%250Avalidate%2520event2vec%2520on%2520classifying%2520the%2520ASL-DVS%2520dataset%252C%2520showing%2520impressive%250Aparameter%2520efficiency%252C%2520accuracy%252C%2520and%2520speed%2520than%2520previous%2520graph/image/voxel-based%250Arepresentations.%2520Beyond%2520task%2520performance%252C%2520the%2520most%2520attractive%2520advantage%2520of%250Aevent2vec%2520is%2520that%2520it%2520aligns%2520events%2520to%2520the%2520domain%2520of%2520natural%2520language%250Aprocessing%252C%2520showing%2520the%2520promising%2520prospect%2520of%2520integrating%2520events%2520into%2520large%250Alanguage%2520and%2520multimodal%2520models.%2520Our%2520codes%252C%2520models%252C%2520and%2520training%2520logs%2520are%250Aavailable%2520at%2520https%253A//github.com/fangwei123456/event2vec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event2Vec%3A%20Processing%20neuromorphic%20events%20directly%20by%20representations%20in%0A%20%20vector%20space&entry.906535625=Wei%20Fang%20and%20Priyadarshini%20Panda&entry.1292438233=%20%20The%20neuromorphic%20event%20cameras%20have%20overwhelming%20advantages%20in%20temporal%0Aresolution%2C%20power%20efficiency%2C%20and%20dynamic%20range%20compared%20to%20traditional%0Acameras.%20However%2C%20the%20event%20cameras%20output%20asynchronous%2C%20sparse%2C%20and%20irregular%0Aevents%2C%20which%20are%20not%20compatible%20with%20mainstream%20computer%20vision%20and%20deep%0Alearning%20methods.%20Various%20methods%20have%20been%20proposed%20to%20solve%20this%20issue%20but%20at%0Athe%20cost%20of%20long%20preprocessing%20procedures%2C%20losing%20temporal%20resolutions%2C%20or%0Abeing%20incompatible%20with%20massively%20parallel%20computation.%20Inspired%20by%20the%20great%0Asuccess%20of%20the%20word%20to%20vector%2C%20we%20summarize%20the%20similarities%20between%20words%20and%0Aevents%2C%20then%20propose%20the%20first%20event%20to%20vector%20%28event2vec%29%20representation.%20We%0Avalidate%20event2vec%20on%20classifying%20the%20ASL-DVS%20dataset%2C%20showing%20impressive%0Aparameter%20efficiency%2C%20accuracy%2C%20and%20speed%20than%20previous%20graph/image/voxel-based%0Arepresentations.%20Beyond%20task%20performance%2C%20the%20most%20attractive%20advantage%20of%0Aevent2vec%20is%20that%20it%20aligns%20events%20to%20the%20domain%20of%20natural%20language%0Aprocessing%2C%20showing%20the%20promising%20prospect%20of%20integrating%20events%20into%20large%0Alanguage%20and%20multimodal%20models.%20Our%20codes%2C%20models%2C%20and%20training%20logs%20are%0Aavailable%20at%20https%3A//github.com/fangwei123456/event2vec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15371v2&entry.124074799=Read"},
{"title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph\n  Question Answering", "author": "Yanbin Wei and Jiangyue Yan and Chun Kang and Yang Chen and Hua Liu and James T. Kwok and Yu Zhang", "abstract": "  Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy\n", "link": "http://arxiv.org/abs/2508.06345v1", "date": "2025-08-08", "relevancy": 2.0844, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Adaptive%20Topology%20Representations%20for%20Zero-Shot%20Graph%0A%20%20Question%20Answering&body=Title%3A%20Harnessing%20Adaptive%20Topology%20Representations%20for%20Zero-Shot%20Graph%0A%20%20Question%20Answering%0AAuthor%3A%20Yanbin%20Wei%20and%20Jiangyue%20Yan%20and%20Chun%20Kang%20and%20Yang%20Chen%20and%20Hua%20Liu%20and%20James%20T.%20Kwok%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20generalized%20zero-shot%20capabilities%0Ain%20diverse%20domain%20question-answering%20%28QA%29%20tasks%2C%20including%20graph%20QA%20that%0Ainvolves%20complex%20graph%20topologies.%20However%2C%20most%20current%20approaches%20use%20only%20a%0Asingle%20type%20of%20graph%20representation%2C%20namely%20Topology%20Representation%20Form%20%28TRF%29%2C%0Asuch%20as%20prompt-unified%20text%20descriptions%20or%20style-fixed%20visual%20styles.%20Those%0A%22one-size-fits-all%22%20approaches%20fail%20to%20consider%20the%20specific%20preferences%20of%0Adifferent%20models%20or%20tasks%2C%20often%20leading%20to%20incorrect%20or%20overly%20long%20responses.%0ATo%20address%20this%2C%20we%20first%20analyze%20the%20characteristics%20and%20weaknesses%20of%0Aexisting%20TRFs%2C%20and%20then%20design%20a%20set%20of%20TRFs%2C%20denoted%20by%20%24F_%7BZS%7D%24%2C%20tailored%20to%0Azero-shot%20graph%20QA.%20We%20then%20introduce%20a%20new%20metric%2C%20Graph%20Response%20Efficiency%0A%28GRE%29%2C%20which%20measures%20the%20balance%20between%20the%20performance%20and%20the%20brevity%20in%0Agraph%20QA.%20Built%20on%20these%2C%20we%20develop%20the%20DynamicTRF%20framework%2C%20which%20aims%20to%0Aimprove%20both%20the%20accuracy%20and%20conciseness%20of%20graph%20QA.%20To%20be%20specific%2C%0ADynamicTRF%20first%20creates%20a%20TRF%20Preference%20%28TRFP%29%20dataset%20that%20ranks%20TRFs%20based%0Aon%20their%20GRE%20scores%2C%20to%20probe%20the%20question-specific%20TRF%20preferences.%20Then%20it%0Atrains%20a%20TRF%20router%20on%20the%20TRFP%20dataset%2C%20to%20adaptively%20assign%20the%20best%20TRF%20from%0A%24F_%7BZS%7D%24%20for%20each%20question%20during%20the%20inference.%20Extensive%20experiments%20across%207%0Ain-domain%20algorithmic%20graph%20QA%20tasks%20and%202%20out-of-domain%20downstream%20tasks%20show%0Athat%20DynamicTRF%20significantly%20enhances%20the%20zero-shot%20graph%20QA%20of%20LMMs%20in%20terms%0Aof%20accuracy%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Adaptive%2520Topology%2520Representations%2520for%2520Zero-Shot%2520Graph%250A%2520%2520Question%2520Answering%26entry.906535625%3DYanbin%2520Wei%2520and%2520Jiangyue%2520Yan%2520and%2520Chun%2520Kang%2520and%2520Yang%2520Chen%2520and%2520Hua%2520Liu%2520and%2520James%2520T.%2520Kwok%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520shown%2520generalized%2520zero-shot%2520capabilities%250Ain%2520diverse%2520domain%2520question-answering%2520%2528QA%2529%2520tasks%252C%2520including%2520graph%2520QA%2520that%250Ainvolves%2520complex%2520graph%2520topologies.%2520However%252C%2520most%2520current%2520approaches%2520use%2520only%2520a%250Asingle%2520type%2520of%2520graph%2520representation%252C%2520namely%2520Topology%2520Representation%2520Form%2520%2528TRF%2529%252C%250Asuch%2520as%2520prompt-unified%2520text%2520descriptions%2520or%2520style-fixed%2520visual%2520styles.%2520Those%250A%2522one-size-fits-all%2522%2520approaches%2520fail%2520to%2520consider%2520the%2520specific%2520preferences%2520of%250Adifferent%2520models%2520or%2520tasks%252C%2520often%2520leading%2520to%2520incorrect%2520or%2520overly%2520long%2520responses.%250ATo%2520address%2520this%252C%2520we%2520first%2520analyze%2520the%2520characteristics%2520and%2520weaknesses%2520of%250Aexisting%2520TRFs%252C%2520and%2520then%2520design%2520a%2520set%2520of%2520TRFs%252C%2520denoted%2520by%2520%2524F_%257BZS%257D%2524%252C%2520tailored%2520to%250Azero-shot%2520graph%2520QA.%2520We%2520then%2520introduce%2520a%2520new%2520metric%252C%2520Graph%2520Response%2520Efficiency%250A%2528GRE%2529%252C%2520which%2520measures%2520the%2520balance%2520between%2520the%2520performance%2520and%2520the%2520brevity%2520in%250Agraph%2520QA.%2520Built%2520on%2520these%252C%2520we%2520develop%2520the%2520DynamicTRF%2520framework%252C%2520which%2520aims%2520to%250Aimprove%2520both%2520the%2520accuracy%2520and%2520conciseness%2520of%2520graph%2520QA.%2520To%2520be%2520specific%252C%250ADynamicTRF%2520first%2520creates%2520a%2520TRF%2520Preference%2520%2528TRFP%2529%2520dataset%2520that%2520ranks%2520TRFs%2520based%250Aon%2520their%2520GRE%2520scores%252C%2520to%2520probe%2520the%2520question-specific%2520TRF%2520preferences.%2520Then%2520it%250Atrains%2520a%2520TRF%2520router%2520on%2520the%2520TRFP%2520dataset%252C%2520to%2520adaptively%2520assign%2520the%2520best%2520TRF%2520from%250A%2524F_%257BZS%257D%2524%2520for%2520each%2520question%2520during%2520the%2520inference.%2520Extensive%2520experiments%2520across%25207%250Ain-domain%2520algorithmic%2520graph%2520QA%2520tasks%2520and%25202%2520out-of-domain%2520downstream%2520tasks%2520show%250Athat%2520DynamicTRF%2520significantly%2520enhances%2520the%2520zero-shot%2520graph%2520QA%2520of%2520LMMs%2520in%2520terms%250Aof%2520accuracy%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Adaptive%20Topology%20Representations%20for%20Zero-Shot%20Graph%0A%20%20Question%20Answering&entry.906535625=Yanbin%20Wei%20and%20Jiangyue%20Yan%20and%20Chun%20Kang%20and%20Yang%20Chen%20and%20Hua%20Liu%20and%20James%20T.%20Kwok%20and%20Yu%20Zhang&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20generalized%20zero-shot%20capabilities%0Ain%20diverse%20domain%20question-answering%20%28QA%29%20tasks%2C%20including%20graph%20QA%20that%0Ainvolves%20complex%20graph%20topologies.%20However%2C%20most%20current%20approaches%20use%20only%20a%0Asingle%20type%20of%20graph%20representation%2C%20namely%20Topology%20Representation%20Form%20%28TRF%29%2C%0Asuch%20as%20prompt-unified%20text%20descriptions%20or%20style-fixed%20visual%20styles.%20Those%0A%22one-size-fits-all%22%20approaches%20fail%20to%20consider%20the%20specific%20preferences%20of%0Adifferent%20models%20or%20tasks%2C%20often%20leading%20to%20incorrect%20or%20overly%20long%20responses.%0ATo%20address%20this%2C%20we%20first%20analyze%20the%20characteristics%20and%20weaknesses%20of%0Aexisting%20TRFs%2C%20and%20then%20design%20a%20set%20of%20TRFs%2C%20denoted%20by%20%24F_%7BZS%7D%24%2C%20tailored%20to%0Azero-shot%20graph%20QA.%20We%20then%20introduce%20a%20new%20metric%2C%20Graph%20Response%20Efficiency%0A%28GRE%29%2C%20which%20measures%20the%20balance%20between%20the%20performance%20and%20the%20brevity%20in%0Agraph%20QA.%20Built%20on%20these%2C%20we%20develop%20the%20DynamicTRF%20framework%2C%20which%20aims%20to%0Aimprove%20both%20the%20accuracy%20and%20conciseness%20of%20graph%20QA.%20To%20be%20specific%2C%0ADynamicTRF%20first%20creates%20a%20TRF%20Preference%20%28TRFP%29%20dataset%20that%20ranks%20TRFs%20based%0Aon%20their%20GRE%20scores%2C%20to%20probe%20the%20question-specific%20TRF%20preferences.%20Then%20it%0Atrains%20a%20TRF%20router%20on%20the%20TRFP%20dataset%2C%20to%20adaptively%20assign%20the%20best%20TRF%20from%0A%24F_%7BZS%7D%24%20for%20each%20question%20during%20the%20inference.%20Extensive%20experiments%20across%207%0Ain-domain%20algorithmic%20graph%20QA%20tasks%20and%202%20out-of-domain%20downstream%20tasks%20show%0Athat%20DynamicTRF%20significantly%20enhances%20the%20zero-shot%20graph%20QA%20of%20LMMs%20in%20terms%0Aof%20accuracy%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06345v1&entry.124074799=Read"},
{"title": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking", "author": "Peihan Li and Jiazhen Liu and Yuwei Wu and Lifeng Zhou", "abstract": "  Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community.\n", "link": "http://arxiv.org/abs/2508.02529v2", "date": "2025-08-08", "relevancy": 2.0825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5429}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Failure-Aware%20Multi-Robot%20Coordination%20for%20Resilient%20and%20Adaptive%20Target%0A%20%20Tracking&body=Title%3A%20Failure-Aware%20Multi-Robot%20Coordination%20for%20Resilient%20and%20Adaptive%20Target%0A%20%20Tracking%0AAuthor%3A%20Peihan%20Li%20and%20Jiazhen%20Liu%20and%20Yuwei%20Wu%20and%20Lifeng%20Zhou%0AAbstract%3A%20%20%20Multi-robot%20coordination%20is%20crucial%20for%20autonomous%20systems%2C%20yet%20real-world%0Adeployments%20often%20encounter%20various%20failures.%20These%20include%20both%20temporary%20and%0Apermanent%20disruptions%20in%20sensing%20and%20communication%2C%20which%20can%20significantly%0Adegrade%20system%20robustness%20and%20performance%20if%20not%20explicitly%20modeled.%20Despite%0Aits%20practical%20importance%2C%20failure-aware%20coordination%20remains%20underexplored%20in%0Athe%20literature.%20To%20bridge%20the%20gap%20between%20idealized%20conditions%20and%20the%0Acomplexities%20of%20real-world%20environments%2C%20we%20propose%20a%20unified%20failure-aware%0Acoordination%20framework%20designed%20to%20enable%20resilient%20and%20adaptive%20multi-robot%0Atarget%20tracking%20under%20both%20temporary%20and%20permanent%20failure%20conditions.%20Our%0Aapproach%20systematically%20distinguishes%20between%20two%20classes%20of%20failures%3A%20%281%29%0Aprobabilistic%20and%20temporary%20disruptions%2C%20where%20robots%20recover%20from%20intermittent%0Asensing%20or%20communication%20losses%20by%20dynamically%20adapting%20paths%20and%20avoiding%0Ainferred%20danger%20zones%2C%20and%20%282%29%20permanent%20failures%2C%20where%20robots%20lose%20sensing%20or%0Acommunication%20capabilities%20irreversibly%2C%20requiring%20sustained%2C%20decentralized%0Abehavioral%20adaptation.%20To%20handle%20these%20scenarios%2C%20the%20robot%20team%20is%20partitioned%0Ainto%20subgroups.%20Robots%20that%20remain%20connected%20form%20a%20communication%20group%20and%0Acollaboratively%20plan%20using%20partially%20centralized%20nonlinear%20optimization.%20Robots%0Aexperiencing%20permanent%20disconnection%20or%20failure%20continue%20to%20operate%0Aindependently%20through%20decentralized%20or%20individual%20optimization%2C%20allowing%20them%0Ato%20contribute%20to%20the%20task%20within%20their%20local%20context.%20We%20extensively%20evaluate%0Aour%20method%20across%20a%20range%20of%20benchmark%20variations%20and%20conduct%20a%20comprehensive%0Aassessment%20under%20diverse%20real-world%20failure%20scenarios.%20Results%20show%20that%20our%0Aframework%20consistently%20achieves%20robust%20performance%20in%20realistic%20environments%0Awith%20unknown%20danger%20zones%2C%20offering%20a%20practical%20and%20generalizable%20solution%20for%0Athe%20multi-robot%20systems%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFailure-Aware%2520Multi-Robot%2520Coordination%2520for%2520Resilient%2520and%2520Adaptive%2520Target%250A%2520%2520Tracking%26entry.906535625%3DPeihan%2520Li%2520and%2520Jiazhen%2520Liu%2520and%2520Yuwei%2520Wu%2520and%2520Lifeng%2520Zhou%26entry.1292438233%3D%2520%2520Multi-robot%2520coordination%2520is%2520crucial%2520for%2520autonomous%2520systems%252C%2520yet%2520real-world%250Adeployments%2520often%2520encounter%2520various%2520failures.%2520These%2520include%2520both%2520temporary%2520and%250Apermanent%2520disruptions%2520in%2520sensing%2520and%2520communication%252C%2520which%2520can%2520significantly%250Adegrade%2520system%2520robustness%2520and%2520performance%2520if%2520not%2520explicitly%2520modeled.%2520Despite%250Aits%2520practical%2520importance%252C%2520failure-aware%2520coordination%2520remains%2520underexplored%2520in%250Athe%2520literature.%2520To%2520bridge%2520the%2520gap%2520between%2520idealized%2520conditions%2520and%2520the%250Acomplexities%2520of%2520real-world%2520environments%252C%2520we%2520propose%2520a%2520unified%2520failure-aware%250Acoordination%2520framework%2520designed%2520to%2520enable%2520resilient%2520and%2520adaptive%2520multi-robot%250Atarget%2520tracking%2520under%2520both%2520temporary%2520and%2520permanent%2520failure%2520conditions.%2520Our%250Aapproach%2520systematically%2520distinguishes%2520between%2520two%2520classes%2520of%2520failures%253A%2520%25281%2529%250Aprobabilistic%2520and%2520temporary%2520disruptions%252C%2520where%2520robots%2520recover%2520from%2520intermittent%250Asensing%2520or%2520communication%2520losses%2520by%2520dynamically%2520adapting%2520paths%2520and%2520avoiding%250Ainferred%2520danger%2520zones%252C%2520and%2520%25282%2529%2520permanent%2520failures%252C%2520where%2520robots%2520lose%2520sensing%2520or%250Acommunication%2520capabilities%2520irreversibly%252C%2520requiring%2520sustained%252C%2520decentralized%250Abehavioral%2520adaptation.%2520To%2520handle%2520these%2520scenarios%252C%2520the%2520robot%2520team%2520is%2520partitioned%250Ainto%2520subgroups.%2520Robots%2520that%2520remain%2520connected%2520form%2520a%2520communication%2520group%2520and%250Acollaboratively%2520plan%2520using%2520partially%2520centralized%2520nonlinear%2520optimization.%2520Robots%250Aexperiencing%2520permanent%2520disconnection%2520or%2520failure%2520continue%2520to%2520operate%250Aindependently%2520through%2520decentralized%2520or%2520individual%2520optimization%252C%2520allowing%2520them%250Ato%2520contribute%2520to%2520the%2520task%2520within%2520their%2520local%2520context.%2520We%2520extensively%2520evaluate%250Aour%2520method%2520across%2520a%2520range%2520of%2520benchmark%2520variations%2520and%2520conduct%2520a%2520comprehensive%250Aassessment%2520under%2520diverse%2520real-world%2520failure%2520scenarios.%2520Results%2520show%2520that%2520our%250Aframework%2520consistently%2520achieves%2520robust%2520performance%2520in%2520realistic%2520environments%250Awith%2520unknown%2520danger%2520zones%252C%2520offering%2520a%2520practical%2520and%2520generalizable%2520solution%2520for%250Athe%2520multi-robot%2520systems%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Failure-Aware%20Multi-Robot%20Coordination%20for%20Resilient%20and%20Adaptive%20Target%0A%20%20Tracking&entry.906535625=Peihan%20Li%20and%20Jiazhen%20Liu%20and%20Yuwei%20Wu%20and%20Lifeng%20Zhou&entry.1292438233=%20%20Multi-robot%20coordination%20is%20crucial%20for%20autonomous%20systems%2C%20yet%20real-world%0Adeployments%20often%20encounter%20various%20failures.%20These%20include%20both%20temporary%20and%0Apermanent%20disruptions%20in%20sensing%20and%20communication%2C%20which%20can%20significantly%0Adegrade%20system%20robustness%20and%20performance%20if%20not%20explicitly%20modeled.%20Despite%0Aits%20practical%20importance%2C%20failure-aware%20coordination%20remains%20underexplored%20in%0Athe%20literature.%20To%20bridge%20the%20gap%20between%20idealized%20conditions%20and%20the%0Acomplexities%20of%20real-world%20environments%2C%20we%20propose%20a%20unified%20failure-aware%0Acoordination%20framework%20designed%20to%20enable%20resilient%20and%20adaptive%20multi-robot%0Atarget%20tracking%20under%20both%20temporary%20and%20permanent%20failure%20conditions.%20Our%0Aapproach%20systematically%20distinguishes%20between%20two%20classes%20of%20failures%3A%20%281%29%0Aprobabilistic%20and%20temporary%20disruptions%2C%20where%20robots%20recover%20from%20intermittent%0Asensing%20or%20communication%20losses%20by%20dynamically%20adapting%20paths%20and%20avoiding%0Ainferred%20danger%20zones%2C%20and%20%282%29%20permanent%20failures%2C%20where%20robots%20lose%20sensing%20or%0Acommunication%20capabilities%20irreversibly%2C%20requiring%20sustained%2C%20decentralized%0Abehavioral%20adaptation.%20To%20handle%20these%20scenarios%2C%20the%20robot%20team%20is%20partitioned%0Ainto%20subgroups.%20Robots%20that%20remain%20connected%20form%20a%20communication%20group%20and%0Acollaboratively%20plan%20using%20partially%20centralized%20nonlinear%20optimization.%20Robots%0Aexperiencing%20permanent%20disconnection%20or%20failure%20continue%20to%20operate%0Aindependently%20through%20decentralized%20or%20individual%20optimization%2C%20allowing%20them%0Ato%20contribute%20to%20the%20task%20within%20their%20local%20context.%20We%20extensively%20evaluate%0Aour%20method%20across%20a%20range%20of%20benchmark%20variations%20and%20conduct%20a%20comprehensive%0Aassessment%20under%20diverse%20real-world%20failure%20scenarios.%20Results%20show%20that%20our%0Aframework%20consistently%20achieves%20robust%20performance%20in%20realistic%20environments%0Awith%20unknown%20danger%20zones%2C%20offering%20a%20practical%20and%20generalizable%20solution%20for%0Athe%20multi-robot%20systems%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02529v2&entry.124074799=Read"},
{"title": "ATM: Improving Model Merging by Alternating Tuning and Merging", "author": "Luca Zhou and Daniele Solombrino and Donato Crisostomi and Maria Sofia Bucarelli and Fabrizio Silvestri and Emanuele Rodol\u00e0", "abstract": "  Model merging has emerged as a cost-efficient approximation to multitask\nlearning. Among merging strategies, task arithmetic is notable for its\nsimplicity and effectiveness. In this work, we provide a theoretical motivation\nfor task vectors by highlighting that, under single-epoch full-batch gradient\ndescent, they are equivalent to multitask gradients. This insight leads us to\nreinterpret model merging as a single step in an iterative procedure that\nAlternates between Tuning and Merging (ATM). We propose two applications of\nATM: (1) as an alternative to multitask learning in scenarios where data\nsharing is restricted (e.g., federated settings), and (2) as a lightweight\nrefinement step to improve existing model merging methods using a small\nvalidation set. Experiments across diverse vision tasks demonstrate the\neffectiveness of ATM.\n", "link": "http://arxiv.org/abs/2411.03055v4", "date": "2025-08-08", "relevancy": 2.069, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&body=Title%3A%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging%0AAuthor%3A%20Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Model%20merging%20has%20emerged%20as%20a%20cost-efficient%20approximation%20to%20multitask%0Alearning.%20Among%20merging%20strategies%2C%20task%20arithmetic%20is%20notable%20for%20its%0Asimplicity%20and%20effectiveness.%20In%20this%20work%2C%20we%20provide%20a%20theoretical%20motivation%0Afor%20task%20vectors%20by%20highlighting%20that%2C%20under%20single-epoch%20full-batch%20gradient%0Adescent%2C%20they%20are%20equivalent%20to%20multitask%20gradients.%20This%20insight%20leads%20us%20to%0Areinterpret%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%20procedure%20that%0AAlternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20We%20propose%20two%20applications%20of%0AATM%3A%20%281%29%20as%20an%20alternative%20to%20multitask%20learning%20in%20scenarios%20where%20data%0Asharing%20is%20restricted%20%28e.g.%2C%20federated%20settings%29%2C%20and%20%282%29%20as%20a%20lightweight%0Arefinement%20step%20to%20improve%20existing%20model%20merging%20methods%20using%20a%20small%0Avalidation%20set.%20Experiments%20across%20diverse%20vision%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20ATM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03055v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATM%253A%2520Improving%2520Model%2520Merging%2520by%2520Alternating%2520Tuning%2520and%2520Merging%26entry.906535625%3DLuca%2520Zhou%2520and%2520Daniele%2520Solombrino%2520and%2520Donato%2520Crisostomi%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Fabrizio%2520Silvestri%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520emerged%2520as%2520a%2520cost-efficient%2520approximation%2520to%2520multitask%250Alearning.%2520Among%2520merging%2520strategies%252C%2520task%2520arithmetic%2520is%2520notable%2520for%2520its%250Asimplicity%2520and%2520effectiveness.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520theoretical%2520motivation%250Afor%2520task%2520vectors%2520by%2520highlighting%2520that%252C%2520under%2520single-epoch%2520full-batch%2520gradient%250Adescent%252C%2520they%2520are%2520equivalent%2520to%2520multitask%2520gradients.%2520This%2520insight%2520leads%2520us%2520to%250Areinterpret%2520model%2520merging%2520as%2520a%2520single%2520step%2520in%2520an%2520iterative%2520procedure%2520that%250AAlternates%2520between%2520Tuning%2520and%2520Merging%2520%2528ATM%2529.%2520We%2520propose%2520two%2520applications%2520of%250AATM%253A%2520%25281%2529%2520as%2520an%2520alternative%2520to%2520multitask%2520learning%2520in%2520scenarios%2520where%2520data%250Asharing%2520is%2520restricted%2520%2528e.g.%252C%2520federated%2520settings%2529%252C%2520and%2520%25282%2529%2520as%2520a%2520lightweight%250Arefinement%2520step%2520to%2520improve%2520existing%2520model%2520merging%2520methods%2520using%2520a%2520small%250Avalidation%2520set.%2520Experiments%2520across%2520diverse%2520vision%2520tasks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520ATM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03055v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&entry.906535625=Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Model%20merging%20has%20emerged%20as%20a%20cost-efficient%20approximation%20to%20multitask%0Alearning.%20Among%20merging%20strategies%2C%20task%20arithmetic%20is%20notable%20for%20its%0Asimplicity%20and%20effectiveness.%20In%20this%20work%2C%20we%20provide%20a%20theoretical%20motivation%0Afor%20task%20vectors%20by%20highlighting%20that%2C%20under%20single-epoch%20full-batch%20gradient%0Adescent%2C%20they%20are%20equivalent%20to%20multitask%20gradients.%20This%20insight%20leads%20us%20to%0Areinterpret%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%20procedure%20that%0AAlternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20We%20propose%20two%20applications%20of%0AATM%3A%20%281%29%20as%20an%20alternative%20to%20multitask%20learning%20in%20scenarios%20where%20data%0Asharing%20is%20restricted%20%28e.g.%2C%20federated%20settings%29%2C%20and%20%282%29%20as%20a%20lightweight%0Arefinement%20step%20to%20improve%20existing%20model%20merging%20methods%20using%20a%20small%0Avalidation%20set.%20Experiments%20across%20diverse%20vision%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20ATM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03055v4&entry.124074799=Read"},
{"title": "DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning", "author": "Jucheng Hu and Surong Yang and Lijun Wu and Dongzhan Zhou", "abstract": "  Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieves superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the whole dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability. Code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2504.14810v2", "date": "2025-08-08", "relevancy": 2.0639, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DONOD%3A%20Efficient%20and%20Generalizable%20Instruction%20Fine-Tuning%20for%20LLMs%20via%0A%20%20Model-Intrinsic%20Dataset%20Pruning&body=Title%3A%20DONOD%3A%20Efficient%20and%20Generalizable%20Instruction%20Fine-Tuning%20for%20LLMs%20via%0A%20%20Model-Intrinsic%20Dataset%20Pruning%0AAuthor%3A%20Jucheng%20Hu%20and%20Surong%20Yang%20and%20Lijun%20Wu%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Ad-hoc%20instruction%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20is%20widely%0Aadopted%20for%20domain-specific%20adaptation.%20While%20domain-specific%20supervised%0Afine-tuning%20%28SFT%29%20is%20effective%20and%20efficient%2C%20it%20often%20weakens%20cross-domain%0Ageneralization%20and%20struggles%20with%20noisy%20training%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20DONOD%2C%20a%20lightweight%20model-intrinsic%20data%20pruning%0Amethod.%20Our%20approach%20evaluates%20data%20using%20two%20model-parameter-based%20metrics%3A%0ADelta%20of%20Norm%20%28DON%29%2C%20which%20captures%20the%20cumulative%20influence%20on%20model%20weights%2C%0Aand%20Norm%20of%20Delta%20%28NOD%29%2C%20which%20quantifies%20weight%20instability.%20Moreover%2C%20by%0Aemploying%20the%20Technique%20for%20Order%20of%20Preference%20by%20Similarity%20to%20Ideal%20Solution%0A%28TOPSIS%29%20algorithm%2C%20we%20effectively%20filter%20noisy%2C%20unlearnable%2C%20and%0Ageneralization-harming%20samples%20without%20relying%20on%20auxiliary%20models%20during%20the%0ASFT%20process.%20Experiments%20on%20mathematical%20tasks%20demonstrate%20that%20data%20selected%0Aby%20DONOD%20achieves%20superior%20fine-tuning%20efficiency%20and%20improved%20robustness%0Aagainst%20noisy%20data.%20By%20filtering%20out%2070%25%20of%20the%20whole%20dataset%2C%20we%20improve%0Atarget-domain%20accuracy%20by%2014.90%25%20and%20cross-domain%20accuracy%20by%205.67%25.%20Meanwhile%2C%0Aour%20selected%20data%20present%20superior%20cross-architecture%20generalization.%20Data%0Apruned%20by%20smaller%20models%20%28e.g.%2C%20Llama%203.1-8B%29%20generalize%20effectively%20on%20larger%0Amodels%20%28e.g.%2C%20Llama%202-13B%29.%20Compared%20to%20existing%20related%20methodologies%2C%20DONOD%0Ademonstrates%20comparable%20or%20superior%20performance%20while%20remaining%0Adataset-agnostic%2C%20enabling%20broader%20applicability.%20Code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDONOD%253A%2520Efficient%2520and%2520Generalizable%2520Instruction%2520Fine-Tuning%2520for%2520LLMs%2520via%250A%2520%2520Model-Intrinsic%2520Dataset%2520Pruning%26entry.906535625%3DJucheng%2520Hu%2520and%2520Surong%2520Yang%2520and%2520Lijun%2520Wu%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Ad-hoc%2520instruction%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520widely%250Aadopted%2520for%2520domain-specific%2520adaptation.%2520While%2520domain-specific%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520is%2520effective%2520and%2520efficient%252C%2520it%2520often%2520weakens%2520cross-domain%250Ageneralization%2520and%2520struggles%2520with%2520noisy%2520training%2520data.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520DONOD%252C%2520a%2520lightweight%2520model-intrinsic%2520data%2520pruning%250Amethod.%2520Our%2520approach%2520evaluates%2520data%2520using%2520two%2520model-parameter-based%2520metrics%253A%250ADelta%2520of%2520Norm%2520%2528DON%2529%252C%2520which%2520captures%2520the%2520cumulative%2520influence%2520on%2520model%2520weights%252C%250Aand%2520Norm%2520of%2520Delta%2520%2528NOD%2529%252C%2520which%2520quantifies%2520weight%2520instability.%2520Moreover%252C%2520by%250Aemploying%2520the%2520Technique%2520for%2520Order%2520of%2520Preference%2520by%2520Similarity%2520to%2520Ideal%2520Solution%250A%2528TOPSIS%2529%2520algorithm%252C%2520we%2520effectively%2520filter%2520noisy%252C%2520unlearnable%252C%2520and%250Ageneralization-harming%2520samples%2520without%2520relying%2520on%2520auxiliary%2520models%2520during%2520the%250ASFT%2520process.%2520Experiments%2520on%2520mathematical%2520tasks%2520demonstrate%2520that%2520data%2520selected%250Aby%2520DONOD%2520achieves%2520superior%2520fine-tuning%2520efficiency%2520and%2520improved%2520robustness%250Aagainst%2520noisy%2520data.%2520By%2520filtering%2520out%252070%2525%2520of%2520the%2520whole%2520dataset%252C%2520we%2520improve%250Atarget-domain%2520accuracy%2520by%252014.90%2525%2520and%2520cross-domain%2520accuracy%2520by%25205.67%2525.%2520Meanwhile%252C%250Aour%2520selected%2520data%2520present%2520superior%2520cross-architecture%2520generalization.%2520Data%250Apruned%2520by%2520smaller%2520models%2520%2528e.g.%252C%2520Llama%25203.1-8B%2529%2520generalize%2520effectively%2520on%2520larger%250Amodels%2520%2528e.g.%252C%2520Llama%25202-13B%2529.%2520Compared%2520to%2520existing%2520related%2520methodologies%252C%2520DONOD%250Ademonstrates%2520comparable%2520or%2520superior%2520performance%2520while%2520remaining%250Adataset-agnostic%252C%2520enabling%2520broader%2520applicability.%2520Code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DONOD%3A%20Efficient%20and%20Generalizable%20Instruction%20Fine-Tuning%20for%20LLMs%20via%0A%20%20Model-Intrinsic%20Dataset%20Pruning&entry.906535625=Jucheng%20Hu%20and%20Surong%20Yang%20and%20Lijun%20Wu%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Ad-hoc%20instruction%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20is%20widely%0Aadopted%20for%20domain-specific%20adaptation.%20While%20domain-specific%20supervised%0Afine-tuning%20%28SFT%29%20is%20effective%20and%20efficient%2C%20it%20often%20weakens%20cross-domain%0Ageneralization%20and%20struggles%20with%20noisy%20training%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20DONOD%2C%20a%20lightweight%20model-intrinsic%20data%20pruning%0Amethod.%20Our%20approach%20evaluates%20data%20using%20two%20model-parameter-based%20metrics%3A%0ADelta%20of%20Norm%20%28DON%29%2C%20which%20captures%20the%20cumulative%20influence%20on%20model%20weights%2C%0Aand%20Norm%20of%20Delta%20%28NOD%29%2C%20which%20quantifies%20weight%20instability.%20Moreover%2C%20by%0Aemploying%20the%20Technique%20for%20Order%20of%20Preference%20by%20Similarity%20to%20Ideal%20Solution%0A%28TOPSIS%29%20algorithm%2C%20we%20effectively%20filter%20noisy%2C%20unlearnable%2C%20and%0Ageneralization-harming%20samples%20without%20relying%20on%20auxiliary%20models%20during%20the%0ASFT%20process.%20Experiments%20on%20mathematical%20tasks%20demonstrate%20that%20data%20selected%0Aby%20DONOD%20achieves%20superior%20fine-tuning%20efficiency%20and%20improved%20robustness%0Aagainst%20noisy%20data.%20By%20filtering%20out%2070%25%20of%20the%20whole%20dataset%2C%20we%20improve%0Atarget-domain%20accuracy%20by%2014.90%25%20and%20cross-domain%20accuracy%20by%205.67%25.%20Meanwhile%2C%0Aour%20selected%20data%20present%20superior%20cross-architecture%20generalization.%20Data%0Apruned%20by%20smaller%20models%20%28e.g.%2C%20Llama%203.1-8B%29%20generalize%20effectively%20on%20larger%0Amodels%20%28e.g.%2C%20Llama%202-13B%29.%20Compared%20to%20existing%20related%20methodologies%2C%20DONOD%0Ademonstrates%20comparable%20or%20superior%20performance%20while%20remaining%0Adataset-agnostic%2C%20enabling%20broader%20applicability.%20Code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14810v2&entry.124074799=Read"},
{"title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise\n  Injection", "author": "Ameya Anjarlekar and Sandeep Pombra", "abstract": "  The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.\n", "link": "http://arxiv.org/abs/2508.06467v1", "date": "2025-08-08", "relevancy": 2.0605, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5424}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5154}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Unlearning%20using%20Gradient%20Ratio-Based%20Influence%20Estimation%20and%20Noise%0A%20%20Injection&body=Title%3A%20LLM%20Unlearning%20using%20Gradient%20Ratio-Based%20Influence%20Estimation%20and%20Noise%0A%20%20Injection%0AAuthor%3A%20Ameya%20Anjarlekar%20and%20Sandeep%20Pombra%0AAbstract%3A%20%20%20The%20growing%20legal%20and%20ethical%20scrutiny%20of%20large%20language%20models%20%28LLMs%29%0Anecessitates%20effective%20machine%20unlearning%2C%20particularly%20for%20sensitive%20or%0Aunauthorized%20data.%20Existing%20empirical%20methods%20often%20yield%20incomplete%20forgetting%0Aor%20unintended%20degradation%20of%20unrelated%20knowledge%20due%20to%20poor%20localization.%20In%0Athis%20work%2C%20we%20propose%20GRIN%3A%20a%20modular%20and%20targeted%20framework%20for%20LLM%0Aunlearning.%20GRIN%20introduces%20a%20novel%20gradient-ratio-based%20metric%20to%20identify%0Aparameters%20most%20responsible%20for%20memorizing%20forget%20data.%20We%20then%20perform%0Aselective%20noise%20injection%20into%20these%20parameters%20prior%20to%20fine-tuning%2C%20which%0Aimproves%20unlearning%20performance%20while%20maintaining%20model%20utility.%20Finally%2C%20we%0Apropose%20new%20evaluation%20metrics%20tailored%20to%20the%20LLM%20setting%20and%20validate%20our%0Aapproach%20on%20standard%20benchmarks%20such%20as%20TOFU%2C%20WMDP%2C%20and%20SafePKU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Unlearning%2520using%2520Gradient%2520Ratio-Based%2520Influence%2520Estimation%2520and%2520Noise%250A%2520%2520Injection%26entry.906535625%3DAmeya%2520Anjarlekar%2520and%2520Sandeep%2520Pombra%26entry.1292438233%3D%2520%2520The%2520growing%2520legal%2520and%2520ethical%2520scrutiny%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Anecessitates%2520effective%2520machine%2520unlearning%252C%2520particularly%2520for%2520sensitive%2520or%250Aunauthorized%2520data.%2520Existing%2520empirical%2520methods%2520often%2520yield%2520incomplete%2520forgetting%250Aor%2520unintended%2520degradation%2520of%2520unrelated%2520knowledge%2520due%2520to%2520poor%2520localization.%2520In%250Athis%2520work%252C%2520we%2520propose%2520GRIN%253A%2520a%2520modular%2520and%2520targeted%2520framework%2520for%2520LLM%250Aunlearning.%2520GRIN%2520introduces%2520a%2520novel%2520gradient-ratio-based%2520metric%2520to%2520identify%250Aparameters%2520most%2520responsible%2520for%2520memorizing%2520forget%2520data.%2520We%2520then%2520perform%250Aselective%2520noise%2520injection%2520into%2520these%2520parameters%2520prior%2520to%2520fine-tuning%252C%2520which%250Aimproves%2520unlearning%2520performance%2520while%2520maintaining%2520model%2520utility.%2520Finally%252C%2520we%250Apropose%2520new%2520evaluation%2520metrics%2520tailored%2520to%2520the%2520LLM%2520setting%2520and%2520validate%2520our%250Aapproach%2520on%2520standard%2520benchmarks%2520such%2520as%2520TOFU%252C%2520WMDP%252C%2520and%2520SafePKU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Unlearning%20using%20Gradient%20Ratio-Based%20Influence%20Estimation%20and%20Noise%0A%20%20Injection&entry.906535625=Ameya%20Anjarlekar%20and%20Sandeep%20Pombra&entry.1292438233=%20%20The%20growing%20legal%20and%20ethical%20scrutiny%20of%20large%20language%20models%20%28LLMs%29%0Anecessitates%20effective%20machine%20unlearning%2C%20particularly%20for%20sensitive%20or%0Aunauthorized%20data.%20Existing%20empirical%20methods%20often%20yield%20incomplete%20forgetting%0Aor%20unintended%20degradation%20of%20unrelated%20knowledge%20due%20to%20poor%20localization.%20In%0Athis%20work%2C%20we%20propose%20GRIN%3A%20a%20modular%20and%20targeted%20framework%20for%20LLM%0Aunlearning.%20GRIN%20introduces%20a%20novel%20gradient-ratio-based%20metric%20to%20identify%0Aparameters%20most%20responsible%20for%20memorizing%20forget%20data.%20We%20then%20perform%0Aselective%20noise%20injection%20into%20these%20parameters%20prior%20to%20fine-tuning%2C%20which%0Aimproves%20unlearning%20performance%20while%20maintaining%20model%20utility.%20Finally%2C%20we%0Apropose%20new%20evaluation%20metrics%20tailored%20to%20the%20LLM%20setting%20and%20validate%20our%0Aapproach%20on%20standard%20benchmarks%20such%20as%20TOFU%2C%20WMDP%2C%20and%20SafePKU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06467v1&entry.124074799=Read"},
{"title": "Decorrelated feature importance from local sample weighting", "author": "Benedikt Fr\u00f6hlich and Alison Durst and Merle Behr", "abstract": "  Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data.\n", "link": "http://arxiv.org/abs/2508.06337v1", "date": "2025-08-08", "relevancy": 2.0604, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decorrelated%20feature%20importance%20from%20local%20sample%20weighting&body=Title%3A%20Decorrelated%20feature%20importance%20from%20local%20sample%20weighting%0AAuthor%3A%20Benedikt%20Fr%C3%B6hlich%20and%20Alison%20Durst%20and%20Merle%20Behr%0AAbstract%3A%20%20%20Feature%20importance%20%28FI%29%20statistics%20provide%20a%20prominent%20and%20valuable%20method%20of%0Ainsight%20into%20the%20decision%20process%20of%20machine%20learning%20%28ML%29%20models%2C%20but%20their%0Aeffectiveness%20has%20well-known%20limitations%20when%20correlation%20is%20present%20among%20the%0Afeatures%20in%20the%20training%20data.%20In%20this%20case%2C%20the%20FI%20often%20tends%20to%20be%0Adistributed%20among%20all%20features%20which%20are%20in%20correlation%20with%20the%0Aresponse-generating%20signal%20features.%20Even%20worse%2C%20if%20multiple%20signal%20features%0Aare%20in%20strong%20correlation%20with%20a%20noise%20feature%2C%20while%20being%20only%20modestly%0Acorrelated%20with%20one%20another%2C%20this%20can%20result%20in%20a%20noise%20feature%20having%20a%0Adistinctly%20larger%20FI%20score%20than%20any%20signal%20feature.%20Here%20we%20propose%20local%0Asample%20weighting%20%28losaw%29%20which%20can%20flexibly%20be%20integrated%20into%20many%20ML%0Aalgorithms%20to%20improve%20FI%20scores%20in%20the%20presence%20of%20feature%20correlation%20in%20the%0Atraining%20data.%20Our%20approach%20is%20motivated%20from%20inverse%20probability%20weighting%20in%0Acausal%20inference%20and%20locally%2C%20within%20the%20ML%20model%2C%20uses%20a%20sample%20weighting%0Ascheme%20to%20decorrelate%20a%20target%20feature%20from%20the%20remaining%20features.%20This%0Areduces%20model%20bias%20locally%2C%20whenever%20the%20effect%20of%20a%20potential%20signal%20feature%0Ais%20evaluated%20and%20compared%20to%20others.%20Moreover%2C%20losaw%20comes%20with%20a%20natural%0Atuning%20parameter%2C%20the%20minimum%20effective%20sample%20size%20of%20the%20weighted%20population%2C%0Awhich%20corresponds%20to%20an%20interpretation-prediction-tradeoff%2C%20analog%20to%20a%0Abias-variance-tradeoff%20as%20for%20classical%20ML%20tuning%20parameters.%20We%20demonstrate%0Ahow%20losaw%20can%20be%20integrated%20within%20decision%20tree-based%20ML%20methods%20and%20within%0Amini-batch%20training%20of%20neural%20networks.%20We%20investigate%20losaw%20for%20random%20forest%0Aand%20convolutional%20neural%20networks%20in%20a%20simulation%20study%20on%20settings%20showing%0Adiverse%20correlation%20patterns.%20We%20found%20that%20losaw%20improves%20FI%20consistently.%0AMoreover%2C%20it%20often%20improves%20prediction%20accuracy%20for%20out-of-distribution%2C%20while%0Amaintaining%20a%20similar%20accuracy%20for%20in-distribution%20test%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecorrelated%2520feature%2520importance%2520from%2520local%2520sample%2520weighting%26entry.906535625%3DBenedikt%2520Fr%25C3%25B6hlich%2520and%2520Alison%2520Durst%2520and%2520Merle%2520Behr%26entry.1292438233%3D%2520%2520Feature%2520importance%2520%2528FI%2529%2520statistics%2520provide%2520a%2520prominent%2520and%2520valuable%2520method%2520of%250Ainsight%2520into%2520the%2520decision%2520process%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%252C%2520but%2520their%250Aeffectiveness%2520has%2520well-known%2520limitations%2520when%2520correlation%2520is%2520present%2520among%2520the%250Afeatures%2520in%2520the%2520training%2520data.%2520In%2520this%2520case%252C%2520the%2520FI%2520often%2520tends%2520to%2520be%250Adistributed%2520among%2520all%2520features%2520which%2520are%2520in%2520correlation%2520with%2520the%250Aresponse-generating%2520signal%2520features.%2520Even%2520worse%252C%2520if%2520multiple%2520signal%2520features%250Aare%2520in%2520strong%2520correlation%2520with%2520a%2520noise%2520feature%252C%2520while%2520being%2520only%2520modestly%250Acorrelated%2520with%2520one%2520another%252C%2520this%2520can%2520result%2520in%2520a%2520noise%2520feature%2520having%2520a%250Adistinctly%2520larger%2520FI%2520score%2520than%2520any%2520signal%2520feature.%2520Here%2520we%2520propose%2520local%250Asample%2520weighting%2520%2528losaw%2529%2520which%2520can%2520flexibly%2520be%2520integrated%2520into%2520many%2520ML%250Aalgorithms%2520to%2520improve%2520FI%2520scores%2520in%2520the%2520presence%2520of%2520feature%2520correlation%2520in%2520the%250Atraining%2520data.%2520Our%2520approach%2520is%2520motivated%2520from%2520inverse%2520probability%2520weighting%2520in%250Acausal%2520inference%2520and%2520locally%252C%2520within%2520the%2520ML%2520model%252C%2520uses%2520a%2520sample%2520weighting%250Ascheme%2520to%2520decorrelate%2520a%2520target%2520feature%2520from%2520the%2520remaining%2520features.%2520This%250Areduces%2520model%2520bias%2520locally%252C%2520whenever%2520the%2520effect%2520of%2520a%2520potential%2520signal%2520feature%250Ais%2520evaluated%2520and%2520compared%2520to%2520others.%2520Moreover%252C%2520losaw%2520comes%2520with%2520a%2520natural%250Atuning%2520parameter%252C%2520the%2520minimum%2520effective%2520sample%2520size%2520of%2520the%2520weighted%2520population%252C%250Awhich%2520corresponds%2520to%2520an%2520interpretation-prediction-tradeoff%252C%2520analog%2520to%2520a%250Abias-variance-tradeoff%2520as%2520for%2520classical%2520ML%2520tuning%2520parameters.%2520We%2520demonstrate%250Ahow%2520losaw%2520can%2520be%2520integrated%2520within%2520decision%2520tree-based%2520ML%2520methods%2520and%2520within%250Amini-batch%2520training%2520of%2520neural%2520networks.%2520We%2520investigate%2520losaw%2520for%2520random%2520forest%250Aand%2520convolutional%2520neural%2520networks%2520in%2520a%2520simulation%2520study%2520on%2520settings%2520showing%250Adiverse%2520correlation%2520patterns.%2520We%2520found%2520that%2520losaw%2520improves%2520FI%2520consistently.%250AMoreover%252C%2520it%2520often%2520improves%2520prediction%2520accuracy%2520for%2520out-of-distribution%252C%2520while%250Amaintaining%2520a%2520similar%2520accuracy%2520for%2520in-distribution%2520test%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decorrelated%20feature%20importance%20from%20local%20sample%20weighting&entry.906535625=Benedikt%20Fr%C3%B6hlich%20and%20Alison%20Durst%20and%20Merle%20Behr&entry.1292438233=%20%20Feature%20importance%20%28FI%29%20statistics%20provide%20a%20prominent%20and%20valuable%20method%20of%0Ainsight%20into%20the%20decision%20process%20of%20machine%20learning%20%28ML%29%20models%2C%20but%20their%0Aeffectiveness%20has%20well-known%20limitations%20when%20correlation%20is%20present%20among%20the%0Afeatures%20in%20the%20training%20data.%20In%20this%20case%2C%20the%20FI%20often%20tends%20to%20be%0Adistributed%20among%20all%20features%20which%20are%20in%20correlation%20with%20the%0Aresponse-generating%20signal%20features.%20Even%20worse%2C%20if%20multiple%20signal%20features%0Aare%20in%20strong%20correlation%20with%20a%20noise%20feature%2C%20while%20being%20only%20modestly%0Acorrelated%20with%20one%20another%2C%20this%20can%20result%20in%20a%20noise%20feature%20having%20a%0Adistinctly%20larger%20FI%20score%20than%20any%20signal%20feature.%20Here%20we%20propose%20local%0Asample%20weighting%20%28losaw%29%20which%20can%20flexibly%20be%20integrated%20into%20many%20ML%0Aalgorithms%20to%20improve%20FI%20scores%20in%20the%20presence%20of%20feature%20correlation%20in%20the%0Atraining%20data.%20Our%20approach%20is%20motivated%20from%20inverse%20probability%20weighting%20in%0Acausal%20inference%20and%20locally%2C%20within%20the%20ML%20model%2C%20uses%20a%20sample%20weighting%0Ascheme%20to%20decorrelate%20a%20target%20feature%20from%20the%20remaining%20features.%20This%0Areduces%20model%20bias%20locally%2C%20whenever%20the%20effect%20of%20a%20potential%20signal%20feature%0Ais%20evaluated%20and%20compared%20to%20others.%20Moreover%2C%20losaw%20comes%20with%20a%20natural%0Atuning%20parameter%2C%20the%20minimum%20effective%20sample%20size%20of%20the%20weighted%20population%2C%0Awhich%20corresponds%20to%20an%20interpretation-prediction-tradeoff%2C%20analog%20to%20a%0Abias-variance-tradeoff%20as%20for%20classical%20ML%20tuning%20parameters.%20We%20demonstrate%0Ahow%20losaw%20can%20be%20integrated%20within%20decision%20tree-based%20ML%20methods%20and%20within%0Amini-batch%20training%20of%20neural%20networks.%20We%20investigate%20losaw%20for%20random%20forest%0Aand%20convolutional%20neural%20networks%20in%20a%20simulation%20study%20on%20settings%20showing%0Adiverse%20correlation%20patterns.%20We%20found%20that%20losaw%20improves%20FI%20consistently.%0AMoreover%2C%20it%20often%20improves%20prediction%20accuracy%20for%20out-of-distribution%2C%20while%0Amaintaining%20a%20similar%20accuracy%20for%20in-distribution%20test%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06337v1&entry.124074799=Read"},
{"title": "Graph Federated Learning for Personalized Privacy Recommendation", "author": "Ce Na and Kai Yang and Dengzhao Fang and Yu Li and Jingtong Gao and Chengcheng Zhu and Jiale Zhang and Xiaobing Sun and Yi Chang", "abstract": "  Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.\n", "link": "http://arxiv.org/abs/2508.06208v1", "date": "2025-08-08", "relevancy": 2.0492, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.414}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4134}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Federated%20Learning%20for%20Personalized%20Privacy%20Recommendation&body=Title%3A%20Graph%20Federated%20Learning%20for%20Personalized%20Privacy%20Recommendation%0AAuthor%3A%20Ce%20Na%20and%20Kai%20Yang%20and%20Dengzhao%20Fang%20and%20Yu%20Li%20and%20Jingtong%20Gao%20and%20Chengcheng%20Zhu%20and%20Jiale%20Zhang%20and%20Xiaobing%20Sun%20and%20Yi%20Chang%0AAbstract%3A%20%20%20Federated%20recommendation%20systems%20%28FedRecs%29%20have%20gained%20significant%20attention%0Afor%20providing%20privacy-preserving%20recommendation%20services.%20However%2C%20existing%0AFedRecs%20assume%20that%20all%20users%20have%20the%20same%20requirements%20for%20privacy%0Aprotection%2C%20i.e.%2C%20they%20do%20not%20upload%20any%20data%20to%20the%20server.%20The%20approaches%0Aoverlook%20the%20potential%20to%20enhance%20the%20recommendation%20service%20by%20utilizing%0Apublicly%20available%20user%20data.%20In%20real-world%20applications%2C%20users%20can%20choose%20to%0Abe%20private%20or%20public.%20Private%20users%27%20interaction%20data%20is%20not%20shared%2C%20while%0Apublic%20users%27%20interaction%20data%20can%20be%20shared.%20Inspired%20by%20the%20issue%2C%20this%20paper%0Aproposes%20a%20novel%20Graph%20Federated%20Learning%20for%20Personalized%20Privacy%0ARecommendation%20%28GFed-PP%29%20that%20adapts%20to%20different%20privacy%20requirements%20while%0Aimproving%20recommendation%20performance.%20GFed-PP%20incorporates%20the%20interaction%20data%0Aof%20public%20users%20to%20build%20a%20user-item%20interaction%20graph%2C%20which%20is%20then%20used%20to%0Aform%20a%20user%20relationship%20graph.%20A%20lightweight%20graph%20convolutional%20network%20%28GCN%29%0Ais%20employed%20to%20learn%20each%20user%27s%20user-specific%20personalized%20item%20embedding.%20To%0Aprotect%20user%20privacy%2C%20each%20client%20learns%20the%20user%20embedding%20and%20the%20scoring%0Afunction%20locally.%20Additionally%2C%20GFed-PP%20achieves%20optimization%20of%20the%20federated%0Arecommendation%20framework%20through%20the%20initialization%20of%20item%20embedding%20on%0Aclients%20and%20the%20aggregation%20of%20the%20user%20relationship%20graph%20on%20the%20server.%0AExperimental%20results%20demonstrate%20that%20GFed-PP%20significantly%20outperforms%0Aexisting%20methods%20for%20five%20datasets%2C%20offering%20superior%20recommendation%20accuracy%0Awithout%20compromising%20privacy.%20This%20framework%20provides%20a%20practical%20solution%20for%0Aaccommodating%20varying%20privacy%20preferences%20in%20federated%20recommendation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Federated%2520Learning%2520for%2520Personalized%2520Privacy%2520Recommendation%26entry.906535625%3DCe%2520Na%2520and%2520Kai%2520Yang%2520and%2520Dengzhao%2520Fang%2520and%2520Yu%2520Li%2520and%2520Jingtong%2520Gao%2520and%2520Chengcheng%2520Zhu%2520and%2520Jiale%2520Zhang%2520and%2520Xiaobing%2520Sun%2520and%2520Yi%2520Chang%26entry.1292438233%3D%2520%2520Federated%2520recommendation%2520systems%2520%2528FedRecs%2529%2520have%2520gained%2520significant%2520attention%250Afor%2520providing%2520privacy-preserving%2520recommendation%2520services.%2520However%252C%2520existing%250AFedRecs%2520assume%2520that%2520all%2520users%2520have%2520the%2520same%2520requirements%2520for%2520privacy%250Aprotection%252C%2520i.e.%252C%2520they%2520do%2520not%2520upload%2520any%2520data%2520to%2520the%2520server.%2520The%2520approaches%250Aoverlook%2520the%2520potential%2520to%2520enhance%2520the%2520recommendation%2520service%2520by%2520utilizing%250Apublicly%2520available%2520user%2520data.%2520In%2520real-world%2520applications%252C%2520users%2520can%2520choose%2520to%250Abe%2520private%2520or%2520public.%2520Private%2520users%2527%2520interaction%2520data%2520is%2520not%2520shared%252C%2520while%250Apublic%2520users%2527%2520interaction%2520data%2520can%2520be%2520shared.%2520Inspired%2520by%2520the%2520issue%252C%2520this%2520paper%250Aproposes%2520a%2520novel%2520Graph%2520Federated%2520Learning%2520for%2520Personalized%2520Privacy%250ARecommendation%2520%2528GFed-PP%2529%2520that%2520adapts%2520to%2520different%2520privacy%2520requirements%2520while%250Aimproving%2520recommendation%2520performance.%2520GFed-PP%2520incorporates%2520the%2520interaction%2520data%250Aof%2520public%2520users%2520to%2520build%2520a%2520user-item%2520interaction%2520graph%252C%2520which%2520is%2520then%2520used%2520to%250Aform%2520a%2520user%2520relationship%2520graph.%2520A%2520lightweight%2520graph%2520convolutional%2520network%2520%2528GCN%2529%250Ais%2520employed%2520to%2520learn%2520each%2520user%2527s%2520user-specific%2520personalized%2520item%2520embedding.%2520To%250Aprotect%2520user%2520privacy%252C%2520each%2520client%2520learns%2520the%2520user%2520embedding%2520and%2520the%2520scoring%250Afunction%2520locally.%2520Additionally%252C%2520GFed-PP%2520achieves%2520optimization%2520of%2520the%2520federated%250Arecommendation%2520framework%2520through%2520the%2520initialization%2520of%2520item%2520embedding%2520on%250Aclients%2520and%2520the%2520aggregation%2520of%2520the%2520user%2520relationship%2520graph%2520on%2520the%2520server.%250AExperimental%2520results%2520demonstrate%2520that%2520GFed-PP%2520significantly%2520outperforms%250Aexisting%2520methods%2520for%2520five%2520datasets%252C%2520offering%2520superior%2520recommendation%2520accuracy%250Awithout%2520compromising%2520privacy.%2520This%2520framework%2520provides%2520a%2520practical%2520solution%2520for%250Aaccommodating%2520varying%2520privacy%2520preferences%2520in%2520federated%2520recommendation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Federated%20Learning%20for%20Personalized%20Privacy%20Recommendation&entry.906535625=Ce%20Na%20and%20Kai%20Yang%20and%20Dengzhao%20Fang%20and%20Yu%20Li%20and%20Jingtong%20Gao%20and%20Chengcheng%20Zhu%20and%20Jiale%20Zhang%20and%20Xiaobing%20Sun%20and%20Yi%20Chang&entry.1292438233=%20%20Federated%20recommendation%20systems%20%28FedRecs%29%20have%20gained%20significant%20attention%0Afor%20providing%20privacy-preserving%20recommendation%20services.%20However%2C%20existing%0AFedRecs%20assume%20that%20all%20users%20have%20the%20same%20requirements%20for%20privacy%0Aprotection%2C%20i.e.%2C%20they%20do%20not%20upload%20any%20data%20to%20the%20server.%20The%20approaches%0Aoverlook%20the%20potential%20to%20enhance%20the%20recommendation%20service%20by%20utilizing%0Apublicly%20available%20user%20data.%20In%20real-world%20applications%2C%20users%20can%20choose%20to%0Abe%20private%20or%20public.%20Private%20users%27%20interaction%20data%20is%20not%20shared%2C%20while%0Apublic%20users%27%20interaction%20data%20can%20be%20shared.%20Inspired%20by%20the%20issue%2C%20this%20paper%0Aproposes%20a%20novel%20Graph%20Federated%20Learning%20for%20Personalized%20Privacy%0ARecommendation%20%28GFed-PP%29%20that%20adapts%20to%20different%20privacy%20requirements%20while%0Aimproving%20recommendation%20performance.%20GFed-PP%20incorporates%20the%20interaction%20data%0Aof%20public%20users%20to%20build%20a%20user-item%20interaction%20graph%2C%20which%20is%20then%20used%20to%0Aform%20a%20user%20relationship%20graph.%20A%20lightweight%20graph%20convolutional%20network%20%28GCN%29%0Ais%20employed%20to%20learn%20each%20user%27s%20user-specific%20personalized%20item%20embedding.%20To%0Aprotect%20user%20privacy%2C%20each%20client%20learns%20the%20user%20embedding%20and%20the%20scoring%0Afunction%20locally.%20Additionally%2C%20GFed-PP%20achieves%20optimization%20of%20the%20federated%0Arecommendation%20framework%20through%20the%20initialization%20of%20item%20embedding%20on%0Aclients%20and%20the%20aggregation%20of%20the%20user%20relationship%20graph%20on%20the%20server.%0AExperimental%20results%20demonstrate%20that%20GFed-PP%20significantly%20outperforms%0Aexisting%20methods%20for%20five%20datasets%2C%20offering%20superior%20recommendation%20accuracy%0Awithout%20compromising%20privacy.%20This%20framework%20provides%20a%20practical%20solution%20for%0Aaccommodating%20varying%20privacy%20preferences%20in%20federated%20recommendation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06208v1&entry.124074799=Read"},
{"title": "Benchmarking Pretrained Molecular Embedding Models For Molecular\n  Representation Learning", "author": "Mateusz Praski and Jakub Adamczyk and Wojciech Czech", "abstract": "  Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations.\n", "link": "http://arxiv.org/abs/2508.06199v1", "date": "2025-08-08", "relevancy": 2.042, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Pretrained%20Molecular%20Embedding%20Models%20For%20Molecular%0A%20%20Representation%20Learning&body=Title%3A%20Benchmarking%20Pretrained%20Molecular%20Embedding%20Models%20For%20Molecular%0A%20%20Representation%20Learning%0AAuthor%3A%20Mateusz%20Praski%20and%20Jakub%20Adamczyk%20and%20Wojciech%20Czech%0AAbstract%3A%20%20%20Pretrained%20neural%20networks%20have%20attracted%20significant%20interest%20in%20chemistry%0Aand%20small%20molecule%20drug%20design.%20Embeddings%20from%20these%20models%20are%20widely%20used%0Afor%20molecular%20property%20prediction%2C%20virtual%20screening%2C%20and%20small%20data%20learning%0Ain%20molecular%20chemistry.%20This%20study%20presents%20the%20most%20extensive%20comparison%20of%0Asuch%20models%20to%20date%2C%20evaluating%2025%20models%20across%2025%20datasets.%20Under%20a%20fair%0Acomparison%20framework%2C%20we%20assess%20models%20spanning%20various%20modalities%2C%0Aarchitectures%2C%20and%20pretraining%20strategies.%20Using%20a%20dedicated%20hierarchical%0ABayesian%20statistical%20testing%20model%2C%20we%20arrive%20at%20a%20surprising%20result%3A%20nearly%0Aall%20neural%20models%20show%20negligible%20or%20no%20improvement%20over%20the%20baseline%20ECFP%0Amolecular%20fingerprint.%20Only%20the%20CLAMP%20model%2C%20which%20is%20also%20based%20on%20molecular%0Afingerprints%2C%20performs%20statistically%20significantly%20better%20than%20the%0Aalternatives.%20These%20findings%20raise%20concerns%20about%20the%20evaluation%20rigor%20in%0Aexisting%20studies.%20We%20discuss%20potential%20causes%2C%20propose%20solutions%2C%20and%20offer%0Apractical%20recommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Pretrained%2520Molecular%2520Embedding%2520Models%2520For%2520Molecular%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMateusz%2520Praski%2520and%2520Jakub%2520Adamczyk%2520and%2520Wojciech%2520Czech%26entry.1292438233%3D%2520%2520Pretrained%2520neural%2520networks%2520have%2520attracted%2520significant%2520interest%2520in%2520chemistry%250Aand%2520small%2520molecule%2520drug%2520design.%2520Embeddings%2520from%2520these%2520models%2520are%2520widely%2520used%250Afor%2520molecular%2520property%2520prediction%252C%2520virtual%2520screening%252C%2520and%2520small%2520data%2520learning%250Ain%2520molecular%2520chemistry.%2520This%2520study%2520presents%2520the%2520most%2520extensive%2520comparison%2520of%250Asuch%2520models%2520to%2520date%252C%2520evaluating%252025%2520models%2520across%252025%2520datasets.%2520Under%2520a%2520fair%250Acomparison%2520framework%252C%2520we%2520assess%2520models%2520spanning%2520various%2520modalities%252C%250Aarchitectures%252C%2520and%2520pretraining%2520strategies.%2520Using%2520a%2520dedicated%2520hierarchical%250ABayesian%2520statistical%2520testing%2520model%252C%2520we%2520arrive%2520at%2520a%2520surprising%2520result%253A%2520nearly%250Aall%2520neural%2520models%2520show%2520negligible%2520or%2520no%2520improvement%2520over%2520the%2520baseline%2520ECFP%250Amolecular%2520fingerprint.%2520Only%2520the%2520CLAMP%2520model%252C%2520which%2520is%2520also%2520based%2520on%2520molecular%250Afingerprints%252C%2520performs%2520statistically%2520significantly%2520better%2520than%2520the%250Aalternatives.%2520These%2520findings%2520raise%2520concerns%2520about%2520the%2520evaluation%2520rigor%2520in%250Aexisting%2520studies.%2520We%2520discuss%2520potential%2520causes%252C%2520propose%2520solutions%252C%2520and%2520offer%250Apractical%2520recommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Pretrained%20Molecular%20Embedding%20Models%20For%20Molecular%0A%20%20Representation%20Learning&entry.906535625=Mateusz%20Praski%20and%20Jakub%20Adamczyk%20and%20Wojciech%20Czech&entry.1292438233=%20%20Pretrained%20neural%20networks%20have%20attracted%20significant%20interest%20in%20chemistry%0Aand%20small%20molecule%20drug%20design.%20Embeddings%20from%20these%20models%20are%20widely%20used%0Afor%20molecular%20property%20prediction%2C%20virtual%20screening%2C%20and%20small%20data%20learning%0Ain%20molecular%20chemistry.%20This%20study%20presents%20the%20most%20extensive%20comparison%20of%0Asuch%20models%20to%20date%2C%20evaluating%2025%20models%20across%2025%20datasets.%20Under%20a%20fair%0Acomparison%20framework%2C%20we%20assess%20models%20spanning%20various%20modalities%2C%0Aarchitectures%2C%20and%20pretraining%20strategies.%20Using%20a%20dedicated%20hierarchical%0ABayesian%20statistical%20testing%20model%2C%20we%20arrive%20at%20a%20surprising%20result%3A%20nearly%0Aall%20neural%20models%20show%20negligible%20or%20no%20improvement%20over%20the%20baseline%20ECFP%0Amolecular%20fingerprint.%20Only%20the%20CLAMP%20model%2C%20which%20is%20also%20based%20on%20molecular%0Afingerprints%2C%20performs%20statistically%20significantly%20better%20than%20the%0Aalternatives.%20These%20findings%20raise%20concerns%20about%20the%20evaluation%20rigor%20in%0Aexisting%20studies.%20We%20discuss%20potential%20causes%2C%20propose%20solutions%2C%20and%20offer%0Apractical%20recommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06199v1&entry.124074799=Read"},
{"title": "A Calibration Tool for Refractive Underwater Vision", "author": "Felix Seegr\u00e4ber and Mengkun She and Felix Woelk and Kevin K\u00f6ser", "abstract": "  Many underwater applications rely on vision sensors and require proper camera\ncalibration, i.e. knowing the incoming light ray for each pixel in the image.\nWhile for the ideal pinhole camera model all viewing rays intersect in a single\n3D point, underwater cameras suffer from - possibly multiple - refractions of\nlight rays at the interfaces of water, glass and air. These changes of\ndirection depend on the position and orientation of the camera inside the\nwater-proof housing, as well as on the shape and properties of the optical\nwindow, the port, itself. In recent years explicit models for underwater vision\nbehind common ports such as flat or dome port have been proposed, but the\nunderwater community is still lacking a calibration tool which can determine\nport parameters through refractive calibration. With this work we provide the\nfirst open source implementation of an underwater refractive camera calibration\ntoolbox. It allows end-to-end calibration of underwater vision systems,\nincluding camera, stereo and housing calibration for systems with dome or flat\nports. The implementation is verified using rendered datasets and real-world\nexperiments.\n", "link": "http://arxiv.org/abs/2405.18018v2", "date": "2025-08-08", "relevancy": 2.0366, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5326}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision&body=Title%3A%20A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision%0AAuthor%3A%20Felix%20Seegr%C3%A4ber%20and%20Mengkun%20She%20and%20Felix%20Woelk%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20Many%20underwater%20applications%20rely%20on%20vision%20sensors%20and%20require%20proper%20camera%0Acalibration%2C%20i.e.%20knowing%20the%20incoming%20light%20ray%20for%20each%20pixel%20in%20the%20image.%0AWhile%20for%20the%20ideal%20pinhole%20camera%20model%20all%20viewing%20rays%20intersect%20in%20a%20single%0A3D%20point%2C%20underwater%20cameras%20suffer%20from%20-%20possibly%20multiple%20-%20refractions%20of%0Alight%20rays%20at%20the%20interfaces%20of%20water%2C%20glass%20and%20air.%20These%20changes%20of%0Adirection%20depend%20on%20the%20position%20and%20orientation%20of%20the%20camera%20inside%20the%0Awater-proof%20housing%2C%20as%20well%20as%20on%20the%20shape%20and%20properties%20of%20the%20optical%0Awindow%2C%20the%20port%2C%20itself.%20In%20recent%20years%20explicit%20models%20for%20underwater%20vision%0Abehind%20common%20ports%20such%20as%20flat%20or%20dome%20port%20have%20been%20proposed%2C%20but%20the%0Aunderwater%20community%20is%20still%20lacking%20a%20calibration%20tool%20which%20can%20determine%0Aport%20parameters%20through%20refractive%20calibration.%20With%20this%20work%20we%20provide%20the%0Afirst%20open%20source%20implementation%20of%20an%20underwater%20refractive%20camera%20calibration%0Atoolbox.%20It%20allows%20end-to-end%20calibration%20of%20underwater%20vision%20systems%2C%0Aincluding%20camera%2C%20stereo%20and%20housing%20calibration%20for%20systems%20with%20dome%20or%20flat%0Aports.%20The%20implementation%20is%20verified%20using%20rendered%20datasets%20and%20real-world%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Calibration%2520Tool%2520for%2520Refractive%2520Underwater%2520Vision%26entry.906535625%3DFelix%2520Seegr%25C3%25A4ber%2520and%2520Mengkun%2520She%2520and%2520Felix%2520Woelk%2520and%2520Kevin%2520K%25C3%25B6ser%26entry.1292438233%3D%2520%2520Many%2520underwater%2520applications%2520rely%2520on%2520vision%2520sensors%2520and%2520require%2520proper%2520camera%250Acalibration%252C%2520i.e.%2520knowing%2520the%2520incoming%2520light%2520ray%2520for%2520each%2520pixel%2520in%2520the%2520image.%250AWhile%2520for%2520the%2520ideal%2520pinhole%2520camera%2520model%2520all%2520viewing%2520rays%2520intersect%2520in%2520a%2520single%250A3D%2520point%252C%2520underwater%2520cameras%2520suffer%2520from%2520-%2520possibly%2520multiple%2520-%2520refractions%2520of%250Alight%2520rays%2520at%2520the%2520interfaces%2520of%2520water%252C%2520glass%2520and%2520air.%2520These%2520changes%2520of%250Adirection%2520depend%2520on%2520the%2520position%2520and%2520orientation%2520of%2520the%2520camera%2520inside%2520the%250Awater-proof%2520housing%252C%2520as%2520well%2520as%2520on%2520the%2520shape%2520and%2520properties%2520of%2520the%2520optical%250Awindow%252C%2520the%2520port%252C%2520itself.%2520In%2520recent%2520years%2520explicit%2520models%2520for%2520underwater%2520vision%250Abehind%2520common%2520ports%2520such%2520as%2520flat%2520or%2520dome%2520port%2520have%2520been%2520proposed%252C%2520but%2520the%250Aunderwater%2520community%2520is%2520still%2520lacking%2520a%2520calibration%2520tool%2520which%2520can%2520determine%250Aport%2520parameters%2520through%2520refractive%2520calibration.%2520With%2520this%2520work%2520we%2520provide%2520the%250Afirst%2520open%2520source%2520implementation%2520of%2520an%2520underwater%2520refractive%2520camera%2520calibration%250Atoolbox.%2520It%2520allows%2520end-to-end%2520calibration%2520of%2520underwater%2520vision%2520systems%252C%250Aincluding%2520camera%252C%2520stereo%2520and%2520housing%2520calibration%2520for%2520systems%2520with%2520dome%2520or%2520flat%250Aports.%2520The%2520implementation%2520is%2520verified%2520using%2520rendered%2520datasets%2520and%2520real-world%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision&entry.906535625=Felix%20Seegr%C3%A4ber%20and%20Mengkun%20She%20and%20Felix%20Woelk%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20Many%20underwater%20applications%20rely%20on%20vision%20sensors%20and%20require%20proper%20camera%0Acalibration%2C%20i.e.%20knowing%20the%20incoming%20light%20ray%20for%20each%20pixel%20in%20the%20image.%0AWhile%20for%20the%20ideal%20pinhole%20camera%20model%20all%20viewing%20rays%20intersect%20in%20a%20single%0A3D%20point%2C%20underwater%20cameras%20suffer%20from%20-%20possibly%20multiple%20-%20refractions%20of%0Alight%20rays%20at%20the%20interfaces%20of%20water%2C%20glass%20and%20air.%20These%20changes%20of%0Adirection%20depend%20on%20the%20position%20and%20orientation%20of%20the%20camera%20inside%20the%0Awater-proof%20housing%2C%20as%20well%20as%20on%20the%20shape%20and%20properties%20of%20the%20optical%0Awindow%2C%20the%20port%2C%20itself.%20In%20recent%20years%20explicit%20models%20for%20underwater%20vision%0Abehind%20common%20ports%20such%20as%20flat%20or%20dome%20port%20have%20been%20proposed%2C%20but%20the%0Aunderwater%20community%20is%20still%20lacking%20a%20calibration%20tool%20which%20can%20determine%0Aport%20parameters%20through%20refractive%20calibration.%20With%20this%20work%20we%20provide%20the%0Afirst%20open%20source%20implementation%20of%20an%20underwater%20refractive%20camera%20calibration%0Atoolbox.%20It%20allows%20end-to-end%20calibration%20of%20underwater%20vision%20systems%2C%0Aincluding%20camera%2C%20stereo%20and%20housing%20calibration%20for%20systems%20with%20dome%20or%20flat%0Aports.%20The%20implementation%20is%20verified%20using%20rendered%20datasets%20and%20real-world%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18018v2&entry.124074799=Read"},
{"title": "Rethinking the Bias of Foundation Model under Long-tailed Distribution", "author": "Jiahao Chen and Bin Qin and Jiangmeng Li and Hao Chen and Bing Su", "abstract": "  Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset. Code is available:\nhttps://github.com/JiahaoChen1/Pre-train-Imbalance\n", "link": "http://arxiv.org/abs/2501.15955v3", "date": "2025-08-08", "relevancy": 2.0356, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5645}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.47}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Bias%20of%20Foundation%20Model%20under%20Long-tailed%20Distribution&body=Title%3A%20Rethinking%20the%20Bias%20of%20Foundation%20Model%20under%20Long-tailed%20Distribution%0AAuthor%3A%20Jiahao%20Chen%20and%20Bin%20Qin%20and%20Jiangmeng%20Li%20and%20Hao%20Chen%20and%20Bing%20Su%0AAbstract%3A%20%20%20Long-tailed%20learning%20has%20garnered%20increasing%20attention%20due%20to%20its%20practical%0Asignificance.%20Among%20the%20various%20approaches%2C%20the%20fine-tuning%20paradigm%20has%20gained%0Aconsiderable%20interest%20with%20the%20advent%20of%20foundation%20models.%20However%2C%20most%0Aexisting%20methods%20primarily%20focus%20on%20leveraging%20knowledge%20from%20these%20models%2C%0Aoverlooking%20the%20inherent%20biases%20introduced%20by%20the%20imbalanced%20training%20data%20they%0Arely%20on.%20In%20this%20paper%2C%20we%20examine%20how%20such%20imbalances%20from%20pre-training%20affect%0Along-tailed%20downstream%20tasks.%20Specifically%2C%20we%20find%20the%20imbalance%20biases%0Ainherited%20in%20foundation%20models%20on%20downstream%20task%20as%20parameter%20imbalance%20and%0Adata%20imbalance.%20During%20fine-tuning%2C%20we%20observe%20that%20parameter%20imbalance%20plays%20a%0Amore%20critical%20role%2C%20while%20data%20imbalance%20can%20be%20mitigated%20using%20existing%0Are-balancing%20strategies.%20Moreover%2C%20we%20find%20that%20parameter%20imbalance%20cannot%20be%0Aeffectively%20addressed%20by%20current%20re-balancing%20techniques%2C%20such%20as%20adjusting%20the%0Alogits%2C%20during%20training%2C%20unlike%20data%20imbalance.%20To%20tackle%20both%20imbalances%0Asimultaneously%2C%20we%20build%20our%20method%20on%20causal%20learning%20and%20view%20the%20incomplete%0Asemantic%20factor%20as%20the%20confounder%2C%20which%20brings%20spurious%20correlations%20between%0Ainput%20samples%20and%20labels.%20To%20resolve%20the%20negative%20effects%20of%20this%2C%20we%20propose%20a%0Anovel%20backdoor%20adjustment%20method%20that%20learns%20the%20true%20causal%20effect%20between%0Ainput%20samples%20and%20labels%2C%20rather%20than%20merely%20fitting%20the%20correlations%20in%20the%0Adata.%20Notably%2C%20we%20achieve%20an%20average%20performance%20increase%20of%20about%20%241.67%5C%25%24%20on%0Aeach%20dataset.%20Code%20is%20available%3A%0Ahttps%3A//github.com/JiahaoChen1/Pre-train-Imbalance%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15955v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Bias%2520of%2520Foundation%2520Model%2520under%2520Long-tailed%2520Distribution%26entry.906535625%3DJiahao%2520Chen%2520and%2520Bin%2520Qin%2520and%2520Jiangmeng%2520Li%2520and%2520Hao%2520Chen%2520and%2520Bing%2520Su%26entry.1292438233%3D%2520%2520Long-tailed%2520learning%2520has%2520garnered%2520increasing%2520attention%2520due%2520to%2520its%2520practical%250Asignificance.%2520Among%2520the%2520various%2520approaches%252C%2520the%2520fine-tuning%2520paradigm%2520has%2520gained%250Aconsiderable%2520interest%2520with%2520the%2520advent%2520of%2520foundation%2520models.%2520However%252C%2520most%250Aexisting%2520methods%2520primarily%2520focus%2520on%2520leveraging%2520knowledge%2520from%2520these%2520models%252C%250Aoverlooking%2520the%2520inherent%2520biases%2520introduced%2520by%2520the%2520imbalanced%2520training%2520data%2520they%250Arely%2520on.%2520In%2520this%2520paper%252C%2520we%2520examine%2520how%2520such%2520imbalances%2520from%2520pre-training%2520affect%250Along-tailed%2520downstream%2520tasks.%2520Specifically%252C%2520we%2520find%2520the%2520imbalance%2520biases%250Ainherited%2520in%2520foundation%2520models%2520on%2520downstream%2520task%2520as%2520parameter%2520imbalance%2520and%250Adata%2520imbalance.%2520During%2520fine-tuning%252C%2520we%2520observe%2520that%2520parameter%2520imbalance%2520plays%2520a%250Amore%2520critical%2520role%252C%2520while%2520data%2520imbalance%2520can%2520be%2520mitigated%2520using%2520existing%250Are-balancing%2520strategies.%2520Moreover%252C%2520we%2520find%2520that%2520parameter%2520imbalance%2520cannot%2520be%250Aeffectively%2520addressed%2520by%2520current%2520re-balancing%2520techniques%252C%2520such%2520as%2520adjusting%2520the%250Alogits%252C%2520during%2520training%252C%2520unlike%2520data%2520imbalance.%2520To%2520tackle%2520both%2520imbalances%250Asimultaneously%252C%2520we%2520build%2520our%2520method%2520on%2520causal%2520learning%2520and%2520view%2520the%2520incomplete%250Asemantic%2520factor%2520as%2520the%2520confounder%252C%2520which%2520brings%2520spurious%2520correlations%2520between%250Ainput%2520samples%2520and%2520labels.%2520To%2520resolve%2520the%2520negative%2520effects%2520of%2520this%252C%2520we%2520propose%2520a%250Anovel%2520backdoor%2520adjustment%2520method%2520that%2520learns%2520the%2520true%2520causal%2520effect%2520between%250Ainput%2520samples%2520and%2520labels%252C%2520rather%2520than%2520merely%2520fitting%2520the%2520correlations%2520in%2520the%250Adata.%2520Notably%252C%2520we%2520achieve%2520an%2520average%2520performance%2520increase%2520of%2520about%2520%25241.67%255C%2525%2524%2520on%250Aeach%2520dataset.%2520Code%2520is%2520available%253A%250Ahttps%253A//github.com/JiahaoChen1/Pre-train-Imbalance%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15955v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Bias%20of%20Foundation%20Model%20under%20Long-tailed%20Distribution&entry.906535625=Jiahao%20Chen%20and%20Bin%20Qin%20and%20Jiangmeng%20Li%20and%20Hao%20Chen%20and%20Bing%20Su&entry.1292438233=%20%20Long-tailed%20learning%20has%20garnered%20increasing%20attention%20due%20to%20its%20practical%0Asignificance.%20Among%20the%20various%20approaches%2C%20the%20fine-tuning%20paradigm%20has%20gained%0Aconsiderable%20interest%20with%20the%20advent%20of%20foundation%20models.%20However%2C%20most%0Aexisting%20methods%20primarily%20focus%20on%20leveraging%20knowledge%20from%20these%20models%2C%0Aoverlooking%20the%20inherent%20biases%20introduced%20by%20the%20imbalanced%20training%20data%20they%0Arely%20on.%20In%20this%20paper%2C%20we%20examine%20how%20such%20imbalances%20from%20pre-training%20affect%0Along-tailed%20downstream%20tasks.%20Specifically%2C%20we%20find%20the%20imbalance%20biases%0Ainherited%20in%20foundation%20models%20on%20downstream%20task%20as%20parameter%20imbalance%20and%0Adata%20imbalance.%20During%20fine-tuning%2C%20we%20observe%20that%20parameter%20imbalance%20plays%20a%0Amore%20critical%20role%2C%20while%20data%20imbalance%20can%20be%20mitigated%20using%20existing%0Are-balancing%20strategies.%20Moreover%2C%20we%20find%20that%20parameter%20imbalance%20cannot%20be%0Aeffectively%20addressed%20by%20current%20re-balancing%20techniques%2C%20such%20as%20adjusting%20the%0Alogits%2C%20during%20training%2C%20unlike%20data%20imbalance.%20To%20tackle%20both%20imbalances%0Asimultaneously%2C%20we%20build%20our%20method%20on%20causal%20learning%20and%20view%20the%20incomplete%0Asemantic%20factor%20as%20the%20confounder%2C%20which%20brings%20spurious%20correlations%20between%0Ainput%20samples%20and%20labels.%20To%20resolve%20the%20negative%20effects%20of%20this%2C%20we%20propose%20a%0Anovel%20backdoor%20adjustment%20method%20that%20learns%20the%20true%20causal%20effect%20between%0Ainput%20samples%20and%20labels%2C%20rather%20than%20merely%20fitting%20the%20correlations%20in%20the%0Adata.%20Notably%2C%20we%20achieve%20an%20average%20performance%20increase%20of%20about%20%241.67%5C%25%24%20on%0Aeach%20dataset.%20Code%20is%20available%3A%0Ahttps%3A//github.com/JiahaoChen1/Pre-train-Imbalance%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15955v3&entry.124074799=Read"},
{"title": "Towards Balanced Behavior Cloning from Imbalanced Datasets", "author": "Sagar Parekh and Heramb Nemlekar and Dylan P. Losey", "abstract": "  Robots should be able to learn complex behaviors from human demonstrations.\nIn practice, these human-provided datasets are inevitably imbalanced: i.e., the\nhuman demonstrates some subtasks more frequently than others. State-of-the-art\nmethods default to treating each element of the human's dataset as equally\nimportant. So if -- for instance -- the majority of the human's data focuses on\nreaching a goal, and only a few state-action pairs move to avoid an obstacle,\nthe learning algorithm will place greater emphasis on goal reaching. More\ngenerally, misalignment between the relative amounts of data and the importance\nof that data causes fundamental problems for imitation learning approaches. In\nthis paper we analyze and develop learning methods that automatically account\nfor mixed datasets. We formally prove that imbalanced data leads to imbalanced\npolicies when each state-action pair is weighted equally; these policies\nemulate the most represented behaviors, and not the human's complex, multi-task\ndemonstrations. We next explore algorithms that rebalance offline datasets\n(i.e., reweight the importance of different state-action pairs) without human\noversight. Reweighting the dataset can enhance the overall policy performance.\nHowever, there is no free lunch: each method for autonomously rebalancing\nbrings its own pros and cons. We formulate these advantages and disadvantages,\nhelping other researchers identify when each type of approach is most\nappropriate. We conclude by introducing a novel meta-gradient rebalancing\nalgorithm that addresses the primary limitations behind existing approaches.\nOur experiments show that dataset rebalancing leads to better downstream\nlearning, improving the performance of general imitation learning algorithms\nwithout requiring additional data collection. See our project website:\nhttps://collab.me.vt.edu/data_curation/.\n", "link": "http://arxiv.org/abs/2508.06319v1", "date": "2025-08-08", "relevancy": 2.0296, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5071}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Balanced%20Behavior%20Cloning%20from%20Imbalanced%20Datasets&body=Title%3A%20Towards%20Balanced%20Behavior%20Cloning%20from%20Imbalanced%20Datasets%0AAuthor%3A%20Sagar%20Parekh%20and%20Heramb%20Nemlekar%20and%20Dylan%20P.%20Losey%0AAbstract%3A%20%20%20Robots%20should%20be%20able%20to%20learn%20complex%20behaviors%20from%20human%20demonstrations.%0AIn%20practice%2C%20these%20human-provided%20datasets%20are%20inevitably%20imbalanced%3A%20i.e.%2C%20the%0Ahuman%20demonstrates%20some%20subtasks%20more%20frequently%20than%20others.%20State-of-the-art%0Amethods%20default%20to%20treating%20each%20element%20of%20the%20human%27s%20dataset%20as%20equally%0Aimportant.%20So%20if%20--%20for%20instance%20--%20the%20majority%20of%20the%20human%27s%20data%20focuses%20on%0Areaching%20a%20goal%2C%20and%20only%20a%20few%20state-action%20pairs%20move%20to%20avoid%20an%20obstacle%2C%0Athe%20learning%20algorithm%20will%20place%20greater%20emphasis%20on%20goal%20reaching.%20More%0Agenerally%2C%20misalignment%20between%20the%20relative%20amounts%20of%20data%20and%20the%20importance%0Aof%20that%20data%20causes%20fundamental%20problems%20for%20imitation%20learning%20approaches.%20In%0Athis%20paper%20we%20analyze%20and%20develop%20learning%20methods%20that%20automatically%20account%0Afor%20mixed%20datasets.%20We%20formally%20prove%20that%20imbalanced%20data%20leads%20to%20imbalanced%0Apolicies%20when%20each%20state-action%20pair%20is%20weighted%20equally%3B%20these%20policies%0Aemulate%20the%20most%20represented%20behaviors%2C%20and%20not%20the%20human%27s%20complex%2C%20multi-task%0Ademonstrations.%20We%20next%20explore%20algorithms%20that%20rebalance%20offline%20datasets%0A%28i.e.%2C%20reweight%20the%20importance%20of%20different%20state-action%20pairs%29%20without%20human%0Aoversight.%20Reweighting%20the%20dataset%20can%20enhance%20the%20overall%20policy%20performance.%0AHowever%2C%20there%20is%20no%20free%20lunch%3A%20each%20method%20for%20autonomously%20rebalancing%0Abrings%20its%20own%20pros%20and%20cons.%20We%20formulate%20these%20advantages%20and%20disadvantages%2C%0Ahelping%20other%20researchers%20identify%20when%20each%20type%20of%20approach%20is%20most%0Aappropriate.%20We%20conclude%20by%20introducing%20a%20novel%20meta-gradient%20rebalancing%0Aalgorithm%20that%20addresses%20the%20primary%20limitations%20behind%20existing%20approaches.%0AOur%20experiments%20show%20that%20dataset%20rebalancing%20leads%20to%20better%20downstream%0Alearning%2C%20improving%20the%20performance%20of%20general%20imitation%20learning%20algorithms%0Awithout%20requiring%20additional%20data%20collection.%20See%20our%20project%20website%3A%0Ahttps%3A//collab.me.vt.edu/data_curation/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Balanced%2520Behavior%2520Cloning%2520from%2520Imbalanced%2520Datasets%26entry.906535625%3DSagar%2520Parekh%2520and%2520Heramb%2520Nemlekar%2520and%2520Dylan%2520P.%2520Losey%26entry.1292438233%3D%2520%2520Robots%2520should%2520be%2520able%2520to%2520learn%2520complex%2520behaviors%2520from%2520human%2520demonstrations.%250AIn%2520practice%252C%2520these%2520human-provided%2520datasets%2520are%2520inevitably%2520imbalanced%253A%2520i.e.%252C%2520the%250Ahuman%2520demonstrates%2520some%2520subtasks%2520more%2520frequently%2520than%2520others.%2520State-of-the-art%250Amethods%2520default%2520to%2520treating%2520each%2520element%2520of%2520the%2520human%2527s%2520dataset%2520as%2520equally%250Aimportant.%2520So%2520if%2520--%2520for%2520instance%2520--%2520the%2520majority%2520of%2520the%2520human%2527s%2520data%2520focuses%2520on%250Areaching%2520a%2520goal%252C%2520and%2520only%2520a%2520few%2520state-action%2520pairs%2520move%2520to%2520avoid%2520an%2520obstacle%252C%250Athe%2520learning%2520algorithm%2520will%2520place%2520greater%2520emphasis%2520on%2520goal%2520reaching.%2520More%250Agenerally%252C%2520misalignment%2520between%2520the%2520relative%2520amounts%2520of%2520data%2520and%2520the%2520importance%250Aof%2520that%2520data%2520causes%2520fundamental%2520problems%2520for%2520imitation%2520learning%2520approaches.%2520In%250Athis%2520paper%2520we%2520analyze%2520and%2520develop%2520learning%2520methods%2520that%2520automatically%2520account%250Afor%2520mixed%2520datasets.%2520We%2520formally%2520prove%2520that%2520imbalanced%2520data%2520leads%2520to%2520imbalanced%250Apolicies%2520when%2520each%2520state-action%2520pair%2520is%2520weighted%2520equally%253B%2520these%2520policies%250Aemulate%2520the%2520most%2520represented%2520behaviors%252C%2520and%2520not%2520the%2520human%2527s%2520complex%252C%2520multi-task%250Ademonstrations.%2520We%2520next%2520explore%2520algorithms%2520that%2520rebalance%2520offline%2520datasets%250A%2528i.e.%252C%2520reweight%2520the%2520importance%2520of%2520different%2520state-action%2520pairs%2529%2520without%2520human%250Aoversight.%2520Reweighting%2520the%2520dataset%2520can%2520enhance%2520the%2520overall%2520policy%2520performance.%250AHowever%252C%2520there%2520is%2520no%2520free%2520lunch%253A%2520each%2520method%2520for%2520autonomously%2520rebalancing%250Abrings%2520its%2520own%2520pros%2520and%2520cons.%2520We%2520formulate%2520these%2520advantages%2520and%2520disadvantages%252C%250Ahelping%2520other%2520researchers%2520identify%2520when%2520each%2520type%2520of%2520approach%2520is%2520most%250Aappropriate.%2520We%2520conclude%2520by%2520introducing%2520a%2520novel%2520meta-gradient%2520rebalancing%250Aalgorithm%2520that%2520addresses%2520the%2520primary%2520limitations%2520behind%2520existing%2520approaches.%250AOur%2520experiments%2520show%2520that%2520dataset%2520rebalancing%2520leads%2520to%2520better%2520downstream%250Alearning%252C%2520improving%2520the%2520performance%2520of%2520general%2520imitation%2520learning%2520algorithms%250Awithout%2520requiring%2520additional%2520data%2520collection.%2520See%2520our%2520project%2520website%253A%250Ahttps%253A//collab.me.vt.edu/data_curation/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Balanced%20Behavior%20Cloning%20from%20Imbalanced%20Datasets&entry.906535625=Sagar%20Parekh%20and%20Heramb%20Nemlekar%20and%20Dylan%20P.%20Losey&entry.1292438233=%20%20Robots%20should%20be%20able%20to%20learn%20complex%20behaviors%20from%20human%20demonstrations.%0AIn%20practice%2C%20these%20human-provided%20datasets%20are%20inevitably%20imbalanced%3A%20i.e.%2C%20the%0Ahuman%20demonstrates%20some%20subtasks%20more%20frequently%20than%20others.%20State-of-the-art%0Amethods%20default%20to%20treating%20each%20element%20of%20the%20human%27s%20dataset%20as%20equally%0Aimportant.%20So%20if%20--%20for%20instance%20--%20the%20majority%20of%20the%20human%27s%20data%20focuses%20on%0Areaching%20a%20goal%2C%20and%20only%20a%20few%20state-action%20pairs%20move%20to%20avoid%20an%20obstacle%2C%0Athe%20learning%20algorithm%20will%20place%20greater%20emphasis%20on%20goal%20reaching.%20More%0Agenerally%2C%20misalignment%20between%20the%20relative%20amounts%20of%20data%20and%20the%20importance%0Aof%20that%20data%20causes%20fundamental%20problems%20for%20imitation%20learning%20approaches.%20In%0Athis%20paper%20we%20analyze%20and%20develop%20learning%20methods%20that%20automatically%20account%0Afor%20mixed%20datasets.%20We%20formally%20prove%20that%20imbalanced%20data%20leads%20to%20imbalanced%0Apolicies%20when%20each%20state-action%20pair%20is%20weighted%20equally%3B%20these%20policies%0Aemulate%20the%20most%20represented%20behaviors%2C%20and%20not%20the%20human%27s%20complex%2C%20multi-task%0Ademonstrations.%20We%20next%20explore%20algorithms%20that%20rebalance%20offline%20datasets%0A%28i.e.%2C%20reweight%20the%20importance%20of%20different%20state-action%20pairs%29%20without%20human%0Aoversight.%20Reweighting%20the%20dataset%20can%20enhance%20the%20overall%20policy%20performance.%0AHowever%2C%20there%20is%20no%20free%20lunch%3A%20each%20method%20for%20autonomously%20rebalancing%0Abrings%20its%20own%20pros%20and%20cons.%20We%20formulate%20these%20advantages%20and%20disadvantages%2C%0Ahelping%20other%20researchers%20identify%20when%20each%20type%20of%20approach%20is%20most%0Aappropriate.%20We%20conclude%20by%20introducing%20a%20novel%20meta-gradient%20rebalancing%0Aalgorithm%20that%20addresses%20the%20primary%20limitations%20behind%20existing%20approaches.%0AOur%20experiments%20show%20that%20dataset%20rebalancing%20leads%20to%20better%20downstream%0Alearning%2C%20improving%20the%20performance%20of%20general%20imitation%20learning%20algorithms%0Awithout%20requiring%20additional%20data%20collection.%20See%20our%20project%20website%3A%0Ahttps%3A//collab.me.vt.edu/data_curation/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06319v1&entry.124074799=Read"},
{"title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets\n  and an Audio Restoration Deep Learning Pipeline", "author": "Minwoo Oh and Minsu Park and Eunil Park", "abstract": "  Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR-160 is a unique benchmark\ndataset comprising 1,121 video and mixed-audio pairs, specifically designed for\nshort video restoration tasks. Experimental results demonstrate that our\npipeline not only removes arbitrary BGM with high accuracy but also restores\nOSTs, ensuring content integrity. This approach provides an ethical and\nscalable solution to copyright challenges in user-generated content on short\nvideo platforms.\n", "link": "http://arxiv.org/abs/2504.21772v3", "date": "2025-08-08", "relevancy": 2.0288, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5251}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5015}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline&body=Title%3A%20Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline%0AAuthor%3A%20Minwoo%20Oh%20and%20Minsu%20Park%20and%20Eunil%20Park%0AAbstract%3A%20%20%20Short%20video%20platforms%20like%20YouTube%20Shorts%20and%20TikTok%20face%20significant%0Acopyright%20compliance%20challenges%2C%20as%20infringers%20frequently%20embed%20arbitrary%0Abackground%20music%20%28BGM%29%20to%20obscure%20original%20soundtracks%20%28OST%29%20and%20evade%20content%0Aoriginality%20detection.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20pipeline%20that%0Aintegrates%20Music%20Source%20Separation%20%28MSS%29%20and%20cross-modal%20video-music%20retrieval%0A%28CMVMR%29.%20Our%20approach%20effectively%20separates%20arbitrary%20BGM%20from%20the%20original%0AOST%2C%20enabling%20the%20restoration%20of%20authentic%20video%20audio%20tracks.%20To%20support%20this%0Awork%2C%20we%20introduce%20two%20domain-specific%20datasets%3A%20OASD-20K%20for%20audio%20separation%0Aand%20OSVAR-160%20for%20pipeline%20evaluation.%20OASD-20K%20contains%2020%2C000%20audio%20clips%0Afeaturing%20mixed%20BGM%20and%20OST%20pairs%2C%20while%20OSVAR-160%20is%20a%20unique%20benchmark%0Adataset%20comprising%201%2C121%20video%20and%20mixed-audio%20pairs%2C%20specifically%20designed%20for%0Ashort%20video%20restoration%20tasks.%20Experimental%20results%20demonstrate%20that%20our%0Apipeline%20not%20only%20removes%20arbitrary%20BGM%20with%20high%20accuracy%20but%20also%20restores%0AOSTs%2C%20ensuring%20content%20integrity.%20This%20approach%20provides%20an%20ethical%20and%0Ascalable%20solution%20to%20copyright%20challenges%20in%20user-generated%20content%20on%20short%0Avideo%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21772v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Copyright%2520Infringement%2520on%2520Short%2520Video%2520Platforms%253A%2520Novel%2520Datasets%250A%2520%2520and%2520an%2520Audio%2520Restoration%2520Deep%2520Learning%2520Pipeline%26entry.906535625%3DMinwoo%2520Oh%2520and%2520Minsu%2520Park%2520and%2520Eunil%2520Park%26entry.1292438233%3D%2520%2520Short%2520video%2520platforms%2520like%2520YouTube%2520Shorts%2520and%2520TikTok%2520face%2520significant%250Acopyright%2520compliance%2520challenges%252C%2520as%2520infringers%2520frequently%2520embed%2520arbitrary%250Abackground%2520music%2520%2528BGM%2529%2520to%2520obscure%2520original%2520soundtracks%2520%2528OST%2529%2520and%2520evade%2520content%250Aoriginality%2520detection.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520pipeline%2520that%250Aintegrates%2520Music%2520Source%2520Separation%2520%2528MSS%2529%2520and%2520cross-modal%2520video-music%2520retrieval%250A%2528CMVMR%2529.%2520Our%2520approach%2520effectively%2520separates%2520arbitrary%2520BGM%2520from%2520the%2520original%250AOST%252C%2520enabling%2520the%2520restoration%2520of%2520authentic%2520video%2520audio%2520tracks.%2520To%2520support%2520this%250Awork%252C%2520we%2520introduce%2520two%2520domain-specific%2520datasets%253A%2520OASD-20K%2520for%2520audio%2520separation%250Aand%2520OSVAR-160%2520for%2520pipeline%2520evaluation.%2520OASD-20K%2520contains%252020%252C000%2520audio%2520clips%250Afeaturing%2520mixed%2520BGM%2520and%2520OST%2520pairs%252C%2520while%2520OSVAR-160%2520is%2520a%2520unique%2520benchmark%250Adataset%2520comprising%25201%252C121%2520video%2520and%2520mixed-audio%2520pairs%252C%2520specifically%2520designed%2520for%250Ashort%2520video%2520restoration%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Apipeline%2520not%2520only%2520removes%2520arbitrary%2520BGM%2520with%2520high%2520accuracy%2520but%2520also%2520restores%250AOSTs%252C%2520ensuring%2520content%2520integrity.%2520This%2520approach%2520provides%2520an%2520ethical%2520and%250Ascalable%2520solution%2520to%2520copyright%2520challenges%2520in%2520user-generated%2520content%2520on%2520short%250Avideo%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21772v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline&entry.906535625=Minwoo%20Oh%20and%20Minsu%20Park%20and%20Eunil%20Park&entry.1292438233=%20%20Short%20video%20platforms%20like%20YouTube%20Shorts%20and%20TikTok%20face%20significant%0Acopyright%20compliance%20challenges%2C%20as%20infringers%20frequently%20embed%20arbitrary%0Abackground%20music%20%28BGM%29%20to%20obscure%20original%20soundtracks%20%28OST%29%20and%20evade%20content%0Aoriginality%20detection.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20pipeline%20that%0Aintegrates%20Music%20Source%20Separation%20%28MSS%29%20and%20cross-modal%20video-music%20retrieval%0A%28CMVMR%29.%20Our%20approach%20effectively%20separates%20arbitrary%20BGM%20from%20the%20original%0AOST%2C%20enabling%20the%20restoration%20of%20authentic%20video%20audio%20tracks.%20To%20support%20this%0Awork%2C%20we%20introduce%20two%20domain-specific%20datasets%3A%20OASD-20K%20for%20audio%20separation%0Aand%20OSVAR-160%20for%20pipeline%20evaluation.%20OASD-20K%20contains%2020%2C000%20audio%20clips%0Afeaturing%20mixed%20BGM%20and%20OST%20pairs%2C%20while%20OSVAR-160%20is%20a%20unique%20benchmark%0Adataset%20comprising%201%2C121%20video%20and%20mixed-audio%20pairs%2C%20specifically%20designed%20for%0Ashort%20video%20restoration%20tasks.%20Experimental%20results%20demonstrate%20that%20our%0Apipeline%20not%20only%20removes%20arbitrary%20BGM%20with%20high%20accuracy%20but%20also%20restores%0AOSTs%2C%20ensuring%20content%20integrity.%20This%20approach%20provides%20an%20ethical%20and%0Ascalable%20solution%20to%20copyright%20challenges%20in%20user-generated%20content%20on%20short%0Avideo%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21772v3&entry.124074799=Read"},
{"title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC", "author": "Jan W\u0119grzynowski and Piotr Kicki and Grzegorz Czechmanowski and Maciej Krupka and Krzysztof Walas", "abstract": "  Model Predictive Control (MPC) is among the most widely adopted and reliable\nmethods for robot control, relying critically on an accurate dynamics model.\nHowever, existing dynamics models used in the gradient-based MPC are limited by\ncomputational complexity and state representation. To address this limitation,\nwe propose the Hyper Prediction Model (HyperPM) - a novel approach in which we\nproject the unmodeled dynamics onto a time-dependent dynamics model. This\ntime-dependency is captured through time-varying model parameters, whose\nevolution over the MPC prediction horizon is learned using a neural network.\nSuch formulation preserves the computational efficiency and robustness of the\nbase model while equipping it with the capacity to anticipate previously\nunmodeled phenomena. We evaluated the proposed approach on several challenging\nsystems, including real-world F1TENTH autonomous racing, and demonstrated that\nit significantly reduces long-horizon prediction errors. Moreover, when\nintegrated within the MPC framework (HyperMPC), our method consistently\noutperforms existing state-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2508.06181v1", "date": "2025-08-08", "relevancy": 2.0287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5805}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.509}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Constant%20Parameters%3A%20Hyper%20Prediction%20Models%20and%20HyperMPC&body=Title%3A%20Beyond%20Constant%20Parameters%3A%20Hyper%20Prediction%20Models%20and%20HyperMPC%0AAuthor%3A%20Jan%20W%C4%99grzynowski%20and%20Piotr%20Kicki%20and%20Grzegorz%20Czechmanowski%20and%20Maciej%20Krupka%20and%20Krzysztof%20Walas%0AAbstract%3A%20%20%20Model%20Predictive%20Control%20%28MPC%29%20is%20among%20the%20most%20widely%20adopted%20and%20reliable%0Amethods%20for%20robot%20control%2C%20relying%20critically%20on%20an%20accurate%20dynamics%20model.%0AHowever%2C%20existing%20dynamics%20models%20used%20in%20the%20gradient-based%20MPC%20are%20limited%20by%0Acomputational%20complexity%20and%20state%20representation.%20To%20address%20this%20limitation%2C%0Awe%20propose%20the%20Hyper%20Prediction%20Model%20%28HyperPM%29%20-%20a%20novel%20approach%20in%20which%20we%0Aproject%20the%20unmodeled%20dynamics%20onto%20a%20time-dependent%20dynamics%20model.%20This%0Atime-dependency%20is%20captured%20through%20time-varying%20model%20parameters%2C%20whose%0Aevolution%20over%20the%20MPC%20prediction%20horizon%20is%20learned%20using%20a%20neural%20network.%0ASuch%20formulation%20preserves%20the%20computational%20efficiency%20and%20robustness%20of%20the%0Abase%20model%20while%20equipping%20it%20with%20the%20capacity%20to%20anticipate%20previously%0Aunmodeled%20phenomena.%20We%20evaluated%20the%20proposed%20approach%20on%20several%20challenging%0Asystems%2C%20including%20real-world%20F1TENTH%20autonomous%20racing%2C%20and%20demonstrated%20that%0Ait%20significantly%20reduces%20long-horizon%20prediction%20errors.%20Moreover%2C%20when%0Aintegrated%20within%20the%20MPC%20framework%20%28HyperMPC%29%2C%20our%20method%20consistently%0Aoutperforms%20existing%20state-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Constant%2520Parameters%253A%2520Hyper%2520Prediction%2520Models%2520and%2520HyperMPC%26entry.906535625%3DJan%2520W%25C4%2599grzynowski%2520and%2520Piotr%2520Kicki%2520and%2520Grzegorz%2520Czechmanowski%2520and%2520Maciej%2520Krupka%2520and%2520Krzysztof%2520Walas%26entry.1292438233%3D%2520%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520is%2520among%2520the%2520most%2520widely%2520adopted%2520and%2520reliable%250Amethods%2520for%2520robot%2520control%252C%2520relying%2520critically%2520on%2520an%2520accurate%2520dynamics%2520model.%250AHowever%252C%2520existing%2520dynamics%2520models%2520used%2520in%2520the%2520gradient-based%2520MPC%2520are%2520limited%2520by%250Acomputational%2520complexity%2520and%2520state%2520representation.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520the%2520Hyper%2520Prediction%2520Model%2520%2528HyperPM%2529%2520-%2520a%2520novel%2520approach%2520in%2520which%2520we%250Aproject%2520the%2520unmodeled%2520dynamics%2520onto%2520a%2520time-dependent%2520dynamics%2520model.%2520This%250Atime-dependency%2520is%2520captured%2520through%2520time-varying%2520model%2520parameters%252C%2520whose%250Aevolution%2520over%2520the%2520MPC%2520prediction%2520horizon%2520is%2520learned%2520using%2520a%2520neural%2520network.%250ASuch%2520formulation%2520preserves%2520the%2520computational%2520efficiency%2520and%2520robustness%2520of%2520the%250Abase%2520model%2520while%2520equipping%2520it%2520with%2520the%2520capacity%2520to%2520anticipate%2520previously%250Aunmodeled%2520phenomena.%2520We%2520evaluated%2520the%2520proposed%2520approach%2520on%2520several%2520challenging%250Asystems%252C%2520including%2520real-world%2520F1TENTH%2520autonomous%2520racing%252C%2520and%2520demonstrated%2520that%250Ait%2520significantly%2520reduces%2520long-horizon%2520prediction%2520errors.%2520Moreover%252C%2520when%250Aintegrated%2520within%2520the%2520MPC%2520framework%2520%2528HyperMPC%2529%252C%2520our%2520method%2520consistently%250Aoutperforms%2520existing%2520state-of-the-art%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Constant%20Parameters%3A%20Hyper%20Prediction%20Models%20and%20HyperMPC&entry.906535625=Jan%20W%C4%99grzynowski%20and%20Piotr%20Kicki%20and%20Grzegorz%20Czechmanowski%20and%20Maciej%20Krupka%20and%20Krzysztof%20Walas&entry.1292438233=%20%20Model%20Predictive%20Control%20%28MPC%29%20is%20among%20the%20most%20widely%20adopted%20and%20reliable%0Amethods%20for%20robot%20control%2C%20relying%20critically%20on%20an%20accurate%20dynamics%20model.%0AHowever%2C%20existing%20dynamics%20models%20used%20in%20the%20gradient-based%20MPC%20are%20limited%20by%0Acomputational%20complexity%20and%20state%20representation.%20To%20address%20this%20limitation%2C%0Awe%20propose%20the%20Hyper%20Prediction%20Model%20%28HyperPM%29%20-%20a%20novel%20approach%20in%20which%20we%0Aproject%20the%20unmodeled%20dynamics%20onto%20a%20time-dependent%20dynamics%20model.%20This%0Atime-dependency%20is%20captured%20through%20time-varying%20model%20parameters%2C%20whose%0Aevolution%20over%20the%20MPC%20prediction%20horizon%20is%20learned%20using%20a%20neural%20network.%0ASuch%20formulation%20preserves%20the%20computational%20efficiency%20and%20robustness%20of%20the%0Abase%20model%20while%20equipping%20it%20with%20the%20capacity%20to%20anticipate%20previously%0Aunmodeled%20phenomena.%20We%20evaluated%20the%20proposed%20approach%20on%20several%20challenging%0Asystems%2C%20including%20real-world%20F1TENTH%20autonomous%20racing%2C%20and%20demonstrated%20that%0Ait%20significantly%20reduces%20long-horizon%20prediction%20errors.%20Moreover%2C%20when%0Aintegrated%20within%20the%20MPC%20framework%20%28HyperMPC%29%2C%20our%20method%20consistently%0Aoutperforms%20existing%20state-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06181v1&entry.124074799=Read"},
{"title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?", "author": "Pedro Orvalho and Marta Kwiatkowska", "abstract": "  Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding. In this work, we evaluate whether\nstate-of-the-art LLMs with up to 8B parameters can reason about Python programs\nor are simply guessing. We apply five semantics-preserving code mutations:\nrenaming variables, mirroring comparison expressions, swapping if-else\nbranches, converting for loops to while, and loop unrolling. These mutations\nmaintain program semantics while altering its syntax. We evaluated six LLMs and\nperformed a human expert analysis using LiveCodeBench to assess whether the\ncorrect predictions are based on sound reasoning. We also evaluated prediction\nstability across different code mutations on LiveCodeBench and CruxEval. Our\nfindings show that LLMs trained for code produce correct predictions based on\nflawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change\npredictions in response to our code mutations, indicating they do not yet\nexhibit stable, semantically grounded reasoning.\n", "link": "http://arxiv.org/abs/2505.10443v2", "date": "2025-08-08", "relevancy": 2.0256, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F&body=Title%3A%20Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F%0AAuthor%3A%20Pedro%20Orvalho%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20Understanding%20the%20reasoning%20and%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Acritical%20for%20their%20reliable%20use%20in%20programming%20tasks.%20While%20recent%20studies%20have%0Aassessed%20LLMs%27%20ability%20to%20predict%20program%20outputs%2C%20most%20focus%20solely%20on%20the%0Aaccuracy%20of%20those%20predictions%2C%20without%20evaluating%20the%20reasoning%20behind%20them.%0AMoreover%2C%20it%20has%20been%20observed%20on%20mathematical%20reasoning%20tasks%20that%20LLMs%20can%0Aarrive%20at%20correct%20answers%20through%20flawed%20logic%2C%20raising%20concerns%20about%20similar%0Aissues%20in%20code%20understanding.%20In%20this%20work%2C%20we%20evaluate%20whether%0Astate-of-the-art%20LLMs%20with%20up%20to%208B%20parameters%20can%20reason%20about%20Python%20programs%0Aor%20are%20simply%20guessing.%20We%20apply%20five%20semantics-preserving%20code%20mutations%3A%0Arenaming%20variables%2C%20mirroring%20comparison%20expressions%2C%20swapping%20if-else%0Abranches%2C%20converting%20for%20loops%20to%20while%2C%20and%20loop%20unrolling.%20These%20mutations%0Amaintain%20program%20semantics%20while%20altering%20its%20syntax.%20We%20evaluated%20six%20LLMs%20and%0Aperformed%20a%20human%20expert%20analysis%20using%20LiveCodeBench%20to%20assess%20whether%20the%0Acorrect%20predictions%20are%20based%20on%20sound%20reasoning.%20We%20also%20evaluated%20prediction%0Astability%20across%20different%20code%20mutations%20on%20LiveCodeBench%20and%20CruxEval.%20Our%0Afindings%20show%20that%20LLMs%20trained%20for%20code%20produce%20correct%20predictions%20based%20on%0Aflawed%20reasoning%20between%2010%25%20and%2050%25%20of%20cases.%20Furthermore%2C%20LLMs%20often%20change%0Apredictions%20in%20response%20to%20our%20code%20mutations%2C%20indicating%20they%20do%20not%20yet%0Aexhibit%20stable%2C%20semantically%20grounded%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Robust%2520in%2520Understanding%2520Code%2520Against%250A%2520%2520Semantics-Preserving%2520Mutations%253F%26entry.906535625%3DPedro%2520Orvalho%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520Understanding%2520the%2520reasoning%2520and%2520robustness%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%250Acritical%2520for%2520their%2520reliable%2520use%2520in%2520programming%2520tasks.%2520While%2520recent%2520studies%2520have%250Aassessed%2520LLMs%2527%2520ability%2520to%2520predict%2520program%2520outputs%252C%2520most%2520focus%2520solely%2520on%2520the%250Aaccuracy%2520of%2520those%2520predictions%252C%2520without%2520evaluating%2520the%2520reasoning%2520behind%2520them.%250AMoreover%252C%2520it%2520has%2520been%2520observed%2520on%2520mathematical%2520reasoning%2520tasks%2520that%2520LLMs%2520can%250Aarrive%2520at%2520correct%2520answers%2520through%2520flawed%2520logic%252C%2520raising%2520concerns%2520about%2520similar%250Aissues%2520in%2520code%2520understanding.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520whether%250Astate-of-the-art%2520LLMs%2520with%2520up%2520to%25208B%2520parameters%2520can%2520reason%2520about%2520Python%2520programs%250Aor%2520are%2520simply%2520guessing.%2520We%2520apply%2520five%2520semantics-preserving%2520code%2520mutations%253A%250Arenaming%2520variables%252C%2520mirroring%2520comparison%2520expressions%252C%2520swapping%2520if-else%250Abranches%252C%2520converting%2520for%2520loops%2520to%2520while%252C%2520and%2520loop%2520unrolling.%2520These%2520mutations%250Amaintain%2520program%2520semantics%2520while%2520altering%2520its%2520syntax.%2520We%2520evaluated%2520six%2520LLMs%2520and%250Aperformed%2520a%2520human%2520expert%2520analysis%2520using%2520LiveCodeBench%2520to%2520assess%2520whether%2520the%250Acorrect%2520predictions%2520are%2520based%2520on%2520sound%2520reasoning.%2520We%2520also%2520evaluated%2520prediction%250Astability%2520across%2520different%2520code%2520mutations%2520on%2520LiveCodeBench%2520and%2520CruxEval.%2520Our%250Afindings%2520show%2520that%2520LLMs%2520trained%2520for%2520code%2520produce%2520correct%2520predictions%2520based%2520on%250Aflawed%2520reasoning%2520between%252010%2525%2520and%252050%2525%2520of%2520cases.%2520Furthermore%252C%2520LLMs%2520often%2520change%250Apredictions%2520in%2520response%2520to%2520our%2520code%2520mutations%252C%2520indicating%2520they%2520do%2520not%2520yet%250Aexhibit%2520stable%252C%2520semantically%2520grounded%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F&entry.906535625=Pedro%20Orvalho%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20Understanding%20the%20reasoning%20and%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Acritical%20for%20their%20reliable%20use%20in%20programming%20tasks.%20While%20recent%20studies%20have%0Aassessed%20LLMs%27%20ability%20to%20predict%20program%20outputs%2C%20most%20focus%20solely%20on%20the%0Aaccuracy%20of%20those%20predictions%2C%20without%20evaluating%20the%20reasoning%20behind%20them.%0AMoreover%2C%20it%20has%20been%20observed%20on%20mathematical%20reasoning%20tasks%20that%20LLMs%20can%0Aarrive%20at%20correct%20answers%20through%20flawed%20logic%2C%20raising%20concerns%20about%20similar%0Aissues%20in%20code%20understanding.%20In%20this%20work%2C%20we%20evaluate%20whether%0Astate-of-the-art%20LLMs%20with%20up%20to%208B%20parameters%20can%20reason%20about%20Python%20programs%0Aor%20are%20simply%20guessing.%20We%20apply%20five%20semantics-preserving%20code%20mutations%3A%0Arenaming%20variables%2C%20mirroring%20comparison%20expressions%2C%20swapping%20if-else%0Abranches%2C%20converting%20for%20loops%20to%20while%2C%20and%20loop%20unrolling.%20These%20mutations%0Amaintain%20program%20semantics%20while%20altering%20its%20syntax.%20We%20evaluated%20six%20LLMs%20and%0Aperformed%20a%20human%20expert%20analysis%20using%20LiveCodeBench%20to%20assess%20whether%20the%0Acorrect%20predictions%20are%20based%20on%20sound%20reasoning.%20We%20also%20evaluated%20prediction%0Astability%20across%20different%20code%20mutations%20on%20LiveCodeBench%20and%20CruxEval.%20Our%0Afindings%20show%20that%20LLMs%20trained%20for%20code%20produce%20correct%20predictions%20based%20on%0Aflawed%20reasoning%20between%2010%25%20and%2050%25%20of%20cases.%20Furthermore%2C%20LLMs%20often%20change%0Apredictions%20in%20response%20to%20our%20code%20mutations%2C%20indicating%20they%20do%20not%20yet%0Aexhibit%20stable%2C%20semantically%20grounded%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10443v2&entry.124074799=Read"},
{"title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated\n  Learning in Remote Sensing", "author": "Bar\u0131\u015f B\u00fcy\u00fckta\u015f and Jonas Klotz and Beg\u00fcm Demir", "abstract": "  Federated learning (FL) enables the collaborative training of deep neural\nnetworks across decentralized data archives (i.e., clients), where each client\nstores data locally and only shares model updates with a central server. This\nmakes FL a suitable learning paradigm for remote sensing (RS) image\nclassification tasks, where data centralization may be restricted due to legal\nand privacy constraints. However, a key challenge in applying FL to RS tasks is\nthe communication overhead caused by the frequent exchange of large model\nupdates between clients and the central server. To address this issue, in this\npaper we propose a novel strategy (denoted as FedX) that uses\nexplanation-guided pruning to reduce communication overhead by minimizing the\nsize of the transmitted models without compromising performance. FedX leverages\nbackpropagation-based explanation methods to estimate the task-specific\nimportance of model components and prunes the least relevant ones at the\ncentral server. The resulting sparse global model is then sent to clients,\nsubstantially reducing communication overhead. We evaluate FedX on multi-label\nscene classification using the BigEarthNet-S2 dataset and single-label scene\nclassification using the EuroSAT dataset. Experimental results show the success\nof FedX in significantly reducing the number of shared model parameters while\nenhancing the generalization capability of the global model, compared to both\nunpruned model and state-of-the-art pruning methods. The code of FedX will be\navailable at https://git.tu-berlin.de/rsim/FedX.\n", "link": "http://arxiv.org/abs/2508.06256v1", "date": "2025-08-08", "relevancy": 2.0172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%0A%20%20Learning%20in%20Remote%20Sensing&body=Title%3A%20FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%0A%20%20Learning%20in%20Remote%20Sensing%0AAuthor%3A%20Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Jonas%20Klotz%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaborative%20training%20of%20deep%20neural%0Anetworks%20across%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%2C%20where%20each%20client%0Astores%20data%20locally%20and%20only%20shares%20model%20updates%20with%20a%20central%20server.%20This%0Amakes%20FL%20a%20suitable%20learning%20paradigm%20for%20remote%20sensing%20%28RS%29%20image%0Aclassification%20tasks%2C%20where%20data%20centralization%20may%20be%20restricted%20due%20to%20legal%0Aand%20privacy%20constraints.%20However%2C%20a%20key%20challenge%20in%20applying%20FL%20to%20RS%20tasks%20is%0Athe%20communication%20overhead%20caused%20by%20the%20frequent%20exchange%20of%20large%20model%0Aupdates%20between%20clients%20and%20the%20central%20server.%20To%20address%20this%20issue%2C%20in%20this%0Apaper%20we%20propose%20a%20novel%20strategy%20%28denoted%20as%20FedX%29%20that%20uses%0Aexplanation-guided%20pruning%20to%20reduce%20communication%20overhead%20by%20minimizing%20the%0Asize%20of%20the%20transmitted%20models%20without%20compromising%20performance.%20FedX%20leverages%0Abackpropagation-based%20explanation%20methods%20to%20estimate%20the%20task-specific%0Aimportance%20of%20model%20components%20and%20prunes%20the%20least%20relevant%20ones%20at%20the%0Acentral%20server.%20The%20resulting%20sparse%20global%20model%20is%20then%20sent%20to%20clients%2C%0Asubstantially%20reducing%20communication%20overhead.%20We%20evaluate%20FedX%20on%20multi-label%0Ascene%20classification%20using%20the%20BigEarthNet-S2%20dataset%20and%20single-label%20scene%0Aclassification%20using%20the%20EuroSAT%20dataset.%20Experimental%20results%20show%20the%20success%0Aof%20FedX%20in%20significantly%20reducing%20the%20number%20of%20shared%20model%20parameters%20while%0Aenhancing%20the%20generalization%20capability%20of%20the%20global%20model%2C%20compared%20to%20both%0Aunpruned%20model%20and%20state-of-the-art%20pruning%20methods.%20The%20code%20of%20FedX%20will%20be%0Aavailable%20at%20https%3A//git.tu-berlin.de/rsim/FedX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedX%253A%2520Explanation-Guided%2520Pruning%2520for%2520Communication-Efficient%2520Federated%250A%2520%2520Learning%2520in%2520Remote%2520Sensing%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520B%25C3%25BCy%25C3%25BCkta%25C5%259F%2520and%2520Jonas%2520Klotz%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520the%2520collaborative%2520training%2520of%2520deep%2520neural%250Anetworks%2520across%2520decentralized%2520data%2520archives%2520%2528i.e.%252C%2520clients%2529%252C%2520where%2520each%2520client%250Astores%2520data%2520locally%2520and%2520only%2520shares%2520model%2520updates%2520with%2520a%2520central%2520server.%2520This%250Amakes%2520FL%2520a%2520suitable%2520learning%2520paradigm%2520for%2520remote%2520sensing%2520%2528RS%2529%2520image%250Aclassification%2520tasks%252C%2520where%2520data%2520centralization%2520may%2520be%2520restricted%2520due%2520to%2520legal%250Aand%2520privacy%2520constraints.%2520However%252C%2520a%2520key%2520challenge%2520in%2520applying%2520FL%2520to%2520RS%2520tasks%2520is%250Athe%2520communication%2520overhead%2520caused%2520by%2520the%2520frequent%2520exchange%2520of%2520large%2520model%250Aupdates%2520between%2520clients%2520and%2520the%2520central%2520server.%2520To%2520address%2520this%2520issue%252C%2520in%2520this%250Apaper%2520we%2520propose%2520a%2520novel%2520strategy%2520%2528denoted%2520as%2520FedX%2529%2520that%2520uses%250Aexplanation-guided%2520pruning%2520to%2520reduce%2520communication%2520overhead%2520by%2520minimizing%2520the%250Asize%2520of%2520the%2520transmitted%2520models%2520without%2520compromising%2520performance.%2520FedX%2520leverages%250Abackpropagation-based%2520explanation%2520methods%2520to%2520estimate%2520the%2520task-specific%250Aimportance%2520of%2520model%2520components%2520and%2520prunes%2520the%2520least%2520relevant%2520ones%2520at%2520the%250Acentral%2520server.%2520The%2520resulting%2520sparse%2520global%2520model%2520is%2520then%2520sent%2520to%2520clients%252C%250Asubstantially%2520reducing%2520communication%2520overhead.%2520We%2520evaluate%2520FedX%2520on%2520multi-label%250Ascene%2520classification%2520using%2520the%2520BigEarthNet-S2%2520dataset%2520and%2520single-label%2520scene%250Aclassification%2520using%2520the%2520EuroSAT%2520dataset.%2520Experimental%2520results%2520show%2520the%2520success%250Aof%2520FedX%2520in%2520significantly%2520reducing%2520the%2520number%2520of%2520shared%2520model%2520parameters%2520while%250Aenhancing%2520the%2520generalization%2520capability%2520of%2520the%2520global%2520model%252C%2520compared%2520to%2520both%250Aunpruned%2520model%2520and%2520state-of-the-art%2520pruning%2520methods.%2520The%2520code%2520of%2520FedX%2520will%2520be%250Aavailable%2520at%2520https%253A//git.tu-berlin.de/rsim/FedX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%0A%20%20Learning%20in%20Remote%20Sensing&entry.906535625=Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Jonas%20Klotz%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaborative%20training%20of%20deep%20neural%0Anetworks%20across%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%2C%20where%20each%20client%0Astores%20data%20locally%20and%20only%20shares%20model%20updates%20with%20a%20central%20server.%20This%0Amakes%20FL%20a%20suitable%20learning%20paradigm%20for%20remote%20sensing%20%28RS%29%20image%0Aclassification%20tasks%2C%20where%20data%20centralization%20may%20be%20restricted%20due%20to%20legal%0Aand%20privacy%20constraints.%20However%2C%20a%20key%20challenge%20in%20applying%20FL%20to%20RS%20tasks%20is%0Athe%20communication%20overhead%20caused%20by%20the%20frequent%20exchange%20of%20large%20model%0Aupdates%20between%20clients%20and%20the%20central%20server.%20To%20address%20this%20issue%2C%20in%20this%0Apaper%20we%20propose%20a%20novel%20strategy%20%28denoted%20as%20FedX%29%20that%20uses%0Aexplanation-guided%20pruning%20to%20reduce%20communication%20overhead%20by%20minimizing%20the%0Asize%20of%20the%20transmitted%20models%20without%20compromising%20performance.%20FedX%20leverages%0Abackpropagation-based%20explanation%20methods%20to%20estimate%20the%20task-specific%0Aimportance%20of%20model%20components%20and%20prunes%20the%20least%20relevant%20ones%20at%20the%0Acentral%20server.%20The%20resulting%20sparse%20global%20model%20is%20then%20sent%20to%20clients%2C%0Asubstantially%20reducing%20communication%20overhead.%20We%20evaluate%20FedX%20on%20multi-label%0Ascene%20classification%20using%20the%20BigEarthNet-S2%20dataset%20and%20single-label%20scene%0Aclassification%20using%20the%20EuroSAT%20dataset.%20Experimental%20results%20show%20the%20success%0Aof%20FedX%20in%20significantly%20reducing%20the%20number%20of%20shared%20model%20parameters%20while%0Aenhancing%20the%20generalization%20capability%20of%20the%20global%20model%2C%20compared%20to%20both%0Aunpruned%20model%20and%20state-of-the-art%20pruning%20methods.%20The%20code%20of%20FedX%20will%20be%0Aavailable%20at%20https%3A//git.tu-berlin.de/rsim/FedX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06256v1&entry.124074799=Read"},
{"title": "Are Your LLMs Capable of Stable Reasoning?", "author": "Junnan Liu and Hongwei Liu and Linchen Xiao and Ziyi Wang and Kuikun Liu and Songyang Gao and Wenwei Zhang and Songyang Zhang and Kai Chen", "abstract": "  The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics.\n", "link": "http://arxiv.org/abs/2412.13147v5", "date": "2025-08-08", "relevancy": 2.0163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&body=Title%3A%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F%0AAuthor%3A%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20shown%20remarkable%0Aprogress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%20disparity%20exists%0Abetween%20benchmark%20performances%20and%20real-world%20applications.%20We%20attribute%20this%0Agap%20primarily%20to%20current%20evaluation%20protocols%20and%20metrics%2C%20which%20inadequately%0Acapture%20the%20full%20spectrum%20of%20LLM%20capabilities%2C%20especially%20in%20complex%20reasoning%0Atasks%20where%20both%20accuracy%20and%20consistency%20are%20essential.%20In%20this%20paper%2C%20we%0Aintroduce%20G-Pass%40%24k%24%2C%20a%20novel%20evaluation%20metric%20that%20continuously%20assesses%0Amodel%20performance%20across%20multiple%20sampling%20attempts%2C%20quantifying%20both%20the%0Amodel%27s%20performance%20potential%20and%20its%20stability.%20Through%20extensive%20experiments%0Aon%20various%20public%20and%20newly%20constructed%20benchmarks%2C%20we%20employ%20G-Pass%40%24k%24%20in%0Aconjunction%20with%20state-of-the-art%20large%20language%20models%20to%20provide%0Acomprehensive%20insights%20into%20their%20potential%20capabilities%20and%20operational%0Aconsistency.%20Our%20findings%20reveal%20a%20significant%20opportunity%20to%20enhance%20the%0Arealistic%20reasoning%20abilities%20of%20LLMs%2C%20underscoring%20the%20necessity%20for%20more%0Arobust%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13147v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Your%2520LLMs%2520Capable%2520of%2520Stable%2520Reasoning%253F%26entry.906535625%3DJunnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Linchen%2520Xiao%2520and%2520Ziyi%2520Wang%2520and%2520Kuikun%2520Liu%2520and%2520Songyang%2520Gao%2520and%2520Wenwei%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520shown%2520remarkable%250Aprogress%2520in%2520complex%2520reasoning%2520tasks.%2520However%252C%2520a%2520significant%2520disparity%2520exists%250Abetween%2520benchmark%2520performances%2520and%2520real-world%2520applications.%2520We%2520attribute%2520this%250Agap%2520primarily%2520to%2520current%2520evaluation%2520protocols%2520and%2520metrics%252C%2520which%2520inadequately%250Acapture%2520the%2520full%2520spectrum%2520of%2520LLM%2520capabilities%252C%2520especially%2520in%2520complex%2520reasoning%250Atasks%2520where%2520both%2520accuracy%2520and%2520consistency%2520are%2520essential.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520G-Pass%2540%2524k%2524%252C%2520a%2520novel%2520evaluation%2520metric%2520that%2520continuously%2520assesses%250Amodel%2520performance%2520across%2520multiple%2520sampling%2520attempts%252C%2520quantifying%2520both%2520the%250Amodel%2527s%2520performance%2520potential%2520and%2520its%2520stability.%2520Through%2520extensive%2520experiments%250Aon%2520various%2520public%2520and%2520newly%2520constructed%2520benchmarks%252C%2520we%2520employ%2520G-Pass%2540%2524k%2524%2520in%250Aconjunction%2520with%2520state-of-the-art%2520large%2520language%2520models%2520to%2520provide%250Acomprehensive%2520insights%2520into%2520their%2520potential%2520capabilities%2520and%2520operational%250Aconsistency.%2520Our%2520findings%2520reveal%2520a%2520significant%2520opportunity%2520to%2520enhance%2520the%250Arealistic%2520reasoning%2520abilities%2520of%2520LLMs%252C%2520underscoring%2520the%2520necessity%2520for%2520more%250Arobust%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13147v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&entry.906535625=Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20shown%20remarkable%0Aprogress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%20disparity%20exists%0Abetween%20benchmark%20performances%20and%20real-world%20applications.%20We%20attribute%20this%0Agap%20primarily%20to%20current%20evaluation%20protocols%20and%20metrics%2C%20which%20inadequately%0Acapture%20the%20full%20spectrum%20of%20LLM%20capabilities%2C%20especially%20in%20complex%20reasoning%0Atasks%20where%20both%20accuracy%20and%20consistency%20are%20essential.%20In%20this%20paper%2C%20we%0Aintroduce%20G-Pass%40%24k%24%2C%20a%20novel%20evaluation%20metric%20that%20continuously%20assesses%0Amodel%20performance%20across%20multiple%20sampling%20attempts%2C%20quantifying%20both%20the%0Amodel%27s%20performance%20potential%20and%20its%20stability.%20Through%20extensive%20experiments%0Aon%20various%20public%20and%20newly%20constructed%20benchmarks%2C%20we%20employ%20G-Pass%40%24k%24%20in%0Aconjunction%20with%20state-of-the-art%20large%20language%20models%20to%20provide%0Acomprehensive%20insights%20into%20their%20potential%20capabilities%20and%20operational%0Aconsistency.%20Our%20findings%20reveal%20a%20significant%20opportunity%20to%20enhance%20the%0Arealistic%20reasoning%20abilities%20of%20LLMs%2C%20underscoring%20the%20necessity%20for%20more%0Arobust%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13147v5&entry.124074799=Read"},
{"title": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and\n  Community Challenge", "author": "Nikolaos Dionelis and Alessandra Feliciotti and Mattia Marconcini and Devis Peressutti and Nika Oman Kadunc and JaeWan Park and Hagai Raja Sinulingga and Steve Andreas Immanuel and Ba Tran and Caroline Arnold and Nicolas Long\u00e9p\u00e9", "abstract": "  Estimating the construction year of buildings is critical for advancing\nsustainability, as older structures often lack energy-efficient features.\nSustainable urban planning relies on accurate building age data to reduce\nenergy consumption and mitigate climate change. In this work, we introduce\nMapYourCity, a novel multi-modal benchmark dataset comprising top-view Very\nHigh Resolution (VHR) imagery, multi-spectral Earth Observation (EO) data from\nthe Copernicus Sentinel-2 constellation, and co-localized street-view images\nacross various European cities. Each building is labeled with its construction\nepoch, and the task is formulated as a seven-class classification problem\ncovering periods from 1900 to the present. To advance research in EO\ngeneralization and multi-modal learning, we organized a community-driven data\nchallenge in 2024, hosted by ESA $\\Phi$-lab, which ran for four months and\nattracted wide participation.\n  This paper presents the Top-4 performing models from the challenge and their\nevaluation results. We assess model generalization on cities excluded from\ntraining to prevent data leakage, and evaluate performance under missing\nmodality scenarios, particularly when street-view data is unavailable. Results\ndemonstrate that building age estimation is both feasible and effective, even\nin previously unseen cities and when relying solely on top-view satellite\nimagery (i.e. with VHR and Sentinel-2 images). The new MapYourCity dataset thus\nprovides a valuable resource for developing scalable, real-world solutions in\nsustainable urban analytics.\n", "link": "http://arxiv.org/abs/2502.13818v3", "date": "2025-08-08", "relevancy": 2.011, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge&body=Title%3A%20Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge%0AAuthor%3A%20Nikolaos%20Dionelis%20and%20Alessandra%20Feliciotti%20and%20Mattia%20Marconcini%20and%20Devis%20Peressutti%20and%20Nika%20Oman%20Kadunc%20and%20JaeWan%20Park%20and%20Hagai%20Raja%20Sinulingga%20and%20Steve%20Andreas%20Immanuel%20and%20Ba%20Tran%20and%20Caroline%20Arnold%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20Estimating%20the%20construction%20year%20of%20buildings%20is%20critical%20for%20advancing%0Asustainability%2C%20as%20older%20structures%20often%20lack%20energy-efficient%20features.%0ASustainable%20urban%20planning%20relies%20on%20accurate%20building%20age%20data%20to%20reduce%0Aenergy%20consumption%20and%20mitigate%20climate%20change.%20In%20this%20work%2C%20we%20introduce%0AMapYourCity%2C%20a%20novel%20multi-modal%20benchmark%20dataset%20comprising%20top-view%20Very%0AHigh%20Resolution%20%28VHR%29%20imagery%2C%20multi-spectral%20Earth%20Observation%20%28EO%29%20data%20from%0Athe%20Copernicus%20Sentinel-2%20constellation%2C%20and%20co-localized%20street-view%20images%0Aacross%20various%20European%20cities.%20Each%20building%20is%20labeled%20with%20its%20construction%0Aepoch%2C%20and%20the%20task%20is%20formulated%20as%20a%20seven-class%20classification%20problem%0Acovering%20periods%20from%201900%20to%20the%20present.%20To%20advance%20research%20in%20EO%0Ageneralization%20and%20multi-modal%20learning%2C%20we%20organized%20a%20community-driven%20data%0Achallenge%20in%202024%2C%20hosted%20by%20ESA%20%24%5CPhi%24-lab%2C%20which%20ran%20for%20four%20months%20and%0Aattracted%20wide%20participation.%0A%20%20This%20paper%20presents%20the%20Top-4%20performing%20models%20from%20the%20challenge%20and%20their%0Aevaluation%20results.%20We%20assess%20model%20generalization%20on%20cities%20excluded%20from%0Atraining%20to%20prevent%20data%20leakage%2C%20and%20evaluate%20performance%20under%20missing%0Amodality%20scenarios%2C%20particularly%20when%20street-view%20data%20is%20unavailable.%20Results%0Ademonstrate%20that%20building%20age%20estimation%20is%20both%20feasible%20and%20effective%2C%20even%0Ain%20previously%20unseen%20cities%20and%20when%20relying%20solely%20on%20top-view%20satellite%0Aimagery%20%28i.e.%20with%20VHR%20and%20Sentinel-2%20images%29.%20The%20new%20MapYourCity%20dataset%20thus%0Aprovides%20a%20valuable%20resource%20for%20developing%20scalable%2C%20real-world%20solutions%20in%0Asustainable%20urban%20analytics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13818v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Age%2520Estimation%253A%2520A%2520New%2520Multi-Modal%2520Benchmark%2520Dataset%2520and%250A%2520%2520Community%2520Challenge%26entry.906535625%3DNikolaos%2520Dionelis%2520and%2520Alessandra%2520Feliciotti%2520and%2520Mattia%2520Marconcini%2520and%2520Devis%2520Peressutti%2520and%2520Nika%2520Oman%2520Kadunc%2520and%2520JaeWan%2520Park%2520and%2520Hagai%2520Raja%2520Sinulingga%2520and%2520Steve%2520Andreas%2520Immanuel%2520and%2520Ba%2520Tran%2520and%2520Caroline%2520Arnold%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520Estimating%2520the%2520construction%2520year%2520of%2520buildings%2520is%2520critical%2520for%2520advancing%250Asustainability%252C%2520as%2520older%2520structures%2520often%2520lack%2520energy-efficient%2520features.%250ASustainable%2520urban%2520planning%2520relies%2520on%2520accurate%2520building%2520age%2520data%2520to%2520reduce%250Aenergy%2520consumption%2520and%2520mitigate%2520climate%2520change.%2520In%2520this%2520work%252C%2520we%2520introduce%250AMapYourCity%252C%2520a%2520novel%2520multi-modal%2520benchmark%2520dataset%2520comprising%2520top-view%2520Very%250AHigh%2520Resolution%2520%2528VHR%2529%2520imagery%252C%2520multi-spectral%2520Earth%2520Observation%2520%2528EO%2529%2520data%2520from%250Athe%2520Copernicus%2520Sentinel-2%2520constellation%252C%2520and%2520co-localized%2520street-view%2520images%250Aacross%2520various%2520European%2520cities.%2520Each%2520building%2520is%2520labeled%2520with%2520its%2520construction%250Aepoch%252C%2520and%2520the%2520task%2520is%2520formulated%2520as%2520a%2520seven-class%2520classification%2520problem%250Acovering%2520periods%2520from%25201900%2520to%2520the%2520present.%2520To%2520advance%2520research%2520in%2520EO%250Ageneralization%2520and%2520multi-modal%2520learning%252C%2520we%2520organized%2520a%2520community-driven%2520data%250Achallenge%2520in%25202024%252C%2520hosted%2520by%2520ESA%2520%2524%255CPhi%2524-lab%252C%2520which%2520ran%2520for%2520four%2520months%2520and%250Aattracted%2520wide%2520participation.%250A%2520%2520This%2520paper%2520presents%2520the%2520Top-4%2520performing%2520models%2520from%2520the%2520challenge%2520and%2520their%250Aevaluation%2520results.%2520We%2520assess%2520model%2520generalization%2520on%2520cities%2520excluded%2520from%250Atraining%2520to%2520prevent%2520data%2520leakage%252C%2520and%2520evaluate%2520performance%2520under%2520missing%250Amodality%2520scenarios%252C%2520particularly%2520when%2520street-view%2520data%2520is%2520unavailable.%2520Results%250Ademonstrate%2520that%2520building%2520age%2520estimation%2520is%2520both%2520feasible%2520and%2520effective%252C%2520even%250Ain%2520previously%2520unseen%2520cities%2520and%2520when%2520relying%2520solely%2520on%2520top-view%2520satellite%250Aimagery%2520%2528i.e.%2520with%2520VHR%2520and%2520Sentinel-2%2520images%2529.%2520The%2520new%2520MapYourCity%2520dataset%2520thus%250Aprovides%2520a%2520valuable%2520resource%2520for%2520developing%2520scalable%252C%2520real-world%2520solutions%2520in%250Asustainable%2520urban%2520analytics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13818v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge&entry.906535625=Nikolaos%20Dionelis%20and%20Alessandra%20Feliciotti%20and%20Mattia%20Marconcini%20and%20Devis%20Peressutti%20and%20Nika%20Oman%20Kadunc%20and%20JaeWan%20Park%20and%20Hagai%20Raja%20Sinulingga%20and%20Steve%20Andreas%20Immanuel%20and%20Ba%20Tran%20and%20Caroline%20Arnold%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20Estimating%20the%20construction%20year%20of%20buildings%20is%20critical%20for%20advancing%0Asustainability%2C%20as%20older%20structures%20often%20lack%20energy-efficient%20features.%0ASustainable%20urban%20planning%20relies%20on%20accurate%20building%20age%20data%20to%20reduce%0Aenergy%20consumption%20and%20mitigate%20climate%20change.%20In%20this%20work%2C%20we%20introduce%0AMapYourCity%2C%20a%20novel%20multi-modal%20benchmark%20dataset%20comprising%20top-view%20Very%0AHigh%20Resolution%20%28VHR%29%20imagery%2C%20multi-spectral%20Earth%20Observation%20%28EO%29%20data%20from%0Athe%20Copernicus%20Sentinel-2%20constellation%2C%20and%20co-localized%20street-view%20images%0Aacross%20various%20European%20cities.%20Each%20building%20is%20labeled%20with%20its%20construction%0Aepoch%2C%20and%20the%20task%20is%20formulated%20as%20a%20seven-class%20classification%20problem%0Acovering%20periods%20from%201900%20to%20the%20present.%20To%20advance%20research%20in%20EO%0Ageneralization%20and%20multi-modal%20learning%2C%20we%20organized%20a%20community-driven%20data%0Achallenge%20in%202024%2C%20hosted%20by%20ESA%20%24%5CPhi%24-lab%2C%20which%20ran%20for%20four%20months%20and%0Aattracted%20wide%20participation.%0A%20%20This%20paper%20presents%20the%20Top-4%20performing%20models%20from%20the%20challenge%20and%20their%0Aevaluation%20results.%20We%20assess%20model%20generalization%20on%20cities%20excluded%20from%0Atraining%20to%20prevent%20data%20leakage%2C%20and%20evaluate%20performance%20under%20missing%0Amodality%20scenarios%2C%20particularly%20when%20street-view%20data%20is%20unavailable.%20Results%0Ademonstrate%20that%20building%20age%20estimation%20is%20both%20feasible%20and%20effective%2C%20even%0Ain%20previously%20unseen%20cities%20and%20when%20relying%20solely%20on%20top-view%20satellite%0Aimagery%20%28i.e.%20with%20VHR%20and%20Sentinel-2%20images%29.%20The%20new%20MapYourCity%20dataset%20thus%0Aprovides%20a%20valuable%20resource%20for%20developing%20scalable%2C%20real-world%20solutions%20in%0Asustainable%20urban%20analytics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13818v3&entry.124074799=Read"},
{"title": "Large Language Model Data Generation for Enhanced Intent Recognition in\n  German Speech", "author": "Theresa Pekarek Rosin and Burak Can Kaplan and Stefan Wermter", "abstract": "  Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.\n", "link": "http://arxiv.org/abs/2508.06277v1", "date": "2025-08-08", "relevancy": 2.0083, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5064}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Data%20Generation%20for%20Enhanced%20Intent%20Recognition%20in%0A%20%20German%20Speech&body=Title%3A%20Large%20Language%20Model%20Data%20Generation%20for%20Enhanced%20Intent%20Recognition%20in%0A%20%20German%20Speech%0AAuthor%3A%20Theresa%20Pekarek%20Rosin%20and%20Burak%20Can%20Kaplan%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Intent%20recognition%20%28IR%29%20for%20speech%20commands%20is%20essential%20for%20artificial%0Aintelligence%20%28AI%29%20assistant%20systems%3B%20however%2C%20most%20existing%20approaches%20are%0Alimited%20to%20short%20commands%20and%20are%20predominantly%20developed%20for%20English.%20This%0Apaper%20addresses%20these%20limitations%20by%20focusing%20on%20IR%20from%20speech%20by%20elderly%0AGerman%20speakers.%20We%20propose%20a%20novel%20approach%20that%20combines%20an%20adapted%20Whisper%0AASR%20model%2C%20fine-tuned%20on%20elderly%20German%20speech%20%28SVC-de%29%2C%20with%20Transformer-based%0Alanguage%20models%20trained%20on%20synthetic%20text%20datasets%20generated%20by%20three%0Awell-known%20large%20language%20models%20%28LLMs%29%3A%20LeoLM%2C%20Llama3%2C%20and%20ChatGPT.%20To%0Aevaluate%20the%20robustness%20of%20our%20approach%2C%20we%20generate%20synthetic%20speech%20with%20a%0Atext-to-speech%20model%20and%20conduct%20extensive%20cross-dataset%20testing.%20Our%20results%0Ashow%20that%20synthetic%20LLM-generated%20data%20significantly%20boosts%20classification%0Aperformance%20and%20robustness%20to%20different%20speaking%20styles%20and%20unseen%20vocabulary.%0ANotably%2C%20we%20find%20that%20LeoLM%2C%20a%20smaller%2C%20domain-specific%2013B%20LLM%2C%20surpasses%20the%0Amuch%20larger%20ChatGPT%20%28175B%29%20in%20dataset%20quality%20for%20German%20intent%20recognition.%0AOur%20approach%20demonstrates%20that%20generative%20AI%20can%20effectively%20bridge%20data%20gaps%0Ain%20low-resource%20domains.%20We%20provide%20detailed%20documentation%20of%20our%20data%0Ageneration%20and%20training%20process%20to%20ensure%20transparency%20and%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Data%2520Generation%2520for%2520Enhanced%2520Intent%2520Recognition%2520in%250A%2520%2520German%2520Speech%26entry.906535625%3DTheresa%2520Pekarek%2520Rosin%2520and%2520Burak%2520Can%2520Kaplan%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520Intent%2520recognition%2520%2528IR%2529%2520for%2520speech%2520commands%2520is%2520essential%2520for%2520artificial%250Aintelligence%2520%2528AI%2529%2520assistant%2520systems%253B%2520however%252C%2520most%2520existing%2520approaches%2520are%250Alimited%2520to%2520short%2520commands%2520and%2520are%2520predominantly%2520developed%2520for%2520English.%2520This%250Apaper%2520addresses%2520these%2520limitations%2520by%2520focusing%2520on%2520IR%2520from%2520speech%2520by%2520elderly%250AGerman%2520speakers.%2520We%2520propose%2520a%2520novel%2520approach%2520that%2520combines%2520an%2520adapted%2520Whisper%250AASR%2520model%252C%2520fine-tuned%2520on%2520elderly%2520German%2520speech%2520%2528SVC-de%2529%252C%2520with%2520Transformer-based%250Alanguage%2520models%2520trained%2520on%2520synthetic%2520text%2520datasets%2520generated%2520by%2520three%250Awell-known%2520large%2520language%2520models%2520%2528LLMs%2529%253A%2520LeoLM%252C%2520Llama3%252C%2520and%2520ChatGPT.%2520To%250Aevaluate%2520the%2520robustness%2520of%2520our%2520approach%252C%2520we%2520generate%2520synthetic%2520speech%2520with%2520a%250Atext-to-speech%2520model%2520and%2520conduct%2520extensive%2520cross-dataset%2520testing.%2520Our%2520results%250Ashow%2520that%2520synthetic%2520LLM-generated%2520data%2520significantly%2520boosts%2520classification%250Aperformance%2520and%2520robustness%2520to%2520different%2520speaking%2520styles%2520and%2520unseen%2520vocabulary.%250ANotably%252C%2520we%2520find%2520that%2520LeoLM%252C%2520a%2520smaller%252C%2520domain-specific%252013B%2520LLM%252C%2520surpasses%2520the%250Amuch%2520larger%2520ChatGPT%2520%2528175B%2529%2520in%2520dataset%2520quality%2520for%2520German%2520intent%2520recognition.%250AOur%2520approach%2520demonstrates%2520that%2520generative%2520AI%2520can%2520effectively%2520bridge%2520data%2520gaps%250Ain%2520low-resource%2520domains.%2520We%2520provide%2520detailed%2520documentation%2520of%2520our%2520data%250Ageneration%2520and%2520training%2520process%2520to%2520ensure%2520transparency%2520and%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Data%20Generation%20for%20Enhanced%20Intent%20Recognition%20in%0A%20%20German%20Speech&entry.906535625=Theresa%20Pekarek%20Rosin%20and%20Burak%20Can%20Kaplan%20and%20Stefan%20Wermter&entry.1292438233=%20%20Intent%20recognition%20%28IR%29%20for%20speech%20commands%20is%20essential%20for%20artificial%0Aintelligence%20%28AI%29%20assistant%20systems%3B%20however%2C%20most%20existing%20approaches%20are%0Alimited%20to%20short%20commands%20and%20are%20predominantly%20developed%20for%20English.%20This%0Apaper%20addresses%20these%20limitations%20by%20focusing%20on%20IR%20from%20speech%20by%20elderly%0AGerman%20speakers.%20We%20propose%20a%20novel%20approach%20that%20combines%20an%20adapted%20Whisper%0AASR%20model%2C%20fine-tuned%20on%20elderly%20German%20speech%20%28SVC-de%29%2C%20with%20Transformer-based%0Alanguage%20models%20trained%20on%20synthetic%20text%20datasets%20generated%20by%20three%0Awell-known%20large%20language%20models%20%28LLMs%29%3A%20LeoLM%2C%20Llama3%2C%20and%20ChatGPT.%20To%0Aevaluate%20the%20robustness%20of%20our%20approach%2C%20we%20generate%20synthetic%20speech%20with%20a%0Atext-to-speech%20model%20and%20conduct%20extensive%20cross-dataset%20testing.%20Our%20results%0Ashow%20that%20synthetic%20LLM-generated%20data%20significantly%20boosts%20classification%0Aperformance%20and%20robustness%20to%20different%20speaking%20styles%20and%20unseen%20vocabulary.%0ANotably%2C%20we%20find%20that%20LeoLM%2C%20a%20smaller%2C%20domain-specific%2013B%20LLM%2C%20surpasses%20the%0Amuch%20larger%20ChatGPT%20%28175B%29%20in%20dataset%20quality%20for%20German%20intent%20recognition.%0AOur%20approach%20demonstrates%20that%20generative%20AI%20can%20effectively%20bridge%20data%20gaps%0Ain%20low-resource%20domains.%20We%20provide%20detailed%20documentation%20of%20our%20data%0Ageneration%20and%20training%20process%20to%20ensure%20transparency%20and%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06277v1&entry.124074799=Read"},
{"title": "Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid\n  Robot", "author": "Davide Gorbani and Giuseppe L'Erario and Hosameldin Awadalla Omer Mohamed and Daniele Pucci", "abstract": "  We propose a novel Model Predictive Control (MPC) framework for a jet-powered\nflying humanoid robot. The controller is based on a linearised centroidal\nmomentum model to represent the flight dynamics, augmented with a second-order\nnonlinear model to explicitly account for the slow and nonlinear dynamics of\njet propulsion. A key contribution is the introduction of a multi-rate MPC\nformulation that handles the different actuation rates of the robot's joints\nand jet engines while embedding the jet dynamics directly into the predictive\nmodel. We validated the framework using the jet-powered humanoid robot iRonCub,\nperforming simulations in Mujoco; the simulation results demonstrate the\nrobot's ability to recover from external disturbances and perform stable,\nnon-abrupt flight manoeuvres, validating the effectiveness of the proposed\napproach.\n", "link": "http://arxiv.org/abs/2505.16478v2", "date": "2025-08-08", "relevancy": 2.003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5718}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5036}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multi-Rate%20Model%20Predictive%20Control%20for%20a%20Jet-Powered%20Humanoid%0A%20%20Robot&body=Title%3A%20Unified%20Multi-Rate%20Model%20Predictive%20Control%20for%20a%20Jet-Powered%20Humanoid%0A%20%20Robot%0AAuthor%3A%20Davide%20Gorbani%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Daniele%20Pucci%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20Model%20Predictive%20Control%20%28MPC%29%20framework%20for%20a%20jet-powered%0Aflying%20humanoid%20robot.%20The%20controller%20is%20based%20on%20a%20linearised%20centroidal%0Amomentum%20model%20to%20represent%20the%20flight%20dynamics%2C%20augmented%20with%20a%20second-order%0Anonlinear%20model%20to%20explicitly%20account%20for%20the%20slow%20and%20nonlinear%20dynamics%20of%0Ajet%20propulsion.%20A%20key%20contribution%20is%20the%20introduction%20of%20a%20multi-rate%20MPC%0Aformulation%20that%20handles%20the%20different%20actuation%20rates%20of%20the%20robot%27s%20joints%0Aand%20jet%20engines%20while%20embedding%20the%20jet%20dynamics%20directly%20into%20the%20predictive%0Amodel.%20We%20validated%20the%20framework%20using%20the%20jet-powered%20humanoid%20robot%20iRonCub%2C%0Aperforming%20simulations%20in%20Mujoco%3B%20the%20simulation%20results%20demonstrate%20the%0Arobot%27s%20ability%20to%20recover%20from%20external%20disturbances%20and%20perform%20stable%2C%0Anon-abrupt%20flight%20manoeuvres%2C%20validating%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multi-Rate%2520Model%2520Predictive%2520Control%2520for%2520a%2520Jet-Powered%2520Humanoid%250A%2520%2520Robot%26entry.906535625%3DDavide%2520Gorbani%2520and%2520Giuseppe%2520L%2527Erario%2520and%2520Hosameldin%2520Awadalla%2520Omer%2520Mohamed%2520and%2520Daniele%2520Pucci%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520framework%2520for%2520a%2520jet-powered%250Aflying%2520humanoid%2520robot.%2520The%2520controller%2520is%2520based%2520on%2520a%2520linearised%2520centroidal%250Amomentum%2520model%2520to%2520represent%2520the%2520flight%2520dynamics%252C%2520augmented%2520with%2520a%2520second-order%250Anonlinear%2520model%2520to%2520explicitly%2520account%2520for%2520the%2520slow%2520and%2520nonlinear%2520dynamics%2520of%250Ajet%2520propulsion.%2520A%2520key%2520contribution%2520is%2520the%2520introduction%2520of%2520a%2520multi-rate%2520MPC%250Aformulation%2520that%2520handles%2520the%2520different%2520actuation%2520rates%2520of%2520the%2520robot%2527s%2520joints%250Aand%2520jet%2520engines%2520while%2520embedding%2520the%2520jet%2520dynamics%2520directly%2520into%2520the%2520predictive%250Amodel.%2520We%2520validated%2520the%2520framework%2520using%2520the%2520jet-powered%2520humanoid%2520robot%2520iRonCub%252C%250Aperforming%2520simulations%2520in%2520Mujoco%253B%2520the%2520simulation%2520results%2520demonstrate%2520the%250Arobot%2527s%2520ability%2520to%2520recover%2520from%2520external%2520disturbances%2520and%2520perform%2520stable%252C%250Anon-abrupt%2520flight%2520manoeuvres%252C%2520validating%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multi-Rate%20Model%20Predictive%20Control%20for%20a%20Jet-Powered%20Humanoid%0A%20%20Robot&entry.906535625=Davide%20Gorbani%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Daniele%20Pucci&entry.1292438233=%20%20We%20propose%20a%20novel%20Model%20Predictive%20Control%20%28MPC%29%20framework%20for%20a%20jet-powered%0Aflying%20humanoid%20robot.%20The%20controller%20is%20based%20on%20a%20linearised%20centroidal%0Amomentum%20model%20to%20represent%20the%20flight%20dynamics%2C%20augmented%20with%20a%20second-order%0Anonlinear%20model%20to%20explicitly%20account%20for%20the%20slow%20and%20nonlinear%20dynamics%20of%0Ajet%20propulsion.%20A%20key%20contribution%20is%20the%20introduction%20of%20a%20multi-rate%20MPC%0Aformulation%20that%20handles%20the%20different%20actuation%20rates%20of%20the%20robot%27s%20joints%0Aand%20jet%20engines%20while%20embedding%20the%20jet%20dynamics%20directly%20into%20the%20predictive%0Amodel.%20We%20validated%20the%20framework%20using%20the%20jet-powered%20humanoid%20robot%20iRonCub%2C%0Aperforming%20simulations%20in%20Mujoco%3B%20the%20simulation%20results%20demonstrate%20the%0Arobot%27s%20ability%20to%20recover%20from%20external%20disturbances%20and%20perform%20stable%2C%0Anon-abrupt%20flight%20manoeuvres%2C%20validating%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16478v2&entry.124074799=Read"},
{"title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with\n  Non-linear Reset Feedback", "author": "Sanja Karilanova and Subhrakanti Dey and Ay\u00e7a \u00d6z\u00e7elikkale", "abstract": "  Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models.\n", "link": "http://arxiv.org/abs/2508.06292v1", "date": "2025-08-08", "relevancy": 2.0013, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5191}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4995}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Bit%20Data%20Processing%20Using%20Multiple-Output%20Spiking%20Neurons%20with%0A%20%20Non-linear%20Reset%20Feedback&body=Title%3A%20Low-Bit%20Data%20Processing%20Using%20Multiple-Output%20Spiking%20Neurons%20with%0A%20%20Non-linear%20Reset%20Feedback%0AAuthor%3A%20Sanja%20Karilanova%20and%20Subhrakanti%20Dey%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale%0AAbstract%3A%20%20%20Neuromorphic%20computing%20is%20an%20emerging%20technology%20enabling%20low-latency%20and%0Aenergy-efficient%20signal%20processing.%20A%20key%20algorithmic%20tool%20in%20neuromorphic%0Acomputing%20is%20spiking%20neural%20networks%20%28SNNs%29.%20SNNs%20are%20biologically%20inspired%0Aneural%20networks%20which%20utilize%20stateful%20neurons%2C%20and%20provide%20low-bit%20data%0Aprocessing%20by%20encoding%20and%20decoding%20information%20using%20spikes.%20Similar%20to%20SNNs%2C%0Adeep%20state-space%20models%20%28SSMs%29%20utilize%20stateful%20building%20blocks.%20However%2C%20deep%0ASSMs%2C%20which%20recently%20achieved%20competitive%20performance%20in%20various%20temporal%0Amodeling%20tasks%2C%20are%20typically%20designed%20with%20high-precision%20activation%20functions%0Aand%20no%20reset%20mechanisms.%20To%20bridge%20the%20gains%20offered%20by%20SNNs%20and%20the%20recent%0Adeep%20SSM%20models%2C%20we%20propose%20a%20novel%20multiple-output%20spiking%20neuron%20model%20that%0Acombines%20a%20linear%2C%20general%20SSM%20state%20transition%20with%20a%20non-linear%20feedback%0Amechanism%20through%20reset.%20Compared%20to%20the%20existing%20neuron%20models%20for%20SNNs%2C%20our%0Aproposed%20model%20clearly%20conceptualizes%20the%20differences%20between%20the%20spiking%0Afunction%2C%20the%20reset%20condition%20and%20the%20reset%20action.%20The%20experimental%20results%20on%0Avarious%20tasks%2C%20i.e.%2C%20a%20keyword%20spotting%20task%2C%20an%20event-based%20vision%20task%20and%20a%0Asequential%20pattern%20recognition%20task%2C%20show%20that%20our%20proposed%20model%20achieves%0Aperformance%20comparable%20to%20existing%20benchmarks%20in%20the%20SNN%20literature.%20Our%0Aresults%20illustrate%20how%20the%20proposed%20reset%20mechanism%20can%20overcome%20instability%0Aand%20enable%20learning%20even%20when%20the%20linear%20part%20of%20neuron%20dynamics%20is%20unstable%2C%0Aallowing%20us%20to%20go%20beyond%20the%20strictly%20enforced%20stability%20of%20linear%20dynamics%20in%0Arecent%20deep%20SSM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Bit%2520Data%2520Processing%2520Using%2520Multiple-Output%2520Spiking%2520Neurons%2520with%250A%2520%2520Non-linear%2520Reset%2520Feedback%26entry.906535625%3DSanja%2520Karilanova%2520and%2520Subhrakanti%2520Dey%2520and%2520Ay%25C3%25A7a%2520%25C3%2596z%25C3%25A7elikkale%26entry.1292438233%3D%2520%2520Neuromorphic%2520computing%2520is%2520an%2520emerging%2520technology%2520enabling%2520low-latency%2520and%250Aenergy-efficient%2520signal%2520processing.%2520A%2520key%2520algorithmic%2520tool%2520in%2520neuromorphic%250Acomputing%2520is%2520spiking%2520neural%2520networks%2520%2528SNNs%2529.%2520SNNs%2520are%2520biologically%2520inspired%250Aneural%2520networks%2520which%2520utilize%2520stateful%2520neurons%252C%2520and%2520provide%2520low-bit%2520data%250Aprocessing%2520by%2520encoding%2520and%2520decoding%2520information%2520using%2520spikes.%2520Similar%2520to%2520SNNs%252C%250Adeep%2520state-space%2520models%2520%2528SSMs%2529%2520utilize%2520stateful%2520building%2520blocks.%2520However%252C%2520deep%250ASSMs%252C%2520which%2520recently%2520achieved%2520competitive%2520performance%2520in%2520various%2520temporal%250Amodeling%2520tasks%252C%2520are%2520typically%2520designed%2520with%2520high-precision%2520activation%2520functions%250Aand%2520no%2520reset%2520mechanisms.%2520To%2520bridge%2520the%2520gains%2520offered%2520by%2520SNNs%2520and%2520the%2520recent%250Adeep%2520SSM%2520models%252C%2520we%2520propose%2520a%2520novel%2520multiple-output%2520spiking%2520neuron%2520model%2520that%250Acombines%2520a%2520linear%252C%2520general%2520SSM%2520state%2520transition%2520with%2520a%2520non-linear%2520feedback%250Amechanism%2520through%2520reset.%2520Compared%2520to%2520the%2520existing%2520neuron%2520models%2520for%2520SNNs%252C%2520our%250Aproposed%2520model%2520clearly%2520conceptualizes%2520the%2520differences%2520between%2520the%2520spiking%250Afunction%252C%2520the%2520reset%2520condition%2520and%2520the%2520reset%2520action.%2520The%2520experimental%2520results%2520on%250Avarious%2520tasks%252C%2520i.e.%252C%2520a%2520keyword%2520spotting%2520task%252C%2520an%2520event-based%2520vision%2520task%2520and%2520a%250Asequential%2520pattern%2520recognition%2520task%252C%2520show%2520that%2520our%2520proposed%2520model%2520achieves%250Aperformance%2520comparable%2520to%2520existing%2520benchmarks%2520in%2520the%2520SNN%2520literature.%2520Our%250Aresults%2520illustrate%2520how%2520the%2520proposed%2520reset%2520mechanism%2520can%2520overcome%2520instability%250Aand%2520enable%2520learning%2520even%2520when%2520the%2520linear%2520part%2520of%2520neuron%2520dynamics%2520is%2520unstable%252C%250Aallowing%2520us%2520to%2520go%2520beyond%2520the%2520strictly%2520enforced%2520stability%2520of%2520linear%2520dynamics%2520in%250Arecent%2520deep%2520SSM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Bit%20Data%20Processing%20Using%20Multiple-Output%20Spiking%20Neurons%20with%0A%20%20Non-linear%20Reset%20Feedback&entry.906535625=Sanja%20Karilanova%20and%20Subhrakanti%20Dey%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale&entry.1292438233=%20%20Neuromorphic%20computing%20is%20an%20emerging%20technology%20enabling%20low-latency%20and%0Aenergy-efficient%20signal%20processing.%20A%20key%20algorithmic%20tool%20in%20neuromorphic%0Acomputing%20is%20spiking%20neural%20networks%20%28SNNs%29.%20SNNs%20are%20biologically%20inspired%0Aneural%20networks%20which%20utilize%20stateful%20neurons%2C%20and%20provide%20low-bit%20data%0Aprocessing%20by%20encoding%20and%20decoding%20information%20using%20spikes.%20Similar%20to%20SNNs%2C%0Adeep%20state-space%20models%20%28SSMs%29%20utilize%20stateful%20building%20blocks.%20However%2C%20deep%0ASSMs%2C%20which%20recently%20achieved%20competitive%20performance%20in%20various%20temporal%0Amodeling%20tasks%2C%20are%20typically%20designed%20with%20high-precision%20activation%20functions%0Aand%20no%20reset%20mechanisms.%20To%20bridge%20the%20gains%20offered%20by%20SNNs%20and%20the%20recent%0Adeep%20SSM%20models%2C%20we%20propose%20a%20novel%20multiple-output%20spiking%20neuron%20model%20that%0Acombines%20a%20linear%2C%20general%20SSM%20state%20transition%20with%20a%20non-linear%20feedback%0Amechanism%20through%20reset.%20Compared%20to%20the%20existing%20neuron%20models%20for%20SNNs%2C%20our%0Aproposed%20model%20clearly%20conceptualizes%20the%20differences%20between%20the%20spiking%0Afunction%2C%20the%20reset%20condition%20and%20the%20reset%20action.%20The%20experimental%20results%20on%0Avarious%20tasks%2C%20i.e.%2C%20a%20keyword%20spotting%20task%2C%20an%20event-based%20vision%20task%20and%20a%0Asequential%20pattern%20recognition%20task%2C%20show%20that%20our%20proposed%20model%20achieves%0Aperformance%20comparable%20to%20existing%20benchmarks%20in%20the%20SNN%20literature.%20Our%0Aresults%20illustrate%20how%20the%20proposed%20reset%20mechanism%20can%20overcome%20instability%0Aand%20enable%20learning%20even%20when%20the%20linear%20part%20of%20neuron%20dynamics%20is%20unstable%2C%0Aallowing%20us%20to%20go%20beyond%20the%20strictly%20enforced%20stability%20of%20linear%20dynamics%20in%0Arecent%20deep%20SSM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06292v1&entry.124074799=Read"},
{"title": "Blockchain-Enabled Federated Learning", "author": "Murtaza Rangwala and Venugopal K R and Rajkumar Buyya", "abstract": "  Blockchain-enabled federated learning (BCFL) addresses fundamental challenges\nof trust, privacy, and coordination in collaborative AI systems. This chapter\nprovides comprehensive architectural analysis of BCFL systems through a\nsystematic four-dimensional taxonomy examining coordination structures,\nconsensus mechanisms, storage architectures, and trust models. We analyze\ndesign patterns from blockchain-verified centralized coordination to fully\ndecentralized peer-to-peer networks, evaluating trade-offs in scalability,\nsecurity, and performance. Through detailed examination of consensus mechanisms\ndesigned for federated learning contexts, including Proof of Quality and Proof\nof Federated Learning, we demonstrate how computational work can be repurposed\nfrom arbitrary cryptographic puzzles to productive machine learning tasks. The\nchapter addresses critical storage challenges by examining multi-tier\narchitectures that balance blockchain's transaction constraints with neural\nnetworks' large parameter requirements while maintaining cryptographic\nintegrity. A technical case study of the TrustMesh framework illustrates\npractical implementation considerations in BCFL systems through distributed\nimage classification training, demonstrating effective collaborative learning\nacross IoT devices with highly non-IID data distributions while maintaining\ncomplete transparency and fault tolerance. Analysis of real-world deployments\nacross healthcare consortiums, financial services, and IoT security\napplications validates the practical viability of BCFL systems, achieving\nperformance comparable to centralized approaches while providing enhanced\nsecurity guarantees and enabling new models of trustless collaborative\nintelligence.\n", "link": "http://arxiv.org/abs/2508.06406v1", "date": "2025-08-08", "relevancy": 1.8297, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blockchain-Enabled%20Federated%20Learning&body=Title%3A%20Blockchain-Enabled%20Federated%20Learning%0AAuthor%3A%20Murtaza%20Rangwala%20and%20Venugopal%20K%20R%20and%20Rajkumar%20Buyya%0AAbstract%3A%20%20%20Blockchain-enabled%20federated%20learning%20%28BCFL%29%20addresses%20fundamental%20challenges%0Aof%20trust%2C%20privacy%2C%20and%20coordination%20in%20collaborative%20AI%20systems.%20This%20chapter%0Aprovides%20comprehensive%20architectural%20analysis%20of%20BCFL%20systems%20through%20a%0Asystematic%20four-dimensional%20taxonomy%20examining%20coordination%20structures%2C%0Aconsensus%20mechanisms%2C%20storage%20architectures%2C%20and%20trust%20models.%20We%20analyze%0Adesign%20patterns%20from%20blockchain-verified%20centralized%20coordination%20to%20fully%0Adecentralized%20peer-to-peer%20networks%2C%20evaluating%20trade-offs%20in%20scalability%2C%0Asecurity%2C%20and%20performance.%20Through%20detailed%20examination%20of%20consensus%20mechanisms%0Adesigned%20for%20federated%20learning%20contexts%2C%20including%20Proof%20of%20Quality%20and%20Proof%0Aof%20Federated%20Learning%2C%20we%20demonstrate%20how%20computational%20work%20can%20be%20repurposed%0Afrom%20arbitrary%20cryptographic%20puzzles%20to%20productive%20machine%20learning%20tasks.%20The%0Achapter%20addresses%20critical%20storage%20challenges%20by%20examining%20multi-tier%0Aarchitectures%20that%20balance%20blockchain%27s%20transaction%20constraints%20with%20neural%0Anetworks%27%20large%20parameter%20requirements%20while%20maintaining%20cryptographic%0Aintegrity.%20A%20technical%20case%20study%20of%20the%20TrustMesh%20framework%20illustrates%0Apractical%20implementation%20considerations%20in%20BCFL%20systems%20through%20distributed%0Aimage%20classification%20training%2C%20demonstrating%20effective%20collaborative%20learning%0Aacross%20IoT%20devices%20with%20highly%20non-IID%20data%20distributions%20while%20maintaining%0Acomplete%20transparency%20and%20fault%20tolerance.%20Analysis%20of%20real-world%20deployments%0Aacross%20healthcare%20consortiums%2C%20financial%20services%2C%20and%20IoT%20security%0Aapplications%20validates%20the%20practical%20viability%20of%20BCFL%20systems%2C%20achieving%0Aperformance%20comparable%20to%20centralized%20approaches%20while%20providing%20enhanced%0Asecurity%20guarantees%20and%20enabling%20new%20models%20of%20trustless%20collaborative%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlockchain-Enabled%2520Federated%2520Learning%26entry.906535625%3DMurtaza%2520Rangwala%2520and%2520Venugopal%2520K%2520R%2520and%2520Rajkumar%2520Buyya%26entry.1292438233%3D%2520%2520Blockchain-enabled%2520federated%2520learning%2520%2528BCFL%2529%2520addresses%2520fundamental%2520challenges%250Aof%2520trust%252C%2520privacy%252C%2520and%2520coordination%2520in%2520collaborative%2520AI%2520systems.%2520This%2520chapter%250Aprovides%2520comprehensive%2520architectural%2520analysis%2520of%2520BCFL%2520systems%2520through%2520a%250Asystematic%2520four-dimensional%2520taxonomy%2520examining%2520coordination%2520structures%252C%250Aconsensus%2520mechanisms%252C%2520storage%2520architectures%252C%2520and%2520trust%2520models.%2520We%2520analyze%250Adesign%2520patterns%2520from%2520blockchain-verified%2520centralized%2520coordination%2520to%2520fully%250Adecentralized%2520peer-to-peer%2520networks%252C%2520evaluating%2520trade-offs%2520in%2520scalability%252C%250Asecurity%252C%2520and%2520performance.%2520Through%2520detailed%2520examination%2520of%2520consensus%2520mechanisms%250Adesigned%2520for%2520federated%2520learning%2520contexts%252C%2520including%2520Proof%2520of%2520Quality%2520and%2520Proof%250Aof%2520Federated%2520Learning%252C%2520we%2520demonstrate%2520how%2520computational%2520work%2520can%2520be%2520repurposed%250Afrom%2520arbitrary%2520cryptographic%2520puzzles%2520to%2520productive%2520machine%2520learning%2520tasks.%2520The%250Achapter%2520addresses%2520critical%2520storage%2520challenges%2520by%2520examining%2520multi-tier%250Aarchitectures%2520that%2520balance%2520blockchain%2527s%2520transaction%2520constraints%2520with%2520neural%250Anetworks%2527%2520large%2520parameter%2520requirements%2520while%2520maintaining%2520cryptographic%250Aintegrity.%2520A%2520technical%2520case%2520study%2520of%2520the%2520TrustMesh%2520framework%2520illustrates%250Apractical%2520implementation%2520considerations%2520in%2520BCFL%2520systems%2520through%2520distributed%250Aimage%2520classification%2520training%252C%2520demonstrating%2520effective%2520collaborative%2520learning%250Aacross%2520IoT%2520devices%2520with%2520highly%2520non-IID%2520data%2520distributions%2520while%2520maintaining%250Acomplete%2520transparency%2520and%2520fault%2520tolerance.%2520Analysis%2520of%2520real-world%2520deployments%250Aacross%2520healthcare%2520consortiums%252C%2520financial%2520services%252C%2520and%2520IoT%2520security%250Aapplications%2520validates%2520the%2520practical%2520viability%2520of%2520BCFL%2520systems%252C%2520achieving%250Aperformance%2520comparable%2520to%2520centralized%2520approaches%2520while%2520providing%2520enhanced%250Asecurity%2520guarantees%2520and%2520enabling%2520new%2520models%2520of%2520trustless%2520collaborative%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blockchain-Enabled%20Federated%20Learning&entry.906535625=Murtaza%20Rangwala%20and%20Venugopal%20K%20R%20and%20Rajkumar%20Buyya&entry.1292438233=%20%20Blockchain-enabled%20federated%20learning%20%28BCFL%29%20addresses%20fundamental%20challenges%0Aof%20trust%2C%20privacy%2C%20and%20coordination%20in%20collaborative%20AI%20systems.%20This%20chapter%0Aprovides%20comprehensive%20architectural%20analysis%20of%20BCFL%20systems%20through%20a%0Asystematic%20four-dimensional%20taxonomy%20examining%20coordination%20structures%2C%0Aconsensus%20mechanisms%2C%20storage%20architectures%2C%20and%20trust%20models.%20We%20analyze%0Adesign%20patterns%20from%20blockchain-verified%20centralized%20coordination%20to%20fully%0Adecentralized%20peer-to-peer%20networks%2C%20evaluating%20trade-offs%20in%20scalability%2C%0Asecurity%2C%20and%20performance.%20Through%20detailed%20examination%20of%20consensus%20mechanisms%0Adesigned%20for%20federated%20learning%20contexts%2C%20including%20Proof%20of%20Quality%20and%20Proof%0Aof%20Federated%20Learning%2C%20we%20demonstrate%20how%20computational%20work%20can%20be%20repurposed%0Afrom%20arbitrary%20cryptographic%20puzzles%20to%20productive%20machine%20learning%20tasks.%20The%0Achapter%20addresses%20critical%20storage%20challenges%20by%20examining%20multi-tier%0Aarchitectures%20that%20balance%20blockchain%27s%20transaction%20constraints%20with%20neural%0Anetworks%27%20large%20parameter%20requirements%20while%20maintaining%20cryptographic%0Aintegrity.%20A%20technical%20case%20study%20of%20the%20TrustMesh%20framework%20illustrates%0Apractical%20implementation%20considerations%20in%20BCFL%20systems%20through%20distributed%0Aimage%20classification%20training%2C%20demonstrating%20effective%20collaborative%20learning%0Aacross%20IoT%20devices%20with%20highly%20non-IID%20data%20distributions%20while%20maintaining%0Acomplete%20transparency%20and%20fault%20tolerance.%20Analysis%20of%20real-world%20deployments%0Aacross%20healthcare%20consortiums%2C%20financial%20services%2C%20and%20IoT%20security%0Aapplications%20validates%20the%20practical%20viability%20of%20BCFL%20systems%2C%20achieving%0Aperformance%20comparable%20to%20centralized%20approaches%20while%20providing%20enhanced%0Asecurity%20guarantees%20and%20enabling%20new%20models%20of%20trustless%20collaborative%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06406v1&entry.124074799=Read"},
{"title": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality", "author": "Sewoong Lee and Adam Davies and Marc E. Canby and Julia Hockenmaier", "abstract": "  Sparse autoencoders (SAEs) are widely used in mechanistic interpretability\nresearch for large language models; however, the state-of-the-art method of\nusing $k$-sparse autoencoders lacks a theoretical grounding for selecting the\nhyperparameter $k$ that represents the number of nonzero activations, often\ndenoted by $\\ell_0$. In this paper, we reveal a theoretical link that the\n$\\ell_2$-norm of the sparse feature vector can be approximated with the\n$\\ell_2$-norm of the dense vector with a closed-form error, which allows sparse\nautoencoders to be trained without the need to manually determine $\\ell_0$.\nSpecifically, we validate two applications of our theoretical findings. First,\nwe introduce a new methodology that can assess the feature activations of\npre-trained SAEs by computing the theoretically expected value from the input\nembedding, which has been overlooked by existing SAE evaluation methods and\nloss functions. Second, we introduce a novel activation function, top-AFA,\nwhich builds upon our formulation of approximate feature activation (AFA). This\nfunction enables top-$k$ style activation without requiring a constant\nhyperparameter $k$ to be tuned, dynamically determining the number of activated\nfeatures for each input. By training SAEs on three intermediate layers to\nreconstruct GPT2 hidden embeddings for over 80 million tokens from the\nOpenWebText dataset, we demonstrate the empirical merits of this approach and\ncompare it with current state-of-the-art $k$-sparse autoencoders. Our code is\navailable at: https://github.com/SewoongLee/top-afa-sae.\n", "link": "http://arxiv.org/abs/2503.24277v2", "date": "2025-08-08", "relevancy": 1.9926, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20and%20Designing%20Sparse%20Autoencoders%20by%20Approximating%0A%20%20Quasi-Orthogonality&body=Title%3A%20Evaluating%20and%20Designing%20Sparse%20Autoencoders%20by%20Approximating%0A%20%20Quasi-Orthogonality%0AAuthor%3A%20Sewoong%20Lee%20and%20Adam%20Davies%20and%20Marc%20E.%20Canby%20and%20Julia%20Hockenmaier%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20widely%20used%20in%20mechanistic%20interpretability%0Aresearch%20for%20large%20language%20models%3B%20however%2C%20the%20state-of-the-art%20method%20of%0Ausing%20%24k%24-sparse%20autoencoders%20lacks%20a%20theoretical%20grounding%20for%20selecting%20the%0Ahyperparameter%20%24k%24%20that%20represents%20the%20number%20of%20nonzero%20activations%2C%20often%0Adenoted%20by%20%24%5Cell_0%24.%20In%20this%20paper%2C%20we%20reveal%20a%20theoretical%20link%20that%20the%0A%24%5Cell_2%24-norm%20of%20the%20sparse%20feature%20vector%20can%20be%20approximated%20with%20the%0A%24%5Cell_2%24-norm%20of%20the%20dense%20vector%20with%20a%20closed-form%20error%2C%20which%20allows%20sparse%0Aautoencoders%20to%20be%20trained%20without%20the%20need%20to%20manually%20determine%20%24%5Cell_0%24.%0ASpecifically%2C%20we%20validate%20two%20applications%20of%20our%20theoretical%20findings.%20First%2C%0Awe%20introduce%20a%20new%20methodology%20that%20can%20assess%20the%20feature%20activations%20of%0Apre-trained%20SAEs%20by%20computing%20the%20theoretically%20expected%20value%20from%20the%20input%0Aembedding%2C%20which%20has%20been%20overlooked%20by%20existing%20SAE%20evaluation%20methods%20and%0Aloss%20functions.%20Second%2C%20we%20introduce%20a%20novel%20activation%20function%2C%20top-AFA%2C%0Awhich%20builds%20upon%20our%20formulation%20of%20approximate%20feature%20activation%20%28AFA%29.%20This%0Afunction%20enables%20top-%24k%24%20style%20activation%20without%20requiring%20a%20constant%0Ahyperparameter%20%24k%24%20to%20be%20tuned%2C%20dynamically%20determining%20the%20number%20of%20activated%0Afeatures%20for%20each%20input.%20By%20training%20SAEs%20on%20three%20intermediate%20layers%20to%0Areconstruct%20GPT2%20hidden%20embeddings%20for%20over%2080%20million%20tokens%20from%20the%0AOpenWebText%20dataset%2C%20we%20demonstrate%20the%20empirical%20merits%20of%20this%20approach%20and%0Acompare%20it%20with%20current%20state-of-the-art%20%24k%24-sparse%20autoencoders.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/SewoongLee/top-afa-sae.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520and%2520Designing%2520Sparse%2520Autoencoders%2520by%2520Approximating%250A%2520%2520Quasi-Orthogonality%26entry.906535625%3DSewoong%2520Lee%2520and%2520Adam%2520Davies%2520and%2520Marc%2520E.%2520Canby%2520and%2520Julia%2520Hockenmaier%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520widely%2520used%2520in%2520mechanistic%2520interpretability%250Aresearch%2520for%2520large%2520language%2520models%253B%2520however%252C%2520the%2520state-of-the-art%2520method%2520of%250Ausing%2520%2524k%2524-sparse%2520autoencoders%2520lacks%2520a%2520theoretical%2520grounding%2520for%2520selecting%2520the%250Ahyperparameter%2520%2524k%2524%2520that%2520represents%2520the%2520number%2520of%2520nonzero%2520activations%252C%2520often%250Adenoted%2520by%2520%2524%255Cell_0%2524.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520a%2520theoretical%2520link%2520that%2520the%250A%2524%255Cell_2%2524-norm%2520of%2520the%2520sparse%2520feature%2520vector%2520can%2520be%2520approximated%2520with%2520the%250A%2524%255Cell_2%2524-norm%2520of%2520the%2520dense%2520vector%2520with%2520a%2520closed-form%2520error%252C%2520which%2520allows%2520sparse%250Aautoencoders%2520to%2520be%2520trained%2520without%2520the%2520need%2520to%2520manually%2520determine%2520%2524%255Cell_0%2524.%250ASpecifically%252C%2520we%2520validate%2520two%2520applications%2520of%2520our%2520theoretical%2520findings.%2520First%252C%250Awe%2520introduce%2520a%2520new%2520methodology%2520that%2520can%2520assess%2520the%2520feature%2520activations%2520of%250Apre-trained%2520SAEs%2520by%2520computing%2520the%2520theoretically%2520expected%2520value%2520from%2520the%2520input%250Aembedding%252C%2520which%2520has%2520been%2520overlooked%2520by%2520existing%2520SAE%2520evaluation%2520methods%2520and%250Aloss%2520functions.%2520Second%252C%2520we%2520introduce%2520a%2520novel%2520activation%2520function%252C%2520top-AFA%252C%250Awhich%2520builds%2520upon%2520our%2520formulation%2520of%2520approximate%2520feature%2520activation%2520%2528AFA%2529.%2520This%250Afunction%2520enables%2520top-%2524k%2524%2520style%2520activation%2520without%2520requiring%2520a%2520constant%250Ahyperparameter%2520%2524k%2524%2520to%2520be%2520tuned%252C%2520dynamically%2520determining%2520the%2520number%2520of%2520activated%250Afeatures%2520for%2520each%2520input.%2520By%2520training%2520SAEs%2520on%2520three%2520intermediate%2520layers%2520to%250Areconstruct%2520GPT2%2520hidden%2520embeddings%2520for%2520over%252080%2520million%2520tokens%2520from%2520the%250AOpenWebText%2520dataset%252C%2520we%2520demonstrate%2520the%2520empirical%2520merits%2520of%2520this%2520approach%2520and%250Acompare%2520it%2520with%2520current%2520state-of-the-art%2520%2524k%2524-sparse%2520autoencoders.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/SewoongLee/top-afa-sae.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20and%20Designing%20Sparse%20Autoencoders%20by%20Approximating%0A%20%20Quasi-Orthogonality&entry.906535625=Sewoong%20Lee%20and%20Adam%20Davies%20and%20Marc%20E.%20Canby%20and%20Julia%20Hockenmaier&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20widely%20used%20in%20mechanistic%20interpretability%0Aresearch%20for%20large%20language%20models%3B%20however%2C%20the%20state-of-the-art%20method%20of%0Ausing%20%24k%24-sparse%20autoencoders%20lacks%20a%20theoretical%20grounding%20for%20selecting%20the%0Ahyperparameter%20%24k%24%20that%20represents%20the%20number%20of%20nonzero%20activations%2C%20often%0Adenoted%20by%20%24%5Cell_0%24.%20In%20this%20paper%2C%20we%20reveal%20a%20theoretical%20link%20that%20the%0A%24%5Cell_2%24-norm%20of%20the%20sparse%20feature%20vector%20can%20be%20approximated%20with%20the%0A%24%5Cell_2%24-norm%20of%20the%20dense%20vector%20with%20a%20closed-form%20error%2C%20which%20allows%20sparse%0Aautoencoders%20to%20be%20trained%20without%20the%20need%20to%20manually%20determine%20%24%5Cell_0%24.%0ASpecifically%2C%20we%20validate%20two%20applications%20of%20our%20theoretical%20findings.%20First%2C%0Awe%20introduce%20a%20new%20methodology%20that%20can%20assess%20the%20feature%20activations%20of%0Apre-trained%20SAEs%20by%20computing%20the%20theoretically%20expected%20value%20from%20the%20input%0Aembedding%2C%20which%20has%20been%20overlooked%20by%20existing%20SAE%20evaluation%20methods%20and%0Aloss%20functions.%20Second%2C%20we%20introduce%20a%20novel%20activation%20function%2C%20top-AFA%2C%0Awhich%20builds%20upon%20our%20formulation%20of%20approximate%20feature%20activation%20%28AFA%29.%20This%0Afunction%20enables%20top-%24k%24%20style%20activation%20without%20requiring%20a%20constant%0Ahyperparameter%20%24k%24%20to%20be%20tuned%2C%20dynamically%20determining%20the%20number%20of%20activated%0Afeatures%20for%20each%20input.%20By%20training%20SAEs%20on%20three%20intermediate%20layers%20to%0Areconstruct%20GPT2%20hidden%20embeddings%20for%20over%2080%20million%20tokens%20from%20the%0AOpenWebText%20dataset%2C%20we%20demonstrate%20the%20empirical%20merits%20of%20this%20approach%20and%0Acompare%20it%20with%20current%20state-of-the-art%20%24k%24-sparse%20autoencoders.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/SewoongLee/top-afa-sae.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24277v2&entry.124074799=Read"},
{"title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "author": "Anirudh Khatry and Robert Zhang and Jia Pan and Ziteng Wang and Qiaochu Chen and Greg Durrett and Isil Dillig", "abstract": "  C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.\n", "link": "http://arxiv.org/abs/2504.15254v2", "date": "2025-08-08", "relevancy": 1.1603, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3903}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3873}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRUST-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20C-to-safe-Rust%20Transpilation&body=Title%3A%20CRUST-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20C-to-safe-Rust%20Transpilation%0AAuthor%3A%20Anirudh%20Khatry%20and%20Robert%20Zhang%20and%20Jia%20Pan%20and%20Ziteng%20Wang%20and%20Qiaochu%20Chen%20and%20Greg%20Durrett%20and%20Isil%20Dillig%0AAbstract%3A%20%20%20C-to-Rust%20transpilation%20is%20essential%20for%20modernizing%20legacy%20C%20code%20while%0Aenhancing%20safety%20and%20interoperability%20with%20modern%20Rust%20ecosystems.%20However%2C%20no%0Adataset%20currently%20exists%20for%20evaluating%20whether%20a%20system%20can%20transpile%20C%20into%0Asafe%20Rust%20that%20passes%20a%20set%20of%20test%20cases.%20We%20introduce%20CRUST-Bench%2C%20a%20dataset%0Aof%20100%20C%20repositories%2C%20each%20paired%20with%20manually-written%20interfaces%20in%20safe%0ARust%20as%20well%20as%20test%20cases%20that%20can%20be%20used%20to%20validate%20correctness%20of%20the%0Atranspilation.%20By%20considering%20entire%20repositories%20rather%20than%20isolated%0Afunctions%2C%20CRUST-Bench%20captures%20the%20challenges%20of%20translating%20complex%20projects%0Awith%20dependencies%20across%20multiple%20files.%20The%20provided%20Rust%20interfaces%20provide%0Aexplicit%20specifications%20that%20ensure%20adherence%20to%20idiomatic%2C%20memory-safe%20Rust%0Apatterns%2C%20while%20the%20accompanying%20test%20cases%20enforce%20functional%20correctness.%20We%0Aevaluate%20state-of-the-art%20large%20language%20models%20%28LLMs%29%20on%20this%20task%20and%20find%0Athat%20safe%20and%20idiomatic%20Rust%20generation%20is%20still%20a%20challenging%20problem%20for%0Avarious%20state-of-the-art%20methods%20and%20techniques.%20We%20also%20provide%20insights%20into%0Athe%20errors%20LLMs%20usually%20make%20in%20transpiling%20code%20from%20C%20to%20safe%20Rust.%20The%20best%0Aperforming%20model%2C%20OpenAI%20o1%2C%20is%20able%20to%20solve%20only%2015%20tasks%20in%20a%20single-shot%0Asetting.%20Improvements%20on%20CRUST-Bench%20would%20lead%20to%20improved%20transpilation%0Asystems%20that%20can%20reason%20about%20complex%20scenarios%20and%20help%20in%20migrating%20legacy%0Acodebases%20from%20C%20into%20languages%20like%20Rust%20that%20ensure%20memory%20safety.%20You%20can%0Afind%20the%20dataset%20and%20code%20at%20https%3A//github.com/anirudhkhatry/CRUST-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRUST-Bench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520C-to-safe-Rust%2520Transpilation%26entry.906535625%3DAnirudh%2520Khatry%2520and%2520Robert%2520Zhang%2520and%2520Jia%2520Pan%2520and%2520Ziteng%2520Wang%2520and%2520Qiaochu%2520Chen%2520and%2520Greg%2520Durrett%2520and%2520Isil%2520Dillig%26entry.1292438233%3D%2520%2520C-to-Rust%2520transpilation%2520is%2520essential%2520for%2520modernizing%2520legacy%2520C%2520code%2520while%250Aenhancing%2520safety%2520and%2520interoperability%2520with%2520modern%2520Rust%2520ecosystems.%2520However%252C%2520no%250Adataset%2520currently%2520exists%2520for%2520evaluating%2520whether%2520a%2520system%2520can%2520transpile%2520C%2520into%250Asafe%2520Rust%2520that%2520passes%2520a%2520set%2520of%2520test%2520cases.%2520We%2520introduce%2520CRUST-Bench%252C%2520a%2520dataset%250Aof%2520100%2520C%2520repositories%252C%2520each%2520paired%2520with%2520manually-written%2520interfaces%2520in%2520safe%250ARust%2520as%2520well%2520as%2520test%2520cases%2520that%2520can%2520be%2520used%2520to%2520validate%2520correctness%2520of%2520the%250Atranspilation.%2520By%2520considering%2520entire%2520repositories%2520rather%2520than%2520isolated%250Afunctions%252C%2520CRUST-Bench%2520captures%2520the%2520challenges%2520of%2520translating%2520complex%2520projects%250Awith%2520dependencies%2520across%2520multiple%2520files.%2520The%2520provided%2520Rust%2520interfaces%2520provide%250Aexplicit%2520specifications%2520that%2520ensure%2520adherence%2520to%2520idiomatic%252C%2520memory-safe%2520Rust%250Apatterns%252C%2520while%2520the%2520accompanying%2520test%2520cases%2520enforce%2520functional%2520correctness.%2520We%250Aevaluate%2520state-of-the-art%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520this%2520task%2520and%2520find%250Athat%2520safe%2520and%2520idiomatic%2520Rust%2520generation%2520is%2520still%2520a%2520challenging%2520problem%2520for%250Avarious%2520state-of-the-art%2520methods%2520and%2520techniques.%2520We%2520also%2520provide%2520insights%2520into%250Athe%2520errors%2520LLMs%2520usually%2520make%2520in%2520transpiling%2520code%2520from%2520C%2520to%2520safe%2520Rust.%2520The%2520best%250Aperforming%2520model%252C%2520OpenAI%2520o1%252C%2520is%2520able%2520to%2520solve%2520only%252015%2520tasks%2520in%2520a%2520single-shot%250Asetting.%2520Improvements%2520on%2520CRUST-Bench%2520would%2520lead%2520to%2520improved%2520transpilation%250Asystems%2520that%2520can%2520reason%2520about%2520complex%2520scenarios%2520and%2520help%2520in%2520migrating%2520legacy%250Acodebases%2520from%2520C%2520into%2520languages%2520like%2520Rust%2520that%2520ensure%2520memory%2520safety.%2520You%2520can%250Afind%2520the%2520dataset%2520and%2520code%2520at%2520https%253A//github.com/anirudhkhatry/CRUST-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRUST-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20C-to-safe-Rust%20Transpilation&entry.906535625=Anirudh%20Khatry%20and%20Robert%20Zhang%20and%20Jia%20Pan%20and%20Ziteng%20Wang%20and%20Qiaochu%20Chen%20and%20Greg%20Durrett%20and%20Isil%20Dillig&entry.1292438233=%20%20C-to-Rust%20transpilation%20is%20essential%20for%20modernizing%20legacy%20C%20code%20while%0Aenhancing%20safety%20and%20interoperability%20with%20modern%20Rust%20ecosystems.%20However%2C%20no%0Adataset%20currently%20exists%20for%20evaluating%20whether%20a%20system%20can%20transpile%20C%20into%0Asafe%20Rust%20that%20passes%20a%20set%20of%20test%20cases.%20We%20introduce%20CRUST-Bench%2C%20a%20dataset%0Aof%20100%20C%20repositories%2C%20each%20paired%20with%20manually-written%20interfaces%20in%20safe%0ARust%20as%20well%20as%20test%20cases%20that%20can%20be%20used%20to%20validate%20correctness%20of%20the%0Atranspilation.%20By%20considering%20entire%20repositories%20rather%20than%20isolated%0Afunctions%2C%20CRUST-Bench%20captures%20the%20challenges%20of%20translating%20complex%20projects%0Awith%20dependencies%20across%20multiple%20files.%20The%20provided%20Rust%20interfaces%20provide%0Aexplicit%20specifications%20that%20ensure%20adherence%20to%20idiomatic%2C%20memory-safe%20Rust%0Apatterns%2C%20while%20the%20accompanying%20test%20cases%20enforce%20functional%20correctness.%20We%0Aevaluate%20state-of-the-art%20large%20language%20models%20%28LLMs%29%20on%20this%20task%20and%20find%0Athat%20safe%20and%20idiomatic%20Rust%20generation%20is%20still%20a%20challenging%20problem%20for%0Avarious%20state-of-the-art%20methods%20and%20techniques.%20We%20also%20provide%20insights%20into%0Athe%20errors%20LLMs%20usually%20make%20in%20transpiling%20code%20from%20C%20to%20safe%20Rust.%20The%20best%0Aperforming%20model%2C%20OpenAI%20o1%2C%20is%20able%20to%20solve%20only%2015%20tasks%20in%20a%20single-shot%0Asetting.%20Improvements%20on%20CRUST-Bench%20would%20lead%20to%20improved%20transpilation%0Asystems%20that%20can%20reason%20about%20complex%20scenarios%20and%20help%20in%20migrating%20legacy%0Acodebases%20from%20C%20into%20languages%20like%20Rust%20that%20ensure%20memory%20safety.%20You%20can%0Afind%20the%20dataset%20and%20code%20at%20https%3A//github.com/anirudhkhatry/CRUST-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15254v2&entry.124074799=Read"},
{"title": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images", "author": "Yeqi Fang and Rong Zhou", "abstract": "  Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging.\n", "link": "http://arxiv.org/abs/2504.00816v4", "date": "2025-08-08", "relevancy": 1.5705, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5283}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images&body=Title%3A%20Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images%0AAuthor%3A%20Yeqi%20Fang%20and%20Rong%20Zhou%0AAbstract%3A%20%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20molecular%20imaging%20tool%0Awidely%20used%20in%20medicine.%20Traditional%20PET%20systems%20rely%20on%20complete%20detector%0Arings%20for%20full%20angular%20coverage%20and%20reliable%20data%20collection.%20However%2C%0Aincomplete-ring%20PET%20scanners%20have%20emerged%20due%20to%20hardware%20failures%2C%20cost%0Aconstraints%2C%20or%20specific%20clinical%20needs.%20Standard%20reconstruction%20algorithms%0Aoften%20suffer%20from%20performance%20degradation%20with%20these%20systems%20because%20of%20reduced%0Adata%20completeness%20and%20geometric%20inconsistencies.%20We%20present%20a%20two-stage%0Adeep-learning%20framework%20that%2C%20without%20incorporating%20any%20time-of-flight%20%28TOF%29%0Ainformation%2C%20restores%20high-quality%20images%20from%20data%20with%20about%2050%25%20missing%0Acoincidences%20-%20double%20the%20loss%20levels%20previously%20addressed%20by%20CNN-based%0Amethods.%20The%20pipeline%20operates%20in%20two%20stages%3A%20a%20projection-domain%20Attention%0AU-Net%20first%20predicts%20the%20missing%20sections%20of%20the%20sinogram%20by%20leveraging%20spatial%0Acontext%20from%20neighbouring%20slices%2C%20after%20which%20the%20completed%20data%20are%0Areconstructed%20with%20OSEM%20algorithm%20and%20passed%20to%20a%20U-Net-diffusion%20module%20that%0Aremoves%20residual%20artefacts%20while%20reinstating%20high-frequency%20detail.%20Using%20206%0Abrain%20volumes%20from%20a%20public%20dataset%2C%20the%20result%20shows%20that%20our%20model%0Asuccessfully%20preserves%20most%20anatomical%20structures%20and%20tracer%20distribution%0Afeatures%20with%20PSNR%20of%2030.92%20dB%20and%20SSIM%20of%200.9708.%20We%20also%20achieve%20higher%0Ainference%20speed%2C%20thus%20providing%20an%20effective%20solution%20for%20incomplete-ring%20PET%0Aimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00816v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-stage%2520deep%2520learning%2520framework%2520for%2520the%2520restoration%2520of%2520incomplete-ring%250A%2520%2520PET%2520images%26entry.906535625%3DYeqi%2520Fang%2520and%2520Rong%2520Zhou%26entry.1292438233%3D%2520%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520is%2520an%2520important%2520molecular%2520imaging%2520tool%250Awidely%2520used%2520in%2520medicine.%2520Traditional%2520PET%2520systems%2520rely%2520on%2520complete%2520detector%250Arings%2520for%2520full%2520angular%2520coverage%2520and%2520reliable%2520data%2520collection.%2520However%252C%250Aincomplete-ring%2520PET%2520scanners%2520have%2520emerged%2520due%2520to%2520hardware%2520failures%252C%2520cost%250Aconstraints%252C%2520or%2520specific%2520clinical%2520needs.%2520Standard%2520reconstruction%2520algorithms%250Aoften%2520suffer%2520from%2520performance%2520degradation%2520with%2520these%2520systems%2520because%2520of%2520reduced%250Adata%2520completeness%2520and%2520geometric%2520inconsistencies.%2520We%2520present%2520a%2520two-stage%250Adeep-learning%2520framework%2520that%252C%2520without%2520incorporating%2520any%2520time-of-flight%2520%2528TOF%2529%250Ainformation%252C%2520restores%2520high-quality%2520images%2520from%2520data%2520with%2520about%252050%2525%2520missing%250Acoincidences%2520-%2520double%2520the%2520loss%2520levels%2520previously%2520addressed%2520by%2520CNN-based%250Amethods.%2520The%2520pipeline%2520operates%2520in%2520two%2520stages%253A%2520a%2520projection-domain%2520Attention%250AU-Net%2520first%2520predicts%2520the%2520missing%2520sections%2520of%2520the%2520sinogram%2520by%2520leveraging%2520spatial%250Acontext%2520from%2520neighbouring%2520slices%252C%2520after%2520which%2520the%2520completed%2520data%2520are%250Areconstructed%2520with%2520OSEM%2520algorithm%2520and%2520passed%2520to%2520a%2520U-Net-diffusion%2520module%2520that%250Aremoves%2520residual%2520artefacts%2520while%2520reinstating%2520high-frequency%2520detail.%2520Using%2520206%250Abrain%2520volumes%2520from%2520a%2520public%2520dataset%252C%2520the%2520result%2520shows%2520that%2520our%2520model%250Asuccessfully%2520preserves%2520most%2520anatomical%2520structures%2520and%2520tracer%2520distribution%250Afeatures%2520with%2520PSNR%2520of%252030.92%2520dB%2520and%2520SSIM%2520of%25200.9708.%2520We%2520also%2520achieve%2520higher%250Ainference%2520speed%252C%2520thus%2520providing%2520an%2520effective%2520solution%2520for%2520incomplete-ring%2520PET%250Aimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00816v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images&entry.906535625=Yeqi%20Fang%20and%20Rong%20Zhou&entry.1292438233=%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20molecular%20imaging%20tool%0Awidely%20used%20in%20medicine.%20Traditional%20PET%20systems%20rely%20on%20complete%20detector%0Arings%20for%20full%20angular%20coverage%20and%20reliable%20data%20collection.%20However%2C%0Aincomplete-ring%20PET%20scanners%20have%20emerged%20due%20to%20hardware%20failures%2C%20cost%0Aconstraints%2C%20or%20specific%20clinical%20needs.%20Standard%20reconstruction%20algorithms%0Aoften%20suffer%20from%20performance%20degradation%20with%20these%20systems%20because%20of%20reduced%0Adata%20completeness%20and%20geometric%20inconsistencies.%20We%20present%20a%20two-stage%0Adeep-learning%20framework%20that%2C%20without%20incorporating%20any%20time-of-flight%20%28TOF%29%0Ainformation%2C%20restores%20high-quality%20images%20from%20data%20with%20about%2050%25%20missing%0Acoincidences%20-%20double%20the%20loss%20levels%20previously%20addressed%20by%20CNN-based%0Amethods.%20The%20pipeline%20operates%20in%20two%20stages%3A%20a%20projection-domain%20Attention%0AU-Net%20first%20predicts%20the%20missing%20sections%20of%20the%20sinogram%20by%20leveraging%20spatial%0Acontext%20from%20neighbouring%20slices%2C%20after%20which%20the%20completed%20data%20are%0Areconstructed%20with%20OSEM%20algorithm%20and%20passed%20to%20a%20U-Net-diffusion%20module%20that%0Aremoves%20residual%20artefacts%20while%20reinstating%20high-frequency%20detail.%20Using%20206%0Abrain%20volumes%20from%20a%20public%20dataset%2C%20the%20result%20shows%20that%20our%20model%0Asuccessfully%20preserves%20most%20anatomical%20structures%20and%20tracer%20distribution%0Afeatures%20with%20PSNR%20of%2030.92%20dB%20and%20SSIM%20of%200.9708.%20We%20also%20achieve%20higher%0Ainference%20speed%2C%20thus%20providing%20an%20effective%20solution%20for%20incomplete-ring%20PET%0Aimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00816v4&entry.124074799=Read"},
{"title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "author": "Daniel Feijoo and Paula Garrido-Mellado and Jaesung Rim and Alvaro Garcia and Marcos V. Conde", "abstract": "  Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.\n", "link": "http://arxiv.org/abs/2508.06228v1", "date": "2025-08-08", "relevancy": 1.1137, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5819}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unified%20Image%20Deblurring%20using%20a%20Mixture-of-Experts%20Decoder&body=Title%3A%20Towards%20Unified%20Image%20Deblurring%20using%20a%20Mixture-of-Experts%20Decoder%0AAuthor%3A%20Daniel%20Feijoo%20and%20Paula%20Garrido-Mellado%20and%20Jaesung%20Rim%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde%0AAbstract%3A%20%20%20Image%20deblurring%2C%20removing%20blurring%20artifacts%20from%20images%2C%20is%20a%20fundamental%0Atask%20in%20computational%20photography%20and%20low-level%20computer%20vision.%20Existing%0Aapproaches%20focus%20on%20specialized%20solutions%20tailored%20to%20particular%20blur%20types%2C%0Athus%2C%20these%20solutions%20lack%20generalization.%20This%20limitation%20in%20current%20methods%0Aimplies%20requiring%20multiple%20models%20to%20cover%20several%20blur%20types%2C%20which%20is%20not%0Apractical%20in%20many%20real%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20the%20first%0Aall-in-one%20deblurring%20method%20capable%20of%20efficiently%20restoring%20images%20affected%0Aby%20diverse%20blur%20degradations%2C%20including%20global%20motion%2C%20local%20motion%2C%20blur%20in%0Alow-light%20conditions%2C%20and%20defocus%20blur.%20We%20propose%20a%20mixture-of-experts%20%28MoE%29%0Adecoding%20module%2C%20which%20dynamically%20routes%20image%20features%20based%20on%20the%0Arecognized%20blur%20degradation%2C%20enabling%20precise%20and%20efficient%20restoration%20in%20an%0Aend-to-end%20manner.%20Our%20unified%20approach%20not%20only%20achieves%20performance%0Acomparable%20to%20dedicated%20task-specific%20models%2C%20but%20also%20demonstrates%20remarkable%0Arobustness%20and%20generalization%20capabilities%20on%20unseen%20blur%20degradation%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unified%2520Image%2520Deblurring%2520using%2520a%2520Mixture-of-Experts%2520Decoder%26entry.906535625%3DDaniel%2520Feijoo%2520and%2520Paula%2520Garrido-Mellado%2520and%2520Jaesung%2520Rim%2520and%2520Alvaro%2520Garcia%2520and%2520Marcos%2520V.%2520Conde%26entry.1292438233%3D%2520%2520Image%2520deblurring%252C%2520removing%2520blurring%2520artifacts%2520from%2520images%252C%2520is%2520a%2520fundamental%250Atask%2520in%2520computational%2520photography%2520and%2520low-level%2520computer%2520vision.%2520Existing%250Aapproaches%2520focus%2520on%2520specialized%2520solutions%2520tailored%2520to%2520particular%2520blur%2520types%252C%250Athus%252C%2520these%2520solutions%2520lack%2520generalization.%2520This%2520limitation%2520in%2520current%2520methods%250Aimplies%2520requiring%2520multiple%2520models%2520to%2520cover%2520several%2520blur%2520types%252C%2520which%2520is%2520not%250Apractical%2520in%2520many%2520real%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520first%250Aall-in-one%2520deblurring%2520method%2520capable%2520of%2520efficiently%2520restoring%2520images%2520affected%250Aby%2520diverse%2520blur%2520degradations%252C%2520including%2520global%2520motion%252C%2520local%2520motion%252C%2520blur%2520in%250Alow-light%2520conditions%252C%2520and%2520defocus%2520blur.%2520We%2520propose%2520a%2520mixture-of-experts%2520%2528MoE%2529%250Adecoding%2520module%252C%2520which%2520dynamically%2520routes%2520image%2520features%2520based%2520on%2520the%250Arecognized%2520blur%2520degradation%252C%2520enabling%2520precise%2520and%2520efficient%2520restoration%2520in%2520an%250Aend-to-end%2520manner.%2520Our%2520unified%2520approach%2520not%2520only%2520achieves%2520performance%250Acomparable%2520to%2520dedicated%2520task-specific%2520models%252C%2520but%2520also%2520demonstrates%2520remarkable%250Arobustness%2520and%2520generalization%2520capabilities%2520on%2520unseen%2520blur%2520degradation%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unified%20Image%20Deblurring%20using%20a%20Mixture-of-Experts%20Decoder&entry.906535625=Daniel%20Feijoo%20and%20Paula%20Garrido-Mellado%20and%20Jaesung%20Rim%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde&entry.1292438233=%20%20Image%20deblurring%2C%20removing%20blurring%20artifacts%20from%20images%2C%20is%20a%20fundamental%0Atask%20in%20computational%20photography%20and%20low-level%20computer%20vision.%20Existing%0Aapproaches%20focus%20on%20specialized%20solutions%20tailored%20to%20particular%20blur%20types%2C%0Athus%2C%20these%20solutions%20lack%20generalization.%20This%20limitation%20in%20current%20methods%0Aimplies%20requiring%20multiple%20models%20to%20cover%20several%20blur%20types%2C%20which%20is%20not%0Apractical%20in%20many%20real%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20the%20first%0Aall-in-one%20deblurring%20method%20capable%20of%20efficiently%20restoring%20images%20affected%0Aby%20diverse%20blur%20degradations%2C%20including%20global%20motion%2C%20local%20motion%2C%20blur%20in%0Alow-light%20conditions%2C%20and%20defocus%20blur.%20We%20propose%20a%20mixture-of-experts%20%28MoE%29%0Adecoding%20module%2C%20which%20dynamically%20routes%20image%20features%20based%20on%20the%0Arecognized%20blur%20degradation%2C%20enabling%20precise%20and%20efficient%20restoration%20in%20an%0Aend-to-end%20manner.%20Our%20unified%20approach%20not%20only%20achieves%20performance%0Acomparable%20to%20dedicated%20task-specific%20models%2C%20but%20also%20demonstrates%20remarkable%0Arobustness%20and%20generalization%20capabilities%20on%20unseen%20blur%20degradation%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06228v1&entry.124074799=Read"},
{"title": "Depth Jitter: Seeing through the Depth", "author": "Md Sazidur Rahman and David Cabecinhas and Ricard Marxer", "abstract": "  Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.\n", "link": "http://arxiv.org/abs/2508.06227v1", "date": "2025-08-08", "relevancy": 1.6429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5558}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5471}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Jitter%3A%20Seeing%20through%20the%20Depth&body=Title%3A%20Depth%20Jitter%3A%20Seeing%20through%20the%20Depth%0AAuthor%3A%20Md%20Sazidur%20Rahman%20and%20David%20Cabecinhas%20and%20Ricard%20Marxer%0AAbstract%3A%20%20%20Depth%20information%20is%20essential%20in%20computer%20vision%2C%20particularly%20in%20underwater%0Aimaging%2C%20robotics%2C%20and%20autonomous%20navigation.%20However%2C%20conventional%0Aaugmentation%20techniques%20overlook%20depth%20aware%20transformations%2C%20limiting%20model%0Arobustness%20in%20real%20world%20depth%20variations.%20In%20this%20paper%2C%20we%20introduce%0ADepth-Jitter%2C%20a%20novel%20depth-based%20augmentation%20technique%20that%20simulates%20natural%0Adepth%20variations%20to%20improve%20generalization.%20Our%20approach%20applies%20adaptive%20depth%0Aoffsetting%2C%20guided%20by%20depth%20variance%20thresholds%2C%20to%20generate%20synthetic%20depth%0Aperturbations%20while%20preserving%20structural%20integrity.%20We%20evaluate%20Depth-Jitter%0Aon%20two%20benchmark%20datasets%2C%20FathomNet%20and%20UTDAC2020%20demonstrating%20its%20impact%20on%0Amodel%20stability%20under%20diverse%20depth%20conditions.%20Extensive%20experiments%20compare%0ADepth-Jitter%20against%20traditional%20augmentation%20strategies%20such%20as%20ColorJitter%2C%0Aanalyzing%20performance%20across%20varying%20learning%20rates%2C%20encoders%2C%20and%20loss%0Afunctions.%20While%20Depth-Jitter%20does%20not%20always%20outperform%20conventional%20methods%0Ain%20absolute%20performance%2C%20it%20consistently%20enhances%20model%20stability%20and%0Ageneralization%20in%20depth-sensitive%20environments.%20These%20findings%20highlight%20the%0Apotential%20of%20depth-aware%20augmentation%20for%20real-world%20applications%20and%20provide%20a%0Afoundation%20for%20further%20research%20into%20depth-based%20learning%20strategies.%20The%0Aproposed%20technique%20is%20publicly%20available%20to%20support%20advancements%20in%20depth-aware%0Aaugmentation.%20The%20code%20is%20publicly%20available%20on%0A%5Chref%7Bhttps%3A//github.com/mim-team/Depth-Jitter%7D%7Bgithub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Jitter%253A%2520Seeing%2520through%2520the%2520Depth%26entry.906535625%3DMd%2520Sazidur%2520Rahman%2520and%2520David%2520Cabecinhas%2520and%2520Ricard%2520Marxer%26entry.1292438233%3D%2520%2520Depth%2520information%2520is%2520essential%2520in%2520computer%2520vision%252C%2520particularly%2520in%2520underwater%250Aimaging%252C%2520robotics%252C%2520and%2520autonomous%2520navigation.%2520However%252C%2520conventional%250Aaugmentation%2520techniques%2520overlook%2520depth%2520aware%2520transformations%252C%2520limiting%2520model%250Arobustness%2520in%2520real%2520world%2520depth%2520variations.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADepth-Jitter%252C%2520a%2520novel%2520depth-based%2520augmentation%2520technique%2520that%2520simulates%2520natural%250Adepth%2520variations%2520to%2520improve%2520generalization.%2520Our%2520approach%2520applies%2520adaptive%2520depth%250Aoffsetting%252C%2520guided%2520by%2520depth%2520variance%2520thresholds%252C%2520to%2520generate%2520synthetic%2520depth%250Aperturbations%2520while%2520preserving%2520structural%2520integrity.%2520We%2520evaluate%2520Depth-Jitter%250Aon%2520two%2520benchmark%2520datasets%252C%2520FathomNet%2520and%2520UTDAC2020%2520demonstrating%2520its%2520impact%2520on%250Amodel%2520stability%2520under%2520diverse%2520depth%2520conditions.%2520Extensive%2520experiments%2520compare%250ADepth-Jitter%2520against%2520traditional%2520augmentation%2520strategies%2520such%2520as%2520ColorJitter%252C%250Aanalyzing%2520performance%2520across%2520varying%2520learning%2520rates%252C%2520encoders%252C%2520and%2520loss%250Afunctions.%2520While%2520Depth-Jitter%2520does%2520not%2520always%2520outperform%2520conventional%2520methods%250Ain%2520absolute%2520performance%252C%2520it%2520consistently%2520enhances%2520model%2520stability%2520and%250Ageneralization%2520in%2520depth-sensitive%2520environments.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520depth-aware%2520augmentation%2520for%2520real-world%2520applications%2520and%2520provide%2520a%250Afoundation%2520for%2520further%2520research%2520into%2520depth-based%2520learning%2520strategies.%2520The%250Aproposed%2520technique%2520is%2520publicly%2520available%2520to%2520support%2520advancements%2520in%2520depth-aware%250Aaugmentation.%2520The%2520code%2520is%2520publicly%2520available%2520on%250A%255Chref%257Bhttps%253A//github.com/mim-team/Depth-Jitter%257D%257Bgithub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Jitter%3A%20Seeing%20through%20the%20Depth&entry.906535625=Md%20Sazidur%20Rahman%20and%20David%20Cabecinhas%20and%20Ricard%20Marxer&entry.1292438233=%20%20Depth%20information%20is%20essential%20in%20computer%20vision%2C%20particularly%20in%20underwater%0Aimaging%2C%20robotics%2C%20and%20autonomous%20navigation.%20However%2C%20conventional%0Aaugmentation%20techniques%20overlook%20depth%20aware%20transformations%2C%20limiting%20model%0Arobustness%20in%20real%20world%20depth%20variations.%20In%20this%20paper%2C%20we%20introduce%0ADepth-Jitter%2C%20a%20novel%20depth-based%20augmentation%20technique%20that%20simulates%20natural%0Adepth%20variations%20to%20improve%20generalization.%20Our%20approach%20applies%20adaptive%20depth%0Aoffsetting%2C%20guided%20by%20depth%20variance%20thresholds%2C%20to%20generate%20synthetic%20depth%0Aperturbations%20while%20preserving%20structural%20integrity.%20We%20evaluate%20Depth-Jitter%0Aon%20two%20benchmark%20datasets%2C%20FathomNet%20and%20UTDAC2020%20demonstrating%20its%20impact%20on%0Amodel%20stability%20under%20diverse%20depth%20conditions.%20Extensive%20experiments%20compare%0ADepth-Jitter%20against%20traditional%20augmentation%20strategies%20such%20as%20ColorJitter%2C%0Aanalyzing%20performance%20across%20varying%20learning%20rates%2C%20encoders%2C%20and%20loss%0Afunctions.%20While%20Depth-Jitter%20does%20not%20always%20outperform%20conventional%20methods%0Ain%20absolute%20performance%2C%20it%20consistently%20enhances%20model%20stability%20and%0Ageneralization%20in%20depth-sensitive%20environments.%20These%20findings%20highlight%20the%0Apotential%20of%20depth-aware%20augmentation%20for%20real-world%20applications%20and%20provide%20a%0Afoundation%20for%20further%20research%20into%20depth-based%20learning%20strategies.%20The%0Aproposed%20technique%20is%20publicly%20available%20to%20support%20advancements%20in%20depth-aware%0Aaugmentation.%20The%20code%20is%20publicly%20available%20on%0A%5Chref%7Bhttps%3A//github.com/mim-team/Depth-Jitter%7D%7Bgithub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06227v1&entry.124074799=Read"},
{"title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled\n  Cross-domain Temporal Grounding", "author": "Jian Hu and Zixu Cheng and Shaogang Gong and Isabel Guan and Jianye Hao and Jun Wang and Kun Shao", "abstract": "  Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.\n", "link": "http://arxiv.org/abs/2508.06317v1", "date": "2025-08-08", "relevancy": 1.7845, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6113}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6064}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-quantified%20Rollout%20Policy%20Adaptation%20for%20Unlabelled%0A%20%20Cross-domain%20Temporal%20Grounding&body=Title%3A%20Uncertainty-quantified%20Rollout%20Policy%20Adaptation%20for%20Unlabelled%0A%20%20Cross-domain%20Temporal%20Grounding%0AAuthor%3A%20Jian%20Hu%20and%20Zixu%20Cheng%20and%20Shaogang%20Gong%20and%20Isabel%20Guan%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao%0AAbstract%3A%20%20%20Video%20Temporal%20Grounding%20%28TG%29%20aims%20to%20temporally%20locate%20video%20segments%0Amatching%20a%20natural%20language%20description%20%28a%20query%29%20in%20a%20long%20video.%20While%0AVision-Language%20Models%20%28VLMs%29%20are%20effective%20at%20holistic%20semantic%20matching%2C%20they%0Aoften%20struggle%20with%20fine-grained%20temporal%20localisation.%20Recently%2C%20Group%0ARelative%20Policy%20Optimisation%20%28GRPO%29%20reformulates%20the%20inference%20process%20as%20a%0Areinforcement%20learning%20task%2C%20enabling%20fine-grained%20grounding%20and%20achieving%0Astrong%20in-domain%20performance.%20However%2C%20GRPO%20relies%20on%20labelled%20data%2C%20making%20it%0Aunsuitable%20in%20unlabelled%20domains.%20Moreover%2C%20because%20videos%20are%20large%20and%0Aexpensive%20to%20store%20and%20process%2C%20performing%20full-scale%20adaptation%20introduces%0Aprohibitive%20latency%20and%20computational%20overhead%2C%20making%20it%20impractical%20for%0Areal-time%20deployment.%20To%20overcome%20both%20problems%2C%20we%20introduce%20a%20Data-Efficient%0AUnlabelled%20Cross-domain%20Temporal%20Grounding%20method%2C%20from%20which%20a%20model%20is%20first%0Atrained%20on%20a%20labelled%20source%20domain%2C%20then%20adapted%20to%20a%20target%20domain%20using%20only%0Aa%20small%20number%20of%20unlabelled%20videos%20from%20the%20target%20domain.%20This%20approach%0Aeliminates%20the%20need%20for%20target%20annotation%20and%20keeps%20both%20computational%20and%0Astorage%20overhead%20low%20enough%20to%20run%20in%20real%20time.%20Specifically%2C%20we%20introduce.%0AUncertainty-quantified%20Rollout%20Policy%20Adaptation%20%28URPA%29%20for%20cross-domain%0Aknowledge%20transfer%20in%20learning%20video%20temporal%20grounding%20without%20target%20labels.%0AURPA%20generates%20multiple%20candidate%20predictions%20using%20GRPO%20rollouts%2C%20averages%0Athem%20to%20form%20a%20pseudo%20label%2C%20and%20estimates%20confidence%20from%20the%20variance%20across%0Athese%20rollouts.%20This%20confidence%20then%20weights%20the%20training%20rewards%2C%20guiding%20the%0Amodel%20to%20focus%20on%20reliable%20supervision.%20Experiments%20on%20three%20datasets%20across%0Asix%20cross-domain%20settings%20show%20that%20URPA%20generalises%20well%20using%20only%20a%20few%0Aunlabelled%20target%20videos.%20Codes%20will%20be%20released%20once%20published.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-quantified%2520Rollout%2520Policy%2520Adaptation%2520for%2520Unlabelled%250A%2520%2520Cross-domain%2520Temporal%2520Grounding%26entry.906535625%3DJian%2520Hu%2520and%2520Zixu%2520Cheng%2520and%2520Shaogang%2520Gong%2520and%2520Isabel%2520Guan%2520and%2520Jianye%2520Hao%2520and%2520Jun%2520Wang%2520and%2520Kun%2520Shao%26entry.1292438233%3D%2520%2520Video%2520Temporal%2520Grounding%2520%2528TG%2529%2520aims%2520to%2520temporally%2520locate%2520video%2520segments%250Amatching%2520a%2520natural%2520language%2520description%2520%2528a%2520query%2529%2520in%2520a%2520long%2520video.%2520While%250AVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520effective%2520at%2520holistic%2520semantic%2520matching%252C%2520they%250Aoften%2520struggle%2520with%2520fine-grained%2520temporal%2520localisation.%2520Recently%252C%2520Group%250ARelative%2520Policy%2520Optimisation%2520%2528GRPO%2529%2520reformulates%2520the%2520inference%2520process%2520as%2520a%250Areinforcement%2520learning%2520task%252C%2520enabling%2520fine-grained%2520grounding%2520and%2520achieving%250Astrong%2520in-domain%2520performance.%2520However%252C%2520GRPO%2520relies%2520on%2520labelled%2520data%252C%2520making%2520it%250Aunsuitable%2520in%2520unlabelled%2520domains.%2520Moreover%252C%2520because%2520videos%2520are%2520large%2520and%250Aexpensive%2520to%2520store%2520and%2520process%252C%2520performing%2520full-scale%2520adaptation%2520introduces%250Aprohibitive%2520latency%2520and%2520computational%2520overhead%252C%2520making%2520it%2520impractical%2520for%250Areal-time%2520deployment.%2520To%2520overcome%2520both%2520problems%252C%2520we%2520introduce%2520a%2520Data-Efficient%250AUnlabelled%2520Cross-domain%2520Temporal%2520Grounding%2520method%252C%2520from%2520which%2520a%2520model%2520is%2520first%250Atrained%2520on%2520a%2520labelled%2520source%2520domain%252C%2520then%2520adapted%2520to%2520a%2520target%2520domain%2520using%2520only%250Aa%2520small%2520number%2520of%2520unlabelled%2520videos%2520from%2520the%2520target%2520domain.%2520This%2520approach%250Aeliminates%2520the%2520need%2520for%2520target%2520annotation%2520and%2520keeps%2520both%2520computational%2520and%250Astorage%2520overhead%2520low%2520enough%2520to%2520run%2520in%2520real%2520time.%2520Specifically%252C%2520we%2520introduce.%250AUncertainty-quantified%2520Rollout%2520Policy%2520Adaptation%2520%2528URPA%2529%2520for%2520cross-domain%250Aknowledge%2520transfer%2520in%2520learning%2520video%2520temporal%2520grounding%2520without%2520target%2520labels.%250AURPA%2520generates%2520multiple%2520candidate%2520predictions%2520using%2520GRPO%2520rollouts%252C%2520averages%250Athem%2520to%2520form%2520a%2520pseudo%2520label%252C%2520and%2520estimates%2520confidence%2520from%2520the%2520variance%2520across%250Athese%2520rollouts.%2520This%2520confidence%2520then%2520weights%2520the%2520training%2520rewards%252C%2520guiding%2520the%250Amodel%2520to%2520focus%2520on%2520reliable%2520supervision.%2520Experiments%2520on%2520three%2520datasets%2520across%250Asix%2520cross-domain%2520settings%2520show%2520that%2520URPA%2520generalises%2520well%2520using%2520only%2520a%2520few%250Aunlabelled%2520target%2520videos.%2520Codes%2520will%2520be%2520released%2520once%2520published.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-quantified%20Rollout%20Policy%20Adaptation%20for%20Unlabelled%0A%20%20Cross-domain%20Temporal%20Grounding&entry.906535625=Jian%20Hu%20and%20Zixu%20Cheng%20and%20Shaogang%20Gong%20and%20Isabel%20Guan%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao&entry.1292438233=%20%20Video%20Temporal%20Grounding%20%28TG%29%20aims%20to%20temporally%20locate%20video%20segments%0Amatching%20a%20natural%20language%20description%20%28a%20query%29%20in%20a%20long%20video.%20While%0AVision-Language%20Models%20%28VLMs%29%20are%20effective%20at%20holistic%20semantic%20matching%2C%20they%0Aoften%20struggle%20with%20fine-grained%20temporal%20localisation.%20Recently%2C%20Group%0ARelative%20Policy%20Optimisation%20%28GRPO%29%20reformulates%20the%20inference%20process%20as%20a%0Areinforcement%20learning%20task%2C%20enabling%20fine-grained%20grounding%20and%20achieving%0Astrong%20in-domain%20performance.%20However%2C%20GRPO%20relies%20on%20labelled%20data%2C%20making%20it%0Aunsuitable%20in%20unlabelled%20domains.%20Moreover%2C%20because%20videos%20are%20large%20and%0Aexpensive%20to%20store%20and%20process%2C%20performing%20full-scale%20adaptation%20introduces%0Aprohibitive%20latency%20and%20computational%20overhead%2C%20making%20it%20impractical%20for%0Areal-time%20deployment.%20To%20overcome%20both%20problems%2C%20we%20introduce%20a%20Data-Efficient%0AUnlabelled%20Cross-domain%20Temporal%20Grounding%20method%2C%20from%20which%20a%20model%20is%20first%0Atrained%20on%20a%20labelled%20source%20domain%2C%20then%20adapted%20to%20a%20target%20domain%20using%20only%0Aa%20small%20number%20of%20unlabelled%20videos%20from%20the%20target%20domain.%20This%20approach%0Aeliminates%20the%20need%20for%20target%20annotation%20and%20keeps%20both%20computational%20and%0Astorage%20overhead%20low%20enough%20to%20run%20in%20real%20time.%20Specifically%2C%20we%20introduce.%0AUncertainty-quantified%20Rollout%20Policy%20Adaptation%20%28URPA%29%20for%20cross-domain%0Aknowledge%20transfer%20in%20learning%20video%20temporal%20grounding%20without%20target%20labels.%0AURPA%20generates%20multiple%20candidate%20predictions%20using%20GRPO%20rollouts%2C%20averages%0Athem%20to%20form%20a%20pseudo%20label%2C%20and%20estimates%20confidence%20from%20the%20variance%20across%0Athese%20rollouts.%20This%20confidence%20then%20weights%20the%20training%20rewards%2C%20guiding%20the%0Amodel%20to%20focus%20on%20reliable%20supervision.%20Experiments%20on%20three%20datasets%20across%0Asix%20cross-domain%20settings%20show%20that%20URPA%20generalises%20well%20using%20only%20a%20few%0Aunlabelled%20target%20videos.%20Codes%20will%20be%20released%20once%20published.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06317v1&entry.124074799=Read"},
{"title": "LLM Robustness Leaderboard v1 --Technical report", "author": "Pierre Peign\u00e9 - Lefebvre and Quentin Feuillade-Montixi and Tom David and Nicolas Miailhe", "abstract": "  This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.\n", "link": "http://arxiv.org/abs/2508.06296v1", "date": "2025-08-08", "relevancy": 0.9241, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4716}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Robustness%20Leaderboard%20v1%20--Technical%20report&body=Title%3A%20LLM%20Robustness%20Leaderboard%20v1%20--Technical%20report%0AAuthor%3A%20Pierre%20Peign%C3%A9%20-%20Lefebvre%20and%20Quentin%20Feuillade-Montixi%20and%20Tom%20David%20and%20Nicolas%20Miailhe%0AAbstract%3A%20%20%20This%20technical%20report%20accompanies%20the%20LLM%20robustness%20leaderboard%20published%20by%0APRISM%20Eval%20for%20the%20Paris%20AI%20Action%20Summit.%20We%20introduce%20PRISM%20Eval%20Behavior%0AElicitation%20Tool%20%28BET%29%2C%20an%20AI%20system%20performing%20automated%20red-teaming%20through%0ADynamic%20Adversarial%20Optimization%20that%20achieves%20100%25%20Attack%20Success%20Rate%20%28ASR%29%0Aagainst%2037%20of%2041%20state-of-the-art%20LLMs.%20Beyond%20binary%20success%20metrics%2C%20we%0Apropose%20a%20fine-grained%20robustness%20metric%20estimating%20the%20average%20number%20of%0Aattempts%20required%20to%20elicit%20harmful%20behaviors%2C%20revealing%20that%20attack%20difficulty%0Avaries%20by%20over%20300-fold%20across%20models%20despite%20universal%20vulnerability.%20We%0Aintroduce%20primitive-level%20vulnerability%20analysis%20to%20identify%20which%20jailbreaking%0Atechniques%20are%20most%20effective%20for%20specific%20hazard%20categories.%20Our%20collaborative%0Aevaluation%20with%20trusted%20third%20parties%20from%20the%20AI%20Safety%20Network%20demonstrates%0Apractical%20pathways%20for%20distributed%20robustness%20assessment%20across%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Robustness%2520Leaderboard%2520v1%2520--Technical%2520report%26entry.906535625%3DPierre%2520Peign%25C3%25A9%2520-%2520Lefebvre%2520and%2520Quentin%2520Feuillade-Montixi%2520and%2520Tom%2520David%2520and%2520Nicolas%2520Miailhe%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520accompanies%2520the%2520LLM%2520robustness%2520leaderboard%2520published%2520by%250APRISM%2520Eval%2520for%2520the%2520Paris%2520AI%2520Action%2520Summit.%2520We%2520introduce%2520PRISM%2520Eval%2520Behavior%250AElicitation%2520Tool%2520%2528BET%2529%252C%2520an%2520AI%2520system%2520performing%2520automated%2520red-teaming%2520through%250ADynamic%2520Adversarial%2520Optimization%2520that%2520achieves%2520100%2525%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%250Aagainst%252037%2520of%252041%2520state-of-the-art%2520LLMs.%2520Beyond%2520binary%2520success%2520metrics%252C%2520we%250Apropose%2520a%2520fine-grained%2520robustness%2520metric%2520estimating%2520the%2520average%2520number%2520of%250Aattempts%2520required%2520to%2520elicit%2520harmful%2520behaviors%252C%2520revealing%2520that%2520attack%2520difficulty%250Avaries%2520by%2520over%2520300-fold%2520across%2520models%2520despite%2520universal%2520vulnerability.%2520We%250Aintroduce%2520primitive-level%2520vulnerability%2520analysis%2520to%2520identify%2520which%2520jailbreaking%250Atechniques%2520are%2520most%2520effective%2520for%2520specific%2520hazard%2520categories.%2520Our%2520collaborative%250Aevaluation%2520with%2520trusted%2520third%2520parties%2520from%2520the%2520AI%2520Safety%2520Network%2520demonstrates%250Apractical%2520pathways%2520for%2520distributed%2520robustness%2520assessment%2520across%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Robustness%20Leaderboard%20v1%20--Technical%20report&entry.906535625=Pierre%20Peign%C3%A9%20-%20Lefebvre%20and%20Quentin%20Feuillade-Montixi%20and%20Tom%20David%20and%20Nicolas%20Miailhe&entry.1292438233=%20%20This%20technical%20report%20accompanies%20the%20LLM%20robustness%20leaderboard%20published%20by%0APRISM%20Eval%20for%20the%20Paris%20AI%20Action%20Summit.%20We%20introduce%20PRISM%20Eval%20Behavior%0AElicitation%20Tool%20%28BET%29%2C%20an%20AI%20system%20performing%20automated%20red-teaming%20through%0ADynamic%20Adversarial%20Optimization%20that%20achieves%20100%25%20Attack%20Success%20Rate%20%28ASR%29%0Aagainst%2037%20of%2041%20state-of-the-art%20LLMs.%20Beyond%20binary%20success%20metrics%2C%20we%0Apropose%20a%20fine-grained%20robustness%20metric%20estimating%20the%20average%20number%20of%0Aattempts%20required%20to%20elicit%20harmful%20behaviors%2C%20revealing%20that%20attack%20difficulty%0Avaries%20by%20over%20300-fold%20across%20models%20despite%20universal%20vulnerability.%20We%0Aintroduce%20primitive-level%20vulnerability%20analysis%20to%20identify%20which%20jailbreaking%0Atechniques%20are%20most%20effective%20for%20specific%20hazard%20categories.%20Our%20collaborative%0Aevaluation%20with%20trusted%20third%20parties%20from%20the%20AI%20Safety%20Network%20demonstrates%0Apractical%20pathways%20for%20distributed%20robustness%20assessment%20across%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06296v1&entry.124074799=Read"},
{"title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "author": "Xin Ci Wong and Duygu Sarikaya and Kieran Zucker and Marc De Kamps and Nishant Ravikumar", "abstract": "  Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.\n", "link": "http://arxiv.org/abs/2508.06327v1", "date": "2025-08-08", "relevancy": 1.7634, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6293}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5969}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Diffusion%20Models%20Bridge%20the%20Domain%20Gap%20in%20Cardiac%20MR%20Imaging%3F&body=Title%3A%20Can%20Diffusion%20Models%20Bridge%20the%20Domain%20Gap%20in%20Cardiac%20MR%20Imaging%3F%0AAuthor%3A%20Xin%20Ci%20Wong%20and%20Duygu%20Sarikaya%20and%20Kieran%20Zucker%20and%20Marc%20De%20Kamps%20and%20Nishant%20Ravikumar%0AAbstract%3A%20%20%20Magnetic%20resonance%20%28MR%29%20imaging%2C%20including%20cardiac%20MR%2C%20is%20prone%20to%20domain%0Ashift%20due%20to%20variations%20in%20imaging%20devices%20and%20acquisition%20protocols.%20This%0Achallenge%20limits%20the%20deployment%20of%20trained%20AI%20models%20in%20real-world%20scenarios%2C%0Awhere%20performance%20degrades%20on%20unseen%20domains.%20Traditional%20solutions%20involve%0Aincreasing%20the%20size%20of%20the%20dataset%20through%20ad-hoc%20image%20augmentation%20or%0Aadditional%20online%20training/transfer%20learning%2C%20which%20have%20several%20limitations.%0ASynthetic%20data%20offers%20a%20promising%20alternative%2C%20but%20anatomical/structural%0Aconsistency%20constraints%20limit%20the%20effectiveness%20of%20generative%20models%20in%0Acreating%20image-label%20pairs.%20To%20address%20this%2C%20we%20propose%20a%20diffusion%20model%20%28DM%29%0Atrained%20on%20a%20source%20domain%20that%20generates%20synthetic%20cardiac%20MR%20images%20that%0Aresemble%20a%20given%20reference.%20The%20synthetic%20data%20maintains%20spatial%20and%20structural%0Afidelity%2C%20ensuring%20similarity%20to%20the%20source%20domain%20and%20compatibility%20with%20the%0Asegmentation%20mask.%20We%20assess%20the%20utility%20of%20our%20generative%20approach%20in%0Amulti-centre%20cardiac%20MR%20segmentation%2C%20using%20the%202D%20nnU-Net%2C%203D%20nnU-Net%20and%0Avanilla%20U-Net%20segmentation%20networks.%20We%20explore%20domain%20generalisation%2C%20where%2C%0Adomain-invariant%20segmentation%20models%20are%20trained%20on%20synthetic%20source%20domain%0Adata%2C%20and%20domain%20adaptation%2C%20where%2C%20we%20shift%20target%20domain%20data%20towards%20the%0Asource%20domain%20using%20the%20DM.%20Both%20strategies%20significantly%20improved%20segmentation%0Aperformance%20on%20data%20from%20an%20unseen%20target%20domain%2C%20in%20terms%20of%20surface-based%0Ametrics%20%28Welch%27s%20t-test%2C%20p%20%3C%200.01%29%2C%20compared%20to%20training%20segmentation%20models%20on%0Areal%20data%20alone.%20The%20proposed%20method%20ameliorates%20the%20need%20for%20transfer%20learning%0Aor%20online%20training%20to%20address%20domain%20shift%20challenges%20in%20cardiac%20MR%20image%0Aanalysis%2C%20especially%20useful%20in%20data-scarce%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Diffusion%2520Models%2520Bridge%2520the%2520Domain%2520Gap%2520in%2520Cardiac%2520MR%2520Imaging%253F%26entry.906535625%3DXin%2520Ci%2520Wong%2520and%2520Duygu%2520Sarikaya%2520and%2520Kieran%2520Zucker%2520and%2520Marc%2520De%2520Kamps%2520and%2520Nishant%2520Ravikumar%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520%2528MR%2529%2520imaging%252C%2520including%2520cardiac%2520MR%252C%2520is%2520prone%2520to%2520domain%250Ashift%2520due%2520to%2520variations%2520in%2520imaging%2520devices%2520and%2520acquisition%2520protocols.%2520This%250Achallenge%2520limits%2520the%2520deployment%2520of%2520trained%2520AI%2520models%2520in%2520real-world%2520scenarios%252C%250Awhere%2520performance%2520degrades%2520on%2520unseen%2520domains.%2520Traditional%2520solutions%2520involve%250Aincreasing%2520the%2520size%2520of%2520the%2520dataset%2520through%2520ad-hoc%2520image%2520augmentation%2520or%250Aadditional%2520online%2520training/transfer%2520learning%252C%2520which%2520have%2520several%2520limitations.%250ASynthetic%2520data%2520offers%2520a%2520promising%2520alternative%252C%2520but%2520anatomical/structural%250Aconsistency%2520constraints%2520limit%2520the%2520effectiveness%2520of%2520generative%2520models%2520in%250Acreating%2520image-label%2520pairs.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520diffusion%2520model%2520%2528DM%2529%250Atrained%2520on%2520a%2520source%2520domain%2520that%2520generates%2520synthetic%2520cardiac%2520MR%2520images%2520that%250Aresemble%2520a%2520given%2520reference.%2520The%2520synthetic%2520data%2520maintains%2520spatial%2520and%2520structural%250Afidelity%252C%2520ensuring%2520similarity%2520to%2520the%2520source%2520domain%2520and%2520compatibility%2520with%2520the%250Asegmentation%2520mask.%2520We%2520assess%2520the%2520utility%2520of%2520our%2520generative%2520approach%2520in%250Amulti-centre%2520cardiac%2520MR%2520segmentation%252C%2520using%2520the%25202D%2520nnU-Net%252C%25203D%2520nnU-Net%2520and%250Avanilla%2520U-Net%2520segmentation%2520networks.%2520We%2520explore%2520domain%2520generalisation%252C%2520where%252C%250Adomain-invariant%2520segmentation%2520models%2520are%2520trained%2520on%2520synthetic%2520source%2520domain%250Adata%252C%2520and%2520domain%2520adaptation%252C%2520where%252C%2520we%2520shift%2520target%2520domain%2520data%2520towards%2520the%250Asource%2520domain%2520using%2520the%2520DM.%2520Both%2520strategies%2520significantly%2520improved%2520segmentation%250Aperformance%2520on%2520data%2520from%2520an%2520unseen%2520target%2520domain%252C%2520in%2520terms%2520of%2520surface-based%250Ametrics%2520%2528Welch%2527s%2520t-test%252C%2520p%2520%253C%25200.01%2529%252C%2520compared%2520to%2520training%2520segmentation%2520models%2520on%250Areal%2520data%2520alone.%2520The%2520proposed%2520method%2520ameliorates%2520the%2520need%2520for%2520transfer%2520learning%250Aor%2520online%2520training%2520to%2520address%2520domain%2520shift%2520challenges%2520in%2520cardiac%2520MR%2520image%250Aanalysis%252C%2520especially%2520useful%2520in%2520data-scarce%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Diffusion%20Models%20Bridge%20the%20Domain%20Gap%20in%20Cardiac%20MR%20Imaging%3F&entry.906535625=Xin%20Ci%20Wong%20and%20Duygu%20Sarikaya%20and%20Kieran%20Zucker%20and%20Marc%20De%20Kamps%20and%20Nishant%20Ravikumar&entry.1292438233=%20%20Magnetic%20resonance%20%28MR%29%20imaging%2C%20including%20cardiac%20MR%2C%20is%20prone%20to%20domain%0Ashift%20due%20to%20variations%20in%20imaging%20devices%20and%20acquisition%20protocols.%20This%0Achallenge%20limits%20the%20deployment%20of%20trained%20AI%20models%20in%20real-world%20scenarios%2C%0Awhere%20performance%20degrades%20on%20unseen%20domains.%20Traditional%20solutions%20involve%0Aincreasing%20the%20size%20of%20the%20dataset%20through%20ad-hoc%20image%20augmentation%20or%0Aadditional%20online%20training/transfer%20learning%2C%20which%20have%20several%20limitations.%0ASynthetic%20data%20offers%20a%20promising%20alternative%2C%20but%20anatomical/structural%0Aconsistency%20constraints%20limit%20the%20effectiveness%20of%20generative%20models%20in%0Acreating%20image-label%20pairs.%20To%20address%20this%2C%20we%20propose%20a%20diffusion%20model%20%28DM%29%0Atrained%20on%20a%20source%20domain%20that%20generates%20synthetic%20cardiac%20MR%20images%20that%0Aresemble%20a%20given%20reference.%20The%20synthetic%20data%20maintains%20spatial%20and%20structural%0Afidelity%2C%20ensuring%20similarity%20to%20the%20source%20domain%20and%20compatibility%20with%20the%0Asegmentation%20mask.%20We%20assess%20the%20utility%20of%20our%20generative%20approach%20in%0Amulti-centre%20cardiac%20MR%20segmentation%2C%20using%20the%202D%20nnU-Net%2C%203D%20nnU-Net%20and%0Avanilla%20U-Net%20segmentation%20networks.%20We%20explore%20domain%20generalisation%2C%20where%2C%0Adomain-invariant%20segmentation%20models%20are%20trained%20on%20synthetic%20source%20domain%0Adata%2C%20and%20domain%20adaptation%2C%20where%2C%20we%20shift%20target%20domain%20data%20towards%20the%0Asource%20domain%20using%20the%20DM.%20Both%20strategies%20significantly%20improved%20segmentation%0Aperformance%20on%20data%20from%20an%20unseen%20target%20domain%2C%20in%20terms%20of%20surface-based%0Ametrics%20%28Welch%27s%20t-test%2C%20p%20%3C%200.01%29%2C%20compared%20to%20training%20segmentation%20models%20on%0Areal%20data%20alone.%20The%20proposed%20method%20ameliorates%20the%20need%20for%20transfer%20learning%0Aor%20online%20training%20to%20address%20domain%20shift%20challenges%20in%20cardiac%20MR%20image%0Aanalysis%2C%20especially%20useful%20in%20data-scarce%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06327v1&entry.124074799=Read"},
{"title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation", "author": "Lucas Caccia and Alan Ansell and Edoardo Ponti and Ivan Vuli\u0107 and Alessandro Sordoni", "abstract": "  Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.\n", "link": "http://arxiv.org/abs/2503.08727v4", "date": "2025-08-08", "relevancy": 1.9586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation&body=Title%3A%20Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation%0AAuthor%3A%20Lucas%20Caccia%20and%20Alan%20Ansell%20and%20Edoardo%20Ponti%20and%20Ivan%20Vuli%C4%87%20and%20Alessandro%20Sordoni%0AAbstract%3A%20%20%20Dynamically%20integrating%20new%20or%20rapidly%20evolving%20information%20after%20%28Large%29%0ALanguage%20Model%20pre-training%20remains%20challenging%2C%20particularly%20in%20low-data%0Ascenarios%20or%20when%20dealing%20with%20private%20and%20specialized%20documents.%20In-context%0Alearning%20and%20retrieval-augmented%20generation%20%28RAG%29%20face%20limitations%2C%20including%0Atheir%20high%20inference%20costs%20and%20their%20inability%20to%20capture%20global%20document%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20way%20of%20modularizing%20knowledge%20by%0Atraining%20document-level%20Knowledge%20Modules%20%28KMs%29.%20KMs%20are%20lightweight%20components%0Aimplemented%20as%20parameter-efficient%20LoRA%20modules%2C%20which%20are%20trained%20to%20store%0Ainformation%20about%20new%20documents%20and%20can%20be%20easily%20plugged%20into%20models%20on%0Ademand.%20We%20show%20that%20next-token%20prediction%20performs%20poorly%20as%20the%20training%0Aobjective%20for%20KMs.%20We%20instead%20propose%20Deep%20Context%20Distillation%3A%20we%20learn%20KMs%0Aparameters%20such%20as%20to%20simulate%20hidden%20states%20and%20logits%20of%20a%20teacher%20that%20takes%0Athe%20document%20in%20context.%20Our%20method%20outperforms%20standard%20next-token%20prediction%0Aand%20pre-instruction%20training%20techniques%2C%20across%20two%20datasets.%20Finally%2C%20we%0Ahighlight%20synergies%20between%20KMs%20and%20RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08727v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Plug-n-Play%2520Knowledge%2520Modules%2520with%2520Deep%2520Context%2520Distillation%26entry.906535625%3DLucas%2520Caccia%2520and%2520Alan%2520Ansell%2520and%2520Edoardo%2520Ponti%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Alessandro%2520Sordoni%26entry.1292438233%3D%2520%2520Dynamically%2520integrating%2520new%2520or%2520rapidly%2520evolving%2520information%2520after%2520%2528Large%2529%250ALanguage%2520Model%2520pre-training%2520remains%2520challenging%252C%2520particularly%2520in%2520low-data%250Ascenarios%2520or%2520when%2520dealing%2520with%2520private%2520and%2520specialized%2520documents.%2520In-context%250Alearning%2520and%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520face%2520limitations%252C%2520including%250Atheir%2520high%2520inference%2520costs%2520and%2520their%2520inability%2520to%2520capture%2520global%2520document%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520way%2520of%2520modularizing%2520knowledge%2520by%250Atraining%2520document-level%2520Knowledge%2520Modules%2520%2528KMs%2529.%2520KMs%2520are%2520lightweight%2520components%250Aimplemented%2520as%2520parameter-efficient%2520LoRA%2520modules%252C%2520which%2520are%2520trained%2520to%2520store%250Ainformation%2520about%2520new%2520documents%2520and%2520can%2520be%2520easily%2520plugged%2520into%2520models%2520on%250Ademand.%2520We%2520show%2520that%2520next-token%2520prediction%2520performs%2520poorly%2520as%2520the%2520training%250Aobjective%2520for%2520KMs.%2520We%2520instead%2520propose%2520Deep%2520Context%2520Distillation%253A%2520we%2520learn%2520KMs%250Aparameters%2520such%2520as%2520to%2520simulate%2520hidden%2520states%2520and%2520logits%2520of%2520a%2520teacher%2520that%2520takes%250Athe%2520document%2520in%2520context.%2520Our%2520method%2520outperforms%2520standard%2520next-token%2520prediction%250Aand%2520pre-instruction%2520training%2520techniques%252C%2520across%2520two%2520datasets.%2520Finally%252C%2520we%250Ahighlight%2520synergies%2520between%2520KMs%2520and%2520RAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08727v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation&entry.906535625=Lucas%20Caccia%20and%20Alan%20Ansell%20and%20Edoardo%20Ponti%20and%20Ivan%20Vuli%C4%87%20and%20Alessandro%20Sordoni&entry.1292438233=%20%20Dynamically%20integrating%20new%20or%20rapidly%20evolving%20information%20after%20%28Large%29%0ALanguage%20Model%20pre-training%20remains%20challenging%2C%20particularly%20in%20low-data%0Ascenarios%20or%20when%20dealing%20with%20private%20and%20specialized%20documents.%20In-context%0Alearning%20and%20retrieval-augmented%20generation%20%28RAG%29%20face%20limitations%2C%20including%0Atheir%20high%20inference%20costs%20and%20their%20inability%20to%20capture%20global%20document%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20way%20of%20modularizing%20knowledge%20by%0Atraining%20document-level%20Knowledge%20Modules%20%28KMs%29.%20KMs%20are%20lightweight%20components%0Aimplemented%20as%20parameter-efficient%20LoRA%20modules%2C%20which%20are%20trained%20to%20store%0Ainformation%20about%20new%20documents%20and%20can%20be%20easily%20plugged%20into%20models%20on%0Ademand.%20We%20show%20that%20next-token%20prediction%20performs%20poorly%20as%20the%20training%0Aobjective%20for%20KMs.%20We%20instead%20propose%20Deep%20Context%20Distillation%3A%20we%20learn%20KMs%0Aparameters%20such%20as%20to%20simulate%20hidden%20states%20and%20logits%20of%20a%20teacher%20that%20takes%0Athe%20document%20in%20context.%20Our%20method%20outperforms%20standard%20next-token%20prediction%0Aand%20pre-instruction%20training%20techniques%2C%20across%20two%20datasets.%20Finally%2C%20we%0Ahighlight%20synergies%20between%20KMs%20and%20RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08727v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


