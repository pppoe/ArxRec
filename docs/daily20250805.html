<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250804.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal\n  Spans via 3D Gaussian Splatting", "author": "Yijun Xu and Jingrui Zhang and Yuhan Chen and Dingwen Wang and Lei Yu and Chu He", "abstract": "  Modeling complex rigid motion across large spatiotemporal spans remains an\nunresolved challenge in dynamic reconstruction. Existing paradigms are mainly\nconfined to short-term, small-scale deformation and offer limited consideration\nfor physical consistency. This study proposes PMGS, focusing on reconstructing\nProjectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:\n1) Target Modeling: achieving object-centralized reconstruction through dynamic\nscene decomposition and an improved point density control; 2) Motion Recovery:\nrestoring full motion sequences by learning per-frame SE(3) poses. We introduce\nan acceleration consistency constraint to bridge Newtonian mechanics and pose\nestimation, and design a dynamic simulated annealing strategy that adaptively\nschedules learning rates based on motion states. Futhermore, we devise a Kalman\nfusion scheme to optimize error accumulation from multi-source observations to\nmitigate disturbances. Experiments show PMGS's superior performance in\nreconstructing high-speed nonlinear rigid motion compared to mainstream dynamic\nmethods.\n", "link": "http://arxiv.org/abs/2508.02660v1", "date": "2025-08-04", "relevancy": 3.4005, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6806}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20across%20Large%20Spatiotemporal%0A%20%20Spans%20via%203D%20Gaussian%20Splatting&body=Title%3A%20PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20across%20Large%20Spatiotemporal%0A%20%20Spans%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Yuhan%20Chen%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He%0AAbstract%3A%20%20%20Modeling%20complex%20rigid%20motion%20across%20large%20spatiotemporal%20spans%20remains%20an%0Aunresolved%20challenge%20in%20dynamic%20reconstruction.%20Existing%20paradigms%20are%20mainly%0Aconfined%20to%20short-term%2C%20small-scale%20deformation%20and%20offer%20limited%20consideration%0Afor%20physical%20consistency.%20This%20study%20proposes%20PMGS%2C%20focusing%20on%20reconstructing%0AProjectile%20Motion%20via%203D%20Gaussian%20Splatting.%20The%20workflow%20comprises%20two%20stages%3A%0A1%29%20Target%20Modeling%3A%20achieving%20object-centralized%20reconstruction%20through%20dynamic%0Ascene%20decomposition%20and%20an%20improved%20point%20density%20control%3B%202%29%20Motion%20Recovery%3A%0Arestoring%20full%20motion%20sequences%20by%20learning%20per-frame%20SE%283%29%20poses.%20We%20introduce%0Aan%20acceleration%20consistency%20constraint%20to%20bridge%20Newtonian%20mechanics%20and%20pose%0Aestimation%2C%20and%20design%20a%20dynamic%20simulated%20annealing%20strategy%20that%20adaptively%0Aschedules%20learning%20rates%20based%20on%20motion%20states.%20Futhermore%2C%20we%20devise%20a%20Kalman%0Afusion%20scheme%20to%20optimize%20error%20accumulation%20from%20multi-source%20observations%20to%0Amitigate%20disturbances.%20Experiments%20show%20PMGS%27s%20superior%20performance%20in%0Areconstructing%20high-speed%20nonlinear%20rigid%20motion%20compared%20to%20mainstream%20dynamic%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMGS%253A%2520Reconstruction%2520of%2520Projectile%2520Motion%2520across%2520Large%2520Spatiotemporal%250A%2520%2520Spans%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYijun%2520Xu%2520and%2520Jingrui%2520Zhang%2520and%2520Yuhan%2520Chen%2520and%2520Dingwen%2520Wang%2520and%2520Lei%2520Yu%2520and%2520Chu%2520He%26entry.1292438233%3D%2520%2520Modeling%2520complex%2520rigid%2520motion%2520across%2520large%2520spatiotemporal%2520spans%2520remains%2520an%250Aunresolved%2520challenge%2520in%2520dynamic%2520reconstruction.%2520Existing%2520paradigms%2520are%2520mainly%250Aconfined%2520to%2520short-term%252C%2520small-scale%2520deformation%2520and%2520offer%2520limited%2520consideration%250Afor%2520physical%2520consistency.%2520This%2520study%2520proposes%2520PMGS%252C%2520focusing%2520on%2520reconstructing%250AProjectile%2520Motion%2520via%25203D%2520Gaussian%2520Splatting.%2520The%2520workflow%2520comprises%2520two%2520stages%253A%250A1%2529%2520Target%2520Modeling%253A%2520achieving%2520object-centralized%2520reconstruction%2520through%2520dynamic%250Ascene%2520decomposition%2520and%2520an%2520improved%2520point%2520density%2520control%253B%25202%2529%2520Motion%2520Recovery%253A%250Arestoring%2520full%2520motion%2520sequences%2520by%2520learning%2520per-frame%2520SE%25283%2529%2520poses.%2520We%2520introduce%250Aan%2520acceleration%2520consistency%2520constraint%2520to%2520bridge%2520Newtonian%2520mechanics%2520and%2520pose%250Aestimation%252C%2520and%2520design%2520a%2520dynamic%2520simulated%2520annealing%2520strategy%2520that%2520adaptively%250Aschedules%2520learning%2520rates%2520based%2520on%2520motion%2520states.%2520Futhermore%252C%2520we%2520devise%2520a%2520Kalman%250Afusion%2520scheme%2520to%2520optimize%2520error%2520accumulation%2520from%2520multi-source%2520observations%2520to%250Amitigate%2520disturbances.%2520Experiments%2520show%2520PMGS%2527s%2520superior%2520performance%2520in%250Areconstructing%2520high-speed%2520nonlinear%2520rigid%2520motion%2520compared%2520to%2520mainstream%2520dynamic%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20across%20Large%20Spatiotemporal%0A%20%20Spans%20via%203D%20Gaussian%20Splatting&entry.906535625=Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Yuhan%20Chen%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He&entry.1292438233=%20%20Modeling%20complex%20rigid%20motion%20across%20large%20spatiotemporal%20spans%20remains%20an%0Aunresolved%20challenge%20in%20dynamic%20reconstruction.%20Existing%20paradigms%20are%20mainly%0Aconfined%20to%20short-term%2C%20small-scale%20deformation%20and%20offer%20limited%20consideration%0Afor%20physical%20consistency.%20This%20study%20proposes%20PMGS%2C%20focusing%20on%20reconstructing%0AProjectile%20Motion%20via%203D%20Gaussian%20Splatting.%20The%20workflow%20comprises%20two%20stages%3A%0A1%29%20Target%20Modeling%3A%20achieving%20object-centralized%20reconstruction%20through%20dynamic%0Ascene%20decomposition%20and%20an%20improved%20point%20density%20control%3B%202%29%20Motion%20Recovery%3A%0Arestoring%20full%20motion%20sequences%20by%20learning%20per-frame%20SE%283%29%20poses.%20We%20introduce%0Aan%20acceleration%20consistency%20constraint%20to%20bridge%20Newtonian%20mechanics%20and%20pose%0Aestimation%2C%20and%20design%20a%20dynamic%20simulated%20annealing%20strategy%20that%20adaptively%0Aschedules%20learning%20rates%20based%20on%20motion%20states.%20Futhermore%2C%20we%20devise%20a%20Kalman%0Afusion%20scheme%20to%20optimize%20error%20accumulation%20from%20multi-source%20observations%20to%0Amitigate%20disturbances.%20Experiments%20show%20PMGS%27s%20superior%20performance%20in%0Areconstructing%20high-speed%20nonlinear%20rigid%20motion%20compared%20to%20mainstream%20dynamic%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02660v1&entry.124074799=Read"},
{"title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction", "author": "Yikuang Yuluo and Yue Ma and Kuan Shen and Tongtong Jin and Wang Liao and Yangpu Ma and Fuquan Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.\n", "link": "http://arxiv.org/abs/2508.02408v1", "date": "2025-08-04", "relevancy": 3.3132, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7125}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&body=Title%3A%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction%0AAuthor%3A%20Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR-Gaussian%253A%2520Graph-Based%2520Radiative%2520Gaussian%2520Splatting%2520for%2520Sparse-View%2520CT%250A%2520%2520Reconstruction%26entry.906535625%3DYikuang%2520Yuluo%2520and%2520Yue%2520Ma%2520and%2520Kuan%2520Shen%2520and%2520Tongtong%2520Jin%2520and%2520Wang%2520Liao%2520and%2520Yangpu%2520Ma%2520and%2520Fuquan%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520CT%250Areconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520the%2520average%2520gradient%250Amagnitude%2520of%2520points%2520within%2520the%2520view%252C%2520often%2520leading%2520to%2520severe%2520needle-like%250Aartifacts%2520under%2520sparse-view%2520conditions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AGR-Gaussian%252C%2520a%2520graph-based%25203D%2520Gaussian%2520Splatting%2520framework%2520that%2520suppresses%250Aneedle-like%2520artifacts%2520and%2520improves%2520reconstruction%2520accuracy%2520under%2520sparse-view%250Aconditions.%2520Our%2520framework%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520Denoised%2520Point%250ACloud%2520Initialization%2520Strategy%2520that%2520reduces%2520initialization%2520errors%2520and%250Aaccelerates%2520convergence%253B%2520and%2520%25282%2529%2520a%2520Pixel-Graph-Aware%2520Gradient%2520Strategy%2520that%250Arefines%2520gradient%2520computation%2520using%2520graph-based%2520density%2520differences%252C%2520improving%250Asplitting%2520accuracy%2520and%2520density%2520representation.%2520Experiments%2520on%2520X-3D%2520and%250Areal-world%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520GR-Gaussian%252C%2520achieving%2520PSNR%250Aimprovements%2520of%25200.67%2520dB%2520and%25200.92%2520dB%252C%2520and%2520SSIM%2520gains%2520of%25200.011%2520and%25200.021.%2520These%250Aresults%2520highlight%2520the%2520applicability%2520of%2520GR-Gaussian%2520for%2520accurate%2520CT%250Areconstruction%2520under%2520challenging%2520sparse-view%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&entry.906535625=Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02408v1&entry.124074799=Read"},
{"title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction", "author": "Yikuang Yuluo and Yue Ma and Kuan Shen and Tongtong Jin and Wang Liao and Yangpu Ma and Fuquan Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.\n", "link": "http://arxiv.org/abs/2508.02408v1", "date": "2025-08-04", "relevancy": 3.3132, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7125}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&body=Title%3A%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction%0AAuthor%3A%20Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR-Gaussian%253A%2520Graph-Based%2520Radiative%2520Gaussian%2520Splatting%2520for%2520Sparse-View%2520CT%250A%2520%2520Reconstruction%26entry.906535625%3DYikuang%2520Yuluo%2520and%2520Yue%2520Ma%2520and%2520Kuan%2520Shen%2520and%2520Tongtong%2520Jin%2520and%2520Wang%2520Liao%2520and%2520Yangpu%2520Ma%2520and%2520Fuquan%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520CT%250Areconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520the%2520average%2520gradient%250Amagnitude%2520of%2520points%2520within%2520the%2520view%252C%2520often%2520leading%2520to%2520severe%2520needle-like%250Aartifacts%2520under%2520sparse-view%2520conditions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AGR-Gaussian%252C%2520a%2520graph-based%25203D%2520Gaussian%2520Splatting%2520framework%2520that%2520suppresses%250Aneedle-like%2520artifacts%2520and%2520improves%2520reconstruction%2520accuracy%2520under%2520sparse-view%250Aconditions.%2520Our%2520framework%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520Denoised%2520Point%250ACloud%2520Initialization%2520Strategy%2520that%2520reduces%2520initialization%2520errors%2520and%250Aaccelerates%2520convergence%253B%2520and%2520%25282%2529%2520a%2520Pixel-Graph-Aware%2520Gradient%2520Strategy%2520that%250Arefines%2520gradient%2520computation%2520using%2520graph-based%2520density%2520differences%252C%2520improving%250Asplitting%2520accuracy%2520and%2520density%2520representation.%2520Experiments%2520on%2520X-3D%2520and%250Areal-world%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520GR-Gaussian%252C%2520achieving%2520PSNR%250Aimprovements%2520of%25200.67%2520dB%2520and%25200.92%2520dB%252C%2520and%2520SSIM%2520gains%2520of%25200.011%2520and%25200.021.%2520These%250Aresults%2520highlight%2520the%2520applicability%2520of%2520GR-Gaussian%2520for%2520accurate%2520CT%250Areconstruction%2520under%2520challenging%2520sparse-view%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&entry.906535625=Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02408v1&entry.124074799=Read"},
{"title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian\n  Splatting", "author": "Jianchao Wang and Peng Zhou and Cen Li and Rong Quan and Jie Qin", "abstract": "  3D Gaussian Splatting (3DGS) is a powerful and computationally efficient\nrepresentation for 3D reconstruction. Despite its strengths, 3DGS often\nproduces floating artifacts, which are erroneous structures detached from the\nactual geometry and significantly degrade visual fidelity. The underlying\nmechanisms causing these artifacts, particularly in low-quality initialization\nscenarios, have not been fully explored. In this paper, we investigate the\norigins of floating artifacts from a frequency-domain perspective and identify\nunder-optimized Gaussians as the primary source. Based on our analysis, we\npropose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),\nwhich selectively expands under-optimized Gaussians to prioritize accurate\nlow-frequency learning. Additionally, we introduce complementary depth-based\nand scale-based strategies to dynamically refine Gaussian expansion,\neffectively mitigating detail erosion. Extensive experiments on both synthetic\nand real-world datasets demonstrate that EFA-GS substantially reduces floating\nartifacts while preserving high-frequency details, achieving an improvement of\n1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we\nvalidate the effectiveness of our approach in downstream 3D editing tasks. Our\nimplementation will be released on GitHub.\n", "link": "http://arxiv.org/abs/2508.02493v1", "date": "2025-08-04", "relevancy": 3.3125, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.69}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6685}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jianchao%20Wang%20and%20Peng%20Zhou%20and%20Cen%20Li%20and%20Rong%20Quan%20and%20Jie%20Qin%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20powerful%20and%20computationally%20efficient%0Arepresentation%20for%203D%20reconstruction.%20Despite%20its%20strengths%2C%203DGS%20often%0Aproduces%20floating%20artifacts%2C%20which%20are%20erroneous%20structures%20detached%20from%20the%0Aactual%20geometry%20and%20significantly%20degrade%20visual%20fidelity.%20The%20underlying%0Amechanisms%20causing%20these%20artifacts%2C%20particularly%20in%20low-quality%20initialization%0Ascenarios%2C%20have%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%20investigate%20the%0Aorigins%20of%20floating%20artifacts%20from%20a%20frequency-domain%20perspective%20and%20identify%0Aunder-optimized%20Gaussians%20as%20the%20primary%20source.%20Based%20on%20our%20analysis%2C%20we%0Apropose%20%5Ctextit%7BEliminating-Floating-Artifacts%7D%20Gaussian%20Splatting%20%28EFA-GS%29%2C%0Awhich%20selectively%20expands%20under-optimized%20Gaussians%20to%20prioritize%20accurate%0Alow-frequency%20learning.%20Additionally%2C%20we%20introduce%20complementary%20depth-based%0Aand%20scale-based%20strategies%20to%20dynamically%20refine%20Gaussian%20expansion%2C%0Aeffectively%20mitigating%20detail%20erosion.%20Extensive%20experiments%20on%20both%20synthetic%0Aand%20real-world%20datasets%20demonstrate%20that%20EFA-GS%20substantially%20reduces%20floating%0Aartifacts%20while%20preserving%20high-frequency%20details%2C%20achieving%20an%20improvement%20of%0A1.68%20dB%20in%20PSNR%20over%20baseline%20method%20on%20our%20RWLQ%20dataset.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20in%20downstream%203D%20editing%20tasks.%20Our%0Aimplementation%20will%20be%20released%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Frequency%2520First%253A%2520Eliminating%2520Floating%2520Artifacts%2520in%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJianchao%2520Wang%2520and%2520Peng%2520Zhou%2520and%2520Cen%2520Li%2520and%2520Rong%2520Quan%2520and%2520Jie%2520Qin%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520a%2520powerful%2520and%2520computationally%2520efficient%250Arepresentation%2520for%25203D%2520reconstruction.%2520Despite%2520its%2520strengths%252C%25203DGS%2520often%250Aproduces%2520floating%2520artifacts%252C%2520which%2520are%2520erroneous%2520structures%2520detached%2520from%2520the%250Aactual%2520geometry%2520and%2520significantly%2520degrade%2520visual%2520fidelity.%2520The%2520underlying%250Amechanisms%2520causing%2520these%2520artifacts%252C%2520particularly%2520in%2520low-quality%2520initialization%250Ascenarios%252C%2520have%2520not%2520been%2520fully%2520explored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aorigins%2520of%2520floating%2520artifacts%2520from%2520a%2520frequency-domain%2520perspective%2520and%2520identify%250Aunder-optimized%2520Gaussians%2520as%2520the%2520primary%2520source.%2520Based%2520on%2520our%2520analysis%252C%2520we%250Apropose%2520%255Ctextit%257BEliminating-Floating-Artifacts%257D%2520Gaussian%2520Splatting%2520%2528EFA-GS%2529%252C%250Awhich%2520selectively%2520expands%2520under-optimized%2520Gaussians%2520to%2520prioritize%2520accurate%250Alow-frequency%2520learning.%2520Additionally%252C%2520we%2520introduce%2520complementary%2520depth-based%250Aand%2520scale-based%2520strategies%2520to%2520dynamically%2520refine%2520Gaussian%2520expansion%252C%250Aeffectively%2520mitigating%2520detail%2520erosion.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%250Aand%2520real-world%2520datasets%2520demonstrate%2520that%2520EFA-GS%2520substantially%2520reduces%2520floating%250Aartifacts%2520while%2520preserving%2520high-frequency%2520details%252C%2520achieving%2520an%2520improvement%2520of%250A1.68%2520dB%2520in%2520PSNR%2520over%2520baseline%2520method%2520on%2520our%2520RWLQ%2520dataset.%2520Furthermore%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520downstream%25203D%2520editing%2520tasks.%2520Our%250Aimplementation%2520will%2520be%2520released%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Jianchao%20Wang%20and%20Peng%20Zhou%20and%20Cen%20Li%20and%20Rong%20Quan%20and%20Jie%20Qin&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20powerful%20and%20computationally%20efficient%0Arepresentation%20for%203D%20reconstruction.%20Despite%20its%20strengths%2C%203DGS%20often%0Aproduces%20floating%20artifacts%2C%20which%20are%20erroneous%20structures%20detached%20from%20the%0Aactual%20geometry%20and%20significantly%20degrade%20visual%20fidelity.%20The%20underlying%0Amechanisms%20causing%20these%20artifacts%2C%20particularly%20in%20low-quality%20initialization%0Ascenarios%2C%20have%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%20investigate%20the%0Aorigins%20of%20floating%20artifacts%20from%20a%20frequency-domain%20perspective%20and%20identify%0Aunder-optimized%20Gaussians%20as%20the%20primary%20source.%20Based%20on%20our%20analysis%2C%20we%0Apropose%20%5Ctextit%7BEliminating-Floating-Artifacts%7D%20Gaussian%20Splatting%20%28EFA-GS%29%2C%0Awhich%20selectively%20expands%20under-optimized%20Gaussians%20to%20prioritize%20accurate%0Alow-frequency%20learning.%20Additionally%2C%20we%20introduce%20complementary%20depth-based%0Aand%20scale-based%20strategies%20to%20dynamically%20refine%20Gaussian%20expansion%2C%0Aeffectively%20mitigating%20detail%20erosion.%20Extensive%20experiments%20on%20both%20synthetic%0Aand%20real-world%20datasets%20demonstrate%20that%20EFA-GS%20substantially%20reduces%20floating%0Aartifacts%20while%20preserving%20high-frequency%20details%2C%20achieving%20an%20improvement%20of%0A1.68%20dB%20in%20PSNR%20over%20baseline%20method%20on%20our%20RWLQ%20dataset.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20in%20downstream%203D%20editing%20tasks.%20Our%0Aimplementation%20will%20be%20released%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02493v1&entry.124074799=Read"},
{"title": "SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene\n  Completion", "author": "Rui Qian and Haozhi Cao and Tianchen Deng and Shenghai Yuan and Lihua Xie", "abstract": "  Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising\ntask that aims to infer dense geometric and semantic descriptions of a scene\nfrom a single image. While recent object-centric paradigms significantly\nimprove efficiency by leveraging flexible 3D Gaussian primitives, they still\nrely heavily on a large number of randomly initialized primitives, which\ninevitably leads to 1) inefficient primitive initialization and 2) outlier\nprimitives that introduce erroneous artifacts. In this paper, we propose\nSplatSSC, a novel framework that resolves these limitations with a depth-guided\ninitialization strategy and a principled Gaussian aggregator. Instead of random\ninitialization, SplatSSC utilizes a dedicated depth branch composed of a\nGroup-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image\nand depth features to generate a sparse yet representative set of initial\nGaussian primitives. To mitigate noise from outlier primitives, we develop the\nDecoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing\ngeometric and semantic predictions during the Gaussian-to-voxel splatting\nprocess. Complemented with a specialized Probability Scale Loss, our method\nachieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming\nprior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both\nlatency and memory consumption by more than 9.3%. The code will be released\nupon acceptance.\n", "link": "http://arxiv.org/abs/2508.02261v1", "date": "2025-08-04", "relevancy": 3.2167, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6744}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6627}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatSSC%3A%20Decoupled%20Depth-Guided%20Gaussian%20Splatting%20for%20Semantic%20Scene%0A%20%20Completion&body=Title%3A%20SplatSSC%3A%20Decoupled%20Depth-Guided%20Gaussian%20Splatting%20for%20Semantic%20Scene%0A%20%20Completion%0AAuthor%3A%20Rui%20Qian%20and%20Haozhi%20Cao%20and%20Tianchen%20Deng%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Monocular%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20is%20a%20challenging%20yet%20promising%0Atask%20that%20aims%20to%20infer%20dense%20geometric%20and%20semantic%20descriptions%20of%20a%20scene%0Afrom%20a%20single%20image.%20While%20recent%20object-centric%20paradigms%20significantly%0Aimprove%20efficiency%20by%20leveraging%20flexible%203D%20Gaussian%20primitives%2C%20they%20still%0Arely%20heavily%20on%20a%20large%20number%20of%20randomly%20initialized%20primitives%2C%20which%0Ainevitably%20leads%20to%201%29%20inefficient%20primitive%20initialization%20and%202%29%20outlier%0Aprimitives%20that%20introduce%20erroneous%20artifacts.%20In%20this%20paper%2C%20we%20propose%0ASplatSSC%2C%20a%20novel%20framework%20that%20resolves%20these%20limitations%20with%20a%20depth-guided%0Ainitialization%20strategy%20and%20a%20principled%20Gaussian%20aggregator.%20Instead%20of%20random%0Ainitialization%2C%20SplatSSC%20utilizes%20a%20dedicated%20depth%20branch%20composed%20of%20a%0AGroup-wise%20Multi-scale%20Fusion%20%28GMF%29%20module%2C%20which%20integrates%20multi-scale%20image%0Aand%20depth%20features%20to%20generate%20a%20sparse%20yet%20representative%20set%20of%20initial%0AGaussian%20primitives.%20To%20mitigate%20noise%20from%20outlier%20primitives%2C%20we%20develop%20the%0ADecoupled%20Gaussian%20Aggregator%20%28DGA%29%2C%20which%20enhances%20robustness%20by%20decomposing%0Ageometric%20and%20semantic%20predictions%20during%20the%20Gaussian-to-voxel%20splatting%0Aprocess.%20Complemented%20with%20a%20specialized%20Probability%20Scale%20Loss%2C%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20the%20Occ-ScanNet%20dataset%2C%20outperforming%0Aprior%20approaches%20by%20over%206.3%25%20in%20IoU%20and%204.1%25%20in%20mIoU%2C%20while%20reducing%20both%0Alatency%20and%20memory%20consumption%20by%20more%20than%209.3%25.%20The%20code%20will%20be%20released%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatSSC%253A%2520Decoupled%2520Depth-Guided%2520Gaussian%2520Splatting%2520for%2520Semantic%2520Scene%250A%2520%2520Completion%26entry.906535625%3DRui%2520Qian%2520and%2520Haozhi%2520Cao%2520and%2520Tianchen%2520Deng%2520and%2520Shenghai%2520Yuan%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Monocular%25203D%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520is%2520a%2520challenging%2520yet%2520promising%250Atask%2520that%2520aims%2520to%2520infer%2520dense%2520geometric%2520and%2520semantic%2520descriptions%2520of%2520a%2520scene%250Afrom%2520a%2520single%2520image.%2520While%2520recent%2520object-centric%2520paradigms%2520significantly%250Aimprove%2520efficiency%2520by%2520leveraging%2520flexible%25203D%2520Gaussian%2520primitives%252C%2520they%2520still%250Arely%2520heavily%2520on%2520a%2520large%2520number%2520of%2520randomly%2520initialized%2520primitives%252C%2520which%250Ainevitably%2520leads%2520to%25201%2529%2520inefficient%2520primitive%2520initialization%2520and%25202%2529%2520outlier%250Aprimitives%2520that%2520introduce%2520erroneous%2520artifacts.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASplatSSC%252C%2520a%2520novel%2520framework%2520that%2520resolves%2520these%2520limitations%2520with%2520a%2520depth-guided%250Ainitialization%2520strategy%2520and%2520a%2520principled%2520Gaussian%2520aggregator.%2520Instead%2520of%2520random%250Ainitialization%252C%2520SplatSSC%2520utilizes%2520a%2520dedicated%2520depth%2520branch%2520composed%2520of%2520a%250AGroup-wise%2520Multi-scale%2520Fusion%2520%2528GMF%2529%2520module%252C%2520which%2520integrates%2520multi-scale%2520image%250Aand%2520depth%2520features%2520to%2520generate%2520a%2520sparse%2520yet%2520representative%2520set%2520of%2520initial%250AGaussian%2520primitives.%2520To%2520mitigate%2520noise%2520from%2520outlier%2520primitives%252C%2520we%2520develop%2520the%250ADecoupled%2520Gaussian%2520Aggregator%2520%2528DGA%2529%252C%2520which%2520enhances%2520robustness%2520by%2520decomposing%250Ageometric%2520and%2520semantic%2520predictions%2520during%2520the%2520Gaussian-to-voxel%2520splatting%250Aprocess.%2520Complemented%2520with%2520a%2520specialized%2520Probability%2520Scale%2520Loss%252C%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520the%2520Occ-ScanNet%2520dataset%252C%2520outperforming%250Aprior%2520approaches%2520by%2520over%25206.3%2525%2520in%2520IoU%2520and%25204.1%2525%2520in%2520mIoU%252C%2520while%2520reducing%2520both%250Alatency%2520and%2520memory%2520consumption%2520by%2520more%2520than%25209.3%2525.%2520The%2520code%2520will%2520be%2520released%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatSSC%3A%20Decoupled%20Depth-Guided%20Gaussian%20Splatting%20for%20Semantic%20Scene%0A%20%20Completion&entry.906535625=Rui%20Qian%20and%20Haozhi%20Cao%20and%20Tianchen%20Deng%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie&entry.1292438233=%20%20Monocular%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20is%20a%20challenging%20yet%20promising%0Atask%20that%20aims%20to%20infer%20dense%20geometric%20and%20semantic%20descriptions%20of%20a%20scene%0Afrom%20a%20single%20image.%20While%20recent%20object-centric%20paradigms%20significantly%0Aimprove%20efficiency%20by%20leveraging%20flexible%203D%20Gaussian%20primitives%2C%20they%20still%0Arely%20heavily%20on%20a%20large%20number%20of%20randomly%20initialized%20primitives%2C%20which%0Ainevitably%20leads%20to%201%29%20inefficient%20primitive%20initialization%20and%202%29%20outlier%0Aprimitives%20that%20introduce%20erroneous%20artifacts.%20In%20this%20paper%2C%20we%20propose%0ASplatSSC%2C%20a%20novel%20framework%20that%20resolves%20these%20limitations%20with%20a%20depth-guided%0Ainitialization%20strategy%20and%20a%20principled%20Gaussian%20aggregator.%20Instead%20of%20random%0Ainitialization%2C%20SplatSSC%20utilizes%20a%20dedicated%20depth%20branch%20composed%20of%20a%0AGroup-wise%20Multi-scale%20Fusion%20%28GMF%29%20module%2C%20which%20integrates%20multi-scale%20image%0Aand%20depth%20features%20to%20generate%20a%20sparse%20yet%20representative%20set%20of%20initial%0AGaussian%20primitives.%20To%20mitigate%20noise%20from%20outlier%20primitives%2C%20we%20develop%20the%0ADecoupled%20Gaussian%20Aggregator%20%28DGA%29%2C%20which%20enhances%20robustness%20by%20decomposing%0Ageometric%20and%20semantic%20predictions%20during%20the%20Gaussian-to-voxel%20splatting%0Aprocess.%20Complemented%20with%20a%20specialized%20Probability%20Scale%20Loss%2C%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20the%20Occ-ScanNet%20dataset%2C%20outperforming%0Aprior%20approaches%20by%20over%206.3%25%20in%20IoU%20and%204.1%25%20in%20mIoU%2C%20while%20reducing%20both%0Alatency%20and%20memory%20consumption%20by%20more%20than%209.3%25.%20The%20code%20will%20be%20released%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02261v1&entry.124074799=Read"},
{"title": "CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via\n  Instruction Editing Data and Long Captions", "author": "Ziteng Wang and Siqi Yang and Limeng Qiao and Lin Ma", "abstract": "  Despite the success of Vision-Language Models (VLMs) like CLIP in aligning\nvision and language, their proficiency in detailed, fine-grained visual\ncomprehension remains a key challenge. We present CLIP-IN, a novel framework\nthat bolsters CLIP's fine-grained perception through two core innovations.\nFirstly, we leverage instruction-editing datasets, originally designed for\nimage manipulation, as a unique source of hard negative image-text pairs.\nCoupled with a symmetric hard negative contrastive loss, this enables the model\nto effectively distinguish subtle visual-semantic differences. Secondly,\nCLIP-IN incorporates long descriptive captions, utilizing rotary positional\nencodings to capture rich semantic context often missed by standard CLIP. Our\nexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVP\nbenchmark and various fine-grained visual recognition tasks, without\ncompromising robust zero-shot performance on broader classification and\nretrieval tasks. Critically, integrating CLIP-IN's visual representations into\nMultimodal Large Language Models significantly reduces visual hallucinations\nand enhances reasoning abilities. This work underscores the considerable\npotential of synergizing targeted, instruction-based contrastive learning with\ncomprehensive descriptive information to elevate the fine-grained understanding\nof VLMs.\n", "link": "http://arxiv.org/abs/2508.02329v1", "date": "2025-08-04", "relevancy": 3.1223, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6277}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-IN%3A%20Enhancing%20Fine-Grained%20Visual%20Understanding%20in%20CLIP%20via%0A%20%20Instruction%20Editing%20Data%20and%20Long%20Captions&body=Title%3A%20CLIP-IN%3A%20Enhancing%20Fine-Grained%20Visual%20Understanding%20in%20CLIP%20via%0A%20%20Instruction%20Editing%20Data%20and%20Long%20Captions%0AAuthor%3A%20Ziteng%20Wang%20and%20Siqi%20Yang%20and%20Limeng%20Qiao%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20in%20aligning%0Avision%20and%20language%2C%20their%20proficiency%20in%20detailed%2C%20fine-grained%20visual%0Acomprehension%20remains%20a%20key%20challenge.%20We%20present%20CLIP-IN%2C%20a%20novel%20framework%0Athat%20bolsters%20CLIP%27s%20fine-grained%20perception%20through%20two%20core%20innovations.%0AFirstly%2C%20we%20leverage%20instruction-editing%20datasets%2C%20originally%20designed%20for%0Aimage%20manipulation%2C%20as%20a%20unique%20source%20of%20hard%20negative%20image-text%20pairs.%0ACoupled%20with%20a%20symmetric%20hard%20negative%20contrastive%20loss%2C%20this%20enables%20the%20model%0Ato%20effectively%20distinguish%20subtle%20visual-semantic%20differences.%20Secondly%2C%0ACLIP-IN%20incorporates%20long%20descriptive%20captions%2C%20utilizing%20rotary%20positional%0Aencodings%20to%20capture%20rich%20semantic%20context%20often%20missed%20by%20standard%20CLIP.%20Our%0Aexperiments%20demonstrate%20that%20CLIP-IN%20achieves%20substantial%20gains%20on%20the%20MMVP%0Abenchmark%20and%20various%20fine-grained%20visual%20recognition%20tasks%2C%20without%0Acompromising%20robust%20zero-shot%20performance%20on%20broader%20classification%20and%0Aretrieval%20tasks.%20Critically%2C%20integrating%20CLIP-IN%27s%20visual%20representations%20into%0AMultimodal%20Large%20Language%20Models%20significantly%20reduces%20visual%20hallucinations%0Aand%20enhances%20reasoning%20abilities.%20This%20work%20underscores%20the%20considerable%0Apotential%20of%20synergizing%20targeted%2C%20instruction-based%20contrastive%20learning%20with%0Acomprehensive%20descriptive%20information%20to%20elevate%20the%20fine-grained%20understanding%0Aof%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-IN%253A%2520Enhancing%2520Fine-Grained%2520Visual%2520Understanding%2520in%2520CLIP%2520via%250A%2520%2520Instruction%2520Editing%2520Data%2520and%2520Long%2520Captions%26entry.906535625%3DZiteng%2520Wang%2520and%2520Siqi%2520Yang%2520and%2520Limeng%2520Qiao%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520in%2520aligning%250Avision%2520and%2520language%252C%2520their%2520proficiency%2520in%2520detailed%252C%2520fine-grained%2520visual%250Acomprehension%2520remains%2520a%2520key%2520challenge.%2520We%2520present%2520CLIP-IN%252C%2520a%2520novel%2520framework%250Athat%2520bolsters%2520CLIP%2527s%2520fine-grained%2520perception%2520through%2520two%2520core%2520innovations.%250AFirstly%252C%2520we%2520leverage%2520instruction-editing%2520datasets%252C%2520originally%2520designed%2520for%250Aimage%2520manipulation%252C%2520as%2520a%2520unique%2520source%2520of%2520hard%2520negative%2520image-text%2520pairs.%250ACoupled%2520with%2520a%2520symmetric%2520hard%2520negative%2520contrastive%2520loss%252C%2520this%2520enables%2520the%2520model%250Ato%2520effectively%2520distinguish%2520subtle%2520visual-semantic%2520differences.%2520Secondly%252C%250ACLIP-IN%2520incorporates%2520long%2520descriptive%2520captions%252C%2520utilizing%2520rotary%2520positional%250Aencodings%2520to%2520capture%2520rich%2520semantic%2520context%2520often%2520missed%2520by%2520standard%2520CLIP.%2520Our%250Aexperiments%2520demonstrate%2520that%2520CLIP-IN%2520achieves%2520substantial%2520gains%2520on%2520the%2520MMVP%250Abenchmark%2520and%2520various%2520fine-grained%2520visual%2520recognition%2520tasks%252C%2520without%250Acompromising%2520robust%2520zero-shot%2520performance%2520on%2520broader%2520classification%2520and%250Aretrieval%2520tasks.%2520Critically%252C%2520integrating%2520CLIP-IN%2527s%2520visual%2520representations%2520into%250AMultimodal%2520Large%2520Language%2520Models%2520significantly%2520reduces%2520visual%2520hallucinations%250Aand%2520enhances%2520reasoning%2520abilities.%2520This%2520work%2520underscores%2520the%2520considerable%250Apotential%2520of%2520synergizing%2520targeted%252C%2520instruction-based%2520contrastive%2520learning%2520with%250Acomprehensive%2520descriptive%2520information%2520to%2520elevate%2520the%2520fine-grained%2520understanding%250Aof%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-IN%3A%20Enhancing%20Fine-Grained%20Visual%20Understanding%20in%20CLIP%20via%0A%20%20Instruction%20Editing%20Data%20and%20Long%20Captions&entry.906535625=Ziteng%20Wang%20and%20Siqi%20Yang%20and%20Limeng%20Qiao%20and%20Lin%20Ma&entry.1292438233=%20%20Despite%20the%20success%20of%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20in%20aligning%0Avision%20and%20language%2C%20their%20proficiency%20in%20detailed%2C%20fine-grained%20visual%0Acomprehension%20remains%20a%20key%20challenge.%20We%20present%20CLIP-IN%2C%20a%20novel%20framework%0Athat%20bolsters%20CLIP%27s%20fine-grained%20perception%20through%20two%20core%20innovations.%0AFirstly%2C%20we%20leverage%20instruction-editing%20datasets%2C%20originally%20designed%20for%0Aimage%20manipulation%2C%20as%20a%20unique%20source%20of%20hard%20negative%20image-text%20pairs.%0ACoupled%20with%20a%20symmetric%20hard%20negative%20contrastive%20loss%2C%20this%20enables%20the%20model%0Ato%20effectively%20distinguish%20subtle%20visual-semantic%20differences.%20Secondly%2C%0ACLIP-IN%20incorporates%20long%20descriptive%20captions%2C%20utilizing%20rotary%20positional%0Aencodings%20to%20capture%20rich%20semantic%20context%20often%20missed%20by%20standard%20CLIP.%20Our%0Aexperiments%20demonstrate%20that%20CLIP-IN%20achieves%20substantial%20gains%20on%20the%20MMVP%0Abenchmark%20and%20various%20fine-grained%20visual%20recognition%20tasks%2C%20without%0Acompromising%20robust%20zero-shot%20performance%20on%20broader%20classification%20and%0Aretrieval%20tasks.%20Critically%2C%20integrating%20CLIP-IN%27s%20visual%20representations%20into%0AMultimodal%20Large%20Language%20Models%20significantly%20reduces%20visual%20hallucinations%0Aand%20enhances%20reasoning%20abilities.%20This%20work%20underscores%20the%20considerable%0Apotential%20of%20synergizing%20targeted%2C%20instruction-based%20contrastive%20learning%20with%0Acomprehensive%20descriptive%20information%20to%20elevate%20the%20fine-grained%20understanding%0Aof%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02329v1&entry.124074799=Read"},
{"title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from\n  Primitive-Based Representations of Error and Visibility", "author": "Thomas Gottwald and Edgar Heinert and Matthias Rottmann", "abstract": "  In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.\n", "link": "http://arxiv.org/abs/2508.02443v1", "date": "2025-08-04", "relevancy": 3.0659, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6181}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.61}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%0A%20%20Primitive-Based%20Representations%20of%20Error%20and%20Visibility&body=Title%3A%20Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%0A%20%20Primitive-Based%20Representations%20of%20Error%20and%20Visibility%0AAuthor%3A%20Thomas%20Gottwald%20and%20Edgar%20Heinert%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%20uncertainty%20estimation%20%28UE%29%20in%0AGaussian%20Splatting.%20UE%20is%20crucial%20for%20using%20Gaussian%20Splatting%20in%20critical%0Aapplications%20such%20as%20robotics%20and%20medicine.%20Previous%20methods%20typically%20estimate%0Athe%20variance%20of%20Gaussian%20primitives%20and%20use%20the%20rendering%20process%20to%20obtain%0Apixel-wise%20uncertainties.%20Our%20method%20establishes%20primitive%20representations%20of%0Aerror%20and%20visibility%20of%20trainings%20views%2C%20which%20carries%20meaningful%20uncertainty%0Ainformation.%20This%20representation%20is%20obtained%20by%20projection%20of%20training%20error%0Aand%20visibility%20onto%20the%20primitives.%20Uncertainties%20of%20novel%20views%20are%20obtained%0Aby%20rendering%20the%20primitive%20representations%20of%20uncertainty%20for%20those%20novel%0Aviews%2C%20yielding%20uncertainty%20feature%20maps.%20To%20aggregate%20these%20uncertainty%0Afeature%20maps%20of%20novel%20views%2C%20we%20perform%20a%20pixel-wise%20regression%20on%20holdout%0Adata.%20In%20our%20experiments%2C%20we%20analyze%20the%20different%20components%20of%20our%20method%2C%0Ainvestigating%20various%20combinations%20of%20uncertainty%20feature%20maps%20and%20regression%0Amodels.%20Furthermore%2C%20we%20considered%20the%20effect%20of%20separating%20splatting%20into%0Aforeground%20and%20background.%20Our%20UEs%20show%20high%20correlations%20to%20true%20errors%2C%0Aoutperforming%20state-of-the-art%20methods%2C%20especially%20on%20foreground%20objects.%20The%0Atrained%20regression%20models%20show%20generalization%20capabilities%20to%20new%20scenes%2C%0Aallowing%20uncertainty%20estimation%20without%20the%20need%20for%20holdout%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Estimation%2520for%2520Novel%2520Views%2520in%2520Gaussian%2520Splatting%2520from%250A%2520%2520Primitive-Based%2520Representations%2520of%2520Error%2520and%2520Visibility%26entry.906535625%3DThomas%2520Gottwald%2520and%2520Edgar%2520Heinert%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520method%2520for%2520uncertainty%2520estimation%2520%2528UE%2529%2520in%250AGaussian%2520Splatting.%2520UE%2520is%2520crucial%2520for%2520using%2520Gaussian%2520Splatting%2520in%2520critical%250Aapplications%2520such%2520as%2520robotics%2520and%2520medicine.%2520Previous%2520methods%2520typically%2520estimate%250Athe%2520variance%2520of%2520Gaussian%2520primitives%2520and%2520use%2520the%2520rendering%2520process%2520to%2520obtain%250Apixel-wise%2520uncertainties.%2520Our%2520method%2520establishes%2520primitive%2520representations%2520of%250Aerror%2520and%2520visibility%2520of%2520trainings%2520views%252C%2520which%2520carries%2520meaningful%2520uncertainty%250Ainformation.%2520This%2520representation%2520is%2520obtained%2520by%2520projection%2520of%2520training%2520error%250Aand%2520visibility%2520onto%2520the%2520primitives.%2520Uncertainties%2520of%2520novel%2520views%2520are%2520obtained%250Aby%2520rendering%2520the%2520primitive%2520representations%2520of%2520uncertainty%2520for%2520those%2520novel%250Aviews%252C%2520yielding%2520uncertainty%2520feature%2520maps.%2520To%2520aggregate%2520these%2520uncertainty%250Afeature%2520maps%2520of%2520novel%2520views%252C%2520we%2520perform%2520a%2520pixel-wise%2520regression%2520on%2520holdout%250Adata.%2520In%2520our%2520experiments%252C%2520we%2520analyze%2520the%2520different%2520components%2520of%2520our%2520method%252C%250Ainvestigating%2520various%2520combinations%2520of%2520uncertainty%2520feature%2520maps%2520and%2520regression%250Amodels.%2520Furthermore%252C%2520we%2520considered%2520the%2520effect%2520of%2520separating%2520splatting%2520into%250Aforeground%2520and%2520background.%2520Our%2520UEs%2520show%2520high%2520correlations%2520to%2520true%2520errors%252C%250Aoutperforming%2520state-of-the-art%2520methods%252C%2520especially%2520on%2520foreground%2520objects.%2520The%250Atrained%2520regression%2520models%2520show%2520generalization%2520capabilities%2520to%2520new%2520scenes%252C%250Aallowing%2520uncertainty%2520estimation%2520without%2520the%2520need%2520for%2520holdout%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%0A%20%20Primitive-Based%20Representations%20of%20Error%20and%20Visibility&entry.906535625=Thomas%20Gottwald%20and%20Edgar%20Heinert%20and%20Matthias%20Rottmann&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%20uncertainty%20estimation%20%28UE%29%20in%0AGaussian%20Splatting.%20UE%20is%20crucial%20for%20using%20Gaussian%20Splatting%20in%20critical%0Aapplications%20such%20as%20robotics%20and%20medicine.%20Previous%20methods%20typically%20estimate%0Athe%20variance%20of%20Gaussian%20primitives%20and%20use%20the%20rendering%20process%20to%20obtain%0Apixel-wise%20uncertainties.%20Our%20method%20establishes%20primitive%20representations%20of%0Aerror%20and%20visibility%20of%20trainings%20views%2C%20which%20carries%20meaningful%20uncertainty%0Ainformation.%20This%20representation%20is%20obtained%20by%20projection%20of%20training%20error%0Aand%20visibility%20onto%20the%20primitives.%20Uncertainties%20of%20novel%20views%20are%20obtained%0Aby%20rendering%20the%20primitive%20representations%20of%20uncertainty%20for%20those%20novel%0Aviews%2C%20yielding%20uncertainty%20feature%20maps.%20To%20aggregate%20these%20uncertainty%0Afeature%20maps%20of%20novel%20views%2C%20we%20perform%20a%20pixel-wise%20regression%20on%20holdout%0Adata.%20In%20our%20experiments%2C%20we%20analyze%20the%20different%20components%20of%20our%20method%2C%0Ainvestigating%20various%20combinations%20of%20uncertainty%20feature%20maps%20and%20regression%0Amodels.%20Furthermore%2C%20we%20considered%20the%20effect%20of%20separating%20splatting%20into%0Aforeground%20and%20background.%20Our%20UEs%20show%20high%20correlations%20to%20true%20errors%2C%0Aoutperforming%20state-of-the-art%20methods%2C%20especially%20on%20foreground%20objects.%20The%0Atrained%20regression%20models%20show%20generalization%20capabilities%20to%20new%20scenes%2C%0Aallowing%20uncertainty%20estimation%20without%20the%20need%20for%20holdout%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02443v1&entry.124074799=Read"},
{"title": "Is It Really You? Exploring Biometric Verification Scenarios in\n  Photorealistic Talking-Head Avatar Videos", "author": "Laura Pedrouzo-Rodriguez and Pedro Delgado-DeRobles and Luis F. Gomez and Ruben Tolosana and Ruben Vera-Rodriguez and Aythami Morales and Julian Fierrez", "abstract": "  Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's avatar,\npreserving his appearance and voice, making it nearly impossible to detect its\nfraudulent usage by sight or sound alone. In this paper, we explore the\nchallenge of biometric verification in such avatar-mediated scenarios. Our main\nquestion is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.\n", "link": "http://arxiv.org/abs/2508.00748v2", "date": "2025-08-04", "relevancy": 3.0354, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6229}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6229}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos&body=Title%3A%20Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos%0AAuthor%3A%20Laura%20Pedrouzo-Rodriguez%20and%20Pedro%20Delgado-DeRobles%20and%20Luis%20F.%20Gomez%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%0AAbstract%3A%20%20%20Photorealistic%20talking-head%20avatars%20are%20becoming%20increasingly%20common%20in%0Avirtual%20meetings%2C%20gaming%2C%20and%20social%20platforms.%20These%20avatars%20allow%20for%20more%0Aimmersive%20communication%2C%20but%20they%20also%20introduce%20serious%20security%20risks.%20One%0Aemerging%20threat%20is%20impersonation%3A%20an%20attacker%20can%20steal%20a%20user%27s%20avatar%2C%0Apreserving%20his%20appearance%20and%20voice%2C%20making%20it%20nearly%20impossible%20to%20detect%20its%0Afraudulent%20usage%20by%20sight%20or%20sound%20alone.%20In%20this%20paper%2C%20we%20explore%20the%0Achallenge%20of%20biometric%20verification%20in%20such%20avatar-mediated%20scenarios.%20Our%20main%0Aquestion%20is%20whether%20an%20individual%27s%20facial%20motion%20patterns%20can%20serve%20as%0Areliable%20behavioral%20biometrics%20to%20verify%20their%20identity%20when%20the%20avatar%27s%0Avisual%20appearance%20is%20a%20facsimile%20of%20its%20owner.%20To%20answer%20this%20question%2C%20we%0Aintroduce%20a%20new%20dataset%20of%20realistic%20avatar%20videos%20created%20using%20a%0Astate-of-the-art%20one-shot%20avatar%20generation%20model%2C%20GAGAvatar%2C%20with%20genuine%20and%0Aimpostor%20avatar%20videos.%20We%20also%20propose%20a%20lightweight%2C%20explainable%0Aspatio-temporal%20Graph%20Convolutional%20Network%20architecture%20with%20temporal%0Aattention%20pooling%2C%20that%20uses%20only%20facial%20landmarks%20to%20model%20dynamic%20facial%0Agestures.%20Experimental%20results%20demonstrate%20that%20facial%20motion%20cues%20enable%0Ameaningful%20identity%20verification%20with%20AUC%20values%20approaching%2080%25.%20The%20proposed%0Abenchmark%20and%20biometric%20system%20are%20available%20for%20the%20research%20community%20in%0Aorder%20to%20bring%20attention%20to%20the%20urgent%20need%20for%20more%20advanced%20behavioral%0Abiometric%20defenses%20in%20avatar-based%20communication%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520It%2520Really%2520You%253F%2520Exploring%2520Biometric%2520Verification%2520Scenarios%2520in%250A%2520%2520Photorealistic%2520Talking-Head%2520Avatar%2520Videos%26entry.906535625%3DLaura%2520Pedrouzo-Rodriguez%2520and%2520Pedro%2520Delgado-DeRobles%2520and%2520Luis%2520F.%2520Gomez%2520and%2520Ruben%2520Tolosana%2520and%2520Ruben%2520Vera-Rodriguez%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%26entry.1292438233%3D%2520%2520Photorealistic%2520talking-head%2520avatars%2520are%2520becoming%2520increasingly%2520common%2520in%250Avirtual%2520meetings%252C%2520gaming%252C%2520and%2520social%2520platforms.%2520These%2520avatars%2520allow%2520for%2520more%250Aimmersive%2520communication%252C%2520but%2520they%2520also%2520introduce%2520serious%2520security%2520risks.%2520One%250Aemerging%2520threat%2520is%2520impersonation%253A%2520an%2520attacker%2520can%2520steal%2520a%2520user%2527s%2520avatar%252C%250Apreserving%2520his%2520appearance%2520and%2520voice%252C%2520making%2520it%2520nearly%2520impossible%2520to%2520detect%2520its%250Afraudulent%2520usage%2520by%2520sight%2520or%2520sound%2520alone.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%250Achallenge%2520of%2520biometric%2520verification%2520in%2520such%2520avatar-mediated%2520scenarios.%2520Our%2520main%250Aquestion%2520is%2520whether%2520an%2520individual%2527s%2520facial%2520motion%2520patterns%2520can%2520serve%2520as%250Areliable%2520behavioral%2520biometrics%2520to%2520verify%2520their%2520identity%2520when%2520the%2520avatar%2527s%250Avisual%2520appearance%2520is%2520a%2520facsimile%2520of%2520its%2520owner.%2520To%2520answer%2520this%2520question%252C%2520we%250Aintroduce%2520a%2520new%2520dataset%2520of%2520realistic%2520avatar%2520videos%2520created%2520using%2520a%250Astate-of-the-art%2520one-shot%2520avatar%2520generation%2520model%252C%2520GAGAvatar%252C%2520with%2520genuine%2520and%250Aimpostor%2520avatar%2520videos.%2520We%2520also%2520propose%2520a%2520lightweight%252C%2520explainable%250Aspatio-temporal%2520Graph%2520Convolutional%2520Network%2520architecture%2520with%2520temporal%250Aattention%2520pooling%252C%2520that%2520uses%2520only%2520facial%2520landmarks%2520to%2520model%2520dynamic%2520facial%250Agestures.%2520Experimental%2520results%2520demonstrate%2520that%2520facial%2520motion%2520cues%2520enable%250Ameaningful%2520identity%2520verification%2520with%2520AUC%2520values%2520approaching%252080%2525.%2520The%2520proposed%250Abenchmark%2520and%2520biometric%2520system%2520are%2520available%2520for%2520the%2520research%2520community%2520in%250Aorder%2520to%2520bring%2520attention%2520to%2520the%2520urgent%2520need%2520for%2520more%2520advanced%2520behavioral%250Abiometric%2520defenses%2520in%2520avatar-based%2520communication%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos&entry.906535625=Laura%20Pedrouzo-Rodriguez%20and%20Pedro%20Delgado-DeRobles%20and%20Luis%20F.%20Gomez%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez&entry.1292438233=%20%20Photorealistic%20talking-head%20avatars%20are%20becoming%20increasingly%20common%20in%0Avirtual%20meetings%2C%20gaming%2C%20and%20social%20platforms.%20These%20avatars%20allow%20for%20more%0Aimmersive%20communication%2C%20but%20they%20also%20introduce%20serious%20security%20risks.%20One%0Aemerging%20threat%20is%20impersonation%3A%20an%20attacker%20can%20steal%20a%20user%27s%20avatar%2C%0Apreserving%20his%20appearance%20and%20voice%2C%20making%20it%20nearly%20impossible%20to%20detect%20its%0Afraudulent%20usage%20by%20sight%20or%20sound%20alone.%20In%20this%20paper%2C%20we%20explore%20the%0Achallenge%20of%20biometric%20verification%20in%20such%20avatar-mediated%20scenarios.%20Our%20main%0Aquestion%20is%20whether%20an%20individual%27s%20facial%20motion%20patterns%20can%20serve%20as%0Areliable%20behavioral%20biometrics%20to%20verify%20their%20identity%20when%20the%20avatar%27s%0Avisual%20appearance%20is%20a%20facsimile%20of%20its%20owner.%20To%20answer%20this%20question%2C%20we%0Aintroduce%20a%20new%20dataset%20of%20realistic%20avatar%20videos%20created%20using%20a%0Astate-of-the-art%20one-shot%20avatar%20generation%20model%2C%20GAGAvatar%2C%20with%20genuine%20and%0Aimpostor%20avatar%20videos.%20We%20also%20propose%20a%20lightweight%2C%20explainable%0Aspatio-temporal%20Graph%20Convolutional%20Network%20architecture%20with%20temporal%0Aattention%20pooling%2C%20that%20uses%20only%20facial%20landmarks%20to%20model%20dynamic%20facial%0Agestures.%20Experimental%20results%20demonstrate%20that%20facial%20motion%20cues%20enable%0Ameaningful%20identity%20verification%20with%20AUC%20values%20approaching%2080%25.%20The%20proposed%0Abenchmark%20and%20biometric%20system%20are%20available%20for%20the%20research%20community%20in%0Aorder%20to%20bring%20attention%20to%20the%20urgent%20need%20for%20more%20advanced%20behavioral%0Abiometric%20defenses%20in%20avatar-based%20communication%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00748v2&entry.124074799=Read"},
{"title": "GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects", "author": "Yidi Shao and Mu Huang and Chen Change Loy and Bo Dai", "abstract": "  We introduce GausSim, a novel neural network-based simulator designed to\ncapture the dynamic behaviors of real-world elastic objects represented through\nGaussian kernels. We leverage continuum mechanics and treat each kernel as a\nCenter of Mass System (CMS) that represents continuous piece of matter,\naccounting for realistic deformations without idealized assumptions. To improve\ncomputational efficiency and fidelity, we employ a hierarchical structure that\nfurther organizes kernels into CMSs with explicit formulations, enabling a\ncoarse-to-fine simulation approach. This structure significantly reduces\ncomputational overhead while preserving detailed dynamics. In addition, GausSim\nincorporates explicit physics constraints, such as mass and momentum\nconservation, ensuring interpretable results and robust, physically plausible\nsimulations. To validate our approach, we present a new dataset, READY,\ncontaining multi-view videos of real-world elastic deformations. Experimental\nresults demonstrate that GausSim achieves superior performance compared to\nexisting physics-driven baselines, offering a practical and accurate solution\nfor simulating complex dynamic behaviors. Code and model are available at our\nproject page: https://www.mmlab-ntu.com/project/gausim/index.html .\n", "link": "http://arxiv.org/abs/2412.17804v3", "date": "2025-08-04", "relevancy": 2.9926, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6207}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.593}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GausSim%3A%20Foreseeing%20Reality%20by%20Gaussian%20Simulator%20for%20Elastic%20Objects&body=Title%3A%20GausSim%3A%20Foreseeing%20Reality%20by%20Gaussian%20Simulator%20for%20Elastic%20Objects%0AAuthor%3A%20Yidi%20Shao%20and%20Mu%20Huang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai%0AAbstract%3A%20%20%20We%20introduce%20GausSim%2C%20a%20novel%20neural%20network-based%20simulator%20designed%20to%0Acapture%20the%20dynamic%20behaviors%20of%20real-world%20elastic%20objects%20represented%20through%0AGaussian%20kernels.%20We%20leverage%20continuum%20mechanics%20and%20treat%20each%20kernel%20as%20a%0ACenter%20of%20Mass%20System%20%28CMS%29%20that%20represents%20continuous%20piece%20of%20matter%2C%0Aaccounting%20for%20realistic%20deformations%20without%20idealized%20assumptions.%20To%20improve%0Acomputational%20efficiency%20and%20fidelity%2C%20we%20employ%20a%20hierarchical%20structure%20that%0Afurther%20organizes%20kernels%20into%20CMSs%20with%20explicit%20formulations%2C%20enabling%20a%0Acoarse-to-fine%20simulation%20approach.%20This%20structure%20significantly%20reduces%0Acomputational%20overhead%20while%20preserving%20detailed%20dynamics.%20In%20addition%2C%20GausSim%0Aincorporates%20explicit%20physics%20constraints%2C%20such%20as%20mass%20and%20momentum%0Aconservation%2C%20ensuring%20interpretable%20results%20and%20robust%2C%20physically%20plausible%0Asimulations.%20To%20validate%20our%20approach%2C%20we%20present%20a%20new%20dataset%2C%20READY%2C%0Acontaining%20multi-view%20videos%20of%20real-world%20elastic%20deformations.%20Experimental%0Aresults%20demonstrate%20that%20GausSim%20achieves%20superior%20performance%20compared%20to%0Aexisting%20physics-driven%20baselines%2C%20offering%20a%20practical%20and%20accurate%20solution%0Afor%20simulating%20complex%20dynamic%20behaviors.%20Code%20and%20model%20are%20available%20at%20our%0Aproject%20page%3A%20https%3A//www.mmlab-ntu.com/project/gausim/index.html%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGausSim%253A%2520Foreseeing%2520Reality%2520by%2520Gaussian%2520Simulator%2520for%2520Elastic%2520Objects%26entry.906535625%3DYidi%2520Shao%2520and%2520Mu%2520Huang%2520and%2520Chen%2520Change%2520Loy%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520We%2520introduce%2520GausSim%252C%2520a%2520novel%2520neural%2520network-based%2520simulator%2520designed%2520to%250Acapture%2520the%2520dynamic%2520behaviors%2520of%2520real-world%2520elastic%2520objects%2520represented%2520through%250AGaussian%2520kernels.%2520We%2520leverage%2520continuum%2520mechanics%2520and%2520treat%2520each%2520kernel%2520as%2520a%250ACenter%2520of%2520Mass%2520System%2520%2528CMS%2529%2520that%2520represents%2520continuous%2520piece%2520of%2520matter%252C%250Aaccounting%2520for%2520realistic%2520deformations%2520without%2520idealized%2520assumptions.%2520To%2520improve%250Acomputational%2520efficiency%2520and%2520fidelity%252C%2520we%2520employ%2520a%2520hierarchical%2520structure%2520that%250Afurther%2520organizes%2520kernels%2520into%2520CMSs%2520with%2520explicit%2520formulations%252C%2520enabling%2520a%250Acoarse-to-fine%2520simulation%2520approach.%2520This%2520structure%2520significantly%2520reduces%250Acomputational%2520overhead%2520while%2520preserving%2520detailed%2520dynamics.%2520In%2520addition%252C%2520GausSim%250Aincorporates%2520explicit%2520physics%2520constraints%252C%2520such%2520as%2520mass%2520and%2520momentum%250Aconservation%252C%2520ensuring%2520interpretable%2520results%2520and%2520robust%252C%2520physically%2520plausible%250Asimulations.%2520To%2520validate%2520our%2520approach%252C%2520we%2520present%2520a%2520new%2520dataset%252C%2520READY%252C%250Acontaining%2520multi-view%2520videos%2520of%2520real-world%2520elastic%2520deformations.%2520Experimental%250Aresults%2520demonstrate%2520that%2520GausSim%2520achieves%2520superior%2520performance%2520compared%2520to%250Aexisting%2520physics-driven%2520baselines%252C%2520offering%2520a%2520practical%2520and%2520accurate%2520solution%250Afor%2520simulating%2520complex%2520dynamic%2520behaviors.%2520Code%2520and%2520model%2520are%2520available%2520at%2520our%250Aproject%2520page%253A%2520https%253A//www.mmlab-ntu.com/project/gausim/index.html%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GausSim%3A%20Foreseeing%20Reality%20by%20Gaussian%20Simulator%20for%20Elastic%20Objects&entry.906535625=Yidi%20Shao%20and%20Mu%20Huang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai&entry.1292438233=%20%20We%20introduce%20GausSim%2C%20a%20novel%20neural%20network-based%20simulator%20designed%20to%0Acapture%20the%20dynamic%20behaviors%20of%20real-world%20elastic%20objects%20represented%20through%0AGaussian%20kernels.%20We%20leverage%20continuum%20mechanics%20and%20treat%20each%20kernel%20as%20a%0ACenter%20of%20Mass%20System%20%28CMS%29%20that%20represents%20continuous%20piece%20of%20matter%2C%0Aaccounting%20for%20realistic%20deformations%20without%20idealized%20assumptions.%20To%20improve%0Acomputational%20efficiency%20and%20fidelity%2C%20we%20employ%20a%20hierarchical%20structure%20that%0Afurther%20organizes%20kernels%20into%20CMSs%20with%20explicit%20formulations%2C%20enabling%20a%0Acoarse-to-fine%20simulation%20approach.%20This%20structure%20significantly%20reduces%0Acomputational%20overhead%20while%20preserving%20detailed%20dynamics.%20In%20addition%2C%20GausSim%0Aincorporates%20explicit%20physics%20constraints%2C%20such%20as%20mass%20and%20momentum%0Aconservation%2C%20ensuring%20interpretable%20results%20and%20robust%2C%20physically%20plausible%0Asimulations.%20To%20validate%20our%20approach%2C%20we%20present%20a%20new%20dataset%2C%20READY%2C%0Acontaining%20multi-view%20videos%20of%20real-world%20elastic%20deformations.%20Experimental%0Aresults%20demonstrate%20that%20GausSim%20achieves%20superior%20performance%20compared%20to%0Aexisting%20physics-driven%20baselines%2C%20offering%20a%20practical%20and%20accurate%20solution%0Afor%20simulating%20complex%20dynamic%20behaviors.%20Code%20and%20model%20are%20available%20at%20our%0Aproject%20page%3A%20https%3A//www.mmlab-ntu.com/project/gausim/index.html%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17804v3&entry.124074799=Read"},
{"title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with\n  Vision Foundation Models", "author": "Yonghuang Wu and Wenwen Zeng and Xuan Xie and Chengqian Zhao and Guoqing Wu and Jinhua Yu", "abstract": "  Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.\n", "link": "http://arxiv.org/abs/2508.02464v1", "date": "2025-08-04", "relevancy": 2.9783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMPO%3A%20Visual%20Preference%20Optimization%20for%20Intent-Aware%20Segmentation%20with%0A%20%20Vision%20Foundation%20Models&body=Title%3A%20SAMPO%3A%20Visual%20Preference%20Optimization%20for%20Intent-Aware%20Segmentation%20with%0A%20%20Vision%20Foundation%20Models%0AAuthor%3A%20Yonghuang%20Wu%20and%20Wenwen%20Zeng%20and%20Xuan%20Xie%20and%20Chengqian%20Zhao%20and%20Guoqing%20Wu%20and%20Jinhua%20Yu%0AAbstract%3A%20%20%20Foundation%20models%20like%20Segment%20Anything%20Model%20%28SAM%29%20excel%20in%20promptable%0Asegmentation%20but%20suffer%20from%20an%20intent%20gap%3A%20they%20segment%20only%20explicitly%0Aprompted%20objects%2C%20failing%20to%20generalize%20to%20semantically%20related%20instances%0Aimplicitly%20desired%20by%20users.%20This%20limitation%20is%20critical%20in%20domains%20with%20dense%0Ahomogeneous%20objects%20%28e.g.%2C%20biomedical%20nuclei%20segmentation%29%2C%20where%20sparse%20visual%0Aprompts%20typically%20yield%20incomplete%20results%2C%20rendering%20dense%20annotations%0Aimpractical%20due%20to%20prohibitive%20cost.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SAMPO%0A%28Segment%20Anything%20Model%20with%20Preference%20Optimization%29%2C%20a%20novel%20framework%20that%0Ateaches%20visual%20foundation%20models%20to%20infer%20high-level%20categorical%20intent%20from%0Asparse%20visual%20interactions.%20Unlike%20conventional%20pixel-level%20fine-tuning%2C%20SAMPO%0Aoptimizes%20models%20to%20implicitly%20capture%20target-class%20characteristics%20through%0Apreference%20optimization.%20This%20approach%2C%20which%20operates%20without%20dependency%20on%0Alanguage%20models%2C%20enables%20robust%20multi-object%20segmentation%20even%20under%20sparse%0Aprompting%20and%20demonstrates%20superior%20data%20efficiency%20during%20fine-tuning.%0AValidated%20on%20three%20medical%20segmentation%20tasks%2C%20SAMPO%20achieves%20state-of-the-art%0Aperformance%3A%20on%20challenging%20tasks%20like%20PanNuke-T2%2C%20our%20method%2C%20when%20fine-tuned%0Awith%20only%2010%25%20of%20the%20training%20data%2C%20significantly%20outperforms%20all%20existing%0Amethods%20trained%20on%20the%20full%20100%25%20dataset%2C%20achieving%20an%20improvement%20of%20over%209%0Apercentage%20points%20compared%20to%20the%20best%20baseline.%20Our%20work%20establishes%20a%20new%0Aparadigm%20for%20intent-aware%20alignment%20in%20visual%20foundation%20models%2C%20removing%0Adependencies%20on%20auxiliary%20prompt%20generators%20or%20language-model-assisted%0Apreference%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMPO%253A%2520Visual%2520Preference%2520Optimization%2520for%2520Intent-Aware%2520Segmentation%2520with%250A%2520%2520Vision%2520Foundation%2520Models%26entry.906535625%3DYonghuang%2520Wu%2520and%2520Wenwen%2520Zeng%2520and%2520Xuan%2520Xie%2520and%2520Chengqian%2520Zhao%2520and%2520Guoqing%2520Wu%2520and%2520Jinhua%2520Yu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520like%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520excel%2520in%2520promptable%250Asegmentation%2520but%2520suffer%2520from%2520an%2520intent%2520gap%253A%2520they%2520segment%2520only%2520explicitly%250Aprompted%2520objects%252C%2520failing%2520to%2520generalize%2520to%2520semantically%2520related%2520instances%250Aimplicitly%2520desired%2520by%2520users.%2520This%2520limitation%2520is%2520critical%2520in%2520domains%2520with%2520dense%250Ahomogeneous%2520objects%2520%2528e.g.%252C%2520biomedical%2520nuclei%2520segmentation%2529%252C%2520where%2520sparse%2520visual%250Aprompts%2520typically%2520yield%2520incomplete%2520results%252C%2520rendering%2520dense%2520annotations%250Aimpractical%2520due%2520to%2520prohibitive%2520cost.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520SAMPO%250A%2528Segment%2520Anything%2520Model%2520with%2520Preference%2520Optimization%2529%252C%2520a%2520novel%2520framework%2520that%250Ateaches%2520visual%2520foundation%2520models%2520to%2520infer%2520high-level%2520categorical%2520intent%2520from%250Asparse%2520visual%2520interactions.%2520Unlike%2520conventional%2520pixel-level%2520fine-tuning%252C%2520SAMPO%250Aoptimizes%2520models%2520to%2520implicitly%2520capture%2520target-class%2520characteristics%2520through%250Apreference%2520optimization.%2520This%2520approach%252C%2520which%2520operates%2520without%2520dependency%2520on%250Alanguage%2520models%252C%2520enables%2520robust%2520multi-object%2520segmentation%2520even%2520under%2520sparse%250Aprompting%2520and%2520demonstrates%2520superior%2520data%2520efficiency%2520during%2520fine-tuning.%250AValidated%2520on%2520three%2520medical%2520segmentation%2520tasks%252C%2520SAMPO%2520achieves%2520state-of-the-art%250Aperformance%253A%2520on%2520challenging%2520tasks%2520like%2520PanNuke-T2%252C%2520our%2520method%252C%2520when%2520fine-tuned%250Awith%2520only%252010%2525%2520of%2520the%2520training%2520data%252C%2520significantly%2520outperforms%2520all%2520existing%250Amethods%2520trained%2520on%2520the%2520full%2520100%2525%2520dataset%252C%2520achieving%2520an%2520improvement%2520of%2520over%25209%250Apercentage%2520points%2520compared%2520to%2520the%2520best%2520baseline.%2520Our%2520work%2520establishes%2520a%2520new%250Aparadigm%2520for%2520intent-aware%2520alignment%2520in%2520visual%2520foundation%2520models%252C%2520removing%250Adependencies%2520on%2520auxiliary%2520prompt%2520generators%2520or%2520language-model-assisted%250Apreference%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMPO%3A%20Visual%20Preference%20Optimization%20for%20Intent-Aware%20Segmentation%20with%0A%20%20Vision%20Foundation%20Models&entry.906535625=Yonghuang%20Wu%20and%20Wenwen%20Zeng%20and%20Xuan%20Xie%20and%20Chengqian%20Zhao%20and%20Guoqing%20Wu%20and%20Jinhua%20Yu&entry.1292438233=%20%20Foundation%20models%20like%20Segment%20Anything%20Model%20%28SAM%29%20excel%20in%20promptable%0Asegmentation%20but%20suffer%20from%20an%20intent%20gap%3A%20they%20segment%20only%20explicitly%0Aprompted%20objects%2C%20failing%20to%20generalize%20to%20semantically%20related%20instances%0Aimplicitly%20desired%20by%20users.%20This%20limitation%20is%20critical%20in%20domains%20with%20dense%0Ahomogeneous%20objects%20%28e.g.%2C%20biomedical%20nuclei%20segmentation%29%2C%20where%20sparse%20visual%0Aprompts%20typically%20yield%20incomplete%20results%2C%20rendering%20dense%20annotations%0Aimpractical%20due%20to%20prohibitive%20cost.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SAMPO%0A%28Segment%20Anything%20Model%20with%20Preference%20Optimization%29%2C%20a%20novel%20framework%20that%0Ateaches%20visual%20foundation%20models%20to%20infer%20high-level%20categorical%20intent%20from%0Asparse%20visual%20interactions.%20Unlike%20conventional%20pixel-level%20fine-tuning%2C%20SAMPO%0Aoptimizes%20models%20to%20implicitly%20capture%20target-class%20characteristics%20through%0Apreference%20optimization.%20This%20approach%2C%20which%20operates%20without%20dependency%20on%0Alanguage%20models%2C%20enables%20robust%20multi-object%20segmentation%20even%20under%20sparse%0Aprompting%20and%20demonstrates%20superior%20data%20efficiency%20during%20fine-tuning.%0AValidated%20on%20three%20medical%20segmentation%20tasks%2C%20SAMPO%20achieves%20state-of-the-art%0Aperformance%3A%20on%20challenging%20tasks%20like%20PanNuke-T2%2C%20our%20method%2C%20when%20fine-tuned%0Awith%20only%2010%25%20of%20the%20training%20data%2C%20significantly%20outperforms%20all%20existing%0Amethods%20trained%20on%20the%20full%20100%25%20dataset%2C%20achieving%20an%20improvement%20of%20over%209%0Apercentage%20points%20compared%20to%20the%20best%20baseline.%20Our%20work%20establishes%20a%20new%0Aparadigm%20for%20intent-aware%20alignment%20in%20visual%20foundation%20models%2C%20removing%0Adependencies%20on%20auxiliary%20prompt%20generators%20or%20language-model-assisted%0Apreference%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02464v1&entry.124074799=Read"},
{"title": "All-in-One Transferring Image Compression from Human Perception to\n  Multi-Machine Perception", "author": "Jiancheng Zhao and Xiang Ji and Yinqiang Zheng", "abstract": "  Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. In this paper, we propose a\nmulti-task adaptation framework that enables transferring a pre-trained base\ncodec to multiple machine vision tasks through a unified model and a single\ntraining process. To achieve this, we design an asymmetric adaptation\narchitecture consisting of a task-agnostic encoder adaptation and task-specific\ndecoder adaptation. Furthermore, we introduce two feature propagation modules\nto facilitate inter-task and inter-scale feature represenation learning.\nExperiments on PASCAL-Context and NYUD-V2 dataset demonstrate that our method\noutperforms both Fully Fine-Tuned and other Parameter Efficient Fine-Tuned\n(PEFT) baselines. Code will be released.\n", "link": "http://arxiv.org/abs/2504.12997v2", "date": "2025-08-04", "relevancy": 2.971, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.622}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&body=Title%3A%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception%0AAuthor%3A%20Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-task%20adaptation%20framework%20that%20enables%20transferring%20a%20pre-trained%20base%0Acodec%20to%20multiple%20machine%20vision%20tasks%20through%20a%20unified%20model%20and%20a%20single%0Atraining%20process.%20To%20achieve%20this%2C%20we%20design%20an%20asymmetric%20adaptation%0Aarchitecture%20consisting%20of%20a%20task-agnostic%20encoder%20adaptation%20and%20task-specific%0Adecoder%20adaptation.%20Furthermore%2C%20we%20introduce%20two%20feature%20propagation%20modules%0Ato%20facilitate%20inter-task%20and%20inter-scale%20feature%20represenation%20learning.%0AExperiments%20on%20PASCAL-Context%20and%20NYUD-V2%20dataset%20demonstrate%20that%20our%20method%0Aoutperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%20Fine-Tuned%0A%28PEFT%29%20baselines.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-in-One%2520Transferring%2520Image%2520Compression%2520from%2520Human%2520Perception%2520to%250A%2520%2520Multi-Machine%2520Perception%26entry.906535625%3DJiancheng%2520Zhao%2520and%2520Xiang%2520Ji%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Efficiently%2520transferring%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520model%2520from%2520human%250Aperception%2520to%2520machine%2520perception%2520is%2520an%2520emerging%2520challenge%2520in%2520vision-centric%250Arepresentation%2520learning.%2520Existing%2520approaches%2520typically%2520adapt%2520LIC%2520to%2520downstream%250Atasks%2520in%2520a%2520single-task%2520manner%252C%2520which%2520is%2520inefficient%252C%2520lacks%2520task%2520interaction%252C%250Aand%2520results%2520in%2520multiple%2520task-specific%2520bitstreams.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Amulti-task%2520adaptation%2520framework%2520that%2520enables%2520transferring%2520a%2520pre-trained%2520base%250Acodec%2520to%2520multiple%2520machine%2520vision%2520tasks%2520through%2520a%2520unified%2520model%2520and%2520a%2520single%250Atraining%2520process.%2520To%2520achieve%2520this%252C%2520we%2520design%2520an%2520asymmetric%2520adaptation%250Aarchitecture%2520consisting%2520of%2520a%2520task-agnostic%2520encoder%2520adaptation%2520and%2520task-specific%250Adecoder%2520adaptation.%2520Furthermore%252C%2520we%2520introduce%2520two%2520feature%2520propagation%2520modules%250Ato%2520facilitate%2520inter-task%2520and%2520inter-scale%2520feature%2520represenation%2520learning.%250AExperiments%2520on%2520PASCAL-Context%2520and%2520NYUD-V2%2520dataset%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520both%2520Fully%2520Fine-Tuned%2520and%2520other%2520Parameter%2520Efficient%2520Fine-Tuned%250A%2528PEFT%2529%2520baselines.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&entry.906535625=Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-task%20adaptation%20framework%20that%20enables%20transferring%20a%20pre-trained%20base%0Acodec%20to%20multiple%20machine%20vision%20tasks%20through%20a%20unified%20model%20and%20a%20single%0Atraining%20process.%20To%20achieve%20this%2C%20we%20design%20an%20asymmetric%20adaptation%0Aarchitecture%20consisting%20of%20a%20task-agnostic%20encoder%20adaptation%20and%20task-specific%0Adecoder%20adaptation.%20Furthermore%2C%20we%20introduce%20two%20feature%20propagation%20modules%0Ato%20facilitate%20inter-task%20and%20inter-scale%20feature%20represenation%20learning.%0AExperiments%20on%20PASCAL-Context%20and%20NYUD-V2%20dataset%20demonstrate%20that%20our%20method%0Aoutperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%20Fine-Tuned%0A%28PEFT%29%20baselines.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12997v2&entry.124074799=Read"},
{"title": "Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation", "author": "Clive Tinashe Marimo and Benedikt Blumenstiel and Maximilian Nitsche and Johannes Jakubik and Thomas Brunschwiler", "abstract": "  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language\nmodel pre-trained with contrastive learning on a large-scale multispectral\ndataset and report on the performance gains due to the extended spectral range.\nFurthermore, we present the largest-to-date image-caption dataset for\nmultispectral data, consisting of one million Sentinel-2 samples and\ncorresponding textual descriptions generated using Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n+6.77% on average and retrieval performance by +4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. The image-caption dataset, code, and model weights\nare available at https://github.com/IBM/MS-CLIP.\n", "link": "http://arxiv.org/abs/2503.15969v3", "date": "2025-08-04", "relevancy": 2.9518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation&body=Title%3A%20Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation%0AAuthor%3A%20Clive%20Tinashe%20Marimo%20and%20Benedikt%20Blumenstiel%20and%20Maximilian%20Nitsche%20and%20Johannes%20Jakubik%20and%20Thomas%20Brunschwiler%0AAbstract%3A%20%20%20Vision-language%20models%20for%20Earth%20observation%20%28EO%29%20typically%20rely%20on%20the%0Avisual%20spectrum%20of%20data%20as%20the%20only%20model%20input%2C%20thus%20failing%20to%20leverage%20the%0Arich%20spectral%20information%20available%20in%20the%20multispectral%20channels%20recorded%20by%0Asatellites.%20Therefore%2C%20we%20introduce%20Llama3-MS-CLIP%2C%20the%20first%20vision-language%0Amodel%20pre-trained%20with%20contrastive%20learning%20on%20a%20large-scale%20multispectral%0Adataset%20and%20report%20on%20the%20performance%20gains%20due%20to%20the%20extended%20spectral%20range.%0AFurthermore%2C%20we%20present%20the%20largest-to-date%20image-caption%20dataset%20for%0Amultispectral%20data%2C%20consisting%20of%20one%20million%20Sentinel-2%20samples%20and%0Acorresponding%20textual%20descriptions%20generated%20using%20Llama3-LLaVA-Next%20and%0AOverture%20Maps%20data.%20We%20develop%20a%20scalable%20captioning%20pipeline%2C%20which%20is%0Avalidated%20by%20domain%20experts.%20We%20evaluate%20Llama3-MS-CLIP%20on%20multispectral%0Azero-shot%20image%20classification%20and%20retrieval%20using%20three%20datasets%20of%20varying%0Acomplexity.%20Our%20results%20demonstrate%20that%20Llama3-MS-CLIP%20significantly%0Aoutperforms%20other%20RGB-based%20approaches%2C%20improving%20classification%20accuracy%20by%0A%2B6.77%25%20on%20average%20and%20retrieval%20performance%20by%20%2B4.63%25%20mAP%20compared%20to%20the%0Asecond-best%20model.%20Our%20results%20emphasize%20the%20relevance%20of%20multispectral%0Avision-language%20learning.%20The%20image-caption%20dataset%2C%20code%2C%20and%20model%20weights%0Aare%20available%20at%20https%3A//github.com/IBM/MS-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15969v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Visible%253A%2520Multispectral%2520Vision-Language%2520Learning%2520for%2520Earth%250A%2520%2520Observation%26entry.906535625%3DClive%2520Tinashe%2520Marimo%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Maximilian%2520Nitsche%2520and%2520Johannes%2520Jakubik%2520and%2520Thomas%2520Brunschwiler%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520for%2520Earth%2520observation%2520%2528EO%2529%2520typically%2520rely%2520on%2520the%250Avisual%2520spectrum%2520of%2520data%2520as%2520the%2520only%2520model%2520input%252C%2520thus%2520failing%2520to%2520leverage%2520the%250Arich%2520spectral%2520information%2520available%2520in%2520the%2520multispectral%2520channels%2520recorded%2520by%250Asatellites.%2520Therefore%252C%2520we%2520introduce%2520Llama3-MS-CLIP%252C%2520the%2520first%2520vision-language%250Amodel%2520pre-trained%2520with%2520contrastive%2520learning%2520on%2520a%2520large-scale%2520multispectral%250Adataset%2520and%2520report%2520on%2520the%2520performance%2520gains%2520due%2520to%2520the%2520extended%2520spectral%2520range.%250AFurthermore%252C%2520we%2520present%2520the%2520largest-to-date%2520image-caption%2520dataset%2520for%250Amultispectral%2520data%252C%2520consisting%2520of%2520one%2520million%2520Sentinel-2%2520samples%2520and%250Acorresponding%2520textual%2520descriptions%2520generated%2520using%2520Llama3-LLaVA-Next%2520and%250AOverture%2520Maps%2520data.%2520We%2520develop%2520a%2520scalable%2520captioning%2520pipeline%252C%2520which%2520is%250Avalidated%2520by%2520domain%2520experts.%2520We%2520evaluate%2520Llama3-MS-CLIP%2520on%2520multispectral%250Azero-shot%2520image%2520classification%2520and%2520retrieval%2520using%2520three%2520datasets%2520of%2520varying%250Acomplexity.%2520Our%2520results%2520demonstrate%2520that%2520Llama3-MS-CLIP%2520significantly%250Aoutperforms%2520other%2520RGB-based%2520approaches%252C%2520improving%2520classification%2520accuracy%2520by%250A%252B6.77%2525%2520on%2520average%2520and%2520retrieval%2520performance%2520by%2520%252B4.63%2525%2520mAP%2520compared%2520to%2520the%250Asecond-best%2520model.%2520Our%2520results%2520emphasize%2520the%2520relevance%2520of%2520multispectral%250Avision-language%2520learning.%2520The%2520image-caption%2520dataset%252C%2520code%252C%2520and%2520model%2520weights%250Aare%2520available%2520at%2520https%253A//github.com/IBM/MS-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15969v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation&entry.906535625=Clive%20Tinashe%20Marimo%20and%20Benedikt%20Blumenstiel%20and%20Maximilian%20Nitsche%20and%20Johannes%20Jakubik%20and%20Thomas%20Brunschwiler&entry.1292438233=%20%20Vision-language%20models%20for%20Earth%20observation%20%28EO%29%20typically%20rely%20on%20the%0Avisual%20spectrum%20of%20data%20as%20the%20only%20model%20input%2C%20thus%20failing%20to%20leverage%20the%0Arich%20spectral%20information%20available%20in%20the%20multispectral%20channels%20recorded%20by%0Asatellites.%20Therefore%2C%20we%20introduce%20Llama3-MS-CLIP%2C%20the%20first%20vision-language%0Amodel%20pre-trained%20with%20contrastive%20learning%20on%20a%20large-scale%20multispectral%0Adataset%20and%20report%20on%20the%20performance%20gains%20due%20to%20the%20extended%20spectral%20range.%0AFurthermore%2C%20we%20present%20the%20largest-to-date%20image-caption%20dataset%20for%0Amultispectral%20data%2C%20consisting%20of%20one%20million%20Sentinel-2%20samples%20and%0Acorresponding%20textual%20descriptions%20generated%20using%20Llama3-LLaVA-Next%20and%0AOverture%20Maps%20data.%20We%20develop%20a%20scalable%20captioning%20pipeline%2C%20which%20is%0Avalidated%20by%20domain%20experts.%20We%20evaluate%20Llama3-MS-CLIP%20on%20multispectral%0Azero-shot%20image%20classification%20and%20retrieval%20using%20three%20datasets%20of%20varying%0Acomplexity.%20Our%20results%20demonstrate%20that%20Llama3-MS-CLIP%20significantly%0Aoutperforms%20other%20RGB-based%20approaches%2C%20improving%20classification%20accuracy%20by%0A%2B6.77%25%20on%20average%20and%20retrieval%20performance%20by%20%2B4.63%25%20mAP%20compared%20to%20the%0Asecond-best%20model.%20Our%20results%20emphasize%20the%20relevance%20of%20multispectral%0Avision-language%20learning.%20The%20image-caption%20dataset%2C%20code%2C%20and%20model%20weights%0Aare%20available%20at%20https%3A//github.com/IBM/MS-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15969v3&entry.124074799=Read"},
{"title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description", "author": "Mahmoud Ahmed and Junjie Fei and Jian Ding and Eslam Mohamed Bakr and Mohamed Elhoseiny", "abstract": "  In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/\n", "link": "http://arxiv.org/abs/2405.18937v2", "date": "2025-08-04", "relevancy": 2.9482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kestrel%3A%203D%20Multimodal%20LLM%20for%20Part-Aware%20Grounded%20Description&body=Title%3A%20Kestrel%3A%203D%20Multimodal%20LLM%20for%20Part-Aware%20Grounded%20Description%0AAuthor%3A%20Mahmoud%20Ahmed%20and%20Junjie%20Fei%20and%20Jian%20Ding%20and%20Eslam%20Mohamed%20Bakr%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Part-Aware%20Point%20Grounded%20Description%20%28PaPGD%29%2C%20a%0Achallenging%20task%20aimed%20at%20advancing%203D%20multimodal%20learning%20for%20fine-grained%2C%0Apart-aware%20segmentation%20grounding%20and%20detailed%20explanation%20of%203D%20objects.%0AExisting%203D%20datasets%20largely%20focus%20on%20either%20vision-only%20part%20segmentation%20or%0Avision-language%20scene%20segmentation%2C%20lacking%20the%20fine-grained%20multimodal%0Asegmentation%20needed%20for%20robotic%20navigation%20and%20interaction%20in%20real-world%0Aenvironments.%20To%20address%20this%20gap%2C%20we%20present%20the%203DCoMPaT%20Grounded%0AInstructions%20%283DCoMPaT-GrIn%29%20Dataset%2C%20a%20comprehensive%20resource%20that%20pairs%20rich%0Apoint%20cloud%20descriptions%20with%20corresponding%20part-level%20segmentation%20masks.%20This%0Adataset%20encompasses%20extensive%20samples%20designed%20for%20both%20PaPGD%20and%20fine-grained%0Asingle-part%20grounding%20tasks.%20To%20tackle%20the%20inherent%20challenges%20of%20grounding%0Aobjects%20and%20generating%20grounded%20descriptions%20at%20the%20part%20level%2C%20we%20propose%0AKestrel%2C%20a%20part-aware%203D%20multimodal%20large%20language%20model%20that%20integrates%20an%0Aadvanced%20language%20model%20for%20nuanced%20language%20comprehension%20with%20multi-level%0Apoint%20feature%20propagation%20and%20query%20refinement%20mechanism%20to%20enhance%20spatial%0Areasoning%20at%20the%20part%20level.%20The%20extensive%20experiments%20demonstrate%20that%20Kestrel%0Aeffectively%20bridges%20the%20gap%20between%20part-aware%20language%20understanding%20and%203D%0Asegmentation%20grounding%2C%20paving%20the%20way%20for%20more%20robust%20and%20interpretable%203D%0Aobject%20comprehension%20that%20meets%20the%20demands%20of%20real-world%20robotic%20applications.%0AProject%20page%20at%20https%3A//feielysia.github.io/Kestrel.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKestrel%253A%25203D%2520Multimodal%2520LLM%2520for%2520Part-Aware%2520Grounded%2520Description%26entry.906535625%3DMahmoud%2520Ahmed%2520and%2520Junjie%2520Fei%2520and%2520Jian%2520Ding%2520and%2520Eslam%2520Mohamed%2520Bakr%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Part-Aware%2520Point%2520Grounded%2520Description%2520%2528PaPGD%2529%252C%2520a%250Achallenging%2520task%2520aimed%2520at%2520advancing%25203D%2520multimodal%2520learning%2520for%2520fine-grained%252C%250Apart-aware%2520segmentation%2520grounding%2520and%2520detailed%2520explanation%2520of%25203D%2520objects.%250AExisting%25203D%2520datasets%2520largely%2520focus%2520on%2520either%2520vision-only%2520part%2520segmentation%2520or%250Avision-language%2520scene%2520segmentation%252C%2520lacking%2520the%2520fine-grained%2520multimodal%250Asegmentation%2520needed%2520for%2520robotic%2520navigation%2520and%2520interaction%2520in%2520real-world%250Aenvironments.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520the%25203DCoMPaT%2520Grounded%250AInstructions%2520%25283DCoMPaT-GrIn%2529%2520Dataset%252C%2520a%2520comprehensive%2520resource%2520that%2520pairs%2520rich%250Apoint%2520cloud%2520descriptions%2520with%2520corresponding%2520part-level%2520segmentation%2520masks.%2520This%250Adataset%2520encompasses%2520extensive%2520samples%2520designed%2520for%2520both%2520PaPGD%2520and%2520fine-grained%250Asingle-part%2520grounding%2520tasks.%2520To%2520tackle%2520the%2520inherent%2520challenges%2520of%2520grounding%250Aobjects%2520and%2520generating%2520grounded%2520descriptions%2520at%2520the%2520part%2520level%252C%2520we%2520propose%250AKestrel%252C%2520a%2520part-aware%25203D%2520multimodal%2520large%2520language%2520model%2520that%2520integrates%2520an%250Aadvanced%2520language%2520model%2520for%2520nuanced%2520language%2520comprehension%2520with%2520multi-level%250Apoint%2520feature%2520propagation%2520and%2520query%2520refinement%2520mechanism%2520to%2520enhance%2520spatial%250Areasoning%2520at%2520the%2520part%2520level.%2520The%2520extensive%2520experiments%2520demonstrate%2520that%2520Kestrel%250Aeffectively%2520bridges%2520the%2520gap%2520between%2520part-aware%2520language%2520understanding%2520and%25203D%250Asegmentation%2520grounding%252C%2520paving%2520the%2520way%2520for%2520more%2520robust%2520and%2520interpretable%25203D%250Aobject%2520comprehension%2520that%2520meets%2520the%2520demands%2520of%2520real-world%2520robotic%2520applications.%250AProject%2520page%2520at%2520https%253A//feielysia.github.io/Kestrel.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kestrel%3A%203D%20Multimodal%20LLM%20for%20Part-Aware%20Grounded%20Description&entry.906535625=Mahmoud%20Ahmed%20and%20Junjie%20Fei%20and%20Jian%20Ding%20and%20Eslam%20Mohamed%20Bakr%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Part-Aware%20Point%20Grounded%20Description%20%28PaPGD%29%2C%20a%0Achallenging%20task%20aimed%20at%20advancing%203D%20multimodal%20learning%20for%20fine-grained%2C%0Apart-aware%20segmentation%20grounding%20and%20detailed%20explanation%20of%203D%20objects.%0AExisting%203D%20datasets%20largely%20focus%20on%20either%20vision-only%20part%20segmentation%20or%0Avision-language%20scene%20segmentation%2C%20lacking%20the%20fine-grained%20multimodal%0Asegmentation%20needed%20for%20robotic%20navigation%20and%20interaction%20in%20real-world%0Aenvironments.%20To%20address%20this%20gap%2C%20we%20present%20the%203DCoMPaT%20Grounded%0AInstructions%20%283DCoMPaT-GrIn%29%20Dataset%2C%20a%20comprehensive%20resource%20that%20pairs%20rich%0Apoint%20cloud%20descriptions%20with%20corresponding%20part-level%20segmentation%20masks.%20This%0Adataset%20encompasses%20extensive%20samples%20designed%20for%20both%20PaPGD%20and%20fine-grained%0Asingle-part%20grounding%20tasks.%20To%20tackle%20the%20inherent%20challenges%20of%20grounding%0Aobjects%20and%20generating%20grounded%20descriptions%20at%20the%20part%20level%2C%20we%20propose%0AKestrel%2C%20a%20part-aware%203D%20multimodal%20large%20language%20model%20that%20integrates%20an%0Aadvanced%20language%20model%20for%20nuanced%20language%20comprehension%20with%20multi-level%0Apoint%20feature%20propagation%20and%20query%20refinement%20mechanism%20to%20enhance%20spatial%0Areasoning%20at%20the%20part%20level.%20The%20extensive%20experiments%20demonstrate%20that%20Kestrel%0Aeffectively%20bridges%20the%20gap%20between%20part-aware%20language%20understanding%20and%203D%0Asegmentation%20grounding%2C%20paving%20the%20way%20for%20more%20robust%20and%20interpretable%203D%0Aobject%20comprehension%20that%20meets%20the%20demands%20of%20real-world%20robotic%20applications.%0AProject%20page%20at%20https%3A//feielysia.github.io/Kestrel.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18937v2&entry.124074799=Read"},
{"title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming", "author": "Shuo Wang and Yongcai Wang and Wanting Li and Yucheng Wang and Maiyue Chen and Kaihui Wang and Zhizhong Su and Xudong Cai and Yeying Jin and Deying Li and Zhaoxin Fan", "abstract": "  Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.\n", "link": "http://arxiv.org/abs/2508.02549v1", "date": "2025-08-04", "relevancy": 2.9374, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoDream%3A%20Monocular%20Vision-Language%20Navigation%20with%20Panoramic%20Dreaming&body=Title%3A%20MonoDream%3A%20Monocular%20Vision-Language%20Navigation%20with%20Panoramic%20Dreaming%0AAuthor%3A%20Shuo%20Wang%20and%20Yongcai%20Wang%20and%20Wanting%20Li%20and%20Yucheng%20Wang%20and%20Maiyue%20Chen%20and%20Kaihui%20Wang%20and%20Zhizhong%20Su%20and%20Xudong%20Cai%20and%20Yeying%20Jin%20and%20Deying%20Li%20and%20Zhaoxin%20Fan%0AAbstract%3A%20%20%20Vision-Language%20Navigation%20%28VLN%29%20tasks%20often%20leverage%20panoramic%20RGB%20and%20depth%0Ainputs%20to%20provide%20rich%20spatial%20cues%20for%20action%20planning%2C%20but%20these%20sensors%20can%0Abe%20costly%20or%20less%20accessible%20in%20real-world%20deployments.%20Recent%20approaches%20based%0Aon%20Vision-Language%20Action%20%28VLA%29%20models%20achieve%20strong%20results%20with%20monocular%0Ainput%2C%20yet%20they%20still%20lag%20behind%20methods%20using%20panoramic%20RGB-D%20information.%20We%0Apresent%20MonoDream%2C%20a%20lightweight%20VLA%20framework%20that%20enables%20monocular%20agents%20to%0Alearn%20a%20Unified%20Navigation%20Representation%20%28UNR%29.%20This%20shared%20feature%0Arepresentation%20jointly%20aligns%20navigation-relevant%20visual%20semantics%20%28e.g.%2C%0Aglobal%20layout%2C%20depth%2C%20and%20future%20cues%29%20and%20language-grounded%20action%20intent%2C%0Aenabling%20more%20reliable%20action%20prediction.%20MonoDream%20further%20introduces%20Latent%0APanoramic%20Dreaming%20%28LPD%29%20tasks%20to%20supervise%20the%20UNR%2C%20which%20train%20the%20model%20to%0Apredict%20latent%20features%20of%20panoramic%20RGB%20and%20depth%20observations%20at%20both%20current%0Aand%20future%20steps%20based%20on%20only%20monocular%20input.%20Experiments%20on%20multiple%20VLN%0Abenchmarks%20show%20that%20MonoDream%20consistently%20improves%20monocular%20navigation%0Aperformance%20and%20significantly%20narrows%20the%20gap%20with%20panoramic-based%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoDream%253A%2520Monocular%2520Vision-Language%2520Navigation%2520with%2520Panoramic%2520Dreaming%26entry.906535625%3DShuo%2520Wang%2520and%2520Yongcai%2520Wang%2520and%2520Wanting%2520Li%2520and%2520Yucheng%2520Wang%2520and%2520Maiyue%2520Chen%2520and%2520Kaihui%2520Wang%2520and%2520Zhizhong%2520Su%2520and%2520Xudong%2520Cai%2520and%2520Yeying%2520Jin%2520and%2520Deying%2520Li%2520and%2520Zhaoxin%2520Fan%26entry.1292438233%3D%2520%2520Vision-Language%2520Navigation%2520%2528VLN%2529%2520tasks%2520often%2520leverage%2520panoramic%2520RGB%2520and%2520depth%250Ainputs%2520to%2520provide%2520rich%2520spatial%2520cues%2520for%2520action%2520planning%252C%2520but%2520these%2520sensors%2520can%250Abe%2520costly%2520or%2520less%2520accessible%2520in%2520real-world%2520deployments.%2520Recent%2520approaches%2520based%250Aon%2520Vision-Language%2520Action%2520%2528VLA%2529%2520models%2520achieve%2520strong%2520results%2520with%2520monocular%250Ainput%252C%2520yet%2520they%2520still%2520lag%2520behind%2520methods%2520using%2520panoramic%2520RGB-D%2520information.%2520We%250Apresent%2520MonoDream%252C%2520a%2520lightweight%2520VLA%2520framework%2520that%2520enables%2520monocular%2520agents%2520to%250Alearn%2520a%2520Unified%2520Navigation%2520Representation%2520%2528UNR%2529.%2520This%2520shared%2520feature%250Arepresentation%2520jointly%2520aligns%2520navigation-relevant%2520visual%2520semantics%2520%2528e.g.%252C%250Aglobal%2520layout%252C%2520depth%252C%2520and%2520future%2520cues%2529%2520and%2520language-grounded%2520action%2520intent%252C%250Aenabling%2520more%2520reliable%2520action%2520prediction.%2520MonoDream%2520further%2520introduces%2520Latent%250APanoramic%2520Dreaming%2520%2528LPD%2529%2520tasks%2520to%2520supervise%2520the%2520UNR%252C%2520which%2520train%2520the%2520model%2520to%250Apredict%2520latent%2520features%2520of%2520panoramic%2520RGB%2520and%2520depth%2520observations%2520at%2520both%2520current%250Aand%2520future%2520steps%2520based%2520on%2520only%2520monocular%2520input.%2520Experiments%2520on%2520multiple%2520VLN%250Abenchmarks%2520show%2520that%2520MonoDream%2520consistently%2520improves%2520monocular%2520navigation%250Aperformance%2520and%2520significantly%2520narrows%2520the%2520gap%2520with%2520panoramic-based%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoDream%3A%20Monocular%20Vision-Language%20Navigation%20with%20Panoramic%20Dreaming&entry.906535625=Shuo%20Wang%20and%20Yongcai%20Wang%20and%20Wanting%20Li%20and%20Yucheng%20Wang%20and%20Maiyue%20Chen%20and%20Kaihui%20Wang%20and%20Zhizhong%20Su%20and%20Xudong%20Cai%20and%20Yeying%20Jin%20and%20Deying%20Li%20and%20Zhaoxin%20Fan&entry.1292438233=%20%20Vision-Language%20Navigation%20%28VLN%29%20tasks%20often%20leverage%20panoramic%20RGB%20and%20depth%0Ainputs%20to%20provide%20rich%20spatial%20cues%20for%20action%20planning%2C%20but%20these%20sensors%20can%0Abe%20costly%20or%20less%20accessible%20in%20real-world%20deployments.%20Recent%20approaches%20based%0Aon%20Vision-Language%20Action%20%28VLA%29%20models%20achieve%20strong%20results%20with%20monocular%0Ainput%2C%20yet%20they%20still%20lag%20behind%20methods%20using%20panoramic%20RGB-D%20information.%20We%0Apresent%20MonoDream%2C%20a%20lightweight%20VLA%20framework%20that%20enables%20monocular%20agents%20to%0Alearn%20a%20Unified%20Navigation%20Representation%20%28UNR%29.%20This%20shared%20feature%0Arepresentation%20jointly%20aligns%20navigation-relevant%20visual%20semantics%20%28e.g.%2C%0Aglobal%20layout%2C%20depth%2C%20and%20future%20cues%29%20and%20language-grounded%20action%20intent%2C%0Aenabling%20more%20reliable%20action%20prediction.%20MonoDream%20further%20introduces%20Latent%0APanoramic%20Dreaming%20%28LPD%29%20tasks%20to%20supervise%20the%20UNR%2C%20which%20train%20the%20model%20to%0Apredict%20latent%20features%20of%20panoramic%20RGB%20and%20depth%20observations%20at%20both%20current%0Aand%20future%20steps%20based%20on%20only%20monocular%20input.%20Experiments%20on%20multiple%20VLN%0Abenchmarks%20show%20that%20MonoDream%20consistently%20improves%20monocular%20navigation%0Aperformance%20and%20significantly%20narrows%20the%20gap%20with%20panoramic-based%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02549v1&entry.124074799=Read"},
{"title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo", "author": "Qianli Ma and Yaowei Zheng and Zhelun Shi and Zhongkai Zhao and Bin Jia and Ziyue Huang and Zhiqi Lin and Youjie Li and Jiacheng Yang and Yanghua Peng and Zhi Zhang and Xin Liu", "abstract": "  Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.\n", "link": "http://arxiv.org/abs/2508.02317v1", "date": "2025-08-04", "relevancy": 2.8779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo&body=Title%3A%20VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo%0AAuthor%3A%20Qianli%20Ma%20and%20Yaowei%20Zheng%20and%20Zhelun%20Shi%20and%20Zhongkai%20Zhao%20and%20Bin%20Jia%20and%20Ziyue%20Huang%20and%20Zhiqi%20Lin%20and%20Youjie%20Li%20and%20Jiacheng%20Yang%20and%20Yanghua%20Peng%20and%20Zhi%20Zhang%20and%20Xin%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20driven%20impressive%0Aprogress%20in%20omni-modal%20understanding%20and%20generation.%20However%2C%20training%0Aomni-modal%20LLMs%20remains%20a%20significant%20challenge%20due%20to%20the%20heterogeneous%20model%0Aarchitectures%20required%20to%20process%20diverse%20modalities%2C%20necessitating%0Asophisticated%20system%20design%20for%20efficient%20large-scale%20training.%20Existing%0Aframeworks%20typically%20entangle%20model%20definition%20with%20parallel%20logic%2C%20incurring%0Alimited%20scalability%20and%20substantial%20engineering%20overhead%20for%20end-to-end%0Aomni-modal%20training.%20%25%20We%20present%20%5Cveomni%2C%20a%20modular%20and%20efficient%20training%0Aframework%20to%20accelerate%20the%20development%20of%20omni-modal%20LLMs.%20%5Cveomni%20introduces%0Amodel-centric%20distributed%20recipes%20that%20decouples%20communication%20from%0Acomputation%2C%20enabling%20efficient%203D%20parallelism%20on%20omni-modal%20LLMs.%20%5Cveomni%20also%0Afeatures%20a%20flexible%20configuration%20interface%20supporting%20seamless%20integration%20of%0Anew%20modalities%20with%20minimal%20code%20change.%20%25%20Using%20%5Cveomni%2C%20a%20omni-modal%0Amixture-of-experts%20%28MoE%29%20model%20with%2030B%20parameters%20can%20be%20trained%20with%20over%0A2%2C800%20tokens/sec/GPU%20throughput%20and%20scale%20to%20160K%20context%20lengths%20via%203D%0Aparallelism%20on%20128%20GPUs%2C%20showcasing%20its%20superior%20efficiency%20and%20scalability%20for%0Atraining%20large%20omni-modal%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeOmni%253A%2520Scaling%2520Any%2520Modality%2520Model%2520Training%2520with%2520Model-Centric%250A%2520%2520Distributed%2520Recipe%2520Zoo%26entry.906535625%3DQianli%2520Ma%2520and%2520Yaowei%2520Zheng%2520and%2520Zhelun%2520Shi%2520and%2520Zhongkai%2520Zhao%2520and%2520Bin%2520Jia%2520and%2520Ziyue%2520Huang%2520and%2520Zhiqi%2520Lin%2520and%2520Youjie%2520Li%2520and%2520Jiacheng%2520Yang%2520and%2520Yanghua%2520Peng%2520and%2520Zhi%2520Zhang%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520driven%2520impressive%250Aprogress%2520in%2520omni-modal%2520understanding%2520and%2520generation.%2520However%252C%2520training%250Aomni-modal%2520LLMs%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520heterogeneous%2520model%250Aarchitectures%2520required%2520to%2520process%2520diverse%2520modalities%252C%2520necessitating%250Asophisticated%2520system%2520design%2520for%2520efficient%2520large-scale%2520training.%2520Existing%250Aframeworks%2520typically%2520entangle%2520model%2520definition%2520with%2520parallel%2520logic%252C%2520incurring%250Alimited%2520scalability%2520and%2520substantial%2520engineering%2520overhead%2520for%2520end-to-end%250Aomni-modal%2520training.%2520%2525%2520We%2520present%2520%255Cveomni%252C%2520a%2520modular%2520and%2520efficient%2520training%250Aframework%2520to%2520accelerate%2520the%2520development%2520of%2520omni-modal%2520LLMs.%2520%255Cveomni%2520introduces%250Amodel-centric%2520distributed%2520recipes%2520that%2520decouples%2520communication%2520from%250Acomputation%252C%2520enabling%2520efficient%25203D%2520parallelism%2520on%2520omni-modal%2520LLMs.%2520%255Cveomni%2520also%250Afeatures%2520a%2520flexible%2520configuration%2520interface%2520supporting%2520seamless%2520integration%2520of%250Anew%2520modalities%2520with%2520minimal%2520code%2520change.%2520%2525%2520Using%2520%255Cveomni%252C%2520a%2520omni-modal%250Amixture-of-experts%2520%2528MoE%2529%2520model%2520with%252030B%2520parameters%2520can%2520be%2520trained%2520with%2520over%250A2%252C800%2520tokens/sec/GPU%2520throughput%2520and%2520scale%2520to%2520160K%2520context%2520lengths%2520via%25203D%250Aparallelism%2520on%2520128%2520GPUs%252C%2520showcasing%2520its%2520superior%2520efficiency%2520and%2520scalability%2520for%250Atraining%2520large%2520omni-modal%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo&entry.906535625=Qianli%20Ma%20and%20Yaowei%20Zheng%20and%20Zhelun%20Shi%20and%20Zhongkai%20Zhao%20and%20Bin%20Jia%20and%20Ziyue%20Huang%20and%20Zhiqi%20Lin%20and%20Youjie%20Li%20and%20Jiacheng%20Yang%20and%20Yanghua%20Peng%20and%20Zhi%20Zhang%20and%20Xin%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20driven%20impressive%0Aprogress%20in%20omni-modal%20understanding%20and%20generation.%20However%2C%20training%0Aomni-modal%20LLMs%20remains%20a%20significant%20challenge%20due%20to%20the%20heterogeneous%20model%0Aarchitectures%20required%20to%20process%20diverse%20modalities%2C%20necessitating%0Asophisticated%20system%20design%20for%20efficient%20large-scale%20training.%20Existing%0Aframeworks%20typically%20entangle%20model%20definition%20with%20parallel%20logic%2C%20incurring%0Alimited%20scalability%20and%20substantial%20engineering%20overhead%20for%20end-to-end%0Aomni-modal%20training.%20%25%20We%20present%20%5Cveomni%2C%20a%20modular%20and%20efficient%20training%0Aframework%20to%20accelerate%20the%20development%20of%20omni-modal%20LLMs.%20%5Cveomni%20introduces%0Amodel-centric%20distributed%20recipes%20that%20decouples%20communication%20from%0Acomputation%2C%20enabling%20efficient%203D%20parallelism%20on%20omni-modal%20LLMs.%20%5Cveomni%20also%0Afeatures%20a%20flexible%20configuration%20interface%20supporting%20seamless%20integration%20of%0Anew%20modalities%20with%20minimal%20code%20change.%20%25%20Using%20%5Cveomni%2C%20a%20omni-modal%0Amixture-of-experts%20%28MoE%29%20model%20with%2030B%20parameters%20can%20be%20trained%20with%20over%0A2%2C800%20tokens/sec/GPU%20throughput%20and%20scale%20to%20160K%20context%20lengths%20via%203D%0Aparallelism%20on%20128%20GPUs%2C%20showcasing%20its%20superior%20efficiency%20and%20scalability%20for%0Atraining%20large%20omni-modal%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02317v1&entry.124074799=Read"},
{"title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding", "author": "Wenwen Zeng and Yonghuang Wu and Yifan Chen and Xuan Xie and Chengqian Zhao and Feiyu Yin and Guoqing Wu and Jinhua Yu", "abstract": "  Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.\n", "link": "http://arxiv.org/abs/2508.02480v1", "date": "2025-08-04", "relevancy": 2.8618, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindShot%3A%20Multi-Shot%20Video%20Reconstruction%20from%20fMRI%20with%20LLM%20Decoding&body=Title%3A%20MindShot%3A%20Multi-Shot%20Video%20Reconstruction%20from%20fMRI%20with%20LLM%20Decoding%0AAuthor%3A%20Wenwen%20Zeng%20and%20Yonghuang%20Wu%20and%20Yifan%20Chen%20and%20Xuan%20Xie%20and%20Chengqian%20Zhao%20and%20Feiyu%20Yin%20and%20Guoqing%20Wu%20and%20Jinhua%20Yu%0AAbstract%3A%20%20%20Reconstructing%20dynamic%20videos%20from%20fMRI%20is%20important%20for%20understanding%20visual%0Acognition%20and%20enabling%20vivid%20brain-computer%20interfaces.%20However%2C%20current%0Amethods%20are%20critically%20limited%20to%20single-shot%20clips%2C%20failing%20to%20address%20the%0Amulti-shot%20nature%20of%20real-world%20experiences.%20Multi-shot%20reconstruction%20faces%0Afundamental%20challenges%3A%20fMRI%20signal%20mixing%20across%20shots%2C%20the%20temporal%0Aresolution%20mismatch%20between%20fMRI%20and%20video%20obscuring%20rapid%20scene%20changes%2C%20and%0Athe%20lack%20of%20dedicated%20multi-shot%20fMRI-video%20datasets.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20novel%20divide-and-decode%20framework%20for%20multi-shot%20fMRI%0Avideo%20reconstruction.%20Our%20core%20innovations%20are%3A%20%281%29%20A%20shot%20boundary%20predictor%0Amodule%20explicitly%20decomposing%20mixed%20fMRI%20signals%20into%20shot-specific%20segments.%0A%282%29%20Generative%20keyframe%20captioning%20using%20LLMs%2C%20which%20decodes%20robust%20textual%0Adescriptions%20from%20each%20segment%2C%20overcoming%20temporal%20blur%20by%20leveraging%0Ahigh-level%20semantics.%20%283%29%20Novel%20large-scale%20data%20synthesis%20%2820k%20samples%29%20from%0Aexisting%20datasets.%20Experimental%20results%20demonstrate%20our%20framework%20outperforms%0Astate-of-the-art%20methods%20in%20multi-shot%20reconstruction%20fidelity.%20Ablation%0Astudies%20confirm%20the%20critical%20role%20of%20fMRI%20decomposition%20and%20semantic%0Acaptioning%2C%20with%20decomposition%20significantly%20improving%20decoded%20caption%20CLIP%0Asimilarity%20by%2071.8%25.%20This%20work%20establishes%20a%20new%20paradigm%20for%20multi-shot%20fMRI%0Areconstruction%2C%20enabling%20accurate%20recovery%20of%20complex%20visual%20narratives%20through%0Aexplicit%20decomposition%20and%20semantic%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindShot%253A%2520Multi-Shot%2520Video%2520Reconstruction%2520from%2520fMRI%2520with%2520LLM%2520Decoding%26entry.906535625%3DWenwen%2520Zeng%2520and%2520Yonghuang%2520Wu%2520and%2520Yifan%2520Chen%2520and%2520Xuan%2520Xie%2520and%2520Chengqian%2520Zhao%2520and%2520Feiyu%2520Yin%2520and%2520Guoqing%2520Wu%2520and%2520Jinhua%2520Yu%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%2520videos%2520from%2520fMRI%2520is%2520important%2520for%2520understanding%2520visual%250Acognition%2520and%2520enabling%2520vivid%2520brain-computer%2520interfaces.%2520However%252C%2520current%250Amethods%2520are%2520critically%2520limited%2520to%2520single-shot%2520clips%252C%2520failing%2520to%2520address%2520the%250Amulti-shot%2520nature%2520of%2520real-world%2520experiences.%2520Multi-shot%2520reconstruction%2520faces%250Afundamental%2520challenges%253A%2520fMRI%2520signal%2520mixing%2520across%2520shots%252C%2520the%2520temporal%250Aresolution%2520mismatch%2520between%2520fMRI%2520and%2520video%2520obscuring%2520rapid%2520scene%2520changes%252C%2520and%250Athe%2520lack%2520of%2520dedicated%2520multi-shot%2520fMRI-video%2520datasets.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520divide-and-decode%2520framework%2520for%2520multi-shot%2520fMRI%250Avideo%2520reconstruction.%2520Our%2520core%2520innovations%2520are%253A%2520%25281%2529%2520A%2520shot%2520boundary%2520predictor%250Amodule%2520explicitly%2520decomposing%2520mixed%2520fMRI%2520signals%2520into%2520shot-specific%2520segments.%250A%25282%2529%2520Generative%2520keyframe%2520captioning%2520using%2520LLMs%252C%2520which%2520decodes%2520robust%2520textual%250Adescriptions%2520from%2520each%2520segment%252C%2520overcoming%2520temporal%2520blur%2520by%2520leveraging%250Ahigh-level%2520semantics.%2520%25283%2529%2520Novel%2520large-scale%2520data%2520synthesis%2520%252820k%2520samples%2529%2520from%250Aexisting%2520datasets.%2520Experimental%2520results%2520demonstrate%2520our%2520framework%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520multi-shot%2520reconstruction%2520fidelity.%2520Ablation%250Astudies%2520confirm%2520the%2520critical%2520role%2520of%2520fMRI%2520decomposition%2520and%2520semantic%250Acaptioning%252C%2520with%2520decomposition%2520significantly%2520improving%2520decoded%2520caption%2520CLIP%250Asimilarity%2520by%252071.8%2525.%2520This%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520multi-shot%2520fMRI%250Areconstruction%252C%2520enabling%2520accurate%2520recovery%2520of%2520complex%2520visual%2520narratives%2520through%250Aexplicit%2520decomposition%2520and%2520semantic%2520prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindShot%3A%20Multi-Shot%20Video%20Reconstruction%20from%20fMRI%20with%20LLM%20Decoding&entry.906535625=Wenwen%20Zeng%20and%20Yonghuang%20Wu%20and%20Yifan%20Chen%20and%20Xuan%20Xie%20and%20Chengqian%20Zhao%20and%20Feiyu%20Yin%20and%20Guoqing%20Wu%20and%20Jinhua%20Yu&entry.1292438233=%20%20Reconstructing%20dynamic%20videos%20from%20fMRI%20is%20important%20for%20understanding%20visual%0Acognition%20and%20enabling%20vivid%20brain-computer%20interfaces.%20However%2C%20current%0Amethods%20are%20critically%20limited%20to%20single-shot%20clips%2C%20failing%20to%20address%20the%0Amulti-shot%20nature%20of%20real-world%20experiences.%20Multi-shot%20reconstruction%20faces%0Afundamental%20challenges%3A%20fMRI%20signal%20mixing%20across%20shots%2C%20the%20temporal%0Aresolution%20mismatch%20between%20fMRI%20and%20video%20obscuring%20rapid%20scene%20changes%2C%20and%0Athe%20lack%20of%20dedicated%20multi-shot%20fMRI-video%20datasets.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20novel%20divide-and-decode%20framework%20for%20multi-shot%20fMRI%0Avideo%20reconstruction.%20Our%20core%20innovations%20are%3A%20%281%29%20A%20shot%20boundary%20predictor%0Amodule%20explicitly%20decomposing%20mixed%20fMRI%20signals%20into%20shot-specific%20segments.%0A%282%29%20Generative%20keyframe%20captioning%20using%20LLMs%2C%20which%20decodes%20robust%20textual%0Adescriptions%20from%20each%20segment%2C%20overcoming%20temporal%20blur%20by%20leveraging%0Ahigh-level%20semantics.%20%283%29%20Novel%20large-scale%20data%20synthesis%20%2820k%20samples%29%20from%0Aexisting%20datasets.%20Experimental%20results%20demonstrate%20our%20framework%20outperforms%0Astate-of-the-art%20methods%20in%20multi-shot%20reconstruction%20fidelity.%20Ablation%0Astudies%20confirm%20the%20critical%20role%20of%20fMRI%20decomposition%20and%20semantic%0Acaptioning%2C%20with%20decomposition%20significantly%20improving%20decoded%20caption%20CLIP%0Asimilarity%20by%2071.8%25.%20This%20work%20establishes%20a%20new%20paradigm%20for%20multi-shot%20fMRI%0Areconstruction%2C%20enabling%20accurate%20recovery%20of%20complex%20visual%20narratives%20through%0Aexplicit%20decomposition%20and%20semantic%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02480v1&entry.124074799=Read"},
{"title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at\n  T-Junctions Utilizing Road Layout Extraction via Camera", "author": "Byeonggyu Park and Hee-Yeun Kim and Byonghyok Choi and Hansang Cho and Byungkwan Kim and Soomok Lee and Mingu Jeon and Seong-Woo Kim", "abstract": "  Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.\n", "link": "http://arxiv.org/abs/2508.02348v1", "date": "2025-08-04", "relevancy": 2.8437, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6002}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5625}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mmWave%20Radar-Based%20Non-Line-of-Sight%20Pedestrian%20Localization%20at%0A%20%20T-Junctions%20Utilizing%20Road%20Layout%20Extraction%20via%20Camera&body=Title%3A%20mmWave%20Radar-Based%20Non-Line-of-Sight%20Pedestrian%20Localization%20at%0A%20%20T-Junctions%20Utilizing%20Road%20Layout%20Extraction%20via%20Camera%0AAuthor%3A%20Byeonggyu%20Park%20and%20Hee-Yeun%20Kim%20and%20Byonghyok%20Choi%20and%20Hansang%20Cho%20and%20Byungkwan%20Kim%20and%20Soomok%20Lee%20and%20Mingu%20Jeon%20and%20Seong-Woo%20Kim%0AAbstract%3A%20%20%20Pedestrians%20Localization%20in%20Non-Line-of-Sight%20%28NLoS%29%20regions%20within%20urban%0Aenvironments%20poses%20a%20significant%20challenge%20for%20autonomous%20driving%20systems.%0AWhile%20mmWave%20radar%20has%20demonstrated%20potential%20for%20detecting%20objects%20in%20such%0Ascenarios%2C%20the%202D%20radar%20point%20cloud%20%28PCD%29%20data%20is%20susceptible%20to%20distortions%0Acaused%20by%20multipath%20reflections%2C%20making%20accurate%20spatial%20inference%20difficult.%0AAdditionally%2C%20although%20camera%20images%20provide%20high-resolution%20visual%0Ainformation%2C%20they%20lack%20depth%20perception%20and%20cannot%20directly%20observe%20objects%20in%0ANLoS%20regions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20interprets%20radar%0APCD%20through%20road%20layout%20inferred%20from%20camera%20for%20localization%20of%20NLoS%0Apedestrians.%20The%20proposed%20method%20leverages%20visual%20information%20from%20the%20camera%0Ato%20interpret%202D%20radar%20PCD%2C%20enabling%20spatial%20scene%20reconstruction.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20is%20validated%20through%20experiments%0Aconducted%20using%20a%20radar-camera%20system%20mounted%20on%20a%20real%20vehicle.%20The%0Alocalization%20performance%20is%20evaluated%20using%20a%20dataset%20collected%20in%20outdoor%20NLoS%0Adriving%20environments%2C%20demonstrating%20the%20practical%20applicability%20of%20the%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmmWave%2520Radar-Based%2520Non-Line-of-Sight%2520Pedestrian%2520Localization%2520at%250A%2520%2520T-Junctions%2520Utilizing%2520Road%2520Layout%2520Extraction%2520via%2520Camera%26entry.906535625%3DByeonggyu%2520Park%2520and%2520Hee-Yeun%2520Kim%2520and%2520Byonghyok%2520Choi%2520and%2520Hansang%2520Cho%2520and%2520Byungkwan%2520Kim%2520and%2520Soomok%2520Lee%2520and%2520Mingu%2520Jeon%2520and%2520Seong-Woo%2520Kim%26entry.1292438233%3D%2520%2520Pedestrians%2520Localization%2520in%2520Non-Line-of-Sight%2520%2528NLoS%2529%2520regions%2520within%2520urban%250Aenvironments%2520poses%2520a%2520significant%2520challenge%2520for%2520autonomous%2520driving%2520systems.%250AWhile%2520mmWave%2520radar%2520has%2520demonstrated%2520potential%2520for%2520detecting%2520objects%2520in%2520such%250Ascenarios%252C%2520the%25202D%2520radar%2520point%2520cloud%2520%2528PCD%2529%2520data%2520is%2520susceptible%2520to%2520distortions%250Acaused%2520by%2520multipath%2520reflections%252C%2520making%2520accurate%2520spatial%2520inference%2520difficult.%250AAdditionally%252C%2520although%2520camera%2520images%2520provide%2520high-resolution%2520visual%250Ainformation%252C%2520they%2520lack%2520depth%2520perception%2520and%2520cannot%2520directly%2520observe%2520objects%2520in%250ANLoS%2520regions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520interprets%2520radar%250APCD%2520through%2520road%2520layout%2520inferred%2520from%2520camera%2520for%2520localization%2520of%2520NLoS%250Apedestrians.%2520The%2520proposed%2520method%2520leverages%2520visual%2520information%2520from%2520the%2520camera%250Ato%2520interpret%25202D%2520radar%2520PCD%252C%2520enabling%2520spatial%2520scene%2520reconstruction.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520is%2520validated%2520through%2520experiments%250Aconducted%2520using%2520a%2520radar-camera%2520system%2520mounted%2520on%2520a%2520real%2520vehicle.%2520The%250Alocalization%2520performance%2520is%2520evaluated%2520using%2520a%2520dataset%2520collected%2520in%2520outdoor%2520NLoS%250Adriving%2520environments%252C%2520demonstrating%2520the%2520practical%2520applicability%2520of%2520the%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mmWave%20Radar-Based%20Non-Line-of-Sight%20Pedestrian%20Localization%20at%0A%20%20T-Junctions%20Utilizing%20Road%20Layout%20Extraction%20via%20Camera&entry.906535625=Byeonggyu%20Park%20and%20Hee-Yeun%20Kim%20and%20Byonghyok%20Choi%20and%20Hansang%20Cho%20and%20Byungkwan%20Kim%20and%20Soomok%20Lee%20and%20Mingu%20Jeon%20and%20Seong-Woo%20Kim&entry.1292438233=%20%20Pedestrians%20Localization%20in%20Non-Line-of-Sight%20%28NLoS%29%20regions%20within%20urban%0Aenvironments%20poses%20a%20significant%20challenge%20for%20autonomous%20driving%20systems.%0AWhile%20mmWave%20radar%20has%20demonstrated%20potential%20for%20detecting%20objects%20in%20such%0Ascenarios%2C%20the%202D%20radar%20point%20cloud%20%28PCD%29%20data%20is%20susceptible%20to%20distortions%0Acaused%20by%20multipath%20reflections%2C%20making%20accurate%20spatial%20inference%20difficult.%0AAdditionally%2C%20although%20camera%20images%20provide%20high-resolution%20visual%0Ainformation%2C%20they%20lack%20depth%20perception%20and%20cannot%20directly%20observe%20objects%20in%0ANLoS%20regions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20interprets%20radar%0APCD%20through%20road%20layout%20inferred%20from%20camera%20for%20localization%20of%20NLoS%0Apedestrians.%20The%20proposed%20method%20leverages%20visual%20information%20from%20the%20camera%0Ato%20interpret%202D%20radar%20PCD%2C%20enabling%20spatial%20scene%20reconstruction.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20is%20validated%20through%20experiments%0Aconducted%20using%20a%20radar-camera%20system%20mounted%20on%20a%20real%20vehicle.%20The%0Alocalization%20performance%20is%20evaluated%20using%20a%20dataset%20collected%20in%20outdoor%20NLoS%0Adriving%20environments%2C%20demonstrating%20the%20practical%20applicability%20of%20the%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02348v1&entry.124074799=Read"},
{"title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching", "author": "Xiangzeng Liu and Chi Wang and Guanglu Shi and Xiaodong Zhang and Qiguang Miao and Miao Fan", "abstract": "  Local feature matching remains a fundamental challenge in computer vision.\nRecent Area to Point Matching (A2PM) methods have improved matching accuracy.\nHowever, existing research based on this framework relies on inefficient\npixel-level comparisons and complex graph matching that limit scalability. In\nthis work, we introduce the Semantic and Geometric-aware Descriptor Network\n(SGAD), which fundamentally rethinks area-based matching by generating highly\ndiscriminative area descriptors that enable direct matching without complex\ngraph optimization. This approach significantly improves both accuracy and\nefficiency of area matching. We further improve the performance of area\nmatching through a novel supervision strategy that decomposes the area matching\ntask into classification and ranking subtasks. Finally, we introduce the\nHierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping\nareas by analyzing containment graphs. SGAD demonstrates remarkable performance\ngains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive\nevaluations show consistent improvements across multiple point matchers:\nSGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy\n(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA\ndelivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2508.02278v1", "date": "2025-08-04", "relevancy": 2.8283, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6301}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGAD%3A%20Semantic%20and%20Geometric-aware%20Descriptor%20for%20Local%20Feature%20Matching&body=Title%3A%20SGAD%3A%20Semantic%20and%20Geometric-aware%20Descriptor%20for%20Local%20Feature%20Matching%0AAuthor%3A%20Xiangzeng%20Liu%20and%20Chi%20Wang%20and%20Guanglu%20Shi%20and%20Xiaodong%20Zhang%20and%20Qiguang%20Miao%20and%20Miao%20Fan%0AAbstract%3A%20%20%20Local%20feature%20matching%20remains%20a%20fundamental%20challenge%20in%20computer%20vision.%0ARecent%20Area%20to%20Point%20Matching%20%28A2PM%29%20methods%20have%20improved%20matching%20accuracy.%0AHowever%2C%20existing%20research%20based%20on%20this%20framework%20relies%20on%20inefficient%0Apixel-level%20comparisons%20and%20complex%20graph%20matching%20that%20limit%20scalability.%20In%0Athis%20work%2C%20we%20introduce%20the%20Semantic%20and%20Geometric-aware%20Descriptor%20Network%0A%28SGAD%29%2C%20which%20fundamentally%20rethinks%20area-based%20matching%20by%20generating%20highly%0Adiscriminative%20area%20descriptors%20that%20enable%20direct%20matching%20without%20complex%0Agraph%20optimization.%20This%20approach%20significantly%20improves%20both%20accuracy%20and%0Aefficiency%20of%20area%20matching.%20We%20further%20improve%20the%20performance%20of%20area%0Amatching%20through%20a%20novel%20supervision%20strategy%20that%20decomposes%20the%20area%20matching%0Atask%20into%20classification%20and%20ranking%20subtasks.%20Finally%2C%20we%20introduce%20the%0AHierarchical%20Containment%20Redundancy%20Filter%20%28HCRF%29%20to%20eliminate%20overlapping%0Aareas%20by%20analyzing%20containment%20graphs.%20SGAD%20demonstrates%20remarkable%20performance%0Agains%2C%20reducing%20runtime%20by%2060x%20%280.82s%20vs.%2060.23s%29%20compared%20to%20MESA.%20Extensive%0Aevaluations%20show%20consistent%20improvements%20across%20multiple%20point%20matchers%3A%0ASGAD%2BLoFTR%20reduces%20runtime%20compared%20to%20DKM%2C%20while%20achieving%20higher%20accuracy%0A%280.82s%20vs.%201.51s%2C%2065.98%20vs.%2061.11%29%20in%20outdoor%20pose%20estimation%2C%20and%20SGAD%2BROMA%0Adelivers%20%2B7.39%25%20AUC%405%7B%5Cdeg%7D%20in%20indoor%20pose%20estimation%2C%20establishing%20a%20new%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGAD%253A%2520Semantic%2520and%2520Geometric-aware%2520Descriptor%2520for%2520Local%2520Feature%2520Matching%26entry.906535625%3DXiangzeng%2520Liu%2520and%2520Chi%2520Wang%2520and%2520Guanglu%2520Shi%2520and%2520Xiaodong%2520Zhang%2520and%2520Qiguang%2520Miao%2520and%2520Miao%2520Fan%26entry.1292438233%3D%2520%2520Local%2520feature%2520matching%2520remains%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision.%250ARecent%2520Area%2520to%2520Point%2520Matching%2520%2528A2PM%2529%2520methods%2520have%2520improved%2520matching%2520accuracy.%250AHowever%252C%2520existing%2520research%2520based%2520on%2520this%2520framework%2520relies%2520on%2520inefficient%250Apixel-level%2520comparisons%2520and%2520complex%2520graph%2520matching%2520that%2520limit%2520scalability.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520the%2520Semantic%2520and%2520Geometric-aware%2520Descriptor%2520Network%250A%2528SGAD%2529%252C%2520which%2520fundamentally%2520rethinks%2520area-based%2520matching%2520by%2520generating%2520highly%250Adiscriminative%2520area%2520descriptors%2520that%2520enable%2520direct%2520matching%2520without%2520complex%250Agraph%2520optimization.%2520This%2520approach%2520significantly%2520improves%2520both%2520accuracy%2520and%250Aefficiency%2520of%2520area%2520matching.%2520We%2520further%2520improve%2520the%2520performance%2520of%2520area%250Amatching%2520through%2520a%2520novel%2520supervision%2520strategy%2520that%2520decomposes%2520the%2520area%2520matching%250Atask%2520into%2520classification%2520and%2520ranking%2520subtasks.%2520Finally%252C%2520we%2520introduce%2520the%250AHierarchical%2520Containment%2520Redundancy%2520Filter%2520%2528HCRF%2529%2520to%2520eliminate%2520overlapping%250Aareas%2520by%2520analyzing%2520containment%2520graphs.%2520SGAD%2520demonstrates%2520remarkable%2520performance%250Agains%252C%2520reducing%2520runtime%2520by%252060x%2520%25280.82s%2520vs.%252060.23s%2529%2520compared%2520to%2520MESA.%2520Extensive%250Aevaluations%2520show%2520consistent%2520improvements%2520across%2520multiple%2520point%2520matchers%253A%250ASGAD%252BLoFTR%2520reduces%2520runtime%2520compared%2520to%2520DKM%252C%2520while%2520achieving%2520higher%2520accuracy%250A%25280.82s%2520vs.%25201.51s%252C%252065.98%2520vs.%252061.11%2529%2520in%2520outdoor%2520pose%2520estimation%252C%2520and%2520SGAD%252BROMA%250Adelivers%2520%252B7.39%2525%2520AUC%25405%257B%255Cdeg%257D%2520in%2520indoor%2520pose%2520estimation%252C%2520establishing%2520a%2520new%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGAD%3A%20Semantic%20and%20Geometric-aware%20Descriptor%20for%20Local%20Feature%20Matching&entry.906535625=Xiangzeng%20Liu%20and%20Chi%20Wang%20and%20Guanglu%20Shi%20and%20Xiaodong%20Zhang%20and%20Qiguang%20Miao%20and%20Miao%20Fan&entry.1292438233=%20%20Local%20feature%20matching%20remains%20a%20fundamental%20challenge%20in%20computer%20vision.%0ARecent%20Area%20to%20Point%20Matching%20%28A2PM%29%20methods%20have%20improved%20matching%20accuracy.%0AHowever%2C%20existing%20research%20based%20on%20this%20framework%20relies%20on%20inefficient%0Apixel-level%20comparisons%20and%20complex%20graph%20matching%20that%20limit%20scalability.%20In%0Athis%20work%2C%20we%20introduce%20the%20Semantic%20and%20Geometric-aware%20Descriptor%20Network%0A%28SGAD%29%2C%20which%20fundamentally%20rethinks%20area-based%20matching%20by%20generating%20highly%0Adiscriminative%20area%20descriptors%20that%20enable%20direct%20matching%20without%20complex%0Agraph%20optimization.%20This%20approach%20significantly%20improves%20both%20accuracy%20and%0Aefficiency%20of%20area%20matching.%20We%20further%20improve%20the%20performance%20of%20area%0Amatching%20through%20a%20novel%20supervision%20strategy%20that%20decomposes%20the%20area%20matching%0Atask%20into%20classification%20and%20ranking%20subtasks.%20Finally%2C%20we%20introduce%20the%0AHierarchical%20Containment%20Redundancy%20Filter%20%28HCRF%29%20to%20eliminate%20overlapping%0Aareas%20by%20analyzing%20containment%20graphs.%20SGAD%20demonstrates%20remarkable%20performance%0Agains%2C%20reducing%20runtime%20by%2060x%20%280.82s%20vs.%2060.23s%29%20compared%20to%20MESA.%20Extensive%0Aevaluations%20show%20consistent%20improvements%20across%20multiple%20point%20matchers%3A%0ASGAD%2BLoFTR%20reduces%20runtime%20compared%20to%20DKM%2C%20while%20achieving%20higher%20accuracy%0A%280.82s%20vs.%201.51s%2C%2065.98%20vs.%2061.11%29%20in%20outdoor%20pose%20estimation%2C%20and%20SGAD%2BROMA%0Adelivers%20%2B7.39%25%20AUC%405%7B%5Cdeg%7D%20in%20indoor%20pose%20estimation%2C%20establishing%20a%20new%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02278v1&entry.124074799=Read"},
{"title": "Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth\n  Distillation from Single Images", "author": "Philipp Wulff and Felix Wimbauer and Dominik Muhle and Daniel Cremers", "abstract": "  Volumetric scene reconstruction from a single image is crucial for a broad\nrange of applications like autonomous driving and robotics. Recent volumetric\nreconstruction methods achieve impressive results, but generally require\nexpensive 3D ground truth or multi-view supervision. We propose to leverage\npre-trained 2D diffusion models and depth prediction models to generate\nsynthetic scene geometry from a single image. This can then be used to distill\na feed-forward scene reconstruction model. Our experiments on the challenging\nKITTI-360 and Waymo datasets demonstrate that our method matches or outperforms\nstate-of-the-art baselines that use multi-view supervision, and offers unique\nadvantages, for example regarding dynamic scenes.\n", "link": "http://arxiv.org/abs/2508.02323v1", "date": "2025-08-04", "relevancy": 2.7726, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6968}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6968}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dream-to-Recon%3A%20Monocular%203D%20Reconstruction%20with%20Diffusion-Depth%0A%20%20Distillation%20from%20Single%20Images&body=Title%3A%20Dream-to-Recon%3A%20Monocular%203D%20Reconstruction%20with%20Diffusion-Depth%0A%20%20Distillation%20from%20Single%20Images%0AAuthor%3A%20Philipp%20Wulff%20and%20Felix%20Wimbauer%20and%20Dominik%20Muhle%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Volumetric%20scene%20reconstruction%20from%20a%20single%20image%20is%20crucial%20for%20a%20broad%0Arange%20of%20applications%20like%20autonomous%20driving%20and%20robotics.%20Recent%20volumetric%0Areconstruction%20methods%20achieve%20impressive%20results%2C%20but%20generally%20require%0Aexpensive%203D%20ground%20truth%20or%20multi-view%20supervision.%20We%20propose%20to%20leverage%0Apre-trained%202D%20diffusion%20models%20and%20depth%20prediction%20models%20to%20generate%0Asynthetic%20scene%20geometry%20from%20a%20single%20image.%20This%20can%20then%20be%20used%20to%20distill%0Aa%20feed-forward%20scene%20reconstruction%20model.%20Our%20experiments%20on%20the%20challenging%0AKITTI-360%20and%20Waymo%20datasets%20demonstrate%20that%20our%20method%20matches%20or%20outperforms%0Astate-of-the-art%20baselines%20that%20use%20multi-view%20supervision%2C%20and%20offers%20unique%0Aadvantages%2C%20for%20example%20regarding%20dynamic%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDream-to-Recon%253A%2520Monocular%25203D%2520Reconstruction%2520with%2520Diffusion-Depth%250A%2520%2520Distillation%2520from%2520Single%2520Images%26entry.906535625%3DPhilipp%2520Wulff%2520and%2520Felix%2520Wimbauer%2520and%2520Dominik%2520Muhle%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Volumetric%2520scene%2520reconstruction%2520from%2520a%2520single%2520image%2520is%2520crucial%2520for%2520a%2520broad%250Arange%2520of%2520applications%2520like%2520autonomous%2520driving%2520and%2520robotics.%2520Recent%2520volumetric%250Areconstruction%2520methods%2520achieve%2520impressive%2520results%252C%2520but%2520generally%2520require%250Aexpensive%25203D%2520ground%2520truth%2520or%2520multi-view%2520supervision.%2520We%2520propose%2520to%2520leverage%250Apre-trained%25202D%2520diffusion%2520models%2520and%2520depth%2520prediction%2520models%2520to%2520generate%250Asynthetic%2520scene%2520geometry%2520from%2520a%2520single%2520image.%2520This%2520can%2520then%2520be%2520used%2520to%2520distill%250Aa%2520feed-forward%2520scene%2520reconstruction%2520model.%2520Our%2520experiments%2520on%2520the%2520challenging%250AKITTI-360%2520and%2520Waymo%2520datasets%2520demonstrate%2520that%2520our%2520method%2520matches%2520or%2520outperforms%250Astate-of-the-art%2520baselines%2520that%2520use%2520multi-view%2520supervision%252C%2520and%2520offers%2520unique%250Aadvantages%252C%2520for%2520example%2520regarding%2520dynamic%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dream-to-Recon%3A%20Monocular%203D%20Reconstruction%20with%20Diffusion-Depth%0A%20%20Distillation%20from%20Single%20Images&entry.906535625=Philipp%20Wulff%20and%20Felix%20Wimbauer%20and%20Dominik%20Muhle%20and%20Daniel%20Cremers&entry.1292438233=%20%20Volumetric%20scene%20reconstruction%20from%20a%20single%20image%20is%20crucial%20for%20a%20broad%0Arange%20of%20applications%20like%20autonomous%20driving%20and%20robotics.%20Recent%20volumetric%0Areconstruction%20methods%20achieve%20impressive%20results%2C%20but%20generally%20require%0Aexpensive%203D%20ground%20truth%20or%20multi-view%20supervision.%20We%20propose%20to%20leverage%0Apre-trained%202D%20diffusion%20models%20and%20depth%20prediction%20models%20to%20generate%0Asynthetic%20scene%20geometry%20from%20a%20single%20image.%20This%20can%20then%20be%20used%20to%20distill%0Aa%20feed-forward%20scene%20reconstruction%20model.%20Our%20experiments%20on%20the%20challenging%0AKITTI-360%20and%20Waymo%20datasets%20demonstrate%20that%20our%20method%20matches%20or%20outperforms%0Astate-of-the-art%20baselines%20that%20use%20multi-view%20supervision%2C%20and%20offers%20unique%0Aadvantages%2C%20for%20example%20regarding%20dynamic%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02323v1&entry.124074799=Read"},
{"title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture", "author": "Thomas F Burns and Tomoki Fukai and Christopher J Earls", "abstract": "  Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales.\n", "link": "http://arxiv.org/abs/2412.15113v2", "date": "2025-08-04", "relevancy": 2.7203, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture&body=Title%3A%20Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture%0AAuthor%3A%20Thomas%20F%20Burns%20and%20Tomoki%20Fukai%20and%20Christopher%20J%20Earls%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20an%20impressive%20ability%20to%20utilise%0Ainformation%20within%20the%20context%20of%20their%20input%20sequences%20to%20appropriately%0Arespond%20to%20data%20unseen%20by%20the%20LLM%20during%20its%20training%20procedure.%20This%20ability%0Ais%20known%20as%20in-context%20learning%20%28ICL%29.%20Humans%20and%20non-human%20animals%20demonstrate%0Asimilar%20abilities%2C%20however%20their%20neural%20architectures%20differ%20substantially%20from%0ALLMs.%20Despite%20this%2C%20a%20critical%20component%20within%20LLMs%2C%20the%20attention%20mechanism%2C%0Aresembles%20modern%20associative%20memory%20models%2C%20widely%20used%20in%20and%20influenced%20by%0Athe%20computational%20neuroscience%20community%20to%20model%20biological%20memory%20systems.%0AUsing%20this%20connection%2C%20we%20introduce%20an%20associative%20memory%20model%20capable%20of%0Aperforming%20ICL.%20We%20use%20this%20as%20inspiration%20for%20a%20novel%20residual%20stream%0Aarchitecture%20which%20allows%20information%20to%20directly%20flow%20between%20attention%20heads.%0AWe%20test%20this%20architecture%20during%20training%20within%20a%20two-layer%20Transformer%20and%0Ashow%20its%20ICL%20abilities%20manifest%20more%20quickly%20than%20without%20this%20modification.%20We%0Athen%20apply%20our%20architecture%20in%20small%20language%20models%20with%208%20million%20and%201%0Abillion%20parameters%2C%20focusing%20on%20attention%20head%20values%2C%20with%20results%20also%0Aindicating%20improved%20performance%20at%20these%20larger%20and%20more%20naturalistic%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssociative%2520memory%2520inspires%2520improvements%2520for%2520in-context%2520learning%2520using%2520a%250A%2520%2520novel%2520attention%2520residual%2520stream%2520architecture%26entry.906535625%3DThomas%2520F%2520Burns%2520and%2520Tomoki%2520Fukai%2520and%2520Christopher%2520J%2520Earls%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520an%2520impressive%2520ability%2520to%2520utilise%250Ainformation%2520within%2520the%2520context%2520of%2520their%2520input%2520sequences%2520to%2520appropriately%250Arespond%2520to%2520data%2520unseen%2520by%2520the%2520LLM%2520during%2520its%2520training%2520procedure.%2520This%2520ability%250Ais%2520known%2520as%2520in-context%2520learning%2520%2528ICL%2529.%2520Humans%2520and%2520non-human%2520animals%2520demonstrate%250Asimilar%2520abilities%252C%2520however%2520their%2520neural%2520architectures%2520differ%2520substantially%2520from%250ALLMs.%2520Despite%2520this%252C%2520a%2520critical%2520component%2520within%2520LLMs%252C%2520the%2520attention%2520mechanism%252C%250Aresembles%2520modern%2520associative%2520memory%2520models%252C%2520widely%2520used%2520in%2520and%2520influenced%2520by%250Athe%2520computational%2520neuroscience%2520community%2520to%2520model%2520biological%2520memory%2520systems.%250AUsing%2520this%2520connection%252C%2520we%2520introduce%2520an%2520associative%2520memory%2520model%2520capable%2520of%250Aperforming%2520ICL.%2520We%2520use%2520this%2520as%2520inspiration%2520for%2520a%2520novel%2520residual%2520stream%250Aarchitecture%2520which%2520allows%2520information%2520to%2520directly%2520flow%2520between%2520attention%2520heads.%250AWe%2520test%2520this%2520architecture%2520during%2520training%2520within%2520a%2520two-layer%2520Transformer%2520and%250Ashow%2520its%2520ICL%2520abilities%2520manifest%2520more%2520quickly%2520than%2520without%2520this%2520modification.%2520We%250Athen%2520apply%2520our%2520architecture%2520in%2520small%2520language%2520models%2520with%25208%2520million%2520and%25201%250Abillion%2520parameters%252C%2520focusing%2520on%2520attention%2520head%2520values%252C%2520with%2520results%2520also%250Aindicating%2520improved%2520performance%2520at%2520these%2520larger%2520and%2520more%2520naturalistic%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture&entry.906535625=Thomas%20F%20Burns%20and%20Tomoki%20Fukai%20and%20Christopher%20J%20Earls&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20an%20impressive%20ability%20to%20utilise%0Ainformation%20within%20the%20context%20of%20their%20input%20sequences%20to%20appropriately%0Arespond%20to%20data%20unseen%20by%20the%20LLM%20during%20its%20training%20procedure.%20This%20ability%0Ais%20known%20as%20in-context%20learning%20%28ICL%29.%20Humans%20and%20non-human%20animals%20demonstrate%0Asimilar%20abilities%2C%20however%20their%20neural%20architectures%20differ%20substantially%20from%0ALLMs.%20Despite%20this%2C%20a%20critical%20component%20within%20LLMs%2C%20the%20attention%20mechanism%2C%0Aresembles%20modern%20associative%20memory%20models%2C%20widely%20used%20in%20and%20influenced%20by%0Athe%20computational%20neuroscience%20community%20to%20model%20biological%20memory%20systems.%0AUsing%20this%20connection%2C%20we%20introduce%20an%20associative%20memory%20model%20capable%20of%0Aperforming%20ICL.%20We%20use%20this%20as%20inspiration%20for%20a%20novel%20residual%20stream%0Aarchitecture%20which%20allows%20information%20to%20directly%20flow%20between%20attention%20heads.%0AWe%20test%20this%20architecture%20during%20training%20within%20a%20two-layer%20Transformer%20and%0Ashow%20its%20ICL%20abilities%20manifest%20more%20quickly%20than%20without%20this%20modification.%20We%0Athen%20apply%20our%20architecture%20in%20small%20language%20models%20with%208%20million%20and%201%0Abillion%20parameters%2C%20focusing%20on%20attention%20head%20values%2C%20with%20results%20also%0Aindicating%20improved%20performance%20at%20these%20larger%20and%20more%20naturalistic%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15113v2&entry.124074799=Read"},
{"title": "Adaptive Riemannian Graph Neural Networks", "author": "Xudong Wang and Tongxin Li and Chris Ding and Jicong Fan", "abstract": "  Graph data often exhibits complex geometric heterogeneity, where structures\nwith varying local curvature, such as tree-like hierarchies and dense\ncommunities, coexist within a single network. Existing geometric GNNs, which\nembed graphs into single fixed-curvature manifolds or discrete product spaces,\nstruggle to capture this diversity. We introduce Adaptive Riemannian Graph\nNeural Networks (ARGNN), a novel framework that learns a continuous and\nanisotropic Riemannian metric tensor field over the graph. It allows each node\nto determine its optimal local geometry, enabling the model to fluidly adapt to\nthe graph's structural landscape. Our core innovation is an efficient\nparameterization of the node-wise metric tensor, specializing to a learnable\ndiagonal form that captures directional geometric information while maintaining\ncomputational tractability. To ensure geometric regularity and stable training,\nwe integrate a Ricci flow-inspired regularization that smooths the learned\nmanifold. Theoretically, we establish the rigorous geometric evolution\nconvergence guarantee for ARGNN and provide a continuous generalization that\nunifies prior fixed or mixed-curvature GNNs. Empirically, our method\ndemonstrates superior performance on both homophilic and heterophilic benchmark\ndatasets with the ability to capture diverse structures adaptively. Moreover,\nthe learned geometries both offer interpretable insights into the underlying\ngraph structure and empirically corroborate our theoretical analysis.\n", "link": "http://arxiv.org/abs/2508.02600v1", "date": "2025-08-04", "relevancy": 2.6415, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.557}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5253}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Riemannian%20Graph%20Neural%20Networks&body=Title%3A%20Adaptive%20Riemannian%20Graph%20Neural%20Networks%0AAuthor%3A%20Xudong%20Wang%20and%20Tongxin%20Li%20and%20Chris%20Ding%20and%20Jicong%20Fan%0AAbstract%3A%20%20%20Graph%20data%20often%20exhibits%20complex%20geometric%20heterogeneity%2C%20where%20structures%0Awith%20varying%20local%20curvature%2C%20such%20as%20tree-like%20hierarchies%20and%20dense%0Acommunities%2C%20coexist%20within%20a%20single%20network.%20Existing%20geometric%20GNNs%2C%20which%0Aembed%20graphs%20into%20single%20fixed-curvature%20manifolds%20or%20discrete%20product%20spaces%2C%0Astruggle%20to%20capture%20this%20diversity.%20We%20introduce%20Adaptive%20Riemannian%20Graph%0ANeural%20Networks%20%28ARGNN%29%2C%20a%20novel%20framework%20that%20learns%20a%20continuous%20and%0Aanisotropic%20Riemannian%20metric%20tensor%20field%20over%20the%20graph.%20It%20allows%20each%20node%0Ato%20determine%20its%20optimal%20local%20geometry%2C%20enabling%20the%20model%20to%20fluidly%20adapt%20to%0Athe%20graph%27s%20structural%20landscape.%20Our%20core%20innovation%20is%20an%20efficient%0Aparameterization%20of%20the%20node-wise%20metric%20tensor%2C%20specializing%20to%20a%20learnable%0Adiagonal%20form%20that%20captures%20directional%20geometric%20information%20while%20maintaining%0Acomputational%20tractability.%20To%20ensure%20geometric%20regularity%20and%20stable%20training%2C%0Awe%20integrate%20a%20Ricci%20flow-inspired%20regularization%20that%20smooths%20the%20learned%0Amanifold.%20Theoretically%2C%20we%20establish%20the%20rigorous%20geometric%20evolution%0Aconvergence%20guarantee%20for%20ARGNN%20and%20provide%20a%20continuous%20generalization%20that%0Aunifies%20prior%20fixed%20or%20mixed-curvature%20GNNs.%20Empirically%2C%20our%20method%0Ademonstrates%20superior%20performance%20on%20both%20homophilic%20and%20heterophilic%20benchmark%0Adatasets%20with%20the%20ability%20to%20capture%20diverse%20structures%20adaptively.%20Moreover%2C%0Athe%20learned%20geometries%20both%20offer%20interpretable%20insights%20into%20the%20underlying%0Agraph%20structure%20and%20empirically%20corroborate%20our%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Riemannian%2520Graph%2520Neural%2520Networks%26entry.906535625%3DXudong%2520Wang%2520and%2520Tongxin%2520Li%2520and%2520Chris%2520Ding%2520and%2520Jicong%2520Fan%26entry.1292438233%3D%2520%2520Graph%2520data%2520often%2520exhibits%2520complex%2520geometric%2520heterogeneity%252C%2520where%2520structures%250Awith%2520varying%2520local%2520curvature%252C%2520such%2520as%2520tree-like%2520hierarchies%2520and%2520dense%250Acommunities%252C%2520coexist%2520within%2520a%2520single%2520network.%2520Existing%2520geometric%2520GNNs%252C%2520which%250Aembed%2520graphs%2520into%2520single%2520fixed-curvature%2520manifolds%2520or%2520discrete%2520product%2520spaces%252C%250Astruggle%2520to%2520capture%2520this%2520diversity.%2520We%2520introduce%2520Adaptive%2520Riemannian%2520Graph%250ANeural%2520Networks%2520%2528ARGNN%2529%252C%2520a%2520novel%2520framework%2520that%2520learns%2520a%2520continuous%2520and%250Aanisotropic%2520Riemannian%2520metric%2520tensor%2520field%2520over%2520the%2520graph.%2520It%2520allows%2520each%2520node%250Ato%2520determine%2520its%2520optimal%2520local%2520geometry%252C%2520enabling%2520the%2520model%2520to%2520fluidly%2520adapt%2520to%250Athe%2520graph%2527s%2520structural%2520landscape.%2520Our%2520core%2520innovation%2520is%2520an%2520efficient%250Aparameterization%2520of%2520the%2520node-wise%2520metric%2520tensor%252C%2520specializing%2520to%2520a%2520learnable%250Adiagonal%2520form%2520that%2520captures%2520directional%2520geometric%2520information%2520while%2520maintaining%250Acomputational%2520tractability.%2520To%2520ensure%2520geometric%2520regularity%2520and%2520stable%2520training%252C%250Awe%2520integrate%2520a%2520Ricci%2520flow-inspired%2520regularization%2520that%2520smooths%2520the%2520learned%250Amanifold.%2520Theoretically%252C%2520we%2520establish%2520the%2520rigorous%2520geometric%2520evolution%250Aconvergence%2520guarantee%2520for%2520ARGNN%2520and%2520provide%2520a%2520continuous%2520generalization%2520that%250Aunifies%2520prior%2520fixed%2520or%2520mixed-curvature%2520GNNs.%2520Empirically%252C%2520our%2520method%250Ademonstrates%2520superior%2520performance%2520on%2520both%2520homophilic%2520and%2520heterophilic%2520benchmark%250Adatasets%2520with%2520the%2520ability%2520to%2520capture%2520diverse%2520structures%2520adaptively.%2520Moreover%252C%250Athe%2520learned%2520geometries%2520both%2520offer%2520interpretable%2520insights%2520into%2520the%2520underlying%250Agraph%2520structure%2520and%2520empirically%2520corroborate%2520our%2520theoretical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Riemannian%20Graph%20Neural%20Networks&entry.906535625=Xudong%20Wang%20and%20Tongxin%20Li%20and%20Chris%20Ding%20and%20Jicong%20Fan&entry.1292438233=%20%20Graph%20data%20often%20exhibits%20complex%20geometric%20heterogeneity%2C%20where%20structures%0Awith%20varying%20local%20curvature%2C%20such%20as%20tree-like%20hierarchies%20and%20dense%0Acommunities%2C%20coexist%20within%20a%20single%20network.%20Existing%20geometric%20GNNs%2C%20which%0Aembed%20graphs%20into%20single%20fixed-curvature%20manifolds%20or%20discrete%20product%20spaces%2C%0Astruggle%20to%20capture%20this%20diversity.%20We%20introduce%20Adaptive%20Riemannian%20Graph%0ANeural%20Networks%20%28ARGNN%29%2C%20a%20novel%20framework%20that%20learns%20a%20continuous%20and%0Aanisotropic%20Riemannian%20metric%20tensor%20field%20over%20the%20graph.%20It%20allows%20each%20node%0Ato%20determine%20its%20optimal%20local%20geometry%2C%20enabling%20the%20model%20to%20fluidly%20adapt%20to%0Athe%20graph%27s%20structural%20landscape.%20Our%20core%20innovation%20is%20an%20efficient%0Aparameterization%20of%20the%20node-wise%20metric%20tensor%2C%20specializing%20to%20a%20learnable%0Adiagonal%20form%20that%20captures%20directional%20geometric%20information%20while%20maintaining%0Acomputational%20tractability.%20To%20ensure%20geometric%20regularity%20and%20stable%20training%2C%0Awe%20integrate%20a%20Ricci%20flow-inspired%20regularization%20that%20smooths%20the%20learned%0Amanifold.%20Theoretically%2C%20we%20establish%20the%20rigorous%20geometric%20evolution%0Aconvergence%20guarantee%20for%20ARGNN%20and%20provide%20a%20continuous%20generalization%20that%0Aunifies%20prior%20fixed%20or%20mixed-curvature%20GNNs.%20Empirically%2C%20our%20method%0Ademonstrates%20superior%20performance%20on%20both%20homophilic%20and%20heterophilic%20benchmark%0Adatasets%20with%20the%20ability%20to%20capture%20diverse%20structures%20adaptively.%20Moreover%2C%0Athe%20learned%20geometries%20both%20offer%20interpretable%20insights%20into%20the%20underlying%0Agraph%20structure%20and%20empirically%20corroborate%20our%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02600v1&entry.124074799=Read"},
{"title": "Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual\n  Representations", "author": "T. Aleksandra Ma and Sile Yin and Li-Chia Yang and Shuo Zhang", "abstract": "  Speech enhancement in audio-only settings remains challenging, particularly\nin the presence of interfering speakers. This paper presents a simple yet\neffective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which\nisolates and enhances the on-screen target speaker while suppressing\ninterfering speakers and background noise. We investigate how visual embeddings\nlearned from audio-visual speech recognition (AVSR) and active speaker\ndetection (ASD) contribute to AVSE across different SNR conditions and numbers\nof interfering speakers. Our results show concatenating embeddings from AVSR\nand ASD models provides the greatest improvement in low-SNR, multi-speaker\nenvironments, while AVSR embeddings alone perform best in noise-only scenarios.\nIn addition, we develop a real-time streaming system that operates on a\ncomputer CPU and we provide a video demonstration and code repository. To our\nknowledge, this is the first open-source implementation of a real-time AVSE\nsystem.\n", "link": "http://arxiv.org/abs/2507.21448v2", "date": "2025-08-04", "relevancy": 2.6203, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Audio-Visual%20Speech%20Enhancement%20Using%20Pre-trained%20Visual%0A%20%20Representations&body=Title%3A%20Real-Time%20Audio-Visual%20Speech%20Enhancement%20Using%20Pre-trained%20Visual%0A%20%20Representations%0AAuthor%3A%20T.%20Aleksandra%20Ma%20and%20Sile%20Yin%20and%20Li-Chia%20Yang%20and%20Shuo%20Zhang%0AAbstract%3A%20%20%20Speech%20enhancement%20in%20audio-only%20settings%20remains%20challenging%2C%20particularly%0Ain%20the%20presence%20of%20interfering%20speakers.%20This%20paper%20presents%20a%20simple%20yet%0Aeffective%20real-time%20audio-visual%20speech%20enhancement%20%28AVSE%29%20system%2C%20RAVEN%2C%20which%0Aisolates%20and%20enhances%20the%20on-screen%20target%20speaker%20while%20suppressing%0Ainterfering%20speakers%20and%20background%20noise.%20We%20investigate%20how%20visual%20embeddings%0Alearned%20from%20audio-visual%20speech%20recognition%20%28AVSR%29%20and%20active%20speaker%0Adetection%20%28ASD%29%20contribute%20to%20AVSE%20across%20different%20SNR%20conditions%20and%20numbers%0Aof%20interfering%20speakers.%20Our%20results%20show%20concatenating%20embeddings%20from%20AVSR%0Aand%20ASD%20models%20provides%20the%20greatest%20improvement%20in%20low-SNR%2C%20multi-speaker%0Aenvironments%2C%20while%20AVSR%20embeddings%20alone%20perform%20best%20in%20noise-only%20scenarios.%0AIn%20addition%2C%20we%20develop%20a%20real-time%20streaming%20system%20that%20operates%20on%20a%0Acomputer%20CPU%20and%20we%20provide%20a%20video%20demonstration%20and%20code%20repository.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20open-source%20implementation%20of%20a%20real-time%20AVSE%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Audio-Visual%2520Speech%2520Enhancement%2520Using%2520Pre-trained%2520Visual%250A%2520%2520Representations%26entry.906535625%3DT.%2520Aleksandra%2520Ma%2520and%2520Sile%2520Yin%2520and%2520Li-Chia%2520Yang%2520and%2520Shuo%2520Zhang%26entry.1292438233%3D%2520%2520Speech%2520enhancement%2520in%2520audio-only%2520settings%2520remains%2520challenging%252C%2520particularly%250Ain%2520the%2520presence%2520of%2520interfering%2520speakers.%2520This%2520paper%2520presents%2520a%2520simple%2520yet%250Aeffective%2520real-time%2520audio-visual%2520speech%2520enhancement%2520%2528AVSE%2529%2520system%252C%2520RAVEN%252C%2520which%250Aisolates%2520and%2520enhances%2520the%2520on-screen%2520target%2520speaker%2520while%2520suppressing%250Ainterfering%2520speakers%2520and%2520background%2520noise.%2520We%2520investigate%2520how%2520visual%2520embeddings%250Alearned%2520from%2520audio-visual%2520speech%2520recognition%2520%2528AVSR%2529%2520and%2520active%2520speaker%250Adetection%2520%2528ASD%2529%2520contribute%2520to%2520AVSE%2520across%2520different%2520SNR%2520conditions%2520and%2520numbers%250Aof%2520interfering%2520speakers.%2520Our%2520results%2520show%2520concatenating%2520embeddings%2520from%2520AVSR%250Aand%2520ASD%2520models%2520provides%2520the%2520greatest%2520improvement%2520in%2520low-SNR%252C%2520multi-speaker%250Aenvironments%252C%2520while%2520AVSR%2520embeddings%2520alone%2520perform%2520best%2520in%2520noise-only%2520scenarios.%250AIn%2520addition%252C%2520we%2520develop%2520a%2520real-time%2520streaming%2520system%2520that%2520operates%2520on%2520a%250Acomputer%2520CPU%2520and%2520we%2520provide%2520a%2520video%2520demonstration%2520and%2520code%2520repository.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520open-source%2520implementation%2520of%2520a%2520real-time%2520AVSE%250Asystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Audio-Visual%20Speech%20Enhancement%20Using%20Pre-trained%20Visual%0A%20%20Representations&entry.906535625=T.%20Aleksandra%20Ma%20and%20Sile%20Yin%20and%20Li-Chia%20Yang%20and%20Shuo%20Zhang&entry.1292438233=%20%20Speech%20enhancement%20in%20audio-only%20settings%20remains%20challenging%2C%20particularly%0Ain%20the%20presence%20of%20interfering%20speakers.%20This%20paper%20presents%20a%20simple%20yet%0Aeffective%20real-time%20audio-visual%20speech%20enhancement%20%28AVSE%29%20system%2C%20RAVEN%2C%20which%0Aisolates%20and%20enhances%20the%20on-screen%20target%20speaker%20while%20suppressing%0Ainterfering%20speakers%20and%20background%20noise.%20We%20investigate%20how%20visual%20embeddings%0Alearned%20from%20audio-visual%20speech%20recognition%20%28AVSR%29%20and%20active%20speaker%0Adetection%20%28ASD%29%20contribute%20to%20AVSE%20across%20different%20SNR%20conditions%20and%20numbers%0Aof%20interfering%20speakers.%20Our%20results%20show%20concatenating%20embeddings%20from%20AVSR%0Aand%20ASD%20models%20provides%20the%20greatest%20improvement%20in%20low-SNR%2C%20multi-speaker%0Aenvironments%2C%20while%20AVSR%20embeddings%20alone%20perform%20best%20in%20noise-only%20scenarios.%0AIn%20addition%2C%20we%20develop%20a%20real-time%20streaming%20system%20that%20operates%20on%20a%0Acomputer%20CPU%20and%20we%20provide%20a%20video%20demonstration%20and%20code%20repository.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20open-source%20implementation%20of%20a%20real-time%20AVSE%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21448v2&entry.124074799=Read"},
{"title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General\n  Knowledge Subtraction", "author": "Mohammadtaha Bagherifard and Sahar Rajabi and Ali Edalat and Yadollah Yaghoobzadeh", "abstract": "  Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.\n", "link": "http://arxiv.org/abs/2505.10939v2", "date": "2025-08-04", "relevancy": 2.6162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenKnowSub%3A%20Improving%20Modularity%20and%20Reusability%20of%20LLMs%20through%20General%0A%20%20Knowledge%20Subtraction&body=Title%3A%20GenKnowSub%3A%20Improving%20Modularity%20and%20Reusability%20of%20LLMs%20through%20General%0A%20%20Knowledge%20Subtraction%0AAuthor%3A%20Mohammadtaha%20Bagherifard%20and%20Sahar%20Rajabi%20and%20Ali%20Edalat%20and%20Yadollah%20Yaghoobzadeh%0AAbstract%3A%20%20%20Large%20language%20models%20often%20struggle%20with%20zero-shot%20generalization%2C%20and%0Aseveral%20modular%20approaches%20have%20been%20proposed%20to%20address%20this%20challenge.%20Yet%2C%0Awe%20hypothesize%20that%20a%20key%20limitation%20remains%3A%20the%20entanglement%20of%20general%0Aknowledge%20and%20task-specific%20adaptations.%20To%20overcome%20this%2C%20we%20propose%20a%20modular%0Aframework%20that%20disentangles%20these%20components%20by%20constructing%20a%20library%20of%0Atask-specific%20LoRA%20modules%20alongside%20a%20general-domain%20LoRA.%20By%20subtracting%20this%0Ageneral%20knowledge%20component%20from%20each%20task-specific%20module%2C%20we%20obtain%20residual%0Amodules%20that%20focus%20more%20exclusively%20on%20task-relevant%20information%2C%20a%20method%20we%0Acall%20general%20knowledge%20subtraction%20%28GenKnowSub%29.%20Leveraging%20the%20refined%0Atask-specific%20modules%20and%20the%20Arrow%20routing%20algorithm%0A%5Ccitep%7Bostapenko2024towards%7D%2C%20we%20dynamically%20select%20and%20combine%20modules%20for%20new%0Ainputs%20without%20additional%20training.%20Our%20studies%20on%20the%20Phi-3%20model%20and%20standard%0AArrow%20as%20baselines%20reveal%20that%20using%20general%20knowledge%20LoRAs%20derived%20from%0Adiverse%20languages%2C%20including%20English%2C%20French%2C%20and%20German%2C%20yields%20consistent%0Aperformance%20gains%20in%20both%20monolingual%20and%20cross-lingual%20settings%20across%20a%20wide%0Aset%20of%20benchmarks.%20Further%20experiments%20on%20Phi-2%20demonstrate%20how%20GenKnowSub%0Ageneralizes%20to%20weaker%20LLMs.%20The%20complete%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/saharsamr/Modular-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenKnowSub%253A%2520Improving%2520Modularity%2520and%2520Reusability%2520of%2520LLMs%2520through%2520General%250A%2520%2520Knowledge%2520Subtraction%26entry.906535625%3DMohammadtaha%2520Bagherifard%2520and%2520Sahar%2520Rajabi%2520and%2520Ali%2520Edalat%2520and%2520Yadollah%2520Yaghoobzadeh%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520often%2520struggle%2520with%2520zero-shot%2520generalization%252C%2520and%250Aseveral%2520modular%2520approaches%2520have%2520been%2520proposed%2520to%2520address%2520this%2520challenge.%2520Yet%252C%250Awe%2520hypothesize%2520that%2520a%2520key%2520limitation%2520remains%253A%2520the%2520entanglement%2520of%2520general%250Aknowledge%2520and%2520task-specific%2520adaptations.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520modular%250Aframework%2520that%2520disentangles%2520these%2520components%2520by%2520constructing%2520a%2520library%2520of%250Atask-specific%2520LoRA%2520modules%2520alongside%2520a%2520general-domain%2520LoRA.%2520By%2520subtracting%2520this%250Ageneral%2520knowledge%2520component%2520from%2520each%2520task-specific%2520module%252C%2520we%2520obtain%2520residual%250Amodules%2520that%2520focus%2520more%2520exclusively%2520on%2520task-relevant%2520information%252C%2520a%2520method%2520we%250Acall%2520general%2520knowledge%2520subtraction%2520%2528GenKnowSub%2529.%2520Leveraging%2520the%2520refined%250Atask-specific%2520modules%2520and%2520the%2520Arrow%2520routing%2520algorithm%250A%255Ccitep%257Bostapenko2024towards%257D%252C%2520we%2520dynamically%2520select%2520and%2520combine%2520modules%2520for%2520new%250Ainputs%2520without%2520additional%2520training.%2520Our%2520studies%2520on%2520the%2520Phi-3%2520model%2520and%2520standard%250AArrow%2520as%2520baselines%2520reveal%2520that%2520using%2520general%2520knowledge%2520LoRAs%2520derived%2520from%250Adiverse%2520languages%252C%2520including%2520English%252C%2520French%252C%2520and%2520German%252C%2520yields%2520consistent%250Aperformance%2520gains%2520in%2520both%2520monolingual%2520and%2520cross-lingual%2520settings%2520across%2520a%2520wide%250Aset%2520of%2520benchmarks.%2520Further%2520experiments%2520on%2520Phi-2%2520demonstrate%2520how%2520GenKnowSub%250Ageneralizes%2520to%2520weaker%2520LLMs.%2520The%2520complete%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/saharsamr/Modular-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenKnowSub%3A%20Improving%20Modularity%20and%20Reusability%20of%20LLMs%20through%20General%0A%20%20Knowledge%20Subtraction&entry.906535625=Mohammadtaha%20Bagherifard%20and%20Sahar%20Rajabi%20and%20Ali%20Edalat%20and%20Yadollah%20Yaghoobzadeh&entry.1292438233=%20%20Large%20language%20models%20often%20struggle%20with%20zero-shot%20generalization%2C%20and%0Aseveral%20modular%20approaches%20have%20been%20proposed%20to%20address%20this%20challenge.%20Yet%2C%0Awe%20hypothesize%20that%20a%20key%20limitation%20remains%3A%20the%20entanglement%20of%20general%0Aknowledge%20and%20task-specific%20adaptations.%20To%20overcome%20this%2C%20we%20propose%20a%20modular%0Aframework%20that%20disentangles%20these%20components%20by%20constructing%20a%20library%20of%0Atask-specific%20LoRA%20modules%20alongside%20a%20general-domain%20LoRA.%20By%20subtracting%20this%0Ageneral%20knowledge%20component%20from%20each%20task-specific%20module%2C%20we%20obtain%20residual%0Amodules%20that%20focus%20more%20exclusively%20on%20task-relevant%20information%2C%20a%20method%20we%0Acall%20general%20knowledge%20subtraction%20%28GenKnowSub%29.%20Leveraging%20the%20refined%0Atask-specific%20modules%20and%20the%20Arrow%20routing%20algorithm%0A%5Ccitep%7Bostapenko2024towards%7D%2C%20we%20dynamically%20select%20and%20combine%20modules%20for%20new%0Ainputs%20without%20additional%20training.%20Our%20studies%20on%20the%20Phi-3%20model%20and%20standard%0AArrow%20as%20baselines%20reveal%20that%20using%20general%20knowledge%20LoRAs%20derived%20from%0Adiverse%20languages%2C%20including%20English%2C%20French%2C%20and%20German%2C%20yields%20consistent%0Aperformance%20gains%20in%20both%20monolingual%20and%20cross-lingual%20settings%20across%20a%20wide%0Aset%20of%20benchmarks.%20Further%20experiments%20on%20Phi-2%20demonstrate%20how%20GenKnowSub%0Ageneralizes%20to%20weaker%20LLMs.%20The%20complete%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/saharsamr/Modular-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10939v2&entry.124074799=Read"},
{"title": "Slicing the Gaussian Mixture Wasserstein Distance", "author": "Moritz Piening and Robert Beinert", "abstract": "  Gaussian mixture models (GMMs) are widely used in machine learning for tasks\nsuch as clustering, classification, image reconstruction, and generative\nmodeling. A key challenge in working with GMMs is defining a computationally\nefficient and geometrically meaningful metric. The mixture Wasserstein (MW)\ndistance adapts the Wasserstein metric to GMMs and has been applied in various\ndomains, including domain adaptation, dataset comparison, and reinforcement\nlearning. However, its high computational cost -- arising from repeated\nWasserstein distance computations involving matrix square root estimations and\nan expensive linear program -- limits its scalability to high-dimensional and\nlarge-scale problems. To address this, we propose multiple novel slicing-based\napproximations to the MW distance that significantly reduce computational\ncomplexity while preserving key optimal transport properties. From a\ntheoretical viewpoint, we establish several weak and strong equivalences\nbetween the introduced metrics, and show the relations to the original MW\ndistance and the well-established sliced Wasserstein distance. Furthermore, we\nvalidate the effectiveness of our approach through numerical experiments,\ndemonstrating computational efficiency and applications in clustering,\nperceptual image comparison, and GMM minimization\n", "link": "http://arxiv.org/abs/2504.08544v2", "date": "2025-08-04", "relevancy": 2.5925, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5294}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5189}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slicing%20the%20Gaussian%20Mixture%20Wasserstein%20Distance&body=Title%3A%20Slicing%20the%20Gaussian%20Mixture%20Wasserstein%20Distance%0AAuthor%3A%20Moritz%20Piening%20and%20Robert%20Beinert%0AAbstract%3A%20%20%20Gaussian%20mixture%20models%20%28GMMs%29%20are%20widely%20used%20in%20machine%20learning%20for%20tasks%0Asuch%20as%20clustering%2C%20classification%2C%20image%20reconstruction%2C%20and%20generative%0Amodeling.%20A%20key%20challenge%20in%20working%20with%20GMMs%20is%20defining%20a%20computationally%0Aefficient%20and%20geometrically%20meaningful%20metric.%20The%20mixture%20Wasserstein%20%28MW%29%0Adistance%20adapts%20the%20Wasserstein%20metric%20to%20GMMs%20and%20has%20been%20applied%20in%20various%0Adomains%2C%20including%20domain%20adaptation%2C%20dataset%20comparison%2C%20and%20reinforcement%0Alearning.%20However%2C%20its%20high%20computational%20cost%20--%20arising%20from%20repeated%0AWasserstein%20distance%20computations%20involving%20matrix%20square%20root%20estimations%20and%0Aan%20expensive%20linear%20program%20--%20limits%20its%20scalability%20to%20high-dimensional%20and%0Alarge-scale%20problems.%20To%20address%20this%2C%20we%20propose%20multiple%20novel%20slicing-based%0Aapproximations%20to%20the%20MW%20distance%20that%20significantly%20reduce%20computational%0Acomplexity%20while%20preserving%20key%20optimal%20transport%20properties.%20From%20a%0Atheoretical%20viewpoint%2C%20we%20establish%20several%20weak%20and%20strong%20equivalences%0Abetween%20the%20introduced%20metrics%2C%20and%20show%20the%20relations%20to%20the%20original%20MW%0Adistance%20and%20the%20well-established%20sliced%20Wasserstein%20distance.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20through%20numerical%20experiments%2C%0Ademonstrating%20computational%20efficiency%20and%20applications%20in%20clustering%2C%0Aperceptual%20image%20comparison%2C%20and%20GMM%20minimization%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlicing%2520the%2520Gaussian%2520Mixture%2520Wasserstein%2520Distance%26entry.906535625%3DMoritz%2520Piening%2520and%2520Robert%2520Beinert%26entry.1292438233%3D%2520%2520Gaussian%2520mixture%2520models%2520%2528GMMs%2529%2520are%2520widely%2520used%2520in%2520machine%2520learning%2520for%2520tasks%250Asuch%2520as%2520clustering%252C%2520classification%252C%2520image%2520reconstruction%252C%2520and%2520generative%250Amodeling.%2520A%2520key%2520challenge%2520in%2520working%2520with%2520GMMs%2520is%2520defining%2520a%2520computationally%250Aefficient%2520and%2520geometrically%2520meaningful%2520metric.%2520The%2520mixture%2520Wasserstein%2520%2528MW%2529%250Adistance%2520adapts%2520the%2520Wasserstein%2520metric%2520to%2520GMMs%2520and%2520has%2520been%2520applied%2520in%2520various%250Adomains%252C%2520including%2520domain%2520adaptation%252C%2520dataset%2520comparison%252C%2520and%2520reinforcement%250Alearning.%2520However%252C%2520its%2520high%2520computational%2520cost%2520--%2520arising%2520from%2520repeated%250AWasserstein%2520distance%2520computations%2520involving%2520matrix%2520square%2520root%2520estimations%2520and%250Aan%2520expensive%2520linear%2520program%2520--%2520limits%2520its%2520scalability%2520to%2520high-dimensional%2520and%250Alarge-scale%2520problems.%2520To%2520address%2520this%252C%2520we%2520propose%2520multiple%2520novel%2520slicing-based%250Aapproximations%2520to%2520the%2520MW%2520distance%2520that%2520significantly%2520reduce%2520computational%250Acomplexity%2520while%2520preserving%2520key%2520optimal%2520transport%2520properties.%2520From%2520a%250Atheoretical%2520viewpoint%252C%2520we%2520establish%2520several%2520weak%2520and%2520strong%2520equivalences%250Abetween%2520the%2520introduced%2520metrics%252C%2520and%2520show%2520the%2520relations%2520to%2520the%2520original%2520MW%250Adistance%2520and%2520the%2520well-established%2520sliced%2520Wasserstein%2520distance.%2520Furthermore%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520numerical%2520experiments%252C%250Ademonstrating%2520computational%2520efficiency%2520and%2520applications%2520in%2520clustering%252C%250Aperceptual%2520image%2520comparison%252C%2520and%2520GMM%2520minimization%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slicing%20the%20Gaussian%20Mixture%20Wasserstein%20Distance&entry.906535625=Moritz%20Piening%20and%20Robert%20Beinert&entry.1292438233=%20%20Gaussian%20mixture%20models%20%28GMMs%29%20are%20widely%20used%20in%20machine%20learning%20for%20tasks%0Asuch%20as%20clustering%2C%20classification%2C%20image%20reconstruction%2C%20and%20generative%0Amodeling.%20A%20key%20challenge%20in%20working%20with%20GMMs%20is%20defining%20a%20computationally%0Aefficient%20and%20geometrically%20meaningful%20metric.%20The%20mixture%20Wasserstein%20%28MW%29%0Adistance%20adapts%20the%20Wasserstein%20metric%20to%20GMMs%20and%20has%20been%20applied%20in%20various%0Adomains%2C%20including%20domain%20adaptation%2C%20dataset%20comparison%2C%20and%20reinforcement%0Alearning.%20However%2C%20its%20high%20computational%20cost%20--%20arising%20from%20repeated%0AWasserstein%20distance%20computations%20involving%20matrix%20square%20root%20estimations%20and%0Aan%20expensive%20linear%20program%20--%20limits%20its%20scalability%20to%20high-dimensional%20and%0Alarge-scale%20problems.%20To%20address%20this%2C%20we%20propose%20multiple%20novel%20slicing-based%0Aapproximations%20to%20the%20MW%20distance%20that%20significantly%20reduce%20computational%0Acomplexity%20while%20preserving%20key%20optimal%20transport%20properties.%20From%20a%0Atheoretical%20viewpoint%2C%20we%20establish%20several%20weak%20and%20strong%20equivalences%0Abetween%20the%20introduced%20metrics%2C%20and%20show%20the%20relations%20to%20the%20original%20MW%0Adistance%20and%20the%20well-established%20sliced%20Wasserstein%20distance.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20through%20numerical%20experiments%2C%0Ademonstrating%20computational%20efficiency%20and%20applications%20in%20clustering%2C%0Aperceptual%20image%20comparison%2C%20and%20GMM%20minimization%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08544v2&entry.124074799=Read"},
{"title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast\n  Lesion Segmentation in Longitudinal DCE-MRI", "author": "Mohammed Kamran and Maria Bernathova and Raoul Varga and Christian F. Singer and Zsuzsanna Bago-Horvath and Thomas Helbich and Georg Langs and Philipp Seeb\u00f6ck", "abstract": "  Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime\n", "link": "http://arxiv.org/abs/2508.00496v2", "date": "2025-08-04", "relevancy": 2.5841, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LesiOnTime%20--%20Joint%20Temporal%20and%20Clinical%20Modeling%20for%20Small%20Breast%0A%20%20Lesion%20Segmentation%20in%20Longitudinal%20DCE-MRI&body=Title%3A%20LesiOnTime%20--%20Joint%20Temporal%20and%20Clinical%20Modeling%20for%20Small%20Breast%0A%20%20Lesion%20Segmentation%20in%20Longitudinal%20DCE-MRI%0AAuthor%3A%20Mohammed%20Kamran%20and%20Maria%20Bernathova%20and%20Raoul%20Varga%20and%20Christian%20F.%20Singer%20and%20Zsuzsanna%20Bago-Horvath%20and%20Thomas%20Helbich%20and%20Georg%20Langs%20and%20Philipp%20Seeb%C3%B6ck%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20small%20lesions%20in%20Breast%20Dynamic%20Contrast-Enhanced%0AMRI%20%28DCE-MRI%29%20is%20critical%20for%20early%20cancer%20detection%2C%20especially%20in%20high-risk%0Apatients.%20While%20recent%20deep%20learning%20methods%20have%20advanced%20lesion%20segmentation%2C%0Athey%20primarily%20target%20large%20lesions%20and%20neglect%20valuable%20longitudinal%20and%0Aclinical%20information%20routinely%20used%20by%20radiologists.%20In%20real-world%20screening%2C%0Adetecting%20subtle%20or%20emerging%20lesions%20requires%20radiologists%20to%20compare%20across%0Atimepoints%20and%20consider%20previous%20radiology%20assessments%2C%20such%20as%20the%20BI-RADS%0Ascore.%20We%20propose%20LesiOnTime%2C%20a%20novel%203D%20segmentation%20approach%20that%20mimics%0Aclinical%20diagnostic%20workflows%20by%20jointly%20leveraging%20longitudinal%20imaging%20and%0ABIRADS%20scores.%20The%20key%20components%20are%3A%20%281%29%20a%20Temporal%20Prior%20Attention%20%28TPA%29%0Ablock%20that%20dynamically%20integrates%20information%20from%20previous%20and%20current%20scans%3B%0Aand%20%282%29%20a%20BI-RADS%20Consistency%20Regularization%20%28BCR%29%20loss%20that%20enforces%20latent%0Aspace%20alignment%20for%20scans%20with%20similar%20radiological%20assessments%2C%20thus%20embedding%0Adomain%20knowledge%20into%20the%20training%20process.%20Evaluated%20on%20a%20curated%20in-house%0Alongitudinal%20dataset%20of%20high-risk%20patients%20with%20DCE-MRI%2C%20our%20approach%0Aoutperforms%20state-of-the-art%20single-timepoint%20and%20longitudinal%20baselines%20by%205%25%0Ain%20terms%20of%20Dice.%20Ablation%20studies%20demonstrate%20that%20both%20TPA%20and%20BCR%20contribute%0Acomplementary%20performance%20gains.%20These%20results%20highlight%20the%20importance%20of%0Aincorporating%20temporal%20and%20clinical%20context%20for%20reliable%20early%20lesion%0Asegmentation%20in%20real-world%20breast%20cancer%20screening.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/cirmuw/LesiOnTime%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLesiOnTime%2520--%2520Joint%2520Temporal%2520and%2520Clinical%2520Modeling%2520for%2520Small%2520Breast%250A%2520%2520Lesion%2520Segmentation%2520in%2520Longitudinal%2520DCE-MRI%26entry.906535625%3DMohammed%2520Kamran%2520and%2520Maria%2520Bernathova%2520and%2520Raoul%2520Varga%2520and%2520Christian%2520F.%2520Singer%2520and%2520Zsuzsanna%2520Bago-Horvath%2520and%2520Thomas%2520Helbich%2520and%2520Georg%2520Langs%2520and%2520Philipp%2520Seeb%25C3%25B6ck%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520small%2520lesions%2520in%2520Breast%2520Dynamic%2520Contrast-Enhanced%250AMRI%2520%2528DCE-MRI%2529%2520is%2520critical%2520for%2520early%2520cancer%2520detection%252C%2520especially%2520in%2520high-risk%250Apatients.%2520While%2520recent%2520deep%2520learning%2520methods%2520have%2520advanced%2520lesion%2520segmentation%252C%250Athey%2520primarily%2520target%2520large%2520lesions%2520and%2520neglect%2520valuable%2520longitudinal%2520and%250Aclinical%2520information%2520routinely%2520used%2520by%2520radiologists.%2520In%2520real-world%2520screening%252C%250Adetecting%2520subtle%2520or%2520emerging%2520lesions%2520requires%2520radiologists%2520to%2520compare%2520across%250Atimepoints%2520and%2520consider%2520previous%2520radiology%2520assessments%252C%2520such%2520as%2520the%2520BI-RADS%250Ascore.%2520We%2520propose%2520LesiOnTime%252C%2520a%2520novel%25203D%2520segmentation%2520approach%2520that%2520mimics%250Aclinical%2520diagnostic%2520workflows%2520by%2520jointly%2520leveraging%2520longitudinal%2520imaging%2520and%250ABIRADS%2520scores.%2520The%2520key%2520components%2520are%253A%2520%25281%2529%2520a%2520Temporal%2520Prior%2520Attention%2520%2528TPA%2529%250Ablock%2520that%2520dynamically%2520integrates%2520information%2520from%2520previous%2520and%2520current%2520scans%253B%250Aand%2520%25282%2529%2520a%2520BI-RADS%2520Consistency%2520Regularization%2520%2528BCR%2529%2520loss%2520that%2520enforces%2520latent%250Aspace%2520alignment%2520for%2520scans%2520with%2520similar%2520radiological%2520assessments%252C%2520thus%2520embedding%250Adomain%2520knowledge%2520into%2520the%2520training%2520process.%2520Evaluated%2520on%2520a%2520curated%2520in-house%250Alongitudinal%2520dataset%2520of%2520high-risk%2520patients%2520with%2520DCE-MRI%252C%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520single-timepoint%2520and%2520longitudinal%2520baselines%2520by%25205%2525%250Ain%2520terms%2520of%2520Dice.%2520Ablation%2520studies%2520demonstrate%2520that%2520both%2520TPA%2520and%2520BCR%2520contribute%250Acomplementary%2520performance%2520gains.%2520These%2520results%2520highlight%2520the%2520importance%2520of%250Aincorporating%2520temporal%2520and%2520clinical%2520context%2520for%2520reliable%2520early%2520lesion%250Asegmentation%2520in%2520real-world%2520breast%2520cancer%2520screening.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/cirmuw/LesiOnTime%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LesiOnTime%20--%20Joint%20Temporal%20and%20Clinical%20Modeling%20for%20Small%20Breast%0A%20%20Lesion%20Segmentation%20in%20Longitudinal%20DCE-MRI&entry.906535625=Mohammed%20Kamran%20and%20Maria%20Bernathova%20and%20Raoul%20Varga%20and%20Christian%20F.%20Singer%20and%20Zsuzsanna%20Bago-Horvath%20and%20Thomas%20Helbich%20and%20Georg%20Langs%20and%20Philipp%20Seeb%C3%B6ck&entry.1292438233=%20%20Accurate%20segmentation%20of%20small%20lesions%20in%20Breast%20Dynamic%20Contrast-Enhanced%0AMRI%20%28DCE-MRI%29%20is%20critical%20for%20early%20cancer%20detection%2C%20especially%20in%20high-risk%0Apatients.%20While%20recent%20deep%20learning%20methods%20have%20advanced%20lesion%20segmentation%2C%0Athey%20primarily%20target%20large%20lesions%20and%20neglect%20valuable%20longitudinal%20and%0Aclinical%20information%20routinely%20used%20by%20radiologists.%20In%20real-world%20screening%2C%0Adetecting%20subtle%20or%20emerging%20lesions%20requires%20radiologists%20to%20compare%20across%0Atimepoints%20and%20consider%20previous%20radiology%20assessments%2C%20such%20as%20the%20BI-RADS%0Ascore.%20We%20propose%20LesiOnTime%2C%20a%20novel%203D%20segmentation%20approach%20that%20mimics%0Aclinical%20diagnostic%20workflows%20by%20jointly%20leveraging%20longitudinal%20imaging%20and%0ABIRADS%20scores.%20The%20key%20components%20are%3A%20%281%29%20a%20Temporal%20Prior%20Attention%20%28TPA%29%0Ablock%20that%20dynamically%20integrates%20information%20from%20previous%20and%20current%20scans%3B%0Aand%20%282%29%20a%20BI-RADS%20Consistency%20Regularization%20%28BCR%29%20loss%20that%20enforces%20latent%0Aspace%20alignment%20for%20scans%20with%20similar%20radiological%20assessments%2C%20thus%20embedding%0Adomain%20knowledge%20into%20the%20training%20process.%20Evaluated%20on%20a%20curated%20in-house%0Alongitudinal%20dataset%20of%20high-risk%20patients%20with%20DCE-MRI%2C%20our%20approach%0Aoutperforms%20state-of-the-art%20single-timepoint%20and%20longitudinal%20baselines%20by%205%25%0Ain%20terms%20of%20Dice.%20Ablation%20studies%20demonstrate%20that%20both%20TPA%20and%20BCR%20contribute%0Acomplementary%20performance%20gains.%20These%20results%20highlight%20the%20importance%20of%0Aincorporating%20temporal%20and%20clinical%20context%20for%20reliable%20early%20lesion%0Asegmentation%20in%20real-world%20breast%20cancer%20screening.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/cirmuw/LesiOnTime%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00496v2&entry.124074799=Read"},
{"title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation", "author": "Xiaolin Lin and Jingcun Wang and Olga Kondrateva and Yiyu Shi and Bing Li and Grace Li Zhang", "abstract": "  Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.\n", "link": "http://arxiv.org/abs/2508.02401v1", "date": "2025-08-04", "relevancy": 2.5622, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompressKV%3A%20Semantic%20Retrieval%20Heads%20Know%20What%20Tokens%20are%20Not%20Important%0A%20%20Before%20Generation&body=Title%3A%20CompressKV%3A%20Semantic%20Retrieval%20Heads%20Know%20What%20Tokens%20are%20Not%20Important%0A%20%20Before%20Generation%0AAuthor%3A%20Xiaolin%20Lin%20and%20Jingcun%20Wang%20and%20Olga%20Kondrateva%20and%20Yiyu%20Shi%20and%20Bing%20Li%20and%20Grace%20Li%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20boosted%0Along-context%20processing.%20However%2C%20the%20increasing%20key-value%20%28KV%29%20cache%20size%0Aposes%20critical%20challenges%20to%20memory%20and%20execution%20efficiency.%20Most%20KV%20cache%0Acompression%20methods%20rely%20on%20heuristic%20token%20eviction%20using%20all%20attention%20heads%0Ain%20Grouped%20Query%20Attention%20%28GQA%29-based%20LLMs.%20This%20method%20ignores%20the%20different%0Afunctionalities%20of%20attention%20heads%2C%20leading%20to%20the%20eviction%20of%20critical%20tokens%0Aand%20thus%20degrades%20the%20performance%20of%20LLMs.%0A%20%20To%20address%20the%20issue%20above%2C%20instead%20of%20using%20all%20the%20attention%20heads%20in%0AGQA-based%20LLMs%20to%20determine%20important%20tokens%20as%20in%20the%20previous%20work%2C%20we%20first%0Aidentify%20the%20attention%20heads%20in%20each%20layer%20that%20are%20not%20only%20capable%20of%0Aretrieving%20the%20initial%20and%20final%20tokens%20of%20a%20prompt%2C%20but%20also%20capable%20of%0Aretrieving%20important%20tokens%20within%20the%20text%20and%20attending%20to%20their%20surrounding%0Asemantic%20context.%20Afterwards%2C%20we%20exploit%20such%20heads%20to%20determine%20the%20important%0Atokens%20and%20retain%20their%20corresponding%20KV%20cache%20pairs.%20Furthermore%2C%20we%20analyze%0Athe%20cache%20eviction%20error%20of%20each%20layer%20individually%20and%20introduce%20a%0Alayer-adaptive%20KV%20cache%20allocation%20strategy.%20Experimental%20results%20demonstrate%0Athe%20proposed%20CompressKV%20consistently%20outperforms%20state-of-the-art%20approaches%0Aunder%20various%20memory%20budgets%20on%20LongBench%20and%20Needle-in-a-Haystack%20benchmarks.%0AOur%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/TUDa-HWAI/CompressKV.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressKV%253A%2520Semantic%2520Retrieval%2520Heads%2520Know%2520What%2520Tokens%2520are%2520Not%2520Important%250A%2520%2520Before%2520Generation%26entry.906535625%3DXiaolin%2520Lin%2520and%2520Jingcun%2520Wang%2520and%2520Olga%2520Kondrateva%2520and%2520Yiyu%2520Shi%2520and%2520Bing%2520Li%2520and%2520Grace%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520boosted%250Along-context%2520processing.%2520However%252C%2520the%2520increasing%2520key-value%2520%2528KV%2529%2520cache%2520size%250Aposes%2520critical%2520challenges%2520to%2520memory%2520and%2520execution%2520efficiency.%2520Most%2520KV%2520cache%250Acompression%2520methods%2520rely%2520on%2520heuristic%2520token%2520eviction%2520using%2520all%2520attention%2520heads%250Ain%2520Grouped%2520Query%2520Attention%2520%2528GQA%2529-based%2520LLMs.%2520This%2520method%2520ignores%2520the%2520different%250Afunctionalities%2520of%2520attention%2520heads%252C%2520leading%2520to%2520the%2520eviction%2520of%2520critical%2520tokens%250Aand%2520thus%2520degrades%2520the%2520performance%2520of%2520LLMs.%250A%2520%2520To%2520address%2520the%2520issue%2520above%252C%2520instead%2520of%2520using%2520all%2520the%2520attention%2520heads%2520in%250AGQA-based%2520LLMs%2520to%2520determine%2520important%2520tokens%2520as%2520in%2520the%2520previous%2520work%252C%2520we%2520first%250Aidentify%2520the%2520attention%2520heads%2520in%2520each%2520layer%2520that%2520are%2520not%2520only%2520capable%2520of%250Aretrieving%2520the%2520initial%2520and%2520final%2520tokens%2520of%2520a%2520prompt%252C%2520but%2520also%2520capable%2520of%250Aretrieving%2520important%2520tokens%2520within%2520the%2520text%2520and%2520attending%2520to%2520their%2520surrounding%250Asemantic%2520context.%2520Afterwards%252C%2520we%2520exploit%2520such%2520heads%2520to%2520determine%2520the%2520important%250Atokens%2520and%2520retain%2520their%2520corresponding%2520KV%2520cache%2520pairs.%2520Furthermore%252C%2520we%2520analyze%250Athe%2520cache%2520eviction%2520error%2520of%2520each%2520layer%2520individually%2520and%2520introduce%2520a%250Alayer-adaptive%2520KV%2520cache%2520allocation%2520strategy.%2520Experimental%2520results%2520demonstrate%250Athe%2520proposed%2520CompressKV%2520consistently%2520outperforms%2520state-of-the-art%2520approaches%250Aunder%2520various%2520memory%2520budgets%2520on%2520LongBench%2520and%2520Needle-in-a-Haystack%2520benchmarks.%250AOur%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/TUDa-HWAI/CompressKV.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompressKV%3A%20Semantic%20Retrieval%20Heads%20Know%20What%20Tokens%20are%20Not%20Important%0A%20%20Before%20Generation&entry.906535625=Xiaolin%20Lin%20and%20Jingcun%20Wang%20and%20Olga%20Kondrateva%20and%20Yiyu%20Shi%20and%20Bing%20Li%20and%20Grace%20Li%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20boosted%0Along-context%20processing.%20However%2C%20the%20increasing%20key-value%20%28KV%29%20cache%20size%0Aposes%20critical%20challenges%20to%20memory%20and%20execution%20efficiency.%20Most%20KV%20cache%0Acompression%20methods%20rely%20on%20heuristic%20token%20eviction%20using%20all%20attention%20heads%0Ain%20Grouped%20Query%20Attention%20%28GQA%29-based%20LLMs.%20This%20method%20ignores%20the%20different%0Afunctionalities%20of%20attention%20heads%2C%20leading%20to%20the%20eviction%20of%20critical%20tokens%0Aand%20thus%20degrades%20the%20performance%20of%20LLMs.%0A%20%20To%20address%20the%20issue%20above%2C%20instead%20of%20using%20all%20the%20attention%20heads%20in%0AGQA-based%20LLMs%20to%20determine%20important%20tokens%20as%20in%20the%20previous%20work%2C%20we%20first%0Aidentify%20the%20attention%20heads%20in%20each%20layer%20that%20are%20not%20only%20capable%20of%0Aretrieving%20the%20initial%20and%20final%20tokens%20of%20a%20prompt%2C%20but%20also%20capable%20of%0Aretrieving%20important%20tokens%20within%20the%20text%20and%20attending%20to%20their%20surrounding%0Asemantic%20context.%20Afterwards%2C%20we%20exploit%20such%20heads%20to%20determine%20the%20important%0Atokens%20and%20retain%20their%20corresponding%20KV%20cache%20pairs.%20Furthermore%2C%20we%20analyze%0Athe%20cache%20eviction%20error%20of%20each%20layer%20individually%20and%20introduce%20a%0Alayer-adaptive%20KV%20cache%20allocation%20strategy.%20Experimental%20results%20demonstrate%0Athe%20proposed%20CompressKV%20consistently%20outperforms%20state-of-the-art%20approaches%0Aunder%20various%20memory%20budgets%20on%20LongBench%20and%20Needle-in-a-Haystack%20benchmarks.%0AOur%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/TUDa-HWAI/CompressKV.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02401v1&entry.124074799=Read"},
{"title": "Subjective Camera 0.1: Bridging Human Cognition and Visual\n  Reconstruction through Sequence-Aware Sketch-Guided Diffusion", "author": "Haoyang Chen and Dongfang Sun and Caoyuan Ma and Shiqin Wang and Kewei Zhang and Zheng Wang and Zhixiang Wang", "abstract": "  We introduce the concept of a subjective camera to reconstruct meaningful\nmoments that physical cameras fail to capture. We propose Subjective Camera\n0.1, a framework for reconstructing real-world scenes from readily accessible\nsubjective readouts, i.e., textual descriptions and progressively drawn rough\nsketches. Built on optimization-based alignment of diffusion models, our\napproach avoids large-scale paired training data and mitigates generalization\nissues. To address the challenge of integrating multiple abstract concepts in\nreal-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion\nframework with three loss terms for concept-wise sequential optimization,\nfollowing the natural order of subjective readouts. Experiments on two datasets\ndemonstrate that our method achieves state-of-the-art performance in image\nquality as well as spatial and semantic alignment with target scenes. User\nstudies with 40 participants further confirm that our approach is consistently\npreferred.Our project page is at: subjective-camera.github.io\n", "link": "http://arxiv.org/abs/2506.23711v2", "date": "2025-08-04", "relevancy": 2.56, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6597}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6361}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subjective%20Camera%200.1%3A%20Bridging%20Human%20Cognition%20and%20Visual%0A%20%20Reconstruction%20through%20Sequence-Aware%20Sketch-Guided%20Diffusion&body=Title%3A%20Subjective%20Camera%200.1%3A%20Bridging%20Human%20Cognition%20and%20Visual%0A%20%20Reconstruction%20through%20Sequence-Aware%20Sketch-Guided%20Diffusion%0AAuthor%3A%20Haoyang%20Chen%20and%20Dongfang%20Sun%20and%20Caoyuan%20Ma%20and%20Shiqin%20Wang%20and%20Kewei%20Zhang%20and%20Zheng%20Wang%20and%20Zhixiang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20the%20concept%20of%20a%20subjective%20camera%20to%20reconstruct%20meaningful%0Amoments%20that%20physical%20cameras%20fail%20to%20capture.%20We%20propose%20Subjective%20Camera%0A0.1%2C%20a%20framework%20for%20reconstructing%20real-world%20scenes%20from%20readily%20accessible%0Asubjective%20readouts%2C%20i.e.%2C%20textual%20descriptions%20and%20progressively%20drawn%20rough%0Asketches.%20Built%20on%20optimization-based%20alignment%20of%20diffusion%20models%2C%20our%0Aapproach%20avoids%20large-scale%20paired%20training%20data%20and%20mitigates%20generalization%0Aissues.%20To%20address%20the%20challenge%20of%20integrating%20multiple%20abstract%20concepts%20in%0Areal-world%20scenarios%2C%20we%20design%20a%20Sequence-Aware%20Sketch-Guided%20Diffusion%0Aframework%20with%20three%20loss%20terms%20for%20concept-wise%20sequential%20optimization%2C%0Afollowing%20the%20natural%20order%20of%20subjective%20readouts.%20Experiments%20on%20two%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20image%0Aquality%20as%20well%20as%20spatial%20and%20semantic%20alignment%20with%20target%20scenes.%20User%0Astudies%20with%2040%20participants%20further%20confirm%20that%20our%20approach%20is%20consistently%0Apreferred.Our%20project%20page%20is%20at%3A%20subjective-camera.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubjective%2520Camera%25200.1%253A%2520Bridging%2520Human%2520Cognition%2520and%2520Visual%250A%2520%2520Reconstruction%2520through%2520Sequence-Aware%2520Sketch-Guided%2520Diffusion%26entry.906535625%3DHaoyang%2520Chen%2520and%2520Dongfang%2520Sun%2520and%2520Caoyuan%2520Ma%2520and%2520Shiqin%2520Wang%2520and%2520Kewei%2520Zhang%2520and%2520Zheng%2520Wang%2520and%2520Zhixiang%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520concept%2520of%2520a%2520subjective%2520camera%2520to%2520reconstruct%2520meaningful%250Amoments%2520that%2520physical%2520cameras%2520fail%2520to%2520capture.%2520We%2520propose%2520Subjective%2520Camera%250A0.1%252C%2520a%2520framework%2520for%2520reconstructing%2520real-world%2520scenes%2520from%2520readily%2520accessible%250Asubjective%2520readouts%252C%2520i.e.%252C%2520textual%2520descriptions%2520and%2520progressively%2520drawn%2520rough%250Asketches.%2520Built%2520on%2520optimization-based%2520alignment%2520of%2520diffusion%2520models%252C%2520our%250Aapproach%2520avoids%2520large-scale%2520paired%2520training%2520data%2520and%2520mitigates%2520generalization%250Aissues.%2520To%2520address%2520the%2520challenge%2520of%2520integrating%2520multiple%2520abstract%2520concepts%2520in%250Areal-world%2520scenarios%252C%2520we%2520design%2520a%2520Sequence-Aware%2520Sketch-Guided%2520Diffusion%250Aframework%2520with%2520three%2520loss%2520terms%2520for%2520concept-wise%2520sequential%2520optimization%252C%250Afollowing%2520the%2520natural%2520order%2520of%2520subjective%2520readouts.%2520Experiments%2520on%2520two%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520image%250Aquality%2520as%2520well%2520as%2520spatial%2520and%2520semantic%2520alignment%2520with%2520target%2520scenes.%2520User%250Astudies%2520with%252040%2520participants%2520further%2520confirm%2520that%2520our%2520approach%2520is%2520consistently%250Apreferred.Our%2520project%2520page%2520is%2520at%253A%2520subjective-camera.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective%20Camera%200.1%3A%20Bridging%20Human%20Cognition%20and%20Visual%0A%20%20Reconstruction%20through%20Sequence-Aware%20Sketch-Guided%20Diffusion&entry.906535625=Haoyang%20Chen%20and%20Dongfang%20Sun%20and%20Caoyuan%20Ma%20and%20Shiqin%20Wang%20and%20Kewei%20Zhang%20and%20Zheng%20Wang%20and%20Zhixiang%20Wang&entry.1292438233=%20%20We%20introduce%20the%20concept%20of%20a%20subjective%20camera%20to%20reconstruct%20meaningful%0Amoments%20that%20physical%20cameras%20fail%20to%20capture.%20We%20propose%20Subjective%20Camera%0A0.1%2C%20a%20framework%20for%20reconstructing%20real-world%20scenes%20from%20readily%20accessible%0Asubjective%20readouts%2C%20i.e.%2C%20textual%20descriptions%20and%20progressively%20drawn%20rough%0Asketches.%20Built%20on%20optimization-based%20alignment%20of%20diffusion%20models%2C%20our%0Aapproach%20avoids%20large-scale%20paired%20training%20data%20and%20mitigates%20generalization%0Aissues.%20To%20address%20the%20challenge%20of%20integrating%20multiple%20abstract%20concepts%20in%0Areal-world%20scenarios%2C%20we%20design%20a%20Sequence-Aware%20Sketch-Guided%20Diffusion%0Aframework%20with%20three%20loss%20terms%20for%20concept-wise%20sequential%20optimization%2C%0Afollowing%20the%20natural%20order%20of%20subjective%20readouts.%20Experiments%20on%20two%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20image%0Aquality%20as%20well%20as%20spatial%20and%20semantic%20alignment%20with%20target%20scenes.%20User%0Astudies%20with%2040%20participants%20further%20confirm%20that%20our%20approach%20is%20consistently%0Apreferred.Our%20project%20page%20is%20at%3A%20subjective-camera.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23711v2&entry.124074799=Read"},
{"title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic\n  Identification and Trait Recognition of Biodiversity Specimens", "author": "Alyson East and Elizabeth G. Campolongo and Luke Meyers and S M Rayeed and Samuel Stevens and Iuliia Zarubiieva and Isadora E. Fluck and Jennifer C. Gir\u00f3n and Maximiliane Jousse and Scott Lowe and Kayla I Perry and Isabelle Betancourt and Noah Charney and Evan Donoso and Nathan Fox and Kim J. Landsbergen and Ekaterina Nepovinnykh and Michelle Ramirez and Parkash Singh and Khum Thapa-Magar and Matthew Thompson and Evan Waite and Tanya Berger-Wolf and Hilmar Lapp and Paula Mabee and Charles Stewart and Graham Taylor and Sydne Record", "abstract": "  1) Biological collections house millions of specimens with digital images\nincreasingly available through open-access platforms. However, most imaging\nprotocols were developed for human interpretation without considering automated\nanalysis requirements. As computer vision applications revolutionize taxonomic\nidentification and trait extraction, a critical gap exists between current\ndigitization practices and computational analysis needs. This review provides\nthe first comprehensive practical framework for optimizing biological specimen\nimaging for computer vision applications. 2) Through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists, we synthesized evidence-based recommendations addressing\nfundamental computer vision concepts and practical imaging considerations. We\nprovide immediately actionable implementation guidance while identifying\ncritical areas requiring community standards development. 3) Our framework\nencompasses ten interconnected considerations for optimizing image capture for\ncomputer vision-powered taxonomic identification and trait extraction. We\ntranslate these into practical implementation checklists, equipment selection\nguidelines, and a roadmap for community standards development including\nfilename conventions, pixel density requirements, and cross-institutional\nprotocols. 4)By bridging biological and computational disciplines, this\napproach unlocks automated analysis potential for millions of existing\nspecimens and guides future digitization efforts toward unprecedented\nanalytical capabilities.\n", "link": "http://arxiv.org/abs/2505.17317v2", "date": "2025-08-04", "relevancy": 2.5525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Image%20Capture%20for%20Computer%20Vision-Powered%20Taxonomic%0A%20%20Identification%20and%20Trait%20Recognition%20of%20Biodiversity%20Specimens&body=Title%3A%20Optimizing%20Image%20Capture%20for%20Computer%20Vision-Powered%20Taxonomic%0A%20%20Identification%20and%20Trait%20Recognition%20of%20Biodiversity%20Specimens%0AAuthor%3A%20Alyson%20East%20and%20Elizabeth%20G.%20Campolongo%20and%20Luke%20Meyers%20and%20S%20M%20Rayeed%20and%20Samuel%20Stevens%20and%20Iuliia%20Zarubiieva%20and%20Isadora%20E.%20Fluck%20and%20Jennifer%20C.%20Gir%C3%B3n%20and%20Maximiliane%20Jousse%20and%20Scott%20Lowe%20and%20Kayla%20I%20Perry%20and%20Isabelle%20Betancourt%20and%20Noah%20Charney%20and%20Evan%20Donoso%20and%20Nathan%20Fox%20and%20Kim%20J.%20Landsbergen%20and%20Ekaterina%20Nepovinnykh%20and%20Michelle%20Ramirez%20and%20Parkash%20Singh%20and%20Khum%20Thapa-Magar%20and%20Matthew%20Thompson%20and%20Evan%20Waite%20and%20Tanya%20Berger-Wolf%20and%20Hilmar%20Lapp%20and%20Paula%20Mabee%20and%20Charles%20Stewart%20and%20Graham%20Taylor%20and%20Sydne%20Record%0AAbstract%3A%20%20%201%29%20Biological%20collections%20house%20millions%20of%20specimens%20with%20digital%20images%0Aincreasingly%20available%20through%20open-access%20platforms.%20However%2C%20most%20imaging%0Aprotocols%20were%20developed%20for%20human%20interpretation%20without%20considering%20automated%0Aanalysis%20requirements.%20As%20computer%20vision%20applications%20revolutionize%20taxonomic%0Aidentification%20and%20trait%20extraction%2C%20a%20critical%20gap%20exists%20between%20current%0Adigitization%20practices%20and%20computational%20analysis%20needs.%20This%20review%20provides%0Athe%20first%20comprehensive%20practical%20framework%20for%20optimizing%20biological%20specimen%0Aimaging%20for%20computer%20vision%20applications.%202%29%20Through%20interdisciplinary%0Acollaboration%20between%20taxonomists%2C%20collection%20managers%2C%20ecologists%2C%20and%0Acomputer%20scientists%2C%20we%20synthesized%20evidence-based%20recommendations%20addressing%0Afundamental%20computer%20vision%20concepts%20and%20practical%20imaging%20considerations.%20We%0Aprovide%20immediately%20actionable%20implementation%20guidance%20while%20identifying%0Acritical%20areas%20requiring%20community%20standards%20development.%203%29%20Our%20framework%0Aencompasses%20ten%20interconnected%20considerations%20for%20optimizing%20image%20capture%20for%0Acomputer%20vision-powered%20taxonomic%20identification%20and%20trait%20extraction.%20We%0Atranslate%20these%20into%20practical%20implementation%20checklists%2C%20equipment%20selection%0Aguidelines%2C%20and%20a%20roadmap%20for%20community%20standards%20development%20including%0Afilename%20conventions%2C%20pixel%20density%20requirements%2C%20and%20cross-institutional%0Aprotocols.%204%29By%20bridging%20biological%20and%20computational%20disciplines%2C%20this%0Aapproach%20unlocks%20automated%20analysis%20potential%20for%20millions%20of%20existing%0Aspecimens%20and%20guides%20future%20digitization%20efforts%20toward%20unprecedented%0Aanalytical%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Image%2520Capture%2520for%2520Computer%2520Vision-Powered%2520Taxonomic%250A%2520%2520Identification%2520and%2520Trait%2520Recognition%2520of%2520Biodiversity%2520Specimens%26entry.906535625%3DAlyson%2520East%2520and%2520Elizabeth%2520G.%2520Campolongo%2520and%2520Luke%2520Meyers%2520and%2520S%2520M%2520Rayeed%2520and%2520Samuel%2520Stevens%2520and%2520Iuliia%2520Zarubiieva%2520and%2520Isadora%2520E.%2520Fluck%2520and%2520Jennifer%2520C.%2520Gir%25C3%25B3n%2520and%2520Maximiliane%2520Jousse%2520and%2520Scott%2520Lowe%2520and%2520Kayla%2520I%2520Perry%2520and%2520Isabelle%2520Betancourt%2520and%2520Noah%2520Charney%2520and%2520Evan%2520Donoso%2520and%2520Nathan%2520Fox%2520and%2520Kim%2520J.%2520Landsbergen%2520and%2520Ekaterina%2520Nepovinnykh%2520and%2520Michelle%2520Ramirez%2520and%2520Parkash%2520Singh%2520and%2520Khum%2520Thapa-Magar%2520and%2520Matthew%2520Thompson%2520and%2520Evan%2520Waite%2520and%2520Tanya%2520Berger-Wolf%2520and%2520Hilmar%2520Lapp%2520and%2520Paula%2520Mabee%2520and%2520Charles%2520Stewart%2520and%2520Graham%2520Taylor%2520and%2520Sydne%2520Record%26entry.1292438233%3D%2520%25201%2529%2520Biological%2520collections%2520house%2520millions%2520of%2520specimens%2520with%2520digital%2520images%250Aincreasingly%2520available%2520through%2520open-access%2520platforms.%2520However%252C%2520most%2520imaging%250Aprotocols%2520were%2520developed%2520for%2520human%2520interpretation%2520without%2520considering%2520automated%250Aanalysis%2520requirements.%2520As%2520computer%2520vision%2520applications%2520revolutionize%2520taxonomic%250Aidentification%2520and%2520trait%2520extraction%252C%2520a%2520critical%2520gap%2520exists%2520between%2520current%250Adigitization%2520practices%2520and%2520computational%2520analysis%2520needs.%2520This%2520review%2520provides%250Athe%2520first%2520comprehensive%2520practical%2520framework%2520for%2520optimizing%2520biological%2520specimen%250Aimaging%2520for%2520computer%2520vision%2520applications.%25202%2529%2520Through%2520interdisciplinary%250Acollaboration%2520between%2520taxonomists%252C%2520collection%2520managers%252C%2520ecologists%252C%2520and%250Acomputer%2520scientists%252C%2520we%2520synthesized%2520evidence-based%2520recommendations%2520addressing%250Afundamental%2520computer%2520vision%2520concepts%2520and%2520practical%2520imaging%2520considerations.%2520We%250Aprovide%2520immediately%2520actionable%2520implementation%2520guidance%2520while%2520identifying%250Acritical%2520areas%2520requiring%2520community%2520standards%2520development.%25203%2529%2520Our%2520framework%250Aencompasses%2520ten%2520interconnected%2520considerations%2520for%2520optimizing%2520image%2520capture%2520for%250Acomputer%2520vision-powered%2520taxonomic%2520identification%2520and%2520trait%2520extraction.%2520We%250Atranslate%2520these%2520into%2520practical%2520implementation%2520checklists%252C%2520equipment%2520selection%250Aguidelines%252C%2520and%2520a%2520roadmap%2520for%2520community%2520standards%2520development%2520including%250Afilename%2520conventions%252C%2520pixel%2520density%2520requirements%252C%2520and%2520cross-institutional%250Aprotocols.%25204%2529By%2520bridging%2520biological%2520and%2520computational%2520disciplines%252C%2520this%250Aapproach%2520unlocks%2520automated%2520analysis%2520potential%2520for%2520millions%2520of%2520existing%250Aspecimens%2520and%2520guides%2520future%2520digitization%2520efforts%2520toward%2520unprecedented%250Aanalytical%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Image%20Capture%20for%20Computer%20Vision-Powered%20Taxonomic%0A%20%20Identification%20and%20Trait%20Recognition%20of%20Biodiversity%20Specimens&entry.906535625=Alyson%20East%20and%20Elizabeth%20G.%20Campolongo%20and%20Luke%20Meyers%20and%20S%20M%20Rayeed%20and%20Samuel%20Stevens%20and%20Iuliia%20Zarubiieva%20and%20Isadora%20E.%20Fluck%20and%20Jennifer%20C.%20Gir%C3%B3n%20and%20Maximiliane%20Jousse%20and%20Scott%20Lowe%20and%20Kayla%20I%20Perry%20and%20Isabelle%20Betancourt%20and%20Noah%20Charney%20and%20Evan%20Donoso%20and%20Nathan%20Fox%20and%20Kim%20J.%20Landsbergen%20and%20Ekaterina%20Nepovinnykh%20and%20Michelle%20Ramirez%20and%20Parkash%20Singh%20and%20Khum%20Thapa-Magar%20and%20Matthew%20Thompson%20and%20Evan%20Waite%20and%20Tanya%20Berger-Wolf%20and%20Hilmar%20Lapp%20and%20Paula%20Mabee%20and%20Charles%20Stewart%20and%20Graham%20Taylor%20and%20Sydne%20Record&entry.1292438233=%20%201%29%20Biological%20collections%20house%20millions%20of%20specimens%20with%20digital%20images%0Aincreasingly%20available%20through%20open-access%20platforms.%20However%2C%20most%20imaging%0Aprotocols%20were%20developed%20for%20human%20interpretation%20without%20considering%20automated%0Aanalysis%20requirements.%20As%20computer%20vision%20applications%20revolutionize%20taxonomic%0Aidentification%20and%20trait%20extraction%2C%20a%20critical%20gap%20exists%20between%20current%0Adigitization%20practices%20and%20computational%20analysis%20needs.%20This%20review%20provides%0Athe%20first%20comprehensive%20practical%20framework%20for%20optimizing%20biological%20specimen%0Aimaging%20for%20computer%20vision%20applications.%202%29%20Through%20interdisciplinary%0Acollaboration%20between%20taxonomists%2C%20collection%20managers%2C%20ecologists%2C%20and%0Acomputer%20scientists%2C%20we%20synthesized%20evidence-based%20recommendations%20addressing%0Afundamental%20computer%20vision%20concepts%20and%20practical%20imaging%20considerations.%20We%0Aprovide%20immediately%20actionable%20implementation%20guidance%20while%20identifying%0Acritical%20areas%20requiring%20community%20standards%20development.%203%29%20Our%20framework%0Aencompasses%20ten%20interconnected%20considerations%20for%20optimizing%20image%20capture%20for%0Acomputer%20vision-powered%20taxonomic%20identification%20and%20trait%20extraction.%20We%0Atranslate%20these%20into%20practical%20implementation%20checklists%2C%20equipment%20selection%0Aguidelines%2C%20and%20a%20roadmap%20for%20community%20standards%20development%20including%0Afilename%20conventions%2C%20pixel%20density%20requirements%2C%20and%20cross-institutional%0Aprotocols.%204%29By%20bridging%20biological%20and%20computational%20disciplines%2C%20this%0Aapproach%20unlocks%20automated%20analysis%20potential%20for%20millions%20of%20existing%0Aspecimens%20and%20guides%20future%20digitization%20efforts%20toward%20unprecedented%0Aanalytical%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17317v2&entry.124074799=Read"},
{"title": "Attack Anything: Blind DNNs via Universal Background Adversarial Attack", "author": "Jiawei Lian and Shaohui Mei and Xiaofei Wang and Yi Wang and Lefan Wang and Yingjie Lu and Mingyang Ma and Lap-Pui Chau", "abstract": "  It has been widely substantiated that deep neural networks (DNNs) are\nsusceptible and vulnerable to adversarial perturbations. Existing studies\nmainly focus on performing attacks by corrupting targeted objects (physical\nattack) or images (digital attack), which is intuitively acceptable and\nunderstandable in terms of the attack's effectiveness. In contrast, our focus\nlies in conducting background adversarial attacks in both digital and physical\ndomains, without causing any disruptions to the targeted objects themselves.\nSpecifically, an effective background adversarial attack framework is proposed\nto attack anything, by which the attack efficacy generalizes well between\ndiverse objects, models, and tasks. Technically, we approach the background\nadversarial attack as an iterative optimization problem, analogous to the\nprocess of DNN learning. Besides, we offer a theoretical demonstration of its\nconvergence under a set of mild but sufficient conditions. To strengthen the\nattack efficacy and transferability, we propose a new ensemble strategy\ntailored for adversarial perturbations and introduce an improved smooth\nconstraint for the seamless connection of integrated perturbations. We conduct\ncomprehensive and rigorous experiments in both digital and physical domains\nacross various objects, models, and tasks, demonstrating the effectiveness of\nattacking anything of the proposed method. The findings of this research\nsubstantiate the significant discrepancy between human and machine vision on\nthe value of background variations, which play a far more critical role than\npreviously recognized, necessitating a reevaluation of the robustness and\nreliability of DNNs. The code will be publicly available at\nhttps://github.com/JiaweiLian/Attack_Anything\n", "link": "http://arxiv.org/abs/2409.00029v2", "date": "2025-08-04", "relevancy": 2.5183, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5223}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4981}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attack%20Anything%3A%20Blind%20DNNs%20via%20Universal%20Background%20Adversarial%20Attack&body=Title%3A%20Attack%20Anything%3A%20Blind%20DNNs%20via%20Universal%20Background%20Adversarial%20Attack%0AAuthor%3A%20Jiawei%20Lian%20and%20Shaohui%20Mei%20and%20Xiaofei%20Wang%20and%20Yi%20Wang%20and%20Lefan%20Wang%20and%20Yingjie%20Lu%20and%20Mingyang%20Ma%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%20It%20has%20been%20widely%20substantiated%20that%20deep%20neural%20networks%20%28DNNs%29%20are%0Asusceptible%20and%20vulnerable%20to%20adversarial%20perturbations.%20Existing%20studies%0Amainly%20focus%20on%20performing%20attacks%20by%20corrupting%20targeted%20objects%20%28physical%0Aattack%29%20or%20images%20%28digital%20attack%29%2C%20which%20is%20intuitively%20acceptable%20and%0Aunderstandable%20in%20terms%20of%20the%20attack%27s%20effectiveness.%20In%20contrast%2C%20our%20focus%0Alies%20in%20conducting%20background%20adversarial%20attacks%20in%20both%20digital%20and%20physical%0Adomains%2C%20without%20causing%20any%20disruptions%20to%20the%20targeted%20objects%20themselves.%0ASpecifically%2C%20an%20effective%20background%20adversarial%20attack%20framework%20is%20proposed%0Ato%20attack%20anything%2C%20by%20which%20the%20attack%20efficacy%20generalizes%20well%20between%0Adiverse%20objects%2C%20models%2C%20and%20tasks.%20Technically%2C%20we%20approach%20the%20background%0Aadversarial%20attack%20as%20an%20iterative%20optimization%20problem%2C%20analogous%20to%20the%0Aprocess%20of%20DNN%20learning.%20Besides%2C%20we%20offer%20a%20theoretical%20demonstration%20of%20its%0Aconvergence%20under%20a%20set%20of%20mild%20but%20sufficient%20conditions.%20To%20strengthen%20the%0Aattack%20efficacy%20and%20transferability%2C%20we%20propose%20a%20new%20ensemble%20strategy%0Atailored%20for%20adversarial%20perturbations%20and%20introduce%20an%20improved%20smooth%0Aconstraint%20for%20the%20seamless%20connection%20of%20integrated%20perturbations.%20We%20conduct%0Acomprehensive%20and%20rigorous%20experiments%20in%20both%20digital%20and%20physical%20domains%0Aacross%20various%20objects%2C%20models%2C%20and%20tasks%2C%20demonstrating%20the%20effectiveness%20of%0Aattacking%20anything%20of%20the%20proposed%20method.%20The%20findings%20of%20this%20research%0Asubstantiate%20the%20significant%20discrepancy%20between%20human%20and%20machine%20vision%20on%0Athe%20value%20of%20background%20variations%2C%20which%20play%20a%20far%20more%20critical%20role%20than%0Apreviously%20recognized%2C%20necessitating%20a%20reevaluation%20of%20the%20robustness%20and%0Areliability%20of%20DNNs.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/JiaweiLian/Attack_Anything%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttack%2520Anything%253A%2520Blind%2520DNNs%2520via%2520Universal%2520Background%2520Adversarial%2520Attack%26entry.906535625%3DJiawei%2520Lian%2520and%2520Shaohui%2520Mei%2520and%2520Xiaofei%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Lefan%2520Wang%2520and%2520Yingjie%2520Lu%2520and%2520Mingyang%2520Ma%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520widely%2520substantiated%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%250Asusceptible%2520and%2520vulnerable%2520to%2520adversarial%2520perturbations.%2520Existing%2520studies%250Amainly%2520focus%2520on%2520performing%2520attacks%2520by%2520corrupting%2520targeted%2520objects%2520%2528physical%250Aattack%2529%2520or%2520images%2520%2528digital%2520attack%2529%252C%2520which%2520is%2520intuitively%2520acceptable%2520and%250Aunderstandable%2520in%2520terms%2520of%2520the%2520attack%2527s%2520effectiveness.%2520In%2520contrast%252C%2520our%2520focus%250Alies%2520in%2520conducting%2520background%2520adversarial%2520attacks%2520in%2520both%2520digital%2520and%2520physical%250Adomains%252C%2520without%2520causing%2520any%2520disruptions%2520to%2520the%2520targeted%2520objects%2520themselves.%250ASpecifically%252C%2520an%2520effective%2520background%2520adversarial%2520attack%2520framework%2520is%2520proposed%250Ato%2520attack%2520anything%252C%2520by%2520which%2520the%2520attack%2520efficacy%2520generalizes%2520well%2520between%250Adiverse%2520objects%252C%2520models%252C%2520and%2520tasks.%2520Technically%252C%2520we%2520approach%2520the%2520background%250Aadversarial%2520attack%2520as%2520an%2520iterative%2520optimization%2520problem%252C%2520analogous%2520to%2520the%250Aprocess%2520of%2520DNN%2520learning.%2520Besides%252C%2520we%2520offer%2520a%2520theoretical%2520demonstration%2520of%2520its%250Aconvergence%2520under%2520a%2520set%2520of%2520mild%2520but%2520sufficient%2520conditions.%2520To%2520strengthen%2520the%250Aattack%2520efficacy%2520and%2520transferability%252C%2520we%2520propose%2520a%2520new%2520ensemble%2520strategy%250Atailored%2520for%2520adversarial%2520perturbations%2520and%2520introduce%2520an%2520improved%2520smooth%250Aconstraint%2520for%2520the%2520seamless%2520connection%2520of%2520integrated%2520perturbations.%2520We%2520conduct%250Acomprehensive%2520and%2520rigorous%2520experiments%2520in%2520both%2520digital%2520and%2520physical%2520domains%250Aacross%2520various%2520objects%252C%2520models%252C%2520and%2520tasks%252C%2520demonstrating%2520the%2520effectiveness%2520of%250Aattacking%2520anything%2520of%2520the%2520proposed%2520method.%2520The%2520findings%2520of%2520this%2520research%250Asubstantiate%2520the%2520significant%2520discrepancy%2520between%2520human%2520and%2520machine%2520vision%2520on%250Athe%2520value%2520of%2520background%2520variations%252C%2520which%2520play%2520a%2520far%2520more%2520critical%2520role%2520than%250Apreviously%2520recognized%252C%2520necessitating%2520a%2520reevaluation%2520of%2520the%2520robustness%2520and%250Areliability%2520of%2520DNNs.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JiaweiLian/Attack_Anything%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attack%20Anything%3A%20Blind%20DNNs%20via%20Universal%20Background%20Adversarial%20Attack&entry.906535625=Jiawei%20Lian%20and%20Shaohui%20Mei%20and%20Xiaofei%20Wang%20and%20Yi%20Wang%20and%20Lefan%20Wang%20and%20Yingjie%20Lu%20and%20Mingyang%20Ma%20and%20Lap-Pui%20Chau&entry.1292438233=%20%20It%20has%20been%20widely%20substantiated%20that%20deep%20neural%20networks%20%28DNNs%29%20are%0Asusceptible%20and%20vulnerable%20to%20adversarial%20perturbations.%20Existing%20studies%0Amainly%20focus%20on%20performing%20attacks%20by%20corrupting%20targeted%20objects%20%28physical%0Aattack%29%20or%20images%20%28digital%20attack%29%2C%20which%20is%20intuitively%20acceptable%20and%0Aunderstandable%20in%20terms%20of%20the%20attack%27s%20effectiveness.%20In%20contrast%2C%20our%20focus%0Alies%20in%20conducting%20background%20adversarial%20attacks%20in%20both%20digital%20and%20physical%0Adomains%2C%20without%20causing%20any%20disruptions%20to%20the%20targeted%20objects%20themselves.%0ASpecifically%2C%20an%20effective%20background%20adversarial%20attack%20framework%20is%20proposed%0Ato%20attack%20anything%2C%20by%20which%20the%20attack%20efficacy%20generalizes%20well%20between%0Adiverse%20objects%2C%20models%2C%20and%20tasks.%20Technically%2C%20we%20approach%20the%20background%0Aadversarial%20attack%20as%20an%20iterative%20optimization%20problem%2C%20analogous%20to%20the%0Aprocess%20of%20DNN%20learning.%20Besides%2C%20we%20offer%20a%20theoretical%20demonstration%20of%20its%0Aconvergence%20under%20a%20set%20of%20mild%20but%20sufficient%20conditions.%20To%20strengthen%20the%0Aattack%20efficacy%20and%20transferability%2C%20we%20propose%20a%20new%20ensemble%20strategy%0Atailored%20for%20adversarial%20perturbations%20and%20introduce%20an%20improved%20smooth%0Aconstraint%20for%20the%20seamless%20connection%20of%20integrated%20perturbations.%20We%20conduct%0Acomprehensive%20and%20rigorous%20experiments%20in%20both%20digital%20and%20physical%20domains%0Aacross%20various%20objects%2C%20models%2C%20and%20tasks%2C%20demonstrating%20the%20effectiveness%20of%0Aattacking%20anything%20of%20the%20proposed%20method.%20The%20findings%20of%20this%20research%0Asubstantiate%20the%20significant%20discrepancy%20between%20human%20and%20machine%20vision%20on%0Athe%20value%20of%20background%20variations%2C%20which%20play%20a%20far%20more%20critical%20role%20than%0Apreviously%20recognized%2C%20necessitating%20a%20reevaluation%20of%20the%20robustness%20and%0Areliability%20of%20DNNs.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/JiaweiLian/Attack_Anything%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00029v2&entry.124074799=Read"},
{"title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration", "author": "Anik Sarker and Alan T. Asbeck", "abstract": "  Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images.\n", "link": "http://arxiv.org/abs/2508.02339v1", "date": "2025-08-04", "relevancy": 2.4982, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5186}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4996}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correspondence-Free%20Fast%20and%20Robust%20Spherical%20Point%20Pattern%20Registration&body=Title%3A%20Correspondence-Free%20Fast%20and%20Robust%20Spherical%20Point%20Pattern%20Registration%0AAuthor%3A%20Anik%20Sarker%20and%20Alan%20T.%20Asbeck%0AAbstract%3A%20%20%20Existing%20methods%20for%20rotation%20estimation%20between%20two%20spherical%0A%28%24%5Cmathbb%7BS%7D%5E2%24%29%20patterns%20typically%20rely%20on%20spherical%20cross-correlation%0Amaximization%20between%20two%20spherical%20function.%20However%2C%20these%20approaches%20exhibit%0Acomputational%20complexities%20greater%20than%20cubic%20%24O%28n%5E3%29%24%20with%20respect%20to%20rotation%0Aspace%20discretization%20and%20lack%20extensive%20evaluation%20under%20significant%20outlier%0Acontamination.%20To%20this%20end%2C%20we%20propose%20a%20rotation%20estimation%20algorithm%20between%0Atwo%20spherical%20patterns%20with%20linear%20time%20complexity%20%24O%28n%29%24.%20Unlike%20existing%0Aspherical-function-based%20methods%2C%20we%20explicitly%20represent%20spherical%20patterns%20as%0Adiscrete%203D%20point%20sets%20on%20the%20unit%20sphere%2C%20reformulating%20rotation%20estimation%20as%0Aa%20spherical%20point-set%20alignment%20%28i.e.%2C%20Wahba%20problem%20for%203D%20unit%20vectors%29.%0AGiven%20the%20geometric%20nature%20of%20our%20formulation%2C%20our%20spherical%20pattern%20alignment%0Aalgorithm%20naturally%20aligns%20with%20the%20Wahba%20problem%20framework%20for%203D%20unit%0Avectors.%20Specifically%2C%20we%20introduce%20three%20novel%20algorithms%3A%20%281%29%20SPMC%20%28Spherical%0APattern%20Matching%20by%20Correlation%29%2C%20%282%29%20FRS%20%28Fast%20Rotation%20Search%29%2C%20and%20%283%29%20a%0Ahybrid%20approach%20%28SPMC%2BFRS%29%20that%20combines%20the%20advantages%20of%20the%20previous%20two%0Amethods.%20Our%20experiments%20demonstrate%20that%20in%20the%20%24%5Cmathbb%7BS%7D%5E2%24%20domain%20and%20in%0Acorrespondence-free%20settings%2C%20our%20algorithms%20are%20over%2010x%20faster%20and%20over%2010x%0Amore%20accurate%20than%20current%20state-of-the-art%20methods%20for%20the%20Wahba%20problem%20with%0Aoutliers.%20We%20validate%20our%20approach%20through%20extensive%20simulations%20on%20a%20new%0Adataset%20of%20spherical%20patterns%2C%20the%20%60%60Robust%20Vector%20Alignment%20Dataset.%0A%22Furthermore%2C%20we%20adapt%20our%20methods%20to%20two%20real-world%20tasks%3A%20%28i%29%20Point%20Cloud%0ARegistration%20%28PCR%29%20and%20%28ii%29%20rotation%20estimation%20for%20spherical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrespondence-Free%2520Fast%2520and%2520Robust%2520Spherical%2520Point%2520Pattern%2520Registration%26entry.906535625%3DAnik%2520Sarker%2520and%2520Alan%2520T.%2520Asbeck%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520rotation%2520estimation%2520between%2520two%2520spherical%250A%2528%2524%255Cmathbb%257BS%257D%255E2%2524%2529%2520patterns%2520typically%2520rely%2520on%2520spherical%2520cross-correlation%250Amaximization%2520between%2520two%2520spherical%2520function.%2520However%252C%2520these%2520approaches%2520exhibit%250Acomputational%2520complexities%2520greater%2520than%2520cubic%2520%2524O%2528n%255E3%2529%2524%2520with%2520respect%2520to%2520rotation%250Aspace%2520discretization%2520and%2520lack%2520extensive%2520evaluation%2520under%2520significant%2520outlier%250Acontamination.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520rotation%2520estimation%2520algorithm%2520between%250Atwo%2520spherical%2520patterns%2520with%2520linear%2520time%2520complexity%2520%2524O%2528n%2529%2524.%2520Unlike%2520existing%250Aspherical-function-based%2520methods%252C%2520we%2520explicitly%2520represent%2520spherical%2520patterns%2520as%250Adiscrete%25203D%2520point%2520sets%2520on%2520the%2520unit%2520sphere%252C%2520reformulating%2520rotation%2520estimation%2520as%250Aa%2520spherical%2520point-set%2520alignment%2520%2528i.e.%252C%2520Wahba%2520problem%2520for%25203D%2520unit%2520vectors%2529.%250AGiven%2520the%2520geometric%2520nature%2520of%2520our%2520formulation%252C%2520our%2520spherical%2520pattern%2520alignment%250Aalgorithm%2520naturally%2520aligns%2520with%2520the%2520Wahba%2520problem%2520framework%2520for%25203D%2520unit%250Avectors.%2520Specifically%252C%2520we%2520introduce%2520three%2520novel%2520algorithms%253A%2520%25281%2529%2520SPMC%2520%2528Spherical%250APattern%2520Matching%2520by%2520Correlation%2529%252C%2520%25282%2529%2520FRS%2520%2528Fast%2520Rotation%2520Search%2529%252C%2520and%2520%25283%2529%2520a%250Ahybrid%2520approach%2520%2528SPMC%252BFRS%2529%2520that%2520combines%2520the%2520advantages%2520of%2520the%2520previous%2520two%250Amethods.%2520Our%2520experiments%2520demonstrate%2520that%2520in%2520the%2520%2524%255Cmathbb%257BS%257D%255E2%2524%2520domain%2520and%2520in%250Acorrespondence-free%2520settings%252C%2520our%2520algorithms%2520are%2520over%252010x%2520faster%2520and%2520over%252010x%250Amore%2520accurate%2520than%2520current%2520state-of-the-art%2520methods%2520for%2520the%2520Wahba%2520problem%2520with%250Aoutliers.%2520We%2520validate%2520our%2520approach%2520through%2520extensive%2520simulations%2520on%2520a%2520new%250Adataset%2520of%2520spherical%2520patterns%252C%2520the%2520%2560%2560Robust%2520Vector%2520Alignment%2520Dataset.%250A%2522Furthermore%252C%2520we%2520adapt%2520our%2520methods%2520to%2520two%2520real-world%2520tasks%253A%2520%2528i%2529%2520Point%2520Cloud%250ARegistration%2520%2528PCR%2529%2520and%2520%2528ii%2529%2520rotation%2520estimation%2520for%2520spherical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correspondence-Free%20Fast%20and%20Robust%20Spherical%20Point%20Pattern%20Registration&entry.906535625=Anik%20Sarker%20and%20Alan%20T.%20Asbeck&entry.1292438233=%20%20Existing%20methods%20for%20rotation%20estimation%20between%20two%20spherical%0A%28%24%5Cmathbb%7BS%7D%5E2%24%29%20patterns%20typically%20rely%20on%20spherical%20cross-correlation%0Amaximization%20between%20two%20spherical%20function.%20However%2C%20these%20approaches%20exhibit%0Acomputational%20complexities%20greater%20than%20cubic%20%24O%28n%5E3%29%24%20with%20respect%20to%20rotation%0Aspace%20discretization%20and%20lack%20extensive%20evaluation%20under%20significant%20outlier%0Acontamination.%20To%20this%20end%2C%20we%20propose%20a%20rotation%20estimation%20algorithm%20between%0Atwo%20spherical%20patterns%20with%20linear%20time%20complexity%20%24O%28n%29%24.%20Unlike%20existing%0Aspherical-function-based%20methods%2C%20we%20explicitly%20represent%20spherical%20patterns%20as%0Adiscrete%203D%20point%20sets%20on%20the%20unit%20sphere%2C%20reformulating%20rotation%20estimation%20as%0Aa%20spherical%20point-set%20alignment%20%28i.e.%2C%20Wahba%20problem%20for%203D%20unit%20vectors%29.%0AGiven%20the%20geometric%20nature%20of%20our%20formulation%2C%20our%20spherical%20pattern%20alignment%0Aalgorithm%20naturally%20aligns%20with%20the%20Wahba%20problem%20framework%20for%203D%20unit%0Avectors.%20Specifically%2C%20we%20introduce%20three%20novel%20algorithms%3A%20%281%29%20SPMC%20%28Spherical%0APattern%20Matching%20by%20Correlation%29%2C%20%282%29%20FRS%20%28Fast%20Rotation%20Search%29%2C%20and%20%283%29%20a%0Ahybrid%20approach%20%28SPMC%2BFRS%29%20that%20combines%20the%20advantages%20of%20the%20previous%20two%0Amethods.%20Our%20experiments%20demonstrate%20that%20in%20the%20%24%5Cmathbb%7BS%7D%5E2%24%20domain%20and%20in%0Acorrespondence-free%20settings%2C%20our%20algorithms%20are%20over%2010x%20faster%20and%20over%2010x%0Amore%20accurate%20than%20current%20state-of-the-art%20methods%20for%20the%20Wahba%20problem%20with%0Aoutliers.%20We%20validate%20our%20approach%20through%20extensive%20simulations%20on%20a%20new%0Adataset%20of%20spherical%20patterns%2C%20the%20%60%60Robust%20Vector%20Alignment%20Dataset.%0A%22Furthermore%2C%20we%20adapt%20our%20methods%20to%20two%20real-world%20tasks%3A%20%28i%29%20Point%20Cloud%0ARegistration%20%28PCR%29%20and%20%28ii%29%20rotation%20estimation%20for%20spherical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02339v1&entry.124074799=Read"},
{"title": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs", "author": "Yao Lai and Souradip Poddar and Sungyoung Lee and Guojin Chen and Mengkang Hu and Bei Yu and Ping Luo and David Z. Pan", "abstract": "  Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance.\n", "link": "http://arxiv.org/abs/2508.02518v1", "date": "2025-08-04", "relevancy": 2.4914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnalogCoder-Pro%3A%20Unifying%20Analog%20Circuit%20Generation%20and%20Optimization%20via%0A%20%20Multi-modal%20LLMs&body=Title%3A%20AnalogCoder-Pro%3A%20Unifying%20Analog%20Circuit%20Generation%20and%20Optimization%20via%0A%20%20Multi-modal%20LLMs%0AAuthor%3A%20Yao%20Lai%20and%20Souradip%20Poddar%20and%20Sungyoung%20Lee%20and%20Guojin%20Chen%20and%20Mengkang%20Hu%20and%20Bei%20Yu%20and%20Ping%20Luo%20and%20David%20Z.%20Pan%0AAbstract%3A%20%20%20Despite%20advances%20in%20analog%20design%20automation%2C%20analog%20front-end%20design%20still%0Aheavily%20depends%20on%20expert%20intuition%20and%20iterative%20simulations%2C%20underscoring%0Acritical%20gaps%20in%20fully%20automated%20optimization%20for%20performance-critical%0Aapplications.%20Recently%2C%20the%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20brought%20new%20promise%20to%20analog%20design%20automation.%20However%2C%20existing%20work%0Aremains%20in%20its%20early%20stages%2C%20and%20holistic%20joint%20optimization%20for%20practical%0Aend-to-end%20solutions%20remains%20largely%20unexplored.%20We%20propose%20AnalogCoder-Pro%2C%20a%0Aunified%20multimodal%20LLM-based%20framework%20that%20integrates%20generative%20capabilities%0Aand%20optimization%20techniques%20to%20jointly%20explore%20circuit%20topologies%20and%20optimize%0Adevice%20sizing%2C%20automatically%20generating%20performance-specific%2C%20fully%20sized%0Aschematic%20netlists.%20AnalogCoder-Pro%20employs%20rejection%20sampling%20for%20fine-tuning%0ALLMs%20on%20high-quality%20synthesized%20circuit%20data%20and%20introduces%20a%20multimodal%0Adiagnosis%20and%20repair%20workflow%20based%20on%20functional%20specifications%20and%20waveform%0Aimages.%20By%20leveraging%20LLMs%20to%20interpret%20generated%20circuit%20netlists%2C%0AAnalogCoder-Pro%20automates%20the%20extraction%20of%20critical%20design%20parameters%20and%20the%0Aformulation%20of%20parameter%20spaces%2C%20establishing%20an%20end-to-end%20workflow%20for%0Asimultaneous%20topology%20generation%20and%20device%20sizing%20optimization.%20Extensive%0Aexperiments%20demonstrate%20that%20these%20orthogonal%20approaches%20significantly%20improve%0Athe%20success%20rate%20of%20analog%20circuit%20design%20and%20enhance%20circuit%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalogCoder-Pro%253A%2520Unifying%2520Analog%2520Circuit%2520Generation%2520and%2520Optimization%2520via%250A%2520%2520Multi-modal%2520LLMs%26entry.906535625%3DYao%2520Lai%2520and%2520Souradip%2520Poddar%2520and%2520Sungyoung%2520Lee%2520and%2520Guojin%2520Chen%2520and%2520Mengkang%2520Hu%2520and%2520Bei%2520Yu%2520and%2520Ping%2520Luo%2520and%2520David%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520Despite%2520advances%2520in%2520analog%2520design%2520automation%252C%2520analog%2520front-end%2520design%2520still%250Aheavily%2520depends%2520on%2520expert%2520intuition%2520and%2520iterative%2520simulations%252C%2520underscoring%250Acritical%2520gaps%2520in%2520fully%2520automated%2520optimization%2520for%2520performance-critical%250Aapplications.%2520Recently%252C%2520the%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ahas%2520brought%2520new%2520promise%2520to%2520analog%2520design%2520automation.%2520However%252C%2520existing%2520work%250Aremains%2520in%2520its%2520early%2520stages%252C%2520and%2520holistic%2520joint%2520optimization%2520for%2520practical%250Aend-to-end%2520solutions%2520remains%2520largely%2520unexplored.%2520We%2520propose%2520AnalogCoder-Pro%252C%2520a%250Aunified%2520multimodal%2520LLM-based%2520framework%2520that%2520integrates%2520generative%2520capabilities%250Aand%2520optimization%2520techniques%2520to%2520jointly%2520explore%2520circuit%2520topologies%2520and%2520optimize%250Adevice%2520sizing%252C%2520automatically%2520generating%2520performance-specific%252C%2520fully%2520sized%250Aschematic%2520netlists.%2520AnalogCoder-Pro%2520employs%2520rejection%2520sampling%2520for%2520fine-tuning%250ALLMs%2520on%2520high-quality%2520synthesized%2520circuit%2520data%2520and%2520introduces%2520a%2520multimodal%250Adiagnosis%2520and%2520repair%2520workflow%2520based%2520on%2520functional%2520specifications%2520and%2520waveform%250Aimages.%2520By%2520leveraging%2520LLMs%2520to%2520interpret%2520generated%2520circuit%2520netlists%252C%250AAnalogCoder-Pro%2520automates%2520the%2520extraction%2520of%2520critical%2520design%2520parameters%2520and%2520the%250Aformulation%2520of%2520parameter%2520spaces%252C%2520establishing%2520an%2520end-to-end%2520workflow%2520for%250Asimultaneous%2520topology%2520generation%2520and%2520device%2520sizing%2520optimization.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520these%2520orthogonal%2520approaches%2520significantly%2520improve%250Athe%2520success%2520rate%2520of%2520analog%2520circuit%2520design%2520and%2520enhance%2520circuit%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnalogCoder-Pro%3A%20Unifying%20Analog%20Circuit%20Generation%20and%20Optimization%20via%0A%20%20Multi-modal%20LLMs&entry.906535625=Yao%20Lai%20and%20Souradip%20Poddar%20and%20Sungyoung%20Lee%20and%20Guojin%20Chen%20and%20Mengkang%20Hu%20and%20Bei%20Yu%20and%20Ping%20Luo%20and%20David%20Z.%20Pan&entry.1292438233=%20%20Despite%20advances%20in%20analog%20design%20automation%2C%20analog%20front-end%20design%20still%0Aheavily%20depends%20on%20expert%20intuition%20and%20iterative%20simulations%2C%20underscoring%0Acritical%20gaps%20in%20fully%20automated%20optimization%20for%20performance-critical%0Aapplications.%20Recently%2C%20the%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20brought%20new%20promise%20to%20analog%20design%20automation.%20However%2C%20existing%20work%0Aremains%20in%20its%20early%20stages%2C%20and%20holistic%20joint%20optimization%20for%20practical%0Aend-to-end%20solutions%20remains%20largely%20unexplored.%20We%20propose%20AnalogCoder-Pro%2C%20a%0Aunified%20multimodal%20LLM-based%20framework%20that%20integrates%20generative%20capabilities%0Aand%20optimization%20techniques%20to%20jointly%20explore%20circuit%20topologies%20and%20optimize%0Adevice%20sizing%2C%20automatically%20generating%20performance-specific%2C%20fully%20sized%0Aschematic%20netlists.%20AnalogCoder-Pro%20employs%20rejection%20sampling%20for%20fine-tuning%0ALLMs%20on%20high-quality%20synthesized%20circuit%20data%20and%20introduces%20a%20multimodal%0Adiagnosis%20and%20repair%20workflow%20based%20on%20functional%20specifications%20and%20waveform%0Aimages.%20By%20leveraging%20LLMs%20to%20interpret%20generated%20circuit%20netlists%2C%0AAnalogCoder-Pro%20automates%20the%20extraction%20of%20critical%20design%20parameters%20and%20the%0Aformulation%20of%20parameter%20spaces%2C%20establishing%20an%20end-to-end%20workflow%20for%0Asimultaneous%20topology%20generation%20and%20device%20sizing%20optimization.%20Extensive%0Aexperiments%20demonstrate%20that%20these%20orthogonal%20approaches%20significantly%20improve%0Athe%20success%20rate%20of%20analog%20circuit%20design%20and%20enhance%20circuit%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02518v1&entry.124074799=Read"},
{"title": "Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation", "author": "Nikos Zarifis and Puqian Wang and Ilias Diakonikolas and Jelena Diakonikolas", "abstract": "  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n", "link": "http://arxiv.org/abs/2502.08611v2", "date": "2025-08-04", "relevancy": 2.4889, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5074}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation&body=Title%3A%20Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation%0AAuthor%3A%20Nikos%20Zarifis%20and%20Puqian%20Wang%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20learning%20Generalized%20Linear%20models%20%28GLMs%29%20in%20the%0Aagnostic%20model%20under%20the%20Gaussian%20distribution.%20We%20give%20the%20first%0Apolynomial-time%20algorithm%20that%20achieves%20a%20constant-factor%20approximation%20for%0A%5Ctextit%7Bany%7D%20monotone%20Lipschitz%20activation.%20Prior%20constant-factor%20GLM%20learners%0Asucceed%20for%20a%20substantially%20smaller%20class%20of%20activations.%20Our%20work%20resolves%20a%0Awell-known%20open%20problem%2C%20by%20developing%20a%20robust%20counterpart%20to%20the%20classical%0AGLMtron%20algorithm%20%28Kakade%20et%20al.%2C%202011%29.%20Our%20robust%20learner%20applies%20more%0Agenerally%2C%20encompassing%20all%20monotone%20activations%20with%20bounded%0A%24%282%2B%5Czeta%29%24-moments%2C%20for%20any%20fixed%20%24%5Czeta%3E0%24%20--%20a%20condition%20that%20is%20essentially%0Anecessary.%20To%20obtain%20our%20results%2C%20we%20leverage%20a%20novel%20data%20augmentation%0Atechnique%20with%20decreasing%20Gaussian%20noise%20injection%20and%20prove%20a%20number%20of%0Astructural%20results%20that%20may%20be%20useful%20in%20other%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustly%2520Learning%2520Monotone%2520Generalized%2520Linear%2520Models%2520via%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DNikos%2520Zarifis%2520and%2520Puqian%2520Wang%2520and%2520Ilias%2520Diakonikolas%2520and%2520Jelena%2520Diakonikolas%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520learning%2520Generalized%2520Linear%2520models%2520%2528GLMs%2529%2520in%2520the%250Aagnostic%2520model%2520under%2520the%2520Gaussian%2520distribution.%2520We%2520give%2520the%2520first%250Apolynomial-time%2520algorithm%2520that%2520achieves%2520a%2520constant-factor%2520approximation%2520for%250A%255Ctextit%257Bany%257D%2520monotone%2520Lipschitz%2520activation.%2520Prior%2520constant-factor%2520GLM%2520learners%250Asucceed%2520for%2520a%2520substantially%2520smaller%2520class%2520of%2520activations.%2520Our%2520work%2520resolves%2520a%250Awell-known%2520open%2520problem%252C%2520by%2520developing%2520a%2520robust%2520counterpart%2520to%2520the%2520classical%250AGLMtron%2520algorithm%2520%2528Kakade%2520et%2520al.%252C%25202011%2529.%2520Our%2520robust%2520learner%2520applies%2520more%250Agenerally%252C%2520encompassing%2520all%2520monotone%2520activations%2520with%2520bounded%250A%2524%25282%252B%255Czeta%2529%2524-moments%252C%2520for%2520any%2520fixed%2520%2524%255Czeta%253E0%2524%2520--%2520a%2520condition%2520that%2520is%2520essentially%250Anecessary.%2520To%2520obtain%2520our%2520results%252C%2520we%2520leverage%2520a%2520novel%2520data%2520augmentation%250Atechnique%2520with%2520decreasing%2520Gaussian%2520noise%2520injection%2520and%2520prove%2520a%2520number%2520of%250Astructural%2520results%2520that%2520may%2520be%2520useful%2520in%2520other%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation&entry.906535625=Nikos%20Zarifis%20and%20Puqian%20Wang%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas&entry.1292438233=%20%20We%20study%20the%20task%20of%20learning%20Generalized%20Linear%20models%20%28GLMs%29%20in%20the%0Aagnostic%20model%20under%20the%20Gaussian%20distribution.%20We%20give%20the%20first%0Apolynomial-time%20algorithm%20that%20achieves%20a%20constant-factor%20approximation%20for%0A%5Ctextit%7Bany%7D%20monotone%20Lipschitz%20activation.%20Prior%20constant-factor%20GLM%20learners%0Asucceed%20for%20a%20substantially%20smaller%20class%20of%20activations.%20Our%20work%20resolves%20a%0Awell-known%20open%20problem%2C%20by%20developing%20a%20robust%20counterpart%20to%20the%20classical%0AGLMtron%20algorithm%20%28Kakade%20et%20al.%2C%202011%29.%20Our%20robust%20learner%20applies%20more%0Agenerally%2C%20encompassing%20all%20monotone%20activations%20with%20bounded%0A%24%282%2B%5Czeta%29%24-moments%2C%20for%20any%20fixed%20%24%5Czeta%3E0%24%20--%20a%20condition%20that%20is%20essentially%0Anecessary.%20To%20obtain%20our%20results%2C%20we%20leverage%20a%20novel%20data%20augmentation%0Atechnique%20with%20decreasing%20Gaussian%20noise%20injection%20and%20prove%20a%20number%20of%0Astructural%20results%20that%20may%20be%20useful%20in%20other%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08611v2&entry.124074799=Read"},
{"title": "ReMoMask: Retrieval-Augmented Masked Motion Generation", "author": "Zhengdao Li and Siheng Wang and Zeyu Zhang and Hao Tang", "abstract": "  Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.\n", "link": "http://arxiv.org/abs/2508.02605v1", "date": "2025-08-04", "relevancy": 2.4696, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.646}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6133}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.61}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReMoMask%3A%20Retrieval-Augmented%20Masked%20Motion%20Generation&body=Title%3A%20ReMoMask%3A%20Retrieval-Augmented%20Masked%20Motion%20Generation%0AAuthor%3A%20Zhengdao%20Li%20and%20Siheng%20Wang%20and%20Zeyu%20Zhang%20and%20Hao%20Tang%0AAbstract%3A%20%20%20Text-to-Motion%20%28T2M%29%20generation%20aims%20to%20synthesize%20realistic%20and%20semantically%0Aaligned%20human%20motion%20sequences%20from%20natural%20language%20descriptions.%20However%2C%0Acurrent%20approaches%20face%20dual%20challenges%3A%20Generative%20models%20%28e.g.%2C%20diffusion%0Amodels%29%20suffer%20from%20limited%20diversity%2C%20error%20accumulation%2C%20and%20physical%0Aimplausibility%2C%20while%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20exhibit%0Adiffusion%20inertia%2C%20partial-mode%20collapse%2C%20and%20asynchronous%20artifacts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20ReMoMask%2C%20a%20unified%20framework%20integrating%0Athree%20key%20innovations%3A%201%29%20A%20Bidirectional%20Momentum%20Text-Motion%20Model%20decouples%0Anegative%20sample%20scale%20from%20batch%20size%20via%20momentum%20queues%2C%20substantially%0Aimproving%20cross-modal%20retrieval%20precision%3B%202%29%20A%20Semantic%20Spatio-temporal%0AAttention%20mechanism%20enforces%20biomechanical%20constraints%20during%20part-level%20fusion%0Ato%20eliminate%20asynchronous%20artifacts%3B%203%29%20RAG-Classier-Free%20Guidance%20incorporates%0Aminor%20unconditional%20generation%20to%20enhance%20generalization.%20Built%20upon%20MoMask%27s%0ARVQ-VAE%2C%20ReMoMask%20efficiently%20generates%20temporally%20coherent%20motions%20in%20minimal%0Asteps.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20ReMoMask%2C%20achieving%20a%203.88%25%20and%2010.97%25%0Aimprovement%20in%20FID%20scores%20on%20HumanML3D%20and%20KIT-ML%2C%20respectively%2C%20compared%20to%0Athe%20previous%20SOTA%20method%20RAG-T2M.%20Code%3A%0Ahttps%3A//github.com/AIGeeksGroup/ReMoMask.%20Website%3A%0Ahttps%3A//aigeeksgroup.github.io/ReMoMask.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReMoMask%253A%2520Retrieval-Augmented%2520Masked%2520Motion%2520Generation%26entry.906535625%3DZhengdao%2520Li%2520and%2520Siheng%2520Wang%2520and%2520Zeyu%2520Zhang%2520and%2520Hao%2520Tang%26entry.1292438233%3D%2520%2520Text-to-Motion%2520%2528T2M%2529%2520generation%2520aims%2520to%2520synthesize%2520realistic%2520and%2520semantically%250Aaligned%2520human%2520motion%2520sequences%2520from%2520natural%2520language%2520descriptions.%2520However%252C%250Acurrent%2520approaches%2520face%2520dual%2520challenges%253A%2520Generative%2520models%2520%2528e.g.%252C%2520diffusion%250Amodels%2529%2520suffer%2520from%2520limited%2520diversity%252C%2520error%2520accumulation%252C%2520and%2520physical%250Aimplausibility%252C%2520while%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520methods%2520exhibit%250Adiffusion%2520inertia%252C%2520partial-mode%2520collapse%252C%2520and%2520asynchronous%2520artifacts.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520ReMoMask%252C%2520a%2520unified%2520framework%2520integrating%250Athree%2520key%2520innovations%253A%25201%2529%2520A%2520Bidirectional%2520Momentum%2520Text-Motion%2520Model%2520decouples%250Anegative%2520sample%2520scale%2520from%2520batch%2520size%2520via%2520momentum%2520queues%252C%2520substantially%250Aimproving%2520cross-modal%2520retrieval%2520precision%253B%25202%2529%2520A%2520Semantic%2520Spatio-temporal%250AAttention%2520mechanism%2520enforces%2520biomechanical%2520constraints%2520during%2520part-level%2520fusion%250Ato%2520eliminate%2520asynchronous%2520artifacts%253B%25203%2529%2520RAG-Classier-Free%2520Guidance%2520incorporates%250Aminor%2520unconditional%2520generation%2520to%2520enhance%2520generalization.%2520Built%2520upon%2520MoMask%2527s%250ARVQ-VAE%252C%2520ReMoMask%2520efficiently%2520generates%2520temporally%2520coherent%2520motions%2520in%2520minimal%250Asteps.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520demonstrate%2520the%250Astate-of-the-art%2520performance%2520of%2520ReMoMask%252C%2520achieving%2520a%25203.88%2525%2520and%252010.97%2525%250Aimprovement%2520in%2520FID%2520scores%2520on%2520HumanML3D%2520and%2520KIT-ML%252C%2520respectively%252C%2520compared%2520to%250Athe%2520previous%2520SOTA%2520method%2520RAG-T2M.%2520Code%253A%250Ahttps%253A//github.com/AIGeeksGroup/ReMoMask.%2520Website%253A%250Ahttps%253A//aigeeksgroup.github.io/ReMoMask.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMoMask%3A%20Retrieval-Augmented%20Masked%20Motion%20Generation&entry.906535625=Zhengdao%20Li%20and%20Siheng%20Wang%20and%20Zeyu%20Zhang%20and%20Hao%20Tang&entry.1292438233=%20%20Text-to-Motion%20%28T2M%29%20generation%20aims%20to%20synthesize%20realistic%20and%20semantically%0Aaligned%20human%20motion%20sequences%20from%20natural%20language%20descriptions.%20However%2C%0Acurrent%20approaches%20face%20dual%20challenges%3A%20Generative%20models%20%28e.g.%2C%20diffusion%0Amodels%29%20suffer%20from%20limited%20diversity%2C%20error%20accumulation%2C%20and%20physical%0Aimplausibility%2C%20while%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20exhibit%0Adiffusion%20inertia%2C%20partial-mode%20collapse%2C%20and%20asynchronous%20artifacts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20ReMoMask%2C%20a%20unified%20framework%20integrating%0Athree%20key%20innovations%3A%201%29%20A%20Bidirectional%20Momentum%20Text-Motion%20Model%20decouples%0Anegative%20sample%20scale%20from%20batch%20size%20via%20momentum%20queues%2C%20substantially%0Aimproving%20cross-modal%20retrieval%20precision%3B%202%29%20A%20Semantic%20Spatio-temporal%0AAttention%20mechanism%20enforces%20biomechanical%20constraints%20during%20part-level%20fusion%0Ato%20eliminate%20asynchronous%20artifacts%3B%203%29%20RAG-Classier-Free%20Guidance%20incorporates%0Aminor%20unconditional%20generation%20to%20enhance%20generalization.%20Built%20upon%20MoMask%27s%0ARVQ-VAE%2C%20ReMoMask%20efficiently%20generates%20temporally%20coherent%20motions%20in%20minimal%0Asteps.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20ReMoMask%2C%20achieving%20a%203.88%25%20and%2010.97%25%0Aimprovement%20in%20FID%20scores%20on%20HumanML3D%20and%20KIT-ML%2C%20respectively%2C%20compared%20to%0Athe%20previous%20SOTA%20method%20RAG-T2M.%20Code%3A%0Ahttps%3A//github.com/AIGeeksGroup/ReMoMask.%20Website%3A%0Ahttps%3A//aigeeksgroup.github.io/ReMoMask.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02605v1&entry.124074799=Read"},
{"title": "A Novel Sliced Fused Gromov-Wasserstein Distance", "author": "Moritz Piening and Robert Beinert", "abstract": "  The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are\npowerful tools for comparing heterogeneous data. Their computation is, however,\nchallenging since both distances are based on non-convex, quadratic optimal\ntransport (OT) problems. Leveraging 1D OT, a sliced version of GW has been\nproposed to lower the computational burden. Unfortunately, this sliced version\nis restricted to Euclidean geometry and loses invariance to isometries,\nstrongly limiting its application in practice. To overcome these issues, we\npropose a novel slicing technique for GW as well as for FGW that is based on an\nappropriate lower bound, hierarchical OT, and suitable quadrature rules for the\nunderlying 1D OT problems. Our novel sliced FGW significantly reduces the\nnumerical effort while remaining invariant to isometric transformations and\nallowing the comparison of arbitrary geometries. We show that our new distance\nactually defines a pseudo-metric for structured spaces that bounds FGW from\nbelow and study its interpolation properties between sliced Wasserstein and GW.\nSince we avoid the underlying quadratic program, our sliced distance is\nnumerically more robust and reliable than the original GW and FGW distance;\nespecially in the context of shape retrieval and graph isomorphism testing.\n", "link": "http://arxiv.org/abs/2508.02364v1", "date": "2025-08-04", "relevancy": 2.4648, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5055}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4935}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance&body=Title%3A%20A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance%0AAuthor%3A%20Moritz%20Piening%20and%20Robert%20Beinert%0AAbstract%3A%20%20%20The%20Gromov--Wasserstein%20%28GW%29%20distance%20and%20its%20fused%20extension%20%28FGW%29%20are%0Apowerful%20tools%20for%20comparing%20heterogeneous%20data.%20Their%20computation%20is%2C%20however%2C%0Achallenging%20since%20both%20distances%20are%20based%20on%20non-convex%2C%20quadratic%20optimal%0Atransport%20%28OT%29%20problems.%20Leveraging%201D%20OT%2C%20a%20sliced%20version%20of%20GW%20has%20been%0Aproposed%20to%20lower%20the%20computational%20burden.%20Unfortunately%2C%20this%20sliced%20version%0Ais%20restricted%20to%20Euclidean%20geometry%20and%20loses%20invariance%20to%20isometries%2C%0Astrongly%20limiting%20its%20application%20in%20practice.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20novel%20slicing%20technique%20for%20GW%20as%20well%20as%20for%20FGW%20that%20is%20based%20on%20an%0Aappropriate%20lower%20bound%2C%20hierarchical%20OT%2C%20and%20suitable%20quadrature%20rules%20for%20the%0Aunderlying%201D%20OT%20problems.%20Our%20novel%20sliced%20FGW%20significantly%20reduces%20the%0Anumerical%20effort%20while%20remaining%20invariant%20to%20isometric%20transformations%20and%0Aallowing%20the%20comparison%20of%20arbitrary%20geometries.%20We%20show%20that%20our%20new%20distance%0Aactually%20defines%20a%20pseudo-metric%20for%20structured%20spaces%20that%20bounds%20FGW%20from%0Abelow%20and%20study%20its%20interpolation%20properties%20between%20sliced%20Wasserstein%20and%20GW.%0ASince%20we%20avoid%20the%20underlying%20quadratic%20program%2C%20our%20sliced%20distance%20is%0Anumerically%20more%20robust%20and%20reliable%20than%20the%20original%20GW%20and%20FGW%20distance%3B%0Aespecially%20in%20the%20context%20of%20shape%20retrieval%20and%20graph%20isomorphism%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Sliced%2520Fused%2520Gromov-Wasserstein%2520Distance%26entry.906535625%3DMoritz%2520Piening%2520and%2520Robert%2520Beinert%26entry.1292438233%3D%2520%2520The%2520Gromov--Wasserstein%2520%2528GW%2529%2520distance%2520and%2520its%2520fused%2520extension%2520%2528FGW%2529%2520are%250Apowerful%2520tools%2520for%2520comparing%2520heterogeneous%2520data.%2520Their%2520computation%2520is%252C%2520however%252C%250Achallenging%2520since%2520both%2520distances%2520are%2520based%2520on%2520non-convex%252C%2520quadratic%2520optimal%250Atransport%2520%2528OT%2529%2520problems.%2520Leveraging%25201D%2520OT%252C%2520a%2520sliced%2520version%2520of%2520GW%2520has%2520been%250Aproposed%2520to%2520lower%2520the%2520computational%2520burden.%2520Unfortunately%252C%2520this%2520sliced%2520version%250Ais%2520restricted%2520to%2520Euclidean%2520geometry%2520and%2520loses%2520invariance%2520to%2520isometries%252C%250Astrongly%2520limiting%2520its%2520application%2520in%2520practice.%2520To%2520overcome%2520these%2520issues%252C%2520we%250Apropose%2520a%2520novel%2520slicing%2520technique%2520for%2520GW%2520as%2520well%2520as%2520for%2520FGW%2520that%2520is%2520based%2520on%2520an%250Aappropriate%2520lower%2520bound%252C%2520hierarchical%2520OT%252C%2520and%2520suitable%2520quadrature%2520rules%2520for%2520the%250Aunderlying%25201D%2520OT%2520problems.%2520Our%2520novel%2520sliced%2520FGW%2520significantly%2520reduces%2520the%250Anumerical%2520effort%2520while%2520remaining%2520invariant%2520to%2520isometric%2520transformations%2520and%250Aallowing%2520the%2520comparison%2520of%2520arbitrary%2520geometries.%2520We%2520show%2520that%2520our%2520new%2520distance%250Aactually%2520defines%2520a%2520pseudo-metric%2520for%2520structured%2520spaces%2520that%2520bounds%2520FGW%2520from%250Abelow%2520and%2520study%2520its%2520interpolation%2520properties%2520between%2520sliced%2520Wasserstein%2520and%2520GW.%250ASince%2520we%2520avoid%2520the%2520underlying%2520quadratic%2520program%252C%2520our%2520sliced%2520distance%2520is%250Anumerically%2520more%2520robust%2520and%2520reliable%2520than%2520the%2520original%2520GW%2520and%2520FGW%2520distance%253B%250Aespecially%2520in%2520the%2520context%2520of%2520shape%2520retrieval%2520and%2520graph%2520isomorphism%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance&entry.906535625=Moritz%20Piening%20and%20Robert%20Beinert&entry.1292438233=%20%20The%20Gromov--Wasserstein%20%28GW%29%20distance%20and%20its%20fused%20extension%20%28FGW%29%20are%0Apowerful%20tools%20for%20comparing%20heterogeneous%20data.%20Their%20computation%20is%2C%20however%2C%0Achallenging%20since%20both%20distances%20are%20based%20on%20non-convex%2C%20quadratic%20optimal%0Atransport%20%28OT%29%20problems.%20Leveraging%201D%20OT%2C%20a%20sliced%20version%20of%20GW%20has%20been%0Aproposed%20to%20lower%20the%20computational%20burden.%20Unfortunately%2C%20this%20sliced%20version%0Ais%20restricted%20to%20Euclidean%20geometry%20and%20loses%20invariance%20to%20isometries%2C%0Astrongly%20limiting%20its%20application%20in%20practice.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20novel%20slicing%20technique%20for%20GW%20as%20well%20as%20for%20FGW%20that%20is%20based%20on%20an%0Aappropriate%20lower%20bound%2C%20hierarchical%20OT%2C%20and%20suitable%20quadrature%20rules%20for%20the%0Aunderlying%201D%20OT%20problems.%20Our%20novel%20sliced%20FGW%20significantly%20reduces%20the%0Anumerical%20effort%20while%20remaining%20invariant%20to%20isometric%20transformations%20and%0Aallowing%20the%20comparison%20of%20arbitrary%20geometries.%20We%20show%20that%20our%20new%20distance%0Aactually%20defines%20a%20pseudo-metric%20for%20structured%20spaces%20that%20bounds%20FGW%20from%0Abelow%20and%20study%20its%20interpolation%20properties%20between%20sliced%20Wasserstein%20and%20GW.%0ASince%20we%20avoid%20the%20underlying%20quadratic%20program%2C%20our%20sliced%20distance%20is%0Anumerically%20more%20robust%20and%20reliable%20than%20the%20original%20GW%20and%20FGW%20distance%3B%0Aespecially%20in%20the%20context%20of%20shape%20retrieval%20and%20graph%20isomorphism%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02364v1&entry.124074799=Read"},
{"title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to\n  Unlocking Effective Reinforcement Learning", "author": "Jia Deng and Jie Chen and Zhipeng Chen and Wayne Xin Zhao and Ji-Rong Wen", "abstract": "  Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.\n", "link": "http://arxiv.org/abs/2508.02260v1", "date": "2025-08-04", "relevancy": 2.4579, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20the%20Entropy-Performance%20Exchange%3A%20The%20Missing%20Keys%20to%0A%20%20Unlocking%20Effective%20Reinforcement%20Learning&body=Title%3A%20Decomposing%20the%20Entropy-Performance%20Exchange%3A%20The%20Missing%20Keys%20to%0A%20%20Unlocking%20Effective%20Reinforcement%20Learning%0AAuthor%3A%20Jia%20Deng%20and%20Jie%20Chen%20and%20Zhipeng%20Chen%20and%20Wayne%20Xin%20Zhao%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Recently%2C%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20been%0Awidely%20used%20for%20enhancing%20the%20reasoning%20abilities%20of%20large%20language%20models%0A%28LLMs%29.%20A%20core%20challenge%20in%20RLVR%20involves%20managing%20the%20exchange%20between%20entropy%0Aand%20performance%20of%20policies.%20Despite%20the%20importance%20of%20this%20exchange%2C%20a%0Afine-grained%20understanding%20of%20when%20and%20how%20this%20exchange%20operates%20most%0Aeffectively%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20conduct%20a%20systematic%0Aempirical%20analysis%20of%20the%20entropy-performance%20exchange%20mechanism%20of%20RLVR%20across%0Adifferent%20levels%20of%20granularity.%20Specifically%2C%20we%20first%20divide%20the%20training%0Aprocess%20into%20two%20distinct%20stages%20based%20on%20entropy%20dynamics%2C%20i.e.%2C%20rising%20stage%0Aand%20plateau%20stage%2C%20and%20then%20systematically%20investigate%20how%20this%20mechanism%0Avaries%20across%20stage-level%2C%20instance-level%2C%20and%20token-level%20granularitiess.%20Our%0Aanalysis%20reveals%20that%2C%20in%20the%20rising%20stage%2C%20entropy%20reduction%20in%20negative%0Asamples%20facilitates%20the%20learning%20of%20effective%20reasoning%20patterns%2C%20which%20in%20turn%0Adrives%20rapid%20performance%20gains.%20Moreover%2C%20in%20the%20plateau%20stage%2C%20learning%0Aefficiency%20strongly%20correlates%20with%20high-entropy%20tokens%20present%20in%0Alow-perplexity%20samples%20and%20those%20located%20at%20the%20end%20of%20sequences.%20Motivated%20by%0Athese%20findings%2C%20we%20propose%20two%20methods%20that%20dynamically%20adjust%20the%20reward%0Asignal%20using%20perplexity%20and%20positional%20information%20to%20focus%20RL%20updates%20on%0Atokens%20that%20exhibit%20high%20learning%20potential%2C%20achieving%20improvements%20compared%20to%0Athe%20baseline%20methods%20on%20various%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520the%2520Entropy-Performance%2520Exchange%253A%2520The%2520Missing%2520Keys%2520to%250A%2520%2520Unlocking%2520Effective%2520Reinforcement%2520Learning%26entry.906535625%3DJia%2520Deng%2520and%2520Jie%2520Chen%2520and%2520Zhipeng%2520Chen%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520been%250Awidely%2520used%2520for%2520enhancing%2520the%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529.%2520A%2520core%2520challenge%2520in%2520RLVR%2520involves%2520managing%2520the%2520exchange%2520between%2520entropy%250Aand%2520performance%2520of%2520policies.%2520Despite%2520the%2520importance%2520of%2520this%2520exchange%252C%2520a%250Afine-grained%2520understanding%2520of%2520when%2520and%2520how%2520this%2520exchange%2520operates%2520most%250Aeffectively%2520remains%2520limited.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520conduct%2520a%2520systematic%250Aempirical%2520analysis%2520of%2520the%2520entropy-performance%2520exchange%2520mechanism%2520of%2520RLVR%2520across%250Adifferent%2520levels%2520of%2520granularity.%2520Specifically%252C%2520we%2520first%2520divide%2520the%2520training%250Aprocess%2520into%2520two%2520distinct%2520stages%2520based%2520on%2520entropy%2520dynamics%252C%2520i.e.%252C%2520rising%2520stage%250Aand%2520plateau%2520stage%252C%2520and%2520then%2520systematically%2520investigate%2520how%2520this%2520mechanism%250Avaries%2520across%2520stage-level%252C%2520instance-level%252C%2520and%2520token-level%2520granularitiess.%2520Our%250Aanalysis%2520reveals%2520that%252C%2520in%2520the%2520rising%2520stage%252C%2520entropy%2520reduction%2520in%2520negative%250Asamples%2520facilitates%2520the%2520learning%2520of%2520effective%2520reasoning%2520patterns%252C%2520which%2520in%2520turn%250Adrives%2520rapid%2520performance%2520gains.%2520Moreover%252C%2520in%2520the%2520plateau%2520stage%252C%2520learning%250Aefficiency%2520strongly%2520correlates%2520with%2520high-entropy%2520tokens%2520present%2520in%250Alow-perplexity%2520samples%2520and%2520those%2520located%2520at%2520the%2520end%2520of%2520sequences.%2520Motivated%2520by%250Athese%2520findings%252C%2520we%2520propose%2520two%2520methods%2520that%2520dynamically%2520adjust%2520the%2520reward%250Asignal%2520using%2520perplexity%2520and%2520positional%2520information%2520to%2520focus%2520RL%2520updates%2520on%250Atokens%2520that%2520exhibit%2520high%2520learning%2520potential%252C%2520achieving%2520improvements%2520compared%2520to%250Athe%2520baseline%2520methods%2520on%2520various%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20the%20Entropy-Performance%20Exchange%3A%20The%20Missing%20Keys%20to%0A%20%20Unlocking%20Effective%20Reinforcement%20Learning&entry.906535625=Jia%20Deng%20and%20Jie%20Chen%20and%20Zhipeng%20Chen%20and%20Wayne%20Xin%20Zhao%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Recently%2C%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20been%0Awidely%20used%20for%20enhancing%20the%20reasoning%20abilities%20of%20large%20language%20models%0A%28LLMs%29.%20A%20core%20challenge%20in%20RLVR%20involves%20managing%20the%20exchange%20between%20entropy%0Aand%20performance%20of%20policies.%20Despite%20the%20importance%20of%20this%20exchange%2C%20a%0Afine-grained%20understanding%20of%20when%20and%20how%20this%20exchange%20operates%20most%0Aeffectively%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20conduct%20a%20systematic%0Aempirical%20analysis%20of%20the%20entropy-performance%20exchange%20mechanism%20of%20RLVR%20across%0Adifferent%20levels%20of%20granularity.%20Specifically%2C%20we%20first%20divide%20the%20training%0Aprocess%20into%20two%20distinct%20stages%20based%20on%20entropy%20dynamics%2C%20i.e.%2C%20rising%20stage%0Aand%20plateau%20stage%2C%20and%20then%20systematically%20investigate%20how%20this%20mechanism%0Avaries%20across%20stage-level%2C%20instance-level%2C%20and%20token-level%20granularitiess.%20Our%0Aanalysis%20reveals%20that%2C%20in%20the%20rising%20stage%2C%20entropy%20reduction%20in%20negative%0Asamples%20facilitates%20the%20learning%20of%20effective%20reasoning%20patterns%2C%20which%20in%20turn%0Adrives%20rapid%20performance%20gains.%20Moreover%2C%20in%20the%20plateau%20stage%2C%20learning%0Aefficiency%20strongly%20correlates%20with%20high-entropy%20tokens%20present%20in%0Alow-perplexity%20samples%20and%20those%20located%20at%20the%20end%20of%20sequences.%20Motivated%20by%0Athese%20findings%2C%20we%20propose%20two%20methods%20that%20dynamically%20adjust%20the%20reward%0Asignal%20using%20perplexity%20and%20positional%20information%20to%20focus%20RL%20updates%20on%0Atokens%20that%20exhibit%20high%20learning%20potential%2C%20achieving%20improvements%20compared%20to%0Athe%20baseline%20methods%20on%20various%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02260v1&entry.124074799=Read"},
{"title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "author": "Ruijie Lu and Yixin Chen and Yu Liu and Jiaxiang Tang and Junfeng Ni and Diwen Wan and Gang Zeng and Siyuan Huang", "abstract": "  Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.\n", "link": "http://arxiv.org/abs/2503.12049v2", "date": "2025-08-04", "relevancy": 2.4575, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACO%3A%20Taming%20Diffusion%20for%20in-the-wild%20Video%20Amodal%20Completion&body=Title%3A%20TACO%3A%20Taming%20Diffusion%20for%20in-the-wild%20Video%20Amodal%20Completion%0AAuthor%3A%20Ruijie%20Lu%20and%20Yixin%20Chen%20and%20Yu%20Liu%20and%20Jiaxiang%20Tang%20and%20Junfeng%20Ni%20and%20Diwen%20Wan%20and%20Gang%20Zeng%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Humans%20can%20infer%20complete%20shapes%20and%20appearances%20of%20objects%20from%20limited%0Avisual%20cues%2C%20relying%20on%20extensive%20prior%20knowledge%20of%20the%20physical%20world.%0AHowever%2C%20completing%20partially%20observable%20objects%20while%20ensuring%20consistency%0Aacross%20video%20frames%20remains%20challenging%20for%20existing%20models%2C%20especially%20for%0Aunstructured%2C%20in-the-wild%20videos.%20This%20paper%20tackles%20the%20task%20of%20Video%20Amodal%0ACompletion%20%28VAC%29%2C%20which%20aims%20to%20generate%20the%20complete%20object%20consistently%0Athroughout%20the%20video%20given%20a%20visual%20prompt%20specifying%20the%20object%20of%20interest.%0ALeveraging%20the%20rich%2C%20consistent%20manifolds%20learned%20by%20pre-trained%20video%0Adiffusion%20models%2C%20we%20propose%20a%20conditional%20diffusion%20model%2C%20TACO%2C%20that%0Arepurposes%20these%20manifolds%20for%20VAC.%20To%20enable%20its%20effective%20and%20robust%0Ageneralization%20to%20challenging%20in-the-wild%20scenarios%2C%20we%20curate%20a%20large-scale%0Asynthetic%20dataset%20with%20multiple%20difficulty%20levels%20by%20systematically%20imposing%0Aocclusions%20onto%20un-occluded%20videos.%20Building%20on%20this%2C%20we%20devise%20a%20progressive%0Afine-tuning%20paradigm%20that%20starts%20with%20simpler%20recovery%20tasks%20and%20gradually%0Aadvances%20to%20more%20complex%20ones.%20We%20demonstrate%20TACO%27s%20versatility%20on%20a%20wide%0Arange%20of%20in-the-wild%20videos%20from%20Internet%2C%20as%20well%20as%20on%20diverse%2C%20unseen%0Adatasets%20commonly%20used%20in%20autonomous%20driving%2C%20robotic%20manipulation%2C%20and%20scene%0Aunderstanding.%20Moreover%2C%20we%20show%20that%20TACO%20can%20be%20effectively%20applied%20to%0Avarious%20downstream%20tasks%20like%20object%20reconstruction%20and%20pose%20estimation%2C%0Ahighlighting%20its%20potential%20to%20facilitate%20physical%20world%20understanding%20and%0Areasoning.%20Our%20project%20page%20is%20available%20at%20https%3A//jason-aplp.github.io/TACO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACO%253A%2520Taming%2520Diffusion%2520for%2520in-the-wild%2520Video%2520Amodal%2520Completion%26entry.906535625%3DRuijie%2520Lu%2520and%2520Yixin%2520Chen%2520and%2520Yu%2520Liu%2520and%2520Jiaxiang%2520Tang%2520and%2520Junfeng%2520Ni%2520and%2520Diwen%2520Wan%2520and%2520Gang%2520Zeng%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Humans%2520can%2520infer%2520complete%2520shapes%2520and%2520appearances%2520of%2520objects%2520from%2520limited%250Avisual%2520cues%252C%2520relying%2520on%2520extensive%2520prior%2520knowledge%2520of%2520the%2520physical%2520world.%250AHowever%252C%2520completing%2520partially%2520observable%2520objects%2520while%2520ensuring%2520consistency%250Aacross%2520video%2520frames%2520remains%2520challenging%2520for%2520existing%2520models%252C%2520especially%2520for%250Aunstructured%252C%2520in-the-wild%2520videos.%2520This%2520paper%2520tackles%2520the%2520task%2520of%2520Video%2520Amodal%250ACompletion%2520%2528VAC%2529%252C%2520which%2520aims%2520to%2520generate%2520the%2520complete%2520object%2520consistently%250Athroughout%2520the%2520video%2520given%2520a%2520visual%2520prompt%2520specifying%2520the%2520object%2520of%2520interest.%250ALeveraging%2520the%2520rich%252C%2520consistent%2520manifolds%2520learned%2520by%2520pre-trained%2520video%250Adiffusion%2520models%252C%2520we%2520propose%2520a%2520conditional%2520diffusion%2520model%252C%2520TACO%252C%2520that%250Arepurposes%2520these%2520manifolds%2520for%2520VAC.%2520To%2520enable%2520its%2520effective%2520and%2520robust%250Ageneralization%2520to%2520challenging%2520in-the-wild%2520scenarios%252C%2520we%2520curate%2520a%2520large-scale%250Asynthetic%2520dataset%2520with%2520multiple%2520difficulty%2520levels%2520by%2520systematically%2520imposing%250Aocclusions%2520onto%2520un-occluded%2520videos.%2520Building%2520on%2520this%252C%2520we%2520devise%2520a%2520progressive%250Afine-tuning%2520paradigm%2520that%2520starts%2520with%2520simpler%2520recovery%2520tasks%2520and%2520gradually%250Aadvances%2520to%2520more%2520complex%2520ones.%2520We%2520demonstrate%2520TACO%2527s%2520versatility%2520on%2520a%2520wide%250Arange%2520of%2520in-the-wild%2520videos%2520from%2520Internet%252C%2520as%2520well%2520as%2520on%2520diverse%252C%2520unseen%250Adatasets%2520commonly%2520used%2520in%2520autonomous%2520driving%252C%2520robotic%2520manipulation%252C%2520and%2520scene%250Aunderstanding.%2520Moreover%252C%2520we%2520show%2520that%2520TACO%2520can%2520be%2520effectively%2520applied%2520to%250Avarious%2520downstream%2520tasks%2520like%2520object%2520reconstruction%2520and%2520pose%2520estimation%252C%250Ahighlighting%2520its%2520potential%2520to%2520facilitate%2520physical%2520world%2520understanding%2520and%250Areasoning.%2520Our%2520project%2520page%2520is%2520available%2520at%2520https%253A//jason-aplp.github.io/TACO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACO%3A%20Taming%20Diffusion%20for%20in-the-wild%20Video%20Amodal%20Completion&entry.906535625=Ruijie%20Lu%20and%20Yixin%20Chen%20and%20Yu%20Liu%20and%20Jiaxiang%20Tang%20and%20Junfeng%20Ni%20and%20Diwen%20Wan%20and%20Gang%20Zeng%20and%20Siyuan%20Huang&entry.1292438233=%20%20Humans%20can%20infer%20complete%20shapes%20and%20appearances%20of%20objects%20from%20limited%0Avisual%20cues%2C%20relying%20on%20extensive%20prior%20knowledge%20of%20the%20physical%20world.%0AHowever%2C%20completing%20partially%20observable%20objects%20while%20ensuring%20consistency%0Aacross%20video%20frames%20remains%20challenging%20for%20existing%20models%2C%20especially%20for%0Aunstructured%2C%20in-the-wild%20videos.%20This%20paper%20tackles%20the%20task%20of%20Video%20Amodal%0ACompletion%20%28VAC%29%2C%20which%20aims%20to%20generate%20the%20complete%20object%20consistently%0Athroughout%20the%20video%20given%20a%20visual%20prompt%20specifying%20the%20object%20of%20interest.%0ALeveraging%20the%20rich%2C%20consistent%20manifolds%20learned%20by%20pre-trained%20video%0Adiffusion%20models%2C%20we%20propose%20a%20conditional%20diffusion%20model%2C%20TACO%2C%20that%0Arepurposes%20these%20manifolds%20for%20VAC.%20To%20enable%20its%20effective%20and%20robust%0Ageneralization%20to%20challenging%20in-the-wild%20scenarios%2C%20we%20curate%20a%20large-scale%0Asynthetic%20dataset%20with%20multiple%20difficulty%20levels%20by%20systematically%20imposing%0Aocclusions%20onto%20un-occluded%20videos.%20Building%20on%20this%2C%20we%20devise%20a%20progressive%0Afine-tuning%20paradigm%20that%20starts%20with%20simpler%20recovery%20tasks%20and%20gradually%0Aadvances%20to%20more%20complex%20ones.%20We%20demonstrate%20TACO%27s%20versatility%20on%20a%20wide%0Arange%20of%20in-the-wild%20videos%20from%20Internet%2C%20as%20well%20as%20on%20diverse%2C%20unseen%0Adatasets%20commonly%20used%20in%20autonomous%20driving%2C%20robotic%20manipulation%2C%20and%20scene%0Aunderstanding.%20Moreover%2C%20we%20show%20that%20TACO%20can%20be%20effectively%20applied%20to%0Avarious%20downstream%20tasks%20like%20object%20reconstruction%20and%20pose%20estimation%2C%0Ahighlighting%20its%20potential%20to%20facilitate%20physical%20world%20understanding%20and%0Areasoning.%20Our%20project%20page%20is%20available%20at%20https%3A//jason-aplp.github.io/TACO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12049v2&entry.124074799=Read"},
{"title": "Solved in Unit Domain: JacobiNet for Differentiable Coordinate\n  Transformations", "author": "Xi Chen and Jianchuan Yang and Junjie Zhang and Runnan Yang and Xu Liu and Hong Wang and Ziyu Ren and Wenqi Hu", "abstract": "  Physics-Informed Neural Networks (PINNs) are effective for solving PDEs by\nincorporating physical laws into the learning process. However, they face\nchallenges with irregular boundaries, leading to instability and slow\nconvergence due to inconsistent normalization, inaccurate boundary enforcement,\nand imbalanced loss terms. A common solution is to map the domain to a regular\nspace, but traditional methods rely on case-specific meshes and simple\ngeometries, limiting their compatibility with modern frameworks. To overcome\nthese limitations, we introduce JacobiNet, a neural network-based coordinate\ntransformation method that learns continuous, differentiable mappings from\nsupervised point pairs. Utilizing lightweight MLPs, JacobiNet allows for direct\nJacobian computation via autograd and integrates seamlessly with downstream\nPINNs, enabling end-to-end differentiable PDE solving without the need for\nmeshing or explicit Jacobian computation. JacobiNet effectively addresses\nnormalization challenges, facilitates hard constraints of boundary conditions,\nand mitigates the long-standing imbalance among loss terms. It demonstrates\nsignificant improvements, reducing the relative L2 error from 0.287-0.637 to\n0.013-0.039, achieving an average accuracy improvement of 18.3*. In vessel-like\ndomains, it enables rapid mapping for unseen geometries, improving prediction\naccuracy by 3.65* and achieving over 10* speedup, showcasing its\ngeneralization, accuracy, and efficiency.\n", "link": "http://arxiv.org/abs/2508.02537v1", "date": "2025-08-04", "relevancy": 2.4529, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5012}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4857}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solved%20in%20Unit%20Domain%3A%20JacobiNet%20for%20Differentiable%20Coordinate%0A%20%20Transformations&body=Title%3A%20Solved%20in%20Unit%20Domain%3A%20JacobiNet%20for%20Differentiable%20Coordinate%0A%20%20Transformations%0AAuthor%3A%20Xi%20Chen%20and%20Jianchuan%20Yang%20and%20Junjie%20Zhang%20and%20Runnan%20Yang%20and%20Xu%20Liu%20and%20Hong%20Wang%20and%20Ziyu%20Ren%20and%20Wenqi%20Hu%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20effective%20for%20solving%20PDEs%20by%0Aincorporating%20physical%20laws%20into%20the%20learning%20process.%20However%2C%20they%20face%0Achallenges%20with%20irregular%20boundaries%2C%20leading%20to%20instability%20and%20slow%0Aconvergence%20due%20to%20inconsistent%20normalization%2C%20inaccurate%20boundary%20enforcement%2C%0Aand%20imbalanced%20loss%20terms.%20A%20common%20solution%20is%20to%20map%20the%20domain%20to%20a%20regular%0Aspace%2C%20but%20traditional%20methods%20rely%20on%20case-specific%20meshes%20and%20simple%0Ageometries%2C%20limiting%20their%20compatibility%20with%20modern%20frameworks.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20JacobiNet%2C%20a%20neural%20network-based%20coordinate%0Atransformation%20method%20that%20learns%20continuous%2C%20differentiable%20mappings%20from%0Asupervised%20point%20pairs.%20Utilizing%20lightweight%20MLPs%2C%20JacobiNet%20allows%20for%20direct%0AJacobian%20computation%20via%20autograd%20and%20integrates%20seamlessly%20with%20downstream%0APINNs%2C%20enabling%20end-to-end%20differentiable%20PDE%20solving%20without%20the%20need%20for%0Ameshing%20or%20explicit%20Jacobian%20computation.%20JacobiNet%20effectively%20addresses%0Anormalization%20challenges%2C%20facilitates%20hard%20constraints%20of%20boundary%20conditions%2C%0Aand%20mitigates%20the%20long-standing%20imbalance%20among%20loss%20terms.%20It%20demonstrates%0Asignificant%20improvements%2C%20reducing%20the%20relative%20L2%20error%20from%200.287-0.637%20to%0A0.013-0.039%2C%20achieving%20an%20average%20accuracy%20improvement%20of%2018.3%2A.%20In%20vessel-like%0Adomains%2C%20it%20enables%20rapid%20mapping%20for%20unseen%20geometries%2C%20improving%20prediction%0Aaccuracy%20by%203.65%2A%20and%20achieving%20over%2010%2A%20speedup%2C%20showcasing%20its%0Ageneralization%2C%20accuracy%2C%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolved%2520in%2520Unit%2520Domain%253A%2520JacobiNet%2520for%2520Differentiable%2520Coordinate%250A%2520%2520Transformations%26entry.906535625%3DXi%2520Chen%2520and%2520Jianchuan%2520Yang%2520and%2520Junjie%2520Zhang%2520and%2520Runnan%2520Yang%2520and%2520Xu%2520Liu%2520and%2520Hong%2520Wang%2520and%2520Ziyu%2520Ren%2520and%2520Wenqi%2520Hu%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520are%2520effective%2520for%2520solving%2520PDEs%2520by%250Aincorporating%2520physical%2520laws%2520into%2520the%2520learning%2520process.%2520However%252C%2520they%2520face%250Achallenges%2520with%2520irregular%2520boundaries%252C%2520leading%2520to%2520instability%2520and%2520slow%250Aconvergence%2520due%2520to%2520inconsistent%2520normalization%252C%2520inaccurate%2520boundary%2520enforcement%252C%250Aand%2520imbalanced%2520loss%2520terms.%2520A%2520common%2520solution%2520is%2520to%2520map%2520the%2520domain%2520to%2520a%2520regular%250Aspace%252C%2520but%2520traditional%2520methods%2520rely%2520on%2520case-specific%2520meshes%2520and%2520simple%250Ageometries%252C%2520limiting%2520their%2520compatibility%2520with%2520modern%2520frameworks.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520JacobiNet%252C%2520a%2520neural%2520network-based%2520coordinate%250Atransformation%2520method%2520that%2520learns%2520continuous%252C%2520differentiable%2520mappings%2520from%250Asupervised%2520point%2520pairs.%2520Utilizing%2520lightweight%2520MLPs%252C%2520JacobiNet%2520allows%2520for%2520direct%250AJacobian%2520computation%2520via%2520autograd%2520and%2520integrates%2520seamlessly%2520with%2520downstream%250APINNs%252C%2520enabling%2520end-to-end%2520differentiable%2520PDE%2520solving%2520without%2520the%2520need%2520for%250Ameshing%2520or%2520explicit%2520Jacobian%2520computation.%2520JacobiNet%2520effectively%2520addresses%250Anormalization%2520challenges%252C%2520facilitates%2520hard%2520constraints%2520of%2520boundary%2520conditions%252C%250Aand%2520mitigates%2520the%2520long-standing%2520imbalance%2520among%2520loss%2520terms.%2520It%2520demonstrates%250Asignificant%2520improvements%252C%2520reducing%2520the%2520relative%2520L2%2520error%2520from%25200.287-0.637%2520to%250A0.013-0.039%252C%2520achieving%2520an%2520average%2520accuracy%2520improvement%2520of%252018.3%252A.%2520In%2520vessel-like%250Adomains%252C%2520it%2520enables%2520rapid%2520mapping%2520for%2520unseen%2520geometries%252C%2520improving%2520prediction%250Aaccuracy%2520by%25203.65%252A%2520and%2520achieving%2520over%252010%252A%2520speedup%252C%2520showcasing%2520its%250Ageneralization%252C%2520accuracy%252C%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solved%20in%20Unit%20Domain%3A%20JacobiNet%20for%20Differentiable%20Coordinate%0A%20%20Transformations&entry.906535625=Xi%20Chen%20and%20Jianchuan%20Yang%20and%20Junjie%20Zhang%20and%20Runnan%20Yang%20and%20Xu%20Liu%20and%20Hong%20Wang%20and%20Ziyu%20Ren%20and%20Wenqi%20Hu&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20effective%20for%20solving%20PDEs%20by%0Aincorporating%20physical%20laws%20into%20the%20learning%20process.%20However%2C%20they%20face%0Achallenges%20with%20irregular%20boundaries%2C%20leading%20to%20instability%20and%20slow%0Aconvergence%20due%20to%20inconsistent%20normalization%2C%20inaccurate%20boundary%20enforcement%2C%0Aand%20imbalanced%20loss%20terms.%20A%20common%20solution%20is%20to%20map%20the%20domain%20to%20a%20regular%0Aspace%2C%20but%20traditional%20methods%20rely%20on%20case-specific%20meshes%20and%20simple%0Ageometries%2C%20limiting%20their%20compatibility%20with%20modern%20frameworks.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20JacobiNet%2C%20a%20neural%20network-based%20coordinate%0Atransformation%20method%20that%20learns%20continuous%2C%20differentiable%20mappings%20from%0Asupervised%20point%20pairs.%20Utilizing%20lightweight%20MLPs%2C%20JacobiNet%20allows%20for%20direct%0AJacobian%20computation%20via%20autograd%20and%20integrates%20seamlessly%20with%20downstream%0APINNs%2C%20enabling%20end-to-end%20differentiable%20PDE%20solving%20without%20the%20need%20for%0Ameshing%20or%20explicit%20Jacobian%20computation.%20JacobiNet%20effectively%20addresses%0Anormalization%20challenges%2C%20facilitates%20hard%20constraints%20of%20boundary%20conditions%2C%0Aand%20mitigates%20the%20long-standing%20imbalance%20among%20loss%20terms.%20It%20demonstrates%0Asignificant%20improvements%2C%20reducing%20the%20relative%20L2%20error%20from%200.287-0.637%20to%0A0.013-0.039%2C%20achieving%20an%20average%20accuracy%20improvement%20of%2018.3%2A.%20In%20vessel-like%0Adomains%2C%20it%20enables%20rapid%20mapping%20for%20unseen%20geometries%2C%20improving%20prediction%0Aaccuracy%20by%203.65%2A%20and%20achieving%20over%2010%2A%20speedup%2C%20showcasing%20its%0Ageneralization%2C%20accuracy%2C%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02537v1&entry.124074799=Read"},
{"title": "LOST: Low-rank and Sparse Pre-training for Large Language Models", "author": "Jiaxi Li and Lu Yin and Li Shen and Jinjin Xu and Liwu Xu and Tianjin Huang and Wenwu Wang and Shiwei Liu and Xilu Wang", "abstract": "  While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}\n", "link": "http://arxiv.org/abs/2508.02668v1", "date": "2025-08-04", "relevancy": 2.4509, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOST%3A%20Low-rank%20and%20Sparse%20Pre-training%20for%20Large%20Language%20Models&body=Title%3A%20LOST%3A%20Low-rank%20and%20Sparse%20Pre-training%20for%20Large%20Language%20Models%0AAuthor%3A%20Jiaxi%20Li%20and%20Lu%20Yin%20and%20Li%20Shen%20and%20Jinjin%20Xu%20and%20Liwu%20Xu%20and%20Tianjin%20Huang%20and%20Wenwu%20Wang%20and%20Shiwei%20Liu%20and%20Xilu%20Wang%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%0Aacross%20a%20wide%20range%20of%20tasks%2C%20their%20massive%20scale%20incurs%20prohibitive%0Acomputational%20and%20memory%20costs%20for%20pre-training%20from%20scratch.%20Recent%20studies%0Ahave%20investigated%20the%20use%20of%20low-rank%20parameterization%20as%20a%20means%20of%20reducing%0Amodel%20size%20and%20training%20cost.%20In%20this%20context%2C%20sparsity%20is%20often%20employed%20as%20a%0Acomplementary%20technique%20to%20recover%20important%20information%20lost%20in%20low-rank%0Acompression%20by%20capturing%20salient%20features%20in%20the%20residual%20space.%20However%2C%0Aexisting%20approaches%20typically%20combine%20low-rank%20and%20sparse%20components%20in%20a%0Asimplistic%20or%20ad%20hoc%20manner%2C%20often%20resulting%20in%20undesirable%20performance%0Adegradation%20compared%20to%20full-rank%20training.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BLO%7Dw-rank%20and%20%5Ctextbf%7BS%7Dparse%20pre-%5Ctextbf%7BT%7Draining%20%28%5Ctextbf%7BLOST%7D%29%20for%0ALLMs%2C%20a%20novel%20method%20that%20ingeniously%20integrates%20low-rank%20and%20sparse%20structures%0Ato%20enable%20effective%20training%20of%20LLMs%20from%20scratch%20under%20strict%20efficiency%0Aconstraints.%20LOST%20applies%20singular%20value%20decomposition%20to%20weight%20matrices%2C%0Apreserving%20the%20dominant%20low-rank%20components%2C%20while%20allocating%20the%20remaining%0Asingular%20values%20to%20construct%20channel-wise%20sparse%20components%20to%20complement%20the%0Aexpressiveness%20of%20low-rank%20training.%20We%20evaluate%20LOST%20on%20LLM%20pretraining%0Aranging%20from%2060M%20to%207B%20parameters.%20Our%20experiments%20show%20that%20LOST%20achieves%0Acompetitive%20or%20superior%20performance%20compared%20to%20full-rank%20models%2C%20while%0Asignificantly%20reducing%20both%20memory%20and%20compute%20overhead.%20Moreover%2C%20Code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models%7D%7BLOST%0ARepo%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOST%253A%2520Low-rank%2520and%2520Sparse%2520Pre-training%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJiaxi%2520Li%2520and%2520Lu%2520Yin%2520and%2520Li%2520Shen%2520and%2520Jinjin%2520Xu%2520and%2520Liwu%2520Xu%2520and%2520Tianjin%2520Huang%2520and%2520Wenwu%2520Wang%2520and%2520Shiwei%2520Liu%2520and%2520Xilu%2520Wang%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%252C%2520their%2520massive%2520scale%2520incurs%2520prohibitive%250Acomputational%2520and%2520memory%2520costs%2520for%2520pre-training%2520from%2520scratch.%2520Recent%2520studies%250Ahave%2520investigated%2520the%2520use%2520of%2520low-rank%2520parameterization%2520as%2520a%2520means%2520of%2520reducing%250Amodel%2520size%2520and%2520training%2520cost.%2520In%2520this%2520context%252C%2520sparsity%2520is%2520often%2520employed%2520as%2520a%250Acomplementary%2520technique%2520to%2520recover%2520important%2520information%2520lost%2520in%2520low-rank%250Acompression%2520by%2520capturing%2520salient%2520features%2520in%2520the%2520residual%2520space.%2520However%252C%250Aexisting%2520approaches%2520typically%2520combine%2520low-rank%2520and%2520sparse%2520components%2520in%2520a%250Asimplistic%2520or%2520ad%2520hoc%2520manner%252C%2520often%2520resulting%2520in%2520undesirable%2520performance%250Adegradation%2520compared%2520to%2520full-rank%2520training.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Ctextbf%257BLO%257Dw-rank%2520and%2520%255Ctextbf%257BS%257Dparse%2520pre-%255Ctextbf%257BT%257Draining%2520%2528%255Ctextbf%257BLOST%257D%2529%2520for%250ALLMs%252C%2520a%2520novel%2520method%2520that%2520ingeniously%2520integrates%2520low-rank%2520and%2520sparse%2520structures%250Ato%2520enable%2520effective%2520training%2520of%2520LLMs%2520from%2520scratch%2520under%2520strict%2520efficiency%250Aconstraints.%2520LOST%2520applies%2520singular%2520value%2520decomposition%2520to%2520weight%2520matrices%252C%250Apreserving%2520the%2520dominant%2520low-rank%2520components%252C%2520while%2520allocating%2520the%2520remaining%250Asingular%2520values%2520to%2520construct%2520channel-wise%2520sparse%2520components%2520to%2520complement%2520the%250Aexpressiveness%2520of%2520low-rank%2520training.%2520We%2520evaluate%2520LOST%2520on%2520LLM%2520pretraining%250Aranging%2520from%252060M%2520to%25207B%2520parameters.%2520Our%2520experiments%2520show%2520that%2520LOST%2520achieves%250Acompetitive%2520or%2520superior%2520performance%2520compared%2520to%2520full-rank%2520models%252C%2520while%250Asignificantly%2520reducing%2520both%2520memory%2520and%2520compute%2520overhead.%2520Moreover%252C%2520Code%2520is%250Aavailable%2520at%250A%255Chref%257Bhttps%253A//github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models%257D%257BLOST%250ARepo%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOST%3A%20Low-rank%20and%20Sparse%20Pre-training%20for%20Large%20Language%20Models&entry.906535625=Jiaxi%20Li%20and%20Lu%20Yin%20and%20Li%20Shen%20and%20Jinjin%20Xu%20and%20Liwu%20Xu%20and%20Tianjin%20Huang%20and%20Wenwu%20Wang%20and%20Shiwei%20Liu%20and%20Xilu%20Wang&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%0Aacross%20a%20wide%20range%20of%20tasks%2C%20their%20massive%20scale%20incurs%20prohibitive%0Acomputational%20and%20memory%20costs%20for%20pre-training%20from%20scratch.%20Recent%20studies%0Ahave%20investigated%20the%20use%20of%20low-rank%20parameterization%20as%20a%20means%20of%20reducing%0Amodel%20size%20and%20training%20cost.%20In%20this%20context%2C%20sparsity%20is%20often%20employed%20as%20a%0Acomplementary%20technique%20to%20recover%20important%20information%20lost%20in%20low-rank%0Acompression%20by%20capturing%20salient%20features%20in%20the%20residual%20space.%20However%2C%0Aexisting%20approaches%20typically%20combine%20low-rank%20and%20sparse%20components%20in%20a%0Asimplistic%20or%20ad%20hoc%20manner%2C%20often%20resulting%20in%20undesirable%20performance%0Adegradation%20compared%20to%20full-rank%20training.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BLO%7Dw-rank%20and%20%5Ctextbf%7BS%7Dparse%20pre-%5Ctextbf%7BT%7Draining%20%28%5Ctextbf%7BLOST%7D%29%20for%0ALLMs%2C%20a%20novel%20method%20that%20ingeniously%20integrates%20low-rank%20and%20sparse%20structures%0Ato%20enable%20effective%20training%20of%20LLMs%20from%20scratch%20under%20strict%20efficiency%0Aconstraints.%20LOST%20applies%20singular%20value%20decomposition%20to%20weight%20matrices%2C%0Apreserving%20the%20dominant%20low-rank%20components%2C%20while%20allocating%20the%20remaining%0Asingular%20values%20to%20construct%20channel-wise%20sparse%20components%20to%20complement%20the%0Aexpressiveness%20of%20low-rank%20training.%20We%20evaluate%20LOST%20on%20LLM%20pretraining%0Aranging%20from%2060M%20to%207B%20parameters.%20Our%20experiments%20show%20that%20LOST%20achieves%0Acompetitive%20or%20superior%20performance%20compared%20to%20full-rank%20models%2C%20while%0Asignificantly%20reducing%20both%20memory%20and%20compute%20overhead.%20Moreover%2C%20Code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models%7D%7BLOST%0ARepo%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02668v1&entry.124074799=Read"},
{"title": "Friend or Foe? Harnessing Controllable Overfitting for Anomaly Detection", "author": "Long Qian and Bingke Zhu and Yingying Chen and Ming Tang and Jinqiao Wang", "abstract": "  Overfitting has traditionally been viewed as detrimental to anomaly\ndetection, where excessive generalization often limits models' sensitivity to\nsubtle anomalies. Our work challenges this conventional view by introducing\nControllable Overfitting-based Anomaly Detection (COAD), a novel framework that\nstrategically leverages overfitting to enhance anomaly discrimination\ncapabilities. We propose the Aberrance Retention Quotient (ARQ), a novel metric\nthat systematically quantifies the extent of overfitting, enabling the\nidentification of an optimal golden overfitting interval wherein model\nsensitivity to anomalies is maximized without sacrificing generalization. To\ncomprehensively capture how overfitting affects detection performance, we\nfurther propose the Relative Anomaly Distribution Index (RADI), a metric\nsuperior to traditional AUROC by explicitly modeling the separation between\nnormal and anomalous score distributions. Theoretically, RADI leverages ARQ to\ntrack and evaluate how overfitting impacts anomaly detection, offering an\nintegrated approach to understanding the relationship between overfitting\ndynamics and model efficacy. We also rigorously validate the statistical\nefficacy of Gaussian noise as pseudo-anomaly generators, reinforcing the\nmethod's broad applicability. Empirical evaluations demonstrate that our\ncontrollable overfitting method achieves State-Of-The-Art(SOTA) performance in\nboth one-class and multi-class anomaly detection tasks, thus redefining\noverfitting as a powerful strategy rather than a limitation.\n", "link": "http://arxiv.org/abs/2412.00560v2", "date": "2025-08-04", "relevancy": 2.4304, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5091}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4764}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Friend%20or%20Foe%3F%20Harnessing%20Controllable%20Overfitting%20for%20Anomaly%20Detection&body=Title%3A%20Friend%20or%20Foe%3F%20Harnessing%20Controllable%20Overfitting%20for%20Anomaly%20Detection%0AAuthor%3A%20Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Overfitting%20has%20traditionally%20been%20viewed%20as%20detrimental%20to%20anomaly%0Adetection%2C%20where%20excessive%20generalization%20often%20limits%20models%27%20sensitivity%20to%0Asubtle%20anomalies.%20Our%20work%20challenges%20this%20conventional%20view%20by%20introducing%0AControllable%20Overfitting-based%20Anomaly%20Detection%20%28COAD%29%2C%20a%20novel%20framework%20that%0Astrategically%20leverages%20overfitting%20to%20enhance%20anomaly%20discrimination%0Acapabilities.%20We%20propose%20the%20Aberrance%20Retention%20Quotient%20%28ARQ%29%2C%20a%20novel%20metric%0Athat%20systematically%20quantifies%20the%20extent%20of%20overfitting%2C%20enabling%20the%0Aidentification%20of%20an%20optimal%20golden%20overfitting%20interval%20wherein%20model%0Asensitivity%20to%20anomalies%20is%20maximized%20without%20sacrificing%20generalization.%20To%0Acomprehensively%20capture%20how%20overfitting%20affects%20detection%20performance%2C%20we%0Afurther%20propose%20the%20Relative%20Anomaly%20Distribution%20Index%20%28RADI%29%2C%20a%20metric%0Asuperior%20to%20traditional%20AUROC%20by%20explicitly%20modeling%20the%20separation%20between%0Anormal%20and%20anomalous%20score%20distributions.%20Theoretically%2C%20RADI%20leverages%20ARQ%20to%0Atrack%20and%20evaluate%20how%20overfitting%20impacts%20anomaly%20detection%2C%20offering%20an%0Aintegrated%20approach%20to%20understanding%20the%20relationship%20between%20overfitting%0Adynamics%20and%20model%20efficacy.%20We%20also%20rigorously%20validate%20the%20statistical%0Aefficacy%20of%20Gaussian%20noise%20as%20pseudo-anomaly%20generators%2C%20reinforcing%20the%0Amethod%27s%20broad%20applicability.%20Empirical%20evaluations%20demonstrate%20that%20our%0Acontrollable%20overfitting%20method%20achieves%20State-Of-The-Art%28SOTA%29%20performance%20in%0Aboth%20one-class%20and%20multi-class%20anomaly%20detection%20tasks%2C%20thus%20redefining%0Aoverfitting%20as%20a%20powerful%20strategy%20rather%20than%20a%20limitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00560v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFriend%2520or%2520Foe%253F%2520Harnessing%2520Controllable%2520Overfitting%2520for%2520Anomaly%2520Detection%26entry.906535625%3DLong%2520Qian%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Overfitting%2520has%2520traditionally%2520been%2520viewed%2520as%2520detrimental%2520to%2520anomaly%250Adetection%252C%2520where%2520excessive%2520generalization%2520often%2520limits%2520models%2527%2520sensitivity%2520to%250Asubtle%2520anomalies.%2520Our%2520work%2520challenges%2520this%2520conventional%2520view%2520by%2520introducing%250AControllable%2520Overfitting-based%2520Anomaly%2520Detection%2520%2528COAD%2529%252C%2520a%2520novel%2520framework%2520that%250Astrategically%2520leverages%2520overfitting%2520to%2520enhance%2520anomaly%2520discrimination%250Acapabilities.%2520We%2520propose%2520the%2520Aberrance%2520Retention%2520Quotient%2520%2528ARQ%2529%252C%2520a%2520novel%2520metric%250Athat%2520systematically%2520quantifies%2520the%2520extent%2520of%2520overfitting%252C%2520enabling%2520the%250Aidentification%2520of%2520an%2520optimal%2520golden%2520overfitting%2520interval%2520wherein%2520model%250Asensitivity%2520to%2520anomalies%2520is%2520maximized%2520without%2520sacrificing%2520generalization.%2520To%250Acomprehensively%2520capture%2520how%2520overfitting%2520affects%2520detection%2520performance%252C%2520we%250Afurther%2520propose%2520the%2520Relative%2520Anomaly%2520Distribution%2520Index%2520%2528RADI%2529%252C%2520a%2520metric%250Asuperior%2520to%2520traditional%2520AUROC%2520by%2520explicitly%2520modeling%2520the%2520separation%2520between%250Anormal%2520and%2520anomalous%2520score%2520distributions.%2520Theoretically%252C%2520RADI%2520leverages%2520ARQ%2520to%250Atrack%2520and%2520evaluate%2520how%2520overfitting%2520impacts%2520anomaly%2520detection%252C%2520offering%2520an%250Aintegrated%2520approach%2520to%2520understanding%2520the%2520relationship%2520between%2520overfitting%250Adynamics%2520and%2520model%2520efficacy.%2520We%2520also%2520rigorously%2520validate%2520the%2520statistical%250Aefficacy%2520of%2520Gaussian%2520noise%2520as%2520pseudo-anomaly%2520generators%252C%2520reinforcing%2520the%250Amethod%2527s%2520broad%2520applicability.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%250Acontrollable%2520overfitting%2520method%2520achieves%2520State-Of-The-Art%2528SOTA%2529%2520performance%2520in%250Aboth%2520one-class%2520and%2520multi-class%2520anomaly%2520detection%2520tasks%252C%2520thus%2520redefining%250Aoverfitting%2520as%2520a%2520powerful%2520strategy%2520rather%2520than%2520a%2520limitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00560v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Friend%20or%20Foe%3F%20Harnessing%20Controllable%20Overfitting%20for%20Anomaly%20Detection&entry.906535625=Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Overfitting%20has%20traditionally%20been%20viewed%20as%20detrimental%20to%20anomaly%0Adetection%2C%20where%20excessive%20generalization%20often%20limits%20models%27%20sensitivity%20to%0Asubtle%20anomalies.%20Our%20work%20challenges%20this%20conventional%20view%20by%20introducing%0AControllable%20Overfitting-based%20Anomaly%20Detection%20%28COAD%29%2C%20a%20novel%20framework%20that%0Astrategically%20leverages%20overfitting%20to%20enhance%20anomaly%20discrimination%0Acapabilities.%20We%20propose%20the%20Aberrance%20Retention%20Quotient%20%28ARQ%29%2C%20a%20novel%20metric%0Athat%20systematically%20quantifies%20the%20extent%20of%20overfitting%2C%20enabling%20the%0Aidentification%20of%20an%20optimal%20golden%20overfitting%20interval%20wherein%20model%0Asensitivity%20to%20anomalies%20is%20maximized%20without%20sacrificing%20generalization.%20To%0Acomprehensively%20capture%20how%20overfitting%20affects%20detection%20performance%2C%20we%0Afurther%20propose%20the%20Relative%20Anomaly%20Distribution%20Index%20%28RADI%29%2C%20a%20metric%0Asuperior%20to%20traditional%20AUROC%20by%20explicitly%20modeling%20the%20separation%20between%0Anormal%20and%20anomalous%20score%20distributions.%20Theoretically%2C%20RADI%20leverages%20ARQ%20to%0Atrack%20and%20evaluate%20how%20overfitting%20impacts%20anomaly%20detection%2C%20offering%20an%0Aintegrated%20approach%20to%20understanding%20the%20relationship%20between%20overfitting%0Adynamics%20and%20model%20efficacy.%20We%20also%20rigorously%20validate%20the%20statistical%0Aefficacy%20of%20Gaussian%20noise%20as%20pseudo-anomaly%20generators%2C%20reinforcing%20the%0Amethod%27s%20broad%20applicability.%20Empirical%20evaluations%20demonstrate%20that%20our%0Acontrollable%20overfitting%20method%20achieves%20State-Of-The-Art%28SOTA%29%20performance%20in%0Aboth%20one-class%20and%20multi-class%20anomaly%20detection%20tasks%2C%20thus%20redefining%0Aoverfitting%20as%20a%20powerful%20strategy%20rather%20than%20a%20limitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00560v2&entry.124074799=Read"},
{"title": "Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs\n  for Customer Expansion", "author": "Xinyue Feng and Shuxin Zhong and Jinquan Hang and Wenjun Lyu and Yuequn Zhang and Guang Yang and Haotian Wang and Desheng Zhang and Guang Wang", "abstract": "  Customer expansion, i.e., growing a business existing customer base by\nacquiring new customers, is critical for scaling operations and sustaining the\nlong-term profitability of logistics companies. Although state-of-the-art works\nmodel this task as a single-node classification problem under a heterogeneous\ngraph learning framework and achieve good performance, they struggle with\nextremely positive label sparsity issues in our scenario. Multi-task learning\n(MTL) offers a promising solution by introducing a correlated, label-rich task\nto enhance the label-sparse task prediction through knowledge sharing. However,\nexisting MTL methods result in performance degradation because they fail to\ndiscriminate task-shared and task-specific structural patterns across tasks.\nThis issue arises from their limited consideration of the inherently complex\nstructure learning process of heterogeneous graph neural networks, which\ninvolves the multi-layer aggregation of multi-type relations. To address the\nchallenge, we propose a Structure-Aware Hierarchical Information Sharing\nFramework (SrucHIS), which explicitly regulates structural information sharing\nacross tasks in logistics customer expansion. SrucHIS breaks down the structure\nlearning phase into multiple stages and introduces sharing mechanisms at each\nstage, effectively mitigating the influence of task-specific structural\npatterns during each stage. We evaluate StrucHIS on both private and public\ndatasets, achieving a 51.41% average precision improvement on the private\ndataset and a 10.52% macro F1 gain on the public dataset. StrucHIS is further\ndeployed at one of the largest logistics companies in China and demonstrates a\n41.67% improvement in the success contract-signing rate over existing\nstrategies, generating over 453K new orders within just two months.\n", "link": "http://arxiv.org/abs/2410.22089v2", "date": "2025-08-04", "relevancy": 2.4223, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4861}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Structure%20Sharing%20Empowers%20Multi-task%20Heterogeneous%20GNNs%0A%20%20for%20Customer%20Expansion&body=Title%3A%20Hierarchical%20Structure%20Sharing%20Empowers%20Multi-task%20Heterogeneous%20GNNs%0A%20%20for%20Customer%20Expansion%0AAuthor%3A%20Xinyue%20Feng%20and%20Shuxin%20Zhong%20and%20Jinquan%20Hang%20and%20Wenjun%20Lyu%20and%20Yuequn%20Zhang%20and%20Guang%20Yang%20and%20Haotian%20Wang%20and%20Desheng%20Zhang%20and%20Guang%20Wang%0AAbstract%3A%20%20%20Customer%20expansion%2C%20i.e.%2C%20growing%20a%20business%20existing%20customer%20base%20by%0Aacquiring%20new%20customers%2C%20is%20critical%20for%20scaling%20operations%20and%20sustaining%20the%0Along-term%20profitability%20of%20logistics%20companies.%20Although%20state-of-the-art%20works%0Amodel%20this%20task%20as%20a%20single-node%20classification%20problem%20under%20a%20heterogeneous%0Agraph%20learning%20framework%20and%20achieve%20good%20performance%2C%20they%20struggle%20with%0Aextremely%20positive%20label%20sparsity%20issues%20in%20our%20scenario.%20Multi-task%20learning%0A%28MTL%29%20offers%20a%20promising%20solution%20by%20introducing%20a%20correlated%2C%20label-rich%20task%0Ato%20enhance%20the%20label-sparse%20task%20prediction%20through%20knowledge%20sharing.%20However%2C%0Aexisting%20MTL%20methods%20result%20in%20performance%20degradation%20because%20they%20fail%20to%0Adiscriminate%20task-shared%20and%20task-specific%20structural%20patterns%20across%20tasks.%0AThis%20issue%20arises%20from%20their%20limited%20consideration%20of%20the%20inherently%20complex%0Astructure%20learning%20process%20of%20heterogeneous%20graph%20neural%20networks%2C%20which%0Ainvolves%20the%20multi-layer%20aggregation%20of%20multi-type%20relations.%20To%20address%20the%0Achallenge%2C%20we%20propose%20a%20Structure-Aware%20Hierarchical%20Information%20Sharing%0AFramework%20%28SrucHIS%29%2C%20which%20explicitly%20regulates%20structural%20information%20sharing%0Aacross%20tasks%20in%20logistics%20customer%20expansion.%20SrucHIS%20breaks%20down%20the%20structure%0Alearning%20phase%20into%20multiple%20stages%20and%20introduces%20sharing%20mechanisms%20at%20each%0Astage%2C%20effectively%20mitigating%20the%20influence%20of%20task-specific%20structural%0Apatterns%20during%20each%20stage.%20We%20evaluate%20StrucHIS%20on%20both%20private%20and%20public%0Adatasets%2C%20achieving%20a%2051.41%25%20average%20precision%20improvement%20on%20the%20private%0Adataset%20and%20a%2010.52%25%20macro%20F1%20gain%20on%20the%20public%20dataset.%20StrucHIS%20is%20further%0Adeployed%20at%20one%20of%20the%20largest%20logistics%20companies%20in%20China%20and%20demonstrates%20a%0A41.67%25%20improvement%20in%20the%20success%20contract-signing%20rate%20over%20existing%0Astrategies%2C%20generating%20over%20453K%20new%20orders%20within%20just%20two%20months.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Structure%2520Sharing%2520Empowers%2520Multi-task%2520Heterogeneous%2520GNNs%250A%2520%2520for%2520Customer%2520Expansion%26entry.906535625%3DXinyue%2520Feng%2520and%2520Shuxin%2520Zhong%2520and%2520Jinquan%2520Hang%2520and%2520Wenjun%2520Lyu%2520and%2520Yuequn%2520Zhang%2520and%2520Guang%2520Yang%2520and%2520Haotian%2520Wang%2520and%2520Desheng%2520Zhang%2520and%2520Guang%2520Wang%26entry.1292438233%3D%2520%2520Customer%2520expansion%252C%2520i.e.%252C%2520growing%2520a%2520business%2520existing%2520customer%2520base%2520by%250Aacquiring%2520new%2520customers%252C%2520is%2520critical%2520for%2520scaling%2520operations%2520and%2520sustaining%2520the%250Along-term%2520profitability%2520of%2520logistics%2520companies.%2520Although%2520state-of-the-art%2520works%250Amodel%2520this%2520task%2520as%2520a%2520single-node%2520classification%2520problem%2520under%2520a%2520heterogeneous%250Agraph%2520learning%2520framework%2520and%2520achieve%2520good%2520performance%252C%2520they%2520struggle%2520with%250Aextremely%2520positive%2520label%2520sparsity%2520issues%2520in%2520our%2520scenario.%2520Multi-task%2520learning%250A%2528MTL%2529%2520offers%2520a%2520promising%2520solution%2520by%2520introducing%2520a%2520correlated%252C%2520label-rich%2520task%250Ato%2520enhance%2520the%2520label-sparse%2520task%2520prediction%2520through%2520knowledge%2520sharing.%2520However%252C%250Aexisting%2520MTL%2520methods%2520result%2520in%2520performance%2520degradation%2520because%2520they%2520fail%2520to%250Adiscriminate%2520task-shared%2520and%2520task-specific%2520structural%2520patterns%2520across%2520tasks.%250AThis%2520issue%2520arises%2520from%2520their%2520limited%2520consideration%2520of%2520the%2520inherently%2520complex%250Astructure%2520learning%2520process%2520of%2520heterogeneous%2520graph%2520neural%2520networks%252C%2520which%250Ainvolves%2520the%2520multi-layer%2520aggregation%2520of%2520multi-type%2520relations.%2520To%2520address%2520the%250Achallenge%252C%2520we%2520propose%2520a%2520Structure-Aware%2520Hierarchical%2520Information%2520Sharing%250AFramework%2520%2528SrucHIS%2529%252C%2520which%2520explicitly%2520regulates%2520structural%2520information%2520sharing%250Aacross%2520tasks%2520in%2520logistics%2520customer%2520expansion.%2520SrucHIS%2520breaks%2520down%2520the%2520structure%250Alearning%2520phase%2520into%2520multiple%2520stages%2520and%2520introduces%2520sharing%2520mechanisms%2520at%2520each%250Astage%252C%2520effectively%2520mitigating%2520the%2520influence%2520of%2520task-specific%2520structural%250Apatterns%2520during%2520each%2520stage.%2520We%2520evaluate%2520StrucHIS%2520on%2520both%2520private%2520and%2520public%250Adatasets%252C%2520achieving%2520a%252051.41%2525%2520average%2520precision%2520improvement%2520on%2520the%2520private%250Adataset%2520and%2520a%252010.52%2525%2520macro%2520F1%2520gain%2520on%2520the%2520public%2520dataset.%2520StrucHIS%2520is%2520further%250Adeployed%2520at%2520one%2520of%2520the%2520largest%2520logistics%2520companies%2520in%2520China%2520and%2520demonstrates%2520a%250A41.67%2525%2520improvement%2520in%2520the%2520success%2520contract-signing%2520rate%2520over%2520existing%250Astrategies%252C%2520generating%2520over%2520453K%2520new%2520orders%2520within%2520just%2520two%2520months.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Structure%20Sharing%20Empowers%20Multi-task%20Heterogeneous%20GNNs%0A%20%20for%20Customer%20Expansion&entry.906535625=Xinyue%20Feng%20and%20Shuxin%20Zhong%20and%20Jinquan%20Hang%20and%20Wenjun%20Lyu%20and%20Yuequn%20Zhang%20and%20Guang%20Yang%20and%20Haotian%20Wang%20and%20Desheng%20Zhang%20and%20Guang%20Wang&entry.1292438233=%20%20Customer%20expansion%2C%20i.e.%2C%20growing%20a%20business%20existing%20customer%20base%20by%0Aacquiring%20new%20customers%2C%20is%20critical%20for%20scaling%20operations%20and%20sustaining%20the%0Along-term%20profitability%20of%20logistics%20companies.%20Although%20state-of-the-art%20works%0Amodel%20this%20task%20as%20a%20single-node%20classification%20problem%20under%20a%20heterogeneous%0Agraph%20learning%20framework%20and%20achieve%20good%20performance%2C%20they%20struggle%20with%0Aextremely%20positive%20label%20sparsity%20issues%20in%20our%20scenario.%20Multi-task%20learning%0A%28MTL%29%20offers%20a%20promising%20solution%20by%20introducing%20a%20correlated%2C%20label-rich%20task%0Ato%20enhance%20the%20label-sparse%20task%20prediction%20through%20knowledge%20sharing.%20However%2C%0Aexisting%20MTL%20methods%20result%20in%20performance%20degradation%20because%20they%20fail%20to%0Adiscriminate%20task-shared%20and%20task-specific%20structural%20patterns%20across%20tasks.%0AThis%20issue%20arises%20from%20their%20limited%20consideration%20of%20the%20inherently%20complex%0Astructure%20learning%20process%20of%20heterogeneous%20graph%20neural%20networks%2C%20which%0Ainvolves%20the%20multi-layer%20aggregation%20of%20multi-type%20relations.%20To%20address%20the%0Achallenge%2C%20we%20propose%20a%20Structure-Aware%20Hierarchical%20Information%20Sharing%0AFramework%20%28SrucHIS%29%2C%20which%20explicitly%20regulates%20structural%20information%20sharing%0Aacross%20tasks%20in%20logistics%20customer%20expansion.%20SrucHIS%20breaks%20down%20the%20structure%0Alearning%20phase%20into%20multiple%20stages%20and%20introduces%20sharing%20mechanisms%20at%20each%0Astage%2C%20effectively%20mitigating%20the%20influence%20of%20task-specific%20structural%0Apatterns%20during%20each%20stage.%20We%20evaluate%20StrucHIS%20on%20both%20private%20and%20public%0Adatasets%2C%20achieving%20a%2051.41%25%20average%20precision%20improvement%20on%20the%20private%0Adataset%20and%20a%2010.52%25%20macro%20F1%20gain%20on%20the%20public%20dataset.%20StrucHIS%20is%20further%0Adeployed%20at%20one%20of%20the%20largest%20logistics%20companies%20in%20China%20and%20demonstrates%20a%0A41.67%25%20improvement%20in%20the%20success%20contract-signing%20rate%20over%20existing%0Astrategies%2C%20generating%20over%20453K%20new%20orders%20within%20just%20two%20months.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22089v2&entry.124074799=Read"},
{"title": "Entity Representation Learning Through Onsite-Offsite Graph for\n  Pinterset Ads", "author": "Jiayin Jin and Zhimeng Pan and Yang Tang and Jiarui Feng and Kungang Li and Chongyuan Xiang and Jiacheng Li and Runze Su and Siping Ji and Han Sun and Ling Leng and Prathibha Deshikachar", "abstract": "  Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.\n", "link": "http://arxiv.org/abs/2508.02609v1", "date": "2025-08-04", "relevancy": 2.4176, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5222}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4696}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entity%20Representation%20Learning%20Through%20Onsite-Offsite%20Graph%20for%0A%20%20Pinterset%20Ads&body=Title%3A%20Entity%20Representation%20Learning%20Through%20Onsite-Offsite%20Graph%20for%0A%20%20Pinterset%20Ads%0AAuthor%3A%20Jiayin%20Jin%20and%20Zhimeng%20Pan%20and%20Yang%20Tang%20and%20Jiarui%20Feng%20and%20Kungang%20Li%20and%20Chongyuan%20Xiang%20and%20Jiacheng%20Li%20and%20Runze%20Su%20and%20Siping%20Ji%20and%20Han%20Sun%20and%20Ling%20Leng%20and%20Prathibha%20Deshikachar%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNN%29%20have%20been%20extensively%20applied%20to%20industry%0Arecommendation%20systems%2C%20as%20seen%20in%20models%20like%20GraphSage%5Ccite%7BGraphSage%7D%2C%0ATwHIM%5Ccite%7BTwHIM%7D%2C%20LiGNN%5Ccite%7BLiGNN%7D%20etc.%20In%20these%20works%2C%20graphs%20were%0Aconstructed%20based%20on%20users%27%20activities%20on%20the%20platforms%2C%20and%20various%20graph%0Amodels%20were%20developed%20to%20effectively%20learn%20node%20embeddings.%20In%20addition%20to%0Ausers%27%20onsite%20activities%2C%20their%20offsite%20conversions%20are%20crucial%20for%20Ads%20models%0Ato%20capture%20their%20shopping%20interest.%20To%20better%20leverage%20offsite%20conversion%20data%0Aand%20explore%20the%20connection%20between%20onsite%20and%20offsite%20activities%2C%20we%0Aconstructed%20a%20large-scale%20heterogeneous%20graph%20based%20on%20users%27%20onsite%20ad%0Ainteractions%20and%20opt-in%20offsite%20conversion%20activities.%20Furthermore%2C%20we%0Aintroduced%20TransRA%20%28TransR%5Ccite%7BTransR%7D%20with%20Anchors%29%2C%20a%20novel%20Knowledge%20Graph%0AEmbedding%20%28KGE%29%20model%2C%20to%20more%20efficiently%20integrate%20graph%20embeddings%20into%20Ads%0Aranking%20models.%20However%2C%20our%20Ads%20ranking%20models%20initially%20struggled%20to%20directly%0Aincorporate%20Knowledge%20Graph%20Embeddings%20%28KGE%29%2C%20and%20only%20modest%20gains%20were%0Aobserved%20during%20offline%20experiments.%20To%20address%20this%20challenge%2C%20we%20employed%20the%0ALarge%20ID%20Embedding%20Table%20technique%20and%20innovated%20an%20attention%20based%20KGE%0Afinetuning%20approach%20within%20the%20Ads%20ranking%20models.%20As%20a%20result%2C%20we%20observed%20a%0Asignificant%20AUC%20lift%20in%20Click-Through%20Rate%20%28CTR%29%20and%20Conversion%20Rate%20%28CVR%29%0Aprediction%20models.%20Moreover%2C%20this%20framework%20has%20been%20deployed%20in%20Pinterest%27s%0AAds%20Engagement%20Model%20and%20contributed%20to%20%242.69%5C%25%24%20CTR%20lift%20and%20%241.34%5C%25%24%20CPC%0Areduction.%20We%20believe%20the%20techniques%20presented%20in%20this%20paper%20can%20be%20leveraged%0Aby%20other%20large-scale%20industrial%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntity%2520Representation%2520Learning%2520Through%2520Onsite-Offsite%2520Graph%2520for%250A%2520%2520Pinterset%2520Ads%26entry.906535625%3DJiayin%2520Jin%2520and%2520Zhimeng%2520Pan%2520and%2520Yang%2520Tang%2520and%2520Jiarui%2520Feng%2520and%2520Kungang%2520Li%2520and%2520Chongyuan%2520Xiang%2520and%2520Jiacheng%2520Li%2520and%2520Runze%2520Su%2520and%2520Siping%2520Ji%2520and%2520Han%2520Sun%2520and%2520Ling%2520Leng%2520and%2520Prathibha%2520Deshikachar%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520have%2520been%2520extensively%2520applied%2520to%2520industry%250Arecommendation%2520systems%252C%2520as%2520seen%2520in%2520models%2520like%2520GraphSage%255Ccite%257BGraphSage%257D%252C%250ATwHIM%255Ccite%257BTwHIM%257D%252C%2520LiGNN%255Ccite%257BLiGNN%257D%2520etc.%2520In%2520these%2520works%252C%2520graphs%2520were%250Aconstructed%2520based%2520on%2520users%2527%2520activities%2520on%2520the%2520platforms%252C%2520and%2520various%2520graph%250Amodels%2520were%2520developed%2520to%2520effectively%2520learn%2520node%2520embeddings.%2520In%2520addition%2520to%250Ausers%2527%2520onsite%2520activities%252C%2520their%2520offsite%2520conversions%2520are%2520crucial%2520for%2520Ads%2520models%250Ato%2520capture%2520their%2520shopping%2520interest.%2520To%2520better%2520leverage%2520offsite%2520conversion%2520data%250Aand%2520explore%2520the%2520connection%2520between%2520onsite%2520and%2520offsite%2520activities%252C%2520we%250Aconstructed%2520a%2520large-scale%2520heterogeneous%2520graph%2520based%2520on%2520users%2527%2520onsite%2520ad%250Ainteractions%2520and%2520opt-in%2520offsite%2520conversion%2520activities.%2520Furthermore%252C%2520we%250Aintroduced%2520TransRA%2520%2528TransR%255Ccite%257BTransR%257D%2520with%2520Anchors%2529%252C%2520a%2520novel%2520Knowledge%2520Graph%250AEmbedding%2520%2528KGE%2529%2520model%252C%2520to%2520more%2520efficiently%2520integrate%2520graph%2520embeddings%2520into%2520Ads%250Aranking%2520models.%2520However%252C%2520our%2520Ads%2520ranking%2520models%2520initially%2520struggled%2520to%2520directly%250Aincorporate%2520Knowledge%2520Graph%2520Embeddings%2520%2528KGE%2529%252C%2520and%2520only%2520modest%2520gains%2520were%250Aobserved%2520during%2520offline%2520experiments.%2520To%2520address%2520this%2520challenge%252C%2520we%2520employed%2520the%250ALarge%2520ID%2520Embedding%2520Table%2520technique%2520and%2520innovated%2520an%2520attention%2520based%2520KGE%250Afinetuning%2520approach%2520within%2520the%2520Ads%2520ranking%2520models.%2520As%2520a%2520result%252C%2520we%2520observed%2520a%250Asignificant%2520AUC%2520lift%2520in%2520Click-Through%2520Rate%2520%2528CTR%2529%2520and%2520Conversion%2520Rate%2520%2528CVR%2529%250Aprediction%2520models.%2520Moreover%252C%2520this%2520framework%2520has%2520been%2520deployed%2520in%2520Pinterest%2527s%250AAds%2520Engagement%2520Model%2520and%2520contributed%2520to%2520%25242.69%255C%2525%2524%2520CTR%2520lift%2520and%2520%25241.34%255C%2525%2524%2520CPC%250Areduction.%2520We%2520believe%2520the%2520techniques%2520presented%2520in%2520this%2520paper%2520can%2520be%2520leveraged%250Aby%2520other%2520large-scale%2520industrial%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entity%20Representation%20Learning%20Through%20Onsite-Offsite%20Graph%20for%0A%20%20Pinterset%20Ads&entry.906535625=Jiayin%20Jin%20and%20Zhimeng%20Pan%20and%20Yang%20Tang%20and%20Jiarui%20Feng%20and%20Kungang%20Li%20and%20Chongyuan%20Xiang%20and%20Jiacheng%20Li%20and%20Runze%20Su%20and%20Siping%20Ji%20and%20Han%20Sun%20and%20Ling%20Leng%20and%20Prathibha%20Deshikachar&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNN%29%20have%20been%20extensively%20applied%20to%20industry%0Arecommendation%20systems%2C%20as%20seen%20in%20models%20like%20GraphSage%5Ccite%7BGraphSage%7D%2C%0ATwHIM%5Ccite%7BTwHIM%7D%2C%20LiGNN%5Ccite%7BLiGNN%7D%20etc.%20In%20these%20works%2C%20graphs%20were%0Aconstructed%20based%20on%20users%27%20activities%20on%20the%20platforms%2C%20and%20various%20graph%0Amodels%20were%20developed%20to%20effectively%20learn%20node%20embeddings.%20In%20addition%20to%0Ausers%27%20onsite%20activities%2C%20their%20offsite%20conversions%20are%20crucial%20for%20Ads%20models%0Ato%20capture%20their%20shopping%20interest.%20To%20better%20leverage%20offsite%20conversion%20data%0Aand%20explore%20the%20connection%20between%20onsite%20and%20offsite%20activities%2C%20we%0Aconstructed%20a%20large-scale%20heterogeneous%20graph%20based%20on%20users%27%20onsite%20ad%0Ainteractions%20and%20opt-in%20offsite%20conversion%20activities.%20Furthermore%2C%20we%0Aintroduced%20TransRA%20%28TransR%5Ccite%7BTransR%7D%20with%20Anchors%29%2C%20a%20novel%20Knowledge%20Graph%0AEmbedding%20%28KGE%29%20model%2C%20to%20more%20efficiently%20integrate%20graph%20embeddings%20into%20Ads%0Aranking%20models.%20However%2C%20our%20Ads%20ranking%20models%20initially%20struggled%20to%20directly%0Aincorporate%20Knowledge%20Graph%20Embeddings%20%28KGE%29%2C%20and%20only%20modest%20gains%20were%0Aobserved%20during%20offline%20experiments.%20To%20address%20this%20challenge%2C%20we%20employed%20the%0ALarge%20ID%20Embedding%20Table%20technique%20and%20innovated%20an%20attention%20based%20KGE%0Afinetuning%20approach%20within%20the%20Ads%20ranking%20models.%20As%20a%20result%2C%20we%20observed%20a%0Asignificant%20AUC%20lift%20in%20Click-Through%20Rate%20%28CTR%29%20and%20Conversion%20Rate%20%28CVR%29%0Aprediction%20models.%20Moreover%2C%20this%20framework%20has%20been%20deployed%20in%20Pinterest%27s%0AAds%20Engagement%20Model%20and%20contributed%20to%20%242.69%5C%25%24%20CTR%20lift%20and%20%241.34%5C%25%24%20CPC%0Areduction.%20We%20believe%20the%20techniques%20presented%20in%20this%20paper%20can%20be%20leveraged%0Aby%20other%20large-scale%20industrial%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02609v1&entry.124074799=Read"},
{"title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling", "author": "Maxime Bouscary and Saurabh Amin", "abstract": "  LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.\n", "link": "http://arxiv.org/abs/2508.02503v1", "date": "2025-08-04", "relevancy": 2.416, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptiHive%3A%20Ensemble%20Selection%20for%20LLM-Based%20Optimization%20via%20Statistical%0A%20%20Modeling&body=Title%3A%20OptiHive%3A%20Ensemble%20Selection%20for%20LLM-Based%20Optimization%20via%20Statistical%0A%20%20Modeling%0AAuthor%3A%20Maxime%20Bouscary%20and%20Saurabh%20Amin%0AAbstract%3A%20%20%20LLM-based%20solvers%20have%20emerged%20as%20a%20promising%20means%20of%20automating%20problem%0Amodeling%20and%20solving.%20However%2C%20they%20remain%20unreliable%20and%20often%20depend%20on%0Aiterative%20repair%20loops%20that%20result%20in%20significant%20latency.%20We%20introduce%0AOptiHive%2C%20an%20LLM-based%20framework%20that%20produces%20high-quality%20solvers%20for%0Aoptimization%20problems%20from%20natural-language%20descriptions%20without%20iterative%0Aself-correction.%20OptiHive%20uses%20a%20single%20batched%20LLM%20query%20to%20generate%20diverse%0Acomponents%20%28solvers%2C%20problem%20instances%2C%20and%20validation%20tests%29%20and%20filters%20out%0Aerroneous%20components%20to%20ensure%20fully%20interpretable%20outputs.%20Taking%20into%20account%0Athe%20imperfection%20of%20the%20generated%20components%2C%20we%20employ%20a%20statistical%20model%20to%0Ainfer%20their%20true%20performance%2C%20enabling%20principled%20uncertainty%20quantification%0Aand%20solver%20selection.%20On%20tasks%20ranging%20from%20traditional%20optimization%20problems%0Ato%20challenging%20variants%20of%20the%20Multi-Depot%20Vehicle%20Routing%20Problem%2C%20OptiHive%0Asignificantly%20outperforms%20baselines%2C%20increasing%20the%20optimality%20rate%20from%205%5C%25%20to%0A92%5C%25%20on%20the%20most%20complex%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptiHive%253A%2520Ensemble%2520Selection%2520for%2520LLM-Based%2520Optimization%2520via%2520Statistical%250A%2520%2520Modeling%26entry.906535625%3DMaxime%2520Bouscary%2520and%2520Saurabh%2520Amin%26entry.1292438233%3D%2520%2520LLM-based%2520solvers%2520have%2520emerged%2520as%2520a%2520promising%2520means%2520of%2520automating%2520problem%250Amodeling%2520and%2520solving.%2520However%252C%2520they%2520remain%2520unreliable%2520and%2520often%2520depend%2520on%250Aiterative%2520repair%2520loops%2520that%2520result%2520in%2520significant%2520latency.%2520We%2520introduce%250AOptiHive%252C%2520an%2520LLM-based%2520framework%2520that%2520produces%2520high-quality%2520solvers%2520for%250Aoptimization%2520problems%2520from%2520natural-language%2520descriptions%2520without%2520iterative%250Aself-correction.%2520OptiHive%2520uses%2520a%2520single%2520batched%2520LLM%2520query%2520to%2520generate%2520diverse%250Acomponents%2520%2528solvers%252C%2520problem%2520instances%252C%2520and%2520validation%2520tests%2529%2520and%2520filters%2520out%250Aerroneous%2520components%2520to%2520ensure%2520fully%2520interpretable%2520outputs.%2520Taking%2520into%2520account%250Athe%2520imperfection%2520of%2520the%2520generated%2520components%252C%2520we%2520employ%2520a%2520statistical%2520model%2520to%250Ainfer%2520their%2520true%2520performance%252C%2520enabling%2520principled%2520uncertainty%2520quantification%250Aand%2520solver%2520selection.%2520On%2520tasks%2520ranging%2520from%2520traditional%2520optimization%2520problems%250Ato%2520challenging%2520variants%2520of%2520the%2520Multi-Depot%2520Vehicle%2520Routing%2520Problem%252C%2520OptiHive%250Asignificantly%2520outperforms%2520baselines%252C%2520increasing%2520the%2520optimality%2520rate%2520from%25205%255C%2525%2520to%250A92%255C%2525%2520on%2520the%2520most%2520complex%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptiHive%3A%20Ensemble%20Selection%20for%20LLM-Based%20Optimization%20via%20Statistical%0A%20%20Modeling&entry.906535625=Maxime%20Bouscary%20and%20Saurabh%20Amin&entry.1292438233=%20%20LLM-based%20solvers%20have%20emerged%20as%20a%20promising%20means%20of%20automating%20problem%0Amodeling%20and%20solving.%20However%2C%20they%20remain%20unreliable%20and%20often%20depend%20on%0Aiterative%20repair%20loops%20that%20result%20in%20significant%20latency.%20We%20introduce%0AOptiHive%2C%20an%20LLM-based%20framework%20that%20produces%20high-quality%20solvers%20for%0Aoptimization%20problems%20from%20natural-language%20descriptions%20without%20iterative%0Aself-correction.%20OptiHive%20uses%20a%20single%20batched%20LLM%20query%20to%20generate%20diverse%0Acomponents%20%28solvers%2C%20problem%20instances%2C%20and%20validation%20tests%29%20and%20filters%20out%0Aerroneous%20components%20to%20ensure%20fully%20interpretable%20outputs.%20Taking%20into%20account%0Athe%20imperfection%20of%20the%20generated%20components%2C%20we%20employ%20a%20statistical%20model%20to%0Ainfer%20their%20true%20performance%2C%20enabling%20principled%20uncertainty%20quantification%0Aand%20solver%20selection.%20On%20tasks%20ranging%20from%20traditional%20optimization%20problems%0Ato%20challenging%20variants%20of%20the%20Multi-Depot%20Vehicle%20Routing%20Problem%2C%20OptiHive%0Asignificantly%20outperforms%20baselines%2C%20increasing%20the%20optimality%20rate%20from%205%5C%25%20to%0A92%5C%25%20on%20the%20most%20complex%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02503v1&entry.124074799=Read"},
{"title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling", "author": "Theodoros Kouzelis and Ioannis Kakogeorgiou and Spyros Gidaris and Nikos Komodakis", "abstract": "  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n", "link": "http://arxiv.org/abs/2502.09509v3", "date": "2025-08-04", "relevancy": 2.4094, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6135}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6012}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&body=Title%3A%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling%0AAuthor%3A%20Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09509v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEQ-VAE%253A%2520Equivariance%2520Regularized%2520Latent%2520Space%2520for%2520Improved%2520Generative%250A%2520%2520Image%2520Modeling%26entry.906535625%3DTheodoros%2520Kouzelis%2520and%2520Ioannis%2520Kakogeorgiou%2520and%2520Spyros%2520Gidaris%2520and%2520Nikos%2520Komodakis%26entry.1292438233%3D%2520%2520Latent%2520generative%2520models%2520have%2520emerged%2520as%2520a%2520leading%2520approach%2520for%2520high-quality%250Aimage%2520synthesis.%2520These%2520models%2520rely%2520on%2520an%2520autoencoder%2520to%2520compress%2520images%2520into%2520a%250Alatent%2520space%252C%2520followed%2520by%2520a%2520generative%2520model%2520to%2520learn%2520the%2520latent%2520distribution.%250AWe%2520identify%2520that%2520existing%2520autoencoders%2520lack%2520equivariance%2520to%2520semantic-preserving%250Atransformations%2520like%2520scaling%2520and%2520rotation%252C%2520resulting%2520in%2520complex%2520latent%2520spaces%250Athat%2520hinder%2520generative%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520EQ-VAE%252C%2520a%250Asimple%2520regularization%2520approach%2520that%2520enforces%2520equivariance%2520in%2520the%2520latent%2520space%252C%250Areducing%2520its%2520complexity%2520without%2520degrading%2520reconstruction%2520quality.%2520By%2520finetuning%250Apre-trained%2520autoencoders%2520with%2520EQ-VAE%252C%2520we%2520enhance%2520the%2520performance%2520of%2520several%250Astate-of-the-art%2520generative%2520models%252C%2520including%2520DiT%252C%2520SiT%252C%2520REPA%2520and%2520MaskGIT%252C%250Aachieving%2520a%25207%2520speedup%2520on%2520DiT-XL/2%2520with%2520only%2520five%2520epochs%2520of%2520SD-VAE%2520fine-tuning.%250AEQ-VAE%2520is%2520compatible%2520with%2520both%2520continuous%2520and%2520discrete%2520autoencoders%252C%2520thus%250Aoffering%2520a%2520versatile%2520enhancement%2520for%2520a%2520wide%2520range%2520of%2520latent%2520generative%2520models.%250AProject%2520page%2520and%2520code%253A%2520https%253A//eq-vae.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09509v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&entry.906535625=Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09509v3&entry.124074799=Read"},
{"title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask", "author": "Yaofeng Cheng and Xinkai Gao and Sen Zhang and Chao Zeng and Fusheng Zha and Lining Sun and Chenguang Yang", "abstract": "  Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.\n", "link": "http://arxiv.org/abs/2508.02507v1", "date": "2025-08-04", "relevancy": 2.3968, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6169}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6063}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Transparent%20Object%20Grasping%3A%20Depth%20Completion%20with%20Monocular%0A%20%20Depth%20Estimation%20and%20Instance%20Mask&body=Title%3A%20Rethinking%20Transparent%20Object%20Grasping%3A%20Depth%20Completion%20with%20Monocular%0A%20%20Depth%20Estimation%20and%20Instance%20Mask%0AAuthor%3A%20Yaofeng%20Cheng%20and%20Xinkai%20Gao%20and%20Sen%20Zhang%20and%20Chao%20Zeng%20and%20Fusheng%20Zha%20and%20Lining%20Sun%20and%20Chenguang%20Yang%0AAbstract%3A%20%20%20Due%20to%20the%20optical%20properties%2C%20transparent%20objects%20often%20lead%20depth%20cameras%0Ato%20generate%20incomplete%20or%20invalid%20depth%20data%2C%20which%20in%20turn%20reduces%20the%0Aaccuracy%20and%20reliability%20of%20robotic%20grasping.%20Existing%20approaches%20typically%0Ainput%20the%20RGB-D%20image%20directly%20into%20the%20network%20to%20output%20the%20complete%20depth%2C%0Aexpecting%20the%20model%20to%20implicitly%20infer%20the%20reliability%20of%20depth%20values.%0AHowever%2C%20while%20effective%20in%20training%20datasets%2C%20such%20methods%20often%20fail%20to%0Ageneralize%20to%20real-world%20scenarios%2C%20where%20complex%20light%20interactions%20lead%20to%0Ahighly%20variable%20distributions%20of%20valid%20and%20invalid%20depth%20data.%20To%20address%20this%2C%0Awe%20propose%20ReMake%2C%20a%20novel%20depth%20completion%20framework%20guided%20by%20an%20instance%0Amask%20and%20monocular%20depth%20estimation.%20By%20explicitly%20distinguishing%20transparent%0Aregions%20from%20non-transparent%20ones%2C%20the%20mask%20enables%20the%20model%20to%20concentrate%20on%0Alearning%20accurate%20depth%20estimation%20in%20these%20areas%20from%20RGB-D%20input%20during%0Atraining.%20This%20targeted%20supervision%20reduces%20reliance%20on%20implicit%20reasoning%20and%0Aimproves%20generalization%20to%20real-world%20scenarios.%20Additionally%2C%20monocular%20depth%0Aestimation%20provides%20depth%20context%20between%20the%20transparent%20object%20and%20its%0Asurroundings%2C%20enhancing%20depth%20prediction%20accuracy.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20existing%20approaches%20on%20both%20benchmark%20datasets%20and%0Areal-world%20scenarios%2C%20demonstrating%20superior%20accuracy%20and%20generalization%0Acapability.%20Code%20and%20videos%20are%20available%20at%0Ahttps%3A//chengyaofeng.github.io/ReMake.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Transparent%2520Object%2520Grasping%253A%2520Depth%2520Completion%2520with%2520Monocular%250A%2520%2520Depth%2520Estimation%2520and%2520Instance%2520Mask%26entry.906535625%3DYaofeng%2520Cheng%2520and%2520Xinkai%2520Gao%2520and%2520Sen%2520Zhang%2520and%2520Chao%2520Zeng%2520and%2520Fusheng%2520Zha%2520and%2520Lining%2520Sun%2520and%2520Chenguang%2520Yang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520optical%2520properties%252C%2520transparent%2520objects%2520often%2520lead%2520depth%2520cameras%250Ato%2520generate%2520incomplete%2520or%2520invalid%2520depth%2520data%252C%2520which%2520in%2520turn%2520reduces%2520the%250Aaccuracy%2520and%2520reliability%2520of%2520robotic%2520grasping.%2520Existing%2520approaches%2520typically%250Ainput%2520the%2520RGB-D%2520image%2520directly%2520into%2520the%2520network%2520to%2520output%2520the%2520complete%2520depth%252C%250Aexpecting%2520the%2520model%2520to%2520implicitly%2520infer%2520the%2520reliability%2520of%2520depth%2520values.%250AHowever%252C%2520while%2520effective%2520in%2520training%2520datasets%252C%2520such%2520methods%2520often%2520fail%2520to%250Ageneralize%2520to%2520real-world%2520scenarios%252C%2520where%2520complex%2520light%2520interactions%2520lead%2520to%250Ahighly%2520variable%2520distributions%2520of%2520valid%2520and%2520invalid%2520depth%2520data.%2520To%2520address%2520this%252C%250Awe%2520propose%2520ReMake%252C%2520a%2520novel%2520depth%2520completion%2520framework%2520guided%2520by%2520an%2520instance%250Amask%2520and%2520monocular%2520depth%2520estimation.%2520By%2520explicitly%2520distinguishing%2520transparent%250Aregions%2520from%2520non-transparent%2520ones%252C%2520the%2520mask%2520enables%2520the%2520model%2520to%2520concentrate%2520on%250Alearning%2520accurate%2520depth%2520estimation%2520in%2520these%2520areas%2520from%2520RGB-D%2520input%2520during%250Atraining.%2520This%2520targeted%2520supervision%2520reduces%2520reliance%2520on%2520implicit%2520reasoning%2520and%250Aimproves%2520generalization%2520to%2520real-world%2520scenarios.%2520Additionally%252C%2520monocular%2520depth%250Aestimation%2520provides%2520depth%2520context%2520between%2520the%2520transparent%2520object%2520and%2520its%250Asurroundings%252C%2520enhancing%2520depth%2520prediction%2520accuracy.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520method%2520outperforms%2520existing%2520approaches%2520on%2520both%2520benchmark%2520datasets%2520and%250Areal-world%2520scenarios%252C%2520demonstrating%2520superior%2520accuracy%2520and%2520generalization%250Acapability.%2520Code%2520and%2520videos%2520are%2520available%2520at%250Ahttps%253A//chengyaofeng.github.io/ReMake.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Transparent%20Object%20Grasping%3A%20Depth%20Completion%20with%20Monocular%0A%20%20Depth%20Estimation%20and%20Instance%20Mask&entry.906535625=Yaofeng%20Cheng%20and%20Xinkai%20Gao%20and%20Sen%20Zhang%20and%20Chao%20Zeng%20and%20Fusheng%20Zha%20and%20Lining%20Sun%20and%20Chenguang%20Yang&entry.1292438233=%20%20Due%20to%20the%20optical%20properties%2C%20transparent%20objects%20often%20lead%20depth%20cameras%0Ato%20generate%20incomplete%20or%20invalid%20depth%20data%2C%20which%20in%20turn%20reduces%20the%0Aaccuracy%20and%20reliability%20of%20robotic%20grasping.%20Existing%20approaches%20typically%0Ainput%20the%20RGB-D%20image%20directly%20into%20the%20network%20to%20output%20the%20complete%20depth%2C%0Aexpecting%20the%20model%20to%20implicitly%20infer%20the%20reliability%20of%20depth%20values.%0AHowever%2C%20while%20effective%20in%20training%20datasets%2C%20such%20methods%20often%20fail%20to%0Ageneralize%20to%20real-world%20scenarios%2C%20where%20complex%20light%20interactions%20lead%20to%0Ahighly%20variable%20distributions%20of%20valid%20and%20invalid%20depth%20data.%20To%20address%20this%2C%0Awe%20propose%20ReMake%2C%20a%20novel%20depth%20completion%20framework%20guided%20by%20an%20instance%0Amask%20and%20monocular%20depth%20estimation.%20By%20explicitly%20distinguishing%20transparent%0Aregions%20from%20non-transparent%20ones%2C%20the%20mask%20enables%20the%20model%20to%20concentrate%20on%0Alearning%20accurate%20depth%20estimation%20in%20these%20areas%20from%20RGB-D%20input%20during%0Atraining.%20This%20targeted%20supervision%20reduces%20reliance%20on%20implicit%20reasoning%20and%0Aimproves%20generalization%20to%20real-world%20scenarios.%20Additionally%2C%20monocular%20depth%0Aestimation%20provides%20depth%20context%20between%20the%20transparent%20object%20and%20its%0Asurroundings%2C%20enhancing%20depth%20prediction%20accuracy.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20existing%20approaches%20on%20both%20benchmark%20datasets%20and%0Areal-world%20scenarios%2C%20demonstrating%20superior%20accuracy%20and%20generalization%0Acapability.%20Code%20and%20videos%20are%20available%20at%0Ahttps%3A//chengyaofeng.github.io/ReMake.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02507v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Perception-Based Control for Autonomous Racing", "author": "Jelena Trisovic and Andrea Carron and Melanie N. Zeilinger", "abstract": "  Autonomous systems operating in unknown environments often rely heavily on\nvisual sensor data, yet making safe and informed control decisions based on\nthese measurements remains a significant challenge. To facilitate the\nintegration of perception and control in autonomous vehicles, we propose a\nnovel perception-based control approach that incorporates road estimation,\nquantification of its uncertainty, and uncertainty-aware control based on this\nestimate. At the core of our method is a parametric road curvature model,\noptimized using visual measurements of the road through a constrained nonlinear\noptimization problem. This process ensures adherence to constraints on both\nmodel parameters and curvature. By leveraging the Frenet frame formulation, we\nembed the estimated track curvature into the system dynamics, allowing the\ncontroller to explicitly account for perception uncertainty and enhancing\nrobustness to estimation errors based on visual input. We validate our approach\nin a simulated environment, using a high-fidelity 3D rendering engine, and\ndemonstrate its effectiveness in achieving reliable and uncertainty-aware\ncontrol for autonomous racing.\n", "link": "http://arxiv.org/abs/2508.02494v1", "date": "2025-08-04", "relevancy": 2.3967, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6259}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Perception-Based%20Control%20for%20Autonomous%20Racing&body=Title%3A%20Uncertainty-Aware%20Perception-Based%20Control%20for%20Autonomous%20Racing%0AAuthor%3A%20Jelena%20Trisovic%20and%20Andrea%20Carron%20and%20Melanie%20N.%20Zeilinger%0AAbstract%3A%20%20%20Autonomous%20systems%20operating%20in%20unknown%20environments%20often%20rely%20heavily%20on%0Avisual%20sensor%20data%2C%20yet%20making%20safe%20and%20informed%20control%20decisions%20based%20on%0Athese%20measurements%20remains%20a%20significant%20challenge.%20To%20facilitate%20the%0Aintegration%20of%20perception%20and%20control%20in%20autonomous%20vehicles%2C%20we%20propose%20a%0Anovel%20perception-based%20control%20approach%20that%20incorporates%20road%20estimation%2C%0Aquantification%20of%20its%20uncertainty%2C%20and%20uncertainty-aware%20control%20based%20on%20this%0Aestimate.%20At%20the%20core%20of%20our%20method%20is%20a%20parametric%20road%20curvature%20model%2C%0Aoptimized%20using%20visual%20measurements%20of%20the%20road%20through%20a%20constrained%20nonlinear%0Aoptimization%20problem.%20This%20process%20ensures%20adherence%20to%20constraints%20on%20both%0Amodel%20parameters%20and%20curvature.%20By%20leveraging%20the%20Frenet%20frame%20formulation%2C%20we%0Aembed%20the%20estimated%20track%20curvature%20into%20the%20system%20dynamics%2C%20allowing%20the%0Acontroller%20to%20explicitly%20account%20for%20perception%20uncertainty%20and%20enhancing%0Arobustness%20to%20estimation%20errors%20based%20on%20visual%20input.%20We%20validate%20our%20approach%0Ain%20a%20simulated%20environment%2C%20using%20a%20high-fidelity%203D%20rendering%20engine%2C%20and%0Ademonstrate%20its%20effectiveness%20in%20achieving%20reliable%20and%20uncertainty-aware%0Acontrol%20for%20autonomous%20racing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Perception-Based%2520Control%2520for%2520Autonomous%2520Racing%26entry.906535625%3DJelena%2520Trisovic%2520and%2520Andrea%2520Carron%2520and%2520Melanie%2520N.%2520Zeilinger%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520operating%2520in%2520unknown%2520environments%2520often%2520rely%2520heavily%2520on%250Avisual%2520sensor%2520data%252C%2520yet%2520making%2520safe%2520and%2520informed%2520control%2520decisions%2520based%2520on%250Athese%2520measurements%2520remains%2520a%2520significant%2520challenge.%2520To%2520facilitate%2520the%250Aintegration%2520of%2520perception%2520and%2520control%2520in%2520autonomous%2520vehicles%252C%2520we%2520propose%2520a%250Anovel%2520perception-based%2520control%2520approach%2520that%2520incorporates%2520road%2520estimation%252C%250Aquantification%2520of%2520its%2520uncertainty%252C%2520and%2520uncertainty-aware%2520control%2520based%2520on%2520this%250Aestimate.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%2520a%2520parametric%2520road%2520curvature%2520model%252C%250Aoptimized%2520using%2520visual%2520measurements%2520of%2520the%2520road%2520through%2520a%2520constrained%2520nonlinear%250Aoptimization%2520problem.%2520This%2520process%2520ensures%2520adherence%2520to%2520constraints%2520on%2520both%250Amodel%2520parameters%2520and%2520curvature.%2520By%2520leveraging%2520the%2520Frenet%2520frame%2520formulation%252C%2520we%250Aembed%2520the%2520estimated%2520track%2520curvature%2520into%2520the%2520system%2520dynamics%252C%2520allowing%2520the%250Acontroller%2520to%2520explicitly%2520account%2520for%2520perception%2520uncertainty%2520and%2520enhancing%250Arobustness%2520to%2520estimation%2520errors%2520based%2520on%2520visual%2520input.%2520We%2520validate%2520our%2520approach%250Ain%2520a%2520simulated%2520environment%252C%2520using%2520a%2520high-fidelity%25203D%2520rendering%2520engine%252C%2520and%250Ademonstrate%2520its%2520effectiveness%2520in%2520achieving%2520reliable%2520and%2520uncertainty-aware%250Acontrol%2520for%2520autonomous%2520racing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Perception-Based%20Control%20for%20Autonomous%20Racing&entry.906535625=Jelena%20Trisovic%20and%20Andrea%20Carron%20and%20Melanie%20N.%20Zeilinger&entry.1292438233=%20%20Autonomous%20systems%20operating%20in%20unknown%20environments%20often%20rely%20heavily%20on%0Avisual%20sensor%20data%2C%20yet%20making%20safe%20and%20informed%20control%20decisions%20based%20on%0Athese%20measurements%20remains%20a%20significant%20challenge.%20To%20facilitate%20the%0Aintegration%20of%20perception%20and%20control%20in%20autonomous%20vehicles%2C%20we%20propose%20a%0Anovel%20perception-based%20control%20approach%20that%20incorporates%20road%20estimation%2C%0Aquantification%20of%20its%20uncertainty%2C%20and%20uncertainty-aware%20control%20based%20on%20this%0Aestimate.%20At%20the%20core%20of%20our%20method%20is%20a%20parametric%20road%20curvature%20model%2C%0Aoptimized%20using%20visual%20measurements%20of%20the%20road%20through%20a%20constrained%20nonlinear%0Aoptimization%20problem.%20This%20process%20ensures%20adherence%20to%20constraints%20on%20both%0Amodel%20parameters%20and%20curvature.%20By%20leveraging%20the%20Frenet%20frame%20formulation%2C%20we%0Aembed%20the%20estimated%20track%20curvature%20into%20the%20system%20dynamics%2C%20allowing%20the%0Acontroller%20to%20explicitly%20account%20for%20perception%20uncertainty%20and%20enhancing%0Arobustness%20to%20estimation%20errors%20based%20on%20visual%20input.%20We%20validate%20our%20approach%0Ain%20a%20simulated%20environment%2C%20using%20a%20high-fidelity%203D%20rendering%20engine%2C%20and%0Ademonstrate%20its%20effectiveness%20in%20achieving%20reliable%20and%20uncertainty-aware%0Acontrol%20for%20autonomous%20racing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02494v1&entry.124074799=Read"},
{"title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic\n  Representations in LLaMA 3.2", "author": "Jack Merullo and Arjun Khurana and Oliver McLaughlin", "abstract": "  Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.\n", "link": "http://arxiv.org/abs/2508.02527v1", "date": "2025-08-04", "relevancy": 2.3717, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2&body=Title%3A%20I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2%0AAuthor%3A%20Jack%20Merullo%20and%20Arjun%20Khurana%20and%20Oliver%20McLaughlin%0AAbstract%3A%20%20%20Large%20language%20models%20demonstrate%20proficiency%20on%20phonetic%20tasks%2C%20such%20as%0Arhyming%2C%20without%20explicit%20phonetic%20or%20auditory%20grounding.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20%5Cverb%7CLlama-3.2-1B-Instruct%7C%20represents%20token-level%20phonetic%0Ainformation.%20Our%20results%20suggest%20that%20Llama%20uses%20a%20rich%20internal%20model%20of%0Aphonemes%20to%20complete%20phonetic%20tasks.%20We%20provide%20evidence%20for%20high-level%0Aorganization%20of%20phoneme%20representations%20in%20its%20latent%20space.%20In%20doing%20so%2C%20we%0Aalso%20identify%20a%20%60%60phoneme%20mover%20head%22%20which%20promotes%20phonetic%20information%0Aduring%20rhyming%20tasks.%20We%20visualize%20the%20output%20space%20of%20this%20head%20and%20find%20that%2C%0Awhile%20notable%20differences%20exist%2C%20Llama%20learns%20a%20model%20of%20vowels%20similar%20to%20the%0Astandard%20IPA%20vowel%20chart%20for%20humans%2C%20despite%20receiving%20no%20direct%20supervision%20to%0Ado%20so.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Have%2520No%2520Mouth%252C%2520and%2520I%2520Must%2520Rhyme%253A%2520Uncovering%2520Internal%2520Phonetic%250A%2520%2520Representations%2520in%2520LLaMA%25203.2%26entry.906535625%3DJack%2520Merullo%2520and%2520Arjun%2520Khurana%2520and%2520Oliver%2520McLaughlin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520demonstrate%2520proficiency%2520on%2520phonetic%2520tasks%252C%2520such%2520as%250Arhyming%252C%2520without%2520explicit%2520phonetic%2520or%2520auditory%2520grounding.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520how%2520%255Cverb%257CLlama-3.2-1B-Instruct%257C%2520represents%2520token-level%2520phonetic%250Ainformation.%2520Our%2520results%2520suggest%2520that%2520Llama%2520uses%2520a%2520rich%2520internal%2520model%2520of%250Aphonemes%2520to%2520complete%2520phonetic%2520tasks.%2520We%2520provide%2520evidence%2520for%2520high-level%250Aorganization%2520of%2520phoneme%2520representations%2520in%2520its%2520latent%2520space.%2520In%2520doing%2520so%252C%2520we%250Aalso%2520identify%2520a%2520%2560%2560phoneme%2520mover%2520head%2522%2520which%2520promotes%2520phonetic%2520information%250Aduring%2520rhyming%2520tasks.%2520We%2520visualize%2520the%2520output%2520space%2520of%2520this%2520head%2520and%2520find%2520that%252C%250Awhile%2520notable%2520differences%2520exist%252C%2520Llama%2520learns%2520a%2520model%2520of%2520vowels%2520similar%2520to%2520the%250Astandard%2520IPA%2520vowel%2520chart%2520for%2520humans%252C%2520despite%2520receiving%2520no%2520direct%2520supervision%2520to%250Ado%2520so.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2&entry.906535625=Jack%20Merullo%20and%20Arjun%20Khurana%20and%20Oliver%20McLaughlin&entry.1292438233=%20%20Large%20language%20models%20demonstrate%20proficiency%20on%20phonetic%20tasks%2C%20such%20as%0Arhyming%2C%20without%20explicit%20phonetic%20or%20auditory%20grounding.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20%5Cverb%7CLlama-3.2-1B-Instruct%7C%20represents%20token-level%20phonetic%0Ainformation.%20Our%20results%20suggest%20that%20Llama%20uses%20a%20rich%20internal%20model%20of%0Aphonemes%20to%20complete%20phonetic%20tasks.%20We%20provide%20evidence%20for%20high-level%0Aorganization%20of%20phoneme%20representations%20in%20its%20latent%20space.%20In%20doing%20so%2C%20we%0Aalso%20identify%20a%20%60%60phoneme%20mover%20head%22%20which%20promotes%20phonetic%20information%0Aduring%20rhyming%20tasks.%20We%20visualize%20the%20output%20space%20of%20this%20head%20and%20find%20that%2C%0Awhile%20notable%20differences%20exist%2C%20Llama%20learns%20a%20model%20of%20vowels%20similar%20to%20the%0Astandard%20IPA%20vowel%20chart%20for%20humans%2C%20despite%20receiving%20no%20direct%20supervision%20to%0Ado%20so.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02527v1&entry.124074799=Read"},
{"title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots", "author": "Sheng Wu and Fei Teng and Hao Shi and Qi Jiang and Kai Luo and Kaiwei Wang and Kailun Yang", "abstract": "  Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.\n", "link": "http://arxiv.org/abs/2508.02512v1", "date": "2025-08-04", "relevancy": 2.3677, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6229}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5892}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuaDreamer%3A%20Controllable%20Panoramic%20Video%20Generation%20for%20Quadruped%20Robots&body=Title%3A%20QuaDreamer%3A%20Controllable%20Panoramic%20Video%20Generation%20for%20Quadruped%20Robots%0AAuthor%3A%20Sheng%20Wu%20and%20Fei%20Teng%20and%20Hao%20Shi%20and%20Qi%20Jiang%20and%20Kai%20Luo%20and%20Kaiwei%20Wang%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20Panoramic%20cameras%2C%20capturing%20comprehensive%20360-degree%20environmental%20data%2C%20are%0Asuitable%20for%20quadruped%20robots%20in%20surrounding%20perception%20and%20interaction%20with%0Acomplex%20environments.%20However%2C%20the%20scarcity%20of%20high-quality%20panoramic%20training%0Adata-caused%20by%20inherent%20kinematic%20constraints%20and%20complex%20sensor%20calibration%0Achallenges-fundamentally%20limits%20the%20development%20of%20robust%20perception%20systems%0Atailored%20to%20these%20embodied%20platforms.%20To%20address%20this%20issue%2C%20we%20propose%0AQuaDreamer-the%20first%20panoramic%20data%20generation%20engine%20specifically%20designed%20for%0Aquadruped%20robots.%20QuaDreamer%20focuses%20on%20mimicking%20the%20motion%20paradigm%20of%0Aquadruped%20robots%20to%20generate%20highly%20controllable%2C%20realistic%20panoramic%20videos%2C%0Aproviding%20a%20data%20source%20for%20downstream%20tasks.%20Specifically%2C%20to%20effectively%0Acapture%20the%20unique%20vertical%20vibration%20characteristics%20exhibited%20during%0Aquadruped%20locomotion%2C%20we%20introduce%20Vertical%20Jitter%20Encoding%20%28VJE%29.%20VJE%20extracts%0Acontrollable%20vertical%20signals%20through%20frequency-domain%20feature%20filtering%20and%0Aprovides%20high-quality%20prompts.%20To%20facilitate%20high-quality%20panoramic%20video%0Ageneration%20under%20jitter%20signal%20control%2C%20we%20propose%20a%20Scene-Object%20Controller%0A%28SOC%29%20that%20effectively%20manages%20object%20motion%20and%20boosts%20background%20jitter%0Acontrol%20through%20the%20attention%20mechanism.%20To%20address%20panoramic%20distortions%20in%0Awide-FoV%20video%20generation%2C%20we%20propose%20the%20Panoramic%20Enhancer%20%28PE%29-a%20dual-stream%0Aarchitecture%20that%20synergizes%20frequency-texture%20refinement%20for%20local%20detail%0Aenhancement%20with%20spatial-structure%20correction%20for%20global%20geometric%20consistency.%0AWe%20further%20demonstrate%20that%20the%20generated%20video%20sequences%20can%20serve%20as%20training%0Adata%20for%20the%20quadruped%20robot%27s%20panoramic%20visual%20perception%20model%2C%20enhancing%20the%0Aperformance%20of%20multi-object%20tracking%20in%20360-degree%20scenes.%20The%20source%20code%20and%0Amodel%20weights%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/losehu/QuaDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaDreamer%253A%2520Controllable%2520Panoramic%2520Video%2520Generation%2520for%2520Quadruped%2520Robots%26entry.906535625%3DSheng%2520Wu%2520and%2520Fei%2520Teng%2520and%2520Hao%2520Shi%2520and%2520Qi%2520Jiang%2520and%2520Kai%2520Luo%2520and%2520Kaiwei%2520Wang%2520and%2520Kailun%2520Yang%26entry.1292438233%3D%2520%2520Panoramic%2520cameras%252C%2520capturing%2520comprehensive%2520360-degree%2520environmental%2520data%252C%2520are%250Asuitable%2520for%2520quadruped%2520robots%2520in%2520surrounding%2520perception%2520and%2520interaction%2520with%250Acomplex%2520environments.%2520However%252C%2520the%2520scarcity%2520of%2520high-quality%2520panoramic%2520training%250Adata-caused%2520by%2520inherent%2520kinematic%2520constraints%2520and%2520complex%2520sensor%2520calibration%250Achallenges-fundamentally%2520limits%2520the%2520development%2520of%2520robust%2520perception%2520systems%250Atailored%2520to%2520these%2520embodied%2520platforms.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AQuaDreamer-the%2520first%2520panoramic%2520data%2520generation%2520engine%2520specifically%2520designed%2520for%250Aquadruped%2520robots.%2520QuaDreamer%2520focuses%2520on%2520mimicking%2520the%2520motion%2520paradigm%2520of%250Aquadruped%2520robots%2520to%2520generate%2520highly%2520controllable%252C%2520realistic%2520panoramic%2520videos%252C%250Aproviding%2520a%2520data%2520source%2520for%2520downstream%2520tasks.%2520Specifically%252C%2520to%2520effectively%250Acapture%2520the%2520unique%2520vertical%2520vibration%2520characteristics%2520exhibited%2520during%250Aquadruped%2520locomotion%252C%2520we%2520introduce%2520Vertical%2520Jitter%2520Encoding%2520%2528VJE%2529.%2520VJE%2520extracts%250Acontrollable%2520vertical%2520signals%2520through%2520frequency-domain%2520feature%2520filtering%2520and%250Aprovides%2520high-quality%2520prompts.%2520To%2520facilitate%2520high-quality%2520panoramic%2520video%250Ageneration%2520under%2520jitter%2520signal%2520control%252C%2520we%2520propose%2520a%2520Scene-Object%2520Controller%250A%2528SOC%2529%2520that%2520effectively%2520manages%2520object%2520motion%2520and%2520boosts%2520background%2520jitter%250Acontrol%2520through%2520the%2520attention%2520mechanism.%2520To%2520address%2520panoramic%2520distortions%2520in%250Awide-FoV%2520video%2520generation%252C%2520we%2520propose%2520the%2520Panoramic%2520Enhancer%2520%2528PE%2529-a%2520dual-stream%250Aarchitecture%2520that%2520synergizes%2520frequency-texture%2520refinement%2520for%2520local%2520detail%250Aenhancement%2520with%2520spatial-structure%2520correction%2520for%2520global%2520geometric%2520consistency.%250AWe%2520further%2520demonstrate%2520that%2520the%2520generated%2520video%2520sequences%2520can%2520serve%2520as%2520training%250Adata%2520for%2520the%2520quadruped%2520robot%2527s%2520panoramic%2520visual%2520perception%2520model%252C%2520enhancing%2520the%250Aperformance%2520of%2520multi-object%2520tracking%2520in%2520360-degree%2520scenes.%2520The%2520source%2520code%2520and%250Amodel%2520weights%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/losehu/QuaDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuaDreamer%3A%20Controllable%20Panoramic%20Video%20Generation%20for%20Quadruped%20Robots&entry.906535625=Sheng%20Wu%20and%20Fei%20Teng%20and%20Hao%20Shi%20and%20Qi%20Jiang%20and%20Kai%20Luo%20and%20Kaiwei%20Wang%20and%20Kailun%20Yang&entry.1292438233=%20%20Panoramic%20cameras%2C%20capturing%20comprehensive%20360-degree%20environmental%20data%2C%20are%0Asuitable%20for%20quadruped%20robots%20in%20surrounding%20perception%20and%20interaction%20with%0Acomplex%20environments.%20However%2C%20the%20scarcity%20of%20high-quality%20panoramic%20training%0Adata-caused%20by%20inherent%20kinematic%20constraints%20and%20complex%20sensor%20calibration%0Achallenges-fundamentally%20limits%20the%20development%20of%20robust%20perception%20systems%0Atailored%20to%20these%20embodied%20platforms.%20To%20address%20this%20issue%2C%20we%20propose%0AQuaDreamer-the%20first%20panoramic%20data%20generation%20engine%20specifically%20designed%20for%0Aquadruped%20robots.%20QuaDreamer%20focuses%20on%20mimicking%20the%20motion%20paradigm%20of%0Aquadruped%20robots%20to%20generate%20highly%20controllable%2C%20realistic%20panoramic%20videos%2C%0Aproviding%20a%20data%20source%20for%20downstream%20tasks.%20Specifically%2C%20to%20effectively%0Acapture%20the%20unique%20vertical%20vibration%20characteristics%20exhibited%20during%0Aquadruped%20locomotion%2C%20we%20introduce%20Vertical%20Jitter%20Encoding%20%28VJE%29.%20VJE%20extracts%0Acontrollable%20vertical%20signals%20through%20frequency-domain%20feature%20filtering%20and%0Aprovides%20high-quality%20prompts.%20To%20facilitate%20high-quality%20panoramic%20video%0Ageneration%20under%20jitter%20signal%20control%2C%20we%20propose%20a%20Scene-Object%20Controller%0A%28SOC%29%20that%20effectively%20manages%20object%20motion%20and%20boosts%20background%20jitter%0Acontrol%20through%20the%20attention%20mechanism.%20To%20address%20panoramic%20distortions%20in%0Awide-FoV%20video%20generation%2C%20we%20propose%20the%20Panoramic%20Enhancer%20%28PE%29-a%20dual-stream%0Aarchitecture%20that%20synergizes%20frequency-texture%20refinement%20for%20local%20detail%0Aenhancement%20with%20spatial-structure%20correction%20for%20global%20geometric%20consistency.%0AWe%20further%20demonstrate%20that%20the%20generated%20video%20sequences%20can%20serve%20as%20training%0Adata%20for%20the%20quadruped%20robot%27s%20panoramic%20visual%20perception%20model%2C%20enhancing%20the%0Aperformance%20of%20multi-object%20tracking%20in%20360-degree%20scenes.%20The%20source%20code%20and%0Amodel%20weights%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/losehu/QuaDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02512v1&entry.124074799=Read"},
{"title": "DRC: Enhancing Personalized Image Generation via Disentangled\n  Representation Composition", "author": "Yiyan Xu and Wuqiang Zheng and Wenjie Wang and Fengbin Zhu and Xinting Hu and Yang Zhang and Fuli Feng and Tat-Seng Chua", "abstract": "  Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.\n", "link": "http://arxiv.org/abs/2504.17349v2", "date": "2025-08-04", "relevancy": 2.3595, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6033}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5811}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRC%3A%20Enhancing%20Personalized%20Image%20Generation%20via%20Disentangled%0A%20%20Representation%20Composition&body=Title%3A%20DRC%3A%20Enhancing%20Personalized%20Image%20Generation%20via%20Disentangled%0A%20%20Representation%20Composition%0AAuthor%3A%20Yiyan%20Xu%20and%20Wuqiang%20Zheng%20and%20Wenjie%20Wang%20and%20Fengbin%20Zhu%20and%20Xinting%20Hu%20and%20Yang%20Zhang%20and%20Fuli%20Feng%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Personalized%20image%20generation%20has%20emerged%20as%20a%20promising%20direction%20in%0Amultimodal%20content%20creation.%20It%20aims%20to%20synthesize%20images%20tailored%20to%0Aindividual%20style%20preferences%20%28e.g.%2C%20color%20schemes%2C%20character%20appearances%2C%0Alayout%29%20and%20semantic%20intentions%20%28e.g.%2C%20emotion%2C%20action%2C%20scene%20contexts%29%20by%0Aleveraging%20user-interacted%20history%20images%20and%20multimodal%20instructions.%20Despite%0Anotable%20progress%2C%20existing%20methods%20--%20whether%20based%20on%20diffusion%20models%2C%20large%0Alanguage%20models%2C%20or%20Large%20Multimodal%20Models%20%28LMMs%29%20--%20struggle%20to%20accurately%0Acapture%20and%20fuse%20user%20style%20preferences%20and%20semantic%20intentions.%20In%20particular%2C%0Athe%20state-of-the-art%20LMM-based%20method%20suffers%20from%20the%20entanglement%20of%20visual%0Afeatures%2C%20leading%20to%20Guidance%20Collapse%2C%20where%20the%20generated%20images%20fail%20to%0Apreserve%20user-preferred%20styles%20or%20reflect%20the%20specified%20semantics.%0A%20%20To%20address%20these%20limitations%2C%20we%20introduce%20DRC%2C%20a%20novel%20personalized%20image%0Ageneration%20framework%20that%20enhances%20LMMs%20through%20Disentangled%20Representation%0AComposition.%20DRC%20explicitly%20extracts%20user%20style%20preferences%20and%20semantic%0Aintentions%20from%20history%20images%20and%20the%20reference%20image%2C%20respectively%2C%20to%20form%0Auser-specific%20latent%20instructions%20that%20guide%20image%20generation%20within%20LMMs.%0ASpecifically%2C%20it%20involves%20two%20critical%20learning%20stages%3A%201%29%20Disentanglement%0Alearning%2C%20which%20employs%20a%20dual-tower%20disentangler%20to%20explicitly%20separate%20style%0Aand%20semantic%20features%2C%20optimized%20via%20a%20reconstruction-driven%20paradigm%20with%0Adifficulty-aware%20importance%20sampling%3B%20and%202%29%20Personalized%20modeling%2C%20which%0Aapplies%20semantic-preserving%20augmentations%20to%20effectively%20adapt%20the%20disentangled%0Arepresentations%20for%20robust%20personalized%20generation.%20Extensive%20experiments%20on%0Atwo%20benchmarks%20demonstrate%20that%20DRC%20shows%20competitive%20performance%20while%0Aeffectively%20mitigating%20the%20guidance%20collapse%20issue%2C%20underscoring%20the%20importance%0Aof%20disentangled%20representation%20learning%20for%20controllable%20and%20effective%0Apersonalized%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRC%253A%2520Enhancing%2520Personalized%2520Image%2520Generation%2520via%2520Disentangled%250A%2520%2520Representation%2520Composition%26entry.906535625%3DYiyan%2520Xu%2520and%2520Wuqiang%2520Zheng%2520and%2520Wenjie%2520Wang%2520and%2520Fengbin%2520Zhu%2520and%2520Xinting%2520Hu%2520and%2520Yang%2520Zhang%2520and%2520Fuli%2520Feng%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Personalized%2520image%2520generation%2520has%2520emerged%2520as%2520a%2520promising%2520direction%2520in%250Amultimodal%2520content%2520creation.%2520It%2520aims%2520to%2520synthesize%2520images%2520tailored%2520to%250Aindividual%2520style%2520preferences%2520%2528e.g.%252C%2520color%2520schemes%252C%2520character%2520appearances%252C%250Alayout%2529%2520and%2520semantic%2520intentions%2520%2528e.g.%252C%2520emotion%252C%2520action%252C%2520scene%2520contexts%2529%2520by%250Aleveraging%2520user-interacted%2520history%2520images%2520and%2520multimodal%2520instructions.%2520Despite%250Anotable%2520progress%252C%2520existing%2520methods%2520--%2520whether%2520based%2520on%2520diffusion%2520models%252C%2520large%250Alanguage%2520models%252C%2520or%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520--%2520struggle%2520to%2520accurately%250Acapture%2520and%2520fuse%2520user%2520style%2520preferences%2520and%2520semantic%2520intentions.%2520In%2520particular%252C%250Athe%2520state-of-the-art%2520LMM-based%2520method%2520suffers%2520from%2520the%2520entanglement%2520of%2520visual%250Afeatures%252C%2520leading%2520to%2520Guidance%2520Collapse%252C%2520where%2520the%2520generated%2520images%2520fail%2520to%250Apreserve%2520user-preferred%2520styles%2520or%2520reflect%2520the%2520specified%2520semantics.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520DRC%252C%2520a%2520novel%2520personalized%2520image%250Ageneration%2520framework%2520that%2520enhances%2520LMMs%2520through%2520Disentangled%2520Representation%250AComposition.%2520DRC%2520explicitly%2520extracts%2520user%2520style%2520preferences%2520and%2520semantic%250Aintentions%2520from%2520history%2520images%2520and%2520the%2520reference%2520image%252C%2520respectively%252C%2520to%2520form%250Auser-specific%2520latent%2520instructions%2520that%2520guide%2520image%2520generation%2520within%2520LMMs.%250ASpecifically%252C%2520it%2520involves%2520two%2520critical%2520learning%2520stages%253A%25201%2529%2520Disentanglement%250Alearning%252C%2520which%2520employs%2520a%2520dual-tower%2520disentangler%2520to%2520explicitly%2520separate%2520style%250Aand%2520semantic%2520features%252C%2520optimized%2520via%2520a%2520reconstruction-driven%2520paradigm%2520with%250Adifficulty-aware%2520importance%2520sampling%253B%2520and%25202%2529%2520Personalized%2520modeling%252C%2520which%250Aapplies%2520semantic-preserving%2520augmentations%2520to%2520effectively%2520adapt%2520the%2520disentangled%250Arepresentations%2520for%2520robust%2520personalized%2520generation.%2520Extensive%2520experiments%2520on%250Atwo%2520benchmarks%2520demonstrate%2520that%2520DRC%2520shows%2520competitive%2520performance%2520while%250Aeffectively%2520mitigating%2520the%2520guidance%2520collapse%2520issue%252C%2520underscoring%2520the%2520importance%250Aof%2520disentangled%2520representation%2520learning%2520for%2520controllable%2520and%2520effective%250Apersonalized%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRC%3A%20Enhancing%20Personalized%20Image%20Generation%20via%20Disentangled%0A%20%20Representation%20Composition&entry.906535625=Yiyan%20Xu%20and%20Wuqiang%20Zheng%20and%20Wenjie%20Wang%20and%20Fengbin%20Zhu%20and%20Xinting%20Hu%20and%20Yang%20Zhang%20and%20Fuli%20Feng%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Personalized%20image%20generation%20has%20emerged%20as%20a%20promising%20direction%20in%0Amultimodal%20content%20creation.%20It%20aims%20to%20synthesize%20images%20tailored%20to%0Aindividual%20style%20preferences%20%28e.g.%2C%20color%20schemes%2C%20character%20appearances%2C%0Alayout%29%20and%20semantic%20intentions%20%28e.g.%2C%20emotion%2C%20action%2C%20scene%20contexts%29%20by%0Aleveraging%20user-interacted%20history%20images%20and%20multimodal%20instructions.%20Despite%0Anotable%20progress%2C%20existing%20methods%20--%20whether%20based%20on%20diffusion%20models%2C%20large%0Alanguage%20models%2C%20or%20Large%20Multimodal%20Models%20%28LMMs%29%20--%20struggle%20to%20accurately%0Acapture%20and%20fuse%20user%20style%20preferences%20and%20semantic%20intentions.%20In%20particular%2C%0Athe%20state-of-the-art%20LMM-based%20method%20suffers%20from%20the%20entanglement%20of%20visual%0Afeatures%2C%20leading%20to%20Guidance%20Collapse%2C%20where%20the%20generated%20images%20fail%20to%0Apreserve%20user-preferred%20styles%20or%20reflect%20the%20specified%20semantics.%0A%20%20To%20address%20these%20limitations%2C%20we%20introduce%20DRC%2C%20a%20novel%20personalized%20image%0Ageneration%20framework%20that%20enhances%20LMMs%20through%20Disentangled%20Representation%0AComposition.%20DRC%20explicitly%20extracts%20user%20style%20preferences%20and%20semantic%0Aintentions%20from%20history%20images%20and%20the%20reference%20image%2C%20respectively%2C%20to%20form%0Auser-specific%20latent%20instructions%20that%20guide%20image%20generation%20within%20LMMs.%0ASpecifically%2C%20it%20involves%20two%20critical%20learning%20stages%3A%201%29%20Disentanglement%0Alearning%2C%20which%20employs%20a%20dual-tower%20disentangler%20to%20explicitly%20separate%20style%0Aand%20semantic%20features%2C%20optimized%20via%20a%20reconstruction-driven%20paradigm%20with%0Adifficulty-aware%20importance%20sampling%3B%20and%202%29%20Personalized%20modeling%2C%20which%0Aapplies%20semantic-preserving%20augmentations%20to%20effectively%20adapt%20the%20disentangled%0Arepresentations%20for%20robust%20personalized%20generation.%20Extensive%20experiments%20on%0Atwo%20benchmarks%20demonstrate%20that%20DRC%20shows%20competitive%20performance%20while%0Aeffectively%20mitigating%20the%20guidance%20collapse%20issue%2C%20underscoring%20the%20importance%0Aof%20disentangled%20representation%20learning%20for%20controllable%20and%20effective%0Apersonalized%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17349v2&entry.124074799=Read"},
{"title": "UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes\n  leveraging Vision Large Language Models", "author": "Jun Yin and Jing Zhong and Peilin Li and Ruolin Pan and Pengyu Zeng and Miao Zhang and Shuai Lu", "abstract": "  Urban cultures and architectural styles vary significantly across cities due\nto geographical, chronological, historical, and socio-political factors.\nUnderstanding these differences is essential for anticipating how cities may\nevolve in the future. As representative cases of historical continuity and\nmodern innovation in China, Beijing and Shenzhen offer valuable perspectives\nfor exploring the transformation of urban streetscapes. However, conventional\napproaches to urban cultural studies often rely on expert interpretation and\nhistorical documentation, which are difficult to standardize across different\ncontexts. To address this, we propose a multimodal research framework based on\nvision-language models, enabling automated and scalable analysis of urban\nstreetscape style differences. This approach enhances the objectivity and\ndata-driven nature of urban form research. The contributions of this study are\nas follows: First, we construct UrbanDiffBench, a curated dataset of urban\nstreetscapes containing architectural images from different periods and\nregions. Second, we develop UrbanSense, the first vision-language-model-based\nframework for urban streetscape analysis, enabling the quantitative generation\nand comparison of urban style representations. Third, experimental results show\nthat Over 80% of generated descriptions pass the t-test (p less than 0.05).\nHigh Phi scores (0.912 for cities, 0.833 for periods) from subjective\nevaluations confirm the method's ability to capture subtle stylistic\ndifferences. These results highlight the method's potential to quantify and\ninterpret urban style evolution, offering a scientifically grounded lens for\nfuture design.\n", "link": "http://arxiv.org/abs/2506.10342v2", "date": "2025-08-04", "relevancy": 2.3595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanSense%3AA%20Framework%20for%20Quantitative%20Analysis%20of%20Urban%20Streetscapes%0A%20%20leveraging%20Vision%20Large%20Language%20Models&body=Title%3A%20UrbanSense%3AA%20Framework%20for%20Quantitative%20Analysis%20of%20Urban%20Streetscapes%0A%20%20leveraging%20Vision%20Large%20Language%20Models%0AAuthor%3A%20Jun%20Yin%20and%20Jing%20Zhong%20and%20Peilin%20Li%20and%20Ruolin%20Pan%20and%20Pengyu%20Zeng%20and%20Miao%20Zhang%20and%20Shuai%20Lu%0AAbstract%3A%20%20%20Urban%20cultures%20and%20architectural%20styles%20vary%20significantly%20across%20cities%20due%0Ato%20geographical%2C%20chronological%2C%20historical%2C%20and%20socio-political%20factors.%0AUnderstanding%20these%20differences%20is%20essential%20for%20anticipating%20how%20cities%20may%0Aevolve%20in%20the%20future.%20As%20representative%20cases%20of%20historical%20continuity%20and%0Amodern%20innovation%20in%20China%2C%20Beijing%20and%20Shenzhen%20offer%20valuable%20perspectives%0Afor%20exploring%20the%20transformation%20of%20urban%20streetscapes.%20However%2C%20conventional%0Aapproaches%20to%20urban%20cultural%20studies%20often%20rely%20on%20expert%20interpretation%20and%0Ahistorical%20documentation%2C%20which%20are%20difficult%20to%20standardize%20across%20different%0Acontexts.%20To%20address%20this%2C%20we%20propose%20a%20multimodal%20research%20framework%20based%20on%0Avision-language%20models%2C%20enabling%20automated%20and%20scalable%20analysis%20of%20urban%0Astreetscape%20style%20differences.%20This%20approach%20enhances%20the%20objectivity%20and%0Adata-driven%20nature%20of%20urban%20form%20research.%20The%20contributions%20of%20this%20study%20are%0Aas%20follows%3A%20First%2C%20we%20construct%20UrbanDiffBench%2C%20a%20curated%20dataset%20of%20urban%0Astreetscapes%20containing%20architectural%20images%20from%20different%20periods%20and%0Aregions.%20Second%2C%20we%20develop%20UrbanSense%2C%20the%20first%20vision-language-model-based%0Aframework%20for%20urban%20streetscape%20analysis%2C%20enabling%20the%20quantitative%20generation%0Aand%20comparison%20of%20urban%20style%20representations.%20Third%2C%20experimental%20results%20show%0Athat%20Over%2080%25%20of%20generated%20descriptions%20pass%20the%20t-test%20%28p%20less%20than%200.05%29.%0AHigh%20Phi%20scores%20%280.912%20for%20cities%2C%200.833%20for%20periods%29%20from%20subjective%0Aevaluations%20confirm%20the%20method%27s%20ability%20to%20capture%20subtle%20stylistic%0Adifferences.%20These%20results%20highlight%20the%20method%27s%20potential%20to%20quantify%20and%0Ainterpret%20urban%20style%20evolution%2C%20offering%20a%20scientifically%20grounded%20lens%20for%0Afuture%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanSense%253AA%2520Framework%2520for%2520Quantitative%2520Analysis%2520of%2520Urban%2520Streetscapes%250A%2520%2520leveraging%2520Vision%2520Large%2520Language%2520Models%26entry.906535625%3DJun%2520Yin%2520and%2520Jing%2520Zhong%2520and%2520Peilin%2520Li%2520and%2520Ruolin%2520Pan%2520and%2520Pengyu%2520Zeng%2520and%2520Miao%2520Zhang%2520and%2520Shuai%2520Lu%26entry.1292438233%3D%2520%2520Urban%2520cultures%2520and%2520architectural%2520styles%2520vary%2520significantly%2520across%2520cities%2520due%250Ato%2520geographical%252C%2520chronological%252C%2520historical%252C%2520and%2520socio-political%2520factors.%250AUnderstanding%2520these%2520differences%2520is%2520essential%2520for%2520anticipating%2520how%2520cities%2520may%250Aevolve%2520in%2520the%2520future.%2520As%2520representative%2520cases%2520of%2520historical%2520continuity%2520and%250Amodern%2520innovation%2520in%2520China%252C%2520Beijing%2520and%2520Shenzhen%2520offer%2520valuable%2520perspectives%250Afor%2520exploring%2520the%2520transformation%2520of%2520urban%2520streetscapes.%2520However%252C%2520conventional%250Aapproaches%2520to%2520urban%2520cultural%2520studies%2520often%2520rely%2520on%2520expert%2520interpretation%2520and%250Ahistorical%2520documentation%252C%2520which%2520are%2520difficult%2520to%2520standardize%2520across%2520different%250Acontexts.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520multimodal%2520research%2520framework%2520based%2520on%250Avision-language%2520models%252C%2520enabling%2520automated%2520and%2520scalable%2520analysis%2520of%2520urban%250Astreetscape%2520style%2520differences.%2520This%2520approach%2520enhances%2520the%2520objectivity%2520and%250Adata-driven%2520nature%2520of%2520urban%2520form%2520research.%2520The%2520contributions%2520of%2520this%2520study%2520are%250Aas%2520follows%253A%2520First%252C%2520we%2520construct%2520UrbanDiffBench%252C%2520a%2520curated%2520dataset%2520of%2520urban%250Astreetscapes%2520containing%2520architectural%2520images%2520from%2520different%2520periods%2520and%250Aregions.%2520Second%252C%2520we%2520develop%2520UrbanSense%252C%2520the%2520first%2520vision-language-model-based%250Aframework%2520for%2520urban%2520streetscape%2520analysis%252C%2520enabling%2520the%2520quantitative%2520generation%250Aand%2520comparison%2520of%2520urban%2520style%2520representations.%2520Third%252C%2520experimental%2520results%2520show%250Athat%2520Over%252080%2525%2520of%2520generated%2520descriptions%2520pass%2520the%2520t-test%2520%2528p%2520less%2520than%25200.05%2529.%250AHigh%2520Phi%2520scores%2520%25280.912%2520for%2520cities%252C%25200.833%2520for%2520periods%2529%2520from%2520subjective%250Aevaluations%2520confirm%2520the%2520method%2527s%2520ability%2520to%2520capture%2520subtle%2520stylistic%250Adifferences.%2520These%2520results%2520highlight%2520the%2520method%2527s%2520potential%2520to%2520quantify%2520and%250Ainterpret%2520urban%2520style%2520evolution%252C%2520offering%2520a%2520scientifically%2520grounded%2520lens%2520for%250Afuture%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanSense%3AA%20Framework%20for%20Quantitative%20Analysis%20of%20Urban%20Streetscapes%0A%20%20leveraging%20Vision%20Large%20Language%20Models&entry.906535625=Jun%20Yin%20and%20Jing%20Zhong%20and%20Peilin%20Li%20and%20Ruolin%20Pan%20and%20Pengyu%20Zeng%20and%20Miao%20Zhang%20and%20Shuai%20Lu&entry.1292438233=%20%20Urban%20cultures%20and%20architectural%20styles%20vary%20significantly%20across%20cities%20due%0Ato%20geographical%2C%20chronological%2C%20historical%2C%20and%20socio-political%20factors.%0AUnderstanding%20these%20differences%20is%20essential%20for%20anticipating%20how%20cities%20may%0Aevolve%20in%20the%20future.%20As%20representative%20cases%20of%20historical%20continuity%20and%0Amodern%20innovation%20in%20China%2C%20Beijing%20and%20Shenzhen%20offer%20valuable%20perspectives%0Afor%20exploring%20the%20transformation%20of%20urban%20streetscapes.%20However%2C%20conventional%0Aapproaches%20to%20urban%20cultural%20studies%20often%20rely%20on%20expert%20interpretation%20and%0Ahistorical%20documentation%2C%20which%20are%20difficult%20to%20standardize%20across%20different%0Acontexts.%20To%20address%20this%2C%20we%20propose%20a%20multimodal%20research%20framework%20based%20on%0Avision-language%20models%2C%20enabling%20automated%20and%20scalable%20analysis%20of%20urban%0Astreetscape%20style%20differences.%20This%20approach%20enhances%20the%20objectivity%20and%0Adata-driven%20nature%20of%20urban%20form%20research.%20The%20contributions%20of%20this%20study%20are%0Aas%20follows%3A%20First%2C%20we%20construct%20UrbanDiffBench%2C%20a%20curated%20dataset%20of%20urban%0Astreetscapes%20containing%20architectural%20images%20from%20different%20periods%20and%0Aregions.%20Second%2C%20we%20develop%20UrbanSense%2C%20the%20first%20vision-language-model-based%0Aframework%20for%20urban%20streetscape%20analysis%2C%20enabling%20the%20quantitative%20generation%0Aand%20comparison%20of%20urban%20style%20representations.%20Third%2C%20experimental%20results%20show%0Athat%20Over%2080%25%20of%20generated%20descriptions%20pass%20the%20t-test%20%28p%20less%20than%200.05%29.%0AHigh%20Phi%20scores%20%280.912%20for%20cities%2C%200.833%20for%20periods%29%20from%20subjective%0Aevaluations%20confirm%20the%20method%27s%20ability%20to%20capture%20subtle%20stylistic%0Adifferences.%20These%20results%20highlight%20the%20method%27s%20potential%20to%20quantify%20and%0Ainterpret%20urban%20style%20evolution%2C%20offering%20a%20scientifically%20grounded%20lens%20for%0Afuture%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10342v2&entry.124074799=Read"},
{"title": "PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs", "author": "Zhan Qu and Shuzhou Yuan and Michael F\u00e4rber", "abstract": "  This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.\n", "link": "http://arxiv.org/abs/2508.02515v1", "date": "2025-08-04", "relevancy": 2.3565, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%0A%20%20Songci%20with%20LLMs&body=Title%3A%20PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%0A%20%20Songci%20with%20LLMs%0AAuthor%3A%20Zhan%20Qu%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20systematic%20investigation%20into%20the%20constrained%0Ageneration%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20producing%20Songci%2C%20a%0Aclassical%20Chinese%20poetry%20form%20characterized%20by%20strict%20structural%2C%20tonal%2C%20and%0Arhyme%20constraints%20defined%20by%20Cipai%20templates.%20We%20first%20develop%20a%20comprehensive%2C%0Amulti-faceted%20evaluation%20framework%20that%20includes%3A%20%28i%29%20a%20formal%20conformity%0Ascore%2C%20%28ii%29%20automated%20quality%20assessment%20using%20LLMs%2C%20%28iii%29%20human%20evaluation%2C%0Aand%20%28iv%29%20classification-based%20probing%20tasks.%20Using%20this%20framework%2C%20we%20evaluate%0Athe%20generative%20performance%20of%2018%20LLMs%2C%20including%203%20proprietary%20models%20and%2015%0Aopen-source%20models%20across%20four%20families%2C%20under%20five%20prompting%20strategies%3A%0Azero-shot%2C%20one-shot%2C%20completion-based%2C%20instruction-tuned%2C%20and%20chain-of-thought.%0AFinally%2C%20we%20propose%20a%20Generate-Critic%20architecture%20in%20which%20the%20evaluation%0Aframework%20functions%20as%20an%20automated%20critic.%20Leveraging%20the%20critic%27s%20feedback%20as%0Aa%20reward%20signal%2C%20we%20fine-tune%20three%20lightweight%20open-source%20LLMs%20via%20supervised%0Afine-tuning%20%28SFT%29%2C%20resulting%20in%20improvements%20of%20up%20to%205.88%25%20in%20formal%0Aconformity.%20Our%20findings%20offer%20new%20insights%20into%20the%20generative%20strengths%20and%0Alimitations%20of%20LLMs%20in%20producing%20culturally%20significant%20and%20formally%0Aconstrained%20literary%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoeTone%253A%2520A%2520Framework%2520for%2520Constrained%2520Generation%2520of%2520Structured%2520Chinese%250A%2520%2520Songci%2520with%2520LLMs%26entry.906535625%3DZhan%2520Qu%2520and%2520Shuzhou%2520Yuan%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520systematic%2520investigation%2520into%2520the%2520constrained%250Ageneration%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520producing%2520Songci%252C%2520a%250Aclassical%2520Chinese%2520poetry%2520form%2520characterized%2520by%2520strict%2520structural%252C%2520tonal%252C%2520and%250Arhyme%2520constraints%2520defined%2520by%2520Cipai%2520templates.%2520We%2520first%2520develop%2520a%2520comprehensive%252C%250Amulti-faceted%2520evaluation%2520framework%2520that%2520includes%253A%2520%2528i%2529%2520a%2520formal%2520conformity%250Ascore%252C%2520%2528ii%2529%2520automated%2520quality%2520assessment%2520using%2520LLMs%252C%2520%2528iii%2529%2520human%2520evaluation%252C%250Aand%2520%2528iv%2529%2520classification-based%2520probing%2520tasks.%2520Using%2520this%2520framework%252C%2520we%2520evaluate%250Athe%2520generative%2520performance%2520of%252018%2520LLMs%252C%2520including%25203%2520proprietary%2520models%2520and%252015%250Aopen-source%2520models%2520across%2520four%2520families%252C%2520under%2520five%2520prompting%2520strategies%253A%250Azero-shot%252C%2520one-shot%252C%2520completion-based%252C%2520instruction-tuned%252C%2520and%2520chain-of-thought.%250AFinally%252C%2520we%2520propose%2520a%2520Generate-Critic%2520architecture%2520in%2520which%2520the%2520evaluation%250Aframework%2520functions%2520as%2520an%2520automated%2520critic.%2520Leveraging%2520the%2520critic%2527s%2520feedback%2520as%250Aa%2520reward%2520signal%252C%2520we%2520fine-tune%2520three%2520lightweight%2520open-source%2520LLMs%2520via%2520supervised%250Afine-tuning%2520%2528SFT%2529%252C%2520resulting%2520in%2520improvements%2520of%2520up%2520to%25205.88%2525%2520in%2520formal%250Aconformity.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520generative%2520strengths%2520and%250Alimitations%2520of%2520LLMs%2520in%2520producing%2520culturally%2520significant%2520and%2520formally%250Aconstrained%2520literary%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%0A%20%20Songci%20with%20LLMs&entry.906535625=Zhan%20Qu%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20This%20paper%20presents%20a%20systematic%20investigation%20into%20the%20constrained%0Ageneration%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20producing%20Songci%2C%20a%0Aclassical%20Chinese%20poetry%20form%20characterized%20by%20strict%20structural%2C%20tonal%2C%20and%0Arhyme%20constraints%20defined%20by%20Cipai%20templates.%20We%20first%20develop%20a%20comprehensive%2C%0Amulti-faceted%20evaluation%20framework%20that%20includes%3A%20%28i%29%20a%20formal%20conformity%0Ascore%2C%20%28ii%29%20automated%20quality%20assessment%20using%20LLMs%2C%20%28iii%29%20human%20evaluation%2C%0Aand%20%28iv%29%20classification-based%20probing%20tasks.%20Using%20this%20framework%2C%20we%20evaluate%0Athe%20generative%20performance%20of%2018%20LLMs%2C%20including%203%20proprietary%20models%20and%2015%0Aopen-source%20models%20across%20four%20families%2C%20under%20five%20prompting%20strategies%3A%0Azero-shot%2C%20one-shot%2C%20completion-based%2C%20instruction-tuned%2C%20and%20chain-of-thought.%0AFinally%2C%20we%20propose%20a%20Generate-Critic%20architecture%20in%20which%20the%20evaluation%0Aframework%20functions%20as%20an%20automated%20critic.%20Leveraging%20the%20critic%27s%20feedback%20as%0Aa%20reward%20signal%2C%20we%20fine-tune%20three%20lightweight%20open-source%20LLMs%20via%20supervised%0Afine-tuning%20%28SFT%29%2C%20resulting%20in%20improvements%20of%20up%20to%205.88%25%20in%20formal%0Aconformity.%20Our%20findings%20offer%20new%20insights%20into%20the%20generative%20strengths%20and%0Alimitations%20of%20LLMs%20in%20producing%20culturally%20significant%20and%20formally%0Aconstrained%20literary%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02515v1&entry.124074799=Read"},
{"title": "Model-Independent Machine Learning Approach for Nanometric Axial\n  Localization and Tracking", "author": "Andrey Alexandrov and Giovanni Acampora and Giovanni De Lellis and Antonia Di Crescenzo and Chiara Errico and Daria Morozova and Valeri Tioukov and Autilia Vittiello", "abstract": "  Accurately tracking particles and determining their coordinate along the\noptical axis is a major challenge in optical microscopy, especially when\nextremely high precision is needed. In this study, we introduce a deep learning\napproach using convolutional neural networks (CNNs) that can determine axial\ncoordinates from dual-focal-plane images without relying on predefined models.\nOur method achieves an axial localization precision of 40 nanometers-six times\nbetter than traditional single-focal-plane techniques. The model's simple\ndesign and strong performance make it suitable for a wide range of uses,\nincluding dark matter detection, proton therapy for cancer, and radiation\nprotection in space. It also shows promise in fields like biological imaging,\nmaterials science, and environmental monitoring. This work highlights how\nmachine learning can turn complex image data into reliable, precise\ninformation, offering a flexible and powerful tool for many scientific\napplications.\n", "link": "http://arxiv.org/abs/2505.14754v2", "date": "2025-08-04", "relevancy": 2.3518, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5904}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Independent%20Machine%20Learning%20Approach%20for%20Nanometric%20Axial%0A%20%20Localization%20and%20Tracking&body=Title%3A%20Model-Independent%20Machine%20Learning%20Approach%20for%20Nanometric%20Axial%0A%20%20Localization%20and%20Tracking%0AAuthor%3A%20Andrey%20Alexandrov%20and%20Giovanni%20Acampora%20and%20Giovanni%20De%20Lellis%20and%20Antonia%20Di%20Crescenzo%20and%20Chiara%20Errico%20and%20Daria%20Morozova%20and%20Valeri%20Tioukov%20and%20Autilia%20Vittiello%0AAbstract%3A%20%20%20Accurately%20tracking%20particles%20and%20determining%20their%20coordinate%20along%20the%0Aoptical%20axis%20is%20a%20major%20challenge%20in%20optical%20microscopy%2C%20especially%20when%0Aextremely%20high%20precision%20is%20needed.%20In%20this%20study%2C%20we%20introduce%20a%20deep%20learning%0Aapproach%20using%20convolutional%20neural%20networks%20%28CNNs%29%20that%20can%20determine%20axial%0Acoordinates%20from%20dual-focal-plane%20images%20without%20relying%20on%20predefined%20models.%0AOur%20method%20achieves%20an%20axial%20localization%20precision%20of%2040%20nanometers-six%20times%0Abetter%20than%20traditional%20single-focal-plane%20techniques.%20The%20model%27s%20simple%0Adesign%20and%20strong%20performance%20make%20it%20suitable%20for%20a%20wide%20range%20of%20uses%2C%0Aincluding%20dark%20matter%20detection%2C%20proton%20therapy%20for%20cancer%2C%20and%20radiation%0Aprotection%20in%20space.%20It%20also%20shows%20promise%20in%20fields%20like%20biological%20imaging%2C%0Amaterials%20science%2C%20and%20environmental%20monitoring.%20This%20work%20highlights%20how%0Amachine%20learning%20can%20turn%20complex%20image%20data%20into%20reliable%2C%20precise%0Ainformation%2C%20offering%20a%20flexible%20and%20powerful%20tool%20for%20many%20scientific%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Independent%2520Machine%2520Learning%2520Approach%2520for%2520Nanometric%2520Axial%250A%2520%2520Localization%2520and%2520Tracking%26entry.906535625%3DAndrey%2520Alexandrov%2520and%2520Giovanni%2520Acampora%2520and%2520Giovanni%2520De%2520Lellis%2520and%2520Antonia%2520Di%2520Crescenzo%2520and%2520Chiara%2520Errico%2520and%2520Daria%2520Morozova%2520and%2520Valeri%2520Tioukov%2520and%2520Autilia%2520Vittiello%26entry.1292438233%3D%2520%2520Accurately%2520tracking%2520particles%2520and%2520determining%2520their%2520coordinate%2520along%2520the%250Aoptical%2520axis%2520is%2520a%2520major%2520challenge%2520in%2520optical%2520microscopy%252C%2520especially%2520when%250Aextremely%2520high%2520precision%2520is%2520needed.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520deep%2520learning%250Aapproach%2520using%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520that%2520can%2520determine%2520axial%250Acoordinates%2520from%2520dual-focal-plane%2520images%2520without%2520relying%2520on%2520predefined%2520models.%250AOur%2520method%2520achieves%2520an%2520axial%2520localization%2520precision%2520of%252040%2520nanometers-six%2520times%250Abetter%2520than%2520traditional%2520single-focal-plane%2520techniques.%2520The%2520model%2527s%2520simple%250Adesign%2520and%2520strong%2520performance%2520make%2520it%2520suitable%2520for%2520a%2520wide%2520range%2520of%2520uses%252C%250Aincluding%2520dark%2520matter%2520detection%252C%2520proton%2520therapy%2520for%2520cancer%252C%2520and%2520radiation%250Aprotection%2520in%2520space.%2520It%2520also%2520shows%2520promise%2520in%2520fields%2520like%2520biological%2520imaging%252C%250Amaterials%2520science%252C%2520and%2520environmental%2520monitoring.%2520This%2520work%2520highlights%2520how%250Amachine%2520learning%2520can%2520turn%2520complex%2520image%2520data%2520into%2520reliable%252C%2520precise%250Ainformation%252C%2520offering%2520a%2520flexible%2520and%2520powerful%2520tool%2520for%2520many%2520scientific%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Independent%20Machine%20Learning%20Approach%20for%20Nanometric%20Axial%0A%20%20Localization%20and%20Tracking&entry.906535625=Andrey%20Alexandrov%20and%20Giovanni%20Acampora%20and%20Giovanni%20De%20Lellis%20and%20Antonia%20Di%20Crescenzo%20and%20Chiara%20Errico%20and%20Daria%20Morozova%20and%20Valeri%20Tioukov%20and%20Autilia%20Vittiello&entry.1292438233=%20%20Accurately%20tracking%20particles%20and%20determining%20their%20coordinate%20along%20the%0Aoptical%20axis%20is%20a%20major%20challenge%20in%20optical%20microscopy%2C%20especially%20when%0Aextremely%20high%20precision%20is%20needed.%20In%20this%20study%2C%20we%20introduce%20a%20deep%20learning%0Aapproach%20using%20convolutional%20neural%20networks%20%28CNNs%29%20that%20can%20determine%20axial%0Acoordinates%20from%20dual-focal-plane%20images%20without%20relying%20on%20predefined%20models.%0AOur%20method%20achieves%20an%20axial%20localization%20precision%20of%2040%20nanometers-six%20times%0Abetter%20than%20traditional%20single-focal-plane%20techniques.%20The%20model%27s%20simple%0Adesign%20and%20strong%20performance%20make%20it%20suitable%20for%20a%20wide%20range%20of%20uses%2C%0Aincluding%20dark%20matter%20detection%2C%20proton%20therapy%20for%20cancer%2C%20and%20radiation%0Aprotection%20in%20space.%20It%20also%20shows%20promise%20in%20fields%20like%20biological%20imaging%2C%0Amaterials%20science%2C%20and%20environmental%20monitoring.%20This%20work%20highlights%20how%0Amachine%20learning%20can%20turn%20complex%20image%20data%20into%20reliable%2C%20precise%0Ainformation%2C%20offering%20a%20flexible%20and%20powerful%20tool%20for%20many%20scientific%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14754v2&entry.124074799=Read"},
{"title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "author": "Hoyoung Lee and Junhyuk Seo and Suhwan Park and Junhyeong Lee and Wonbin Ahn and Chanyeol Choi and Alejandro Lopez-Lira and Yongjae Lee", "abstract": "  In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence.\n", "link": "http://arxiv.org/abs/2507.20957v2", "date": "2025-08-04", "relevancy": 2.3496, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&body=Title%3A%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis%0AAuthor%3A%20Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee%0AAbstract%3A%20%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Adue%20to%20discrepancies%20between%20pre-trained%20parametric%20knowledge%20and%20real-time%0Amarket%20data.%20These%20conflicts%20become%20particularly%20problematic%20when%20LLMs%20are%0Adeployed%20in%20real-world%20investment%20services%2C%20where%20misalignment%20between%20a%0Amodel%27s%20embedded%20preferences%20and%20those%20of%20the%20financial%20institution%20can%20lead%20to%0Aunreliable%20recommendations.%20Yet%20little%20research%20has%20examined%20what%20investment%0Aviews%20LLMs%20actually%20hold.%20We%20propose%20an%20experimental%20framework%20to%20investigate%0Asuch%20conflicts%2C%20offering%20the%20first%20quantitative%20analysis%20of%20confirmation%20bias%0Ain%20LLM-based%20investment%20analysis.%20Using%20hypothetical%20scenarios%20with%20balanced%0Aand%20imbalanced%20arguments%2C%20we%20extract%20the%20latent%20preferences%20of%20models%20and%0Ameasure%20their%20persistence.%20Focusing%20on%20sector%2C%20size%2C%20and%20momentum%2C%20our%20analysis%0Areveals%20distinct%2C%20model-specific%20tendencies.%20In%20particular%2C%20we%20observe%20a%0Aconsistent%20preference%20for%20large-cap%20stocks%20and%20contrarian%20strategies%20across%0Amost%20models.%20These%20preferences%20often%20harden%20into%20confirmation%20bias%2C%20with%20models%0Aclinging%20to%20initial%20judgments%20despite%20counter-evidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520AI%252C%2520Not%2520Your%2520View%253A%2520The%2520Bias%2520of%2520LLMs%2520in%2520Investment%2520Analysis%26entry.906535625%3DHoyoung%2520Lee%2520and%2520Junhyuk%2520Seo%2520and%2520Suhwan%2520Park%2520and%2520Junhyeong%2520Lee%2520and%2520Wonbin%2520Ahn%2520and%2520Chanyeol%2520Choi%2520and%2520Alejandro%2520Lopez-Lira%2520and%2520Yongjae%2520Lee%26entry.1292438233%3D%2520%2520In%2520finance%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520frequent%2520knowledge%2520conflicts%250Adue%2520to%2520discrepancies%2520between%2520pre-trained%2520parametric%2520knowledge%2520and%2520real-time%250Amarket%2520data.%2520These%2520conflicts%2520become%2520particularly%2520problematic%2520when%2520LLMs%2520are%250Adeployed%2520in%2520real-world%2520investment%2520services%252C%2520where%2520misalignment%2520between%2520a%250Amodel%2527s%2520embedded%2520preferences%2520and%2520those%2520of%2520the%2520financial%2520institution%2520can%2520lead%2520to%250Aunreliable%2520recommendations.%2520Yet%2520little%2520research%2520has%2520examined%2520what%2520investment%250Aviews%2520LLMs%2520actually%2520hold.%2520We%2520propose%2520an%2520experimental%2520framework%2520to%2520investigate%250Asuch%2520conflicts%252C%2520offering%2520the%2520first%2520quantitative%2520analysis%2520of%2520confirmation%2520bias%250Ain%2520LLM-based%2520investment%2520analysis.%2520Using%2520hypothetical%2520scenarios%2520with%2520balanced%250Aand%2520imbalanced%2520arguments%252C%2520we%2520extract%2520the%2520latent%2520preferences%2520of%2520models%2520and%250Ameasure%2520their%2520persistence.%2520Focusing%2520on%2520sector%252C%2520size%252C%2520and%2520momentum%252C%2520our%2520analysis%250Areveals%2520distinct%252C%2520model-specific%2520tendencies.%2520In%2520particular%252C%2520we%2520observe%2520a%250Aconsistent%2520preference%2520for%2520large-cap%2520stocks%2520and%2520contrarian%2520strategies%2520across%250Amost%2520models.%2520These%2520preferences%2520often%2520harden%2520into%2520confirmation%2520bias%252C%2520with%2520models%250Aclinging%2520to%2520initial%2520judgments%2520despite%2520counter-evidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&entry.906535625=Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee&entry.1292438233=%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Adue%20to%20discrepancies%20between%20pre-trained%20parametric%20knowledge%20and%20real-time%0Amarket%20data.%20These%20conflicts%20become%20particularly%20problematic%20when%20LLMs%20are%0Adeployed%20in%20real-world%20investment%20services%2C%20where%20misalignment%20between%20a%0Amodel%27s%20embedded%20preferences%20and%20those%20of%20the%20financial%20institution%20can%20lead%20to%0Aunreliable%20recommendations.%20Yet%20little%20research%20has%20examined%20what%20investment%0Aviews%20LLMs%20actually%20hold.%20We%20propose%20an%20experimental%20framework%20to%20investigate%0Asuch%20conflicts%2C%20offering%20the%20first%20quantitative%20analysis%20of%20confirmation%20bias%0Ain%20LLM-based%20investment%20analysis.%20Using%20hypothetical%20scenarios%20with%20balanced%0Aand%20imbalanced%20arguments%2C%20we%20extract%20the%20latent%20preferences%20of%20models%20and%0Ameasure%20their%20persistence.%20Focusing%20on%20sector%2C%20size%2C%20and%20momentum%2C%20our%20analysis%0Areveals%20distinct%2C%20model-specific%20tendencies.%20In%20particular%2C%20we%20observe%20a%0Aconsistent%20preference%20for%20large-cap%20stocks%20and%20contrarian%20strategies%20across%0Amost%20models.%20These%20preferences%20often%20harden%20into%20confirmation%20bias%2C%20with%20models%0Aclinging%20to%20initial%20judgments%20despite%20counter-evidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20957v2&entry.124074799=Read"},
{"title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in\n  Intelligent Tutoring Systems: The Case of MathAIde app", "author": "Guilherme Guerino and Luiz Rodrigues and Luana Bianchini and Mariana Alves and Marcelo Marinho and Thomaz Veloso and Valmir Macario and Diego Dermeval and Thales Vieira and Ig Bittencourt and Seiji Isotani", "abstract": "  Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments.\n", "link": "http://arxiv.org/abs/2508.00103v2", "date": "2025-08-04", "relevancy": 2.3459, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4706}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mixed%20User-Centered%20Approach%20to%20Enable%20Augmented%20Intelligence%20in%0A%20%20Intelligent%20Tutoring%20Systems%3A%20The%20Case%20of%20MathAIde%20app&body=Title%3A%20A%20Mixed%20User-Centered%20Approach%20to%20Enable%20Augmented%20Intelligence%20in%0A%20%20Intelligent%20Tutoring%20Systems%3A%20The%20Case%20of%20MathAIde%20app%0AAuthor%3A%20Guilherme%20Guerino%20and%20Luiz%20Rodrigues%20and%20Luana%20Bianchini%20and%20Mariana%20Alves%20and%20Marcelo%20Marinho%20and%20Thomaz%20Veloso%20and%20Valmir%20Macario%20and%20Diego%20Dermeval%20and%20Thales%20Vieira%20and%20Ig%20Bittencourt%20and%20Seiji%20Isotani%0AAbstract%3A%20%20%20Integrating%20Artificial%20Intelligence%20in%20Education%20%28AIED%29%20aims%20to%20enhance%0Alearning%20experiences%20through%20technologies%20like%20Intelligent%20Tutoring%20Systems%0A%28ITS%29%2C%20offering%20personalized%20learning%2C%20increased%20engagement%2C%20and%20improved%0Aretention%20rates.%20However%2C%20AIED%20faces%20three%20main%20challenges%3A%20the%20critical%20role%0Aof%20teachers%20in%20the%20design%20process%2C%20the%20limitations%20and%20reliability%20of%20AI%20tools%2C%0Aand%20the%20accessibility%20of%20technological%20resources.%20Augmented%20Intelligence%20%28AuI%29%0Aaddresses%20these%20challenges%20by%20enhancing%20human%20capabilities%20rather%20than%0Areplacing%20them%2C%20allowing%20systems%20to%20suggest%20solutions.%20In%20contrast%2C%20humans%0Aprovide%20final%20assessments%2C%20thus%20improving%20AI%20over%20time.%20In%20this%20sense%2C%20this%0Astudy%20focuses%20on%20designing%2C%20developing%2C%20and%20evaluating%20MathAIde%2C%20an%20ITS%20that%0Acorrects%20mathematics%20exercises%20using%20computer%20vision%20and%20AI%20and%20provides%0Afeedback%20based%20on%20photos%20of%20student%20work.%20The%20methodology%20included%0Abrainstorming%20sessions%20with%20potential%20users%2C%20high-fidelity%20prototyping%2C%20A/B%0Atesting%2C%20and%20a%20case%20study%20involving%20real-world%20classroom%20environments%20for%0Ateachers%20and%20students.%20Our%20research%20identified%20several%20design%20possibilities%20for%0Aimplementing%20AuI%20in%20ITSs%2C%20emphasizing%20a%20balance%20between%20user%20needs%20and%0Atechnological%20feasibility.%20Prioritization%20and%20validation%20through%20prototyping%0Aand%20testing%20highlighted%20the%20importance%20of%20efficiency%20metrics%2C%20ultimately%0Aleading%20to%20a%20solution%20that%20offers%20pre-defined%20remediation%20alternatives%20for%0Ateachers.%20Real-world%20deployment%20demonstrated%20the%20usefulness%20of%20the%20proposed%0Asolution.%20Our%20research%20contributes%20to%20the%20literature%20by%20providing%20a%20usable%2C%0Ateacher-centered%20design%20approach%20that%20involves%20teachers%20in%20all%20design%20phases.%0AAs%20a%20practical%20implication%2C%20we%20highlight%20that%20the%20user-centered%20design%20approach%0Aincreases%20the%20usefulness%20and%20adoption%20potential%20of%20AIED%20systems%2C%20especially%20in%0Aresource-limited%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mixed%2520User-Centered%2520Approach%2520to%2520Enable%2520Augmented%2520Intelligence%2520in%250A%2520%2520Intelligent%2520Tutoring%2520Systems%253A%2520The%2520Case%2520of%2520MathAIde%2520app%26entry.906535625%3DGuilherme%2520Guerino%2520and%2520Luiz%2520Rodrigues%2520and%2520Luana%2520Bianchini%2520and%2520Mariana%2520Alves%2520and%2520Marcelo%2520Marinho%2520and%2520Thomaz%2520Veloso%2520and%2520Valmir%2520Macario%2520and%2520Diego%2520Dermeval%2520and%2520Thales%2520Vieira%2520and%2520Ig%2520Bittencourt%2520and%2520Seiji%2520Isotani%26entry.1292438233%3D%2520%2520Integrating%2520Artificial%2520Intelligence%2520in%2520Education%2520%2528AIED%2529%2520aims%2520to%2520enhance%250Alearning%2520experiences%2520through%2520technologies%2520like%2520Intelligent%2520Tutoring%2520Systems%250A%2528ITS%2529%252C%2520offering%2520personalized%2520learning%252C%2520increased%2520engagement%252C%2520and%2520improved%250Aretention%2520rates.%2520However%252C%2520AIED%2520faces%2520three%2520main%2520challenges%253A%2520the%2520critical%2520role%250Aof%2520teachers%2520in%2520the%2520design%2520process%252C%2520the%2520limitations%2520and%2520reliability%2520of%2520AI%2520tools%252C%250Aand%2520the%2520accessibility%2520of%2520technological%2520resources.%2520Augmented%2520Intelligence%2520%2528AuI%2529%250Aaddresses%2520these%2520challenges%2520by%2520enhancing%2520human%2520capabilities%2520rather%2520than%250Areplacing%2520them%252C%2520allowing%2520systems%2520to%2520suggest%2520solutions.%2520In%2520contrast%252C%2520humans%250Aprovide%2520final%2520assessments%252C%2520thus%2520improving%2520AI%2520over%2520time.%2520In%2520this%2520sense%252C%2520this%250Astudy%2520focuses%2520on%2520designing%252C%2520developing%252C%2520and%2520evaluating%2520MathAIde%252C%2520an%2520ITS%2520that%250Acorrects%2520mathematics%2520exercises%2520using%2520computer%2520vision%2520and%2520AI%2520and%2520provides%250Afeedback%2520based%2520on%2520photos%2520of%2520student%2520work.%2520The%2520methodology%2520included%250Abrainstorming%2520sessions%2520with%2520potential%2520users%252C%2520high-fidelity%2520prototyping%252C%2520A/B%250Atesting%252C%2520and%2520a%2520case%2520study%2520involving%2520real-world%2520classroom%2520environments%2520for%250Ateachers%2520and%2520students.%2520Our%2520research%2520identified%2520several%2520design%2520possibilities%2520for%250Aimplementing%2520AuI%2520in%2520ITSs%252C%2520emphasizing%2520a%2520balance%2520between%2520user%2520needs%2520and%250Atechnological%2520feasibility.%2520Prioritization%2520and%2520validation%2520through%2520prototyping%250Aand%2520testing%2520highlighted%2520the%2520importance%2520of%2520efficiency%2520metrics%252C%2520ultimately%250Aleading%2520to%2520a%2520solution%2520that%2520offers%2520pre-defined%2520remediation%2520alternatives%2520for%250Ateachers.%2520Real-world%2520deployment%2520demonstrated%2520the%2520usefulness%2520of%2520the%2520proposed%250Asolution.%2520Our%2520research%2520contributes%2520to%2520the%2520literature%2520by%2520providing%2520a%2520usable%252C%250Ateacher-centered%2520design%2520approach%2520that%2520involves%2520teachers%2520in%2520all%2520design%2520phases.%250AAs%2520a%2520practical%2520implication%252C%2520we%2520highlight%2520that%2520the%2520user-centered%2520design%2520approach%250Aincreases%2520the%2520usefulness%2520and%2520adoption%2520potential%2520of%2520AIED%2520systems%252C%2520especially%2520in%250Aresource-limited%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mixed%20User-Centered%20Approach%20to%20Enable%20Augmented%20Intelligence%20in%0A%20%20Intelligent%20Tutoring%20Systems%3A%20The%20Case%20of%20MathAIde%20app&entry.906535625=Guilherme%20Guerino%20and%20Luiz%20Rodrigues%20and%20Luana%20Bianchini%20and%20Mariana%20Alves%20and%20Marcelo%20Marinho%20and%20Thomaz%20Veloso%20and%20Valmir%20Macario%20and%20Diego%20Dermeval%20and%20Thales%20Vieira%20and%20Ig%20Bittencourt%20and%20Seiji%20Isotani&entry.1292438233=%20%20Integrating%20Artificial%20Intelligence%20in%20Education%20%28AIED%29%20aims%20to%20enhance%0Alearning%20experiences%20through%20technologies%20like%20Intelligent%20Tutoring%20Systems%0A%28ITS%29%2C%20offering%20personalized%20learning%2C%20increased%20engagement%2C%20and%20improved%0Aretention%20rates.%20However%2C%20AIED%20faces%20three%20main%20challenges%3A%20the%20critical%20role%0Aof%20teachers%20in%20the%20design%20process%2C%20the%20limitations%20and%20reliability%20of%20AI%20tools%2C%0Aand%20the%20accessibility%20of%20technological%20resources.%20Augmented%20Intelligence%20%28AuI%29%0Aaddresses%20these%20challenges%20by%20enhancing%20human%20capabilities%20rather%20than%0Areplacing%20them%2C%20allowing%20systems%20to%20suggest%20solutions.%20In%20contrast%2C%20humans%0Aprovide%20final%20assessments%2C%20thus%20improving%20AI%20over%20time.%20In%20this%20sense%2C%20this%0Astudy%20focuses%20on%20designing%2C%20developing%2C%20and%20evaluating%20MathAIde%2C%20an%20ITS%20that%0Acorrects%20mathematics%20exercises%20using%20computer%20vision%20and%20AI%20and%20provides%0Afeedback%20based%20on%20photos%20of%20student%20work.%20The%20methodology%20included%0Abrainstorming%20sessions%20with%20potential%20users%2C%20high-fidelity%20prototyping%2C%20A/B%0Atesting%2C%20and%20a%20case%20study%20involving%20real-world%20classroom%20environments%20for%0Ateachers%20and%20students.%20Our%20research%20identified%20several%20design%20possibilities%20for%0Aimplementing%20AuI%20in%20ITSs%2C%20emphasizing%20a%20balance%20between%20user%20needs%20and%0Atechnological%20feasibility.%20Prioritization%20and%20validation%20through%20prototyping%0Aand%20testing%20highlighted%20the%20importance%20of%20efficiency%20metrics%2C%20ultimately%0Aleading%20to%20a%20solution%20that%20offers%20pre-defined%20remediation%20alternatives%20for%0Ateachers.%20Real-world%20deployment%20demonstrated%20the%20usefulness%20of%20the%20proposed%0Asolution.%20Our%20research%20contributes%20to%20the%20literature%20by%20providing%20a%20usable%2C%0Ateacher-centered%20design%20approach%20that%20involves%20teachers%20in%20all%20design%20phases.%0AAs%20a%20practical%20implication%2C%20we%20highlight%20that%20the%20user-centered%20design%20approach%0Aincreases%20the%20usefulness%20and%20adoption%20potential%20of%20AIED%20systems%2C%20especially%20in%0Aresource-limited%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00103v2&entry.124074799=Read"},
{"title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment", "author": "Wentao Zhang and Yilei Zhao and Chuqiao Zong and Xinrun Wang and Bo An", "abstract": "  Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.\n", "link": "http://arxiv.org/abs/2508.02292v1", "date": "2025-08-04", "relevancy": 2.3441, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinWorld%3A%20An%20All-in-One%20Open-Source%20Platform%20for%20End-to-End%20Financial%20AI%0A%20%20Research%20and%20Deployment&body=Title%3A%20FinWorld%3A%20An%20All-in-One%20Open-Source%20Platform%20for%20End-to-End%20Financial%20AI%0A%20%20Research%20and%20Deployment%0AAuthor%3A%20Wentao%20Zhang%20and%20Yilei%20Zhao%20and%20Chuqiao%20Zong%20and%20Xinrun%20Wang%20and%20Bo%20An%0AAbstract%3A%20%20%20Financial%20AI%20holds%20great%20promise%20for%20transforming%20modern%20finance%2C%20with%20the%0Apotential%20to%20support%20a%20wide%20range%20of%20tasks%20such%20as%20market%20forecasting%2C%0Aportfolio%20management%2C%20quantitative%20trading%2C%20and%20automated%20analysis.%20However%2C%0Aexisting%20platforms%20remain%20limited%20in%20task%20coverage%2C%20lack%20robust%20multimodal%20data%0Aintegration%2C%20and%20offer%20insufficient%20support%20for%20the%20training%20and%20deployment%20of%0Alarge%20language%20models%20%28LLMs%29.%20In%20response%20to%20these%20limitations%2C%20we%20present%0AFinWorld%2C%20an%20all-in-one%20open-source%20platform%20that%20provides%20end-to-end%20support%0Afor%20the%20entire%20financial%20AI%20workflow%2C%20from%20data%20acquisition%20to%20experimentation%0Aand%20deployment.%20FinWorld%20distinguishes%20itself%20through%20native%20integration%20of%0Aheterogeneous%20financial%20data%2C%20unified%20support%20for%20diverse%20AI%20paradigms%2C%20and%0Aadvanced%20agent%20automation%2C%20enabling%20seamless%20development%20and%20deployment.%0ALeveraging%20data%20from%202%20representative%20markets%2C%204%20stock%20pools%2C%20and%20over%20800%0Amillion%20financial%20data%20points%2C%20we%20conduct%20comprehensive%20experiments%20on%204%20key%0Afinancial%20AI%20tasks.%20These%20experiments%20systematically%20evaluate%20deep%20learning%20and%0Areinforcement%20learning%20algorithms%2C%20with%20particular%20emphasis%20on%20RL-based%0Afinetuning%20for%20LLMs%20and%20LLM%20Agents.%20The%20empirical%20results%20demonstrate%20that%0AFinWorld%20significantly%20enhances%20reproducibility%2C%20supports%20transparent%0Abenchmarking%2C%20and%20streamlines%20deployment%2C%20thereby%20providing%20a%20strong%20foundation%0Afor%20future%20research%20and%20real-world%20applications.%20Code%20is%20available%20at%0AGithub~%5Cfootnote%7Bhttps%3A//github.com/DVampire/FinWorld%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinWorld%253A%2520An%2520All-in-One%2520Open-Source%2520Platform%2520for%2520End-to-End%2520Financial%2520AI%250A%2520%2520Research%2520and%2520Deployment%26entry.906535625%3DWentao%2520Zhang%2520and%2520Yilei%2520Zhao%2520and%2520Chuqiao%2520Zong%2520and%2520Xinrun%2520Wang%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Financial%2520AI%2520holds%2520great%2520promise%2520for%2520transforming%2520modern%2520finance%252C%2520with%2520the%250Apotential%2520to%2520support%2520a%2520wide%2520range%2520of%2520tasks%2520such%2520as%2520market%2520forecasting%252C%250Aportfolio%2520management%252C%2520quantitative%2520trading%252C%2520and%2520automated%2520analysis.%2520However%252C%250Aexisting%2520platforms%2520remain%2520limited%2520in%2520task%2520coverage%252C%2520lack%2520robust%2520multimodal%2520data%250Aintegration%252C%2520and%2520offer%2520insufficient%2520support%2520for%2520the%2520training%2520and%2520deployment%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520In%2520response%2520to%2520these%2520limitations%252C%2520we%2520present%250AFinWorld%252C%2520an%2520all-in-one%2520open-source%2520platform%2520that%2520provides%2520end-to-end%2520support%250Afor%2520the%2520entire%2520financial%2520AI%2520workflow%252C%2520from%2520data%2520acquisition%2520to%2520experimentation%250Aand%2520deployment.%2520FinWorld%2520distinguishes%2520itself%2520through%2520native%2520integration%2520of%250Aheterogeneous%2520financial%2520data%252C%2520unified%2520support%2520for%2520diverse%2520AI%2520paradigms%252C%2520and%250Aadvanced%2520agent%2520automation%252C%2520enabling%2520seamless%2520development%2520and%2520deployment.%250ALeveraging%2520data%2520from%25202%2520representative%2520markets%252C%25204%2520stock%2520pools%252C%2520and%2520over%2520800%250Amillion%2520financial%2520data%2520points%252C%2520we%2520conduct%2520comprehensive%2520experiments%2520on%25204%2520key%250Afinancial%2520AI%2520tasks.%2520These%2520experiments%2520systematically%2520evaluate%2520deep%2520learning%2520and%250Areinforcement%2520learning%2520algorithms%252C%2520with%2520particular%2520emphasis%2520on%2520RL-based%250Afinetuning%2520for%2520LLMs%2520and%2520LLM%2520Agents.%2520The%2520empirical%2520results%2520demonstrate%2520that%250AFinWorld%2520significantly%2520enhances%2520reproducibility%252C%2520supports%2520transparent%250Abenchmarking%252C%2520and%2520streamlines%2520deployment%252C%2520thereby%2520providing%2520a%2520strong%2520foundation%250Afor%2520future%2520research%2520and%2520real-world%2520applications.%2520Code%2520is%2520available%2520at%250AGithub~%255Cfootnote%257Bhttps%253A//github.com/DVampire/FinWorld%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinWorld%3A%20An%20All-in-One%20Open-Source%20Platform%20for%20End-to-End%20Financial%20AI%0A%20%20Research%20and%20Deployment&entry.906535625=Wentao%20Zhang%20and%20Yilei%20Zhao%20and%20Chuqiao%20Zong%20and%20Xinrun%20Wang%20and%20Bo%20An&entry.1292438233=%20%20Financial%20AI%20holds%20great%20promise%20for%20transforming%20modern%20finance%2C%20with%20the%0Apotential%20to%20support%20a%20wide%20range%20of%20tasks%20such%20as%20market%20forecasting%2C%0Aportfolio%20management%2C%20quantitative%20trading%2C%20and%20automated%20analysis.%20However%2C%0Aexisting%20platforms%20remain%20limited%20in%20task%20coverage%2C%20lack%20robust%20multimodal%20data%0Aintegration%2C%20and%20offer%20insufficient%20support%20for%20the%20training%20and%20deployment%20of%0Alarge%20language%20models%20%28LLMs%29.%20In%20response%20to%20these%20limitations%2C%20we%20present%0AFinWorld%2C%20an%20all-in-one%20open-source%20platform%20that%20provides%20end-to-end%20support%0Afor%20the%20entire%20financial%20AI%20workflow%2C%20from%20data%20acquisition%20to%20experimentation%0Aand%20deployment.%20FinWorld%20distinguishes%20itself%20through%20native%20integration%20of%0Aheterogeneous%20financial%20data%2C%20unified%20support%20for%20diverse%20AI%20paradigms%2C%20and%0Aadvanced%20agent%20automation%2C%20enabling%20seamless%20development%20and%20deployment.%0ALeveraging%20data%20from%202%20representative%20markets%2C%204%20stock%20pools%2C%20and%20over%20800%0Amillion%20financial%20data%20points%2C%20we%20conduct%20comprehensive%20experiments%20on%204%20key%0Afinancial%20AI%20tasks.%20These%20experiments%20systematically%20evaluate%20deep%20learning%20and%0Areinforcement%20learning%20algorithms%2C%20with%20particular%20emphasis%20on%20RL-based%0Afinetuning%20for%20LLMs%20and%20LLM%20Agents.%20The%20empirical%20results%20demonstrate%20that%0AFinWorld%20significantly%20enhances%20reproducibility%2C%20supports%20transparent%0Abenchmarking%2C%20and%20streamlines%20deployment%2C%20thereby%20providing%20a%20strong%20foundation%0Afor%20future%20research%20and%20real-world%20applications.%20Code%20is%20available%20at%0AGithub~%5Cfootnote%7Bhttps%3A//github.com/DVampire/FinWorld%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02292v1&entry.124074799=Read"},
{"title": "DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations", "author": "Sophia Sirko-Galouchenko and Spyros Gidaris and Antonin Vobecky and Andrei Bursuc and Nicolas Thome", "abstract": "  We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP\n", "link": "http://arxiv.org/abs/2506.18463v2", "date": "2025-08-04", "relevancy": 2.3374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations&body=Title%3A%20DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations%0AAuthor%3A%20Sophia%20Sirko-Galouchenko%20and%20Spyros%20Gidaris%20and%20Antonin%20Vobecky%20and%20Andrei%20Bursuc%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20We%20introduce%20DIP%2C%20a%20novel%20unsupervised%20post-training%20method%20designed%20to%0Aenhance%20dense%20image%20representations%20in%20large-scale%20pretrained%20vision%20encoders%0Afor%20in-context%20scene%20understanding.%20Unlike%20prior%20approaches%20that%20rely%20on%0Acomplex%20self-distillation%20architectures%2C%20our%20method%20trains%20the%20vision%20encoder%0Ausing%20pseudo-tasks%20that%20explicitly%20simulate%20downstream%20in-context%20scenarios%2C%0Ainspired%20by%20meta-learning%20principles.%20To%20enable%20post-training%20on%20unlabeled%0Adata%2C%20we%20propose%20an%20automatic%20mechanism%20for%20generating%20in-context%20tasks%20that%0Acombines%20a%20pretrained%20diffusion%20model%20and%20the%20vision%20encoder%20itself.%20DIP%20is%0Asimple%2C%20unsupervised%2C%20and%20computationally%20efficient%2C%20requiring%20less%20than%209%0Ahours%20on%20a%20single%20A100%20GPU.%20By%20learning%20dense%20representations%20through%20pseudo%0Ain-context%20tasks%2C%20it%20achieves%20strong%20performance%20across%20a%20wide%20variety%20of%0Adownstream%20real-world%20in-context%20scene%20understanding%20tasks.%20It%20outperforms%20both%0Athe%20initial%20vision%20encoder%20and%20prior%20methods%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20improving%20dense%20representations.%20Code%20available%20here%3A%0Ahttps%3A//github.com/sirkosophia/DIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIP%253A%2520Unsupervised%2520Dense%2520In-Context%2520Post-training%2520of%2520Visual%250A%2520%2520Representations%26entry.906535625%3DSophia%2520Sirko-Galouchenko%2520and%2520Spyros%2520Gidaris%2520and%2520Antonin%2520Vobecky%2520and%2520Andrei%2520Bursuc%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520We%2520introduce%2520DIP%252C%2520a%2520novel%2520unsupervised%2520post-training%2520method%2520designed%2520to%250Aenhance%2520dense%2520image%2520representations%2520in%2520large-scale%2520pretrained%2520vision%2520encoders%250Afor%2520in-context%2520scene%2520understanding.%2520Unlike%2520prior%2520approaches%2520that%2520rely%2520on%250Acomplex%2520self-distillation%2520architectures%252C%2520our%2520method%2520trains%2520the%2520vision%2520encoder%250Ausing%2520pseudo-tasks%2520that%2520explicitly%2520simulate%2520downstream%2520in-context%2520scenarios%252C%250Ainspired%2520by%2520meta-learning%2520principles.%2520To%2520enable%2520post-training%2520on%2520unlabeled%250Adata%252C%2520we%2520propose%2520an%2520automatic%2520mechanism%2520for%2520generating%2520in-context%2520tasks%2520that%250Acombines%2520a%2520pretrained%2520diffusion%2520model%2520and%2520the%2520vision%2520encoder%2520itself.%2520DIP%2520is%250Asimple%252C%2520unsupervised%252C%2520and%2520computationally%2520efficient%252C%2520requiring%2520less%2520than%25209%250Ahours%2520on%2520a%2520single%2520A100%2520GPU.%2520By%2520learning%2520dense%2520representations%2520through%2520pseudo%250Ain-context%2520tasks%252C%2520it%2520achieves%2520strong%2520performance%2520across%2520a%2520wide%2520variety%2520of%250Adownstream%2520real-world%2520in-context%2520scene%2520understanding%2520tasks.%2520It%2520outperforms%2520both%250Athe%2520initial%2520vision%2520encoder%2520and%2520prior%2520methods%252C%2520offering%2520a%2520practical%2520and%250Aeffective%2520solution%2520for%2520improving%2520dense%2520representations.%2520Code%2520available%2520here%253A%250Ahttps%253A//github.com/sirkosophia/DIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations&entry.906535625=Sophia%20Sirko-Galouchenko%20and%20Spyros%20Gidaris%20and%20Antonin%20Vobecky%20and%20Andrei%20Bursuc%20and%20Nicolas%20Thome&entry.1292438233=%20%20We%20introduce%20DIP%2C%20a%20novel%20unsupervised%20post-training%20method%20designed%20to%0Aenhance%20dense%20image%20representations%20in%20large-scale%20pretrained%20vision%20encoders%0Afor%20in-context%20scene%20understanding.%20Unlike%20prior%20approaches%20that%20rely%20on%0Acomplex%20self-distillation%20architectures%2C%20our%20method%20trains%20the%20vision%20encoder%0Ausing%20pseudo-tasks%20that%20explicitly%20simulate%20downstream%20in-context%20scenarios%2C%0Ainspired%20by%20meta-learning%20principles.%20To%20enable%20post-training%20on%20unlabeled%0Adata%2C%20we%20propose%20an%20automatic%20mechanism%20for%20generating%20in-context%20tasks%20that%0Acombines%20a%20pretrained%20diffusion%20model%20and%20the%20vision%20encoder%20itself.%20DIP%20is%0Asimple%2C%20unsupervised%2C%20and%20computationally%20efficient%2C%20requiring%20less%20than%209%0Ahours%20on%20a%20single%20A100%20GPU.%20By%20learning%20dense%20representations%20through%20pseudo%0Ain-context%20tasks%2C%20it%20achieves%20strong%20performance%20across%20a%20wide%20variety%20of%0Adownstream%20real-world%20in-context%20scene%20understanding%20tasks.%20It%20outperforms%20both%0Athe%20initial%20vision%20encoder%20and%20prior%20methods%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20improving%20dense%20representations.%20Code%20available%20here%3A%0Ahttps%3A//github.com/sirkosophia/DIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18463v2&entry.124074799=Read"},
{"title": "Instance-Optimal Uniformity Testing and Tracking", "author": "Guy Blanc and Cl\u00e9ment L. Canonne and Erik Waingarten", "abstract": "  In the uniformity testing task, an algorithm is provided with samples from an\nunknown probability distribution over a (known) finite domain, and must decide\nwhether it is the uniform distribution, or, alternatively, if its total\nvariation distance from uniform exceeds some input distance parameter. This\nquestion has received a significant amount of interest and its complexity is,\nby now, fully settled. Yet, we argue that it fails to capture many scenarios of\ninterest, and that its very definition as a gap problem in terms of a\nprespecified distance may lead to suboptimal performance.\n  To address these shortcomings, we introduce the problem of uniformity\ntracking, whereby an algorithm is required to detect deviations from uniformity\n(however they may manifest themselves) using as few samples as possible, and be\ncompetitive against an optimal algorithm knowing the distribution profile in\nhindsight. Our main contribution is a\n$\\operatorname{polylog}(\\operatorname{opt})$-competitive uniformity tracking\nalgorithm. We obtain this result by leveraging new structural results on\nPoisson mixtures, which we believe to be of independent interest.\n", "link": "http://arxiv.org/abs/2508.02637v1", "date": "2025-08-04", "relevancy": 2.3223, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4668}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4654}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Optimal%20Uniformity%20Testing%20and%20Tracking&body=Title%3A%20Instance-Optimal%20Uniformity%20Testing%20and%20Tracking%0AAuthor%3A%20Guy%20Blanc%20and%20Cl%C3%A9ment%20L.%20Canonne%20and%20Erik%20Waingarten%0AAbstract%3A%20%20%20In%20the%20uniformity%20testing%20task%2C%20an%20algorithm%20is%20provided%20with%20samples%20from%20an%0Aunknown%20probability%20distribution%20over%20a%20%28known%29%20finite%20domain%2C%20and%20must%20decide%0Awhether%20it%20is%20the%20uniform%20distribution%2C%20or%2C%20alternatively%2C%20if%20its%20total%0Avariation%20distance%20from%20uniform%20exceeds%20some%20input%20distance%20parameter.%20This%0Aquestion%20has%20received%20a%20significant%20amount%20of%20interest%20and%20its%20complexity%20is%2C%0Aby%20now%2C%20fully%20settled.%20Yet%2C%20we%20argue%20that%20it%20fails%20to%20capture%20many%20scenarios%20of%0Ainterest%2C%20and%20that%20its%20very%20definition%20as%20a%20gap%20problem%20in%20terms%20of%20a%0Aprespecified%20distance%20may%20lead%20to%20suboptimal%20performance.%0A%20%20To%20address%20these%20shortcomings%2C%20we%20introduce%20the%20problem%20of%20uniformity%0Atracking%2C%20whereby%20an%20algorithm%20is%20required%20to%20detect%20deviations%20from%20uniformity%0A%28however%20they%20may%20manifest%20themselves%29%20using%20as%20few%20samples%20as%20possible%2C%20and%20be%0Acompetitive%20against%20an%20optimal%20algorithm%20knowing%20the%20distribution%20profile%20in%0Ahindsight.%20Our%20main%20contribution%20is%20a%0A%24%5Coperatorname%7Bpolylog%7D%28%5Coperatorname%7Bopt%7D%29%24-competitive%20uniformity%20tracking%0Aalgorithm.%20We%20obtain%20this%20result%20by%20leveraging%20new%20structural%20results%20on%0APoisson%20mixtures%2C%20which%20we%20believe%20to%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Optimal%2520Uniformity%2520Testing%2520and%2520Tracking%26entry.906535625%3DGuy%2520Blanc%2520and%2520Cl%25C3%25A9ment%2520L.%2520Canonne%2520and%2520Erik%2520Waingarten%26entry.1292438233%3D%2520%2520In%2520the%2520uniformity%2520testing%2520task%252C%2520an%2520algorithm%2520is%2520provided%2520with%2520samples%2520from%2520an%250Aunknown%2520probability%2520distribution%2520over%2520a%2520%2528known%2529%2520finite%2520domain%252C%2520and%2520must%2520decide%250Awhether%2520it%2520is%2520the%2520uniform%2520distribution%252C%2520or%252C%2520alternatively%252C%2520if%2520its%2520total%250Avariation%2520distance%2520from%2520uniform%2520exceeds%2520some%2520input%2520distance%2520parameter.%2520This%250Aquestion%2520has%2520received%2520a%2520significant%2520amount%2520of%2520interest%2520and%2520its%2520complexity%2520is%252C%250Aby%2520now%252C%2520fully%2520settled.%2520Yet%252C%2520we%2520argue%2520that%2520it%2520fails%2520to%2520capture%2520many%2520scenarios%2520of%250Ainterest%252C%2520and%2520that%2520its%2520very%2520definition%2520as%2520a%2520gap%2520problem%2520in%2520terms%2520of%2520a%250Aprespecified%2520distance%2520may%2520lead%2520to%2520suboptimal%2520performance.%250A%2520%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520introduce%2520the%2520problem%2520of%2520uniformity%250Atracking%252C%2520whereby%2520an%2520algorithm%2520is%2520required%2520to%2520detect%2520deviations%2520from%2520uniformity%250A%2528however%2520they%2520may%2520manifest%2520themselves%2529%2520using%2520as%2520few%2520samples%2520as%2520possible%252C%2520and%2520be%250Acompetitive%2520against%2520an%2520optimal%2520algorithm%2520knowing%2520the%2520distribution%2520profile%2520in%250Ahindsight.%2520Our%2520main%2520contribution%2520is%2520a%250A%2524%255Coperatorname%257Bpolylog%257D%2528%255Coperatorname%257Bopt%257D%2529%2524-competitive%2520uniformity%2520tracking%250Aalgorithm.%2520We%2520obtain%2520this%2520result%2520by%2520leveraging%2520new%2520structural%2520results%2520on%250APoisson%2520mixtures%252C%2520which%2520we%2520believe%2520to%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Optimal%20Uniformity%20Testing%20and%20Tracking&entry.906535625=Guy%20Blanc%20and%20Cl%C3%A9ment%20L.%20Canonne%20and%20Erik%20Waingarten&entry.1292438233=%20%20In%20the%20uniformity%20testing%20task%2C%20an%20algorithm%20is%20provided%20with%20samples%20from%20an%0Aunknown%20probability%20distribution%20over%20a%20%28known%29%20finite%20domain%2C%20and%20must%20decide%0Awhether%20it%20is%20the%20uniform%20distribution%2C%20or%2C%20alternatively%2C%20if%20its%20total%0Avariation%20distance%20from%20uniform%20exceeds%20some%20input%20distance%20parameter.%20This%0Aquestion%20has%20received%20a%20significant%20amount%20of%20interest%20and%20its%20complexity%20is%2C%0Aby%20now%2C%20fully%20settled.%20Yet%2C%20we%20argue%20that%20it%20fails%20to%20capture%20many%20scenarios%20of%0Ainterest%2C%20and%20that%20its%20very%20definition%20as%20a%20gap%20problem%20in%20terms%20of%20a%0Aprespecified%20distance%20may%20lead%20to%20suboptimal%20performance.%0A%20%20To%20address%20these%20shortcomings%2C%20we%20introduce%20the%20problem%20of%20uniformity%0Atracking%2C%20whereby%20an%20algorithm%20is%20required%20to%20detect%20deviations%20from%20uniformity%0A%28however%20they%20may%20manifest%20themselves%29%20using%20as%20few%20samples%20as%20possible%2C%20and%20be%0Acompetitive%20against%20an%20optimal%20algorithm%20knowing%20the%20distribution%20profile%20in%0Ahindsight.%20Our%20main%20contribution%20is%20a%0A%24%5Coperatorname%7Bpolylog%7D%28%5Coperatorname%7Bopt%7D%29%24-competitive%20uniformity%20tracking%0Aalgorithm.%20We%20obtain%20this%20result%20by%20leveraging%20new%20structural%20results%20on%0APoisson%20mixtures%2C%20which%20we%20believe%20to%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02637v1&entry.124074799=Read"},
{"title": "UDC-VIT: A Real-World Video Dataset for Under-Display Cameras", "author": "Kyusu Ahn and JiSoo Kim and Sangik Lee and HyunGyu Lee and Byeonghyun Ko and Chanwoo Park and Jaejin Lee", "abstract": "  Even though an Under-Display Camera (UDC) is an advanced imaging system, the\ndisplay panel significantly degrades captured images or videos, introducing low\ntransmittance, blur, noise, and flare issues. Tackling such issues is\nchallenging because of the complex degradation of UDCs, including diverse flare\npatterns. However, no dataset contains videos of real-world UDC degradation. In\nthis paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike\nexisting datasets, UDC-VIT exclusively includes human motions for facial\nrecognition. We propose a video-capturing system to acquire clean and\nUDC-degraded videos of the same scene simultaneously. Then, we align a pair of\ncaptured videos frame by frame, using discrete Fourier transform (DFT). We\ncompare UDC-VIT with six representative UDC still image datasets and two\nexisting UDC video datasets. Using six deep-learning models, we compare UDC-VIT\nand an existing synthetic UDC video dataset. The results indicate the\nineffectiveness of models trained on earlier synthetic UDC video datasets, as\nthey do not reflect the actual characteristics of UDC-degraded videos. We also\ndemonstrate the importance of effective UDC restoration by evaluating face\nrecognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is\navailable at our official GitHub repository.\n", "link": "http://arxiv.org/abs/2501.18545v2", "date": "2025-08-04", "relevancy": 2.3185, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5829}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5809}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UDC-VIT%3A%20A%20Real-World%20Video%20Dataset%20for%20Under-Display%20Cameras&body=Title%3A%20UDC-VIT%3A%20A%20Real-World%20Video%20Dataset%20for%20Under-Display%20Cameras%0AAuthor%3A%20Kyusu%20Ahn%20and%20JiSoo%20Kim%20and%20Sangik%20Lee%20and%20HyunGyu%20Lee%20and%20Byeonghyun%20Ko%20and%20Chanwoo%20Park%20and%20Jaejin%20Lee%0AAbstract%3A%20%20%20Even%20though%20an%20Under-Display%20Camera%20%28UDC%29%20is%20an%20advanced%20imaging%20system%2C%20the%0Adisplay%20panel%20significantly%20degrades%20captured%20images%20or%20videos%2C%20introducing%20low%0Atransmittance%2C%20blur%2C%20noise%2C%20and%20flare%20issues.%20Tackling%20such%20issues%20is%0Achallenging%20because%20of%20the%20complex%20degradation%20of%20UDCs%2C%20including%20diverse%20flare%0Apatterns.%20However%2C%20no%20dataset%20contains%20videos%20of%20real-world%20UDC%20degradation.%20In%0Athis%20paper%2C%20we%20propose%20a%20real-world%20UDC%20video%20dataset%20called%20UDC-VIT.%20Unlike%0Aexisting%20datasets%2C%20UDC-VIT%20exclusively%20includes%20human%20motions%20for%20facial%0Arecognition.%20We%20propose%20a%20video-capturing%20system%20to%20acquire%20clean%20and%0AUDC-degraded%20videos%20of%20the%20same%20scene%20simultaneously.%20Then%2C%20we%20align%20a%20pair%20of%0Acaptured%20videos%20frame%20by%20frame%2C%20using%20discrete%20Fourier%20transform%20%28DFT%29.%20We%0Acompare%20UDC-VIT%20with%20six%20representative%20UDC%20still%20image%20datasets%20and%20two%0Aexisting%20UDC%20video%20datasets.%20Using%20six%20deep-learning%20models%2C%20we%20compare%20UDC-VIT%0Aand%20an%20existing%20synthetic%20UDC%20video%20dataset.%20The%20results%20indicate%20the%0Aineffectiveness%20of%20models%20trained%20on%20earlier%20synthetic%20UDC%20video%20datasets%2C%20as%0Athey%20do%20not%20reflect%20the%20actual%20characteristics%20of%20UDC-degraded%20videos.%20We%20also%0Ademonstrate%20the%20importance%20of%20effective%20UDC%20restoration%20by%20evaluating%20face%0Arecognition%20accuracy%20concerning%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20scores.%20UDC-VIT%20is%0Aavailable%20at%20our%20official%20GitHub%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUDC-VIT%253A%2520A%2520Real-World%2520Video%2520Dataset%2520for%2520Under-Display%2520Cameras%26entry.906535625%3DKyusu%2520Ahn%2520and%2520JiSoo%2520Kim%2520and%2520Sangik%2520Lee%2520and%2520HyunGyu%2520Lee%2520and%2520Byeonghyun%2520Ko%2520and%2520Chanwoo%2520Park%2520and%2520Jaejin%2520Lee%26entry.1292438233%3D%2520%2520Even%2520though%2520an%2520Under-Display%2520Camera%2520%2528UDC%2529%2520is%2520an%2520advanced%2520imaging%2520system%252C%2520the%250Adisplay%2520panel%2520significantly%2520degrades%2520captured%2520images%2520or%2520videos%252C%2520introducing%2520low%250Atransmittance%252C%2520blur%252C%2520noise%252C%2520and%2520flare%2520issues.%2520Tackling%2520such%2520issues%2520is%250Achallenging%2520because%2520of%2520the%2520complex%2520degradation%2520of%2520UDCs%252C%2520including%2520diverse%2520flare%250Apatterns.%2520However%252C%2520no%2520dataset%2520contains%2520videos%2520of%2520real-world%2520UDC%2520degradation.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520real-world%2520UDC%2520video%2520dataset%2520called%2520UDC-VIT.%2520Unlike%250Aexisting%2520datasets%252C%2520UDC-VIT%2520exclusively%2520includes%2520human%2520motions%2520for%2520facial%250Arecognition.%2520We%2520propose%2520a%2520video-capturing%2520system%2520to%2520acquire%2520clean%2520and%250AUDC-degraded%2520videos%2520of%2520the%2520same%2520scene%2520simultaneously.%2520Then%252C%2520we%2520align%2520a%2520pair%2520of%250Acaptured%2520videos%2520frame%2520by%2520frame%252C%2520using%2520discrete%2520Fourier%2520transform%2520%2528DFT%2529.%2520We%250Acompare%2520UDC-VIT%2520with%2520six%2520representative%2520UDC%2520still%2520image%2520datasets%2520and%2520two%250Aexisting%2520UDC%2520video%2520datasets.%2520Using%2520six%2520deep-learning%2520models%252C%2520we%2520compare%2520UDC-VIT%250Aand%2520an%2520existing%2520synthetic%2520UDC%2520video%2520dataset.%2520The%2520results%2520indicate%2520the%250Aineffectiveness%2520of%2520models%2520trained%2520on%2520earlier%2520synthetic%2520UDC%2520video%2520datasets%252C%2520as%250Athey%2520do%2520not%2520reflect%2520the%2520actual%2520characteristics%2520of%2520UDC-degraded%2520videos.%2520We%2520also%250Ademonstrate%2520the%2520importance%2520of%2520effective%2520UDC%2520restoration%2520by%2520evaluating%2520face%250Arecognition%2520accuracy%2520concerning%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520scores.%2520UDC-VIT%2520is%250Aavailable%2520at%2520our%2520official%2520GitHub%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UDC-VIT%3A%20A%20Real-World%20Video%20Dataset%20for%20Under-Display%20Cameras&entry.906535625=Kyusu%20Ahn%20and%20JiSoo%20Kim%20and%20Sangik%20Lee%20and%20HyunGyu%20Lee%20and%20Byeonghyun%20Ko%20and%20Chanwoo%20Park%20and%20Jaejin%20Lee&entry.1292438233=%20%20Even%20though%20an%20Under-Display%20Camera%20%28UDC%29%20is%20an%20advanced%20imaging%20system%2C%20the%0Adisplay%20panel%20significantly%20degrades%20captured%20images%20or%20videos%2C%20introducing%20low%0Atransmittance%2C%20blur%2C%20noise%2C%20and%20flare%20issues.%20Tackling%20such%20issues%20is%0Achallenging%20because%20of%20the%20complex%20degradation%20of%20UDCs%2C%20including%20diverse%20flare%0Apatterns.%20However%2C%20no%20dataset%20contains%20videos%20of%20real-world%20UDC%20degradation.%20In%0Athis%20paper%2C%20we%20propose%20a%20real-world%20UDC%20video%20dataset%20called%20UDC-VIT.%20Unlike%0Aexisting%20datasets%2C%20UDC-VIT%20exclusively%20includes%20human%20motions%20for%20facial%0Arecognition.%20We%20propose%20a%20video-capturing%20system%20to%20acquire%20clean%20and%0AUDC-degraded%20videos%20of%20the%20same%20scene%20simultaneously.%20Then%2C%20we%20align%20a%20pair%20of%0Acaptured%20videos%20frame%20by%20frame%2C%20using%20discrete%20Fourier%20transform%20%28DFT%29.%20We%0Acompare%20UDC-VIT%20with%20six%20representative%20UDC%20still%20image%20datasets%20and%20two%0Aexisting%20UDC%20video%20datasets.%20Using%20six%20deep-learning%20models%2C%20we%20compare%20UDC-VIT%0Aand%20an%20existing%20synthetic%20UDC%20video%20dataset.%20The%20results%20indicate%20the%0Aineffectiveness%20of%20models%20trained%20on%20earlier%20synthetic%20UDC%20video%20datasets%2C%20as%0Athey%20do%20not%20reflect%20the%20actual%20characteristics%20of%20UDC-degraded%20videos.%20We%20also%0Ademonstrate%20the%20importance%20of%20effective%20UDC%20restoration%20by%20evaluating%20face%0Arecognition%20accuracy%20concerning%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20scores.%20UDC-VIT%20is%0Aavailable%20at%20our%20official%20GitHub%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18545v2&entry.124074799=Read"},
{"title": "Globally Optimal Data-Association-Free Landmark-Based Localization Using\n  Semidefinite Relaxations", "author": "Vassili Korotkine and Mitchell Cohen and James Richard Forbes", "abstract": "  This paper proposes a semidefinite relaxation for landmark-based localization\nwith unknown data associations in planar environments. The proposed method\nsimultaneously solves for the optimal robot states and data associations in a\nglobally optimal fashion. Relative position measurements to known landmarks are\nused, but the data association is unknown in tha tthe robot does not know which\nlandmark each measurement is generated from. The relaxation is shown to be\ntight in a majority of cases for moderate noise levels. The proposed algorithm\nis compared to local Gauss-Newton baselines initialized at the dead-reckoned\ntrajectory, and is shown to significantly improve convergence to the problem's\nglobal optimum in simulation and experiment. Accompanying software and\nsupplementary material may be found at\nhttps://github.com/decargroup/certifiable_uda_loc .\n", "link": "http://arxiv.org/abs/2504.08547v2", "date": "2025-08-04", "relevancy": 2.3168, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6159}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Globally%20Optimal%20Data-Association-Free%20Landmark-Based%20Localization%20Using%0A%20%20Semidefinite%20Relaxations&body=Title%3A%20Globally%20Optimal%20Data-Association-Free%20Landmark-Based%20Localization%20Using%0A%20%20Semidefinite%20Relaxations%0AAuthor%3A%20Vassili%20Korotkine%20and%20Mitchell%20Cohen%20and%20James%20Richard%20Forbes%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20semidefinite%20relaxation%20for%20landmark-based%20localization%0Awith%20unknown%20data%20associations%20in%20planar%20environments.%20The%20proposed%20method%0Asimultaneously%20solves%20for%20the%20optimal%20robot%20states%20and%20data%20associations%20in%20a%0Aglobally%20optimal%20fashion.%20Relative%20position%20measurements%20to%20known%20landmarks%20are%0Aused%2C%20but%20the%20data%20association%20is%20unknown%20in%20tha%20tthe%20robot%20does%20not%20know%20which%0Alandmark%20each%20measurement%20is%20generated%20from.%20The%20relaxation%20is%20shown%20to%20be%0Atight%20in%20a%20majority%20of%20cases%20for%20moderate%20noise%20levels.%20The%20proposed%20algorithm%0Ais%20compared%20to%20local%20Gauss-Newton%20baselines%20initialized%20at%20the%20dead-reckoned%0Atrajectory%2C%20and%20is%20shown%20to%20significantly%20improve%20convergence%20to%20the%20problem%27s%0Aglobal%20optimum%20in%20simulation%20and%20experiment.%20Accompanying%20software%20and%0Asupplementary%20material%20may%20be%20found%20at%0Ahttps%3A//github.com/decargroup/certifiable_uda_loc%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobally%2520Optimal%2520Data-Association-Free%2520Landmark-Based%2520Localization%2520Using%250A%2520%2520Semidefinite%2520Relaxations%26entry.906535625%3DVassili%2520Korotkine%2520and%2520Mitchell%2520Cohen%2520and%2520James%2520Richard%2520Forbes%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520semidefinite%2520relaxation%2520for%2520landmark-based%2520localization%250Awith%2520unknown%2520data%2520associations%2520in%2520planar%2520environments.%2520The%2520proposed%2520method%250Asimultaneously%2520solves%2520for%2520the%2520optimal%2520robot%2520states%2520and%2520data%2520associations%2520in%2520a%250Aglobally%2520optimal%2520fashion.%2520Relative%2520position%2520measurements%2520to%2520known%2520landmarks%2520are%250Aused%252C%2520but%2520the%2520data%2520association%2520is%2520unknown%2520in%2520tha%2520tthe%2520robot%2520does%2520not%2520know%2520which%250Alandmark%2520each%2520measurement%2520is%2520generated%2520from.%2520The%2520relaxation%2520is%2520shown%2520to%2520be%250Atight%2520in%2520a%2520majority%2520of%2520cases%2520for%2520moderate%2520noise%2520levels.%2520The%2520proposed%2520algorithm%250Ais%2520compared%2520to%2520local%2520Gauss-Newton%2520baselines%2520initialized%2520at%2520the%2520dead-reckoned%250Atrajectory%252C%2520and%2520is%2520shown%2520to%2520significantly%2520improve%2520convergence%2520to%2520the%2520problem%2527s%250Aglobal%2520optimum%2520in%2520simulation%2520and%2520experiment.%2520Accompanying%2520software%2520and%250Asupplementary%2520material%2520may%2520be%2520found%2520at%250Ahttps%253A//github.com/decargroup/certifiable_uda_loc%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Globally%20Optimal%20Data-Association-Free%20Landmark-Based%20Localization%20Using%0A%20%20Semidefinite%20Relaxations&entry.906535625=Vassili%20Korotkine%20and%20Mitchell%20Cohen%20and%20James%20Richard%20Forbes&entry.1292438233=%20%20This%20paper%20proposes%20a%20semidefinite%20relaxation%20for%20landmark-based%20localization%0Awith%20unknown%20data%20associations%20in%20planar%20environments.%20The%20proposed%20method%0Asimultaneously%20solves%20for%20the%20optimal%20robot%20states%20and%20data%20associations%20in%20a%0Aglobally%20optimal%20fashion.%20Relative%20position%20measurements%20to%20known%20landmarks%20are%0Aused%2C%20but%20the%20data%20association%20is%20unknown%20in%20tha%20tthe%20robot%20does%20not%20know%20which%0Alandmark%20each%20measurement%20is%20generated%20from.%20The%20relaxation%20is%20shown%20to%20be%0Atight%20in%20a%20majority%20of%20cases%20for%20moderate%20noise%20levels.%20The%20proposed%20algorithm%0Ais%20compared%20to%20local%20Gauss-Newton%20baselines%20initialized%20at%20the%20dead-reckoned%0Atrajectory%2C%20and%20is%20shown%20to%20significantly%20improve%20convergence%20to%20the%20problem%27s%0Aglobal%20optimum%20in%20simulation%20and%20experiment.%20Accompanying%20software%20and%0Asupplementary%20material%20may%20be%20found%20at%0Ahttps%3A//github.com/decargroup/certifiable_uda_loc%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08547v2&entry.124074799=Read"},
{"title": "Unleashing the Temporal Potential of Stereo Event Cameras for\n  Continuous-Time 3D Object Detection", "author": "Jae-Young Kang and Hoonhee Cho and Kuk-Jin Yoon", "abstract": "  3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.\n", "link": "http://arxiv.org/abs/2508.02288v1", "date": "2025-08-04", "relevancy": 2.3127, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Temporal%20Potential%20of%20Stereo%20Event%20Cameras%20for%0A%20%20Continuous-Time%203D%20Object%20Detection&body=Title%3A%20Unleashing%20the%20Temporal%20Potential%20of%20Stereo%20Event%20Cameras%20for%0A%20%20Continuous-Time%203D%20Object%20Detection%0AAuthor%3A%20Jae-Young%20Kang%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%203D%20object%20detection%20is%20essential%20for%20autonomous%20systems%2C%20enabling%20precise%0Alocalization%20and%20dimension%20estimation.%20While%20LiDAR%20and%20RGB%20cameras%20are%20widely%0Aused%2C%20their%20fixed%20frame%20rates%20create%20perception%20gaps%20in%20high-speed%20scenarios.%0AEvent%20cameras%2C%20with%20their%20asynchronous%20nature%20and%20high%20temporal%20resolution%2C%0Aoffer%20a%20solution%20by%20capturing%20motion%20continuously.%20The%20recent%20approach%2C%20which%0Aintegrates%20event%20cameras%20with%20conventional%20sensors%20for%20continuous-time%0Adetection%2C%20struggles%20in%20fast-motion%20scenarios%20due%20to%20its%20dependency%20on%0Asynchronized%20sensors.%20We%20propose%20a%20novel%20stereo%203D%20object%20detection%20framework%0Athat%20relies%20solely%20on%20event%20cameras%2C%20eliminating%20the%20need%20for%20conventional%203D%0Asensors.%20To%20compensate%20for%20the%20lack%20of%20semantic%20and%20geometric%20information%20in%0Aevent%20data%2C%20we%20introduce%20a%20dual%20filter%20mechanism%20that%20extracts%20both.%0AAdditionally%2C%20we%20enhance%20regression%20by%20aligning%20bounding%20boxes%20with%0Aobject-centric%20information.%20Experiments%20show%20that%20our%20method%20outperforms%20prior%0Aapproaches%20in%20dynamic%20environments%2C%20demonstrating%20the%20potential%20of%20event%0Acameras%20for%20robust%2C%20continuous-time%203D%20perception.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mickeykang16/Ev-Stereo3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Temporal%2520Potential%2520of%2520Stereo%2520Event%2520Cameras%2520for%250A%2520%2520Continuous-Time%25203D%2520Object%2520Detection%26entry.906535625%3DJae-Young%2520Kang%2520and%2520Hoonhee%2520Cho%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520is%2520essential%2520for%2520autonomous%2520systems%252C%2520enabling%2520precise%250Alocalization%2520and%2520dimension%2520estimation.%2520While%2520LiDAR%2520and%2520RGB%2520cameras%2520are%2520widely%250Aused%252C%2520their%2520fixed%2520frame%2520rates%2520create%2520perception%2520gaps%2520in%2520high-speed%2520scenarios.%250AEvent%2520cameras%252C%2520with%2520their%2520asynchronous%2520nature%2520and%2520high%2520temporal%2520resolution%252C%250Aoffer%2520a%2520solution%2520by%2520capturing%2520motion%2520continuously.%2520The%2520recent%2520approach%252C%2520which%250Aintegrates%2520event%2520cameras%2520with%2520conventional%2520sensors%2520for%2520continuous-time%250Adetection%252C%2520struggles%2520in%2520fast-motion%2520scenarios%2520due%2520to%2520its%2520dependency%2520on%250Asynchronized%2520sensors.%2520We%2520propose%2520a%2520novel%2520stereo%25203D%2520object%2520detection%2520framework%250Athat%2520relies%2520solely%2520on%2520event%2520cameras%252C%2520eliminating%2520the%2520need%2520for%2520conventional%25203D%250Asensors.%2520To%2520compensate%2520for%2520the%2520lack%2520of%2520semantic%2520and%2520geometric%2520information%2520in%250Aevent%2520data%252C%2520we%2520introduce%2520a%2520dual%2520filter%2520mechanism%2520that%2520extracts%2520both.%250AAdditionally%252C%2520we%2520enhance%2520regression%2520by%2520aligning%2520bounding%2520boxes%2520with%250Aobject-centric%2520information.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%2520prior%250Aapproaches%2520in%2520dynamic%2520environments%252C%2520demonstrating%2520the%2520potential%2520of%2520event%250Acameras%2520for%2520robust%252C%2520continuous-time%25203D%2520perception.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mickeykang16/Ev-Stereo3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Temporal%20Potential%20of%20Stereo%20Event%20Cameras%20for%0A%20%20Continuous-Time%203D%20Object%20Detection&entry.906535625=Jae-Young%20Kang%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%203D%20object%20detection%20is%20essential%20for%20autonomous%20systems%2C%20enabling%20precise%0Alocalization%20and%20dimension%20estimation.%20While%20LiDAR%20and%20RGB%20cameras%20are%20widely%0Aused%2C%20their%20fixed%20frame%20rates%20create%20perception%20gaps%20in%20high-speed%20scenarios.%0AEvent%20cameras%2C%20with%20their%20asynchronous%20nature%20and%20high%20temporal%20resolution%2C%0Aoffer%20a%20solution%20by%20capturing%20motion%20continuously.%20The%20recent%20approach%2C%20which%0Aintegrates%20event%20cameras%20with%20conventional%20sensors%20for%20continuous-time%0Adetection%2C%20struggles%20in%20fast-motion%20scenarios%20due%20to%20its%20dependency%20on%0Asynchronized%20sensors.%20We%20propose%20a%20novel%20stereo%203D%20object%20detection%20framework%0Athat%20relies%20solely%20on%20event%20cameras%2C%20eliminating%20the%20need%20for%20conventional%203D%0Asensors.%20To%20compensate%20for%20the%20lack%20of%20semantic%20and%20geometric%20information%20in%0Aevent%20data%2C%20we%20introduce%20a%20dual%20filter%20mechanism%20that%20extracts%20both.%0AAdditionally%2C%20we%20enhance%20regression%20by%20aligning%20bounding%20boxes%20with%0Aobject-centric%20information.%20Experiments%20show%20that%20our%20method%20outperforms%20prior%0Aapproaches%20in%20dynamic%20environments%2C%20demonstrating%20the%20potential%20of%20event%0Acameras%20for%20robust%2C%20continuous-time%203D%20perception.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mickeykang16/Ev-Stereo3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02288v1&entry.124074799=Read"},
{"title": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment", "author": "Amir Aghdam and Vincent Tao Hu and Bj\u00f6rn Ommer", "abstract": "  We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.\n", "link": "http://arxiv.org/abs/2506.22967v2", "date": "2025-08-04", "relevancy": 2.2997, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5867}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActAlign%3A%20Zero-Shot%20Fine-Grained%20Video%20Classification%20via%0A%20%20Language-Guided%20Sequence%20Alignment&body=Title%3A%20ActAlign%3A%20Zero-Shot%20Fine-Grained%20Video%20Classification%20via%0A%20%20Language-Guided%20Sequence%20Alignment%0AAuthor%3A%20Amir%20Aghdam%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20We%20address%20the%20task%20of%20zero-shot%20video%20classification%20for%20extremely%0Afine-grained%20actions%20%28e.g.%2C%20Windmill%20Dunk%20in%20basketball%29%2C%20where%20no%20video%0Aexamples%20or%20temporal%20annotations%20are%20available%20for%20unseen%20classes.%20While%0Aimage-language%20models%20%28e.g.%2C%20CLIP%2C%20SigLIP%29%20show%20strong%20open-set%20recognition%2C%0Athey%20lack%20temporal%20modeling%20needed%20for%20video%20understanding.%20We%20propose%0AActAlign%2C%20a%20truly%20zero-shot%2C%20training-free%20method%20that%20formulates%20video%0Aclassification%20as%20a%20sequence%20alignment%20problem%2C%20preserving%20the%20generalization%0Astrength%20of%20pretrained%20image-language%20models.%20For%20each%20class%2C%20a%20large%20language%0Amodel%20%28LLM%29%20generates%20an%20ordered%20sequence%20of%20sub-actions%2C%20which%20we%20align%20with%0Avideo%20frames%20using%20Dynamic%20Time%20Warping%20%28DTW%29%20in%20a%20shared%20embedding%20space.%0AWithout%20any%20video-text%20supervision%20or%20fine-tuning%2C%20ActAlign%20achieves%2030.5%25%0Aaccuracy%20on%20ActionAtlas--the%20most%20diverse%20benchmark%20of%20fine-grained%20actions%0Aacross%20multiple%20sports--where%20human%20performance%20is%20only%2061.6%25.%20ActAlign%0Aoutperforms%20billion-parameter%20video-language%20models%20while%20using%208x%20fewer%0Aparameters.%20Our%20approach%20is%20model-agnostic%20and%20domain-general%2C%20demonstrating%0Athat%20structured%20language%20priors%20combined%20with%20classical%20alignment%20methods%20can%0Aunlock%20the%20open-set%20recognition%20potential%20of%20image-language%20models%20for%0Afine-grained%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActAlign%253A%2520Zero-Shot%2520Fine-Grained%2520Video%2520Classification%2520via%250A%2520%2520Language-Guided%2520Sequence%2520Alignment%26entry.906535625%3DAmir%2520Aghdam%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520task%2520of%2520zero-shot%2520video%2520classification%2520for%2520extremely%250Afine-grained%2520actions%2520%2528e.g.%252C%2520Windmill%2520Dunk%2520in%2520basketball%2529%252C%2520where%2520no%2520video%250Aexamples%2520or%2520temporal%2520annotations%2520are%2520available%2520for%2520unseen%2520classes.%2520While%250Aimage-language%2520models%2520%2528e.g.%252C%2520CLIP%252C%2520SigLIP%2529%2520show%2520strong%2520open-set%2520recognition%252C%250Athey%2520lack%2520temporal%2520modeling%2520needed%2520for%2520video%2520understanding.%2520We%2520propose%250AActAlign%252C%2520a%2520truly%2520zero-shot%252C%2520training-free%2520method%2520that%2520formulates%2520video%250Aclassification%2520as%2520a%2520sequence%2520alignment%2520problem%252C%2520preserving%2520the%2520generalization%250Astrength%2520of%2520pretrained%2520image-language%2520models.%2520For%2520each%2520class%252C%2520a%2520large%2520language%250Amodel%2520%2528LLM%2529%2520generates%2520an%2520ordered%2520sequence%2520of%2520sub-actions%252C%2520which%2520we%2520align%2520with%250Avideo%2520frames%2520using%2520Dynamic%2520Time%2520Warping%2520%2528DTW%2529%2520in%2520a%2520shared%2520embedding%2520space.%250AWithout%2520any%2520video-text%2520supervision%2520or%2520fine-tuning%252C%2520ActAlign%2520achieves%252030.5%2525%250Aaccuracy%2520on%2520ActionAtlas--the%2520most%2520diverse%2520benchmark%2520of%2520fine-grained%2520actions%250Aacross%2520multiple%2520sports--where%2520human%2520performance%2520is%2520only%252061.6%2525.%2520ActAlign%250Aoutperforms%2520billion-parameter%2520video-language%2520models%2520while%2520using%25208x%2520fewer%250Aparameters.%2520Our%2520approach%2520is%2520model-agnostic%2520and%2520domain-general%252C%2520demonstrating%250Athat%2520structured%2520language%2520priors%2520combined%2520with%2520classical%2520alignment%2520methods%2520can%250Aunlock%2520the%2520open-set%2520recognition%2520potential%2520of%2520image-language%2520models%2520for%250Afine-grained%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActAlign%3A%20Zero-Shot%20Fine-Grained%20Video%20Classification%20via%0A%20%20Language-Guided%20Sequence%20Alignment&entry.906535625=Amir%20Aghdam%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20We%20address%20the%20task%20of%20zero-shot%20video%20classification%20for%20extremely%0Afine-grained%20actions%20%28e.g.%2C%20Windmill%20Dunk%20in%20basketball%29%2C%20where%20no%20video%0Aexamples%20or%20temporal%20annotations%20are%20available%20for%20unseen%20classes.%20While%0Aimage-language%20models%20%28e.g.%2C%20CLIP%2C%20SigLIP%29%20show%20strong%20open-set%20recognition%2C%0Athey%20lack%20temporal%20modeling%20needed%20for%20video%20understanding.%20We%20propose%0AActAlign%2C%20a%20truly%20zero-shot%2C%20training-free%20method%20that%20formulates%20video%0Aclassification%20as%20a%20sequence%20alignment%20problem%2C%20preserving%20the%20generalization%0Astrength%20of%20pretrained%20image-language%20models.%20For%20each%20class%2C%20a%20large%20language%0Amodel%20%28LLM%29%20generates%20an%20ordered%20sequence%20of%20sub-actions%2C%20which%20we%20align%20with%0Avideo%20frames%20using%20Dynamic%20Time%20Warping%20%28DTW%29%20in%20a%20shared%20embedding%20space.%0AWithout%20any%20video-text%20supervision%20or%20fine-tuning%2C%20ActAlign%20achieves%2030.5%25%0Aaccuracy%20on%20ActionAtlas--the%20most%20diverse%20benchmark%20of%20fine-grained%20actions%0Aacross%20multiple%20sports--where%20human%20performance%20is%20only%2061.6%25.%20ActAlign%0Aoutperforms%20billion-parameter%20video-language%20models%20while%20using%208x%20fewer%0Aparameters.%20Our%20approach%20is%20model-agnostic%20and%20domain-general%2C%20demonstrating%0Athat%20structured%20language%20priors%20combined%20with%20classical%20alignment%20methods%20can%0Aunlock%20the%20open-set%20recognition%20potential%20of%20image-language%20models%20for%0Afine-grained%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22967v2&entry.124074799=Read"},
{"title": "RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted\n  Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation", "author": "Jierui Qu and Jianchun Zhao", "abstract": "  Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach.\n", "link": "http://arxiv.org/abs/2508.02557v1", "date": "2025-08-04", "relevancy": 2.2957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-U%24%5E2%24Net%3A%20A%20Dual-Branch%20UNet%20with%20Reinforcement%20Learning-Assisted%0A%20%20Multimodal%20Feature%20Fusion%20for%20Accurate%203D%20Whole-Heart%20Segmentation&body=Title%3A%20RL-U%24%5E2%24Net%3A%20A%20Dual-Branch%20UNet%20with%20Reinforcement%20Learning-Assisted%0A%20%20Multimodal%20Feature%20Fusion%20for%20Accurate%203D%20Whole-Heart%20Segmentation%0AAuthor%3A%20Jierui%20Qu%20and%20Jianchun%20Zhao%0AAbstract%3A%20%20%20Accurate%20whole-heart%20segmentation%20is%20a%20critical%20component%20in%20the%20precise%0Adiagnosis%20and%20interventional%20planning%20of%20cardiovascular%20diseases.%20Integrating%0Acomplementary%20information%20from%20modalities%20such%20as%20computed%20tomography%20%28CT%29%20and%0Amagnetic%20resonance%20imaging%20%28MRI%29%20can%20significantly%20enhance%20segmentation%0Aaccuracy%20and%20robustness.%20However%2C%20existing%20multi-modal%20segmentation%20methods%0Aface%20several%20limitations%3A%20severe%20spatial%20inconsistency%20between%20modalities%0Ahinders%20effective%20feature%20fusion%3B%20fusion%20strategies%20are%20often%20static%20and%20lack%0Aadaptability%3B%20and%20the%20processes%20of%20feature%20alignment%20and%20segmentation%20are%0Adecoupled%20and%20inefficient.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Adual-branch%20U-Net%20architecture%20enhanced%20by%20reinforcement%20learning%20for%20feature%0Aalignment%2C%20termed%20RL-U%24%5E2%24Net%2C%20designed%20for%20precise%20and%20efficient%20multi-modal%0A3D%20whole-heart%20segmentation.%20The%20model%20employs%20a%20dual-branch%20U-shaped%20network%0Ato%20process%20CT%20and%20MRI%20patches%20in%20parallel%2C%20and%20introduces%20a%20novel%20RL-XAlign%0Amodule%20between%20the%20encoders.%20The%20module%20employs%20a%20cross-modal%20attention%0Amechanism%20to%20capture%20semantic%20correspondences%20between%20modalities%20and%20a%0Areinforcement-learning%20agent%20learns%20an%20optimal%20rotation%20strategy%20that%0Aconsistently%20aligns%20anatomical%20pose%20and%20texture%20features.%20The%20aligned%20features%0Aare%20then%20reconstructed%20through%20their%20respective%20decoders.%20Finally%2C%20an%0Aensemble-learning-based%20decision%20module%20integrates%20the%20predictions%20from%0Aindividual%20patches%20to%20produce%20the%20final%20segmentation%20result.%20Experimental%0Aresults%20on%20the%20publicly%20available%20MM-WHS%202017%20dataset%20demonstrate%20that%20the%0Aproposed%20RL-U%24%5E2%24Net%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%0ADice%20coefficients%20of%2093.1%25%20on%20CT%20and%2087.0%25%20on%20MRI%2C%20thereby%20validating%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-U%2524%255E2%2524Net%253A%2520A%2520Dual-Branch%2520UNet%2520with%2520Reinforcement%2520Learning-Assisted%250A%2520%2520Multimodal%2520Feature%2520Fusion%2520for%2520Accurate%25203D%2520Whole-Heart%2520Segmentation%26entry.906535625%3DJierui%2520Qu%2520and%2520Jianchun%2520Zhao%26entry.1292438233%3D%2520%2520Accurate%2520whole-heart%2520segmentation%2520is%2520a%2520critical%2520component%2520in%2520the%2520precise%250Adiagnosis%2520and%2520interventional%2520planning%2520of%2520cardiovascular%2520diseases.%2520Integrating%250Acomplementary%2520information%2520from%2520modalities%2520such%2520as%2520computed%2520tomography%2520%2528CT%2529%2520and%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520can%2520significantly%2520enhance%2520segmentation%250Aaccuracy%2520and%2520robustness.%2520However%252C%2520existing%2520multi-modal%2520segmentation%2520methods%250Aface%2520several%2520limitations%253A%2520severe%2520spatial%2520inconsistency%2520between%2520modalities%250Ahinders%2520effective%2520feature%2520fusion%253B%2520fusion%2520strategies%2520are%2520often%2520static%2520and%2520lack%250Aadaptability%253B%2520and%2520the%2520processes%2520of%2520feature%2520alignment%2520and%2520segmentation%2520are%250Adecoupled%2520and%2520inefficient.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Adual-branch%2520U-Net%2520architecture%2520enhanced%2520by%2520reinforcement%2520learning%2520for%2520feature%250Aalignment%252C%2520termed%2520RL-U%2524%255E2%2524Net%252C%2520designed%2520for%2520precise%2520and%2520efficient%2520multi-modal%250A3D%2520whole-heart%2520segmentation.%2520The%2520model%2520employs%2520a%2520dual-branch%2520U-shaped%2520network%250Ato%2520process%2520CT%2520and%2520MRI%2520patches%2520in%2520parallel%252C%2520and%2520introduces%2520a%2520novel%2520RL-XAlign%250Amodule%2520between%2520the%2520encoders.%2520The%2520module%2520employs%2520a%2520cross-modal%2520attention%250Amechanism%2520to%2520capture%2520semantic%2520correspondences%2520between%2520modalities%2520and%2520a%250Areinforcement-learning%2520agent%2520learns%2520an%2520optimal%2520rotation%2520strategy%2520that%250Aconsistently%2520aligns%2520anatomical%2520pose%2520and%2520texture%2520features.%2520The%2520aligned%2520features%250Aare%2520then%2520reconstructed%2520through%2520their%2520respective%2520decoders.%2520Finally%252C%2520an%250Aensemble-learning-based%2520decision%2520module%2520integrates%2520the%2520predictions%2520from%250Aindividual%2520patches%2520to%2520produce%2520the%2520final%2520segmentation%2520result.%2520Experimental%250Aresults%2520on%2520the%2520publicly%2520available%2520MM-WHS%25202017%2520dataset%2520demonstrate%2520that%2520the%250Aproposed%2520RL-U%2524%255E2%2524Net%2520outperforms%2520existing%2520state-of-the-art%2520methods%252C%2520achieving%250ADice%2520coefficients%2520of%252093.1%2525%2520on%2520CT%2520and%252087.0%2525%2520on%2520MRI%252C%2520thereby%2520validating%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-U%24%5E2%24Net%3A%20A%20Dual-Branch%20UNet%20with%20Reinforcement%20Learning-Assisted%0A%20%20Multimodal%20Feature%20Fusion%20for%20Accurate%203D%20Whole-Heart%20Segmentation&entry.906535625=Jierui%20Qu%20and%20Jianchun%20Zhao&entry.1292438233=%20%20Accurate%20whole-heart%20segmentation%20is%20a%20critical%20component%20in%20the%20precise%0Adiagnosis%20and%20interventional%20planning%20of%20cardiovascular%20diseases.%20Integrating%0Acomplementary%20information%20from%20modalities%20such%20as%20computed%20tomography%20%28CT%29%20and%0Amagnetic%20resonance%20imaging%20%28MRI%29%20can%20significantly%20enhance%20segmentation%0Aaccuracy%20and%20robustness.%20However%2C%20existing%20multi-modal%20segmentation%20methods%0Aface%20several%20limitations%3A%20severe%20spatial%20inconsistency%20between%20modalities%0Ahinders%20effective%20feature%20fusion%3B%20fusion%20strategies%20are%20often%20static%20and%20lack%0Aadaptability%3B%20and%20the%20processes%20of%20feature%20alignment%20and%20segmentation%20are%0Adecoupled%20and%20inefficient.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Adual-branch%20U-Net%20architecture%20enhanced%20by%20reinforcement%20learning%20for%20feature%0Aalignment%2C%20termed%20RL-U%24%5E2%24Net%2C%20designed%20for%20precise%20and%20efficient%20multi-modal%0A3D%20whole-heart%20segmentation.%20The%20model%20employs%20a%20dual-branch%20U-shaped%20network%0Ato%20process%20CT%20and%20MRI%20patches%20in%20parallel%2C%20and%20introduces%20a%20novel%20RL-XAlign%0Amodule%20between%20the%20encoders.%20The%20module%20employs%20a%20cross-modal%20attention%0Amechanism%20to%20capture%20semantic%20correspondences%20between%20modalities%20and%20a%0Areinforcement-learning%20agent%20learns%20an%20optimal%20rotation%20strategy%20that%0Aconsistently%20aligns%20anatomical%20pose%20and%20texture%20features.%20The%20aligned%20features%0Aare%20then%20reconstructed%20through%20their%20respective%20decoders.%20Finally%2C%20an%0Aensemble-learning-based%20decision%20module%20integrates%20the%20predictions%20from%0Aindividual%20patches%20to%20produce%20the%20final%20segmentation%20result.%20Experimental%0Aresults%20on%20the%20publicly%20available%20MM-WHS%202017%20dataset%20demonstrate%20that%20the%0Aproposed%20RL-U%24%5E2%24Net%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%0ADice%20coefficients%20of%2093.1%25%20on%20CT%20and%2087.0%25%20on%20MRI%2C%20thereby%20validating%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02557v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF\n  Pose Estimation", "author": "Nassim Ali Ousalah and Anis Kacem and Enjie Ghorbel and Emmanuel Koumandakis and Djamila Aouada", "abstract": "  Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.\n", "link": "http://arxiv.org/abs/2503.13053v2", "date": "2025-08-04", "relevancy": 2.293, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6175}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Knowledge%20Distillation%20for%20Compact%20and%20Efficient%206DoF%0A%20%20Pose%20Estimation&body=Title%3A%20Uncertainty-Aware%20Knowledge%20Distillation%20for%20Compact%20and%20Efficient%206DoF%0A%20%20Pose%20Estimation%0AAuthor%3A%20Nassim%20Ali%20Ousalah%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Emmanuel%20Koumandakis%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Compact%20and%20efficient%206DoF%20object%20pose%20estimation%20is%20crucial%20in%20applications%0Asuch%20as%20robotics%2C%20augmented%20reality%2C%20and%20space%20autonomous%20navigation%20systems%2C%0Awhere%20lightweight%20models%20are%20critical%20for%20real-time%20accurate%20performance.%20This%0Apaper%20introduces%20a%20novel%20uncertainty-aware%20end-to-end%20Knowledge%20Distillation%0A%28KD%29%20framework%20focused%20on%20keypoint-based%206DoF%20pose%20estimation.%20Keypoints%0Apredicted%20by%20a%20large%20teacher%20model%20exhibit%20varying%20levels%20of%20uncertainty%20that%0Acan%20be%20exploited%20within%20the%20distillation%20process%20to%20enhance%20the%20accuracy%20of%20the%0Astudent%20model%20while%20ensuring%20its%20compactness.%20To%20this%20end%2C%20we%20propose%20a%0Adistillation%20strategy%20that%20aligns%20the%20student%20and%20teacher%20predictions%20by%0Aadjusting%20the%20knowledge%20transfer%20based%20on%20the%20uncertainty%20associated%20with%20each%0Ateacher%20keypoint%20prediction.%20Additionally%2C%20the%20proposed%20KD%20leverages%20this%0Auncertainty-aware%20alignment%20of%20keypoints%20to%20transfer%20the%20knowledge%20at%20key%0Alocations%20of%20their%20respective%20feature%20maps.%20Experiments%20on%20the%20widely-used%0ALINEMOD%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%0Asuperior%206DoF%20object%20pose%20estimation%20with%20lightweight%20models%20compared%20to%0Astate-of-the-art%20approaches.%20Further%20validation%20on%20the%20SPEED%2B%20dataset%20for%0Aspacecraft%20pose%20estimation%20highlights%20the%20robustness%20of%20our%20approach%20under%0Adiverse%206DoF%20pose%20estimation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Knowledge%2520Distillation%2520for%2520Compact%2520and%2520Efficient%25206DoF%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DNassim%2520Ali%2520Ousalah%2520and%2520Anis%2520Kacem%2520and%2520Enjie%2520Ghorbel%2520and%2520Emmanuel%2520Koumandakis%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Compact%2520and%2520efficient%25206DoF%2520object%2520pose%2520estimation%2520is%2520crucial%2520in%2520applications%250Asuch%2520as%2520robotics%252C%2520augmented%2520reality%252C%2520and%2520space%2520autonomous%2520navigation%2520systems%252C%250Awhere%2520lightweight%2520models%2520are%2520critical%2520for%2520real-time%2520accurate%2520performance.%2520This%250Apaper%2520introduces%2520a%2520novel%2520uncertainty-aware%2520end-to-end%2520Knowledge%2520Distillation%250A%2528KD%2529%2520framework%2520focused%2520on%2520keypoint-based%25206DoF%2520pose%2520estimation.%2520Keypoints%250Apredicted%2520by%2520a%2520large%2520teacher%2520model%2520exhibit%2520varying%2520levels%2520of%2520uncertainty%2520that%250Acan%2520be%2520exploited%2520within%2520the%2520distillation%2520process%2520to%2520enhance%2520the%2520accuracy%2520of%2520the%250Astudent%2520model%2520while%2520ensuring%2520its%2520compactness.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Adistillation%2520strategy%2520that%2520aligns%2520the%2520student%2520and%2520teacher%2520predictions%2520by%250Aadjusting%2520the%2520knowledge%2520transfer%2520based%2520on%2520the%2520uncertainty%2520associated%2520with%2520each%250Ateacher%2520keypoint%2520prediction.%2520Additionally%252C%2520the%2520proposed%2520KD%2520leverages%2520this%250Auncertainty-aware%2520alignment%2520of%2520keypoints%2520to%2520transfer%2520the%2520knowledge%2520at%2520key%250Alocations%2520of%2520their%2520respective%2520feature%2520maps.%2520Experiments%2520on%2520the%2520widely-used%250ALINEMOD%2520benchmark%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520achieving%250Asuperior%25206DoF%2520object%2520pose%2520estimation%2520with%2520lightweight%2520models%2520compared%2520to%250Astate-of-the-art%2520approaches.%2520Further%2520validation%2520on%2520the%2520SPEED%252B%2520dataset%2520for%250Aspacecraft%2520pose%2520estimation%2520highlights%2520the%2520robustness%2520of%2520our%2520approach%2520under%250Adiverse%25206DoF%2520pose%2520estimation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Knowledge%20Distillation%20for%20Compact%20and%20Efficient%206DoF%0A%20%20Pose%20Estimation&entry.906535625=Nassim%20Ali%20Ousalah%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Emmanuel%20Koumandakis%20and%20Djamila%20Aouada&entry.1292438233=%20%20Compact%20and%20efficient%206DoF%20object%20pose%20estimation%20is%20crucial%20in%20applications%0Asuch%20as%20robotics%2C%20augmented%20reality%2C%20and%20space%20autonomous%20navigation%20systems%2C%0Awhere%20lightweight%20models%20are%20critical%20for%20real-time%20accurate%20performance.%20This%0Apaper%20introduces%20a%20novel%20uncertainty-aware%20end-to-end%20Knowledge%20Distillation%0A%28KD%29%20framework%20focused%20on%20keypoint-based%206DoF%20pose%20estimation.%20Keypoints%0Apredicted%20by%20a%20large%20teacher%20model%20exhibit%20varying%20levels%20of%20uncertainty%20that%0Acan%20be%20exploited%20within%20the%20distillation%20process%20to%20enhance%20the%20accuracy%20of%20the%0Astudent%20model%20while%20ensuring%20its%20compactness.%20To%20this%20end%2C%20we%20propose%20a%0Adistillation%20strategy%20that%20aligns%20the%20student%20and%20teacher%20predictions%20by%0Aadjusting%20the%20knowledge%20transfer%20based%20on%20the%20uncertainty%20associated%20with%20each%0Ateacher%20keypoint%20prediction.%20Additionally%2C%20the%20proposed%20KD%20leverages%20this%0Auncertainty-aware%20alignment%20of%20keypoints%20to%20transfer%20the%20knowledge%20at%20key%0Alocations%20of%20their%20respective%20feature%20maps.%20Experiments%20on%20the%20widely-used%0ALINEMOD%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%0Asuperior%206DoF%20object%20pose%20estimation%20with%20lightweight%20models%20compared%20to%0Astate-of-the-art%20approaches.%20Further%20validation%20on%20the%20SPEED%2B%20dataset%20for%0Aspacecraft%20pose%20estimation%20highlights%20the%20robustness%20of%20our%20approach%20under%0Adiverse%206DoF%20pose%20estimation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13053v2&entry.124074799=Read"},
{"title": "Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An\n  Imitation Learning Approach", "author": "Peng Wei and Prabhash Ragbir and Stavros G. Vougioukas and Zhaodan Kong", "abstract": "  Autonomous unmanned aerial vehicle (UAV) navigation in orchards presents\nsignificant challenges due to obstacles and GPS-deprived environments. In this\nwork, we introduce a learning-based approach to achieve vision-based navigation\nof UAVs within orchard rows. Our method employs a variational autoencoder\n(VAE)-based controller, trained with an intervention-based learning framework\nthat allows the UAV to learn a visuomotor policy from human experience. We\nvalidate our approach in real orchard environments with a custom-built\nquadrotor platform. Field experiments demonstrate that after only a few\niterations of training, the proposed VAE-based controller can autonomously\nnavigate the UAV based on a front-mounted camera stream. The controller\nexhibits strong obstacle avoidance performance, achieves longer flying\ndistances with less human assistance, and outperforms existing algorithms.\nFurthermore, we show that the policy generalizes effectively to novel\nenvironments and maintains competitive performance across varying conditions\nand speeds. This research not only advances UAV autonomy but also holds\nsignificant potential for precision agriculture, improving efficiency in\norchard monitoring and management.\n", "link": "http://arxiv.org/abs/2508.02617v1", "date": "2025-08-04", "relevancy": 2.2801, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5923}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5594}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-based%20Navigation%20of%20Unmanned%20Aerial%20Vehicles%20in%20Orchards%3A%20An%0A%20%20Imitation%20Learning%20Approach&body=Title%3A%20Vision-based%20Navigation%20of%20Unmanned%20Aerial%20Vehicles%20in%20Orchards%3A%20An%0A%20%20Imitation%20Learning%20Approach%0AAuthor%3A%20Peng%20Wei%20and%20Prabhash%20Ragbir%20and%20Stavros%20G.%20Vougioukas%20and%20Zhaodan%20Kong%0AAbstract%3A%20%20%20Autonomous%20unmanned%20aerial%20vehicle%20%28UAV%29%20navigation%20in%20orchards%20presents%0Asignificant%20challenges%20due%20to%20obstacles%20and%20GPS-deprived%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20learning-based%20approach%20to%20achieve%20vision-based%20navigation%0Aof%20UAVs%20within%20orchard%20rows.%20Our%20method%20employs%20a%20variational%20autoencoder%0A%28VAE%29-based%20controller%2C%20trained%20with%20an%20intervention-based%20learning%20framework%0Athat%20allows%20the%20UAV%20to%20learn%20a%20visuomotor%20policy%20from%20human%20experience.%20We%0Avalidate%20our%20approach%20in%20real%20orchard%20environments%20with%20a%20custom-built%0Aquadrotor%20platform.%20Field%20experiments%20demonstrate%20that%20after%20only%20a%20few%0Aiterations%20of%20training%2C%20the%20proposed%20VAE-based%20controller%20can%20autonomously%0Anavigate%20the%20UAV%20based%20on%20a%20front-mounted%20camera%20stream.%20The%20controller%0Aexhibits%20strong%20obstacle%20avoidance%20performance%2C%20achieves%20longer%20flying%0Adistances%20with%20less%20human%20assistance%2C%20and%20outperforms%20existing%20algorithms.%0AFurthermore%2C%20we%20show%20that%20the%20policy%20generalizes%20effectively%20to%20novel%0Aenvironments%20and%20maintains%20competitive%20performance%20across%20varying%20conditions%0Aand%20speeds.%20This%20research%20not%20only%20advances%20UAV%20autonomy%20but%20also%20holds%0Asignificant%20potential%20for%20precision%20agriculture%2C%20improving%20efficiency%20in%0Aorchard%20monitoring%20and%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-based%2520Navigation%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520in%2520Orchards%253A%2520An%250A%2520%2520Imitation%2520Learning%2520Approach%26entry.906535625%3DPeng%2520Wei%2520and%2520Prabhash%2520Ragbir%2520and%2520Stavros%2520G.%2520Vougioukas%2520and%2520Zhaodan%2520Kong%26entry.1292438233%3D%2520%2520Autonomous%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520navigation%2520in%2520orchards%2520presents%250Asignificant%2520challenges%2520due%2520to%2520obstacles%2520and%2520GPS-deprived%2520environments.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520learning-based%2520approach%2520to%2520achieve%2520vision-based%2520navigation%250Aof%2520UAVs%2520within%2520orchard%2520rows.%2520Our%2520method%2520employs%2520a%2520variational%2520autoencoder%250A%2528VAE%2529-based%2520controller%252C%2520trained%2520with%2520an%2520intervention-based%2520learning%2520framework%250Athat%2520allows%2520the%2520UAV%2520to%2520learn%2520a%2520visuomotor%2520policy%2520from%2520human%2520experience.%2520We%250Avalidate%2520our%2520approach%2520in%2520real%2520orchard%2520environments%2520with%2520a%2520custom-built%250Aquadrotor%2520platform.%2520Field%2520experiments%2520demonstrate%2520that%2520after%2520only%2520a%2520few%250Aiterations%2520of%2520training%252C%2520the%2520proposed%2520VAE-based%2520controller%2520can%2520autonomously%250Anavigate%2520the%2520UAV%2520based%2520on%2520a%2520front-mounted%2520camera%2520stream.%2520The%2520controller%250Aexhibits%2520strong%2520obstacle%2520avoidance%2520performance%252C%2520achieves%2520longer%2520flying%250Adistances%2520with%2520less%2520human%2520assistance%252C%2520and%2520outperforms%2520existing%2520algorithms.%250AFurthermore%252C%2520we%2520show%2520that%2520the%2520policy%2520generalizes%2520effectively%2520to%2520novel%250Aenvironments%2520and%2520maintains%2520competitive%2520performance%2520across%2520varying%2520conditions%250Aand%2520speeds.%2520This%2520research%2520not%2520only%2520advances%2520UAV%2520autonomy%2520but%2520also%2520holds%250Asignificant%2520potential%2520for%2520precision%2520agriculture%252C%2520improving%2520efficiency%2520in%250Aorchard%2520monitoring%2520and%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20Navigation%20of%20Unmanned%20Aerial%20Vehicles%20in%20Orchards%3A%20An%0A%20%20Imitation%20Learning%20Approach&entry.906535625=Peng%20Wei%20and%20Prabhash%20Ragbir%20and%20Stavros%20G.%20Vougioukas%20and%20Zhaodan%20Kong&entry.1292438233=%20%20Autonomous%20unmanned%20aerial%20vehicle%20%28UAV%29%20navigation%20in%20orchards%20presents%0Asignificant%20challenges%20due%20to%20obstacles%20and%20GPS-deprived%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20learning-based%20approach%20to%20achieve%20vision-based%20navigation%0Aof%20UAVs%20within%20orchard%20rows.%20Our%20method%20employs%20a%20variational%20autoencoder%0A%28VAE%29-based%20controller%2C%20trained%20with%20an%20intervention-based%20learning%20framework%0Athat%20allows%20the%20UAV%20to%20learn%20a%20visuomotor%20policy%20from%20human%20experience.%20We%0Avalidate%20our%20approach%20in%20real%20orchard%20environments%20with%20a%20custom-built%0Aquadrotor%20platform.%20Field%20experiments%20demonstrate%20that%20after%20only%20a%20few%0Aiterations%20of%20training%2C%20the%20proposed%20VAE-based%20controller%20can%20autonomously%0Anavigate%20the%20UAV%20based%20on%20a%20front-mounted%20camera%20stream.%20The%20controller%0Aexhibits%20strong%20obstacle%20avoidance%20performance%2C%20achieves%20longer%20flying%0Adistances%20with%20less%20human%20assistance%2C%20and%20outperforms%20existing%20algorithms.%0AFurthermore%2C%20we%20show%20that%20the%20policy%20generalizes%20effectively%20to%20novel%0Aenvironments%20and%20maintains%20competitive%20performance%20across%20varying%20conditions%0Aand%20speeds.%20This%20research%20not%20only%20advances%20UAV%20autonomy%20but%20also%20holds%0Asignificant%20potential%20for%20precision%20agriculture%2C%20improving%20efficiency%20in%0Aorchard%20monitoring%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02617v1&entry.124074799=Read"},
{"title": "NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds", "author": "Boran Zhao and Haiduo Huang and Qiwei Dang and Wenzhe Zhao and Tian Xia and Pengju Ren", "abstract": "  Training deep neural networks (DNNs) on edge devices has attracted increasing\nattention due to its potential to address challenges related to domain\nadaptation and privacy preservation. However, DNNs typically rely on large\ndatasets for training, which results in substantial energy consumption, making\nthe training in edge devices impractical. Some dataset compression methods have\nbeen proposed to solve this challenge. For instance, the coreset selection and\ndataset distillation reduce the training cost by selecting and generating\nrepresentative samples respectively. Nevertheless, these methods have two\nsignificant defects: (1) The necessary of leveraging a DNN model to evaluate\nthe quality of representative samples, which inevitably introduces inductive\nbias of DNN, resulting in a severe generalization issue; (2) All training\nimages require multiple accesses to the DDR via long-distance PCB connections,\nleading to substantial energy overhead. To address these issues, inspired by\nthe nonlinear manifold stationary of the human brain, we firstly propose a\nDNN-free sample-selecting algorithm, called DE-SNE, to improve the\ngeneralization issue. Secondly, we innovatively utilize the near-memory\ncomputing technique to implement DE-SNE, thus only a small fraction of images\nneed to access the DDR via long-distance PCB. It significantly reduces DDR\nenergy consumption. As a result, we build a novel expedited DNN training system\nwith a more efficient in-place Near-Memory Sampling characteristic for edge\ndevices, dubbed NMS. As far as we know, our NMS is the first DNN-free\nnear-memory sampling technique that can effectively alleviate generalization\nissues and significantly reduce DDR energy caused by dataset access. The\nexperimental results show that our NMS outperforms the current state-of-the-art\n(SOTA) approaches, namely DQ, DQAS, and NeSSA, in model accuracy.\n", "link": "http://arxiv.org/abs/2508.02313v1", "date": "2025-08-04", "relevancy": 2.2794, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5943}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5837}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NMS%3A%20Efficient%20Edge%20DNN%20Training%20via%20Near-Memory%20Sampling%20on%20Manifolds&body=Title%3A%20NMS%3A%20Efficient%20Edge%20DNN%20Training%20via%20Near-Memory%20Sampling%20on%20Manifolds%0AAuthor%3A%20Boran%20Zhao%20and%20Haiduo%20Huang%20and%20Qiwei%20Dang%20and%20Wenzhe%20Zhao%20and%20Tian%20Xia%20and%20Pengju%20Ren%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20on%20edge%20devices%20has%20attracted%20increasing%0Aattention%20due%20to%20its%20potential%20to%20address%20challenges%20related%20to%20domain%0Aadaptation%20and%20privacy%20preservation.%20However%2C%20DNNs%20typically%20rely%20on%20large%0Adatasets%20for%20training%2C%20which%20results%20in%20substantial%20energy%20consumption%2C%20making%0Athe%20training%20in%20edge%20devices%20impractical.%20Some%20dataset%20compression%20methods%20have%0Abeen%20proposed%20to%20solve%20this%20challenge.%20For%20instance%2C%20the%20coreset%20selection%20and%0Adataset%20distillation%20reduce%20the%20training%20cost%20by%20selecting%20and%20generating%0Arepresentative%20samples%20respectively.%20Nevertheless%2C%20these%20methods%20have%20two%0Asignificant%20defects%3A%20%281%29%20The%20necessary%20of%20leveraging%20a%20DNN%20model%20to%20evaluate%0Athe%20quality%20of%20representative%20samples%2C%20which%20inevitably%20introduces%20inductive%0Abias%20of%20DNN%2C%20resulting%20in%20a%20severe%20generalization%20issue%3B%20%282%29%20All%20training%0Aimages%20require%20multiple%20accesses%20to%20the%20DDR%20via%20long-distance%20PCB%20connections%2C%0Aleading%20to%20substantial%20energy%20overhead.%20To%20address%20these%20issues%2C%20inspired%20by%0Athe%20nonlinear%20manifold%20stationary%20of%20the%20human%20brain%2C%20we%20firstly%20propose%20a%0ADNN-free%20sample-selecting%20algorithm%2C%20called%20DE-SNE%2C%20to%20improve%20the%0Ageneralization%20issue.%20Secondly%2C%20we%20innovatively%20utilize%20the%20near-memory%0Acomputing%20technique%20to%20implement%20DE-SNE%2C%20thus%20only%20a%20small%20fraction%20of%20images%0Aneed%20to%20access%20the%20DDR%20via%20long-distance%20PCB.%20It%20significantly%20reduces%20DDR%0Aenergy%20consumption.%20As%20a%20result%2C%20we%20build%20a%20novel%20expedited%20DNN%20training%20system%0Awith%20a%20more%20efficient%20in-place%20Near-Memory%20Sampling%20characteristic%20for%20edge%0Adevices%2C%20dubbed%20NMS.%20As%20far%20as%20we%20know%2C%20our%20NMS%20is%20the%20first%20DNN-free%0Anear-memory%20sampling%20technique%20that%20can%20effectively%20alleviate%20generalization%0Aissues%20and%20significantly%20reduce%20DDR%20energy%20caused%20by%20dataset%20access.%20The%0Aexperimental%20results%20show%20that%20our%20NMS%20outperforms%20the%20current%20state-of-the-art%0A%28SOTA%29%20approaches%2C%20namely%20DQ%2C%20DQAS%2C%20and%20NeSSA%2C%20in%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNMS%253A%2520Efficient%2520Edge%2520DNN%2520Training%2520via%2520Near-Memory%2520Sampling%2520on%2520Manifolds%26entry.906535625%3DBoran%2520Zhao%2520and%2520Haiduo%2520Huang%2520and%2520Qiwei%2520Dang%2520and%2520Wenzhe%2520Zhao%2520and%2520Tian%2520Xia%2520and%2520Pengju%2520Ren%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520on%2520edge%2520devices%2520has%2520attracted%2520increasing%250Aattention%2520due%2520to%2520its%2520potential%2520to%2520address%2520challenges%2520related%2520to%2520domain%250Aadaptation%2520and%2520privacy%2520preservation.%2520However%252C%2520DNNs%2520typically%2520rely%2520on%2520large%250Adatasets%2520for%2520training%252C%2520which%2520results%2520in%2520substantial%2520energy%2520consumption%252C%2520making%250Athe%2520training%2520in%2520edge%2520devices%2520impractical.%2520Some%2520dataset%2520compression%2520methods%2520have%250Abeen%2520proposed%2520to%2520solve%2520this%2520challenge.%2520For%2520instance%252C%2520the%2520coreset%2520selection%2520and%250Adataset%2520distillation%2520reduce%2520the%2520training%2520cost%2520by%2520selecting%2520and%2520generating%250Arepresentative%2520samples%2520respectively.%2520Nevertheless%252C%2520these%2520methods%2520have%2520two%250Asignificant%2520defects%253A%2520%25281%2529%2520The%2520necessary%2520of%2520leveraging%2520a%2520DNN%2520model%2520to%2520evaluate%250Athe%2520quality%2520of%2520representative%2520samples%252C%2520which%2520inevitably%2520introduces%2520inductive%250Abias%2520of%2520DNN%252C%2520resulting%2520in%2520a%2520severe%2520generalization%2520issue%253B%2520%25282%2529%2520All%2520training%250Aimages%2520require%2520multiple%2520accesses%2520to%2520the%2520DDR%2520via%2520long-distance%2520PCB%2520connections%252C%250Aleading%2520to%2520substantial%2520energy%2520overhead.%2520To%2520address%2520these%2520issues%252C%2520inspired%2520by%250Athe%2520nonlinear%2520manifold%2520stationary%2520of%2520the%2520human%2520brain%252C%2520we%2520firstly%2520propose%2520a%250ADNN-free%2520sample-selecting%2520algorithm%252C%2520called%2520DE-SNE%252C%2520to%2520improve%2520the%250Ageneralization%2520issue.%2520Secondly%252C%2520we%2520innovatively%2520utilize%2520the%2520near-memory%250Acomputing%2520technique%2520to%2520implement%2520DE-SNE%252C%2520thus%2520only%2520a%2520small%2520fraction%2520of%2520images%250Aneed%2520to%2520access%2520the%2520DDR%2520via%2520long-distance%2520PCB.%2520It%2520significantly%2520reduces%2520DDR%250Aenergy%2520consumption.%2520As%2520a%2520result%252C%2520we%2520build%2520a%2520novel%2520expedited%2520DNN%2520training%2520system%250Awith%2520a%2520more%2520efficient%2520in-place%2520Near-Memory%2520Sampling%2520characteristic%2520for%2520edge%250Adevices%252C%2520dubbed%2520NMS.%2520As%2520far%2520as%2520we%2520know%252C%2520our%2520NMS%2520is%2520the%2520first%2520DNN-free%250Anear-memory%2520sampling%2520technique%2520that%2520can%2520effectively%2520alleviate%2520generalization%250Aissues%2520and%2520significantly%2520reduce%2520DDR%2520energy%2520caused%2520by%2520dataset%2520access.%2520The%250Aexperimental%2520results%2520show%2520that%2520our%2520NMS%2520outperforms%2520the%2520current%2520state-of-the-art%250A%2528SOTA%2529%2520approaches%252C%2520namely%2520DQ%252C%2520DQAS%252C%2520and%2520NeSSA%252C%2520in%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NMS%3A%20Efficient%20Edge%20DNN%20Training%20via%20Near-Memory%20Sampling%20on%20Manifolds&entry.906535625=Boran%20Zhao%20and%20Haiduo%20Huang%20and%20Qiwei%20Dang%20and%20Wenzhe%20Zhao%20and%20Tian%20Xia%20and%20Pengju%20Ren&entry.1292438233=%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20on%20edge%20devices%20has%20attracted%20increasing%0Aattention%20due%20to%20its%20potential%20to%20address%20challenges%20related%20to%20domain%0Aadaptation%20and%20privacy%20preservation.%20However%2C%20DNNs%20typically%20rely%20on%20large%0Adatasets%20for%20training%2C%20which%20results%20in%20substantial%20energy%20consumption%2C%20making%0Athe%20training%20in%20edge%20devices%20impractical.%20Some%20dataset%20compression%20methods%20have%0Abeen%20proposed%20to%20solve%20this%20challenge.%20For%20instance%2C%20the%20coreset%20selection%20and%0Adataset%20distillation%20reduce%20the%20training%20cost%20by%20selecting%20and%20generating%0Arepresentative%20samples%20respectively.%20Nevertheless%2C%20these%20methods%20have%20two%0Asignificant%20defects%3A%20%281%29%20The%20necessary%20of%20leveraging%20a%20DNN%20model%20to%20evaluate%0Athe%20quality%20of%20representative%20samples%2C%20which%20inevitably%20introduces%20inductive%0Abias%20of%20DNN%2C%20resulting%20in%20a%20severe%20generalization%20issue%3B%20%282%29%20All%20training%0Aimages%20require%20multiple%20accesses%20to%20the%20DDR%20via%20long-distance%20PCB%20connections%2C%0Aleading%20to%20substantial%20energy%20overhead.%20To%20address%20these%20issues%2C%20inspired%20by%0Athe%20nonlinear%20manifold%20stationary%20of%20the%20human%20brain%2C%20we%20firstly%20propose%20a%0ADNN-free%20sample-selecting%20algorithm%2C%20called%20DE-SNE%2C%20to%20improve%20the%0Ageneralization%20issue.%20Secondly%2C%20we%20innovatively%20utilize%20the%20near-memory%0Acomputing%20technique%20to%20implement%20DE-SNE%2C%20thus%20only%20a%20small%20fraction%20of%20images%0Aneed%20to%20access%20the%20DDR%20via%20long-distance%20PCB.%20It%20significantly%20reduces%20DDR%0Aenergy%20consumption.%20As%20a%20result%2C%20we%20build%20a%20novel%20expedited%20DNN%20training%20system%0Awith%20a%20more%20efficient%20in-place%20Near-Memory%20Sampling%20characteristic%20for%20edge%0Adevices%2C%20dubbed%20NMS.%20As%20far%20as%20we%20know%2C%20our%20NMS%20is%20the%20first%20DNN-free%0Anear-memory%20sampling%20technique%20that%20can%20effectively%20alleviate%20generalization%0Aissues%20and%20significantly%20reduce%20DDR%20energy%20caused%20by%20dataset%20access.%20The%0Aexperimental%20results%20show%20that%20our%20NMS%20outperforms%20the%20current%20state-of-the-art%0A%28SOTA%29%20approaches%2C%20namely%20DQ%2C%20DQAS%2C%20and%20NeSSA%2C%20in%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02313v1&entry.124074799=Read"},
{"title": "Transformer Meets Twicing: Harnessing Unattended Residual Information", "author": "Laziz Abdullaev and Tan M. Nguyen", "abstract": "  Transformer-based deep learning models have achieved state-of-the-art\nperformance across numerous language and vision tasks. While the self-attention\nmechanism, a core component of transformers, has proven capable of handling\ncomplex data patterns, it has been observed that the representational capacity\nof the attention matrix degrades significantly across transformer layers,\nthereby hurting its overall performance. In this work, we leverage the\nconnection between self-attention computations and low-pass non-local means\n(NLM) smoothing filters and propose the Twicing Attention, a novel attention\nmechanism that uses kernel twicing procedure in nonparametric regression to\nalleviate the low-pass behavior of associated NLM smoothing with compelling\ntheoretical guarantees and enhanced adversarial robustness. This approach\nenables the extraction and reuse of meaningful information retained in the\nresiduals following the imperfect smoothing operation at each layer. Our\nproposed method offers two key advantages over standard self-attention: 1) a\nprovably slower decay of representational capacity and 2) improved robustness\nand accuracy across various data modalities and tasks. We empirically\ndemonstrate the performance gains of our model over baseline transformers on\nmultiple tasks and benchmarks, including image classification and language\nmodeling, on both clean and corrupted data.\n", "link": "http://arxiv.org/abs/2503.00687v3", "date": "2025-08-04", "relevancy": 2.2755, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6052}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5694}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Meets%20Twicing%3A%20Harnessing%20Unattended%20Residual%20Information&body=Title%3A%20Transformer%20Meets%20Twicing%3A%20Harnessing%20Unattended%20Residual%20Information%0AAuthor%3A%20Laziz%20Abdullaev%20and%20Tan%20M.%20Nguyen%0AAbstract%3A%20%20%20Transformer-based%20deep%20learning%20models%20have%20achieved%20state-of-the-art%0Aperformance%20across%20numerous%20language%20and%20vision%20tasks.%20While%20the%20self-attention%0Amechanism%2C%20a%20core%20component%20of%20transformers%2C%20has%20proven%20capable%20of%20handling%0Acomplex%20data%20patterns%2C%20it%20has%20been%20observed%20that%20the%20representational%20capacity%0Aof%20the%20attention%20matrix%20degrades%20significantly%20across%20transformer%20layers%2C%0Athereby%20hurting%20its%20overall%20performance.%20In%20this%20work%2C%20we%20leverage%20the%0Aconnection%20between%20self-attention%20computations%20and%20low-pass%20non-local%20means%0A%28NLM%29%20smoothing%20filters%20and%20propose%20the%20Twicing%20Attention%2C%20a%20novel%20attention%0Amechanism%20that%20uses%20kernel%20twicing%20procedure%20in%20nonparametric%20regression%20to%0Aalleviate%20the%20low-pass%20behavior%20of%20associated%20NLM%20smoothing%20with%20compelling%0Atheoretical%20guarantees%20and%20enhanced%20adversarial%20robustness.%20This%20approach%0Aenables%20the%20extraction%20and%20reuse%20of%20meaningful%20information%20retained%20in%20the%0Aresiduals%20following%20the%20imperfect%20smoothing%20operation%20at%20each%20layer.%20Our%0Aproposed%20method%20offers%20two%20key%20advantages%20over%20standard%20self-attention%3A%201%29%20a%0Aprovably%20slower%20decay%20of%20representational%20capacity%20and%202%29%20improved%20robustness%0Aand%20accuracy%20across%20various%20data%20modalities%20and%20tasks.%20We%20empirically%0Ademonstrate%20the%20performance%20gains%20of%20our%20model%20over%20baseline%20transformers%20on%0Amultiple%20tasks%20and%20benchmarks%2C%20including%20image%20classification%20and%20language%0Amodeling%2C%20on%20both%20clean%20and%20corrupted%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00687v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Meets%2520Twicing%253A%2520Harnessing%2520Unattended%2520Residual%2520Information%26entry.906535625%3DLaziz%2520Abdullaev%2520and%2520Tan%2520M.%2520Nguyen%26entry.1292438233%3D%2520%2520Transformer-based%2520deep%2520learning%2520models%2520have%2520achieved%2520state-of-the-art%250Aperformance%2520across%2520numerous%2520language%2520and%2520vision%2520tasks.%2520While%2520the%2520self-attention%250Amechanism%252C%2520a%2520core%2520component%2520of%2520transformers%252C%2520has%2520proven%2520capable%2520of%2520handling%250Acomplex%2520data%2520patterns%252C%2520it%2520has%2520been%2520observed%2520that%2520the%2520representational%2520capacity%250Aof%2520the%2520attention%2520matrix%2520degrades%2520significantly%2520across%2520transformer%2520layers%252C%250Athereby%2520hurting%2520its%2520overall%2520performance.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%250Aconnection%2520between%2520self-attention%2520computations%2520and%2520low-pass%2520non-local%2520means%250A%2528NLM%2529%2520smoothing%2520filters%2520and%2520propose%2520the%2520Twicing%2520Attention%252C%2520a%2520novel%2520attention%250Amechanism%2520that%2520uses%2520kernel%2520twicing%2520procedure%2520in%2520nonparametric%2520regression%2520to%250Aalleviate%2520the%2520low-pass%2520behavior%2520of%2520associated%2520NLM%2520smoothing%2520with%2520compelling%250Atheoretical%2520guarantees%2520and%2520enhanced%2520adversarial%2520robustness.%2520This%2520approach%250Aenables%2520the%2520extraction%2520and%2520reuse%2520of%2520meaningful%2520information%2520retained%2520in%2520the%250Aresiduals%2520following%2520the%2520imperfect%2520smoothing%2520operation%2520at%2520each%2520layer.%2520Our%250Aproposed%2520method%2520offers%2520two%2520key%2520advantages%2520over%2520standard%2520self-attention%253A%25201%2529%2520a%250Aprovably%2520slower%2520decay%2520of%2520representational%2520capacity%2520and%25202%2529%2520improved%2520robustness%250Aand%2520accuracy%2520across%2520various%2520data%2520modalities%2520and%2520tasks.%2520We%2520empirically%250Ademonstrate%2520the%2520performance%2520gains%2520of%2520our%2520model%2520over%2520baseline%2520transformers%2520on%250Amultiple%2520tasks%2520and%2520benchmarks%252C%2520including%2520image%2520classification%2520and%2520language%250Amodeling%252C%2520on%2520both%2520clean%2520and%2520corrupted%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00687v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Meets%20Twicing%3A%20Harnessing%20Unattended%20Residual%20Information&entry.906535625=Laziz%20Abdullaev%20and%20Tan%20M.%20Nguyen&entry.1292438233=%20%20Transformer-based%20deep%20learning%20models%20have%20achieved%20state-of-the-art%0Aperformance%20across%20numerous%20language%20and%20vision%20tasks.%20While%20the%20self-attention%0Amechanism%2C%20a%20core%20component%20of%20transformers%2C%20has%20proven%20capable%20of%20handling%0Acomplex%20data%20patterns%2C%20it%20has%20been%20observed%20that%20the%20representational%20capacity%0Aof%20the%20attention%20matrix%20degrades%20significantly%20across%20transformer%20layers%2C%0Athereby%20hurting%20its%20overall%20performance.%20In%20this%20work%2C%20we%20leverage%20the%0Aconnection%20between%20self-attention%20computations%20and%20low-pass%20non-local%20means%0A%28NLM%29%20smoothing%20filters%20and%20propose%20the%20Twicing%20Attention%2C%20a%20novel%20attention%0Amechanism%20that%20uses%20kernel%20twicing%20procedure%20in%20nonparametric%20regression%20to%0Aalleviate%20the%20low-pass%20behavior%20of%20associated%20NLM%20smoothing%20with%20compelling%0Atheoretical%20guarantees%20and%20enhanced%20adversarial%20robustness.%20This%20approach%0Aenables%20the%20extraction%20and%20reuse%20of%20meaningful%20information%20retained%20in%20the%0Aresiduals%20following%20the%20imperfect%20smoothing%20operation%20at%20each%20layer.%20Our%0Aproposed%20method%20offers%20two%20key%20advantages%20over%20standard%20self-attention%3A%201%29%20a%0Aprovably%20slower%20decay%20of%20representational%20capacity%20and%202%29%20improved%20robustness%0Aand%20accuracy%20across%20various%20data%20modalities%20and%20tasks.%20We%20empirically%0Ademonstrate%20the%20performance%20gains%20of%20our%20model%20over%20baseline%20transformers%20on%0Amultiple%20tasks%20and%20benchmarks%2C%20including%20image%20classification%20and%20language%0Amodeling%2C%20on%20both%20clean%20and%20corrupted%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00687v3&entry.124074799=Read"},
{"title": "Evaluating Variance in Visual Question Answering Benchmarks", "author": "Nikitha SR", "abstract": "  Multimodal large language models (MLLMs) have emerged as powerful tools for\nvisual question answering (VQA), enabling reasoning and contextual\nunderstanding across visual and textual modalities. Despite their advancements,\nthe evaluation of MLLMs on VQA benchmarks often relies on point estimates,\noverlooking the significant variance in performance caused by factors such as\nstochastic model outputs, training seed sensitivity, and hyperparameter\nconfigurations. This paper critically examines these issues by analyzing\nvariance across 14 widely used VQA benchmarks, covering diverse tasks such as\nvisual reasoning, text understanding, and commonsense reasoning. We\nsystematically study the impact of training seed, framework non-determinism,\nmodel scale, and extended instruction finetuning on performance variability.\nAdditionally, we explore Cloze-style evaluation as an alternate assessment\nstrategy, studying its effectiveness in reducing stochasticity and improving\nreliability across benchmarks. Our findings highlight the limitations of\ncurrent evaluation practices and advocate for variance-aware methodologies to\nfoster more robust and reliable development of MLLMs.\n", "link": "http://arxiv.org/abs/2508.02645v1", "date": "2025-08-04", "relevancy": 2.2731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Variance%20in%20Visual%20Question%20Answering%20Benchmarks&body=Title%3A%20Evaluating%20Variance%20in%20Visual%20Question%20Answering%20Benchmarks%0AAuthor%3A%20Nikitha%20SR%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20powerful%20tools%20for%0Avisual%20question%20answering%20%28VQA%29%2C%20enabling%20reasoning%20and%20contextual%0Aunderstanding%20across%20visual%20and%20textual%20modalities.%20Despite%20their%20advancements%2C%0Athe%20evaluation%20of%20MLLMs%20on%20VQA%20benchmarks%20often%20relies%20on%20point%20estimates%2C%0Aoverlooking%20the%20significant%20variance%20in%20performance%20caused%20by%20factors%20such%20as%0Astochastic%20model%20outputs%2C%20training%20seed%20sensitivity%2C%20and%20hyperparameter%0Aconfigurations.%20This%20paper%20critically%20examines%20these%20issues%20by%20analyzing%0Avariance%20across%2014%20widely%20used%20VQA%20benchmarks%2C%20covering%20diverse%20tasks%20such%20as%0Avisual%20reasoning%2C%20text%20understanding%2C%20and%20commonsense%20reasoning.%20We%0Asystematically%20study%20the%20impact%20of%20training%20seed%2C%20framework%20non-determinism%2C%0Amodel%20scale%2C%20and%20extended%20instruction%20finetuning%20on%20performance%20variability.%0AAdditionally%2C%20we%20explore%20Cloze-style%20evaluation%20as%20an%20alternate%20assessment%0Astrategy%2C%20studying%20its%20effectiveness%20in%20reducing%20stochasticity%20and%20improving%0Areliability%20across%20benchmarks.%20Our%20findings%20highlight%20the%20limitations%20of%0Acurrent%20evaluation%20practices%20and%20advocate%20for%20variance-aware%20methodologies%20to%0Afoster%20more%20robust%20and%20reliable%20development%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Variance%2520in%2520Visual%2520Question%2520Answering%2520Benchmarks%26entry.906535625%3DNikitha%2520SR%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%250Avisual%2520question%2520answering%2520%2528VQA%2529%252C%2520enabling%2520reasoning%2520and%2520contextual%250Aunderstanding%2520across%2520visual%2520and%2520textual%2520modalities.%2520Despite%2520their%2520advancements%252C%250Athe%2520evaluation%2520of%2520MLLMs%2520on%2520VQA%2520benchmarks%2520often%2520relies%2520on%2520point%2520estimates%252C%250Aoverlooking%2520the%2520significant%2520variance%2520in%2520performance%2520caused%2520by%2520factors%2520such%2520as%250Astochastic%2520model%2520outputs%252C%2520training%2520seed%2520sensitivity%252C%2520and%2520hyperparameter%250Aconfigurations.%2520This%2520paper%2520critically%2520examines%2520these%2520issues%2520by%2520analyzing%250Avariance%2520across%252014%2520widely%2520used%2520VQA%2520benchmarks%252C%2520covering%2520diverse%2520tasks%2520such%2520as%250Avisual%2520reasoning%252C%2520text%2520understanding%252C%2520and%2520commonsense%2520reasoning.%2520We%250Asystematically%2520study%2520the%2520impact%2520of%2520training%2520seed%252C%2520framework%2520non-determinism%252C%250Amodel%2520scale%252C%2520and%2520extended%2520instruction%2520finetuning%2520on%2520performance%2520variability.%250AAdditionally%252C%2520we%2520explore%2520Cloze-style%2520evaluation%2520as%2520an%2520alternate%2520assessment%250Astrategy%252C%2520studying%2520its%2520effectiveness%2520in%2520reducing%2520stochasticity%2520and%2520improving%250Areliability%2520across%2520benchmarks.%2520Our%2520findings%2520highlight%2520the%2520limitations%2520of%250Acurrent%2520evaluation%2520practices%2520and%2520advocate%2520for%2520variance-aware%2520methodologies%2520to%250Afoster%2520more%2520robust%2520and%2520reliable%2520development%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Variance%20in%20Visual%20Question%20Answering%20Benchmarks&entry.906535625=Nikitha%20SR&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20powerful%20tools%20for%0Avisual%20question%20answering%20%28VQA%29%2C%20enabling%20reasoning%20and%20contextual%0Aunderstanding%20across%20visual%20and%20textual%20modalities.%20Despite%20their%20advancements%2C%0Athe%20evaluation%20of%20MLLMs%20on%20VQA%20benchmarks%20often%20relies%20on%20point%20estimates%2C%0Aoverlooking%20the%20significant%20variance%20in%20performance%20caused%20by%20factors%20such%20as%0Astochastic%20model%20outputs%2C%20training%20seed%20sensitivity%2C%20and%20hyperparameter%0Aconfigurations.%20This%20paper%20critically%20examines%20these%20issues%20by%20analyzing%0Avariance%20across%2014%20widely%20used%20VQA%20benchmarks%2C%20covering%20diverse%20tasks%20such%20as%0Avisual%20reasoning%2C%20text%20understanding%2C%20and%20commonsense%20reasoning.%20We%0Asystematically%20study%20the%20impact%20of%20training%20seed%2C%20framework%20non-determinism%2C%0Amodel%20scale%2C%20and%20extended%20instruction%20finetuning%20on%20performance%20variability.%0AAdditionally%2C%20we%20explore%20Cloze-style%20evaluation%20as%20an%20alternate%20assessment%0Astrategy%2C%20studying%20its%20effectiveness%20in%20reducing%20stochasticity%20and%20improving%0Areliability%20across%20benchmarks.%20Our%20findings%20highlight%20the%20limitations%20of%0Acurrent%20evaluation%20practices%20and%20advocate%20for%20variance-aware%20methodologies%20to%0Afoster%20more%20robust%20and%20reliable%20development%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02645v1&entry.124074799=Read"},
{"title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning", "author": "Xiaoke Huang and Juncheng Wu and Hui Liu and Xianfeng Tang and Yuyin Zhou", "abstract": "  Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.\n", "link": "http://arxiv.org/abs/2508.02669v1", "date": "2025-08-04", "relevancy": 2.253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning&body=Title%3A%20MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning%0AAuthor%3A%20Xiaoke%20Huang%20and%20Juncheng%20Wu%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Yuyin%20Zhou%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20introduced%20a%20new%20paradigm%20in%20AI%20by%0Aenabling%20models%20to%20%60%60think%20before%20responding%22%20via%20chain-of-thought%20reasoning.%0AHowever%2C%20the%20absence%20of%20open%20and%20reproducible%20recipes%20for%20building%0Areasoning-centric%20medical%20LMMs%20hinders%20community-wide%20research%2C%20analysis%2C%20and%0Acomparison.%20In%20this%20paper%2C%20we%20present%20MedVLThinker%2C%20a%20suite%20of%20simple%20yet%0Astrong%20baselines.%20Our%20fully%20open%20recipe%20consists%20of%3A%20%281%29%20systematic%20data%0Acuration%20for%20both%20text-only%20and%20image-text%20medical%20data%2C%20filtered%20according%20to%0Avarying%20levels%20of%20reasoning%20difficulty%2C%20and%20%282%29%20two%20training%20paradigms%3A%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20distilled%20reasoning%20traces%20and%20Reinforcement%0ALearning%20with%20Verifiable%20Rewards%20%28RLVR%29%20based%20on%20final%20answer%20correctness.%0AAcross%20extensive%20experiments%20on%20the%20Qwen2.5-VL%20model%20family%20%283B%2C%207B%29%20and%20six%0Amedical%20QA%20benchmarks%2C%20we%20find%20that%20RLVR%20consistently%20and%20significantly%0Aoutperforms%20SFT.%20Additionally%2C%20under%20the%20RLVR%20framework%2C%20a%20key%2C%0Acounter-intuitive%20finding%20is%20that%20training%20on%20our%20curated%20text-only%20reasoning%0Adata%20provides%20a%20more%20substantial%20performance%20boost%20than%20training%20on%20multimodal%0Aimage-text%20data.%20Our%20best%20open%207B%20model%2C%20trained%20using%20the%20RLVR%20recipe%20on%0Atext-only%20data%2C%20establishes%20a%20new%20state-of-the-art%20on%20existing%20public%20VQA%0Abenchmarks%2C%20surpassing%20all%20previous%20open-source%20medical%20LMMs.%20Furthermore%2C%0Ascaling%20our%20model%20to%2032B%20achieves%20performance%20on%20par%20with%20the%20proprietary%0AGPT-4o.%20We%20release%20all%20curated%20data%2C%20models%2C%20and%20code%20to%20provide%20the%20community%0Awith%20a%20strong%2C%20open%20foundation%20for%20future%20research%20in%20multimodal%20medical%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedVLThinker%253A%2520Simple%2520Baselines%2520for%2520Multimodal%2520Medical%2520Reasoning%26entry.906535625%3DXiaoke%2520Huang%2520and%2520Juncheng%2520Wu%2520and%2520Hui%2520Liu%2520and%2520Xianfeng%2520Tang%2520and%2520Yuyin%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520introduced%2520a%2520new%2520paradigm%2520in%2520AI%2520by%250Aenabling%2520models%2520to%2520%2560%2560think%2520before%2520responding%2522%2520via%2520chain-of-thought%2520reasoning.%250AHowever%252C%2520the%2520absence%2520of%2520open%2520and%2520reproducible%2520recipes%2520for%2520building%250Areasoning-centric%2520medical%2520LMMs%2520hinders%2520community-wide%2520research%252C%2520analysis%252C%2520and%250Acomparison.%2520In%2520this%2520paper%252C%2520we%2520present%2520MedVLThinker%252C%2520a%2520suite%2520of%2520simple%2520yet%250Astrong%2520baselines.%2520Our%2520fully%2520open%2520recipe%2520consists%2520of%253A%2520%25281%2529%2520systematic%2520data%250Acuration%2520for%2520both%2520text-only%2520and%2520image-text%2520medical%2520data%252C%2520filtered%2520according%2520to%250Avarying%2520levels%2520of%2520reasoning%2520difficulty%252C%2520and%2520%25282%2529%2520two%2520training%2520paradigms%253A%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520distilled%2520reasoning%2520traces%2520and%2520Reinforcement%250ALearning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520based%2520on%2520final%2520answer%2520correctness.%250AAcross%2520extensive%2520experiments%2520on%2520the%2520Qwen2.5-VL%2520model%2520family%2520%25283B%252C%25207B%2529%2520and%2520six%250Amedical%2520QA%2520benchmarks%252C%2520we%2520find%2520that%2520RLVR%2520consistently%2520and%2520significantly%250Aoutperforms%2520SFT.%2520Additionally%252C%2520under%2520the%2520RLVR%2520framework%252C%2520a%2520key%252C%250Acounter-intuitive%2520finding%2520is%2520that%2520training%2520on%2520our%2520curated%2520text-only%2520reasoning%250Adata%2520provides%2520a%2520more%2520substantial%2520performance%2520boost%2520than%2520training%2520on%2520multimodal%250Aimage-text%2520data.%2520Our%2520best%2520open%25207B%2520model%252C%2520trained%2520using%2520the%2520RLVR%2520recipe%2520on%250Atext-only%2520data%252C%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520existing%2520public%2520VQA%250Abenchmarks%252C%2520surpassing%2520all%2520previous%2520open-source%2520medical%2520LMMs.%2520Furthermore%252C%250Ascaling%2520our%2520model%2520to%252032B%2520achieves%2520performance%2520on%2520par%2520with%2520the%2520proprietary%250AGPT-4o.%2520We%2520release%2520all%2520curated%2520data%252C%2520models%252C%2520and%2520code%2520to%2520provide%2520the%2520community%250Awith%2520a%2520strong%252C%2520open%2520foundation%2520for%2520future%2520research%2520in%2520multimodal%2520medical%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning&entry.906535625=Xiaoke%20Huang%20and%20Juncheng%20Wu%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Yuyin%20Zhou&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20introduced%20a%20new%20paradigm%20in%20AI%20by%0Aenabling%20models%20to%20%60%60think%20before%20responding%22%20via%20chain-of-thought%20reasoning.%0AHowever%2C%20the%20absence%20of%20open%20and%20reproducible%20recipes%20for%20building%0Areasoning-centric%20medical%20LMMs%20hinders%20community-wide%20research%2C%20analysis%2C%20and%0Acomparison.%20In%20this%20paper%2C%20we%20present%20MedVLThinker%2C%20a%20suite%20of%20simple%20yet%0Astrong%20baselines.%20Our%20fully%20open%20recipe%20consists%20of%3A%20%281%29%20systematic%20data%0Acuration%20for%20both%20text-only%20and%20image-text%20medical%20data%2C%20filtered%20according%20to%0Avarying%20levels%20of%20reasoning%20difficulty%2C%20and%20%282%29%20two%20training%20paradigms%3A%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20distilled%20reasoning%20traces%20and%20Reinforcement%0ALearning%20with%20Verifiable%20Rewards%20%28RLVR%29%20based%20on%20final%20answer%20correctness.%0AAcross%20extensive%20experiments%20on%20the%20Qwen2.5-VL%20model%20family%20%283B%2C%207B%29%20and%20six%0Amedical%20QA%20benchmarks%2C%20we%20find%20that%20RLVR%20consistently%20and%20significantly%0Aoutperforms%20SFT.%20Additionally%2C%20under%20the%20RLVR%20framework%2C%20a%20key%2C%0Acounter-intuitive%20finding%20is%20that%20training%20on%20our%20curated%20text-only%20reasoning%0Adata%20provides%20a%20more%20substantial%20performance%20boost%20than%20training%20on%20multimodal%0Aimage-text%20data.%20Our%20best%20open%207B%20model%2C%20trained%20using%20the%20RLVR%20recipe%20on%0Atext-only%20data%2C%20establishes%20a%20new%20state-of-the-art%20on%20existing%20public%20VQA%0Abenchmarks%2C%20surpassing%20all%20previous%20open-source%20medical%20LMMs.%20Furthermore%2C%0Ascaling%20our%20model%20to%2032B%20achieves%20performance%20on%20par%20with%20the%20proprietary%0AGPT-4o.%20We%20release%20all%20curated%20data%2C%20models%2C%20and%20code%20to%20provide%20the%20community%0Awith%20a%20strong%2C%20open%20foundation%20for%20future%20research%20in%20multimodal%20medical%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02669v1&entry.124074799=Read"},
{"title": "An Electrocardiogram Foundation Model Built on over 10 Million\n  Recordings with External Evaluation across Multiple Domains", "author": "Jun Li and Aaron Aguirre and Junior Moura and Che Liu and Lanhai Zhong and Chenxi Sun and Gari Clifford and Brandon Westover and Shenda Hong", "abstract": "  Artificial intelligence (AI) has demonstrated significant potential in ECG\nanalysis and cardiovascular disease assessment. Recently, foundation models\nhave played a remarkable role in advancing medical AI. The development of an\nECG foundation model holds the promise of elevating AI-ECG research to new\nheights. However, building such a model faces several challenges, including\ninsufficient database sample sizes and inadequate generalization across\nmultiple domains. Additionally, there is a notable performance gap between\nsingle-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model\n(ECGFounder), a general-purpose model that leverages real-world ECG annotations\nfrom cardiology experts to broaden the diagnostic capabilities of ECG analysis.\nECGFounder was trained on over 10 million ECGs with 150 label categories from\nthe Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease\ndiagnosis through ECG analysis. The model is designed to be both an effective\nout-of-the-box solution, and a to be fine-tunable for downstream tasks,\nmaximizing usability. Importantly, we extended its application to lower rank\nECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to\nsupporting various downstream tasks in mobile monitoring scenarios.\nExperimental results demonstrate that ECGFounder achieves expert-level\nperformance on internal validation sets, with AUROC exceeding 0.95 for eighty\ndiagnoses. It also shows strong classification performance and generalization\nacross various diagnoses on external validation sets. When fine-tuned,\nECGFounder outperforms baseline models in demographic analysis, clinical event\ndetection, and cross-modality cardiac rhythm diagnosis. The trained model and\ndata will be publicly released upon publication through the bdsp.io. Our code\nis available at https://github.com/PKUDigitalHealth/ECGFounder\n", "link": "http://arxiv.org/abs/2410.04133v4", "date": "2025-08-04", "relevancy": 2.253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Electrocardiogram%20Foundation%20Model%20Built%20on%20over%2010%20Million%0A%20%20Recordings%20with%20External%20Evaluation%20across%20Multiple%20Domains&body=Title%3A%20An%20Electrocardiogram%20Foundation%20Model%20Built%20on%20over%2010%20Million%0A%20%20Recordings%20with%20External%20Evaluation%20across%20Multiple%20Domains%0AAuthor%3A%20Jun%20Li%20and%20Aaron%20Aguirre%20and%20Junior%20Moura%20and%20Che%20Liu%20and%20Lanhai%20Zhong%20and%20Chenxi%20Sun%20and%20Gari%20Clifford%20and%20Brandon%20Westover%20and%20Shenda%20Hong%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20demonstrated%20significant%20potential%20in%20ECG%0Aanalysis%20and%20cardiovascular%20disease%20assessment.%20Recently%2C%20foundation%20models%0Ahave%20played%20a%20remarkable%20role%20in%20advancing%20medical%20AI.%20The%20development%20of%20an%0AECG%20foundation%20model%20holds%20the%20promise%20of%20elevating%20AI-ECG%20research%20to%20new%0Aheights.%20However%2C%20building%20such%20a%20model%20faces%20several%20challenges%2C%20including%0Ainsufficient%20database%20sample%20sizes%20and%20inadequate%20generalization%20across%0Amultiple%20domains.%20Additionally%2C%20there%20is%20a%20notable%20performance%20gap%20between%0Asingle-lead%20and%20multi-lead%20ECG%20analyses.%20We%20introduced%20an%20ECG%20Foundation%20Model%0A%28ECGFounder%29%2C%20a%20general-purpose%20model%20that%20leverages%20real-world%20ECG%20annotations%0Afrom%20cardiology%20experts%20to%20broaden%20the%20diagnostic%20capabilities%20of%20ECG%20analysis.%0AECGFounder%20was%20trained%20on%20over%2010%20million%20ECGs%20with%20150%20label%20categories%20from%0Athe%20Harvard-Emory%20ECG%20Database%2C%20enabling%20comprehensive%20cardiovascular%20disease%0Adiagnosis%20through%20ECG%20analysis.%20The%20model%20is%20designed%20to%20be%20both%20an%20effective%0Aout-of-the-box%20solution%2C%20and%20a%20to%20be%20fine-tunable%20for%20downstream%20tasks%2C%0Amaximizing%20usability.%20Importantly%2C%20we%20extended%20its%20application%20to%20lower%20rank%0AECGs%2C%20and%20arbitrary%20single-lead%20ECGs%20in%20particular.%20ECGFounder%20is%20applicable%20to%0Asupporting%20various%20downstream%20tasks%20in%20mobile%20monitoring%20scenarios.%0AExperimental%20results%20demonstrate%20that%20ECGFounder%20achieves%20expert-level%0Aperformance%20on%20internal%20validation%20sets%2C%20with%20AUROC%20exceeding%200.95%20for%20eighty%0Adiagnoses.%20It%20also%20shows%20strong%20classification%20performance%20and%20generalization%0Aacross%20various%20diagnoses%20on%20external%20validation%20sets.%20When%20fine-tuned%2C%0AECGFounder%20outperforms%20baseline%20models%20in%20demographic%20analysis%2C%20clinical%20event%0Adetection%2C%20and%20cross-modality%20cardiac%20rhythm%20diagnosis.%20The%20trained%20model%20and%0Adata%20will%20be%20publicly%20released%20upon%20publication%20through%20the%20bdsp.io.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/PKUDigitalHealth/ECGFounder%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04133v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Electrocardiogram%2520Foundation%2520Model%2520Built%2520on%2520over%252010%2520Million%250A%2520%2520Recordings%2520with%2520External%2520Evaluation%2520across%2520Multiple%2520Domains%26entry.906535625%3DJun%2520Li%2520and%2520Aaron%2520Aguirre%2520and%2520Junior%2520Moura%2520and%2520Che%2520Liu%2520and%2520Lanhai%2520Zhong%2520and%2520Chenxi%2520Sun%2520and%2520Gari%2520Clifford%2520and%2520Brandon%2520Westover%2520and%2520Shenda%2520Hong%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520demonstrated%2520significant%2520potential%2520in%2520ECG%250Aanalysis%2520and%2520cardiovascular%2520disease%2520assessment.%2520Recently%252C%2520foundation%2520models%250Ahave%2520played%2520a%2520remarkable%2520role%2520in%2520advancing%2520medical%2520AI.%2520The%2520development%2520of%2520an%250AECG%2520foundation%2520model%2520holds%2520the%2520promise%2520of%2520elevating%2520AI-ECG%2520research%2520to%2520new%250Aheights.%2520However%252C%2520building%2520such%2520a%2520model%2520faces%2520several%2520challenges%252C%2520including%250Ainsufficient%2520database%2520sample%2520sizes%2520and%2520inadequate%2520generalization%2520across%250Amultiple%2520domains.%2520Additionally%252C%2520there%2520is%2520a%2520notable%2520performance%2520gap%2520between%250Asingle-lead%2520and%2520multi-lead%2520ECG%2520analyses.%2520We%2520introduced%2520an%2520ECG%2520Foundation%2520Model%250A%2528ECGFounder%2529%252C%2520a%2520general-purpose%2520model%2520that%2520leverages%2520real-world%2520ECG%2520annotations%250Afrom%2520cardiology%2520experts%2520to%2520broaden%2520the%2520diagnostic%2520capabilities%2520of%2520ECG%2520analysis.%250AECGFounder%2520was%2520trained%2520on%2520over%252010%2520million%2520ECGs%2520with%2520150%2520label%2520categories%2520from%250Athe%2520Harvard-Emory%2520ECG%2520Database%252C%2520enabling%2520comprehensive%2520cardiovascular%2520disease%250Adiagnosis%2520through%2520ECG%2520analysis.%2520The%2520model%2520is%2520designed%2520to%2520be%2520both%2520an%2520effective%250Aout-of-the-box%2520solution%252C%2520and%2520a%2520to%2520be%2520fine-tunable%2520for%2520downstream%2520tasks%252C%250Amaximizing%2520usability.%2520Importantly%252C%2520we%2520extended%2520its%2520application%2520to%2520lower%2520rank%250AECGs%252C%2520and%2520arbitrary%2520single-lead%2520ECGs%2520in%2520particular.%2520ECGFounder%2520is%2520applicable%2520to%250Asupporting%2520various%2520downstream%2520tasks%2520in%2520mobile%2520monitoring%2520scenarios.%250AExperimental%2520results%2520demonstrate%2520that%2520ECGFounder%2520achieves%2520expert-level%250Aperformance%2520on%2520internal%2520validation%2520sets%252C%2520with%2520AUROC%2520exceeding%25200.95%2520for%2520eighty%250Adiagnoses.%2520It%2520also%2520shows%2520strong%2520classification%2520performance%2520and%2520generalization%250Aacross%2520various%2520diagnoses%2520on%2520external%2520validation%2520sets.%2520When%2520fine-tuned%252C%250AECGFounder%2520outperforms%2520baseline%2520models%2520in%2520demographic%2520analysis%252C%2520clinical%2520event%250Adetection%252C%2520and%2520cross-modality%2520cardiac%2520rhythm%2520diagnosis.%2520The%2520trained%2520model%2520and%250Adata%2520will%2520be%2520publicly%2520released%2520upon%2520publication%2520through%2520the%2520bdsp.io.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/PKUDigitalHealth/ECGFounder%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04133v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Electrocardiogram%20Foundation%20Model%20Built%20on%20over%2010%20Million%0A%20%20Recordings%20with%20External%20Evaluation%20across%20Multiple%20Domains&entry.906535625=Jun%20Li%20and%20Aaron%20Aguirre%20and%20Junior%20Moura%20and%20Che%20Liu%20and%20Lanhai%20Zhong%20and%20Chenxi%20Sun%20and%20Gari%20Clifford%20and%20Brandon%20Westover%20and%20Shenda%20Hong&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20demonstrated%20significant%20potential%20in%20ECG%0Aanalysis%20and%20cardiovascular%20disease%20assessment.%20Recently%2C%20foundation%20models%0Ahave%20played%20a%20remarkable%20role%20in%20advancing%20medical%20AI.%20The%20development%20of%20an%0AECG%20foundation%20model%20holds%20the%20promise%20of%20elevating%20AI-ECG%20research%20to%20new%0Aheights.%20However%2C%20building%20such%20a%20model%20faces%20several%20challenges%2C%20including%0Ainsufficient%20database%20sample%20sizes%20and%20inadequate%20generalization%20across%0Amultiple%20domains.%20Additionally%2C%20there%20is%20a%20notable%20performance%20gap%20between%0Asingle-lead%20and%20multi-lead%20ECG%20analyses.%20We%20introduced%20an%20ECG%20Foundation%20Model%0A%28ECGFounder%29%2C%20a%20general-purpose%20model%20that%20leverages%20real-world%20ECG%20annotations%0Afrom%20cardiology%20experts%20to%20broaden%20the%20diagnostic%20capabilities%20of%20ECG%20analysis.%0AECGFounder%20was%20trained%20on%20over%2010%20million%20ECGs%20with%20150%20label%20categories%20from%0Athe%20Harvard-Emory%20ECG%20Database%2C%20enabling%20comprehensive%20cardiovascular%20disease%0Adiagnosis%20through%20ECG%20analysis.%20The%20model%20is%20designed%20to%20be%20both%20an%20effective%0Aout-of-the-box%20solution%2C%20and%20a%20to%20be%20fine-tunable%20for%20downstream%20tasks%2C%0Amaximizing%20usability.%20Importantly%2C%20we%20extended%20its%20application%20to%20lower%20rank%0AECGs%2C%20and%20arbitrary%20single-lead%20ECGs%20in%20particular.%20ECGFounder%20is%20applicable%20to%0Asupporting%20various%20downstream%20tasks%20in%20mobile%20monitoring%20scenarios.%0AExperimental%20results%20demonstrate%20that%20ECGFounder%20achieves%20expert-level%0Aperformance%20on%20internal%20validation%20sets%2C%20with%20AUROC%20exceeding%200.95%20for%20eighty%0Adiagnoses.%20It%20also%20shows%20strong%20classification%20performance%20and%20generalization%0Aacross%20various%20diagnoses%20on%20external%20validation%20sets.%20When%20fine-tuned%2C%0AECGFounder%20outperforms%20baseline%20models%20in%20demographic%20analysis%2C%20clinical%20event%0Adetection%2C%20and%20cross-modality%20cardiac%20rhythm%20diagnosis.%20The%20trained%20model%20and%0Adata%20will%20be%20publicly%20released%20upon%20publication%20through%20the%20bdsp.io.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/PKUDigitalHealth/ECGFounder%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04133v4&entry.124074799=Read"},
{"title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and\n  Evaluation", "author": "Shuo Lu and Yanyin Chen and Wei Feng and Jiahao Fan and Fengheng Li and Zheng Zhang and Jingjing Lv and Junjie Shen and Ching Law and Jian Liang", "abstract": "  Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.\n", "link": "http://arxiv.org/abs/2508.02374v1", "date": "2025-08-04", "relevancy": 2.249, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6318}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5178}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-Layout%3A%20Integrating%20Human%20Feedback%20in%20Unified%20Layout%20Generation%20and%0A%20%20Evaluation&body=Title%3A%20Uni-Layout%3A%20Integrating%20Human%20Feedback%20in%20Unified%20Layout%20Generation%20and%0A%20%20Evaluation%0AAuthor%3A%20Shuo%20Lu%20and%20Yanyin%20Chen%20and%20Wei%20Feng%20and%20Jiahao%20Fan%20and%20Fengheng%20Li%20and%20Zheng%20Zhang%20and%20Jingjing%20Lv%20and%20Junjie%20Shen%20and%20Ching%20Law%20and%20Jian%20Liang%0AAbstract%3A%20%20%20Layout%20generation%20plays%20a%20crucial%20role%20in%20enhancing%20both%20user%20experience%20and%0Adesign%20efficiency.%20However%2C%20current%20approaches%20suffer%20from%20task-specific%0Ageneration%20capabilities%20and%20perceptually%20misaligned%20evaluation%20metrics%2C%20leading%0Ato%20limited%20applicability%20and%20ineffective%20measurement.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextit%7BUni-Layout%7D%2C%20a%20novel%20framework%20that%20achieves%20unified%20generation%2C%0Ahuman-mimicking%20evaluation%20and%20alignment%20between%20the%20two.%20For%20universal%0Ageneration%2C%20we%20incorporate%20various%20layout%20tasks%20into%20a%20single%20taxonomy%20and%0Adevelop%20a%20unified%20generator%20that%20handles%20background%20or%20element%20contents%0Aconstrained%20tasks%20via%20natural%20language%20prompts.%20To%20introduce%20human%20feedback%20for%0Athe%20effective%20evaluation%20of%20layouts%2C%20we%20build%20%5Ctextit%7BLayout-HF100k%7D%2C%20the%20first%0Alarge-scale%20human%20feedback%20dataset%20with%20100%2C000%20expertly%20annotated%20layouts.%0ABased%20on%20%5Ctextit%7BLayout-HF100k%7D%2C%20we%20introduce%20a%20human-mimicking%20evaluator%20that%0Aintegrates%20visual%20and%20geometric%20information%2C%20employing%20a%20Chain-of-Thought%0Amechanism%20to%20conduct%20qualitative%20assessments%20alongside%20a%20confidence%20estimation%0Amodule%20to%20yield%20quantitative%20measurements.%20For%20better%20alignment%20between%20the%0Agenerator%20and%20the%20evaluator%2C%20we%20integrate%20them%20into%20a%20cohesive%20system%20by%0Aadopting%20Dynamic-Margin%20Preference%20Optimization%20%28DMPO%29%2C%20which%20dynamically%0Aadjusts%20margins%20based%20on%20preference%20strength%20to%20better%20align%20with%20human%0Ajudgments.%20Extensive%20experiments%20show%20that%20%5Ctextit%7BUni-Layout%7D%20significantly%0Aoutperforms%20both%20task-specific%20and%20general-purpose%20methods.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/JD-GenX/Uni-Layout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-Layout%253A%2520Integrating%2520Human%2520Feedback%2520in%2520Unified%2520Layout%2520Generation%2520and%250A%2520%2520Evaluation%26entry.906535625%3DShuo%2520Lu%2520and%2520Yanyin%2520Chen%2520and%2520Wei%2520Feng%2520and%2520Jiahao%2520Fan%2520and%2520Fengheng%2520Li%2520and%2520Zheng%2520Zhang%2520and%2520Jingjing%2520Lv%2520and%2520Junjie%2520Shen%2520and%2520Ching%2520Law%2520and%2520Jian%2520Liang%26entry.1292438233%3D%2520%2520Layout%2520generation%2520plays%2520a%2520crucial%2520role%2520in%2520enhancing%2520both%2520user%2520experience%2520and%250Adesign%2520efficiency.%2520However%252C%2520current%2520approaches%2520suffer%2520from%2520task-specific%250Ageneration%2520capabilities%2520and%2520perceptually%2520misaligned%2520evaluation%2520metrics%252C%2520leading%250Ato%2520limited%2520applicability%2520and%2520ineffective%2520measurement.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Ctextit%257BUni-Layout%257D%252C%2520a%2520novel%2520framework%2520that%2520achieves%2520unified%2520generation%252C%250Ahuman-mimicking%2520evaluation%2520and%2520alignment%2520between%2520the%2520two.%2520For%2520universal%250Ageneration%252C%2520we%2520incorporate%2520various%2520layout%2520tasks%2520into%2520a%2520single%2520taxonomy%2520and%250Adevelop%2520a%2520unified%2520generator%2520that%2520handles%2520background%2520or%2520element%2520contents%250Aconstrained%2520tasks%2520via%2520natural%2520language%2520prompts.%2520To%2520introduce%2520human%2520feedback%2520for%250Athe%2520effective%2520evaluation%2520of%2520layouts%252C%2520we%2520build%2520%255Ctextit%257BLayout-HF100k%257D%252C%2520the%2520first%250Alarge-scale%2520human%2520feedback%2520dataset%2520with%2520100%252C000%2520expertly%2520annotated%2520layouts.%250ABased%2520on%2520%255Ctextit%257BLayout-HF100k%257D%252C%2520we%2520introduce%2520a%2520human-mimicking%2520evaluator%2520that%250Aintegrates%2520visual%2520and%2520geometric%2520information%252C%2520employing%2520a%2520Chain-of-Thought%250Amechanism%2520to%2520conduct%2520qualitative%2520assessments%2520alongside%2520a%2520confidence%2520estimation%250Amodule%2520to%2520yield%2520quantitative%2520measurements.%2520For%2520better%2520alignment%2520between%2520the%250Agenerator%2520and%2520the%2520evaluator%252C%2520we%2520integrate%2520them%2520into%2520a%2520cohesive%2520system%2520by%250Aadopting%2520Dynamic-Margin%2520Preference%2520Optimization%2520%2528DMPO%2529%252C%2520which%2520dynamically%250Aadjusts%2520margins%2520based%2520on%2520preference%2520strength%2520to%2520better%2520align%2520with%2520human%250Ajudgments.%2520Extensive%2520experiments%2520show%2520that%2520%255Ctextit%257BUni-Layout%257D%2520significantly%250Aoutperforms%2520both%2520task-specific%2520and%2520general-purpose%2520methods.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/JD-GenX/Uni-Layout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-Layout%3A%20Integrating%20Human%20Feedback%20in%20Unified%20Layout%20Generation%20and%0A%20%20Evaluation&entry.906535625=Shuo%20Lu%20and%20Yanyin%20Chen%20and%20Wei%20Feng%20and%20Jiahao%20Fan%20and%20Fengheng%20Li%20and%20Zheng%20Zhang%20and%20Jingjing%20Lv%20and%20Junjie%20Shen%20and%20Ching%20Law%20and%20Jian%20Liang&entry.1292438233=%20%20Layout%20generation%20plays%20a%20crucial%20role%20in%20enhancing%20both%20user%20experience%20and%0Adesign%20efficiency.%20However%2C%20current%20approaches%20suffer%20from%20task-specific%0Ageneration%20capabilities%20and%20perceptually%20misaligned%20evaluation%20metrics%2C%20leading%0Ato%20limited%20applicability%20and%20ineffective%20measurement.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextit%7BUni-Layout%7D%2C%20a%20novel%20framework%20that%20achieves%20unified%20generation%2C%0Ahuman-mimicking%20evaluation%20and%20alignment%20between%20the%20two.%20For%20universal%0Ageneration%2C%20we%20incorporate%20various%20layout%20tasks%20into%20a%20single%20taxonomy%20and%0Adevelop%20a%20unified%20generator%20that%20handles%20background%20or%20element%20contents%0Aconstrained%20tasks%20via%20natural%20language%20prompts.%20To%20introduce%20human%20feedback%20for%0Athe%20effective%20evaluation%20of%20layouts%2C%20we%20build%20%5Ctextit%7BLayout-HF100k%7D%2C%20the%20first%0Alarge-scale%20human%20feedback%20dataset%20with%20100%2C000%20expertly%20annotated%20layouts.%0ABased%20on%20%5Ctextit%7BLayout-HF100k%7D%2C%20we%20introduce%20a%20human-mimicking%20evaluator%20that%0Aintegrates%20visual%20and%20geometric%20information%2C%20employing%20a%20Chain-of-Thought%0Amechanism%20to%20conduct%20qualitative%20assessments%20alongside%20a%20confidence%20estimation%0Amodule%20to%20yield%20quantitative%20measurements.%20For%20better%20alignment%20between%20the%0Agenerator%20and%20the%20evaluator%2C%20we%20integrate%20them%20into%20a%20cohesive%20system%20by%0Aadopting%20Dynamic-Margin%20Preference%20Optimization%20%28DMPO%29%2C%20which%20dynamically%0Aadjusts%20margins%20based%20on%20preference%20strength%20to%20better%20align%20with%20human%0Ajudgments.%20Extensive%20experiments%20show%20that%20%5Ctextit%7BUni-Layout%7D%20significantly%0Aoutperforms%20both%20task-specific%20and%20general-purpose%20methods.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/JD-GenX/Uni-Layout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02374v1&entry.124074799=Read"},
{"title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search", "author": "Fan Hu and Zijie Xin and Xirong Li", "abstract": "  Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.\n", "link": "http://arxiv.org/abs/2508.02340v1", "date": "2025-08-04", "relevancy": 2.2471, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Partially-Decorrelated%20Common%20Spaces%20for%20Ad-hoc%20Video%20Search&body=Title%3A%20Learning%20Partially-Decorrelated%20Common%20Spaces%20for%20Ad-hoc%20Video%20Search%0AAuthor%3A%20Fan%20Hu%20and%20Zijie%20Xin%20and%20Xirong%20Li%0AAbstract%3A%20%20%20Ad-hoc%20Video%20Search%20%28AVS%29%20involves%20using%20a%20textual%20query%20to%20search%20for%0Amultiple%20relevant%20videos%20in%20a%20large%20collection%20of%20unlabeled%20short%20videos.%20The%0Amain%20challenge%20of%20AVS%20is%20the%20visual%20diversity%20of%20relevant%20videos.%20A%20simple%0Aquery%20such%20as%20%22Find%20shots%20of%20a%20man%20and%20a%20woman%20dancing%20together%20indoors%22%20can%0Aspan%20a%20multitude%20of%20environments%2C%20from%20brightly%20lit%20halls%20and%20shadowy%20bars%20to%0Adance%20scenes%20in%20black-and-white%20animations.%20It%20is%20therefore%20essential%20to%0Aretrieve%20relevant%20videos%20as%20comprehensively%20as%20possible.%20Current%20solutions%20for%0Athe%20AVS%20task%20primarily%20fuse%20multiple%20features%20into%20one%20or%20more%20common%20spaces%2C%0Ayet%20overlook%20the%20need%20for%20diverse%20spaces.%20To%20fully%20exploit%20the%20expressive%0Acapability%20of%20individual%20features%2C%20we%20propose%20LPD%2C%20short%20for%20Learning%20Partially%0ADecorrelated%20common%20spaces.%20LPD%20incorporates%20two%20key%20innovations%3A%0Afeature-specific%20common%20space%20construction%20and%20the%20de-correlation%20loss.%0ASpecifically%2C%20LPD%20learns%20a%20separate%20common%20space%20for%20each%20video%20and%20text%0Afeature%2C%20and%20employs%20de-correlation%20loss%20to%20diversify%20the%20ordering%20of%20negative%0Asamples%20across%20different%20spaces.%20To%20enhance%20the%20consistency%20of%20multi-space%0Aconvergence%2C%20we%20designed%20an%20entropy-based%20fair%20multi-space%20triplet%20ranking%0Aloss.%20Extensive%20experiments%20on%20the%20TRECVID%20AVS%20benchmarks%20%282016-2023%29%20justify%0Athe%20effectiveness%20of%20LPD.%20Moreover%2C%20diversity%20visualizations%20of%20LPD%27s%20spaces%0Ahighlight%20its%20ability%20to%20enhance%20result%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Partially-Decorrelated%2520Common%2520Spaces%2520for%2520Ad-hoc%2520Video%2520Search%26entry.906535625%3DFan%2520Hu%2520and%2520Zijie%2520Xin%2520and%2520Xirong%2520Li%26entry.1292438233%3D%2520%2520Ad-hoc%2520Video%2520Search%2520%2528AVS%2529%2520involves%2520using%2520a%2520textual%2520query%2520to%2520search%2520for%250Amultiple%2520relevant%2520videos%2520in%2520a%2520large%2520collection%2520of%2520unlabeled%2520short%2520videos.%2520The%250Amain%2520challenge%2520of%2520AVS%2520is%2520the%2520visual%2520diversity%2520of%2520relevant%2520videos.%2520A%2520simple%250Aquery%2520such%2520as%2520%2522Find%2520shots%2520of%2520a%2520man%2520and%2520a%2520woman%2520dancing%2520together%2520indoors%2522%2520can%250Aspan%2520a%2520multitude%2520of%2520environments%252C%2520from%2520brightly%2520lit%2520halls%2520and%2520shadowy%2520bars%2520to%250Adance%2520scenes%2520in%2520black-and-white%2520animations.%2520It%2520is%2520therefore%2520essential%2520to%250Aretrieve%2520relevant%2520videos%2520as%2520comprehensively%2520as%2520possible.%2520Current%2520solutions%2520for%250Athe%2520AVS%2520task%2520primarily%2520fuse%2520multiple%2520features%2520into%2520one%2520or%2520more%2520common%2520spaces%252C%250Ayet%2520overlook%2520the%2520need%2520for%2520diverse%2520spaces.%2520To%2520fully%2520exploit%2520the%2520expressive%250Acapability%2520of%2520individual%2520features%252C%2520we%2520propose%2520LPD%252C%2520short%2520for%2520Learning%2520Partially%250ADecorrelated%2520common%2520spaces.%2520LPD%2520incorporates%2520two%2520key%2520innovations%253A%250Afeature-specific%2520common%2520space%2520construction%2520and%2520the%2520de-correlation%2520loss.%250ASpecifically%252C%2520LPD%2520learns%2520a%2520separate%2520common%2520space%2520for%2520each%2520video%2520and%2520text%250Afeature%252C%2520and%2520employs%2520de-correlation%2520loss%2520to%2520diversify%2520the%2520ordering%2520of%2520negative%250Asamples%2520across%2520different%2520spaces.%2520To%2520enhance%2520the%2520consistency%2520of%2520multi-space%250Aconvergence%252C%2520we%2520designed%2520an%2520entropy-based%2520fair%2520multi-space%2520triplet%2520ranking%250Aloss.%2520Extensive%2520experiments%2520on%2520the%2520TRECVID%2520AVS%2520benchmarks%2520%25282016-2023%2529%2520justify%250Athe%2520effectiveness%2520of%2520LPD.%2520Moreover%252C%2520diversity%2520visualizations%2520of%2520LPD%2527s%2520spaces%250Ahighlight%2520its%2520ability%2520to%2520enhance%2520result%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Partially-Decorrelated%20Common%20Spaces%20for%20Ad-hoc%20Video%20Search&entry.906535625=Fan%20Hu%20and%20Zijie%20Xin%20and%20Xirong%20Li&entry.1292438233=%20%20Ad-hoc%20Video%20Search%20%28AVS%29%20involves%20using%20a%20textual%20query%20to%20search%20for%0Amultiple%20relevant%20videos%20in%20a%20large%20collection%20of%20unlabeled%20short%20videos.%20The%0Amain%20challenge%20of%20AVS%20is%20the%20visual%20diversity%20of%20relevant%20videos.%20A%20simple%0Aquery%20such%20as%20%22Find%20shots%20of%20a%20man%20and%20a%20woman%20dancing%20together%20indoors%22%20can%0Aspan%20a%20multitude%20of%20environments%2C%20from%20brightly%20lit%20halls%20and%20shadowy%20bars%20to%0Adance%20scenes%20in%20black-and-white%20animations.%20It%20is%20therefore%20essential%20to%0Aretrieve%20relevant%20videos%20as%20comprehensively%20as%20possible.%20Current%20solutions%20for%0Athe%20AVS%20task%20primarily%20fuse%20multiple%20features%20into%20one%20or%20more%20common%20spaces%2C%0Ayet%20overlook%20the%20need%20for%20diverse%20spaces.%20To%20fully%20exploit%20the%20expressive%0Acapability%20of%20individual%20features%2C%20we%20propose%20LPD%2C%20short%20for%20Learning%20Partially%0ADecorrelated%20common%20spaces.%20LPD%20incorporates%20two%20key%20innovations%3A%0Afeature-specific%20common%20space%20construction%20and%20the%20de-correlation%20loss.%0ASpecifically%2C%20LPD%20learns%20a%20separate%20common%20space%20for%20each%20video%20and%20text%0Afeature%2C%20and%20employs%20de-correlation%20loss%20to%20diversify%20the%20ordering%20of%20negative%0Asamples%20across%20different%20spaces.%20To%20enhance%20the%20consistency%20of%20multi-space%0Aconvergence%2C%20we%20designed%20an%20entropy-based%20fair%20multi-space%20triplet%20ranking%0Aloss.%20Extensive%20experiments%20on%20the%20TRECVID%20AVS%20benchmarks%20%282016-2023%29%20justify%0Athe%20effectiveness%20of%20LPD.%20Moreover%2C%20diversity%20visualizations%20of%20LPD%27s%20spaces%0Ahighlight%20its%20ability%20to%20enhance%20result%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02340v1&entry.124074799=Read"},
{"title": "Engagement Prediction of Short Videos with Large Multimodal Models", "author": "Wei Sun and Linhan Cao and Yuqin Cao and Weixia Zhang and Wen Wen and Kaiwei Zhang and Zijian Chen and Fangfang Lu and Xiongkuo Min and Guangtao Zhai", "abstract": "  The rapid proliferation of user-generated content (UGC) on short-form video\nplatforms has made video engagement prediction increasingly important for\noptimizing recommendation systems and guiding content creation. However, this\ntask remains challenging due to the complex interplay of factors such as\nsemantic content, visual quality, audio characteristics, and user background.\nPrior studies have leveraged various types of features from different\nmodalities, such as visual quality, semantic content, background sound, etc.,\nbut often struggle to effectively model their cross-feature and cross-modality\ninteractions. In this work, we empirically investigate the potential of large\nmultimodal models (LMMs) for video engagement prediction. We adopt two\nrepresentative LMMs: VideoLLaMA2, which integrates audio, visual, and language\nmodalities, and Qwen2.5-VL, which models only visual and language modalities.\nSpecifically, VideoLLaMA2 jointly processes key video frames, text-based\nmetadata, and background sound, while Qwen2.5-VL utilizes only key video frames\nand text-based metadata. Trained on the SnapUGC dataset, both models\ndemonstrate competitive performance against state-of-the-art baselines,\nshowcasing the effectiveness of LMMs in engagement prediction. Notably,\nVideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of\naudio features in engagement prediction. By ensembling two types of models, our\nmethod achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on\nshort-form video engagement prediction. The code is available at\nhttps://github.com/sunwei925/LMM-EVQA.git.\n", "link": "http://arxiv.org/abs/2508.02516v1", "date": "2025-08-04", "relevancy": 2.2412, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5769}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Engagement%20Prediction%20of%20Short%20Videos%20with%20Large%20Multimodal%20Models&body=Title%3A%20Engagement%20Prediction%20of%20Short%20Videos%20with%20Large%20Multimodal%20Models%0AAuthor%3A%20Wei%20Sun%20and%20Linhan%20Cao%20and%20Yuqin%20Cao%20and%20Weixia%20Zhang%20and%20Wen%20Wen%20and%20Kaiwei%20Zhang%20and%20Zijian%20Chen%20and%20Fangfang%20Lu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20user-generated%20content%20%28UGC%29%20on%20short-form%20video%0Aplatforms%20has%20made%20video%20engagement%20prediction%20increasingly%20important%20for%0Aoptimizing%20recommendation%20systems%20and%20guiding%20content%20creation.%20However%2C%20this%0Atask%20remains%20challenging%20due%20to%20the%20complex%20interplay%20of%20factors%20such%20as%0Asemantic%20content%2C%20visual%20quality%2C%20audio%20characteristics%2C%20and%20user%20background.%0APrior%20studies%20have%20leveraged%20various%20types%20of%20features%20from%20different%0Amodalities%2C%20such%20as%20visual%20quality%2C%20semantic%20content%2C%20background%20sound%2C%20etc.%2C%0Abut%20often%20struggle%20to%20effectively%20model%20their%20cross-feature%20and%20cross-modality%0Ainteractions.%20In%20this%20work%2C%20we%20empirically%20investigate%20the%20potential%20of%20large%0Amultimodal%20models%20%28LMMs%29%20for%20video%20engagement%20prediction.%20We%20adopt%20two%0Arepresentative%20LMMs%3A%20VideoLLaMA2%2C%20which%20integrates%20audio%2C%20visual%2C%20and%20language%0Amodalities%2C%20and%20Qwen2.5-VL%2C%20which%20models%20only%20visual%20and%20language%20modalities.%0ASpecifically%2C%20VideoLLaMA2%20jointly%20processes%20key%20video%20frames%2C%20text-based%0Ametadata%2C%20and%20background%20sound%2C%20while%20Qwen2.5-VL%20utilizes%20only%20key%20video%20frames%0Aand%20text-based%20metadata.%20Trained%20on%20the%20SnapUGC%20dataset%2C%20both%20models%0Ademonstrate%20competitive%20performance%20against%20state-of-the-art%20baselines%2C%0Ashowcasing%20the%20effectiveness%20of%20LMMs%20in%20engagement%20prediction.%20Notably%2C%0AVideoLLaMA2%20consistently%20outperforms%20Qwen2.5-VL%2C%20highlighting%20the%20importance%20of%0Aaudio%20features%20in%20engagement%20prediction.%20By%20ensembling%20two%20types%20of%20models%2C%20our%0Amethod%20achieves%20first%20place%20in%20the%20ICCV%20VQualA%202025%20EVQA-SnapUGC%20Challenge%20on%0Ashort-form%20video%20engagement%20prediction.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunwei925/LMM-EVQA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngagement%2520Prediction%2520of%2520Short%2520Videos%2520with%2520Large%2520Multimodal%2520Models%26entry.906535625%3DWei%2520Sun%2520and%2520Linhan%2520Cao%2520and%2520Yuqin%2520Cao%2520and%2520Weixia%2520Zhang%2520and%2520Wen%2520Wen%2520and%2520Kaiwei%2520Zhang%2520and%2520Zijian%2520Chen%2520and%2520Fangfang%2520Lu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520user-generated%2520content%2520%2528UGC%2529%2520on%2520short-form%2520video%250Aplatforms%2520has%2520made%2520video%2520engagement%2520prediction%2520increasingly%2520important%2520for%250Aoptimizing%2520recommendation%2520systems%2520and%2520guiding%2520content%2520creation.%2520However%252C%2520this%250Atask%2520remains%2520challenging%2520due%2520to%2520the%2520complex%2520interplay%2520of%2520factors%2520such%2520as%250Asemantic%2520content%252C%2520visual%2520quality%252C%2520audio%2520characteristics%252C%2520and%2520user%2520background.%250APrior%2520studies%2520have%2520leveraged%2520various%2520types%2520of%2520features%2520from%2520different%250Amodalities%252C%2520such%2520as%2520visual%2520quality%252C%2520semantic%2520content%252C%2520background%2520sound%252C%2520etc.%252C%250Abut%2520often%2520struggle%2520to%2520effectively%2520model%2520their%2520cross-feature%2520and%2520cross-modality%250Ainteractions.%2520In%2520this%2520work%252C%2520we%2520empirically%2520investigate%2520the%2520potential%2520of%2520large%250Amultimodal%2520models%2520%2528LMMs%2529%2520for%2520video%2520engagement%2520prediction.%2520We%2520adopt%2520two%250Arepresentative%2520LMMs%253A%2520VideoLLaMA2%252C%2520which%2520integrates%2520audio%252C%2520visual%252C%2520and%2520language%250Amodalities%252C%2520and%2520Qwen2.5-VL%252C%2520which%2520models%2520only%2520visual%2520and%2520language%2520modalities.%250ASpecifically%252C%2520VideoLLaMA2%2520jointly%2520processes%2520key%2520video%2520frames%252C%2520text-based%250Ametadata%252C%2520and%2520background%2520sound%252C%2520while%2520Qwen2.5-VL%2520utilizes%2520only%2520key%2520video%2520frames%250Aand%2520text-based%2520metadata.%2520Trained%2520on%2520the%2520SnapUGC%2520dataset%252C%2520both%2520models%250Ademonstrate%2520competitive%2520performance%2520against%2520state-of-the-art%2520baselines%252C%250Ashowcasing%2520the%2520effectiveness%2520of%2520LMMs%2520in%2520engagement%2520prediction.%2520Notably%252C%250AVideoLLaMA2%2520consistently%2520outperforms%2520Qwen2.5-VL%252C%2520highlighting%2520the%2520importance%2520of%250Aaudio%2520features%2520in%2520engagement%2520prediction.%2520By%2520ensembling%2520two%2520types%2520of%2520models%252C%2520our%250Amethod%2520achieves%2520first%2520place%2520in%2520the%2520ICCV%2520VQualA%25202025%2520EVQA-SnapUGC%2520Challenge%2520on%250Ashort-form%2520video%2520engagement%2520prediction.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sunwei925/LMM-EVQA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Engagement%20Prediction%20of%20Short%20Videos%20with%20Large%20Multimodal%20Models&entry.906535625=Wei%20Sun%20and%20Linhan%20Cao%20and%20Yuqin%20Cao%20and%20Weixia%20Zhang%20and%20Wen%20Wen%20and%20Kaiwei%20Zhang%20and%20Zijian%20Chen%20and%20Fangfang%20Lu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20user-generated%20content%20%28UGC%29%20on%20short-form%20video%0Aplatforms%20has%20made%20video%20engagement%20prediction%20increasingly%20important%20for%0Aoptimizing%20recommendation%20systems%20and%20guiding%20content%20creation.%20However%2C%20this%0Atask%20remains%20challenging%20due%20to%20the%20complex%20interplay%20of%20factors%20such%20as%0Asemantic%20content%2C%20visual%20quality%2C%20audio%20characteristics%2C%20and%20user%20background.%0APrior%20studies%20have%20leveraged%20various%20types%20of%20features%20from%20different%0Amodalities%2C%20such%20as%20visual%20quality%2C%20semantic%20content%2C%20background%20sound%2C%20etc.%2C%0Abut%20often%20struggle%20to%20effectively%20model%20their%20cross-feature%20and%20cross-modality%0Ainteractions.%20In%20this%20work%2C%20we%20empirically%20investigate%20the%20potential%20of%20large%0Amultimodal%20models%20%28LMMs%29%20for%20video%20engagement%20prediction.%20We%20adopt%20two%0Arepresentative%20LMMs%3A%20VideoLLaMA2%2C%20which%20integrates%20audio%2C%20visual%2C%20and%20language%0Amodalities%2C%20and%20Qwen2.5-VL%2C%20which%20models%20only%20visual%20and%20language%20modalities.%0ASpecifically%2C%20VideoLLaMA2%20jointly%20processes%20key%20video%20frames%2C%20text-based%0Ametadata%2C%20and%20background%20sound%2C%20while%20Qwen2.5-VL%20utilizes%20only%20key%20video%20frames%0Aand%20text-based%20metadata.%20Trained%20on%20the%20SnapUGC%20dataset%2C%20both%20models%0Ademonstrate%20competitive%20performance%20against%20state-of-the-art%20baselines%2C%0Ashowcasing%20the%20effectiveness%20of%20LMMs%20in%20engagement%20prediction.%20Notably%2C%0AVideoLLaMA2%20consistently%20outperforms%20Qwen2.5-VL%2C%20highlighting%20the%20importance%20of%0Aaudio%20features%20in%20engagement%20prediction.%20By%20ensembling%20two%20types%20of%20models%2C%20our%0Amethod%20achieves%20first%20place%20in%20the%20ICCV%20VQualA%202025%20EVQA-SnapUGC%20Challenge%20on%0Ashort-form%20video%20engagement%20prediction.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunwei925/LMM-EVQA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02516v1&entry.124074799=Read"},
{"title": "Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via\n  Viseme-Guided Rendering", "author": "Xu Wang and Shengeng Tang and Fei Wang and Lechao Cheng and Dan Guo and Feng Xue and Richang Hong", "abstract": "  Generating semantically coherent and visually accurate talking faces requires\nbridging the gap between linguistic meaning and facial articulation. Although\naudio-driven methods remain prevalent, their reliance on high-quality paired\naudio visual data and the inherent ambiguity in mapping acoustics to lip motion\npose significant challenges in terms of scalability and robustness. To address\nthese issues, we propose Text2Lip, a viseme-centric framework that constructs\nan interpretable phonetic-visual bridge by embedding textual input into\nstructured viseme sequences. These mid-level units serve as a linguistically\ngrounded prior for lip motion prediction. Furthermore, we design a progressive\nviseme-audio replacement strategy based on curriculum learning, enabling the\nmodel to gradually transition from real audio to pseudo-audio reconstructed\nfrom enhanced viseme features via cross-modal attention. This allows for robust\ngeneration in both audio-present and audio-free scenarios. Finally, a\nlandmark-guided renderer synthesizes photorealistic facial videos with accurate\nlip synchronization. Extensive evaluations show that Text2Lip outperforms\nexisting approaches in semantic fidelity, visual realism, and modality\nrobustness, establishing a new paradigm for controllable and flexible talking\nface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.\n", "link": "http://arxiv.org/abs/2508.02362v1", "date": "2025-08-04", "relevancy": 2.2335, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5677}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5598}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Lip%3A%20Progressive%20Lip-Synced%20Talking%20Face%20Generation%20from%20Text%20via%0A%20%20Viseme-Guided%20Rendering&body=Title%3A%20Text2Lip%3A%20Progressive%20Lip-Synced%20Talking%20Face%20Generation%20from%20Text%20via%0A%20%20Viseme-Guided%20Rendering%0AAuthor%3A%20Xu%20Wang%20and%20Shengeng%20Tang%20and%20Fei%20Wang%20and%20Lechao%20Cheng%20and%20Dan%20Guo%20and%20Feng%20Xue%20and%20Richang%20Hong%0AAbstract%3A%20%20%20Generating%20semantically%20coherent%20and%20visually%20accurate%20talking%20faces%20requires%0Abridging%20the%20gap%20between%20linguistic%20meaning%20and%20facial%20articulation.%20Although%0Aaudio-driven%20methods%20remain%20prevalent%2C%20their%20reliance%20on%20high-quality%20paired%0Aaudio%20visual%20data%20and%20the%20inherent%20ambiguity%20in%20mapping%20acoustics%20to%20lip%20motion%0Apose%20significant%20challenges%20in%20terms%20of%20scalability%20and%20robustness.%20To%20address%0Athese%20issues%2C%20we%20propose%20Text2Lip%2C%20a%20viseme-centric%20framework%20that%20constructs%0Aan%20interpretable%20phonetic-visual%20bridge%20by%20embedding%20textual%20input%20into%0Astructured%20viseme%20sequences.%20These%20mid-level%20units%20serve%20as%20a%20linguistically%0Agrounded%20prior%20for%20lip%20motion%20prediction.%20Furthermore%2C%20we%20design%20a%20progressive%0Aviseme-audio%20replacement%20strategy%20based%20on%20curriculum%20learning%2C%20enabling%20the%0Amodel%20to%20gradually%20transition%20from%20real%20audio%20to%20pseudo-audio%20reconstructed%0Afrom%20enhanced%20viseme%20features%20via%20cross-modal%20attention.%20This%20allows%20for%20robust%0Ageneration%20in%20both%20audio-present%20and%20audio-free%20scenarios.%20Finally%2C%20a%0Alandmark-guided%20renderer%20synthesizes%20photorealistic%20facial%20videos%20with%20accurate%0Alip%20synchronization.%20Extensive%20evaluations%20show%20that%20Text2Lip%20outperforms%0Aexisting%20approaches%20in%20semantic%20fidelity%2C%20visual%20realism%2C%20and%20modality%0Arobustness%2C%20establishing%20a%20new%20paradigm%20for%20controllable%20and%20flexible%20talking%0Aface%20generation.%20Our%20project%20homepage%20is%20https%3A//plyon1.github.io/Text2Lip/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Lip%253A%2520Progressive%2520Lip-Synced%2520Talking%2520Face%2520Generation%2520from%2520Text%2520via%250A%2520%2520Viseme-Guided%2520Rendering%26entry.906535625%3DXu%2520Wang%2520and%2520Shengeng%2520Tang%2520and%2520Fei%2520Wang%2520and%2520Lechao%2520Cheng%2520and%2520Dan%2520Guo%2520and%2520Feng%2520Xue%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520Generating%2520semantically%2520coherent%2520and%2520visually%2520accurate%2520talking%2520faces%2520requires%250Abridging%2520the%2520gap%2520between%2520linguistic%2520meaning%2520and%2520facial%2520articulation.%2520Although%250Aaudio-driven%2520methods%2520remain%2520prevalent%252C%2520their%2520reliance%2520on%2520high-quality%2520paired%250Aaudio%2520visual%2520data%2520and%2520the%2520inherent%2520ambiguity%2520in%2520mapping%2520acoustics%2520to%2520lip%2520motion%250Apose%2520significant%2520challenges%2520in%2520terms%2520of%2520scalability%2520and%2520robustness.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520Text2Lip%252C%2520a%2520viseme-centric%2520framework%2520that%2520constructs%250Aan%2520interpretable%2520phonetic-visual%2520bridge%2520by%2520embedding%2520textual%2520input%2520into%250Astructured%2520viseme%2520sequences.%2520These%2520mid-level%2520units%2520serve%2520as%2520a%2520linguistically%250Agrounded%2520prior%2520for%2520lip%2520motion%2520prediction.%2520Furthermore%252C%2520we%2520design%2520a%2520progressive%250Aviseme-audio%2520replacement%2520strategy%2520based%2520on%2520curriculum%2520learning%252C%2520enabling%2520the%250Amodel%2520to%2520gradually%2520transition%2520from%2520real%2520audio%2520to%2520pseudo-audio%2520reconstructed%250Afrom%2520enhanced%2520viseme%2520features%2520via%2520cross-modal%2520attention.%2520This%2520allows%2520for%2520robust%250Ageneration%2520in%2520both%2520audio-present%2520and%2520audio-free%2520scenarios.%2520Finally%252C%2520a%250Alandmark-guided%2520renderer%2520synthesizes%2520photorealistic%2520facial%2520videos%2520with%2520accurate%250Alip%2520synchronization.%2520Extensive%2520evaluations%2520show%2520that%2520Text2Lip%2520outperforms%250Aexisting%2520approaches%2520in%2520semantic%2520fidelity%252C%2520visual%2520realism%252C%2520and%2520modality%250Arobustness%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520controllable%2520and%2520flexible%2520talking%250Aface%2520generation.%2520Our%2520project%2520homepage%2520is%2520https%253A//plyon1.github.io/Text2Lip/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Lip%3A%20Progressive%20Lip-Synced%20Talking%20Face%20Generation%20from%20Text%20via%0A%20%20Viseme-Guided%20Rendering&entry.906535625=Xu%20Wang%20and%20Shengeng%20Tang%20and%20Fei%20Wang%20and%20Lechao%20Cheng%20and%20Dan%20Guo%20and%20Feng%20Xue%20and%20Richang%20Hong&entry.1292438233=%20%20Generating%20semantically%20coherent%20and%20visually%20accurate%20talking%20faces%20requires%0Abridging%20the%20gap%20between%20linguistic%20meaning%20and%20facial%20articulation.%20Although%0Aaudio-driven%20methods%20remain%20prevalent%2C%20their%20reliance%20on%20high-quality%20paired%0Aaudio%20visual%20data%20and%20the%20inherent%20ambiguity%20in%20mapping%20acoustics%20to%20lip%20motion%0Apose%20significant%20challenges%20in%20terms%20of%20scalability%20and%20robustness.%20To%20address%0Athese%20issues%2C%20we%20propose%20Text2Lip%2C%20a%20viseme-centric%20framework%20that%20constructs%0Aan%20interpretable%20phonetic-visual%20bridge%20by%20embedding%20textual%20input%20into%0Astructured%20viseme%20sequences.%20These%20mid-level%20units%20serve%20as%20a%20linguistically%0Agrounded%20prior%20for%20lip%20motion%20prediction.%20Furthermore%2C%20we%20design%20a%20progressive%0Aviseme-audio%20replacement%20strategy%20based%20on%20curriculum%20learning%2C%20enabling%20the%0Amodel%20to%20gradually%20transition%20from%20real%20audio%20to%20pseudo-audio%20reconstructed%0Afrom%20enhanced%20viseme%20features%20via%20cross-modal%20attention.%20This%20allows%20for%20robust%0Ageneration%20in%20both%20audio-present%20and%20audio-free%20scenarios.%20Finally%2C%20a%0Alandmark-guided%20renderer%20synthesizes%20photorealistic%20facial%20videos%20with%20accurate%0Alip%20synchronization.%20Extensive%20evaluations%20show%20that%20Text2Lip%20outperforms%0Aexisting%20approaches%20in%20semantic%20fidelity%2C%20visual%20realism%2C%20and%20modality%0Arobustness%2C%20establishing%20a%20new%20paradigm%20for%20controllable%20and%20flexible%20talking%0Aface%20generation.%20Our%20project%20homepage%20is%20https%3A//plyon1.github.io/Text2Lip/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02362v1&entry.124074799=Read"},
{"title": "Test-time Prompt Intervention", "author": "Chenxu Yang and Qingyi Si and Mz Dai and Dingyu Yao and Mingyu Zheng and Minghui Chen and Zheng Lin and Weiping Wang", "abstract": "  Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.\n", "link": "http://arxiv.org/abs/2508.02511v1", "date": "2025-08-04", "relevancy": 2.2284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Prompt%20Intervention&body=Title%3A%20Test-time%20Prompt%20Intervention%0AAuthor%3A%20Chenxu%20Yang%20and%20Qingyi%20Si%20and%20Mz%20Dai%20and%20Dingyu%20Yao%20and%20Mingyu%20Zheng%20and%20Minghui%20Chen%20and%20Zheng%20Lin%20and%20Weiping%20Wang%0AAbstract%3A%20%20%20Test-time%20compute%20has%20led%20to%20remarkable%20success%20in%20the%20large%20language%20model%0A%28LLM%29%20community%2C%20particularly%20for%20complex%20tasks%2C%20where%20longer%20chains%20of%20thought%0A%28CoTs%29%20are%20generated%20to%20enhance%20reasoning%20capabilities.%20However%2C%20growing%0Aevidence%20reveals%20that%20such%20reasoning%20models%20often%20produce%20CoTs%20plagued%20by%0Aexcessive%20redundancy%2C%20including%20unnecessary%20verification%20steps%20and%20repetitive%0Areasoning%20shifts.%20The%20root%20cause%20lies%20in%20post-training%20of%20them%20that%20overly%20rely%0Aon%20outcome%20reward%20paradigms%2C%20as%20the%20data%20of%20process%20reward%20paradigms%2C%20which%0Aregulate%20intermediate%20reasoning%20steps%2C%20is%20difficult%20to%20construct%20at%20scale.%20To%0Aaddress%20this%2C%20we%20propose%20PI%2C%20a%20novel%20framework%20for%20Test-time%20Prompt%0AIntervention.%20PI%20provides%20an%20interface%20to%20dynamically%20guide%20and%20regulate%0Areasoning%20paths%20during%20inference%20through%20timely%20%28When%20module%29%20and%20proper%20%28How%0Amodule%29%20interventions%20and%20post-intervention%20sampling%20%28Which%20module%29.%20This%0Aallows%20human%20problem-solving%20expertise%20and%20cognitive%20science%20principles%20to%20be%0Aseamlessly%20integrated%20into%20LLMs%27%20reasoning%20processes%2C%20enhancing%20controllability%0Aand%20interpretability.%20Extensive%20experiments%20across%20multiple%20models%20and%20datasets%0Ademonstrate%20that%20PI%20significantly%20shortens%20CoTs%20while%20reducing%20hallucination%2C%0Ayielding%20more%20concise%20and%20reliable%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Prompt%2520Intervention%26entry.906535625%3DChenxu%2520Yang%2520and%2520Qingyi%2520Si%2520and%2520Mz%2520Dai%2520and%2520Dingyu%2520Yao%2520and%2520Mingyu%2520Zheng%2520and%2520Minghui%2520Chen%2520and%2520Zheng%2520Lin%2520and%2520Weiping%2520Wang%26entry.1292438233%3D%2520%2520Test-time%2520compute%2520has%2520led%2520to%2520remarkable%2520success%2520in%2520the%2520large%2520language%2520model%250A%2528LLM%2529%2520community%252C%2520particularly%2520for%2520complex%2520tasks%252C%2520where%2520longer%2520chains%2520of%2520thought%250A%2528CoTs%2529%2520are%2520generated%2520to%2520enhance%2520reasoning%2520capabilities.%2520However%252C%2520growing%250Aevidence%2520reveals%2520that%2520such%2520reasoning%2520models%2520often%2520produce%2520CoTs%2520plagued%2520by%250Aexcessive%2520redundancy%252C%2520including%2520unnecessary%2520verification%2520steps%2520and%2520repetitive%250Areasoning%2520shifts.%2520The%2520root%2520cause%2520lies%2520in%2520post-training%2520of%2520them%2520that%2520overly%2520rely%250Aon%2520outcome%2520reward%2520paradigms%252C%2520as%2520the%2520data%2520of%2520process%2520reward%2520paradigms%252C%2520which%250Aregulate%2520intermediate%2520reasoning%2520steps%252C%2520is%2520difficult%2520to%2520construct%2520at%2520scale.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520PI%252C%2520a%2520novel%2520framework%2520for%2520Test-time%2520Prompt%250AIntervention.%2520PI%2520provides%2520an%2520interface%2520to%2520dynamically%2520guide%2520and%2520regulate%250Areasoning%2520paths%2520during%2520inference%2520through%2520timely%2520%2528When%2520module%2529%2520and%2520proper%2520%2528How%250Amodule%2529%2520interventions%2520and%2520post-intervention%2520sampling%2520%2528Which%2520module%2529.%2520This%250Aallows%2520human%2520problem-solving%2520expertise%2520and%2520cognitive%2520science%2520principles%2520to%2520be%250Aseamlessly%2520integrated%2520into%2520LLMs%2527%2520reasoning%2520processes%252C%2520enhancing%2520controllability%250Aand%2520interpretability.%2520Extensive%2520experiments%2520across%2520multiple%2520models%2520and%2520datasets%250Ademonstrate%2520that%2520PI%2520significantly%2520shortens%2520CoTs%2520while%2520reducing%2520hallucination%252C%250Ayielding%2520more%2520concise%2520and%2520reliable%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Prompt%20Intervention&entry.906535625=Chenxu%20Yang%20and%20Qingyi%20Si%20and%20Mz%20Dai%20and%20Dingyu%20Yao%20and%20Mingyu%20Zheng%20and%20Minghui%20Chen%20and%20Zheng%20Lin%20and%20Weiping%20Wang&entry.1292438233=%20%20Test-time%20compute%20has%20led%20to%20remarkable%20success%20in%20the%20large%20language%20model%0A%28LLM%29%20community%2C%20particularly%20for%20complex%20tasks%2C%20where%20longer%20chains%20of%20thought%0A%28CoTs%29%20are%20generated%20to%20enhance%20reasoning%20capabilities.%20However%2C%20growing%0Aevidence%20reveals%20that%20such%20reasoning%20models%20often%20produce%20CoTs%20plagued%20by%0Aexcessive%20redundancy%2C%20including%20unnecessary%20verification%20steps%20and%20repetitive%0Areasoning%20shifts.%20The%20root%20cause%20lies%20in%20post-training%20of%20them%20that%20overly%20rely%0Aon%20outcome%20reward%20paradigms%2C%20as%20the%20data%20of%20process%20reward%20paradigms%2C%20which%0Aregulate%20intermediate%20reasoning%20steps%2C%20is%20difficult%20to%20construct%20at%20scale.%20To%0Aaddress%20this%2C%20we%20propose%20PI%2C%20a%20novel%20framework%20for%20Test-time%20Prompt%0AIntervention.%20PI%20provides%20an%20interface%20to%20dynamically%20guide%20and%20regulate%0Areasoning%20paths%20during%20inference%20through%20timely%20%28When%20module%29%20and%20proper%20%28How%0Amodule%29%20interventions%20and%20post-intervention%20sampling%20%28Which%20module%29.%20This%0Aallows%20human%20problem-solving%20expertise%20and%20cognitive%20science%20principles%20to%20be%0Aseamlessly%20integrated%20into%20LLMs%27%20reasoning%20processes%2C%20enhancing%20controllability%0Aand%20interpretability.%20Extensive%20experiments%20across%20multiple%20models%20and%20datasets%0Ademonstrate%20that%20PI%20significantly%20shortens%20CoTs%20while%20reducing%20hallucination%2C%0Ayielding%20more%20concise%20and%20reliable%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02511v1&entry.124074799=Read"},
{"title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented\n  Generation for Pathology VLMs via Reinforcement Learning", "author": "Wenchuan Zhang and Jingru Guo and Hengzhe Zhang and Penghao Zhang and Jie Chen and Shuwan Zhang and Zhang Zhang and Yuhao Yi and Hong Bu", "abstract": "  Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.\n", "link": "http://arxiv.org/abs/2508.02258v1", "date": "2025-08-04", "relevancy": 2.2259, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patho-AgenticRAG%3A%20Towards%20Multimodal%20Agentic%20Retrieval-Augmented%0A%20%20Generation%20for%20Pathology%20VLMs%20via%20Reinforcement%20Learning&body=Title%3A%20Patho-AgenticRAG%3A%20Towards%20Multimodal%20Agentic%20Retrieval-Augmented%0A%20%20Generation%20for%20Pathology%20VLMs%20via%20Reinforcement%20Learning%0AAuthor%3A%20Wenchuan%20Zhang%20and%20Jingru%20Guo%20and%20Hengzhe%20Zhang%20and%20Penghao%20Zhang%20and%20Jie%20Chen%20and%20Shuwan%20Zhang%20and%20Zhang%20Zhang%20and%20Yuhao%20Yi%20and%20Hong%20Bu%0AAbstract%3A%20%20%20Although%20Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20strong%20generalization%20in%0Amedical%20imaging%2C%20pathology%20presents%20unique%20challenges%20due%20to%20ultra-high%0Aresolution%2C%20complex%20tissue%20structures%2C%20and%20nuanced%20clinical%20semantics.%20These%0Afactors%20make%20pathology%20VLMs%20prone%20to%20hallucinations%2C%20i.e.%2C%20generating%20outputs%0Ainconsistent%20with%20visual%20evidence%2C%20which%20undermines%20clinical%20trust.%20Existing%0ARAG%20approaches%20in%20this%20domain%20largely%20depend%20on%20text-based%20knowledge%20bases%2C%0Alimiting%20their%20ability%20to%20leverage%20diagnostic%20visual%20cues.%20To%20address%20this%2C%20we%0Apropose%20Patho-AgenticRAG%2C%20a%20multimodal%20RAG%20framework%20with%20a%20database%20built%20on%0Apage-level%20embeddings%20from%20authoritative%20pathology%20textbooks.%20Unlike%0Atraditional%20text-only%20retrieval%20systems%2C%20it%20supports%20joint%20text-image%20search%2C%0Aenabling%20direct%20retrieval%20of%20textbook%20pages%20that%20contain%20both%20the%20queried%20text%0Aand%20relevant%20visual%20cues%2C%20thus%20avoiding%20the%20loss%20of%20critical%20image-based%0Ainformation.%20Patho-AgenticRAG%20also%20supports%20reasoning%2C%20task%20decomposition%2C%20and%0Amulti-turn%20search%20interactions%2C%20improving%20accuracy%20in%20complex%20diagnostic%0Ascenarios.%20Experiments%20show%20that%20Patho-AgenticRAG%20significantly%20outperforms%0Aexisting%20multimodal%20models%20in%20complex%20pathology%20tasks%20like%20multiple-choice%0Adiagnosis%20and%20visual%20question%20answering.%20Our%20project%20is%20available%20at%20the%0APatho-AgenticRAG%20repository%3A%0Ahttps%3A//github.com/Wenchuan-Zhang/Patho-AgenticRAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatho-AgenticRAG%253A%2520Towards%2520Multimodal%2520Agentic%2520Retrieval-Augmented%250A%2520%2520Generation%2520for%2520Pathology%2520VLMs%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DWenchuan%2520Zhang%2520and%2520Jingru%2520Guo%2520and%2520Hengzhe%2520Zhang%2520and%2520Penghao%2520Zhang%2520and%2520Jie%2520Chen%2520and%2520Shuwan%2520Zhang%2520and%2520Zhang%2520Zhang%2520and%2520Yuhao%2520Yi%2520and%2520Hong%2520Bu%26entry.1292438233%3D%2520%2520Although%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520generalization%2520in%250Amedical%2520imaging%252C%2520pathology%2520presents%2520unique%2520challenges%2520due%2520to%2520ultra-high%250Aresolution%252C%2520complex%2520tissue%2520structures%252C%2520and%2520nuanced%2520clinical%2520semantics.%2520These%250Afactors%2520make%2520pathology%2520VLMs%2520prone%2520to%2520hallucinations%252C%2520i.e.%252C%2520generating%2520outputs%250Ainconsistent%2520with%2520visual%2520evidence%252C%2520which%2520undermines%2520clinical%2520trust.%2520Existing%250ARAG%2520approaches%2520in%2520this%2520domain%2520largely%2520depend%2520on%2520text-based%2520knowledge%2520bases%252C%250Alimiting%2520their%2520ability%2520to%2520leverage%2520diagnostic%2520visual%2520cues.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Patho-AgenticRAG%252C%2520a%2520multimodal%2520RAG%2520framework%2520with%2520a%2520database%2520built%2520on%250Apage-level%2520embeddings%2520from%2520authoritative%2520pathology%2520textbooks.%2520Unlike%250Atraditional%2520text-only%2520retrieval%2520systems%252C%2520it%2520supports%2520joint%2520text-image%2520search%252C%250Aenabling%2520direct%2520retrieval%2520of%2520textbook%2520pages%2520that%2520contain%2520both%2520the%2520queried%2520text%250Aand%2520relevant%2520visual%2520cues%252C%2520thus%2520avoiding%2520the%2520loss%2520of%2520critical%2520image-based%250Ainformation.%2520Patho-AgenticRAG%2520also%2520supports%2520reasoning%252C%2520task%2520decomposition%252C%2520and%250Amulti-turn%2520search%2520interactions%252C%2520improving%2520accuracy%2520in%2520complex%2520diagnostic%250Ascenarios.%2520Experiments%2520show%2520that%2520Patho-AgenticRAG%2520significantly%2520outperforms%250Aexisting%2520multimodal%2520models%2520in%2520complex%2520pathology%2520tasks%2520like%2520multiple-choice%250Adiagnosis%2520and%2520visual%2520question%2520answering.%2520Our%2520project%2520is%2520available%2520at%2520the%250APatho-AgenticRAG%2520repository%253A%250Ahttps%253A//github.com/Wenchuan-Zhang/Patho-AgenticRAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patho-AgenticRAG%3A%20Towards%20Multimodal%20Agentic%20Retrieval-Augmented%0A%20%20Generation%20for%20Pathology%20VLMs%20via%20Reinforcement%20Learning&entry.906535625=Wenchuan%20Zhang%20and%20Jingru%20Guo%20and%20Hengzhe%20Zhang%20and%20Penghao%20Zhang%20and%20Jie%20Chen%20and%20Shuwan%20Zhang%20and%20Zhang%20Zhang%20and%20Yuhao%20Yi%20and%20Hong%20Bu&entry.1292438233=%20%20Although%20Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20strong%20generalization%20in%0Amedical%20imaging%2C%20pathology%20presents%20unique%20challenges%20due%20to%20ultra-high%0Aresolution%2C%20complex%20tissue%20structures%2C%20and%20nuanced%20clinical%20semantics.%20These%0Afactors%20make%20pathology%20VLMs%20prone%20to%20hallucinations%2C%20i.e.%2C%20generating%20outputs%0Ainconsistent%20with%20visual%20evidence%2C%20which%20undermines%20clinical%20trust.%20Existing%0ARAG%20approaches%20in%20this%20domain%20largely%20depend%20on%20text-based%20knowledge%20bases%2C%0Alimiting%20their%20ability%20to%20leverage%20diagnostic%20visual%20cues.%20To%20address%20this%2C%20we%0Apropose%20Patho-AgenticRAG%2C%20a%20multimodal%20RAG%20framework%20with%20a%20database%20built%20on%0Apage-level%20embeddings%20from%20authoritative%20pathology%20textbooks.%20Unlike%0Atraditional%20text-only%20retrieval%20systems%2C%20it%20supports%20joint%20text-image%20search%2C%0Aenabling%20direct%20retrieval%20of%20textbook%20pages%20that%20contain%20both%20the%20queried%20text%0Aand%20relevant%20visual%20cues%2C%20thus%20avoiding%20the%20loss%20of%20critical%20image-based%0Ainformation.%20Patho-AgenticRAG%20also%20supports%20reasoning%2C%20task%20decomposition%2C%20and%0Amulti-turn%20search%20interactions%2C%20improving%20accuracy%20in%20complex%20diagnostic%0Ascenarios.%20Experiments%20show%20that%20Patho-AgenticRAG%20significantly%20outperforms%0Aexisting%20multimodal%20models%20in%20complex%20pathology%20tasks%20like%20multiple-choice%0Adiagnosis%20and%20visual%20question%20answering.%20Our%20project%20is%20available%20at%20the%0APatho-AgenticRAG%20repository%3A%0Ahttps%3A//github.com/Wenchuan-Zhang/Patho-AgenticRAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02258v1&entry.124074799=Read"},
{"title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back", "author": "Ruixing Zhang and Yang Zhang and Tongyu Zhu and Leilei Sun and Weifeng Lv", "abstract": "  Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.\n", "link": "http://arxiv.org/abs/2507.18661v3", "date": "2025-08-04", "relevancy": 2.2144, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5578}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20Will%20Shut%3A%20A%20Vision-Based%20Next%20GPS%20Location%20Prediction%20Model%20by%0A%20%20Reinforcement%20Learning%20from%20Visual%20Map%20Feed%20Back&body=Title%3A%20Eyes%20Will%20Shut%3A%20A%20Vision-Based%20Next%20GPS%20Location%20Prediction%20Model%20by%0A%20%20Reinforcement%20Learning%20from%20Visual%20Map%20Feed%20Back%0AAuthor%3A%20Ruixing%20Zhang%20and%20Yang%20Zhang%20and%20Tongyu%20Zhu%20and%20Leilei%20Sun%20and%20Weifeng%20Lv%0AAbstract%3A%20%20%20Next%20Location%20Prediction%20is%20a%20fundamental%20task%20in%20the%20study%20of%20human%0Amobility%2C%20with%20wide-ranging%20applications%20in%20transportation%20planning%2C%20urban%0Agovernance%2C%20and%20epidemic%20forecasting.%20In%20practice%2C%20when%20humans%20attempt%20to%0Apredict%20the%20next%20location%20in%20a%20trajectory%2C%20they%20often%20visualize%20the%20trajectory%0Aon%20a%20map%20and%20reason%20based%20on%20road%20connectivity%20and%20movement%20trends.%20However%2C%0Athe%20vast%20majority%20of%20existing%20next-location%20prediction%20models%20do%20not%20reason%0Aover%20maps%20%5Ctextbf%7Bin%20the%20way%20that%20humans%20do%7D.%20Fortunately%2C%20the%20recent%0Adevelopment%20of%20Vision-Language%20Models%20%28VLMs%29%20has%20demonstrated%20strong%0Acapabilities%20in%20visual%20perception%20and%20even%20visual%20reasoning.%20This%20opens%20up%20a%0Anew%20possibility%3A%20by%20rendering%20both%20the%20road%20network%20and%20trajectory%20onto%20an%0Aimage%20and%20leveraging%20the%20reasoning%20abilities%20of%20VLMs%2C%20we%20can%20enable%20models%20to%0Aperform%20trajectory%20inference%20in%20a%20human-like%20manner.%20To%20explore%20this%20idea%2C%20we%0Afirst%20propose%20a%20method%20called%20Vision-Guided%20Location%20Search%20%28VGLS%29%2C%20which%0Aevaluates%20whether%20a%20general-purpose%20VLM%20is%20capable%20of%20trajectory-based%0Areasoning%20without%20modifying%20any%20of%20its%20internal%20parameters.%20Based%20on%20insights%0Afrom%20the%20VGLS%20results%2C%20we%20further%20propose%20our%20main%20approach%3A%20VLMLocPredictor%2C%0Awhich%20is%20composed%20of%20two%20stages%3A%20In%20the%20first%20stage%2C%20we%20design%20two%20Supervised%0AFine-Tuning%20%28SFT%29%20tasks%20that%20help%20the%20VLM%20understand%20road%20network%20and%0Atrajectory%20structures%20and%20acquire%20basic%20reasoning%20ability%20on%20such%20visual%0Ainputs.%20In%20the%20second%20stage%2C%20we%20introduce%20Reinforcement%20Learning%20from%20Visual%0AMap%20Feedback%2C%20enabling%20the%20model%20to%20self-improve%20its%20next-location%20prediction%0Aability%20through%20interaction%20with%20the%20environment.%20Experiments%20conducted%20on%0Adatasets%20from%20four%20different%20cities%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20and%20exhibits%20superior%20cross-city%0Ageneralization%20compared%20to%20other%20LLM-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520Will%2520Shut%253A%2520A%2520Vision-Based%2520Next%2520GPS%2520Location%2520Prediction%2520Model%2520by%250A%2520%2520Reinforcement%2520Learning%2520from%2520Visual%2520Map%2520Feed%2520Back%26entry.906535625%3DRuixing%2520Zhang%2520and%2520Yang%2520Zhang%2520and%2520Tongyu%2520Zhu%2520and%2520Leilei%2520Sun%2520and%2520Weifeng%2520Lv%26entry.1292438233%3D%2520%2520Next%2520Location%2520Prediction%2520is%2520a%2520fundamental%2520task%2520in%2520the%2520study%2520of%2520human%250Amobility%252C%2520with%2520wide-ranging%2520applications%2520in%2520transportation%2520planning%252C%2520urban%250Agovernance%252C%2520and%2520epidemic%2520forecasting.%2520In%2520practice%252C%2520when%2520humans%2520attempt%2520to%250Apredict%2520the%2520next%2520location%2520in%2520a%2520trajectory%252C%2520they%2520often%2520visualize%2520the%2520trajectory%250Aon%2520a%2520map%2520and%2520reason%2520based%2520on%2520road%2520connectivity%2520and%2520movement%2520trends.%2520However%252C%250Athe%2520vast%2520majority%2520of%2520existing%2520next-location%2520prediction%2520models%2520do%2520not%2520reason%250Aover%2520maps%2520%255Ctextbf%257Bin%2520the%2520way%2520that%2520humans%2520do%257D.%2520Fortunately%252C%2520the%2520recent%250Adevelopment%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520demonstrated%2520strong%250Acapabilities%2520in%2520visual%2520perception%2520and%2520even%2520visual%2520reasoning.%2520This%2520opens%2520up%2520a%250Anew%2520possibility%253A%2520by%2520rendering%2520both%2520the%2520road%2520network%2520and%2520trajectory%2520onto%2520an%250Aimage%2520and%2520leveraging%2520the%2520reasoning%2520abilities%2520of%2520VLMs%252C%2520we%2520can%2520enable%2520models%2520to%250Aperform%2520trajectory%2520inference%2520in%2520a%2520human-like%2520manner.%2520To%2520explore%2520this%2520idea%252C%2520we%250Afirst%2520propose%2520a%2520method%2520called%2520Vision-Guided%2520Location%2520Search%2520%2528VGLS%2529%252C%2520which%250Aevaluates%2520whether%2520a%2520general-purpose%2520VLM%2520is%2520capable%2520of%2520trajectory-based%250Areasoning%2520without%2520modifying%2520any%2520of%2520its%2520internal%2520parameters.%2520Based%2520on%2520insights%250Afrom%2520the%2520VGLS%2520results%252C%2520we%2520further%2520propose%2520our%2520main%2520approach%253A%2520VLMLocPredictor%252C%250Awhich%2520is%2520composed%2520of%2520two%2520stages%253A%2520In%2520the%2520first%2520stage%252C%2520we%2520design%2520two%2520Supervised%250AFine-Tuning%2520%2528SFT%2529%2520tasks%2520that%2520help%2520the%2520VLM%2520understand%2520road%2520network%2520and%250Atrajectory%2520structures%2520and%2520acquire%2520basic%2520reasoning%2520ability%2520on%2520such%2520visual%250Ainputs.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520Reinforcement%2520Learning%2520from%2520Visual%250AMap%2520Feedback%252C%2520enabling%2520the%2520model%2520to%2520self-improve%2520its%2520next-location%2520prediction%250Aability%2520through%2520interaction%2520with%2520the%2520environment.%2520Experiments%2520conducted%2520on%250Adatasets%2520from%2520four%2520different%2520cities%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520and%2520exhibits%2520superior%2520cross-city%250Ageneralization%2520compared%2520to%2520other%2520LLM-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20Will%20Shut%3A%20A%20Vision-Based%20Next%20GPS%20Location%20Prediction%20Model%20by%0A%20%20Reinforcement%20Learning%20from%20Visual%20Map%20Feed%20Back&entry.906535625=Ruixing%20Zhang%20and%20Yang%20Zhang%20and%20Tongyu%20Zhu%20and%20Leilei%20Sun%20and%20Weifeng%20Lv&entry.1292438233=%20%20Next%20Location%20Prediction%20is%20a%20fundamental%20task%20in%20the%20study%20of%20human%0Amobility%2C%20with%20wide-ranging%20applications%20in%20transportation%20planning%2C%20urban%0Agovernance%2C%20and%20epidemic%20forecasting.%20In%20practice%2C%20when%20humans%20attempt%20to%0Apredict%20the%20next%20location%20in%20a%20trajectory%2C%20they%20often%20visualize%20the%20trajectory%0Aon%20a%20map%20and%20reason%20based%20on%20road%20connectivity%20and%20movement%20trends.%20However%2C%0Athe%20vast%20majority%20of%20existing%20next-location%20prediction%20models%20do%20not%20reason%0Aover%20maps%20%5Ctextbf%7Bin%20the%20way%20that%20humans%20do%7D.%20Fortunately%2C%20the%20recent%0Adevelopment%20of%20Vision-Language%20Models%20%28VLMs%29%20has%20demonstrated%20strong%0Acapabilities%20in%20visual%20perception%20and%20even%20visual%20reasoning.%20This%20opens%20up%20a%0Anew%20possibility%3A%20by%20rendering%20both%20the%20road%20network%20and%20trajectory%20onto%20an%0Aimage%20and%20leveraging%20the%20reasoning%20abilities%20of%20VLMs%2C%20we%20can%20enable%20models%20to%0Aperform%20trajectory%20inference%20in%20a%20human-like%20manner.%20To%20explore%20this%20idea%2C%20we%0Afirst%20propose%20a%20method%20called%20Vision-Guided%20Location%20Search%20%28VGLS%29%2C%20which%0Aevaluates%20whether%20a%20general-purpose%20VLM%20is%20capable%20of%20trajectory-based%0Areasoning%20without%20modifying%20any%20of%20its%20internal%20parameters.%20Based%20on%20insights%0Afrom%20the%20VGLS%20results%2C%20we%20further%20propose%20our%20main%20approach%3A%20VLMLocPredictor%2C%0Awhich%20is%20composed%20of%20two%20stages%3A%20In%20the%20first%20stage%2C%20we%20design%20two%20Supervised%0AFine-Tuning%20%28SFT%29%20tasks%20that%20help%20the%20VLM%20understand%20road%20network%20and%0Atrajectory%20structures%20and%20acquire%20basic%20reasoning%20ability%20on%20such%20visual%0Ainputs.%20In%20the%20second%20stage%2C%20we%20introduce%20Reinforcement%20Learning%20from%20Visual%0AMap%20Feedback%2C%20enabling%20the%20model%20to%20self-improve%20its%20next-location%20prediction%0Aability%20through%20interaction%20with%20the%20environment.%20Experiments%20conducted%20on%0Adatasets%20from%20four%20different%20cities%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20and%20exhibits%20superior%20cross-city%0Ageneralization%20compared%20to%20other%20LLM-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18661v3&entry.124074799=Read"},
{"title": "D-Judge: How Far Are We? Evaluating the Discrepancies Between\n  AI-synthesized Images and Natural Images through Multimodal Guidance", "author": "Renyang Liu and Ziyu Lyu and Wei Zhou and See-Kiong Ng", "abstract": "  In the rapidly evolving field of Artificial Intelligence Generated Content\n(AIGC), a central challenge is distinguishing AI-synthesized images from\nnatural images. Despite the impressive capabilities of advanced AI generative\nmodels in producing visually compelling content, significant discrepancies\nremain when compared to natural images. To systematically investigate and\nquantify these differences, we construct a large-scale multimodal dataset named\nDANI, comprising 5,000 natural images and over 440,000 AI-generated image\n(AIGI) samples produced by nine representative models using both unimodal and\nmultimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and\nText and Image-to-Image (TI2I). We then introduce D-Judge, a benchmark designed\nto answer the critical question: how far are AI-generated images from truly\nrealistic images? Our fine-grained evaluation framework assesses DANI across\nfive key dimensions: naive visual quality, semantic alignment, aesthetic\nappeal, downstream task applicability, and coordinated human validation.\nExtensive experiments reveal substantial discrepancies across these dimensions,\nhighlighting the importance of aligning quantitative metrics with human\njudgment to achieve a comprehensive understanding of AI-generated image\nquality. The code and dataset are publicly available at:\nhttps://github.com/ryliu68/DJudge and\nhttps://huggingface.co/datasets/Renyang/DANI.\n", "link": "http://arxiv.org/abs/2412.17632v3", "date": "2025-08-04", "relevancy": 2.1993, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5492}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-Judge%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance&body=Title%3A%20D-Judge%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance%0AAuthor%3A%20Renyang%20Liu%20and%20Ziyu%20Lyu%20and%20Wei%20Zhou%20and%20See-Kiong%20Ng%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20Artificial%20Intelligence%20Generated%20Content%0A%28AIGC%29%2C%20a%20central%20challenge%20is%20distinguishing%20AI-synthesized%20images%20from%0Anatural%20images.%20Despite%20the%20impressive%20capabilities%20of%20advanced%20AI%20generative%0Amodels%20in%20producing%20visually%20compelling%20content%2C%20significant%20discrepancies%0Aremain%20when%20compared%20to%20natural%20images.%20To%20systematically%20investigate%20and%0Aquantify%20these%20differences%2C%20we%20construct%20a%20large-scale%20multimodal%20dataset%20named%0ADANI%2C%20comprising%205%2C000%20natural%20images%20and%20over%20440%2C000%20AI-generated%20image%0A%28AIGI%29%20samples%20produced%20by%20nine%20representative%20models%20using%20both%20unimodal%20and%0Amultimodal%20prompts%2C%20including%20Text-to-Image%20%28T2I%29%2C%20Image-to-Image%20%28I2I%29%2C%20and%0AText%20and%20Image-to-Image%20%28TI2I%29.%20We%20then%20introduce%20D-Judge%2C%20a%20benchmark%20designed%0Ato%20answer%20the%20critical%20question%3A%20how%20far%20are%20AI-generated%20images%20from%20truly%0Arealistic%20images%3F%20Our%20fine-grained%20evaluation%20framework%20assesses%20DANI%20across%0Afive%20key%20dimensions%3A%20naive%20visual%20quality%2C%20semantic%20alignment%2C%20aesthetic%0Aappeal%2C%20downstream%20task%20applicability%2C%20and%20coordinated%20human%20validation.%0AExtensive%20experiments%20reveal%20substantial%20discrepancies%20across%20these%20dimensions%2C%0Ahighlighting%20the%20importance%20of%20aligning%20quantitative%20metrics%20with%20human%0Ajudgment%20to%20achieve%20a%20comprehensive%20understanding%20of%20AI-generated%20image%0Aquality.%20The%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ryliu68/DJudge%20and%0Ahttps%3A//huggingface.co/datasets/Renyang/DANI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17632v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-Judge%253A%2520How%2520Far%2520Are%2520We%253F%2520Evaluating%2520the%2520Discrepancies%2520Between%250A%2520%2520AI-synthesized%2520Images%2520and%2520Natural%2520Images%2520through%2520Multimodal%2520Guidance%26entry.906535625%3DRenyang%2520Liu%2520and%2520Ziyu%2520Lyu%2520and%2520Wei%2520Zhou%2520and%2520See-Kiong%2520Ng%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520Artificial%2520Intelligence%2520Generated%2520Content%250A%2528AIGC%2529%252C%2520a%2520central%2520challenge%2520is%2520distinguishing%2520AI-synthesized%2520images%2520from%250Anatural%2520images.%2520Despite%2520the%2520impressive%2520capabilities%2520of%2520advanced%2520AI%2520generative%250Amodels%2520in%2520producing%2520visually%2520compelling%2520content%252C%2520significant%2520discrepancies%250Aremain%2520when%2520compared%2520to%2520natural%2520images.%2520To%2520systematically%2520investigate%2520and%250Aquantify%2520these%2520differences%252C%2520we%2520construct%2520a%2520large-scale%2520multimodal%2520dataset%2520named%250ADANI%252C%2520comprising%25205%252C000%2520natural%2520images%2520and%2520over%2520440%252C000%2520AI-generated%2520image%250A%2528AIGI%2529%2520samples%2520produced%2520by%2520nine%2520representative%2520models%2520using%2520both%2520unimodal%2520and%250Amultimodal%2520prompts%252C%2520including%2520Text-to-Image%2520%2528T2I%2529%252C%2520Image-to-Image%2520%2528I2I%2529%252C%2520and%250AText%2520and%2520Image-to-Image%2520%2528TI2I%2529.%2520We%2520then%2520introduce%2520D-Judge%252C%2520a%2520benchmark%2520designed%250Ato%2520answer%2520the%2520critical%2520question%253A%2520how%2520far%2520are%2520AI-generated%2520images%2520from%2520truly%250Arealistic%2520images%253F%2520Our%2520fine-grained%2520evaluation%2520framework%2520assesses%2520DANI%2520across%250Afive%2520key%2520dimensions%253A%2520naive%2520visual%2520quality%252C%2520semantic%2520alignment%252C%2520aesthetic%250Aappeal%252C%2520downstream%2520task%2520applicability%252C%2520and%2520coordinated%2520human%2520validation.%250AExtensive%2520experiments%2520reveal%2520substantial%2520discrepancies%2520across%2520these%2520dimensions%252C%250Ahighlighting%2520the%2520importance%2520of%2520aligning%2520quantitative%2520metrics%2520with%2520human%250Ajudgment%2520to%2520achieve%2520a%2520comprehensive%2520understanding%2520of%2520AI-generated%2520image%250Aquality.%2520The%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/ryliu68/DJudge%2520and%250Ahttps%253A//huggingface.co/datasets/Renyang/DANI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17632v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-Judge%3A%20How%20Far%20Are%20We%3F%20Evaluating%20the%20Discrepancies%20Between%0A%20%20AI-synthesized%20Images%20and%20Natural%20Images%20through%20Multimodal%20Guidance&entry.906535625=Renyang%20Liu%20and%20Ziyu%20Lyu%20and%20Wei%20Zhou%20and%20See-Kiong%20Ng&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20Artificial%20Intelligence%20Generated%20Content%0A%28AIGC%29%2C%20a%20central%20challenge%20is%20distinguishing%20AI-synthesized%20images%20from%0Anatural%20images.%20Despite%20the%20impressive%20capabilities%20of%20advanced%20AI%20generative%0Amodels%20in%20producing%20visually%20compelling%20content%2C%20significant%20discrepancies%0Aremain%20when%20compared%20to%20natural%20images.%20To%20systematically%20investigate%20and%0Aquantify%20these%20differences%2C%20we%20construct%20a%20large-scale%20multimodal%20dataset%20named%0ADANI%2C%20comprising%205%2C000%20natural%20images%20and%20over%20440%2C000%20AI-generated%20image%0A%28AIGI%29%20samples%20produced%20by%20nine%20representative%20models%20using%20both%20unimodal%20and%0Amultimodal%20prompts%2C%20including%20Text-to-Image%20%28T2I%29%2C%20Image-to-Image%20%28I2I%29%2C%20and%0AText%20and%20Image-to-Image%20%28TI2I%29.%20We%20then%20introduce%20D-Judge%2C%20a%20benchmark%20designed%0Ato%20answer%20the%20critical%20question%3A%20how%20far%20are%20AI-generated%20images%20from%20truly%0Arealistic%20images%3F%20Our%20fine-grained%20evaluation%20framework%20assesses%20DANI%20across%0Afive%20key%20dimensions%3A%20naive%20visual%20quality%2C%20semantic%20alignment%2C%20aesthetic%0Aappeal%2C%20downstream%20task%20applicability%2C%20and%20coordinated%20human%20validation.%0AExtensive%20experiments%20reveal%20substantial%20discrepancies%20across%20these%20dimensions%2C%0Ahighlighting%20the%20importance%20of%20aligning%20quantitative%20metrics%20with%20human%0Ajudgment%20to%20achieve%20a%20comprehensive%20understanding%20of%20AI-generated%20image%0Aquality.%20The%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ryliu68/DJudge%20and%0Ahttps%3A//huggingface.co/datasets/Renyang/DANI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17632v3&entry.124074799=Read"},
{"title": "Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction", "author": "Taras Kucherenko and Derek Peristy and Judith B\u00fctepage", "abstract": "  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. Additionally, we introduce and\nevaluate a set of better-correlated metrics that can drive progress in the\nfield.\n", "link": "http://arxiv.org/abs/2410.14334v3", "date": "2025-08-04", "relevancy": 2.1948, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5834}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5427}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20evaluators%3A%20Towards%20human-aligned%20metrics%20for%20missing%0A%20%20markers%20reconstruction&body=Title%3A%20Evaluating%20the%20evaluators%3A%20Towards%20human-aligned%20metrics%20for%20missing%0A%20%20markers%20reconstruction%0AAuthor%3A%20Taras%20Kucherenko%20and%20Derek%20Peristy%20and%20Judith%20B%C3%BCtepage%0AAbstract%3A%20%20%20Animation%20data%20is%20often%20obtained%20through%20optical%20motion%20capture%20systems%2C%0Awhich%20utilize%20a%20multitude%20of%20cameras%20to%20establish%20the%20position%20of%20optical%0Amarkers.%20However%2C%20system%20errors%20or%20occlusions%20can%20result%20in%20missing%20markers%2C%0Athe%20manual%20cleaning%20of%20which%20can%20be%20time-consuming.%20This%20has%20sparked%20interest%0Ain%20machine%20learning-based%20solutions%20for%20missing%20marker%20reconstruction%20in%20the%0Aacademic%20community.%20Most%20academic%20papers%20utilize%20a%20simplistic%20mean%20square%20error%0Aas%20the%20main%20metric.%20In%20this%20paper%2C%20we%20show%20that%20this%20metric%20does%20not%20correlate%0Awith%20subjective%20perception%20of%20the%20fill%20quality.%20Additionally%2C%20we%20introduce%20and%0Aevaluate%20a%20set%20of%20better-correlated%20metrics%20that%20can%20drive%20progress%20in%20the%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520evaluators%253A%2520Towards%2520human-aligned%2520metrics%2520for%2520missing%250A%2520%2520markers%2520reconstruction%26entry.906535625%3DTaras%2520Kucherenko%2520and%2520Derek%2520Peristy%2520and%2520Judith%2520B%25C3%25BCtepage%26entry.1292438233%3D%2520%2520Animation%2520data%2520is%2520often%2520obtained%2520through%2520optical%2520motion%2520capture%2520systems%252C%250Awhich%2520utilize%2520a%2520multitude%2520of%2520cameras%2520to%2520establish%2520the%2520position%2520of%2520optical%250Amarkers.%2520However%252C%2520system%2520errors%2520or%2520occlusions%2520can%2520result%2520in%2520missing%2520markers%252C%250Athe%2520manual%2520cleaning%2520of%2520which%2520can%2520be%2520time-consuming.%2520This%2520has%2520sparked%2520interest%250Ain%2520machine%2520learning-based%2520solutions%2520for%2520missing%2520marker%2520reconstruction%2520in%2520the%250Aacademic%2520community.%2520Most%2520academic%2520papers%2520utilize%2520a%2520simplistic%2520mean%2520square%2520error%250Aas%2520the%2520main%2520metric.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520this%2520metric%2520does%2520not%2520correlate%250Awith%2520subjective%2520perception%2520of%2520the%2520fill%2520quality.%2520Additionally%252C%2520we%2520introduce%2520and%250Aevaluate%2520a%2520set%2520of%2520better-correlated%2520metrics%2520that%2520can%2520drive%2520progress%2520in%2520the%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20evaluators%3A%20Towards%20human-aligned%20metrics%20for%20missing%0A%20%20markers%20reconstruction&entry.906535625=Taras%20Kucherenko%20and%20Derek%20Peristy%20and%20Judith%20B%C3%BCtepage&entry.1292438233=%20%20Animation%20data%20is%20often%20obtained%20through%20optical%20motion%20capture%20systems%2C%0Awhich%20utilize%20a%20multitude%20of%20cameras%20to%20establish%20the%20position%20of%20optical%0Amarkers.%20However%2C%20system%20errors%20or%20occlusions%20can%20result%20in%20missing%20markers%2C%0Athe%20manual%20cleaning%20of%20which%20can%20be%20time-consuming.%20This%20has%20sparked%20interest%0Ain%20machine%20learning-based%20solutions%20for%20missing%20marker%20reconstruction%20in%20the%0Aacademic%20community.%20Most%20academic%20papers%20utilize%20a%20simplistic%20mean%20square%20error%0Aas%20the%20main%20metric.%20In%20this%20paper%2C%20we%20show%20that%20this%20metric%20does%20not%20correlate%0Awith%20subjective%20perception%20of%20the%20fill%20quality.%20Additionally%2C%20we%20introduce%20and%0Aevaluate%20a%20set%20of%20better-correlated%20metrics%20that%20can%20drive%20progress%20in%20the%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14334v3&entry.124074799=Read"},
{"title": "Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical\n  Image Segmentation", "author": "Paul Zaha and Lars B\u00f6cking and Simeon Allmendinger and Leopold M\u00fcller and Niklas K\u00fchl", "abstract": "  Medical image segmentation is crucial for disease diagnosis and treatment\nplanning, yet developing robust segmentation models often requires substantial\ncomputational resources and large datasets. Existing research shows that\npre-trained and finetuned foundation models can boost segmentation performance.\nHowever, questions remain about how particular image preprocessing steps may\ninfluence segmentation performance across different medical imaging modalities.\nIn particular, edges-abrupt transitions in pixel intensity-are widely\nacknowledged as vital cues for object boundaries but have not been\nsystematically examined in the pre-training of foundation models. We address\nthis gap by investigating to which extend pre-training with data processed\nusing computationally efficient edge kernels, such as kirsch, can improve\ncross-modality segmentation capabilities of a foundation model. Two versions of\na foundation model are first trained on either raw or edge-enhanced data across\nmultiple medical imaging modalities, then finetuned on selected raw subsets\ntailored to specific medical modalities. After systematic investigation using\nthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and\nXRay, we discover both increased and reduced segmentation performance across\nmodalities using edge-focused pre-training, indicating the need for a selective\napplication of this approach. To guide such selective applications, we propose\na meta-learning strategy. It uses standard deviation and image entropy of the\nraw image to choose between a model pre-trained on edge-enhanced or on raw data\nfor optimal performance. Our experiments show that integrating this\nmeta-learning layer yields an overall segmentation performance improvement\nacross diverse medical imaging tasks by 16.42% compared to models pre-trained\non edge-enhanced data only and 19.30% compared to models pre-trained on raw\ndata only.\n", "link": "http://arxiv.org/abs/2508.02281v1", "date": "2025-08-04", "relevancy": 2.1868, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Edges%20Matter%3F%20Investigating%20Edge-Enhanced%20Pre-Training%20for%20Medical%0A%20%20Image%20Segmentation&body=Title%3A%20Do%20Edges%20Matter%3F%20Investigating%20Edge-Enhanced%20Pre-Training%20for%20Medical%0A%20%20Image%20Segmentation%0AAuthor%3A%20Paul%20Zaha%20and%20Lars%20B%C3%B6cking%20and%20Simeon%20Allmendinger%20and%20Leopold%20M%C3%BCller%20and%20Niklas%20K%C3%BChl%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20crucial%20for%20disease%20diagnosis%20and%20treatment%0Aplanning%2C%20yet%20developing%20robust%20segmentation%20models%20often%20requires%20substantial%0Acomputational%20resources%20and%20large%20datasets.%20Existing%20research%20shows%20that%0Apre-trained%20and%20finetuned%20foundation%20models%20can%20boost%20segmentation%20performance.%0AHowever%2C%20questions%20remain%20about%20how%20particular%20image%20preprocessing%20steps%20may%0Ainfluence%20segmentation%20performance%20across%20different%20medical%20imaging%20modalities.%0AIn%20particular%2C%20edges-abrupt%20transitions%20in%20pixel%20intensity-are%20widely%0Aacknowledged%20as%20vital%20cues%20for%20object%20boundaries%20but%20have%20not%20been%0Asystematically%20examined%20in%20the%20pre-training%20of%20foundation%20models.%20We%20address%0Athis%20gap%20by%20investigating%20to%20which%20extend%20pre-training%20with%20data%20processed%0Ausing%20computationally%20efficient%20edge%20kernels%2C%20such%20as%20kirsch%2C%20can%20improve%0Across-modality%20segmentation%20capabilities%20of%20a%20foundation%20model.%20Two%20versions%20of%0Aa%20foundation%20model%20are%20first%20trained%20on%20either%20raw%20or%20edge-enhanced%20data%20across%0Amultiple%20medical%20imaging%20modalities%2C%20then%20finetuned%20on%20selected%20raw%20subsets%0Atailored%20to%20specific%20medical%20modalities.%20After%20systematic%20investigation%20using%0Athe%20medical%20domains%20Dermoscopy%2C%20Fundus%2C%20Mammography%2C%20Microscopy%2C%20OCT%2C%20US%2C%20and%0AXRay%2C%20we%20discover%20both%20increased%20and%20reduced%20segmentation%20performance%20across%0Amodalities%20using%20edge-focused%20pre-training%2C%20indicating%20the%20need%20for%20a%20selective%0Aapplication%20of%20this%20approach.%20To%20guide%20such%20selective%20applications%2C%20we%20propose%0Aa%20meta-learning%20strategy.%20It%20uses%20standard%20deviation%20and%20image%20entropy%20of%20the%0Araw%20image%20to%20choose%20between%20a%20model%20pre-trained%20on%20edge-enhanced%20or%20on%20raw%20data%0Afor%20optimal%20performance.%20Our%20experiments%20show%20that%20integrating%20this%0Ameta-learning%20layer%20yields%20an%20overall%20segmentation%20performance%20improvement%0Aacross%20diverse%20medical%20imaging%20tasks%20by%2016.42%25%20compared%20to%20models%20pre-trained%0Aon%20edge-enhanced%20data%20only%20and%2019.30%25%20compared%20to%20models%20pre-trained%20on%20raw%0Adata%20only.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Edges%2520Matter%253F%2520Investigating%2520Edge-Enhanced%2520Pre-Training%2520for%2520Medical%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DPaul%2520Zaha%2520and%2520Lars%2520B%25C3%25B6cking%2520and%2520Simeon%2520Allmendinger%2520and%2520Leopold%2520M%25C3%25BCller%2520and%2520Niklas%2520K%25C3%25BChl%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520disease%2520diagnosis%2520and%2520treatment%250Aplanning%252C%2520yet%2520developing%2520robust%2520segmentation%2520models%2520often%2520requires%2520substantial%250Acomputational%2520resources%2520and%2520large%2520datasets.%2520Existing%2520research%2520shows%2520that%250Apre-trained%2520and%2520finetuned%2520foundation%2520models%2520can%2520boost%2520segmentation%2520performance.%250AHowever%252C%2520questions%2520remain%2520about%2520how%2520particular%2520image%2520preprocessing%2520steps%2520may%250Ainfluence%2520segmentation%2520performance%2520across%2520different%2520medical%2520imaging%2520modalities.%250AIn%2520particular%252C%2520edges-abrupt%2520transitions%2520in%2520pixel%2520intensity-are%2520widely%250Aacknowledged%2520as%2520vital%2520cues%2520for%2520object%2520boundaries%2520but%2520have%2520not%2520been%250Asystematically%2520examined%2520in%2520the%2520pre-training%2520of%2520foundation%2520models.%2520We%2520address%250Athis%2520gap%2520by%2520investigating%2520to%2520which%2520extend%2520pre-training%2520with%2520data%2520processed%250Ausing%2520computationally%2520efficient%2520edge%2520kernels%252C%2520such%2520as%2520kirsch%252C%2520can%2520improve%250Across-modality%2520segmentation%2520capabilities%2520of%2520a%2520foundation%2520model.%2520Two%2520versions%2520of%250Aa%2520foundation%2520model%2520are%2520first%2520trained%2520on%2520either%2520raw%2520or%2520edge-enhanced%2520data%2520across%250Amultiple%2520medical%2520imaging%2520modalities%252C%2520then%2520finetuned%2520on%2520selected%2520raw%2520subsets%250Atailored%2520to%2520specific%2520medical%2520modalities.%2520After%2520systematic%2520investigation%2520using%250Athe%2520medical%2520domains%2520Dermoscopy%252C%2520Fundus%252C%2520Mammography%252C%2520Microscopy%252C%2520OCT%252C%2520US%252C%2520and%250AXRay%252C%2520we%2520discover%2520both%2520increased%2520and%2520reduced%2520segmentation%2520performance%2520across%250Amodalities%2520using%2520edge-focused%2520pre-training%252C%2520indicating%2520the%2520need%2520for%2520a%2520selective%250Aapplication%2520of%2520this%2520approach.%2520To%2520guide%2520such%2520selective%2520applications%252C%2520we%2520propose%250Aa%2520meta-learning%2520strategy.%2520It%2520uses%2520standard%2520deviation%2520and%2520image%2520entropy%2520of%2520the%250Araw%2520image%2520to%2520choose%2520between%2520a%2520model%2520pre-trained%2520on%2520edge-enhanced%2520or%2520on%2520raw%2520data%250Afor%2520optimal%2520performance.%2520Our%2520experiments%2520show%2520that%2520integrating%2520this%250Ameta-learning%2520layer%2520yields%2520an%2520overall%2520segmentation%2520performance%2520improvement%250Aacross%2520diverse%2520medical%2520imaging%2520tasks%2520by%252016.42%2525%2520compared%2520to%2520models%2520pre-trained%250Aon%2520edge-enhanced%2520data%2520only%2520and%252019.30%2525%2520compared%2520to%2520models%2520pre-trained%2520on%2520raw%250Adata%2520only.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Edges%20Matter%3F%20Investigating%20Edge-Enhanced%20Pre-Training%20for%20Medical%0A%20%20Image%20Segmentation&entry.906535625=Paul%20Zaha%20and%20Lars%20B%C3%B6cking%20and%20Simeon%20Allmendinger%20and%20Leopold%20M%C3%BCller%20and%20Niklas%20K%C3%BChl&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20crucial%20for%20disease%20diagnosis%20and%20treatment%0Aplanning%2C%20yet%20developing%20robust%20segmentation%20models%20often%20requires%20substantial%0Acomputational%20resources%20and%20large%20datasets.%20Existing%20research%20shows%20that%0Apre-trained%20and%20finetuned%20foundation%20models%20can%20boost%20segmentation%20performance.%0AHowever%2C%20questions%20remain%20about%20how%20particular%20image%20preprocessing%20steps%20may%0Ainfluence%20segmentation%20performance%20across%20different%20medical%20imaging%20modalities.%0AIn%20particular%2C%20edges-abrupt%20transitions%20in%20pixel%20intensity-are%20widely%0Aacknowledged%20as%20vital%20cues%20for%20object%20boundaries%20but%20have%20not%20been%0Asystematically%20examined%20in%20the%20pre-training%20of%20foundation%20models.%20We%20address%0Athis%20gap%20by%20investigating%20to%20which%20extend%20pre-training%20with%20data%20processed%0Ausing%20computationally%20efficient%20edge%20kernels%2C%20such%20as%20kirsch%2C%20can%20improve%0Across-modality%20segmentation%20capabilities%20of%20a%20foundation%20model.%20Two%20versions%20of%0Aa%20foundation%20model%20are%20first%20trained%20on%20either%20raw%20or%20edge-enhanced%20data%20across%0Amultiple%20medical%20imaging%20modalities%2C%20then%20finetuned%20on%20selected%20raw%20subsets%0Atailored%20to%20specific%20medical%20modalities.%20After%20systematic%20investigation%20using%0Athe%20medical%20domains%20Dermoscopy%2C%20Fundus%2C%20Mammography%2C%20Microscopy%2C%20OCT%2C%20US%2C%20and%0AXRay%2C%20we%20discover%20both%20increased%20and%20reduced%20segmentation%20performance%20across%0Amodalities%20using%20edge-focused%20pre-training%2C%20indicating%20the%20need%20for%20a%20selective%0Aapplication%20of%20this%20approach.%20To%20guide%20such%20selective%20applications%2C%20we%20propose%0Aa%20meta-learning%20strategy.%20It%20uses%20standard%20deviation%20and%20image%20entropy%20of%20the%0Araw%20image%20to%20choose%20between%20a%20model%20pre-trained%20on%20edge-enhanced%20or%20on%20raw%20data%0Afor%20optimal%20performance.%20Our%20experiments%20show%20that%20integrating%20this%0Ameta-learning%20layer%20yields%20an%20overall%20segmentation%20performance%20improvement%0Aacross%20diverse%20medical%20imaging%20tasks%20by%2016.42%25%20compared%20to%20models%20pre-trained%0Aon%20edge-enhanced%20data%20only%20and%2019.30%25%20compared%20to%20models%20pre-trained%20on%20raw%0Adata%20only.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02281v1&entry.124074799=Read"},
{"title": "SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting", "author": "Paschalis Giakoumoglou and Dimitrios Karageorgiou and Symeon Papadopoulos and Panagiotis C. Petrantonakis", "abstract": "  Recent advancements in generative AI have made text-guided image inpainting -\nadding, removing, or altering image regions using textual prompts - widely\naccessible. However, generating semantically correct photorealistic imagery,\ntypically requires carefully-crafted prompts and iterative refinement by\nevaluating the realism of the generated content - tasks commonly performed by\nhumans. To automate the generative process, we propose Semantically Aligned and\nUncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to\nsample prompts from a distribution that closely aligns with human perception\nand to evaluate the generated content and discard instances that deviate from\nsuch a distribution, which we approximate using pretrained large language\nmodels and vision-language models. By applying this pipeline on multiple\nstate-of-the-art inpainting models, we create the SAGI Dataset (SAGI-D),\ncurrently the largest and most diverse dataset of AI-generated inpaintings,\ncomprising over 95k inpainted images and a human-evaluated subset. Our\nexperiments show that semantic alignment significantly improves image quality\nand aesthetics, while uncertainty guidance effectively identifies realistic\nmanipulations - human ability to distinguish inpainted images from real ones\ndrops from 74% to 35% in terms of accuracy, after applying our pipeline.\nMoreover, using SAGI-D for training several image forensic approaches increases\nin-domain detection performance on average by 37.4% and out-of-domain\ngeneralization by 26.1% in terms of IoU, also demonstrating its utility in\ncountering malicious exploitation of generative AI. Code and dataset are\navailable at https://mever-team.github.io/SAGI/\n", "link": "http://arxiv.org/abs/2502.06593v3", "date": "2025-08-04", "relevancy": 2.1797, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5575}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5392}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGI%3A%20Semantically%20Aligned%20and%20Uncertainty%20Guided%20AI%20Image%20Inpainting&body=Title%3A%20SAGI%3A%20Semantically%20Aligned%20and%20Uncertainty%20Guided%20AI%20Image%20Inpainting%0AAuthor%3A%20Paschalis%20Giakoumoglou%20and%20Dimitrios%20Karageorgiou%20and%20Symeon%20Papadopoulos%20and%20Panagiotis%20C.%20Petrantonakis%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20AI%20have%20made%20text-guided%20image%20inpainting%20-%0Aadding%2C%20removing%2C%20or%20altering%20image%20regions%20using%20textual%20prompts%20-%20widely%0Aaccessible.%20However%2C%20generating%20semantically%20correct%20photorealistic%20imagery%2C%0Atypically%20requires%20carefully-crafted%20prompts%20and%20iterative%20refinement%20by%0Aevaluating%20the%20realism%20of%20the%20generated%20content%20-%20tasks%20commonly%20performed%20by%0Ahumans.%20To%20automate%20the%20generative%20process%2C%20we%20propose%20Semantically%20Aligned%20and%0AUncertainty%20Guided%20AI%20Image%20Inpainting%20%28SAGI%29%2C%20a%20model-agnostic%20pipeline%2C%20to%0Asample%20prompts%20from%20a%20distribution%20that%20closely%20aligns%20with%20human%20perception%0Aand%20to%20evaluate%20the%20generated%20content%20and%20discard%20instances%20that%20deviate%20from%0Asuch%20a%20distribution%2C%20which%20we%20approximate%20using%20pretrained%20large%20language%0Amodels%20and%20vision-language%20models.%20By%20applying%20this%20pipeline%20on%20multiple%0Astate-of-the-art%20inpainting%20models%2C%20we%20create%20the%20SAGI%20Dataset%20%28SAGI-D%29%2C%0Acurrently%20the%20largest%20and%20most%20diverse%20dataset%20of%20AI-generated%20inpaintings%2C%0Acomprising%20over%2095k%20inpainted%20images%20and%20a%20human-evaluated%20subset.%20Our%0Aexperiments%20show%20that%20semantic%20alignment%20significantly%20improves%20image%20quality%0Aand%20aesthetics%2C%20while%20uncertainty%20guidance%20effectively%20identifies%20realistic%0Amanipulations%20-%20human%20ability%20to%20distinguish%20inpainted%20images%20from%20real%20ones%0Adrops%20from%2074%25%20to%2035%25%20in%20terms%20of%20accuracy%2C%20after%20applying%20our%20pipeline.%0AMoreover%2C%20using%20SAGI-D%20for%20training%20several%20image%20forensic%20approaches%20increases%0Ain-domain%20detection%20performance%20on%20average%20by%2037.4%25%20and%20out-of-domain%0Ageneralization%20by%2026.1%25%20in%20terms%20of%20IoU%2C%20also%20demonstrating%20its%20utility%20in%0Acountering%20malicious%20exploitation%20of%20generative%20AI.%20Code%20and%20dataset%20are%0Aavailable%20at%20https%3A//mever-team.github.io/SAGI/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06593v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGI%253A%2520Semantically%2520Aligned%2520and%2520Uncertainty%2520Guided%2520AI%2520Image%2520Inpainting%26entry.906535625%3DPaschalis%2520Giakoumoglou%2520and%2520Dimitrios%2520Karageorgiou%2520and%2520Symeon%2520Papadopoulos%2520and%2520Panagiotis%2520C.%2520Petrantonakis%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520AI%2520have%2520made%2520text-guided%2520image%2520inpainting%2520-%250Aadding%252C%2520removing%252C%2520or%2520altering%2520image%2520regions%2520using%2520textual%2520prompts%2520-%2520widely%250Aaccessible.%2520However%252C%2520generating%2520semantically%2520correct%2520photorealistic%2520imagery%252C%250Atypically%2520requires%2520carefully-crafted%2520prompts%2520and%2520iterative%2520refinement%2520by%250Aevaluating%2520the%2520realism%2520of%2520the%2520generated%2520content%2520-%2520tasks%2520commonly%2520performed%2520by%250Ahumans.%2520To%2520automate%2520the%2520generative%2520process%252C%2520we%2520propose%2520Semantically%2520Aligned%2520and%250AUncertainty%2520Guided%2520AI%2520Image%2520Inpainting%2520%2528SAGI%2529%252C%2520a%2520model-agnostic%2520pipeline%252C%2520to%250Asample%2520prompts%2520from%2520a%2520distribution%2520that%2520closely%2520aligns%2520with%2520human%2520perception%250Aand%2520to%2520evaluate%2520the%2520generated%2520content%2520and%2520discard%2520instances%2520that%2520deviate%2520from%250Asuch%2520a%2520distribution%252C%2520which%2520we%2520approximate%2520using%2520pretrained%2520large%2520language%250Amodels%2520and%2520vision-language%2520models.%2520By%2520applying%2520this%2520pipeline%2520on%2520multiple%250Astate-of-the-art%2520inpainting%2520models%252C%2520we%2520create%2520the%2520SAGI%2520Dataset%2520%2528SAGI-D%2529%252C%250Acurrently%2520the%2520largest%2520and%2520most%2520diverse%2520dataset%2520of%2520AI-generated%2520inpaintings%252C%250Acomprising%2520over%252095k%2520inpainted%2520images%2520and%2520a%2520human-evaluated%2520subset.%2520Our%250Aexperiments%2520show%2520that%2520semantic%2520alignment%2520significantly%2520improves%2520image%2520quality%250Aand%2520aesthetics%252C%2520while%2520uncertainty%2520guidance%2520effectively%2520identifies%2520realistic%250Amanipulations%2520-%2520human%2520ability%2520to%2520distinguish%2520inpainted%2520images%2520from%2520real%2520ones%250Adrops%2520from%252074%2525%2520to%252035%2525%2520in%2520terms%2520of%2520accuracy%252C%2520after%2520applying%2520our%2520pipeline.%250AMoreover%252C%2520using%2520SAGI-D%2520for%2520training%2520several%2520image%2520forensic%2520approaches%2520increases%250Ain-domain%2520detection%2520performance%2520on%2520average%2520by%252037.4%2525%2520and%2520out-of-domain%250Ageneralization%2520by%252026.1%2525%2520in%2520terms%2520of%2520IoU%252C%2520also%2520demonstrating%2520its%2520utility%2520in%250Acountering%2520malicious%2520exploitation%2520of%2520generative%2520AI.%2520Code%2520and%2520dataset%2520are%250Aavailable%2520at%2520https%253A//mever-team.github.io/SAGI/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06593v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGI%3A%20Semantically%20Aligned%20and%20Uncertainty%20Guided%20AI%20Image%20Inpainting&entry.906535625=Paschalis%20Giakoumoglou%20and%20Dimitrios%20Karageorgiou%20and%20Symeon%20Papadopoulos%20and%20Panagiotis%20C.%20Petrantonakis&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20AI%20have%20made%20text-guided%20image%20inpainting%20-%0Aadding%2C%20removing%2C%20or%20altering%20image%20regions%20using%20textual%20prompts%20-%20widely%0Aaccessible.%20However%2C%20generating%20semantically%20correct%20photorealistic%20imagery%2C%0Atypically%20requires%20carefully-crafted%20prompts%20and%20iterative%20refinement%20by%0Aevaluating%20the%20realism%20of%20the%20generated%20content%20-%20tasks%20commonly%20performed%20by%0Ahumans.%20To%20automate%20the%20generative%20process%2C%20we%20propose%20Semantically%20Aligned%20and%0AUncertainty%20Guided%20AI%20Image%20Inpainting%20%28SAGI%29%2C%20a%20model-agnostic%20pipeline%2C%20to%0Asample%20prompts%20from%20a%20distribution%20that%20closely%20aligns%20with%20human%20perception%0Aand%20to%20evaluate%20the%20generated%20content%20and%20discard%20instances%20that%20deviate%20from%0Asuch%20a%20distribution%2C%20which%20we%20approximate%20using%20pretrained%20large%20language%0Amodels%20and%20vision-language%20models.%20By%20applying%20this%20pipeline%20on%20multiple%0Astate-of-the-art%20inpainting%20models%2C%20we%20create%20the%20SAGI%20Dataset%20%28SAGI-D%29%2C%0Acurrently%20the%20largest%20and%20most%20diverse%20dataset%20of%20AI-generated%20inpaintings%2C%0Acomprising%20over%2095k%20inpainted%20images%20and%20a%20human-evaluated%20subset.%20Our%0Aexperiments%20show%20that%20semantic%20alignment%20significantly%20improves%20image%20quality%0Aand%20aesthetics%2C%20while%20uncertainty%20guidance%20effectively%20identifies%20realistic%0Amanipulations%20-%20human%20ability%20to%20distinguish%20inpainted%20images%20from%20real%20ones%0Adrops%20from%2074%25%20to%2035%25%20in%20terms%20of%20accuracy%2C%20after%20applying%20our%20pipeline.%0AMoreover%2C%20using%20SAGI-D%20for%20training%20several%20image%20forensic%20approaches%20increases%0Ain-domain%20detection%20performance%20on%20average%20by%2037.4%25%20and%20out-of-domain%0Ageneralization%20by%2026.1%25%20in%20terms%20of%20IoU%2C%20also%20demonstrating%20its%20utility%20in%0Acountering%20malicious%20exploitation%20of%20generative%20AI.%20Code%20and%20dataset%20are%0Aavailable%20at%20https%3A//mever-team.github.io/SAGI/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06593v3&entry.124074799=Read"},
{"title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens", "author": "Haohan Zheng and Zhenguo Zhang", "abstract": "  Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.\n", "link": "http://arxiv.org/abs/2508.02419v1", "date": "2025-08-04", "relevancy": 2.1519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality%20Bias%20in%20LVLMs%3A%20Analyzing%20and%20Mitigating%20Object%20Hallucination%0A%20%20via%20Attention%20Lens&body=Title%3A%20Modality%20Bias%20in%20LVLMs%3A%20Analyzing%20and%20Mitigating%20Object%20Hallucination%0A%20%20via%20Attention%20Lens%0AAuthor%3A%20Haohan%20Zheng%20and%20Zhenguo%20Zhang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20multimodal%0Acomprehension%20and%20reasoning%20capabilities%2C%20but%20they%20still%20suffer%20from%20severe%0Aobject%20hallucination.%20Previous%20studies%20primarily%20attribute%20the%20flaw%20to%0Alinguistic%20prior%20caused%20by%20the%20scale%20mismatch%20between%20visual%20encoders%20and%20large%0Alanguage%20models%20%28LLMs%29%20in%20LVLMs.%20Specifically%2C%20as%20current%20LVLMs%20are%20built%20upon%0ALLMs%2C%20they%20tend%20to%20over-rely%20on%20textual%20prompts%20and%20internal%20knowledge%20of%20LLMs%2C%0Agenerating%20descriptions%20inconsistent%20with%20visual%20cues.%20However%2C%20through%20an%0Ain-depth%20investigation%20of%20the%20hallucinated%20mechanisms%2C%20we%20empirically%20reveal%20a%0Apreviously%20overlooked%20phenomenon%3A%20LVLMs%20may%20ignore%20not%20only%20visual%20information%0Abut%20also%20textual%20modality%20during%20hallucination%2C%20a%20behavior%20termed%20as%20modality%0Abias%2C%20which%20indicates%20that%20LVLMs%20struggle%20to%20simultaneously%20attend%20to%20both%0Avisual%20and%20textual%20modalities%2C%20leading%20to%20fragmented%20understanding%20of%0Auser-provided%20instructions.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20training-free%20method%20to%20mitigate%20object%20hallucination.%20Concretely%2C%20we%0Aintervene%20and%20adjust%20the%20attention%20weights%20of%20textual%20and%20visual%20tokens%2C%0Abalancing%20cross-modal%20compatibility%20for%20better%20alignment%20with%20user%20intentions.%0AFurthermore%2C%20we%20adopt%20a%20contrastive%20decoding%20strategy%20to%20reduce%20the%20LVLM%27s%0Aoverreliance%20on%20its%20parametric%20knowledge%2C%20synergistically%20enhancing%20our%0Aattention%20manipulation.%20Extensive%20experiments%20confirm%20the%20widespread%20presence%0Aof%20modality%20bias%20in%20LVLMs.%20Notably%2C%20our%20method%20effectively%20mitigates%0Ahallucination%20across%20multiple%20open-source%20LVLMs%20and%20benchmarks%2C%20highlighting%0Aits%20generalizability%20and%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality%2520Bias%2520in%2520LVLMs%253A%2520Analyzing%2520and%2520Mitigating%2520Object%2520Hallucination%250A%2520%2520via%2520Attention%2520Lens%26entry.906535625%3DHaohan%2520Zheng%2520and%2520Zhenguo%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%2520multimodal%250Acomprehension%2520and%2520reasoning%2520capabilities%252C%2520but%2520they%2520still%2520suffer%2520from%2520severe%250Aobject%2520hallucination.%2520Previous%2520studies%2520primarily%2520attribute%2520the%2520flaw%2520to%250Alinguistic%2520prior%2520caused%2520by%2520the%2520scale%2520mismatch%2520between%2520visual%2520encoders%2520and%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520in%2520LVLMs.%2520Specifically%252C%2520as%2520current%2520LVLMs%2520are%2520built%2520upon%250ALLMs%252C%2520they%2520tend%2520to%2520over-rely%2520on%2520textual%2520prompts%2520and%2520internal%2520knowledge%2520of%2520LLMs%252C%250Agenerating%2520descriptions%2520inconsistent%2520with%2520visual%2520cues.%2520However%252C%2520through%2520an%250Ain-depth%2520investigation%2520of%2520the%2520hallucinated%2520mechanisms%252C%2520we%2520empirically%2520reveal%2520a%250Apreviously%2520overlooked%2520phenomenon%253A%2520LVLMs%2520may%2520ignore%2520not%2520only%2520visual%2520information%250Abut%2520also%2520textual%2520modality%2520during%2520hallucination%252C%2520a%2520behavior%2520termed%2520as%2520modality%250Abias%252C%2520which%2520indicates%2520that%2520LVLMs%2520struggle%2520to%2520simultaneously%2520attend%2520to%2520both%250Avisual%2520and%2520textual%2520modalities%252C%2520leading%2520to%2520fragmented%2520understanding%2520of%250Auser-provided%2520instructions.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aeffective%2520training-free%2520method%2520to%2520mitigate%2520object%2520hallucination.%2520Concretely%252C%2520we%250Aintervene%2520and%2520adjust%2520the%2520attention%2520weights%2520of%2520textual%2520and%2520visual%2520tokens%252C%250Abalancing%2520cross-modal%2520compatibility%2520for%2520better%2520alignment%2520with%2520user%2520intentions.%250AFurthermore%252C%2520we%2520adopt%2520a%2520contrastive%2520decoding%2520strategy%2520to%2520reduce%2520the%2520LVLM%2527s%250Aoverreliance%2520on%2520its%2520parametric%2520knowledge%252C%2520synergistically%2520enhancing%2520our%250Aattention%2520manipulation.%2520Extensive%2520experiments%2520confirm%2520the%2520widespread%2520presence%250Aof%2520modality%2520bias%2520in%2520LVLMs.%2520Notably%252C%2520our%2520method%2520effectively%2520mitigates%250Ahallucination%2520across%2520multiple%2520open-source%2520LVLMs%2520and%2520benchmarks%252C%2520highlighting%250Aits%2520generalizability%2520and%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality%20Bias%20in%20LVLMs%3A%20Analyzing%20and%20Mitigating%20Object%20Hallucination%0A%20%20via%20Attention%20Lens&entry.906535625=Haohan%20Zheng%20and%20Zhenguo%20Zhang&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20multimodal%0Acomprehension%20and%20reasoning%20capabilities%2C%20but%20they%20still%20suffer%20from%20severe%0Aobject%20hallucination.%20Previous%20studies%20primarily%20attribute%20the%20flaw%20to%0Alinguistic%20prior%20caused%20by%20the%20scale%20mismatch%20between%20visual%20encoders%20and%20large%0Alanguage%20models%20%28LLMs%29%20in%20LVLMs.%20Specifically%2C%20as%20current%20LVLMs%20are%20built%20upon%0ALLMs%2C%20they%20tend%20to%20over-rely%20on%20textual%20prompts%20and%20internal%20knowledge%20of%20LLMs%2C%0Agenerating%20descriptions%20inconsistent%20with%20visual%20cues.%20However%2C%20through%20an%0Ain-depth%20investigation%20of%20the%20hallucinated%20mechanisms%2C%20we%20empirically%20reveal%20a%0Apreviously%20overlooked%20phenomenon%3A%20LVLMs%20may%20ignore%20not%20only%20visual%20information%0Abut%20also%20textual%20modality%20during%20hallucination%2C%20a%20behavior%20termed%20as%20modality%0Abias%2C%20which%20indicates%20that%20LVLMs%20struggle%20to%20simultaneously%20attend%20to%20both%0Avisual%20and%20textual%20modalities%2C%20leading%20to%20fragmented%20understanding%20of%0Auser-provided%20instructions.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20training-free%20method%20to%20mitigate%20object%20hallucination.%20Concretely%2C%20we%0Aintervene%20and%20adjust%20the%20attention%20weights%20of%20textual%20and%20visual%20tokens%2C%0Abalancing%20cross-modal%20compatibility%20for%20better%20alignment%20with%20user%20intentions.%0AFurthermore%2C%20we%20adopt%20a%20contrastive%20decoding%20strategy%20to%20reduce%20the%20LVLM%27s%0Aoverreliance%20on%20its%20parametric%20knowledge%2C%20synergistically%20enhancing%20our%0Aattention%20manipulation.%20Extensive%20experiments%20confirm%20the%20widespread%20presence%0Aof%20modality%20bias%20in%20LVLMs.%20Notably%2C%20our%20method%20effectively%20mitigates%0Ahallucination%20across%20multiple%20open-source%20LVLMs%20and%20benchmarks%2C%20highlighting%0Aits%20generalizability%20and%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02419v1&entry.124074799=Read"},
{"title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis", "author": "Yuzhuang Xu and Xu Han and Yuanchi Zhang and Yixuan Wang and Yijun Liu and Shiyu Ji and Qingfu Zhu and Wanxiang Che", "abstract": "  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.\n", "link": "http://arxiv.org/abs/2508.02322v1", "date": "2025-08-04", "relevancy": 2.1517, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis&body=Title%3A%20CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis%0AAuthor%3A%20Yuzhuang%20Xu%20and%20Xu%20Han%20and%20Yuanchi%20Zhang%20and%20Yixuan%20Wang%20and%20Yijun%20Liu%20and%20Shiyu%20Ji%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20Mixture-of-Experts%20%28MoE%29%20architectures%20are%0Adistinguished%20by%20their%20strong%20performance%20scaling%20with%20increasing%20parameters%0Aacross%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20also%20suffer%20from%20substantial%0Acomputational%20and%20storage%20overheads.%20Notably%2C%20the%20performance%20gains%20of%20MoE%0Amodels%20do%20not%20scale%20proportionally%20with%20the%20growth%20in%20expert%20parameters.%20While%0Aprior%20works%20attempt%20to%20reduce%20parameters%20via%20expert-level%20pruning%2C%20merging%2C%20or%0Adecomposition%2C%20they%20still%20suffer%20from%20challenges%20in%20both%20performance%20and%0Acomputational%20efficiency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%0Aintroducing%20micro-expert%20as%20a%20finer-grained%20compression%20unit%20that%20spans%20across%0Amatrices.%20We%20first%20establish%20a%20more%20fundamental%20perspective%2C%20viewing%20MoE%20layers%0Aas%20mixtures%20of%20micro-experts%2C%20and%20present%20CAMERA%2C%20a%20lightweight%20and%0Atraining-free%20framework%20for%20identifying%20micro-expert%20redundancy.%20Our%20analysis%0Auncovers%20significant%20variance%20in%20micro-expert%20contributions%20during%20decoding.%0ABased%20on%20this%20insight%2C%20we%20further%20propose%20CAMERA-P%2C%20a%20structured%20micro-expert%0Apruning%20framework%2C%20and%20CAMERA-Q%2C%20a%20mixed-precision%20quantization%20idea%20designed%0Afor%20micro-experts.%20Extensive%20experiments%20on%20nine%20downstream%20tasks%20show%20that%0ACAMERA-P%20consistently%20outperforms%20strong%20baselines%20under%20pruning%20ratios%20ranging%0Afrom%2020%25%20to%2060%25.%20Furthermore%2C%20CAMERA-Q%20achieves%20superior%20results%20under%0Aaggressive%202-bit%20quantization%2C%20surpassing%20existing%20matrix-%20and%20channel-level%0Aideas.%20Notably%2C%20our%20method%20enables%20complete%20micro-expert%20analysis%20of%0AQwen2-57B-A14B%20in%20less%20than%205%20minutes%20on%20a%20single%20NVIDIA%20A100-40GB%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMERA%253A%2520Multi-Matrix%2520Joint%2520Compression%2520for%2520MoE%2520Models%2520via%2520Micro-Expert%250A%2520%2520Redundancy%2520Analysis%26entry.906535625%3DYuzhuang%2520Xu%2520and%2520Xu%2520Han%2520and%2520Yuanchi%2520Zhang%2520and%2520Yixuan%2520Wang%2520and%2520Yijun%2520Liu%2520and%2520Shiyu%2520Ji%2520and%2520Qingfu%2520Zhu%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520are%250Adistinguished%2520by%2520their%2520strong%2520performance%2520scaling%2520with%2520increasing%2520parameters%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%252C%2520yet%2520they%2520also%2520suffer%2520from%2520substantial%250Acomputational%2520and%2520storage%2520overheads.%2520Notably%252C%2520the%2520performance%2520gains%2520of%2520MoE%250Amodels%2520do%2520not%2520scale%2520proportionally%2520with%2520the%2520growth%2520in%2520expert%2520parameters.%2520While%250Aprior%2520works%2520attempt%2520to%2520reduce%2520parameters%2520via%2520expert-level%2520pruning%252C%2520merging%252C%2520or%250Adecomposition%252C%2520they%2520still%2520suffer%2520from%2520challenges%2520in%2520both%2520performance%2520and%250Acomputational%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%250Aintroducing%2520micro-expert%2520as%2520a%2520finer-grained%2520compression%2520unit%2520that%2520spans%2520across%250Amatrices.%2520We%2520first%2520establish%2520a%2520more%2520fundamental%2520perspective%252C%2520viewing%2520MoE%2520layers%250Aas%2520mixtures%2520of%2520micro-experts%252C%2520and%2520present%2520CAMERA%252C%2520a%2520lightweight%2520and%250Atraining-free%2520framework%2520for%2520identifying%2520micro-expert%2520redundancy.%2520Our%2520analysis%250Auncovers%2520significant%2520variance%2520in%2520micro-expert%2520contributions%2520during%2520decoding.%250ABased%2520on%2520this%2520insight%252C%2520we%2520further%2520propose%2520CAMERA-P%252C%2520a%2520structured%2520micro-expert%250Apruning%2520framework%252C%2520and%2520CAMERA-Q%252C%2520a%2520mixed-precision%2520quantization%2520idea%2520designed%250Afor%2520micro-experts.%2520Extensive%2520experiments%2520on%2520nine%2520downstream%2520tasks%2520show%2520that%250ACAMERA-P%2520consistently%2520outperforms%2520strong%2520baselines%2520under%2520pruning%2520ratios%2520ranging%250Afrom%252020%2525%2520to%252060%2525.%2520Furthermore%252C%2520CAMERA-Q%2520achieves%2520superior%2520results%2520under%250Aaggressive%25202-bit%2520quantization%252C%2520surpassing%2520existing%2520matrix-%2520and%2520channel-level%250Aideas.%2520Notably%252C%2520our%2520method%2520enables%2520complete%2520micro-expert%2520analysis%2520of%250AQwen2-57B-A14B%2520in%2520less%2520than%25205%2520minutes%2520on%2520a%2520single%2520NVIDIA%2520A100-40GB%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis&entry.906535625=Yuzhuang%20Xu%20and%20Xu%20Han%20and%20Yuanchi%20Zhang%20and%20Yixuan%20Wang%20and%20Yijun%20Liu%20and%20Shiyu%20Ji%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20Mixture-of-Experts%20%28MoE%29%20architectures%20are%0Adistinguished%20by%20their%20strong%20performance%20scaling%20with%20increasing%20parameters%0Aacross%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20also%20suffer%20from%20substantial%0Acomputational%20and%20storage%20overheads.%20Notably%2C%20the%20performance%20gains%20of%20MoE%0Amodels%20do%20not%20scale%20proportionally%20with%20the%20growth%20in%20expert%20parameters.%20While%0Aprior%20works%20attempt%20to%20reduce%20parameters%20via%20expert-level%20pruning%2C%20merging%2C%20or%0Adecomposition%2C%20they%20still%20suffer%20from%20challenges%20in%20both%20performance%20and%0Acomputational%20efficiency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%0Aintroducing%20micro-expert%20as%20a%20finer-grained%20compression%20unit%20that%20spans%20across%0Amatrices.%20We%20first%20establish%20a%20more%20fundamental%20perspective%2C%20viewing%20MoE%20layers%0Aas%20mixtures%20of%20micro-experts%2C%20and%20present%20CAMERA%2C%20a%20lightweight%20and%0Atraining-free%20framework%20for%20identifying%20micro-expert%20redundancy.%20Our%20analysis%0Auncovers%20significant%20variance%20in%20micro-expert%20contributions%20during%20decoding.%0ABased%20on%20this%20insight%2C%20we%20further%20propose%20CAMERA-P%2C%20a%20structured%20micro-expert%0Apruning%20framework%2C%20and%20CAMERA-Q%2C%20a%20mixed-precision%20quantization%20idea%20designed%0Afor%20micro-experts.%20Extensive%20experiments%20on%20nine%20downstream%20tasks%20show%20that%0ACAMERA-P%20consistently%20outperforms%20strong%20baselines%20under%20pruning%20ratios%20ranging%0Afrom%2020%25%20to%2060%25.%20Furthermore%2C%20CAMERA-Q%20achieves%20superior%20results%20under%0Aaggressive%202-bit%20quantization%2C%20surpassing%20existing%20matrix-%20and%20channel-level%0Aideas.%20Notably%2C%20our%20method%20enables%20complete%20micro-expert%20analysis%20of%0AQwen2-57B-A14B%20in%20less%20than%205%20minutes%20on%20a%20single%20NVIDIA%20A100-40GB%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02322v1&entry.124074799=Read"},
{"title": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection", "author": "Biqiao Xin and Qianchen Mao and Bingshu Wang and Jiangbin Zheng and Yong Zhao and C. L. Philip Chen", "abstract": "  The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models.\n", "link": "http://arxiv.org/abs/2503.02662v3", "date": "2025-08-04", "relevancy": 2.1435, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5619}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5381}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%2010K%20is%20Enough%3A%20An%20Ultra-Lightweight%20Binarized%20Network%20for%20Infrared%0A%20%20Small-Target%20Detection&body=Title%3A%2010K%20is%20Enough%3A%20An%20Ultra-Lightweight%20Binarized%20Network%20for%20Infrared%0A%20%20Small-Target%20Detection%0AAuthor%3A%20Biqiao%20Xin%20and%20Qianchen%20Mao%20and%20Bingshu%20Wang%20and%20Jiangbin%20Zheng%20and%20Yong%20Zhao%20and%20C.%20L.%20Philip%20Chen%0AAbstract%3A%20%20%20The%20widespread%20deployment%20of%20Infrared%20Small-Target%20Detection%20%28IRSTD%29%0Aalgorithms%20on%20edge%20devices%20necessitates%20the%20exploration%20of%20model%20compression%0Atechniques.%20Binarized%20neural%20networks%20%28BNNs%29%20are%20distinguished%20by%20their%0Aexceptional%20efficiency%20in%20model%20compression.%20However%2C%20the%20small%20size%20of%0Ainfrared%20targets%20introduces%20stringent%20precision%20requirements%20for%20the%20IRSTD%0Atask%2C%20while%20the%20inherent%20precision%20loss%20during%20binarization%20presents%20a%0Asignificant%20challenge.%20To%20address%20this%2C%20we%20propose%20the%20Binarized%20Infrared%0ASmall-Target%20Detection%20Network%20%28BiisNet%29%2C%20which%20preserves%20the%20core%20operations%0Aof%20binarized%20convolutions%20while%20integrating%20full-precision%20features%20into%20the%0Anetwork%27s%20information%20flow.%20Specifically%2C%20we%20propose%20the%20Dot%20Binary%0AConvolution%2C%20which%20retains%20fine-grained%20semantic%20information%20in%20feature%20maps%0Awhile%20still%20leveraging%20the%20binarized%20convolution%20operations.%20In%20addition%2C%20we%0Aintroduce%20a%20smooth%20and%20adaptive%20Dynamic%20Softsign%20function%2C%20which%20provides%20more%0Acomprehensive%20and%20progressively%20finer%20gradient%20during%20backpropagation%2C%0Aenhancing%20model%20stability%20and%20promoting%20an%20optimal%20weight%20distribution.%0AExperimental%20results%20demonstrate%20that%20BiisNet%20not%20only%20significantly%0Aoutperforms%20other%20binary%20architectures%20but%20also%20has%20strong%20competitiveness%0Aamong%20state-of-the-art%20full-precision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02662v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D10K%2520is%2520Enough%253A%2520An%2520Ultra-Lightweight%2520Binarized%2520Network%2520for%2520Infrared%250A%2520%2520Small-Target%2520Detection%26entry.906535625%3DBiqiao%2520Xin%2520and%2520Qianchen%2520Mao%2520and%2520Bingshu%2520Wang%2520and%2520Jiangbin%2520Zheng%2520and%2520Yong%2520Zhao%2520and%2520C.%2520L.%2520Philip%2520Chen%26entry.1292438233%3D%2520%2520The%2520widespread%2520deployment%2520of%2520Infrared%2520Small-Target%2520Detection%2520%2528IRSTD%2529%250Aalgorithms%2520on%2520edge%2520devices%2520necessitates%2520the%2520exploration%2520of%2520model%2520compression%250Atechniques.%2520Binarized%2520neural%2520networks%2520%2528BNNs%2529%2520are%2520distinguished%2520by%2520their%250Aexceptional%2520efficiency%2520in%2520model%2520compression.%2520However%252C%2520the%2520small%2520size%2520of%250Ainfrared%2520targets%2520introduces%2520stringent%2520precision%2520requirements%2520for%2520the%2520IRSTD%250Atask%252C%2520while%2520the%2520inherent%2520precision%2520loss%2520during%2520binarization%2520presents%2520a%250Asignificant%2520challenge.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Binarized%2520Infrared%250ASmall-Target%2520Detection%2520Network%2520%2528BiisNet%2529%252C%2520which%2520preserves%2520the%2520core%2520operations%250Aof%2520binarized%2520convolutions%2520while%2520integrating%2520full-precision%2520features%2520into%2520the%250Anetwork%2527s%2520information%2520flow.%2520Specifically%252C%2520we%2520propose%2520the%2520Dot%2520Binary%250AConvolution%252C%2520which%2520retains%2520fine-grained%2520semantic%2520information%2520in%2520feature%2520maps%250Awhile%2520still%2520leveraging%2520the%2520binarized%2520convolution%2520operations.%2520In%2520addition%252C%2520we%250Aintroduce%2520a%2520smooth%2520and%2520adaptive%2520Dynamic%2520Softsign%2520function%252C%2520which%2520provides%2520more%250Acomprehensive%2520and%2520progressively%2520finer%2520gradient%2520during%2520backpropagation%252C%250Aenhancing%2520model%2520stability%2520and%2520promoting%2520an%2520optimal%2520weight%2520distribution.%250AExperimental%2520results%2520demonstrate%2520that%2520BiisNet%2520not%2520only%2520significantly%250Aoutperforms%2520other%2520binary%2520architectures%2520but%2520also%2520has%2520strong%2520competitiveness%250Aamong%2520state-of-the-art%2520full-precision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02662v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=10K%20is%20Enough%3A%20An%20Ultra-Lightweight%20Binarized%20Network%20for%20Infrared%0A%20%20Small-Target%20Detection&entry.906535625=Biqiao%20Xin%20and%20Qianchen%20Mao%20and%20Bingshu%20Wang%20and%20Jiangbin%20Zheng%20and%20Yong%20Zhao%20and%20C.%20L.%20Philip%20Chen&entry.1292438233=%20%20The%20widespread%20deployment%20of%20Infrared%20Small-Target%20Detection%20%28IRSTD%29%0Aalgorithms%20on%20edge%20devices%20necessitates%20the%20exploration%20of%20model%20compression%0Atechniques.%20Binarized%20neural%20networks%20%28BNNs%29%20are%20distinguished%20by%20their%0Aexceptional%20efficiency%20in%20model%20compression.%20However%2C%20the%20small%20size%20of%0Ainfrared%20targets%20introduces%20stringent%20precision%20requirements%20for%20the%20IRSTD%0Atask%2C%20while%20the%20inherent%20precision%20loss%20during%20binarization%20presents%20a%0Asignificant%20challenge.%20To%20address%20this%2C%20we%20propose%20the%20Binarized%20Infrared%0ASmall-Target%20Detection%20Network%20%28BiisNet%29%2C%20which%20preserves%20the%20core%20operations%0Aof%20binarized%20convolutions%20while%20integrating%20full-precision%20features%20into%20the%0Anetwork%27s%20information%20flow.%20Specifically%2C%20we%20propose%20the%20Dot%20Binary%0AConvolution%2C%20which%20retains%20fine-grained%20semantic%20information%20in%20feature%20maps%0Awhile%20still%20leveraging%20the%20binarized%20convolution%20operations.%20In%20addition%2C%20we%0Aintroduce%20a%20smooth%20and%20adaptive%20Dynamic%20Softsign%20function%2C%20which%20provides%20more%0Acomprehensive%20and%20progressively%20finer%20gradient%20during%20backpropagation%2C%0Aenhancing%20model%20stability%20and%20promoting%20an%20optimal%20weight%20distribution.%0AExperimental%20results%20demonstrate%20that%20BiisNet%20not%20only%20significantly%0Aoutperforms%20other%20binary%20architectures%20but%20also%20has%20strong%20competitiveness%0Aamong%20state-of-the-art%20full-precision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02662v3&entry.124074799=Read"},
{"title": "DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on\n  Regulatory DNA", "author": "Aman Patel and Arpita Singhal and Austin Wang and Anusri Pampari and Maya Kasowski and Anshul Kundaje", "abstract": "  Recent advances in self-supervised models for natural language, vision, and\nprotein sequences have inspired the development of large genomic DNA language\nmodels (DNALMs). These models aim to learn generalizable representations of\ndiverse DNA elements, potentially enabling various genomic prediction,\ninterpretation and design tasks. Despite their potential, existing benchmarks\ndo not adequately assess the capabilities of DNALMs on key downstream\napplications involving an important class of non-coding DNA elements critical\nfor regulating gene activity. In this study, we introduce DART-Eval, a suite of\nrepresentative benchmarks specifically focused on regulatory DNA to evaluate\nmodel performance across zero-shot, probed, and fine-tuned scenarios against\ncontemporary ab initio models as baselines. Our benchmarks target biologically\nmeaningful downstream tasks such as functional sequence feature discovery,\npredicting cell-type specific regulatory activity, and counterfactual\nprediction of the impacts of genetic variants. We find that current DNALMs\nexhibit inconsistent performance and do not offer compelling gains over\nalternative baseline models for most tasks, while requiring significantly more\ncomputational resources. We discuss potentially promising modeling, data\ncuration, and evaluation strategies for the next generation of DNALMs. Our code\nis available at https://github.com/kundajelab/DART-Eval.\n", "link": "http://arxiv.org/abs/2412.05430v2", "date": "2025-08-04", "relevancy": 2.1434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART-Eval%3A%20A%20Comprehensive%20DNA%20Language%20Model%20Evaluation%20Benchmark%20on%0A%20%20Regulatory%20DNA&body=Title%3A%20DART-Eval%3A%20A%20Comprehensive%20DNA%20Language%20Model%20Evaluation%20Benchmark%20on%0A%20%20Regulatory%20DNA%0AAuthor%3A%20Aman%20Patel%20and%20Arpita%20Singhal%20and%20Austin%20Wang%20and%20Anusri%20Pampari%20and%20Maya%20Kasowski%20and%20Anshul%20Kundaje%0AAbstract%3A%20%20%20Recent%20advances%20in%20self-supervised%20models%20for%20natural%20language%2C%20vision%2C%20and%0Aprotein%20sequences%20have%20inspired%20the%20development%20of%20large%20genomic%20DNA%20language%0Amodels%20%28DNALMs%29.%20These%20models%20aim%20to%20learn%20generalizable%20representations%20of%0Adiverse%20DNA%20elements%2C%20potentially%20enabling%20various%20genomic%20prediction%2C%0Ainterpretation%20and%20design%20tasks.%20Despite%20their%20potential%2C%20existing%20benchmarks%0Ado%20not%20adequately%20assess%20the%20capabilities%20of%20DNALMs%20on%20key%20downstream%0Aapplications%20involving%20an%20important%20class%20of%20non-coding%20DNA%20elements%20critical%0Afor%20regulating%20gene%20activity.%20In%20this%20study%2C%20we%20introduce%20DART-Eval%2C%20a%20suite%20of%0Arepresentative%20benchmarks%20specifically%20focused%20on%20regulatory%20DNA%20to%20evaluate%0Amodel%20performance%20across%20zero-shot%2C%20probed%2C%20and%20fine-tuned%20scenarios%20against%0Acontemporary%20ab%20initio%20models%20as%20baselines.%20Our%20benchmarks%20target%20biologically%0Ameaningful%20downstream%20tasks%20such%20as%20functional%20sequence%20feature%20discovery%2C%0Apredicting%20cell-type%20specific%20regulatory%20activity%2C%20and%20counterfactual%0Aprediction%20of%20the%20impacts%20of%20genetic%20variants.%20We%20find%20that%20current%20DNALMs%0Aexhibit%20inconsistent%20performance%20and%20do%20not%20offer%20compelling%20gains%20over%0Aalternative%20baseline%20models%20for%20most%20tasks%2C%20while%20requiring%20significantly%20more%0Acomputational%20resources.%20We%20discuss%20potentially%20promising%20modeling%2C%20data%0Acuration%2C%20and%20evaluation%20strategies%20for%20the%20next%20generation%20of%20DNALMs.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/kundajelab/DART-Eval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART-Eval%253A%2520A%2520Comprehensive%2520DNA%2520Language%2520Model%2520Evaluation%2520Benchmark%2520on%250A%2520%2520Regulatory%2520DNA%26entry.906535625%3DAman%2520Patel%2520and%2520Arpita%2520Singhal%2520and%2520Austin%2520Wang%2520and%2520Anusri%2520Pampari%2520and%2520Maya%2520Kasowski%2520and%2520Anshul%2520Kundaje%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520self-supervised%2520models%2520for%2520natural%2520language%252C%2520vision%252C%2520and%250Aprotein%2520sequences%2520have%2520inspired%2520the%2520development%2520of%2520large%2520genomic%2520DNA%2520language%250Amodels%2520%2528DNALMs%2529.%2520These%2520models%2520aim%2520to%2520learn%2520generalizable%2520representations%2520of%250Adiverse%2520DNA%2520elements%252C%2520potentially%2520enabling%2520various%2520genomic%2520prediction%252C%250Ainterpretation%2520and%2520design%2520tasks.%2520Despite%2520their%2520potential%252C%2520existing%2520benchmarks%250Ado%2520not%2520adequately%2520assess%2520the%2520capabilities%2520of%2520DNALMs%2520on%2520key%2520downstream%250Aapplications%2520involving%2520an%2520important%2520class%2520of%2520non-coding%2520DNA%2520elements%2520critical%250Afor%2520regulating%2520gene%2520activity.%2520In%2520this%2520study%252C%2520we%2520introduce%2520DART-Eval%252C%2520a%2520suite%2520of%250Arepresentative%2520benchmarks%2520specifically%2520focused%2520on%2520regulatory%2520DNA%2520to%2520evaluate%250Amodel%2520performance%2520across%2520zero-shot%252C%2520probed%252C%2520and%2520fine-tuned%2520scenarios%2520against%250Acontemporary%2520ab%2520initio%2520models%2520as%2520baselines.%2520Our%2520benchmarks%2520target%2520biologically%250Ameaningful%2520downstream%2520tasks%2520such%2520as%2520functional%2520sequence%2520feature%2520discovery%252C%250Apredicting%2520cell-type%2520specific%2520regulatory%2520activity%252C%2520and%2520counterfactual%250Aprediction%2520of%2520the%2520impacts%2520of%2520genetic%2520variants.%2520We%2520find%2520that%2520current%2520DNALMs%250Aexhibit%2520inconsistent%2520performance%2520and%2520do%2520not%2520offer%2520compelling%2520gains%2520over%250Aalternative%2520baseline%2520models%2520for%2520most%2520tasks%252C%2520while%2520requiring%2520significantly%2520more%250Acomputational%2520resources.%2520We%2520discuss%2520potentially%2520promising%2520modeling%252C%2520data%250Acuration%252C%2520and%2520evaluation%2520strategies%2520for%2520the%2520next%2520generation%2520of%2520DNALMs.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/kundajelab/DART-Eval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART-Eval%3A%20A%20Comprehensive%20DNA%20Language%20Model%20Evaluation%20Benchmark%20on%0A%20%20Regulatory%20DNA&entry.906535625=Aman%20Patel%20and%20Arpita%20Singhal%20and%20Austin%20Wang%20and%20Anusri%20Pampari%20and%20Maya%20Kasowski%20and%20Anshul%20Kundaje&entry.1292438233=%20%20Recent%20advances%20in%20self-supervised%20models%20for%20natural%20language%2C%20vision%2C%20and%0Aprotein%20sequences%20have%20inspired%20the%20development%20of%20large%20genomic%20DNA%20language%0Amodels%20%28DNALMs%29.%20These%20models%20aim%20to%20learn%20generalizable%20representations%20of%0Adiverse%20DNA%20elements%2C%20potentially%20enabling%20various%20genomic%20prediction%2C%0Ainterpretation%20and%20design%20tasks.%20Despite%20their%20potential%2C%20existing%20benchmarks%0Ado%20not%20adequately%20assess%20the%20capabilities%20of%20DNALMs%20on%20key%20downstream%0Aapplications%20involving%20an%20important%20class%20of%20non-coding%20DNA%20elements%20critical%0Afor%20regulating%20gene%20activity.%20In%20this%20study%2C%20we%20introduce%20DART-Eval%2C%20a%20suite%20of%0Arepresentative%20benchmarks%20specifically%20focused%20on%20regulatory%20DNA%20to%20evaluate%0Amodel%20performance%20across%20zero-shot%2C%20probed%2C%20and%20fine-tuned%20scenarios%20against%0Acontemporary%20ab%20initio%20models%20as%20baselines.%20Our%20benchmarks%20target%20biologically%0Ameaningful%20downstream%20tasks%20such%20as%20functional%20sequence%20feature%20discovery%2C%0Apredicting%20cell-type%20specific%20regulatory%20activity%2C%20and%20counterfactual%0Aprediction%20of%20the%20impacts%20of%20genetic%20variants.%20We%20find%20that%20current%20DNALMs%0Aexhibit%20inconsistent%20performance%20and%20do%20not%20offer%20compelling%20gains%20over%0Aalternative%20baseline%20models%20for%20most%20tasks%2C%20while%20requiring%20significantly%20more%0Acomputational%20resources.%20We%20discuss%20potentially%20promising%20modeling%2C%20data%0Acuration%2C%20and%20evaluation%20strategies%20for%20the%20next%20generation%20of%20DNALMs.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/kundajelab/DART-Eval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05430v2&entry.124074799=Read"},
{"title": "DHO$_2$: Accelerating Distributed Hybrid Order Optimization via Model\n  Parallelism and ADMM", "author": "Shunxian Gu and Chaoqun You and Bangbang Ren and Lailong Luo and Junxu Xia and Deke Guo", "abstract": "  Scaling deep neural network (DNN) training to more devices can reduce\ntime-to-solution. However, it is impractical for users with limited computing\nresources. FOSI, as a hybrid order optimizer, converges faster than\nconventional optimizers by taking advantage of both gradient information and\ncurvature information when updating the DNN model. Therefore, it provides a new\nchance for accelerating DNN training in the resource-constrained setting. In\nthis paper, we explore its distributed design, namely DHO$_2$, including\ndistributed calculation of curvature information and model update with partial\ncurvature information to accelerate DNN training with a low memory burden. To\nfurther reduce the training time, we design a novel strategy to parallelize the\ncalculation of curvature information and the model update on different devices.\nExperimentally, our distributed design can achieve an approximate linear\nreduction of memory burden on each device with the increase of the device\nnumber. Meanwhile, it achieves $1.4\\times\\sim2.1\\times$ speedup in the total\ntraining time compared with other distributed designs based on conventional\nfirst- and second-order optimizers.\n", "link": "http://arxiv.org/abs/2505.00982v2", "date": "2025-08-04", "relevancy": 2.1396, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5461}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5317}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DHO%24_2%24%3A%20Accelerating%20Distributed%20Hybrid%20Order%20Optimization%20via%20Model%0A%20%20Parallelism%20and%20ADMM&body=Title%3A%20DHO%24_2%24%3A%20Accelerating%20Distributed%20Hybrid%20Order%20Optimization%20via%20Model%0A%20%20Parallelism%20and%20ADMM%0AAuthor%3A%20Shunxian%20Gu%20and%20Chaoqun%20You%20and%20Bangbang%20Ren%20and%20Lailong%20Luo%20and%20Junxu%20Xia%20and%20Deke%20Guo%0AAbstract%3A%20%20%20Scaling%20deep%20neural%20network%20%28DNN%29%20training%20to%20more%20devices%20can%20reduce%0Atime-to-solution.%20However%2C%20it%20is%20impractical%20for%20users%20with%20limited%20computing%0Aresources.%20FOSI%2C%20as%20a%20hybrid%20order%20optimizer%2C%20converges%20faster%20than%0Aconventional%20optimizers%20by%20taking%20advantage%20of%20both%20gradient%20information%20and%0Acurvature%20information%20when%20updating%20the%20DNN%20model.%20Therefore%2C%20it%20provides%20a%20new%0Achance%20for%20accelerating%20DNN%20training%20in%20the%20resource-constrained%20setting.%20In%0Athis%20paper%2C%20we%20explore%20its%20distributed%20design%2C%20namely%20DHO%24_2%24%2C%20including%0Adistributed%20calculation%20of%20curvature%20information%20and%20model%20update%20with%20partial%0Acurvature%20information%20to%20accelerate%20DNN%20training%20with%20a%20low%20memory%20burden.%20To%0Afurther%20reduce%20the%20training%20time%2C%20we%20design%20a%20novel%20strategy%20to%20parallelize%20the%0Acalculation%20of%20curvature%20information%20and%20the%20model%20update%20on%20different%20devices.%0AExperimentally%2C%20our%20distributed%20design%20can%20achieve%20an%20approximate%20linear%0Areduction%20of%20memory%20burden%20on%20each%20device%20with%20the%20increase%20of%20the%20device%0Anumber.%20Meanwhile%2C%20it%20achieves%20%241.4%5Ctimes%5Csim2.1%5Ctimes%24%20speedup%20in%20the%20total%0Atraining%20time%20compared%20with%20other%20distributed%20designs%20based%20on%20conventional%0Afirst-%20and%20second-order%20optimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDHO%2524_2%2524%253A%2520Accelerating%2520Distributed%2520Hybrid%2520Order%2520Optimization%2520via%2520Model%250A%2520%2520Parallelism%2520and%2520ADMM%26entry.906535625%3DShunxian%2520Gu%2520and%2520Chaoqun%2520You%2520and%2520Bangbang%2520Ren%2520and%2520Lailong%2520Luo%2520and%2520Junxu%2520Xia%2520and%2520Deke%2520Guo%26entry.1292438233%3D%2520%2520Scaling%2520deep%2520neural%2520network%2520%2528DNN%2529%2520training%2520to%2520more%2520devices%2520can%2520reduce%250Atime-to-solution.%2520However%252C%2520it%2520is%2520impractical%2520for%2520users%2520with%2520limited%2520computing%250Aresources.%2520FOSI%252C%2520as%2520a%2520hybrid%2520order%2520optimizer%252C%2520converges%2520faster%2520than%250Aconventional%2520optimizers%2520by%2520taking%2520advantage%2520of%2520both%2520gradient%2520information%2520and%250Acurvature%2520information%2520when%2520updating%2520the%2520DNN%2520model.%2520Therefore%252C%2520it%2520provides%2520a%2520new%250Achance%2520for%2520accelerating%2520DNN%2520training%2520in%2520the%2520resource-constrained%2520setting.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520its%2520distributed%2520design%252C%2520namely%2520DHO%2524_2%2524%252C%2520including%250Adistributed%2520calculation%2520of%2520curvature%2520information%2520and%2520model%2520update%2520with%2520partial%250Acurvature%2520information%2520to%2520accelerate%2520DNN%2520training%2520with%2520a%2520low%2520memory%2520burden.%2520To%250Afurther%2520reduce%2520the%2520training%2520time%252C%2520we%2520design%2520a%2520novel%2520strategy%2520to%2520parallelize%2520the%250Acalculation%2520of%2520curvature%2520information%2520and%2520the%2520model%2520update%2520on%2520different%2520devices.%250AExperimentally%252C%2520our%2520distributed%2520design%2520can%2520achieve%2520an%2520approximate%2520linear%250Areduction%2520of%2520memory%2520burden%2520on%2520each%2520device%2520with%2520the%2520increase%2520of%2520the%2520device%250Anumber.%2520Meanwhile%252C%2520it%2520achieves%2520%25241.4%255Ctimes%255Csim2.1%255Ctimes%2524%2520speedup%2520in%2520the%2520total%250Atraining%2520time%2520compared%2520with%2520other%2520distributed%2520designs%2520based%2520on%2520conventional%250Afirst-%2520and%2520second-order%2520optimizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DHO%24_2%24%3A%20Accelerating%20Distributed%20Hybrid%20Order%20Optimization%20via%20Model%0A%20%20Parallelism%20and%20ADMM&entry.906535625=Shunxian%20Gu%20and%20Chaoqun%20You%20and%20Bangbang%20Ren%20and%20Lailong%20Luo%20and%20Junxu%20Xia%20and%20Deke%20Guo&entry.1292438233=%20%20Scaling%20deep%20neural%20network%20%28DNN%29%20training%20to%20more%20devices%20can%20reduce%0Atime-to-solution.%20However%2C%20it%20is%20impractical%20for%20users%20with%20limited%20computing%0Aresources.%20FOSI%2C%20as%20a%20hybrid%20order%20optimizer%2C%20converges%20faster%20than%0Aconventional%20optimizers%20by%20taking%20advantage%20of%20both%20gradient%20information%20and%0Acurvature%20information%20when%20updating%20the%20DNN%20model.%20Therefore%2C%20it%20provides%20a%20new%0Achance%20for%20accelerating%20DNN%20training%20in%20the%20resource-constrained%20setting.%20In%0Athis%20paper%2C%20we%20explore%20its%20distributed%20design%2C%20namely%20DHO%24_2%24%2C%20including%0Adistributed%20calculation%20of%20curvature%20information%20and%20model%20update%20with%20partial%0Acurvature%20information%20to%20accelerate%20DNN%20training%20with%20a%20low%20memory%20burden.%20To%0Afurther%20reduce%20the%20training%20time%2C%20we%20design%20a%20novel%20strategy%20to%20parallelize%20the%0Acalculation%20of%20curvature%20information%20and%20the%20model%20update%20on%20different%20devices.%0AExperimentally%2C%20our%20distributed%20design%20can%20achieve%20an%20approximate%20linear%0Areduction%20of%20memory%20burden%20on%20each%20device%20with%20the%20increase%20of%20the%20device%0Anumber.%20Meanwhile%2C%20it%20achieves%20%241.4%5Ctimes%5Csim2.1%5Ctimes%24%20speedup%20in%20the%20total%0Atraining%20time%20compared%20with%20other%20distributed%20designs%20based%20on%20conventional%0Afirst-%20and%20second-order%20optimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00982v2&entry.124074799=Read"},
{"title": "From Pixels to Pathology: Restoration Diffusion for\n  Diagnostic-Consistent Virtual IHC", "author": "Jingsong Liu and Xiaofeng Deng and Han Li and Azar Kazemi and Christian Grashei and Gesa Wilkens and Xin You and Tanja Groll and Nassir Navab and Carolin Mogler and Peter J. Sch\u00fcffler", "abstract": "  Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis.\n", "link": "http://arxiv.org/abs/2508.02528v1", "date": "2025-08-04", "relevancy": 2.1344, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5452}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Pathology%3A%20Restoration%20Diffusion%20for%0A%20%20Diagnostic-Consistent%20Virtual%20IHC&body=Title%3A%20From%20Pixels%20to%20Pathology%3A%20Restoration%20Diffusion%20for%0A%20%20Diagnostic-Consistent%20Virtual%20IHC%0AAuthor%3A%20Jingsong%20Liu%20and%20Xiaofeng%20Deng%20and%20Han%20Li%20and%20Azar%20Kazemi%20and%20Christian%20Grashei%20and%20Gesa%20Wilkens%20and%20Xin%20You%20and%20Tanja%20Groll%20and%20Nassir%20Navab%20and%20Carolin%20Mogler%20and%20Peter%20J.%20Sch%C3%BCffler%0AAbstract%3A%20%20%20Hematoxylin%20and%20eosin%20%28H%26E%29%20staining%20is%20the%20clinical%20standard%20for%20assessing%0Atissue%20morphology%2C%20but%20it%20lacks%20molecular-level%20diagnostic%20information.%20In%0Acontrast%2C%20immunohistochemistry%20%28IHC%29%20provides%20crucial%20insights%20into%20biomarker%0Aexpression%2C%20such%20as%20HER2%20status%20for%20breast%20cancer%20grading%2C%20but%20remains%20costly%0Aand%20time-consuming%2C%20limiting%20its%20use%20in%20time-sensitive%20clinical%20workflows.%20To%0Aaddress%20this%20gap%2C%20virtual%20staining%20from%20H%26E%20to%20IHC%20has%20emerged%20as%20a%20promising%0Aalternative%2C%20yet%20faces%20two%20core%20challenges%3A%20%281%29%20Lack%20of%20fair%20evaluation%20of%0Asynthetic%20images%20against%20misaligned%20IHC%20ground%20truths%2C%20and%20%282%29%20preserving%0Astructural%20integrity%20and%20biological%20variability%20during%20translation.%20To%20this%0Aend%2C%20we%20present%20an%20end-to-end%20framework%20encompassing%20both%20generation%20and%0Aevaluation%20in%20this%20work.%20We%20introduce%20Star-Diff%2C%20a%20structure-aware%20staining%0Arestoration%20diffusion%20model%20that%20reformulates%20virtual%20staining%20as%20an%20image%0Arestoration%20task.%20By%20combining%20residual%20and%20noise-based%20generation%20pathways%2C%0AStar-Diff%20maintains%20tissue%20structure%20while%20modeling%20realistic%20biomarker%0Avariability.%20To%20evaluate%20the%20diagnostic%20consistency%20of%20the%20generated%20IHC%0Apatches%2C%20we%20propose%20the%20Semantic%20Fidelity%20Score%20%28SFS%29%2C%20a%0Aclinical-grading-task-driven%20metric%20that%20quantifies%20class-wise%20semantic%0Adegradation%20based%20on%20biomarker%20classification%20accuracy.%20Unlike%20pixel-level%0Ametrics%20such%20as%20SSIM%20and%20PSNR%2C%20SFS%20remains%20robust%20under%20spatial%20misalignment%0Aand%20classifier%20uncertainty.%20Experiments%20on%20the%20BCI%20dataset%20demonstrate%20that%0AStar-Diff%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20in%20both%20visual%20fidelity%0Aand%20diagnostic%20relevance.%20With%20rapid%20inference%20and%20strong%20clinical%20alignment%2Cit%0Apresents%20a%20practical%20solution%20for%20applications%20such%20as%20intraoperative%20virtual%0AIHC%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Pathology%253A%2520Restoration%2520Diffusion%2520for%250A%2520%2520Diagnostic-Consistent%2520Virtual%2520IHC%26entry.906535625%3DJingsong%2520Liu%2520and%2520Xiaofeng%2520Deng%2520and%2520Han%2520Li%2520and%2520Azar%2520Kazemi%2520and%2520Christian%2520Grashei%2520and%2520Gesa%2520Wilkens%2520and%2520Xin%2520You%2520and%2520Tanja%2520Groll%2520and%2520Nassir%2520Navab%2520and%2520Carolin%2520Mogler%2520and%2520Peter%2520J.%2520Sch%25C3%25BCffler%26entry.1292438233%3D%2520%2520Hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520staining%2520is%2520the%2520clinical%2520standard%2520for%2520assessing%250Atissue%2520morphology%252C%2520but%2520it%2520lacks%2520molecular-level%2520diagnostic%2520information.%2520In%250Acontrast%252C%2520immunohistochemistry%2520%2528IHC%2529%2520provides%2520crucial%2520insights%2520into%2520biomarker%250Aexpression%252C%2520such%2520as%2520HER2%2520status%2520for%2520breast%2520cancer%2520grading%252C%2520but%2520remains%2520costly%250Aand%2520time-consuming%252C%2520limiting%2520its%2520use%2520in%2520time-sensitive%2520clinical%2520workflows.%2520To%250Aaddress%2520this%2520gap%252C%2520virtual%2520staining%2520from%2520H%2526E%2520to%2520IHC%2520has%2520emerged%2520as%2520a%2520promising%250Aalternative%252C%2520yet%2520faces%2520two%2520core%2520challenges%253A%2520%25281%2529%2520Lack%2520of%2520fair%2520evaluation%2520of%250Asynthetic%2520images%2520against%2520misaligned%2520IHC%2520ground%2520truths%252C%2520and%2520%25282%2529%2520preserving%250Astructural%2520integrity%2520and%2520biological%2520variability%2520during%2520translation.%2520To%2520this%250Aend%252C%2520we%2520present%2520an%2520end-to-end%2520framework%2520encompassing%2520both%2520generation%2520and%250Aevaluation%2520in%2520this%2520work.%2520We%2520introduce%2520Star-Diff%252C%2520a%2520structure-aware%2520staining%250Arestoration%2520diffusion%2520model%2520that%2520reformulates%2520virtual%2520staining%2520as%2520an%2520image%250Arestoration%2520task.%2520By%2520combining%2520residual%2520and%2520noise-based%2520generation%2520pathways%252C%250AStar-Diff%2520maintains%2520tissue%2520structure%2520while%2520modeling%2520realistic%2520biomarker%250Avariability.%2520To%2520evaluate%2520the%2520diagnostic%2520consistency%2520of%2520the%2520generated%2520IHC%250Apatches%252C%2520we%2520propose%2520the%2520Semantic%2520Fidelity%2520Score%2520%2528SFS%2529%252C%2520a%250Aclinical-grading-task-driven%2520metric%2520that%2520quantifies%2520class-wise%2520semantic%250Adegradation%2520based%2520on%2520biomarker%2520classification%2520accuracy.%2520Unlike%2520pixel-level%250Ametrics%2520such%2520as%2520SSIM%2520and%2520PSNR%252C%2520SFS%2520remains%2520robust%2520under%2520spatial%2520misalignment%250Aand%2520classifier%2520uncertainty.%2520Experiments%2520on%2520the%2520BCI%2520dataset%2520demonstrate%2520that%250AStar-Diff%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520both%2520visual%2520fidelity%250Aand%2520diagnostic%2520relevance.%2520With%2520rapid%2520inference%2520and%2520strong%2520clinical%2520alignment%252Cit%250Apresents%2520a%2520practical%2520solution%2520for%2520applications%2520such%2520as%2520intraoperative%2520virtual%250AIHC%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Pathology%3A%20Restoration%20Diffusion%20for%0A%20%20Diagnostic-Consistent%20Virtual%20IHC&entry.906535625=Jingsong%20Liu%20and%20Xiaofeng%20Deng%20and%20Han%20Li%20and%20Azar%20Kazemi%20and%20Christian%20Grashei%20and%20Gesa%20Wilkens%20and%20Xin%20You%20and%20Tanja%20Groll%20and%20Nassir%20Navab%20and%20Carolin%20Mogler%20and%20Peter%20J.%20Sch%C3%BCffler&entry.1292438233=%20%20Hematoxylin%20and%20eosin%20%28H%26E%29%20staining%20is%20the%20clinical%20standard%20for%20assessing%0Atissue%20morphology%2C%20but%20it%20lacks%20molecular-level%20diagnostic%20information.%20In%0Acontrast%2C%20immunohistochemistry%20%28IHC%29%20provides%20crucial%20insights%20into%20biomarker%0Aexpression%2C%20such%20as%20HER2%20status%20for%20breast%20cancer%20grading%2C%20but%20remains%20costly%0Aand%20time-consuming%2C%20limiting%20its%20use%20in%20time-sensitive%20clinical%20workflows.%20To%0Aaddress%20this%20gap%2C%20virtual%20staining%20from%20H%26E%20to%20IHC%20has%20emerged%20as%20a%20promising%0Aalternative%2C%20yet%20faces%20two%20core%20challenges%3A%20%281%29%20Lack%20of%20fair%20evaluation%20of%0Asynthetic%20images%20against%20misaligned%20IHC%20ground%20truths%2C%20and%20%282%29%20preserving%0Astructural%20integrity%20and%20biological%20variability%20during%20translation.%20To%20this%0Aend%2C%20we%20present%20an%20end-to-end%20framework%20encompassing%20both%20generation%20and%0Aevaluation%20in%20this%20work.%20We%20introduce%20Star-Diff%2C%20a%20structure-aware%20staining%0Arestoration%20diffusion%20model%20that%20reformulates%20virtual%20staining%20as%20an%20image%0Arestoration%20task.%20By%20combining%20residual%20and%20noise-based%20generation%20pathways%2C%0AStar-Diff%20maintains%20tissue%20structure%20while%20modeling%20realistic%20biomarker%0Avariability.%20To%20evaluate%20the%20diagnostic%20consistency%20of%20the%20generated%20IHC%0Apatches%2C%20we%20propose%20the%20Semantic%20Fidelity%20Score%20%28SFS%29%2C%20a%0Aclinical-grading-task-driven%20metric%20that%20quantifies%20class-wise%20semantic%0Adegradation%20based%20on%20biomarker%20classification%20accuracy.%20Unlike%20pixel-level%0Ametrics%20such%20as%20SSIM%20and%20PSNR%2C%20SFS%20remains%20robust%20under%20spatial%20misalignment%0Aand%20classifier%20uncertainty.%20Experiments%20on%20the%20BCI%20dataset%20demonstrate%20that%0AStar-Diff%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20in%20both%20visual%20fidelity%0Aand%20diagnostic%20relevance.%20With%20rapid%20inference%20and%20strong%20clinical%20alignment%2Cit%0Apresents%20a%20practical%20solution%20for%20applications%20such%20as%20intraoperative%20virtual%0AIHC%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02528v1&entry.124074799=Read"},
{"title": "Adaptive Lattice-based Motion Planning", "author": "Abhishek Dhar and Sarthak Mishra and Spandan Roy and Daniel Axehill", "abstract": "  This paper proposes an adaptive lattice-based motion planning solution to\naddress the problem of generating feasible trajectories for systems,\nrepresented by a linearly parameterizable non-linear model operating within a\ncluttered environment. The system model is considered to have uncertain model\nparameters. The key idea here is to utilize input/output data online to update\nthe model set containing the uncertain system parameter, as well as a dynamic\nestimated parameter of the model, so that the associated model estimation error\nreduces over time. This in turn improves the quality of the motion primitives\ngenerated by the lattice-based motion planner using a nominal estimated model\nselected on the basis of suitable criteria. The motion primitives are also\nequipped with tubes to account for the model mismatch between the nominal\nestimated model and the true system model, to guarantee collision-free overall\nmotion. The tubes are of uniform size, which is directly proportional to the\nsize of the model set containing the uncertain system parameter. The adaptive\nlearning module guarantees a reduction in the diameter of the model set as well\nas in the parameter estimation error between the dynamic estimated parameter\nand the true system parameter. This directly implies a reduction in the size of\nthe implemented tubes and guarantees that the utilized motion primitives go\narbitrarily close to the resolution-optimal motion primitives associated with\nthe true model of the system, thus significantly improving the overall motion\nplanning performance over time. The efficiency of the motion planner is\ndemonstrated by a suitable simulation example that considers a drone model\nrepresented by Euler-Lagrange dynamics containing uncertain parameters and\noperating within a cluttered environment.\n", "link": "http://arxiv.org/abs/2508.02350v1", "date": "2025-08-04", "relevancy": 2.1282, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5387}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Lattice-based%20Motion%20Planning&body=Title%3A%20Adaptive%20Lattice-based%20Motion%20Planning%0AAuthor%3A%20Abhishek%20Dhar%20and%20Sarthak%20Mishra%20and%20Spandan%20Roy%20and%20Daniel%20Axehill%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20adaptive%20lattice-based%20motion%20planning%20solution%20to%0Aaddress%20the%20problem%20of%20generating%20feasible%20trajectories%20for%20systems%2C%0Arepresented%20by%20a%20linearly%20parameterizable%20non-linear%20model%20operating%20within%20a%0Acluttered%20environment.%20The%20system%20model%20is%20considered%20to%20have%20uncertain%20model%0Aparameters.%20The%20key%20idea%20here%20is%20to%20utilize%20input/output%20data%20online%20to%20update%0Athe%20model%20set%20containing%20the%20uncertain%20system%20parameter%2C%20as%20well%20as%20a%20dynamic%0Aestimated%20parameter%20of%20the%20model%2C%20so%20that%20the%20associated%20model%20estimation%20error%0Areduces%20over%20time.%20This%20in%20turn%20improves%20the%20quality%20of%20the%20motion%20primitives%0Agenerated%20by%20the%20lattice-based%20motion%20planner%20using%20a%20nominal%20estimated%20model%0Aselected%20on%20the%20basis%20of%20suitable%20criteria.%20The%20motion%20primitives%20are%20also%0Aequipped%20with%20tubes%20to%20account%20for%20the%20model%20mismatch%20between%20the%20nominal%0Aestimated%20model%20and%20the%20true%20system%20model%2C%20to%20guarantee%20collision-free%20overall%0Amotion.%20The%20tubes%20are%20of%20uniform%20size%2C%20which%20is%20directly%20proportional%20to%20the%0Asize%20of%20the%20model%20set%20containing%20the%20uncertain%20system%20parameter.%20The%20adaptive%0Alearning%20module%20guarantees%20a%20reduction%20in%20the%20diameter%20of%20the%20model%20set%20as%20well%0Aas%20in%20the%20parameter%20estimation%20error%20between%20the%20dynamic%20estimated%20parameter%0Aand%20the%20true%20system%20parameter.%20This%20directly%20implies%20a%20reduction%20in%20the%20size%20of%0Athe%20implemented%20tubes%20and%20guarantees%20that%20the%20utilized%20motion%20primitives%20go%0Aarbitrarily%20close%20to%20the%20resolution-optimal%20motion%20primitives%20associated%20with%0Athe%20true%20model%20of%20the%20system%2C%20thus%20significantly%20improving%20the%20overall%20motion%0Aplanning%20performance%20over%20time.%20The%20efficiency%20of%20the%20motion%20planner%20is%0Ademonstrated%20by%20a%20suitable%20simulation%20example%20that%20considers%20a%20drone%20model%0Arepresented%20by%20Euler-Lagrange%20dynamics%20containing%20uncertain%20parameters%20and%0Aoperating%20within%20a%20cluttered%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Lattice-based%2520Motion%2520Planning%26entry.906535625%3DAbhishek%2520Dhar%2520and%2520Sarthak%2520Mishra%2520and%2520Spandan%2520Roy%2520and%2520Daniel%2520Axehill%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520an%2520adaptive%2520lattice-based%2520motion%2520planning%2520solution%2520to%250Aaddress%2520the%2520problem%2520of%2520generating%2520feasible%2520trajectories%2520for%2520systems%252C%250Arepresented%2520by%2520a%2520linearly%2520parameterizable%2520non-linear%2520model%2520operating%2520within%2520a%250Acluttered%2520environment.%2520The%2520system%2520model%2520is%2520considered%2520to%2520have%2520uncertain%2520model%250Aparameters.%2520The%2520key%2520idea%2520here%2520is%2520to%2520utilize%2520input/output%2520data%2520online%2520to%2520update%250Athe%2520model%2520set%2520containing%2520the%2520uncertain%2520system%2520parameter%252C%2520as%2520well%2520as%2520a%2520dynamic%250Aestimated%2520parameter%2520of%2520the%2520model%252C%2520so%2520that%2520the%2520associated%2520model%2520estimation%2520error%250Areduces%2520over%2520time.%2520This%2520in%2520turn%2520improves%2520the%2520quality%2520of%2520the%2520motion%2520primitives%250Agenerated%2520by%2520the%2520lattice-based%2520motion%2520planner%2520using%2520a%2520nominal%2520estimated%2520model%250Aselected%2520on%2520the%2520basis%2520of%2520suitable%2520criteria.%2520The%2520motion%2520primitives%2520are%2520also%250Aequipped%2520with%2520tubes%2520to%2520account%2520for%2520the%2520model%2520mismatch%2520between%2520the%2520nominal%250Aestimated%2520model%2520and%2520the%2520true%2520system%2520model%252C%2520to%2520guarantee%2520collision-free%2520overall%250Amotion.%2520The%2520tubes%2520are%2520of%2520uniform%2520size%252C%2520which%2520is%2520directly%2520proportional%2520to%2520the%250Asize%2520of%2520the%2520model%2520set%2520containing%2520the%2520uncertain%2520system%2520parameter.%2520The%2520adaptive%250Alearning%2520module%2520guarantees%2520a%2520reduction%2520in%2520the%2520diameter%2520of%2520the%2520model%2520set%2520as%2520well%250Aas%2520in%2520the%2520parameter%2520estimation%2520error%2520between%2520the%2520dynamic%2520estimated%2520parameter%250Aand%2520the%2520true%2520system%2520parameter.%2520This%2520directly%2520implies%2520a%2520reduction%2520in%2520the%2520size%2520of%250Athe%2520implemented%2520tubes%2520and%2520guarantees%2520that%2520the%2520utilized%2520motion%2520primitives%2520go%250Aarbitrarily%2520close%2520to%2520the%2520resolution-optimal%2520motion%2520primitives%2520associated%2520with%250Athe%2520true%2520model%2520of%2520the%2520system%252C%2520thus%2520significantly%2520improving%2520the%2520overall%2520motion%250Aplanning%2520performance%2520over%2520time.%2520The%2520efficiency%2520of%2520the%2520motion%2520planner%2520is%250Ademonstrated%2520by%2520a%2520suitable%2520simulation%2520example%2520that%2520considers%2520a%2520drone%2520model%250Arepresented%2520by%2520Euler-Lagrange%2520dynamics%2520containing%2520uncertain%2520parameters%2520and%250Aoperating%2520within%2520a%2520cluttered%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Lattice-based%20Motion%20Planning&entry.906535625=Abhishek%20Dhar%20and%20Sarthak%20Mishra%20and%20Spandan%20Roy%20and%20Daniel%20Axehill&entry.1292438233=%20%20This%20paper%20proposes%20an%20adaptive%20lattice-based%20motion%20planning%20solution%20to%0Aaddress%20the%20problem%20of%20generating%20feasible%20trajectories%20for%20systems%2C%0Arepresented%20by%20a%20linearly%20parameterizable%20non-linear%20model%20operating%20within%20a%0Acluttered%20environment.%20The%20system%20model%20is%20considered%20to%20have%20uncertain%20model%0Aparameters.%20The%20key%20idea%20here%20is%20to%20utilize%20input/output%20data%20online%20to%20update%0Athe%20model%20set%20containing%20the%20uncertain%20system%20parameter%2C%20as%20well%20as%20a%20dynamic%0Aestimated%20parameter%20of%20the%20model%2C%20so%20that%20the%20associated%20model%20estimation%20error%0Areduces%20over%20time.%20This%20in%20turn%20improves%20the%20quality%20of%20the%20motion%20primitives%0Agenerated%20by%20the%20lattice-based%20motion%20planner%20using%20a%20nominal%20estimated%20model%0Aselected%20on%20the%20basis%20of%20suitable%20criteria.%20The%20motion%20primitives%20are%20also%0Aequipped%20with%20tubes%20to%20account%20for%20the%20model%20mismatch%20between%20the%20nominal%0Aestimated%20model%20and%20the%20true%20system%20model%2C%20to%20guarantee%20collision-free%20overall%0Amotion.%20The%20tubes%20are%20of%20uniform%20size%2C%20which%20is%20directly%20proportional%20to%20the%0Asize%20of%20the%20model%20set%20containing%20the%20uncertain%20system%20parameter.%20The%20adaptive%0Alearning%20module%20guarantees%20a%20reduction%20in%20the%20diameter%20of%20the%20model%20set%20as%20well%0Aas%20in%20the%20parameter%20estimation%20error%20between%20the%20dynamic%20estimated%20parameter%0Aand%20the%20true%20system%20parameter.%20This%20directly%20implies%20a%20reduction%20in%20the%20size%20of%0Athe%20implemented%20tubes%20and%20guarantees%20that%20the%20utilized%20motion%20primitives%20go%0Aarbitrarily%20close%20to%20the%20resolution-optimal%20motion%20primitives%20associated%20with%0Athe%20true%20model%20of%20the%20system%2C%20thus%20significantly%20improving%20the%20overall%20motion%0Aplanning%20performance%20over%20time.%20The%20efficiency%20of%20the%20motion%20planner%20is%0Ademonstrated%20by%20a%20suitable%20simulation%20example%20that%20considers%20a%20drone%20model%0Arepresented%20by%20Euler-Lagrange%20dynamics%20containing%20uncertain%20parameters%20and%0Aoperating%20within%20a%20cluttered%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02350v1&entry.124074799=Read"},
{"title": "Toward Using Machine Learning as a Shape Quality Metric for Liver Point\n  Cloud Generation", "author": "Khoa Tuan Nguyen and Gaeun Oh and Ho-min Park and Francesca Tozzi and Wouter Willaert and Joris Vankerschaver and Niki Rashidian and Wesley De Neve", "abstract": "  While 3D medical shape generative models such as diffusion models have shown\npromise in synthesizing diverse and anatomically plausible structures, the\nabsence of ground truth makes quality evaluation challenging. Existing\nevaluation metrics commonly measure distributional distances between training\nand generated sets, while the medical field requires assessing quality at the\nindividual level for each generated shape, which demands labor-intensive expert\nreview.\n  In this paper, we investigate the use of classical machine learning (ML)\nmethods and PointNet as an alternative, interpretable approach for assessing\nthe quality of generated liver shapes. We sample point clouds from the surfaces\nof the generated liver shapes, extract handcrafted geometric features, and\ntrain a group of supervised ML and PointNet models to classify liver shapes as\ngood or bad. These trained models are then used as proxy discriminators to\nassess the quality of synthetic liver shapes produced by generative models.\n  Our results show that ML-based shape classifiers provide not only\ninterpretable feedback but also complementary insights compared to expert\nevaluation. This suggests that ML classifiers can serve as lightweight,\ntask-relevant quality metrics in 3D organ shape generation, supporting more\ntransparent and clinically aligned evaluation protocols in medical shape\nmodeling.\n", "link": "http://arxiv.org/abs/2508.02482v1", "date": "2025-08-04", "relevancy": 2.1186, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5329}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Using%20Machine%20Learning%20as%20a%20Shape%20Quality%20Metric%20for%20Liver%20Point%0A%20%20Cloud%20Generation&body=Title%3A%20Toward%20Using%20Machine%20Learning%20as%20a%20Shape%20Quality%20Metric%20for%20Liver%20Point%0A%20%20Cloud%20Generation%0AAuthor%3A%20Khoa%20Tuan%20Nguyen%20and%20Gaeun%20Oh%20and%20Ho-min%20Park%20and%20Francesca%20Tozzi%20and%20Wouter%20Willaert%20and%20Joris%20Vankerschaver%20and%20Niki%20Rashidian%20and%20Wesley%20De%20Neve%0AAbstract%3A%20%20%20While%203D%20medical%20shape%20generative%20models%20such%20as%20diffusion%20models%20have%20shown%0Apromise%20in%20synthesizing%20diverse%20and%20anatomically%20plausible%20structures%2C%20the%0Aabsence%20of%20ground%20truth%20makes%20quality%20evaluation%20challenging.%20Existing%0Aevaluation%20metrics%20commonly%20measure%20distributional%20distances%20between%20training%0Aand%20generated%20sets%2C%20while%20the%20medical%20field%20requires%20assessing%20quality%20at%20the%0Aindividual%20level%20for%20each%20generated%20shape%2C%20which%20demands%20labor-intensive%20expert%0Areview.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20use%20of%20classical%20machine%20learning%20%28ML%29%0Amethods%20and%20PointNet%20as%20an%20alternative%2C%20interpretable%20approach%20for%20assessing%0Athe%20quality%20of%20generated%20liver%20shapes.%20We%20sample%20point%20clouds%20from%20the%20surfaces%0Aof%20the%20generated%20liver%20shapes%2C%20extract%20handcrafted%20geometric%20features%2C%20and%0Atrain%20a%20group%20of%20supervised%20ML%20and%20PointNet%20models%20to%20classify%20liver%20shapes%20as%0Agood%20or%20bad.%20These%20trained%20models%20are%20then%20used%20as%20proxy%20discriminators%20to%0Aassess%20the%20quality%20of%20synthetic%20liver%20shapes%20produced%20by%20generative%20models.%0A%20%20Our%20results%20show%20that%20ML-based%20shape%20classifiers%20provide%20not%20only%0Ainterpretable%20feedback%20but%20also%20complementary%20insights%20compared%20to%20expert%0Aevaluation.%20This%20suggests%20that%20ML%20classifiers%20can%20serve%20as%20lightweight%2C%0Atask-relevant%20quality%20metrics%20in%203D%20organ%20shape%20generation%2C%20supporting%20more%0Atransparent%20and%20clinically%20aligned%20evaluation%20protocols%20in%20medical%20shape%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Using%2520Machine%2520Learning%2520as%2520a%2520Shape%2520Quality%2520Metric%2520for%2520Liver%2520Point%250A%2520%2520Cloud%2520Generation%26entry.906535625%3DKhoa%2520Tuan%2520Nguyen%2520and%2520Gaeun%2520Oh%2520and%2520Ho-min%2520Park%2520and%2520Francesca%2520Tozzi%2520and%2520Wouter%2520Willaert%2520and%2520Joris%2520Vankerschaver%2520and%2520Niki%2520Rashidian%2520and%2520Wesley%2520De%2520Neve%26entry.1292438233%3D%2520%2520While%25203D%2520medical%2520shape%2520generative%2520models%2520such%2520as%2520diffusion%2520models%2520have%2520shown%250Apromise%2520in%2520synthesizing%2520diverse%2520and%2520anatomically%2520plausible%2520structures%252C%2520the%250Aabsence%2520of%2520ground%2520truth%2520makes%2520quality%2520evaluation%2520challenging.%2520Existing%250Aevaluation%2520metrics%2520commonly%2520measure%2520distributional%2520distances%2520between%2520training%250Aand%2520generated%2520sets%252C%2520while%2520the%2520medical%2520field%2520requires%2520assessing%2520quality%2520at%2520the%250Aindividual%2520level%2520for%2520each%2520generated%2520shape%252C%2520which%2520demands%2520labor-intensive%2520expert%250Areview.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520use%2520of%2520classical%2520machine%2520learning%2520%2528ML%2529%250Amethods%2520and%2520PointNet%2520as%2520an%2520alternative%252C%2520interpretable%2520approach%2520for%2520assessing%250Athe%2520quality%2520of%2520generated%2520liver%2520shapes.%2520We%2520sample%2520point%2520clouds%2520from%2520the%2520surfaces%250Aof%2520the%2520generated%2520liver%2520shapes%252C%2520extract%2520handcrafted%2520geometric%2520features%252C%2520and%250Atrain%2520a%2520group%2520of%2520supervised%2520ML%2520and%2520PointNet%2520models%2520to%2520classify%2520liver%2520shapes%2520as%250Agood%2520or%2520bad.%2520These%2520trained%2520models%2520are%2520then%2520used%2520as%2520proxy%2520discriminators%2520to%250Aassess%2520the%2520quality%2520of%2520synthetic%2520liver%2520shapes%2520produced%2520by%2520generative%2520models.%250A%2520%2520Our%2520results%2520show%2520that%2520ML-based%2520shape%2520classifiers%2520provide%2520not%2520only%250Ainterpretable%2520feedback%2520but%2520also%2520complementary%2520insights%2520compared%2520to%2520expert%250Aevaluation.%2520This%2520suggests%2520that%2520ML%2520classifiers%2520can%2520serve%2520as%2520lightweight%252C%250Atask-relevant%2520quality%2520metrics%2520in%25203D%2520organ%2520shape%2520generation%252C%2520supporting%2520more%250Atransparent%2520and%2520clinically%2520aligned%2520evaluation%2520protocols%2520in%2520medical%2520shape%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Using%20Machine%20Learning%20as%20a%20Shape%20Quality%20Metric%20for%20Liver%20Point%0A%20%20Cloud%20Generation&entry.906535625=Khoa%20Tuan%20Nguyen%20and%20Gaeun%20Oh%20and%20Ho-min%20Park%20and%20Francesca%20Tozzi%20and%20Wouter%20Willaert%20and%20Joris%20Vankerschaver%20and%20Niki%20Rashidian%20and%20Wesley%20De%20Neve&entry.1292438233=%20%20While%203D%20medical%20shape%20generative%20models%20such%20as%20diffusion%20models%20have%20shown%0Apromise%20in%20synthesizing%20diverse%20and%20anatomically%20plausible%20structures%2C%20the%0Aabsence%20of%20ground%20truth%20makes%20quality%20evaluation%20challenging.%20Existing%0Aevaluation%20metrics%20commonly%20measure%20distributional%20distances%20between%20training%0Aand%20generated%20sets%2C%20while%20the%20medical%20field%20requires%20assessing%20quality%20at%20the%0Aindividual%20level%20for%20each%20generated%20shape%2C%20which%20demands%20labor-intensive%20expert%0Areview.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20use%20of%20classical%20machine%20learning%20%28ML%29%0Amethods%20and%20PointNet%20as%20an%20alternative%2C%20interpretable%20approach%20for%20assessing%0Athe%20quality%20of%20generated%20liver%20shapes.%20We%20sample%20point%20clouds%20from%20the%20surfaces%0Aof%20the%20generated%20liver%20shapes%2C%20extract%20handcrafted%20geometric%20features%2C%20and%0Atrain%20a%20group%20of%20supervised%20ML%20and%20PointNet%20models%20to%20classify%20liver%20shapes%20as%0Agood%20or%20bad.%20These%20trained%20models%20are%20then%20used%20as%20proxy%20discriminators%20to%0Aassess%20the%20quality%20of%20synthetic%20liver%20shapes%20produced%20by%20generative%20models.%0A%20%20Our%20results%20show%20that%20ML-based%20shape%20classifiers%20provide%20not%20only%0Ainterpretable%20feedback%20but%20also%20complementary%20insights%20compared%20to%20expert%0Aevaluation.%20This%20suggests%20that%20ML%20classifiers%20can%20serve%20as%20lightweight%2C%0Atask-relevant%20quality%20metrics%20in%203D%20organ%20shape%20generation%2C%20supporting%20more%0Atransparent%20and%20clinically%20aligned%20evaluation%20protocols%20in%20medical%20shape%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02482v1&entry.124074799=Read"},
{"title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models", "author": "Dewi Sid William Gould and George De Ath and Ben Carvell and Nick Pepper", "abstract": "  The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, AirTrafficGen, that leverages large language models (LLMs)\nto automate and control the generation of complex ATC scenarios. Our method\nuses a purpose-built, graph-based representation to encode sector topology\n(including airspace geometry, routes, and fixes) into a format LLMs can\nprocess. Through rigorous benchmarking, we show that state-of-the-art models\nlike Gemini 2.5 Pro and OpenAI o3 can generate high-traffic scenarios whilst\nmaintaining operational realism. Our engineered prompting enables fine-grained\ncontrol over interaction presence, type, and location. Initial findings suggest\nthese models are also capable of iterative refinement, correcting flawed\nscenarios based on simple textual feedback. This approach provides a scalable\nalternative to manual scenario design, addressing the need for a greater volume\nand variety of ATC training and validation simulations. More broadly, this work\nshowcases the potential of LLMs for complex planning in safety-critical\ndomains.\n", "link": "http://arxiv.org/abs/2508.02269v1", "date": "2025-08-04", "relevancy": 2.1172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Dewi%20Sid%20William%20Gould%20and%20George%20De%20Ath%20and%20Ben%20Carvell%20and%20Nick%20Pepper%0AAbstract%3A%20%20%20The%20manual%20design%20of%20scenarios%20for%20Air%20Traffic%20Control%20%28ATC%29%20training%20is%20a%0Ademanding%20and%20time-consuming%20bottleneck%20that%20limits%20the%20diversity%20of%0Asimulations%20available%20to%20controllers.%20To%20address%20this%2C%20we%20introduce%20a%20novel%2C%0Aend-to-end%20approach%2C%20AirTrafficGen%2C%20that%20leverages%20large%20language%20models%20%28LLMs%29%0Ato%20automate%20and%20control%20the%20generation%20of%20complex%20ATC%20scenarios.%20Our%20method%0Auses%20a%20purpose-built%2C%20graph-based%20representation%20to%20encode%20sector%20topology%0A%28including%20airspace%20geometry%2C%20routes%2C%20and%20fixes%29%20into%20a%20format%20LLMs%20can%0Aprocess.%20Through%20rigorous%20benchmarking%2C%20we%20show%20that%20state-of-the-art%20models%0Alike%20Gemini%202.5%20Pro%20and%20OpenAI%20o3%20can%20generate%20high-traffic%20scenarios%20whilst%0Amaintaining%20operational%20realism.%20Our%20engineered%20prompting%20enables%20fine-grained%0Acontrol%20over%20interaction%20presence%2C%20type%2C%20and%20location.%20Initial%20findings%20suggest%0Athese%20models%20are%20also%20capable%20of%20iterative%20refinement%2C%20correcting%20flawed%0Ascenarios%20based%20on%20simple%20textual%20feedback.%20This%20approach%20provides%20a%20scalable%0Aalternative%20to%20manual%20scenario%20design%2C%20addressing%20the%20need%20for%20a%20greater%20volume%0Aand%20variety%20of%20ATC%20training%20and%20validation%20simulations.%20More%20broadly%2C%20this%20work%0Ashowcases%20the%20potential%20of%20LLMs%20for%20complex%20planning%20in%20safety-critical%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirTrafficGen%253A%2520Configurable%2520Air%2520Traffic%2520Scenario%2520Generation%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DDewi%2520Sid%2520William%2520Gould%2520and%2520George%2520De%2520Ath%2520and%2520Ben%2520Carvell%2520and%2520Nick%2520Pepper%26entry.1292438233%3D%2520%2520The%2520manual%2520design%2520of%2520scenarios%2520for%2520Air%2520Traffic%2520Control%2520%2528ATC%2529%2520training%2520is%2520a%250Ademanding%2520and%2520time-consuming%2520bottleneck%2520that%2520limits%2520the%2520diversity%2520of%250Asimulations%2520available%2520to%2520controllers.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%252C%250Aend-to-end%2520approach%252C%2520AirTrafficGen%252C%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%250Ato%2520automate%2520and%2520control%2520the%2520generation%2520of%2520complex%2520ATC%2520scenarios.%2520Our%2520method%250Auses%2520a%2520purpose-built%252C%2520graph-based%2520representation%2520to%2520encode%2520sector%2520topology%250A%2528including%2520airspace%2520geometry%252C%2520routes%252C%2520and%2520fixes%2529%2520into%2520a%2520format%2520LLMs%2520can%250Aprocess.%2520Through%2520rigorous%2520benchmarking%252C%2520we%2520show%2520that%2520state-of-the-art%2520models%250Alike%2520Gemini%25202.5%2520Pro%2520and%2520OpenAI%2520o3%2520can%2520generate%2520high-traffic%2520scenarios%2520whilst%250Amaintaining%2520operational%2520realism.%2520Our%2520engineered%2520prompting%2520enables%2520fine-grained%250Acontrol%2520over%2520interaction%2520presence%252C%2520type%252C%2520and%2520location.%2520Initial%2520findings%2520suggest%250Athese%2520models%2520are%2520also%2520capable%2520of%2520iterative%2520refinement%252C%2520correcting%2520flawed%250Ascenarios%2520based%2520on%2520simple%2520textual%2520feedback.%2520This%2520approach%2520provides%2520a%2520scalable%250Aalternative%2520to%2520manual%2520scenario%2520design%252C%2520addressing%2520the%2520need%2520for%2520a%2520greater%2520volume%250Aand%2520variety%2520of%2520ATC%2520training%2520and%2520validation%2520simulations.%2520More%2520broadly%252C%2520this%2520work%250Ashowcases%2520the%2520potential%2520of%2520LLMs%2520for%2520complex%2520planning%2520in%2520safety-critical%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models&entry.906535625=Dewi%20Sid%20William%20Gould%20and%20George%20De%20Ath%20and%20Ben%20Carvell%20and%20Nick%20Pepper&entry.1292438233=%20%20The%20manual%20design%20of%20scenarios%20for%20Air%20Traffic%20Control%20%28ATC%29%20training%20is%20a%0Ademanding%20and%20time-consuming%20bottleneck%20that%20limits%20the%20diversity%20of%0Asimulations%20available%20to%20controllers.%20To%20address%20this%2C%20we%20introduce%20a%20novel%2C%0Aend-to-end%20approach%2C%20AirTrafficGen%2C%20that%20leverages%20large%20language%20models%20%28LLMs%29%0Ato%20automate%20and%20control%20the%20generation%20of%20complex%20ATC%20scenarios.%20Our%20method%0Auses%20a%20purpose-built%2C%20graph-based%20representation%20to%20encode%20sector%20topology%0A%28including%20airspace%20geometry%2C%20routes%2C%20and%20fixes%29%20into%20a%20format%20LLMs%20can%0Aprocess.%20Through%20rigorous%20benchmarking%2C%20we%20show%20that%20state-of-the-art%20models%0Alike%20Gemini%202.5%20Pro%20and%20OpenAI%20o3%20can%20generate%20high-traffic%20scenarios%20whilst%0Amaintaining%20operational%20realism.%20Our%20engineered%20prompting%20enables%20fine-grained%0Acontrol%20over%20interaction%20presence%2C%20type%2C%20and%20location.%20Initial%20findings%20suggest%0Athese%20models%20are%20also%20capable%20of%20iterative%20refinement%2C%20correcting%20flawed%0Ascenarios%20based%20on%20simple%20textual%20feedback.%20This%20approach%20provides%20a%20scalable%0Aalternative%20to%20manual%20scenario%20design%2C%20addressing%20the%20need%20for%20a%20greater%20volume%0Aand%20variety%20of%20ATC%20training%20and%20validation%20simulations.%20More%20broadly%2C%20this%20work%0Ashowcases%20the%20potential%20of%20LLMs%20for%20complex%20planning%20in%20safety-critical%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02269v1&entry.124074799=Read"},
{"title": "Clinical Expert Uncertainty Guided Generalized Label Smoothing for\n  Medical Noisy Label Learning", "author": "Kunyu Zhang and Lin Gu and Liangchen Liu and Yingke Chen and Bingyang Wang and Jin Yan and Yingying Zhu", "abstract": "  Many previous studies have proposed extracting image labels from clinical\nnotes to create large-scale medical image datasets at a low cost. However,\nthese approaches inherently suffer from label noise due to uncertainty from the\nclinical experts. When radiologists and physicians analyze medical images to\nmake diagnoses, they often include uncertainty-aware notes such as ``maybe'' or\n``not excluded''. Unfortunately, current text-mining methods overlook these\nnuances, resulting in the creation of noisy labels. Existing methods for\nhandling noisy labels in medical image analysis, which typically address the\nproblem through post-processing techniques, have largely ignored the important\nissue of expert-driven uncertainty contributing to label noise. To better\nincorporate the expert-written uncertainty in clinical notes into medical image\nanalysis and address the label noise issue, we first examine the impact of\nclinical expert uncertainty on label noise. We then propose a clinical expert\nuncertainty-aware benchmark, along with a label smoothing method, which\nsignificantly improves performance compared to current state-of-the-art\napproaches.\n", "link": "http://arxiv.org/abs/2508.02495v1", "date": "2025-08-04", "relevancy": 2.1058, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6123}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5111}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clinical%20Expert%20Uncertainty%20Guided%20Generalized%20Label%20Smoothing%20for%0A%20%20Medical%20Noisy%20Label%20Learning&body=Title%3A%20Clinical%20Expert%20Uncertainty%20Guided%20Generalized%20Label%20Smoothing%20for%0A%20%20Medical%20Noisy%20Label%20Learning%0AAuthor%3A%20Kunyu%20Zhang%20and%20Lin%20Gu%20and%20Liangchen%20Liu%20and%20Yingke%20Chen%20and%20Bingyang%20Wang%20and%20Jin%20Yan%20and%20Yingying%20Zhu%0AAbstract%3A%20%20%20Many%20previous%20studies%20have%20proposed%20extracting%20image%20labels%20from%20clinical%0Anotes%20to%20create%20large-scale%20medical%20image%20datasets%20at%20a%20low%20cost.%20However%2C%0Athese%20approaches%20inherently%20suffer%20from%20label%20noise%20due%20to%20uncertainty%20from%20the%0Aclinical%20experts.%20When%20radiologists%20and%20physicians%20analyze%20medical%20images%20to%0Amake%20diagnoses%2C%20they%20often%20include%20uncertainty-aware%20notes%20such%20as%20%60%60maybe%27%27%20or%0A%60%60not%20excluded%27%27.%20Unfortunately%2C%20current%20text-mining%20methods%20overlook%20these%0Anuances%2C%20resulting%20in%20the%20creation%20of%20noisy%20labels.%20Existing%20methods%20for%0Ahandling%20noisy%20labels%20in%20medical%20image%20analysis%2C%20which%20typically%20address%20the%0Aproblem%20through%20post-processing%20techniques%2C%20have%20largely%20ignored%20the%20important%0Aissue%20of%20expert-driven%20uncertainty%20contributing%20to%20label%20noise.%20To%20better%0Aincorporate%20the%20expert-written%20uncertainty%20in%20clinical%20notes%20into%20medical%20image%0Aanalysis%20and%20address%20the%20label%20noise%20issue%2C%20we%20first%20examine%20the%20impact%20of%0Aclinical%20expert%20uncertainty%20on%20label%20noise.%20We%20then%20propose%20a%20clinical%20expert%0Auncertainty-aware%20benchmark%2C%20along%20with%20a%20label%20smoothing%20method%2C%20which%0Asignificantly%20improves%20performance%20compared%20to%20current%20state-of-the-art%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClinical%2520Expert%2520Uncertainty%2520Guided%2520Generalized%2520Label%2520Smoothing%2520for%250A%2520%2520Medical%2520Noisy%2520Label%2520Learning%26entry.906535625%3DKunyu%2520Zhang%2520and%2520Lin%2520Gu%2520and%2520Liangchen%2520Liu%2520and%2520Yingke%2520Chen%2520and%2520Bingyang%2520Wang%2520and%2520Jin%2520Yan%2520and%2520Yingying%2520Zhu%26entry.1292438233%3D%2520%2520Many%2520previous%2520studies%2520have%2520proposed%2520extracting%2520image%2520labels%2520from%2520clinical%250Anotes%2520to%2520create%2520large-scale%2520medical%2520image%2520datasets%2520at%2520a%2520low%2520cost.%2520However%252C%250Athese%2520approaches%2520inherently%2520suffer%2520from%2520label%2520noise%2520due%2520to%2520uncertainty%2520from%2520the%250Aclinical%2520experts.%2520When%2520radiologists%2520and%2520physicians%2520analyze%2520medical%2520images%2520to%250Amake%2520diagnoses%252C%2520they%2520often%2520include%2520uncertainty-aware%2520notes%2520such%2520as%2520%2560%2560maybe%2527%2527%2520or%250A%2560%2560not%2520excluded%2527%2527.%2520Unfortunately%252C%2520current%2520text-mining%2520methods%2520overlook%2520these%250Anuances%252C%2520resulting%2520in%2520the%2520creation%2520of%2520noisy%2520labels.%2520Existing%2520methods%2520for%250Ahandling%2520noisy%2520labels%2520in%2520medical%2520image%2520analysis%252C%2520which%2520typically%2520address%2520the%250Aproblem%2520through%2520post-processing%2520techniques%252C%2520have%2520largely%2520ignored%2520the%2520important%250Aissue%2520of%2520expert-driven%2520uncertainty%2520contributing%2520to%2520label%2520noise.%2520To%2520better%250Aincorporate%2520the%2520expert-written%2520uncertainty%2520in%2520clinical%2520notes%2520into%2520medical%2520image%250Aanalysis%2520and%2520address%2520the%2520label%2520noise%2520issue%252C%2520we%2520first%2520examine%2520the%2520impact%2520of%250Aclinical%2520expert%2520uncertainty%2520on%2520label%2520noise.%2520We%2520then%2520propose%2520a%2520clinical%2520expert%250Auncertainty-aware%2520benchmark%252C%2520along%2520with%2520a%2520label%2520smoothing%2520method%252C%2520which%250Asignificantly%2520improves%2520performance%2520compared%2520to%2520current%2520state-of-the-art%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clinical%20Expert%20Uncertainty%20Guided%20Generalized%20Label%20Smoothing%20for%0A%20%20Medical%20Noisy%20Label%20Learning&entry.906535625=Kunyu%20Zhang%20and%20Lin%20Gu%20and%20Liangchen%20Liu%20and%20Yingke%20Chen%20and%20Bingyang%20Wang%20and%20Jin%20Yan%20and%20Yingying%20Zhu&entry.1292438233=%20%20Many%20previous%20studies%20have%20proposed%20extracting%20image%20labels%20from%20clinical%0Anotes%20to%20create%20large-scale%20medical%20image%20datasets%20at%20a%20low%20cost.%20However%2C%0Athese%20approaches%20inherently%20suffer%20from%20label%20noise%20due%20to%20uncertainty%20from%20the%0Aclinical%20experts.%20When%20radiologists%20and%20physicians%20analyze%20medical%20images%20to%0Amake%20diagnoses%2C%20they%20often%20include%20uncertainty-aware%20notes%20such%20as%20%60%60maybe%27%27%20or%0A%60%60not%20excluded%27%27.%20Unfortunately%2C%20current%20text-mining%20methods%20overlook%20these%0Anuances%2C%20resulting%20in%20the%20creation%20of%20noisy%20labels.%20Existing%20methods%20for%0Ahandling%20noisy%20labels%20in%20medical%20image%20analysis%2C%20which%20typically%20address%20the%0Aproblem%20through%20post-processing%20techniques%2C%20have%20largely%20ignored%20the%20important%0Aissue%20of%20expert-driven%20uncertainty%20contributing%20to%20label%20noise.%20To%20better%0Aincorporate%20the%20expert-written%20uncertainty%20in%20clinical%20notes%20into%20medical%20image%0Aanalysis%20and%20address%20the%20label%20noise%20issue%2C%20we%20first%20examine%20the%20impact%20of%0Aclinical%20expert%20uncertainty%20on%20label%20noise.%20We%20then%20propose%20a%20clinical%20expert%0Auncertainty-aware%20benchmark%2C%20along%20with%20a%20label%20smoothing%20method%2C%20which%0Asignificantly%20improves%20performance%20compared%20to%20current%20state-of-the-art%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02495v1&entry.124074799=Read"},
{"title": "Contrast-Invariant Self-supervised Segmentation for Quantitative\n  Placental MRI", "author": "Xinliu Zhong and Ruiying Liu and Emily S. Nichols and Xuzhe Zhang and Andrew F. Laine and Emma G. Duerden and Yun Wang", "abstract": "  Accurate placental segmentation is essential for quantitative analysis of the\nplacenta. However, this task is particularly challenging in T2*-weighted\nplacental imaging due to: (1) weak and inconsistent boundary contrast across\nindividual echoes; (2) the absence of manual ground truth annotations for all\necho times; and (3) motion artifacts across echoes caused by fetal and maternal\nmovement. In this work, we propose a contrast-augmented segmentation framework\nthat leverages complementary information across multi-echo T2*-weighted MRI to\nlearn robust, contrast-invariant representations. Our method integrates: (i)\nmasked autoencoding (MAE) for self-supervised pretraining on unlabeled\nmulti-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain\nadaptation across echo times; and (iii) global-local collaboration to align\nfine-grained features with global anatomical context. We further introduce a\nsemantic matching loss to encourage representation consistency across echoes of\nthe same subject. Experiments on a clinical multi-echo placental MRI dataset\ndemonstrate that our approach generalizes effectively across echo times and\noutperforms both single-echo and naive fusion baselines. To our knowledge, this\nis the first work to systematically exploit multi-echo T2*-weighted MRI for\nplacental segmentation.\n", "link": "http://arxiv.org/abs/2505.24739v3", "date": "2025-08-04", "relevancy": 2.1012, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5252}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast-Invariant%20Self-supervised%20Segmentation%20for%20Quantitative%0A%20%20Placental%20MRI&body=Title%3A%20Contrast-Invariant%20Self-supervised%20Segmentation%20for%20Quantitative%0A%20%20Placental%20MRI%0AAuthor%3A%20Xinliu%20Zhong%20and%20Ruiying%20Liu%20and%20Emily%20S.%20Nichols%20and%20Xuzhe%20Zhang%20and%20Andrew%20F.%20Laine%20and%20Emma%20G.%20Duerden%20and%20Yun%20Wang%0AAbstract%3A%20%20%20Accurate%20placental%20segmentation%20is%20essential%20for%20quantitative%20analysis%20of%20the%0Aplacenta.%20However%2C%20this%20task%20is%20particularly%20challenging%20in%20T2%2A-weighted%0Aplacental%20imaging%20due%20to%3A%20%281%29%20weak%20and%20inconsistent%20boundary%20contrast%20across%0Aindividual%20echoes%3B%20%282%29%20the%20absence%20of%20manual%20ground%20truth%20annotations%20for%20all%0Aecho%20times%3B%20and%20%283%29%20motion%20artifacts%20across%20echoes%20caused%20by%20fetal%20and%20maternal%0Amovement.%20In%20this%20work%2C%20we%20propose%20a%20contrast-augmented%20segmentation%20framework%0Athat%20leverages%20complementary%20information%20across%20multi-echo%20T2%2A-weighted%20MRI%20to%0Alearn%20robust%2C%20contrast-invariant%20representations.%20Our%20method%20integrates%3A%20%28i%29%0Amasked%20autoencoding%20%28MAE%29%20for%20self-supervised%20pretraining%20on%20unlabeled%0Amulti-echo%20slices%3B%20%28ii%29%20masked%20pseudo-labeling%20%28MPL%29%20for%20unsupervised%20domain%0Aadaptation%20across%20echo%20times%3B%20and%20%28iii%29%20global-local%20collaboration%20to%20align%0Afine-grained%20features%20with%20global%20anatomical%20context.%20We%20further%20introduce%20a%0Asemantic%20matching%20loss%20to%20encourage%20representation%20consistency%20across%20echoes%20of%0Athe%20same%20subject.%20Experiments%20on%20a%20clinical%20multi-echo%20placental%20MRI%20dataset%0Ademonstrate%20that%20our%20approach%20generalizes%20effectively%20across%20echo%20times%20and%0Aoutperforms%20both%20single-echo%20and%20naive%20fusion%20baselines.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20to%20systematically%20exploit%20multi-echo%20T2%2A-weighted%20MRI%20for%0Aplacental%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24739v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast-Invariant%2520Self-supervised%2520Segmentation%2520for%2520Quantitative%250A%2520%2520Placental%2520MRI%26entry.906535625%3DXinliu%2520Zhong%2520and%2520Ruiying%2520Liu%2520and%2520Emily%2520S.%2520Nichols%2520and%2520Xuzhe%2520Zhang%2520and%2520Andrew%2520F.%2520Laine%2520and%2520Emma%2520G.%2520Duerden%2520and%2520Yun%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520placental%2520segmentation%2520is%2520essential%2520for%2520quantitative%2520analysis%2520of%2520the%250Aplacenta.%2520However%252C%2520this%2520task%2520is%2520particularly%2520challenging%2520in%2520T2%252A-weighted%250Aplacental%2520imaging%2520due%2520to%253A%2520%25281%2529%2520weak%2520and%2520inconsistent%2520boundary%2520contrast%2520across%250Aindividual%2520echoes%253B%2520%25282%2529%2520the%2520absence%2520of%2520manual%2520ground%2520truth%2520annotations%2520for%2520all%250Aecho%2520times%253B%2520and%2520%25283%2529%2520motion%2520artifacts%2520across%2520echoes%2520caused%2520by%2520fetal%2520and%2520maternal%250Amovement.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520contrast-augmented%2520segmentation%2520framework%250Athat%2520leverages%2520complementary%2520information%2520across%2520multi-echo%2520T2%252A-weighted%2520MRI%2520to%250Alearn%2520robust%252C%2520contrast-invariant%2520representations.%2520Our%2520method%2520integrates%253A%2520%2528i%2529%250Amasked%2520autoencoding%2520%2528MAE%2529%2520for%2520self-supervised%2520pretraining%2520on%2520unlabeled%250Amulti-echo%2520slices%253B%2520%2528ii%2529%2520masked%2520pseudo-labeling%2520%2528MPL%2529%2520for%2520unsupervised%2520domain%250Aadaptation%2520across%2520echo%2520times%253B%2520and%2520%2528iii%2529%2520global-local%2520collaboration%2520to%2520align%250Afine-grained%2520features%2520with%2520global%2520anatomical%2520context.%2520We%2520further%2520introduce%2520a%250Asemantic%2520matching%2520loss%2520to%2520encourage%2520representation%2520consistency%2520across%2520echoes%2520of%250Athe%2520same%2520subject.%2520Experiments%2520on%2520a%2520clinical%2520multi-echo%2520placental%2520MRI%2520dataset%250Ademonstrate%2520that%2520our%2520approach%2520generalizes%2520effectively%2520across%2520echo%2520times%2520and%250Aoutperforms%2520both%2520single-echo%2520and%2520naive%2520fusion%2520baselines.%2520To%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520work%2520to%2520systematically%2520exploit%2520multi-echo%2520T2%252A-weighted%2520MRI%2520for%250Aplacental%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24739v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast-Invariant%20Self-supervised%20Segmentation%20for%20Quantitative%0A%20%20Placental%20MRI&entry.906535625=Xinliu%20Zhong%20and%20Ruiying%20Liu%20and%20Emily%20S.%20Nichols%20and%20Xuzhe%20Zhang%20and%20Andrew%20F.%20Laine%20and%20Emma%20G.%20Duerden%20and%20Yun%20Wang&entry.1292438233=%20%20Accurate%20placental%20segmentation%20is%20essential%20for%20quantitative%20analysis%20of%20the%0Aplacenta.%20However%2C%20this%20task%20is%20particularly%20challenging%20in%20T2%2A-weighted%0Aplacental%20imaging%20due%20to%3A%20%281%29%20weak%20and%20inconsistent%20boundary%20contrast%20across%0Aindividual%20echoes%3B%20%282%29%20the%20absence%20of%20manual%20ground%20truth%20annotations%20for%20all%0Aecho%20times%3B%20and%20%283%29%20motion%20artifacts%20across%20echoes%20caused%20by%20fetal%20and%20maternal%0Amovement.%20In%20this%20work%2C%20we%20propose%20a%20contrast-augmented%20segmentation%20framework%0Athat%20leverages%20complementary%20information%20across%20multi-echo%20T2%2A-weighted%20MRI%20to%0Alearn%20robust%2C%20contrast-invariant%20representations.%20Our%20method%20integrates%3A%20%28i%29%0Amasked%20autoencoding%20%28MAE%29%20for%20self-supervised%20pretraining%20on%20unlabeled%0Amulti-echo%20slices%3B%20%28ii%29%20masked%20pseudo-labeling%20%28MPL%29%20for%20unsupervised%20domain%0Aadaptation%20across%20echo%20times%3B%20and%20%28iii%29%20global-local%20collaboration%20to%20align%0Afine-grained%20features%20with%20global%20anatomical%20context.%20We%20further%20introduce%20a%0Asemantic%20matching%20loss%20to%20encourage%20representation%20consistency%20across%20echoes%20of%0Athe%20same%20subject.%20Experiments%20on%20a%20clinical%20multi-echo%20placental%20MRI%20dataset%0Ademonstrate%20that%20our%20approach%20generalizes%20effectively%20across%20echo%20times%20and%0Aoutperforms%20both%20single-echo%20and%20naive%20fusion%20baselines.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20to%20systematically%20exploit%20multi-echo%20T2%2A-weighted%20MRI%20for%0Aplacental%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24739v3&entry.124074799=Read"},
{"title": "BOOST: Bayesian Optimization with Optimal Kernel and Acquisition\n  Function Selection Technique", "author": "Joon-Hyun Park and Mujin Cheon and Dong-Yeun Koh", "abstract": "  The performance of Bayesian optimization (BO), a highly sample-efficient\nmethod for expensive black-box problems, is critically governed by the\nselection of its hyperparameters, including the kernel and acquisition\nfunctions. This presents a challenge: an inappropriate combination of these can\nlead to poor performance and wasted evaluations. While individual improvements\nto kernel functions (e.g., tree-based kernels, deep kernel learning) and\nacquisition functions (e.g., multi-step lookahead, tree-based planning) have\nbeen explored, the joint and autonomous selection of the best pair of these\nfundamental hyperparameters has been overlooked. This forces practitioners to\nrely on heuristics or costly manual training. We propose a simple yet effective\nframework, BOOST (Bayesian Optimization with Optimal Kernel and Acquisition\nFunction Selection Technique), that automates this selection. BOOST utilizes a\nlightweight, offline evaluation stage to predict the performance of various\nkernel-acquisition function pairs and identify the most suitable configuration\nbefore expensive evaluations. BOOST partitions data-in-hand into two subsets: a\nreference subset and a query subset, and it prepares all possible\nkernel-acquisition pairs from the user's chosen candidates. For each\nconfiguration, BOOST conducts internal BO runs using the reference subset,\nevaluating how effectively each pair guides the search toward the optimum in\nthe unknown query subset, thereby identifying the configuration with the best\nretrospective performance for future optimization. Experiments on both\nsynthetic benchmark functions and real-world hyperparameter optimization tasks\ndemonstrate that BOOST consistently outperforms standard BO approaches with\nfixed hyperparameters, highlighting its effectiveness and robustness in diverse\nproblem landscapes.\n", "link": "http://arxiv.org/abs/2508.02332v1", "date": "2025-08-04", "relevancy": 1.979, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOOST%3A%20Bayesian%20Optimization%20with%20Optimal%20Kernel%20and%20Acquisition%0A%20%20Function%20Selection%20Technique&body=Title%3A%20BOOST%3A%20Bayesian%20Optimization%20with%20Optimal%20Kernel%20and%20Acquisition%0A%20%20Function%20Selection%20Technique%0AAuthor%3A%20Joon-Hyun%20Park%20and%20Mujin%20Cheon%20and%20Dong-Yeun%20Koh%0AAbstract%3A%20%20%20The%20performance%20of%20Bayesian%20optimization%20%28BO%29%2C%20a%20highly%20sample-efficient%0Amethod%20for%20expensive%20black-box%20problems%2C%20is%20critically%20governed%20by%20the%0Aselection%20of%20its%20hyperparameters%2C%20including%20the%20kernel%20and%20acquisition%0Afunctions.%20This%20presents%20a%20challenge%3A%20an%20inappropriate%20combination%20of%20these%20can%0Alead%20to%20poor%20performance%20and%20wasted%20evaluations.%20While%20individual%20improvements%0Ato%20kernel%20functions%20%28e.g.%2C%20tree-based%20kernels%2C%20deep%20kernel%20learning%29%20and%0Aacquisition%20functions%20%28e.g.%2C%20multi-step%20lookahead%2C%20tree-based%20planning%29%20have%0Abeen%20explored%2C%20the%20joint%20and%20autonomous%20selection%20of%20the%20best%20pair%20of%20these%0Afundamental%20hyperparameters%20has%20been%20overlooked.%20This%20forces%20practitioners%20to%0Arely%20on%20heuristics%20or%20costly%20manual%20training.%20We%20propose%20a%20simple%20yet%20effective%0Aframework%2C%20BOOST%20%28Bayesian%20Optimization%20with%20Optimal%20Kernel%20and%20Acquisition%0AFunction%20Selection%20Technique%29%2C%20that%20automates%20this%20selection.%20BOOST%20utilizes%20a%0Alightweight%2C%20offline%20evaluation%20stage%20to%20predict%20the%20performance%20of%20various%0Akernel-acquisition%20function%20pairs%20and%20identify%20the%20most%20suitable%20configuration%0Abefore%20expensive%20evaluations.%20BOOST%20partitions%20data-in-hand%20into%20two%20subsets%3A%20a%0Areference%20subset%20and%20a%20query%20subset%2C%20and%20it%20prepares%20all%20possible%0Akernel-acquisition%20pairs%20from%20the%20user%27s%20chosen%20candidates.%20For%20each%0Aconfiguration%2C%20BOOST%20conducts%20internal%20BO%20runs%20using%20the%20reference%20subset%2C%0Aevaluating%20how%20effectively%20each%20pair%20guides%20the%20search%20toward%20the%20optimum%20in%0Athe%20unknown%20query%20subset%2C%20thereby%20identifying%20the%20configuration%20with%20the%20best%0Aretrospective%20performance%20for%20future%20optimization.%20Experiments%20on%20both%0Asynthetic%20benchmark%20functions%20and%20real-world%20hyperparameter%20optimization%20tasks%0Ademonstrate%20that%20BOOST%20consistently%20outperforms%20standard%20BO%20approaches%20with%0Afixed%20hyperparameters%2C%20highlighting%20its%20effectiveness%20and%20robustness%20in%20diverse%0Aproblem%20landscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOOST%253A%2520Bayesian%2520Optimization%2520with%2520Optimal%2520Kernel%2520and%2520Acquisition%250A%2520%2520Function%2520Selection%2520Technique%26entry.906535625%3DJoon-Hyun%2520Park%2520and%2520Mujin%2520Cheon%2520and%2520Dong-Yeun%2520Koh%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520Bayesian%2520optimization%2520%2528BO%2529%252C%2520a%2520highly%2520sample-efficient%250Amethod%2520for%2520expensive%2520black-box%2520problems%252C%2520is%2520critically%2520governed%2520by%2520the%250Aselection%2520of%2520its%2520hyperparameters%252C%2520including%2520the%2520kernel%2520and%2520acquisition%250Afunctions.%2520This%2520presents%2520a%2520challenge%253A%2520an%2520inappropriate%2520combination%2520of%2520these%2520can%250Alead%2520to%2520poor%2520performance%2520and%2520wasted%2520evaluations.%2520While%2520individual%2520improvements%250Ato%2520kernel%2520functions%2520%2528e.g.%252C%2520tree-based%2520kernels%252C%2520deep%2520kernel%2520learning%2529%2520and%250Aacquisition%2520functions%2520%2528e.g.%252C%2520multi-step%2520lookahead%252C%2520tree-based%2520planning%2529%2520have%250Abeen%2520explored%252C%2520the%2520joint%2520and%2520autonomous%2520selection%2520of%2520the%2520best%2520pair%2520of%2520these%250Afundamental%2520hyperparameters%2520has%2520been%2520overlooked.%2520This%2520forces%2520practitioners%2520to%250Arely%2520on%2520heuristics%2520or%2520costly%2520manual%2520training.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%250Aframework%252C%2520BOOST%2520%2528Bayesian%2520Optimization%2520with%2520Optimal%2520Kernel%2520and%2520Acquisition%250AFunction%2520Selection%2520Technique%2529%252C%2520that%2520automates%2520this%2520selection.%2520BOOST%2520utilizes%2520a%250Alightweight%252C%2520offline%2520evaluation%2520stage%2520to%2520predict%2520the%2520performance%2520of%2520various%250Akernel-acquisition%2520function%2520pairs%2520and%2520identify%2520the%2520most%2520suitable%2520configuration%250Abefore%2520expensive%2520evaluations.%2520BOOST%2520partitions%2520data-in-hand%2520into%2520two%2520subsets%253A%2520a%250Areference%2520subset%2520and%2520a%2520query%2520subset%252C%2520and%2520it%2520prepares%2520all%2520possible%250Akernel-acquisition%2520pairs%2520from%2520the%2520user%2527s%2520chosen%2520candidates.%2520For%2520each%250Aconfiguration%252C%2520BOOST%2520conducts%2520internal%2520BO%2520runs%2520using%2520the%2520reference%2520subset%252C%250Aevaluating%2520how%2520effectively%2520each%2520pair%2520guides%2520the%2520search%2520toward%2520the%2520optimum%2520in%250Athe%2520unknown%2520query%2520subset%252C%2520thereby%2520identifying%2520the%2520configuration%2520with%2520the%2520best%250Aretrospective%2520performance%2520for%2520future%2520optimization.%2520Experiments%2520on%2520both%250Asynthetic%2520benchmark%2520functions%2520and%2520real-world%2520hyperparameter%2520optimization%2520tasks%250Ademonstrate%2520that%2520BOOST%2520consistently%2520outperforms%2520standard%2520BO%2520approaches%2520with%250Afixed%2520hyperparameters%252C%2520highlighting%2520its%2520effectiveness%2520and%2520robustness%2520in%2520diverse%250Aproblem%2520landscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOOST%3A%20Bayesian%20Optimization%20with%20Optimal%20Kernel%20and%20Acquisition%0A%20%20Function%20Selection%20Technique&entry.906535625=Joon-Hyun%20Park%20and%20Mujin%20Cheon%20and%20Dong-Yeun%20Koh&entry.1292438233=%20%20The%20performance%20of%20Bayesian%20optimization%20%28BO%29%2C%20a%20highly%20sample-efficient%0Amethod%20for%20expensive%20black-box%20problems%2C%20is%20critically%20governed%20by%20the%0Aselection%20of%20its%20hyperparameters%2C%20including%20the%20kernel%20and%20acquisition%0Afunctions.%20This%20presents%20a%20challenge%3A%20an%20inappropriate%20combination%20of%20these%20can%0Alead%20to%20poor%20performance%20and%20wasted%20evaluations.%20While%20individual%20improvements%0Ato%20kernel%20functions%20%28e.g.%2C%20tree-based%20kernels%2C%20deep%20kernel%20learning%29%20and%0Aacquisition%20functions%20%28e.g.%2C%20multi-step%20lookahead%2C%20tree-based%20planning%29%20have%0Abeen%20explored%2C%20the%20joint%20and%20autonomous%20selection%20of%20the%20best%20pair%20of%20these%0Afundamental%20hyperparameters%20has%20been%20overlooked.%20This%20forces%20practitioners%20to%0Arely%20on%20heuristics%20or%20costly%20manual%20training.%20We%20propose%20a%20simple%20yet%20effective%0Aframework%2C%20BOOST%20%28Bayesian%20Optimization%20with%20Optimal%20Kernel%20and%20Acquisition%0AFunction%20Selection%20Technique%29%2C%20that%20automates%20this%20selection.%20BOOST%20utilizes%20a%0Alightweight%2C%20offline%20evaluation%20stage%20to%20predict%20the%20performance%20of%20various%0Akernel-acquisition%20function%20pairs%20and%20identify%20the%20most%20suitable%20configuration%0Abefore%20expensive%20evaluations.%20BOOST%20partitions%20data-in-hand%20into%20two%20subsets%3A%20a%0Areference%20subset%20and%20a%20query%20subset%2C%20and%20it%20prepares%20all%20possible%0Akernel-acquisition%20pairs%20from%20the%20user%27s%20chosen%20candidates.%20For%20each%0Aconfiguration%2C%20BOOST%20conducts%20internal%20BO%20runs%20using%20the%20reference%20subset%2C%0Aevaluating%20how%20effectively%20each%20pair%20guides%20the%20search%20toward%20the%20optimum%20in%0Athe%20unknown%20query%20subset%2C%20thereby%20identifying%20the%20configuration%20with%20the%20best%0Aretrospective%20performance%20for%20future%20optimization.%20Experiments%20on%20both%0Asynthetic%20benchmark%20functions%20and%20real-world%20hyperparameter%20optimization%20tasks%0Ademonstrate%20that%20BOOST%20consistently%20outperforms%20standard%20BO%20approaches%20with%0Afixed%20hyperparameters%2C%20highlighting%20its%20effectiveness%20and%20robustness%20in%20diverse%0Aproblem%20landscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02332v1&entry.124074799=Read"},
{"title": "Prototype Embedding Optimization for Human-Object Interaction Detection\n  in Livestreaming", "author": "Menghui Zhang and Jing Zhang and Lin Chen and Li Zhuo", "abstract": "  Livestreaming often involves interactions between streamers and objects,\nwhich is critical for understanding and regulating web content. While\nhuman-object interaction (HOI) detection has made some progress in\ngeneral-purpose video downstream tasks, when applied to recognize the\ninteraction behaviors between a streamer and different objects in\nlivestreaming, it tends to focuses too much on the objects and neglects their\ninteractions with the streamer, which leads to object bias. To solve this\nissue, we propose a prototype embedding optimization for human-object\ninteraction detection (PeO-HOI). First, the livestreaming is preprocessed using\nobject detection and tracking techniques to extract features of the\nhuman-object (HO) pairs. Then, prototype embedding optimization is adopted to\nmitigate the effect of object bias on HOI. Finally, after modelling the\nspatio-temporal context between HO pairs, the HOI detection results are\nobtained by the prediction head. The experimental results show that the\ndetection accuracy of the proposed PeO-HOI method has detection accuracies of\n37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset\nVidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset\nBJUT-HOI, which effectively improves the HOI detection performance in\nlivestreaming.\n", "link": "http://arxiv.org/abs/2505.22011v2", "date": "2025-08-04", "relevancy": 2.0475, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5416}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Embedding%20Optimization%20for%20Human-Object%20Interaction%20Detection%0A%20%20in%20Livestreaming&body=Title%3A%20Prototype%20Embedding%20Optimization%20for%20Human-Object%20Interaction%20Detection%0A%20%20in%20Livestreaming%0AAuthor%3A%20Menghui%20Zhang%20and%20Jing%20Zhang%20and%20Lin%20Chen%20and%20Li%20Zhuo%0AAbstract%3A%20%20%20Livestreaming%20often%20involves%20interactions%20between%20streamers%20and%20objects%2C%0Awhich%20is%20critical%20for%20understanding%20and%20regulating%20web%20content.%20While%0Ahuman-object%20interaction%20%28HOI%29%20detection%20has%20made%20some%20progress%20in%0Ageneral-purpose%20video%20downstream%20tasks%2C%20when%20applied%20to%20recognize%20the%0Ainteraction%20behaviors%20between%20a%20streamer%20and%20different%20objects%20in%0Alivestreaming%2C%20it%20tends%20to%20focuses%20too%20much%20on%20the%20objects%20and%20neglects%20their%0Ainteractions%20with%20the%20streamer%2C%20which%20leads%20to%20object%20bias.%20To%20solve%20this%0Aissue%2C%20we%20propose%20a%20prototype%20embedding%20optimization%20for%20human-object%0Ainteraction%20detection%20%28PeO-HOI%29.%20First%2C%20the%20livestreaming%20is%20preprocessed%20using%0Aobject%20detection%20and%20tracking%20techniques%20to%20extract%20features%20of%20the%0Ahuman-object%20%28HO%29%20pairs.%20Then%2C%20prototype%20embedding%20optimization%20is%20adopted%20to%0Amitigate%20the%20effect%20of%20object%20bias%20on%20HOI.%20Finally%2C%20after%20modelling%20the%0Aspatio-temporal%20context%20between%20HO%20pairs%2C%20the%20HOI%20detection%20results%20are%0Aobtained%20by%20the%20prediction%20head.%20The%20experimental%20results%20show%20that%20the%0Adetection%20accuracy%20of%20the%20proposed%20PeO-HOI%20method%20has%20detection%20accuracies%20of%0A37.19%25%40full%2C%2051.42%25%40non-rare%2C%2026.20%25%40rare%20on%20the%20publicly%20available%20dataset%0AVidHOI%2C%2045.13%25%40full%2C%2062.78%25%40non-rare%20and%2030.37%25%40rare%20on%20the%20self-built%20dataset%0ABJUT-HOI%2C%20which%20effectively%20improves%20the%20HOI%20detection%20performance%20in%0Alivestreaming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Embedding%2520Optimization%2520for%2520Human-Object%2520Interaction%2520Detection%250A%2520%2520in%2520Livestreaming%26entry.906535625%3DMenghui%2520Zhang%2520and%2520Jing%2520Zhang%2520and%2520Lin%2520Chen%2520and%2520Li%2520Zhuo%26entry.1292438233%3D%2520%2520Livestreaming%2520often%2520involves%2520interactions%2520between%2520streamers%2520and%2520objects%252C%250Awhich%2520is%2520critical%2520for%2520understanding%2520and%2520regulating%2520web%2520content.%2520While%250Ahuman-object%2520interaction%2520%2528HOI%2529%2520detection%2520has%2520made%2520some%2520progress%2520in%250Ageneral-purpose%2520video%2520downstream%2520tasks%252C%2520when%2520applied%2520to%2520recognize%2520the%250Ainteraction%2520behaviors%2520between%2520a%2520streamer%2520and%2520different%2520objects%2520in%250Alivestreaming%252C%2520it%2520tends%2520to%2520focuses%2520too%2520much%2520on%2520the%2520objects%2520and%2520neglects%2520their%250Ainteractions%2520with%2520the%2520streamer%252C%2520which%2520leads%2520to%2520object%2520bias.%2520To%2520solve%2520this%250Aissue%252C%2520we%2520propose%2520a%2520prototype%2520embedding%2520optimization%2520for%2520human-object%250Ainteraction%2520detection%2520%2528PeO-HOI%2529.%2520First%252C%2520the%2520livestreaming%2520is%2520preprocessed%2520using%250Aobject%2520detection%2520and%2520tracking%2520techniques%2520to%2520extract%2520features%2520of%2520the%250Ahuman-object%2520%2528HO%2529%2520pairs.%2520Then%252C%2520prototype%2520embedding%2520optimization%2520is%2520adopted%2520to%250Amitigate%2520the%2520effect%2520of%2520object%2520bias%2520on%2520HOI.%2520Finally%252C%2520after%2520modelling%2520the%250Aspatio-temporal%2520context%2520between%2520HO%2520pairs%252C%2520the%2520HOI%2520detection%2520results%2520are%250Aobtained%2520by%2520the%2520prediction%2520head.%2520The%2520experimental%2520results%2520show%2520that%2520the%250Adetection%2520accuracy%2520of%2520the%2520proposed%2520PeO-HOI%2520method%2520has%2520detection%2520accuracies%2520of%250A37.19%2525%2540full%252C%252051.42%2525%2540non-rare%252C%252026.20%2525%2540rare%2520on%2520the%2520publicly%2520available%2520dataset%250AVidHOI%252C%252045.13%2525%2540full%252C%252062.78%2525%2540non-rare%2520and%252030.37%2525%2540rare%2520on%2520the%2520self-built%2520dataset%250ABJUT-HOI%252C%2520which%2520effectively%2520improves%2520the%2520HOI%2520detection%2520performance%2520in%250Alivestreaming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Embedding%20Optimization%20for%20Human-Object%20Interaction%20Detection%0A%20%20in%20Livestreaming&entry.906535625=Menghui%20Zhang%20and%20Jing%20Zhang%20and%20Lin%20Chen%20and%20Li%20Zhuo&entry.1292438233=%20%20Livestreaming%20often%20involves%20interactions%20between%20streamers%20and%20objects%2C%0Awhich%20is%20critical%20for%20understanding%20and%20regulating%20web%20content.%20While%0Ahuman-object%20interaction%20%28HOI%29%20detection%20has%20made%20some%20progress%20in%0Ageneral-purpose%20video%20downstream%20tasks%2C%20when%20applied%20to%20recognize%20the%0Ainteraction%20behaviors%20between%20a%20streamer%20and%20different%20objects%20in%0Alivestreaming%2C%20it%20tends%20to%20focuses%20too%20much%20on%20the%20objects%20and%20neglects%20their%0Ainteractions%20with%20the%20streamer%2C%20which%20leads%20to%20object%20bias.%20To%20solve%20this%0Aissue%2C%20we%20propose%20a%20prototype%20embedding%20optimization%20for%20human-object%0Ainteraction%20detection%20%28PeO-HOI%29.%20First%2C%20the%20livestreaming%20is%20preprocessed%20using%0Aobject%20detection%20and%20tracking%20techniques%20to%20extract%20features%20of%20the%0Ahuman-object%20%28HO%29%20pairs.%20Then%2C%20prototype%20embedding%20optimization%20is%20adopted%20to%0Amitigate%20the%20effect%20of%20object%20bias%20on%20HOI.%20Finally%2C%20after%20modelling%20the%0Aspatio-temporal%20context%20between%20HO%20pairs%2C%20the%20HOI%20detection%20results%20are%0Aobtained%20by%20the%20prediction%20head.%20The%20experimental%20results%20show%20that%20the%0Adetection%20accuracy%20of%20the%20proposed%20PeO-HOI%20method%20has%20detection%20accuracies%20of%0A37.19%25%40full%2C%2051.42%25%40non-rare%2C%2026.20%25%40rare%20on%20the%20publicly%20available%20dataset%0AVidHOI%2C%2045.13%25%40full%2C%2062.78%25%40non-rare%20and%2030.37%25%40rare%20on%20the%20self-built%20dataset%0ABJUT-HOI%2C%20which%20effectively%20improves%20the%20HOI%20detection%20performance%20in%0Alivestreaming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22011v2&entry.124074799=Read"},
{"title": "SEAL: Semantic Aware Image Watermarking", "author": "Kasra Arabi and R. Teal Witter and Chinmay Hegde and Niv Cohen", "abstract": "  Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.\n", "link": "http://arxiv.org/abs/2503.12172v3", "date": "2025-08-04", "relevancy": 1.6562, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5649}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5516}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Semantic%20Aware%20Image%20Watermarking&body=Title%3A%20SEAL%3A%20Semantic%20Aware%20Image%20Watermarking%0AAuthor%3A%20Kasra%20Arabi%20and%20R.%20Teal%20Witter%20and%20Chinmay%20Hegde%20and%20Niv%20Cohen%0AAbstract%3A%20%20%20Generative%20models%20have%20rapidly%20evolved%20to%20generate%20realistic%20outputs.%0AHowever%2C%20their%20synthetic%20outputs%20increasingly%20challenge%20the%20clear%20distinction%0Abetween%20natural%20and%20AI-generated%20content%2C%20necessitating%20robust%20watermarking%0Atechniques.%20Watermarks%20are%20typically%20expected%20to%20preserve%20the%20integrity%20of%20the%0Atarget%20image%2C%20withstand%20removal%20attempts%2C%20and%20prevent%20unauthorized%20replication%0Aonto%20unrelated%20images.%20To%20address%20this%20need%2C%20recent%20methods%20embed%20persistent%0Awatermarks%20into%20images%20produced%20by%20diffusion%20models%20using%20the%20initial%20noise.%0AYet%2C%20to%20do%20so%2C%20they%20either%20distort%20the%20distribution%20of%20generated%20images%20or%20rely%0Aon%20searching%20through%20a%20long%20dictionary%20of%20used%20keys%20for%20detection.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20watermarking%20method%20that%20embeds%20semantic%0Ainformation%20about%20the%20generated%20image%20directly%20into%20the%20watermark%2C%20enabling%20a%0Adistortion-free%20watermark%20that%20can%20be%20verified%20without%20requiring%20a%20database%20of%0Akey%20patterns.%20Instead%2C%20the%20key%20pattern%20can%20be%20inferred%20from%20the%20semantic%0Aembedding%20of%20the%20image%20using%20locality-sensitive%20hashing.%20Furthermore%2C%0Aconditioning%20the%20watermark%20detection%20on%20the%20original%20image%20content%20improves%0Arobustness%20against%20forgery%20attacks.%20To%20demonstrate%20that%2C%20we%20consider%20two%0Alargely%20overlooked%20attack%20strategies%3A%20%28i%29%20an%20attacker%20extracting%20the%20initial%0Anoise%20and%20generating%20a%20novel%20image%20with%20the%20same%20pattern%3B%20%28ii%29%20an%20attacker%0Ainserting%20an%20unrelated%20%28potentially%20harmful%29%20object%20into%20a%20watermarked%20image%2C%0Apossibly%20while%20preserving%20the%20watermark.%20We%20empirically%20validate%20our%20method%27s%0Aincreased%20robustness%20to%20these%20attacks.%20Taken%20together%2C%20our%20results%20suggest%20that%0Acontent-aware%20watermarks%20can%20mitigate%20risks%20arising%20from%20image-generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12172v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Semantic%2520Aware%2520Image%2520Watermarking%26entry.906535625%3DKasra%2520Arabi%2520and%2520R.%2520Teal%2520Witter%2520and%2520Chinmay%2520Hegde%2520and%2520Niv%2520Cohen%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520rapidly%2520evolved%2520to%2520generate%2520realistic%2520outputs.%250AHowever%252C%2520their%2520synthetic%2520outputs%2520increasingly%2520challenge%2520the%2520clear%2520distinction%250Abetween%2520natural%2520and%2520AI-generated%2520content%252C%2520necessitating%2520robust%2520watermarking%250Atechniques.%2520Watermarks%2520are%2520typically%2520expected%2520to%2520preserve%2520the%2520integrity%2520of%2520the%250Atarget%2520image%252C%2520withstand%2520removal%2520attempts%252C%2520and%2520prevent%2520unauthorized%2520replication%250Aonto%2520unrelated%2520images.%2520To%2520address%2520this%2520need%252C%2520recent%2520methods%2520embed%2520persistent%250Awatermarks%2520into%2520images%2520produced%2520by%2520diffusion%2520models%2520using%2520the%2520initial%2520noise.%250AYet%252C%2520to%2520do%2520so%252C%2520they%2520either%2520distort%2520the%2520distribution%2520of%2520generated%2520images%2520or%2520rely%250Aon%2520searching%2520through%2520a%2520long%2520dictionary%2520of%2520used%2520keys%2520for%2520detection.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520watermarking%2520method%2520that%2520embeds%2520semantic%250Ainformation%2520about%2520the%2520generated%2520image%2520directly%2520into%2520the%2520watermark%252C%2520enabling%2520a%250Adistortion-free%2520watermark%2520that%2520can%2520be%2520verified%2520without%2520requiring%2520a%2520database%2520of%250Akey%2520patterns.%2520Instead%252C%2520the%2520key%2520pattern%2520can%2520be%2520inferred%2520from%2520the%2520semantic%250Aembedding%2520of%2520the%2520image%2520using%2520locality-sensitive%2520hashing.%2520Furthermore%252C%250Aconditioning%2520the%2520watermark%2520detection%2520on%2520the%2520original%2520image%2520content%2520improves%250Arobustness%2520against%2520forgery%2520attacks.%2520To%2520demonstrate%2520that%252C%2520we%2520consider%2520two%250Alargely%2520overlooked%2520attack%2520strategies%253A%2520%2528i%2529%2520an%2520attacker%2520extracting%2520the%2520initial%250Anoise%2520and%2520generating%2520a%2520novel%2520image%2520with%2520the%2520same%2520pattern%253B%2520%2528ii%2529%2520an%2520attacker%250Ainserting%2520an%2520unrelated%2520%2528potentially%2520harmful%2529%2520object%2520into%2520a%2520watermarked%2520image%252C%250Apossibly%2520while%2520preserving%2520the%2520watermark.%2520We%2520empirically%2520validate%2520our%2520method%2527s%250Aincreased%2520robustness%2520to%2520these%2520attacks.%2520Taken%2520together%252C%2520our%2520results%2520suggest%2520that%250Acontent-aware%2520watermarks%2520can%2520mitigate%2520risks%2520arising%2520from%2520image-generative%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12172v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Semantic%20Aware%20Image%20Watermarking&entry.906535625=Kasra%20Arabi%20and%20R.%20Teal%20Witter%20and%20Chinmay%20Hegde%20and%20Niv%20Cohen&entry.1292438233=%20%20Generative%20models%20have%20rapidly%20evolved%20to%20generate%20realistic%20outputs.%0AHowever%2C%20their%20synthetic%20outputs%20increasingly%20challenge%20the%20clear%20distinction%0Abetween%20natural%20and%20AI-generated%20content%2C%20necessitating%20robust%20watermarking%0Atechniques.%20Watermarks%20are%20typically%20expected%20to%20preserve%20the%20integrity%20of%20the%0Atarget%20image%2C%20withstand%20removal%20attempts%2C%20and%20prevent%20unauthorized%20replication%0Aonto%20unrelated%20images.%20To%20address%20this%20need%2C%20recent%20methods%20embed%20persistent%0Awatermarks%20into%20images%20produced%20by%20diffusion%20models%20using%20the%20initial%20noise.%0AYet%2C%20to%20do%20so%2C%20they%20either%20distort%20the%20distribution%20of%20generated%20images%20or%20rely%0Aon%20searching%20through%20a%20long%20dictionary%20of%20used%20keys%20for%20detection.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20watermarking%20method%20that%20embeds%20semantic%0Ainformation%20about%20the%20generated%20image%20directly%20into%20the%20watermark%2C%20enabling%20a%0Adistortion-free%20watermark%20that%20can%20be%20verified%20without%20requiring%20a%20database%20of%0Akey%20patterns.%20Instead%2C%20the%20key%20pattern%20can%20be%20inferred%20from%20the%20semantic%0Aembedding%20of%20the%20image%20using%20locality-sensitive%20hashing.%20Furthermore%2C%0Aconditioning%20the%20watermark%20detection%20on%20the%20original%20image%20content%20improves%0Arobustness%20against%20forgery%20attacks.%20To%20demonstrate%20that%2C%20we%20consider%20two%0Alargely%20overlooked%20attack%20strategies%3A%20%28i%29%20an%20attacker%20extracting%20the%20initial%0Anoise%20and%20generating%20a%20novel%20image%20with%20the%20same%20pattern%3B%20%28ii%29%20an%20attacker%0Ainserting%20an%20unrelated%20%28potentially%20harmful%29%20object%20into%20a%20watermarked%20image%2C%0Apossibly%20while%20preserving%20the%20watermark.%20We%20empirically%20validate%20our%20method%27s%0Aincreased%20robustness%20to%20these%20attacks.%20Taken%20together%2C%20our%20results%20suggest%20that%0Acontent-aware%20watermarks%20can%20mitigate%20risks%20arising%20from%20image-generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12172v3&entry.124074799=Read"},
{"title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction", "author": "Enrico De Santis and Antonello Rizzi", "abstract": "  This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.\n", "link": "http://arxiv.org/abs/2508.02622v1", "date": "2025-08-04", "relevancy": 1.2628, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4299}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4229}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noosemia%3A%20toward%20a%20Cognitive%20and%20Phenomenological%20Account%20of%0A%20%20Intentionality%20Attribution%20in%20Human-Generative%20AI%20Interaction&body=Title%3A%20Noosemia%3A%20toward%20a%20Cognitive%20and%20Phenomenological%20Account%20of%0A%20%20Intentionality%20Attribution%20in%20Human-Generative%20AI%20Interaction%0AAuthor%3A%20Enrico%20De%20Santis%20and%20Antonello%20Rizzi%0AAbstract%3A%20%20%20This%20paper%20introduces%20and%20formalizes%20Noosemia%2C%20a%20novel%0Acognitive-phenomenological%20phenomenon%20emerging%20from%20human%20interaction%20with%0Agenerative%20AI%20systems%2C%20particularly%20those%20enabling%20dialogic%20or%20multimodal%0Aexchanges.%20We%20propose%20a%20multidisciplinary%20framework%20to%20explain%20how%2C%20under%0Acertain%20conditions%2C%20users%20attribute%20intentionality%2C%20agency%2C%20and%20even%0Ainteriority%20to%20these%20systems%20-%20a%20process%20grounded%20not%20in%20physical%20resemblance%2C%0Abut%20in%20linguistic%20performance%2C%20epistemic%20opacity%2C%20and%20emergent%20technological%0Acomplexity.%20By%20linking%20an%20LLM%20declination%20of%20meaning%20holism%20to%20our%20technical%0Anotion%20of%20the%20LLM%20Contextual%20Cognitive%20Field%2C%20we%20clarify%20how%20LLMs%20construct%0Ameaning%20relationally%20and%20how%20coherence%20and%20a%20simulacrum%20of%20agency%20arise%20at%20the%0Ahuman-AI%20interface.%20The%20analysis%20situates%20noosemia%20alongside%20pareidolia%2C%0Aanimism%2C%20the%20intentional%20stance%20and%20the%20uncanny%20valley%2C%20distinguishing%20its%0Aunique%20characteristics.%20We%20also%20introduce%20a-noosemia%20to%20describe%20the%0Aphenomenological%20withdrawal%20of%20such%20projections.%20The%20paper%20concludes%20with%0Areflections%20on%20the%20broader%20philosophical%2C%20epistemological%2C%20and%20social%0Aimplications%20of%20noosemic%20dynamics%20and%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoosemia%253A%2520toward%2520a%2520Cognitive%2520and%2520Phenomenological%2520Account%2520of%250A%2520%2520Intentionality%2520Attribution%2520in%2520Human-Generative%2520AI%2520Interaction%26entry.906535625%3DEnrico%2520De%2520Santis%2520and%2520Antonello%2520Rizzi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520and%2520formalizes%2520Noosemia%252C%2520a%2520novel%250Acognitive-phenomenological%2520phenomenon%2520emerging%2520from%2520human%2520interaction%2520with%250Agenerative%2520AI%2520systems%252C%2520particularly%2520those%2520enabling%2520dialogic%2520or%2520multimodal%250Aexchanges.%2520We%2520propose%2520a%2520multidisciplinary%2520framework%2520to%2520explain%2520how%252C%2520under%250Acertain%2520conditions%252C%2520users%2520attribute%2520intentionality%252C%2520agency%252C%2520and%2520even%250Ainteriority%2520to%2520these%2520systems%2520-%2520a%2520process%2520grounded%2520not%2520in%2520physical%2520resemblance%252C%250Abut%2520in%2520linguistic%2520performance%252C%2520epistemic%2520opacity%252C%2520and%2520emergent%2520technological%250Acomplexity.%2520By%2520linking%2520an%2520LLM%2520declination%2520of%2520meaning%2520holism%2520to%2520our%2520technical%250Anotion%2520of%2520the%2520LLM%2520Contextual%2520Cognitive%2520Field%252C%2520we%2520clarify%2520how%2520LLMs%2520construct%250Ameaning%2520relationally%2520and%2520how%2520coherence%2520and%2520a%2520simulacrum%2520of%2520agency%2520arise%2520at%2520the%250Ahuman-AI%2520interface.%2520The%2520analysis%2520situates%2520noosemia%2520alongside%2520pareidolia%252C%250Aanimism%252C%2520the%2520intentional%2520stance%2520and%2520the%2520uncanny%2520valley%252C%2520distinguishing%2520its%250Aunique%2520characteristics.%2520We%2520also%2520introduce%2520a-noosemia%2520to%2520describe%2520the%250Aphenomenological%2520withdrawal%2520of%2520such%2520projections.%2520The%2520paper%2520concludes%2520with%250Areflections%2520on%2520the%2520broader%2520philosophical%252C%2520epistemological%252C%2520and%2520social%250Aimplications%2520of%2520noosemic%2520dynamics%2520and%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noosemia%3A%20toward%20a%20Cognitive%20and%20Phenomenological%20Account%20of%0A%20%20Intentionality%20Attribution%20in%20Human-Generative%20AI%20Interaction&entry.906535625=Enrico%20De%20Santis%20and%20Antonello%20Rizzi&entry.1292438233=%20%20This%20paper%20introduces%20and%20formalizes%20Noosemia%2C%20a%20novel%0Acognitive-phenomenological%20phenomenon%20emerging%20from%20human%20interaction%20with%0Agenerative%20AI%20systems%2C%20particularly%20those%20enabling%20dialogic%20or%20multimodal%0Aexchanges.%20We%20propose%20a%20multidisciplinary%20framework%20to%20explain%20how%2C%20under%0Acertain%20conditions%2C%20users%20attribute%20intentionality%2C%20agency%2C%20and%20even%0Ainteriority%20to%20these%20systems%20-%20a%20process%20grounded%20not%20in%20physical%20resemblance%2C%0Abut%20in%20linguistic%20performance%2C%20epistemic%20opacity%2C%20and%20emergent%20technological%0Acomplexity.%20By%20linking%20an%20LLM%20declination%20of%20meaning%20holism%20to%20our%20technical%0Anotion%20of%20the%20LLM%20Contextual%20Cognitive%20Field%2C%20we%20clarify%20how%20LLMs%20construct%0Ameaning%20relationally%20and%20how%20coherence%20and%20a%20simulacrum%20of%20agency%20arise%20at%20the%0Ahuman-AI%20interface.%20The%20analysis%20situates%20noosemia%20alongside%20pareidolia%2C%0Aanimism%2C%20the%20intentional%20stance%20and%20the%20uncanny%20valley%2C%20distinguishing%20its%0Aunique%20characteristics.%20We%20also%20introduce%20a-noosemia%20to%20describe%20the%0Aphenomenological%20withdrawal%20of%20such%20projections.%20The%20paper%20concludes%20with%0Areflections%20on%20the%20broader%20philosophical%2C%20epistemological%2C%20and%20social%0Aimplications%20of%20noosemic%20dynamics%20and%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02622v1&entry.124074799=Read"},
{"title": "Multimodal Large Language Models for End-to-End Affective Computing:\n  Benchmarking and Boosting with Generative Knowledge Prompting", "author": "Miaosen Luo and Jiesen Long and Zequn Li and Yunying Yang and Yuncheng Jiang and Sijie Mai", "abstract": "  Multimodal Affective Computing (MAC) aims to recognize and interpret human\nemotions by integrating information from diverse modalities such as text,\nvideo, and audio. Recent advancements in Multimodal Large Language Models\n(MLLMs) have significantly reshaped the landscape of MAC by offering a unified\nframework for processing and aligning cross-modal information. However,\npractical challenges remain, including performance variability across complex\nMAC tasks and insufficient understanding of how architectural designs and data\ncharacteristics impact affective analysis. To address these gaps, we conduct a\nsystematic benchmark evaluation of state-of-the-art open-source MLLMs capable\nof concurrently processing audio, visual, and textual modalities across\nmultiple established MAC datasets. Our evaluation not only compares the\nperformance of these MLLMs but also provides actionable insights into model\noptimization by analyzing the influence of model architectures and dataset\nproperties. Furthermore, we propose a novel hybrid strategy that combines\ngenerative knowledge prompting with supervised fine-tuning to enhance MLLMs'\naffective computing capabilities. Experimental results demonstrate that this\nintegrated approach significantly improves performance across various MAC\ntasks, offering a promising avenue for future research and development in this\nfield. Our code is released on https://github.com/LuoMSen/MLLM-MAC.\n", "link": "http://arxiv.org/abs/2508.02429v1", "date": "2025-08-04", "relevancy": 1.5723, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Large%20Language%20Models%20for%20End-to-End%20Affective%20Computing%3A%0A%20%20Benchmarking%20and%20Boosting%20with%20Generative%20Knowledge%20Prompting&body=Title%3A%20Multimodal%20Large%20Language%20Models%20for%20End-to-End%20Affective%20Computing%3A%0A%20%20Benchmarking%20and%20Boosting%20with%20Generative%20Knowledge%20Prompting%0AAuthor%3A%20Miaosen%20Luo%20and%20Jiesen%20Long%20and%20Zequn%20Li%20and%20Yunying%20Yang%20and%20Yuncheng%20Jiang%20and%20Sijie%20Mai%0AAbstract%3A%20%20%20Multimodal%20Affective%20Computing%20%28MAC%29%20aims%20to%20recognize%20and%20interpret%20human%0Aemotions%20by%20integrating%20information%20from%20diverse%20modalities%20such%20as%20text%2C%0Avideo%2C%20and%20audio.%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20have%20significantly%20reshaped%20the%20landscape%20of%20MAC%20by%20offering%20a%20unified%0Aframework%20for%20processing%20and%20aligning%20cross-modal%20information.%20However%2C%0Apractical%20challenges%20remain%2C%20including%20performance%20variability%20across%20complex%0AMAC%20tasks%20and%20insufficient%20understanding%20of%20how%20architectural%20designs%20and%20data%0Acharacteristics%20impact%20affective%20analysis.%20To%20address%20these%20gaps%2C%20we%20conduct%20a%0Asystematic%20benchmark%20evaluation%20of%20state-of-the-art%20open-source%20MLLMs%20capable%0Aof%20concurrently%20processing%20audio%2C%20visual%2C%20and%20textual%20modalities%20across%0Amultiple%20established%20MAC%20datasets.%20Our%20evaluation%20not%20only%20compares%20the%0Aperformance%20of%20these%20MLLMs%20but%20also%20provides%20actionable%20insights%20into%20model%0Aoptimization%20by%20analyzing%20the%20influence%20of%20model%20architectures%20and%20dataset%0Aproperties.%20Furthermore%2C%20we%20propose%20a%20novel%20hybrid%20strategy%20that%20combines%0Agenerative%20knowledge%20prompting%20with%20supervised%20fine-tuning%20to%20enhance%20MLLMs%27%0Aaffective%20computing%20capabilities.%20Experimental%20results%20demonstrate%20that%20this%0Aintegrated%20approach%20significantly%20improves%20performance%20across%20various%20MAC%0Atasks%2C%20offering%20a%20promising%20avenue%20for%20future%20research%20and%20development%20in%20this%0Afield.%20Our%20code%20is%20released%20on%20https%3A//github.com/LuoMSen/MLLM-MAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Large%2520Language%2520Models%2520for%2520End-to-End%2520Affective%2520Computing%253A%250A%2520%2520Benchmarking%2520and%2520Boosting%2520with%2520Generative%2520Knowledge%2520Prompting%26entry.906535625%3DMiaosen%2520Luo%2520and%2520Jiesen%2520Long%2520and%2520Zequn%2520Li%2520and%2520Yunying%2520Yang%2520and%2520Yuncheng%2520Jiang%2520and%2520Sijie%2520Mai%26entry.1292438233%3D%2520%2520Multimodal%2520Affective%2520Computing%2520%2528MAC%2529%2520aims%2520to%2520recognize%2520and%2520interpret%2520human%250Aemotions%2520by%2520integrating%2520information%2520from%2520diverse%2520modalities%2520such%2520as%2520text%252C%250Avideo%252C%2520and%2520audio.%2520Recent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520have%2520significantly%2520reshaped%2520the%2520landscape%2520of%2520MAC%2520by%2520offering%2520a%2520unified%250Aframework%2520for%2520processing%2520and%2520aligning%2520cross-modal%2520information.%2520However%252C%250Apractical%2520challenges%2520remain%252C%2520including%2520performance%2520variability%2520across%2520complex%250AMAC%2520tasks%2520and%2520insufficient%2520understanding%2520of%2520how%2520architectural%2520designs%2520and%2520data%250Acharacteristics%2520impact%2520affective%2520analysis.%2520To%2520address%2520these%2520gaps%252C%2520we%2520conduct%2520a%250Asystematic%2520benchmark%2520evaluation%2520of%2520state-of-the-art%2520open-source%2520MLLMs%2520capable%250Aof%2520concurrently%2520processing%2520audio%252C%2520visual%252C%2520and%2520textual%2520modalities%2520across%250Amultiple%2520established%2520MAC%2520datasets.%2520Our%2520evaluation%2520not%2520only%2520compares%2520the%250Aperformance%2520of%2520these%2520MLLMs%2520but%2520also%2520provides%2520actionable%2520insights%2520into%2520model%250Aoptimization%2520by%2520analyzing%2520the%2520influence%2520of%2520model%2520architectures%2520and%2520dataset%250Aproperties.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520strategy%2520that%2520combines%250Agenerative%2520knowledge%2520prompting%2520with%2520supervised%2520fine-tuning%2520to%2520enhance%2520MLLMs%2527%250Aaffective%2520computing%2520capabilities.%2520Experimental%2520results%2520demonstrate%2520that%2520this%250Aintegrated%2520approach%2520significantly%2520improves%2520performance%2520across%2520various%2520MAC%250Atasks%252C%2520offering%2520a%2520promising%2520avenue%2520for%2520future%2520research%2520and%2520development%2520in%2520this%250Afield.%2520Our%2520code%2520is%2520released%2520on%2520https%253A//github.com/LuoMSen/MLLM-MAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Large%20Language%20Models%20for%20End-to-End%20Affective%20Computing%3A%0A%20%20Benchmarking%20and%20Boosting%20with%20Generative%20Knowledge%20Prompting&entry.906535625=Miaosen%20Luo%20and%20Jiesen%20Long%20and%20Zequn%20Li%20and%20Yunying%20Yang%20and%20Yuncheng%20Jiang%20and%20Sijie%20Mai&entry.1292438233=%20%20Multimodal%20Affective%20Computing%20%28MAC%29%20aims%20to%20recognize%20and%20interpret%20human%0Aemotions%20by%20integrating%20information%20from%20diverse%20modalities%20such%20as%20text%2C%0Avideo%2C%20and%20audio.%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20have%20significantly%20reshaped%20the%20landscape%20of%20MAC%20by%20offering%20a%20unified%0Aframework%20for%20processing%20and%20aligning%20cross-modal%20information.%20However%2C%0Apractical%20challenges%20remain%2C%20including%20performance%20variability%20across%20complex%0AMAC%20tasks%20and%20insufficient%20understanding%20of%20how%20architectural%20designs%20and%20data%0Acharacteristics%20impact%20affective%20analysis.%20To%20address%20these%20gaps%2C%20we%20conduct%20a%0Asystematic%20benchmark%20evaluation%20of%20state-of-the-art%20open-source%20MLLMs%20capable%0Aof%20concurrently%20processing%20audio%2C%20visual%2C%20and%20textual%20modalities%20across%0Amultiple%20established%20MAC%20datasets.%20Our%20evaluation%20not%20only%20compares%20the%0Aperformance%20of%20these%20MLLMs%20but%20also%20provides%20actionable%20insights%20into%20model%0Aoptimization%20by%20analyzing%20the%20influence%20of%20model%20architectures%20and%20dataset%0Aproperties.%20Furthermore%2C%20we%20propose%20a%20novel%20hybrid%20strategy%20that%20combines%0Agenerative%20knowledge%20prompting%20with%20supervised%20fine-tuning%20to%20enhance%20MLLMs%27%0Aaffective%20computing%20capabilities.%20Experimental%20results%20demonstrate%20that%20this%0Aintegrated%20approach%20significantly%20improves%20performance%20across%20various%20MAC%0Atasks%2C%20offering%20a%20promising%20avenue%20for%20future%20research%20and%20development%20in%20this%0Afield.%20Our%20code%20is%20released%20on%20https%3A//github.com/LuoMSen/MLLM-MAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02429v1&entry.124074799=Read"},
{"title": "Dynamic Feature Selection based on Rule-based Learning for Explainable\n  Classification with Uncertainty Quantification", "author": "Javier Fumanal-Idocin and Raquel Fernandez-Peralta and Javier Andreu-Perez", "abstract": "  Dynamic feature selection (DFS) offers a compelling alternative to\ntraditional, static feature selection by adapting the selected features to each\nindividual sample. Unlike classical methods that apply a uniform feature set,\nDFS customizes feature selection per sample, providing insight into the\ndecision-making process for each case. DFS is especially significant in\nsettings where decision transparency is key, i.e., clinical decisions; however,\nexisting methods use opaque models, which hinder their applicability in\nreal-life scenarios. This paper introduces a novel approach leveraging a\nrule-based system as a base classifier for the DFS process, which enhances\ndecision interpretability compared to neural estimators. We also show how this\nmethod provides a quantitative measure of uncertainty for each feature query\nand can make the feature selection process computationally lighter by\nconstraining the feature search space. We also discuss when greedy selection of\nconditional mutual information is equivalent to selecting features that\nminimize the difference with respect to the global model predictions. Finally,\nwe demonstrate the competitive performance of our rule-based DFS approach\nagainst established and state-of-the-art greedy and RL methods, which are\nmostly considered opaque, compared to our explainable rule-based system.\n", "link": "http://arxiv.org/abs/2508.02566v1", "date": "2025-08-04", "relevancy": 2.0189, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Feature%20Selection%20based%20on%20Rule-based%20Learning%20for%20Explainable%0A%20%20Classification%20with%20Uncertainty%20Quantification&body=Title%3A%20Dynamic%20Feature%20Selection%20based%20on%20Rule-based%20Learning%20for%20Explainable%0A%20%20Classification%20with%20Uncertainty%20Quantification%0AAuthor%3A%20Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez%0AAbstract%3A%20%20%20Dynamic%20feature%20selection%20%28DFS%29%20offers%20a%20compelling%20alternative%20to%0Atraditional%2C%20static%20feature%20selection%20by%20adapting%20the%20selected%20features%20to%20each%0Aindividual%20sample.%20Unlike%20classical%20methods%20that%20apply%20a%20uniform%20feature%20set%2C%0ADFS%20customizes%20feature%20selection%20per%20sample%2C%20providing%20insight%20into%20the%0Adecision-making%20process%20for%20each%20case.%20DFS%20is%20especially%20significant%20in%0Asettings%20where%20decision%20transparency%20is%20key%2C%20i.e.%2C%20clinical%20decisions%3B%20however%2C%0Aexisting%20methods%20use%20opaque%20models%2C%20which%20hinder%20their%20applicability%20in%0Areal-life%20scenarios.%20This%20paper%20introduces%20a%20novel%20approach%20leveraging%20a%0Arule-based%20system%20as%20a%20base%20classifier%20for%20the%20DFS%20process%2C%20which%20enhances%0Adecision%20interpretability%20compared%20to%20neural%20estimators.%20We%20also%20show%20how%20this%0Amethod%20provides%20a%20quantitative%20measure%20of%20uncertainty%20for%20each%20feature%20query%0Aand%20can%20make%20the%20feature%20selection%20process%20computationally%20lighter%20by%0Aconstraining%20the%20feature%20search%20space.%20We%20also%20discuss%20when%20greedy%20selection%20of%0Aconditional%20mutual%20information%20is%20equivalent%20to%20selecting%20features%20that%0Aminimize%20the%20difference%20with%20respect%20to%20the%20global%20model%20predictions.%20Finally%2C%0Awe%20demonstrate%20the%20competitive%20performance%20of%20our%20rule-based%20DFS%20approach%0Aagainst%20established%20and%20state-of-the-art%20greedy%20and%20RL%20methods%2C%20which%20are%0Amostly%20considered%20opaque%2C%20compared%20to%20our%20explainable%20rule-based%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Feature%2520Selection%2520based%2520on%2520Rule-based%2520Learning%2520for%2520Explainable%250A%2520%2520Classification%2520with%2520Uncertainty%2520Quantification%26entry.906535625%3DJavier%2520Fumanal-Idocin%2520and%2520Raquel%2520Fernandez-Peralta%2520and%2520Javier%2520Andreu-Perez%26entry.1292438233%3D%2520%2520Dynamic%2520feature%2520selection%2520%2528DFS%2529%2520offers%2520a%2520compelling%2520alternative%2520to%250Atraditional%252C%2520static%2520feature%2520selection%2520by%2520adapting%2520the%2520selected%2520features%2520to%2520each%250Aindividual%2520sample.%2520Unlike%2520classical%2520methods%2520that%2520apply%2520a%2520uniform%2520feature%2520set%252C%250ADFS%2520customizes%2520feature%2520selection%2520per%2520sample%252C%2520providing%2520insight%2520into%2520the%250Adecision-making%2520process%2520for%2520each%2520case.%2520DFS%2520is%2520especially%2520significant%2520in%250Asettings%2520where%2520decision%2520transparency%2520is%2520key%252C%2520i.e.%252C%2520clinical%2520decisions%253B%2520however%252C%250Aexisting%2520methods%2520use%2520opaque%2520models%252C%2520which%2520hinder%2520their%2520applicability%2520in%250Areal-life%2520scenarios.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520leveraging%2520a%250Arule-based%2520system%2520as%2520a%2520base%2520classifier%2520for%2520the%2520DFS%2520process%252C%2520which%2520enhances%250Adecision%2520interpretability%2520compared%2520to%2520neural%2520estimators.%2520We%2520also%2520show%2520how%2520this%250Amethod%2520provides%2520a%2520quantitative%2520measure%2520of%2520uncertainty%2520for%2520each%2520feature%2520query%250Aand%2520can%2520make%2520the%2520feature%2520selection%2520process%2520computationally%2520lighter%2520by%250Aconstraining%2520the%2520feature%2520search%2520space.%2520We%2520also%2520discuss%2520when%2520greedy%2520selection%2520of%250Aconditional%2520mutual%2520information%2520is%2520equivalent%2520to%2520selecting%2520features%2520that%250Aminimize%2520the%2520difference%2520with%2520respect%2520to%2520the%2520global%2520model%2520predictions.%2520Finally%252C%250Awe%2520demonstrate%2520the%2520competitive%2520performance%2520of%2520our%2520rule-based%2520DFS%2520approach%250Aagainst%2520established%2520and%2520state-of-the-art%2520greedy%2520and%2520RL%2520methods%252C%2520which%2520are%250Amostly%2520considered%2520opaque%252C%2520compared%2520to%2520our%2520explainable%2520rule-based%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Feature%20Selection%20based%20on%20Rule-based%20Learning%20for%20Explainable%0A%20%20Classification%20with%20Uncertainty%20Quantification&entry.906535625=Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez&entry.1292438233=%20%20Dynamic%20feature%20selection%20%28DFS%29%20offers%20a%20compelling%20alternative%20to%0Atraditional%2C%20static%20feature%20selection%20by%20adapting%20the%20selected%20features%20to%20each%0Aindividual%20sample.%20Unlike%20classical%20methods%20that%20apply%20a%20uniform%20feature%20set%2C%0ADFS%20customizes%20feature%20selection%20per%20sample%2C%20providing%20insight%20into%20the%0Adecision-making%20process%20for%20each%20case.%20DFS%20is%20especially%20significant%20in%0Asettings%20where%20decision%20transparency%20is%20key%2C%20i.e.%2C%20clinical%20decisions%3B%20however%2C%0Aexisting%20methods%20use%20opaque%20models%2C%20which%20hinder%20their%20applicability%20in%0Areal-life%20scenarios.%20This%20paper%20introduces%20a%20novel%20approach%20leveraging%20a%0Arule-based%20system%20as%20a%20base%20classifier%20for%20the%20DFS%20process%2C%20which%20enhances%0Adecision%20interpretability%20compared%20to%20neural%20estimators.%20We%20also%20show%20how%20this%0Amethod%20provides%20a%20quantitative%20measure%20of%20uncertainty%20for%20each%20feature%20query%0Aand%20can%20make%20the%20feature%20selection%20process%20computationally%20lighter%20by%0Aconstraining%20the%20feature%20search%20space.%20We%20also%20discuss%20when%20greedy%20selection%20of%0Aconditional%20mutual%20information%20is%20equivalent%20to%20selecting%20features%20that%0Aminimize%20the%20difference%20with%20respect%20to%20the%20global%20model%20predictions.%20Finally%2C%0Awe%20demonstrate%20the%20competitive%20performance%20of%20our%20rule-based%20DFS%20approach%0Aagainst%20established%20and%20state-of-the-art%20greedy%20and%20RL%20methods%2C%20which%20are%0Amostly%20considered%20opaque%2C%20compared%20to%20our%20explainable%20rule-based%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02566v1&entry.124074799=Read"},
{"title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models", "author": "Yule Liu and Jingyi Zheng and Zhen Sun and Zifan Peng and Wenhan Dong and Zeyang Sha and Shiwen Cui and Weiqiang Wang and Xinlei He", "abstract": "  Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications.\n", "link": "http://arxiv.org/abs/2504.13626v2", "date": "2025-08-04", "relevancy": 1.4731, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4898}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thought%20Manipulation%3A%20External%20Thought%20Can%20Be%20Efficient%20for%20Large%0A%20%20Reasoning%20Models&body=Title%3A%20Thought%20Manipulation%3A%20External%20Thought%20Can%20Be%20Efficient%20for%20Large%0A%20%20Reasoning%20Models%0AAuthor%3A%20Yule%20Liu%20and%20Jingyi%20Zheng%20and%20Zhen%20Sun%20and%20Zifan%20Peng%20and%20Wenhan%20Dong%20and%20Zeyang%20Sha%20and%20Shiwen%20Cui%20and%20Weiqiang%20Wang%20and%20Xinlei%20He%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20the%0Aeffectiveness%20of%20scaling%20test-time%20computation%20to%20enhance%20reasoning%0Acapabilities%20on%20various%20tasks.%20However%2C%20LRMs%20often%20suffer%20from%20an%0A%60%60overthinking%27%27%20problem%2C%20where%20the%20model%20generates%20excessively%20redundant%0Areasoning%20steps%20with%20limited%20performance%20gains.%20In%20this%20work%2C%20we%20empirically%0Areveal%20an%20important%20characteristic%20of%20LRM%20behaviors%20that%20placing%20external%20CoTs%0Agenerated%20by%20smaller%20models%20between%20the%20thinking%20token%20%28%5Ctexttt%7B%3Cthink%3E%7D%20and%0A%5Ctexttt%7B%3C/think%3E%7D%29%20can%20effectively%20manipulate%20the%20model%20to%20generate%20fewer%0Athoughts.%20Building%20on%20this%20finding%2C%20we%20propose%20a%20simple%20yet%20efficient%20pipeline%2C%0A%5CMethod%2C%20to%20enable%20LRMs%20to%20bypass%20unnecessary%20intermediate%20steps%2C%20thereby%0Asignificantly%20reducing%20computational%20costs.%20We%20conduct%20extensive%20experiments%20to%0Aevaluate%20the%20utility%20and%20efficiency%20of%20%5CMethod.%20For%20instance%2C%20when%20applied%20to%0AQwQ-32B%20on%20the%20LiveBench/Code%20dataset%2C%20%5CMethod%20keeps%20the%20original%20performance%0Awhile%20reducing%20output%20token%20counts%20by%20approximately%2030%5C%25%2C%20with%20minimal%20overhead%0Aintroduced%20by%20the%20CoT%20generator.%20Furthermore%2C%20we%20identify%20two%20suboptimal%20modes%2C%0Ablindly%20following%20flawed%20external%20thoughts%20and%20unnecessary%20rethinking%2C%20and%20show%0Athat%20simple%20mitigations%2C%20such%20as%20difficulty-aware%20fallbacks%2C%20can%20further%0Aimprove%20performance.%20Overall%2C%20%5CMethod%20offers%20a%20practical%2C%20general%2C%20and%0Aefficient%20way%20to%20optimize%20LRM%20inference%2C%20making%20powerful%20reasoning%20models%20more%0Aaccessible%20and%20scalable%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThought%2520Manipulation%253A%2520External%2520Thought%2520Can%2520Be%2520Efficient%2520for%2520Large%250A%2520%2520Reasoning%2520Models%26entry.906535625%3DYule%2520Liu%2520and%2520Jingyi%2520Zheng%2520and%2520Zhen%2520Sun%2520and%2520Zifan%2520Peng%2520and%2520Wenhan%2520Dong%2520and%2520Zeyang%2520Sha%2520and%2520Shiwen%2520Cui%2520and%2520Weiqiang%2520Wang%2520and%2520Xinlei%2520He%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520demonstrated%2520the%250Aeffectiveness%2520of%2520scaling%2520test-time%2520computation%2520to%2520enhance%2520reasoning%250Acapabilities%2520on%2520various%2520tasks.%2520However%252C%2520LRMs%2520often%2520suffer%2520from%2520an%250A%2560%2560overthinking%2527%2527%2520problem%252C%2520where%2520the%2520model%2520generates%2520excessively%2520redundant%250Areasoning%2520steps%2520with%2520limited%2520performance%2520gains.%2520In%2520this%2520work%252C%2520we%2520empirically%250Areveal%2520an%2520important%2520characteristic%2520of%2520LRM%2520behaviors%2520that%2520placing%2520external%2520CoTs%250Agenerated%2520by%2520smaller%2520models%2520between%2520the%2520thinking%2520token%2520%2528%255Ctexttt%257B%253Cthink%253E%257D%2520and%250A%255Ctexttt%257B%253C/think%253E%257D%2529%2520can%2520effectively%2520manipulate%2520the%2520model%2520to%2520generate%2520fewer%250Athoughts.%2520Building%2520on%2520this%2520finding%252C%2520we%2520propose%2520a%2520simple%2520yet%2520efficient%2520pipeline%252C%250A%255CMethod%252C%2520to%2520enable%2520LRMs%2520to%2520bypass%2520unnecessary%2520intermediate%2520steps%252C%2520thereby%250Asignificantly%2520reducing%2520computational%2520costs.%2520We%2520conduct%2520extensive%2520experiments%2520to%250Aevaluate%2520the%2520utility%2520and%2520efficiency%2520of%2520%255CMethod.%2520For%2520instance%252C%2520when%2520applied%2520to%250AQwQ-32B%2520on%2520the%2520LiveBench/Code%2520dataset%252C%2520%255CMethod%2520keeps%2520the%2520original%2520performance%250Awhile%2520reducing%2520output%2520token%2520counts%2520by%2520approximately%252030%255C%2525%252C%2520with%2520minimal%2520overhead%250Aintroduced%2520by%2520the%2520CoT%2520generator.%2520Furthermore%252C%2520we%2520identify%2520two%2520suboptimal%2520modes%252C%250Ablindly%2520following%2520flawed%2520external%2520thoughts%2520and%2520unnecessary%2520rethinking%252C%2520and%2520show%250Athat%2520simple%2520mitigations%252C%2520such%2520as%2520difficulty-aware%2520fallbacks%252C%2520can%2520further%250Aimprove%2520performance.%2520Overall%252C%2520%255CMethod%2520offers%2520a%2520practical%252C%2520general%252C%2520and%250Aefficient%2520way%2520to%2520optimize%2520LRM%2520inference%252C%2520making%2520powerful%2520reasoning%2520models%2520more%250Aaccessible%2520and%2520scalable%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thought%20Manipulation%3A%20External%20Thought%20Can%20Be%20Efficient%20for%20Large%0A%20%20Reasoning%20Models&entry.906535625=Yule%20Liu%20and%20Jingyi%20Zheng%20and%20Zhen%20Sun%20and%20Zifan%20Peng%20and%20Wenhan%20Dong%20and%20Zeyang%20Sha%20and%20Shiwen%20Cui%20and%20Weiqiang%20Wang%20and%20Xinlei%20He&entry.1292438233=%20%20Recent%20advancements%20in%20large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20the%0Aeffectiveness%20of%20scaling%20test-time%20computation%20to%20enhance%20reasoning%0Acapabilities%20on%20various%20tasks.%20However%2C%20LRMs%20often%20suffer%20from%20an%0A%60%60overthinking%27%27%20problem%2C%20where%20the%20model%20generates%20excessively%20redundant%0Areasoning%20steps%20with%20limited%20performance%20gains.%20In%20this%20work%2C%20we%20empirically%0Areveal%20an%20important%20characteristic%20of%20LRM%20behaviors%20that%20placing%20external%20CoTs%0Agenerated%20by%20smaller%20models%20between%20the%20thinking%20token%20%28%5Ctexttt%7B%3Cthink%3E%7D%20and%0A%5Ctexttt%7B%3C/think%3E%7D%29%20can%20effectively%20manipulate%20the%20model%20to%20generate%20fewer%0Athoughts.%20Building%20on%20this%20finding%2C%20we%20propose%20a%20simple%20yet%20efficient%20pipeline%2C%0A%5CMethod%2C%20to%20enable%20LRMs%20to%20bypass%20unnecessary%20intermediate%20steps%2C%20thereby%0Asignificantly%20reducing%20computational%20costs.%20We%20conduct%20extensive%20experiments%20to%0Aevaluate%20the%20utility%20and%20efficiency%20of%20%5CMethod.%20For%20instance%2C%20when%20applied%20to%0AQwQ-32B%20on%20the%20LiveBench/Code%20dataset%2C%20%5CMethod%20keeps%20the%20original%20performance%0Awhile%20reducing%20output%20token%20counts%20by%20approximately%2030%5C%25%2C%20with%20minimal%20overhead%0Aintroduced%20by%20the%20CoT%20generator.%20Furthermore%2C%20we%20identify%20two%20suboptimal%20modes%2C%0Ablindly%20following%20flawed%20external%20thoughts%20and%20unnecessary%20rethinking%2C%20and%20show%0Athat%20simple%20mitigations%2C%20such%20as%20difficulty-aware%20fallbacks%2C%20can%20further%0Aimprove%20performance.%20Overall%2C%20%5CMethod%20offers%20a%20practical%2C%20general%2C%20and%0Aefficient%20way%20to%20optimize%20LRM%20inference%2C%20making%20powerful%20reasoning%20models%20more%0Aaccessible%20and%20scalable%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13626v2&entry.124074799=Read"},
{"title": "An RGB-D Camera-Based Multi-Small Flying Anchors Control for Wire-Driven\n  Robots Connecting to the Environment", "author": "Shintaro Inoue and Kento Kawaharazuka and Keita Yoneda and Sota Yuzaki and Yuta Sahara and Temma Suzuki and Kei Okada", "abstract": "  In order to expand the operational range and payload capacity of robots,\nwire-driven robots that leverage the external environment have been proposed.\nIt can exert forces and operate in spaces far beyond those dictated by its own\nstructural limits. However, for practical use, robots must autonomously attach\nmultiple wires to the environment based on environmental recognition-an\noperation so difficult that many wire-driven robots remain restricted to\nspecialized, pre-designed environments. Here, in this study, we propose a robot\nthat autonomously connects multiple wires to the environment by employing a\nmulti-small flying anchor system, as well as an RGB-D camera-based control and\nenvironmental recognition method. Each flying anchor is a drone with an\nanchoring mechanism at the wire tip, allowing the robot to attach wires by\nflying into position. Using the robot's RGB-D camera to identify suitable\nattachment points and a flying anchor position, the system can connect wires in\nenvironments that are not specially prepared, and can also attach multiple\nwires simultaneously. Through this approach, a wire-driven robot can\nautonomously attach its wires to the environment, thereby realizing the\nbenefits of wire-driven operation at any location.\n", "link": "http://arxiv.org/abs/2508.02544v1", "date": "2025-08-04", "relevancy": 1.5, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20RGB-D%20Camera-Based%20Multi-Small%20Flying%20Anchors%20Control%20for%20Wire-Driven%0A%20%20Robots%20Connecting%20to%20the%20Environment&body=Title%3A%20An%20RGB-D%20Camera-Based%20Multi-Small%20Flying%20Anchors%20Control%20for%20Wire-Driven%0A%20%20Robots%20Connecting%20to%20the%20Environment%0AAuthor%3A%20Shintaro%20Inoue%20and%20Kento%20Kawaharazuka%20and%20Keita%20Yoneda%20and%20Sota%20Yuzaki%20and%20Yuta%20Sahara%20and%20Temma%20Suzuki%20and%20Kei%20Okada%0AAbstract%3A%20%20%20In%20order%20to%20expand%20the%20operational%20range%20and%20payload%20capacity%20of%20robots%2C%0Awire-driven%20robots%20that%20leverage%20the%20external%20environment%20have%20been%20proposed.%0AIt%20can%20exert%20forces%20and%20operate%20in%20spaces%20far%20beyond%20those%20dictated%20by%20its%20own%0Astructural%20limits.%20However%2C%20for%20practical%20use%2C%20robots%20must%20autonomously%20attach%0Amultiple%20wires%20to%20the%20environment%20based%20on%20environmental%20recognition-an%0Aoperation%20so%20difficult%20that%20many%20wire-driven%20robots%20remain%20restricted%20to%0Aspecialized%2C%20pre-designed%20environments.%20Here%2C%20in%20this%20study%2C%20we%20propose%20a%20robot%0Athat%20autonomously%20connects%20multiple%20wires%20to%20the%20environment%20by%20employing%20a%0Amulti-small%20flying%20anchor%20system%2C%20as%20well%20as%20an%20RGB-D%20camera-based%20control%20and%0Aenvironmental%20recognition%20method.%20Each%20flying%20anchor%20is%20a%20drone%20with%20an%0Aanchoring%20mechanism%20at%20the%20wire%20tip%2C%20allowing%20the%20robot%20to%20attach%20wires%20by%0Aflying%20into%20position.%20Using%20the%20robot%27s%20RGB-D%20camera%20to%20identify%20suitable%0Aattachment%20points%20and%20a%20flying%20anchor%20position%2C%20the%20system%20can%20connect%20wires%20in%0Aenvironments%20that%20are%20not%20specially%20prepared%2C%20and%20can%20also%20attach%20multiple%0Awires%20simultaneously.%20Through%20this%20approach%2C%20a%20wire-driven%20robot%20can%0Aautonomously%20attach%20its%20wires%20to%20the%20environment%2C%20thereby%20realizing%20the%0Abenefits%20of%20wire-driven%20operation%20at%20any%20location.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520RGB-D%2520Camera-Based%2520Multi-Small%2520Flying%2520Anchors%2520Control%2520for%2520Wire-Driven%250A%2520%2520Robots%2520Connecting%2520to%2520the%2520Environment%26entry.906535625%3DShintaro%2520Inoue%2520and%2520Kento%2520Kawaharazuka%2520and%2520Keita%2520Yoneda%2520and%2520Sota%2520Yuzaki%2520and%2520Yuta%2520Sahara%2520and%2520Temma%2520Suzuki%2520and%2520Kei%2520Okada%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520expand%2520the%2520operational%2520range%2520and%2520payload%2520capacity%2520of%2520robots%252C%250Awire-driven%2520robots%2520that%2520leverage%2520the%2520external%2520environment%2520have%2520been%2520proposed.%250AIt%2520can%2520exert%2520forces%2520and%2520operate%2520in%2520spaces%2520far%2520beyond%2520those%2520dictated%2520by%2520its%2520own%250Astructural%2520limits.%2520However%252C%2520for%2520practical%2520use%252C%2520robots%2520must%2520autonomously%2520attach%250Amultiple%2520wires%2520to%2520the%2520environment%2520based%2520on%2520environmental%2520recognition-an%250Aoperation%2520so%2520difficult%2520that%2520many%2520wire-driven%2520robots%2520remain%2520restricted%2520to%250Aspecialized%252C%2520pre-designed%2520environments.%2520Here%252C%2520in%2520this%2520study%252C%2520we%2520propose%2520a%2520robot%250Athat%2520autonomously%2520connects%2520multiple%2520wires%2520to%2520the%2520environment%2520by%2520employing%2520a%250Amulti-small%2520flying%2520anchor%2520system%252C%2520as%2520well%2520as%2520an%2520RGB-D%2520camera-based%2520control%2520and%250Aenvironmental%2520recognition%2520method.%2520Each%2520flying%2520anchor%2520is%2520a%2520drone%2520with%2520an%250Aanchoring%2520mechanism%2520at%2520the%2520wire%2520tip%252C%2520allowing%2520the%2520robot%2520to%2520attach%2520wires%2520by%250Aflying%2520into%2520position.%2520Using%2520the%2520robot%2527s%2520RGB-D%2520camera%2520to%2520identify%2520suitable%250Aattachment%2520points%2520and%2520a%2520flying%2520anchor%2520position%252C%2520the%2520system%2520can%2520connect%2520wires%2520in%250Aenvironments%2520that%2520are%2520not%2520specially%2520prepared%252C%2520and%2520can%2520also%2520attach%2520multiple%250Awires%2520simultaneously.%2520Through%2520this%2520approach%252C%2520a%2520wire-driven%2520robot%2520can%250Aautonomously%2520attach%2520its%2520wires%2520to%2520the%2520environment%252C%2520thereby%2520realizing%2520the%250Abenefits%2520of%2520wire-driven%2520operation%2520at%2520any%2520location.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20RGB-D%20Camera-Based%20Multi-Small%20Flying%20Anchors%20Control%20for%20Wire-Driven%0A%20%20Robots%20Connecting%20to%20the%20Environment&entry.906535625=Shintaro%20Inoue%20and%20Kento%20Kawaharazuka%20and%20Keita%20Yoneda%20and%20Sota%20Yuzaki%20and%20Yuta%20Sahara%20and%20Temma%20Suzuki%20and%20Kei%20Okada&entry.1292438233=%20%20In%20order%20to%20expand%20the%20operational%20range%20and%20payload%20capacity%20of%20robots%2C%0Awire-driven%20robots%20that%20leverage%20the%20external%20environment%20have%20been%20proposed.%0AIt%20can%20exert%20forces%20and%20operate%20in%20spaces%20far%20beyond%20those%20dictated%20by%20its%20own%0Astructural%20limits.%20However%2C%20for%20practical%20use%2C%20robots%20must%20autonomously%20attach%0Amultiple%20wires%20to%20the%20environment%20based%20on%20environmental%20recognition-an%0Aoperation%20so%20difficult%20that%20many%20wire-driven%20robots%20remain%20restricted%20to%0Aspecialized%2C%20pre-designed%20environments.%20Here%2C%20in%20this%20study%2C%20we%20propose%20a%20robot%0Athat%20autonomously%20connects%20multiple%20wires%20to%20the%20environment%20by%20employing%20a%0Amulti-small%20flying%20anchor%20system%2C%20as%20well%20as%20an%20RGB-D%20camera-based%20control%20and%0Aenvironmental%20recognition%20method.%20Each%20flying%20anchor%20is%20a%20drone%20with%20an%0Aanchoring%20mechanism%20at%20the%20wire%20tip%2C%20allowing%20the%20robot%20to%20attach%20wires%20by%0Aflying%20into%20position.%20Using%20the%20robot%27s%20RGB-D%20camera%20to%20identify%20suitable%0Aattachment%20points%20and%20a%20flying%20anchor%20position%2C%20the%20system%20can%20connect%20wires%20in%0Aenvironments%20that%20are%20not%20specially%20prepared%2C%20and%20can%20also%20attach%20multiple%0Awires%20simultaneously.%20Through%20this%20approach%2C%20a%20wire-driven%20robot%20can%0Aautonomously%20attach%20its%20wires%20to%20the%20environment%2C%20thereby%20realizing%20the%0Abenefits%20of%20wire-driven%20operation%20at%20any%20location.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02544v1&entry.124074799=Read"},
{"title": "HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time\n  Series Analysis", "author": "Xiao Wang and Hao Si and Fan Zhang and Xiaoya Zhou and Dengdi Sun and Wanli Lyu and Qingquan Yang and Jin Tang", "abstract": "  Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.\n", "link": "http://arxiv.org/abs/2508.02411v1", "date": "2025-08-04", "relevancy": 1.8986, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4913}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGTS-Former%3A%20Hierarchical%20HyperGraph%20Transformer%20for%20Multivariate%20Time%0A%20%20Series%20Analysis&body=Title%3A%20HGTS-Former%3A%20Hierarchical%20HyperGraph%20Transformer%20for%20Multivariate%20Time%0A%20%20Series%20Analysis%0AAuthor%3A%20Xiao%20Wang%20and%20Hao%20Si%20and%20Fan%20Zhang%20and%20Xiaoya%20Zhou%20and%20Dengdi%20Sun%20and%20Wanli%20Lyu%20and%20Qingquan%20Yang%20and%20Jin%20Tang%0AAbstract%3A%20%20%20Multivariate%20time%20series%20analysis%20has%20long%20been%20one%20of%20the%20key%20research%0Atopics%20in%20the%20field%20of%20artificial%20intelligence.%20However%2C%20analyzing%20complex%20time%0Aseries%20data%20remains%20a%20challenging%20and%20unresolved%20problem%20due%20to%20its%20high%0Adimensionality%2C%20dynamic%20nature%2C%20and%20complex%20interactions%20among%20variables.%0AInspired%20by%20the%20strong%20structural%20modeling%20capability%20of%20hypergraphs%2C%20this%0Apaper%20proposes%20a%20novel%20hypergraph-based%20time%20series%20transformer%20backbone%0Anetwork%2C%20termed%20HGTS-Former%2C%20to%20address%20the%20multivariate%20coupling%20in%20time%0Aseries%20data.%20Specifically%2C%20given%20the%20multivariate%20time%20series%20signal%2C%20we%20first%0Anormalize%20and%20embed%20each%20patch%20into%20tokens.%20Then%2C%20we%20adopt%20the%20multi-head%0Aself-attention%20to%20enhance%20the%20temporal%20representation%20of%20each%20patch.%20The%0Ahierarchical%20hypergraphs%20are%20constructed%20to%20aggregate%20the%20temporal%20patterns%0Awithin%20each%20channel%20and%20fine-grained%20relations%20between%20different%20variables.%0AAfter%20that%2C%20we%20convert%20the%20hyperedge%20into%20node%20features%20through%20the%20EdgeToNode%0Amodule%20and%20adopt%20the%20feed-forward%20network%20to%20further%20enhance%20the%20output%0Afeatures.%20Extensive%20experiments%20conducted%20on%20two%20multivariate%20time%20series%20tasks%0Aand%20eight%20datasets%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%0AHGTS-Former.%20The%20source%20code%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/Time_Series_Analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGTS-Former%253A%2520Hierarchical%2520HyperGraph%2520Transformer%2520for%2520Multivariate%2520Time%250A%2520%2520Series%2520Analysis%26entry.906535625%3DXiao%2520Wang%2520and%2520Hao%2520Si%2520and%2520Fan%2520Zhang%2520and%2520Xiaoya%2520Zhou%2520and%2520Dengdi%2520Sun%2520and%2520Wanli%2520Lyu%2520and%2520Qingquan%2520Yang%2520and%2520Jin%2520Tang%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520analysis%2520has%2520long%2520been%2520one%2520of%2520the%2520key%2520research%250Atopics%2520in%2520the%2520field%2520of%2520artificial%2520intelligence.%2520However%252C%2520analyzing%2520complex%2520time%250Aseries%2520data%2520remains%2520a%2520challenging%2520and%2520unresolved%2520problem%2520due%2520to%2520its%2520high%250Adimensionality%252C%2520dynamic%2520nature%252C%2520and%2520complex%2520interactions%2520among%2520variables.%250AInspired%2520by%2520the%2520strong%2520structural%2520modeling%2520capability%2520of%2520hypergraphs%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520hypergraph-based%2520time%2520series%2520transformer%2520backbone%250Anetwork%252C%2520termed%2520HGTS-Former%252C%2520to%2520address%2520the%2520multivariate%2520coupling%2520in%2520time%250Aseries%2520data.%2520Specifically%252C%2520given%2520the%2520multivariate%2520time%2520series%2520signal%252C%2520we%2520first%250Anormalize%2520and%2520embed%2520each%2520patch%2520into%2520tokens.%2520Then%252C%2520we%2520adopt%2520the%2520multi-head%250Aself-attention%2520to%2520enhance%2520the%2520temporal%2520representation%2520of%2520each%2520patch.%2520The%250Ahierarchical%2520hypergraphs%2520are%2520constructed%2520to%2520aggregate%2520the%2520temporal%2520patterns%250Awithin%2520each%2520channel%2520and%2520fine-grained%2520relations%2520between%2520different%2520variables.%250AAfter%2520that%252C%2520we%2520convert%2520the%2520hyperedge%2520into%2520node%2520features%2520through%2520the%2520EdgeToNode%250Amodule%2520and%2520adopt%2520the%2520feed-forward%2520network%2520to%2520further%2520enhance%2520the%2520output%250Afeatures.%2520Extensive%2520experiments%2520conducted%2520on%2520two%2520multivariate%2520time%2520series%2520tasks%250Aand%2520eight%2520datasets%2520fully%2520validated%2520the%2520effectiveness%2520of%2520our%2520proposed%250AHGTS-Former.%2520The%2520source%2520code%2520will%2520be%2520released%2520on%250Ahttps%253A//github.com/Event-AHU/Time_Series_Analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGTS-Former%3A%20Hierarchical%20HyperGraph%20Transformer%20for%20Multivariate%20Time%0A%20%20Series%20Analysis&entry.906535625=Xiao%20Wang%20and%20Hao%20Si%20and%20Fan%20Zhang%20and%20Xiaoya%20Zhou%20and%20Dengdi%20Sun%20and%20Wanli%20Lyu%20and%20Qingquan%20Yang%20and%20Jin%20Tang&entry.1292438233=%20%20Multivariate%20time%20series%20analysis%20has%20long%20been%20one%20of%20the%20key%20research%0Atopics%20in%20the%20field%20of%20artificial%20intelligence.%20However%2C%20analyzing%20complex%20time%0Aseries%20data%20remains%20a%20challenging%20and%20unresolved%20problem%20due%20to%20its%20high%0Adimensionality%2C%20dynamic%20nature%2C%20and%20complex%20interactions%20among%20variables.%0AInspired%20by%20the%20strong%20structural%20modeling%20capability%20of%20hypergraphs%2C%20this%0Apaper%20proposes%20a%20novel%20hypergraph-based%20time%20series%20transformer%20backbone%0Anetwork%2C%20termed%20HGTS-Former%2C%20to%20address%20the%20multivariate%20coupling%20in%20time%0Aseries%20data.%20Specifically%2C%20given%20the%20multivariate%20time%20series%20signal%2C%20we%20first%0Anormalize%20and%20embed%20each%20patch%20into%20tokens.%20Then%2C%20we%20adopt%20the%20multi-head%0Aself-attention%20to%20enhance%20the%20temporal%20representation%20of%20each%20patch.%20The%0Ahierarchical%20hypergraphs%20are%20constructed%20to%20aggregate%20the%20temporal%20patterns%0Awithin%20each%20channel%20and%20fine-grained%20relations%20between%20different%20variables.%0AAfter%20that%2C%20we%20convert%20the%20hyperedge%20into%20node%20features%20through%20the%20EdgeToNode%0Amodule%20and%20adopt%20the%20feed-forward%20network%20to%20further%20enhance%20the%20output%0Afeatures.%20Extensive%20experiments%20conducted%20on%20two%20multivariate%20time%20series%20tasks%0Aand%20eight%20datasets%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%0AHGTS-Former.%20The%20source%20code%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/Time_Series_Analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02411v1&entry.124074799=Read"},
{"title": "Pre-Tactical Flight-Delay and Turnaround Forecasting with Synthetic\n  Aviation Data", "author": "Abdulmajid Murad and Massimiliano Ruocco", "abstract": "  Access to comprehensive flight operations data remains severely restricted in\naviation due to commercial sensitivity and competitive considerations,\nhindering the development of predictive models for operational planning. This\npaper investigates whether synthetic data can effectively replace real\noperational data for training machine learning models in pre-tactical aviation\nscenarios-predictions made hours to days before operations using only scheduled\nflight information. We evaluate four state-of-the-art synthetic data generators\non three prediction tasks: aircraft turnaround time, departure delays, and\narrival delays. Using a Train on Synthetic, Test on Real (TSTR) methodology on\nover 1.7 million European flight records, we first validate synthetic data\nquality through fidelity assessments, then assess both predictive performance\nand the preservation of operational relationships. Our results show that\nadvanced neural network architectures, specifically transformer-based\ngenerators, can retain 94-97% of real-data predictive performance while\nmaintaining feature importance patterns informative for operational\ndecision-making. Our analysis reveals that even with real data, prediction\naccuracy is inherently limited when only scheduled information is\navailable-establishing realistic baselines for pre-tactical forecasting. These\nfindings suggest that high-quality synthetic data can enable broader access to\naviation analytics capabilities while preserving commercial confidentiality,\nthough stakeholders must maintain realistic expectations about pre-tactical\nprediction accuracy given the stochastic nature of flight operations.\n", "link": "http://arxiv.org/abs/2508.02294v1", "date": "2025-08-04", "relevancy": 1.3952, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4858}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.461}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Tactical%20Flight-Delay%20and%20Turnaround%20Forecasting%20with%20Synthetic%0A%20%20Aviation%20Data&body=Title%3A%20Pre-Tactical%20Flight-Delay%20and%20Turnaround%20Forecasting%20with%20Synthetic%0A%20%20Aviation%20Data%0AAuthor%3A%20Abdulmajid%20Murad%20and%20Massimiliano%20Ruocco%0AAbstract%3A%20%20%20Access%20to%20comprehensive%20flight%20operations%20data%20remains%20severely%20restricted%20in%0Aaviation%20due%20to%20commercial%20sensitivity%20and%20competitive%20considerations%2C%0Ahindering%20the%20development%20of%20predictive%20models%20for%20operational%20planning.%20This%0Apaper%20investigates%20whether%20synthetic%20data%20can%20effectively%20replace%20real%0Aoperational%20data%20for%20training%20machine%20learning%20models%20in%20pre-tactical%20aviation%0Ascenarios-predictions%20made%20hours%20to%20days%20before%20operations%20using%20only%20scheduled%0Aflight%20information.%20We%20evaluate%20four%20state-of-the-art%20synthetic%20data%20generators%0Aon%20three%20prediction%20tasks%3A%20aircraft%20turnaround%20time%2C%20departure%20delays%2C%20and%0Aarrival%20delays.%20Using%20a%20Train%20on%20Synthetic%2C%20Test%20on%20Real%20%28TSTR%29%20methodology%20on%0Aover%201.7%20million%20European%20flight%20records%2C%20we%20first%20validate%20synthetic%20data%0Aquality%20through%20fidelity%20assessments%2C%20then%20assess%20both%20predictive%20performance%0Aand%20the%20preservation%20of%20operational%20relationships.%20Our%20results%20show%20that%0Aadvanced%20neural%20network%20architectures%2C%20specifically%20transformer-based%0Agenerators%2C%20can%20retain%2094-97%25%20of%20real-data%20predictive%20performance%20while%0Amaintaining%20feature%20importance%20patterns%20informative%20for%20operational%0Adecision-making.%20Our%20analysis%20reveals%20that%20even%20with%20real%20data%2C%20prediction%0Aaccuracy%20is%20inherently%20limited%20when%20only%20scheduled%20information%20is%0Aavailable-establishing%20realistic%20baselines%20for%20pre-tactical%20forecasting.%20These%0Afindings%20suggest%20that%20high-quality%20synthetic%20data%20can%20enable%20broader%20access%20to%0Aaviation%20analytics%20capabilities%20while%20preserving%20commercial%20confidentiality%2C%0Athough%20stakeholders%20must%20maintain%20realistic%20expectations%20about%20pre-tactical%0Aprediction%20accuracy%20given%20the%20stochastic%20nature%20of%20flight%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Tactical%2520Flight-Delay%2520and%2520Turnaround%2520Forecasting%2520with%2520Synthetic%250A%2520%2520Aviation%2520Data%26entry.906535625%3DAbdulmajid%2520Murad%2520and%2520Massimiliano%2520Ruocco%26entry.1292438233%3D%2520%2520Access%2520to%2520comprehensive%2520flight%2520operations%2520data%2520remains%2520severely%2520restricted%2520in%250Aaviation%2520due%2520to%2520commercial%2520sensitivity%2520and%2520competitive%2520considerations%252C%250Ahindering%2520the%2520development%2520of%2520predictive%2520models%2520for%2520operational%2520planning.%2520This%250Apaper%2520investigates%2520whether%2520synthetic%2520data%2520can%2520effectively%2520replace%2520real%250Aoperational%2520data%2520for%2520training%2520machine%2520learning%2520models%2520in%2520pre-tactical%2520aviation%250Ascenarios-predictions%2520made%2520hours%2520to%2520days%2520before%2520operations%2520using%2520only%2520scheduled%250Aflight%2520information.%2520We%2520evaluate%2520four%2520state-of-the-art%2520synthetic%2520data%2520generators%250Aon%2520three%2520prediction%2520tasks%253A%2520aircraft%2520turnaround%2520time%252C%2520departure%2520delays%252C%2520and%250Aarrival%2520delays.%2520Using%2520a%2520Train%2520on%2520Synthetic%252C%2520Test%2520on%2520Real%2520%2528TSTR%2529%2520methodology%2520on%250Aover%25201.7%2520million%2520European%2520flight%2520records%252C%2520we%2520first%2520validate%2520synthetic%2520data%250Aquality%2520through%2520fidelity%2520assessments%252C%2520then%2520assess%2520both%2520predictive%2520performance%250Aand%2520the%2520preservation%2520of%2520operational%2520relationships.%2520Our%2520results%2520show%2520that%250Aadvanced%2520neural%2520network%2520architectures%252C%2520specifically%2520transformer-based%250Agenerators%252C%2520can%2520retain%252094-97%2525%2520of%2520real-data%2520predictive%2520performance%2520while%250Amaintaining%2520feature%2520importance%2520patterns%2520informative%2520for%2520operational%250Adecision-making.%2520Our%2520analysis%2520reveals%2520that%2520even%2520with%2520real%2520data%252C%2520prediction%250Aaccuracy%2520is%2520inherently%2520limited%2520when%2520only%2520scheduled%2520information%2520is%250Aavailable-establishing%2520realistic%2520baselines%2520for%2520pre-tactical%2520forecasting.%2520These%250Afindings%2520suggest%2520that%2520high-quality%2520synthetic%2520data%2520can%2520enable%2520broader%2520access%2520to%250Aaviation%2520analytics%2520capabilities%2520while%2520preserving%2520commercial%2520confidentiality%252C%250Athough%2520stakeholders%2520must%2520maintain%2520realistic%2520expectations%2520about%2520pre-tactical%250Aprediction%2520accuracy%2520given%2520the%2520stochastic%2520nature%2520of%2520flight%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Tactical%20Flight-Delay%20and%20Turnaround%20Forecasting%20with%20Synthetic%0A%20%20Aviation%20Data&entry.906535625=Abdulmajid%20Murad%20and%20Massimiliano%20Ruocco&entry.1292438233=%20%20Access%20to%20comprehensive%20flight%20operations%20data%20remains%20severely%20restricted%20in%0Aaviation%20due%20to%20commercial%20sensitivity%20and%20competitive%20considerations%2C%0Ahindering%20the%20development%20of%20predictive%20models%20for%20operational%20planning.%20This%0Apaper%20investigates%20whether%20synthetic%20data%20can%20effectively%20replace%20real%0Aoperational%20data%20for%20training%20machine%20learning%20models%20in%20pre-tactical%20aviation%0Ascenarios-predictions%20made%20hours%20to%20days%20before%20operations%20using%20only%20scheduled%0Aflight%20information.%20We%20evaluate%20four%20state-of-the-art%20synthetic%20data%20generators%0Aon%20three%20prediction%20tasks%3A%20aircraft%20turnaround%20time%2C%20departure%20delays%2C%20and%0Aarrival%20delays.%20Using%20a%20Train%20on%20Synthetic%2C%20Test%20on%20Real%20%28TSTR%29%20methodology%20on%0Aover%201.7%20million%20European%20flight%20records%2C%20we%20first%20validate%20synthetic%20data%0Aquality%20through%20fidelity%20assessments%2C%20then%20assess%20both%20predictive%20performance%0Aand%20the%20preservation%20of%20operational%20relationships.%20Our%20results%20show%20that%0Aadvanced%20neural%20network%20architectures%2C%20specifically%20transformer-based%0Agenerators%2C%20can%20retain%2094-97%25%20of%20real-data%20predictive%20performance%20while%0Amaintaining%20feature%20importance%20patterns%20informative%20for%20operational%0Adecision-making.%20Our%20analysis%20reveals%20that%20even%20with%20real%20data%2C%20prediction%0Aaccuracy%20is%20inherently%20limited%20when%20only%20scheduled%20information%20is%0Aavailable-establishing%20realistic%20baselines%20for%20pre-tactical%20forecasting.%20These%0Afindings%20suggest%20that%20high-quality%20synthetic%20data%20can%20enable%20broader%20access%20to%0Aaviation%20analytics%20capabilities%20while%20preserving%20commercial%20confidentiality%2C%0Athough%20stakeholders%20must%20maintain%20realistic%20expectations%20about%20pre-tactical%0Aprediction%20accuracy%20given%20the%20stochastic%20nature%20of%20flight%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02294v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


