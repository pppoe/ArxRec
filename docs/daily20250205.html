<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250204.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "author": "Zhihao Guo and Jingxuan Su and Shenglin Wang and Jinlong Fan and Jing Zhang and Liangxiu Han and Peng Wang", "abstract": "  3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.\n", "link": "http://arxiv.org/abs/2502.02283v1", "date": "2025-02-04", "relevancy": 3.5896, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7521}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7244}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&body=Title%3A%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting%0AAuthor%3A%20Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Liangxiu%20Han%20and%20Peng%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20consistently%20compromises%20the%20scene%20reconstruction%20quality.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%0Aframework%20Gaussian%20Processes%20Gaussian%20Splatting%20%28GP-GS%29%2C%20where%20a%20multi-output%0AGaussian%20Process%20model%20is%20developed%20to%20achieve%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20The%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%20to%20enhance%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGP-GS%253A%2520Gaussian%2520Processes%2520for%2520Enhanced%2520Gaussian%2520Splatting%26entry.906535625%3DZhihao%2520Guo%2520and%2520Jingxuan%2520Su%2520and%2520Shenglin%2520Wang%2520and%2520Jinlong%2520Fan%2520and%2520Jing%2520Zhang%2520and%2520Liangxiu%2520Han%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520an%2520efficient%2520photorealistic%2520novel%2520view%250Asynthesis%2520method.%2520However%252C%2520its%2520reliance%2520on%2520sparse%2520Structure-from-Motion%2520%2528SfM%2529%250Apoint%2520clouds%2520consistently%2520compromises%2520the%2520scene%2520reconstruction%2520quality.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%25203D%2520reconstruction%250Aframework%2520Gaussian%2520Processes%2520Gaussian%2520Splatting%2520%2528GP-GS%2529%252C%2520where%2520a%2520multi-output%250AGaussian%2520Process%2520model%2520is%2520developed%2520to%2520achieve%2520adaptive%2520and%2520uncertainty-guided%250Adensification%2520of%2520sparse%2520SfM%2520point%2520clouds.%2520Specifically%252C%2520we%2520propose%2520a%2520dynamic%250Asampling%2520and%2520filtering%2520pipeline%2520that%2520adaptively%2520expands%2520the%2520SfM%2520point%2520clouds%2520by%250Aleveraging%2520GP-based%2520predictions%2520to%2520infer%2520new%2520candidate%2520points%2520from%2520the%2520input%25202D%250Apixels%2520and%2520depth%2520maps.%2520The%2520pipeline%2520utilizes%2520uncertainty%2520estimates%2520to%2520guide%2520the%250Apruning%2520of%2520high-variance%2520predictions%252C%2520ensuring%2520geometric%2520consistency%2520and%250Aenabling%2520the%2520generation%2520of%2520dense%2520point%2520clouds.%2520The%2520densified%2520point%2520clouds%250Aprovide%2520high-quality%2520initial%25203D%2520Gaussians%2520to%2520enhance%2520reconstruction%250Aperformance.%2520Extensive%2520experiments%2520conducted%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520across%2520various%2520scales%2520validate%2520the%2520effectiveness%2520and%2520practicality%2520of%250Athe%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&entry.906535625=Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Liangxiu%20Han%20and%20Peng%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20consistently%20compromises%20the%20scene%20reconstruction%20quality.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%0Aframework%20Gaussian%20Processes%20Gaussian%20Splatting%20%28GP-GS%29%2C%20where%20a%20multi-output%0AGaussian%20Process%20model%20is%20developed%20to%20achieve%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20The%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%20to%20enhance%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02283v1&entry.124074799=Read"},
{"title": "Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D\n  Segmentation", "author": "Junha Lee and Chunghyun Park and Jaesung Choe and Yu-Chiang Frank Wang and Jan Kautz and Minsu Cho and Chris Choy", "abstract": "  We tackle open-vocabulary 3D scene understanding by introducing a novel data\ngeneration pipeline and training framework. Our method addresses three critical\nrequirements for effective training: precise 3D region segmentation,\ncomprehensive textual descriptions, and sufficient dataset scale. By leveraging\nstate-of-the-art open-vocabulary image segmentation models and region-aware\nVision-Language Models, we develop an automatic pipeline that generates\nhigh-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene\ndatasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with\n5.6M mask-text pairs, significantly larger than existing datasets. Building\nupon this data, we propose Mosaic3D, a foundation model combining a 3D encoder\ntrained with contrastive learning and a lightweight mask decoder for\nopen-vocabulary 3D semantic and instance segmentation. Our approach achieves\nstate-of-the-art results on open-vocabulary 3D semantic and instance\nsegmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with\nablation studies validating the effectiveness of our large-scale training data.\n", "link": "http://arxiv.org/abs/2502.02548v1", "date": "2025-02-04", "relevancy": 3.339, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6908}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mosaic3D%3A%20Foundation%20Dataset%20and%20Model%20for%20Open-Vocabulary%203D%0A%20%20Segmentation&body=Title%3A%20Mosaic3D%3A%20Foundation%20Dataset%20and%20Model%20for%20Open-Vocabulary%203D%0A%20%20Segmentation%0AAuthor%3A%20Junha%20Lee%20and%20Chunghyun%20Park%20and%20Jaesung%20Choe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Jan%20Kautz%20and%20Minsu%20Cho%20and%20Chris%20Choy%0AAbstract%3A%20%20%20We%20tackle%20open-vocabulary%203D%20scene%20understanding%20by%20introducing%20a%20novel%20data%0Ageneration%20pipeline%20and%20training%20framework.%20Our%20method%20addresses%20three%20critical%0Arequirements%20for%20effective%20training%3A%20precise%203D%20region%20segmentation%2C%0Acomprehensive%20textual%20descriptions%2C%20and%20sufficient%20dataset%20scale.%20By%20leveraging%0Astate-of-the-art%20open-vocabulary%20image%20segmentation%20models%20and%20region-aware%0AVision-Language%20Models%2C%20we%20develop%20an%20automatic%20pipeline%20that%20generates%0Ahigh-quality%203D%20mask-text%20pairs.%20Applying%20this%20pipeline%20to%20multiple%203D%20scene%0Adatasets%2C%20we%20create%20Mosaic3D-5.6M%2C%20a%20dataset%20of%20over%2030K%20annotated%20scenes%20with%0A5.6M%20mask-text%20pairs%2C%20significantly%20larger%20than%20existing%20datasets.%20Building%0Aupon%20this%20data%2C%20we%20propose%20Mosaic3D%2C%20a%20foundation%20model%20combining%20a%203D%20encoder%0Atrained%20with%20contrastive%20learning%20and%20a%20lightweight%20mask%20decoder%20for%0Aopen-vocabulary%203D%20semantic%20and%20instance%20segmentation.%20Our%20approach%20achieves%0Astate-of-the-art%20results%20on%20open-vocabulary%203D%20semantic%20and%20instance%0Asegmentation%20tasks%20including%20ScanNet200%2C%20Matterport3D%2C%20and%20ScanNet%2B%2B%2C%20with%0Aablation%20studies%20validating%20the%20effectiveness%20of%20our%20large-scale%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMosaic3D%253A%2520Foundation%2520Dataset%2520and%2520Model%2520for%2520Open-Vocabulary%25203D%250A%2520%2520Segmentation%26entry.906535625%3DJunha%2520Lee%2520and%2520Chunghyun%2520Park%2520and%2520Jaesung%2520Choe%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Jan%2520Kautz%2520and%2520Minsu%2520Cho%2520and%2520Chris%2520Choy%26entry.1292438233%3D%2520%2520We%2520tackle%2520open-vocabulary%25203D%2520scene%2520understanding%2520by%2520introducing%2520a%2520novel%2520data%250Ageneration%2520pipeline%2520and%2520training%2520framework.%2520Our%2520method%2520addresses%2520three%2520critical%250Arequirements%2520for%2520effective%2520training%253A%2520precise%25203D%2520region%2520segmentation%252C%250Acomprehensive%2520textual%2520descriptions%252C%2520and%2520sufficient%2520dataset%2520scale.%2520By%2520leveraging%250Astate-of-the-art%2520open-vocabulary%2520image%2520segmentation%2520models%2520and%2520region-aware%250AVision-Language%2520Models%252C%2520we%2520develop%2520an%2520automatic%2520pipeline%2520that%2520generates%250Ahigh-quality%25203D%2520mask-text%2520pairs.%2520Applying%2520this%2520pipeline%2520to%2520multiple%25203D%2520scene%250Adatasets%252C%2520we%2520create%2520Mosaic3D-5.6M%252C%2520a%2520dataset%2520of%2520over%252030K%2520annotated%2520scenes%2520with%250A5.6M%2520mask-text%2520pairs%252C%2520significantly%2520larger%2520than%2520existing%2520datasets.%2520Building%250Aupon%2520this%2520data%252C%2520we%2520propose%2520Mosaic3D%252C%2520a%2520foundation%2520model%2520combining%2520a%25203D%2520encoder%250Atrained%2520with%2520contrastive%2520learning%2520and%2520a%2520lightweight%2520mask%2520decoder%2520for%250Aopen-vocabulary%25203D%2520semantic%2520and%2520instance%2520segmentation.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520results%2520on%2520open-vocabulary%25203D%2520semantic%2520and%2520instance%250Asegmentation%2520tasks%2520including%2520ScanNet200%252C%2520Matterport3D%252C%2520and%2520ScanNet%252B%252B%252C%2520with%250Aablation%2520studies%2520validating%2520the%2520effectiveness%2520of%2520our%2520large-scale%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mosaic3D%3A%20Foundation%20Dataset%20and%20Model%20for%20Open-Vocabulary%203D%0A%20%20Segmentation&entry.906535625=Junha%20Lee%20and%20Chunghyun%20Park%20and%20Jaesung%20Choe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Jan%20Kautz%20and%20Minsu%20Cho%20and%20Chris%20Choy&entry.1292438233=%20%20We%20tackle%20open-vocabulary%203D%20scene%20understanding%20by%20introducing%20a%20novel%20data%0Ageneration%20pipeline%20and%20training%20framework.%20Our%20method%20addresses%20three%20critical%0Arequirements%20for%20effective%20training%3A%20precise%203D%20region%20segmentation%2C%0Acomprehensive%20textual%20descriptions%2C%20and%20sufficient%20dataset%20scale.%20By%20leveraging%0Astate-of-the-art%20open-vocabulary%20image%20segmentation%20models%20and%20region-aware%0AVision-Language%20Models%2C%20we%20develop%20an%20automatic%20pipeline%20that%20generates%0Ahigh-quality%203D%20mask-text%20pairs.%20Applying%20this%20pipeline%20to%20multiple%203D%20scene%0Adatasets%2C%20we%20create%20Mosaic3D-5.6M%2C%20a%20dataset%20of%20over%2030K%20annotated%20scenes%20with%0A5.6M%20mask-text%20pairs%2C%20significantly%20larger%20than%20existing%20datasets.%20Building%0Aupon%20this%20data%2C%20we%20propose%20Mosaic3D%2C%20a%20foundation%20model%20combining%20a%203D%20encoder%0Atrained%20with%20contrastive%20learning%20and%20a%20lightweight%20mask%20decoder%20for%0Aopen-vocabulary%203D%20semantic%20and%20instance%20segmentation.%20Our%20approach%20achieves%0Astate-of-the-art%20results%20on%20open-vocabulary%203D%20semantic%20and%20instance%0Asegmentation%20tasks%20including%20ScanNet200%2C%20Matterport3D%2C%20and%20ScanNet%2B%2B%2C%20with%0Aablation%20studies%20validating%20the%20effectiveness%20of%20our%20large-scale%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02548v1&entry.124074799=Read"},
{"title": "High-Fidelity Human Avatars from Laptop Webcams using Edge Compute", "author": "Akash Haridas and Imran N. Junejo", "abstract": "  Applications of generating photo-realistic human avatars are many, however,\nhigh-fidelity avatar generation traditionally required expensive professional\ncamera rigs and artistic labor, but recent research has enabled constructing\nthem automatically from smartphones with RGB and IR sensors. However, these new\nmethods still rely on the presence of high-resolution cameras on modern\nsmartphones and often require offloading the processing to powerful servers\nwith GPUs. Modern applications such as video conferencing call for the ability\nto generate these avatars from consumer-grade laptop webcams using limited\ncompute available on-device. In this work, we develop a novel method based on\n3D morphable models, landmark detection, photo-realistic texture GANs, and\ndifferentiable rendering to tackle the problem of low webcam image quality and\nedge computation. We build an automatic system to generate high-fidelity\nanimatable avatars under these limitations, leveraging the neural compute\ncapabilities of mobile chips.\n", "link": "http://arxiv.org/abs/2502.02468v1", "date": "2025-02-04", "relevancy": 2.971, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5956}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5956}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20Human%20Avatars%20from%20Laptop%20Webcams%20using%20Edge%20Compute&body=Title%3A%20High-Fidelity%20Human%20Avatars%20from%20Laptop%20Webcams%20using%20Edge%20Compute%0AAuthor%3A%20Akash%20Haridas%20and%20Imran%20N.%20Junejo%0AAbstract%3A%20%20%20Applications%20of%20generating%20photo-realistic%20human%20avatars%20are%20many%2C%20however%2C%0Ahigh-fidelity%20avatar%20generation%20traditionally%20required%20expensive%20professional%0Acamera%20rigs%20and%20artistic%20labor%2C%20but%20recent%20research%20has%20enabled%20constructing%0Athem%20automatically%20from%20smartphones%20with%20RGB%20and%20IR%20sensors.%20However%2C%20these%20new%0Amethods%20still%20rely%20on%20the%20presence%20of%20high-resolution%20cameras%20on%20modern%0Asmartphones%20and%20often%20require%20offloading%20the%20processing%20to%20powerful%20servers%0Awith%20GPUs.%20Modern%20applications%20such%20as%20video%20conferencing%20call%20for%20the%20ability%0Ato%20generate%20these%20avatars%20from%20consumer-grade%20laptop%20webcams%20using%20limited%0Acompute%20available%20on-device.%20In%20this%20work%2C%20we%20develop%20a%20novel%20method%20based%20on%0A3D%20morphable%20models%2C%20landmark%20detection%2C%20photo-realistic%20texture%20GANs%2C%20and%0Adifferentiable%20rendering%20to%20tackle%20the%20problem%20of%20low%20webcam%20image%20quality%20and%0Aedge%20computation.%20We%20build%20an%20automatic%20system%20to%20generate%20high-fidelity%0Aanimatable%20avatars%20under%20these%20limitations%2C%20leveraging%20the%20neural%20compute%0Acapabilities%20of%20mobile%20chips.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520Human%2520Avatars%2520from%2520Laptop%2520Webcams%2520using%2520Edge%2520Compute%26entry.906535625%3DAkash%2520Haridas%2520and%2520Imran%2520N.%2520Junejo%26entry.1292438233%3D%2520%2520Applications%2520of%2520generating%2520photo-realistic%2520human%2520avatars%2520are%2520many%252C%2520however%252C%250Ahigh-fidelity%2520avatar%2520generation%2520traditionally%2520required%2520expensive%2520professional%250Acamera%2520rigs%2520and%2520artistic%2520labor%252C%2520but%2520recent%2520research%2520has%2520enabled%2520constructing%250Athem%2520automatically%2520from%2520smartphones%2520with%2520RGB%2520and%2520IR%2520sensors.%2520However%252C%2520these%2520new%250Amethods%2520still%2520rely%2520on%2520the%2520presence%2520of%2520high-resolution%2520cameras%2520on%2520modern%250Asmartphones%2520and%2520often%2520require%2520offloading%2520the%2520processing%2520to%2520powerful%2520servers%250Awith%2520GPUs.%2520Modern%2520applications%2520such%2520as%2520video%2520conferencing%2520call%2520for%2520the%2520ability%250Ato%2520generate%2520these%2520avatars%2520from%2520consumer-grade%2520laptop%2520webcams%2520using%2520limited%250Acompute%2520available%2520on-device.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520novel%2520method%2520based%2520on%250A3D%2520morphable%2520models%252C%2520landmark%2520detection%252C%2520photo-realistic%2520texture%2520GANs%252C%2520and%250Adifferentiable%2520rendering%2520to%2520tackle%2520the%2520problem%2520of%2520low%2520webcam%2520image%2520quality%2520and%250Aedge%2520computation.%2520We%2520build%2520an%2520automatic%2520system%2520to%2520generate%2520high-fidelity%250Aanimatable%2520avatars%2520under%2520these%2520limitations%252C%2520leveraging%2520the%2520neural%2520compute%250Acapabilities%2520of%2520mobile%2520chips.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20Human%20Avatars%20from%20Laptop%20Webcams%20using%20Edge%20Compute&entry.906535625=Akash%20Haridas%20and%20Imran%20N.%20Junejo&entry.1292438233=%20%20Applications%20of%20generating%20photo-realistic%20human%20avatars%20are%20many%2C%20however%2C%0Ahigh-fidelity%20avatar%20generation%20traditionally%20required%20expensive%20professional%0Acamera%20rigs%20and%20artistic%20labor%2C%20but%20recent%20research%20has%20enabled%20constructing%0Athem%20automatically%20from%20smartphones%20with%20RGB%20and%20IR%20sensors.%20However%2C%20these%20new%0Amethods%20still%20rely%20on%20the%20presence%20of%20high-resolution%20cameras%20on%20modern%0Asmartphones%20and%20often%20require%20offloading%20the%20processing%20to%20powerful%20servers%0Awith%20GPUs.%20Modern%20applications%20such%20as%20video%20conferencing%20call%20for%20the%20ability%0Ato%20generate%20these%20avatars%20from%20consumer-grade%20laptop%20webcams%20using%20limited%0Acompute%20available%20on-device.%20In%20this%20work%2C%20we%20develop%20a%20novel%20method%20based%20on%0A3D%20morphable%20models%2C%20landmark%20detection%2C%20photo-realistic%20texture%20GANs%2C%20and%0Adifferentiable%20rendering%20to%20tackle%20the%20problem%20of%20low%20webcam%20image%20quality%20and%0Aedge%20computation.%20We%20build%20an%20automatic%20system%20to%20generate%20high-fidelity%0Aanimatable%20avatars%20under%20these%20limitations%2C%20leveraging%20the%20neural%20compute%0Acapabilities%20of%20mobile%20chips.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02468v1&entry.124074799=Read"},
{"title": "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and\n  Histopathology Foundation Models for Cell Segmentation and Classification", "author": "Valentina Vadori and Antonella Peruffo and Jean-Marie Gra\u00efc and Livio Finos and Enrico Grisan", "abstract": "  Recent advancements in foundation models have transformed computer vision,\ndriving significant performance improvements across diverse domains, including\ndigital histopathology. However, the advantages of domain-specific\nhistopathology foundation models over general-purpose models for specialized\ntasks such as cell analysis remain underexplored. This study investigates the\nrepresentation learning gap between these two categories by analyzing\nmulti-level patch embeddings applied to cell instance segmentation and\nclassification. We implement an encoder-decoder architecture with a consistent\ndecoder and various encoders. These include convolutional, vision transformer\n(ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M,\nrepresenting general-purpose foundation models. These are compared against ViT\nencoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation\nmodels, trained on patches extracted from hundreds of thousands of\nhistopathology whole-slide images. The decoder integrates patch embeddings from\ndifferent encoder depths via skip connections to generate semantic and distance\nmaps. These maps are then post-processed to create instance segmentation masks\nwhere each label corresponds to an individual cell and to perform cell-type\nclassification. All encoders remain frozen during training to assess their\npre-trained feature extraction capabilities. Using the PanNuke and CoNIC\nhistopathology datasets, and the newly introduced Nissl-stained CytoDArk0\ndataset for brain cytoarchitecture studies, we evaluate instance-level\ndetection, segmentation accuracy, and cell-type classification. This study\nprovides insights into the comparative strengths and limitations of\ngeneral-purpose vs. histopathology foundation models, offering guidance for\nmodel selection in cell-focused histopathology and brain cytoarchitecture\nanalysis workflows.\n", "link": "http://arxiv.org/abs/2502.02471v1", "date": "2025-02-04", "relevancy": 2.9436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Evaluating%20Patch%20Embeddings%20from%20General-Purpose%20and%0A%20%20Histopathology%20Foundation%20Models%20for%20Cell%20Segmentation%20and%20Classification&body=Title%3A%20Mind%20the%20Gap%3A%20Evaluating%20Patch%20Embeddings%20from%20General-Purpose%20and%0A%20%20Histopathology%20Foundation%20Models%20for%20Cell%20Segmentation%20and%20Classification%0AAuthor%3A%20Valentina%20Vadori%20and%20Antonella%20Peruffo%20and%20Jean-Marie%20Gra%C3%AFc%20and%20Livio%20Finos%20and%20Enrico%20Grisan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%20have%20transformed%20computer%20vision%2C%0Adriving%20significant%20performance%20improvements%20across%20diverse%20domains%2C%20including%0Adigital%20histopathology.%20However%2C%20the%20advantages%20of%20domain-specific%0Ahistopathology%20foundation%20models%20over%20general-purpose%20models%20for%20specialized%0Atasks%20such%20as%20cell%20analysis%20remain%20underexplored.%20This%20study%20investigates%20the%0Arepresentation%20learning%20gap%20between%20these%20two%20categories%20by%20analyzing%0Amulti-level%20patch%20embeddings%20applied%20to%20cell%20instance%20segmentation%20and%0Aclassification.%20We%20implement%20an%20encoder-decoder%20architecture%20with%20a%20consistent%0Adecoder%20and%20various%20encoders.%20These%20include%20convolutional%2C%20vision%20transformer%0A%28ViT%29%2C%20and%20hybrid%20encoders%20pre-trained%20on%20ImageNet-22K%20or%20LVD-142M%2C%0Arepresenting%20general-purpose%20foundation%20models.%20These%20are%20compared%20against%20ViT%0Aencoders%20from%20the%20recently%20released%20UNI%2C%20Virchow2%2C%20and%20Prov-GigaPath%20foundation%0Amodels%2C%20trained%20on%20patches%20extracted%20from%20hundreds%20of%20thousands%20of%0Ahistopathology%20whole-slide%20images.%20The%20decoder%20integrates%20patch%20embeddings%20from%0Adifferent%20encoder%20depths%20via%20skip%20connections%20to%20generate%20semantic%20and%20distance%0Amaps.%20These%20maps%20are%20then%20post-processed%20to%20create%20instance%20segmentation%20masks%0Awhere%20each%20label%20corresponds%20to%20an%20individual%20cell%20and%20to%20perform%20cell-type%0Aclassification.%20All%20encoders%20remain%20frozen%20during%20training%20to%20assess%20their%0Apre-trained%20feature%20extraction%20capabilities.%20Using%20the%20PanNuke%20and%20CoNIC%0Ahistopathology%20datasets%2C%20and%20the%20newly%20introduced%20Nissl-stained%20CytoDArk0%0Adataset%20for%20brain%20cytoarchitecture%20studies%2C%20we%20evaluate%20instance-level%0Adetection%2C%20segmentation%20accuracy%2C%20and%20cell-type%20classification.%20This%20study%0Aprovides%20insights%20into%20the%20comparative%20strengths%20and%20limitations%20of%0Ageneral-purpose%20vs.%20histopathology%20foundation%20models%2C%20offering%20guidance%20for%0Amodel%20selection%20in%20cell-focused%20histopathology%20and%20brain%20cytoarchitecture%0Aanalysis%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Evaluating%2520Patch%2520Embeddings%2520from%2520General-Purpose%2520and%250A%2520%2520Histopathology%2520Foundation%2520Models%2520for%2520Cell%2520Segmentation%2520and%2520Classification%26entry.906535625%3DValentina%2520Vadori%2520and%2520Antonella%2520Peruffo%2520and%2520Jean-Marie%2520Gra%25C3%25AFc%2520and%2520Livio%2520Finos%2520and%2520Enrico%2520Grisan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%2520have%2520transformed%2520computer%2520vision%252C%250Adriving%2520significant%2520performance%2520improvements%2520across%2520diverse%2520domains%252C%2520including%250Adigital%2520histopathology.%2520However%252C%2520the%2520advantages%2520of%2520domain-specific%250Ahistopathology%2520foundation%2520models%2520over%2520general-purpose%2520models%2520for%2520specialized%250Atasks%2520such%2520as%2520cell%2520analysis%2520remain%2520underexplored.%2520This%2520study%2520investigates%2520the%250Arepresentation%2520learning%2520gap%2520between%2520these%2520two%2520categories%2520by%2520analyzing%250Amulti-level%2520patch%2520embeddings%2520applied%2520to%2520cell%2520instance%2520segmentation%2520and%250Aclassification.%2520We%2520implement%2520an%2520encoder-decoder%2520architecture%2520with%2520a%2520consistent%250Adecoder%2520and%2520various%2520encoders.%2520These%2520include%2520convolutional%252C%2520vision%2520transformer%250A%2528ViT%2529%252C%2520and%2520hybrid%2520encoders%2520pre-trained%2520on%2520ImageNet-22K%2520or%2520LVD-142M%252C%250Arepresenting%2520general-purpose%2520foundation%2520models.%2520These%2520are%2520compared%2520against%2520ViT%250Aencoders%2520from%2520the%2520recently%2520released%2520UNI%252C%2520Virchow2%252C%2520and%2520Prov-GigaPath%2520foundation%250Amodels%252C%2520trained%2520on%2520patches%2520extracted%2520from%2520hundreds%2520of%2520thousands%2520of%250Ahistopathology%2520whole-slide%2520images.%2520The%2520decoder%2520integrates%2520patch%2520embeddings%2520from%250Adifferent%2520encoder%2520depths%2520via%2520skip%2520connections%2520to%2520generate%2520semantic%2520and%2520distance%250Amaps.%2520These%2520maps%2520are%2520then%2520post-processed%2520to%2520create%2520instance%2520segmentation%2520masks%250Awhere%2520each%2520label%2520corresponds%2520to%2520an%2520individual%2520cell%2520and%2520to%2520perform%2520cell-type%250Aclassification.%2520All%2520encoders%2520remain%2520frozen%2520during%2520training%2520to%2520assess%2520their%250Apre-trained%2520feature%2520extraction%2520capabilities.%2520Using%2520the%2520PanNuke%2520and%2520CoNIC%250Ahistopathology%2520datasets%252C%2520and%2520the%2520newly%2520introduced%2520Nissl-stained%2520CytoDArk0%250Adataset%2520for%2520brain%2520cytoarchitecture%2520studies%252C%2520we%2520evaluate%2520instance-level%250Adetection%252C%2520segmentation%2520accuracy%252C%2520and%2520cell-type%2520classification.%2520This%2520study%250Aprovides%2520insights%2520into%2520the%2520comparative%2520strengths%2520and%2520limitations%2520of%250Ageneral-purpose%2520vs.%2520histopathology%2520foundation%2520models%252C%2520offering%2520guidance%2520for%250Amodel%2520selection%2520in%2520cell-focused%2520histopathology%2520and%2520brain%2520cytoarchitecture%250Aanalysis%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Evaluating%20Patch%20Embeddings%20from%20General-Purpose%20and%0A%20%20Histopathology%20Foundation%20Models%20for%20Cell%20Segmentation%20and%20Classification&entry.906535625=Valentina%20Vadori%20and%20Antonella%20Peruffo%20and%20Jean-Marie%20Gra%C3%AFc%20and%20Livio%20Finos%20and%20Enrico%20Grisan&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%20have%20transformed%20computer%20vision%2C%0Adriving%20significant%20performance%20improvements%20across%20diverse%20domains%2C%20including%0Adigital%20histopathology.%20However%2C%20the%20advantages%20of%20domain-specific%0Ahistopathology%20foundation%20models%20over%20general-purpose%20models%20for%20specialized%0Atasks%20such%20as%20cell%20analysis%20remain%20underexplored.%20This%20study%20investigates%20the%0Arepresentation%20learning%20gap%20between%20these%20two%20categories%20by%20analyzing%0Amulti-level%20patch%20embeddings%20applied%20to%20cell%20instance%20segmentation%20and%0Aclassification.%20We%20implement%20an%20encoder-decoder%20architecture%20with%20a%20consistent%0Adecoder%20and%20various%20encoders.%20These%20include%20convolutional%2C%20vision%20transformer%0A%28ViT%29%2C%20and%20hybrid%20encoders%20pre-trained%20on%20ImageNet-22K%20or%20LVD-142M%2C%0Arepresenting%20general-purpose%20foundation%20models.%20These%20are%20compared%20against%20ViT%0Aencoders%20from%20the%20recently%20released%20UNI%2C%20Virchow2%2C%20and%20Prov-GigaPath%20foundation%0Amodels%2C%20trained%20on%20patches%20extracted%20from%20hundreds%20of%20thousands%20of%0Ahistopathology%20whole-slide%20images.%20The%20decoder%20integrates%20patch%20embeddings%20from%0Adifferent%20encoder%20depths%20via%20skip%20connections%20to%20generate%20semantic%20and%20distance%0Amaps.%20These%20maps%20are%20then%20post-processed%20to%20create%20instance%20segmentation%20masks%0Awhere%20each%20label%20corresponds%20to%20an%20individual%20cell%20and%20to%20perform%20cell-type%0Aclassification.%20All%20encoders%20remain%20frozen%20during%20training%20to%20assess%20their%0Apre-trained%20feature%20extraction%20capabilities.%20Using%20the%20PanNuke%20and%20CoNIC%0Ahistopathology%20datasets%2C%20and%20the%20newly%20introduced%20Nissl-stained%20CytoDArk0%0Adataset%20for%20brain%20cytoarchitecture%20studies%2C%20we%20evaluate%20instance-level%0Adetection%2C%20segmentation%20accuracy%2C%20and%20cell-type%20classification.%20This%20study%0Aprovides%20insights%20into%20the%20comparative%20strengths%20and%20limitations%20of%0Ageneral-purpose%20vs.%20histopathology%20foundation%20models%2C%20offering%20guidance%20for%0Amodel%20selection%20in%20cell-focused%20histopathology%20and%20brain%20cytoarchitecture%0Aanalysis%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02471v1&entry.124074799=Read"},
{"title": "Improving Generalization Ability for 3D Object Detection by Learning\n  Sparsity-invariant Features", "author": "Hsin-Cheng Lu and Chung-Yi Lin and Winston H. Hsu", "abstract": "  In autonomous driving, 3D object detection is essential for accurately\nidentifying and tracking objects. Despite the continuous development of various\ntechnologies for this task, a significant drawback is observed in most of\nthem-they experience substantial performance degradation when detecting objects\nin unseen domains. In this paper, we propose a method to improve the\ngeneralization ability for 3D object detection on a single domain. We primarily\nfocus on generalizing from a single source domain to target domains with\ndistinct sensor configurations and scene distributions. To learn\nsparsity-invariant features from a single source domain, we selectively\nsubsample the source data to a specific beam, using confidence scores\ndetermined by the current detector to identify the density that holds utmost\nimportance for the detector. Subsequently, we employ the teacher-student\nframework to align the Bird's Eye View (BEV) features for different point\nclouds densities. We also utilize feature content alignment (FCA) and\ngraph-based embedding relationship alignment (GERA) to instruct the detector to\nbe domain-agnostic. Extensive experiments demonstrate that our method exhibits\nsuperior generalization capabilities compared to other baselines. Furthermore,\nour approach even outperforms certain domain adaptation methods that can access\nto the target domain data.\n", "link": "http://arxiv.org/abs/2502.02322v1", "date": "2025-02-04", "relevancy": 2.9159, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6229}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5655}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generalization%20Ability%20for%203D%20Object%20Detection%20by%20Learning%0A%20%20Sparsity-invariant%20Features&body=Title%3A%20Improving%20Generalization%20Ability%20for%203D%20Object%20Detection%20by%20Learning%0A%20%20Sparsity-invariant%20Features%0AAuthor%3A%20Hsin-Cheng%20Lu%20and%20Chung-Yi%20Lin%20and%20Winston%20H.%20Hsu%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%203D%20object%20detection%20is%20essential%20for%20accurately%0Aidentifying%20and%20tracking%20objects.%20Despite%20the%20continuous%20development%20of%20various%0Atechnologies%20for%20this%20task%2C%20a%20significant%20drawback%20is%20observed%20in%20most%20of%0Athem-they%20experience%20substantial%20performance%20degradation%20when%20detecting%20objects%0Ain%20unseen%20domains.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20improve%20the%0Ageneralization%20ability%20for%203D%20object%20detection%20on%20a%20single%20domain.%20We%20primarily%0Afocus%20on%20generalizing%20from%20a%20single%20source%20domain%20to%20target%20domains%20with%0Adistinct%20sensor%20configurations%20and%20scene%20distributions.%20To%20learn%0Asparsity-invariant%20features%20from%20a%20single%20source%20domain%2C%20we%20selectively%0Asubsample%20the%20source%20data%20to%20a%20specific%20beam%2C%20using%20confidence%20scores%0Adetermined%20by%20the%20current%20detector%20to%20identify%20the%20density%20that%20holds%20utmost%0Aimportance%20for%20the%20detector.%20Subsequently%2C%20we%20employ%20the%20teacher-student%0Aframework%20to%20align%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20for%20different%20point%0Aclouds%20densities.%20We%20also%20utilize%20feature%20content%20alignment%20%28FCA%29%20and%0Agraph-based%20embedding%20relationship%20alignment%20%28GERA%29%20to%20instruct%20the%20detector%20to%0Abe%20domain-agnostic.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20exhibits%0Asuperior%20generalization%20capabilities%20compared%20to%20other%20baselines.%20Furthermore%2C%0Aour%20approach%20even%20outperforms%20certain%20domain%20adaptation%20methods%20that%20can%20access%0Ato%20the%20target%20domain%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generalization%2520Ability%2520for%25203D%2520Object%2520Detection%2520by%2520Learning%250A%2520%2520Sparsity-invariant%2520Features%26entry.906535625%3DHsin-Cheng%2520Lu%2520and%2520Chung-Yi%2520Lin%2520and%2520Winston%2520H.%2520Hsu%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%25203D%2520object%2520detection%2520is%2520essential%2520for%2520accurately%250Aidentifying%2520and%2520tracking%2520objects.%2520Despite%2520the%2520continuous%2520development%2520of%2520various%250Atechnologies%2520for%2520this%2520task%252C%2520a%2520significant%2520drawback%2520is%2520observed%2520in%2520most%2520of%250Athem-they%2520experience%2520substantial%2520performance%2520degradation%2520when%2520detecting%2520objects%250Ain%2520unseen%2520domains.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520to%2520improve%2520the%250Ageneralization%2520ability%2520for%25203D%2520object%2520detection%2520on%2520a%2520single%2520domain.%2520We%2520primarily%250Afocus%2520on%2520generalizing%2520from%2520a%2520single%2520source%2520domain%2520to%2520target%2520domains%2520with%250Adistinct%2520sensor%2520configurations%2520and%2520scene%2520distributions.%2520To%2520learn%250Asparsity-invariant%2520features%2520from%2520a%2520single%2520source%2520domain%252C%2520we%2520selectively%250Asubsample%2520the%2520source%2520data%2520to%2520a%2520specific%2520beam%252C%2520using%2520confidence%2520scores%250Adetermined%2520by%2520the%2520current%2520detector%2520to%2520identify%2520the%2520density%2520that%2520holds%2520utmost%250Aimportance%2520for%2520the%2520detector.%2520Subsequently%252C%2520we%2520employ%2520the%2520teacher-student%250Aframework%2520to%2520align%2520the%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520features%2520for%2520different%2520point%250Aclouds%2520densities.%2520We%2520also%2520utilize%2520feature%2520content%2520alignment%2520%2528FCA%2529%2520and%250Agraph-based%2520embedding%2520relationship%2520alignment%2520%2528GERA%2529%2520to%2520instruct%2520the%2520detector%2520to%250Abe%2520domain-agnostic.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520exhibits%250Asuperior%2520generalization%2520capabilities%2520compared%2520to%2520other%2520baselines.%2520Furthermore%252C%250Aour%2520approach%2520even%2520outperforms%2520certain%2520domain%2520adaptation%2520methods%2520that%2520can%2520access%250Ato%2520the%2520target%2520domain%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generalization%20Ability%20for%203D%20Object%20Detection%20by%20Learning%0A%20%20Sparsity-invariant%20Features&entry.906535625=Hsin-Cheng%20Lu%20and%20Chung-Yi%20Lin%20and%20Winston%20H.%20Hsu&entry.1292438233=%20%20In%20autonomous%20driving%2C%203D%20object%20detection%20is%20essential%20for%20accurately%0Aidentifying%20and%20tracking%20objects.%20Despite%20the%20continuous%20development%20of%20various%0Atechnologies%20for%20this%20task%2C%20a%20significant%20drawback%20is%20observed%20in%20most%20of%0Athem-they%20experience%20substantial%20performance%20degradation%20when%20detecting%20objects%0Ain%20unseen%20domains.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20improve%20the%0Ageneralization%20ability%20for%203D%20object%20detection%20on%20a%20single%20domain.%20We%20primarily%0Afocus%20on%20generalizing%20from%20a%20single%20source%20domain%20to%20target%20domains%20with%0Adistinct%20sensor%20configurations%20and%20scene%20distributions.%20To%20learn%0Asparsity-invariant%20features%20from%20a%20single%20source%20domain%2C%20we%20selectively%0Asubsample%20the%20source%20data%20to%20a%20specific%20beam%2C%20using%20confidence%20scores%0Adetermined%20by%20the%20current%20detector%20to%20identify%20the%20density%20that%20holds%20utmost%0Aimportance%20for%20the%20detector.%20Subsequently%2C%20we%20employ%20the%20teacher-student%0Aframework%20to%20align%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20for%20different%20point%0Aclouds%20densities.%20We%20also%20utilize%20feature%20content%20alignment%20%28FCA%29%20and%0Agraph-based%20embedding%20relationship%20alignment%20%28GERA%29%20to%20instruct%20the%20detector%20to%0Abe%20domain-agnostic.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20exhibits%0Asuperior%20generalization%20capabilities%20compared%20to%20other%20baselines.%20Furthermore%2C%0Aour%20approach%20even%20outperforms%20certain%20domain%20adaptation%20methods%20that%20can%20access%0Ato%20the%20target%20domain%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02322v1&entry.124074799=Read"},
{"title": "Rotation-Adaptive Point Cloud Domain Generalization via Intricate\n  Orientation Learning", "author": "Bangzhen Liu and Chenxi Zheng and Xuemiao Xu and Cheng Xu and Huaidong Zhang and Shengfeng He", "abstract": "  The vulnerability of 3D point cloud analysis to unpredictable rotations poses\nan open yet challenging problem: orientation-aware 3D domain generalization.\nCross-domain robustness and adaptability of 3D representations are crucial but\nnot easily achieved through rotation augmentation. Motivated by the inherent\nadvantages of intricate orientations in enhancing generalizability, we propose\nan innovative rotation-adaptive domain generalization framework for 3D point\ncloud analysis. Our approach aims to alleviate orientational shifts by\nleveraging intricate samples in an iterative learning process. Specifically, we\nidentify the most challenging rotation for each point cloud and construct an\nintricate orientation set by optimizing intricate orientations. Subsequently,\nwe employ an orientation-aware contrastive learning framework that incorporates\nan orientation consistency loss and a margin separation loss, enabling\neffective learning of categorically discriminative and generalizable features\nwith rotation consistency. Extensive experiments and ablations conducted on 3D\ncross-domain benchmarks firmly establish the state-of-the-art performance of\nour proposed approach in the context of orientation-aware 3D domain\ngeneralization.\n", "link": "http://arxiv.org/abs/2502.02247v1", "date": "2025-02-04", "relevancy": 2.9059, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5865}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotation-Adaptive%20Point%20Cloud%20Domain%20Generalization%20via%20Intricate%0A%20%20Orientation%20Learning&body=Title%3A%20Rotation-Adaptive%20Point%20Cloud%20Domain%20Generalization%20via%20Intricate%0A%20%20Orientation%20Learning%0AAuthor%3A%20Bangzhen%20Liu%20and%20Chenxi%20Zheng%20and%20Xuemiao%20Xu%20and%20Cheng%20Xu%20and%20Huaidong%20Zhang%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20The%20vulnerability%20of%203D%20point%20cloud%20analysis%20to%20unpredictable%20rotations%20poses%0Aan%20open%20yet%20challenging%20problem%3A%20orientation-aware%203D%20domain%20generalization.%0ACross-domain%20robustness%20and%20adaptability%20of%203D%20representations%20are%20crucial%20but%0Anot%20easily%20achieved%20through%20rotation%20augmentation.%20Motivated%20by%20the%20inherent%0Aadvantages%20of%20intricate%20orientations%20in%20enhancing%20generalizability%2C%20we%20propose%0Aan%20innovative%20rotation-adaptive%20domain%20generalization%20framework%20for%203D%20point%0Acloud%20analysis.%20Our%20approach%20aims%20to%20alleviate%20orientational%20shifts%20by%0Aleveraging%20intricate%20samples%20in%20an%20iterative%20learning%20process.%20Specifically%2C%20we%0Aidentify%20the%20most%20challenging%20rotation%20for%20each%20point%20cloud%20and%20construct%20an%0Aintricate%20orientation%20set%20by%20optimizing%20intricate%20orientations.%20Subsequently%2C%0Awe%20employ%20an%20orientation-aware%20contrastive%20learning%20framework%20that%20incorporates%0Aan%20orientation%20consistency%20loss%20and%20a%20margin%20separation%20loss%2C%20enabling%0Aeffective%20learning%20of%20categorically%20discriminative%20and%20generalizable%20features%0Awith%20rotation%20consistency.%20Extensive%20experiments%20and%20ablations%20conducted%20on%203D%0Across-domain%20benchmarks%20firmly%20establish%20the%20state-of-the-art%20performance%20of%0Aour%20proposed%20approach%20in%20the%20context%20of%20orientation-aware%203D%20domain%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotation-Adaptive%2520Point%2520Cloud%2520Domain%2520Generalization%2520via%2520Intricate%250A%2520%2520Orientation%2520Learning%26entry.906535625%3DBangzhen%2520Liu%2520and%2520Chenxi%2520Zheng%2520and%2520Xuemiao%2520Xu%2520and%2520Cheng%2520Xu%2520and%2520Huaidong%2520Zhang%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520The%2520vulnerability%2520of%25203D%2520point%2520cloud%2520analysis%2520to%2520unpredictable%2520rotations%2520poses%250Aan%2520open%2520yet%2520challenging%2520problem%253A%2520orientation-aware%25203D%2520domain%2520generalization.%250ACross-domain%2520robustness%2520and%2520adaptability%2520of%25203D%2520representations%2520are%2520crucial%2520but%250Anot%2520easily%2520achieved%2520through%2520rotation%2520augmentation.%2520Motivated%2520by%2520the%2520inherent%250Aadvantages%2520of%2520intricate%2520orientations%2520in%2520enhancing%2520generalizability%252C%2520we%2520propose%250Aan%2520innovative%2520rotation-adaptive%2520domain%2520generalization%2520framework%2520for%25203D%2520point%250Acloud%2520analysis.%2520Our%2520approach%2520aims%2520to%2520alleviate%2520orientational%2520shifts%2520by%250Aleveraging%2520intricate%2520samples%2520in%2520an%2520iterative%2520learning%2520process.%2520Specifically%252C%2520we%250Aidentify%2520the%2520most%2520challenging%2520rotation%2520for%2520each%2520point%2520cloud%2520and%2520construct%2520an%250Aintricate%2520orientation%2520set%2520by%2520optimizing%2520intricate%2520orientations.%2520Subsequently%252C%250Awe%2520employ%2520an%2520orientation-aware%2520contrastive%2520learning%2520framework%2520that%2520incorporates%250Aan%2520orientation%2520consistency%2520loss%2520and%2520a%2520margin%2520separation%2520loss%252C%2520enabling%250Aeffective%2520learning%2520of%2520categorically%2520discriminative%2520and%2520generalizable%2520features%250Awith%2520rotation%2520consistency.%2520Extensive%2520experiments%2520and%2520ablations%2520conducted%2520on%25203D%250Across-domain%2520benchmarks%2520firmly%2520establish%2520the%2520state-of-the-art%2520performance%2520of%250Aour%2520proposed%2520approach%2520in%2520the%2520context%2520of%2520orientation-aware%25203D%2520domain%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotation-Adaptive%20Point%20Cloud%20Domain%20Generalization%20via%20Intricate%0A%20%20Orientation%20Learning&entry.906535625=Bangzhen%20Liu%20and%20Chenxi%20Zheng%20and%20Xuemiao%20Xu%20and%20Cheng%20Xu%20and%20Huaidong%20Zhang%20and%20Shengfeng%20He&entry.1292438233=%20%20The%20vulnerability%20of%203D%20point%20cloud%20analysis%20to%20unpredictable%20rotations%20poses%0Aan%20open%20yet%20challenging%20problem%3A%20orientation-aware%203D%20domain%20generalization.%0ACross-domain%20robustness%20and%20adaptability%20of%203D%20representations%20are%20crucial%20but%0Anot%20easily%20achieved%20through%20rotation%20augmentation.%20Motivated%20by%20the%20inherent%0Aadvantages%20of%20intricate%20orientations%20in%20enhancing%20generalizability%2C%20we%20propose%0Aan%20innovative%20rotation-adaptive%20domain%20generalization%20framework%20for%203D%20point%0Acloud%20analysis.%20Our%20approach%20aims%20to%20alleviate%20orientational%20shifts%20by%0Aleveraging%20intricate%20samples%20in%20an%20iterative%20learning%20process.%20Specifically%2C%20we%0Aidentify%20the%20most%20challenging%20rotation%20for%20each%20point%20cloud%20and%20construct%20an%0Aintricate%20orientation%20set%20by%20optimizing%20intricate%20orientations.%20Subsequently%2C%0Awe%20employ%20an%20orientation-aware%20contrastive%20learning%20framework%20that%20incorporates%0Aan%20orientation%20consistency%20loss%20and%20a%20margin%20separation%20loss%2C%20enabling%0Aeffective%20learning%20of%20categorically%20discriminative%20and%20generalizable%20features%0Awith%20rotation%20consistency.%20Extensive%20experiments%20and%20ablations%20conducted%20on%203D%0Across-domain%20benchmarks%20firmly%20establish%20the%20state-of-the-art%20performance%20of%0Aour%20proposed%20approach%20in%20the%20context%20of%20orientation-aware%203D%20domain%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02247v1&entry.124074799=Read"},
{"title": "MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by\n  Continual Learning", "author": "Shengbo Gu and Yu-Kun Qiu and Yu-Ming Tang and Ancong Wu and Wei-Shi Zheng", "abstract": "  The generation of a virtual digital avatar is a crucial research topic in the\nfield of computer vision. Many existing works utilize Neural Radiance Fields\n(NeRF) to address this issue and have achieved impressive results. However,\nprevious works assume the images of the training person are available and fixed\nwhile the appearances and poses of a subject could constantly change and\nincrease in real-world scenarios. How to update the human avatar but also\nmaintain the ability to render the old appearance of the person is a practical\nchallenge. One trivial solution is to combine the existing virtual avatar\nmodels based on NeRF with continual learning methods. However, there are some\ncritical issues in this approach: learning new appearances and poses can cause\nthe model to forget past information, which in turn leads to a degradation in\nthe rendering quality of past appearances, especially color bleeding issues,\nand incorrect human body poses. In this work, we propose a maintainable avatar\n(MaintaAvatar) based on neural radiance fields by continual learning, which\nresolves the issues by utilizing a Global-Local Joint Storage Module and a Pose\nDistillation Module. Overall, our model requires only limited data collection\nto quickly fine-tune the model while avoiding catastrophic forgetting, thus\nachieving a maintainable virtual avatar. The experimental results validate the\neffectiveness of our MaintaAvatar model.\n", "link": "http://arxiv.org/abs/2502.02372v1", "date": "2025-02-04", "relevancy": 2.854, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5777}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5777}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaintaAvatar%3A%20A%20Maintainable%20Avatar%20Based%20on%20Neural%20Radiance%20Fields%20by%0A%20%20Continual%20Learning&body=Title%3A%20MaintaAvatar%3A%20A%20Maintainable%20Avatar%20Based%20on%20Neural%20Radiance%20Fields%20by%0A%20%20Continual%20Learning%0AAuthor%3A%20Shengbo%20Gu%20and%20Yu-Kun%20Qiu%20and%20Yu-Ming%20Tang%20and%20Ancong%20Wu%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20The%20generation%20of%20a%20virtual%20digital%20avatar%20is%20a%20crucial%20research%20topic%20in%20the%0Afield%20of%20computer%20vision.%20Many%20existing%20works%20utilize%20Neural%20Radiance%20Fields%0A%28NeRF%29%20to%20address%20this%20issue%20and%20have%20achieved%20impressive%20results.%20However%2C%0Aprevious%20works%20assume%20the%20images%20of%20the%20training%20person%20are%20available%20and%20fixed%0Awhile%20the%20appearances%20and%20poses%20of%20a%20subject%20could%20constantly%20change%20and%0Aincrease%20in%20real-world%20scenarios.%20How%20to%20update%20the%20human%20avatar%20but%20also%0Amaintain%20the%20ability%20to%20render%20the%20old%20appearance%20of%20the%20person%20is%20a%20practical%0Achallenge.%20One%20trivial%20solution%20is%20to%20combine%20the%20existing%20virtual%20avatar%0Amodels%20based%20on%20NeRF%20with%20continual%20learning%20methods.%20However%2C%20there%20are%20some%0Acritical%20issues%20in%20this%20approach%3A%20learning%20new%20appearances%20and%20poses%20can%20cause%0Athe%20model%20to%20forget%20past%20information%2C%20which%20in%20turn%20leads%20to%20a%20degradation%20in%0Athe%20rendering%20quality%20of%20past%20appearances%2C%20especially%20color%20bleeding%20issues%2C%0Aand%20incorrect%20human%20body%20poses.%20In%20this%20work%2C%20we%20propose%20a%20maintainable%20avatar%0A%28MaintaAvatar%29%20based%20on%20neural%20radiance%20fields%20by%20continual%20learning%2C%20which%0Aresolves%20the%20issues%20by%20utilizing%20a%20Global-Local%20Joint%20Storage%20Module%20and%20a%20Pose%0ADistillation%20Module.%20Overall%2C%20our%20model%20requires%20only%20limited%20data%20collection%0Ato%20quickly%20fine-tune%20the%20model%20while%20avoiding%20catastrophic%20forgetting%2C%20thus%0Aachieving%20a%20maintainable%20virtual%20avatar.%20The%20experimental%20results%20validate%20the%0Aeffectiveness%20of%20our%20MaintaAvatar%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaintaAvatar%253A%2520A%2520Maintainable%2520Avatar%2520Based%2520on%2520Neural%2520Radiance%2520Fields%2520by%250A%2520%2520Continual%2520Learning%26entry.906535625%3DShengbo%2520Gu%2520and%2520Yu-Kun%2520Qiu%2520and%2520Yu-Ming%2520Tang%2520and%2520Ancong%2520Wu%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520a%2520virtual%2520digital%2520avatar%2520is%2520a%2520crucial%2520research%2520topic%2520in%2520the%250Afield%2520of%2520computer%2520vision.%2520Many%2520existing%2520works%2520utilize%2520Neural%2520Radiance%2520Fields%250A%2528NeRF%2529%2520to%2520address%2520this%2520issue%2520and%2520have%2520achieved%2520impressive%2520results.%2520However%252C%250Aprevious%2520works%2520assume%2520the%2520images%2520of%2520the%2520training%2520person%2520are%2520available%2520and%2520fixed%250Awhile%2520the%2520appearances%2520and%2520poses%2520of%2520a%2520subject%2520could%2520constantly%2520change%2520and%250Aincrease%2520in%2520real-world%2520scenarios.%2520How%2520to%2520update%2520the%2520human%2520avatar%2520but%2520also%250Amaintain%2520the%2520ability%2520to%2520render%2520the%2520old%2520appearance%2520of%2520the%2520person%2520is%2520a%2520practical%250Achallenge.%2520One%2520trivial%2520solution%2520is%2520to%2520combine%2520the%2520existing%2520virtual%2520avatar%250Amodels%2520based%2520on%2520NeRF%2520with%2520continual%2520learning%2520methods.%2520However%252C%2520there%2520are%2520some%250Acritical%2520issues%2520in%2520this%2520approach%253A%2520learning%2520new%2520appearances%2520and%2520poses%2520can%2520cause%250Athe%2520model%2520to%2520forget%2520past%2520information%252C%2520which%2520in%2520turn%2520leads%2520to%2520a%2520degradation%2520in%250Athe%2520rendering%2520quality%2520of%2520past%2520appearances%252C%2520especially%2520color%2520bleeding%2520issues%252C%250Aand%2520incorrect%2520human%2520body%2520poses.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520maintainable%2520avatar%250A%2528MaintaAvatar%2529%2520based%2520on%2520neural%2520radiance%2520fields%2520by%2520continual%2520learning%252C%2520which%250Aresolves%2520the%2520issues%2520by%2520utilizing%2520a%2520Global-Local%2520Joint%2520Storage%2520Module%2520and%2520a%2520Pose%250ADistillation%2520Module.%2520Overall%252C%2520our%2520model%2520requires%2520only%2520limited%2520data%2520collection%250Ato%2520quickly%2520fine-tune%2520the%2520model%2520while%2520avoiding%2520catastrophic%2520forgetting%252C%2520thus%250Aachieving%2520a%2520maintainable%2520virtual%2520avatar.%2520The%2520experimental%2520results%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520MaintaAvatar%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaintaAvatar%3A%20A%20Maintainable%20Avatar%20Based%20on%20Neural%20Radiance%20Fields%20by%0A%20%20Continual%20Learning&entry.906535625=Shengbo%20Gu%20and%20Yu-Kun%20Qiu%20and%20Yu-Ming%20Tang%20and%20Ancong%20Wu%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20The%20generation%20of%20a%20virtual%20digital%20avatar%20is%20a%20crucial%20research%20topic%20in%20the%0Afield%20of%20computer%20vision.%20Many%20existing%20works%20utilize%20Neural%20Radiance%20Fields%0A%28NeRF%29%20to%20address%20this%20issue%20and%20have%20achieved%20impressive%20results.%20However%2C%0Aprevious%20works%20assume%20the%20images%20of%20the%20training%20person%20are%20available%20and%20fixed%0Awhile%20the%20appearances%20and%20poses%20of%20a%20subject%20could%20constantly%20change%20and%0Aincrease%20in%20real-world%20scenarios.%20How%20to%20update%20the%20human%20avatar%20but%20also%0Amaintain%20the%20ability%20to%20render%20the%20old%20appearance%20of%20the%20person%20is%20a%20practical%0Achallenge.%20One%20trivial%20solution%20is%20to%20combine%20the%20existing%20virtual%20avatar%0Amodels%20based%20on%20NeRF%20with%20continual%20learning%20methods.%20However%2C%20there%20are%20some%0Acritical%20issues%20in%20this%20approach%3A%20learning%20new%20appearances%20and%20poses%20can%20cause%0Athe%20model%20to%20forget%20past%20information%2C%20which%20in%20turn%20leads%20to%20a%20degradation%20in%0Athe%20rendering%20quality%20of%20past%20appearances%2C%20especially%20color%20bleeding%20issues%2C%0Aand%20incorrect%20human%20body%20poses.%20In%20this%20work%2C%20we%20propose%20a%20maintainable%20avatar%0A%28MaintaAvatar%29%20based%20on%20neural%20radiance%20fields%20by%20continual%20learning%2C%20which%0Aresolves%20the%20issues%20by%20utilizing%20a%20Global-Local%20Joint%20Storage%20Module%20and%20a%20Pose%0ADistillation%20Module.%20Overall%2C%20our%20model%20requires%20only%20limited%20data%20collection%0Ato%20quickly%20fine-tune%20the%20model%20while%20avoiding%20catastrophic%20forgetting%2C%20thus%0Aachieving%20a%20maintainable%20virtual%20avatar.%20The%20experimental%20results%20validate%20the%0Aeffectiveness%20of%20our%20MaintaAvatar%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02372v1&entry.124074799=Read"},
{"title": "Unity in Diversity: Multi-expert Knowledge Confrontation and\n  Collaboration for Generalizable Vehicle Re-identification", "author": "Zhenyu Kuang and Hongyang Zhang and Mang Ye and Bin Yang and Yinhao Liu and Yue Huang and Xinghao Ding and Huafeng Li", "abstract": "  Generalizable vehicle re-identification (ReID) seeks to develop models that\ncan adapt to unknown target domains without the need for additional fine-tuning\nor retraining. Previous works have mainly focused on extracting\ndomain-invariant features by aligning data distributions between source\ndomains. However, interfered by the inherent domain-related redundancy in the\nsource images, solely relying on common features is insufficient for accurately\ncapturing the complementary features with lower occurrence probability and\nsmaller energy. To solve this unique problem, we propose a two-stage\nMulti-expert Knowledge Confrontation and Collaboration (MiKeCoCo) method, which\nfully leverages the high-level semantics of Contrastive Language-Image\nPretraining (CLIP) to obtain a diversified prompt set and achieve complementary\nfeature representations. Specifically, this paper first designs a\nSpectrum-based Transformation for Redundancy Elimination and Augmentation\nModule (STREAM) through simple image preprocessing to obtain two types of image\ninputs for the training process. Since STREAM eliminates domain-related\nredundancy in source images, it enables the model to pay closer attention to\nthe detailed prompt set that is crucial for distinguishing fine-grained\nvehicles. This learned prompt set related to the vehicle identity is then\nutilized to guide the comprehensive representation learning of complementary\nfeatures for final knowledge fusion and identity recognition. Inspired by the\nunity principle, MiKeCoCo integrates the diverse evaluation ways of experts to\nensure the accuracy and consistency of ReID. Extensive experimental results\ndemonstrate that our method achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2407.07351v2", "date": "2025-02-04", "relevancy": 2.8126, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.582}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unity%20in%20Diversity%3A%20Multi-expert%20Knowledge%20Confrontation%20and%0A%20%20Collaboration%20for%20Generalizable%20Vehicle%20Re-identification&body=Title%3A%20Unity%20in%20Diversity%3A%20Multi-expert%20Knowledge%20Confrontation%20and%0A%20%20Collaboration%20for%20Generalizable%20Vehicle%20Re-identification%0AAuthor%3A%20Zhenyu%20Kuang%20and%20Hongyang%20Zhang%20and%20Mang%20Ye%20and%20Bin%20Yang%20and%20Yinhao%20Liu%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Huafeng%20Li%0AAbstract%3A%20%20%20Generalizable%20vehicle%20re-identification%20%28ReID%29%20seeks%20to%20develop%20models%20that%0Acan%20adapt%20to%20unknown%20target%20domains%20without%20the%20need%20for%20additional%20fine-tuning%0Aor%20retraining.%20Previous%20works%20have%20mainly%20focused%20on%20extracting%0Adomain-invariant%20features%20by%20aligning%20data%20distributions%20between%20source%0Adomains.%20However%2C%20interfered%20by%20the%20inherent%20domain-related%20redundancy%20in%20the%0Asource%20images%2C%20solely%20relying%20on%20common%20features%20is%20insufficient%20for%20accurately%0Acapturing%20the%20complementary%20features%20with%20lower%20occurrence%20probability%20and%0Asmaller%20energy.%20To%20solve%20this%20unique%20problem%2C%20we%20propose%20a%20two-stage%0AMulti-expert%20Knowledge%20Confrontation%20and%20Collaboration%20%28MiKeCoCo%29%20method%2C%20which%0Afully%20leverages%20the%20high-level%20semantics%20of%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%20to%20obtain%20a%20diversified%20prompt%20set%20and%20achieve%20complementary%0Afeature%20representations.%20Specifically%2C%20this%20paper%20first%20designs%20a%0ASpectrum-based%20Transformation%20for%20Redundancy%20Elimination%20and%20Augmentation%0AModule%20%28STREAM%29%20through%20simple%20image%20preprocessing%20to%20obtain%20two%20types%20of%20image%0Ainputs%20for%20the%20training%20process.%20Since%20STREAM%20eliminates%20domain-related%0Aredundancy%20in%20source%20images%2C%20it%20enables%20the%20model%20to%20pay%20closer%20attention%20to%0Athe%20detailed%20prompt%20set%20that%20is%20crucial%20for%20distinguishing%20fine-grained%0Avehicles.%20This%20learned%20prompt%20set%20related%20to%20the%20vehicle%20identity%20is%20then%0Autilized%20to%20guide%20the%20comprehensive%20representation%20learning%20of%20complementary%0Afeatures%20for%20final%20knowledge%20fusion%20and%20identity%20recognition.%20Inspired%20by%20the%0Aunity%20principle%2C%20MiKeCoCo%20integrates%20the%20diverse%20evaluation%20ways%20of%20experts%20to%0Aensure%20the%20accuracy%20and%20consistency%20of%20ReID.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnity%2520in%2520Diversity%253A%2520Multi-expert%2520Knowledge%2520Confrontation%2520and%250A%2520%2520Collaboration%2520for%2520Generalizable%2520Vehicle%2520Re-identification%26entry.906535625%3DZhenyu%2520Kuang%2520and%2520Hongyang%2520Zhang%2520and%2520Mang%2520Ye%2520and%2520Bin%2520Yang%2520and%2520Yinhao%2520Liu%2520and%2520Yue%2520Huang%2520and%2520Xinghao%2520Ding%2520and%2520Huafeng%2520Li%26entry.1292438233%3D%2520%2520Generalizable%2520vehicle%2520re-identification%2520%2528ReID%2529%2520seeks%2520to%2520develop%2520models%2520that%250Acan%2520adapt%2520to%2520unknown%2520target%2520domains%2520without%2520the%2520need%2520for%2520additional%2520fine-tuning%250Aor%2520retraining.%2520Previous%2520works%2520have%2520mainly%2520focused%2520on%2520extracting%250Adomain-invariant%2520features%2520by%2520aligning%2520data%2520distributions%2520between%2520source%250Adomains.%2520However%252C%2520interfered%2520by%2520the%2520inherent%2520domain-related%2520redundancy%2520in%2520the%250Asource%2520images%252C%2520solely%2520relying%2520on%2520common%2520features%2520is%2520insufficient%2520for%2520accurately%250Acapturing%2520the%2520complementary%2520features%2520with%2520lower%2520occurrence%2520probability%2520and%250Asmaller%2520energy.%2520To%2520solve%2520this%2520unique%2520problem%252C%2520we%2520propose%2520a%2520two-stage%250AMulti-expert%2520Knowledge%2520Confrontation%2520and%2520Collaboration%2520%2528MiKeCoCo%2529%2520method%252C%2520which%250Afully%2520leverages%2520the%2520high-level%2520semantics%2520of%2520Contrastive%2520Language-Image%250APretraining%2520%2528CLIP%2529%2520to%2520obtain%2520a%2520diversified%2520prompt%2520set%2520and%2520achieve%2520complementary%250Afeature%2520representations.%2520Specifically%252C%2520this%2520paper%2520first%2520designs%2520a%250ASpectrum-based%2520Transformation%2520for%2520Redundancy%2520Elimination%2520and%2520Augmentation%250AModule%2520%2528STREAM%2529%2520through%2520simple%2520image%2520preprocessing%2520to%2520obtain%2520two%2520types%2520of%2520image%250Ainputs%2520for%2520the%2520training%2520process.%2520Since%2520STREAM%2520eliminates%2520domain-related%250Aredundancy%2520in%2520source%2520images%252C%2520it%2520enables%2520the%2520model%2520to%2520pay%2520closer%2520attention%2520to%250Athe%2520detailed%2520prompt%2520set%2520that%2520is%2520crucial%2520for%2520distinguishing%2520fine-grained%250Avehicles.%2520This%2520learned%2520prompt%2520set%2520related%2520to%2520the%2520vehicle%2520identity%2520is%2520then%250Autilized%2520to%2520guide%2520the%2520comprehensive%2520representation%2520learning%2520of%2520complementary%250Afeatures%2520for%2520final%2520knowledge%2520fusion%2520and%2520identity%2520recognition.%2520Inspired%2520by%2520the%250Aunity%2520principle%252C%2520MiKeCoCo%2520integrates%2520the%2520diverse%2520evaluation%2520ways%2520of%2520experts%2520to%250Aensure%2520the%2520accuracy%2520and%2520consistency%2520of%2520ReID.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unity%20in%20Diversity%3A%20Multi-expert%20Knowledge%20Confrontation%20and%0A%20%20Collaboration%20for%20Generalizable%20Vehicle%20Re-identification&entry.906535625=Zhenyu%20Kuang%20and%20Hongyang%20Zhang%20and%20Mang%20Ye%20and%20Bin%20Yang%20and%20Yinhao%20Liu%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Huafeng%20Li&entry.1292438233=%20%20Generalizable%20vehicle%20re-identification%20%28ReID%29%20seeks%20to%20develop%20models%20that%0Acan%20adapt%20to%20unknown%20target%20domains%20without%20the%20need%20for%20additional%20fine-tuning%0Aor%20retraining.%20Previous%20works%20have%20mainly%20focused%20on%20extracting%0Adomain-invariant%20features%20by%20aligning%20data%20distributions%20between%20source%0Adomains.%20However%2C%20interfered%20by%20the%20inherent%20domain-related%20redundancy%20in%20the%0Asource%20images%2C%20solely%20relying%20on%20common%20features%20is%20insufficient%20for%20accurately%0Acapturing%20the%20complementary%20features%20with%20lower%20occurrence%20probability%20and%0Asmaller%20energy.%20To%20solve%20this%20unique%20problem%2C%20we%20propose%20a%20two-stage%0AMulti-expert%20Knowledge%20Confrontation%20and%20Collaboration%20%28MiKeCoCo%29%20method%2C%20which%0Afully%20leverages%20the%20high-level%20semantics%20of%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%20to%20obtain%20a%20diversified%20prompt%20set%20and%20achieve%20complementary%0Afeature%20representations.%20Specifically%2C%20this%20paper%20first%20designs%20a%0ASpectrum-based%20Transformation%20for%20Redundancy%20Elimination%20and%20Augmentation%0AModule%20%28STREAM%29%20through%20simple%20image%20preprocessing%20to%20obtain%20two%20types%20of%20image%0Ainputs%20for%20the%20training%20process.%20Since%20STREAM%20eliminates%20domain-related%0Aredundancy%20in%20source%20images%2C%20it%20enables%20the%20model%20to%20pay%20closer%20attention%20to%0Athe%20detailed%20prompt%20set%20that%20is%20crucial%20for%20distinguishing%20fine-grained%0Avehicles.%20This%20learned%20prompt%20set%20related%20to%20the%20vehicle%20identity%20is%20then%0Autilized%20to%20guide%20the%20comprehensive%20representation%20learning%20of%20complementary%0Afeatures%20for%20final%20knowledge%20fusion%20and%20identity%20recognition.%20Inspired%20by%20the%0Aunity%20principle%2C%20MiKeCoCo%20integrates%20the%20diverse%20evaluation%20ways%20of%20experts%20to%0Aensure%20the%20accuracy%20and%20consistency%20of%20ReID.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07351v2&entry.124074799=Read"},
{"title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions", "author": "Chao He and Jianqiang Ren and Yuan Dong and Jianjing Xiang and Xiejie Shen and Weihao Yuan and Liefeng Bo", "abstract": "  The 2D cartoon style is a prominent art form in digital character creation,\nparticularly popular among younger audiences. While advancements in digital\nhuman technology have spurred extensive research into photorealistic digital\nhumans and 3D characters, interactive 2D cartoon characters have received\ncomparatively less attention. Unlike 3D counterparts, which require\nsophisticated construction and resource-intensive rendering, Live2D, a\nwidely-used format for 2D cartoon characters, offers a more efficient\nalternative, which allows to animate 2D characters in a manner that simulates\n3D movement without the necessity of building a complete 3D model. Furthermore,\nLive2D employs lightweight HTML5 (H5) rendering, improving both accessibility\nand efficiency. In this technical report, we introduce Textoon, an innovative\nmethod for generating diverse 2D cartoon characters in the Live2D format based\non text descriptions. The Textoon leverages cutting-edge language and vision\nmodels to comprehend textual intentions and generate 2D appearance, capable of\ncreating a wide variety of stunning and interactive 2D characters within one\nminute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.\n", "link": "http://arxiv.org/abs/2501.10020v2", "date": "2025-02-04", "relevancy": 2.7872, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5812}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5485}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textoon%3A%20Generating%20Vivid%202D%20Cartoon%20Characters%20from%20Text%20Descriptions&body=Title%3A%20Textoon%3A%20Generating%20Vivid%202D%20Cartoon%20Characters%20from%20Text%20Descriptions%0AAuthor%3A%20Chao%20He%20and%20Jianqiang%20Ren%20and%20Yuan%20Dong%20and%20Jianjing%20Xiang%20and%20Xiejie%20Shen%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo%0AAbstract%3A%20%20%20The%202D%20cartoon%20style%20is%20a%20prominent%20art%20form%20in%20digital%20character%20creation%2C%0Aparticularly%20popular%20among%20younger%20audiences.%20While%20advancements%20in%20digital%0Ahuman%20technology%20have%20spurred%20extensive%20research%20into%20photorealistic%20digital%0Ahumans%20and%203D%20characters%2C%20interactive%202D%20cartoon%20characters%20have%20received%0Acomparatively%20less%20attention.%20Unlike%203D%20counterparts%2C%20which%20require%0Asophisticated%20construction%20and%20resource-intensive%20rendering%2C%20Live2D%2C%20a%0Awidely-used%20format%20for%202D%20cartoon%20characters%2C%20offers%20a%20more%20efficient%0Aalternative%2C%20which%20allows%20to%20animate%202D%20characters%20in%20a%20manner%20that%20simulates%0A3D%20movement%20without%20the%20necessity%20of%20building%20a%20complete%203D%20model.%20Furthermore%2C%0ALive2D%20employs%20lightweight%20HTML5%20%28H5%29%20rendering%2C%20improving%20both%20accessibility%0Aand%20efficiency.%20In%20this%20technical%20report%2C%20we%20introduce%20Textoon%2C%20an%20innovative%0Amethod%20for%20generating%20diverse%202D%20cartoon%20characters%20in%20the%20Live2D%20format%20based%0Aon%20text%20descriptions.%20The%20Textoon%20leverages%20cutting-edge%20language%20and%20vision%0Amodels%20to%20comprehend%20textual%20intentions%20and%20generate%202D%20appearance%2C%20capable%20of%0Acreating%20a%20wide%20variety%20of%20stunning%20and%20interactive%202D%20characters%20within%20one%0Aminute.%20The%20project%20homepage%20is%20https%3A//human3daigc.github.io/Textoon_webpage/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextoon%253A%2520Generating%2520Vivid%25202D%2520Cartoon%2520Characters%2520from%2520Text%2520Descriptions%26entry.906535625%3DChao%2520He%2520and%2520Jianqiang%2520Ren%2520and%2520Yuan%2520Dong%2520and%2520Jianjing%2520Xiang%2520and%2520Xiejie%2520Shen%2520and%2520Weihao%2520Yuan%2520and%2520Liefeng%2520Bo%26entry.1292438233%3D%2520%2520The%25202D%2520cartoon%2520style%2520is%2520a%2520prominent%2520art%2520form%2520in%2520digital%2520character%2520creation%252C%250Aparticularly%2520popular%2520among%2520younger%2520audiences.%2520While%2520advancements%2520in%2520digital%250Ahuman%2520technology%2520have%2520spurred%2520extensive%2520research%2520into%2520photorealistic%2520digital%250Ahumans%2520and%25203D%2520characters%252C%2520interactive%25202D%2520cartoon%2520characters%2520have%2520received%250Acomparatively%2520less%2520attention.%2520Unlike%25203D%2520counterparts%252C%2520which%2520require%250Asophisticated%2520construction%2520and%2520resource-intensive%2520rendering%252C%2520Live2D%252C%2520a%250Awidely-used%2520format%2520for%25202D%2520cartoon%2520characters%252C%2520offers%2520a%2520more%2520efficient%250Aalternative%252C%2520which%2520allows%2520to%2520animate%25202D%2520characters%2520in%2520a%2520manner%2520that%2520simulates%250A3D%2520movement%2520without%2520the%2520necessity%2520of%2520building%2520a%2520complete%25203D%2520model.%2520Furthermore%252C%250ALive2D%2520employs%2520lightweight%2520HTML5%2520%2528H5%2529%2520rendering%252C%2520improving%2520both%2520accessibility%250Aand%2520efficiency.%2520In%2520this%2520technical%2520report%252C%2520we%2520introduce%2520Textoon%252C%2520an%2520innovative%250Amethod%2520for%2520generating%2520diverse%25202D%2520cartoon%2520characters%2520in%2520the%2520Live2D%2520format%2520based%250Aon%2520text%2520descriptions.%2520The%2520Textoon%2520leverages%2520cutting-edge%2520language%2520and%2520vision%250Amodels%2520to%2520comprehend%2520textual%2520intentions%2520and%2520generate%25202D%2520appearance%252C%2520capable%2520of%250Acreating%2520a%2520wide%2520variety%2520of%2520stunning%2520and%2520interactive%25202D%2520characters%2520within%2520one%250Aminute.%2520The%2520project%2520homepage%2520is%2520https%253A//human3daigc.github.io/Textoon_webpage/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textoon%3A%20Generating%20Vivid%202D%20Cartoon%20Characters%20from%20Text%20Descriptions&entry.906535625=Chao%20He%20and%20Jianqiang%20Ren%20and%20Yuan%20Dong%20and%20Jianjing%20Xiang%20and%20Xiejie%20Shen%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo&entry.1292438233=%20%20The%202D%20cartoon%20style%20is%20a%20prominent%20art%20form%20in%20digital%20character%20creation%2C%0Aparticularly%20popular%20among%20younger%20audiences.%20While%20advancements%20in%20digital%0Ahuman%20technology%20have%20spurred%20extensive%20research%20into%20photorealistic%20digital%0Ahumans%20and%203D%20characters%2C%20interactive%202D%20cartoon%20characters%20have%20received%0Acomparatively%20less%20attention.%20Unlike%203D%20counterparts%2C%20which%20require%0Asophisticated%20construction%20and%20resource-intensive%20rendering%2C%20Live2D%2C%20a%0Awidely-used%20format%20for%202D%20cartoon%20characters%2C%20offers%20a%20more%20efficient%0Aalternative%2C%20which%20allows%20to%20animate%202D%20characters%20in%20a%20manner%20that%20simulates%0A3D%20movement%20without%20the%20necessity%20of%20building%20a%20complete%203D%20model.%20Furthermore%2C%0ALive2D%20employs%20lightweight%20HTML5%20%28H5%29%20rendering%2C%20improving%20both%20accessibility%0Aand%20efficiency.%20In%20this%20technical%20report%2C%20we%20introduce%20Textoon%2C%20an%20innovative%0Amethod%20for%20generating%20diverse%202D%20cartoon%20characters%20in%20the%20Live2D%20format%20based%0Aon%20text%20descriptions.%20The%20Textoon%20leverages%20cutting-edge%20language%20and%20vision%0Amodels%20to%20comprehend%20textual%20intentions%20and%20generate%202D%20appearance%2C%20capable%20of%0Acreating%20a%20wide%20variety%20of%20stunning%20and%20interactive%202D%20characters%20within%20one%0Aminute.%20The%20project%20homepage%20is%20https%3A//human3daigc.github.io/Textoon_webpage/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10020v2&entry.124074799=Read"},
{"title": "An interpretable generative multimodal neuroimaging-genomics framework\n  for decoding Alzheimer's disease", "author": "Giorgio Dolci and Federica Cruciani and Md Abdur Rahaman and Anees Abrol and Jiayu Chen and Zening Fu and Ilaria Boscolo Galazzo and Gloria Menegaz and Vince D. Calhoun", "abstract": "  \\textbf{Objective:} Alzheimer's disease (AD) is the most prevalent form of\ndementia worldwide, encompassing a prodromal stage known as Mild Cognitive\nImpairment (MCI), where patients may either progress to AD or remain stable.\nThe objective of the work was to capture structural and functional modulations\nof brain structure and function relying on multimodal MRI data and Single\nNucleotide Polymorphisms, also in case of missing views, with the twofold goal\nof classifying AD patients versus healthy controls and detecting MCI\nconverters. % in two distinct tasks, dealing with also missing data.\\\\\n\\textbf{Approach:} We propose a multimodal DL-based classification framework\nwhere a generative module employing Cycle Generative Adversarial Networks was\nintroduced in the latent space for imputing missing data (a common issue of\nmultimodal approaches). Explainable AI method was then used to extract input\nfeatures' relevance allowing for post-hoc validation and enhancing the\ninterpretability of the learned representations. \\textbf{Main results:}\nExperimental results on two tasks, AD detection and MCI conversion, showed that\nour framework reached competitive performance in the state-of-the-art with an\naccuracy of $0.926\\pm0.02$ and $0.711\\pm0.01$ in the two tasks, respectively.\nThe interpretability analysis revealed gray matter modulations in cortical and\nsubcortical brain areas typically associated with AD. Moreover, impairments in\nsensory-motor and visual resting state networks along the disease continuum, as\nwell as genetic mutations defining biological processes linked to endocytosis,\namyloid-beta, and cholesterol, were identified. \\textbf{Significance:} Our\nintegrative and interpretable DL approach shows promising performance for AD\ndetection and MCI prediction while shedding light on important biological\ninsights.\n", "link": "http://arxiv.org/abs/2406.13292v3", "date": "2025-02-04", "relevancy": 2.6874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease&body=Title%3A%20An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease%0AAuthor%3A%20Giorgio%20Dolci%20and%20Federica%20Cruciani%20and%20Md%20Abdur%20Rahaman%20and%20Anees%20Abrol%20and%20Jiayu%20Chen%20and%20Zening%20Fu%20and%20Ilaria%20Boscolo%20Galazzo%20and%20Gloria%20Menegaz%20and%20Vince%20D.%20Calhoun%0AAbstract%3A%20%20%20%5Ctextbf%7BObjective%3A%7D%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20prevalent%20form%20of%0Adementia%20worldwide%2C%20encompassing%20a%20prodromal%20stage%20known%20as%20Mild%20Cognitive%0AImpairment%20%28MCI%29%2C%20where%20patients%20may%20either%20progress%20to%20AD%20or%20remain%20stable.%0AThe%20objective%20of%20the%20work%20was%20to%20capture%20structural%20and%20functional%20modulations%0Aof%20brain%20structure%20and%20function%20relying%20on%20multimodal%20MRI%20data%20and%20Single%0ANucleotide%20Polymorphisms%2C%20also%20in%20case%20of%20missing%20views%2C%20with%20the%20twofold%20goal%0Aof%20classifying%20AD%20patients%20versus%20healthy%20controls%20and%20detecting%20MCI%0Aconverters.%20%25%20in%20two%20distinct%20tasks%2C%20dealing%20with%20also%20missing%20data.%5C%5C%0A%5Ctextbf%7BApproach%3A%7D%20We%20propose%20a%20multimodal%20DL-based%20classification%20framework%0Awhere%20a%20generative%20module%20employing%20Cycle%20Generative%20Adversarial%20Networks%20was%0Aintroduced%20in%20the%20latent%20space%20for%20imputing%20missing%20data%20%28a%20common%20issue%20of%0Amultimodal%20approaches%29.%20Explainable%20AI%20method%20was%20then%20used%20to%20extract%20input%0Afeatures%27%20relevance%20allowing%20for%20post-hoc%20validation%20and%20enhancing%20the%0Ainterpretability%20of%20the%20learned%20representations.%20%5Ctextbf%7BMain%20results%3A%7D%0AExperimental%20results%20on%20two%20tasks%2C%20AD%20detection%20and%20MCI%20conversion%2C%20showed%20that%0Aour%20framework%20reached%20competitive%20performance%20in%20the%20state-of-the-art%20with%20an%0Aaccuracy%20of%20%240.926%5Cpm0.02%24%20and%20%240.711%5Cpm0.01%24%20in%20the%20two%20tasks%2C%20respectively.%0AThe%20interpretability%20analysis%20revealed%20gray%20matter%20modulations%20in%20cortical%20and%0Asubcortical%20brain%20areas%20typically%20associated%20with%20AD.%20Moreover%2C%20impairments%20in%0Asensory-motor%20and%20visual%20resting%20state%20networks%20along%20the%20disease%20continuum%2C%20as%0Awell%20as%20genetic%20mutations%20defining%20biological%20processes%20linked%20to%20endocytosis%2C%0Aamyloid-beta%2C%20and%20cholesterol%2C%20were%20identified.%20%5Ctextbf%7BSignificance%3A%7D%20Our%0Aintegrative%20and%20interpretable%20DL%20approach%20shows%20promising%20performance%20for%20AD%0Adetection%20and%20MCI%20prediction%20while%20shedding%20light%20on%20important%20biological%0Ainsights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13292v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520interpretable%2520generative%2520multimodal%2520neuroimaging-genomics%2520framework%250A%2520%2520for%2520decoding%2520Alzheimer%2527s%2520disease%26entry.906535625%3DGiorgio%2520Dolci%2520and%2520Federica%2520Cruciani%2520and%2520Md%2520Abdur%2520Rahaman%2520and%2520Anees%2520Abrol%2520and%2520Jiayu%2520Chen%2520and%2520Zening%2520Fu%2520and%2520Ilaria%2520Boscolo%2520Galazzo%2520and%2520Gloria%2520Menegaz%2520and%2520Vince%2520D.%2520Calhoun%26entry.1292438233%3D%2520%2520%255Ctextbf%257BObjective%253A%257D%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520is%2520the%2520most%2520prevalent%2520form%2520of%250Adementia%2520worldwide%252C%2520encompassing%2520a%2520prodromal%2520stage%2520known%2520as%2520Mild%2520Cognitive%250AImpairment%2520%2528MCI%2529%252C%2520where%2520patients%2520may%2520either%2520progress%2520to%2520AD%2520or%2520remain%2520stable.%250AThe%2520objective%2520of%2520the%2520work%2520was%2520to%2520capture%2520structural%2520and%2520functional%2520modulations%250Aof%2520brain%2520structure%2520and%2520function%2520relying%2520on%2520multimodal%2520MRI%2520data%2520and%2520Single%250ANucleotide%2520Polymorphisms%252C%2520also%2520in%2520case%2520of%2520missing%2520views%252C%2520with%2520the%2520twofold%2520goal%250Aof%2520classifying%2520AD%2520patients%2520versus%2520healthy%2520controls%2520and%2520detecting%2520MCI%250Aconverters.%2520%2525%2520in%2520two%2520distinct%2520tasks%252C%2520dealing%2520with%2520also%2520missing%2520data.%255C%255C%250A%255Ctextbf%257BApproach%253A%257D%2520We%2520propose%2520a%2520multimodal%2520DL-based%2520classification%2520framework%250Awhere%2520a%2520generative%2520module%2520employing%2520Cycle%2520Generative%2520Adversarial%2520Networks%2520was%250Aintroduced%2520in%2520the%2520latent%2520space%2520for%2520imputing%2520missing%2520data%2520%2528a%2520common%2520issue%2520of%250Amultimodal%2520approaches%2529.%2520Explainable%2520AI%2520method%2520was%2520then%2520used%2520to%2520extract%2520input%250Afeatures%2527%2520relevance%2520allowing%2520for%2520post-hoc%2520validation%2520and%2520enhancing%2520the%250Ainterpretability%2520of%2520the%2520learned%2520representations.%2520%255Ctextbf%257BMain%2520results%253A%257D%250AExperimental%2520results%2520on%2520two%2520tasks%252C%2520AD%2520detection%2520and%2520MCI%2520conversion%252C%2520showed%2520that%250Aour%2520framework%2520reached%2520competitive%2520performance%2520in%2520the%2520state-of-the-art%2520with%2520an%250Aaccuracy%2520of%2520%25240.926%255Cpm0.02%2524%2520and%2520%25240.711%255Cpm0.01%2524%2520in%2520the%2520two%2520tasks%252C%2520respectively.%250AThe%2520interpretability%2520analysis%2520revealed%2520gray%2520matter%2520modulations%2520in%2520cortical%2520and%250Asubcortical%2520brain%2520areas%2520typically%2520associated%2520with%2520AD.%2520Moreover%252C%2520impairments%2520in%250Asensory-motor%2520and%2520visual%2520resting%2520state%2520networks%2520along%2520the%2520disease%2520continuum%252C%2520as%250Awell%2520as%2520genetic%2520mutations%2520defining%2520biological%2520processes%2520linked%2520to%2520endocytosis%252C%250Aamyloid-beta%252C%2520and%2520cholesterol%252C%2520were%2520identified.%2520%255Ctextbf%257BSignificance%253A%257D%2520Our%250Aintegrative%2520and%2520interpretable%2520DL%2520approach%2520shows%2520promising%2520performance%2520for%2520AD%250Adetection%2520and%2520MCI%2520prediction%2520while%2520shedding%2520light%2520on%2520important%2520biological%250Ainsights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13292v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease&entry.906535625=Giorgio%20Dolci%20and%20Federica%20Cruciani%20and%20Md%20Abdur%20Rahaman%20and%20Anees%20Abrol%20and%20Jiayu%20Chen%20and%20Zening%20Fu%20and%20Ilaria%20Boscolo%20Galazzo%20and%20Gloria%20Menegaz%20and%20Vince%20D.%20Calhoun&entry.1292438233=%20%20%5Ctextbf%7BObjective%3A%7D%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20prevalent%20form%20of%0Adementia%20worldwide%2C%20encompassing%20a%20prodromal%20stage%20known%20as%20Mild%20Cognitive%0AImpairment%20%28MCI%29%2C%20where%20patients%20may%20either%20progress%20to%20AD%20or%20remain%20stable.%0AThe%20objective%20of%20the%20work%20was%20to%20capture%20structural%20and%20functional%20modulations%0Aof%20brain%20structure%20and%20function%20relying%20on%20multimodal%20MRI%20data%20and%20Single%0ANucleotide%20Polymorphisms%2C%20also%20in%20case%20of%20missing%20views%2C%20with%20the%20twofold%20goal%0Aof%20classifying%20AD%20patients%20versus%20healthy%20controls%20and%20detecting%20MCI%0Aconverters.%20%25%20in%20two%20distinct%20tasks%2C%20dealing%20with%20also%20missing%20data.%5C%5C%0A%5Ctextbf%7BApproach%3A%7D%20We%20propose%20a%20multimodal%20DL-based%20classification%20framework%0Awhere%20a%20generative%20module%20employing%20Cycle%20Generative%20Adversarial%20Networks%20was%0Aintroduced%20in%20the%20latent%20space%20for%20imputing%20missing%20data%20%28a%20common%20issue%20of%0Amultimodal%20approaches%29.%20Explainable%20AI%20method%20was%20then%20used%20to%20extract%20input%0Afeatures%27%20relevance%20allowing%20for%20post-hoc%20validation%20and%20enhancing%20the%0Ainterpretability%20of%20the%20learned%20representations.%20%5Ctextbf%7BMain%20results%3A%7D%0AExperimental%20results%20on%20two%20tasks%2C%20AD%20detection%20and%20MCI%20conversion%2C%20showed%20that%0Aour%20framework%20reached%20competitive%20performance%20in%20the%20state-of-the-art%20with%20an%0Aaccuracy%20of%20%240.926%5Cpm0.02%24%20and%20%240.711%5Cpm0.01%24%20in%20the%20two%20tasks%2C%20respectively.%0AThe%20interpretability%20analysis%20revealed%20gray%20matter%20modulations%20in%20cortical%20and%0Asubcortical%20brain%20areas%20typically%20associated%20with%20AD.%20Moreover%2C%20impairments%20in%0Asensory-motor%20and%20visual%20resting%20state%20networks%20along%20the%20disease%20continuum%2C%20as%0Awell%20as%20genetic%20mutations%20defining%20biological%20processes%20linked%20to%20endocytosis%2C%0Aamyloid-beta%2C%20and%20cholesterol%2C%20were%20identified.%20%5Ctextbf%7BSignificance%3A%7D%20Our%0Aintegrative%20and%20interpretable%20DL%20approach%20shows%20promising%20performance%20for%20AD%0Adetection%20and%20MCI%20prediction%20while%20shedding%20light%20on%20important%20biological%0Ainsights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13292v3&entry.124074799=Read"},
{"title": "Graph-based Document Structure Analysis", "author": "Yufan Chen and Ruiping Liu and Junwei Zheng and Di Wen and Kunyu Peng and Jiaming Zhang and Rainer Stiefelhagen", "abstract": "  When reading a document, glancing at the spatial layout of a document is an\ninitial step to understand it roughly. Traditional document layout analysis\n(DLA) methods, however, offer only a superficial parsing of documents, focusing\non basic instance detection and often failing to capture the nuanced spatial\nand logical relations between instances. These limitations hinder DLA-based\nmodels from achieving a gradually deeper comprehension akin to human reading.\nIn this work, we propose a novel graph-based Document Structure Analysis (gDSA)\ntask. This task requires that model not only detects document elements but also\ngenerates spatial and logical relations in form of a graph structure, allowing\nto understand documents in a holistic and intuitive manner. For this new task,\nwe construct a relation graph-based document structure analysis dataset\n(GraphDoc) with 80K document images and 4.13M relation annotations, enabling\ntraining models to complete multiple tasks like reading order, hierarchical\nstructures analysis, and complex inter-element relation inference. Furthermore,\na document relation graph generator (DRGG) is proposed to address the gDSA\ntask, which achieves performance with 57.6% at mAP$_g$@0.5 for a strong\nbenchmark baseline on this novel task and dataset. We hope this graphical\nrepresentation of document structure can mark an innovative advancement in\ndocument structure analysis and understanding. The new dataset and code will be\nmade publicly available at https://yufanchen96.github.io/projects/GraphDoc.\n", "link": "http://arxiv.org/abs/2502.02501v1", "date": "2025-02-04", "relevancy": 2.6538, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%20Document%20Structure%20Analysis&body=Title%3A%20Graph-based%20Document%20Structure%20Analysis%0AAuthor%3A%20Yufan%20Chen%20and%20Ruiping%20Liu%20and%20Junwei%20Zheng%20and%20Di%20Wen%20and%20Kunyu%20Peng%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20When%20reading%20a%20document%2C%20glancing%20at%20the%20spatial%20layout%20of%20a%20document%20is%20an%0Ainitial%20step%20to%20understand%20it%20roughly.%20Traditional%20document%20layout%20analysis%0A%28DLA%29%20methods%2C%20however%2C%20offer%20only%20a%20superficial%20parsing%20of%20documents%2C%20focusing%0Aon%20basic%20instance%20detection%20and%20often%20failing%20to%20capture%20the%20nuanced%20spatial%0Aand%20logical%20relations%20between%20instances.%20These%20limitations%20hinder%20DLA-based%0Amodels%20from%20achieving%20a%20gradually%20deeper%20comprehension%20akin%20to%20human%20reading.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20graph-based%20Document%20Structure%20Analysis%20%28gDSA%29%0Atask.%20This%20task%20requires%20that%20model%20not%20only%20detects%20document%20elements%20but%20also%0Agenerates%20spatial%20and%20logical%20relations%20in%20form%20of%20a%20graph%20structure%2C%20allowing%0Ato%20understand%20documents%20in%20a%20holistic%20and%20intuitive%20manner.%20For%20this%20new%20task%2C%0Awe%20construct%20a%20relation%20graph-based%20document%20structure%20analysis%20dataset%0A%28GraphDoc%29%20with%2080K%20document%20images%20and%204.13M%20relation%20annotations%2C%20enabling%0Atraining%20models%20to%20complete%20multiple%20tasks%20like%20reading%20order%2C%20hierarchical%0Astructures%20analysis%2C%20and%20complex%20inter-element%20relation%20inference.%20Furthermore%2C%0Aa%20document%20relation%20graph%20generator%20%28DRGG%29%20is%20proposed%20to%20address%20the%20gDSA%0Atask%2C%20which%20achieves%20performance%20with%2057.6%25%20at%20mAP%24_g%24%400.5%20for%20a%20strong%0Abenchmark%20baseline%20on%20this%20novel%20task%20and%20dataset.%20We%20hope%20this%20graphical%0Arepresentation%20of%20document%20structure%20can%20mark%20an%20innovative%20advancement%20in%0Adocument%20structure%20analysis%20and%20understanding.%20The%20new%20dataset%20and%20code%20will%20be%0Amade%20publicly%20available%20at%20https%3A//yufanchen96.github.io/projects/GraphDoc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%2520Document%2520Structure%2520Analysis%26entry.906535625%3DYufan%2520Chen%2520and%2520Ruiping%2520Liu%2520and%2520Junwei%2520Zheng%2520and%2520Di%2520Wen%2520and%2520Kunyu%2520Peng%2520and%2520Jiaming%2520Zhang%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520When%2520reading%2520a%2520document%252C%2520glancing%2520at%2520the%2520spatial%2520layout%2520of%2520a%2520document%2520is%2520an%250Ainitial%2520step%2520to%2520understand%2520it%2520roughly.%2520Traditional%2520document%2520layout%2520analysis%250A%2528DLA%2529%2520methods%252C%2520however%252C%2520offer%2520only%2520a%2520superficial%2520parsing%2520of%2520documents%252C%2520focusing%250Aon%2520basic%2520instance%2520detection%2520and%2520often%2520failing%2520to%2520capture%2520the%2520nuanced%2520spatial%250Aand%2520logical%2520relations%2520between%2520instances.%2520These%2520limitations%2520hinder%2520DLA-based%250Amodels%2520from%2520achieving%2520a%2520gradually%2520deeper%2520comprehension%2520akin%2520to%2520human%2520reading.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520graph-based%2520Document%2520Structure%2520Analysis%2520%2528gDSA%2529%250Atask.%2520This%2520task%2520requires%2520that%2520model%2520not%2520only%2520detects%2520document%2520elements%2520but%2520also%250Agenerates%2520spatial%2520and%2520logical%2520relations%2520in%2520form%2520of%2520a%2520graph%2520structure%252C%2520allowing%250Ato%2520understand%2520documents%2520in%2520a%2520holistic%2520and%2520intuitive%2520manner.%2520For%2520this%2520new%2520task%252C%250Awe%2520construct%2520a%2520relation%2520graph-based%2520document%2520structure%2520analysis%2520dataset%250A%2528GraphDoc%2529%2520with%252080K%2520document%2520images%2520and%25204.13M%2520relation%2520annotations%252C%2520enabling%250Atraining%2520models%2520to%2520complete%2520multiple%2520tasks%2520like%2520reading%2520order%252C%2520hierarchical%250Astructures%2520analysis%252C%2520and%2520complex%2520inter-element%2520relation%2520inference.%2520Furthermore%252C%250Aa%2520document%2520relation%2520graph%2520generator%2520%2528DRGG%2529%2520is%2520proposed%2520to%2520address%2520the%2520gDSA%250Atask%252C%2520which%2520achieves%2520performance%2520with%252057.6%2525%2520at%2520mAP%2524_g%2524%25400.5%2520for%2520a%2520strong%250Abenchmark%2520baseline%2520on%2520this%2520novel%2520task%2520and%2520dataset.%2520We%2520hope%2520this%2520graphical%250Arepresentation%2520of%2520document%2520structure%2520can%2520mark%2520an%2520innovative%2520advancement%2520in%250Adocument%2520structure%2520analysis%2520and%2520understanding.%2520The%2520new%2520dataset%2520and%2520code%2520will%2520be%250Amade%2520publicly%2520available%2520at%2520https%253A//yufanchen96.github.io/projects/GraphDoc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%20Document%20Structure%20Analysis&entry.906535625=Yufan%20Chen%20and%20Ruiping%20Liu%20and%20Junwei%20Zheng%20and%20Di%20Wen%20and%20Kunyu%20Peng%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20When%20reading%20a%20document%2C%20glancing%20at%20the%20spatial%20layout%20of%20a%20document%20is%20an%0Ainitial%20step%20to%20understand%20it%20roughly.%20Traditional%20document%20layout%20analysis%0A%28DLA%29%20methods%2C%20however%2C%20offer%20only%20a%20superficial%20parsing%20of%20documents%2C%20focusing%0Aon%20basic%20instance%20detection%20and%20often%20failing%20to%20capture%20the%20nuanced%20spatial%0Aand%20logical%20relations%20between%20instances.%20These%20limitations%20hinder%20DLA-based%0Amodels%20from%20achieving%20a%20gradually%20deeper%20comprehension%20akin%20to%20human%20reading.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20graph-based%20Document%20Structure%20Analysis%20%28gDSA%29%0Atask.%20This%20task%20requires%20that%20model%20not%20only%20detects%20document%20elements%20but%20also%0Agenerates%20spatial%20and%20logical%20relations%20in%20form%20of%20a%20graph%20structure%2C%20allowing%0Ato%20understand%20documents%20in%20a%20holistic%20and%20intuitive%20manner.%20For%20this%20new%20task%2C%0Awe%20construct%20a%20relation%20graph-based%20document%20structure%20analysis%20dataset%0A%28GraphDoc%29%20with%2080K%20document%20images%20and%204.13M%20relation%20annotations%2C%20enabling%0Atraining%20models%20to%20complete%20multiple%20tasks%20like%20reading%20order%2C%20hierarchical%0Astructures%20analysis%2C%20and%20complex%20inter-element%20relation%20inference.%20Furthermore%2C%0Aa%20document%20relation%20graph%20generator%20%28DRGG%29%20is%20proposed%20to%20address%20the%20gDSA%0Atask%2C%20which%20achieves%20performance%20with%2057.6%25%20at%20mAP%24_g%24%400.5%20for%20a%20strong%0Abenchmark%20baseline%20on%20this%20novel%20task%20and%20dataset.%20We%20hope%20this%20graphical%0Arepresentation%20of%20document%20structure%20can%20mark%20an%20innovative%20advancement%20in%0Adocument%20structure%20analysis%20and%20understanding.%20The%20new%20dataset%20and%20code%20will%20be%0Amade%20publicly%20available%20at%20https%3A//yufanchen96.github.io/projects/GraphDoc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02501v1&entry.124074799=Read"},
{"title": "Point-Level Topological Representation Learning on Point Clouds", "author": "Vincent P. Grande and Michael T. Schaub", "abstract": "  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise and heterogeneous sampling.\n", "link": "http://arxiv.org/abs/2406.02300v3", "date": "2025-02-04", "relevancy": 2.6353, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&body=Title%3A%20Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds%0AAuthor%3A%20Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub%0AAbstract%3A%20%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise%20and%20heterogeneous%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-Level%2520Topological%2520Representation%2520Learning%2520on%2520Point%2520Clouds%26entry.906535625%3DVincent%2520P.%2520Grande%2520and%2520Michael%2520T.%2520Schaub%26entry.1292438233%3D%2520%2520Topological%2520Data%2520Analysis%2520%2528TDA%2529%2520allows%2520us%2520to%2520extract%2520powerful%2520topological%2520and%250Ahigher-order%2520information%2520on%2520the%2520global%2520shape%2520of%2520a%2520data%2520set%2520or%2520point%2520cloud.%250ATools%2520like%2520Persistent%2520Homology%2520or%2520the%2520Euler%2520Transform%2520give%2520a%2520single%2520complex%250Adescription%2520of%2520the%2520global%2520structure%2520of%2520the%2520point%2520cloud.%2520However%252C%2520common%2520machine%250Alearning%2520applications%2520like%2520classification%2520require%2520point-level%2520information%2520and%250Afeatures%2520to%2520be%2520available.%2520In%2520this%2520paper%252C%2520we%2520bridge%2520this%2520gap%2520and%2520propose%2520a%2520novel%250Amethod%2520to%2520extract%2520node-level%2520topological%2520features%2520from%2520complex%2520point%2520clouds%250Ausing%2520discrete%2520variants%2520of%2520concepts%2520from%2520algebraic%2520topology%2520and%2520differential%250Ageometry.%2520We%2520verify%2520the%2520effectiveness%2520of%2520these%2520topological%2520point%2520features%250A%2528TOPF%2529%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520and%2520study%2520their%2520robustness%2520under%250Anoise%2520and%2520heterogeneous%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&entry.906535625=Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub&entry.1292438233=%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise%20and%20heterogeneous%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02300v3&entry.124074799=Read"},
{"title": "Learning Compact and Robust Representations for Anomaly Detection", "author": "Willian T. Lunardi and Abdulrahman Banabila and Dania Herzalla and Martin Andreoni", "abstract": "  Distance-based anomaly detection methods rely on compact and separable\nin-distribution (ID) embeddings to effectively delineate anomaly boundaries.\nSingle-positive contrastive formulations suffer from class collision, promoting\nunnecessary intra-class variance within ID samples. While multi-positive\nformulations can improve inlier compactness, they fail to preserve the\ndiversity among synthetic outliers. We address these limitations by proposing a\ncontrastive pretext task for anomaly detection that enforces three key\nproperties: (1) compact ID clustering to reduce intra-class variance, (2)\ninlier-outlier separation to enhance inter-class separation, and (3)\noutlier-outlier separation to maintain diversity among synthetic outliers and\nprevent representation collapse. These properties work together to ensure a\nmore robust and discriminative feature space for anomaly detection. Our\napproach achieves approximately 12x faster convergence than NT-Xent and 7x\nfaster than Rot-SupCon, with superior performance. On CIFAR-10, it delivers an\naverage performance boost of 6.2% over NT-Xent and 2% over Rot-SupCon, with\nclass-specific improvements of up to 16.9%. Our code is available at\nhttps://anonymous.4open.science/r/firm-98B6.\n", "link": "http://arxiv.org/abs/2501.05130v4", "date": "2025-02-04", "relevancy": 2.6142, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5453}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Compact%20and%20Robust%20Representations%20for%20Anomaly%20Detection&body=Title%3A%20Learning%20Compact%20and%20Robust%20Representations%20for%20Anomaly%20Detection%0AAuthor%3A%20Willian%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20Andreoni%0AAbstract%3A%20%20%20Distance-based%20anomaly%20detection%20methods%20rely%20on%20compact%20and%20separable%0Ain-distribution%20%28ID%29%20embeddings%20to%20effectively%20delineate%20anomaly%20boundaries.%0ASingle-positive%20contrastive%20formulations%20suffer%20from%20class%20collision%2C%20promoting%0Aunnecessary%20intra-class%20variance%20within%20ID%20samples.%20While%20multi-positive%0Aformulations%20can%20improve%20inlier%20compactness%2C%20they%20fail%20to%20preserve%20the%0Adiversity%20among%20synthetic%20outliers.%20We%20address%20these%20limitations%20by%20proposing%20a%0Acontrastive%20pretext%20task%20for%20anomaly%20detection%20that%20enforces%20three%20key%0Aproperties%3A%20%281%29%20compact%20ID%20clustering%20to%20reduce%20intra-class%20variance%2C%20%282%29%0Ainlier-outlier%20separation%20to%20enhance%20inter-class%20separation%2C%20and%20%283%29%0Aoutlier-outlier%20separation%20to%20maintain%20diversity%20among%20synthetic%20outliers%20and%0Aprevent%20representation%20collapse.%20These%20properties%20work%20together%20to%20ensure%20a%0Amore%20robust%20and%20discriminative%20feature%20space%20for%20anomaly%20detection.%20Our%0Aapproach%20achieves%20approximately%2012x%20faster%20convergence%20than%20NT-Xent%20and%207x%0Afaster%20than%20Rot-SupCon%2C%20with%20superior%20performance.%20On%20CIFAR-10%2C%20it%20delivers%20an%0Aaverage%20performance%20boost%20of%206.2%25%20over%20NT-Xent%20and%202%25%20over%20Rot-SupCon%2C%20with%0Aclass-specific%20improvements%20of%20up%20to%2016.9%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/firm-98B6.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05130v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Compact%2520and%2520Robust%2520Representations%2520for%2520Anomaly%2520Detection%26entry.906535625%3DWillian%2520T.%2520Lunardi%2520and%2520Abdulrahman%2520Banabila%2520and%2520Dania%2520Herzalla%2520and%2520Martin%2520Andreoni%26entry.1292438233%3D%2520%2520Distance-based%2520anomaly%2520detection%2520methods%2520rely%2520on%2520compact%2520and%2520separable%250Ain-distribution%2520%2528ID%2529%2520embeddings%2520to%2520effectively%2520delineate%2520anomaly%2520boundaries.%250ASingle-positive%2520contrastive%2520formulations%2520suffer%2520from%2520class%2520collision%252C%2520promoting%250Aunnecessary%2520intra-class%2520variance%2520within%2520ID%2520samples.%2520While%2520multi-positive%250Aformulations%2520can%2520improve%2520inlier%2520compactness%252C%2520they%2520fail%2520to%2520preserve%2520the%250Adiversity%2520among%2520synthetic%2520outliers.%2520We%2520address%2520these%2520limitations%2520by%2520proposing%2520a%250Acontrastive%2520pretext%2520task%2520for%2520anomaly%2520detection%2520that%2520enforces%2520three%2520key%250Aproperties%253A%2520%25281%2529%2520compact%2520ID%2520clustering%2520to%2520reduce%2520intra-class%2520variance%252C%2520%25282%2529%250Ainlier-outlier%2520separation%2520to%2520enhance%2520inter-class%2520separation%252C%2520and%2520%25283%2529%250Aoutlier-outlier%2520separation%2520to%2520maintain%2520diversity%2520among%2520synthetic%2520outliers%2520and%250Aprevent%2520representation%2520collapse.%2520These%2520properties%2520work%2520together%2520to%2520ensure%2520a%250Amore%2520robust%2520and%2520discriminative%2520feature%2520space%2520for%2520anomaly%2520detection.%2520Our%250Aapproach%2520achieves%2520approximately%252012x%2520faster%2520convergence%2520than%2520NT-Xent%2520and%25207x%250Afaster%2520than%2520Rot-SupCon%252C%2520with%2520superior%2520performance.%2520On%2520CIFAR-10%252C%2520it%2520delivers%2520an%250Aaverage%2520performance%2520boost%2520of%25206.2%2525%2520over%2520NT-Xent%2520and%25202%2525%2520over%2520Rot-SupCon%252C%2520with%250Aclass-specific%2520improvements%2520of%2520up%2520to%252016.9%2525.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/firm-98B6.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05130v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Compact%20and%20Robust%20Representations%20for%20Anomaly%20Detection&entry.906535625=Willian%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20Andreoni&entry.1292438233=%20%20Distance-based%20anomaly%20detection%20methods%20rely%20on%20compact%20and%20separable%0Ain-distribution%20%28ID%29%20embeddings%20to%20effectively%20delineate%20anomaly%20boundaries.%0ASingle-positive%20contrastive%20formulations%20suffer%20from%20class%20collision%2C%20promoting%0Aunnecessary%20intra-class%20variance%20within%20ID%20samples.%20While%20multi-positive%0Aformulations%20can%20improve%20inlier%20compactness%2C%20they%20fail%20to%20preserve%20the%0Adiversity%20among%20synthetic%20outliers.%20We%20address%20these%20limitations%20by%20proposing%20a%0Acontrastive%20pretext%20task%20for%20anomaly%20detection%20that%20enforces%20three%20key%0Aproperties%3A%20%281%29%20compact%20ID%20clustering%20to%20reduce%20intra-class%20variance%2C%20%282%29%0Ainlier-outlier%20separation%20to%20enhance%20inter-class%20separation%2C%20and%20%283%29%0Aoutlier-outlier%20separation%20to%20maintain%20diversity%20among%20synthetic%20outliers%20and%0Aprevent%20representation%20collapse.%20These%20properties%20work%20together%20to%20ensure%20a%0Amore%20robust%20and%20discriminative%20feature%20space%20for%20anomaly%20detection.%20Our%0Aapproach%20achieves%20approximately%2012x%20faster%20convergence%20than%20NT-Xent%20and%207x%0Afaster%20than%20Rot-SupCon%2C%20with%20superior%20performance.%20On%20CIFAR-10%2C%20it%20delivers%20an%0Aaverage%20performance%20boost%20of%206.2%25%20over%20NT-Xent%20and%202%25%20over%20Rot-SupCon%2C%20with%0Aclass-specific%20improvements%20of%20up%20to%2016.9%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/firm-98B6.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05130v4&entry.124074799=Read"},
{"title": "Particle Trajectory Representation Learning with Masked Point Modeling", "author": "Sam Young and Yeon-jae Jwa and Kazuhiro Terao", "abstract": "  Effective self-supervised learning (SSL) techniques have been key to\nunlocking large datasets for representation learning. While many promising\nmethods have been developed using online corpora and captioned photographs,\ntheir application to scientific domains, where data encodes highly specialized\nknowledge, remains in its early stages. We present a self-supervised masked\nmodeling framework for 3D particle trajectory analysis in Time Projection\nChambers (TPCs). These detectors produce globally sparse (<1% occupancy) but\nlocally dense point clouds, capturing meter-scale particle trajectories at\nmillimeter resolution. Starting with PointMAE, this work proposes volumetric\ntokenization to group sparse ionization points into resolution-agnostic\npatches, as well as an auxiliary energy infilling task to improve trajectory\nsemantics. This approach -- which we call Point-based Liquid Argon Masked\nAutoencoder (PoLAr-MAE) -- achieves 99.4% track and 97.7% shower classification\nF-scores, matching that of supervised baselines without any labeled data. While\nthe model learns rich particle trajectory representations, it struggles with\nsub-token phenomena like overlapping or short-lived particle trajectories. To\nsupport further research, we release PILArNet-M -- the largest open LArTPC\ndataset (1M+ events, 5.2B labeled points) -- to advance SSL in high energy\nphysics (HEP). Project site: https://youngsm.com/polarmae/\n", "link": "http://arxiv.org/abs/2502.02558v1", "date": "2025-02-04", "relevancy": 2.6056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particle%20Trajectory%20Representation%20Learning%20with%20Masked%20Point%20Modeling&body=Title%3A%20Particle%20Trajectory%20Representation%20Learning%20with%20Masked%20Point%20Modeling%0AAuthor%3A%20Sam%20Young%20and%20Yeon-jae%20Jwa%20and%20Kazuhiro%20Terao%0AAbstract%3A%20%20%20Effective%20self-supervised%20learning%20%28SSL%29%20techniques%20have%20been%20key%20to%0Aunlocking%20large%20datasets%20for%20representation%20learning.%20While%20many%20promising%0Amethods%20have%20been%20developed%20using%20online%20corpora%20and%20captioned%20photographs%2C%0Atheir%20application%20to%20scientific%20domains%2C%20where%20data%20encodes%20highly%20specialized%0Aknowledge%2C%20remains%20in%20its%20early%20stages.%20We%20present%20a%20self-supervised%20masked%0Amodeling%20framework%20for%203D%20particle%20trajectory%20analysis%20in%20Time%20Projection%0AChambers%20%28TPCs%29.%20These%20detectors%20produce%20globally%20sparse%20%28%3C1%25%20occupancy%29%20but%0Alocally%20dense%20point%20clouds%2C%20capturing%20meter-scale%20particle%20trajectories%20at%0Amillimeter%20resolution.%20Starting%20with%20PointMAE%2C%20this%20work%20proposes%20volumetric%0Atokenization%20to%20group%20sparse%20ionization%20points%20into%20resolution-agnostic%0Apatches%2C%20as%20well%20as%20an%20auxiliary%20energy%20infilling%20task%20to%20improve%20trajectory%0Asemantics.%20This%20approach%20--%20which%20we%20call%20Point-based%20Liquid%20Argon%20Masked%0AAutoencoder%20%28PoLAr-MAE%29%20--%20achieves%2099.4%25%20track%20and%2097.7%25%20shower%20classification%0AF-scores%2C%20matching%20that%20of%20supervised%20baselines%20without%20any%20labeled%20data.%20While%0Athe%20model%20learns%20rich%20particle%20trajectory%20representations%2C%20it%20struggles%20with%0Asub-token%20phenomena%20like%20overlapping%20or%20short-lived%20particle%20trajectories.%20To%0Asupport%20further%20research%2C%20we%20release%20PILArNet-M%20--%20the%20largest%20open%20LArTPC%0Adataset%20%281M%2B%20events%2C%205.2B%20labeled%20points%29%20--%20to%20advance%20SSL%20in%20high%20energy%0Aphysics%20%28HEP%29.%20Project%20site%3A%20https%3A//youngsm.com/polarmae/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticle%2520Trajectory%2520Representation%2520Learning%2520with%2520Masked%2520Point%2520Modeling%26entry.906535625%3DSam%2520Young%2520and%2520Yeon-jae%2520Jwa%2520and%2520Kazuhiro%2520Terao%26entry.1292438233%3D%2520%2520Effective%2520self-supervised%2520learning%2520%2528SSL%2529%2520techniques%2520have%2520been%2520key%2520to%250Aunlocking%2520large%2520datasets%2520for%2520representation%2520learning.%2520While%2520many%2520promising%250Amethods%2520have%2520been%2520developed%2520using%2520online%2520corpora%2520and%2520captioned%2520photographs%252C%250Atheir%2520application%2520to%2520scientific%2520domains%252C%2520where%2520data%2520encodes%2520highly%2520specialized%250Aknowledge%252C%2520remains%2520in%2520its%2520early%2520stages.%2520We%2520present%2520a%2520self-supervised%2520masked%250Amodeling%2520framework%2520for%25203D%2520particle%2520trajectory%2520analysis%2520in%2520Time%2520Projection%250AChambers%2520%2528TPCs%2529.%2520These%2520detectors%2520produce%2520globally%2520sparse%2520%2528%253C1%2525%2520occupancy%2529%2520but%250Alocally%2520dense%2520point%2520clouds%252C%2520capturing%2520meter-scale%2520particle%2520trajectories%2520at%250Amillimeter%2520resolution.%2520Starting%2520with%2520PointMAE%252C%2520this%2520work%2520proposes%2520volumetric%250Atokenization%2520to%2520group%2520sparse%2520ionization%2520points%2520into%2520resolution-agnostic%250Apatches%252C%2520as%2520well%2520as%2520an%2520auxiliary%2520energy%2520infilling%2520task%2520to%2520improve%2520trajectory%250Asemantics.%2520This%2520approach%2520--%2520which%2520we%2520call%2520Point-based%2520Liquid%2520Argon%2520Masked%250AAutoencoder%2520%2528PoLAr-MAE%2529%2520--%2520achieves%252099.4%2525%2520track%2520and%252097.7%2525%2520shower%2520classification%250AF-scores%252C%2520matching%2520that%2520of%2520supervised%2520baselines%2520without%2520any%2520labeled%2520data.%2520While%250Athe%2520model%2520learns%2520rich%2520particle%2520trajectory%2520representations%252C%2520it%2520struggles%2520with%250Asub-token%2520phenomena%2520like%2520overlapping%2520or%2520short-lived%2520particle%2520trajectories.%2520To%250Asupport%2520further%2520research%252C%2520we%2520release%2520PILArNet-M%2520--%2520the%2520largest%2520open%2520LArTPC%250Adataset%2520%25281M%252B%2520events%252C%25205.2B%2520labeled%2520points%2529%2520--%2520to%2520advance%2520SSL%2520in%2520high%2520energy%250Aphysics%2520%2528HEP%2529.%2520Project%2520site%253A%2520https%253A//youngsm.com/polarmae/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particle%20Trajectory%20Representation%20Learning%20with%20Masked%20Point%20Modeling&entry.906535625=Sam%20Young%20and%20Yeon-jae%20Jwa%20and%20Kazuhiro%20Terao&entry.1292438233=%20%20Effective%20self-supervised%20learning%20%28SSL%29%20techniques%20have%20been%20key%20to%0Aunlocking%20large%20datasets%20for%20representation%20learning.%20While%20many%20promising%0Amethods%20have%20been%20developed%20using%20online%20corpora%20and%20captioned%20photographs%2C%0Atheir%20application%20to%20scientific%20domains%2C%20where%20data%20encodes%20highly%20specialized%0Aknowledge%2C%20remains%20in%20its%20early%20stages.%20We%20present%20a%20self-supervised%20masked%0Amodeling%20framework%20for%203D%20particle%20trajectory%20analysis%20in%20Time%20Projection%0AChambers%20%28TPCs%29.%20These%20detectors%20produce%20globally%20sparse%20%28%3C1%25%20occupancy%29%20but%0Alocally%20dense%20point%20clouds%2C%20capturing%20meter-scale%20particle%20trajectories%20at%0Amillimeter%20resolution.%20Starting%20with%20PointMAE%2C%20this%20work%20proposes%20volumetric%0Atokenization%20to%20group%20sparse%20ionization%20points%20into%20resolution-agnostic%0Apatches%2C%20as%20well%20as%20an%20auxiliary%20energy%20infilling%20task%20to%20improve%20trajectory%0Asemantics.%20This%20approach%20--%20which%20we%20call%20Point-based%20Liquid%20Argon%20Masked%0AAutoencoder%20%28PoLAr-MAE%29%20--%20achieves%2099.4%25%20track%20and%2097.7%25%20shower%20classification%0AF-scores%2C%20matching%20that%20of%20supervised%20baselines%20without%20any%20labeled%20data.%20While%0Athe%20model%20learns%20rich%20particle%20trajectory%20representations%2C%20it%20struggles%20with%0Asub-token%20phenomena%20like%20overlapping%20or%20short-lived%20particle%20trajectories.%20To%0Asupport%20further%20research%2C%20we%20release%20PILArNet-M%20--%20the%20largest%20open%20LArTPC%0Adataset%20%281M%2B%20events%2C%205.2B%20labeled%20points%29%20--%20to%20advance%20SSL%20in%20high%20energy%0Aphysics%20%28HEP%29.%20Project%20site%3A%20https%3A//youngsm.com/polarmae/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02558v1&entry.124074799=Read"},
{"title": "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object\n  Pose Estimation", "author": "Jian Liu and Wei Sun and Hui Yang and Pengchao Deng and Chongpei Liu and Nicu Sebe and Hossein Rahmani and Ajmal Mian", "abstract": "  Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial\nfor enabling augmented reality and robotic manipulation. Category-level methods\nhave received extensive research attention due to their potential for\ngeneralization to intra-class unknown objects. However, these methods require\nmanual collection and labeling of large-scale real-world training data. To\naddress this problem, we introduce a diffusion-based paradigm for\ndomain-generalized category-level 9-DoF object pose estimation. Our motivation\nis to leverage the latent generalization ability of the diffusion model to\naddress the domain generalization challenge in object pose estimation. This\nentails training the model exclusively on rendered synthetic data to achieve\ngeneralization to real-world scenes. We propose an effective diffusion model to\nredefine 9-DoF object pose estimation from a generative perspective. Our model\ndoes not require any 3D shape priors during training or inference. By employing\nthe Denoising Diffusion Implicit Model, we demonstrate that the reverse\ndiffusion process can be executed in as few as 3 steps, achieving near\nreal-time performance. Finally, we design a robotic grasping system comprising\nboth hardware and software components. Through comprehensive experiments on two\nbenchmark datasets and the real-world robotic system, we show that our method\nachieves state-of-the-art domain generalization performance. Our code will be\nmade public at https://github.com/CNJianLiu/Diff9D.\n", "link": "http://arxiv.org/abs/2502.02525v1", "date": "2025-02-04", "relevancy": 2.599, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6623}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6523}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff9D%3A%20Diffusion-Based%20Domain-Generalized%20Category-Level%209-DoF%20Object%0A%20%20Pose%20Estimation&body=Title%3A%20Diff9D%3A%20Diffusion-Based%20Domain-Generalized%20Category-Level%209-DoF%20Object%0A%20%20Pose%20Estimation%0AAuthor%3A%20Jian%20Liu%20and%20Wei%20Sun%20and%20Hui%20Yang%20and%20Pengchao%20Deng%20and%20Chongpei%20Liu%20and%20Nicu%20Sebe%20and%20Hossein%20Rahmani%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Nine-degrees-of-freedom%20%289-DoF%29%20object%20pose%20and%20size%20estimation%20is%20crucial%0Afor%20enabling%20augmented%20reality%20and%20robotic%20manipulation.%20Category-level%20methods%0Ahave%20received%20extensive%20research%20attention%20due%20to%20their%20potential%20for%0Ageneralization%20to%20intra-class%20unknown%20objects.%20However%2C%20these%20methods%20require%0Amanual%20collection%20and%20labeling%20of%20large-scale%20real-world%20training%20data.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20a%20diffusion-based%20paradigm%20for%0Adomain-generalized%20category-level%209-DoF%20object%20pose%20estimation.%20Our%20motivation%0Ais%20to%20leverage%20the%20latent%20generalization%20ability%20of%20the%20diffusion%20model%20to%0Aaddress%20the%20domain%20generalization%20challenge%20in%20object%20pose%20estimation.%20This%0Aentails%20training%20the%20model%20exclusively%20on%20rendered%20synthetic%20data%20to%20achieve%0Ageneralization%20to%20real-world%20scenes.%20We%20propose%20an%20effective%20diffusion%20model%20to%0Aredefine%209-DoF%20object%20pose%20estimation%20from%20a%20generative%20perspective.%20Our%20model%0Adoes%20not%20require%20any%203D%20shape%20priors%20during%20training%20or%20inference.%20By%20employing%0Athe%20Denoising%20Diffusion%20Implicit%20Model%2C%20we%20demonstrate%20that%20the%20reverse%0Adiffusion%20process%20can%20be%20executed%20in%20as%20few%20as%203%20steps%2C%20achieving%20near%0Areal-time%20performance.%20Finally%2C%20we%20design%20a%20robotic%20grasping%20system%20comprising%0Aboth%20hardware%20and%20software%20components.%20Through%20comprehensive%20experiments%20on%20two%0Abenchmark%20datasets%20and%20the%20real-world%20robotic%20system%2C%20we%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20domain%20generalization%20performance.%20Our%20code%20will%20be%0Amade%20public%20at%20https%3A//github.com/CNJianLiu/Diff9D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff9D%253A%2520Diffusion-Based%2520Domain-Generalized%2520Category-Level%25209-DoF%2520Object%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DJian%2520Liu%2520and%2520Wei%2520Sun%2520and%2520Hui%2520Yang%2520and%2520Pengchao%2520Deng%2520and%2520Chongpei%2520Liu%2520and%2520Nicu%2520Sebe%2520and%2520Hossein%2520Rahmani%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Nine-degrees-of-freedom%2520%25289-DoF%2529%2520object%2520pose%2520and%2520size%2520estimation%2520is%2520crucial%250Afor%2520enabling%2520augmented%2520reality%2520and%2520robotic%2520manipulation.%2520Category-level%2520methods%250Ahave%2520received%2520extensive%2520research%2520attention%2520due%2520to%2520their%2520potential%2520for%250Ageneralization%2520to%2520intra-class%2520unknown%2520objects.%2520However%252C%2520these%2520methods%2520require%250Amanual%2520collection%2520and%2520labeling%2520of%2520large-scale%2520real-world%2520training%2520data.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520introduce%2520a%2520diffusion-based%2520paradigm%2520for%250Adomain-generalized%2520category-level%25209-DoF%2520object%2520pose%2520estimation.%2520Our%2520motivation%250Ais%2520to%2520leverage%2520the%2520latent%2520generalization%2520ability%2520of%2520the%2520diffusion%2520model%2520to%250Aaddress%2520the%2520domain%2520generalization%2520challenge%2520in%2520object%2520pose%2520estimation.%2520This%250Aentails%2520training%2520the%2520model%2520exclusively%2520on%2520rendered%2520synthetic%2520data%2520to%2520achieve%250Ageneralization%2520to%2520real-world%2520scenes.%2520We%2520propose%2520an%2520effective%2520diffusion%2520model%2520to%250Aredefine%25209-DoF%2520object%2520pose%2520estimation%2520from%2520a%2520generative%2520perspective.%2520Our%2520model%250Adoes%2520not%2520require%2520any%25203D%2520shape%2520priors%2520during%2520training%2520or%2520inference.%2520By%2520employing%250Athe%2520Denoising%2520Diffusion%2520Implicit%2520Model%252C%2520we%2520demonstrate%2520that%2520the%2520reverse%250Adiffusion%2520process%2520can%2520be%2520executed%2520in%2520as%2520few%2520as%25203%2520steps%252C%2520achieving%2520near%250Areal-time%2520performance.%2520Finally%252C%2520we%2520design%2520a%2520robotic%2520grasping%2520system%2520comprising%250Aboth%2520hardware%2520and%2520software%2520components.%2520Through%2520comprehensive%2520experiments%2520on%2520two%250Abenchmark%2520datasets%2520and%2520the%2520real-world%2520robotic%2520system%252C%2520we%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520domain%2520generalization%2520performance.%2520Our%2520code%2520will%2520be%250Amade%2520public%2520at%2520https%253A//github.com/CNJianLiu/Diff9D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff9D%3A%20Diffusion-Based%20Domain-Generalized%20Category-Level%209-DoF%20Object%0A%20%20Pose%20Estimation&entry.906535625=Jian%20Liu%20and%20Wei%20Sun%20and%20Hui%20Yang%20and%20Pengchao%20Deng%20and%20Chongpei%20Liu%20and%20Nicu%20Sebe%20and%20Hossein%20Rahmani%20and%20Ajmal%20Mian&entry.1292438233=%20%20Nine-degrees-of-freedom%20%289-DoF%29%20object%20pose%20and%20size%20estimation%20is%20crucial%0Afor%20enabling%20augmented%20reality%20and%20robotic%20manipulation.%20Category-level%20methods%0Ahave%20received%20extensive%20research%20attention%20due%20to%20their%20potential%20for%0Ageneralization%20to%20intra-class%20unknown%20objects.%20However%2C%20these%20methods%20require%0Amanual%20collection%20and%20labeling%20of%20large-scale%20real-world%20training%20data.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20a%20diffusion-based%20paradigm%20for%0Adomain-generalized%20category-level%209-DoF%20object%20pose%20estimation.%20Our%20motivation%0Ais%20to%20leverage%20the%20latent%20generalization%20ability%20of%20the%20diffusion%20model%20to%0Aaddress%20the%20domain%20generalization%20challenge%20in%20object%20pose%20estimation.%20This%0Aentails%20training%20the%20model%20exclusively%20on%20rendered%20synthetic%20data%20to%20achieve%0Ageneralization%20to%20real-world%20scenes.%20We%20propose%20an%20effective%20diffusion%20model%20to%0Aredefine%209-DoF%20object%20pose%20estimation%20from%20a%20generative%20perspective.%20Our%20model%0Adoes%20not%20require%20any%203D%20shape%20priors%20during%20training%20or%20inference.%20By%20employing%0Athe%20Denoising%20Diffusion%20Implicit%20Model%2C%20we%20demonstrate%20that%20the%20reverse%0Adiffusion%20process%20can%20be%20executed%20in%20as%20few%20as%203%20steps%2C%20achieving%20near%0Areal-time%20performance.%20Finally%2C%20we%20design%20a%20robotic%20grasping%20system%20comprising%0Aboth%20hardware%20and%20software%20components.%20Through%20comprehensive%20experiments%20on%20two%0Abenchmark%20datasets%20and%20the%20real-world%20robotic%20system%2C%20we%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20domain%20generalization%20performance.%20Our%20code%20will%20be%0Amade%20public%20at%20https%3A//github.com/CNJianLiu/Diff9D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02525v1&entry.124074799=Read"},
{"title": "Autoencoder-based General Purpose Representation Learning for Customer\n  Embedding", "author": "Jan Henrik Bertrand and David B. Hoffmann and Jacopo Pio Gargano and Laurent Mombaerts and Jonathan Taws", "abstract": "  Recent advances in representation learning have successfully leveraged the\nunderlying domain-specific structure of data across various fields. However,\nrepresenting diverse and complex entities stored in tabular format within a\nlatent space remains challenging. In this paper, we introduce DEEPCAE, a novel\nmethod for calculating the regularization term for multi-layer contractive\nautoencoders (CAEs). Additionally, we formalize a general-purpose entity\nembedding framework and use it to empirically show that DEEPCAE outperforms all\nother tested autoencoder variants in both reconstruction performance and\ndownstream prediction performance. Notably, when compared to a stacked CAE\nacross 13 datasets, DEEPCAE achieves a 34% improvement in reconstruction error.\n", "link": "http://arxiv.org/abs/2402.18164v2", "date": "2025-02-04", "relevancy": 2.582, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5979}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoder-based%20General%20Purpose%20Representation%20Learning%20for%20Customer%0A%20%20Embedding&body=Title%3A%20Autoencoder-based%20General%20Purpose%20Representation%20Learning%20for%20Customer%0A%20%20Embedding%0AAuthor%3A%20Jan%20Henrik%20Bertrand%20and%20David%20B.%20Hoffmann%20and%20Jacopo%20Pio%20Gargano%20and%20Laurent%20Mombaerts%20and%20Jonathan%20Taws%0AAbstract%3A%20%20%20Recent%20advances%20in%20representation%20learning%20have%20successfully%20leveraged%20the%0Aunderlying%20domain-specific%20structure%20of%20data%20across%20various%20fields.%20However%2C%0Arepresenting%20diverse%20and%20complex%20entities%20stored%20in%20tabular%20format%20within%20a%0Alatent%20space%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20DEEPCAE%2C%20a%20novel%0Amethod%20for%20calculating%20the%20regularization%20term%20for%20multi-layer%20contractive%0Aautoencoders%20%28CAEs%29.%20Additionally%2C%20we%20formalize%20a%20general-purpose%20entity%0Aembedding%20framework%20and%20use%20it%20to%20empirically%20show%20that%20DEEPCAE%20outperforms%20all%0Aother%20tested%20autoencoder%20variants%20in%20both%20reconstruction%20performance%20and%0Adownstream%20prediction%20performance.%20Notably%2C%20when%20compared%20to%20a%20stacked%20CAE%0Aacross%2013%20datasets%2C%20DEEPCAE%20achieves%20a%2034%25%20improvement%20in%20reconstruction%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoder-based%2520General%2520Purpose%2520Representation%2520Learning%2520for%2520Customer%250A%2520%2520Embedding%26entry.906535625%3DJan%2520Henrik%2520Bertrand%2520and%2520David%2520B.%2520Hoffmann%2520and%2520Jacopo%2520Pio%2520Gargano%2520and%2520Laurent%2520Mombaerts%2520and%2520Jonathan%2520Taws%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520representation%2520learning%2520have%2520successfully%2520leveraged%2520the%250Aunderlying%2520domain-specific%2520structure%2520of%2520data%2520across%2520various%2520fields.%2520However%252C%250Arepresenting%2520diverse%2520and%2520complex%2520entities%2520stored%2520in%2520tabular%2520format%2520within%2520a%250Alatent%2520space%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DEEPCAE%252C%2520a%2520novel%250Amethod%2520for%2520calculating%2520the%2520regularization%2520term%2520for%2520multi-layer%2520contractive%250Aautoencoders%2520%2528CAEs%2529.%2520Additionally%252C%2520we%2520formalize%2520a%2520general-purpose%2520entity%250Aembedding%2520framework%2520and%2520use%2520it%2520to%2520empirically%2520show%2520that%2520DEEPCAE%2520outperforms%2520all%250Aother%2520tested%2520autoencoder%2520variants%2520in%2520both%2520reconstruction%2520performance%2520and%250Adownstream%2520prediction%2520performance.%2520Notably%252C%2520when%2520compared%2520to%2520a%2520stacked%2520CAE%250Aacross%252013%2520datasets%252C%2520DEEPCAE%2520achieves%2520a%252034%2525%2520improvement%2520in%2520reconstruction%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoder-based%20General%20Purpose%20Representation%20Learning%20for%20Customer%0A%20%20Embedding&entry.906535625=Jan%20Henrik%20Bertrand%20and%20David%20B.%20Hoffmann%20and%20Jacopo%20Pio%20Gargano%20and%20Laurent%20Mombaerts%20and%20Jonathan%20Taws&entry.1292438233=%20%20Recent%20advances%20in%20representation%20learning%20have%20successfully%20leveraged%20the%0Aunderlying%20domain-specific%20structure%20of%20data%20across%20various%20fields.%20However%2C%0Arepresenting%20diverse%20and%20complex%20entities%20stored%20in%20tabular%20format%20within%20a%0Alatent%20space%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20DEEPCAE%2C%20a%20novel%0Amethod%20for%20calculating%20the%20regularization%20term%20for%20multi-layer%20contractive%0Aautoencoders%20%28CAEs%29.%20Additionally%2C%20we%20formalize%20a%20general-purpose%20entity%0Aembedding%20framework%20and%20use%20it%20to%20empirically%20show%20that%20DEEPCAE%20outperforms%20all%0Aother%20tested%20autoencoder%20variants%20in%20both%20reconstruction%20performance%20and%0Adownstream%20prediction%20performance.%20Notably%2C%20when%20compared%20to%20a%20stacked%20CAE%0Aacross%2013%20datasets%2C%20DEEPCAE%20achieves%20a%2034%25%20improvement%20in%20reconstruction%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18164v2&entry.124074799=Read"},
{"title": "Extending SEEDS to a Supervoxel Algorithm for Medical Image Analysis", "author": "Chenhui Zhao and Yan Jiang and Todd C. Hollon", "abstract": "  In this work, we extend the SEEDS superpixel algorithm from 2D images to 3D\nvolumes, resulting in 3D SEEDS, a faster, better, and open-source supervoxel\nalgorithm for medical image analysis. We compare 3D SEEDS with the widely used\nsupervoxel algorithm SLIC on 13 segmentation tasks across 10 organs. 3D SEEDS\naccelerates supervoxel generation by a factor of 10, improves the achievable\nDice score by +6.5%, and reduces the under-segmentation error by -0.16%. The\ncode is available at https://github.com/Zch0414/3d_seeds\n", "link": "http://arxiv.org/abs/2502.02409v1", "date": "2025-02-04", "relevancy": 2.5726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20SEEDS%20to%20a%20Supervoxel%20Algorithm%20for%20Medical%20Image%20Analysis&body=Title%3A%20Extending%20SEEDS%20to%20a%20Supervoxel%20Algorithm%20for%20Medical%20Image%20Analysis%0AAuthor%3A%20Chenhui%20Zhao%20and%20Yan%20Jiang%20and%20Todd%20C.%20Hollon%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20extend%20the%20SEEDS%20superpixel%20algorithm%20from%202D%20images%20to%203D%0Avolumes%2C%20resulting%20in%203D%20SEEDS%2C%20a%20faster%2C%20better%2C%20and%20open-source%20supervoxel%0Aalgorithm%20for%20medical%20image%20analysis.%20We%20compare%203D%20SEEDS%20with%20the%20widely%20used%0Asupervoxel%20algorithm%20SLIC%20on%2013%20segmentation%20tasks%20across%2010%20organs.%203D%20SEEDS%0Aaccelerates%20supervoxel%20generation%20by%20a%20factor%20of%2010%2C%20improves%20the%20achievable%0ADice%20score%20by%20%2B6.5%25%2C%20and%20reduces%20the%20under-segmentation%20error%20by%20-0.16%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Zch0414/3d_seeds%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520SEEDS%2520to%2520a%2520Supervoxel%2520Algorithm%2520for%2520Medical%2520Image%2520Analysis%26entry.906535625%3DChenhui%2520Zhao%2520and%2520Yan%2520Jiang%2520and%2520Todd%2520C.%2520Hollon%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520SEEDS%2520superpixel%2520algorithm%2520from%25202D%2520images%2520to%25203D%250Avolumes%252C%2520resulting%2520in%25203D%2520SEEDS%252C%2520a%2520faster%252C%2520better%252C%2520and%2520open-source%2520supervoxel%250Aalgorithm%2520for%2520medical%2520image%2520analysis.%2520We%2520compare%25203D%2520SEEDS%2520with%2520the%2520widely%2520used%250Asupervoxel%2520algorithm%2520SLIC%2520on%252013%2520segmentation%2520tasks%2520across%252010%2520organs.%25203D%2520SEEDS%250Aaccelerates%2520supervoxel%2520generation%2520by%2520a%2520factor%2520of%252010%252C%2520improves%2520the%2520achievable%250ADice%2520score%2520by%2520%252B6.5%2525%252C%2520and%2520reduces%2520the%2520under-segmentation%2520error%2520by%2520-0.16%2525.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Zch0414/3d_seeds%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20SEEDS%20to%20a%20Supervoxel%20Algorithm%20for%20Medical%20Image%20Analysis&entry.906535625=Chenhui%20Zhao%20and%20Yan%20Jiang%20and%20Todd%20C.%20Hollon&entry.1292438233=%20%20In%20this%20work%2C%20we%20extend%20the%20SEEDS%20superpixel%20algorithm%20from%202D%20images%20to%203D%0Avolumes%2C%20resulting%20in%203D%20SEEDS%2C%20a%20faster%2C%20better%2C%20and%20open-source%20supervoxel%0Aalgorithm%20for%20medical%20image%20analysis.%20We%20compare%203D%20SEEDS%20with%20the%20widely%20used%0Asupervoxel%20algorithm%20SLIC%20on%2013%20segmentation%20tasks%20across%2010%20organs.%203D%20SEEDS%0Aaccelerates%20supervoxel%20generation%20by%20a%20factor%20of%2010%2C%20improves%20the%20achievable%0ADice%20score%20by%20%2B6.5%25%2C%20and%20reduces%20the%20under-segmentation%20error%20by%20-0.16%25.%20The%0Acode%20is%20available%20at%20https%3A//github.com/Zch0414/3d_seeds%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02409v1&entry.124074799=Read"},
{"title": "ERASMO: Leveraging Large Language Models for Enhanced Clustering\n  Segmentation", "author": "Fillipe dos Santos Silva and Gabriel Kenzo Kakimoto and Julio Cesar dos Reis and Marcelo S. Reis", "abstract": "  Cluster analysis plays a crucial role in various domains and applications,\nsuch as customer segmentation in marketing. These contexts often involve\nmultimodal data, including both tabular and textual datasets, making it\nchallenging to represent hidden patterns for obtaining meaningful clusters.\nThis study introduces ERASMO, a framework designed to fine-tune a pretrained\nlanguage model on textually encoded tabular data and generate embeddings from\nthe fine-tuned model. ERASMO employs a textual converter to transform tabular\ndata into a textual format, enabling the language model to process and\nunderstand the data more effectively. Additionally, ERASMO produces\ncontextually rich and structurally representative embeddings through techniques\nsuch as random feature sequence shuffling and number verbalization. Extensive\nexperimental evaluations were conducted using multiple datasets and baseline\napproaches. Our results demonstrate that ERASMO fully leverages the specific\ncontext of each tabular dataset, leading to more precise and nuanced embeddings\nfor accurate clustering. This approach enhances clustering performance by\ncapturing complex relationship patterns within diverse tabular data.\n", "link": "http://arxiv.org/abs/2410.03738v2", "date": "2025-02-04", "relevancy": 2.5332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERASMO%3A%20Leveraging%20Large%20Language%20Models%20for%20Enhanced%20Clustering%0A%20%20Segmentation&body=Title%3A%20ERASMO%3A%20Leveraging%20Large%20Language%20Models%20for%20Enhanced%20Clustering%0A%20%20Segmentation%0AAuthor%3A%20Fillipe%20dos%20Santos%20Silva%20and%20Gabriel%20Kenzo%20Kakimoto%20and%20Julio%20Cesar%20dos%20Reis%20and%20Marcelo%20S.%20Reis%0AAbstract%3A%20%20%20Cluster%20analysis%20plays%20a%20crucial%20role%20in%20various%20domains%20and%20applications%2C%0Asuch%20as%20customer%20segmentation%20in%20marketing.%20These%20contexts%20often%20involve%0Amultimodal%20data%2C%20including%20both%20tabular%20and%20textual%20datasets%2C%20making%20it%0Achallenging%20to%20represent%20hidden%20patterns%20for%20obtaining%20meaningful%20clusters.%0AThis%20study%20introduces%20ERASMO%2C%20a%20framework%20designed%20to%20fine-tune%20a%20pretrained%0Alanguage%20model%20on%20textually%20encoded%20tabular%20data%20and%20generate%20embeddings%20from%0Athe%20fine-tuned%20model.%20ERASMO%20employs%20a%20textual%20converter%20to%20transform%20tabular%0Adata%20into%20a%20textual%20format%2C%20enabling%20the%20language%20model%20to%20process%20and%0Aunderstand%20the%20data%20more%20effectively.%20Additionally%2C%20ERASMO%20produces%0Acontextually%20rich%20and%20structurally%20representative%20embeddings%20through%20techniques%0Asuch%20as%20random%20feature%20sequence%20shuffling%20and%20number%20verbalization.%20Extensive%0Aexperimental%20evaluations%20were%20conducted%20using%20multiple%20datasets%20and%20baseline%0Aapproaches.%20Our%20results%20demonstrate%20that%20ERASMO%20fully%20leverages%20the%20specific%0Acontext%20of%20each%20tabular%20dataset%2C%20leading%20to%20more%20precise%20and%20nuanced%20embeddings%0Afor%20accurate%20clustering.%20This%20approach%20enhances%20clustering%20performance%20by%0Acapturing%20complex%20relationship%20patterns%20within%20diverse%20tabular%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERASMO%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Enhanced%2520Clustering%250A%2520%2520Segmentation%26entry.906535625%3DFillipe%2520dos%2520Santos%2520Silva%2520and%2520Gabriel%2520Kenzo%2520Kakimoto%2520and%2520Julio%2520Cesar%2520dos%2520Reis%2520and%2520Marcelo%2520S.%2520Reis%26entry.1292438233%3D%2520%2520Cluster%2520analysis%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520domains%2520and%2520applications%252C%250Asuch%2520as%2520customer%2520segmentation%2520in%2520marketing.%2520These%2520contexts%2520often%2520involve%250Amultimodal%2520data%252C%2520including%2520both%2520tabular%2520and%2520textual%2520datasets%252C%2520making%2520it%250Achallenging%2520to%2520represent%2520hidden%2520patterns%2520for%2520obtaining%2520meaningful%2520clusters.%250AThis%2520study%2520introduces%2520ERASMO%252C%2520a%2520framework%2520designed%2520to%2520fine-tune%2520a%2520pretrained%250Alanguage%2520model%2520on%2520textually%2520encoded%2520tabular%2520data%2520and%2520generate%2520embeddings%2520from%250Athe%2520fine-tuned%2520model.%2520ERASMO%2520employs%2520a%2520textual%2520converter%2520to%2520transform%2520tabular%250Adata%2520into%2520a%2520textual%2520format%252C%2520enabling%2520the%2520language%2520model%2520to%2520process%2520and%250Aunderstand%2520the%2520data%2520more%2520effectively.%2520Additionally%252C%2520ERASMO%2520produces%250Acontextually%2520rich%2520and%2520structurally%2520representative%2520embeddings%2520through%2520techniques%250Asuch%2520as%2520random%2520feature%2520sequence%2520shuffling%2520and%2520number%2520verbalization.%2520Extensive%250Aexperimental%2520evaluations%2520were%2520conducted%2520using%2520multiple%2520datasets%2520and%2520baseline%250Aapproaches.%2520Our%2520results%2520demonstrate%2520that%2520ERASMO%2520fully%2520leverages%2520the%2520specific%250Acontext%2520of%2520each%2520tabular%2520dataset%252C%2520leading%2520to%2520more%2520precise%2520and%2520nuanced%2520embeddings%250Afor%2520accurate%2520clustering.%2520This%2520approach%2520enhances%2520clustering%2520performance%2520by%250Acapturing%2520complex%2520relationship%2520patterns%2520within%2520diverse%2520tabular%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERASMO%3A%20Leveraging%20Large%20Language%20Models%20for%20Enhanced%20Clustering%0A%20%20Segmentation&entry.906535625=Fillipe%20dos%20Santos%20Silva%20and%20Gabriel%20Kenzo%20Kakimoto%20and%20Julio%20Cesar%20dos%20Reis%20and%20Marcelo%20S.%20Reis&entry.1292438233=%20%20Cluster%20analysis%20plays%20a%20crucial%20role%20in%20various%20domains%20and%20applications%2C%0Asuch%20as%20customer%20segmentation%20in%20marketing.%20These%20contexts%20often%20involve%0Amultimodal%20data%2C%20including%20both%20tabular%20and%20textual%20datasets%2C%20making%20it%0Achallenging%20to%20represent%20hidden%20patterns%20for%20obtaining%20meaningful%20clusters.%0AThis%20study%20introduces%20ERASMO%2C%20a%20framework%20designed%20to%20fine-tune%20a%20pretrained%0Alanguage%20model%20on%20textually%20encoded%20tabular%20data%20and%20generate%20embeddings%20from%0Athe%20fine-tuned%20model.%20ERASMO%20employs%20a%20textual%20converter%20to%20transform%20tabular%0Adata%20into%20a%20textual%20format%2C%20enabling%20the%20language%20model%20to%20process%20and%0Aunderstand%20the%20data%20more%20effectively.%20Additionally%2C%20ERASMO%20produces%0Acontextually%20rich%20and%20structurally%20representative%20embeddings%20through%20techniques%0Asuch%20as%20random%20feature%20sequence%20shuffling%20and%20number%20verbalization.%20Extensive%0Aexperimental%20evaluations%20were%20conducted%20using%20multiple%20datasets%20and%20baseline%0Aapproaches.%20Our%20results%20demonstrate%20that%20ERASMO%20fully%20leverages%20the%20specific%0Acontext%20of%20each%20tabular%20dataset%2C%20leading%20to%20more%20precise%20and%20nuanced%20embeddings%0Afor%20accurate%20clustering.%20This%20approach%20enhances%20clustering%20performance%20by%0Acapturing%20complex%20relationship%20patterns%20within%20diverse%20tabular%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03738v2&entry.124074799=Read"},
{"title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search", "author": "Andr\u00e9 Schidler and Stefan Szeider", "abstract": "  Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.\n", "link": "http://arxiv.org/abs/2501.14630v2", "date": "2025-02-04", "relevancy": 2.5313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Problem%20Structure%20with%20LLMs%20for%20Optimized%20SAT%20Local%20Search&body=Title%3A%20Extracting%20Problem%20Structure%20with%20LLMs%20for%20Optimized%20SAT%20Local%20Search%0AAuthor%3A%20Andr%C3%A9%20Schidler%20and%20Stefan%20Szeider%0AAbstract%3A%20%20%20Local%20search%20preprocessing%20makes%20Conflict-Driven%20Clause%20Learning%20%28CDCL%29%0Asolvers%20faster%20by%20providing%20high-quality%20starting%20points%20and%20modern%20SAT%20solvers%0Ahave%20incorporated%20this%20technique%20into%20their%20preprocessing%20steps.%20However%2C%20these%0Atools%20rely%20on%20basic%20strategies%20that%20miss%20the%20structural%20patterns%20in%20problems.%0AWe%20present%20a%20method%20that%20applies%20Large%20Language%20Models%20%28LLMs%29%20to%20analyze%0APython-based%20encoding%20code.%20This%20reveals%20hidden%20structural%20patterns%20in%20how%0Aproblems%20convert%20into%20SAT.%20Our%20method%20automatically%20generates%20specialized%20local%0Asearch%20algorithms%20that%20find%20these%20patterns%20and%20use%20them%20to%20create%20strong%0Ainitial%20assignments.%20This%20works%20for%20any%20problem%20instance%20from%20the%20same%20encoding%0Atype.%20Our%20tests%20show%20encouraging%20results%2C%20achieving%20faster%20solving%20times%0Acompared%20to%20baseline%20preprocessing%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Problem%2520Structure%2520with%2520LLMs%2520for%2520Optimized%2520SAT%2520Local%2520Search%26entry.906535625%3DAndr%25C3%25A9%2520Schidler%2520and%2520Stefan%2520Szeider%26entry.1292438233%3D%2520%2520Local%2520search%2520preprocessing%2520makes%2520Conflict-Driven%2520Clause%2520Learning%2520%2528CDCL%2529%250Asolvers%2520faster%2520by%2520providing%2520high-quality%2520starting%2520points%2520and%2520modern%2520SAT%2520solvers%250Ahave%2520incorporated%2520this%2520technique%2520into%2520their%2520preprocessing%2520steps.%2520However%252C%2520these%250Atools%2520rely%2520on%2520basic%2520strategies%2520that%2520miss%2520the%2520structural%2520patterns%2520in%2520problems.%250AWe%2520present%2520a%2520method%2520that%2520applies%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520analyze%250APython-based%2520encoding%2520code.%2520This%2520reveals%2520hidden%2520structural%2520patterns%2520in%2520how%250Aproblems%2520convert%2520into%2520SAT.%2520Our%2520method%2520automatically%2520generates%2520specialized%2520local%250Asearch%2520algorithms%2520that%2520find%2520these%2520patterns%2520and%2520use%2520them%2520to%2520create%2520strong%250Ainitial%2520assignments.%2520This%2520works%2520for%2520any%2520problem%2520instance%2520from%2520the%2520same%2520encoding%250Atype.%2520Our%2520tests%2520show%2520encouraging%2520results%252C%2520achieving%2520faster%2520solving%2520times%250Acompared%2520to%2520baseline%2520preprocessing%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Problem%20Structure%20with%20LLMs%20for%20Optimized%20SAT%20Local%20Search&entry.906535625=Andr%C3%A9%20Schidler%20and%20Stefan%20Szeider&entry.1292438233=%20%20Local%20search%20preprocessing%20makes%20Conflict-Driven%20Clause%20Learning%20%28CDCL%29%0Asolvers%20faster%20by%20providing%20high-quality%20starting%20points%20and%20modern%20SAT%20solvers%0Ahave%20incorporated%20this%20technique%20into%20their%20preprocessing%20steps.%20However%2C%20these%0Atools%20rely%20on%20basic%20strategies%20that%20miss%20the%20structural%20patterns%20in%20problems.%0AWe%20present%20a%20method%20that%20applies%20Large%20Language%20Models%20%28LLMs%29%20to%20analyze%0APython-based%20encoding%20code.%20This%20reveals%20hidden%20structural%20patterns%20in%20how%0Aproblems%20convert%20into%20SAT.%20Our%20method%20automatically%20generates%20specialized%20local%0Asearch%20algorithms%20that%20find%20these%20patterns%20and%20use%20them%20to%20create%20strong%0Ainitial%20assignments.%20This%20works%20for%20any%20problem%20instance%20from%20the%20same%20encoding%0Atype.%20Our%20tests%20show%20encouraging%20results%2C%20achieving%20faster%20solving%20times%0Acompared%20to%20baseline%20preprocessing%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14630v2&entry.124074799=Read"},
{"title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding", "author": "Zhengzhuo Xu and Bowen Qu and Yiyan Qi and Sinan Du and Chengjin Xu and Chun Yuan and Jian Guo", "abstract": "  Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.\n", "link": "http://arxiv.org/abs/2409.03277v2", "date": "2025-02-04", "relevancy": 2.5309, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartMoE%3A%20Mixture%20of%20Expert%20Connector%20for%20Advanced%20Chart%20Understanding&body=Title%3A%20ChartMoE%3A%20Mixture%20of%20Expert%20Connector%20for%20Advanced%20Chart%20Understanding%0AAuthor%3A%20Zhengzhuo%20Xu%20and%20Bowen%20Qu%20and%20Yiyan%20Qi%20and%20Sinan%20Du%20and%20Chengjin%20Xu%20and%20Chun%20Yuan%20and%20Jian%20Guo%0AAbstract%3A%20%20%20Automatic%20chart%20understanding%20is%20crucial%20for%20content%20comprehension%20and%0Adocument%20parsing.%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20in%20chart%20understanding%20through%20domain-specific%0Aalignment%20and%20fine-tuning.%20However%2C%20the%20application%20of%20alignment%20training%0Awithin%20the%20chart%20domain%20is%20still%20underexplored.%20To%20address%20this%2C%20we%20propose%0AChartMoE%2C%20which%20employs%20the%20mixture%20of%20expert%20%28MoE%29%20architecture%20to%20replace%20the%0Atraditional%20linear%20projector%20to%20bridge%20the%20modality%20gap.%20Specifically%2C%20we%20train%0Amultiple%20linear%20connectors%20through%20distinct%20alignment%20tasks%2C%20which%20are%20utilized%0Aas%20the%20foundational%20initialization%20parameters%20for%20different%20experts.%0AAdditionally%2C%20we%20introduce%20ChartMoE-Align%2C%20a%20dataset%20with%20over%20900K%0Achart-table-JSON-code%20quadruples%20to%20conduct%20three%20alignment%20tasks%0A%28chart-table/JSON/code%29.%20Combined%20with%20the%20vanilla%20connector%2C%20we%20initialize%0Adifferent%20experts%20in%20four%20distinct%20ways%20and%20adopt%20high-quality%20knowledge%0Alearning%20to%20further%20refine%20the%20MoE%20connector%20and%20LLM%20parameters.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20MoE%20connector%20and%20our%0Ainitialization%20strategy%2C%20e.g.%2C%20ChartMoE%20improves%20the%20accuracy%20of%20the%20previous%0Astate-of-the-art%20from%2080.48%25%20to%2084.64%25%20on%20the%20ChartQA%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartMoE%253A%2520Mixture%2520of%2520Expert%2520Connector%2520for%2520Advanced%2520Chart%2520Understanding%26entry.906535625%3DZhengzhuo%2520Xu%2520and%2520Bowen%2520Qu%2520and%2520Yiyan%2520Qi%2520and%2520Sinan%2520Du%2520and%2520Chengjin%2520Xu%2520and%2520Chun%2520Yuan%2520and%2520Jian%2520Guo%26entry.1292438233%3D%2520%2520Automatic%2520chart%2520understanding%2520is%2520crucial%2520for%2520content%2520comprehension%2520and%250Adocument%2520parsing.%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%250Aremarkable%2520capabilities%2520in%2520chart%2520understanding%2520through%2520domain-specific%250Aalignment%2520and%2520fine-tuning.%2520However%252C%2520the%2520application%2520of%2520alignment%2520training%250Awithin%2520the%2520chart%2520domain%2520is%2520still%2520underexplored.%2520To%2520address%2520this%252C%2520we%2520propose%250AChartMoE%252C%2520which%2520employs%2520the%2520mixture%2520of%2520expert%2520%2528MoE%2529%2520architecture%2520to%2520replace%2520the%250Atraditional%2520linear%2520projector%2520to%2520bridge%2520the%2520modality%2520gap.%2520Specifically%252C%2520we%2520train%250Amultiple%2520linear%2520connectors%2520through%2520distinct%2520alignment%2520tasks%252C%2520which%2520are%2520utilized%250Aas%2520the%2520foundational%2520initialization%2520parameters%2520for%2520different%2520experts.%250AAdditionally%252C%2520we%2520introduce%2520ChartMoE-Align%252C%2520a%2520dataset%2520with%2520over%2520900K%250Achart-table-JSON-code%2520quadruples%2520to%2520conduct%2520three%2520alignment%2520tasks%250A%2528chart-table/JSON/code%2529.%2520Combined%2520with%2520the%2520vanilla%2520connector%252C%2520we%2520initialize%250Adifferent%2520experts%2520in%2520four%2520distinct%2520ways%2520and%2520adopt%2520high-quality%2520knowledge%250Alearning%2520to%2520further%2520refine%2520the%2520MoE%2520connector%2520and%2520LLM%2520parameters.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520MoE%2520connector%2520and%2520our%250Ainitialization%2520strategy%252C%2520e.g.%252C%2520ChartMoE%2520improves%2520the%2520accuracy%2520of%2520the%2520previous%250Astate-of-the-art%2520from%252080.48%2525%2520to%252084.64%2525%2520on%2520the%2520ChartQA%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartMoE%3A%20Mixture%20of%20Expert%20Connector%20for%20Advanced%20Chart%20Understanding&entry.906535625=Zhengzhuo%20Xu%20and%20Bowen%20Qu%20and%20Yiyan%20Qi%20and%20Sinan%20Du%20and%20Chengjin%20Xu%20and%20Chun%20Yuan%20and%20Jian%20Guo&entry.1292438233=%20%20Automatic%20chart%20understanding%20is%20crucial%20for%20content%20comprehension%20and%0Adocument%20parsing.%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20in%20chart%20understanding%20through%20domain-specific%0Aalignment%20and%20fine-tuning.%20However%2C%20the%20application%20of%20alignment%20training%0Awithin%20the%20chart%20domain%20is%20still%20underexplored.%20To%20address%20this%2C%20we%20propose%0AChartMoE%2C%20which%20employs%20the%20mixture%20of%20expert%20%28MoE%29%20architecture%20to%20replace%20the%0Atraditional%20linear%20projector%20to%20bridge%20the%20modality%20gap.%20Specifically%2C%20we%20train%0Amultiple%20linear%20connectors%20through%20distinct%20alignment%20tasks%2C%20which%20are%20utilized%0Aas%20the%20foundational%20initialization%20parameters%20for%20different%20experts.%0AAdditionally%2C%20we%20introduce%20ChartMoE-Align%2C%20a%20dataset%20with%20over%20900K%0Achart-table-JSON-code%20quadruples%20to%20conduct%20three%20alignment%20tasks%0A%28chart-table/JSON/code%29.%20Combined%20with%20the%20vanilla%20connector%2C%20we%20initialize%0Adifferent%20experts%20in%20four%20distinct%20ways%20and%20adopt%20high-quality%20knowledge%0Alearning%20to%20further%20refine%20the%20MoE%20connector%20and%20LLM%20parameters.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20MoE%20connector%20and%20our%0Ainitialization%20strategy%2C%20e.g.%2C%20ChartMoE%20improves%20the%20accuracy%20of%20the%20previous%0Astate-of-the-art%20from%2080.48%25%20to%2084.64%25%20on%20the%20ChartQA%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03277v2&entry.124074799=Read"},
{"title": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm", "author": "Ziyan Guo and Zeyu Hu and Na Zhao and De Wen Soh", "abstract": "  Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion.Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions.In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/.\n", "link": "http://arxiv.org/abs/2502.02358v1", "date": "2025-02-04", "relevancy": 2.5299, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6759}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6109}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionLab%3A%20Unified%20Human%20Motion%20Generation%20and%20Editing%20via%20the%0A%20%20Motion-Condition-Motion%20Paradigm&body=Title%3A%20MotionLab%3A%20Unified%20Human%20Motion%20Generation%20and%20Editing%20via%20the%0A%20%20Motion-Condition-Motion%20Paradigm%0AAuthor%3A%20Ziyan%20Guo%20and%20Zeyu%20Hu%20and%20Na%20Zhao%20and%20De%20Wen%20Soh%0AAbstract%3A%20%20%20Human%20motion%20generation%20and%20editing%20are%20key%20components%20of%20computer%20graphics%0Aand%20vision.%20However%2C%20current%20approaches%20in%20this%20field%20tend%20to%20offer%20isolated%0Asolutions%20tailored%20to%20specific%20tasks%2C%20which%20can%20be%20inefficient%20and%20impractical%0Afor%20real-world%20applications.%20While%20some%20efforts%20have%20aimed%20to%20unify%0Amotion-related%20tasks%2C%20these%20methods%20simply%20use%20different%20modalities%20as%0Aconditions%20to%20guide%20motion%20generation.%20Consequently%2C%20they%20lack%20editing%0Acapabilities%2C%20fine-grained%20control%2C%20and%20fail%20to%20facilitate%20knowledge%20sharing%0Aacross%20tasks.%20To%20address%20these%20limitations%20and%20provide%20a%20versatile%2C%20unified%0Aframework%20capable%20of%20handling%20both%20human%20motion%20generation%20and%20editing%2C%20we%0Aintroduce%20a%20novel%20paradigm%3A%20Motion-Condition-Motion%2C%20which%20enables%20the%20unified%0Aformulation%20of%20diverse%20tasks%20with%20three%20concepts%3A%20source%20motion%2C%20condition%2C%20and%0Atarget%20motion.Based%20on%20this%20paradigm%2C%20we%20propose%20a%20unified%20framework%2C%0AMotionLab%2C%20which%20incorporates%20rectified%20flows%20to%20learn%20the%20mapping%20from%20source%0Amotion%20to%20target%20motion%2C%20guided%20by%20the%20specified%20conditions.In%20MotionLab%2C%20we%0Aintroduce%20the%201%29%20MotionFlow%20Transformer%20to%20enhance%20conditional%20generation%20and%0Aediting%20without%20task-specific%20modules%3B%202%29%20Aligned%20Rotational%20Position%20Encoding%7D%0Ato%20guarantee%20the%20time%20synchronization%20between%20source%20motion%20and%20target%20motion%3B%0A3%29%20Task%20Specified%20Instruction%20Modulation%3B%20and%204%29%20Motion%20Curriculum%20Learning%20for%0Aeffective%20multi-task%20learning%20and%20knowledge%20sharing%20across%20tasks.%20Notably%2C%20our%0AMotionLab%20demonstrates%20promising%20generalization%20capabilities%20and%20inference%0Aefficiency%20across%20multiple%20benchmarks%20for%20human%20motion.%20Our%20code%20and%20additional%0Avideo%20results%20are%20available%20at%3A%20https%3A//diouo.github.io/motionlab.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionLab%253A%2520Unified%2520Human%2520Motion%2520Generation%2520and%2520Editing%2520via%2520the%250A%2520%2520Motion-Condition-Motion%2520Paradigm%26entry.906535625%3DZiyan%2520Guo%2520and%2520Zeyu%2520Hu%2520and%2520Na%2520Zhao%2520and%2520De%2520Wen%2520Soh%26entry.1292438233%3D%2520%2520Human%2520motion%2520generation%2520and%2520editing%2520are%2520key%2520components%2520of%2520computer%2520graphics%250Aand%2520vision.%2520However%252C%2520current%2520approaches%2520in%2520this%2520field%2520tend%2520to%2520offer%2520isolated%250Asolutions%2520tailored%2520to%2520specific%2520tasks%252C%2520which%2520can%2520be%2520inefficient%2520and%2520impractical%250Afor%2520real-world%2520applications.%2520While%2520some%2520efforts%2520have%2520aimed%2520to%2520unify%250Amotion-related%2520tasks%252C%2520these%2520methods%2520simply%2520use%2520different%2520modalities%2520as%250Aconditions%2520to%2520guide%2520motion%2520generation.%2520Consequently%252C%2520they%2520lack%2520editing%250Acapabilities%252C%2520fine-grained%2520control%252C%2520and%2520fail%2520to%2520facilitate%2520knowledge%2520sharing%250Aacross%2520tasks.%2520To%2520address%2520these%2520limitations%2520and%2520provide%2520a%2520versatile%252C%2520unified%250Aframework%2520capable%2520of%2520handling%2520both%2520human%2520motion%2520generation%2520and%2520editing%252C%2520we%250Aintroduce%2520a%2520novel%2520paradigm%253A%2520Motion-Condition-Motion%252C%2520which%2520enables%2520the%2520unified%250Aformulation%2520of%2520diverse%2520tasks%2520with%2520three%2520concepts%253A%2520source%2520motion%252C%2520condition%252C%2520and%250Atarget%2520motion.Based%2520on%2520this%2520paradigm%252C%2520we%2520propose%2520a%2520unified%2520framework%252C%250AMotionLab%252C%2520which%2520incorporates%2520rectified%2520flows%2520to%2520learn%2520the%2520mapping%2520from%2520source%250Amotion%2520to%2520target%2520motion%252C%2520guided%2520by%2520the%2520specified%2520conditions.In%2520MotionLab%252C%2520we%250Aintroduce%2520the%25201%2529%2520MotionFlow%2520Transformer%2520to%2520enhance%2520conditional%2520generation%2520and%250Aediting%2520without%2520task-specific%2520modules%253B%25202%2529%2520Aligned%2520Rotational%2520Position%2520Encoding%257D%250Ato%2520guarantee%2520the%2520time%2520synchronization%2520between%2520source%2520motion%2520and%2520target%2520motion%253B%250A3%2529%2520Task%2520Specified%2520Instruction%2520Modulation%253B%2520and%25204%2529%2520Motion%2520Curriculum%2520Learning%2520for%250Aeffective%2520multi-task%2520learning%2520and%2520knowledge%2520sharing%2520across%2520tasks.%2520Notably%252C%2520our%250AMotionLab%2520demonstrates%2520promising%2520generalization%2520capabilities%2520and%2520inference%250Aefficiency%2520across%2520multiple%2520benchmarks%2520for%2520human%2520motion.%2520Our%2520code%2520and%2520additional%250Avideo%2520results%2520are%2520available%2520at%253A%2520https%253A//diouo.github.io/motionlab.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionLab%3A%20Unified%20Human%20Motion%20Generation%20and%20Editing%20via%20the%0A%20%20Motion-Condition-Motion%20Paradigm&entry.906535625=Ziyan%20Guo%20and%20Zeyu%20Hu%20and%20Na%20Zhao%20and%20De%20Wen%20Soh&entry.1292438233=%20%20Human%20motion%20generation%20and%20editing%20are%20key%20components%20of%20computer%20graphics%0Aand%20vision.%20However%2C%20current%20approaches%20in%20this%20field%20tend%20to%20offer%20isolated%0Asolutions%20tailored%20to%20specific%20tasks%2C%20which%20can%20be%20inefficient%20and%20impractical%0Afor%20real-world%20applications.%20While%20some%20efforts%20have%20aimed%20to%20unify%0Amotion-related%20tasks%2C%20these%20methods%20simply%20use%20different%20modalities%20as%0Aconditions%20to%20guide%20motion%20generation.%20Consequently%2C%20they%20lack%20editing%0Acapabilities%2C%20fine-grained%20control%2C%20and%20fail%20to%20facilitate%20knowledge%20sharing%0Aacross%20tasks.%20To%20address%20these%20limitations%20and%20provide%20a%20versatile%2C%20unified%0Aframework%20capable%20of%20handling%20both%20human%20motion%20generation%20and%20editing%2C%20we%0Aintroduce%20a%20novel%20paradigm%3A%20Motion-Condition-Motion%2C%20which%20enables%20the%20unified%0Aformulation%20of%20diverse%20tasks%20with%20three%20concepts%3A%20source%20motion%2C%20condition%2C%20and%0Atarget%20motion.Based%20on%20this%20paradigm%2C%20we%20propose%20a%20unified%20framework%2C%0AMotionLab%2C%20which%20incorporates%20rectified%20flows%20to%20learn%20the%20mapping%20from%20source%0Amotion%20to%20target%20motion%2C%20guided%20by%20the%20specified%20conditions.In%20MotionLab%2C%20we%0Aintroduce%20the%201%29%20MotionFlow%20Transformer%20to%20enhance%20conditional%20generation%20and%0Aediting%20without%20task-specific%20modules%3B%202%29%20Aligned%20Rotational%20Position%20Encoding%7D%0Ato%20guarantee%20the%20time%20synchronization%20between%20source%20motion%20and%20target%20motion%3B%0A3%29%20Task%20Specified%20Instruction%20Modulation%3B%20and%204%29%20Motion%20Curriculum%20Learning%20for%0Aeffective%20multi-task%20learning%20and%20knowledge%20sharing%20across%20tasks.%20Notably%2C%20our%0AMotionLab%20demonstrates%20promising%20generalization%20capabilities%20and%20inference%0Aefficiency%20across%20multiple%20benchmarks%20for%20human%20motion.%20Our%20code%20and%20additional%0Avideo%20results%20are%20available%20at%3A%20https%3A//diouo.github.io/motionlab.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02358v1&entry.124074799=Read"},
{"title": "Boundary Constraint-free Biomechanical Model-Based Surface Matching for\n  Intraoperative Liver Deformation Correction", "author": "Zixin Yang and Richard Simon and Kelly Merrell and Cristian. A. Linte", "abstract": "  In image-guided liver surgery, 3D-3D non-rigid registration methods play a\ncrucial role in estimating the mapping between the preoperative model and the\nintraoperative surface represented as point clouds, addressing the challenge of\ntissue deformation. Typically, these methods incorporate a biomechanical model,\nrepresented as a finite element model (FEM), used to regularize a surface\nmatching term. This paper introduces a novel 3D-3D non-rigid registration\nmethod. In contrast to the preceding techniques, our method uniquely\nincorporates the FEM within the surface matching term itself, ensuring that the\nestimated deformation maintains geometric consistency throughout the\nregistration process. Additionally, we eliminate the need to determine\nzero-boundary conditions and applied force locations in the FEM. We achieve\nthis by integrating soft springs into the stiffness matrix and allowing forces\nto be distributed across the entire liver surface. To further improve\nrobustness, we introduce a regularization technique focused on the gradient of\nthe force magnitudes. This regularization imposes spatial smoothness and helps\nprevent the overfitting of irregular noise in intraoperative data. Optimization\nis achieved through an accelerated proximal gradient algorithm, further\nenhanced by our proposed method for determining the optimal step size. Our\nmethod is evaluated and compared to both a learning-based method and a\ntraditional method that features FEM regularization using data collected on our\ncustom-developed phantom, as well as two publicly available datasets. Our\nmethod consistently outperforms or is comparable to the baseline techniques.\nOur code and datasets will be available at\nhttps://github.com/zixinyang9109/BCF-FEM.\n", "link": "http://arxiv.org/abs/2403.09964v3", "date": "2025-02-04", "relevancy": 2.5269, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5214}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4994}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction&body=Title%3A%20Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction%0AAuthor%3A%20Zixin%20Yang%20and%20Richard%20Simon%20and%20Kelly%20Merrell%20and%20Cristian.%20A.%20Linte%0AAbstract%3A%20%20%20In%20image-guided%20liver%20surgery%2C%203D-3D%20non-rigid%20registration%20methods%20play%20a%0Acrucial%20role%20in%20estimating%20the%20mapping%20between%20the%20preoperative%20model%20and%20the%0Aintraoperative%20surface%20represented%20as%20point%20clouds%2C%20addressing%20the%20challenge%20of%0Atissue%20deformation.%20Typically%2C%20these%20methods%20incorporate%20a%20biomechanical%20model%2C%0Arepresented%20as%20a%20finite%20element%20model%20%28FEM%29%2C%20used%20to%20regularize%20a%20surface%0Amatching%20term.%20This%20paper%20introduces%20a%20novel%203D-3D%20non-rigid%20registration%0Amethod.%20In%20contrast%20to%20the%20preceding%20techniques%2C%20our%20method%20uniquely%0Aincorporates%20the%20FEM%20within%20the%20surface%20matching%20term%20itself%2C%20ensuring%20that%20the%0Aestimated%20deformation%20maintains%20geometric%20consistency%20throughout%20the%0Aregistration%20process.%20Additionally%2C%20we%20eliminate%20the%20need%20to%20determine%0Azero-boundary%20conditions%20and%20applied%20force%20locations%20in%20the%20FEM.%20We%20achieve%0Athis%20by%20integrating%20soft%20springs%20into%20the%20stiffness%20matrix%20and%20allowing%20forces%0Ato%20be%20distributed%20across%20the%20entire%20liver%20surface.%20To%20further%20improve%0Arobustness%2C%20we%20introduce%20a%20regularization%20technique%20focused%20on%20the%20gradient%20of%0Athe%20force%20magnitudes.%20This%20regularization%20imposes%20spatial%20smoothness%20and%20helps%0Aprevent%20the%20overfitting%20of%20irregular%20noise%20in%20intraoperative%20data.%20Optimization%0Ais%20achieved%20through%20an%20accelerated%20proximal%20gradient%20algorithm%2C%20further%0Aenhanced%20by%20our%20proposed%20method%20for%20determining%20the%20optimal%20step%20size.%20Our%0Amethod%20is%20evaluated%20and%20compared%20to%20both%20a%20learning-based%20method%20and%20a%0Atraditional%20method%20that%20features%20FEM%20regularization%20using%20data%20collected%20on%20our%0Acustom-developed%20phantom%2C%20as%20well%20as%20two%20publicly%20available%20datasets.%20Our%0Amethod%20consistently%20outperforms%20or%20is%20comparable%20to%20the%20baseline%20techniques.%0AOur%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/zixinyang9109/BCF-FEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09964v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary%2520Constraint-free%2520Biomechanical%2520Model-Based%2520Surface%2520Matching%2520for%250A%2520%2520Intraoperative%2520Liver%2520Deformation%2520Correction%26entry.906535625%3DZixin%2520Yang%2520and%2520Richard%2520Simon%2520and%2520Kelly%2520Merrell%2520and%2520Cristian.%2520A.%2520Linte%26entry.1292438233%3D%2520%2520In%2520image-guided%2520liver%2520surgery%252C%25203D-3D%2520non-rigid%2520registration%2520methods%2520play%2520a%250Acrucial%2520role%2520in%2520estimating%2520the%2520mapping%2520between%2520the%2520preoperative%2520model%2520and%2520the%250Aintraoperative%2520surface%2520represented%2520as%2520point%2520clouds%252C%2520addressing%2520the%2520challenge%2520of%250Atissue%2520deformation.%2520Typically%252C%2520these%2520methods%2520incorporate%2520a%2520biomechanical%2520model%252C%250Arepresented%2520as%2520a%2520finite%2520element%2520model%2520%2528FEM%2529%252C%2520used%2520to%2520regularize%2520a%2520surface%250Amatching%2520term.%2520This%2520paper%2520introduces%2520a%2520novel%25203D-3D%2520non-rigid%2520registration%250Amethod.%2520In%2520contrast%2520to%2520the%2520preceding%2520techniques%252C%2520our%2520method%2520uniquely%250Aincorporates%2520the%2520FEM%2520within%2520the%2520surface%2520matching%2520term%2520itself%252C%2520ensuring%2520that%2520the%250Aestimated%2520deformation%2520maintains%2520geometric%2520consistency%2520throughout%2520the%250Aregistration%2520process.%2520Additionally%252C%2520we%2520eliminate%2520the%2520need%2520to%2520determine%250Azero-boundary%2520conditions%2520and%2520applied%2520force%2520locations%2520in%2520the%2520FEM.%2520We%2520achieve%250Athis%2520by%2520integrating%2520soft%2520springs%2520into%2520the%2520stiffness%2520matrix%2520and%2520allowing%2520forces%250Ato%2520be%2520distributed%2520across%2520the%2520entire%2520liver%2520surface.%2520To%2520further%2520improve%250Arobustness%252C%2520we%2520introduce%2520a%2520regularization%2520technique%2520focused%2520on%2520the%2520gradient%2520of%250Athe%2520force%2520magnitudes.%2520This%2520regularization%2520imposes%2520spatial%2520smoothness%2520and%2520helps%250Aprevent%2520the%2520overfitting%2520of%2520irregular%2520noise%2520in%2520intraoperative%2520data.%2520Optimization%250Ais%2520achieved%2520through%2520an%2520accelerated%2520proximal%2520gradient%2520algorithm%252C%2520further%250Aenhanced%2520by%2520our%2520proposed%2520method%2520for%2520determining%2520the%2520optimal%2520step%2520size.%2520Our%250Amethod%2520is%2520evaluated%2520and%2520compared%2520to%2520both%2520a%2520learning-based%2520method%2520and%2520a%250Atraditional%2520method%2520that%2520features%2520FEM%2520regularization%2520using%2520data%2520collected%2520on%2520our%250Acustom-developed%2520phantom%252C%2520as%2520well%2520as%2520two%2520publicly%2520available%2520datasets.%2520Our%250Amethod%2520consistently%2520outperforms%2520or%2520is%2520comparable%2520to%2520the%2520baseline%2520techniques.%250AOur%2520code%2520and%2520datasets%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/zixinyang9109/BCF-FEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09964v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction&entry.906535625=Zixin%20Yang%20and%20Richard%20Simon%20and%20Kelly%20Merrell%20and%20Cristian.%20A.%20Linte&entry.1292438233=%20%20In%20image-guided%20liver%20surgery%2C%203D-3D%20non-rigid%20registration%20methods%20play%20a%0Acrucial%20role%20in%20estimating%20the%20mapping%20between%20the%20preoperative%20model%20and%20the%0Aintraoperative%20surface%20represented%20as%20point%20clouds%2C%20addressing%20the%20challenge%20of%0Atissue%20deformation.%20Typically%2C%20these%20methods%20incorporate%20a%20biomechanical%20model%2C%0Arepresented%20as%20a%20finite%20element%20model%20%28FEM%29%2C%20used%20to%20regularize%20a%20surface%0Amatching%20term.%20This%20paper%20introduces%20a%20novel%203D-3D%20non-rigid%20registration%0Amethod.%20In%20contrast%20to%20the%20preceding%20techniques%2C%20our%20method%20uniquely%0Aincorporates%20the%20FEM%20within%20the%20surface%20matching%20term%20itself%2C%20ensuring%20that%20the%0Aestimated%20deformation%20maintains%20geometric%20consistency%20throughout%20the%0Aregistration%20process.%20Additionally%2C%20we%20eliminate%20the%20need%20to%20determine%0Azero-boundary%20conditions%20and%20applied%20force%20locations%20in%20the%20FEM.%20We%20achieve%0Athis%20by%20integrating%20soft%20springs%20into%20the%20stiffness%20matrix%20and%20allowing%20forces%0Ato%20be%20distributed%20across%20the%20entire%20liver%20surface.%20To%20further%20improve%0Arobustness%2C%20we%20introduce%20a%20regularization%20technique%20focused%20on%20the%20gradient%20of%0Athe%20force%20magnitudes.%20This%20regularization%20imposes%20spatial%20smoothness%20and%20helps%0Aprevent%20the%20overfitting%20of%20irregular%20noise%20in%20intraoperative%20data.%20Optimization%0Ais%20achieved%20through%20an%20accelerated%20proximal%20gradient%20algorithm%2C%20further%0Aenhanced%20by%20our%20proposed%20method%20for%20determining%20the%20optimal%20step%20size.%20Our%0Amethod%20is%20evaluated%20and%20compared%20to%20both%20a%20learning-based%20method%20and%20a%0Atraditional%20method%20that%20features%20FEM%20regularization%20using%20data%20collected%20on%20our%0Acustom-developed%20phantom%2C%20as%20well%20as%20two%20publicly%20available%20datasets.%20Our%0Amethod%20consistently%20outperforms%20or%20is%20comparable%20to%20the%20baseline%20techniques.%0AOur%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/zixinyang9109/BCF-FEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09964v3&entry.124074799=Read"},
{"title": "EdgeGFL: Rethinking Edge Information in Graph Feature Preference\n  Learning", "author": "Shengda Zhuo and Jiwang Fang and Hongguang Lin and Yin Tang and Min Chen and Changdong Wang and Shuqiang Huang", "abstract": "  Graph Neural Networks (GNNs) have significant advantages in handling\nnon-Euclidean data and have been widely applied across various areas, thus\nreceiving increasing attention in recent years. The framework of GNN models\nmainly includes the information propagation phase and the aggregation phase,\ntreating nodes and edges as information entities and propagation channels,\nrespectively. However, most existing GNN models face the challenge of\ndisconnection between node and edge feature information, as these models\ntypically treat the learning of edge and node features as independent tasks. To\naddress this limitation, we aim to develop an edge-empowered graph feature\npreference learning framework that can capture edge embeddings to assist node\nembeddings. By leveraging the learned multidimensional edge feature matrix, we\nconstruct multi-channel filters to more effectively capture accurate node\nfeatures, thereby obtaining the non-local structural characteristics and\nfine-grained high-order node features. Specifically, the inclusion of\nmultidimensional edge information enhances the functionality and flexibility of\nthe GNN model, enabling it to handle complex and diverse graph data more\neffectively. Additionally, integrating relational representation learning into\nthe message passing framework allows graph nodes to receive more useful\ninformation, thereby facilitating node representation learning. Finally,\nexperiments on four real-world heterogeneous graphs demonstrate the\neffectiveness of theproposed model.\n", "link": "http://arxiv.org/abs/2502.02302v1", "date": "2025-02-04", "relevancy": 2.5226, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5516}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.485}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeGFL%3A%20Rethinking%20Edge%20Information%20in%20Graph%20Feature%20Preference%0A%20%20Learning&body=Title%3A%20EdgeGFL%3A%20Rethinking%20Edge%20Information%20in%20Graph%20Feature%20Preference%0A%20%20Learning%0AAuthor%3A%20Shengda%20Zhuo%20and%20Jiwang%20Fang%20and%20Hongguang%20Lin%20and%20Yin%20Tang%20and%20Min%20Chen%20and%20Changdong%20Wang%20and%20Shuqiang%20Huang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20significant%20advantages%20in%20handling%0Anon-Euclidean%20data%20and%20have%20been%20widely%20applied%20across%20various%20areas%2C%20thus%0Areceiving%20increasing%20attention%20in%20recent%20years.%20The%20framework%20of%20GNN%20models%0Amainly%20includes%20the%20information%20propagation%20phase%20and%20the%20aggregation%20phase%2C%0Atreating%20nodes%20and%20edges%20as%20information%20entities%20and%20propagation%20channels%2C%0Arespectively.%20However%2C%20most%20existing%20GNN%20models%20face%20the%20challenge%20of%0Adisconnection%20between%20node%20and%20edge%20feature%20information%2C%20as%20these%20models%0Atypically%20treat%20the%20learning%20of%20edge%20and%20node%20features%20as%20independent%20tasks.%20To%0Aaddress%20this%20limitation%2C%20we%20aim%20to%20develop%20an%20edge-empowered%20graph%20feature%0Apreference%20learning%20framework%20that%20can%20capture%20edge%20embeddings%20to%20assist%20node%0Aembeddings.%20By%20leveraging%20the%20learned%20multidimensional%20edge%20feature%20matrix%2C%20we%0Aconstruct%20multi-channel%20filters%20to%20more%20effectively%20capture%20accurate%20node%0Afeatures%2C%20thereby%20obtaining%20the%20non-local%20structural%20characteristics%20and%0Afine-grained%20high-order%20node%20features.%20Specifically%2C%20the%20inclusion%20of%0Amultidimensional%20edge%20information%20enhances%20the%20functionality%20and%20flexibility%20of%0Athe%20GNN%20model%2C%20enabling%20it%20to%20handle%20complex%20and%20diverse%20graph%20data%20more%0Aeffectively.%20Additionally%2C%20integrating%20relational%20representation%20learning%20into%0Athe%20message%20passing%20framework%20allows%20graph%20nodes%20to%20receive%20more%20useful%0Ainformation%2C%20thereby%20facilitating%20node%20representation%20learning.%20Finally%2C%0Aexperiments%20on%20four%20real-world%20heterogeneous%20graphs%20demonstrate%20the%0Aeffectiveness%20of%20theproposed%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeGFL%253A%2520Rethinking%2520Edge%2520Information%2520in%2520Graph%2520Feature%2520Preference%250A%2520%2520Learning%26entry.906535625%3DShengda%2520Zhuo%2520and%2520Jiwang%2520Fang%2520and%2520Hongguang%2520Lin%2520and%2520Yin%2520Tang%2520and%2520Min%2520Chen%2520and%2520Changdong%2520Wang%2520and%2520Shuqiang%2520Huang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520significant%2520advantages%2520in%2520handling%250Anon-Euclidean%2520data%2520and%2520have%2520been%2520widely%2520applied%2520across%2520various%2520areas%252C%2520thus%250Areceiving%2520increasing%2520attention%2520in%2520recent%2520years.%2520The%2520framework%2520of%2520GNN%2520models%250Amainly%2520includes%2520the%2520information%2520propagation%2520phase%2520and%2520the%2520aggregation%2520phase%252C%250Atreating%2520nodes%2520and%2520edges%2520as%2520information%2520entities%2520and%2520propagation%2520channels%252C%250Arespectively.%2520However%252C%2520most%2520existing%2520GNN%2520models%2520face%2520the%2520challenge%2520of%250Adisconnection%2520between%2520node%2520and%2520edge%2520feature%2520information%252C%2520as%2520these%2520models%250Atypically%2520treat%2520the%2520learning%2520of%2520edge%2520and%2520node%2520features%2520as%2520independent%2520tasks.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520aim%2520to%2520develop%2520an%2520edge-empowered%2520graph%2520feature%250Apreference%2520learning%2520framework%2520that%2520can%2520capture%2520edge%2520embeddings%2520to%2520assist%2520node%250Aembeddings.%2520By%2520leveraging%2520the%2520learned%2520multidimensional%2520edge%2520feature%2520matrix%252C%2520we%250Aconstruct%2520multi-channel%2520filters%2520to%2520more%2520effectively%2520capture%2520accurate%2520node%250Afeatures%252C%2520thereby%2520obtaining%2520the%2520non-local%2520structural%2520characteristics%2520and%250Afine-grained%2520high-order%2520node%2520features.%2520Specifically%252C%2520the%2520inclusion%2520of%250Amultidimensional%2520edge%2520information%2520enhances%2520the%2520functionality%2520and%2520flexibility%2520of%250Athe%2520GNN%2520model%252C%2520enabling%2520it%2520to%2520handle%2520complex%2520and%2520diverse%2520graph%2520data%2520more%250Aeffectively.%2520Additionally%252C%2520integrating%2520relational%2520representation%2520learning%2520into%250Athe%2520message%2520passing%2520framework%2520allows%2520graph%2520nodes%2520to%2520receive%2520more%2520useful%250Ainformation%252C%2520thereby%2520facilitating%2520node%2520representation%2520learning.%2520Finally%252C%250Aexperiments%2520on%2520four%2520real-world%2520heterogeneous%2520graphs%2520demonstrate%2520the%250Aeffectiveness%2520of%2520theproposed%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeGFL%3A%20Rethinking%20Edge%20Information%20in%20Graph%20Feature%20Preference%0A%20%20Learning&entry.906535625=Shengda%20Zhuo%20and%20Jiwang%20Fang%20and%20Hongguang%20Lin%20and%20Yin%20Tang%20and%20Min%20Chen%20and%20Changdong%20Wang%20and%20Shuqiang%20Huang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20significant%20advantages%20in%20handling%0Anon-Euclidean%20data%20and%20have%20been%20widely%20applied%20across%20various%20areas%2C%20thus%0Areceiving%20increasing%20attention%20in%20recent%20years.%20The%20framework%20of%20GNN%20models%0Amainly%20includes%20the%20information%20propagation%20phase%20and%20the%20aggregation%20phase%2C%0Atreating%20nodes%20and%20edges%20as%20information%20entities%20and%20propagation%20channels%2C%0Arespectively.%20However%2C%20most%20existing%20GNN%20models%20face%20the%20challenge%20of%0Adisconnection%20between%20node%20and%20edge%20feature%20information%2C%20as%20these%20models%0Atypically%20treat%20the%20learning%20of%20edge%20and%20node%20features%20as%20independent%20tasks.%20To%0Aaddress%20this%20limitation%2C%20we%20aim%20to%20develop%20an%20edge-empowered%20graph%20feature%0Apreference%20learning%20framework%20that%20can%20capture%20edge%20embeddings%20to%20assist%20node%0Aembeddings.%20By%20leveraging%20the%20learned%20multidimensional%20edge%20feature%20matrix%2C%20we%0Aconstruct%20multi-channel%20filters%20to%20more%20effectively%20capture%20accurate%20node%0Afeatures%2C%20thereby%20obtaining%20the%20non-local%20structural%20characteristics%20and%0Afine-grained%20high-order%20node%20features.%20Specifically%2C%20the%20inclusion%20of%0Amultidimensional%20edge%20information%20enhances%20the%20functionality%20and%20flexibility%20of%0Athe%20GNN%20model%2C%20enabling%20it%20to%20handle%20complex%20and%20diverse%20graph%20data%20more%0Aeffectively.%20Additionally%2C%20integrating%20relational%20representation%20learning%20into%0Athe%20message%20passing%20framework%20allows%20graph%20nodes%20to%20receive%20more%20useful%0Ainformation%2C%20thereby%20facilitating%20node%20representation%20learning.%20Finally%2C%0Aexperiments%20on%20four%20real-world%20heterogeneous%20graphs%20demonstrate%20the%0Aeffectiveness%20of%20theproposed%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02302v1&entry.124074799=Read"},
{"title": "Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided\n  Mobile Manipulation", "author": "Zhijie Yan and Shufei Li and Zuoxu Wang and Lixiu Wu and Han Wang and Jun Zhu and Lijiang Chen and Jihong Liu", "abstract": "  Enabling mobile robots to perform long-term tasks in dynamic real-world\nenvironments is a formidable challenge, especially when the environment changes\nfrequently due to human-robot interactions or the robot's own actions.\nTraditional methods typically assume static scenes, which limits their\napplicability in the continuously changing real world. To overcome these\nlimitations, we present DovSG, a novel mobile manipulation framework that\nleverages dynamic open-vocabulary 3D scene graphs and a language-guided task\nplanning module for long-term task execution. DovSG takes RGB-D sequences as\ninput and utilizes vision-language models (VLMs) for object detection to obtain\nhigh-level object semantic features. Based on the segmented objects, a\nstructured 3D scene graph is generated for low-level spatial relationships.\nFurthermore, an efficient mechanism for locally updating the scene graph,\nallows the robot to adjust parts of the graph dynamically during interactions\nwithout the need for full scene reconstruction. This mechanism is particularly\nvaluable in dynamic environments, enabling the robot to continually adapt to\nscene changes and effectively support the execution of long-term tasks. We\nvalidated our system in real-world environments with varying degrees of manual\nmodifications, demonstrating its effectiveness and superior performance in\nlong-term tasks. Our project page is available at:\nhttps://bjhyzj.github.io/dovsg-web.\n", "link": "http://arxiv.org/abs/2410.11989v4", "date": "2025-02-04", "relevancy": 2.4942, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation&body=Title%3A%20Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation%0AAuthor%3A%20Zhijie%20Yan%20and%20Shufei%20Li%20and%20Zuoxu%20Wang%20and%20Lixiu%20Wu%20and%20Han%20Wang%20and%20Jun%20Zhu%20and%20Lijiang%20Chen%20and%20Jihong%20Liu%0AAbstract%3A%20%20%20Enabling%20mobile%20robots%20to%20perform%20long-term%20tasks%20in%20dynamic%20real-world%0Aenvironments%20is%20a%20formidable%20challenge%2C%20especially%20when%20the%20environment%20changes%0Afrequently%20due%20to%20human-robot%20interactions%20or%20the%20robot%27s%20own%20actions.%0ATraditional%20methods%20typically%20assume%20static%20scenes%2C%20which%20limits%20their%0Aapplicability%20in%20the%20continuously%20changing%20real%20world.%20To%20overcome%20these%0Alimitations%2C%20we%20present%20DovSG%2C%20a%20novel%20mobile%20manipulation%20framework%20that%0Aleverages%20dynamic%20open-vocabulary%203D%20scene%20graphs%20and%20a%20language-guided%20task%0Aplanning%20module%20for%20long-term%20task%20execution.%20DovSG%20takes%20RGB-D%20sequences%20as%0Ainput%20and%20utilizes%20vision-language%20models%20%28VLMs%29%20for%20object%20detection%20to%20obtain%0Ahigh-level%20object%20semantic%20features.%20Based%20on%20the%20segmented%20objects%2C%20a%0Astructured%203D%20scene%20graph%20is%20generated%20for%20low-level%20spatial%20relationships.%0AFurthermore%2C%20an%20efficient%20mechanism%20for%20locally%20updating%20the%20scene%20graph%2C%0Aallows%20the%20robot%20to%20adjust%20parts%20of%20the%20graph%20dynamically%20during%20interactions%0Awithout%20the%20need%20for%20full%20scene%20reconstruction.%20This%20mechanism%20is%20particularly%0Avaluable%20in%20dynamic%20environments%2C%20enabling%20the%20robot%20to%20continually%20adapt%20to%0Ascene%20changes%20and%20effectively%20support%20the%20execution%20of%20long-term%20tasks.%20We%0Avalidated%20our%20system%20in%20real-world%20environments%20with%20varying%20degrees%20of%20manual%0Amodifications%2C%20demonstrating%20its%20effectiveness%20and%20superior%20performance%20in%0Along-term%20tasks.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//bjhyzj.github.io/dovsg-web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11989v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Open-Vocabulary%25203D%2520Scene%2520Graphs%2520for%2520Long-term%2520Language-Guided%250A%2520%2520Mobile%2520Manipulation%26entry.906535625%3DZhijie%2520Yan%2520and%2520Shufei%2520Li%2520and%2520Zuoxu%2520Wang%2520and%2520Lixiu%2520Wu%2520and%2520Han%2520Wang%2520and%2520Jun%2520Zhu%2520and%2520Lijiang%2520Chen%2520and%2520Jihong%2520Liu%26entry.1292438233%3D%2520%2520Enabling%2520mobile%2520robots%2520to%2520perform%2520long-term%2520tasks%2520in%2520dynamic%2520real-world%250Aenvironments%2520is%2520a%2520formidable%2520challenge%252C%2520especially%2520when%2520the%2520environment%2520changes%250Afrequently%2520due%2520to%2520human-robot%2520interactions%2520or%2520the%2520robot%2527s%2520own%2520actions.%250ATraditional%2520methods%2520typically%2520assume%2520static%2520scenes%252C%2520which%2520limits%2520their%250Aapplicability%2520in%2520the%2520continuously%2520changing%2520real%2520world.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520present%2520DovSG%252C%2520a%2520novel%2520mobile%2520manipulation%2520framework%2520that%250Aleverages%2520dynamic%2520open-vocabulary%25203D%2520scene%2520graphs%2520and%2520a%2520language-guided%2520task%250Aplanning%2520module%2520for%2520long-term%2520task%2520execution.%2520DovSG%2520takes%2520RGB-D%2520sequences%2520as%250Ainput%2520and%2520utilizes%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520object%2520detection%2520to%2520obtain%250Ahigh-level%2520object%2520semantic%2520features.%2520Based%2520on%2520the%2520segmented%2520objects%252C%2520a%250Astructured%25203D%2520scene%2520graph%2520is%2520generated%2520for%2520low-level%2520spatial%2520relationships.%250AFurthermore%252C%2520an%2520efficient%2520mechanism%2520for%2520locally%2520updating%2520the%2520scene%2520graph%252C%250Aallows%2520the%2520robot%2520to%2520adjust%2520parts%2520of%2520the%2520graph%2520dynamically%2520during%2520interactions%250Awithout%2520the%2520need%2520for%2520full%2520scene%2520reconstruction.%2520This%2520mechanism%2520is%2520particularly%250Avaluable%2520in%2520dynamic%2520environments%252C%2520enabling%2520the%2520robot%2520to%2520continually%2520adapt%2520to%250Ascene%2520changes%2520and%2520effectively%2520support%2520the%2520execution%2520of%2520long-term%2520tasks.%2520We%250Avalidated%2520our%2520system%2520in%2520real-world%2520environments%2520with%2520varying%2520degrees%2520of%2520manual%250Amodifications%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520superior%2520performance%2520in%250Along-term%2520tasks.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//bjhyzj.github.io/dovsg-web.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11989v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation&entry.906535625=Zhijie%20Yan%20and%20Shufei%20Li%20and%20Zuoxu%20Wang%20and%20Lixiu%20Wu%20and%20Han%20Wang%20and%20Jun%20Zhu%20and%20Lijiang%20Chen%20and%20Jihong%20Liu&entry.1292438233=%20%20Enabling%20mobile%20robots%20to%20perform%20long-term%20tasks%20in%20dynamic%20real-world%0Aenvironments%20is%20a%20formidable%20challenge%2C%20especially%20when%20the%20environment%20changes%0Afrequently%20due%20to%20human-robot%20interactions%20or%20the%20robot%27s%20own%20actions.%0ATraditional%20methods%20typically%20assume%20static%20scenes%2C%20which%20limits%20their%0Aapplicability%20in%20the%20continuously%20changing%20real%20world.%20To%20overcome%20these%0Alimitations%2C%20we%20present%20DovSG%2C%20a%20novel%20mobile%20manipulation%20framework%20that%0Aleverages%20dynamic%20open-vocabulary%203D%20scene%20graphs%20and%20a%20language-guided%20task%0Aplanning%20module%20for%20long-term%20task%20execution.%20DovSG%20takes%20RGB-D%20sequences%20as%0Ainput%20and%20utilizes%20vision-language%20models%20%28VLMs%29%20for%20object%20detection%20to%20obtain%0Ahigh-level%20object%20semantic%20features.%20Based%20on%20the%20segmented%20objects%2C%20a%0Astructured%203D%20scene%20graph%20is%20generated%20for%20low-level%20spatial%20relationships.%0AFurthermore%2C%20an%20efficient%20mechanism%20for%20locally%20updating%20the%20scene%20graph%2C%0Aallows%20the%20robot%20to%20adjust%20parts%20of%20the%20graph%20dynamically%20during%20interactions%0Awithout%20the%20need%20for%20full%20scene%20reconstruction.%20This%20mechanism%20is%20particularly%0Avaluable%20in%20dynamic%20environments%2C%20enabling%20the%20robot%20to%20continually%20adapt%20to%0Ascene%20changes%20and%20effectively%20support%20the%20execution%20of%20long-term%20tasks.%20We%0Avalidated%20our%20system%20in%20real-world%20environments%20with%20varying%20degrees%20of%20manual%0Amodifications%2C%20demonstrating%20its%20effectiveness%20and%20superior%20performance%20in%0Along-term%20tasks.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//bjhyzj.github.io/dovsg-web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11989v4&entry.124074799=Read"},
{"title": "TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised\n  Learning", "author": "Yupei Liu and Yanting Wang and Jinyuan Jia", "abstract": "  An image encoder pre-trained by self-supervised learning can be used as a\ngeneral-purpose feature extractor to build downstream classifiers for various\ndownstream tasks. However, many studies showed that an attacker can embed a\ntrojan into an encoder such that multiple downstream classifiers built based on\nthe trojaned encoder simultaneously inherit the trojan behavior. In this work,\nwe propose TrojanDec, the first data-free method to identify and recover a test\ninput embedded with a trigger. Given a (trojaned or clean) encoder and a test\ninput, TrojanDec first predicts whether the test input is trojaned. If not, the\ntest input is processed in a normal way to maintain the utility. Otherwise, the\ntest input will be further restored to remove the trigger. Our extensive\nevaluation shows that TrojanDec can effectively identify the trojan (if any)\nfrom a given test input and recover it under state-of-the-art trojan attacks.\nWe further demonstrate by experiments that our TrojanDec outperforms the\nstate-of-the-art defenses.\n", "link": "http://arxiv.org/abs/2501.04108v2", "date": "2025-02-04", "relevancy": 2.4868, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4995}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrojanDec%3A%20Data-free%20Detection%20of%20Trojan%20Inputs%20in%20Self-supervised%0A%20%20Learning&body=Title%3A%20TrojanDec%3A%20Data-free%20Detection%20of%20Trojan%20Inputs%20in%20Self-supervised%0A%20%20Learning%0AAuthor%3A%20Yupei%20Liu%20and%20Yanting%20Wang%20and%20Jinyuan%20Jia%0AAbstract%3A%20%20%20An%20image%20encoder%20pre-trained%20by%20self-supervised%20learning%20can%20be%20used%20as%20a%0Ageneral-purpose%20feature%20extractor%20to%20build%20downstream%20classifiers%20for%20various%0Adownstream%20tasks.%20However%2C%20many%20studies%20showed%20that%20an%20attacker%20can%20embed%20a%0Atrojan%20into%20an%20encoder%20such%20that%20multiple%20downstream%20classifiers%20built%20based%20on%0Athe%20trojaned%20encoder%20simultaneously%20inherit%20the%20trojan%20behavior.%20In%20this%20work%2C%0Awe%20propose%20TrojanDec%2C%20the%20first%20data-free%20method%20to%20identify%20and%20recover%20a%20test%0Ainput%20embedded%20with%20a%20trigger.%20Given%20a%20%28trojaned%20or%20clean%29%20encoder%20and%20a%20test%0Ainput%2C%20TrojanDec%20first%20predicts%20whether%20the%20test%20input%20is%20trojaned.%20If%20not%2C%20the%0Atest%20input%20is%20processed%20in%20a%20normal%20way%20to%20maintain%20the%20utility.%20Otherwise%2C%20the%0Atest%20input%20will%20be%20further%20restored%20to%20remove%20the%20trigger.%20Our%20extensive%0Aevaluation%20shows%20that%20TrojanDec%20can%20effectively%20identify%20the%20trojan%20%28if%20any%29%0Afrom%20a%20given%20test%20input%20and%20recover%20it%20under%20state-of-the-art%20trojan%20attacks.%0AWe%20further%20demonstrate%20by%20experiments%20that%20our%20TrojanDec%20outperforms%20the%0Astate-of-the-art%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrojanDec%253A%2520Data-free%2520Detection%2520of%2520Trojan%2520Inputs%2520in%2520Self-supervised%250A%2520%2520Learning%26entry.906535625%3DYupei%2520Liu%2520and%2520Yanting%2520Wang%2520and%2520Jinyuan%2520Jia%26entry.1292438233%3D%2520%2520An%2520image%2520encoder%2520pre-trained%2520by%2520self-supervised%2520learning%2520can%2520be%2520used%2520as%2520a%250Ageneral-purpose%2520feature%2520extractor%2520to%2520build%2520downstream%2520classifiers%2520for%2520various%250Adownstream%2520tasks.%2520However%252C%2520many%2520studies%2520showed%2520that%2520an%2520attacker%2520can%2520embed%2520a%250Atrojan%2520into%2520an%2520encoder%2520such%2520that%2520multiple%2520downstream%2520classifiers%2520built%2520based%2520on%250Athe%2520trojaned%2520encoder%2520simultaneously%2520inherit%2520the%2520trojan%2520behavior.%2520In%2520this%2520work%252C%250Awe%2520propose%2520TrojanDec%252C%2520the%2520first%2520data-free%2520method%2520to%2520identify%2520and%2520recover%2520a%2520test%250Ainput%2520embedded%2520with%2520a%2520trigger.%2520Given%2520a%2520%2528trojaned%2520or%2520clean%2529%2520encoder%2520and%2520a%2520test%250Ainput%252C%2520TrojanDec%2520first%2520predicts%2520whether%2520the%2520test%2520input%2520is%2520trojaned.%2520If%2520not%252C%2520the%250Atest%2520input%2520is%2520processed%2520in%2520a%2520normal%2520way%2520to%2520maintain%2520the%2520utility.%2520Otherwise%252C%2520the%250Atest%2520input%2520will%2520be%2520further%2520restored%2520to%2520remove%2520the%2520trigger.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520TrojanDec%2520can%2520effectively%2520identify%2520the%2520trojan%2520%2528if%2520any%2529%250Afrom%2520a%2520given%2520test%2520input%2520and%2520recover%2520it%2520under%2520state-of-the-art%2520trojan%2520attacks.%250AWe%2520further%2520demonstrate%2520by%2520experiments%2520that%2520our%2520TrojanDec%2520outperforms%2520the%250Astate-of-the-art%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrojanDec%3A%20Data-free%20Detection%20of%20Trojan%20Inputs%20in%20Self-supervised%0A%20%20Learning&entry.906535625=Yupei%20Liu%20and%20Yanting%20Wang%20and%20Jinyuan%20Jia&entry.1292438233=%20%20An%20image%20encoder%20pre-trained%20by%20self-supervised%20learning%20can%20be%20used%20as%20a%0Ageneral-purpose%20feature%20extractor%20to%20build%20downstream%20classifiers%20for%20various%0Adownstream%20tasks.%20However%2C%20many%20studies%20showed%20that%20an%20attacker%20can%20embed%20a%0Atrojan%20into%20an%20encoder%20such%20that%20multiple%20downstream%20classifiers%20built%20based%20on%0Athe%20trojaned%20encoder%20simultaneously%20inherit%20the%20trojan%20behavior.%20In%20this%20work%2C%0Awe%20propose%20TrojanDec%2C%20the%20first%20data-free%20method%20to%20identify%20and%20recover%20a%20test%0Ainput%20embedded%20with%20a%20trigger.%20Given%20a%20%28trojaned%20or%20clean%29%20encoder%20and%20a%20test%0Ainput%2C%20TrojanDec%20first%20predicts%20whether%20the%20test%20input%20is%20trojaned.%20If%20not%2C%20the%0Atest%20input%20is%20processed%20in%20a%20normal%20way%20to%20maintain%20the%20utility.%20Otherwise%2C%20the%0Atest%20input%20will%20be%20further%20restored%20to%20remove%20the%20trigger.%20Our%20extensive%0Aevaluation%20shows%20that%20TrojanDec%20can%20effectively%20identify%20the%20trojan%20%28if%20any%29%0Afrom%20a%20given%20test%20input%20and%20recover%20it%20under%20state-of-the-art%20trojan%20attacks.%0AWe%20further%20demonstrate%20by%20experiments%20that%20our%20TrojanDec%20outperforms%20the%0Astate-of-the-art%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04108v2&entry.124074799=Read"},
{"title": "FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named\n  Entity Recognition in a Multilingual Framework", "author": "Ibrahim Bouabdallaoui and Fatima Guerouate and Samya Bouhaddour and Chaimae Saadi and Mohammed Sbihi", "abstract": "  We introduce FewTopNER, a novel framework that integrates few-shot named\nentity recognition (NER) with topic-aware contextual modeling to address the\nchallenges of cross-lingual and low-resource scenarios. FewTopNER leverages a\nshared multilingual encoder based on XLM-RoBERTa, augmented with\nlanguage-specific calibration mechanisms, to generate robust contextual\nembeddings. The architecture comprises a prototype-based entity recognition\nbranch, employing BiLSTM and Conditional Random Fields for sequence labeling,\nand a topic modeling branch that extracts document-level semantic features\nthrough hybrid probabilistic and neural methods. A cross-task bridge\nfacilitates dynamic bidirectional attention and feature fusion between entity\nand topic representations, thereby enhancing entity disambiguation by\nincorporating global semantic context. Empirical evaluations on multilingual\nbenchmarks across English, French, Spanish, German, and Italian demonstrate\nthat FewTopNER significantly outperforms existing state-of-the-art few-shot NER\nmodels. In particular, the framework achieves improvements of 2.5-4.0\npercentage points in F1 score and exhibits enhanced topic coherence, as\nmeasured by normalized pointwise mutual information. Ablation studies further\nconfirm the critical contributions of the shared encoder and cross-task\nintegration mechanisms to the overall performance. These results underscore the\nefficacy of incorporating topic-aware context into few-shot NER and highlight\nthe potential of FewTopNER for robust cross-lingual applications in\nlow-resource settings.\n", "link": "http://arxiv.org/abs/2502.02391v1", "date": "2025-02-04", "relevancy": 2.4663, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FewTopNER%3A%20Integrating%20Few-Shot%20Learning%20with%20Topic%20Modeling%20and%20Named%0A%20%20Entity%20Recognition%20in%20a%20Multilingual%20Framework&body=Title%3A%20FewTopNER%3A%20Integrating%20Few-Shot%20Learning%20with%20Topic%20Modeling%20and%20Named%0A%20%20Entity%20Recognition%20in%20a%20Multilingual%20Framework%0AAuthor%3A%20Ibrahim%20Bouabdallaoui%20and%20Fatima%20Guerouate%20and%20Samya%20Bouhaddour%20and%20Chaimae%20Saadi%20and%20Mohammed%20Sbihi%0AAbstract%3A%20%20%20We%20introduce%20FewTopNER%2C%20a%20novel%20framework%20that%20integrates%20few-shot%20named%0Aentity%20recognition%20%28NER%29%20with%20topic-aware%20contextual%20modeling%20to%20address%20the%0Achallenges%20of%20cross-lingual%20and%20low-resource%20scenarios.%20FewTopNER%20leverages%20a%0Ashared%20multilingual%20encoder%20based%20on%20XLM-RoBERTa%2C%20augmented%20with%0Alanguage-specific%20calibration%20mechanisms%2C%20to%20generate%20robust%20contextual%0Aembeddings.%20The%20architecture%20comprises%20a%20prototype-based%20entity%20recognition%0Abranch%2C%20employing%20BiLSTM%20and%20Conditional%20Random%20Fields%20for%20sequence%20labeling%2C%0Aand%20a%20topic%20modeling%20branch%20that%20extracts%20document-level%20semantic%20features%0Athrough%20hybrid%20probabilistic%20and%20neural%20methods.%20A%20cross-task%20bridge%0Afacilitates%20dynamic%20bidirectional%20attention%20and%20feature%20fusion%20between%20entity%0Aand%20topic%20representations%2C%20thereby%20enhancing%20entity%20disambiguation%20by%0Aincorporating%20global%20semantic%20context.%20Empirical%20evaluations%20on%20multilingual%0Abenchmarks%20across%20English%2C%20French%2C%20Spanish%2C%20German%2C%20and%20Italian%20demonstrate%0Athat%20FewTopNER%20significantly%20outperforms%20existing%20state-of-the-art%20few-shot%20NER%0Amodels.%20In%20particular%2C%20the%20framework%20achieves%20improvements%20of%202.5-4.0%0Apercentage%20points%20in%20F1%20score%20and%20exhibits%20enhanced%20topic%20coherence%2C%20as%0Ameasured%20by%20normalized%20pointwise%20mutual%20information.%20Ablation%20studies%20further%0Aconfirm%20the%20critical%20contributions%20of%20the%20shared%20encoder%20and%20cross-task%0Aintegration%20mechanisms%20to%20the%20overall%20performance.%20These%20results%20underscore%20the%0Aefficacy%20of%20incorporating%20topic-aware%20context%20into%20few-shot%20NER%20and%20highlight%0Athe%20potential%20of%20FewTopNER%20for%20robust%20cross-lingual%20applications%20in%0Alow-resource%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFewTopNER%253A%2520Integrating%2520Few-Shot%2520Learning%2520with%2520Topic%2520Modeling%2520and%2520Named%250A%2520%2520Entity%2520Recognition%2520in%2520a%2520Multilingual%2520Framework%26entry.906535625%3DIbrahim%2520Bouabdallaoui%2520and%2520Fatima%2520Guerouate%2520and%2520Samya%2520Bouhaddour%2520and%2520Chaimae%2520Saadi%2520and%2520Mohammed%2520Sbihi%26entry.1292438233%3D%2520%2520We%2520introduce%2520FewTopNER%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520few-shot%2520named%250Aentity%2520recognition%2520%2528NER%2529%2520with%2520topic-aware%2520contextual%2520modeling%2520to%2520address%2520the%250Achallenges%2520of%2520cross-lingual%2520and%2520low-resource%2520scenarios.%2520FewTopNER%2520leverages%2520a%250Ashared%2520multilingual%2520encoder%2520based%2520on%2520XLM-RoBERTa%252C%2520augmented%2520with%250Alanguage-specific%2520calibration%2520mechanisms%252C%2520to%2520generate%2520robust%2520contextual%250Aembeddings.%2520The%2520architecture%2520comprises%2520a%2520prototype-based%2520entity%2520recognition%250Abranch%252C%2520employing%2520BiLSTM%2520and%2520Conditional%2520Random%2520Fields%2520for%2520sequence%2520labeling%252C%250Aand%2520a%2520topic%2520modeling%2520branch%2520that%2520extracts%2520document-level%2520semantic%2520features%250Athrough%2520hybrid%2520probabilistic%2520and%2520neural%2520methods.%2520A%2520cross-task%2520bridge%250Afacilitates%2520dynamic%2520bidirectional%2520attention%2520and%2520feature%2520fusion%2520between%2520entity%250Aand%2520topic%2520representations%252C%2520thereby%2520enhancing%2520entity%2520disambiguation%2520by%250Aincorporating%2520global%2520semantic%2520context.%2520Empirical%2520evaluations%2520on%2520multilingual%250Abenchmarks%2520across%2520English%252C%2520French%252C%2520Spanish%252C%2520German%252C%2520and%2520Italian%2520demonstrate%250Athat%2520FewTopNER%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520few-shot%2520NER%250Amodels.%2520In%2520particular%252C%2520the%2520framework%2520achieves%2520improvements%2520of%25202.5-4.0%250Apercentage%2520points%2520in%2520F1%2520score%2520and%2520exhibits%2520enhanced%2520topic%2520coherence%252C%2520as%250Ameasured%2520by%2520normalized%2520pointwise%2520mutual%2520information.%2520Ablation%2520studies%2520further%250Aconfirm%2520the%2520critical%2520contributions%2520of%2520the%2520shared%2520encoder%2520and%2520cross-task%250Aintegration%2520mechanisms%2520to%2520the%2520overall%2520performance.%2520These%2520results%2520underscore%2520the%250Aefficacy%2520of%2520incorporating%2520topic-aware%2520context%2520into%2520few-shot%2520NER%2520and%2520highlight%250Athe%2520potential%2520of%2520FewTopNER%2520for%2520robust%2520cross-lingual%2520applications%2520in%250Alow-resource%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FewTopNER%3A%20Integrating%20Few-Shot%20Learning%20with%20Topic%20Modeling%20and%20Named%0A%20%20Entity%20Recognition%20in%20a%20Multilingual%20Framework&entry.906535625=Ibrahim%20Bouabdallaoui%20and%20Fatima%20Guerouate%20and%20Samya%20Bouhaddour%20and%20Chaimae%20Saadi%20and%20Mohammed%20Sbihi&entry.1292438233=%20%20We%20introduce%20FewTopNER%2C%20a%20novel%20framework%20that%20integrates%20few-shot%20named%0Aentity%20recognition%20%28NER%29%20with%20topic-aware%20contextual%20modeling%20to%20address%20the%0Achallenges%20of%20cross-lingual%20and%20low-resource%20scenarios.%20FewTopNER%20leverages%20a%0Ashared%20multilingual%20encoder%20based%20on%20XLM-RoBERTa%2C%20augmented%20with%0Alanguage-specific%20calibration%20mechanisms%2C%20to%20generate%20robust%20contextual%0Aembeddings.%20The%20architecture%20comprises%20a%20prototype-based%20entity%20recognition%0Abranch%2C%20employing%20BiLSTM%20and%20Conditional%20Random%20Fields%20for%20sequence%20labeling%2C%0Aand%20a%20topic%20modeling%20branch%20that%20extracts%20document-level%20semantic%20features%0Athrough%20hybrid%20probabilistic%20and%20neural%20methods.%20A%20cross-task%20bridge%0Afacilitates%20dynamic%20bidirectional%20attention%20and%20feature%20fusion%20between%20entity%0Aand%20topic%20representations%2C%20thereby%20enhancing%20entity%20disambiguation%20by%0Aincorporating%20global%20semantic%20context.%20Empirical%20evaluations%20on%20multilingual%0Abenchmarks%20across%20English%2C%20French%2C%20Spanish%2C%20German%2C%20and%20Italian%20demonstrate%0Athat%20FewTopNER%20significantly%20outperforms%20existing%20state-of-the-art%20few-shot%20NER%0Amodels.%20In%20particular%2C%20the%20framework%20achieves%20improvements%20of%202.5-4.0%0Apercentage%20points%20in%20F1%20score%20and%20exhibits%20enhanced%20topic%20coherence%2C%20as%0Ameasured%20by%20normalized%20pointwise%20mutual%20information.%20Ablation%20studies%20further%0Aconfirm%20the%20critical%20contributions%20of%20the%20shared%20encoder%20and%20cross-task%0Aintegration%20mechanisms%20to%20the%20overall%20performance.%20These%20results%20underscore%20the%0Aefficacy%20of%20incorporating%20topic-aware%20context%20into%20few-shot%20NER%20and%20highlight%0Athe%20potential%20of%20FewTopNER%20for%20robust%20cross-lingual%20applications%20in%0Alow-resource%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02391v1&entry.124074799=Read"},
{"title": "TabPFN Unleashed: A Scalable and Effective Solution to Tabular\n  Classification Problems", "author": "Si-Yang Liu and Han-Jia Ye", "abstract": "  TabPFN has emerged as a promising in-context learning model for tabular data,\ncapable of directly predicting the labels of test samples given labeled\ntraining examples. It has demonstrated competitive performance, particularly on\nsmall-scale classification tasks. However, despite its effectiveness, TabPFN\nstill requires further refinement in several areas, including handling\nhigh-dimensional features, aligning with downstream datasets, and scaling to\nlarger datasets. In this paper, we revisit existing variants of TabPFN and\nobserve that most approaches focus either on reducing bias or variance, often\nneglecting the need to address the other side, while also increasing inference\noverhead. To fill this gap, we propose Beta (Bagging and Encoder-based\nFine-tuning for TabPFN Adaptation), a novel and effective method designed to\nminimize both bias and variance. To reduce bias, we introduce a lightweight\nencoder to better align downstream tasks with the pre-trained TabPFN. By\nincreasing the number of encoders in a lightweight manner, Beta mitigate\nvariance, thereby further improving the model's performance. Additionally,\nbootstrapped sampling is employed to further reduce the impact of data\nperturbations on the model, all while maintaining computational efficiency\nduring inference. Our approach enhances TabPFN's ability to handle\nhigh-dimensional data and scale to larger datasets. Experimental results on\nover 200 benchmark classification datasets demonstrate that Beta either\noutperforms or matches state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2502.02527v1", "date": "2025-02-04", "relevancy": 2.4647, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5075}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabPFN%20Unleashed%3A%20A%20Scalable%20and%20Effective%20Solution%20to%20Tabular%0A%20%20Classification%20Problems&body=Title%3A%20TabPFN%20Unleashed%3A%20A%20Scalable%20and%20Effective%20Solution%20to%20Tabular%0A%20%20Classification%20Problems%0AAuthor%3A%20Si-Yang%20Liu%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20TabPFN%20has%20emerged%20as%20a%20promising%20in-context%20learning%20model%20for%20tabular%20data%2C%0Acapable%20of%20directly%20predicting%20the%20labels%20of%20test%20samples%20given%20labeled%0Atraining%20examples.%20It%20has%20demonstrated%20competitive%20performance%2C%20particularly%20on%0Asmall-scale%20classification%20tasks.%20However%2C%20despite%20its%20effectiveness%2C%20TabPFN%0Astill%20requires%20further%20refinement%20in%20several%20areas%2C%20including%20handling%0Ahigh-dimensional%20features%2C%20aligning%20with%20downstream%20datasets%2C%20and%20scaling%20to%0Alarger%20datasets.%20In%20this%20paper%2C%20we%20revisit%20existing%20variants%20of%20TabPFN%20and%0Aobserve%20that%20most%20approaches%20focus%20either%20on%20reducing%20bias%20or%20variance%2C%20often%0Aneglecting%20the%20need%20to%20address%20the%20other%20side%2C%20while%20also%20increasing%20inference%0Aoverhead.%20To%20fill%20this%20gap%2C%20we%20propose%20Beta%20%28Bagging%20and%20Encoder-based%0AFine-tuning%20for%20TabPFN%20Adaptation%29%2C%20a%20novel%20and%20effective%20method%20designed%20to%0Aminimize%20both%20bias%20and%20variance.%20To%20reduce%20bias%2C%20we%20introduce%20a%20lightweight%0Aencoder%20to%20better%20align%20downstream%20tasks%20with%20the%20pre-trained%20TabPFN.%20By%0Aincreasing%20the%20number%20of%20encoders%20in%20a%20lightweight%20manner%2C%20Beta%20mitigate%0Avariance%2C%20thereby%20further%20improving%20the%20model%27s%20performance.%20Additionally%2C%0Abootstrapped%20sampling%20is%20employed%20to%20further%20reduce%20the%20impact%20of%20data%0Aperturbations%20on%20the%20model%2C%20all%20while%20maintaining%20computational%20efficiency%0Aduring%20inference.%20Our%20approach%20enhances%20TabPFN%27s%20ability%20to%20handle%0Ahigh-dimensional%20data%20and%20scale%20to%20larger%20datasets.%20Experimental%20results%20on%0Aover%20200%20benchmark%20classification%20datasets%20demonstrate%20that%20Beta%20either%0Aoutperforms%20or%20matches%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabPFN%2520Unleashed%253A%2520A%2520Scalable%2520and%2520Effective%2520Solution%2520to%2520Tabular%250A%2520%2520Classification%2520Problems%26entry.906535625%3DSi-Yang%2520Liu%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520TabPFN%2520has%2520emerged%2520as%2520a%2520promising%2520in-context%2520learning%2520model%2520for%2520tabular%2520data%252C%250Acapable%2520of%2520directly%2520predicting%2520the%2520labels%2520of%2520test%2520samples%2520given%2520labeled%250Atraining%2520examples.%2520It%2520has%2520demonstrated%2520competitive%2520performance%252C%2520particularly%2520on%250Asmall-scale%2520classification%2520tasks.%2520However%252C%2520despite%2520its%2520effectiveness%252C%2520TabPFN%250Astill%2520requires%2520further%2520refinement%2520in%2520several%2520areas%252C%2520including%2520handling%250Ahigh-dimensional%2520features%252C%2520aligning%2520with%2520downstream%2520datasets%252C%2520and%2520scaling%2520to%250Alarger%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520existing%2520variants%2520of%2520TabPFN%2520and%250Aobserve%2520that%2520most%2520approaches%2520focus%2520either%2520on%2520reducing%2520bias%2520or%2520variance%252C%2520often%250Aneglecting%2520the%2520need%2520to%2520address%2520the%2520other%2520side%252C%2520while%2520also%2520increasing%2520inference%250Aoverhead.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520Beta%2520%2528Bagging%2520and%2520Encoder-based%250AFine-tuning%2520for%2520TabPFN%2520Adaptation%2529%252C%2520a%2520novel%2520and%2520effective%2520method%2520designed%2520to%250Aminimize%2520both%2520bias%2520and%2520variance.%2520To%2520reduce%2520bias%252C%2520we%2520introduce%2520a%2520lightweight%250Aencoder%2520to%2520better%2520align%2520downstream%2520tasks%2520with%2520the%2520pre-trained%2520TabPFN.%2520By%250Aincreasing%2520the%2520number%2520of%2520encoders%2520in%2520a%2520lightweight%2520manner%252C%2520Beta%2520mitigate%250Avariance%252C%2520thereby%2520further%2520improving%2520the%2520model%2527s%2520performance.%2520Additionally%252C%250Abootstrapped%2520sampling%2520is%2520employed%2520to%2520further%2520reduce%2520the%2520impact%2520of%2520data%250Aperturbations%2520on%2520the%2520model%252C%2520all%2520while%2520maintaining%2520computational%2520efficiency%250Aduring%2520inference.%2520Our%2520approach%2520enhances%2520TabPFN%2527s%2520ability%2520to%2520handle%250Ahigh-dimensional%2520data%2520and%2520scale%2520to%2520larger%2520datasets.%2520Experimental%2520results%2520on%250Aover%2520200%2520benchmark%2520classification%2520datasets%2520demonstrate%2520that%2520Beta%2520either%250Aoutperforms%2520or%2520matches%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabPFN%20Unleashed%3A%20A%20Scalable%20and%20Effective%20Solution%20to%20Tabular%0A%20%20Classification%20Problems&entry.906535625=Si-Yang%20Liu%20and%20Han-Jia%20Ye&entry.1292438233=%20%20TabPFN%20has%20emerged%20as%20a%20promising%20in-context%20learning%20model%20for%20tabular%20data%2C%0Acapable%20of%20directly%20predicting%20the%20labels%20of%20test%20samples%20given%20labeled%0Atraining%20examples.%20It%20has%20demonstrated%20competitive%20performance%2C%20particularly%20on%0Asmall-scale%20classification%20tasks.%20However%2C%20despite%20its%20effectiveness%2C%20TabPFN%0Astill%20requires%20further%20refinement%20in%20several%20areas%2C%20including%20handling%0Ahigh-dimensional%20features%2C%20aligning%20with%20downstream%20datasets%2C%20and%20scaling%20to%0Alarger%20datasets.%20In%20this%20paper%2C%20we%20revisit%20existing%20variants%20of%20TabPFN%20and%0Aobserve%20that%20most%20approaches%20focus%20either%20on%20reducing%20bias%20or%20variance%2C%20often%0Aneglecting%20the%20need%20to%20address%20the%20other%20side%2C%20while%20also%20increasing%20inference%0Aoverhead.%20To%20fill%20this%20gap%2C%20we%20propose%20Beta%20%28Bagging%20and%20Encoder-based%0AFine-tuning%20for%20TabPFN%20Adaptation%29%2C%20a%20novel%20and%20effective%20method%20designed%20to%0Aminimize%20both%20bias%20and%20variance.%20To%20reduce%20bias%2C%20we%20introduce%20a%20lightweight%0Aencoder%20to%20better%20align%20downstream%20tasks%20with%20the%20pre-trained%20TabPFN.%20By%0Aincreasing%20the%20number%20of%20encoders%20in%20a%20lightweight%20manner%2C%20Beta%20mitigate%0Avariance%2C%20thereby%20further%20improving%20the%20model%27s%20performance.%20Additionally%2C%0Abootstrapped%20sampling%20is%20employed%20to%20further%20reduce%20the%20impact%20of%20data%0Aperturbations%20on%20the%20model%2C%20all%20while%20maintaining%20computational%20efficiency%0Aduring%20inference.%20Our%20approach%20enhances%20TabPFN%27s%20ability%20to%20handle%0Ahigh-dimensional%20data%20and%20scale%20to%20larger%20datasets.%20Experimental%20results%20on%0Aover%20200%20benchmark%20classification%20datasets%20demonstrate%20that%20Beta%20either%0Aoutperforms%20or%20matches%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02527v1&entry.124074799=Read"},
{"title": "Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining", "author": "Dylan Sam and Ayan Chakrabarti and Afshin Rostamizadeh and Srikumar Ramalingam and Gui Citovsky and Sanjiv Kumar", "abstract": "  Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.\n", "link": "http://arxiv.org/abs/2502.02494v1", "date": "2025-02-04", "relevancy": 2.4623, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Similarity%20Metrics%20for%20Data%20Selection%20for%20Language%20Model%0A%20%20Pretraining&body=Title%3A%20Analyzing%20Similarity%20Metrics%20for%20Data%20Selection%20for%20Language%20Model%0A%20%20Pretraining%0AAuthor%3A%20Dylan%20Sam%20and%20Ayan%20Chakrabarti%20and%20Afshin%20Rostamizadeh%20and%20Srikumar%20Ramalingam%20and%20Gui%20Citovsky%20and%20Sanjiv%20Kumar%0AAbstract%3A%20%20%20Similarity%20between%20training%20examples%20is%20used%20to%20curate%20pretraining%20datasets%0Afor%20language%20models%20by%20many%20methods%20--%20for%20diversification%20and%20to%20select%0Aexamples%20similar%20to%20high-quality%20data.%20However%2C%20similarity%20is%20typically%0Ameasured%20with%20off-the-shelf%20embedding%20models%20that%20are%20generic%20or%20trained%20for%0Atasks%20such%20as%20retrieval.%20This%20paper%20introduces%20a%20framework%20to%20analyze%20the%0Asuitability%20of%20embedding%20models%20specifically%20for%20data%20curation%20in%20the%20language%0Amodel%20pretraining%20setting.%20We%20quantify%20the%20correlation%20between%20similarity%20in%0Athe%20embedding%20space%20to%20similarity%20in%20pretraining%20loss%20between%20different%0Atraining%20examples%2C%20and%20how%20diversifying%20in%20the%20embedding%20space%20affects%0Apretraining%20quality.%20We%20analyze%20a%20variety%20of%20embedding%20models%20in%20our%20framework%2C%0Awith%20experiments%20using%20the%20Pile%20dataset%20for%20pretraining%20a%201.7B%20parameter%0Adecoder-only%20language%20model.%20We%20find%20that%20the%20embedding%20models%20we%20consider%20are%0Aall%20useful%20for%20pretraining%20data%20curation.%20Moreover%2C%20a%20simple%20approach%20of%0Aaveraging%20per-token%20embeddings%20proves%20to%20be%20surprisingly%20competitive%20with%20more%0Asophisticated%20embedding%20models%20--%20likely%20because%20the%20latter%20are%20not%20designed%0Aspecifically%20for%20pretraining%20data%20curation.%20Indeed%2C%20we%20believe%20our%20analysis%20and%0Aevaluation%20framework%20can%20serve%20as%20a%20foundation%20for%20the%20design%20of%20embedding%0Amodels%20that%20specifically%20reason%20about%20similarity%20in%20pretraining%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Similarity%2520Metrics%2520for%2520Data%2520Selection%2520for%2520Language%2520Model%250A%2520%2520Pretraining%26entry.906535625%3DDylan%2520Sam%2520and%2520Ayan%2520Chakrabarti%2520and%2520Afshin%2520Rostamizadeh%2520and%2520Srikumar%2520Ramalingam%2520and%2520Gui%2520Citovsky%2520and%2520Sanjiv%2520Kumar%26entry.1292438233%3D%2520%2520Similarity%2520between%2520training%2520examples%2520is%2520used%2520to%2520curate%2520pretraining%2520datasets%250Afor%2520language%2520models%2520by%2520many%2520methods%2520--%2520for%2520diversification%2520and%2520to%2520select%250Aexamples%2520similar%2520to%2520high-quality%2520data.%2520However%252C%2520similarity%2520is%2520typically%250Ameasured%2520with%2520off-the-shelf%2520embedding%2520models%2520that%2520are%2520generic%2520or%2520trained%2520for%250Atasks%2520such%2520as%2520retrieval.%2520This%2520paper%2520introduces%2520a%2520framework%2520to%2520analyze%2520the%250Asuitability%2520of%2520embedding%2520models%2520specifically%2520for%2520data%2520curation%2520in%2520the%2520language%250Amodel%2520pretraining%2520setting.%2520We%2520quantify%2520the%2520correlation%2520between%2520similarity%2520in%250Athe%2520embedding%2520space%2520to%2520similarity%2520in%2520pretraining%2520loss%2520between%2520different%250Atraining%2520examples%252C%2520and%2520how%2520diversifying%2520in%2520the%2520embedding%2520space%2520affects%250Apretraining%2520quality.%2520We%2520analyze%2520a%2520variety%2520of%2520embedding%2520models%2520in%2520our%2520framework%252C%250Awith%2520experiments%2520using%2520the%2520Pile%2520dataset%2520for%2520pretraining%2520a%25201.7B%2520parameter%250Adecoder-only%2520language%2520model.%2520We%2520find%2520that%2520the%2520embedding%2520models%2520we%2520consider%2520are%250Aall%2520useful%2520for%2520pretraining%2520data%2520curation.%2520Moreover%252C%2520a%2520simple%2520approach%2520of%250Aaveraging%2520per-token%2520embeddings%2520proves%2520to%2520be%2520surprisingly%2520competitive%2520with%2520more%250Asophisticated%2520embedding%2520models%2520--%2520likely%2520because%2520the%2520latter%2520are%2520not%2520designed%250Aspecifically%2520for%2520pretraining%2520data%2520curation.%2520Indeed%252C%2520we%2520believe%2520our%2520analysis%2520and%250Aevaluation%2520framework%2520can%2520serve%2520as%2520a%2520foundation%2520for%2520the%2520design%2520of%2520embedding%250Amodels%2520that%2520specifically%2520reason%2520about%2520similarity%2520in%2520pretraining%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Similarity%20Metrics%20for%20Data%20Selection%20for%20Language%20Model%0A%20%20Pretraining&entry.906535625=Dylan%20Sam%20and%20Ayan%20Chakrabarti%20and%20Afshin%20Rostamizadeh%20and%20Srikumar%20Ramalingam%20and%20Gui%20Citovsky%20and%20Sanjiv%20Kumar&entry.1292438233=%20%20Similarity%20between%20training%20examples%20is%20used%20to%20curate%20pretraining%20datasets%0Afor%20language%20models%20by%20many%20methods%20--%20for%20diversification%20and%20to%20select%0Aexamples%20similar%20to%20high-quality%20data.%20However%2C%20similarity%20is%20typically%0Ameasured%20with%20off-the-shelf%20embedding%20models%20that%20are%20generic%20or%20trained%20for%0Atasks%20such%20as%20retrieval.%20This%20paper%20introduces%20a%20framework%20to%20analyze%20the%0Asuitability%20of%20embedding%20models%20specifically%20for%20data%20curation%20in%20the%20language%0Amodel%20pretraining%20setting.%20We%20quantify%20the%20correlation%20between%20similarity%20in%0Athe%20embedding%20space%20to%20similarity%20in%20pretraining%20loss%20between%20different%0Atraining%20examples%2C%20and%20how%20diversifying%20in%20the%20embedding%20space%20affects%0Apretraining%20quality.%20We%20analyze%20a%20variety%20of%20embedding%20models%20in%20our%20framework%2C%0Awith%20experiments%20using%20the%20Pile%20dataset%20for%20pretraining%20a%201.7B%20parameter%0Adecoder-only%20language%20model.%20We%20find%20that%20the%20embedding%20models%20we%20consider%20are%0Aall%20useful%20for%20pretraining%20data%20curation.%20Moreover%2C%20a%20simple%20approach%20of%0Aaveraging%20per-token%20embeddings%20proves%20to%20be%20surprisingly%20competitive%20with%20more%0Asophisticated%20embedding%20models%20--%20likely%20because%20the%20latter%20are%20not%20designed%0Aspecifically%20for%20pretraining%20data%20curation.%20Indeed%2C%20we%20believe%20our%20analysis%20and%0Aevaluation%20framework%20can%20serve%20as%20a%20foundation%20for%20the%20design%20of%20embedding%0Amodels%20that%20specifically%20reason%20about%20similarity%20in%20pretraining%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02494v1&entry.124074799=Read"},
{"title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video\n  Restoration", "author": "Jianyi Wang and Zhijie Lin and Meng Wei and Yang Zhao and Ceyuan Yang and Fei Xiao and Chen Change Loy and Lu Jiang", "abstract": "  Video restoration poses non-trivial challenges in maintaining fidelity while\nrecovering temporally consistent details from unknown degradations in the wild.\nDespite recent advances in diffusion-based restoration, these methods often\nface limitations in generation capability and sampling efficiency. In this\nwork, we present SeedVR, a diffusion transformer designed to handle real-world\nvideo restoration with arbitrary length and resolution. The core design of\nSeedVR lies in the shifted window attention that facilitates effective\nrestoration on long video sequences. SeedVR further supports variable-sized\nwindows near the boundary of both spatial and temporal dimensions, overcoming\nthe resolution constraints of traditional window attention. Equipped with\ncontemporary practices, including causal video autoencoder, mixed image and\nvideo training, and progressive training, SeedVR achieves highly-competitive\nperformance on both synthetic and real-world benchmarks, as well as\nAI-generated videos. Extensive experiments demonstrate SeedVR's superiority\nover existing methods for generic video restoration.\n", "link": "http://arxiv.org/abs/2501.01320v3", "date": "2025-02-04", "relevancy": 2.4611, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.631}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6044}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeedVR%3A%20Seeding%20Infinity%20in%20Diffusion%20Transformer%20Towards%20Generic%20Video%0A%20%20Restoration&body=Title%3A%20SeedVR%3A%20Seeding%20Infinity%20in%20Diffusion%20Transformer%20Towards%20Generic%20Video%0A%20%20Restoration%0AAuthor%3A%20Jianyi%20Wang%20and%20Zhijie%20Lin%20and%20Meng%20Wei%20and%20Yang%20Zhao%20and%20Ceyuan%20Yang%20and%20Fei%20Xiao%20and%20Chen%20Change%20Loy%20and%20Lu%20Jiang%0AAbstract%3A%20%20%20Video%20restoration%20poses%20non-trivial%20challenges%20in%20maintaining%20fidelity%20while%0Arecovering%20temporally%20consistent%20details%20from%20unknown%20degradations%20in%20the%20wild.%0ADespite%20recent%20advances%20in%20diffusion-based%20restoration%2C%20these%20methods%20often%0Aface%20limitations%20in%20generation%20capability%20and%20sampling%20efficiency.%20In%20this%0Awork%2C%20we%20present%20SeedVR%2C%20a%20diffusion%20transformer%20designed%20to%20handle%20real-world%0Avideo%20restoration%20with%20arbitrary%20length%20and%20resolution.%20The%20core%20design%20of%0ASeedVR%20lies%20in%20the%20shifted%20window%20attention%20that%20facilitates%20effective%0Arestoration%20on%20long%20video%20sequences.%20SeedVR%20further%20supports%20variable-sized%0Awindows%20near%20the%20boundary%20of%20both%20spatial%20and%20temporal%20dimensions%2C%20overcoming%0Athe%20resolution%20constraints%20of%20traditional%20window%20attention.%20Equipped%20with%0Acontemporary%20practices%2C%20including%20causal%20video%20autoencoder%2C%20mixed%20image%20and%0Avideo%20training%2C%20and%20progressive%20training%2C%20SeedVR%20achieves%20highly-competitive%0Aperformance%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20as%20well%20as%0AAI-generated%20videos.%20Extensive%20experiments%20demonstrate%20SeedVR%27s%20superiority%0Aover%20existing%20methods%20for%20generic%20video%20restoration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01320v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedVR%253A%2520Seeding%2520Infinity%2520in%2520Diffusion%2520Transformer%2520Towards%2520Generic%2520Video%250A%2520%2520Restoration%26entry.906535625%3DJianyi%2520Wang%2520and%2520Zhijie%2520Lin%2520and%2520Meng%2520Wei%2520and%2520Yang%2520Zhao%2520and%2520Ceyuan%2520Yang%2520and%2520Fei%2520Xiao%2520and%2520Chen%2520Change%2520Loy%2520and%2520Lu%2520Jiang%26entry.1292438233%3D%2520%2520Video%2520restoration%2520poses%2520non-trivial%2520challenges%2520in%2520maintaining%2520fidelity%2520while%250Arecovering%2520temporally%2520consistent%2520details%2520from%2520unknown%2520degradations%2520in%2520the%2520wild.%250ADespite%2520recent%2520advances%2520in%2520diffusion-based%2520restoration%252C%2520these%2520methods%2520often%250Aface%2520limitations%2520in%2520generation%2520capability%2520and%2520sampling%2520efficiency.%2520In%2520this%250Awork%252C%2520we%2520present%2520SeedVR%252C%2520a%2520diffusion%2520transformer%2520designed%2520to%2520handle%2520real-world%250Avideo%2520restoration%2520with%2520arbitrary%2520length%2520and%2520resolution.%2520The%2520core%2520design%2520of%250ASeedVR%2520lies%2520in%2520the%2520shifted%2520window%2520attention%2520that%2520facilitates%2520effective%250Arestoration%2520on%2520long%2520video%2520sequences.%2520SeedVR%2520further%2520supports%2520variable-sized%250Awindows%2520near%2520the%2520boundary%2520of%2520both%2520spatial%2520and%2520temporal%2520dimensions%252C%2520overcoming%250Athe%2520resolution%2520constraints%2520of%2520traditional%2520window%2520attention.%2520Equipped%2520with%250Acontemporary%2520practices%252C%2520including%2520causal%2520video%2520autoencoder%252C%2520mixed%2520image%2520and%250Avideo%2520training%252C%2520and%2520progressive%2520training%252C%2520SeedVR%2520achieves%2520highly-competitive%250Aperformance%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%252C%2520as%2520well%2520as%250AAI-generated%2520videos.%2520Extensive%2520experiments%2520demonstrate%2520SeedVR%2527s%2520superiority%250Aover%2520existing%2520methods%2520for%2520generic%2520video%2520restoration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01320v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeedVR%3A%20Seeding%20Infinity%20in%20Diffusion%20Transformer%20Towards%20Generic%20Video%0A%20%20Restoration&entry.906535625=Jianyi%20Wang%20and%20Zhijie%20Lin%20and%20Meng%20Wei%20and%20Yang%20Zhao%20and%20Ceyuan%20Yang%20and%20Fei%20Xiao%20and%20Chen%20Change%20Loy%20and%20Lu%20Jiang&entry.1292438233=%20%20Video%20restoration%20poses%20non-trivial%20challenges%20in%20maintaining%20fidelity%20while%0Arecovering%20temporally%20consistent%20details%20from%20unknown%20degradations%20in%20the%20wild.%0ADespite%20recent%20advances%20in%20diffusion-based%20restoration%2C%20these%20methods%20often%0Aface%20limitations%20in%20generation%20capability%20and%20sampling%20efficiency.%20In%20this%0Awork%2C%20we%20present%20SeedVR%2C%20a%20diffusion%20transformer%20designed%20to%20handle%20real-world%0Avideo%20restoration%20with%20arbitrary%20length%20and%20resolution.%20The%20core%20design%20of%0ASeedVR%20lies%20in%20the%20shifted%20window%20attention%20that%20facilitates%20effective%0Arestoration%20on%20long%20video%20sequences.%20SeedVR%20further%20supports%20variable-sized%0Awindows%20near%20the%20boundary%20of%20both%20spatial%20and%20temporal%20dimensions%2C%20overcoming%0Athe%20resolution%20constraints%20of%20traditional%20window%20attention.%20Equipped%20with%0Acontemporary%20practices%2C%20including%20causal%20video%20autoencoder%2C%20mixed%20image%20and%0Avideo%20training%2C%20and%20progressive%20training%2C%20SeedVR%20achieves%20highly-competitive%0Aperformance%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20as%20well%20as%0AAI-generated%20videos.%20Extensive%20experiments%20demonstrate%20SeedVR%27s%20superiority%0Aover%20existing%20methods%20for%20generic%20video%20restoration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01320v3&entry.124074799=Read"},
{"title": "Activation-Informed Merging of Large Language Models", "author": "Amin Heyrani Nobari and Kaveh Alimohammadi and Ali ArjomandBigdeli and Akash Srivastava and Faez Ahmed and Navid Azizan", "abstract": "  Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance.\n", "link": "http://arxiv.org/abs/2502.02421v1", "date": "2025-02-04", "relevancy": 2.4583, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation-Informed%20Merging%20of%20Large%20Language%20Models&body=Title%3A%20Activation-Informed%20Merging%20of%20Large%20Language%20Models%0AAuthor%3A%20Amin%20Heyrani%20Nobari%20and%20Kaveh%20Alimohammadi%20and%20Ali%20ArjomandBigdeli%20and%20Akash%20Srivastava%20and%20Faez%20Ahmed%20and%20Navid%20Azizan%0AAbstract%3A%20%20%20Model%20merging%2C%20a%20method%20that%20combines%20the%20parameters%20and%20embeddings%20of%0Amultiple%20fine-tuned%20large%20language%20models%20%28LLMs%29%2C%20offers%20a%20promising%20approach%0Ato%20enhance%20model%20performance%20across%20various%20tasks%20while%20maintaining%0Acomputational%20efficiency.%20This%20paper%20introduces%20Activation-Informed%20Merging%0A%28AIM%29%2C%20a%20technique%20that%20integrates%20the%20information%20from%20the%20activation%20space%20of%0ALLMs%20into%20the%20merging%20process%20to%20improve%20performance%20and%20robustness.%20AIM%20is%0Adesigned%20as%20a%20flexible%2C%20complementary%20solution%20that%20is%20applicable%20to%20any%0Aexisting%20merging%20method.%20It%20aims%20to%20preserve%20critical%20weights%20from%20the%20base%0Amodel%2C%20drawing%20on%20principles%20from%20continual%20learning~%28CL%29%20and%20model%0Acompression.%20Utilizing%20a%20task-agnostic%20calibration%20set%2C%20AIM%20selectively%0Aprioritizes%20essential%20weights%20during%20merging.%20We%20empirically%20demonstrate%20that%0AAIM%20significantly%20enhances%20the%20performance%20of%20merged%20models%20across%20multiple%0Abenchmarks.%20Our%20findings%20suggest%20that%20considering%20the%20activation-space%0Ainformation%20can%20provide%20substantial%20advancements%20in%20the%20model%20merging%0Astrategies%20for%20LLMs%20with%20up%20to%2040%5C%25%20increase%20in%20benchmark%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation-Informed%2520Merging%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DAmin%2520Heyrani%2520Nobari%2520and%2520Kaveh%2520Alimohammadi%2520and%2520Ali%2520ArjomandBigdeli%2520and%2520Akash%2520Srivastava%2520and%2520Faez%2520Ahmed%2520and%2520Navid%2520Azizan%26entry.1292438233%3D%2520%2520Model%2520merging%252C%2520a%2520method%2520that%2520combines%2520the%2520parameters%2520and%2520embeddings%2520of%250Amultiple%2520fine-tuned%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520offers%2520a%2520promising%2520approach%250Ato%2520enhance%2520model%2520performance%2520across%2520various%2520tasks%2520while%2520maintaining%250Acomputational%2520efficiency.%2520This%2520paper%2520introduces%2520Activation-Informed%2520Merging%250A%2528AIM%2529%252C%2520a%2520technique%2520that%2520integrates%2520the%2520information%2520from%2520the%2520activation%2520space%2520of%250ALLMs%2520into%2520the%2520merging%2520process%2520to%2520improve%2520performance%2520and%2520robustness.%2520AIM%2520is%250Adesigned%2520as%2520a%2520flexible%252C%2520complementary%2520solution%2520that%2520is%2520applicable%2520to%2520any%250Aexisting%2520merging%2520method.%2520It%2520aims%2520to%2520preserve%2520critical%2520weights%2520from%2520the%2520base%250Amodel%252C%2520drawing%2520on%2520principles%2520from%2520continual%2520learning~%2528CL%2529%2520and%2520model%250Acompression.%2520Utilizing%2520a%2520task-agnostic%2520calibration%2520set%252C%2520AIM%2520selectively%250Aprioritizes%2520essential%2520weights%2520during%2520merging.%2520We%2520empirically%2520demonstrate%2520that%250AAIM%2520significantly%2520enhances%2520the%2520performance%2520of%2520merged%2520models%2520across%2520multiple%250Abenchmarks.%2520Our%2520findings%2520suggest%2520that%2520considering%2520the%2520activation-space%250Ainformation%2520can%2520provide%2520substantial%2520advancements%2520in%2520the%2520model%2520merging%250Astrategies%2520for%2520LLMs%2520with%2520up%2520to%252040%255C%2525%2520increase%2520in%2520benchmark%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation-Informed%20Merging%20of%20Large%20Language%20Models&entry.906535625=Amin%20Heyrani%20Nobari%20and%20Kaveh%20Alimohammadi%20and%20Ali%20ArjomandBigdeli%20and%20Akash%20Srivastava%20and%20Faez%20Ahmed%20and%20Navid%20Azizan&entry.1292438233=%20%20Model%20merging%2C%20a%20method%20that%20combines%20the%20parameters%20and%20embeddings%20of%0Amultiple%20fine-tuned%20large%20language%20models%20%28LLMs%29%2C%20offers%20a%20promising%20approach%0Ato%20enhance%20model%20performance%20across%20various%20tasks%20while%20maintaining%0Acomputational%20efficiency.%20This%20paper%20introduces%20Activation-Informed%20Merging%0A%28AIM%29%2C%20a%20technique%20that%20integrates%20the%20information%20from%20the%20activation%20space%20of%0ALLMs%20into%20the%20merging%20process%20to%20improve%20performance%20and%20robustness.%20AIM%20is%0Adesigned%20as%20a%20flexible%2C%20complementary%20solution%20that%20is%20applicable%20to%20any%0Aexisting%20merging%20method.%20It%20aims%20to%20preserve%20critical%20weights%20from%20the%20base%0Amodel%2C%20drawing%20on%20principles%20from%20continual%20learning~%28CL%29%20and%20model%0Acompression.%20Utilizing%20a%20task-agnostic%20calibration%20set%2C%20AIM%20selectively%0Aprioritizes%20essential%20weights%20during%20merging.%20We%20empirically%20demonstrate%20that%0AAIM%20significantly%20enhances%20the%20performance%20of%20merged%20models%20across%20multiple%0Abenchmarks.%20Our%20findings%20suggest%20that%20considering%20the%20activation-space%0Ainformation%20can%20provide%20substantial%20advancements%20in%20the%20model%20merging%0Astrategies%20for%20LLMs%20with%20up%20to%2040%5C%25%20increase%20in%20benchmark%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02421v1&entry.124074799=Read"},
{"title": "Orientation-Aware Graph Neural Networks for Protein Structure\n  Representation Learning", "author": "Jiahan Li and Shitong Luo and Congyue Deng and Chaoran Cheng and Jiaqi Guan and Leonidas Guibas and Jian Peng and Jianzhu Ma", "abstract": "  By folding into particular 3D structures, proteins play a key role in living\nbeings. To learn meaningful representation from a protein structure for\ndownstream tasks, not only the global backbone topology but the local\nfine-grained orientational relations between amino acids should also be\nconsidered. In this work, we propose the Orientation-Aware Graph Neural\nNetworks (OAGNNs) to better sense the geometric characteristics in protein\nstructure (e.g. inner-residue torsion angles, inter-residue orientations).\nExtending a single weight from a scalar to a 3D vector, we construct a rich set\nof geometric-meaningful operations to process both the classical and SO(3)\nrepresentations of a given structure. To plug our designed perceptron unit into\nexisting Graph Neural Networks, we further introduce an equivariant message\npassing paradigm, showing superior versatility in maintaining\nSO(3)-equivariance at the global scale. Experiments have shown that our OAGNNs\nhave a remarkable ability to sense geometric orientational features compared to\nclassical networks. OAGNNs have also achieved state-of-the-art performance on\nvarious computational biology applications related to protein 3D structures.\nThe code is available at https://github.com/Ced3-han/OAGNN/tree/main.\n", "link": "http://arxiv.org/abs/2201.13299v6", "date": "2025-02-04", "relevancy": 2.4533, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning&body=Title%3A%20Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning%0AAuthor%3A%20Jiahan%20Li%20and%20Shitong%20Luo%20and%20Congyue%20Deng%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Leonidas%20Guibas%20and%20Jian%20Peng%20and%20Jianzhu%20Ma%0AAbstract%3A%20%20%20By%20folding%20into%20particular%203D%20structures%2C%20proteins%20play%20a%20key%20role%20in%20living%0Abeings.%20To%20learn%20meaningful%20representation%20from%20a%20protein%20structure%20for%0Adownstream%20tasks%2C%20not%20only%20the%20global%20backbone%20topology%20but%20the%20local%0Afine-grained%20orientational%20relations%20between%20amino%20acids%20should%20also%20be%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20the%20Orientation-Aware%20Graph%20Neural%0ANetworks%20%28OAGNNs%29%20to%20better%20sense%20the%20geometric%20characteristics%20in%20protein%0Astructure%20%28e.g.%20inner-residue%20torsion%20angles%2C%20inter-residue%20orientations%29.%0AExtending%20a%20single%20weight%20from%20a%20scalar%20to%20a%203D%20vector%2C%20we%20construct%20a%20rich%20set%0Aof%20geometric-meaningful%20operations%20to%20process%20both%20the%20classical%20and%20SO%283%29%0Arepresentations%20of%20a%20given%20structure.%20To%20plug%20our%20designed%20perceptron%20unit%20into%0Aexisting%20Graph%20Neural%20Networks%2C%20we%20further%20introduce%20an%20equivariant%20message%0Apassing%20paradigm%2C%20showing%20superior%20versatility%20in%20maintaining%0ASO%283%29-equivariance%20at%20the%20global%20scale.%20Experiments%20have%20shown%20that%20our%20OAGNNs%0Ahave%20a%20remarkable%20ability%20to%20sense%20geometric%20orientational%20features%20compared%20to%0Aclassical%20networks.%20OAGNNs%20have%20also%20achieved%20state-of-the-art%20performance%20on%0Avarious%20computational%20biology%20applications%20related%20to%20protein%203D%20structures.%0AThe%20code%20is%20available%20at%20https%3A//github.com/Ced3-han/OAGNN/tree/main.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.13299v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrientation-Aware%2520Graph%2520Neural%2520Networks%2520for%2520Protein%2520Structure%250A%2520%2520Representation%2520Learning%26entry.906535625%3DJiahan%2520Li%2520and%2520Shitong%2520Luo%2520and%2520Congyue%2520Deng%2520and%2520Chaoran%2520Cheng%2520and%2520Jiaqi%2520Guan%2520and%2520Leonidas%2520Guibas%2520and%2520Jian%2520Peng%2520and%2520Jianzhu%2520Ma%26entry.1292438233%3D%2520%2520By%2520folding%2520into%2520particular%25203D%2520structures%252C%2520proteins%2520play%2520a%2520key%2520role%2520in%2520living%250Abeings.%2520To%2520learn%2520meaningful%2520representation%2520from%2520a%2520protein%2520structure%2520for%250Adownstream%2520tasks%252C%2520not%2520only%2520the%2520global%2520backbone%2520topology%2520but%2520the%2520local%250Afine-grained%2520orientational%2520relations%2520between%2520amino%2520acids%2520should%2520also%2520be%250Aconsidered.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Orientation-Aware%2520Graph%2520Neural%250ANetworks%2520%2528OAGNNs%2529%2520to%2520better%2520sense%2520the%2520geometric%2520characteristics%2520in%2520protein%250Astructure%2520%2528e.g.%2520inner-residue%2520torsion%2520angles%252C%2520inter-residue%2520orientations%2529.%250AExtending%2520a%2520single%2520weight%2520from%2520a%2520scalar%2520to%2520a%25203D%2520vector%252C%2520we%2520construct%2520a%2520rich%2520set%250Aof%2520geometric-meaningful%2520operations%2520to%2520process%2520both%2520the%2520classical%2520and%2520SO%25283%2529%250Arepresentations%2520of%2520a%2520given%2520structure.%2520To%2520plug%2520our%2520designed%2520perceptron%2520unit%2520into%250Aexisting%2520Graph%2520Neural%2520Networks%252C%2520we%2520further%2520introduce%2520an%2520equivariant%2520message%250Apassing%2520paradigm%252C%2520showing%2520superior%2520versatility%2520in%2520maintaining%250ASO%25283%2529-equivariance%2520at%2520the%2520global%2520scale.%2520Experiments%2520have%2520shown%2520that%2520our%2520OAGNNs%250Ahave%2520a%2520remarkable%2520ability%2520to%2520sense%2520geometric%2520orientational%2520features%2520compared%2520to%250Aclassical%2520networks.%2520OAGNNs%2520have%2520also%2520achieved%2520state-of-the-art%2520performance%2520on%250Avarious%2520computational%2520biology%2520applications%2520related%2520to%2520protein%25203D%2520structures.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/Ced3-han/OAGNN/tree/main.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.13299v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning&entry.906535625=Jiahan%20Li%20and%20Shitong%20Luo%20and%20Congyue%20Deng%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Leonidas%20Guibas%20and%20Jian%20Peng%20and%20Jianzhu%20Ma&entry.1292438233=%20%20By%20folding%20into%20particular%203D%20structures%2C%20proteins%20play%20a%20key%20role%20in%20living%0Abeings.%20To%20learn%20meaningful%20representation%20from%20a%20protein%20structure%20for%0Adownstream%20tasks%2C%20not%20only%20the%20global%20backbone%20topology%20but%20the%20local%0Afine-grained%20orientational%20relations%20between%20amino%20acids%20should%20also%20be%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20the%20Orientation-Aware%20Graph%20Neural%0ANetworks%20%28OAGNNs%29%20to%20better%20sense%20the%20geometric%20characteristics%20in%20protein%0Astructure%20%28e.g.%20inner-residue%20torsion%20angles%2C%20inter-residue%20orientations%29.%0AExtending%20a%20single%20weight%20from%20a%20scalar%20to%20a%203D%20vector%2C%20we%20construct%20a%20rich%20set%0Aof%20geometric-meaningful%20operations%20to%20process%20both%20the%20classical%20and%20SO%283%29%0Arepresentations%20of%20a%20given%20structure.%20To%20plug%20our%20designed%20perceptron%20unit%20into%0Aexisting%20Graph%20Neural%20Networks%2C%20we%20further%20introduce%20an%20equivariant%20message%0Apassing%20paradigm%2C%20showing%20superior%20versatility%20in%20maintaining%0ASO%283%29-equivariance%20at%20the%20global%20scale.%20Experiments%20have%20shown%20that%20our%20OAGNNs%0Ahave%20a%20remarkable%20ability%20to%20sense%20geometric%20orientational%20features%20compared%20to%0Aclassical%20networks.%20OAGNNs%20have%20also%20achieved%20state-of-the-art%20performance%20on%0Avarious%20computational%20biology%20applications%20related%20to%20protein%203D%20structures.%0AThe%20code%20is%20available%20at%20https%3A//github.com/Ced3-han/OAGNN/tree/main.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.13299v6&entry.124074799=Read"},
{"title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\n  Generation in Video Models", "author": "Hila Chefer and Uriel Singer and Amit Zohar and Yuval Kirstain and Adam Polyak and Yaniv Taigman and Lior Wolf and Shelly Sheynin", "abstract": "  Despite tremendous recent progress, generative video models still struggle to\ncapture real-world motion, dynamics, and physics. We show that this limitation\narises from the conventional pixel reconstruction objective, which biases\nmodels toward appearance fidelity at the expense of motion coherence. To\naddress this, we introduce VideoJAM, a novel framework that instills an\neffective motion prior to video generators, by encouraging the model to learn a\njoint appearance-motion representation. VideoJAM is composed of two\ncomplementary units. During training, we extend the objective to predict both\nthe generated pixels and their corresponding motion from a single learned\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\nsteers the generation toward coherent motion by leveraging the model's own\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\ncan be applied to any video model with minimal adaptations, requiring no\nmodifications to the training data or scaling of the model. VideoJAM achieves\nstate-of-the-art performance in motion coherence, surpassing highly competitive\nproprietary models while also enhancing the perceived visual quality of the\ngenerations. These findings emphasize that appearance and motion can be\ncomplementary and, when effectively integrated, enhance both the visual quality\nand the coherence of video generation. Project website:\nhttps://hila-chefer.github.io/videojam-paper.github.io/\n", "link": "http://arxiv.org/abs/2502.02492v1", "date": "2025-02-04", "relevancy": 2.4508, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6264}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6174}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models&body=Title%3A%20VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models%0AAuthor%3A%20Hila%20Chefer%20and%20Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Adam%20Polyak%20and%20Yaniv%20Taigman%20and%20Lior%20Wolf%20and%20Shelly%20Sheynin%0AAbstract%3A%20%20%20Despite%20tremendous%20recent%20progress%2C%20generative%20video%20models%20still%20struggle%20to%0Acapture%20real-world%20motion%2C%20dynamics%2C%20and%20physics.%20We%20show%20that%20this%20limitation%0Aarises%20from%20the%20conventional%20pixel%20reconstruction%20objective%2C%20which%20biases%0Amodels%20toward%20appearance%20fidelity%20at%20the%20expense%20of%20motion%20coherence.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoJAM%2C%20a%20novel%20framework%20that%20instills%20an%0Aeffective%20motion%20prior%20to%20video%20generators%2C%20by%20encouraging%20the%20model%20to%20learn%20a%0Ajoint%20appearance-motion%20representation.%20VideoJAM%20is%20composed%20of%20two%0Acomplementary%20units.%20During%20training%2C%20we%20extend%20the%20objective%20to%20predict%20both%0Athe%20generated%20pixels%20and%20their%20corresponding%20motion%20from%20a%20single%20learned%0Arepresentation.%20During%20inference%2C%20we%20introduce%20Inner-Guidance%2C%20a%20mechanism%20that%0Asteers%20the%20generation%20toward%20coherent%20motion%20by%20leveraging%20the%20model%27s%20own%0Aevolving%20motion%20prediction%20as%20a%20dynamic%20guidance%20signal.%20Notably%2C%20our%20framework%0Acan%20be%20applied%20to%20any%20video%20model%20with%20minimal%20adaptations%2C%20requiring%20no%0Amodifications%20to%20the%20training%20data%20or%20scaling%20of%20the%20model.%20VideoJAM%20achieves%0Astate-of-the-art%20performance%20in%20motion%20coherence%2C%20surpassing%20highly%20competitive%0Aproprietary%20models%20while%20also%20enhancing%20the%20perceived%20visual%20quality%20of%20the%0Agenerations.%20These%20findings%20emphasize%20that%20appearance%20and%20motion%20can%20be%0Acomplementary%20and%2C%20when%20effectively%20integrated%2C%20enhance%20both%20the%20visual%20quality%0Aand%20the%20coherence%20of%20video%20generation.%20Project%20website%3A%0Ahttps%3A//hila-chefer.github.io/videojam-paper.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoJAM%253A%2520Joint%2520Appearance-Motion%2520Representations%2520for%2520Enhanced%2520Motion%250A%2520%2520Generation%2520in%2520Video%2520Models%26entry.906535625%3DHila%2520Chefer%2520and%2520Uriel%2520Singer%2520and%2520Amit%2520Zohar%2520and%2520Yuval%2520Kirstain%2520and%2520Adam%2520Polyak%2520and%2520Yaniv%2520Taigman%2520and%2520Lior%2520Wolf%2520and%2520Shelly%2520Sheynin%26entry.1292438233%3D%2520%2520Despite%2520tremendous%2520recent%2520progress%252C%2520generative%2520video%2520models%2520still%2520struggle%2520to%250Acapture%2520real-world%2520motion%252C%2520dynamics%252C%2520and%2520physics.%2520We%2520show%2520that%2520this%2520limitation%250Aarises%2520from%2520the%2520conventional%2520pixel%2520reconstruction%2520objective%252C%2520which%2520biases%250Amodels%2520toward%2520appearance%2520fidelity%2520at%2520the%2520expense%2520of%2520motion%2520coherence.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520VideoJAM%252C%2520a%2520novel%2520framework%2520that%2520instills%2520an%250Aeffective%2520motion%2520prior%2520to%2520video%2520generators%252C%2520by%2520encouraging%2520the%2520model%2520to%2520learn%2520a%250Ajoint%2520appearance-motion%2520representation.%2520VideoJAM%2520is%2520composed%2520of%2520two%250Acomplementary%2520units.%2520During%2520training%252C%2520we%2520extend%2520the%2520objective%2520to%2520predict%2520both%250Athe%2520generated%2520pixels%2520and%2520their%2520corresponding%2520motion%2520from%2520a%2520single%2520learned%250Arepresentation.%2520During%2520inference%252C%2520we%2520introduce%2520Inner-Guidance%252C%2520a%2520mechanism%2520that%250Asteers%2520the%2520generation%2520toward%2520coherent%2520motion%2520by%2520leveraging%2520the%2520model%2527s%2520own%250Aevolving%2520motion%2520prediction%2520as%2520a%2520dynamic%2520guidance%2520signal.%2520Notably%252C%2520our%2520framework%250Acan%2520be%2520applied%2520to%2520any%2520video%2520model%2520with%2520minimal%2520adaptations%252C%2520requiring%2520no%250Amodifications%2520to%2520the%2520training%2520data%2520or%2520scaling%2520of%2520the%2520model.%2520VideoJAM%2520achieves%250Astate-of-the-art%2520performance%2520in%2520motion%2520coherence%252C%2520surpassing%2520highly%2520competitive%250Aproprietary%2520models%2520while%2520also%2520enhancing%2520the%2520perceived%2520visual%2520quality%2520of%2520the%250Agenerations.%2520These%2520findings%2520emphasize%2520that%2520appearance%2520and%2520motion%2520can%2520be%250Acomplementary%2520and%252C%2520when%2520effectively%2520integrated%252C%2520enhance%2520both%2520the%2520visual%2520quality%250Aand%2520the%2520coherence%2520of%2520video%2520generation.%2520Project%2520website%253A%250Ahttps%253A//hila-chefer.github.io/videojam-paper.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models&entry.906535625=Hila%20Chefer%20and%20Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Adam%20Polyak%20and%20Yaniv%20Taigman%20and%20Lior%20Wolf%20and%20Shelly%20Sheynin&entry.1292438233=%20%20Despite%20tremendous%20recent%20progress%2C%20generative%20video%20models%20still%20struggle%20to%0Acapture%20real-world%20motion%2C%20dynamics%2C%20and%20physics.%20We%20show%20that%20this%20limitation%0Aarises%20from%20the%20conventional%20pixel%20reconstruction%20objective%2C%20which%20biases%0Amodels%20toward%20appearance%20fidelity%20at%20the%20expense%20of%20motion%20coherence.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoJAM%2C%20a%20novel%20framework%20that%20instills%20an%0Aeffective%20motion%20prior%20to%20video%20generators%2C%20by%20encouraging%20the%20model%20to%20learn%20a%0Ajoint%20appearance-motion%20representation.%20VideoJAM%20is%20composed%20of%20two%0Acomplementary%20units.%20During%20training%2C%20we%20extend%20the%20objective%20to%20predict%20both%0Athe%20generated%20pixels%20and%20their%20corresponding%20motion%20from%20a%20single%20learned%0Arepresentation.%20During%20inference%2C%20we%20introduce%20Inner-Guidance%2C%20a%20mechanism%20that%0Asteers%20the%20generation%20toward%20coherent%20motion%20by%20leveraging%20the%20model%27s%20own%0Aevolving%20motion%20prediction%20as%20a%20dynamic%20guidance%20signal.%20Notably%2C%20our%20framework%0Acan%20be%20applied%20to%20any%20video%20model%20with%20minimal%20adaptations%2C%20requiring%20no%0Amodifications%20to%20the%20training%20data%20or%20scaling%20of%20the%20model.%20VideoJAM%20achieves%0Astate-of-the-art%20performance%20in%20motion%20coherence%2C%20surpassing%20highly%20competitive%0Aproprietary%20models%20while%20also%20enhancing%20the%20perceived%20visual%20quality%20of%20the%0Agenerations.%20These%20findings%20emphasize%20that%20appearance%20and%20motion%20can%20be%0Acomplementary%20and%2C%20when%20effectively%20integrated%2C%20enhance%20both%20the%20visual%20quality%0Aand%20the%20coherence%20of%20video%20generation.%20Project%20website%3A%0Ahttps%3A//hila-chefer.github.io/videojam-paper.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02492v1&entry.124074799=Read"},
{"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models", "author": "Haoran Ye and Tianze Zhang and Yuhang Xie and Liyuan Zhang and Yuanyi Ren and Xin Zhang and Guojie Song", "abstract": "  Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.\n", "link": "http://arxiv.org/abs/2502.02444v1", "date": "2025-02-04", "relevancy": 2.4466, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song%0AAbstract%3A%20%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Psycho-Lexical%2520Approach%2520for%2520Constructing%2520Value%2520Systems%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520Ye%2520and%2520Tianze%2520Zhang%2520and%2520Yuhang%2520Xie%2520and%2520Liyuan%2520Zhang%2520and%2520Yuanyi%2520Ren%2520and%2520Xin%2520Zhang%2520and%2520Guojie%2520Song%26entry.1292438233%3D%2520%2520Values%2520are%2520core%2520drivers%2520of%2520individual%2520and%2520collective%2520perception%252C%2520cognition%252C%250Aand%2520behavior.%2520Value%2520systems%252C%2520such%2520as%2520Schwartz%2527s%2520Theory%2520of%2520Basic%2520Human%2520Values%252C%250Adelineate%2520the%2520hierarchy%2520and%2520interplay%2520among%2520these%2520values%252C%2520enabling%250Across-disciplinary%2520investigations%2520into%2520decision-making%2520and%2520societal%2520dynamics.%250ARecently%252C%2520the%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520raised%2520concerns%250Aregarding%2520their%2520elusive%2520intrinsic%2520values.%2520Despite%2520growing%2520efforts%2520in%250Aevaluating%252C%2520understanding%252C%2520and%2520aligning%2520LLM%2520values%252C%2520a%2520psychologically%2520grounded%250ALLM%2520value%2520system%2520remains%2520underexplored.%2520This%2520study%2520addresses%2520the%2520gap%2520by%250Aintroducing%2520the%2520Generative%2520Psycho-Lexical%2520Approach%2520%2528GPLA%2529%252C%2520a%2520scalable%252C%250Aadaptable%252C%2520and%2520theoretically%2520informed%2520method%2520for%2520constructing%2520value%2520systems.%250ALeveraging%2520GPLA%252C%2520we%2520propose%2520a%2520psychologically%2520grounded%2520five-factor%2520value%2520system%250Atailored%2520for%2520LLMs.%2520For%2520systematic%2520validation%252C%2520we%2520present%2520three%2520benchmarking%250Atasks%2520that%2520integrate%2520psychological%2520principles%2520with%2520cutting-edge%2520AI%2520priorities.%250AOur%2520results%2520reveal%2520that%2520the%2520proposed%2520value%2520system%2520meets%2520standard%2520psychological%250Acriteria%252C%2520better%2520captures%2520LLM%2520values%252C%2520improves%2520LLM%2520safety%2520prediction%252C%2520and%250Aenhances%2520LLM%2520alignment%252C%2520when%2520compared%2520to%2520the%2520canonical%2520Schwartz%2527s%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&entry.906535625=Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song&entry.1292438233=%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02444v1&entry.124074799=Read"},
{"title": "Prostate-Specific Foundation Models for Enhanced Detection of Clinically\n  Significant Cancer", "author": "Jeong Hoon Lee and Cynthia Xinran Li and Hassan Jahanandish and Indrani Bhattacharya and Sulaiman Vesal and Lichun Zhang and Shengtian Sang and Moon Hyung Choi and Simon John Christoph Soerensen and Steve Ran Zhou and Elijah Richard Sommer and Richard Fan and Pejman Ghanouni and Yuze Song and Tyler M. Seibert and Geoffrey A. Sonn and Mirabela Rusu", "abstract": "  Accurate prostate cancer diagnosis remains challenging. Even when using MRI,\nradiologists exhibit low specificity and significant inter-observer\nvariability, leading to potential delays or inaccuracies in identifying\nclinically significant cancers. This leads to numerous unnecessary biopsies and\nrisks of missing clinically significant cancers. Here we present prostate\nvision contrastive network (ProViCNet), prostate organ-specific vision\nfoundation models for Magnetic Resonance Imaging (MRI) and Trans-Rectal\nUltrasound imaging (TRUS) for comprehensive cancer detection. ProViCNet was\ntrained and validated using 4,401 patients across six institutions, as a\nprostate cancer detection model on radiology images relying on patch-level\ncontrastive learning guided by biopsy confirmed radiologist annotations.\nProViCNet demonstrated consistent performance across multiple internal and\nexternal validation cohorts with area under the receiver operating curve values\nranging from 0.875 to 0.966, significantly outperforming radiologists in the\nreader study (0.907 versus 0.805, p<0.001) for mpMRI, while achieving 0.670 to\n0.740 for TRUS. We also integrated ProViCNet with standard PSA to develop a\nvirtual screening test, and we showed that we can maintain the high sensitivity\nfor detecting clinically significant cancers while more than doubling\nspecificity from 15% to 38% (p<0.001), thereby substantially reducing\nunnecessary biopsies. These findings highlight that ProViCNet's potential for\nenhancing prostate cancer diagnosis accuracy and reduce unnecessary biopsies,\nthereby optimizing diagnostic pathways.\n", "link": "http://arxiv.org/abs/2502.00366v2", "date": "2025-02-04", "relevancy": 2.4308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prostate-Specific%20Foundation%20Models%20for%20Enhanced%20Detection%20of%20Clinically%0A%20%20Significant%20Cancer&body=Title%3A%20Prostate-Specific%20Foundation%20Models%20for%20Enhanced%20Detection%20of%20Clinically%0A%20%20Significant%20Cancer%0AAuthor%3A%20Jeong%20Hoon%20Lee%20and%20Cynthia%20Xinran%20Li%20and%20Hassan%20Jahanandish%20and%20Indrani%20Bhattacharya%20and%20Sulaiman%20Vesal%20and%20Lichun%20Zhang%20and%20Shengtian%20Sang%20and%20Moon%20Hyung%20Choi%20and%20Simon%20John%20Christoph%20Soerensen%20and%20Steve%20Ran%20Zhou%20and%20Elijah%20Richard%20Sommer%20and%20Richard%20Fan%20and%20Pejman%20Ghanouni%20and%20Yuze%20Song%20and%20Tyler%20M.%20Seibert%20and%20Geoffrey%20A.%20Sonn%20and%20Mirabela%20Rusu%0AAbstract%3A%20%20%20Accurate%20prostate%20cancer%20diagnosis%20remains%20challenging.%20Even%20when%20using%20MRI%2C%0Aradiologists%20exhibit%20low%20specificity%20and%20significant%20inter-observer%0Avariability%2C%20leading%20to%20potential%20delays%20or%20inaccuracies%20in%20identifying%0Aclinically%20significant%20cancers.%20This%20leads%20to%20numerous%20unnecessary%20biopsies%20and%0Arisks%20of%20missing%20clinically%20significant%20cancers.%20Here%20we%20present%20prostate%0Avision%20contrastive%20network%20%28ProViCNet%29%2C%20prostate%20organ-specific%20vision%0Afoundation%20models%20for%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20and%20Trans-Rectal%0AUltrasound%20imaging%20%28TRUS%29%20for%20comprehensive%20cancer%20detection.%20ProViCNet%20was%0Atrained%20and%20validated%20using%204%2C401%20patients%20across%20six%20institutions%2C%20as%20a%0Aprostate%20cancer%20detection%20model%20on%20radiology%20images%20relying%20on%20patch-level%0Acontrastive%20learning%20guided%20by%20biopsy%20confirmed%20radiologist%20annotations.%0AProViCNet%20demonstrated%20consistent%20performance%20across%20multiple%20internal%20and%0Aexternal%20validation%20cohorts%20with%20area%20under%20the%20receiver%20operating%20curve%20values%0Aranging%20from%200.875%20to%200.966%2C%20significantly%20outperforming%20radiologists%20in%20the%0Areader%20study%20%280.907%20versus%200.805%2C%20p%3C0.001%29%20for%20mpMRI%2C%20while%20achieving%200.670%20to%0A0.740%20for%20TRUS.%20We%20also%20integrated%20ProViCNet%20with%20standard%20PSA%20to%20develop%20a%0Avirtual%20screening%20test%2C%20and%20we%20showed%20that%20we%20can%20maintain%20the%20high%20sensitivity%0Afor%20detecting%20clinically%20significant%20cancers%20while%20more%20than%20doubling%0Aspecificity%20from%2015%25%20to%2038%25%20%28p%3C0.001%29%2C%20thereby%20substantially%20reducing%0Aunnecessary%20biopsies.%20These%20findings%20highlight%20that%20ProViCNet%27s%20potential%20for%0Aenhancing%20prostate%20cancer%20diagnosis%20accuracy%20and%20reduce%20unnecessary%20biopsies%2C%0Athereby%20optimizing%20diagnostic%20pathways.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProstate-Specific%2520Foundation%2520Models%2520for%2520Enhanced%2520Detection%2520of%2520Clinically%250A%2520%2520Significant%2520Cancer%26entry.906535625%3DJeong%2520Hoon%2520Lee%2520and%2520Cynthia%2520Xinran%2520Li%2520and%2520Hassan%2520Jahanandish%2520and%2520Indrani%2520Bhattacharya%2520and%2520Sulaiman%2520Vesal%2520and%2520Lichun%2520Zhang%2520and%2520Shengtian%2520Sang%2520and%2520Moon%2520Hyung%2520Choi%2520and%2520Simon%2520John%2520Christoph%2520Soerensen%2520and%2520Steve%2520Ran%2520Zhou%2520and%2520Elijah%2520Richard%2520Sommer%2520and%2520Richard%2520Fan%2520and%2520Pejman%2520Ghanouni%2520and%2520Yuze%2520Song%2520and%2520Tyler%2520M.%2520Seibert%2520and%2520Geoffrey%2520A.%2520Sonn%2520and%2520Mirabela%2520Rusu%26entry.1292438233%3D%2520%2520Accurate%2520prostate%2520cancer%2520diagnosis%2520remains%2520challenging.%2520Even%2520when%2520using%2520MRI%252C%250Aradiologists%2520exhibit%2520low%2520specificity%2520and%2520significant%2520inter-observer%250Avariability%252C%2520leading%2520to%2520potential%2520delays%2520or%2520inaccuracies%2520in%2520identifying%250Aclinically%2520significant%2520cancers.%2520This%2520leads%2520to%2520numerous%2520unnecessary%2520biopsies%2520and%250Arisks%2520of%2520missing%2520clinically%2520significant%2520cancers.%2520Here%2520we%2520present%2520prostate%250Avision%2520contrastive%2520network%2520%2528ProViCNet%2529%252C%2520prostate%2520organ-specific%2520vision%250Afoundation%2520models%2520for%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520and%2520Trans-Rectal%250AUltrasound%2520imaging%2520%2528TRUS%2529%2520for%2520comprehensive%2520cancer%2520detection.%2520ProViCNet%2520was%250Atrained%2520and%2520validated%2520using%25204%252C401%2520patients%2520across%2520six%2520institutions%252C%2520as%2520a%250Aprostate%2520cancer%2520detection%2520model%2520on%2520radiology%2520images%2520relying%2520on%2520patch-level%250Acontrastive%2520learning%2520guided%2520by%2520biopsy%2520confirmed%2520radiologist%2520annotations.%250AProViCNet%2520demonstrated%2520consistent%2520performance%2520across%2520multiple%2520internal%2520and%250Aexternal%2520validation%2520cohorts%2520with%2520area%2520under%2520the%2520receiver%2520operating%2520curve%2520values%250Aranging%2520from%25200.875%2520to%25200.966%252C%2520significantly%2520outperforming%2520radiologists%2520in%2520the%250Areader%2520study%2520%25280.907%2520versus%25200.805%252C%2520p%253C0.001%2529%2520for%2520mpMRI%252C%2520while%2520achieving%25200.670%2520to%250A0.740%2520for%2520TRUS.%2520We%2520also%2520integrated%2520ProViCNet%2520with%2520standard%2520PSA%2520to%2520develop%2520a%250Avirtual%2520screening%2520test%252C%2520and%2520we%2520showed%2520that%2520we%2520can%2520maintain%2520the%2520high%2520sensitivity%250Afor%2520detecting%2520clinically%2520significant%2520cancers%2520while%2520more%2520than%2520doubling%250Aspecificity%2520from%252015%2525%2520to%252038%2525%2520%2528p%253C0.001%2529%252C%2520thereby%2520substantially%2520reducing%250Aunnecessary%2520biopsies.%2520These%2520findings%2520highlight%2520that%2520ProViCNet%2527s%2520potential%2520for%250Aenhancing%2520prostate%2520cancer%2520diagnosis%2520accuracy%2520and%2520reduce%2520unnecessary%2520biopsies%252C%250Athereby%2520optimizing%2520diagnostic%2520pathways.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prostate-Specific%20Foundation%20Models%20for%20Enhanced%20Detection%20of%20Clinically%0A%20%20Significant%20Cancer&entry.906535625=Jeong%20Hoon%20Lee%20and%20Cynthia%20Xinran%20Li%20and%20Hassan%20Jahanandish%20and%20Indrani%20Bhattacharya%20and%20Sulaiman%20Vesal%20and%20Lichun%20Zhang%20and%20Shengtian%20Sang%20and%20Moon%20Hyung%20Choi%20and%20Simon%20John%20Christoph%20Soerensen%20and%20Steve%20Ran%20Zhou%20and%20Elijah%20Richard%20Sommer%20and%20Richard%20Fan%20and%20Pejman%20Ghanouni%20and%20Yuze%20Song%20and%20Tyler%20M.%20Seibert%20and%20Geoffrey%20A.%20Sonn%20and%20Mirabela%20Rusu&entry.1292438233=%20%20Accurate%20prostate%20cancer%20diagnosis%20remains%20challenging.%20Even%20when%20using%20MRI%2C%0Aradiologists%20exhibit%20low%20specificity%20and%20significant%20inter-observer%0Avariability%2C%20leading%20to%20potential%20delays%20or%20inaccuracies%20in%20identifying%0Aclinically%20significant%20cancers.%20This%20leads%20to%20numerous%20unnecessary%20biopsies%20and%0Arisks%20of%20missing%20clinically%20significant%20cancers.%20Here%20we%20present%20prostate%0Avision%20contrastive%20network%20%28ProViCNet%29%2C%20prostate%20organ-specific%20vision%0Afoundation%20models%20for%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20and%20Trans-Rectal%0AUltrasound%20imaging%20%28TRUS%29%20for%20comprehensive%20cancer%20detection.%20ProViCNet%20was%0Atrained%20and%20validated%20using%204%2C401%20patients%20across%20six%20institutions%2C%20as%20a%0Aprostate%20cancer%20detection%20model%20on%20radiology%20images%20relying%20on%20patch-level%0Acontrastive%20learning%20guided%20by%20biopsy%20confirmed%20radiologist%20annotations.%0AProViCNet%20demonstrated%20consistent%20performance%20across%20multiple%20internal%20and%0Aexternal%20validation%20cohorts%20with%20area%20under%20the%20receiver%20operating%20curve%20values%0Aranging%20from%200.875%20to%200.966%2C%20significantly%20outperforming%20radiologists%20in%20the%0Areader%20study%20%280.907%20versus%200.805%2C%20p%3C0.001%29%20for%20mpMRI%2C%20while%20achieving%200.670%20to%0A0.740%20for%20TRUS.%20We%20also%20integrated%20ProViCNet%20with%20standard%20PSA%20to%20develop%20a%0Avirtual%20screening%20test%2C%20and%20we%20showed%20that%20we%20can%20maintain%20the%20high%20sensitivity%0Afor%20detecting%20clinically%20significant%20cancers%20while%20more%20than%20doubling%0Aspecificity%20from%2015%25%20to%2038%25%20%28p%3C0.001%29%2C%20thereby%20substantially%20reducing%0Aunnecessary%20biopsies.%20These%20findings%20highlight%20that%20ProViCNet%27s%20potential%20for%0Aenhancing%20prostate%20cancer%20diagnosis%20accuracy%20and%20reduce%20unnecessary%20biopsies%2C%0Athereby%20optimizing%20diagnostic%20pathways.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00366v2&entry.124074799=Read"},
{"title": "Representational Alignment Supports Effective Machine Teaching", "author": "Ilia Sucholutsky and Katherine M. Collins and Maya Malaviya and Nori Jacoby and Weiyang Liu and Theodore R. Sumers and Michalis Korakakis and Umang Bhatt and Mark Ho and Joshua B. Tenenbaum and Brad Love and Zachary A. Pardos and Adrian Weller and Thomas L. Griffiths", "abstract": "  A good teacher should not only be knowledgeable, but should also be able to\ncommunicate in a way that the student understands -- to share the student's\nrepresentation of the world. In this work, we introduce a new controlled\nexperimental setting, GRADE, to study pedagogy and representational alignment.\nWe use GRADE through a series of machine-machine and machine-human teaching\nexperiments to characterize a utility curve defining a relationship between\nrepresentational alignment, teacher expertise, and student learning outcomes.\nWe find that improved representational alignment with a student improves\nstudent learning outcomes (i.e., task accuracy), but that this effect is\nmoderated by the size and representational diversity of the class being taught.\nWe use these insights to design a preliminary classroom matching procedure,\nGRADE-Match, that optimizes the assignment of students to teachers. When\ndesigning machine teachers, our results suggest that it is important to focus\nnot only on accuracy, but also on representational alignment with human\nlearners.\n", "link": "http://arxiv.org/abs/2406.04302v2", "date": "2025-02-04", "relevancy": 2.4053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5082}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4752}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching&body=Title%3A%20Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching%0AAuthor%3A%20Ilia%20Sucholutsky%20and%20Katherine%20M.%20Collins%20and%20Maya%20Malaviya%20and%20Nori%20Jacoby%20and%20Weiyang%20Liu%20and%20Theodore%20R.%20Sumers%20and%20Michalis%20Korakakis%20and%20Umang%20Bhatt%20and%20Mark%20Ho%20and%20Joshua%20B.%20Tenenbaum%20and%20Brad%20Love%20and%20Zachary%20A.%20Pardos%20and%20Adrian%20Weller%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20A%20good%20teacher%20should%20not%20only%20be%20knowledgeable%2C%20but%20should%20also%20be%20able%20to%0Acommunicate%20in%20a%20way%20that%20the%20student%20understands%20--%20to%20share%20the%20student%27s%0Arepresentation%20of%20the%20world.%20In%20this%20work%2C%20we%20introduce%20a%20new%20controlled%0Aexperimental%20setting%2C%20GRADE%2C%20to%20study%20pedagogy%20and%20representational%20alignment.%0AWe%20use%20GRADE%20through%20a%20series%20of%20machine-machine%20and%20machine-human%20teaching%0Aexperiments%20to%20characterize%20a%20utility%20curve%20defining%20a%20relationship%20between%0Arepresentational%20alignment%2C%20teacher%20expertise%2C%20and%20student%20learning%20outcomes.%0AWe%20find%20that%20improved%20representational%20alignment%20with%20a%20student%20improves%0Astudent%20learning%20outcomes%20%28i.e.%2C%20task%20accuracy%29%2C%20but%20that%20this%20effect%20is%0Amoderated%20by%20the%20size%20and%20representational%20diversity%20of%20the%20class%20being%20taught.%0AWe%20use%20these%20insights%20to%20design%20a%20preliminary%20classroom%20matching%20procedure%2C%0AGRADE-Match%2C%20that%20optimizes%20the%20assignment%20of%20students%20to%20teachers.%20When%0Adesigning%20machine%20teachers%2C%20our%20results%20suggest%20that%20it%20is%20important%20to%20focus%0Anot%20only%20on%20accuracy%2C%20but%20also%20on%20representational%20alignment%20with%20human%0Alearners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentational%2520Alignment%2520Supports%2520Effective%2520Machine%2520Teaching%26entry.906535625%3DIlia%2520Sucholutsky%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Maya%2520Malaviya%2520and%2520Nori%2520Jacoby%2520and%2520Weiyang%2520Liu%2520and%2520Theodore%2520R.%2520Sumers%2520and%2520Michalis%2520Korakakis%2520and%2520Umang%2520Bhatt%2520and%2520Mark%2520Ho%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Brad%2520Love%2520and%2520Zachary%2520A.%2520Pardos%2520and%2520Adrian%2520Weller%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520A%2520good%2520teacher%2520should%2520not%2520only%2520be%2520knowledgeable%252C%2520but%2520should%2520also%2520be%2520able%2520to%250Acommunicate%2520in%2520a%2520way%2520that%2520the%2520student%2520understands%2520--%2520to%2520share%2520the%2520student%2527s%250Arepresentation%2520of%2520the%2520world.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520controlled%250Aexperimental%2520setting%252C%2520GRADE%252C%2520to%2520study%2520pedagogy%2520and%2520representational%2520alignment.%250AWe%2520use%2520GRADE%2520through%2520a%2520series%2520of%2520machine-machine%2520and%2520machine-human%2520teaching%250Aexperiments%2520to%2520characterize%2520a%2520utility%2520curve%2520defining%2520a%2520relationship%2520between%250Arepresentational%2520alignment%252C%2520teacher%2520expertise%252C%2520and%2520student%2520learning%2520outcomes.%250AWe%2520find%2520that%2520improved%2520representational%2520alignment%2520with%2520a%2520student%2520improves%250Astudent%2520learning%2520outcomes%2520%2528i.e.%252C%2520task%2520accuracy%2529%252C%2520but%2520that%2520this%2520effect%2520is%250Amoderated%2520by%2520the%2520size%2520and%2520representational%2520diversity%2520of%2520the%2520class%2520being%2520taught.%250AWe%2520use%2520these%2520insights%2520to%2520design%2520a%2520preliminary%2520classroom%2520matching%2520procedure%252C%250AGRADE-Match%252C%2520that%2520optimizes%2520the%2520assignment%2520of%2520students%2520to%2520teachers.%2520When%250Adesigning%2520machine%2520teachers%252C%2520our%2520results%2520suggest%2520that%2520it%2520is%2520important%2520to%2520focus%250Anot%2520only%2520on%2520accuracy%252C%2520but%2520also%2520on%2520representational%2520alignment%2520with%2520human%250Alearners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching&entry.906535625=Ilia%20Sucholutsky%20and%20Katherine%20M.%20Collins%20and%20Maya%20Malaviya%20and%20Nori%20Jacoby%20and%20Weiyang%20Liu%20and%20Theodore%20R.%20Sumers%20and%20Michalis%20Korakakis%20and%20Umang%20Bhatt%20and%20Mark%20Ho%20and%20Joshua%20B.%20Tenenbaum%20and%20Brad%20Love%20and%20Zachary%20A.%20Pardos%20and%20Adrian%20Weller%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20A%20good%20teacher%20should%20not%20only%20be%20knowledgeable%2C%20but%20should%20also%20be%20able%20to%0Acommunicate%20in%20a%20way%20that%20the%20student%20understands%20--%20to%20share%20the%20student%27s%0Arepresentation%20of%20the%20world.%20In%20this%20work%2C%20we%20introduce%20a%20new%20controlled%0Aexperimental%20setting%2C%20GRADE%2C%20to%20study%20pedagogy%20and%20representational%20alignment.%0AWe%20use%20GRADE%20through%20a%20series%20of%20machine-machine%20and%20machine-human%20teaching%0Aexperiments%20to%20characterize%20a%20utility%20curve%20defining%20a%20relationship%20between%0Arepresentational%20alignment%2C%20teacher%20expertise%2C%20and%20student%20learning%20outcomes.%0AWe%20find%20that%20improved%20representational%20alignment%20with%20a%20student%20improves%0Astudent%20learning%20outcomes%20%28i.e.%2C%20task%20accuracy%29%2C%20but%20that%20this%20effect%20is%0Amoderated%20by%20the%20size%20and%20representational%20diversity%20of%20the%20class%20being%20taught.%0AWe%20use%20these%20insights%20to%20design%20a%20preliminary%20classroom%20matching%20procedure%2C%0AGRADE-Match%2C%20that%20optimizes%20the%20assignment%20of%20students%20to%20teachers.%20When%0Adesigning%20machine%20teachers%2C%20our%20results%20suggest%20that%20it%20is%20important%20to%20focus%0Anot%20only%20on%20accuracy%2C%20but%20also%20on%20representational%20alignment%20with%20human%0Alearners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04302v2&entry.124074799=Read"},
{"title": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation", "author": "Taewoo Kang and Kjerstin Thorson and Tai-Quan Peng and Dan Hiaeshutter-Rice and Sanguk Lee and Stuart Soroka", "abstract": "  This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications.\n", "link": "http://arxiv.org/abs/2502.00903v2", "date": "2025-02-04", "relevancy": 2.4035, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Dialectic%20Intersubjectivity%3A%20Coordination%20of%20Different%0A%20%20Perspectives%20in%20Content%20Analysis%20with%20LLM%20Persona%20Simulation&body=Title%3A%20Embracing%20Dialectic%20Intersubjectivity%3A%20Coordination%20of%20Different%0A%20%20Perspectives%20in%20Content%20Analysis%20with%20LLM%20Persona%20Simulation%0AAuthor%3A%20Taewoo%20Kang%20and%20Kjerstin%20Thorson%20and%20Tai-Quan%20Peng%20and%20Dan%20Hiaeshutter-Rice%20and%20Sanguk%20Lee%20and%20Stuart%20Soroka%0AAbstract%3A%20%20%20This%20study%20attempts%20to%20advancing%20content%20analysis%20methodology%20from%0Aconsensus-oriented%20to%20coordination-oriented%20practices%2C%20thereby%20embracing%0Adiverse%20coding%20outputs%20and%20exploring%20the%20dynamics%20among%20differential%0Aperspectives.%20As%20an%20exploratory%20investigation%20of%20this%20approach%2C%20we%20evaluate%20six%0AGPT-4o%20configurations%20to%20analyze%20sentiment%20in%20Fox%20News%20and%20MSNBC%20transcripts%20on%0ABiden%20and%20Trump%20during%20the%202020%20U.S.%20presidential%20campaign%2C%20examining%20patterns%0Aacross%20these%20models.%20By%20assessing%20each%20model%27s%20alignment%20with%20ideological%0Aperspectives%2C%20we%20explore%20how%20partisan%20selective%20processing%20could%20be%20identified%0Ain%20LLM-Assisted%20Content%20Analysis%20%28LACA%29.%20Findings%20reveal%20that%20partisan%20persona%0ALLMs%20exhibit%20stronger%20ideological%20biases%20when%20processing%20politically%20congruent%0Acontent.%20Additionally%2C%20intercoder%20reliability%20is%20higher%20among%20same-partisan%0Apersonas%20compared%20to%20cross-partisan%20pairs.%20This%20approach%20enhances%20the%20nuanced%0Aunderstanding%20of%20LLM%20outputs%20and%20advances%20the%20integrity%20of%20AI-driven%20social%0Ascience%20research%2C%20enabling%20simulations%20of%20real-world%20implications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Dialectic%2520Intersubjectivity%253A%2520Coordination%2520of%2520Different%250A%2520%2520Perspectives%2520in%2520Content%2520Analysis%2520with%2520LLM%2520Persona%2520Simulation%26entry.906535625%3DTaewoo%2520Kang%2520and%2520Kjerstin%2520Thorson%2520and%2520Tai-Quan%2520Peng%2520and%2520Dan%2520Hiaeshutter-Rice%2520and%2520Sanguk%2520Lee%2520and%2520Stuart%2520Soroka%26entry.1292438233%3D%2520%2520This%2520study%2520attempts%2520to%2520advancing%2520content%2520analysis%2520methodology%2520from%250Aconsensus-oriented%2520to%2520coordination-oriented%2520practices%252C%2520thereby%2520embracing%250Adiverse%2520coding%2520outputs%2520and%2520exploring%2520the%2520dynamics%2520among%2520differential%250Aperspectives.%2520As%2520an%2520exploratory%2520investigation%2520of%2520this%2520approach%252C%2520we%2520evaluate%2520six%250AGPT-4o%2520configurations%2520to%2520analyze%2520sentiment%2520in%2520Fox%2520News%2520and%2520MSNBC%2520transcripts%2520on%250ABiden%2520and%2520Trump%2520during%2520the%25202020%2520U.S.%2520presidential%2520campaign%252C%2520examining%2520patterns%250Aacross%2520these%2520models.%2520By%2520assessing%2520each%2520model%2527s%2520alignment%2520with%2520ideological%250Aperspectives%252C%2520we%2520explore%2520how%2520partisan%2520selective%2520processing%2520could%2520be%2520identified%250Ain%2520LLM-Assisted%2520Content%2520Analysis%2520%2528LACA%2529.%2520Findings%2520reveal%2520that%2520partisan%2520persona%250ALLMs%2520exhibit%2520stronger%2520ideological%2520biases%2520when%2520processing%2520politically%2520congruent%250Acontent.%2520Additionally%252C%2520intercoder%2520reliability%2520is%2520higher%2520among%2520same-partisan%250Apersonas%2520compared%2520to%2520cross-partisan%2520pairs.%2520This%2520approach%2520enhances%2520the%2520nuanced%250Aunderstanding%2520of%2520LLM%2520outputs%2520and%2520advances%2520the%2520integrity%2520of%2520AI-driven%2520social%250Ascience%2520research%252C%2520enabling%2520simulations%2520of%2520real-world%2520implications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Dialectic%20Intersubjectivity%3A%20Coordination%20of%20Different%0A%20%20Perspectives%20in%20Content%20Analysis%20with%20LLM%20Persona%20Simulation&entry.906535625=Taewoo%20Kang%20and%20Kjerstin%20Thorson%20and%20Tai-Quan%20Peng%20and%20Dan%20Hiaeshutter-Rice%20and%20Sanguk%20Lee%20and%20Stuart%20Soroka&entry.1292438233=%20%20This%20study%20attempts%20to%20advancing%20content%20analysis%20methodology%20from%0Aconsensus-oriented%20to%20coordination-oriented%20practices%2C%20thereby%20embracing%0Adiverse%20coding%20outputs%20and%20exploring%20the%20dynamics%20among%20differential%0Aperspectives.%20As%20an%20exploratory%20investigation%20of%20this%20approach%2C%20we%20evaluate%20six%0AGPT-4o%20configurations%20to%20analyze%20sentiment%20in%20Fox%20News%20and%20MSNBC%20transcripts%20on%0ABiden%20and%20Trump%20during%20the%202020%20U.S.%20presidential%20campaign%2C%20examining%20patterns%0Aacross%20these%20models.%20By%20assessing%20each%20model%27s%20alignment%20with%20ideological%0Aperspectives%2C%20we%20explore%20how%20partisan%20selective%20processing%20could%20be%20identified%0Ain%20LLM-Assisted%20Content%20Analysis%20%28LACA%29.%20Findings%20reveal%20that%20partisan%20persona%0ALLMs%20exhibit%20stronger%20ideological%20biases%20when%20processing%20politically%20congruent%0Acontent.%20Additionally%2C%20intercoder%20reliability%20is%20higher%20among%20same-partisan%0Apersonas%20compared%20to%20cross-partisan%20pairs.%20This%20approach%20enhances%20the%20nuanced%0Aunderstanding%20of%20LLM%20outputs%20and%20advances%20the%20integrity%20of%20AI-driven%20social%0Ascience%20research%2C%20enabling%20simulations%20of%20real-world%20implications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00903v2&entry.124074799=Read"},
{"title": "Sparse Data Generation Using Diffusion Models", "author": "Phil Ostheimer and Mayank Nagda and Marius Kloft and Sophie Fellenz", "abstract": "  Sparse data is ubiquitous, appearing in numerous domains, from economics and\nrecommender systems to astronomy and biomedical sciences. However, efficiently\nand realistically generating sparse data remains a significant challenge. We\nintroduce Sparse Data Diffusion (SDD), a novel method for generating sparse\ndata. SDD extends continuous state-space diffusion models by explicitly\nmodeling sparsity through the introduction of Sparsity Bits. Empirical\nvalidation on image data from various domains-including two scientific\napplications, physics and biology-demonstrates that SDD achieves high fidelity\nin representing data sparsity while preserving the quality of the generated\ndata.\n", "link": "http://arxiv.org/abs/2502.02448v1", "date": "2025-02-04", "relevancy": 2.3888, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6366}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6092}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Data%20Generation%20Using%20Diffusion%20Models&body=Title%3A%20Sparse%20Data%20Generation%20Using%20Diffusion%20Models%0AAuthor%3A%20Phil%20Ostheimer%20and%20Mayank%20Nagda%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20Sparse%20data%20is%20ubiquitous%2C%20appearing%20in%20numerous%20domains%2C%20from%20economics%20and%0Arecommender%20systems%20to%20astronomy%20and%20biomedical%20sciences.%20However%2C%20efficiently%0Aand%20realistically%20generating%20sparse%20data%20remains%20a%20significant%20challenge.%20We%0Aintroduce%20Sparse%20Data%20Diffusion%20%28SDD%29%2C%20a%20novel%20method%20for%20generating%20sparse%0Adata.%20SDD%20extends%20continuous%20state-space%20diffusion%20models%20by%20explicitly%0Amodeling%20sparsity%20through%20the%20introduction%20of%20Sparsity%20Bits.%20Empirical%0Avalidation%20on%20image%20data%20from%20various%20domains-including%20two%20scientific%0Aapplications%2C%20physics%20and%20biology-demonstrates%20that%20SDD%20achieves%20high%20fidelity%0Ain%20representing%20data%20sparsity%20while%20preserving%20the%20quality%20of%20the%20generated%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Data%2520Generation%2520Using%2520Diffusion%2520Models%26entry.906535625%3DPhil%2520Ostheimer%2520and%2520Mayank%2520Nagda%2520and%2520Marius%2520Kloft%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520Sparse%2520data%2520is%2520ubiquitous%252C%2520appearing%2520in%2520numerous%2520domains%252C%2520from%2520economics%2520and%250Arecommender%2520systems%2520to%2520astronomy%2520and%2520biomedical%2520sciences.%2520However%252C%2520efficiently%250Aand%2520realistically%2520generating%2520sparse%2520data%2520remains%2520a%2520significant%2520challenge.%2520We%250Aintroduce%2520Sparse%2520Data%2520Diffusion%2520%2528SDD%2529%252C%2520a%2520novel%2520method%2520for%2520generating%2520sparse%250Adata.%2520SDD%2520extends%2520continuous%2520state-space%2520diffusion%2520models%2520by%2520explicitly%250Amodeling%2520sparsity%2520through%2520the%2520introduction%2520of%2520Sparsity%2520Bits.%2520Empirical%250Avalidation%2520on%2520image%2520data%2520from%2520various%2520domains-including%2520two%2520scientific%250Aapplications%252C%2520physics%2520and%2520biology-demonstrates%2520that%2520SDD%2520achieves%2520high%2520fidelity%250Ain%2520representing%2520data%2520sparsity%2520while%2520preserving%2520the%2520quality%2520of%2520the%2520generated%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Data%20Generation%20Using%20Diffusion%20Models&entry.906535625=Phil%20Ostheimer%20and%20Mayank%20Nagda%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz&entry.1292438233=%20%20Sparse%20data%20is%20ubiquitous%2C%20appearing%20in%20numerous%20domains%2C%20from%20economics%20and%0Arecommender%20systems%20to%20astronomy%20and%20biomedical%20sciences.%20However%2C%20efficiently%0Aand%20realistically%20generating%20sparse%20data%20remains%20a%20significant%20challenge.%20We%0Aintroduce%20Sparse%20Data%20Diffusion%20%28SDD%29%2C%20a%20novel%20method%20for%20generating%20sparse%0Adata.%20SDD%20extends%20continuous%20state-space%20diffusion%20models%20by%20explicitly%0Amodeling%20sparsity%20through%20the%20introduction%20of%20Sparsity%20Bits.%20Empirical%0Avalidation%20on%20image%20data%20from%20various%20domains-including%20two%20scientific%0Aapplications%2C%20physics%20and%20biology-demonstrates%20that%20SDD%20achieves%20high%20fidelity%0Ain%20representing%20data%20sparsity%20while%20preserving%20the%20quality%20of%20the%20generated%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02448v1&entry.124074799=Read"},
{"title": "No Metric to Rule Them All: Toward Principled Evaluations of\n  Graph-Learning Datasets", "author": "Corinna Coupette and Jeremy Wayland and Emily Simons and Bastian Rieck", "abstract": "  Benchmark datasets have proved pivotal to the success of graph learning, and\ngood benchmark datasets are crucial to guide the development of the field.\nRecent research has highlighted problems with graph-learning datasets and\nbenchmarking practices -- revealing, for example, that methods which ignore the\ngraph structure can outperform graph-based approaches on popular benchmark\ndatasets. Such findings raise two questions: (1) What makes a good\ngraph-learning dataset, and (2) how can we evaluate dataset quality in graph\nlearning? Our work addresses these questions. As the classic evaluation setup\nuses datasets to evaluate models, it does not apply to dataset evaluation.\nHence, we start from first principles. Observing that graph-learning datasets\nuniquely combine two modes -- the graph structure and the node features -- , we\nintroduce RINGS, a flexible and extensible mode-perturbation framework to\nassess the quality of graph-learning datasets based on dataset ablations --\ni.e., by quantifying differences between the original dataset and its perturbed\nrepresentations. Within this framework, we propose two measures -- performance\nseparability and mode complementarity -- as evaluation tools, each assessing,\nfrom a distinct angle, the capacity of a graph dataset to benchmark the power\nand efficacy of graph-learning methods. We demonstrate the utility of our\nframework for graph-learning dataset evaluation in an extensive set of\nexperiments and derive actionable recommendations for improving the evaluation\nof graph-learning methods. Our work opens new research directions in\ndata-centric graph learning, and it constitutes a first step toward the\nsystematic evaluation of evaluations.\n", "link": "http://arxiv.org/abs/2502.02379v1", "date": "2025-02-04", "relevancy": 2.3771, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Metric%20to%20Rule%20Them%20All%3A%20Toward%20Principled%20Evaluations%20of%0A%20%20Graph-Learning%20Datasets&body=Title%3A%20No%20Metric%20to%20Rule%20Them%20All%3A%20Toward%20Principled%20Evaluations%20of%0A%20%20Graph-Learning%20Datasets%0AAuthor%3A%20Corinna%20Coupette%20and%20Jeremy%20Wayland%20and%20Emily%20Simons%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20Benchmark%20datasets%20have%20proved%20pivotal%20to%20the%20success%20of%20graph%20learning%2C%20and%0Agood%20benchmark%20datasets%20are%20crucial%20to%20guide%20the%20development%20of%20the%20field.%0ARecent%20research%20has%20highlighted%20problems%20with%20graph-learning%20datasets%20and%0Abenchmarking%20practices%20--%20revealing%2C%20for%20example%2C%20that%20methods%20which%20ignore%20the%0Agraph%20structure%20can%20outperform%20graph-based%20approaches%20on%20popular%20benchmark%0Adatasets.%20Such%20findings%20raise%20two%20questions%3A%20%281%29%20What%20makes%20a%20good%0Agraph-learning%20dataset%2C%20and%20%282%29%20how%20can%20we%20evaluate%20dataset%20quality%20in%20graph%0Alearning%3F%20Our%20work%20addresses%20these%20questions.%20As%20the%20classic%20evaluation%20setup%0Auses%20datasets%20to%20evaluate%20models%2C%20it%20does%20not%20apply%20to%20dataset%20evaluation.%0AHence%2C%20we%20start%20from%20first%20principles.%20Observing%20that%20graph-learning%20datasets%0Auniquely%20combine%20two%20modes%20--%20the%20graph%20structure%20and%20the%20node%20features%20--%20%2C%20we%0Aintroduce%20RINGS%2C%20a%20flexible%20and%20extensible%20mode-perturbation%20framework%20to%0Aassess%20the%20quality%20of%20graph-learning%20datasets%20based%20on%20dataset%20ablations%20--%0Ai.e.%2C%20by%20quantifying%20differences%20between%20the%20original%20dataset%20and%20its%20perturbed%0Arepresentations.%20Within%20this%20framework%2C%20we%20propose%20two%20measures%20--%20performance%0Aseparability%20and%20mode%20complementarity%20--%20as%20evaluation%20tools%2C%20each%20assessing%2C%0Afrom%20a%20distinct%20angle%2C%20the%20capacity%20of%20a%20graph%20dataset%20to%20benchmark%20the%20power%0Aand%20efficacy%20of%20graph-learning%20methods.%20We%20demonstrate%20the%20utility%20of%20our%0Aframework%20for%20graph-learning%20dataset%20evaluation%20in%20an%20extensive%20set%20of%0Aexperiments%20and%20derive%20actionable%20recommendations%20for%20improving%20the%20evaluation%0Aof%20graph-learning%20methods.%20Our%20work%20opens%20new%20research%20directions%20in%0Adata-centric%20graph%20learning%2C%20and%20it%20constitutes%20a%20first%20step%20toward%20the%0Asystematic%20evaluation%20of%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Metric%2520to%2520Rule%2520Them%2520All%253A%2520Toward%2520Principled%2520Evaluations%2520of%250A%2520%2520Graph-Learning%2520Datasets%26entry.906535625%3DCorinna%2520Coupette%2520and%2520Jeremy%2520Wayland%2520and%2520Emily%2520Simons%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520Benchmark%2520datasets%2520have%2520proved%2520pivotal%2520to%2520the%2520success%2520of%2520graph%2520learning%252C%2520and%250Agood%2520benchmark%2520datasets%2520are%2520crucial%2520to%2520guide%2520the%2520development%2520of%2520the%2520field.%250ARecent%2520research%2520has%2520highlighted%2520problems%2520with%2520graph-learning%2520datasets%2520and%250Abenchmarking%2520practices%2520--%2520revealing%252C%2520for%2520example%252C%2520that%2520methods%2520which%2520ignore%2520the%250Agraph%2520structure%2520can%2520outperform%2520graph-based%2520approaches%2520on%2520popular%2520benchmark%250Adatasets.%2520Such%2520findings%2520raise%2520two%2520questions%253A%2520%25281%2529%2520What%2520makes%2520a%2520good%250Agraph-learning%2520dataset%252C%2520and%2520%25282%2529%2520how%2520can%2520we%2520evaluate%2520dataset%2520quality%2520in%2520graph%250Alearning%253F%2520Our%2520work%2520addresses%2520these%2520questions.%2520As%2520the%2520classic%2520evaluation%2520setup%250Auses%2520datasets%2520to%2520evaluate%2520models%252C%2520it%2520does%2520not%2520apply%2520to%2520dataset%2520evaluation.%250AHence%252C%2520we%2520start%2520from%2520first%2520principles.%2520Observing%2520that%2520graph-learning%2520datasets%250Auniquely%2520combine%2520two%2520modes%2520--%2520the%2520graph%2520structure%2520and%2520the%2520node%2520features%2520--%2520%252C%2520we%250Aintroduce%2520RINGS%252C%2520a%2520flexible%2520and%2520extensible%2520mode-perturbation%2520framework%2520to%250Aassess%2520the%2520quality%2520of%2520graph-learning%2520datasets%2520based%2520on%2520dataset%2520ablations%2520--%250Ai.e.%252C%2520by%2520quantifying%2520differences%2520between%2520the%2520original%2520dataset%2520and%2520its%2520perturbed%250Arepresentations.%2520Within%2520this%2520framework%252C%2520we%2520propose%2520two%2520measures%2520--%2520performance%250Aseparability%2520and%2520mode%2520complementarity%2520--%2520as%2520evaluation%2520tools%252C%2520each%2520assessing%252C%250Afrom%2520a%2520distinct%2520angle%252C%2520the%2520capacity%2520of%2520a%2520graph%2520dataset%2520to%2520benchmark%2520the%2520power%250Aand%2520efficacy%2520of%2520graph-learning%2520methods.%2520We%2520demonstrate%2520the%2520utility%2520of%2520our%250Aframework%2520for%2520graph-learning%2520dataset%2520evaluation%2520in%2520an%2520extensive%2520set%2520of%250Aexperiments%2520and%2520derive%2520actionable%2520recommendations%2520for%2520improving%2520the%2520evaluation%250Aof%2520graph-learning%2520methods.%2520Our%2520work%2520opens%2520new%2520research%2520directions%2520in%250Adata-centric%2520graph%2520learning%252C%2520and%2520it%2520constitutes%2520a%2520first%2520step%2520toward%2520the%250Asystematic%2520evaluation%2520of%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Metric%20to%20Rule%20Them%20All%3A%20Toward%20Principled%20Evaluations%20of%0A%20%20Graph-Learning%20Datasets&entry.906535625=Corinna%20Coupette%20and%20Jeremy%20Wayland%20and%20Emily%20Simons%20and%20Bastian%20Rieck&entry.1292438233=%20%20Benchmark%20datasets%20have%20proved%20pivotal%20to%20the%20success%20of%20graph%20learning%2C%20and%0Agood%20benchmark%20datasets%20are%20crucial%20to%20guide%20the%20development%20of%20the%20field.%0ARecent%20research%20has%20highlighted%20problems%20with%20graph-learning%20datasets%20and%0Abenchmarking%20practices%20--%20revealing%2C%20for%20example%2C%20that%20methods%20which%20ignore%20the%0Agraph%20structure%20can%20outperform%20graph-based%20approaches%20on%20popular%20benchmark%0Adatasets.%20Such%20findings%20raise%20two%20questions%3A%20%281%29%20What%20makes%20a%20good%0Agraph-learning%20dataset%2C%20and%20%282%29%20how%20can%20we%20evaluate%20dataset%20quality%20in%20graph%0Alearning%3F%20Our%20work%20addresses%20these%20questions.%20As%20the%20classic%20evaluation%20setup%0Auses%20datasets%20to%20evaluate%20models%2C%20it%20does%20not%20apply%20to%20dataset%20evaluation.%0AHence%2C%20we%20start%20from%20first%20principles.%20Observing%20that%20graph-learning%20datasets%0Auniquely%20combine%20two%20modes%20--%20the%20graph%20structure%20and%20the%20node%20features%20--%20%2C%20we%0Aintroduce%20RINGS%2C%20a%20flexible%20and%20extensible%20mode-perturbation%20framework%20to%0Aassess%20the%20quality%20of%20graph-learning%20datasets%20based%20on%20dataset%20ablations%20--%0Ai.e.%2C%20by%20quantifying%20differences%20between%20the%20original%20dataset%20and%20its%20perturbed%0Arepresentations.%20Within%20this%20framework%2C%20we%20propose%20two%20measures%20--%20performance%0Aseparability%20and%20mode%20complementarity%20--%20as%20evaluation%20tools%2C%20each%20assessing%2C%0Afrom%20a%20distinct%20angle%2C%20the%20capacity%20of%20a%20graph%20dataset%20to%20benchmark%20the%20power%0Aand%20efficacy%20of%20graph-learning%20methods.%20We%20demonstrate%20the%20utility%20of%20our%0Aframework%20for%20graph-learning%20dataset%20evaluation%20in%20an%20extensive%20set%20of%0Aexperiments%20and%20derive%20actionable%20recommendations%20for%20improving%20the%20evaluation%0Aof%20graph-learning%20methods.%20Our%20work%20opens%20new%20research%20directions%20in%0Adata-centric%20graph%20learning%2C%20and%20it%20constitutes%20a%20first%20step%20toward%20the%0Asystematic%20evaluation%20of%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02379v1&entry.124074799=Read"},
{"title": "FRAUD-RLA: A new reinforcement learning adversarial attack against\n  credit card fraud detection", "author": "Daniele Lunghi and Yannick Molinghen and Alkis Simitsis and Tom Lenaerts and Gianluca Bontempi", "abstract": "  Adversarial attacks pose a significant threat to data-driven systems, and\nresearchers have spent considerable resources studying them. Despite its\neconomic relevance, this trend largely overlooked the issue of credit card\nfraud detection. To address this gap, we propose a new threat model that\ndemonstrates the limitations of existing attacks and highlights the necessity\nto investigate new approaches. We then design a new adversarial attack for\ncredit card fraud detection, employing reinforcement learning to bypass\nclassifiers. This attack, called FRAUD-RLA, is designed to maximize the\nattacker's reward by optimizing the exploration-exploitation tradeoff and\nworking with significantly less required knowledge than competitors. Our\nexperiments, conducted on three different heterogeneous datasets and against\ntwo fraud detection systems, indicate that FRAUD-RLA is effective, even\nconsidering the severe limitations imposed by our threat model.\n", "link": "http://arxiv.org/abs/2502.02290v1", "date": "2025-02-04", "relevancy": 2.3737, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4736}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRAUD-RLA%3A%20A%20new%20reinforcement%20learning%20adversarial%20attack%20against%0A%20%20credit%20card%20fraud%20detection&body=Title%3A%20FRAUD-RLA%3A%20A%20new%20reinforcement%20learning%20adversarial%20attack%20against%0A%20%20credit%20card%20fraud%20detection%0AAuthor%3A%20Daniele%20Lunghi%20and%20Yannick%20Molinghen%20and%20Alkis%20Simitsis%20and%20Tom%20Lenaerts%20and%20Gianluca%20Bontempi%0AAbstract%3A%20%20%20Adversarial%20attacks%20pose%20a%20significant%20threat%20to%20data-driven%20systems%2C%20and%0Aresearchers%20have%20spent%20considerable%20resources%20studying%20them.%20Despite%20its%0Aeconomic%20relevance%2C%20this%20trend%20largely%20overlooked%20the%20issue%20of%20credit%20card%0Afraud%20detection.%20To%20address%20this%20gap%2C%20we%20propose%20a%20new%20threat%20model%20that%0Ademonstrates%20the%20limitations%20of%20existing%20attacks%20and%20highlights%20the%20necessity%0Ato%20investigate%20new%20approaches.%20We%20then%20design%20a%20new%20adversarial%20attack%20for%0Acredit%20card%20fraud%20detection%2C%20employing%20reinforcement%20learning%20to%20bypass%0Aclassifiers.%20This%20attack%2C%20called%20FRAUD-RLA%2C%20is%20designed%20to%20maximize%20the%0Aattacker%27s%20reward%20by%20optimizing%20the%20exploration-exploitation%20tradeoff%20and%0Aworking%20with%20significantly%20less%20required%20knowledge%20than%20competitors.%20Our%0Aexperiments%2C%20conducted%20on%20three%20different%20heterogeneous%20datasets%20and%20against%0Atwo%20fraud%20detection%20systems%2C%20indicate%20that%20FRAUD-RLA%20is%20effective%2C%20even%0Aconsidering%20the%20severe%20limitations%20imposed%20by%20our%20threat%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRAUD-RLA%253A%2520A%2520new%2520reinforcement%2520learning%2520adversarial%2520attack%2520against%250A%2520%2520credit%2520card%2520fraud%2520detection%26entry.906535625%3DDaniele%2520Lunghi%2520and%2520Yannick%2520Molinghen%2520and%2520Alkis%2520Simitsis%2520and%2520Tom%2520Lenaerts%2520and%2520Gianluca%2520Bontempi%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520data-driven%2520systems%252C%2520and%250Aresearchers%2520have%2520spent%2520considerable%2520resources%2520studying%2520them.%2520Despite%2520its%250Aeconomic%2520relevance%252C%2520this%2520trend%2520largely%2520overlooked%2520the%2520issue%2520of%2520credit%2520card%250Afraud%2520detection.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520new%2520threat%2520model%2520that%250Ademonstrates%2520the%2520limitations%2520of%2520existing%2520attacks%2520and%2520highlights%2520the%2520necessity%250Ato%2520investigate%2520new%2520approaches.%2520We%2520then%2520design%2520a%2520new%2520adversarial%2520attack%2520for%250Acredit%2520card%2520fraud%2520detection%252C%2520employing%2520reinforcement%2520learning%2520to%2520bypass%250Aclassifiers.%2520This%2520attack%252C%2520called%2520FRAUD-RLA%252C%2520is%2520designed%2520to%2520maximize%2520the%250Aattacker%2527s%2520reward%2520by%2520optimizing%2520the%2520exploration-exploitation%2520tradeoff%2520and%250Aworking%2520with%2520significantly%2520less%2520required%2520knowledge%2520than%2520competitors.%2520Our%250Aexperiments%252C%2520conducted%2520on%2520three%2520different%2520heterogeneous%2520datasets%2520and%2520against%250Atwo%2520fraud%2520detection%2520systems%252C%2520indicate%2520that%2520FRAUD-RLA%2520is%2520effective%252C%2520even%250Aconsidering%2520the%2520severe%2520limitations%2520imposed%2520by%2520our%2520threat%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRAUD-RLA%3A%20A%20new%20reinforcement%20learning%20adversarial%20attack%20against%0A%20%20credit%20card%20fraud%20detection&entry.906535625=Daniele%20Lunghi%20and%20Yannick%20Molinghen%20and%20Alkis%20Simitsis%20and%20Tom%20Lenaerts%20and%20Gianluca%20Bontempi&entry.1292438233=%20%20Adversarial%20attacks%20pose%20a%20significant%20threat%20to%20data-driven%20systems%2C%20and%0Aresearchers%20have%20spent%20considerable%20resources%20studying%20them.%20Despite%20its%0Aeconomic%20relevance%2C%20this%20trend%20largely%20overlooked%20the%20issue%20of%20credit%20card%0Afraud%20detection.%20To%20address%20this%20gap%2C%20we%20propose%20a%20new%20threat%20model%20that%0Ademonstrates%20the%20limitations%20of%20existing%20attacks%20and%20highlights%20the%20necessity%0Ato%20investigate%20new%20approaches.%20We%20then%20design%20a%20new%20adversarial%20attack%20for%0Acredit%20card%20fraud%20detection%2C%20employing%20reinforcement%20learning%20to%20bypass%0Aclassifiers.%20This%20attack%2C%20called%20FRAUD-RLA%2C%20is%20designed%20to%20maximize%20the%0Aattacker%27s%20reward%20by%20optimizing%20the%20exploration-exploitation%20tradeoff%20and%0Aworking%20with%20significantly%20less%20required%20knowledge%20than%20competitors.%20Our%0Aexperiments%2C%20conducted%20on%20three%20different%20heterogeneous%20datasets%20and%20against%0Atwo%20fraud%20detection%20systems%2C%20indicate%20that%20FRAUD-RLA%20is%20effective%2C%20even%0Aconsidering%20the%20severe%20limitations%20imposed%20by%20our%20threat%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02290v1&entry.124074799=Read"},
{"title": "ControlFace: Harnessing Facial Parametric Control for Face Rigging", "author": "Wooseok Jang and Youngjun Hong and Geonho Cha and Seungryong Kim", "abstract": "  Manipulation of facial images to meet specific controls such as pose,\nexpression, and lighting, also known as face rigging, is a complex task in\ncomputer vision. Existing methods are limited by their reliance on image\ndatasets, which necessitates individual-specific fine-tuning and limits their\nability to retain fine-grained identity and semantic details, reducing\npractical usability. To overcome these limitations, we introduce ControlFace, a\nnovel face rigging method conditioned on 3DMM renderings that enables flexible,\nhigh-fidelity control. We employ a dual-branch U-Nets: one, referred to as\nFaceNet, captures identity and fine details, while the other focuses on\ngeneration. To enhance control precision, the control mixer module encodes the\ncorrelated features between the target-aligned control and reference-aligned\ncontrol, and a novel guidance method, reference control guidance, steers the\ngeneration process for better control adherence. By training on a facial video\ndataset, we fully utilize FaceNet's rich representations while ensuring control\nadherence. Extensive experiments demonstrate ControlFace's superior performance\nin identity preservation and control precision, highlighting its practicality.\nPlease see the project website: https://cvlab-kaist.github.io/ControlFace/.\n", "link": "http://arxiv.org/abs/2412.01160v4", "date": "2025-02-04", "relevancy": 2.3609, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6096}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5918}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlFace%3A%20Harnessing%20Facial%20Parametric%20Control%20for%20Face%20Rigging&body=Title%3A%20ControlFace%3A%20Harnessing%20Facial%20Parametric%20Control%20for%20Face%20Rigging%0AAuthor%3A%20Wooseok%20Jang%20and%20Youngjun%20Hong%20and%20Geonho%20Cha%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Manipulation%20of%20facial%20images%20to%20meet%20specific%20controls%20such%20as%20pose%2C%0Aexpression%2C%20and%20lighting%2C%20also%20known%20as%20face%20rigging%2C%20is%20a%20complex%20task%20in%0Acomputer%20vision.%20Existing%20methods%20are%20limited%20by%20their%20reliance%20on%20image%0Adatasets%2C%20which%20necessitates%20individual-specific%20fine-tuning%20and%20limits%20their%0Aability%20to%20retain%20fine-grained%20identity%20and%20semantic%20details%2C%20reducing%0Apractical%20usability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20ControlFace%2C%20a%0Anovel%20face%20rigging%20method%20conditioned%20on%203DMM%20renderings%20that%20enables%20flexible%2C%0Ahigh-fidelity%20control.%20We%20employ%20a%20dual-branch%20U-Nets%3A%20one%2C%20referred%20to%20as%0AFaceNet%2C%20captures%20identity%20and%20fine%20details%2C%20while%20the%20other%20focuses%20on%0Ageneration.%20To%20enhance%20control%20precision%2C%20the%20control%20mixer%20module%20encodes%20the%0Acorrelated%20features%20between%20the%20target-aligned%20control%20and%20reference-aligned%0Acontrol%2C%20and%20a%20novel%20guidance%20method%2C%20reference%20control%20guidance%2C%20steers%20the%0Ageneration%20process%20for%20better%20control%20adherence.%20By%20training%20on%20a%20facial%20video%0Adataset%2C%20we%20fully%20utilize%20FaceNet%27s%20rich%20representations%20while%20ensuring%20control%0Aadherence.%20Extensive%20experiments%20demonstrate%20ControlFace%27s%20superior%20performance%0Ain%20identity%20preservation%20and%20control%20precision%2C%20highlighting%20its%20practicality.%0APlease%20see%20the%20project%20website%3A%20https%3A//cvlab-kaist.github.io/ControlFace/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01160v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlFace%253A%2520Harnessing%2520Facial%2520Parametric%2520Control%2520for%2520Face%2520Rigging%26entry.906535625%3DWooseok%2520Jang%2520and%2520Youngjun%2520Hong%2520and%2520Geonho%2520Cha%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Manipulation%2520of%2520facial%2520images%2520to%2520meet%2520specific%2520controls%2520such%2520as%2520pose%252C%250Aexpression%252C%2520and%2520lighting%252C%2520also%2520known%2520as%2520face%2520rigging%252C%2520is%2520a%2520complex%2520task%2520in%250Acomputer%2520vision.%2520Existing%2520methods%2520are%2520limited%2520by%2520their%2520reliance%2520on%2520image%250Adatasets%252C%2520which%2520necessitates%2520individual-specific%2520fine-tuning%2520and%2520limits%2520their%250Aability%2520to%2520retain%2520fine-grained%2520identity%2520and%2520semantic%2520details%252C%2520reducing%250Apractical%2520usability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520ControlFace%252C%2520a%250Anovel%2520face%2520rigging%2520method%2520conditioned%2520on%25203DMM%2520renderings%2520that%2520enables%2520flexible%252C%250Ahigh-fidelity%2520control.%2520We%2520employ%2520a%2520dual-branch%2520U-Nets%253A%2520one%252C%2520referred%2520to%2520as%250AFaceNet%252C%2520captures%2520identity%2520and%2520fine%2520details%252C%2520while%2520the%2520other%2520focuses%2520on%250Ageneration.%2520To%2520enhance%2520control%2520precision%252C%2520the%2520control%2520mixer%2520module%2520encodes%2520the%250Acorrelated%2520features%2520between%2520the%2520target-aligned%2520control%2520and%2520reference-aligned%250Acontrol%252C%2520and%2520a%2520novel%2520guidance%2520method%252C%2520reference%2520control%2520guidance%252C%2520steers%2520the%250Ageneration%2520process%2520for%2520better%2520control%2520adherence.%2520By%2520training%2520on%2520a%2520facial%2520video%250Adataset%252C%2520we%2520fully%2520utilize%2520FaceNet%2527s%2520rich%2520representations%2520while%2520ensuring%2520control%250Aadherence.%2520Extensive%2520experiments%2520demonstrate%2520ControlFace%2527s%2520superior%2520performance%250Ain%2520identity%2520preservation%2520and%2520control%2520precision%252C%2520highlighting%2520its%2520practicality.%250APlease%2520see%2520the%2520project%2520website%253A%2520https%253A//cvlab-kaist.github.io/ControlFace/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01160v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlFace%3A%20Harnessing%20Facial%20Parametric%20Control%20for%20Face%20Rigging&entry.906535625=Wooseok%20Jang%20and%20Youngjun%20Hong%20and%20Geonho%20Cha%20and%20Seungryong%20Kim&entry.1292438233=%20%20Manipulation%20of%20facial%20images%20to%20meet%20specific%20controls%20such%20as%20pose%2C%0Aexpression%2C%20and%20lighting%2C%20also%20known%20as%20face%20rigging%2C%20is%20a%20complex%20task%20in%0Acomputer%20vision.%20Existing%20methods%20are%20limited%20by%20their%20reliance%20on%20image%0Adatasets%2C%20which%20necessitates%20individual-specific%20fine-tuning%20and%20limits%20their%0Aability%20to%20retain%20fine-grained%20identity%20and%20semantic%20details%2C%20reducing%0Apractical%20usability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20ControlFace%2C%20a%0Anovel%20face%20rigging%20method%20conditioned%20on%203DMM%20renderings%20that%20enables%20flexible%2C%0Ahigh-fidelity%20control.%20We%20employ%20a%20dual-branch%20U-Nets%3A%20one%2C%20referred%20to%20as%0AFaceNet%2C%20captures%20identity%20and%20fine%20details%2C%20while%20the%20other%20focuses%20on%0Ageneration.%20To%20enhance%20control%20precision%2C%20the%20control%20mixer%20module%20encodes%20the%0Acorrelated%20features%20between%20the%20target-aligned%20control%20and%20reference-aligned%0Acontrol%2C%20and%20a%20novel%20guidance%20method%2C%20reference%20control%20guidance%2C%20steers%20the%0Ageneration%20process%20for%20better%20control%20adherence.%20By%20training%20on%20a%20facial%20video%0Adataset%2C%20we%20fully%20utilize%20FaceNet%27s%20rich%20representations%20while%20ensuring%20control%0Aadherence.%20Extensive%20experiments%20demonstrate%20ControlFace%27s%20superior%20performance%0Ain%20identity%20preservation%20and%20control%20precision%2C%20highlighting%20its%20practicality.%0APlease%20see%20the%20project%20website%3A%20https%3A//cvlab-kaist.github.io/ControlFace/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01160v4&entry.124074799=Read"},
{"title": "Video Latent Flow Matching: Optimal Polynomial Projections for Video\n  Interpolation and Extrapolation", "author": "Yang Cao and Zhao Song and Chiwun Yang", "abstract": "  This paper considers an efficient video modeling process called Video Latent\nFlow Matching (VLFM). Unlike prior works, which randomly sampled latent patches\nfor video generation, our method relies on current strong pre-trained image\ngeneration models, modeling a certain caption-guided flow of latent patches\nthat can be decoded to time-dependent video frames. We first speculate multiple\nimages of a video are differentiable with respect to time in some latent space.\nBased on this conjecture, we introduce the HiPPO framework to approximate the\noptimal projection for polynomials to generate the probability path. Our\napproach gains the theoretical benefits of the bounded universal approximation\nerror and timescale robustness. Moreover, VLFM processes the interpolation and\nextrapolation abilities for video generation with arbitrary frame rates. We\nconduct experiments on several text-to-video datasets to showcase the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2502.00500v2", "date": "2025-02-04", "relevancy": 2.359, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6029}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Latent%20Flow%20Matching%3A%20Optimal%20Polynomial%20Projections%20for%20Video%0A%20%20Interpolation%20and%20Extrapolation&body=Title%3A%20Video%20Latent%20Flow%20Matching%3A%20Optimal%20Polynomial%20Projections%20for%20Video%0A%20%20Interpolation%20and%20Extrapolation%0AAuthor%3A%20Yang%20Cao%20and%20Zhao%20Song%20and%20Chiwun%20Yang%0AAbstract%3A%20%20%20This%20paper%20considers%20an%20efficient%20video%20modeling%20process%20called%20Video%20Latent%0AFlow%20Matching%20%28VLFM%29.%20Unlike%20prior%20works%2C%20which%20randomly%20sampled%20latent%20patches%0Afor%20video%20generation%2C%20our%20method%20relies%20on%20current%20strong%20pre-trained%20image%0Ageneration%20models%2C%20modeling%20a%20certain%20caption-guided%20flow%20of%20latent%20patches%0Athat%20can%20be%20decoded%20to%20time-dependent%20video%20frames.%20We%20first%20speculate%20multiple%0Aimages%20of%20a%20video%20are%20differentiable%20with%20respect%20to%20time%20in%20some%20latent%20space.%0ABased%20on%20this%20conjecture%2C%20we%20introduce%20the%20HiPPO%20framework%20to%20approximate%20the%0Aoptimal%20projection%20for%20polynomials%20to%20generate%20the%20probability%20path.%20Our%0Aapproach%20gains%20the%20theoretical%20benefits%20of%20the%20bounded%20universal%20approximation%0Aerror%20and%20timescale%20robustness.%20Moreover%2C%20VLFM%20processes%20the%20interpolation%20and%0Aextrapolation%20abilities%20for%20video%20generation%20with%20arbitrary%20frame%20rates.%20We%0Aconduct%20experiments%20on%20several%20text-to-video%20datasets%20to%20showcase%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Latent%2520Flow%2520Matching%253A%2520Optimal%2520Polynomial%2520Projections%2520for%2520Video%250A%2520%2520Interpolation%2520and%2520Extrapolation%26entry.906535625%3DYang%2520Cao%2520and%2520Zhao%2520Song%2520and%2520Chiwun%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520an%2520efficient%2520video%2520modeling%2520process%2520called%2520Video%2520Latent%250AFlow%2520Matching%2520%2528VLFM%2529.%2520Unlike%2520prior%2520works%252C%2520which%2520randomly%2520sampled%2520latent%2520patches%250Afor%2520video%2520generation%252C%2520our%2520method%2520relies%2520on%2520current%2520strong%2520pre-trained%2520image%250Ageneration%2520models%252C%2520modeling%2520a%2520certain%2520caption-guided%2520flow%2520of%2520latent%2520patches%250Athat%2520can%2520be%2520decoded%2520to%2520time-dependent%2520video%2520frames.%2520We%2520first%2520speculate%2520multiple%250Aimages%2520of%2520a%2520video%2520are%2520differentiable%2520with%2520respect%2520to%2520time%2520in%2520some%2520latent%2520space.%250ABased%2520on%2520this%2520conjecture%252C%2520we%2520introduce%2520the%2520HiPPO%2520framework%2520to%2520approximate%2520the%250Aoptimal%2520projection%2520for%2520polynomials%2520to%2520generate%2520the%2520probability%2520path.%2520Our%250Aapproach%2520gains%2520the%2520theoretical%2520benefits%2520of%2520the%2520bounded%2520universal%2520approximation%250Aerror%2520and%2520timescale%2520robustness.%2520Moreover%252C%2520VLFM%2520processes%2520the%2520interpolation%2520and%250Aextrapolation%2520abilities%2520for%2520video%2520generation%2520with%2520arbitrary%2520frame%2520rates.%2520We%250Aconduct%2520experiments%2520on%2520several%2520text-to-video%2520datasets%2520to%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Latent%20Flow%20Matching%3A%20Optimal%20Polynomial%20Projections%20for%20Video%0A%20%20Interpolation%20and%20Extrapolation&entry.906535625=Yang%20Cao%20and%20Zhao%20Song%20and%20Chiwun%20Yang&entry.1292438233=%20%20This%20paper%20considers%20an%20efficient%20video%20modeling%20process%20called%20Video%20Latent%0AFlow%20Matching%20%28VLFM%29.%20Unlike%20prior%20works%2C%20which%20randomly%20sampled%20latent%20patches%0Afor%20video%20generation%2C%20our%20method%20relies%20on%20current%20strong%20pre-trained%20image%0Ageneration%20models%2C%20modeling%20a%20certain%20caption-guided%20flow%20of%20latent%20patches%0Athat%20can%20be%20decoded%20to%20time-dependent%20video%20frames.%20We%20first%20speculate%20multiple%0Aimages%20of%20a%20video%20are%20differentiable%20with%20respect%20to%20time%20in%20some%20latent%20space.%0ABased%20on%20this%20conjecture%2C%20we%20introduce%20the%20HiPPO%20framework%20to%20approximate%20the%0Aoptimal%20projection%20for%20polynomials%20to%20generate%20the%20probability%20path.%20Our%0Aapproach%20gains%20the%20theoretical%20benefits%20of%20the%20bounded%20universal%20approximation%0Aerror%20and%20timescale%20robustness.%20Moreover%2C%20VLFM%20processes%20the%20interpolation%20and%0Aextrapolation%20abilities%20for%20video%20generation%20with%20arbitrary%20frame%20rates.%20We%0Aconduct%20experiments%20on%20several%20text-to-video%20datasets%20to%20showcase%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00500v2&entry.124074799=Read"},
{"title": "Modular Training of Neural Networks aids Interpretability", "author": "Satvik Golechha and Maheep Chaudhary and Joan Velja and Alessandro Abate and Nandi Schoots", "abstract": "  An approach to improve neural network interpretability is via clusterability,\ni.e., splitting a model into disjoint clusters that can be studied\nindependently. We define a measure for clusterability and show that pre-trained\nmodels form highly enmeshed clusters via spectral graph clustering. We thus\ntrain models to be more modular using a ``clusterability loss'' function that\nencourages the formation of non-interacting clusters. Using automated\ninterpretability techniques, we show that our method can help train models that\nare more modular and learn different, disjoint, and smaller circuits. We\ninvestigate CNNs trained on MNIST and CIFAR, small transformers trained on\nmodular addition, and language models. Our approach provides a promising\ndirection for training neural networks that learn simpler functions and are\neasier to interpret.\n", "link": "http://arxiv.org/abs/2502.02470v1", "date": "2025-02-04", "relevancy": 2.336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Training%20of%20Neural%20Networks%20aids%20Interpretability&body=Title%3A%20Modular%20Training%20of%20Neural%20Networks%20aids%20Interpretability%0AAuthor%3A%20Satvik%20Golechha%20and%20Maheep%20Chaudhary%20and%20Joan%20Velja%20and%20Alessandro%20Abate%20and%20Nandi%20Schoots%0AAbstract%3A%20%20%20An%20approach%20to%20improve%20neural%20network%20interpretability%20is%20via%20clusterability%2C%0Ai.e.%2C%20splitting%20a%20model%20into%20disjoint%20clusters%20that%20can%20be%20studied%0Aindependently.%20We%20define%20a%20measure%20for%20clusterability%20and%20show%20that%20pre-trained%0Amodels%20form%20highly%20enmeshed%20clusters%20via%20spectral%20graph%20clustering.%20We%20thus%0Atrain%20models%20to%20be%20more%20modular%20using%20a%20%60%60clusterability%20loss%27%27%20function%20that%0Aencourages%20the%20formation%20of%20non-interacting%20clusters.%20Using%20automated%0Ainterpretability%20techniques%2C%20we%20show%20that%20our%20method%20can%20help%20train%20models%20that%0Aare%20more%20modular%20and%20learn%20different%2C%20disjoint%2C%20and%20smaller%20circuits.%20We%0Ainvestigate%20CNNs%20trained%20on%20MNIST%20and%20CIFAR%2C%20small%20transformers%20trained%20on%0Amodular%20addition%2C%20and%20language%20models.%20Our%20approach%20provides%20a%20promising%0Adirection%20for%20training%20neural%20networks%20that%20learn%20simpler%20functions%20and%20are%0Aeasier%20to%20interpret.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Training%2520of%2520Neural%2520Networks%2520aids%2520Interpretability%26entry.906535625%3DSatvik%2520Golechha%2520and%2520Maheep%2520Chaudhary%2520and%2520Joan%2520Velja%2520and%2520Alessandro%2520Abate%2520and%2520Nandi%2520Schoots%26entry.1292438233%3D%2520%2520An%2520approach%2520to%2520improve%2520neural%2520network%2520interpretability%2520is%2520via%2520clusterability%252C%250Ai.e.%252C%2520splitting%2520a%2520model%2520into%2520disjoint%2520clusters%2520that%2520can%2520be%2520studied%250Aindependently.%2520We%2520define%2520a%2520measure%2520for%2520clusterability%2520and%2520show%2520that%2520pre-trained%250Amodels%2520form%2520highly%2520enmeshed%2520clusters%2520via%2520spectral%2520graph%2520clustering.%2520We%2520thus%250Atrain%2520models%2520to%2520be%2520more%2520modular%2520using%2520a%2520%2560%2560clusterability%2520loss%2527%2527%2520function%2520that%250Aencourages%2520the%2520formation%2520of%2520non-interacting%2520clusters.%2520Using%2520automated%250Ainterpretability%2520techniques%252C%2520we%2520show%2520that%2520our%2520method%2520can%2520help%2520train%2520models%2520that%250Aare%2520more%2520modular%2520and%2520learn%2520different%252C%2520disjoint%252C%2520and%2520smaller%2520circuits.%2520We%250Ainvestigate%2520CNNs%2520trained%2520on%2520MNIST%2520and%2520CIFAR%252C%2520small%2520transformers%2520trained%2520on%250Amodular%2520addition%252C%2520and%2520language%2520models.%2520Our%2520approach%2520provides%2520a%2520promising%250Adirection%2520for%2520training%2520neural%2520networks%2520that%2520learn%2520simpler%2520functions%2520and%2520are%250Aeasier%2520to%2520interpret.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Training%20of%20Neural%20Networks%20aids%20Interpretability&entry.906535625=Satvik%20Golechha%20and%20Maheep%20Chaudhary%20and%20Joan%20Velja%20and%20Alessandro%20Abate%20and%20Nandi%20Schoots&entry.1292438233=%20%20An%20approach%20to%20improve%20neural%20network%20interpretability%20is%20via%20clusterability%2C%0Ai.e.%2C%20splitting%20a%20model%20into%20disjoint%20clusters%20that%20can%20be%20studied%0Aindependently.%20We%20define%20a%20measure%20for%20clusterability%20and%20show%20that%20pre-trained%0Amodels%20form%20highly%20enmeshed%20clusters%20via%20spectral%20graph%20clustering.%20We%20thus%0Atrain%20models%20to%20be%20more%20modular%20using%20a%20%60%60clusterability%20loss%27%27%20function%20that%0Aencourages%20the%20formation%20of%20non-interacting%20clusters.%20Using%20automated%0Ainterpretability%20techniques%2C%20we%20show%20that%20our%20method%20can%20help%20train%20models%20that%0Aare%20more%20modular%20and%20learn%20different%2C%20disjoint%2C%20and%20smaller%20circuits.%20We%0Ainvestigate%20CNNs%20trained%20on%20MNIST%20and%20CIFAR%2C%20small%20transformers%20trained%20on%0Amodular%20addition%2C%20and%20language%20models.%20Our%20approach%20provides%20a%20promising%0Adirection%20for%20training%20neural%20networks%20that%20learn%20simpler%20functions%20and%20are%0Aeasier%20to%20interpret.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02470v1&entry.124074799=Read"},
{"title": "Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent\n  Autoencoders for the Compression of the Stimulation Patterns of Cochlear\n  Implants at Zero Delay", "author": "Reemt Hinrichs and J\u00f6rn Ostermann", "abstract": "  Cochlear implants (CIs) are surgically implanted hearing devices, which allow\nto restore a sense of hearing in people suffering from profound hearing loss.\nWireless streaming of audio from external devices to CI signal processors has\nbecome common place. Specialized compression based on the stimulation patterns\nof a CI by deep recurrent autoencoders can decrease the power consumption in\nsuch a wireless streaming application through bit-rate reduction at zero\nlatency.\n  While previous research achieved considerable bit-rate reductions, model\nsizes were ignored, which can be of crucial importance in hearing-aids due to\ntheir limited computational resources. This work investigates maximizing\nobjective speech intelligibility of the coded stimulation patterns of deep\nrecurrent autoencoders while minimizing model size. For this purpose, a\npruning-aware loss is proposed, which captures the impact of pruning during\ntraining. This training with a pruning-aware loss is compared to conventional\nmagnitude-informed pruning and is found to yield considerable improvements in\nobjective intelligibility, especially at higher pruning rates. After\nfine-tuning, little to no degradation of objective intelligibility is observed\nup to a pruning rate of about 55\\,\\%. The proposed pruning-aware loss yields\nsubstantial gains in objective speech intelligibility scores after pruning\ncompared to the magnitude-informed baseline for pruning rates above 45\\,\\%.\n", "link": "http://arxiv.org/abs/2502.02424v1", "date": "2025-02-04", "relevancy": 2.3342, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4678}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning-aware%20Loss%20Functions%20for%20STOI-Optimized%20Pruned%20Recurrent%0A%20%20Autoencoders%20for%20the%20Compression%20of%20the%20Stimulation%20Patterns%20of%20Cochlear%0A%20%20Implants%20at%20Zero%20Delay&body=Title%3A%20Pruning-aware%20Loss%20Functions%20for%20STOI-Optimized%20Pruned%20Recurrent%0A%20%20Autoencoders%20for%20the%20Compression%20of%20the%20Stimulation%20Patterns%20of%20Cochlear%0A%20%20Implants%20at%20Zero%20Delay%0AAuthor%3A%20Reemt%20Hinrichs%20and%20J%C3%B6rn%20Ostermann%0AAbstract%3A%20%20%20Cochlear%20implants%20%28CIs%29%20are%20surgically%20implanted%20hearing%20devices%2C%20which%20allow%0Ato%20restore%20a%20sense%20of%20hearing%20in%20people%20suffering%20from%20profound%20hearing%20loss.%0AWireless%20streaming%20of%20audio%20from%20external%20devices%20to%20CI%20signal%20processors%20has%0Abecome%20common%20place.%20Specialized%20compression%20based%20on%20the%20stimulation%20patterns%0Aof%20a%20CI%20by%20deep%20recurrent%20autoencoders%20can%20decrease%20the%20power%20consumption%20in%0Asuch%20a%20wireless%20streaming%20application%20through%20bit-rate%20reduction%20at%20zero%0Alatency.%0A%20%20While%20previous%20research%20achieved%20considerable%20bit-rate%20reductions%2C%20model%0Asizes%20were%20ignored%2C%20which%20can%20be%20of%20crucial%20importance%20in%20hearing-aids%20due%20to%0Atheir%20limited%20computational%20resources.%20This%20work%20investigates%20maximizing%0Aobjective%20speech%20intelligibility%20of%20the%20coded%20stimulation%20patterns%20of%20deep%0Arecurrent%20autoencoders%20while%20minimizing%20model%20size.%20For%20this%20purpose%2C%20a%0Apruning-aware%20loss%20is%20proposed%2C%20which%20captures%20the%20impact%20of%20pruning%20during%0Atraining.%20This%20training%20with%20a%20pruning-aware%20loss%20is%20compared%20to%20conventional%0Amagnitude-informed%20pruning%20and%20is%20found%20to%20yield%20considerable%20improvements%20in%0Aobjective%20intelligibility%2C%20especially%20at%20higher%20pruning%20rates.%20After%0Afine-tuning%2C%20little%20to%20no%20degradation%20of%20objective%20intelligibility%20is%20observed%0Aup%20to%20a%20pruning%20rate%20of%20about%2055%5C%2C%5C%25.%20The%20proposed%20pruning-aware%20loss%20yields%0Asubstantial%20gains%20in%20objective%20speech%20intelligibility%20scores%20after%20pruning%0Acompared%20to%20the%20magnitude-informed%20baseline%20for%20pruning%20rates%20above%2045%5C%2C%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning-aware%2520Loss%2520Functions%2520for%2520STOI-Optimized%2520Pruned%2520Recurrent%250A%2520%2520Autoencoders%2520for%2520the%2520Compression%2520of%2520the%2520Stimulation%2520Patterns%2520of%2520Cochlear%250A%2520%2520Implants%2520at%2520Zero%2520Delay%26entry.906535625%3DReemt%2520Hinrichs%2520and%2520J%25C3%25B6rn%2520Ostermann%26entry.1292438233%3D%2520%2520Cochlear%2520implants%2520%2528CIs%2529%2520are%2520surgically%2520implanted%2520hearing%2520devices%252C%2520which%2520allow%250Ato%2520restore%2520a%2520sense%2520of%2520hearing%2520in%2520people%2520suffering%2520from%2520profound%2520hearing%2520loss.%250AWireless%2520streaming%2520of%2520audio%2520from%2520external%2520devices%2520to%2520CI%2520signal%2520processors%2520has%250Abecome%2520common%2520place.%2520Specialized%2520compression%2520based%2520on%2520the%2520stimulation%2520patterns%250Aof%2520a%2520CI%2520by%2520deep%2520recurrent%2520autoencoders%2520can%2520decrease%2520the%2520power%2520consumption%2520in%250Asuch%2520a%2520wireless%2520streaming%2520application%2520through%2520bit-rate%2520reduction%2520at%2520zero%250Alatency.%250A%2520%2520While%2520previous%2520research%2520achieved%2520considerable%2520bit-rate%2520reductions%252C%2520model%250Asizes%2520were%2520ignored%252C%2520which%2520can%2520be%2520of%2520crucial%2520importance%2520in%2520hearing-aids%2520due%2520to%250Atheir%2520limited%2520computational%2520resources.%2520This%2520work%2520investigates%2520maximizing%250Aobjective%2520speech%2520intelligibility%2520of%2520the%2520coded%2520stimulation%2520patterns%2520of%2520deep%250Arecurrent%2520autoencoders%2520while%2520minimizing%2520model%2520size.%2520For%2520this%2520purpose%252C%2520a%250Apruning-aware%2520loss%2520is%2520proposed%252C%2520which%2520captures%2520the%2520impact%2520of%2520pruning%2520during%250Atraining.%2520This%2520training%2520with%2520a%2520pruning-aware%2520loss%2520is%2520compared%2520to%2520conventional%250Amagnitude-informed%2520pruning%2520and%2520is%2520found%2520to%2520yield%2520considerable%2520improvements%2520in%250Aobjective%2520intelligibility%252C%2520especially%2520at%2520higher%2520pruning%2520rates.%2520After%250Afine-tuning%252C%2520little%2520to%2520no%2520degradation%2520of%2520objective%2520intelligibility%2520is%2520observed%250Aup%2520to%2520a%2520pruning%2520rate%2520of%2520about%252055%255C%252C%255C%2525.%2520The%2520proposed%2520pruning-aware%2520loss%2520yields%250Asubstantial%2520gains%2520in%2520objective%2520speech%2520intelligibility%2520scores%2520after%2520pruning%250Acompared%2520to%2520the%2520magnitude-informed%2520baseline%2520for%2520pruning%2520rates%2520above%252045%255C%252C%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning-aware%20Loss%20Functions%20for%20STOI-Optimized%20Pruned%20Recurrent%0A%20%20Autoencoders%20for%20the%20Compression%20of%20the%20Stimulation%20Patterns%20of%20Cochlear%0A%20%20Implants%20at%20Zero%20Delay&entry.906535625=Reemt%20Hinrichs%20and%20J%C3%B6rn%20Ostermann&entry.1292438233=%20%20Cochlear%20implants%20%28CIs%29%20are%20surgically%20implanted%20hearing%20devices%2C%20which%20allow%0Ato%20restore%20a%20sense%20of%20hearing%20in%20people%20suffering%20from%20profound%20hearing%20loss.%0AWireless%20streaming%20of%20audio%20from%20external%20devices%20to%20CI%20signal%20processors%20has%0Abecome%20common%20place.%20Specialized%20compression%20based%20on%20the%20stimulation%20patterns%0Aof%20a%20CI%20by%20deep%20recurrent%20autoencoders%20can%20decrease%20the%20power%20consumption%20in%0Asuch%20a%20wireless%20streaming%20application%20through%20bit-rate%20reduction%20at%20zero%0Alatency.%0A%20%20While%20previous%20research%20achieved%20considerable%20bit-rate%20reductions%2C%20model%0Asizes%20were%20ignored%2C%20which%20can%20be%20of%20crucial%20importance%20in%20hearing-aids%20due%20to%0Atheir%20limited%20computational%20resources.%20This%20work%20investigates%20maximizing%0Aobjective%20speech%20intelligibility%20of%20the%20coded%20stimulation%20patterns%20of%20deep%0Arecurrent%20autoencoders%20while%20minimizing%20model%20size.%20For%20this%20purpose%2C%20a%0Apruning-aware%20loss%20is%20proposed%2C%20which%20captures%20the%20impact%20of%20pruning%20during%0Atraining.%20This%20training%20with%20a%20pruning-aware%20loss%20is%20compared%20to%20conventional%0Amagnitude-informed%20pruning%20and%20is%20found%20to%20yield%20considerable%20improvements%20in%0Aobjective%20intelligibility%2C%20especially%20at%20higher%20pruning%20rates.%20After%0Afine-tuning%2C%20little%20to%20no%20degradation%20of%20objective%20intelligibility%20is%20observed%0Aup%20to%20a%20pruning%20rate%20of%20about%2055%5C%2C%5C%25.%20The%20proposed%20pruning-aware%20loss%20yields%0Asubstantial%20gains%20in%20objective%20speech%20intelligibility%20scores%20after%20pruning%0Acompared%20to%20the%20magnitude-informed%20baseline%20for%20pruning%20rates%20above%2045%5C%2C%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02424v1&entry.124074799=Read"},
{"title": "UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training", "author": "Jiawei Qin and Xucong Zhang and Yusuke Sugano", "abstract": "  Despite decades of research on data collection and model architectures,\ncurrent gaze estimation models face significant challenges in generalizing\nacross diverse data domains. While recent advances in self-supervised\npre-training have shown remarkable potential for improving model generalization\nin various vision tasks, their effectiveness in gaze estimation remains\nunexplored due to the geometric nature of the gaze regression task. We propose\nUniGaze, which leverages large-scale, in-the-wild facial datasets through\nself-supervised pre-training for gaze estimation. We carefully curate multiple\nfacial datasets that capture diverse variations in identity, lighting,\nbackground, and head poses. By directly applying Masked Autoencoder (MAE)\npre-training on normalized face images with a Vision Transformer (ViT)\nbackbone, our UniGaze learns appropriate feature representations within the\nspecific input space required by downstream gaze estimation models. Through\ncomprehensive experiments using challenging cross-dataset evaluation and novel\nprotocols, including leave-one-dataset-out and joint-dataset settings, we\ndemonstrate that UniGaze significantly improves generalization across multiple\ndata domains while minimizing reliance on costly labeled data. The source code\nand pre-trained models will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2502.02307v1", "date": "2025-02-04", "relevancy": 2.3263, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5878}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5792}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGaze%3A%20Towards%20Universal%20Gaze%20Estimation%20via%20Large-scale%20Pre-Training&body=Title%3A%20UniGaze%3A%20Towards%20Universal%20Gaze%20Estimation%20via%20Large-scale%20Pre-Training%0AAuthor%3A%20Jiawei%20Qin%20and%20Xucong%20Zhang%20and%20Yusuke%20Sugano%0AAbstract%3A%20%20%20Despite%20decades%20of%20research%20on%20data%20collection%20and%20model%20architectures%2C%0Acurrent%20gaze%20estimation%20models%20face%20significant%20challenges%20in%20generalizing%0Aacross%20diverse%20data%20domains.%20While%20recent%20advances%20in%20self-supervised%0Apre-training%20have%20shown%20remarkable%20potential%20for%20improving%20model%20generalization%0Ain%20various%20vision%20tasks%2C%20their%20effectiveness%20in%20gaze%20estimation%20remains%0Aunexplored%20due%20to%20the%20geometric%20nature%20of%20the%20gaze%20regression%20task.%20We%20propose%0AUniGaze%2C%20which%20leverages%20large-scale%2C%20in-the-wild%20facial%20datasets%20through%0Aself-supervised%20pre-training%20for%20gaze%20estimation.%20We%20carefully%20curate%20multiple%0Afacial%20datasets%20that%20capture%20diverse%20variations%20in%20identity%2C%20lighting%2C%0Abackground%2C%20and%20head%20poses.%20By%20directly%20applying%20Masked%20Autoencoder%20%28MAE%29%0Apre-training%20on%20normalized%20face%20images%20with%20a%20Vision%20Transformer%20%28ViT%29%0Abackbone%2C%20our%20UniGaze%20learns%20appropriate%20feature%20representations%20within%20the%0Aspecific%20input%20space%20required%20by%20downstream%20gaze%20estimation%20models.%20Through%0Acomprehensive%20experiments%20using%20challenging%20cross-dataset%20evaluation%20and%20novel%0Aprotocols%2C%20including%20leave-one-dataset-out%20and%20joint-dataset%20settings%2C%20we%0Ademonstrate%20that%20UniGaze%20significantly%20improves%20generalization%20across%20multiple%0Adata%20domains%20while%20minimizing%20reliance%20on%20costly%20labeled%20data.%20The%20source%20code%0Aand%20pre-trained%20models%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGaze%253A%2520Towards%2520Universal%2520Gaze%2520Estimation%2520via%2520Large-scale%2520Pre-Training%26entry.906535625%3DJiawei%2520Qin%2520and%2520Xucong%2520Zhang%2520and%2520Yusuke%2520Sugano%26entry.1292438233%3D%2520%2520Despite%2520decades%2520of%2520research%2520on%2520data%2520collection%2520and%2520model%2520architectures%252C%250Acurrent%2520gaze%2520estimation%2520models%2520face%2520significant%2520challenges%2520in%2520generalizing%250Aacross%2520diverse%2520data%2520domains.%2520While%2520recent%2520advances%2520in%2520self-supervised%250Apre-training%2520have%2520shown%2520remarkable%2520potential%2520for%2520improving%2520model%2520generalization%250Ain%2520various%2520vision%2520tasks%252C%2520their%2520effectiveness%2520in%2520gaze%2520estimation%2520remains%250Aunexplored%2520due%2520to%2520the%2520geometric%2520nature%2520of%2520the%2520gaze%2520regression%2520task.%2520We%2520propose%250AUniGaze%252C%2520which%2520leverages%2520large-scale%252C%2520in-the-wild%2520facial%2520datasets%2520through%250Aself-supervised%2520pre-training%2520for%2520gaze%2520estimation.%2520We%2520carefully%2520curate%2520multiple%250Afacial%2520datasets%2520that%2520capture%2520diverse%2520variations%2520in%2520identity%252C%2520lighting%252C%250Abackground%252C%2520and%2520head%2520poses.%2520By%2520directly%2520applying%2520Masked%2520Autoencoder%2520%2528MAE%2529%250Apre-training%2520on%2520normalized%2520face%2520images%2520with%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%250Abackbone%252C%2520our%2520UniGaze%2520learns%2520appropriate%2520feature%2520representations%2520within%2520the%250Aspecific%2520input%2520space%2520required%2520by%2520downstream%2520gaze%2520estimation%2520models.%2520Through%250Acomprehensive%2520experiments%2520using%2520challenging%2520cross-dataset%2520evaluation%2520and%2520novel%250Aprotocols%252C%2520including%2520leave-one-dataset-out%2520and%2520joint-dataset%2520settings%252C%2520we%250Ademonstrate%2520that%2520UniGaze%2520significantly%2520improves%2520generalization%2520across%2520multiple%250Adata%2520domains%2520while%2520minimizing%2520reliance%2520on%2520costly%2520labeled%2520data.%2520The%2520source%2520code%250Aand%2520pre-trained%2520models%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGaze%3A%20Towards%20Universal%20Gaze%20Estimation%20via%20Large-scale%20Pre-Training&entry.906535625=Jiawei%20Qin%20and%20Xucong%20Zhang%20and%20Yusuke%20Sugano&entry.1292438233=%20%20Despite%20decades%20of%20research%20on%20data%20collection%20and%20model%20architectures%2C%0Acurrent%20gaze%20estimation%20models%20face%20significant%20challenges%20in%20generalizing%0Aacross%20diverse%20data%20domains.%20While%20recent%20advances%20in%20self-supervised%0Apre-training%20have%20shown%20remarkable%20potential%20for%20improving%20model%20generalization%0Ain%20various%20vision%20tasks%2C%20their%20effectiveness%20in%20gaze%20estimation%20remains%0Aunexplored%20due%20to%20the%20geometric%20nature%20of%20the%20gaze%20regression%20task.%20We%20propose%0AUniGaze%2C%20which%20leverages%20large-scale%2C%20in-the-wild%20facial%20datasets%20through%0Aself-supervised%20pre-training%20for%20gaze%20estimation.%20We%20carefully%20curate%20multiple%0Afacial%20datasets%20that%20capture%20diverse%20variations%20in%20identity%2C%20lighting%2C%0Abackground%2C%20and%20head%20poses.%20By%20directly%20applying%20Masked%20Autoencoder%20%28MAE%29%0Apre-training%20on%20normalized%20face%20images%20with%20a%20Vision%20Transformer%20%28ViT%29%0Abackbone%2C%20our%20UniGaze%20learns%20appropriate%20feature%20representations%20within%20the%0Aspecific%20input%20space%20required%20by%20downstream%20gaze%20estimation%20models.%20Through%0Acomprehensive%20experiments%20using%20challenging%20cross-dataset%20evaluation%20and%20novel%0Aprotocols%2C%20including%20leave-one-dataset-out%20and%20joint-dataset%20settings%2C%20we%0Ademonstrate%20that%20UniGaze%20significantly%20improves%20generalization%20across%20multiple%0Adata%20domains%20while%20minimizing%20reliance%20on%20costly%20labeled%20data.%20The%20source%20code%0Aand%20pre-trained%20models%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02307v1&entry.124074799=Read"},
{"title": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal\n  Video Grounding", "author": "Akash Kumar and Zsolt Kira and Yogesh Singh Rawat", "abstract": "  In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding\n(WSTVG). It is a multimodal task aimed at localizing specific subjects\nspatio-temporally based on textual queries without bounding box supervision.\nMotivated by recent advancements in multi-modal foundation models for grounding\ntasks, we first explore the potential of state-of-the-art object detection\nmodels for WSTVG. Despite their robust zero-shot capabilities, our adaptation\nreveals significant limitations, including inconsistent temporal predictions,\ninadequate understanding of complex queries, and challenges in adapting to\ndifficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a\nnovel approach which is designed to overcome these limitations. CoSPaL\nintegrates three core components: (1) Tubelet Phrase Grounding (TPG), which\nintroduces spatio-temporal prediction by linking textual queries to tubelets;\n(2) Contextual Referral Grounding (CRG), which improves comprehension of\ncomplex queries by extracting contextual information to refine object\nidentification over time; and (3) Self-Paced Scene Understanding (SPS), a\ntraining paradigm that progressively increases task difficulty, enabling the\nmodel to adapt to complex scenarios by transitioning from coarse to\nfine-grained understanding.\n", "link": "http://arxiv.org/abs/2501.17053v2", "date": "2025-02-04", "relevancy": 2.3226, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6114}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5663}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding&body=Title%3A%20Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding%0AAuthor%3A%20Akash%20Kumar%20and%20Zsolt%20Kira%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20Weakly%20Supervised%20Spatio-Temporal%20Video%20Grounding%0A%28WSTVG%29.%20It%20is%20a%20multimodal%20task%20aimed%20at%20localizing%20specific%20subjects%0Aspatio-temporally%20based%20on%20textual%20queries%20without%20bounding%20box%20supervision.%0AMotivated%20by%20recent%20advancements%20in%20multi-modal%20foundation%20models%20for%20grounding%0Atasks%2C%20we%20first%20explore%20the%20potential%20of%20state-of-the-art%20object%20detection%0Amodels%20for%20WSTVG.%20Despite%20their%20robust%20zero-shot%20capabilities%2C%20our%20adaptation%0Areveals%20significant%20limitations%2C%20including%20inconsistent%20temporal%20predictions%2C%0Ainadequate%20understanding%20of%20complex%20queries%2C%20and%20challenges%20in%20adapting%20to%0Adifficult%20scenarios.%20We%20propose%20CoSPaL%20%28Contextual%20Self-Paced%20Learning%29%2C%20a%0Anovel%20approach%20which%20is%20designed%20to%20overcome%20these%20limitations.%20CoSPaL%0Aintegrates%20three%20core%20components%3A%20%281%29%20Tubelet%20Phrase%20Grounding%20%28TPG%29%2C%20which%0Aintroduces%20spatio-temporal%20prediction%20by%20linking%20textual%20queries%20to%20tubelets%3B%0A%282%29%20Contextual%20Referral%20Grounding%20%28CRG%29%2C%20which%20improves%20comprehension%20of%0Acomplex%20queries%20by%20extracting%20contextual%20information%20to%20refine%20object%0Aidentification%20over%20time%3B%20and%20%283%29%20Self-Paced%20Scene%20Understanding%20%28SPS%29%2C%20a%0Atraining%20paradigm%20that%20progressively%20increases%20task%20difficulty%2C%20enabling%20the%0Amodel%20to%20adapt%20to%20complex%20scenarios%20by%20transitioning%20from%20coarse%20to%0Afine-grained%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Self-paced%2520Learning%2520for%2520Weakly%2520Supervised%2520Spatio-Temporal%250A%2520%2520Video%2520Grounding%26entry.906535625%3DAkash%2520Kumar%2520and%2520Zsolt%2520Kira%2520and%2520Yogesh%2520Singh%2520Rawat%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520Weakly%2520Supervised%2520Spatio-Temporal%2520Video%2520Grounding%250A%2528WSTVG%2529.%2520It%2520is%2520a%2520multimodal%2520task%2520aimed%2520at%2520localizing%2520specific%2520subjects%250Aspatio-temporally%2520based%2520on%2520textual%2520queries%2520without%2520bounding%2520box%2520supervision.%250AMotivated%2520by%2520recent%2520advancements%2520in%2520multi-modal%2520foundation%2520models%2520for%2520grounding%250Atasks%252C%2520we%2520first%2520explore%2520the%2520potential%2520of%2520state-of-the-art%2520object%2520detection%250Amodels%2520for%2520WSTVG.%2520Despite%2520their%2520robust%2520zero-shot%2520capabilities%252C%2520our%2520adaptation%250Areveals%2520significant%2520limitations%252C%2520including%2520inconsistent%2520temporal%2520predictions%252C%250Ainadequate%2520understanding%2520of%2520complex%2520queries%252C%2520and%2520challenges%2520in%2520adapting%2520to%250Adifficult%2520scenarios.%2520We%2520propose%2520CoSPaL%2520%2528Contextual%2520Self-Paced%2520Learning%2529%252C%2520a%250Anovel%2520approach%2520which%2520is%2520designed%2520to%2520overcome%2520these%2520limitations.%2520CoSPaL%250Aintegrates%2520three%2520core%2520components%253A%2520%25281%2529%2520Tubelet%2520Phrase%2520Grounding%2520%2528TPG%2529%252C%2520which%250Aintroduces%2520spatio-temporal%2520prediction%2520by%2520linking%2520textual%2520queries%2520to%2520tubelets%253B%250A%25282%2529%2520Contextual%2520Referral%2520Grounding%2520%2528CRG%2529%252C%2520which%2520improves%2520comprehension%2520of%250Acomplex%2520queries%2520by%2520extracting%2520contextual%2520information%2520to%2520refine%2520object%250Aidentification%2520over%2520time%253B%2520and%2520%25283%2529%2520Self-Paced%2520Scene%2520Understanding%2520%2528SPS%2529%252C%2520a%250Atraining%2520paradigm%2520that%2520progressively%2520increases%2520task%2520difficulty%252C%2520enabling%2520the%250Amodel%2520to%2520adapt%2520to%2520complex%2520scenarios%2520by%2520transitioning%2520from%2520coarse%2520to%250Afine-grained%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding&entry.906535625=Akash%20Kumar%20and%20Zsolt%20Kira%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20Weakly%20Supervised%20Spatio-Temporal%20Video%20Grounding%0A%28WSTVG%29.%20It%20is%20a%20multimodal%20task%20aimed%20at%20localizing%20specific%20subjects%0Aspatio-temporally%20based%20on%20textual%20queries%20without%20bounding%20box%20supervision.%0AMotivated%20by%20recent%20advancements%20in%20multi-modal%20foundation%20models%20for%20grounding%0Atasks%2C%20we%20first%20explore%20the%20potential%20of%20state-of-the-art%20object%20detection%0Amodels%20for%20WSTVG.%20Despite%20their%20robust%20zero-shot%20capabilities%2C%20our%20adaptation%0Areveals%20significant%20limitations%2C%20including%20inconsistent%20temporal%20predictions%2C%0Ainadequate%20understanding%20of%20complex%20queries%2C%20and%20challenges%20in%20adapting%20to%0Adifficult%20scenarios.%20We%20propose%20CoSPaL%20%28Contextual%20Self-Paced%20Learning%29%2C%20a%0Anovel%20approach%20which%20is%20designed%20to%20overcome%20these%20limitations.%20CoSPaL%0Aintegrates%20three%20core%20components%3A%20%281%29%20Tubelet%20Phrase%20Grounding%20%28TPG%29%2C%20which%0Aintroduces%20spatio-temporal%20prediction%20by%20linking%20textual%20queries%20to%20tubelets%3B%0A%282%29%20Contextual%20Referral%20Grounding%20%28CRG%29%2C%20which%20improves%20comprehension%20of%0Acomplex%20queries%20by%20extracting%20contextual%20information%20to%20refine%20object%0Aidentification%20over%20time%3B%20and%20%283%29%20Self-Paced%20Scene%20Understanding%20%28SPS%29%2C%20a%0Atraining%20paradigm%20that%20progressively%20increases%20task%20difficulty%2C%20enabling%20the%0Amodel%20to%20adapt%20to%20complex%20scenarios%20by%20transitioning%20from%20coarse%20to%0Afine-grained%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17053v2&entry.124074799=Read"},
{"title": "Degree Distribution based Spiking Graph Networks for Domain Adaptation", "author": "Yingxu Wang and Mengzhu Wang and Siwei Liu and Nan Yin", "abstract": "  Spiking Graph Networks (SGNs) have garnered significant attraction from\nresearchers and industry due to their ability to address energy consumption\nchallenges in graph classification. However, SGNs are only effective for\nin-distribution data and cannot tackle out-of-distribution data. In this paper,\nwe first propose the domain adaptation problem in SGNs, and introduce a novel\nframework named Degree-aware Spiking Graph Domain Adaptation for Classification\n(DeSGDA). The proposed DeSGDA addresses the spiking graph domain adaptation\nproblem in three aspects: node degree-aware personalized spiking\nrepresentation, adversarial feature distribution alignment, and pseudo-label\ndistillation. First, we introduce the personalized spiking representation\nmethod for generating degree-dependent spiking signals. Specifically, the node\ndegree determines the threshold of triggering a spike, allowing this\npersonalized approach to capture more expressive information for\nclassification. Then, we propose the graph feature distribution alignment\nmodule that is adversarially trained using membrane potential against a domain\ndiscriminator. Such an alignment module can efficiently maintain high\nperformance and low energy consumption in the case of inconsistent\ndistribution. Additionally, we extract consistent predictions across two spaces\nto create reliable pseudo-labels, effectively leveraging unlabeled data to\nenhance classification performance. Extensive experiments on benchmark datasets\nvalidate the superiority of the DeSGDA compared with competitive baselines.\n", "link": "http://arxiv.org/abs/2410.06883v3", "date": "2025-02-04", "relevancy": 2.3214, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4656}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation&body=Title%3A%20Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation%0AAuthor%3A%20Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu%20and%20Nan%20Yin%0AAbstract%3A%20%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20garnered%20significant%20attraction%20from%0Aresearchers%20and%20industry%20due%20to%20their%20ability%20to%20address%20energy%20consumption%0Achallenges%20in%20graph%20classification.%20However%2C%20SGNs%20are%20only%20effective%20for%0Ain-distribution%20data%20and%20cannot%20tackle%20out-of-distribution%20data.%20In%20this%20paper%2C%0Awe%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%20novel%0Aframework%20named%20Degree-aware%20Spiking%20Graph%20Domain%20Adaptation%20for%20Classification%0A%28DeSGDA%29.%20The%20proposed%20DeSGDA%20addresses%20the%20spiking%20graph%20domain%20adaptation%0Aproblem%20in%20three%20aspects%3A%20node%20degree-aware%20personalized%20spiking%0Arepresentation%2C%20adversarial%20feature%20distribution%20alignment%2C%20and%20pseudo-label%0Adistillation.%20First%2C%20we%20introduce%20the%20personalized%20spiking%20representation%0Amethod%20for%20generating%20degree-dependent%20spiking%20signals.%20Specifically%2C%20the%20node%0Adegree%20determines%20the%20threshold%20of%20triggering%20a%20spike%2C%20allowing%20this%0Apersonalized%20approach%20to%20capture%20more%20expressive%20information%20for%0Aclassification.%20Then%2C%20we%20propose%20the%20graph%20feature%20distribution%20alignment%0Amodule%20that%20is%20adversarially%20trained%20using%20membrane%20potential%20against%20a%20domain%0Adiscriminator.%20Such%20an%20alignment%20module%20can%20efficiently%20maintain%20high%0Aperformance%20and%20low%20energy%20consumption%20in%20the%20case%20of%20inconsistent%0Adistribution.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%20spaces%0Ato%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%20to%0Aenhance%20classification%20performance.%20Extensive%20experiments%20on%20benchmark%20datasets%0Avalidate%20the%20superiority%20of%20the%20DeSGDA%20compared%20with%20competitive%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06883v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegree%2520Distribution%2520based%2520Spiking%2520Graph%2520Networks%2520for%2520Domain%2520Adaptation%26entry.906535625%3DYingxu%2520Wang%2520and%2520Mengzhu%2520Wang%2520and%2520Siwei%2520Liu%2520and%2520Nan%2520Yin%26entry.1292438233%3D%2520%2520Spiking%2520Graph%2520Networks%2520%2528SGNs%2529%2520have%2520garnered%2520significant%2520attraction%2520from%250Aresearchers%2520and%2520industry%2520due%2520to%2520their%2520ability%2520to%2520address%2520energy%2520consumption%250Achallenges%2520in%2520graph%2520classification.%2520However%252C%2520SGNs%2520are%2520only%2520effective%2520for%250Ain-distribution%2520data%2520and%2520cannot%2520tackle%2520out-of-distribution%2520data.%2520In%2520this%2520paper%252C%250Awe%2520first%2520propose%2520the%2520domain%2520adaptation%2520problem%2520in%2520SGNs%252C%2520and%2520introduce%2520a%2520novel%250Aframework%2520named%2520Degree-aware%2520Spiking%2520Graph%2520Domain%2520Adaptation%2520for%2520Classification%250A%2528DeSGDA%2529.%2520The%2520proposed%2520DeSGDA%2520addresses%2520the%2520spiking%2520graph%2520domain%2520adaptation%250Aproblem%2520in%2520three%2520aspects%253A%2520node%2520degree-aware%2520personalized%2520spiking%250Arepresentation%252C%2520adversarial%2520feature%2520distribution%2520alignment%252C%2520and%2520pseudo-label%250Adistillation.%2520First%252C%2520we%2520introduce%2520the%2520personalized%2520spiking%2520representation%250Amethod%2520for%2520generating%2520degree-dependent%2520spiking%2520signals.%2520Specifically%252C%2520the%2520node%250Adegree%2520determines%2520the%2520threshold%2520of%2520triggering%2520a%2520spike%252C%2520allowing%2520this%250Apersonalized%2520approach%2520to%2520capture%2520more%2520expressive%2520information%2520for%250Aclassification.%2520Then%252C%2520we%2520propose%2520the%2520graph%2520feature%2520distribution%2520alignment%250Amodule%2520that%2520is%2520adversarially%2520trained%2520using%2520membrane%2520potential%2520against%2520a%2520domain%250Adiscriminator.%2520Such%2520an%2520alignment%2520module%2520can%2520efficiently%2520maintain%2520high%250Aperformance%2520and%2520low%2520energy%2520consumption%2520in%2520the%2520case%2520of%2520inconsistent%250Adistribution.%2520Additionally%252C%2520we%2520extract%2520consistent%2520predictions%2520across%2520two%2520spaces%250Ato%2520create%2520reliable%2520pseudo-labels%252C%2520effectively%2520leveraging%2520unlabeled%2520data%2520to%250Aenhance%2520classification%2520performance.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%250Avalidate%2520the%2520superiority%2520of%2520the%2520DeSGDA%2520compared%2520with%2520competitive%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06883v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation&entry.906535625=Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu%20and%20Nan%20Yin&entry.1292438233=%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20garnered%20significant%20attraction%20from%0Aresearchers%20and%20industry%20due%20to%20their%20ability%20to%20address%20energy%20consumption%0Achallenges%20in%20graph%20classification.%20However%2C%20SGNs%20are%20only%20effective%20for%0Ain-distribution%20data%20and%20cannot%20tackle%20out-of-distribution%20data.%20In%20this%20paper%2C%0Awe%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%20novel%0Aframework%20named%20Degree-aware%20Spiking%20Graph%20Domain%20Adaptation%20for%20Classification%0A%28DeSGDA%29.%20The%20proposed%20DeSGDA%20addresses%20the%20spiking%20graph%20domain%20adaptation%0Aproblem%20in%20three%20aspects%3A%20node%20degree-aware%20personalized%20spiking%0Arepresentation%2C%20adversarial%20feature%20distribution%20alignment%2C%20and%20pseudo-label%0Adistillation.%20First%2C%20we%20introduce%20the%20personalized%20spiking%20representation%0Amethod%20for%20generating%20degree-dependent%20spiking%20signals.%20Specifically%2C%20the%20node%0Adegree%20determines%20the%20threshold%20of%20triggering%20a%20spike%2C%20allowing%20this%0Apersonalized%20approach%20to%20capture%20more%20expressive%20information%20for%0Aclassification.%20Then%2C%20we%20propose%20the%20graph%20feature%20distribution%20alignment%0Amodule%20that%20is%20adversarially%20trained%20using%20membrane%20potential%20against%20a%20domain%0Adiscriminator.%20Such%20an%20alignment%20module%20can%20efficiently%20maintain%20high%0Aperformance%20and%20low%20energy%20consumption%20in%20the%20case%20of%20inconsistent%0Adistribution.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%20spaces%0Ato%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%20to%0Aenhance%20classification%20performance.%20Extensive%20experiments%20on%20benchmark%20datasets%0Avalidate%20the%20superiority%20of%20the%20DeSGDA%20compared%20with%20competitive%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06883v3&entry.124074799=Read"},
{"title": "Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling", "author": "Xiaowen Qiu and Jincheng Yang and Yian Wang and Zhehuan Chen and Yufei Wang and Tsun-Hsuan Wang and Zhou Xian and Chuang Gan", "abstract": "  3D articulated objects modeling has long been a challenging problem, since it\nrequires to capture both accurate surface geometries and semantically\nmeaningful and spatially precise structures, parts, and joints. Existing\nmethods heavily depend on training data from a limited set of handcrafted\narticulated object categories (e.g., cabinets and drawers), which restricts\ntheir ability to model a wide range of articulated objects in an\nopen-vocabulary context. To address these limitations, we propose Articulate\nAnymesh, an automated framework that is able to convert any rigid 3D mesh into\nits articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our\nframework utilizes advanced Vision-Language Models and visual prompting\ntechniques to extract semantic information, allowing for both the segmentation\nof object parts and the construction of functional joints. Our experiments show\nthat Articulate Anymesh can generate large-scale, high-quality 3D articulated\nobjects, including tools, toys, mechanical devices, and vehicles, significantly\nexpanding the coverage of existing 3D articulated object datasets.\nAdditionally, we show that these generated assets can facilitate the\nacquisition of new articulated object manipulation skills in simulation, which\ncan then be transferred to a real robotic system. Our Github website is\nhttps://articulate-anymesh.github.io.\n", "link": "http://arxiv.org/abs/2502.02590v1", "date": "2025-02-04", "relevancy": 2.3009, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5854}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5745}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Articulate%20AnyMesh%3A%20Open-Vocabulary%203D%20Articulated%20Objects%20Modeling&body=Title%3A%20Articulate%20AnyMesh%3A%20Open-Vocabulary%203D%20Articulated%20Objects%20Modeling%0AAuthor%3A%20Xiaowen%20Qiu%20and%20Jincheng%20Yang%20and%20Yian%20Wang%20and%20Zhehuan%20Chen%20and%20Yufei%20Wang%20and%20Tsun-Hsuan%20Wang%20and%20Zhou%20Xian%20and%20Chuang%20Gan%0AAbstract%3A%20%20%203D%20articulated%20objects%20modeling%20has%20long%20been%20a%20challenging%20problem%2C%20since%20it%0Arequires%20to%20capture%20both%20accurate%20surface%20geometries%20and%20semantically%0Ameaningful%20and%20spatially%20precise%20structures%2C%20parts%2C%20and%20joints.%20Existing%0Amethods%20heavily%20depend%20on%20training%20data%20from%20a%20limited%20set%20of%20handcrafted%0Aarticulated%20object%20categories%20%28e.g.%2C%20cabinets%20and%20drawers%29%2C%20which%20restricts%0Atheir%20ability%20to%20model%20a%20wide%20range%20of%20articulated%20objects%20in%20an%0Aopen-vocabulary%20context.%20To%20address%20these%20limitations%2C%20we%20propose%20Articulate%0AAnymesh%2C%20an%20automated%20framework%20that%20is%20able%20to%20convert%20any%20rigid%203D%20mesh%20into%0Aits%20articulated%20counterpart%20in%20an%20open-vocabulary%20manner.%20Given%20a%203D%20mesh%2C%20our%0Aframework%20utilizes%20advanced%20Vision-Language%20Models%20and%20visual%20prompting%0Atechniques%20to%20extract%20semantic%20information%2C%20allowing%20for%20both%20the%20segmentation%0Aof%20object%20parts%20and%20the%20construction%20of%20functional%20joints.%20Our%20experiments%20show%0Athat%20Articulate%20Anymesh%20can%20generate%20large-scale%2C%20high-quality%203D%20articulated%0Aobjects%2C%20including%20tools%2C%20toys%2C%20mechanical%20devices%2C%20and%20vehicles%2C%20significantly%0Aexpanding%20the%20coverage%20of%20existing%203D%20articulated%20object%20datasets.%0AAdditionally%2C%20we%20show%20that%20these%20generated%20assets%20can%20facilitate%20the%0Aacquisition%20of%20new%20articulated%20object%20manipulation%20skills%20in%20simulation%2C%20which%0Acan%20then%20be%20transferred%20to%20a%20real%20robotic%20system.%20Our%20Github%20website%20is%0Ahttps%3A//articulate-anymesh.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArticulate%2520AnyMesh%253A%2520Open-Vocabulary%25203D%2520Articulated%2520Objects%2520Modeling%26entry.906535625%3DXiaowen%2520Qiu%2520and%2520Jincheng%2520Yang%2520and%2520Yian%2520Wang%2520and%2520Zhehuan%2520Chen%2520and%2520Yufei%2520Wang%2520and%2520Tsun-Hsuan%2520Wang%2520and%2520Zhou%2520Xian%2520and%2520Chuang%2520Gan%26entry.1292438233%3D%2520%25203D%2520articulated%2520objects%2520modeling%2520has%2520long%2520been%2520a%2520challenging%2520problem%252C%2520since%2520it%250Arequires%2520to%2520capture%2520both%2520accurate%2520surface%2520geometries%2520and%2520semantically%250Ameaningful%2520and%2520spatially%2520precise%2520structures%252C%2520parts%252C%2520and%2520joints.%2520Existing%250Amethods%2520heavily%2520depend%2520on%2520training%2520data%2520from%2520a%2520limited%2520set%2520of%2520handcrafted%250Aarticulated%2520object%2520categories%2520%2528e.g.%252C%2520cabinets%2520and%2520drawers%2529%252C%2520which%2520restricts%250Atheir%2520ability%2520to%2520model%2520a%2520wide%2520range%2520of%2520articulated%2520objects%2520in%2520an%250Aopen-vocabulary%2520context.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Articulate%250AAnymesh%252C%2520an%2520automated%2520framework%2520that%2520is%2520able%2520to%2520convert%2520any%2520rigid%25203D%2520mesh%2520into%250Aits%2520articulated%2520counterpart%2520in%2520an%2520open-vocabulary%2520manner.%2520Given%2520a%25203D%2520mesh%252C%2520our%250Aframework%2520utilizes%2520advanced%2520Vision-Language%2520Models%2520and%2520visual%2520prompting%250Atechniques%2520to%2520extract%2520semantic%2520information%252C%2520allowing%2520for%2520both%2520the%2520segmentation%250Aof%2520object%2520parts%2520and%2520the%2520construction%2520of%2520functional%2520joints.%2520Our%2520experiments%2520show%250Athat%2520Articulate%2520Anymesh%2520can%2520generate%2520large-scale%252C%2520high-quality%25203D%2520articulated%250Aobjects%252C%2520including%2520tools%252C%2520toys%252C%2520mechanical%2520devices%252C%2520and%2520vehicles%252C%2520significantly%250Aexpanding%2520the%2520coverage%2520of%2520existing%25203D%2520articulated%2520object%2520datasets.%250AAdditionally%252C%2520we%2520show%2520that%2520these%2520generated%2520assets%2520can%2520facilitate%2520the%250Aacquisition%2520of%2520new%2520articulated%2520object%2520manipulation%2520skills%2520in%2520simulation%252C%2520which%250Acan%2520then%2520be%2520transferred%2520to%2520a%2520real%2520robotic%2520system.%2520Our%2520Github%2520website%2520is%250Ahttps%253A//articulate-anymesh.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Articulate%20AnyMesh%3A%20Open-Vocabulary%203D%20Articulated%20Objects%20Modeling&entry.906535625=Xiaowen%20Qiu%20and%20Jincheng%20Yang%20and%20Yian%20Wang%20and%20Zhehuan%20Chen%20and%20Yufei%20Wang%20and%20Tsun-Hsuan%20Wang%20and%20Zhou%20Xian%20and%20Chuang%20Gan&entry.1292438233=%20%203D%20articulated%20objects%20modeling%20has%20long%20been%20a%20challenging%20problem%2C%20since%20it%0Arequires%20to%20capture%20both%20accurate%20surface%20geometries%20and%20semantically%0Ameaningful%20and%20spatially%20precise%20structures%2C%20parts%2C%20and%20joints.%20Existing%0Amethods%20heavily%20depend%20on%20training%20data%20from%20a%20limited%20set%20of%20handcrafted%0Aarticulated%20object%20categories%20%28e.g.%2C%20cabinets%20and%20drawers%29%2C%20which%20restricts%0Atheir%20ability%20to%20model%20a%20wide%20range%20of%20articulated%20objects%20in%20an%0Aopen-vocabulary%20context.%20To%20address%20these%20limitations%2C%20we%20propose%20Articulate%0AAnymesh%2C%20an%20automated%20framework%20that%20is%20able%20to%20convert%20any%20rigid%203D%20mesh%20into%0Aits%20articulated%20counterpart%20in%20an%20open-vocabulary%20manner.%20Given%20a%203D%20mesh%2C%20our%0Aframework%20utilizes%20advanced%20Vision-Language%20Models%20and%20visual%20prompting%0Atechniques%20to%20extract%20semantic%20information%2C%20allowing%20for%20both%20the%20segmentation%0Aof%20object%20parts%20and%20the%20construction%20of%20functional%20joints.%20Our%20experiments%20show%0Athat%20Articulate%20Anymesh%20can%20generate%20large-scale%2C%20high-quality%203D%20articulated%0Aobjects%2C%20including%20tools%2C%20toys%2C%20mechanical%20devices%2C%20and%20vehicles%2C%20significantly%0Aexpanding%20the%20coverage%20of%20existing%203D%20articulated%20object%20datasets.%0AAdditionally%2C%20we%20show%20that%20these%20generated%20assets%20can%20facilitate%20the%0Aacquisition%20of%20new%20articulated%20object%20manipulation%20skills%20in%20simulation%2C%20which%0Acan%20then%20be%20transferred%20to%20a%20real%20robotic%20system.%20Our%20Github%20website%20is%0Ahttps%3A//articulate-anymesh.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02590v1&entry.124074799=Read"},
{"title": "Monocular Per-Object Distance Estimation with Masked Object Modeling", "author": "Aniello Panariello and Gianluca Mancusi and Fedy Haj Ali and Angelo Porrello and Simone Calderara and Rita Cucchiara", "abstract": "  Per-object distance estimation is critical in surveillance and autonomous\ndriving, where safety is crucial. While existing methods rely on geometric or\ndeep supervised features, only a few attempts have been made to leverage\nself-supervised learning. In this respect, our paper draws inspiration from\nMasked Image Modeling (MiM) and extends it to multi-object tasks. While MiM\nfocuses on extracting global image-level representations, it struggles with\nindividual objects within the image. This is detrimental for distance\nestimation, as objects far away correspond to negligible portions of the image.\nConversely, our strategy, termed Masked Object Modeling (MoM), enables a novel\napplication of masking techniques. In a few words, we devise an auxiliary\nobjective that reconstructs the portions of the image pertaining to the objects\ndetected in the scene. The training phase is performed in a single unified\nstage, simultaneously optimizing the masking objective and the downstream loss\n(i.e., distance estimation).\n  We evaluate the effectiveness of MoM on a novel reference architecture\n(DistFormer) on the standard KITTI, NuScenes, and MOTSynth datasets. Our\nevaluation reveals that our framework surpasses the SoTA and highlights its\nrobust regularization properties. The MoM strategy enhances both zero-shot and\nfew-shot capabilities, from synthetic to real domain. Finally, it furthers the\nrobustness of the model in the presence of occluded or poorly detected objects.\nCode is available at https://github.com/apanariello4/DistFormer\n", "link": "http://arxiv.org/abs/2401.03191v2", "date": "2025-02-04", "relevancy": 2.2997, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5763}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Per-Object%20Distance%20Estimation%20with%20Masked%20Object%20Modeling&body=Title%3A%20Monocular%20Per-Object%20Distance%20Estimation%20with%20Masked%20Object%20Modeling%0AAuthor%3A%20Aniello%20Panariello%20and%20Gianluca%20Mancusi%20and%20Fedy%20Haj%20Ali%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Per-object%20distance%20estimation%20is%20critical%20in%20surveillance%20and%20autonomous%0Adriving%2C%20where%20safety%20is%20crucial.%20While%20existing%20methods%20rely%20on%20geometric%20or%0Adeep%20supervised%20features%2C%20only%20a%20few%20attempts%20have%20been%20made%20to%20leverage%0Aself-supervised%20learning.%20In%20this%20respect%2C%20our%20paper%20draws%20inspiration%20from%0AMasked%20Image%20Modeling%20%28MiM%29%20and%20extends%20it%20to%20multi-object%20tasks.%20While%20MiM%0Afocuses%20on%20extracting%20global%20image-level%20representations%2C%20it%20struggles%20with%0Aindividual%20objects%20within%20the%20image.%20This%20is%20detrimental%20for%20distance%0Aestimation%2C%20as%20objects%20far%20away%20correspond%20to%20negligible%20portions%20of%20the%20image.%0AConversely%2C%20our%20strategy%2C%20termed%20Masked%20Object%20Modeling%20%28MoM%29%2C%20enables%20a%20novel%0Aapplication%20of%20masking%20techniques.%20In%20a%20few%20words%2C%20we%20devise%20an%20auxiliary%0Aobjective%20that%20reconstructs%20the%20portions%20of%20the%20image%20pertaining%20to%20the%20objects%0Adetected%20in%20the%20scene.%20The%20training%20phase%20is%20performed%20in%20a%20single%20unified%0Astage%2C%20simultaneously%20optimizing%20the%20masking%20objective%20and%20the%20downstream%20loss%0A%28i.e.%2C%20distance%20estimation%29.%0A%20%20We%20evaluate%20the%20effectiveness%20of%20MoM%20on%20a%20novel%20reference%20architecture%0A%28DistFormer%29%20on%20the%20standard%20KITTI%2C%20NuScenes%2C%20and%20MOTSynth%20datasets.%20Our%0Aevaluation%20reveals%20that%20our%20framework%20surpasses%20the%20SoTA%20and%20highlights%20its%0Arobust%20regularization%20properties.%20The%20MoM%20strategy%20enhances%20both%20zero-shot%20and%0Afew-shot%20capabilities%2C%20from%20synthetic%20to%20real%20domain.%20Finally%2C%20it%20furthers%20the%0Arobustness%20of%20the%20model%20in%20the%20presence%20of%20occluded%20or%20poorly%20detected%20objects.%0ACode%20is%20available%20at%20https%3A//github.com/apanariello4/DistFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Per-Object%2520Distance%2520Estimation%2520with%2520Masked%2520Object%2520Modeling%26entry.906535625%3DAniello%2520Panariello%2520and%2520Gianluca%2520Mancusi%2520and%2520Fedy%2520Haj%2520Ali%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Per-object%2520distance%2520estimation%2520is%2520critical%2520in%2520surveillance%2520and%2520autonomous%250Adriving%252C%2520where%2520safety%2520is%2520crucial.%2520While%2520existing%2520methods%2520rely%2520on%2520geometric%2520or%250Adeep%2520supervised%2520features%252C%2520only%2520a%2520few%2520attempts%2520have%2520been%2520made%2520to%2520leverage%250Aself-supervised%2520learning.%2520In%2520this%2520respect%252C%2520our%2520paper%2520draws%2520inspiration%2520from%250AMasked%2520Image%2520Modeling%2520%2528MiM%2529%2520and%2520extends%2520it%2520to%2520multi-object%2520tasks.%2520While%2520MiM%250Afocuses%2520on%2520extracting%2520global%2520image-level%2520representations%252C%2520it%2520struggles%2520with%250Aindividual%2520objects%2520within%2520the%2520image.%2520This%2520is%2520detrimental%2520for%2520distance%250Aestimation%252C%2520as%2520objects%2520far%2520away%2520correspond%2520to%2520negligible%2520portions%2520of%2520the%2520image.%250AConversely%252C%2520our%2520strategy%252C%2520termed%2520Masked%2520Object%2520Modeling%2520%2528MoM%2529%252C%2520enables%2520a%2520novel%250Aapplication%2520of%2520masking%2520techniques.%2520In%2520a%2520few%2520words%252C%2520we%2520devise%2520an%2520auxiliary%250Aobjective%2520that%2520reconstructs%2520the%2520portions%2520of%2520the%2520image%2520pertaining%2520to%2520the%2520objects%250Adetected%2520in%2520the%2520scene.%2520The%2520training%2520phase%2520is%2520performed%2520in%2520a%2520single%2520unified%250Astage%252C%2520simultaneously%2520optimizing%2520the%2520masking%2520objective%2520and%2520the%2520downstream%2520loss%250A%2528i.e.%252C%2520distance%2520estimation%2529.%250A%2520%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520MoM%2520on%2520a%2520novel%2520reference%2520architecture%250A%2528DistFormer%2529%2520on%2520the%2520standard%2520KITTI%252C%2520NuScenes%252C%2520and%2520MOTSynth%2520datasets.%2520Our%250Aevaluation%2520reveals%2520that%2520our%2520framework%2520surpasses%2520the%2520SoTA%2520and%2520highlights%2520its%250Arobust%2520regularization%2520properties.%2520The%2520MoM%2520strategy%2520enhances%2520both%2520zero-shot%2520and%250Afew-shot%2520capabilities%252C%2520from%2520synthetic%2520to%2520real%2520domain.%2520Finally%252C%2520it%2520furthers%2520the%250Arobustness%2520of%2520the%2520model%2520in%2520the%2520presence%2520of%2520occluded%2520or%2520poorly%2520detected%2520objects.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/apanariello4/DistFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Per-Object%20Distance%20Estimation%20with%20Masked%20Object%20Modeling&entry.906535625=Aniello%20Panariello%20and%20Gianluca%20Mancusi%20and%20Fedy%20Haj%20Ali%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Per-object%20distance%20estimation%20is%20critical%20in%20surveillance%20and%20autonomous%0Adriving%2C%20where%20safety%20is%20crucial.%20While%20existing%20methods%20rely%20on%20geometric%20or%0Adeep%20supervised%20features%2C%20only%20a%20few%20attempts%20have%20been%20made%20to%20leverage%0Aself-supervised%20learning.%20In%20this%20respect%2C%20our%20paper%20draws%20inspiration%20from%0AMasked%20Image%20Modeling%20%28MiM%29%20and%20extends%20it%20to%20multi-object%20tasks.%20While%20MiM%0Afocuses%20on%20extracting%20global%20image-level%20representations%2C%20it%20struggles%20with%0Aindividual%20objects%20within%20the%20image.%20This%20is%20detrimental%20for%20distance%0Aestimation%2C%20as%20objects%20far%20away%20correspond%20to%20negligible%20portions%20of%20the%20image.%0AConversely%2C%20our%20strategy%2C%20termed%20Masked%20Object%20Modeling%20%28MoM%29%2C%20enables%20a%20novel%0Aapplication%20of%20masking%20techniques.%20In%20a%20few%20words%2C%20we%20devise%20an%20auxiliary%0Aobjective%20that%20reconstructs%20the%20portions%20of%20the%20image%20pertaining%20to%20the%20objects%0Adetected%20in%20the%20scene.%20The%20training%20phase%20is%20performed%20in%20a%20single%20unified%0Astage%2C%20simultaneously%20optimizing%20the%20masking%20objective%20and%20the%20downstream%20loss%0A%28i.e.%2C%20distance%20estimation%29.%0A%20%20We%20evaluate%20the%20effectiveness%20of%20MoM%20on%20a%20novel%20reference%20architecture%0A%28DistFormer%29%20on%20the%20standard%20KITTI%2C%20NuScenes%2C%20and%20MOTSynth%20datasets.%20Our%0Aevaluation%20reveals%20that%20our%20framework%20surpasses%20the%20SoTA%20and%20highlights%20its%0Arobust%20regularization%20properties.%20The%20MoM%20strategy%20enhances%20both%20zero-shot%20and%0Afew-shot%20capabilities%2C%20from%20synthetic%20to%20real%20domain.%20Finally%2C%20it%20furthers%20the%0Arobustness%20of%20the%20model%20in%20the%20presence%20of%20occluded%20or%20poorly%20detected%20objects.%0ACode%20is%20available%20at%20https%3A//github.com/apanariello4/DistFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03191v2&entry.124074799=Read"},
{"title": "AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late\n  Dynamic Contrast Enhanced Prostate MRI Synthesis", "author": "Divya Bharti and Sriprabha Ramanarayanan and Sadhana S and Kishore Kumar M and Keerthi Ram and Harsh Agarwal and Ramesh Venkatesan and Mohanasankar Sivaprakasam", "abstract": "  Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical\nimaging technique that plays a crucial role in the detailed visualization and\nidentification of tissue perfusion in abnormal lesions and radiological\nsuggestions for biopsy. However, DCE-MRI involves the administration of a\nGadolinium based (Gad) contrast agent, which is associated with a risk of\ntoxicity in the body. Previous deep learning approaches that synthesize DCE-MR\nimages employ unimodal non-contrast or low-dose contrast MRI images lacking\nfocus on the local perfusion information within the anatomy of interest. We\npropose AAD-DCE, a generative adversarial network (GAN) with an aggregated\nattention discriminator module consisting of global and local discriminators.\nThe discriminators provide a spatial embedded attention map to drive the\ngenerator to synthesize early and late response DCE-MRI images. Our method\nemploys multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient\n(ADC), and T1 pre-contrast for image synthesis. Extensive comparative and\nablation studies on the ProstateX dataset show that our model (i) is agnostic\nto various generator benchmarks and (ii) outperforms other DCE-MRI synthesis\napproaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE\nfor early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late\nresponse, and (ii) emphasize the importance of attention ensembling. Our code\nis available at https://github.com/bhartidivya/AAD-DCE.\n", "link": "http://arxiv.org/abs/2502.02555v1", "date": "2025-02-04", "relevancy": 2.2946, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5811}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis&body=Title%3A%20AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis%0AAuthor%3A%20Divya%20Bharti%20and%20Sriprabha%20Ramanarayanan%20and%20Sadhana%20S%20and%20Kishore%20Kumar%20M%20and%20Keerthi%20Ram%20and%20Harsh%20Agarwal%20and%20Ramesh%20Venkatesan%20and%20Mohanasankar%20Sivaprakasam%0AAbstract%3A%20%20%20Dynamic%20Contrast-Enhanced%20Magnetic%20Resonance%20Imaging%20%28DCE-MRI%29%20is%20a%20medical%0Aimaging%20technique%20that%20plays%20a%20crucial%20role%20in%20the%20detailed%20visualization%20and%0Aidentification%20of%20tissue%20perfusion%20in%20abnormal%20lesions%20and%20radiological%0Asuggestions%20for%20biopsy.%20However%2C%20DCE-MRI%20involves%20the%20administration%20of%20a%0AGadolinium%20based%20%28Gad%29%20contrast%20agent%2C%20which%20is%20associated%20with%20a%20risk%20of%0Atoxicity%20in%20the%20body.%20Previous%20deep%20learning%20approaches%20that%20synthesize%20DCE-MR%0Aimages%20employ%20unimodal%20non-contrast%20or%20low-dose%20contrast%20MRI%20images%20lacking%0Afocus%20on%20the%20local%20perfusion%20information%20within%20the%20anatomy%20of%20interest.%20We%0Apropose%20AAD-DCE%2C%20a%20generative%20adversarial%20network%20%28GAN%29%20with%20an%20aggregated%0Aattention%20discriminator%20module%20consisting%20of%20global%20and%20local%20discriminators.%0AThe%20discriminators%20provide%20a%20spatial%20embedded%20attention%20map%20to%20drive%20the%0Agenerator%20to%20synthesize%20early%20and%20late%20response%20DCE-MRI%20images.%20Our%20method%0Aemploys%20multimodal%20inputs%20-%20T2%20weighted%20%28T2W%29%2C%20Apparent%20Diffusion%20Coefficient%0A%28ADC%29%2C%20and%20T1%20pre-contrast%20for%20image%20synthesis.%20Extensive%20comparative%20and%0Aablation%20studies%20on%20the%20ProstateX%20dataset%20show%20that%20our%20model%20%28i%29%20is%20agnostic%0Ato%20various%20generator%20benchmarks%20and%20%28ii%29%20outperforms%20other%20DCE-MRI%20synthesis%0Aapproaches%20with%20improvement%20margins%20of%20%2B0.64%20dB%20PSNR%2C%20%2B0.0518%20SSIM%2C%20-0.015%20MAE%0Afor%20early%20response%20and%20%2B0.1%20dB%20PSNR%2C%20%2B0.0424%20SSIM%2C%20-0.021%20MAE%20for%20late%0Aresponse%2C%20and%20%28ii%29%20emphasize%20the%20importance%20of%20attention%20ensembling.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/bhartidivya/AAD-DCE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAD-DCE%253A%2520An%2520Aggregated%2520Multimodal%2520Attention%2520Mechanism%2520for%2520Early%2520and%2520Late%250A%2520%2520Dynamic%2520Contrast%2520Enhanced%2520Prostate%2520MRI%2520Synthesis%26entry.906535625%3DDivya%2520Bharti%2520and%2520Sriprabha%2520Ramanarayanan%2520and%2520Sadhana%2520S%2520and%2520Kishore%2520Kumar%2520M%2520and%2520Keerthi%2520Ram%2520and%2520Harsh%2520Agarwal%2520and%2520Ramesh%2520Venkatesan%2520and%2520Mohanasankar%2520Sivaprakasam%26entry.1292438233%3D%2520%2520Dynamic%2520Contrast-Enhanced%2520Magnetic%2520Resonance%2520Imaging%2520%2528DCE-MRI%2529%2520is%2520a%2520medical%250Aimaging%2520technique%2520that%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520detailed%2520visualization%2520and%250Aidentification%2520of%2520tissue%2520perfusion%2520in%2520abnormal%2520lesions%2520and%2520radiological%250Asuggestions%2520for%2520biopsy.%2520However%252C%2520DCE-MRI%2520involves%2520the%2520administration%2520of%2520a%250AGadolinium%2520based%2520%2528Gad%2529%2520contrast%2520agent%252C%2520which%2520is%2520associated%2520with%2520a%2520risk%2520of%250Atoxicity%2520in%2520the%2520body.%2520Previous%2520deep%2520learning%2520approaches%2520that%2520synthesize%2520DCE-MR%250Aimages%2520employ%2520unimodal%2520non-contrast%2520or%2520low-dose%2520contrast%2520MRI%2520images%2520lacking%250Afocus%2520on%2520the%2520local%2520perfusion%2520information%2520within%2520the%2520anatomy%2520of%2520interest.%2520We%250Apropose%2520AAD-DCE%252C%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520with%2520an%2520aggregated%250Aattention%2520discriminator%2520module%2520consisting%2520of%2520global%2520and%2520local%2520discriminators.%250AThe%2520discriminators%2520provide%2520a%2520spatial%2520embedded%2520attention%2520map%2520to%2520drive%2520the%250Agenerator%2520to%2520synthesize%2520early%2520and%2520late%2520response%2520DCE-MRI%2520images.%2520Our%2520method%250Aemploys%2520multimodal%2520inputs%2520-%2520T2%2520weighted%2520%2528T2W%2529%252C%2520Apparent%2520Diffusion%2520Coefficient%250A%2528ADC%2529%252C%2520and%2520T1%2520pre-contrast%2520for%2520image%2520synthesis.%2520Extensive%2520comparative%2520and%250Aablation%2520studies%2520on%2520the%2520ProstateX%2520dataset%2520show%2520that%2520our%2520model%2520%2528i%2529%2520is%2520agnostic%250Ato%2520various%2520generator%2520benchmarks%2520and%2520%2528ii%2529%2520outperforms%2520other%2520DCE-MRI%2520synthesis%250Aapproaches%2520with%2520improvement%2520margins%2520of%2520%252B0.64%2520dB%2520PSNR%252C%2520%252B0.0518%2520SSIM%252C%2520-0.015%2520MAE%250Afor%2520early%2520response%2520and%2520%252B0.1%2520dB%2520PSNR%252C%2520%252B0.0424%2520SSIM%252C%2520-0.021%2520MAE%2520for%2520late%250Aresponse%252C%2520and%2520%2528ii%2529%2520emphasize%2520the%2520importance%2520of%2520attention%2520ensembling.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/bhartidivya/AAD-DCE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis&entry.906535625=Divya%20Bharti%20and%20Sriprabha%20Ramanarayanan%20and%20Sadhana%20S%20and%20Kishore%20Kumar%20M%20and%20Keerthi%20Ram%20and%20Harsh%20Agarwal%20and%20Ramesh%20Venkatesan%20and%20Mohanasankar%20Sivaprakasam&entry.1292438233=%20%20Dynamic%20Contrast-Enhanced%20Magnetic%20Resonance%20Imaging%20%28DCE-MRI%29%20is%20a%20medical%0Aimaging%20technique%20that%20plays%20a%20crucial%20role%20in%20the%20detailed%20visualization%20and%0Aidentification%20of%20tissue%20perfusion%20in%20abnormal%20lesions%20and%20radiological%0Asuggestions%20for%20biopsy.%20However%2C%20DCE-MRI%20involves%20the%20administration%20of%20a%0AGadolinium%20based%20%28Gad%29%20contrast%20agent%2C%20which%20is%20associated%20with%20a%20risk%20of%0Atoxicity%20in%20the%20body.%20Previous%20deep%20learning%20approaches%20that%20synthesize%20DCE-MR%0Aimages%20employ%20unimodal%20non-contrast%20or%20low-dose%20contrast%20MRI%20images%20lacking%0Afocus%20on%20the%20local%20perfusion%20information%20within%20the%20anatomy%20of%20interest.%20We%0Apropose%20AAD-DCE%2C%20a%20generative%20adversarial%20network%20%28GAN%29%20with%20an%20aggregated%0Aattention%20discriminator%20module%20consisting%20of%20global%20and%20local%20discriminators.%0AThe%20discriminators%20provide%20a%20spatial%20embedded%20attention%20map%20to%20drive%20the%0Agenerator%20to%20synthesize%20early%20and%20late%20response%20DCE-MRI%20images.%20Our%20method%0Aemploys%20multimodal%20inputs%20-%20T2%20weighted%20%28T2W%29%2C%20Apparent%20Diffusion%20Coefficient%0A%28ADC%29%2C%20and%20T1%20pre-contrast%20for%20image%20synthesis.%20Extensive%20comparative%20and%0Aablation%20studies%20on%20the%20ProstateX%20dataset%20show%20that%20our%20model%20%28i%29%20is%20agnostic%0Ato%20various%20generator%20benchmarks%20and%20%28ii%29%20outperforms%20other%20DCE-MRI%20synthesis%0Aapproaches%20with%20improvement%20margins%20of%20%2B0.64%20dB%20PSNR%2C%20%2B0.0518%20SSIM%2C%20-0.015%20MAE%0Afor%20early%20response%20and%20%2B0.1%20dB%20PSNR%2C%20%2B0.0424%20SSIM%2C%20-0.021%20MAE%20for%20late%0Aresponse%2C%20and%20%28ii%29%20emphasize%20the%20importance%20of%20attention%20ensembling.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/bhartidivya/AAD-DCE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02555v1&entry.124074799=Read"},
{"title": "Event-aided Semantic Scene Completion", "author": "Shangwei Guo and Hao Shi and Song Wang and Xiaoting Yin and Kailun Yang and Kaiwei Wang", "abstract": "  Autonomous driving systems rely on robust 3D scene understanding. Recent\nadvances in Semantic Scene Completion (SSC) for autonomous driving underscore\nthe limitations of RGB-based approaches, which struggle under motion blur, poor\nlighting, and adverse weather. Event cameras, offering high dynamic range and\nlow latency, address these challenges by providing asynchronous data that\ncomplements RGB inputs. We present DSEC-SSC, the first real-world benchmark\nspecifically designed for event-aided SSC, which includes a novel 4D labeling\npipeline for generating dense, visibility-aware labels that adapt dynamically\nto object motion. Our proposed RGB-Event fusion framework, EvSSC, introduces an\nEvent-aided Lifting Module (ELM) that effectively bridges 2D RGB-Event features\nto 3D space, enhancing view transformation and the robustness of 3D volume\nconstruction across SSC models. Extensive experiments on DSEC-SSC and simulated\nSemanticKITTI-E demonstrate that EvSSC is adaptable to both transformer-based\nand LSS-based SSC architectures. Notably, evaluations on SemanticKITTI-C\ndemonstrate that EvSSC achieves consistently improved prediction accuracy\nacross five degradation modes and both In-domain and Out-of-domain settings,\nachieving up to a 52.5% relative improvement in mIoU when the image sensor\npartially fails. Additionally, we quantitatively and qualitatively validate the\nsuperiority of EvSSC under motion blur and extreme weather conditions, where\nautonomous driving is challenged. The established datasets and our codebase\nwill be made publicly at https://github.com/Pandapan01/EvSSC.\n", "link": "http://arxiv.org/abs/2502.02334v1", "date": "2025-02-04", "relevancy": 2.2776, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-aided%20Semantic%20Scene%20Completion&body=Title%3A%20Event-aided%20Semantic%20Scene%20Completion%0AAuthor%3A%20Shangwei%20Guo%20and%20Hao%20Shi%20and%20Song%20Wang%20and%20Xiaoting%20Yin%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20rely%20on%20robust%203D%20scene%20understanding.%20Recent%0Aadvances%20in%20Semantic%20Scene%20Completion%20%28SSC%29%20for%20autonomous%20driving%20underscore%0Athe%20limitations%20of%20RGB-based%20approaches%2C%20which%20struggle%20under%20motion%20blur%2C%20poor%0Alighting%2C%20and%20adverse%20weather.%20Event%20cameras%2C%20offering%20high%20dynamic%20range%20and%0Alow%20latency%2C%20address%20these%20challenges%20by%20providing%20asynchronous%20data%20that%0Acomplements%20RGB%20inputs.%20We%20present%20DSEC-SSC%2C%20the%20first%20real-world%20benchmark%0Aspecifically%20designed%20for%20event-aided%20SSC%2C%20which%20includes%20a%20novel%204D%20labeling%0Apipeline%20for%20generating%20dense%2C%20visibility-aware%20labels%20that%20adapt%20dynamically%0Ato%20object%20motion.%20Our%20proposed%20RGB-Event%20fusion%20framework%2C%20EvSSC%2C%20introduces%20an%0AEvent-aided%20Lifting%20Module%20%28ELM%29%20that%20effectively%20bridges%202D%20RGB-Event%20features%0Ato%203D%20space%2C%20enhancing%20view%20transformation%20and%20the%20robustness%20of%203D%20volume%0Aconstruction%20across%20SSC%20models.%20Extensive%20experiments%20on%20DSEC-SSC%20and%20simulated%0ASemanticKITTI-E%20demonstrate%20that%20EvSSC%20is%20adaptable%20to%20both%20transformer-based%0Aand%20LSS-based%20SSC%20architectures.%20Notably%2C%20evaluations%20on%20SemanticKITTI-C%0Ademonstrate%20that%20EvSSC%20achieves%20consistently%20improved%20prediction%20accuracy%0Aacross%20five%20degradation%20modes%20and%20both%20In-domain%20and%20Out-of-domain%20settings%2C%0Aachieving%20up%20to%20a%2052.5%25%20relative%20improvement%20in%20mIoU%20when%20the%20image%20sensor%0Apartially%20fails.%20Additionally%2C%20we%20quantitatively%20and%20qualitatively%20validate%20the%0Asuperiority%20of%20EvSSC%20under%20motion%20blur%20and%20extreme%20weather%20conditions%2C%20where%0Aautonomous%20driving%20is%20challenged.%20The%20established%20datasets%20and%20our%20codebase%0Awill%20be%20made%20publicly%20at%20https%3A//github.com/Pandapan01/EvSSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-aided%2520Semantic%2520Scene%2520Completion%26entry.906535625%3DShangwei%2520Guo%2520and%2520Hao%2520Shi%2520and%2520Song%2520Wang%2520and%2520Xiaoting%2520Yin%2520and%2520Kailun%2520Yang%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520rely%2520on%2520robust%25203D%2520scene%2520understanding.%2520Recent%250Aadvances%2520in%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520for%2520autonomous%2520driving%2520underscore%250Athe%2520limitations%2520of%2520RGB-based%2520approaches%252C%2520which%2520struggle%2520under%2520motion%2520blur%252C%2520poor%250Alighting%252C%2520and%2520adverse%2520weather.%2520Event%2520cameras%252C%2520offering%2520high%2520dynamic%2520range%2520and%250Alow%2520latency%252C%2520address%2520these%2520challenges%2520by%2520providing%2520asynchronous%2520data%2520that%250Acomplements%2520RGB%2520inputs.%2520We%2520present%2520DSEC-SSC%252C%2520the%2520first%2520real-world%2520benchmark%250Aspecifically%2520designed%2520for%2520event-aided%2520SSC%252C%2520which%2520includes%2520a%2520novel%25204D%2520labeling%250Apipeline%2520for%2520generating%2520dense%252C%2520visibility-aware%2520labels%2520that%2520adapt%2520dynamically%250Ato%2520object%2520motion.%2520Our%2520proposed%2520RGB-Event%2520fusion%2520framework%252C%2520EvSSC%252C%2520introduces%2520an%250AEvent-aided%2520Lifting%2520Module%2520%2528ELM%2529%2520that%2520effectively%2520bridges%25202D%2520RGB-Event%2520features%250Ato%25203D%2520space%252C%2520enhancing%2520view%2520transformation%2520and%2520the%2520robustness%2520of%25203D%2520volume%250Aconstruction%2520across%2520SSC%2520models.%2520Extensive%2520experiments%2520on%2520DSEC-SSC%2520and%2520simulated%250ASemanticKITTI-E%2520demonstrate%2520that%2520EvSSC%2520is%2520adaptable%2520to%2520both%2520transformer-based%250Aand%2520LSS-based%2520SSC%2520architectures.%2520Notably%252C%2520evaluations%2520on%2520SemanticKITTI-C%250Ademonstrate%2520that%2520EvSSC%2520achieves%2520consistently%2520improved%2520prediction%2520accuracy%250Aacross%2520five%2520degradation%2520modes%2520and%2520both%2520In-domain%2520and%2520Out-of-domain%2520settings%252C%250Aachieving%2520up%2520to%2520a%252052.5%2525%2520relative%2520improvement%2520in%2520mIoU%2520when%2520the%2520image%2520sensor%250Apartially%2520fails.%2520Additionally%252C%2520we%2520quantitatively%2520and%2520qualitatively%2520validate%2520the%250Asuperiority%2520of%2520EvSSC%2520under%2520motion%2520blur%2520and%2520extreme%2520weather%2520conditions%252C%2520where%250Aautonomous%2520driving%2520is%2520challenged.%2520The%2520established%2520datasets%2520and%2520our%2520codebase%250Awill%2520be%2520made%2520publicly%2520at%2520https%253A//github.com/Pandapan01/EvSSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-aided%20Semantic%20Scene%20Completion&entry.906535625=Shangwei%20Guo%20and%20Hao%20Shi%20and%20Song%20Wang%20and%20Xiaoting%20Yin%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang&entry.1292438233=%20%20Autonomous%20driving%20systems%20rely%20on%20robust%203D%20scene%20understanding.%20Recent%0Aadvances%20in%20Semantic%20Scene%20Completion%20%28SSC%29%20for%20autonomous%20driving%20underscore%0Athe%20limitations%20of%20RGB-based%20approaches%2C%20which%20struggle%20under%20motion%20blur%2C%20poor%0Alighting%2C%20and%20adverse%20weather.%20Event%20cameras%2C%20offering%20high%20dynamic%20range%20and%0Alow%20latency%2C%20address%20these%20challenges%20by%20providing%20asynchronous%20data%20that%0Acomplements%20RGB%20inputs.%20We%20present%20DSEC-SSC%2C%20the%20first%20real-world%20benchmark%0Aspecifically%20designed%20for%20event-aided%20SSC%2C%20which%20includes%20a%20novel%204D%20labeling%0Apipeline%20for%20generating%20dense%2C%20visibility-aware%20labels%20that%20adapt%20dynamically%0Ato%20object%20motion.%20Our%20proposed%20RGB-Event%20fusion%20framework%2C%20EvSSC%2C%20introduces%20an%0AEvent-aided%20Lifting%20Module%20%28ELM%29%20that%20effectively%20bridges%202D%20RGB-Event%20features%0Ato%203D%20space%2C%20enhancing%20view%20transformation%20and%20the%20robustness%20of%203D%20volume%0Aconstruction%20across%20SSC%20models.%20Extensive%20experiments%20on%20DSEC-SSC%20and%20simulated%0ASemanticKITTI-E%20demonstrate%20that%20EvSSC%20is%20adaptable%20to%20both%20transformer-based%0Aand%20LSS-based%20SSC%20architectures.%20Notably%2C%20evaluations%20on%20SemanticKITTI-C%0Ademonstrate%20that%20EvSSC%20achieves%20consistently%20improved%20prediction%20accuracy%0Aacross%20five%20degradation%20modes%20and%20both%20In-domain%20and%20Out-of-domain%20settings%2C%0Aachieving%20up%20to%20a%2052.5%25%20relative%20improvement%20in%20mIoU%20when%20the%20image%20sensor%0Apartially%20fails.%20Additionally%2C%20we%20quantitatively%20and%20qualitatively%20validate%20the%0Asuperiority%20of%20EvSSC%20under%20motion%20blur%20and%20extreme%20weather%20conditions%2C%20where%0Aautonomous%20driving%20is%20challenged.%20The%20established%20datasets%20and%20our%20codebase%0Awill%20be%20made%20publicly%20at%20https%3A//github.com/Pandapan01/EvSSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02334v1&entry.124074799=Read"},
{"title": "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING", "author": "Connor Schenck and Isaac Reid and Mithun George Jacob and Alex Bewley and Joshua Ainslie and David Rendleman and Deepali Jain and Mohit Sharma and Avinava Dubey and Ayzaan Wahid and Sumeet Singh and Rene Wagner and Tianli Ding and Chuyuan Fu and Arunkumar Byravan and Jake Varley and Alexey Gritsenko and Matthias Minderer and Dmitry Kalashnikov and Jonathan Tompson and Vikas Sindhwani and Krzysztof Choromanski", "abstract": "  We introduce STRING: Separable Translationally Invariant Position Encodings.\nSTRING extends Rotary Position Encodings, a recently proposed and widely used\nalgorithm in large language models, via a unifying theoretical framework.\nImportantly, STRING still provides exact translation invariance, including\ntoken coordinates of arbitrary dimensionality, whilst maintaining a low\ncomputational footprint. These properties are especially important in robotics,\nwhere efficient 3D token representation is key. We integrate STRING into Vision\nTransformers with RGB(-D) inputs (color plus optional depth), showing\nsubstantial gains, e.g. in open-vocabulary object detection and for robotics\ncontrollers. We complement our experiments with a rigorous mathematical\nanalysis, proving the universality of our methods.\n", "link": "http://arxiv.org/abs/2502.02562v1", "date": "2025-02-04", "relevancy": 2.2688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20RoPEs%3A%20Better%202D%20and%203D%20Position%20Encodings%20with%20STRING&body=Title%3A%20Learning%20the%20RoPEs%3A%20Better%202D%20and%203D%20Position%20Encodings%20with%20STRING%0AAuthor%3A%20Connor%20Schenck%20and%20Isaac%20Reid%20and%20Mithun%20George%20Jacob%20and%20Alex%20Bewley%20and%20Joshua%20Ainslie%20and%20David%20Rendleman%20and%20Deepali%20Jain%20and%20Mohit%20Sharma%20and%20Avinava%20Dubey%20and%20Ayzaan%20Wahid%20and%20Sumeet%20Singh%20and%20Rene%20Wagner%20and%20Tianli%20Ding%20and%20Chuyuan%20Fu%20and%20Arunkumar%20Byravan%20and%20Jake%20Varley%20and%20Alexey%20Gritsenko%20and%20Matthias%20Minderer%20and%20Dmitry%20Kalashnikov%20and%20Jonathan%20Tompson%20and%20Vikas%20Sindhwani%20and%20Krzysztof%20Choromanski%0AAbstract%3A%20%20%20We%20introduce%20STRING%3A%20Separable%20Translationally%20Invariant%20Position%20Encodings.%0ASTRING%20extends%20Rotary%20Position%20Encodings%2C%20a%20recently%20proposed%20and%20widely%20used%0Aalgorithm%20in%20large%20language%20models%2C%20via%20a%20unifying%20theoretical%20framework.%0AImportantly%2C%20STRING%20still%20provides%20exact%20translation%20invariance%2C%20including%0Atoken%20coordinates%20of%20arbitrary%20dimensionality%2C%20whilst%20maintaining%20a%20low%0Acomputational%20footprint.%20These%20properties%20are%20especially%20important%20in%20robotics%2C%0Awhere%20efficient%203D%20token%20representation%20is%20key.%20We%20integrate%20STRING%20into%20Vision%0ATransformers%20with%20RGB%28-D%29%20inputs%20%28color%20plus%20optional%20depth%29%2C%20showing%0Asubstantial%20gains%2C%20e.g.%20in%20open-vocabulary%20object%20detection%20and%20for%20robotics%0Acontrollers.%20We%20complement%20our%20experiments%20with%20a%20rigorous%20mathematical%0Aanalysis%2C%20proving%20the%20universality%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520RoPEs%253A%2520Better%25202D%2520and%25203D%2520Position%2520Encodings%2520with%2520STRING%26entry.906535625%3DConnor%2520Schenck%2520and%2520Isaac%2520Reid%2520and%2520Mithun%2520George%2520Jacob%2520and%2520Alex%2520Bewley%2520and%2520Joshua%2520Ainslie%2520and%2520David%2520Rendleman%2520and%2520Deepali%2520Jain%2520and%2520Mohit%2520Sharma%2520and%2520Avinava%2520Dubey%2520and%2520Ayzaan%2520Wahid%2520and%2520Sumeet%2520Singh%2520and%2520Rene%2520Wagner%2520and%2520Tianli%2520Ding%2520and%2520Chuyuan%2520Fu%2520and%2520Arunkumar%2520Byravan%2520and%2520Jake%2520Varley%2520and%2520Alexey%2520Gritsenko%2520and%2520Matthias%2520Minderer%2520and%2520Dmitry%2520Kalashnikov%2520and%2520Jonathan%2520Tompson%2520and%2520Vikas%2520Sindhwani%2520and%2520Krzysztof%2520Choromanski%26entry.1292438233%3D%2520%2520We%2520introduce%2520STRING%253A%2520Separable%2520Translationally%2520Invariant%2520Position%2520Encodings.%250ASTRING%2520extends%2520Rotary%2520Position%2520Encodings%252C%2520a%2520recently%2520proposed%2520and%2520widely%2520used%250Aalgorithm%2520in%2520large%2520language%2520models%252C%2520via%2520a%2520unifying%2520theoretical%2520framework.%250AImportantly%252C%2520STRING%2520still%2520provides%2520exact%2520translation%2520invariance%252C%2520including%250Atoken%2520coordinates%2520of%2520arbitrary%2520dimensionality%252C%2520whilst%2520maintaining%2520a%2520low%250Acomputational%2520footprint.%2520These%2520properties%2520are%2520especially%2520important%2520in%2520robotics%252C%250Awhere%2520efficient%25203D%2520token%2520representation%2520is%2520key.%2520We%2520integrate%2520STRING%2520into%2520Vision%250ATransformers%2520with%2520RGB%2528-D%2529%2520inputs%2520%2528color%2520plus%2520optional%2520depth%2529%252C%2520showing%250Asubstantial%2520gains%252C%2520e.g.%2520in%2520open-vocabulary%2520object%2520detection%2520and%2520for%2520robotics%250Acontrollers.%2520We%2520complement%2520our%2520experiments%2520with%2520a%2520rigorous%2520mathematical%250Aanalysis%252C%2520proving%2520the%2520universality%2520of%2520our%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20RoPEs%3A%20Better%202D%20and%203D%20Position%20Encodings%20with%20STRING&entry.906535625=Connor%20Schenck%20and%20Isaac%20Reid%20and%20Mithun%20George%20Jacob%20and%20Alex%20Bewley%20and%20Joshua%20Ainslie%20and%20David%20Rendleman%20and%20Deepali%20Jain%20and%20Mohit%20Sharma%20and%20Avinava%20Dubey%20and%20Ayzaan%20Wahid%20and%20Sumeet%20Singh%20and%20Rene%20Wagner%20and%20Tianli%20Ding%20and%20Chuyuan%20Fu%20and%20Arunkumar%20Byravan%20and%20Jake%20Varley%20and%20Alexey%20Gritsenko%20and%20Matthias%20Minderer%20and%20Dmitry%20Kalashnikov%20and%20Jonathan%20Tompson%20and%20Vikas%20Sindhwani%20and%20Krzysztof%20Choromanski&entry.1292438233=%20%20We%20introduce%20STRING%3A%20Separable%20Translationally%20Invariant%20Position%20Encodings.%0ASTRING%20extends%20Rotary%20Position%20Encodings%2C%20a%20recently%20proposed%20and%20widely%20used%0Aalgorithm%20in%20large%20language%20models%2C%20via%20a%20unifying%20theoretical%20framework.%0AImportantly%2C%20STRING%20still%20provides%20exact%20translation%20invariance%2C%20including%0Atoken%20coordinates%20of%20arbitrary%20dimensionality%2C%20whilst%20maintaining%20a%20low%0Acomputational%20footprint.%20These%20properties%20are%20especially%20important%20in%20robotics%2C%0Awhere%20efficient%203D%20token%20representation%20is%20key.%20We%20integrate%20STRING%20into%20Vision%0ATransformers%20with%20RGB%28-D%29%20inputs%20%28color%20plus%20optional%20depth%29%2C%20showing%0Asubstantial%20gains%2C%20e.g.%20in%20open-vocabulary%20object%20detection%20and%20for%20robotics%0Acontrollers.%20We%20complement%20our%20experiments%20with%20a%20rigorous%20mathematical%0Aanalysis%2C%20proving%20the%20universality%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02562v1&entry.124074799=Read"},
{"title": "Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse\n  Task Perspectives", "author": "Simone Alberto Peirone and Francesca Pistilli and Antonio Alliegro and Tatiana Tommasi and Giuseppe Averta", "abstract": "  Our comprehension of video streams depicting human activities is naturally\nmultifaceted: in just a few moments, we can grasp what is happening, identify\nthe relevance and interactions of objects in the scene, and forecast what will\nhappen soon, everything all at once. To endow autonomous systems with such a\nholistic perception, learning how to correlate concepts, abstract knowledge\nacross diverse tasks, and leverage tasks synergies when learning novel skills\nis essential. A significant step in this direction is EgoPack, a unified\nframework for understanding human activities across diverse tasks with minimal\noverhead. EgoPack promotes information sharing and collaboration among\ndownstream tasks, essential for efficiently learning new skills. In this paper,\nwe introduce Hier-EgoPack, which advances EgoPack by enabling reasoning also\nacross diverse temporal granularities, which expands its applicability to a\nbroader range of downstream tasks. To achieve this, we propose a novel\nhierarchical architecture for temporal reasoning equipped with a GNN layer\nspecifically designed to tackle the challenges of multi-granularity reasoning\neffectively. We evaluate our approach on multiple Ego4d benchmarks involving\nboth clip-level and frame-level reasoning, demonstrating how our hierarchical\nunified architecture effectively solves these diverse tasks simultaneously.\n", "link": "http://arxiv.org/abs/2502.02487v1", "date": "2025-02-04", "relevancy": 2.266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hier-EgoPack%3A%20Hierarchical%20Egocentric%20Video%20Understanding%20with%20Diverse%0A%20%20Task%20Perspectives&body=Title%3A%20Hier-EgoPack%3A%20Hierarchical%20Egocentric%20Video%20Understanding%20with%20Diverse%0A%20%20Task%20Perspectives%0AAuthor%3A%20Simone%20Alberto%20Peirone%20and%20Francesca%20Pistilli%20and%20Antonio%20Alliegro%20and%20Tatiana%20Tommasi%20and%20Giuseppe%20Averta%0AAbstract%3A%20%20%20Our%20comprehension%20of%20video%20streams%20depicting%20human%20activities%20is%20naturally%0Amultifaceted%3A%20in%20just%20a%20few%20moments%2C%20we%20can%20grasp%20what%20is%20happening%2C%20identify%0Athe%20relevance%20and%20interactions%20of%20objects%20in%20the%20scene%2C%20and%20forecast%20what%20will%0Ahappen%20soon%2C%20everything%20all%20at%20once.%20To%20endow%20autonomous%20systems%20with%20such%20a%0Aholistic%20perception%2C%20learning%20how%20to%20correlate%20concepts%2C%20abstract%20knowledge%0Aacross%20diverse%20tasks%2C%20and%20leverage%20tasks%20synergies%20when%20learning%20novel%20skills%0Ais%20essential.%20A%20significant%20step%20in%20this%20direction%20is%20EgoPack%2C%20a%20unified%0Aframework%20for%20understanding%20human%20activities%20across%20diverse%20tasks%20with%20minimal%0Aoverhead.%20EgoPack%20promotes%20information%20sharing%20and%20collaboration%20among%0Adownstream%20tasks%2C%20essential%20for%20efficiently%20learning%20new%20skills.%20In%20this%20paper%2C%0Awe%20introduce%20Hier-EgoPack%2C%20which%20advances%20EgoPack%20by%20enabling%20reasoning%20also%0Aacross%20diverse%20temporal%20granularities%2C%20which%20expands%20its%20applicability%20to%20a%0Abroader%20range%20of%20downstream%20tasks.%20To%20achieve%20this%2C%20we%20propose%20a%20novel%0Ahierarchical%20architecture%20for%20temporal%20reasoning%20equipped%20with%20a%20GNN%20layer%0Aspecifically%20designed%20to%20tackle%20the%20challenges%20of%20multi-granularity%20reasoning%0Aeffectively.%20We%20evaluate%20our%20approach%20on%20multiple%20Ego4d%20benchmarks%20involving%0Aboth%20clip-level%20and%20frame-level%20reasoning%2C%20demonstrating%20how%20our%20hierarchical%0Aunified%20architecture%20effectively%20solves%20these%20diverse%20tasks%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHier-EgoPack%253A%2520Hierarchical%2520Egocentric%2520Video%2520Understanding%2520with%2520Diverse%250A%2520%2520Task%2520Perspectives%26entry.906535625%3DSimone%2520Alberto%2520Peirone%2520and%2520Francesca%2520Pistilli%2520and%2520Antonio%2520Alliegro%2520and%2520Tatiana%2520Tommasi%2520and%2520Giuseppe%2520Averta%26entry.1292438233%3D%2520%2520Our%2520comprehension%2520of%2520video%2520streams%2520depicting%2520human%2520activities%2520is%2520naturally%250Amultifaceted%253A%2520in%2520just%2520a%2520few%2520moments%252C%2520we%2520can%2520grasp%2520what%2520is%2520happening%252C%2520identify%250Athe%2520relevance%2520and%2520interactions%2520of%2520objects%2520in%2520the%2520scene%252C%2520and%2520forecast%2520what%2520will%250Ahappen%2520soon%252C%2520everything%2520all%2520at%2520once.%2520To%2520endow%2520autonomous%2520systems%2520with%2520such%2520a%250Aholistic%2520perception%252C%2520learning%2520how%2520to%2520correlate%2520concepts%252C%2520abstract%2520knowledge%250Aacross%2520diverse%2520tasks%252C%2520and%2520leverage%2520tasks%2520synergies%2520when%2520learning%2520novel%2520skills%250Ais%2520essential.%2520A%2520significant%2520step%2520in%2520this%2520direction%2520is%2520EgoPack%252C%2520a%2520unified%250Aframework%2520for%2520understanding%2520human%2520activities%2520across%2520diverse%2520tasks%2520with%2520minimal%250Aoverhead.%2520EgoPack%2520promotes%2520information%2520sharing%2520and%2520collaboration%2520among%250Adownstream%2520tasks%252C%2520essential%2520for%2520efficiently%2520learning%2520new%2520skills.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520Hier-EgoPack%252C%2520which%2520advances%2520EgoPack%2520by%2520enabling%2520reasoning%2520also%250Aacross%2520diverse%2520temporal%2520granularities%252C%2520which%2520expands%2520its%2520applicability%2520to%2520a%250Abroader%2520range%2520of%2520downstream%2520tasks.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520novel%250Ahierarchical%2520architecture%2520for%2520temporal%2520reasoning%2520equipped%2520with%2520a%2520GNN%2520layer%250Aspecifically%2520designed%2520to%2520tackle%2520the%2520challenges%2520of%2520multi-granularity%2520reasoning%250Aeffectively.%2520We%2520evaluate%2520our%2520approach%2520on%2520multiple%2520Ego4d%2520benchmarks%2520involving%250Aboth%2520clip-level%2520and%2520frame-level%2520reasoning%252C%2520demonstrating%2520how%2520our%2520hierarchical%250Aunified%2520architecture%2520effectively%2520solves%2520these%2520diverse%2520tasks%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hier-EgoPack%3A%20Hierarchical%20Egocentric%20Video%20Understanding%20with%20Diverse%0A%20%20Task%20Perspectives&entry.906535625=Simone%20Alberto%20Peirone%20and%20Francesca%20Pistilli%20and%20Antonio%20Alliegro%20and%20Tatiana%20Tommasi%20and%20Giuseppe%20Averta&entry.1292438233=%20%20Our%20comprehension%20of%20video%20streams%20depicting%20human%20activities%20is%20naturally%0Amultifaceted%3A%20in%20just%20a%20few%20moments%2C%20we%20can%20grasp%20what%20is%20happening%2C%20identify%0Athe%20relevance%20and%20interactions%20of%20objects%20in%20the%20scene%2C%20and%20forecast%20what%20will%0Ahappen%20soon%2C%20everything%20all%20at%20once.%20To%20endow%20autonomous%20systems%20with%20such%20a%0Aholistic%20perception%2C%20learning%20how%20to%20correlate%20concepts%2C%20abstract%20knowledge%0Aacross%20diverse%20tasks%2C%20and%20leverage%20tasks%20synergies%20when%20learning%20novel%20skills%0Ais%20essential.%20A%20significant%20step%20in%20this%20direction%20is%20EgoPack%2C%20a%20unified%0Aframework%20for%20understanding%20human%20activities%20across%20diverse%20tasks%20with%20minimal%0Aoverhead.%20EgoPack%20promotes%20information%20sharing%20and%20collaboration%20among%0Adownstream%20tasks%2C%20essential%20for%20efficiently%20learning%20new%20skills.%20In%20this%20paper%2C%0Awe%20introduce%20Hier-EgoPack%2C%20which%20advances%20EgoPack%20by%20enabling%20reasoning%20also%0Aacross%20diverse%20temporal%20granularities%2C%20which%20expands%20its%20applicability%20to%20a%0Abroader%20range%20of%20downstream%20tasks.%20To%20achieve%20this%2C%20we%20propose%20a%20novel%0Ahierarchical%20architecture%20for%20temporal%20reasoning%20equipped%20with%20a%20GNN%20layer%0Aspecifically%20designed%20to%20tackle%20the%20challenges%20of%20multi-granularity%20reasoning%0Aeffectively.%20We%20evaluate%20our%20approach%20on%20multiple%20Ego4d%20benchmarks%20involving%0Aboth%20clip-level%20and%20frame-level%20reasoning%2C%20demonstrating%20how%20our%20hierarchical%0Aunified%20architecture%20effectively%20solves%20these%20diverse%20tasks%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02487v1&entry.124074799=Read"},
{"title": "UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic\n  Segmentation", "author": "Tao Zhang and Jinyong Wen and Zhen Chen and Kun Ding and Shiming Xiang and Chunhong Pan", "abstract": "  Pre-training techniques significantly enhance the performance of semantic\nsegmentation tasks with limited training data. However, the efficacy under a\nlarge domain gap between pre-training (e.g. RGB) and fine-tuning (e.g.\ninfrared) remains underexplored. In this study, we first benchmark the infrared\nsemantic segmentation performance of various pre-training methods and reveal\nseveral phenomena distinct from the RGB domain. Next, our layerwise analysis of\npre-trained attention maps uncovers that: (1) There are three typical attention\npatterns (local, hybrid, and global); (2) Pre-training tasks notably influence\nthe pattern distribution across layers; (3) The hybrid pattern is crucial for\nsemantic segmentation as it attends to both nearby and foreground elements; (4)\nThe texture bias impedes model generalization in infrared tasks. Building on\nthese insights, we propose UNIP, a UNified Infrared Pre-training framework, to\nenhance the pre-trained model performance. This framework uses the\nhybrid-attention distillation NMI-HAD as the pre-training target, a large-scale\nmixed dataset InfMix for pre-training, and a last-layer feature pyramid network\nLL-FPN for fine-tuning. Experimental results show that UNIP outperforms various\npre-training methods by up to 13.5\\% in average mIoU on three infrared\nsegmentation tasks, evaluated using fine-tuning and linear probing metrics.\nUNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the\ncomputational cost. Furthermore, UNIP significantly surpasses state-of-the-art\n(SOTA) infrared or RGB segmentation methods and demonstrates broad potential\nfor application in other modalities, such as RGB and depth. Our code is\navailable at https://github.com/casiatao/UNIP.\n", "link": "http://arxiv.org/abs/2502.02257v1", "date": "2025-02-04", "relevancy": 2.2655, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5809}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNIP%3A%20Rethinking%20Pre-trained%20Attention%20Patterns%20for%20Infrared%20Semantic%0A%20%20Segmentation&body=Title%3A%20UNIP%3A%20Rethinking%20Pre-trained%20Attention%20Patterns%20for%20Infrared%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Tao%20Zhang%20and%20Jinyong%20Wen%20and%20Zhen%20Chen%20and%20Kun%20Ding%20and%20Shiming%20Xiang%20and%20Chunhong%20Pan%0AAbstract%3A%20%20%20Pre-training%20techniques%20significantly%20enhance%20the%20performance%20of%20semantic%0Asegmentation%20tasks%20with%20limited%20training%20data.%20However%2C%20the%20efficacy%20under%20a%0Alarge%20domain%20gap%20between%20pre-training%20%28e.g.%20RGB%29%20and%20fine-tuning%20%28e.g.%0Ainfrared%29%20remains%20underexplored.%20In%20this%20study%2C%20we%20first%20benchmark%20the%20infrared%0Asemantic%20segmentation%20performance%20of%20various%20pre-training%20methods%20and%20reveal%0Aseveral%20phenomena%20distinct%20from%20the%20RGB%20domain.%20Next%2C%20our%20layerwise%20analysis%20of%0Apre-trained%20attention%20maps%20uncovers%20that%3A%20%281%29%20There%20are%20three%20typical%20attention%0Apatterns%20%28local%2C%20hybrid%2C%20and%20global%29%3B%20%282%29%20Pre-training%20tasks%20notably%20influence%0Athe%20pattern%20distribution%20across%20layers%3B%20%283%29%20The%20hybrid%20pattern%20is%20crucial%20for%0Asemantic%20segmentation%20as%20it%20attends%20to%20both%20nearby%20and%20foreground%20elements%3B%20%284%29%0AThe%20texture%20bias%20impedes%20model%20generalization%20in%20infrared%20tasks.%20Building%20on%0Athese%20insights%2C%20we%20propose%20UNIP%2C%20a%20UNified%20Infrared%20Pre-training%20framework%2C%20to%0Aenhance%20the%20pre-trained%20model%20performance.%20This%20framework%20uses%20the%0Ahybrid-attention%20distillation%20NMI-HAD%20as%20the%20pre-training%20target%2C%20a%20large-scale%0Amixed%20dataset%20InfMix%20for%20pre-training%2C%20and%20a%20last-layer%20feature%20pyramid%20network%0ALL-FPN%20for%20fine-tuning.%20Experimental%20results%20show%20that%20UNIP%20outperforms%20various%0Apre-training%20methods%20by%20up%20to%2013.5%5C%25%20in%20average%20mIoU%20on%20three%20infrared%0Asegmentation%20tasks%2C%20evaluated%20using%20fine-tuning%20and%20linear%20probing%20metrics.%0AUNIP-S%20achieves%20performance%20on%20par%20with%20MAE-L%20while%20requiring%20only%201/10%20of%20the%0Acomputational%20cost.%20Furthermore%2C%20UNIP%20significantly%20surpasses%20state-of-the-art%0A%28SOTA%29%20infrared%20or%20RGB%20segmentation%20methods%20and%20demonstrates%20broad%20potential%0Afor%20application%20in%20other%20modalities%2C%20such%20as%20RGB%20and%20depth.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/casiatao/UNIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNIP%253A%2520Rethinking%2520Pre-trained%2520Attention%2520Patterns%2520for%2520Infrared%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DTao%2520Zhang%2520and%2520Jinyong%2520Wen%2520and%2520Zhen%2520Chen%2520and%2520Kun%2520Ding%2520and%2520Shiming%2520Xiang%2520and%2520Chunhong%2520Pan%26entry.1292438233%3D%2520%2520Pre-training%2520techniques%2520significantly%2520enhance%2520the%2520performance%2520of%2520semantic%250Asegmentation%2520tasks%2520with%2520limited%2520training%2520data.%2520However%252C%2520the%2520efficacy%2520under%2520a%250Alarge%2520domain%2520gap%2520between%2520pre-training%2520%2528e.g.%2520RGB%2529%2520and%2520fine-tuning%2520%2528e.g.%250Ainfrared%2529%2520remains%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520first%2520benchmark%2520the%2520infrared%250Asemantic%2520segmentation%2520performance%2520of%2520various%2520pre-training%2520methods%2520and%2520reveal%250Aseveral%2520phenomena%2520distinct%2520from%2520the%2520RGB%2520domain.%2520Next%252C%2520our%2520layerwise%2520analysis%2520of%250Apre-trained%2520attention%2520maps%2520uncovers%2520that%253A%2520%25281%2529%2520There%2520are%2520three%2520typical%2520attention%250Apatterns%2520%2528local%252C%2520hybrid%252C%2520and%2520global%2529%253B%2520%25282%2529%2520Pre-training%2520tasks%2520notably%2520influence%250Athe%2520pattern%2520distribution%2520across%2520layers%253B%2520%25283%2529%2520The%2520hybrid%2520pattern%2520is%2520crucial%2520for%250Asemantic%2520segmentation%2520as%2520it%2520attends%2520to%2520both%2520nearby%2520and%2520foreground%2520elements%253B%2520%25284%2529%250AThe%2520texture%2520bias%2520impedes%2520model%2520generalization%2520in%2520infrared%2520tasks.%2520Building%2520on%250Athese%2520insights%252C%2520we%2520propose%2520UNIP%252C%2520a%2520UNified%2520Infrared%2520Pre-training%2520framework%252C%2520to%250Aenhance%2520the%2520pre-trained%2520model%2520performance.%2520This%2520framework%2520uses%2520the%250Ahybrid-attention%2520distillation%2520NMI-HAD%2520as%2520the%2520pre-training%2520target%252C%2520a%2520large-scale%250Amixed%2520dataset%2520InfMix%2520for%2520pre-training%252C%2520and%2520a%2520last-layer%2520feature%2520pyramid%2520network%250ALL-FPN%2520for%2520fine-tuning.%2520Experimental%2520results%2520show%2520that%2520UNIP%2520outperforms%2520various%250Apre-training%2520methods%2520by%2520up%2520to%252013.5%255C%2525%2520in%2520average%2520mIoU%2520on%2520three%2520infrared%250Asegmentation%2520tasks%252C%2520evaluated%2520using%2520fine-tuning%2520and%2520linear%2520probing%2520metrics.%250AUNIP-S%2520achieves%2520performance%2520on%2520par%2520with%2520MAE-L%2520while%2520requiring%2520only%25201/10%2520of%2520the%250Acomputational%2520cost.%2520Furthermore%252C%2520UNIP%2520significantly%2520surpasses%2520state-of-the-art%250A%2528SOTA%2529%2520infrared%2520or%2520RGB%2520segmentation%2520methods%2520and%2520demonstrates%2520broad%2520potential%250Afor%2520application%2520in%2520other%2520modalities%252C%2520such%2520as%2520RGB%2520and%2520depth.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/casiatao/UNIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNIP%3A%20Rethinking%20Pre-trained%20Attention%20Patterns%20for%20Infrared%20Semantic%0A%20%20Segmentation&entry.906535625=Tao%20Zhang%20and%20Jinyong%20Wen%20and%20Zhen%20Chen%20and%20Kun%20Ding%20and%20Shiming%20Xiang%20and%20Chunhong%20Pan&entry.1292438233=%20%20Pre-training%20techniques%20significantly%20enhance%20the%20performance%20of%20semantic%0Asegmentation%20tasks%20with%20limited%20training%20data.%20However%2C%20the%20efficacy%20under%20a%0Alarge%20domain%20gap%20between%20pre-training%20%28e.g.%20RGB%29%20and%20fine-tuning%20%28e.g.%0Ainfrared%29%20remains%20underexplored.%20In%20this%20study%2C%20we%20first%20benchmark%20the%20infrared%0Asemantic%20segmentation%20performance%20of%20various%20pre-training%20methods%20and%20reveal%0Aseveral%20phenomena%20distinct%20from%20the%20RGB%20domain.%20Next%2C%20our%20layerwise%20analysis%20of%0Apre-trained%20attention%20maps%20uncovers%20that%3A%20%281%29%20There%20are%20three%20typical%20attention%0Apatterns%20%28local%2C%20hybrid%2C%20and%20global%29%3B%20%282%29%20Pre-training%20tasks%20notably%20influence%0Athe%20pattern%20distribution%20across%20layers%3B%20%283%29%20The%20hybrid%20pattern%20is%20crucial%20for%0Asemantic%20segmentation%20as%20it%20attends%20to%20both%20nearby%20and%20foreground%20elements%3B%20%284%29%0AThe%20texture%20bias%20impedes%20model%20generalization%20in%20infrared%20tasks.%20Building%20on%0Athese%20insights%2C%20we%20propose%20UNIP%2C%20a%20UNified%20Infrared%20Pre-training%20framework%2C%20to%0Aenhance%20the%20pre-trained%20model%20performance.%20This%20framework%20uses%20the%0Ahybrid-attention%20distillation%20NMI-HAD%20as%20the%20pre-training%20target%2C%20a%20large-scale%0Amixed%20dataset%20InfMix%20for%20pre-training%2C%20and%20a%20last-layer%20feature%20pyramid%20network%0ALL-FPN%20for%20fine-tuning.%20Experimental%20results%20show%20that%20UNIP%20outperforms%20various%0Apre-training%20methods%20by%20up%20to%2013.5%5C%25%20in%20average%20mIoU%20on%20three%20infrared%0Asegmentation%20tasks%2C%20evaluated%20using%20fine-tuning%20and%20linear%20probing%20metrics.%0AUNIP-S%20achieves%20performance%20on%20par%20with%20MAE-L%20while%20requiring%20only%201/10%20of%20the%0Acomputational%20cost.%20Furthermore%2C%20UNIP%20significantly%20surpasses%20state-of-the-art%0A%28SOTA%29%20infrared%20or%20RGB%20segmentation%20methods%20and%20demonstrates%20broad%20potential%0Afor%20application%20in%20other%20modalities%2C%20such%20as%20RGB%20and%20depth.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/casiatao/UNIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02257v1&entry.124074799=Read"},
{"title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs", "author": "Sergey Berezin and Reza Farahbakhsh and Noel Crespi", "abstract": "  We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.\n", "link": "http://arxiv.org/abs/2501.18626v3", "date": "2025-02-04", "relevancy": 2.2583, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4696}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-in-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs&body=Title%3A%20The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-in-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs%0AAuthor%3A%20Sergey%20Berezin%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi%0AAbstract%3A%20%20%20We%20present%20a%20novel%20class%20of%20jailbreak%20adversarial%20attacks%20on%20LLMs%2C%20termed%0ATask-in-Prompt%20%28TIP%29%20attacks.%20Our%20approach%20embeds%20sequence-to-sequence%20tasks%0A%28e.g.%2C%20cipher%20decoding%2C%20riddles%2C%20code%20execution%29%20into%20the%20model%27s%20prompt%20to%0Aindirectly%20generate%20prohibited%20inputs.%20To%20systematically%20assess%20the%0Aeffectiveness%20of%20these%20attacks%2C%20we%20introduce%20the%20PHRYGE%20benchmark.%20We%0Ademonstrate%20that%20our%20techniques%20successfully%20circumvent%20safeguards%20in%20six%0Astate-of-the-art%20language%20models%2C%20including%20GPT-4o%20and%20LLaMA%203.2.%20Our%20findings%0Ahighlight%20critical%20weaknesses%20in%20current%20LLM%20safety%20alignments%20and%20underscore%0Athe%20urgent%20need%20for%20more%20sophisticated%20defence%20strategies.%0A%20%20Warning%3A%20this%20paper%20contains%20examples%20of%20unethical%20inquiries%20used%20solely%20for%0Aresearch%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18626v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520TIP%2520of%2520the%2520Iceberg%253A%2520Revealing%2520a%2520Hidden%2520Class%2520of%2520Task-in-Prompt%250A%2520%2520Adversarial%2520Attacks%2520on%2520LLMs%26entry.906535625%3DSergey%2520Berezin%2520and%2520Reza%2520Farahbakhsh%2520and%2520Noel%2520Crespi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520class%2520of%2520jailbreak%2520adversarial%2520attacks%2520on%2520LLMs%252C%2520termed%250ATask-in-Prompt%2520%2528TIP%2529%2520attacks.%2520Our%2520approach%2520embeds%2520sequence-to-sequence%2520tasks%250A%2528e.g.%252C%2520cipher%2520decoding%252C%2520riddles%252C%2520code%2520execution%2529%2520into%2520the%2520model%2527s%2520prompt%2520to%250Aindirectly%2520generate%2520prohibited%2520inputs.%2520To%2520systematically%2520assess%2520the%250Aeffectiveness%2520of%2520these%2520attacks%252C%2520we%2520introduce%2520the%2520PHRYGE%2520benchmark.%2520We%250Ademonstrate%2520that%2520our%2520techniques%2520successfully%2520circumvent%2520safeguards%2520in%2520six%250Astate-of-the-art%2520language%2520models%252C%2520including%2520GPT-4o%2520and%2520LLaMA%25203.2.%2520Our%2520findings%250Ahighlight%2520critical%2520weaknesses%2520in%2520current%2520LLM%2520safety%2520alignments%2520and%2520underscore%250Athe%2520urgent%2520need%2520for%2520more%2520sophisticated%2520defence%2520strategies.%250A%2520%2520Warning%253A%2520this%2520paper%2520contains%2520examples%2520of%2520unethical%2520inquiries%2520used%2520solely%2520for%250Aresearch%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18626v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-in-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs&entry.906535625=Sergey%20Berezin%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi&entry.1292438233=%20%20We%20present%20a%20novel%20class%20of%20jailbreak%20adversarial%20attacks%20on%20LLMs%2C%20termed%0ATask-in-Prompt%20%28TIP%29%20attacks.%20Our%20approach%20embeds%20sequence-to-sequence%20tasks%0A%28e.g.%2C%20cipher%20decoding%2C%20riddles%2C%20code%20execution%29%20into%20the%20model%27s%20prompt%20to%0Aindirectly%20generate%20prohibited%20inputs.%20To%20systematically%20assess%20the%0Aeffectiveness%20of%20these%20attacks%2C%20we%20introduce%20the%20PHRYGE%20benchmark.%20We%0Ademonstrate%20that%20our%20techniques%20successfully%20circumvent%20safeguards%20in%20six%0Astate-of-the-art%20language%20models%2C%20including%20GPT-4o%20and%20LLaMA%203.2.%20Our%20findings%0Ahighlight%20critical%20weaknesses%20in%20current%20LLM%20safety%20alignments%20and%20underscore%0Athe%20urgent%20need%20for%20more%20sophisticated%20defence%20strategies.%0A%20%20Warning%3A%20this%20paper%20contains%20examples%20of%20unethical%20inquiries%20used%20solely%20for%0Aresearch%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18626v3&entry.124074799=Read"},
{"title": "Exploring the Feasibility of AI-Assisted Spine MRI Protocol Optimization\n  Using DICOM Image Metadata", "author": "Alice Vian and Diego Andre Eifer and Mauricio Anes and Guilherme Ribeiro Garcia and Mariana Recamonde-Mendoza", "abstract": "  Artificial intelligence (AI) is increasingly being utilized to optimize\nmagnetic resonance imaging (MRI) protocols. Given that image details are\ncritical for diagnostic accuracy, optimizing MRI acquisition protocols is\nessential for enhancing image quality. While medical physicists are responsible\nfor this optimization, the variability in equipment usage and the wide range of\nMRI protocols in clinical settings pose significant challenges. This study aims\nto validate the application of AI in optimizing MRI protocols using dynamic\ndata from clinical practice, specifically DICOM metadata. To achieve this, four\nMRI spine exam databases were created, with the target attribute being the\nbinary classification of image quality (good or bad). Five AI models were\ntrained to identify trends in acquisition parameters that influence image\nquality, grounded in MRI theory. These trends were analyzed using SHAP graphs.\nThe models achieved F1 performance ranging from 77% to 93% for datasets\ncontaining 292 or more instances, with the observed trends aligning with MRI\ntheory. The models effectively reflected the practical realities of clinical\nMRI settings, offering a valuable tool for medical physicists in quality\ncontrol tasks. In conclusion, AI has demonstrated its potential to optimize MRI\nprotocols, supporting medical physicists in improving image quality and\nenhancing the efficiency of quality control in clinical practice.\n", "link": "http://arxiv.org/abs/2502.02351v1", "date": "2025-02-04", "relevancy": 2.2384, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4505}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Feasibility%20of%20AI-Assisted%20Spine%20MRI%20Protocol%20Optimization%0A%20%20Using%20DICOM%20Image%20Metadata&body=Title%3A%20Exploring%20the%20Feasibility%20of%20AI-Assisted%20Spine%20MRI%20Protocol%20Optimization%0A%20%20Using%20DICOM%20Image%20Metadata%0AAuthor%3A%20Alice%20Vian%20and%20Diego%20Andre%20Eifer%20and%20Mauricio%20Anes%20and%20Guilherme%20Ribeiro%20Garcia%20and%20Mariana%20Recamonde-Mendoza%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20is%20increasingly%20being%20utilized%20to%20optimize%0Amagnetic%20resonance%20imaging%20%28MRI%29%20protocols.%20Given%20that%20image%20details%20are%0Acritical%20for%20diagnostic%20accuracy%2C%20optimizing%20MRI%20acquisition%20protocols%20is%0Aessential%20for%20enhancing%20image%20quality.%20While%20medical%20physicists%20are%20responsible%0Afor%20this%20optimization%2C%20the%20variability%20in%20equipment%20usage%20and%20the%20wide%20range%20of%0AMRI%20protocols%20in%20clinical%20settings%20pose%20significant%20challenges.%20This%20study%20aims%0Ato%20validate%20the%20application%20of%20AI%20in%20optimizing%20MRI%20protocols%20using%20dynamic%0Adata%20from%20clinical%20practice%2C%20specifically%20DICOM%20metadata.%20To%20achieve%20this%2C%20four%0AMRI%20spine%20exam%20databases%20were%20created%2C%20with%20the%20target%20attribute%20being%20the%0Abinary%20classification%20of%20image%20quality%20%28good%20or%20bad%29.%20Five%20AI%20models%20were%0Atrained%20to%20identify%20trends%20in%20acquisition%20parameters%20that%20influence%20image%0Aquality%2C%20grounded%20in%20MRI%20theory.%20These%20trends%20were%20analyzed%20using%20SHAP%20graphs.%0AThe%20models%20achieved%20F1%20performance%20ranging%20from%2077%25%20to%2093%25%20for%20datasets%0Acontaining%20292%20or%20more%20instances%2C%20with%20the%20observed%20trends%20aligning%20with%20MRI%0Atheory.%20The%20models%20effectively%20reflected%20the%20practical%20realities%20of%20clinical%0AMRI%20settings%2C%20offering%20a%20valuable%20tool%20for%20medical%20physicists%20in%20quality%0Acontrol%20tasks.%20In%20conclusion%2C%20AI%20has%20demonstrated%20its%20potential%20to%20optimize%20MRI%0Aprotocols%2C%20supporting%20medical%20physicists%20in%20improving%20image%20quality%20and%0Aenhancing%20the%20efficiency%20of%20quality%20control%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Feasibility%2520of%2520AI-Assisted%2520Spine%2520MRI%2520Protocol%2520Optimization%250A%2520%2520Using%2520DICOM%2520Image%2520Metadata%26entry.906535625%3DAlice%2520Vian%2520and%2520Diego%2520Andre%2520Eifer%2520and%2520Mauricio%2520Anes%2520and%2520Guilherme%2520Ribeiro%2520Garcia%2520and%2520Mariana%2520Recamonde-Mendoza%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520is%2520increasingly%2520being%2520utilized%2520to%2520optimize%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520protocols.%2520Given%2520that%2520image%2520details%2520are%250Acritical%2520for%2520diagnostic%2520accuracy%252C%2520optimizing%2520MRI%2520acquisition%2520protocols%2520is%250Aessential%2520for%2520enhancing%2520image%2520quality.%2520While%2520medical%2520physicists%2520are%2520responsible%250Afor%2520this%2520optimization%252C%2520the%2520variability%2520in%2520equipment%2520usage%2520and%2520the%2520wide%2520range%2520of%250AMRI%2520protocols%2520in%2520clinical%2520settings%2520pose%2520significant%2520challenges.%2520This%2520study%2520aims%250Ato%2520validate%2520the%2520application%2520of%2520AI%2520in%2520optimizing%2520MRI%2520protocols%2520using%2520dynamic%250Adata%2520from%2520clinical%2520practice%252C%2520specifically%2520DICOM%2520metadata.%2520To%2520achieve%2520this%252C%2520four%250AMRI%2520spine%2520exam%2520databases%2520were%2520created%252C%2520with%2520the%2520target%2520attribute%2520being%2520the%250Abinary%2520classification%2520of%2520image%2520quality%2520%2528good%2520or%2520bad%2529.%2520Five%2520AI%2520models%2520were%250Atrained%2520to%2520identify%2520trends%2520in%2520acquisition%2520parameters%2520that%2520influence%2520image%250Aquality%252C%2520grounded%2520in%2520MRI%2520theory.%2520These%2520trends%2520were%2520analyzed%2520using%2520SHAP%2520graphs.%250AThe%2520models%2520achieved%2520F1%2520performance%2520ranging%2520from%252077%2525%2520to%252093%2525%2520for%2520datasets%250Acontaining%2520292%2520or%2520more%2520instances%252C%2520with%2520the%2520observed%2520trends%2520aligning%2520with%2520MRI%250Atheory.%2520The%2520models%2520effectively%2520reflected%2520the%2520practical%2520realities%2520of%2520clinical%250AMRI%2520settings%252C%2520offering%2520a%2520valuable%2520tool%2520for%2520medical%2520physicists%2520in%2520quality%250Acontrol%2520tasks.%2520In%2520conclusion%252C%2520AI%2520has%2520demonstrated%2520its%2520potential%2520to%2520optimize%2520MRI%250Aprotocols%252C%2520supporting%2520medical%2520physicists%2520in%2520improving%2520image%2520quality%2520and%250Aenhancing%2520the%2520efficiency%2520of%2520quality%2520control%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Feasibility%20of%20AI-Assisted%20Spine%20MRI%20Protocol%20Optimization%0A%20%20Using%20DICOM%20Image%20Metadata&entry.906535625=Alice%20Vian%20and%20Diego%20Andre%20Eifer%20and%20Mauricio%20Anes%20and%20Guilherme%20Ribeiro%20Garcia%20and%20Mariana%20Recamonde-Mendoza&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20is%20increasingly%20being%20utilized%20to%20optimize%0Amagnetic%20resonance%20imaging%20%28MRI%29%20protocols.%20Given%20that%20image%20details%20are%0Acritical%20for%20diagnostic%20accuracy%2C%20optimizing%20MRI%20acquisition%20protocols%20is%0Aessential%20for%20enhancing%20image%20quality.%20While%20medical%20physicists%20are%20responsible%0Afor%20this%20optimization%2C%20the%20variability%20in%20equipment%20usage%20and%20the%20wide%20range%20of%0AMRI%20protocols%20in%20clinical%20settings%20pose%20significant%20challenges.%20This%20study%20aims%0Ato%20validate%20the%20application%20of%20AI%20in%20optimizing%20MRI%20protocols%20using%20dynamic%0Adata%20from%20clinical%20practice%2C%20specifically%20DICOM%20metadata.%20To%20achieve%20this%2C%20four%0AMRI%20spine%20exam%20databases%20were%20created%2C%20with%20the%20target%20attribute%20being%20the%0Abinary%20classification%20of%20image%20quality%20%28good%20or%20bad%29.%20Five%20AI%20models%20were%0Atrained%20to%20identify%20trends%20in%20acquisition%20parameters%20that%20influence%20image%0Aquality%2C%20grounded%20in%20MRI%20theory.%20These%20trends%20were%20analyzed%20using%20SHAP%20graphs.%0AThe%20models%20achieved%20F1%20performance%20ranging%20from%2077%25%20to%2093%25%20for%20datasets%0Acontaining%20292%20or%20more%20instances%2C%20with%20the%20observed%20trends%20aligning%20with%20MRI%0Atheory.%20The%20models%20effectively%20reflected%20the%20practical%20realities%20of%20clinical%0AMRI%20settings%2C%20offering%20a%20valuable%20tool%20for%20medical%20physicists%20in%20quality%0Acontrol%20tasks.%20In%20conclusion%2C%20AI%20has%20demonstrated%20its%20potential%20to%20optimize%20MRI%0Aprotocols%2C%20supporting%20medical%20physicists%20in%20improving%20image%20quality%20and%0Aenhancing%20the%20efficiency%20of%20quality%20control%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02351v1&entry.124074799=Read"},
{"title": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs", "author": "Abhinav Kumar and Jaechul Roh and Ali Naseh and Marzena Karpinska and Mohit Iyyer and Amir Houmansadr and Eugene Bagdasarian", "abstract": "  We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 46x slowdown and high transferability of the\nattack across models. To protect applications, we discuss and implement\ndefenses leveraging LLM-based and system design approaches. Finally, we discuss\nsocietal, financial, and energy impacts of OVERTHINK attack which could amplify\nthe costs for third party applications operating reasoning models.\n", "link": "http://arxiv.org/abs/2502.02542v1", "date": "2025-02-04", "relevancy": 2.2332, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OVERTHINKING%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&body=Title%3A%20OVERTHINKING%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs%0AAuthor%3A%20Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian%0AAbstract%3A%20%20%20We%20increase%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20LLMs-we%20force%0Amodels%20to%20spend%20an%20amplified%20number%20of%20reasoning%20tokens%2C%20i.e.%2C%20%22overthink%22%2C%20to%0Arespond%20to%20the%20user%20query%20while%20providing%20contextually%20correct%20answers.%20The%0Aadversary%20performs%20an%20OVERTHINK%20attack%20by%20injecting%20decoy%20reasoning%20problems%0Ainto%20the%20public%20content%20that%20is%20used%20by%20the%20reasoning%20LLM%20%28e.g.%2C%20for%20RAG%0Aapplications%29%20during%20inference%20time.%20Due%20to%20the%20nature%20of%20our%20decoy%20problems%0A%28e.g.%2C%20a%20Markov%20Decision%20Process%29%2C%20modified%20texts%20do%20not%20violate%20safety%0Aguardrails.%20We%20evaluated%20our%20attack%20across%20closed-%28OpenAI%20o1%2C%20o1-mini%2C%20o3-mini%29%0Aand%20open-%28DeepSeek%20R1%29%20weights%20reasoning%20models%20on%20the%20FreshQA%20and%20SQuAD%0Adatasets.%20Our%20results%20show%20up%20to%2046x%20slowdown%20and%20high%20transferability%20of%20the%0Aattack%20across%20models.%20To%20protect%20applications%2C%20we%20discuss%20and%20implement%0Adefenses%20leveraging%20LLM-based%20and%20system%20design%20approaches.%20Finally%2C%20we%20discuss%0Asocietal%2C%20financial%2C%20and%20energy%20impacts%20of%20OVERTHINK%20attack%20which%20could%20amplify%0Athe%20costs%20for%20third%20party%20applications%20operating%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOVERTHINKING%253A%2520Slowdown%2520Attacks%2520on%2520Reasoning%2520LLMs%26entry.906535625%3DAbhinav%2520Kumar%2520and%2520Jaechul%2520Roh%2520and%2520Ali%2520Naseh%2520and%2520Marzena%2520Karpinska%2520and%2520Mohit%2520Iyyer%2520and%2520Amir%2520Houmansadr%2520and%2520Eugene%2520Bagdasarian%26entry.1292438233%3D%2520%2520We%2520increase%2520overhead%2520for%2520applications%2520that%2520rely%2520on%2520reasoning%2520LLMs-we%2520force%250Amodels%2520to%2520spend%2520an%2520amplified%2520number%2520of%2520reasoning%2520tokens%252C%2520i.e.%252C%2520%2522overthink%2522%252C%2520to%250Arespond%2520to%2520the%2520user%2520query%2520while%2520providing%2520contextually%2520correct%2520answers.%2520The%250Aadversary%2520performs%2520an%2520OVERTHINK%2520attack%2520by%2520injecting%2520decoy%2520reasoning%2520problems%250Ainto%2520the%2520public%2520content%2520that%2520is%2520used%2520by%2520the%2520reasoning%2520LLM%2520%2528e.g.%252C%2520for%2520RAG%250Aapplications%2529%2520during%2520inference%2520time.%2520Due%2520to%2520the%2520nature%2520of%2520our%2520decoy%2520problems%250A%2528e.g.%252C%2520a%2520Markov%2520Decision%2520Process%2529%252C%2520modified%2520texts%2520do%2520not%2520violate%2520safety%250Aguardrails.%2520We%2520evaluated%2520our%2520attack%2520across%2520closed-%2528OpenAI%2520o1%252C%2520o1-mini%252C%2520o3-mini%2529%250Aand%2520open-%2528DeepSeek%2520R1%2529%2520weights%2520reasoning%2520models%2520on%2520the%2520FreshQA%2520and%2520SQuAD%250Adatasets.%2520Our%2520results%2520show%2520up%2520to%252046x%2520slowdown%2520and%2520high%2520transferability%2520of%2520the%250Aattack%2520across%2520models.%2520To%2520protect%2520applications%252C%2520we%2520discuss%2520and%2520implement%250Adefenses%2520leveraging%2520LLM-based%2520and%2520system%2520design%2520approaches.%2520Finally%252C%2520we%2520discuss%250Asocietal%252C%2520financial%252C%2520and%2520energy%2520impacts%2520of%2520OVERTHINK%2520attack%2520which%2520could%2520amplify%250Athe%2520costs%2520for%2520third%2520party%2520applications%2520operating%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OVERTHINKING%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&entry.906535625=Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian&entry.1292438233=%20%20We%20increase%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20LLMs-we%20force%0Amodels%20to%20spend%20an%20amplified%20number%20of%20reasoning%20tokens%2C%20i.e.%2C%20%22overthink%22%2C%20to%0Arespond%20to%20the%20user%20query%20while%20providing%20contextually%20correct%20answers.%20The%0Aadversary%20performs%20an%20OVERTHINK%20attack%20by%20injecting%20decoy%20reasoning%20problems%0Ainto%20the%20public%20content%20that%20is%20used%20by%20the%20reasoning%20LLM%20%28e.g.%2C%20for%20RAG%0Aapplications%29%20during%20inference%20time.%20Due%20to%20the%20nature%20of%20our%20decoy%20problems%0A%28e.g.%2C%20a%20Markov%20Decision%20Process%29%2C%20modified%20texts%20do%20not%20violate%20safety%0Aguardrails.%20We%20evaluated%20our%20attack%20across%20closed-%28OpenAI%20o1%2C%20o1-mini%2C%20o3-mini%29%0Aand%20open-%28DeepSeek%20R1%29%20weights%20reasoning%20models%20on%20the%20FreshQA%20and%20SQuAD%0Adatasets.%20Our%20results%20show%20up%20to%2046x%20slowdown%20and%20high%20transferability%20of%20the%0Aattack%20across%20models.%20To%20protect%20applications%2C%20we%20discuss%20and%20implement%0Adefenses%20leveraging%20LLM-based%20and%20system%20design%20approaches.%20Finally%2C%20we%20discuss%0Asocietal%2C%20financial%2C%20and%20energy%20impacts%20of%20OVERTHINK%20attack%20which%20could%20amplify%0Athe%20costs%20for%20third%20party%20applications%20operating%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02542v1&entry.124074799=Read"},
{"title": "LoRA-X: Bridging Foundation Models with Training-Free Cross-Model\n  Adaptation", "author": "Farzad Farhadzadeh and Debasmit Das and Shubhankar Borse and Fatih Porikli", "abstract": "  The rising popularity of large foundation models has led to a heightened\ndemand for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation\n(LoRA), which offer performance comparable to full model fine-tuning while\nrequiring only a few additional parameters tailored to the specific base model.\nWhen such base models are deprecated and replaced, all associated LoRA modules\nmust be retrained, requiring access to either the original training data or a\nsubstantial amount of synthetic data that mirrors the original distribution.\nHowever, the original data is often inaccessible due to privacy or licensing\nissues, and generating synthetic data may be impractical and insufficiently\nrepresentative. These factors complicate the fine-tuning process considerably.\nTo address this challenge, we introduce a new adapter, Cross-Model Low-Rank\nAdaptation (LoRA-X), which enables the training-free transfer of LoRA\nparameters across source and target models, eliminating the need for original\nor synthetic training data. Our approach imposes the adapter to operate within\nthe subspace of the source base model. This constraint is necessary because our\nprior knowledge of the target model is limited to its weights, and the criteria\nfor ensuring the adapter's transferability are restricted to the target base\nmodel's weights and subspace. To facilitate the transfer of LoRA parameters of\nthe source model to a target model, we employ the adapter only in the layers of\nthe target model that exhibit an acceptable level of subspace similarity. Our\nextensive experiments demonstrate the effectiveness of LoRA-X for text-to-image\ngeneration, including Stable Diffusion v1.5 and Stable Diffusion XL.\n", "link": "http://arxiv.org/abs/2501.16559v2", "date": "2025-02-04", "relevancy": 2.2248, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-X%3A%20Bridging%20Foundation%20Models%20with%20Training-Free%20Cross-Model%0A%20%20Adaptation&body=Title%3A%20LoRA-X%3A%20Bridging%20Foundation%20Models%20with%20Training-Free%20Cross-Model%0A%20%20Adaptation%0AAuthor%3A%20Farzad%20Farhadzadeh%20and%20Debasmit%20Das%20and%20Shubhankar%20Borse%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20The%20rising%20popularity%20of%20large%20foundation%20models%20has%20led%20to%20a%20heightened%0Ademand%20for%20parameter-efficient%20fine-tuning%20methods%2C%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20which%20offer%20performance%20comparable%20to%20full%20model%20fine-tuning%20while%0Arequiring%20only%20a%20few%20additional%20parameters%20tailored%20to%20the%20specific%20base%20model.%0AWhen%20such%20base%20models%20are%20deprecated%20and%20replaced%2C%20all%20associated%20LoRA%20modules%0Amust%20be%20retrained%2C%20requiring%20access%20to%20either%20the%20original%20training%20data%20or%20a%0Asubstantial%20amount%20of%20synthetic%20data%20that%20mirrors%20the%20original%20distribution.%0AHowever%2C%20the%20original%20data%20is%20often%20inaccessible%20due%20to%20privacy%20or%20licensing%0Aissues%2C%20and%20generating%20synthetic%20data%20may%20be%20impractical%20and%20insufficiently%0Arepresentative.%20These%20factors%20complicate%20the%20fine-tuning%20process%20considerably.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20a%20new%20adapter%2C%20Cross-Model%20Low-Rank%0AAdaptation%20%28LoRA-X%29%2C%20which%20enables%20the%20training-free%20transfer%20of%20LoRA%0Aparameters%20across%20source%20and%20target%20models%2C%20eliminating%20the%20need%20for%20original%0Aor%20synthetic%20training%20data.%20Our%20approach%20imposes%20the%20adapter%20to%20operate%20within%0Athe%20subspace%20of%20the%20source%20base%20model.%20This%20constraint%20is%20necessary%20because%20our%0Aprior%20knowledge%20of%20the%20target%20model%20is%20limited%20to%20its%20weights%2C%20and%20the%20criteria%0Afor%20ensuring%20the%20adapter%27s%20transferability%20are%20restricted%20to%20the%20target%20base%0Amodel%27s%20weights%20and%20subspace.%20To%20facilitate%20the%20transfer%20of%20LoRA%20parameters%20of%0Athe%20source%20model%20to%20a%20target%20model%2C%20we%20employ%20the%20adapter%20only%20in%20the%20layers%20of%0Athe%20target%20model%20that%20exhibit%20an%20acceptable%20level%20of%20subspace%20similarity.%20Our%0Aextensive%20experiments%20demonstrate%20the%20effectiveness%20of%20LoRA-X%20for%20text-to-image%0Ageneration%2C%20including%20Stable%20Diffusion%20v1.5%20and%20Stable%20Diffusion%20XL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-X%253A%2520Bridging%2520Foundation%2520Models%2520with%2520Training-Free%2520Cross-Model%250A%2520%2520Adaptation%26entry.906535625%3DFarzad%2520Farhadzadeh%2520and%2520Debasmit%2520Das%2520and%2520Shubhankar%2520Borse%2520and%2520Fatih%2520Porikli%26entry.1292438233%3D%2520%2520The%2520rising%2520popularity%2520of%2520large%2520foundation%2520models%2520has%2520led%2520to%2520a%2520heightened%250Ademand%2520for%2520parameter-efficient%2520fine-tuning%2520methods%252C%2520such%2520as%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%252C%2520which%2520offer%2520performance%2520comparable%2520to%2520full%2520model%2520fine-tuning%2520while%250Arequiring%2520only%2520a%2520few%2520additional%2520parameters%2520tailored%2520to%2520the%2520specific%2520base%2520model.%250AWhen%2520such%2520base%2520models%2520are%2520deprecated%2520and%2520replaced%252C%2520all%2520associated%2520LoRA%2520modules%250Amust%2520be%2520retrained%252C%2520requiring%2520access%2520to%2520either%2520the%2520original%2520training%2520data%2520or%2520a%250Asubstantial%2520amount%2520of%2520synthetic%2520data%2520that%2520mirrors%2520the%2520original%2520distribution.%250AHowever%252C%2520the%2520original%2520data%2520is%2520often%2520inaccessible%2520due%2520to%2520privacy%2520or%2520licensing%250Aissues%252C%2520and%2520generating%2520synthetic%2520data%2520may%2520be%2520impractical%2520and%2520insufficiently%250Arepresentative.%2520These%2520factors%2520complicate%2520the%2520fine-tuning%2520process%2520considerably.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520new%2520adapter%252C%2520Cross-Model%2520Low-Rank%250AAdaptation%2520%2528LoRA-X%2529%252C%2520which%2520enables%2520the%2520training-free%2520transfer%2520of%2520LoRA%250Aparameters%2520across%2520source%2520and%2520target%2520models%252C%2520eliminating%2520the%2520need%2520for%2520original%250Aor%2520synthetic%2520training%2520data.%2520Our%2520approach%2520imposes%2520the%2520adapter%2520to%2520operate%2520within%250Athe%2520subspace%2520of%2520the%2520source%2520base%2520model.%2520This%2520constraint%2520is%2520necessary%2520because%2520our%250Aprior%2520knowledge%2520of%2520the%2520target%2520model%2520is%2520limited%2520to%2520its%2520weights%252C%2520and%2520the%2520criteria%250Afor%2520ensuring%2520the%2520adapter%2527s%2520transferability%2520are%2520restricted%2520to%2520the%2520target%2520base%250Amodel%2527s%2520weights%2520and%2520subspace.%2520To%2520facilitate%2520the%2520transfer%2520of%2520LoRA%2520parameters%2520of%250Athe%2520source%2520model%2520to%2520a%2520target%2520model%252C%2520we%2520employ%2520the%2520adapter%2520only%2520in%2520the%2520layers%2520of%250Athe%2520target%2520model%2520that%2520exhibit%2520an%2520acceptable%2520level%2520of%2520subspace%2520similarity.%2520Our%250Aextensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520LoRA-X%2520for%2520text-to-image%250Ageneration%252C%2520including%2520Stable%2520Diffusion%2520v1.5%2520and%2520Stable%2520Diffusion%2520XL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-X%3A%20Bridging%20Foundation%20Models%20with%20Training-Free%20Cross-Model%0A%20%20Adaptation&entry.906535625=Farzad%20Farhadzadeh%20and%20Debasmit%20Das%20and%20Shubhankar%20Borse%20and%20Fatih%20Porikli&entry.1292438233=%20%20The%20rising%20popularity%20of%20large%20foundation%20models%20has%20led%20to%20a%20heightened%0Ademand%20for%20parameter-efficient%20fine-tuning%20methods%2C%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20which%20offer%20performance%20comparable%20to%20full%20model%20fine-tuning%20while%0Arequiring%20only%20a%20few%20additional%20parameters%20tailored%20to%20the%20specific%20base%20model.%0AWhen%20such%20base%20models%20are%20deprecated%20and%20replaced%2C%20all%20associated%20LoRA%20modules%0Amust%20be%20retrained%2C%20requiring%20access%20to%20either%20the%20original%20training%20data%20or%20a%0Asubstantial%20amount%20of%20synthetic%20data%20that%20mirrors%20the%20original%20distribution.%0AHowever%2C%20the%20original%20data%20is%20often%20inaccessible%20due%20to%20privacy%20or%20licensing%0Aissues%2C%20and%20generating%20synthetic%20data%20may%20be%20impractical%20and%20insufficiently%0Arepresentative.%20These%20factors%20complicate%20the%20fine-tuning%20process%20considerably.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20a%20new%20adapter%2C%20Cross-Model%20Low-Rank%0AAdaptation%20%28LoRA-X%29%2C%20which%20enables%20the%20training-free%20transfer%20of%20LoRA%0Aparameters%20across%20source%20and%20target%20models%2C%20eliminating%20the%20need%20for%20original%0Aor%20synthetic%20training%20data.%20Our%20approach%20imposes%20the%20adapter%20to%20operate%20within%0Athe%20subspace%20of%20the%20source%20base%20model.%20This%20constraint%20is%20necessary%20because%20our%0Aprior%20knowledge%20of%20the%20target%20model%20is%20limited%20to%20its%20weights%2C%20and%20the%20criteria%0Afor%20ensuring%20the%20adapter%27s%20transferability%20are%20restricted%20to%20the%20target%20base%0Amodel%27s%20weights%20and%20subspace.%20To%20facilitate%20the%20transfer%20of%20LoRA%20parameters%20of%0Athe%20source%20model%20to%20a%20target%20model%2C%20we%20employ%20the%20adapter%20only%20in%20the%20layers%20of%0Athe%20target%20model%20that%20exhibit%20an%20acceptable%20level%20of%20subspace%20similarity.%20Our%0Aextensive%20experiments%20demonstrate%20the%20effectiveness%20of%20LoRA-X%20for%20text-to-image%0Ageneration%2C%20including%20Stable%20Diffusion%20v1.5%20and%20Stable%20Diffusion%20XL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16559v2&entry.124074799=Read"},
{"title": "DCBM: Data-Efficient Visual Concept Bottleneck Models", "author": "Katharina Prasse and Patrick Knab and Sascha Marton and Christian Bartelt and Margret Keuper", "abstract": "  Concept Bottleneck Models (CBMs) enhance the interpretability of neural\nnetworks by basing predictions on human-understandable concepts. However,\ncurrent CBMs typically rely on concept sets extracted from large language\nmodels or extensive image corpora, limiting their effectiveness in data-sparse\nscenarios. We propose Data-efficient CBMs (DCBMs), which reduce the need for\nlarge sample sizes during concept generation while preserving interpretability.\nDCBMs define concepts as image regions detected by segmentation or detection\nfoundation models, allowing each image to generate multiple concepts across\ndifferent granularities. This removes reliance on textual descriptions and\nlarge-scale pre-training, making DCBMs applicable for fine-grained\nclassification and out-of-distribution tasks. Attribution analysis using\nGrad-CAM demonstrates that DCBMs deliver visual concepts that can be localized\nin test images. By leveraging dataset-specific concepts instead of predefined\nones, DCBMs enhance adaptability to new domains.\n", "link": "http://arxiv.org/abs/2412.11576v2", "date": "2025-02-04", "relevancy": 2.2126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5544}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCBM%3A%20Data-Efficient%20Visual%20Concept%20Bottleneck%20Models&body=Title%3A%20DCBM%3A%20Data-Efficient%20Visual%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Katharina%20Prasse%20and%20Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20the%20interpretability%20of%20neural%0Anetworks%20by%20basing%20predictions%20on%20human-understandable%20concepts.%20However%2C%0Acurrent%20CBMs%20typically%20rely%20on%20concept%20sets%20extracted%20from%20large%20language%0Amodels%20or%20extensive%20image%20corpora%2C%20limiting%20their%20effectiveness%20in%20data-sparse%0Ascenarios.%20We%20propose%20Data-efficient%20CBMs%20%28DCBMs%29%2C%20which%20reduce%20the%20need%20for%0Alarge%20sample%20sizes%20during%20concept%20generation%20while%20preserving%20interpretability.%0ADCBMs%20define%20concepts%20as%20image%20regions%20detected%20by%20segmentation%20or%20detection%0Afoundation%20models%2C%20allowing%20each%20image%20to%20generate%20multiple%20concepts%20across%0Adifferent%20granularities.%20This%20removes%20reliance%20on%20textual%20descriptions%20and%0Alarge-scale%20pre-training%2C%20making%20DCBMs%20applicable%20for%20fine-grained%0Aclassification%20and%20out-of-distribution%20tasks.%20Attribution%20analysis%20using%0AGrad-CAM%20demonstrates%20that%20DCBMs%20deliver%20visual%20concepts%20that%20can%20be%20localized%0Ain%20test%20images.%20By%20leveraging%20dataset-specific%20concepts%20instead%20of%20predefined%0Aones%2C%20DCBMs%20enhance%20adaptability%20to%20new%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCBM%253A%2520Data-Efficient%2520Visual%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DKatharina%2520Prasse%2520and%2520Patrick%2520Knab%2520and%2520Sascha%2520Marton%2520and%2520Christian%2520Bartelt%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520enhance%2520the%2520interpretability%2520of%2520neural%250Anetworks%2520by%2520basing%2520predictions%2520on%2520human-understandable%2520concepts.%2520However%252C%250Acurrent%2520CBMs%2520typically%2520rely%2520on%2520concept%2520sets%2520extracted%2520from%2520large%2520language%250Amodels%2520or%2520extensive%2520image%2520corpora%252C%2520limiting%2520their%2520effectiveness%2520in%2520data-sparse%250Ascenarios.%2520We%2520propose%2520Data-efficient%2520CBMs%2520%2528DCBMs%2529%252C%2520which%2520reduce%2520the%2520need%2520for%250Alarge%2520sample%2520sizes%2520during%2520concept%2520generation%2520while%2520preserving%2520interpretability.%250ADCBMs%2520define%2520concepts%2520as%2520image%2520regions%2520detected%2520by%2520segmentation%2520or%2520detection%250Afoundation%2520models%252C%2520allowing%2520each%2520image%2520to%2520generate%2520multiple%2520concepts%2520across%250Adifferent%2520granularities.%2520This%2520removes%2520reliance%2520on%2520textual%2520descriptions%2520and%250Alarge-scale%2520pre-training%252C%2520making%2520DCBMs%2520applicable%2520for%2520fine-grained%250Aclassification%2520and%2520out-of-distribution%2520tasks.%2520Attribution%2520analysis%2520using%250AGrad-CAM%2520demonstrates%2520that%2520DCBMs%2520deliver%2520visual%2520concepts%2520that%2520can%2520be%2520localized%250Ain%2520test%2520images.%2520By%2520leveraging%2520dataset-specific%2520concepts%2520instead%2520of%2520predefined%250Aones%252C%2520DCBMs%2520enhance%2520adaptability%2520to%2520new%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCBM%3A%20Data-Efficient%20Visual%20Concept%20Bottleneck%20Models&entry.906535625=Katharina%20Prasse%20and%20Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt%20and%20Margret%20Keuper&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20the%20interpretability%20of%20neural%0Anetworks%20by%20basing%20predictions%20on%20human-understandable%20concepts.%20However%2C%0Acurrent%20CBMs%20typically%20rely%20on%20concept%20sets%20extracted%20from%20large%20language%0Amodels%20or%20extensive%20image%20corpora%2C%20limiting%20their%20effectiveness%20in%20data-sparse%0Ascenarios.%20We%20propose%20Data-efficient%20CBMs%20%28DCBMs%29%2C%20which%20reduce%20the%20need%20for%0Alarge%20sample%20sizes%20during%20concept%20generation%20while%20preserving%20interpretability.%0ADCBMs%20define%20concepts%20as%20image%20regions%20detected%20by%20segmentation%20or%20detection%0Afoundation%20models%2C%20allowing%20each%20image%20to%20generate%20multiple%20concepts%20across%0Adifferent%20granularities.%20This%20removes%20reliance%20on%20textual%20descriptions%20and%0Alarge-scale%20pre-training%2C%20making%20DCBMs%20applicable%20for%20fine-grained%0Aclassification%20and%20out-of-distribution%20tasks.%20Attribution%20analysis%20using%0AGrad-CAM%20demonstrates%20that%20DCBMs%20deliver%20visual%20concepts%20that%20can%20be%20localized%0Ain%20test%20images.%20By%20leveraging%20dataset-specific%20concepts%20instead%20of%20predefined%0Aones%2C%20DCBMs%20enhance%20adaptability%20to%20new%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11576v2&entry.124074799=Read"},
{"title": "AI Reliance and Decision Quality: Fundamentals, Interdependence, and the\n  Effects of Interventions", "author": "Jakob Schoeffer and Johannes Jakubik and Michael Voessing and Niklas Kuehl and Gerhard Satzger", "abstract": "  In AI-assisted decision-making, a central promise of having a\nhuman-in-the-loop is that they should be able to complement the AI system by\noverriding its wrong recommendations. In practice, however, we often see that\nhumans cannot assess the correctness of AI recommendations and, as a result,\nadhere to wrong or override correct advice. Different ways of relying on AI\nrecommendations have immediate, yet distinct, implications for decision\nquality. Unfortunately, reliance and decision quality are often inappropriately\nconflated in the current literature on AI-assisted decision-making. In this\nwork, we disentangle and formalize the relationship between reliance and\ndecision quality, and we characterize the conditions under which human-AI\ncomplementarity is achievable. To illustrate how reliance and decision quality\nrelate to one another, we propose a visual framework and demonstrate its\nusefulness for interpreting empirical findings, including the effects of\ninterventions like explanations. Overall, our research highlights the\nimportance of distinguishing between reliance behavior and decision quality in\nAI-assisted decision-making.\n", "link": "http://arxiv.org/abs/2304.08804v4", "date": "2025-02-04", "relevancy": 2.2091, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Reliance%20and%20Decision%20Quality%3A%20Fundamentals%2C%20Interdependence%2C%20and%20the%0A%20%20Effects%20of%20Interventions&body=Title%3A%20AI%20Reliance%20and%20Decision%20Quality%3A%20Fundamentals%2C%20Interdependence%2C%20and%20the%0A%20%20Effects%20of%20Interventions%0AAuthor%3A%20Jakob%20Schoeffer%20and%20Johannes%20Jakubik%20and%20Michael%20Voessing%20and%20Niklas%20Kuehl%20and%20Gerhard%20Satzger%0AAbstract%3A%20%20%20In%20AI-assisted%20decision-making%2C%20a%20central%20promise%20of%20having%20a%0Ahuman-in-the-loop%20is%20that%20they%20should%20be%20able%20to%20complement%20the%20AI%20system%20by%0Aoverriding%20its%20wrong%20recommendations.%20In%20practice%2C%20however%2C%20we%20often%20see%20that%0Ahumans%20cannot%20assess%20the%20correctness%20of%20AI%20recommendations%20and%2C%20as%20a%20result%2C%0Aadhere%20to%20wrong%20or%20override%20correct%20advice.%20Different%20ways%20of%20relying%20on%20AI%0Arecommendations%20have%20immediate%2C%20yet%20distinct%2C%20implications%20for%20decision%0Aquality.%20Unfortunately%2C%20reliance%20and%20decision%20quality%20are%20often%20inappropriately%0Aconflated%20in%20the%20current%20literature%20on%20AI-assisted%20decision-making.%20In%20this%0Awork%2C%20we%20disentangle%20and%20formalize%20the%20relationship%20between%20reliance%20and%0Adecision%20quality%2C%20and%20we%20characterize%20the%20conditions%20under%20which%20human-AI%0Acomplementarity%20is%20achievable.%20To%20illustrate%20how%20reliance%20and%20decision%20quality%0Arelate%20to%20one%20another%2C%20we%20propose%20a%20visual%20framework%20and%20demonstrate%20its%0Ausefulness%20for%20interpreting%20empirical%20findings%2C%20including%20the%20effects%20of%0Ainterventions%20like%20explanations.%20Overall%2C%20our%20research%20highlights%20the%0Aimportance%20of%20distinguishing%20between%20reliance%20behavior%20and%20decision%20quality%20in%0AAI-assisted%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08804v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Reliance%2520and%2520Decision%2520Quality%253A%2520Fundamentals%252C%2520Interdependence%252C%2520and%2520the%250A%2520%2520Effects%2520of%2520Interventions%26entry.906535625%3DJakob%2520Schoeffer%2520and%2520Johannes%2520Jakubik%2520and%2520Michael%2520Voessing%2520and%2520Niklas%2520Kuehl%2520and%2520Gerhard%2520Satzger%26entry.1292438233%3D%2520%2520In%2520AI-assisted%2520decision-making%252C%2520a%2520central%2520promise%2520of%2520having%2520a%250Ahuman-in-the-loop%2520is%2520that%2520they%2520should%2520be%2520able%2520to%2520complement%2520the%2520AI%2520system%2520by%250Aoverriding%2520its%2520wrong%2520recommendations.%2520In%2520practice%252C%2520however%252C%2520we%2520often%2520see%2520that%250Ahumans%2520cannot%2520assess%2520the%2520correctness%2520of%2520AI%2520recommendations%2520and%252C%2520as%2520a%2520result%252C%250Aadhere%2520to%2520wrong%2520or%2520override%2520correct%2520advice.%2520Different%2520ways%2520of%2520relying%2520on%2520AI%250Arecommendations%2520have%2520immediate%252C%2520yet%2520distinct%252C%2520implications%2520for%2520decision%250Aquality.%2520Unfortunately%252C%2520reliance%2520and%2520decision%2520quality%2520are%2520often%2520inappropriately%250Aconflated%2520in%2520the%2520current%2520literature%2520on%2520AI-assisted%2520decision-making.%2520In%2520this%250Awork%252C%2520we%2520disentangle%2520and%2520formalize%2520the%2520relationship%2520between%2520reliance%2520and%250Adecision%2520quality%252C%2520and%2520we%2520characterize%2520the%2520conditions%2520under%2520which%2520human-AI%250Acomplementarity%2520is%2520achievable.%2520To%2520illustrate%2520how%2520reliance%2520and%2520decision%2520quality%250Arelate%2520to%2520one%2520another%252C%2520we%2520propose%2520a%2520visual%2520framework%2520and%2520demonstrate%2520its%250Ausefulness%2520for%2520interpreting%2520empirical%2520findings%252C%2520including%2520the%2520effects%2520of%250Ainterventions%2520like%2520explanations.%2520Overall%252C%2520our%2520research%2520highlights%2520the%250Aimportance%2520of%2520distinguishing%2520between%2520reliance%2520behavior%2520and%2520decision%2520quality%2520in%250AAI-assisted%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.08804v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Reliance%20and%20Decision%20Quality%3A%20Fundamentals%2C%20Interdependence%2C%20and%20the%0A%20%20Effects%20of%20Interventions&entry.906535625=Jakob%20Schoeffer%20and%20Johannes%20Jakubik%20and%20Michael%20Voessing%20and%20Niklas%20Kuehl%20and%20Gerhard%20Satzger&entry.1292438233=%20%20In%20AI-assisted%20decision-making%2C%20a%20central%20promise%20of%20having%20a%0Ahuman-in-the-loop%20is%20that%20they%20should%20be%20able%20to%20complement%20the%20AI%20system%20by%0Aoverriding%20its%20wrong%20recommendations.%20In%20practice%2C%20however%2C%20we%20often%20see%20that%0Ahumans%20cannot%20assess%20the%20correctness%20of%20AI%20recommendations%20and%2C%20as%20a%20result%2C%0Aadhere%20to%20wrong%20or%20override%20correct%20advice.%20Different%20ways%20of%20relying%20on%20AI%0Arecommendations%20have%20immediate%2C%20yet%20distinct%2C%20implications%20for%20decision%0Aquality.%20Unfortunately%2C%20reliance%20and%20decision%20quality%20are%20often%20inappropriately%0Aconflated%20in%20the%20current%20literature%20on%20AI-assisted%20decision-making.%20In%20this%0Awork%2C%20we%20disentangle%20and%20formalize%20the%20relationship%20between%20reliance%20and%0Adecision%20quality%2C%20and%20we%20characterize%20the%20conditions%20under%20which%20human-AI%0Acomplementarity%20is%20achievable.%20To%20illustrate%20how%20reliance%20and%20decision%20quality%0Arelate%20to%20one%20another%2C%20we%20propose%20a%20visual%20framework%20and%20demonstrate%20its%0Ausefulness%20for%20interpreting%20empirical%20findings%2C%20including%20the%20effects%20of%0Ainterventions%20like%20explanations.%20Overall%2C%20our%20research%20highlights%20the%0Aimportance%20of%20distinguishing%20between%20reliance%20behavior%20and%20decision%20quality%20in%0AAI-assisted%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08804v4&entry.124074799=Read"},
{"title": "Coreset-Based Task Selection for Sample-Efficient Meta-Reinforcement\n  Learning", "author": "Donglin Zhan and Leonardo F. Toso and James Anderson", "abstract": "  We study task selection to enhance sample efficiency in model-agnostic\nmeta-reinforcement learning (MAML-RL). Traditional meta-RL typically assumes\nthat all available tasks are equally important, which can lead to task\nredundancy when they share significant similarities. To address this, we\npropose a coreset-based task selection approach that selects a weighted subset\nof tasks based on how diverse they are in gradient space, prioritizing the most\ninformative and diverse tasks. Such task selection reduces the number of\nsamples needed to find an $\\epsilon$-close stationary solution by a factor of\nO(1/$\\epsilon$). Consequently, it guarantees a faster adaptation to unseen\ntasks while focusing training on the most relevant tasks. As a case study, we\nincorporate task selection to MAML-LQR (Toso et al., 2024b), and prove a sample\ncomplexity reduction proportional to O(log(1/$\\epsilon$)) when the task\nspecific cost also satisfy gradient dominance. Our theoretical guarantees\nunderscore task selection as a key component for scalable and sample-efficient\nmeta-RL. We numerically validate this trend across multiple RL benchmark\nproblems, illustrating the benefits of task selection beyond the LQR baseline.\n", "link": "http://arxiv.org/abs/2502.02332v1", "date": "2025-02-04", "relevancy": 2.2051, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4622}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4313}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coreset-Based%20Task%20Selection%20for%20Sample-Efficient%20Meta-Reinforcement%0A%20%20Learning&body=Title%3A%20Coreset-Based%20Task%20Selection%20for%20Sample-Efficient%20Meta-Reinforcement%0A%20%20Learning%0AAuthor%3A%20Donglin%20Zhan%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson%0AAbstract%3A%20%20%20We%20study%20task%20selection%20to%20enhance%20sample%20efficiency%20in%20model-agnostic%0Ameta-reinforcement%20learning%20%28MAML-RL%29.%20Traditional%20meta-RL%20typically%20assumes%0Athat%20all%20available%20tasks%20are%20equally%20important%2C%20which%20can%20lead%20to%20task%0Aredundancy%20when%20they%20share%20significant%20similarities.%20To%20address%20this%2C%20we%0Apropose%20a%20coreset-based%20task%20selection%20approach%20that%20selects%20a%20weighted%20subset%0Aof%20tasks%20based%20on%20how%20diverse%20they%20are%20in%20gradient%20space%2C%20prioritizing%20the%20most%0Ainformative%20and%20diverse%20tasks.%20Such%20task%20selection%20reduces%20the%20number%20of%0Asamples%20needed%20to%20find%20an%20%24%5Cepsilon%24-close%20stationary%20solution%20by%20a%20factor%20of%0AO%281/%24%5Cepsilon%24%29.%20Consequently%2C%20it%20guarantees%20a%20faster%20adaptation%20to%20unseen%0Atasks%20while%20focusing%20training%20on%20the%20most%20relevant%20tasks.%20As%20a%20case%20study%2C%20we%0Aincorporate%20task%20selection%20to%20MAML-LQR%20%28Toso%20et%20al.%2C%202024b%29%2C%20and%20prove%20a%20sample%0Acomplexity%20reduction%20proportional%20to%20O%28log%281/%24%5Cepsilon%24%29%29%20when%20the%20task%0Aspecific%20cost%20also%20satisfy%20gradient%20dominance.%20Our%20theoretical%20guarantees%0Aunderscore%20task%20selection%20as%20a%20key%20component%20for%20scalable%20and%20sample-efficient%0Ameta-RL.%20We%20numerically%20validate%20this%20trend%20across%20multiple%20RL%20benchmark%0Aproblems%2C%20illustrating%20the%20benefits%20of%20task%20selection%20beyond%20the%20LQR%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoreset-Based%2520Task%2520Selection%2520for%2520Sample-Efficient%2520Meta-Reinforcement%250A%2520%2520Learning%26entry.906535625%3DDonglin%2520Zhan%2520and%2520Leonardo%2520F.%2520Toso%2520and%2520James%2520Anderson%26entry.1292438233%3D%2520%2520We%2520study%2520task%2520selection%2520to%2520enhance%2520sample%2520efficiency%2520in%2520model-agnostic%250Ameta-reinforcement%2520learning%2520%2528MAML-RL%2529.%2520Traditional%2520meta-RL%2520typically%2520assumes%250Athat%2520all%2520available%2520tasks%2520are%2520equally%2520important%252C%2520which%2520can%2520lead%2520to%2520task%250Aredundancy%2520when%2520they%2520share%2520significant%2520similarities.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520coreset-based%2520task%2520selection%2520approach%2520that%2520selects%2520a%2520weighted%2520subset%250Aof%2520tasks%2520based%2520on%2520how%2520diverse%2520they%2520are%2520in%2520gradient%2520space%252C%2520prioritizing%2520the%2520most%250Ainformative%2520and%2520diverse%2520tasks.%2520Such%2520task%2520selection%2520reduces%2520the%2520number%2520of%250Asamples%2520needed%2520to%2520find%2520an%2520%2524%255Cepsilon%2524-close%2520stationary%2520solution%2520by%2520a%2520factor%2520of%250AO%25281/%2524%255Cepsilon%2524%2529.%2520Consequently%252C%2520it%2520guarantees%2520a%2520faster%2520adaptation%2520to%2520unseen%250Atasks%2520while%2520focusing%2520training%2520on%2520the%2520most%2520relevant%2520tasks.%2520As%2520a%2520case%2520study%252C%2520we%250Aincorporate%2520task%2520selection%2520to%2520MAML-LQR%2520%2528Toso%2520et%2520al.%252C%25202024b%2529%252C%2520and%2520prove%2520a%2520sample%250Acomplexity%2520reduction%2520proportional%2520to%2520O%2528log%25281/%2524%255Cepsilon%2524%2529%2529%2520when%2520the%2520task%250Aspecific%2520cost%2520also%2520satisfy%2520gradient%2520dominance.%2520Our%2520theoretical%2520guarantees%250Aunderscore%2520task%2520selection%2520as%2520a%2520key%2520component%2520for%2520scalable%2520and%2520sample-efficient%250Ameta-RL.%2520We%2520numerically%2520validate%2520this%2520trend%2520across%2520multiple%2520RL%2520benchmark%250Aproblems%252C%2520illustrating%2520the%2520benefits%2520of%2520task%2520selection%2520beyond%2520the%2520LQR%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coreset-Based%20Task%20Selection%20for%20Sample-Efficient%20Meta-Reinforcement%0A%20%20Learning&entry.906535625=Donglin%20Zhan%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson&entry.1292438233=%20%20We%20study%20task%20selection%20to%20enhance%20sample%20efficiency%20in%20model-agnostic%0Ameta-reinforcement%20learning%20%28MAML-RL%29.%20Traditional%20meta-RL%20typically%20assumes%0Athat%20all%20available%20tasks%20are%20equally%20important%2C%20which%20can%20lead%20to%20task%0Aredundancy%20when%20they%20share%20significant%20similarities.%20To%20address%20this%2C%20we%0Apropose%20a%20coreset-based%20task%20selection%20approach%20that%20selects%20a%20weighted%20subset%0Aof%20tasks%20based%20on%20how%20diverse%20they%20are%20in%20gradient%20space%2C%20prioritizing%20the%20most%0Ainformative%20and%20diverse%20tasks.%20Such%20task%20selection%20reduces%20the%20number%20of%0Asamples%20needed%20to%20find%20an%20%24%5Cepsilon%24-close%20stationary%20solution%20by%20a%20factor%20of%0AO%281/%24%5Cepsilon%24%29.%20Consequently%2C%20it%20guarantees%20a%20faster%20adaptation%20to%20unseen%0Atasks%20while%20focusing%20training%20on%20the%20most%20relevant%20tasks.%20As%20a%20case%20study%2C%20we%0Aincorporate%20task%20selection%20to%20MAML-LQR%20%28Toso%20et%20al.%2C%202024b%29%2C%20and%20prove%20a%20sample%0Acomplexity%20reduction%20proportional%20to%20O%28log%281/%24%5Cepsilon%24%29%29%20when%20the%20task%0Aspecific%20cost%20also%20satisfy%20gradient%20dominance.%20Our%20theoretical%20guarantees%0Aunderscore%20task%20selection%20as%20a%20key%20component%20for%20scalable%20and%20sample-efficient%0Ameta-RL.%20We%20numerically%20validate%20this%20trend%20across%20multiple%20RL%20benchmark%0Aproblems%2C%20illustrating%20the%20benefits%20of%20task%20selection%20beyond%20the%20LQR%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02332v1&entry.124074799=Read"},
{"title": "Test Time Training for 4D Medical Image Interpolation", "author": "Qikang Zhang and Yingjie Lei and Zihao Zheng and Ziyang Chen and Zhonghao Xie", "abstract": "  4D medical image interpolation is essential for improving temporal resolution\nand diagnostic precision in clinical applications. Previous works ignore the\nproblem of distribution shifts, resulting in poor generalization under\ndifferent distribution. A natural solution would be to adapt the model to a new\ntest distribution, but this cannot be done if the test input comes without a\nground truth label. In this paper, we propose a novel test time training\nframework which uses self-supervision to adapt the model to a new distribution\nwithout requiring any labels. Indeed, before performing frame interpolation on\neach test video, the model is trained on the same instance using a\nself-supervised task, such as rotation prediction or image reconstruction. We\nconduct experiments on two publicly available 4D medical image interpolation\ndatasets, Cardiac and 4D-Lung. The experimental results show that the proposed\nmethod achieves significant performance across various evaluation metrics on\nboth datasets. It achieves higher peak signal-to-noise ratio values, 33.73dB on\nCardiac and 34.02dB on 4D-Lung. Our method not only advances 4D medical image\ninterpolation but also provides a template for domain adaptation in other\nfields such as image segmentation and image registration.\n", "link": "http://arxiv.org/abs/2502.02341v1", "date": "2025-02-04", "relevancy": 2.1996, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5577}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5474}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test%20Time%20Training%20for%204D%20Medical%20Image%20Interpolation&body=Title%3A%20Test%20Time%20Training%20for%204D%20Medical%20Image%20Interpolation%0AAuthor%3A%20Qikang%20Zhang%20and%20Yingjie%20Lei%20and%20Zihao%20Zheng%20and%20Ziyang%20Chen%20and%20Zhonghao%20Xie%0AAbstract%3A%20%20%204D%20medical%20image%20interpolation%20is%20essential%20for%20improving%20temporal%20resolution%0Aand%20diagnostic%20precision%20in%20clinical%20applications.%20Previous%20works%20ignore%20the%0Aproblem%20of%20distribution%20shifts%2C%20resulting%20in%20poor%20generalization%20under%0Adifferent%20distribution.%20A%20natural%20solution%20would%20be%20to%20adapt%20the%20model%20to%20a%20new%0Atest%20distribution%2C%20but%20this%20cannot%20be%20done%20if%20the%20test%20input%20comes%20without%20a%0Aground%20truth%20label.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20test%20time%20training%0Aframework%20which%20uses%20self-supervision%20to%20adapt%20the%20model%20to%20a%20new%20distribution%0Awithout%20requiring%20any%20labels.%20Indeed%2C%20before%20performing%20frame%20interpolation%20on%0Aeach%20test%20video%2C%20the%20model%20is%20trained%20on%20the%20same%20instance%20using%20a%0Aself-supervised%20task%2C%20such%20as%20rotation%20prediction%20or%20image%20reconstruction.%20We%0Aconduct%20experiments%20on%20two%20publicly%20available%204D%20medical%20image%20interpolation%0Adatasets%2C%20Cardiac%20and%204D-Lung.%20The%20experimental%20results%20show%20that%20the%20proposed%0Amethod%20achieves%20significant%20performance%20across%20various%20evaluation%20metrics%20on%0Aboth%20datasets.%20It%20achieves%20higher%20peak%20signal-to-noise%20ratio%20values%2C%2033.73dB%20on%0ACardiac%20and%2034.02dB%20on%204D-Lung.%20Our%20method%20not%20only%20advances%204D%20medical%20image%0Ainterpolation%20but%20also%20provides%20a%20template%20for%20domain%20adaptation%20in%20other%0Afields%20such%20as%20image%20segmentation%20and%20image%20registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest%2520Time%2520Training%2520for%25204D%2520Medical%2520Image%2520Interpolation%26entry.906535625%3DQikang%2520Zhang%2520and%2520Yingjie%2520Lei%2520and%2520Zihao%2520Zheng%2520and%2520Ziyang%2520Chen%2520and%2520Zhonghao%2520Xie%26entry.1292438233%3D%2520%25204D%2520medical%2520image%2520interpolation%2520is%2520essential%2520for%2520improving%2520temporal%2520resolution%250Aand%2520diagnostic%2520precision%2520in%2520clinical%2520applications.%2520Previous%2520works%2520ignore%2520the%250Aproblem%2520of%2520distribution%2520shifts%252C%2520resulting%2520in%2520poor%2520generalization%2520under%250Adifferent%2520distribution.%2520A%2520natural%2520solution%2520would%2520be%2520to%2520adapt%2520the%2520model%2520to%2520a%2520new%250Atest%2520distribution%252C%2520but%2520this%2520cannot%2520be%2520done%2520if%2520the%2520test%2520input%2520comes%2520without%2520a%250Aground%2520truth%2520label.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520test%2520time%2520training%250Aframework%2520which%2520uses%2520self-supervision%2520to%2520adapt%2520the%2520model%2520to%2520a%2520new%2520distribution%250Awithout%2520requiring%2520any%2520labels.%2520Indeed%252C%2520before%2520performing%2520frame%2520interpolation%2520on%250Aeach%2520test%2520video%252C%2520the%2520model%2520is%2520trained%2520on%2520the%2520same%2520instance%2520using%2520a%250Aself-supervised%2520task%252C%2520such%2520as%2520rotation%2520prediction%2520or%2520image%2520reconstruction.%2520We%250Aconduct%2520experiments%2520on%2520two%2520publicly%2520available%25204D%2520medical%2520image%2520interpolation%250Adatasets%252C%2520Cardiac%2520and%25204D-Lung.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520proposed%250Amethod%2520achieves%2520significant%2520performance%2520across%2520various%2520evaluation%2520metrics%2520on%250Aboth%2520datasets.%2520It%2520achieves%2520higher%2520peak%2520signal-to-noise%2520ratio%2520values%252C%252033.73dB%2520on%250ACardiac%2520and%252034.02dB%2520on%25204D-Lung.%2520Our%2520method%2520not%2520only%2520advances%25204D%2520medical%2520image%250Ainterpolation%2520but%2520also%2520provides%2520a%2520template%2520for%2520domain%2520adaptation%2520in%2520other%250Afields%2520such%2520as%2520image%2520segmentation%2520and%2520image%2520registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test%20Time%20Training%20for%204D%20Medical%20Image%20Interpolation&entry.906535625=Qikang%20Zhang%20and%20Yingjie%20Lei%20and%20Zihao%20Zheng%20and%20Ziyang%20Chen%20and%20Zhonghao%20Xie&entry.1292438233=%20%204D%20medical%20image%20interpolation%20is%20essential%20for%20improving%20temporal%20resolution%0Aand%20diagnostic%20precision%20in%20clinical%20applications.%20Previous%20works%20ignore%20the%0Aproblem%20of%20distribution%20shifts%2C%20resulting%20in%20poor%20generalization%20under%0Adifferent%20distribution.%20A%20natural%20solution%20would%20be%20to%20adapt%20the%20model%20to%20a%20new%0Atest%20distribution%2C%20but%20this%20cannot%20be%20done%20if%20the%20test%20input%20comes%20without%20a%0Aground%20truth%20label.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20test%20time%20training%0Aframework%20which%20uses%20self-supervision%20to%20adapt%20the%20model%20to%20a%20new%20distribution%0Awithout%20requiring%20any%20labels.%20Indeed%2C%20before%20performing%20frame%20interpolation%20on%0Aeach%20test%20video%2C%20the%20model%20is%20trained%20on%20the%20same%20instance%20using%20a%0Aself-supervised%20task%2C%20such%20as%20rotation%20prediction%20or%20image%20reconstruction.%20We%0Aconduct%20experiments%20on%20two%20publicly%20available%204D%20medical%20image%20interpolation%0Adatasets%2C%20Cardiac%20and%204D-Lung.%20The%20experimental%20results%20show%20that%20the%20proposed%0Amethod%20achieves%20significant%20performance%20across%20various%20evaluation%20metrics%20on%0Aboth%20datasets.%20It%20achieves%20higher%20peak%20signal-to-noise%20ratio%20values%2C%2033.73dB%20on%0ACardiac%20and%2034.02dB%20on%204D-Lung.%20Our%20method%20not%20only%20advances%204D%20medical%20image%0Ainterpolation%20but%20also%20provides%20a%20template%20for%20domain%20adaptation%20in%20other%0Afields%20such%20as%20image%20segmentation%20and%20image%20registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02341v1&entry.124074799=Read"},
{"title": "Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models", "author": "Soroush Seifi and Vaggelis Dorovatas and Daniel Olmeda Reino and Rahaf Aljundi", "abstract": "  Large Vision Language Models (LVLMs) have significant potential to deliver\npersonalized assistance by adapting to individual users' unique needs and\npreferences. Personalization of LVLMs is an emerging area that involves\ncustomizing models to recognize specific object instances and provide tailored\nresponses. However, existing approaches rely on time-consuming test-time\ntraining for each user and object, rendering them impractical. This paper\nproposes a novel, training-free approach to LVLM personalization by leveraging\npre-trained vision foundation models to extract distinct features,\nretrieval-augmented generation (RAG) techniques to recognize instances in the\nvisual input, and visual prompting methods. Our model-agnostic vision toolkit\nenables flexible and efficient personalization without extensive retraining. We\ndemonstrate state-of-the-art results, outperforming conventional training-based\napproaches and establish a new standard for LVLM personalization.\n", "link": "http://arxiv.org/abs/2502.02452v1", "date": "2025-02-04", "relevancy": 2.1948, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models&body=Title%3A%20Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Soroush%20Seifi%20and%20Vaggelis%20Dorovatas%20and%20Daniel%20Olmeda%20Reino%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20significant%20potential%20to%20deliver%0Apersonalized%20assistance%20by%20adapting%20to%20individual%20users%27%20unique%20needs%20and%0Apreferences.%20Personalization%20of%20LVLMs%20is%20an%20emerging%20area%20that%20involves%0Acustomizing%20models%20to%20recognize%20specific%20object%20instances%20and%20provide%20tailored%0Aresponses.%20However%2C%20existing%20approaches%20rely%20on%20time-consuming%20test-time%0Atraining%20for%20each%20user%20and%20object%2C%20rendering%20them%20impractical.%20This%20paper%0Aproposes%20a%20novel%2C%20training-free%20approach%20to%20LVLM%20personalization%20by%20leveraging%0Apre-trained%20vision%20foundation%20models%20to%20extract%20distinct%20features%2C%0Aretrieval-augmented%20generation%20%28RAG%29%20techniques%20to%20recognize%20instances%20in%20the%0Avisual%20input%2C%20and%20visual%20prompting%20methods.%20Our%20model-agnostic%20vision%20toolkit%0Aenables%20flexible%20and%20efficient%20personalization%20without%20extensive%20retraining.%20We%0Ademonstrate%20state-of-the-art%20results%2C%20outperforming%20conventional%20training-based%0Aapproaches%20and%20establish%20a%20new%20standard%20for%20LVLM%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalization%2520Toolkit%253A%2520Training%2520Free%2520Personalization%2520of%2520Large%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DSoroush%2520Seifi%2520and%2520Vaggelis%2520Dorovatas%2520and%2520Daniel%2520Olmeda%2520Reino%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520significant%2520potential%2520to%2520deliver%250Apersonalized%2520assistance%2520by%2520adapting%2520to%2520individual%2520users%2527%2520unique%2520needs%2520and%250Apreferences.%2520Personalization%2520of%2520LVLMs%2520is%2520an%2520emerging%2520area%2520that%2520involves%250Acustomizing%2520models%2520to%2520recognize%2520specific%2520object%2520instances%2520and%2520provide%2520tailored%250Aresponses.%2520However%252C%2520existing%2520approaches%2520rely%2520on%2520time-consuming%2520test-time%250Atraining%2520for%2520each%2520user%2520and%2520object%252C%2520rendering%2520them%2520impractical.%2520This%2520paper%250Aproposes%2520a%2520novel%252C%2520training-free%2520approach%2520to%2520LVLM%2520personalization%2520by%2520leveraging%250Apre-trained%2520vision%2520foundation%2520models%2520to%2520extract%2520distinct%2520features%252C%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520techniques%2520to%2520recognize%2520instances%2520in%2520the%250Avisual%2520input%252C%2520and%2520visual%2520prompting%2520methods.%2520Our%2520model-agnostic%2520vision%2520toolkit%250Aenables%2520flexible%2520and%2520efficient%2520personalization%2520without%2520extensive%2520retraining.%2520We%250Ademonstrate%2520state-of-the-art%2520results%252C%2520outperforming%2520conventional%2520training-based%250Aapproaches%2520and%2520establish%2520a%2520new%2520standard%2520for%2520LVLM%2520personalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models&entry.906535625=Soroush%20Seifi%20and%20Vaggelis%20Dorovatas%20and%20Daniel%20Olmeda%20Reino%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20significant%20potential%20to%20deliver%0Apersonalized%20assistance%20by%20adapting%20to%20individual%20users%27%20unique%20needs%20and%0Apreferences.%20Personalization%20of%20LVLMs%20is%20an%20emerging%20area%20that%20involves%0Acustomizing%20models%20to%20recognize%20specific%20object%20instances%20and%20provide%20tailored%0Aresponses.%20However%2C%20existing%20approaches%20rely%20on%20time-consuming%20test-time%0Atraining%20for%20each%20user%20and%20object%2C%20rendering%20them%20impractical.%20This%20paper%0Aproposes%20a%20novel%2C%20training-free%20approach%20to%20LVLM%20personalization%20by%20leveraging%0Apre-trained%20vision%20foundation%20models%20to%20extract%20distinct%20features%2C%0Aretrieval-augmented%20generation%20%28RAG%29%20techniques%20to%20recognize%20instances%20in%20the%0Avisual%20input%2C%20and%20visual%20prompting%20methods.%20Our%20model-agnostic%20vision%20toolkit%0Aenables%20flexible%20and%20efficient%20personalization%20without%20extensive%20retraining.%20We%0Ademonstrate%20state-of-the-art%20results%2C%20outperforming%20conventional%20training-based%0Aapproaches%20and%20establish%20a%20new%20standard%20for%20LVLM%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02452v1&entry.124074799=Read"},
{"title": "A Revisit of Total Correlation in Disentangled Variational Auto-Encoder\n  with Partial Disentanglement", "author": "Chengrui Li and Yunmiao Wang and Yule Wang and Weihan Li and Dieter Jaeger and Anqi Wu", "abstract": "  A fully disentangled variational auto-encoder (VAE) aims to identify\ndisentangled latent components from observations. However, enforcing full\nindependence between all latent components may be too strict for certain\ndatasets. In some cases, multiple factors may be entangled together in a\nnon-separable manner, or a single independent semantic meaning could be\nrepresented by multiple latent components within a higher-dimensional manifold.\nTo address such scenarios with greater flexibility, we develop the Partially\nDisentangled VAE (PDisVAE), which generalizes the total correlation (TC) term\nin fully disentangled VAEs to a partial correlation (PC) term. This framework\ncan handle group-wise independence and can naturally reduce to either the\nstandard VAE or the fully disentangled VAE. Validation through three synthetic\nexperiments demonstrates the correctness and practicality of PDisVAE. When\napplied to real-world datasets, PDisVAE discovers valuable information that is\ndifficult to find using fully disentangled VAEs, implying its versatility and\neffectiveness.\n", "link": "http://arxiv.org/abs/2502.02279v1", "date": "2025-02-04", "relevancy": 2.1894, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4553}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Revisit%20of%20Total%20Correlation%20in%20Disentangled%20Variational%20Auto-Encoder%0A%20%20with%20Partial%20Disentanglement&body=Title%3A%20A%20Revisit%20of%20Total%20Correlation%20in%20Disentangled%20Variational%20Auto-Encoder%0A%20%20with%20Partial%20Disentanglement%0AAuthor%3A%20Chengrui%20Li%20and%20Yunmiao%20Wang%20and%20Yule%20Wang%20and%20Weihan%20Li%20and%20Dieter%20Jaeger%20and%20Anqi%20Wu%0AAbstract%3A%20%20%20A%20fully%20disentangled%20variational%20auto-encoder%20%28VAE%29%20aims%20to%20identify%0Adisentangled%20latent%20components%20from%20observations.%20However%2C%20enforcing%20full%0Aindependence%20between%20all%20latent%20components%20may%20be%20too%20strict%20for%20certain%0Adatasets.%20In%20some%20cases%2C%20multiple%20factors%20may%20be%20entangled%20together%20in%20a%0Anon-separable%20manner%2C%20or%20a%20single%20independent%20semantic%20meaning%20could%20be%0Arepresented%20by%20multiple%20latent%20components%20within%20a%20higher-dimensional%20manifold.%0ATo%20address%20such%20scenarios%20with%20greater%20flexibility%2C%20we%20develop%20the%20Partially%0ADisentangled%20VAE%20%28PDisVAE%29%2C%20which%20generalizes%20the%20total%20correlation%20%28TC%29%20term%0Ain%20fully%20disentangled%20VAEs%20to%20a%20partial%20correlation%20%28PC%29%20term.%20This%20framework%0Acan%20handle%20group-wise%20independence%20and%20can%20naturally%20reduce%20to%20either%20the%0Astandard%20VAE%20or%20the%20fully%20disentangled%20VAE.%20Validation%20through%20three%20synthetic%0Aexperiments%20demonstrates%20the%20correctness%20and%20practicality%20of%20PDisVAE.%20When%0Aapplied%20to%20real-world%20datasets%2C%20PDisVAE%20discovers%20valuable%20information%20that%20is%0Adifficult%20to%20find%20using%20fully%20disentangled%20VAEs%2C%20implying%20its%20versatility%20and%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Revisit%2520of%2520Total%2520Correlation%2520in%2520Disentangled%2520Variational%2520Auto-Encoder%250A%2520%2520with%2520Partial%2520Disentanglement%26entry.906535625%3DChengrui%2520Li%2520and%2520Yunmiao%2520Wang%2520and%2520Yule%2520Wang%2520and%2520Weihan%2520Li%2520and%2520Dieter%2520Jaeger%2520and%2520Anqi%2520Wu%26entry.1292438233%3D%2520%2520A%2520fully%2520disentangled%2520variational%2520auto-encoder%2520%2528VAE%2529%2520aims%2520to%2520identify%250Adisentangled%2520latent%2520components%2520from%2520observations.%2520However%252C%2520enforcing%2520full%250Aindependence%2520between%2520all%2520latent%2520components%2520may%2520be%2520too%2520strict%2520for%2520certain%250Adatasets.%2520In%2520some%2520cases%252C%2520multiple%2520factors%2520may%2520be%2520entangled%2520together%2520in%2520a%250Anon-separable%2520manner%252C%2520or%2520a%2520single%2520independent%2520semantic%2520meaning%2520could%2520be%250Arepresented%2520by%2520multiple%2520latent%2520components%2520within%2520a%2520higher-dimensional%2520manifold.%250ATo%2520address%2520such%2520scenarios%2520with%2520greater%2520flexibility%252C%2520we%2520develop%2520the%2520Partially%250ADisentangled%2520VAE%2520%2528PDisVAE%2529%252C%2520which%2520generalizes%2520the%2520total%2520correlation%2520%2528TC%2529%2520term%250Ain%2520fully%2520disentangled%2520VAEs%2520to%2520a%2520partial%2520correlation%2520%2528PC%2529%2520term.%2520This%2520framework%250Acan%2520handle%2520group-wise%2520independence%2520and%2520can%2520naturally%2520reduce%2520to%2520either%2520the%250Astandard%2520VAE%2520or%2520the%2520fully%2520disentangled%2520VAE.%2520Validation%2520through%2520three%2520synthetic%250Aexperiments%2520demonstrates%2520the%2520correctness%2520and%2520practicality%2520of%2520PDisVAE.%2520When%250Aapplied%2520to%2520real-world%2520datasets%252C%2520PDisVAE%2520discovers%2520valuable%2520information%2520that%2520is%250Adifficult%2520to%2520find%2520using%2520fully%2520disentangled%2520VAEs%252C%2520implying%2520its%2520versatility%2520and%250Aeffectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Revisit%20of%20Total%20Correlation%20in%20Disentangled%20Variational%20Auto-Encoder%0A%20%20with%20Partial%20Disentanglement&entry.906535625=Chengrui%20Li%20and%20Yunmiao%20Wang%20and%20Yule%20Wang%20and%20Weihan%20Li%20and%20Dieter%20Jaeger%20and%20Anqi%20Wu&entry.1292438233=%20%20A%20fully%20disentangled%20variational%20auto-encoder%20%28VAE%29%20aims%20to%20identify%0Adisentangled%20latent%20components%20from%20observations.%20However%2C%20enforcing%20full%0Aindependence%20between%20all%20latent%20components%20may%20be%20too%20strict%20for%20certain%0Adatasets.%20In%20some%20cases%2C%20multiple%20factors%20may%20be%20entangled%20together%20in%20a%0Anon-separable%20manner%2C%20or%20a%20single%20independent%20semantic%20meaning%20could%20be%0Arepresented%20by%20multiple%20latent%20components%20within%20a%20higher-dimensional%20manifold.%0ATo%20address%20such%20scenarios%20with%20greater%20flexibility%2C%20we%20develop%20the%20Partially%0ADisentangled%20VAE%20%28PDisVAE%29%2C%20which%20generalizes%20the%20total%20correlation%20%28TC%29%20term%0Ain%20fully%20disentangled%20VAEs%20to%20a%20partial%20correlation%20%28PC%29%20term.%20This%20framework%0Acan%20handle%20group-wise%20independence%20and%20can%20naturally%20reduce%20to%20either%20the%0Astandard%20VAE%20or%20the%20fully%20disentangled%20VAE.%20Validation%20through%20three%20synthetic%0Aexperiments%20demonstrates%20the%20correctness%20and%20practicality%20of%20PDisVAE.%20When%0Aapplied%20to%20real-world%20datasets%2C%20PDisVAE%20discovers%20valuable%20information%20that%20is%0Adifficult%20to%20find%20using%20fully%20disentangled%20VAEs%2C%20implying%20its%20versatility%20and%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02279v1&entry.124074799=Read"},
{"title": "A Self-Supervised Framework for Improved Generalisability in Ultrasound\n  B-mode Image Segmentation", "author": "Edward Ellis and Andrew Bulpitt and Nasim Parsa and Michael F Byrne and Sharib Ali", "abstract": "  Ultrasound (US) imaging is clinically invaluable due to its noninvasive and\nsafe nature. However, interpreting US images is challenging, requires\nsignificant expertise, and time, and is often prone to errors. Deep learning\noffers assistive solutions such as segmentation. Supervised methods rely on\nlarge, high-quality, and consistently labeled datasets, which are challenging\nto curate. Moreover, these methods tend to underperform on out-of-distribution\ndata, limiting their clinical utility. Self-supervised learning (SSL) has\nemerged as a promising alternative, leveraging unlabeled data to enhance model\nperformance and generalisability. We introduce a contrastive SSL approach\ntailored for B-mode US images, incorporating a novel Relation Contrastive Loss\n(RCL). RCL encourages learning of distinct features by differentiating positive\nand negative sample pairs through a learnable metric. Additionally, we propose\nspatial and frequency-based augmentation strategies for the representation\nlearning on US images. Our approach significantly outperforms traditional\nsupervised segmentation methods across three public breast US datasets,\nparticularly in data-limited scenarios. Notable improvements on the Dice\nsimilarity metric include a 4% increase on 20% and 50% of the BUSI dataset,\nnearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4%\nand 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.\nFurthermore, we demonstrate superior generalisability on the\nout-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6%\ncompared to the supervised baseline using 20% and 50% of the BUSI and BrEaST\ntraining data, respectively. Our research highlights that domain-inspired SSL\ncan improve US segmentation, especially under data-limited conditions.\n", "link": "http://arxiv.org/abs/2502.02489v1", "date": "2025-02-04", "relevancy": 2.1823, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Self-Supervised%20Framework%20for%20Improved%20Generalisability%20in%20Ultrasound%0A%20%20B-mode%20Image%20Segmentation&body=Title%3A%20A%20Self-Supervised%20Framework%20for%20Improved%20Generalisability%20in%20Ultrasound%0A%20%20B-mode%20Image%20Segmentation%0AAuthor%3A%20Edward%20Ellis%20and%20Andrew%20Bulpitt%20and%20Nasim%20Parsa%20and%20Michael%20F%20Byrne%20and%20Sharib%20Ali%0AAbstract%3A%20%20%20Ultrasound%20%28US%29%20imaging%20is%20clinically%20invaluable%20due%20to%20its%20noninvasive%20and%0Asafe%20nature.%20However%2C%20interpreting%20US%20images%20is%20challenging%2C%20requires%0Asignificant%20expertise%2C%20and%20time%2C%20and%20is%20often%20prone%20to%20errors.%20Deep%20learning%0Aoffers%20assistive%20solutions%20such%20as%20segmentation.%20Supervised%20methods%20rely%20on%0Alarge%2C%20high-quality%2C%20and%20consistently%20labeled%20datasets%2C%20which%20are%20challenging%0Ato%20curate.%20Moreover%2C%20these%20methods%20tend%20to%20underperform%20on%20out-of-distribution%0Adata%2C%20limiting%20their%20clinical%20utility.%20Self-supervised%20learning%20%28SSL%29%20has%0Aemerged%20as%20a%20promising%20alternative%2C%20leveraging%20unlabeled%20data%20to%20enhance%20model%0Aperformance%20and%20generalisability.%20We%20introduce%20a%20contrastive%20SSL%20approach%0Atailored%20for%20B-mode%20US%20images%2C%20incorporating%20a%20novel%20Relation%20Contrastive%20Loss%0A%28RCL%29.%20RCL%20encourages%20learning%20of%20distinct%20features%20by%20differentiating%20positive%0Aand%20negative%20sample%20pairs%20through%20a%20learnable%20metric.%20Additionally%2C%20we%20propose%0Aspatial%20and%20frequency-based%20augmentation%20strategies%20for%20the%20representation%0Alearning%20on%20US%20images.%20Our%20approach%20significantly%20outperforms%20traditional%0Asupervised%20segmentation%20methods%20across%20three%20public%20breast%20US%20datasets%2C%0Aparticularly%20in%20data-limited%20scenarios.%20Notable%20improvements%20on%20the%20Dice%0Asimilarity%20metric%20include%20a%204%25%20increase%20on%2020%25%20and%2050%25%20of%20the%20BUSI%20dataset%2C%0Anearly%206%25%20and%209%25%20improvements%20on%2020%25%20and%2050%25%20of%20the%20BrEaST%20dataset%2C%20and%206.4%25%0Aand%203.7%25%20improvements%20on%2020%25%20and%2050%25%20of%20the%20UDIAT%20dataset%2C%20respectively.%0AFurthermore%2C%20we%20demonstrate%20superior%20generalisability%20on%20the%0Aout-of-distribution%20UDIAT%20dataset%20with%20performance%20boosts%20of%2020.6%25%20and%2013.6%25%0Acompared%20to%20the%20supervised%20baseline%20using%2020%25%20and%2050%25%20of%20the%20BUSI%20and%20BrEaST%0Atraining%20data%2C%20respectively.%20Our%20research%20highlights%20that%20domain-inspired%20SSL%0Acan%20improve%20US%20segmentation%2C%20especially%20under%20data-limited%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Self-Supervised%2520Framework%2520for%2520Improved%2520Generalisability%2520in%2520Ultrasound%250A%2520%2520B-mode%2520Image%2520Segmentation%26entry.906535625%3DEdward%2520Ellis%2520and%2520Andrew%2520Bulpitt%2520and%2520Nasim%2520Parsa%2520and%2520Michael%2520F%2520Byrne%2520and%2520Sharib%2520Ali%26entry.1292438233%3D%2520%2520Ultrasound%2520%2528US%2529%2520imaging%2520is%2520clinically%2520invaluable%2520due%2520to%2520its%2520noninvasive%2520and%250Asafe%2520nature.%2520However%252C%2520interpreting%2520US%2520images%2520is%2520challenging%252C%2520requires%250Asignificant%2520expertise%252C%2520and%2520time%252C%2520and%2520is%2520often%2520prone%2520to%2520errors.%2520Deep%2520learning%250Aoffers%2520assistive%2520solutions%2520such%2520as%2520segmentation.%2520Supervised%2520methods%2520rely%2520on%250Alarge%252C%2520high-quality%252C%2520and%2520consistently%2520labeled%2520datasets%252C%2520which%2520are%2520challenging%250Ato%2520curate.%2520Moreover%252C%2520these%2520methods%2520tend%2520to%2520underperform%2520on%2520out-of-distribution%250Adata%252C%2520limiting%2520their%2520clinical%2520utility.%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%250Aemerged%2520as%2520a%2520promising%2520alternative%252C%2520leveraging%2520unlabeled%2520data%2520to%2520enhance%2520model%250Aperformance%2520and%2520generalisability.%2520We%2520introduce%2520a%2520contrastive%2520SSL%2520approach%250Atailored%2520for%2520B-mode%2520US%2520images%252C%2520incorporating%2520a%2520novel%2520Relation%2520Contrastive%2520Loss%250A%2528RCL%2529.%2520RCL%2520encourages%2520learning%2520of%2520distinct%2520features%2520by%2520differentiating%2520positive%250Aand%2520negative%2520sample%2520pairs%2520through%2520a%2520learnable%2520metric.%2520Additionally%252C%2520we%2520propose%250Aspatial%2520and%2520frequency-based%2520augmentation%2520strategies%2520for%2520the%2520representation%250Alearning%2520on%2520US%2520images.%2520Our%2520approach%2520significantly%2520outperforms%2520traditional%250Asupervised%2520segmentation%2520methods%2520across%2520three%2520public%2520breast%2520US%2520datasets%252C%250Aparticularly%2520in%2520data-limited%2520scenarios.%2520Notable%2520improvements%2520on%2520the%2520Dice%250Asimilarity%2520metric%2520include%2520a%25204%2525%2520increase%2520on%252020%2525%2520and%252050%2525%2520of%2520the%2520BUSI%2520dataset%252C%250Anearly%25206%2525%2520and%25209%2525%2520improvements%2520on%252020%2525%2520and%252050%2525%2520of%2520the%2520BrEaST%2520dataset%252C%2520and%25206.4%2525%250Aand%25203.7%2525%2520improvements%2520on%252020%2525%2520and%252050%2525%2520of%2520the%2520UDIAT%2520dataset%252C%2520respectively.%250AFurthermore%252C%2520we%2520demonstrate%2520superior%2520generalisability%2520on%2520the%250Aout-of-distribution%2520UDIAT%2520dataset%2520with%2520performance%2520boosts%2520of%252020.6%2525%2520and%252013.6%2525%250Acompared%2520to%2520the%2520supervised%2520baseline%2520using%252020%2525%2520and%252050%2525%2520of%2520the%2520BUSI%2520and%2520BrEaST%250Atraining%2520data%252C%2520respectively.%2520Our%2520research%2520highlights%2520that%2520domain-inspired%2520SSL%250Acan%2520improve%2520US%2520segmentation%252C%2520especially%2520under%2520data-limited%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Supervised%20Framework%20for%20Improved%20Generalisability%20in%20Ultrasound%0A%20%20B-mode%20Image%20Segmentation&entry.906535625=Edward%20Ellis%20and%20Andrew%20Bulpitt%20and%20Nasim%20Parsa%20and%20Michael%20F%20Byrne%20and%20Sharib%20Ali&entry.1292438233=%20%20Ultrasound%20%28US%29%20imaging%20is%20clinically%20invaluable%20due%20to%20its%20noninvasive%20and%0Asafe%20nature.%20However%2C%20interpreting%20US%20images%20is%20challenging%2C%20requires%0Asignificant%20expertise%2C%20and%20time%2C%20and%20is%20often%20prone%20to%20errors.%20Deep%20learning%0Aoffers%20assistive%20solutions%20such%20as%20segmentation.%20Supervised%20methods%20rely%20on%0Alarge%2C%20high-quality%2C%20and%20consistently%20labeled%20datasets%2C%20which%20are%20challenging%0Ato%20curate.%20Moreover%2C%20these%20methods%20tend%20to%20underperform%20on%20out-of-distribution%0Adata%2C%20limiting%20their%20clinical%20utility.%20Self-supervised%20learning%20%28SSL%29%20has%0Aemerged%20as%20a%20promising%20alternative%2C%20leveraging%20unlabeled%20data%20to%20enhance%20model%0Aperformance%20and%20generalisability.%20We%20introduce%20a%20contrastive%20SSL%20approach%0Atailored%20for%20B-mode%20US%20images%2C%20incorporating%20a%20novel%20Relation%20Contrastive%20Loss%0A%28RCL%29.%20RCL%20encourages%20learning%20of%20distinct%20features%20by%20differentiating%20positive%0Aand%20negative%20sample%20pairs%20through%20a%20learnable%20metric.%20Additionally%2C%20we%20propose%0Aspatial%20and%20frequency-based%20augmentation%20strategies%20for%20the%20representation%0Alearning%20on%20US%20images.%20Our%20approach%20significantly%20outperforms%20traditional%0Asupervised%20segmentation%20methods%20across%20three%20public%20breast%20US%20datasets%2C%0Aparticularly%20in%20data-limited%20scenarios.%20Notable%20improvements%20on%20the%20Dice%0Asimilarity%20metric%20include%20a%204%25%20increase%20on%2020%25%20and%2050%25%20of%20the%20BUSI%20dataset%2C%0Anearly%206%25%20and%209%25%20improvements%20on%2020%25%20and%2050%25%20of%20the%20BrEaST%20dataset%2C%20and%206.4%25%0Aand%203.7%25%20improvements%20on%2020%25%20and%2050%25%20of%20the%20UDIAT%20dataset%2C%20respectively.%0AFurthermore%2C%20we%20demonstrate%20superior%20generalisability%20on%20the%0Aout-of-distribution%20UDIAT%20dataset%20with%20performance%20boosts%20of%2020.6%25%20and%2013.6%25%0Acompared%20to%20the%20supervised%20baseline%20using%2020%25%20and%2050%25%20of%20the%20BUSI%20and%20BrEaST%0Atraining%20data%2C%20respectively.%20Our%20research%20highlights%20that%20domain-inspired%20SSL%0Acan%20improve%20US%20segmentation%2C%20especially%20under%20data-limited%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02489v1&entry.124074799=Read"},
{"title": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency", "author": "Qianhao Yuan and Yanjiang Liu and Yaojie Lu and Hongyu Lin and Ben He and Xianpei Han and Le Sun", "abstract": "  Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA.\n", "link": "http://arxiv.org/abs/2502.02458v1", "date": "2025-02-04", "relevancy": 2.1811, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAISA%3A%20Towards%20Multimodal%20Large%20Language%20Models%20with%20Both%20Training%20and%0A%20%20Inference%20Efficiency&body=Title%3A%20SAISA%3A%20Towards%20Multimodal%20Large%20Language%20Models%20with%20Both%20Training%20and%0A%20%20Inference%20Efficiency%0AAuthor%3A%20Qianhao%20Yuan%20and%20Yanjiang%20Liu%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20mainly%20fall%20into%20two%20architectures%2C%0Aeach%20involving%20a%20trade-off%20between%20training%20and%20inference%20efficiency%3A%20embedding%0Aspace%20alignment%20%28e.g.%2C%20LLaVA-1.5%29%20is%20inefficient%20during%20inference%2C%20while%0Across-attention%20space%20alignment%20%28e.g.%2C%20Flamingo%29%20is%20inefficient%20in%20training.%20In%0Athis%20paper%2C%20we%20compare%20these%20two%20architectures%20and%20identify%20the%20key%20factors%20for%0Abuilding%20efficient%20MLLMs.%20A%20primary%20difference%20between%20them%20lies%20in%20how%0Aattention%20is%20applied%20to%20visual%20tokens%2C%20particularly%20in%20their%20interactions%20with%0Aeach%20other.%20To%20investigate%20whether%20attention%20among%20visual%20tokens%20is%20necessary%2C%0Awe%20propose%20a%20new%20self-attention%20mechanism%2C%20NAAViT%20%28%5Ctextbf%7BN%7Do%0A%5Ctextbf%7BA%7Dttention%20%5Ctextbf%7BA%7Dmong%20%5Ctextbf%7BVi%7Dsual%20%5Ctextbf%7BT%7Dokens%29%2C%20which%0Aeliminates%20this%20type%20of%20attention.%20Our%20pilot%20experiment%20on%20LLaVA-1.5%20shows%20that%0Aattention%20among%20visual%20tokens%20is%20highly%20redundant.%20Based%20on%20these%20insights%2C%20we%0Aintroduce%20SAISA%20%28%5Ctextbf%7BS%7Delf-%5Ctextbf%7BA%7Dttention%20%5Ctextbf%7BI%7Dnput%20%5Ctextbf%7BS%7Dpace%0A%5Ctextbf%7BA%7Dlignment%29%2C%20a%20novel%20architecture%20that%20enhance%20both%20training%20and%0Ainference%20efficiency.%20SAISA%20directly%20aligns%20visual%20features%20with%20the%20input%0Aspaces%20of%20NAAViT%20self-attention%20blocks%2C%20reducing%20computational%20overhead%20in%20both%0Aself-attention%20blocks%20and%20feed-forward%20networks%20%28FFNs%29.%20Using%20the%20same%0Aconfiguration%20as%20LLaVA-1.5%2C%20SAISA%20reduces%20inference%20FLOPs%20by%2066%5C%25%20and%20training%0Abudget%20by%2026%5C%25%2C%20while%20achieving%20superior%20performance%20in%20terms%20of%20accuracy.%0AComprehensive%20ablation%20studies%20further%20validate%20the%20effectiveness%20of%20SAISA%0Aacross%20various%20LLMs%20and%20visual%20encoders.%20The%20code%20and%20model%20will%20be%20publicly%0Aavailable%20at%20https%3A//github.com/icip-cas/SAISA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAISA%253A%2520Towards%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Both%2520Training%2520and%250A%2520%2520Inference%2520Efficiency%26entry.906535625%3DQianhao%2520Yuan%2520and%2520Yanjiang%2520Liu%2520and%2520Yaojie%2520Lu%2520and%2520Hongyu%2520Lin%2520and%2520Ben%2520He%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520mainly%2520fall%2520into%2520two%2520architectures%252C%250Aeach%2520involving%2520a%2520trade-off%2520between%2520training%2520and%2520inference%2520efficiency%253A%2520embedding%250Aspace%2520alignment%2520%2528e.g.%252C%2520LLaVA-1.5%2529%2520is%2520inefficient%2520during%2520inference%252C%2520while%250Across-attention%2520space%2520alignment%2520%2528e.g.%252C%2520Flamingo%2529%2520is%2520inefficient%2520in%2520training.%2520In%250Athis%2520paper%252C%2520we%2520compare%2520these%2520two%2520architectures%2520and%2520identify%2520the%2520key%2520factors%2520for%250Abuilding%2520efficient%2520MLLMs.%2520A%2520primary%2520difference%2520between%2520them%2520lies%2520in%2520how%250Aattention%2520is%2520applied%2520to%2520visual%2520tokens%252C%2520particularly%2520in%2520their%2520interactions%2520with%250Aeach%2520other.%2520To%2520investigate%2520whether%2520attention%2520among%2520visual%2520tokens%2520is%2520necessary%252C%250Awe%2520propose%2520a%2520new%2520self-attention%2520mechanism%252C%2520NAAViT%2520%2528%255Ctextbf%257BN%257Do%250A%255Ctextbf%257BA%257Dttention%2520%255Ctextbf%257BA%257Dmong%2520%255Ctextbf%257BVi%257Dsual%2520%255Ctextbf%257BT%257Dokens%2529%252C%2520which%250Aeliminates%2520this%2520type%2520of%2520attention.%2520Our%2520pilot%2520experiment%2520on%2520LLaVA-1.5%2520shows%2520that%250Aattention%2520among%2520visual%2520tokens%2520is%2520highly%2520redundant.%2520Based%2520on%2520these%2520insights%252C%2520we%250Aintroduce%2520SAISA%2520%2528%255Ctextbf%257BS%257Delf-%255Ctextbf%257BA%257Dttention%2520%255Ctextbf%257BI%257Dnput%2520%255Ctextbf%257BS%257Dpace%250A%255Ctextbf%257BA%257Dlignment%2529%252C%2520a%2520novel%2520architecture%2520that%2520enhance%2520both%2520training%2520and%250Ainference%2520efficiency.%2520SAISA%2520directly%2520aligns%2520visual%2520features%2520with%2520the%2520input%250Aspaces%2520of%2520NAAViT%2520self-attention%2520blocks%252C%2520reducing%2520computational%2520overhead%2520in%2520both%250Aself-attention%2520blocks%2520and%2520feed-forward%2520networks%2520%2528FFNs%2529.%2520Using%2520the%2520same%250Aconfiguration%2520as%2520LLaVA-1.5%252C%2520SAISA%2520reduces%2520inference%2520FLOPs%2520by%252066%255C%2525%2520and%2520training%250Abudget%2520by%252026%255C%2525%252C%2520while%2520achieving%2520superior%2520performance%2520in%2520terms%2520of%2520accuracy.%250AComprehensive%2520ablation%2520studies%2520further%2520validate%2520the%2520effectiveness%2520of%2520SAISA%250Aacross%2520various%2520LLMs%2520and%2520visual%2520encoders.%2520The%2520code%2520and%2520model%2520will%2520be%2520publicly%250Aavailable%2520at%2520https%253A//github.com/icip-cas/SAISA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAISA%3A%20Towards%20Multimodal%20Large%20Language%20Models%20with%20Both%20Training%20and%0A%20%20Inference%20Efficiency&entry.906535625=Qianhao%20Yuan%20and%20Yanjiang%20Liu%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20mainly%20fall%20into%20two%20architectures%2C%0Aeach%20involving%20a%20trade-off%20between%20training%20and%20inference%20efficiency%3A%20embedding%0Aspace%20alignment%20%28e.g.%2C%20LLaVA-1.5%29%20is%20inefficient%20during%20inference%2C%20while%0Across-attention%20space%20alignment%20%28e.g.%2C%20Flamingo%29%20is%20inefficient%20in%20training.%20In%0Athis%20paper%2C%20we%20compare%20these%20two%20architectures%20and%20identify%20the%20key%20factors%20for%0Abuilding%20efficient%20MLLMs.%20A%20primary%20difference%20between%20them%20lies%20in%20how%0Aattention%20is%20applied%20to%20visual%20tokens%2C%20particularly%20in%20their%20interactions%20with%0Aeach%20other.%20To%20investigate%20whether%20attention%20among%20visual%20tokens%20is%20necessary%2C%0Awe%20propose%20a%20new%20self-attention%20mechanism%2C%20NAAViT%20%28%5Ctextbf%7BN%7Do%0A%5Ctextbf%7BA%7Dttention%20%5Ctextbf%7BA%7Dmong%20%5Ctextbf%7BVi%7Dsual%20%5Ctextbf%7BT%7Dokens%29%2C%20which%0Aeliminates%20this%20type%20of%20attention.%20Our%20pilot%20experiment%20on%20LLaVA-1.5%20shows%20that%0Aattention%20among%20visual%20tokens%20is%20highly%20redundant.%20Based%20on%20these%20insights%2C%20we%0Aintroduce%20SAISA%20%28%5Ctextbf%7BS%7Delf-%5Ctextbf%7BA%7Dttention%20%5Ctextbf%7BI%7Dnput%20%5Ctextbf%7BS%7Dpace%0A%5Ctextbf%7BA%7Dlignment%29%2C%20a%20novel%20architecture%20that%20enhance%20both%20training%20and%0Ainference%20efficiency.%20SAISA%20directly%20aligns%20visual%20features%20with%20the%20input%0Aspaces%20of%20NAAViT%20self-attention%20blocks%2C%20reducing%20computational%20overhead%20in%20both%0Aself-attention%20blocks%20and%20feed-forward%20networks%20%28FFNs%29.%20Using%20the%20same%0Aconfiguration%20as%20LLaVA-1.5%2C%20SAISA%20reduces%20inference%20FLOPs%20by%2066%5C%25%20and%20training%0Abudget%20by%2026%5C%25%2C%20while%20achieving%20superior%20performance%20in%20terms%20of%20accuracy.%0AComprehensive%20ablation%20studies%20further%20validate%20the%20effectiveness%20of%20SAISA%0Aacross%20various%20LLMs%20and%20visual%20encoders.%20The%20code%20and%20model%20will%20be%20publicly%0Aavailable%20at%20https%3A//github.com/icip-cas/SAISA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02458v1&entry.124074799=Read"},
{"title": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement", "author": "Soheil Abbasloo", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning.\n", "link": "http://arxiv.org/abs/2502.02573v1", "date": "2025-02-04", "relevancy": 2.1732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Language%20Models%20Up%20to%20Sequential%20Optimization%20Problems%3F%20From%0A%20%20Evaluation%20to%20a%20Hegelian-Inspired%20Enhancement&body=Title%3A%20Are%20Language%20Models%20Up%20to%20Sequential%20Optimization%20Problems%3F%20From%0A%20%20Evaluation%20to%20a%20Hegelian-Inspired%20Enhancement%0AAuthor%3A%20Soheil%20Abbasloo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Anumerous%20fields%2C%20presenting%20an%20opportunity%20to%20revolutionize%20optimization%0Aproblem-solving%2C%20a%20crucial%2C%20ubiquitous%2C%20and%20complex%20domain.%20This%20paper%20explores%0Athe%20proficiency%20of%20LLMs%20in%20handling%20Sequential%20Optimization%20Problems%20%28SOPs%29.%20We%0Aintroduce%20WorldGen%2C%20a%20dynamic%20framework%20for%20generating%20unseen%20SOPs%20with%0Acontrollable%20complexities%2C%20to%20evaluate%20LLM%20performance.%20Our%20initial%0Aobservations%20reveal%20that%20while%20LLMs%20perform%20well%20on%20simple%20SOPs%2C%20their%0Aperformance%20significantly%20degrades%20with%20increased%20complexity.%20Motivated%20by%0Athis%2C%20we%20revisit%20philosophical%20hypotheses%20on%20reasoning%20to%20enhance%20LLM%0Aperformance.%20Inspired%20by%20the%20influential%20framework%20of%20Hegelian%20Dialectics%2C%20we%0Apropose%20ACE%2C%20demonstrating%20how%20the%20performance%20of%20LLMs%20in%20SOP%20contexts%20can%20be%0Asignificantly%20improved%20without%20any%20retraining%20or%20further%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Language%2520Models%2520Up%2520to%2520Sequential%2520Optimization%2520Problems%253F%2520From%250A%2520%2520Evaluation%2520to%2520a%2520Hegelian-Inspired%2520Enhancement%26entry.906535625%3DSoheil%2520Abbasloo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%250Anumerous%2520fields%252C%2520presenting%2520an%2520opportunity%2520to%2520revolutionize%2520optimization%250Aproblem-solving%252C%2520a%2520crucial%252C%2520ubiquitous%252C%2520and%2520complex%2520domain.%2520This%2520paper%2520explores%250Athe%2520proficiency%2520of%2520LLMs%2520in%2520handling%2520Sequential%2520Optimization%2520Problems%2520%2528SOPs%2529.%2520We%250Aintroduce%2520WorldGen%252C%2520a%2520dynamic%2520framework%2520for%2520generating%2520unseen%2520SOPs%2520with%250Acontrollable%2520complexities%252C%2520to%2520evaluate%2520LLM%2520performance.%2520Our%2520initial%250Aobservations%2520reveal%2520that%2520while%2520LLMs%2520perform%2520well%2520on%2520simple%2520SOPs%252C%2520their%250Aperformance%2520significantly%2520degrades%2520with%2520increased%2520complexity.%2520Motivated%2520by%250Athis%252C%2520we%2520revisit%2520philosophical%2520hypotheses%2520on%2520reasoning%2520to%2520enhance%2520LLM%250Aperformance.%2520Inspired%2520by%2520the%2520influential%2520framework%2520of%2520Hegelian%2520Dialectics%252C%2520we%250Apropose%2520ACE%252C%2520demonstrating%2520how%2520the%2520performance%2520of%2520LLMs%2520in%2520SOP%2520contexts%2520can%2520be%250Asignificantly%2520improved%2520without%2520any%2520retraining%2520or%2520further%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Language%20Models%20Up%20to%20Sequential%20Optimization%20Problems%3F%20From%0A%20%20Evaluation%20to%20a%20Hegelian-Inspired%20Enhancement&entry.906535625=Soheil%20Abbasloo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Anumerous%20fields%2C%20presenting%20an%20opportunity%20to%20revolutionize%20optimization%0Aproblem-solving%2C%20a%20crucial%2C%20ubiquitous%2C%20and%20complex%20domain.%20This%20paper%20explores%0Athe%20proficiency%20of%20LLMs%20in%20handling%20Sequential%20Optimization%20Problems%20%28SOPs%29.%20We%0Aintroduce%20WorldGen%2C%20a%20dynamic%20framework%20for%20generating%20unseen%20SOPs%20with%0Acontrollable%20complexities%2C%20to%20evaluate%20LLM%20performance.%20Our%20initial%0Aobservations%20reveal%20that%20while%20LLMs%20perform%20well%20on%20simple%20SOPs%2C%20their%0Aperformance%20significantly%20degrades%20with%20increased%20complexity.%20Motivated%20by%0Athis%2C%20we%20revisit%20philosophical%20hypotheses%20on%20reasoning%20to%20enhance%20LLM%0Aperformance.%20Inspired%20by%20the%20influential%20framework%20of%20Hegelian%20Dialectics%2C%20we%0Apropose%20ACE%2C%20demonstrating%20how%20the%20performance%20of%20LLMs%20in%20SOP%20contexts%20can%20be%0Asignificantly%20improved%20without%20any%20retraining%20or%20further%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02573v1&entry.124074799=Read"},
{"title": "The Skin Game: Revolutionizing Standards for AI Dermatology Model\n  Comparison", "author": "\u0141ukasz Mi\u0119tkiewicz and Leon Ciechanowski and Dariusz Jemielniak", "abstract": "  Deep Learning approaches in dermatological image classification have shown\npromising results, yet the field faces significant methodological challenges\nthat impede proper evaluation. This paper presents a dual contribution: first,\na systematic analysis of current methodological practices in skin disease\nclassification research, revealing substantial inconsistencies in data\npreparation, augmentation strategies, and performance reporting; second, a\ncomprehensive training and evaluation framework demonstrated through\nexperiments with the DINOv2-Large vision transformer across three benchmark\ndatasets (HAM10000, DermNet, ISIC Atlas). The analysis identifies concerning\npatterns, including pre-split data augmentation and validation-based reporting,\npotentially leading to overestimated metrics, while highlighting the lack of\nunified methodology standards. The experimental results demonstrate DINOv2's\nperformance in skin disease classification, achieving macro-averaged F1-scores\nof 0.85 (HAM10000), 0.71 (DermNet), and 0.84 (ISIC Atlas). Attention map\nanalysis reveals critical patterns in the model's decision-making, showing\nsophisticated feature recognition in typical presentations but significant\nvulnerabilities with atypical cases and composite images. Our findings\nhighlight the need for standardized evaluation protocols and careful\nimplementation strategies in clinical settings. We propose comprehensive\nmethodological recommendations for model development, evaluation, and clinical\ndeployment, emphasizing rigorous data preparation, systematic error analysis,\nand specialized protocols for different image types. To promote\nreproducibility, we provide our implementation code through GitHub. This work\nestablishes a foundation for rigorous evaluation standards in dermatological\nimage classification and provides insights for responsible AI implementation in\nclinical dermatology.\n", "link": "http://arxiv.org/abs/2502.02500v1", "date": "2025-02-04", "relevancy": 2.1723, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Skin%20Game%3A%20Revolutionizing%20Standards%20for%20AI%20Dermatology%20Model%0A%20%20Comparison&body=Title%3A%20The%20Skin%20Game%3A%20Revolutionizing%20Standards%20for%20AI%20Dermatology%20Model%0A%20%20Comparison%0AAuthor%3A%20%C5%81ukasz%20Mi%C4%99tkiewicz%20and%20Leon%20Ciechanowski%20and%20Dariusz%20Jemielniak%0AAbstract%3A%20%20%20Deep%20Learning%20approaches%20in%20dermatological%20image%20classification%20have%20shown%0Apromising%20results%2C%20yet%20the%20field%20faces%20significant%20methodological%20challenges%0Athat%20impede%20proper%20evaluation.%20This%20paper%20presents%20a%20dual%20contribution%3A%20first%2C%0Aa%20systematic%20analysis%20of%20current%20methodological%20practices%20in%20skin%20disease%0Aclassification%20research%2C%20revealing%20substantial%20inconsistencies%20in%20data%0Apreparation%2C%20augmentation%20strategies%2C%20and%20performance%20reporting%3B%20second%2C%20a%0Acomprehensive%20training%20and%20evaluation%20framework%20demonstrated%20through%0Aexperiments%20with%20the%20DINOv2-Large%20vision%20transformer%20across%20three%20benchmark%0Adatasets%20%28HAM10000%2C%20DermNet%2C%20ISIC%20Atlas%29.%20The%20analysis%20identifies%20concerning%0Apatterns%2C%20including%20pre-split%20data%20augmentation%20and%20validation-based%20reporting%2C%0Apotentially%20leading%20to%20overestimated%20metrics%2C%20while%20highlighting%20the%20lack%20of%0Aunified%20methodology%20standards.%20The%20experimental%20results%20demonstrate%20DINOv2%27s%0Aperformance%20in%20skin%20disease%20classification%2C%20achieving%20macro-averaged%20F1-scores%0Aof%200.85%20%28HAM10000%29%2C%200.71%20%28DermNet%29%2C%20and%200.84%20%28ISIC%20Atlas%29.%20Attention%20map%0Aanalysis%20reveals%20critical%20patterns%20in%20the%20model%27s%20decision-making%2C%20showing%0Asophisticated%20feature%20recognition%20in%20typical%20presentations%20but%20significant%0Avulnerabilities%20with%20atypical%20cases%20and%20composite%20images.%20Our%20findings%0Ahighlight%20the%20need%20for%20standardized%20evaluation%20protocols%20and%20careful%0Aimplementation%20strategies%20in%20clinical%20settings.%20We%20propose%20comprehensive%0Amethodological%20recommendations%20for%20model%20development%2C%20evaluation%2C%20and%20clinical%0Adeployment%2C%20emphasizing%20rigorous%20data%20preparation%2C%20systematic%20error%20analysis%2C%0Aand%20specialized%20protocols%20for%20different%20image%20types.%20To%20promote%0Areproducibility%2C%20we%20provide%20our%20implementation%20code%20through%20GitHub.%20This%20work%0Aestablishes%20a%20foundation%20for%20rigorous%20evaluation%20standards%20in%20dermatological%0Aimage%20classification%20and%20provides%20insights%20for%20responsible%20AI%20implementation%20in%0Aclinical%20dermatology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Skin%2520Game%253A%2520Revolutionizing%2520Standards%2520for%2520AI%2520Dermatology%2520Model%250A%2520%2520Comparison%26entry.906535625%3D%25C5%2581ukasz%2520Mi%25C4%2599tkiewicz%2520and%2520Leon%2520Ciechanowski%2520and%2520Dariusz%2520Jemielniak%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520approaches%2520in%2520dermatological%2520image%2520classification%2520have%2520shown%250Apromising%2520results%252C%2520yet%2520the%2520field%2520faces%2520significant%2520methodological%2520challenges%250Athat%2520impede%2520proper%2520evaluation.%2520This%2520paper%2520presents%2520a%2520dual%2520contribution%253A%2520first%252C%250Aa%2520systematic%2520analysis%2520of%2520current%2520methodological%2520practices%2520in%2520skin%2520disease%250Aclassification%2520research%252C%2520revealing%2520substantial%2520inconsistencies%2520in%2520data%250Apreparation%252C%2520augmentation%2520strategies%252C%2520and%2520performance%2520reporting%253B%2520second%252C%2520a%250Acomprehensive%2520training%2520and%2520evaluation%2520framework%2520demonstrated%2520through%250Aexperiments%2520with%2520the%2520DINOv2-Large%2520vision%2520transformer%2520across%2520three%2520benchmark%250Adatasets%2520%2528HAM10000%252C%2520DermNet%252C%2520ISIC%2520Atlas%2529.%2520The%2520analysis%2520identifies%2520concerning%250Apatterns%252C%2520including%2520pre-split%2520data%2520augmentation%2520and%2520validation-based%2520reporting%252C%250Apotentially%2520leading%2520to%2520overestimated%2520metrics%252C%2520while%2520highlighting%2520the%2520lack%2520of%250Aunified%2520methodology%2520standards.%2520The%2520experimental%2520results%2520demonstrate%2520DINOv2%2527s%250Aperformance%2520in%2520skin%2520disease%2520classification%252C%2520achieving%2520macro-averaged%2520F1-scores%250Aof%25200.85%2520%2528HAM10000%2529%252C%25200.71%2520%2528DermNet%2529%252C%2520and%25200.84%2520%2528ISIC%2520Atlas%2529.%2520Attention%2520map%250Aanalysis%2520reveals%2520critical%2520patterns%2520in%2520the%2520model%2527s%2520decision-making%252C%2520showing%250Asophisticated%2520feature%2520recognition%2520in%2520typical%2520presentations%2520but%2520significant%250Avulnerabilities%2520with%2520atypical%2520cases%2520and%2520composite%2520images.%2520Our%2520findings%250Ahighlight%2520the%2520need%2520for%2520standardized%2520evaluation%2520protocols%2520and%2520careful%250Aimplementation%2520strategies%2520in%2520clinical%2520settings.%2520We%2520propose%2520comprehensive%250Amethodological%2520recommendations%2520for%2520model%2520development%252C%2520evaluation%252C%2520and%2520clinical%250Adeployment%252C%2520emphasizing%2520rigorous%2520data%2520preparation%252C%2520systematic%2520error%2520analysis%252C%250Aand%2520specialized%2520protocols%2520for%2520different%2520image%2520types.%2520To%2520promote%250Areproducibility%252C%2520we%2520provide%2520our%2520implementation%2520code%2520through%2520GitHub.%2520This%2520work%250Aestablishes%2520a%2520foundation%2520for%2520rigorous%2520evaluation%2520standards%2520in%2520dermatological%250Aimage%2520classification%2520and%2520provides%2520insights%2520for%2520responsible%2520AI%2520implementation%2520in%250Aclinical%2520dermatology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Skin%20Game%3A%20Revolutionizing%20Standards%20for%20AI%20Dermatology%20Model%0A%20%20Comparison&entry.906535625=%C5%81ukasz%20Mi%C4%99tkiewicz%20and%20Leon%20Ciechanowski%20and%20Dariusz%20Jemielniak&entry.1292438233=%20%20Deep%20Learning%20approaches%20in%20dermatological%20image%20classification%20have%20shown%0Apromising%20results%2C%20yet%20the%20field%20faces%20significant%20methodological%20challenges%0Athat%20impede%20proper%20evaluation.%20This%20paper%20presents%20a%20dual%20contribution%3A%20first%2C%0Aa%20systematic%20analysis%20of%20current%20methodological%20practices%20in%20skin%20disease%0Aclassification%20research%2C%20revealing%20substantial%20inconsistencies%20in%20data%0Apreparation%2C%20augmentation%20strategies%2C%20and%20performance%20reporting%3B%20second%2C%20a%0Acomprehensive%20training%20and%20evaluation%20framework%20demonstrated%20through%0Aexperiments%20with%20the%20DINOv2-Large%20vision%20transformer%20across%20three%20benchmark%0Adatasets%20%28HAM10000%2C%20DermNet%2C%20ISIC%20Atlas%29.%20The%20analysis%20identifies%20concerning%0Apatterns%2C%20including%20pre-split%20data%20augmentation%20and%20validation-based%20reporting%2C%0Apotentially%20leading%20to%20overestimated%20metrics%2C%20while%20highlighting%20the%20lack%20of%0Aunified%20methodology%20standards.%20The%20experimental%20results%20demonstrate%20DINOv2%27s%0Aperformance%20in%20skin%20disease%20classification%2C%20achieving%20macro-averaged%20F1-scores%0Aof%200.85%20%28HAM10000%29%2C%200.71%20%28DermNet%29%2C%20and%200.84%20%28ISIC%20Atlas%29.%20Attention%20map%0Aanalysis%20reveals%20critical%20patterns%20in%20the%20model%27s%20decision-making%2C%20showing%0Asophisticated%20feature%20recognition%20in%20typical%20presentations%20but%20significant%0Avulnerabilities%20with%20atypical%20cases%20and%20composite%20images.%20Our%20findings%0Ahighlight%20the%20need%20for%20standardized%20evaluation%20protocols%20and%20careful%0Aimplementation%20strategies%20in%20clinical%20settings.%20We%20propose%20comprehensive%0Amethodological%20recommendations%20for%20model%20development%2C%20evaluation%2C%20and%20clinical%0Adeployment%2C%20emphasizing%20rigorous%20data%20preparation%2C%20systematic%20error%20analysis%2C%0Aand%20specialized%20protocols%20for%20different%20image%20types.%20To%20promote%0Areproducibility%2C%20we%20provide%20our%20implementation%20code%20through%20GitHub.%20This%20work%0Aestablishes%20a%20foundation%20for%20rigorous%20evaluation%20standards%20in%20dermatological%0Aimage%20classification%20and%20provides%20insights%20for%20responsible%20AI%20implementation%20in%0Aclinical%20dermatology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02500v1&entry.124074799=Read"},
{"title": "Real-Time Operator Takeover for Visuomotor Diffusion Policy Training", "author": "Nils Ingelhag and Jesper Munkeby and Michael C. Welle and Marco Moletta and Danica Kragic", "abstract": "  We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators\nto seamlessly take control of a live visuomotor diffusion policy, guiding the\nsystem back into desirable states or reinforcing specific demonstrations. We\npresents new insights in using the Mahalonobis distance to automaicaly identify\nundesirable states. Once the operator has intervened and redirected the system,\nthe control is seamlessly returned to the policy, which resumes generating\nactions until further intervention is required. We demonstrate that\nincorporating the targeted takeover demonstrations significantly improves\npolicy performance compared to training solely with an equivalent number of,\nbut longer, initial demonstrations. We provide an in-depth analysis of using\nthe Mahalanobis distance to detect out-of-distribution states, illustrating its\nutility for identifying critical failure points during execution. Supporting\nmaterials, including videos of initial and takeover demonstrations and all\nrice-scooping experiments, are available on the project website:\nhttps://operator-takeover.github.io/\n", "link": "http://arxiv.org/abs/2502.02308v1", "date": "2025-02-04", "relevancy": 2.1721, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5243}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Operator%20Takeover%20for%20Visuomotor%20Diffusion%20Policy%20Training&body=Title%3A%20Real-Time%20Operator%20Takeover%20for%20Visuomotor%20Diffusion%20Policy%20Training%0AAuthor%3A%20Nils%20Ingelhag%20and%20Jesper%20Munkeby%20and%20Michael%20C.%20Welle%20and%20Marco%20Moletta%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20We%20present%20a%20Real-Time%20Operator%20Takeover%20%28RTOT%29%20paradigm%20enabling%20operators%0Ato%20seamlessly%20take%20control%20of%20a%20live%20visuomotor%20diffusion%20policy%2C%20guiding%20the%0Asystem%20back%20into%20desirable%20states%20or%20reinforcing%20specific%20demonstrations.%20We%0Apresents%20new%20insights%20in%20using%20the%20Mahalonobis%20distance%20to%20automaicaly%20identify%0Aundesirable%20states.%20Once%20the%20operator%20has%20intervened%20and%20redirected%20the%20system%2C%0Athe%20control%20is%20seamlessly%20returned%20to%20the%20policy%2C%20which%20resumes%20generating%0Aactions%20until%20further%20intervention%20is%20required.%20We%20demonstrate%20that%0Aincorporating%20the%20targeted%20takeover%20demonstrations%20significantly%20improves%0Apolicy%20performance%20compared%20to%20training%20solely%20with%20an%20equivalent%20number%20of%2C%0Abut%20longer%2C%20initial%20demonstrations.%20We%20provide%20an%20in-depth%20analysis%20of%20using%0Athe%20Mahalanobis%20distance%20to%20detect%20out-of-distribution%20states%2C%20illustrating%20its%0Autility%20for%20identifying%20critical%20failure%20points%20during%20execution.%20Supporting%0Amaterials%2C%20including%20videos%20of%20initial%20and%20takeover%20demonstrations%20and%20all%0Arice-scooping%20experiments%2C%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//operator-takeover.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Operator%2520Takeover%2520for%2520Visuomotor%2520Diffusion%2520Policy%2520Training%26entry.906535625%3DNils%2520Ingelhag%2520and%2520Jesper%2520Munkeby%2520and%2520Michael%2520C.%2520Welle%2520and%2520Marco%2520Moletta%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520Real-Time%2520Operator%2520Takeover%2520%2528RTOT%2529%2520paradigm%2520enabling%2520operators%250Ato%2520seamlessly%2520take%2520control%2520of%2520a%2520live%2520visuomotor%2520diffusion%2520policy%252C%2520guiding%2520the%250Asystem%2520back%2520into%2520desirable%2520states%2520or%2520reinforcing%2520specific%2520demonstrations.%2520We%250Apresents%2520new%2520insights%2520in%2520using%2520the%2520Mahalonobis%2520distance%2520to%2520automaicaly%2520identify%250Aundesirable%2520states.%2520Once%2520the%2520operator%2520has%2520intervened%2520and%2520redirected%2520the%2520system%252C%250Athe%2520control%2520is%2520seamlessly%2520returned%2520to%2520the%2520policy%252C%2520which%2520resumes%2520generating%250Aactions%2520until%2520further%2520intervention%2520is%2520required.%2520We%2520demonstrate%2520that%250Aincorporating%2520the%2520targeted%2520takeover%2520demonstrations%2520significantly%2520improves%250Apolicy%2520performance%2520compared%2520to%2520training%2520solely%2520with%2520an%2520equivalent%2520number%2520of%252C%250Abut%2520longer%252C%2520initial%2520demonstrations.%2520We%2520provide%2520an%2520in-depth%2520analysis%2520of%2520using%250Athe%2520Mahalanobis%2520distance%2520to%2520detect%2520out-of-distribution%2520states%252C%2520illustrating%2520its%250Autility%2520for%2520identifying%2520critical%2520failure%2520points%2520during%2520execution.%2520Supporting%250Amaterials%252C%2520including%2520videos%2520of%2520initial%2520and%2520takeover%2520demonstrations%2520and%2520all%250Arice-scooping%2520experiments%252C%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//operator-takeover.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Operator%20Takeover%20for%20Visuomotor%20Diffusion%20Policy%20Training&entry.906535625=Nils%20Ingelhag%20and%20Jesper%20Munkeby%20and%20Michael%20C.%20Welle%20and%20Marco%20Moletta%20and%20Danica%20Kragic&entry.1292438233=%20%20We%20present%20a%20Real-Time%20Operator%20Takeover%20%28RTOT%29%20paradigm%20enabling%20operators%0Ato%20seamlessly%20take%20control%20of%20a%20live%20visuomotor%20diffusion%20policy%2C%20guiding%20the%0Asystem%20back%20into%20desirable%20states%20or%20reinforcing%20specific%20demonstrations.%20We%0Apresents%20new%20insights%20in%20using%20the%20Mahalonobis%20distance%20to%20automaicaly%20identify%0Aundesirable%20states.%20Once%20the%20operator%20has%20intervened%20and%20redirected%20the%20system%2C%0Athe%20control%20is%20seamlessly%20returned%20to%20the%20policy%2C%20which%20resumes%20generating%0Aactions%20until%20further%20intervention%20is%20required.%20We%20demonstrate%20that%0Aincorporating%20the%20targeted%20takeover%20demonstrations%20significantly%20improves%0Apolicy%20performance%20compared%20to%20training%20solely%20with%20an%20equivalent%20number%20of%2C%0Abut%20longer%2C%20initial%20demonstrations.%20We%20provide%20an%20in-depth%20analysis%20of%20using%0Athe%20Mahalanobis%20distance%20to%20detect%20out-of-distribution%20states%2C%20illustrating%20its%0Autility%20for%20identifying%20critical%20failure%20points%20during%20execution.%20Supporting%0Amaterials%2C%20including%20videos%20of%20initial%20and%20takeover%20demonstrations%20and%20all%0Arice-scooping%20experiments%2C%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//operator-takeover.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02308v1&entry.124074799=Read"},
{"title": "COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation", "author": "Xueqing Deng and Qihang Yu and Ali Athar and Chenglin Yang and Linjie Yang and Xiaojie Jin and Xiaohui Shen and Liang-Chieh Chen", "abstract": "  This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.\n", "link": "http://arxiv.org/abs/2502.02589v1", "date": "2025-02-04", "relevancy": 2.1701, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5511}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COCONut-PanCap%3A%20Joint%20Panoptic%20Segmentation%20and%20Grounded%20Captions%20for%0A%20%20Fine-Grained%20Understanding%20and%20Generation&body=Title%3A%20COCONut-PanCap%3A%20Joint%20Panoptic%20Segmentation%20and%20Grounded%20Captions%20for%0A%20%20Fine-Grained%20Understanding%20and%20Generation%0AAuthor%3A%20Xueqing%20Deng%20and%20Qihang%20Yu%20and%20Ali%20Athar%20and%20Chenglin%20Yang%20and%20Linjie%20Yang%20and%20Xiaojie%20Jin%20and%20Xiaohui%20Shen%20and%20Liang-Chieh%20Chen%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20COCONut-PanCap%20dataset%2C%20created%20to%20enhance%20panoptic%0Asegmentation%20and%20grounded%20image%20captioning.%20Building%20upon%20the%20COCO%20dataset%20with%0Aadvanced%20COCONut%20panoptic%20masks%2C%20this%20dataset%20aims%20to%20overcome%20limitations%20in%0Aexisting%20image-text%20datasets%20that%20often%20lack%20detailed%2C%20scene-comprehensive%0Adescriptions.%20The%20COCONut-PanCap%20dataset%20incorporates%20fine-grained%2C%0Aregion-level%20captions%20grounded%20in%20panoptic%20segmentation%20masks%2C%20ensuring%0Aconsistency%20and%20improving%20the%20detail%20of%20generated%20captions.%20Through%0Ahuman-edited%2C%20densely%20annotated%20descriptions%2C%20COCONut-PanCap%20supports%20improved%0Atraining%20of%20vision-language%20models%20%28VLMs%29%20for%20image%20understanding%20and%0Agenerative%20models%20for%20text-to-image%20tasks.%20Experimental%20results%20demonstrate%0Athat%20COCONut-PanCap%20significantly%20boosts%20performance%20across%20understanding%20and%0Ageneration%20tasks%2C%20offering%20complementary%20benefits%20to%20large-scale%20datasets.%20This%0Adataset%20sets%20a%20new%20benchmark%20for%20evaluating%20models%20on%20joint%20panoptic%0Asegmentation%20and%20grounded%20captioning%20tasks%2C%20addressing%20the%20need%20for%0Ahigh-quality%2C%20detailed%20image-text%20annotations%20in%20multi-modal%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOCONut-PanCap%253A%2520Joint%2520Panoptic%2520Segmentation%2520and%2520Grounded%2520Captions%2520for%250A%2520%2520Fine-Grained%2520Understanding%2520and%2520Generation%26entry.906535625%3DXueqing%2520Deng%2520and%2520Qihang%2520Yu%2520and%2520Ali%2520Athar%2520and%2520Chenglin%2520Yang%2520and%2520Linjie%2520Yang%2520and%2520Xiaojie%2520Jin%2520and%2520Xiaohui%2520Shen%2520and%2520Liang-Chieh%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520COCONut-PanCap%2520dataset%252C%2520created%2520to%2520enhance%2520panoptic%250Asegmentation%2520and%2520grounded%2520image%2520captioning.%2520Building%2520upon%2520the%2520COCO%2520dataset%2520with%250Aadvanced%2520COCONut%2520panoptic%2520masks%252C%2520this%2520dataset%2520aims%2520to%2520overcome%2520limitations%2520in%250Aexisting%2520image-text%2520datasets%2520that%2520often%2520lack%2520detailed%252C%2520scene-comprehensive%250Adescriptions.%2520The%2520COCONut-PanCap%2520dataset%2520incorporates%2520fine-grained%252C%250Aregion-level%2520captions%2520grounded%2520in%2520panoptic%2520segmentation%2520masks%252C%2520ensuring%250Aconsistency%2520and%2520improving%2520the%2520detail%2520of%2520generated%2520captions.%2520Through%250Ahuman-edited%252C%2520densely%2520annotated%2520descriptions%252C%2520COCONut-PanCap%2520supports%2520improved%250Atraining%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520image%2520understanding%2520and%250Agenerative%2520models%2520for%2520text-to-image%2520tasks.%2520Experimental%2520results%2520demonstrate%250Athat%2520COCONut-PanCap%2520significantly%2520boosts%2520performance%2520across%2520understanding%2520and%250Ageneration%2520tasks%252C%2520offering%2520complementary%2520benefits%2520to%2520large-scale%2520datasets.%2520This%250Adataset%2520sets%2520a%2520new%2520benchmark%2520for%2520evaluating%2520models%2520on%2520joint%2520panoptic%250Asegmentation%2520and%2520grounded%2520captioning%2520tasks%252C%2520addressing%2520the%2520need%2520for%250Ahigh-quality%252C%2520detailed%2520image-text%2520annotations%2520in%2520multi-modal%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COCONut-PanCap%3A%20Joint%20Panoptic%20Segmentation%20and%20Grounded%20Captions%20for%0A%20%20Fine-Grained%20Understanding%20and%20Generation&entry.906535625=Xueqing%20Deng%20and%20Qihang%20Yu%20and%20Ali%20Athar%20and%20Chenglin%20Yang%20and%20Linjie%20Yang%20and%20Xiaojie%20Jin%20and%20Xiaohui%20Shen%20and%20Liang-Chieh%20Chen&entry.1292438233=%20%20This%20paper%20introduces%20the%20COCONut-PanCap%20dataset%2C%20created%20to%20enhance%20panoptic%0Asegmentation%20and%20grounded%20image%20captioning.%20Building%20upon%20the%20COCO%20dataset%20with%0Aadvanced%20COCONut%20panoptic%20masks%2C%20this%20dataset%20aims%20to%20overcome%20limitations%20in%0Aexisting%20image-text%20datasets%20that%20often%20lack%20detailed%2C%20scene-comprehensive%0Adescriptions.%20The%20COCONut-PanCap%20dataset%20incorporates%20fine-grained%2C%0Aregion-level%20captions%20grounded%20in%20panoptic%20segmentation%20masks%2C%20ensuring%0Aconsistency%20and%20improving%20the%20detail%20of%20generated%20captions.%20Through%0Ahuman-edited%2C%20densely%20annotated%20descriptions%2C%20COCONut-PanCap%20supports%20improved%0Atraining%20of%20vision-language%20models%20%28VLMs%29%20for%20image%20understanding%20and%0Agenerative%20models%20for%20text-to-image%20tasks.%20Experimental%20results%20demonstrate%0Athat%20COCONut-PanCap%20significantly%20boosts%20performance%20across%20understanding%20and%0Ageneration%20tasks%2C%20offering%20complementary%20benefits%20to%20large-scale%20datasets.%20This%0Adataset%20sets%20a%20new%20benchmark%20for%20evaluating%20models%20on%20joint%20panoptic%0Asegmentation%20and%20grounded%20captioning%20tasks%2C%20addressing%20the%20need%20for%0Ahigh-quality%2C%20detailed%20image-text%20annotations%20in%20multi-modal%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02589v1&entry.124074799=Read"},
{"title": "GAN-Based Architecture for Low-dose Computed Tomography Imaging\n  Denoising", "author": "Yunuo Wang and Ningning Yang and Jialin Li", "abstract": "  Generative Adversarial Networks (GANs) have surfaced as a revolutionary\nelement within the domain of low-dose computed tomography (LDCT) imaging,\nproviding an advanced resolution to the enduring issue of reconciling radiation\nexposure with image quality. This comprehensive review synthesizes the rapid\nadvancements in GAN-based LDCT denoising techniques, examining the evolution\nfrom foundational architectures to state-of-the-art models incorporating\nadvanced features such as anatomical priors, perceptual loss functions, and\ninnovative regularization strategies. We critically analyze various GAN\narchitectures, including conditional GANs (cGANs), CycleGANs, and\nSuper-Resolution GANs (SRGANs), elucidating their unique strengths and\nlimitations in the context of LDCT denoising. The evaluation provides both\nqualitative and quantitative results related to the improvements in performance\nin benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.\nAfter highlighting the positive results, we discuss some of the challenges\npreventing a wider clinical use, including the interpretability of the images\ngenerated by GANs, synthetic artifacts, and the need for clinically relevant\nmetrics. The review concludes by highlighting the essential significance of\nGAN-based methodologies in the progression of precision medicine via tailored\nLDCT denoising models, underlining the transformative possibilities presented\nby artificial intelligence within contemporary radiological practice.\n", "link": "http://arxiv.org/abs/2411.09512v2", "date": "2025-02-04", "relevancy": 2.169, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5381}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising&body=Title%3A%20GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising%0AAuthor%3A%20Yunuo%20Wang%20and%20Ningning%20Yang%20and%20Jialin%20Li%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20surfaced%20as%20a%20revolutionary%0Aelement%20within%20the%20domain%20of%20low-dose%20computed%20tomography%20%28LDCT%29%20imaging%2C%0Aproviding%20an%20advanced%20resolution%20to%20the%20enduring%20issue%20of%20reconciling%20radiation%0Aexposure%20with%20image%20quality.%20This%20comprehensive%20review%20synthesizes%20the%20rapid%0Aadvancements%20in%20GAN-based%20LDCT%20denoising%20techniques%2C%20examining%20the%20evolution%0Afrom%20foundational%20architectures%20to%20state-of-the-art%20models%20incorporating%0Aadvanced%20features%20such%20as%20anatomical%20priors%2C%20perceptual%20loss%20functions%2C%20and%0Ainnovative%20regularization%20strategies.%20We%20critically%20analyze%20various%20GAN%0Aarchitectures%2C%20including%20conditional%20GANs%20%28cGANs%29%2C%20CycleGANs%2C%20and%0ASuper-Resolution%20GANs%20%28SRGANs%29%2C%20elucidating%20their%20unique%20strengths%20and%0Alimitations%20in%20the%20context%20of%20LDCT%20denoising.%20The%20evaluation%20provides%20both%0Aqualitative%20and%20quantitative%20results%20related%20to%20the%20improvements%20in%20performance%0Ain%20benchmark%20and%20clinical%20datasets%20with%20metrics%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0AAfter%20highlighting%20the%20positive%20results%2C%20we%20discuss%20some%20of%20the%20challenges%0Apreventing%20a%20wider%20clinical%20use%2C%20including%20the%20interpretability%20of%20the%20images%0Agenerated%20by%20GANs%2C%20synthetic%20artifacts%2C%20and%20the%20need%20for%20clinically%20relevant%0Ametrics.%20The%20review%20concludes%20by%20highlighting%20the%20essential%20significance%20of%0AGAN-based%20methodologies%20in%20the%20progression%20of%20precision%20medicine%20via%20tailored%0ALDCT%20denoising%20models%2C%20underlining%20the%20transformative%20possibilities%20presented%0Aby%20artificial%20intelligence%20within%20contemporary%20radiological%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAN-Based%2520Architecture%2520for%2520Low-dose%2520Computed%2520Tomography%2520Imaging%250A%2520%2520Denoising%26entry.906535625%3DYunuo%2520Wang%2520and%2520Ningning%2520Yang%2520and%2520Jialin%2520Li%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520surfaced%2520as%2520a%2520revolutionary%250Aelement%2520within%2520the%2520domain%2520of%2520low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520imaging%252C%250Aproviding%2520an%2520advanced%2520resolution%2520to%2520the%2520enduring%2520issue%2520of%2520reconciling%2520radiation%250Aexposure%2520with%2520image%2520quality.%2520This%2520comprehensive%2520review%2520synthesizes%2520the%2520rapid%250Aadvancements%2520in%2520GAN-based%2520LDCT%2520denoising%2520techniques%252C%2520examining%2520the%2520evolution%250Afrom%2520foundational%2520architectures%2520to%2520state-of-the-art%2520models%2520incorporating%250Aadvanced%2520features%2520such%2520as%2520anatomical%2520priors%252C%2520perceptual%2520loss%2520functions%252C%2520and%250Ainnovative%2520regularization%2520strategies.%2520We%2520critically%2520analyze%2520various%2520GAN%250Aarchitectures%252C%2520including%2520conditional%2520GANs%2520%2528cGANs%2529%252C%2520CycleGANs%252C%2520and%250ASuper-Resolution%2520GANs%2520%2528SRGANs%2529%252C%2520elucidating%2520their%2520unique%2520strengths%2520and%250Alimitations%2520in%2520the%2520context%2520of%2520LDCT%2520denoising.%2520The%2520evaluation%2520provides%2520both%250Aqualitative%2520and%2520quantitative%2520results%2520related%2520to%2520the%2520improvements%2520in%2520performance%250Ain%2520benchmark%2520and%2520clinical%2520datasets%2520with%2520metrics%2520such%2520as%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS.%250AAfter%2520highlighting%2520the%2520positive%2520results%252C%2520we%2520discuss%2520some%2520of%2520the%2520challenges%250Apreventing%2520a%2520wider%2520clinical%2520use%252C%2520including%2520the%2520interpretability%2520of%2520the%2520images%250Agenerated%2520by%2520GANs%252C%2520synthetic%2520artifacts%252C%2520and%2520the%2520need%2520for%2520clinically%2520relevant%250Ametrics.%2520The%2520review%2520concludes%2520by%2520highlighting%2520the%2520essential%2520significance%2520of%250AGAN-based%2520methodologies%2520in%2520the%2520progression%2520of%2520precision%2520medicine%2520via%2520tailored%250ALDCT%2520denoising%2520models%252C%2520underlining%2520the%2520transformative%2520possibilities%2520presented%250Aby%2520artificial%2520intelligence%2520within%2520contemporary%2520radiological%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising&entry.906535625=Yunuo%20Wang%20and%20Ningning%20Yang%20and%20Jialin%20Li&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20surfaced%20as%20a%20revolutionary%0Aelement%20within%20the%20domain%20of%20low-dose%20computed%20tomography%20%28LDCT%29%20imaging%2C%0Aproviding%20an%20advanced%20resolution%20to%20the%20enduring%20issue%20of%20reconciling%20radiation%0Aexposure%20with%20image%20quality.%20This%20comprehensive%20review%20synthesizes%20the%20rapid%0Aadvancements%20in%20GAN-based%20LDCT%20denoising%20techniques%2C%20examining%20the%20evolution%0Afrom%20foundational%20architectures%20to%20state-of-the-art%20models%20incorporating%0Aadvanced%20features%20such%20as%20anatomical%20priors%2C%20perceptual%20loss%20functions%2C%20and%0Ainnovative%20regularization%20strategies.%20We%20critically%20analyze%20various%20GAN%0Aarchitectures%2C%20including%20conditional%20GANs%20%28cGANs%29%2C%20CycleGANs%2C%20and%0ASuper-Resolution%20GANs%20%28SRGANs%29%2C%20elucidating%20their%20unique%20strengths%20and%0Alimitations%20in%20the%20context%20of%20LDCT%20denoising.%20The%20evaluation%20provides%20both%0Aqualitative%20and%20quantitative%20results%20related%20to%20the%20improvements%20in%20performance%0Ain%20benchmark%20and%20clinical%20datasets%20with%20metrics%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0AAfter%20highlighting%20the%20positive%20results%2C%20we%20discuss%20some%20of%20the%20challenges%0Apreventing%20a%20wider%20clinical%20use%2C%20including%20the%20interpretability%20of%20the%20images%0Agenerated%20by%20GANs%2C%20synthetic%20artifacts%2C%20and%20the%20need%20for%20clinically%20relevant%0Ametrics.%20The%20review%20concludes%20by%20highlighting%20the%20essential%20significance%20of%0AGAN-based%20methodologies%20in%20the%20progression%20of%20precision%20medicine%20via%20tailored%0ALDCT%20denoising%20models%2C%20underlining%20the%20transformative%20possibilities%20presented%0Aby%20artificial%20intelligence%20within%20contemporary%20radiological%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09512v2&entry.124074799=Read"},
{"title": "Geometric Neural Process Fields", "author": "Wenzhe Yin and Zehao Xiao and Jiayi Shen and Yunlu Chen and Cees G. M. Snoek and Jan-Jakob Sonke and Efstratios Gavves", "abstract": "  This paper addresses the challenge of Neural Field (NeF) generalization,\nwhere models must efficiently adapt to new signals given only a few\nobservations. To tackle this, we propose Geometric Neural Process Fields\n(G-NPF), a probabilistic framework for neural radiance fields that explicitly\ncaptures uncertainty. We formulate NeF generalization as a probabilistic\nproblem, enabling direct inference of NeF function distributions from limited\ncontext observations. To incorporate structural inductive biases, we introduce\na set of geometric bases that encode spatial structure and facilitate the\ninference of NeF function distributions. Building on these bases, we design a\nhierarchical latent variable model, allowing G-NPF to integrate structural\ninformation across multiple spatial levels and effectively parameterize INR\nfunctions. This hierarchical approach improves generalization to novel scenes\nand unseen signals. Experiments on novel-view synthesis for 3D scenes, as well\nas 2D image and 1D signal regression, demonstrate the effectiveness of our\nmethod in capturing uncertainty and leveraging structural information for\nimproved generalization.\n", "link": "http://arxiv.org/abs/2502.02338v1", "date": "2025-02-04", "relevancy": 2.1688, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5722}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.522}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Neural%20Process%20Fields&body=Title%3A%20Geometric%20Neural%20Process%20Fields%0AAuthor%3A%20Wenzhe%20Yin%20and%20Zehao%20Xiao%20and%20Jiayi%20Shen%20and%20Yunlu%20Chen%20and%20Cees%20G.%20M.%20Snoek%20and%20Jan-Jakob%20Sonke%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20Neural%20Field%20%28NeF%29%20generalization%2C%0Awhere%20models%20must%20efficiently%20adapt%20to%20new%20signals%20given%20only%20a%20few%0Aobservations.%20To%20tackle%20this%2C%20we%20propose%20Geometric%20Neural%20Process%20Fields%0A%28G-NPF%29%2C%20a%20probabilistic%20framework%20for%20neural%20radiance%20fields%20that%20explicitly%0Acaptures%20uncertainty.%20We%20formulate%20NeF%20generalization%20as%20a%20probabilistic%0Aproblem%2C%20enabling%20direct%20inference%20of%20NeF%20function%20distributions%20from%20limited%0Acontext%20observations.%20To%20incorporate%20structural%20inductive%20biases%2C%20we%20introduce%0Aa%20set%20of%20geometric%20bases%20that%20encode%20spatial%20structure%20and%20facilitate%20the%0Ainference%20of%20NeF%20function%20distributions.%20Building%20on%20these%20bases%2C%20we%20design%20a%0Ahierarchical%20latent%20variable%20model%2C%20allowing%20G-NPF%20to%20integrate%20structural%0Ainformation%20across%20multiple%20spatial%20levels%20and%20effectively%20parameterize%20INR%0Afunctions.%20This%20hierarchical%20approach%20improves%20generalization%20to%20novel%20scenes%0Aand%20unseen%20signals.%20Experiments%20on%20novel-view%20synthesis%20for%203D%20scenes%2C%20as%20well%0Aas%202D%20image%20and%201D%20signal%20regression%2C%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20capturing%20uncertainty%20and%20leveraging%20structural%20information%20for%0Aimproved%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Neural%2520Process%2520Fields%26entry.906535625%3DWenzhe%2520Yin%2520and%2520Zehao%2520Xiao%2520and%2520Jiayi%2520Shen%2520and%2520Yunlu%2520Chen%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520Neural%2520Field%2520%2528NeF%2529%2520generalization%252C%250Awhere%2520models%2520must%2520efficiently%2520adapt%2520to%2520new%2520signals%2520given%2520only%2520a%2520few%250Aobservations.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520Geometric%2520Neural%2520Process%2520Fields%250A%2528G-NPF%2529%252C%2520a%2520probabilistic%2520framework%2520for%2520neural%2520radiance%2520fields%2520that%2520explicitly%250Acaptures%2520uncertainty.%2520We%2520formulate%2520NeF%2520generalization%2520as%2520a%2520probabilistic%250Aproblem%252C%2520enabling%2520direct%2520inference%2520of%2520NeF%2520function%2520distributions%2520from%2520limited%250Acontext%2520observations.%2520To%2520incorporate%2520structural%2520inductive%2520biases%252C%2520we%2520introduce%250Aa%2520set%2520of%2520geometric%2520bases%2520that%2520encode%2520spatial%2520structure%2520and%2520facilitate%2520the%250Ainference%2520of%2520NeF%2520function%2520distributions.%2520Building%2520on%2520these%2520bases%252C%2520we%2520design%2520a%250Ahierarchical%2520latent%2520variable%2520model%252C%2520allowing%2520G-NPF%2520to%2520integrate%2520structural%250Ainformation%2520across%2520multiple%2520spatial%2520levels%2520and%2520effectively%2520parameterize%2520INR%250Afunctions.%2520This%2520hierarchical%2520approach%2520improves%2520generalization%2520to%2520novel%2520scenes%250Aand%2520unseen%2520signals.%2520Experiments%2520on%2520novel-view%2520synthesis%2520for%25203D%2520scenes%252C%2520as%2520well%250Aas%25202D%2520image%2520and%25201D%2520signal%2520regression%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520in%2520capturing%2520uncertainty%2520and%2520leveraging%2520structural%2520information%2520for%250Aimproved%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Neural%20Process%20Fields&entry.906535625=Wenzhe%20Yin%20and%20Zehao%20Xiao%20and%20Jiayi%20Shen%20and%20Yunlu%20Chen%20and%20Cees%20G.%20M.%20Snoek%20and%20Jan-Jakob%20Sonke%20and%20Efstratios%20Gavves&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20Neural%20Field%20%28NeF%29%20generalization%2C%0Awhere%20models%20must%20efficiently%20adapt%20to%20new%20signals%20given%20only%20a%20few%0Aobservations.%20To%20tackle%20this%2C%20we%20propose%20Geometric%20Neural%20Process%20Fields%0A%28G-NPF%29%2C%20a%20probabilistic%20framework%20for%20neural%20radiance%20fields%20that%20explicitly%0Acaptures%20uncertainty.%20We%20formulate%20NeF%20generalization%20as%20a%20probabilistic%0Aproblem%2C%20enabling%20direct%20inference%20of%20NeF%20function%20distributions%20from%20limited%0Acontext%20observations.%20To%20incorporate%20structural%20inductive%20biases%2C%20we%20introduce%0Aa%20set%20of%20geometric%20bases%20that%20encode%20spatial%20structure%20and%20facilitate%20the%0Ainference%20of%20NeF%20function%20distributions.%20Building%20on%20these%20bases%2C%20we%20design%20a%0Ahierarchical%20latent%20variable%20model%2C%20allowing%20G-NPF%20to%20integrate%20structural%0Ainformation%20across%20multiple%20spatial%20levels%20and%20effectively%20parameterize%20INR%0Afunctions.%20This%20hierarchical%20approach%20improves%20generalization%20to%20novel%20scenes%0Aand%20unseen%20signals.%20Experiments%20on%20novel-view%20synthesis%20for%203D%20scenes%2C%20as%20well%0Aas%202D%20image%20and%201D%20signal%20regression%2C%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20capturing%20uncertainty%20and%20leveraging%20structural%20information%20for%0Aimproved%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02338v1&entry.124074799=Read"},
{"title": "TUMTraffic-VideoQA: A Benchmark for Unified Spatio-Temporal Video\n  Understanding in Traffic Scenes", "author": "Xingcheng Zhou and Konstantinos Larintzakis and Hao Guo and Walter Zimmer and Mingyu Liu and Hu Cao and Jiajie Zhang and Venkatnarayanan Lakshminarasimhan and Leah Strand and Alois C. Knoll", "abstract": "  We present TUMTraffic-VideoQA, a novel dataset and benchmark designed for\nspatio-temporal video understanding in complex roadside traffic scenarios. The\ndataset comprises 1,000 videos, featuring 85,000 multiple-choice QA pairs,\n2,300 object captioning, and 5,700 object grounding annotations, encompassing\ndiverse real-world conditions such as adverse weather and traffic anomalies. By\nincorporating tuple-based spatio-temporal object expressions,\nTUMTraffic-VideoQA unifies three essential tasks-multiple-choice video question\nanswering, referred object captioning, and spatio-temporal object\ngrounding-within a cohesive evaluation framework. We further introduce the\nTUMTraffic-Qwen baseline model, enhanced with visual token sampling strategies,\nproviding valuable insights into the challenges of fine-grained spatio-temporal\nreasoning. Extensive experiments demonstrate the dataset's complexity,\nhighlight the limitations of existing models, and position TUMTraffic-VideoQA\nas a robust foundation for advancing research in intelligent transportation\nsystems. The dataset and benchmark are publicly available to facilitate further\nexploration.\n", "link": "http://arxiv.org/abs/2502.02449v1", "date": "2025-02-04", "relevancy": 2.1681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUMTraffic-VideoQA%3A%20A%20Benchmark%20for%20Unified%20Spatio-Temporal%20Video%0A%20%20Understanding%20in%20Traffic%20Scenes&body=Title%3A%20TUMTraffic-VideoQA%3A%20A%20Benchmark%20for%20Unified%20Spatio-Temporal%20Video%0A%20%20Understanding%20in%20Traffic%20Scenes%0AAuthor%3A%20Xingcheng%20Zhou%20and%20Konstantinos%20Larintzakis%20and%20Hao%20Guo%20and%20Walter%20Zimmer%20and%20Mingyu%20Liu%20and%20Hu%20Cao%20and%20Jiajie%20Zhang%20and%20Venkatnarayanan%20Lakshminarasimhan%20and%20Leah%20Strand%20and%20Alois%20C.%20Knoll%0AAbstract%3A%20%20%20We%20present%20TUMTraffic-VideoQA%2C%20a%20novel%20dataset%20and%20benchmark%20designed%20for%0Aspatio-temporal%20video%20understanding%20in%20complex%20roadside%20traffic%20scenarios.%20The%0Adataset%20comprises%201%2C000%20videos%2C%20featuring%2085%2C000%20multiple-choice%20QA%20pairs%2C%0A2%2C300%20object%20captioning%2C%20and%205%2C700%20object%20grounding%20annotations%2C%20encompassing%0Adiverse%20real-world%20conditions%20such%20as%20adverse%20weather%20and%20traffic%20anomalies.%20By%0Aincorporating%20tuple-based%20spatio-temporal%20object%20expressions%2C%0ATUMTraffic-VideoQA%20unifies%20three%20essential%20tasks-multiple-choice%20video%20question%0Aanswering%2C%20referred%20object%20captioning%2C%20and%20spatio-temporal%20object%0Agrounding-within%20a%20cohesive%20evaluation%20framework.%20We%20further%20introduce%20the%0ATUMTraffic-Qwen%20baseline%20model%2C%20enhanced%20with%20visual%20token%20sampling%20strategies%2C%0Aproviding%20valuable%20insights%20into%20the%20challenges%20of%20fine-grained%20spatio-temporal%0Areasoning.%20Extensive%20experiments%20demonstrate%20the%20dataset%27s%20complexity%2C%0Ahighlight%20the%20limitations%20of%20existing%20models%2C%20and%20position%20TUMTraffic-VideoQA%0Aas%20a%20robust%20foundation%20for%20advancing%20research%20in%20intelligent%20transportation%0Asystems.%20The%20dataset%20and%20benchmark%20are%20publicly%20available%20to%20facilitate%20further%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUMTraffic-VideoQA%253A%2520A%2520Benchmark%2520for%2520Unified%2520Spatio-Temporal%2520Video%250A%2520%2520Understanding%2520in%2520Traffic%2520Scenes%26entry.906535625%3DXingcheng%2520Zhou%2520and%2520Konstantinos%2520Larintzakis%2520and%2520Hao%2520Guo%2520and%2520Walter%2520Zimmer%2520and%2520Mingyu%2520Liu%2520and%2520Hu%2520Cao%2520and%2520Jiajie%2520Zhang%2520and%2520Venkatnarayanan%2520Lakshminarasimhan%2520and%2520Leah%2520Strand%2520and%2520Alois%2520C.%2520Knoll%26entry.1292438233%3D%2520%2520We%2520present%2520TUMTraffic-VideoQA%252C%2520a%2520novel%2520dataset%2520and%2520benchmark%2520designed%2520for%250Aspatio-temporal%2520video%2520understanding%2520in%2520complex%2520roadside%2520traffic%2520scenarios.%2520The%250Adataset%2520comprises%25201%252C000%2520videos%252C%2520featuring%252085%252C000%2520multiple-choice%2520QA%2520pairs%252C%250A2%252C300%2520object%2520captioning%252C%2520and%25205%252C700%2520object%2520grounding%2520annotations%252C%2520encompassing%250Adiverse%2520real-world%2520conditions%2520such%2520as%2520adverse%2520weather%2520and%2520traffic%2520anomalies.%2520By%250Aincorporating%2520tuple-based%2520spatio-temporal%2520object%2520expressions%252C%250ATUMTraffic-VideoQA%2520unifies%2520three%2520essential%2520tasks-multiple-choice%2520video%2520question%250Aanswering%252C%2520referred%2520object%2520captioning%252C%2520and%2520spatio-temporal%2520object%250Agrounding-within%2520a%2520cohesive%2520evaluation%2520framework.%2520We%2520further%2520introduce%2520the%250ATUMTraffic-Qwen%2520baseline%2520model%252C%2520enhanced%2520with%2520visual%2520token%2520sampling%2520strategies%252C%250Aproviding%2520valuable%2520insights%2520into%2520the%2520challenges%2520of%2520fine-grained%2520spatio-temporal%250Areasoning.%2520Extensive%2520experiments%2520demonstrate%2520the%2520dataset%2527s%2520complexity%252C%250Ahighlight%2520the%2520limitations%2520of%2520existing%2520models%252C%2520and%2520position%2520TUMTraffic-VideoQA%250Aas%2520a%2520robust%2520foundation%2520for%2520advancing%2520research%2520in%2520intelligent%2520transportation%250Asystems.%2520The%2520dataset%2520and%2520benchmark%2520are%2520publicly%2520available%2520to%2520facilitate%2520further%250Aexploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUMTraffic-VideoQA%3A%20A%20Benchmark%20for%20Unified%20Spatio-Temporal%20Video%0A%20%20Understanding%20in%20Traffic%20Scenes&entry.906535625=Xingcheng%20Zhou%20and%20Konstantinos%20Larintzakis%20and%20Hao%20Guo%20and%20Walter%20Zimmer%20and%20Mingyu%20Liu%20and%20Hu%20Cao%20and%20Jiajie%20Zhang%20and%20Venkatnarayanan%20Lakshminarasimhan%20and%20Leah%20Strand%20and%20Alois%20C.%20Knoll&entry.1292438233=%20%20We%20present%20TUMTraffic-VideoQA%2C%20a%20novel%20dataset%20and%20benchmark%20designed%20for%0Aspatio-temporal%20video%20understanding%20in%20complex%20roadside%20traffic%20scenarios.%20The%0Adataset%20comprises%201%2C000%20videos%2C%20featuring%2085%2C000%20multiple-choice%20QA%20pairs%2C%0A2%2C300%20object%20captioning%2C%20and%205%2C700%20object%20grounding%20annotations%2C%20encompassing%0Adiverse%20real-world%20conditions%20such%20as%20adverse%20weather%20and%20traffic%20anomalies.%20By%0Aincorporating%20tuple-based%20spatio-temporal%20object%20expressions%2C%0ATUMTraffic-VideoQA%20unifies%20three%20essential%20tasks-multiple-choice%20video%20question%0Aanswering%2C%20referred%20object%20captioning%2C%20and%20spatio-temporal%20object%0Agrounding-within%20a%20cohesive%20evaluation%20framework.%20We%20further%20introduce%20the%0ATUMTraffic-Qwen%20baseline%20model%2C%20enhanced%20with%20visual%20token%20sampling%20strategies%2C%0Aproviding%20valuable%20insights%20into%20the%20challenges%20of%20fine-grained%20spatio-temporal%0Areasoning.%20Extensive%20experiments%20demonstrate%20the%20dataset%27s%20complexity%2C%0Ahighlight%20the%20limitations%20of%20existing%20models%2C%20and%20position%20TUMTraffic-VideoQA%0Aas%20a%20robust%20foundation%20for%20advancing%20research%20in%20intelligent%20transportation%0Asystems.%20The%20dataset%20and%20benchmark%20are%20publicly%20available%20to%20facilitate%20further%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02449v1&entry.124074799=Read"},
{"title": "A User Guide to Sampling Strategies for Sliced Optimal Transport", "author": "Keanu Sisouk and Julie Delon and Julien Tierny", "abstract": "  This paper serves as a user guide to sampling strategies for sliced optimal\ntransport. We provide reminders and additional regularity results on the Sliced\nWasserstein distance. We detail the construction methods, generation time\ncomplexity, theoretical guarantees, and conditions for each strategy.\nAdditionally, we provide insights into their suitability for sliced optimal\ntransport in theory. Extensive experiments on both simulated and real-world\ndata offer a representative comparison of the strategies, culminating in\npractical recommendations for their best usage.\n", "link": "http://arxiv.org/abs/2502.02275v1", "date": "2025-02-04", "relevancy": 2.1566, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4294}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20User%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&body=Title%3A%20A%20User%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport%0AAuthor%3A%20Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20serves%20as%20a%20user%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520User%2520Guide%2520to%2520Sampling%2520Strategies%2520for%2520Sliced%2520Optimal%2520Transport%26entry.906535625%3DKeanu%2520Sisouk%2520and%2520Julie%2520Delon%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520serves%2520as%2520a%2520user%2520guide%2520to%2520sampling%2520strategies%2520for%2520sliced%2520optimal%250Atransport.%2520We%2520provide%2520reminders%2520and%2520additional%2520regularity%2520results%2520on%2520the%2520Sliced%250AWasserstein%2520distance.%2520We%2520detail%2520the%2520construction%2520methods%252C%2520generation%2520time%250Acomplexity%252C%2520theoretical%2520guarantees%252C%2520and%2520conditions%2520for%2520each%2520strategy.%250AAdditionally%252C%2520we%2520provide%2520insights%2520into%2520their%2520suitability%2520for%2520sliced%2520optimal%250Atransport%2520in%2520theory.%2520Extensive%2520experiments%2520on%2520both%2520simulated%2520and%2520real-world%250Adata%2520offer%2520a%2520representative%2520comparison%2520of%2520the%2520strategies%252C%2520culminating%2520in%250Apractical%2520recommendations%2520for%2520their%2520best%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20User%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&entry.906535625=Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20serves%20as%20a%20user%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02275v1&entry.124074799=Read"},
{"title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in\n  Multimodal Large Language Models", "author": "Tzu-Tao Chang and Shivaram Venkataraman", "abstract": "  Cross-attention is commonly adopted in multimodal large language models\n(MLLMs) for integrating visual information into the language backbone. However,\nin applications with large visual inputs, such as video understanding,\nprocessing a large number of visual tokens in cross-attention layers leads to\nhigh memory demands and often necessitates distributed computation across\nmultiple GPUs. Existing distributed attention mechanisms face significant\ncommunication overheads, making cross-attention layers a critical bottleneck\nfor efficient training and inference of MLLMs. To address this, we propose\nLV-XAttn, a distributed, exact cross-attention mechanism with minimal\ncommunication overhead. We observe that in applications involving large visual\ninputs the size of the query block is typically much smaller than that of the\nkey-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally\non each GPU and exchange smaller query blocks across GPUs. We also introduce an\nefficient activation recomputation technique enabling support for longer visual\ncontext. We theoretically analyze the communication benefits of LV-XAttn and\nshow that it can achieve speedups for a wide range of models. Our evaluations\nwith mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to\n5.58$\\times$ end-to-end speedup compared to existing approaches.\n", "link": "http://arxiv.org/abs/2502.02406v1", "date": "2025-02-04", "relevancy": 2.1419, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5388}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LV-XAttn%3A%20Distributed%20Cross-Attention%20for%20Long%20Visual%20Inputs%20in%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20LV-XAttn%3A%20Distributed%20Cross-Attention%20for%20Long%20Visual%20Inputs%20in%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Tzu-Tao%20Chang%20and%20Shivaram%20Venkataraman%0AAbstract%3A%20%20%20Cross-attention%20is%20commonly%20adopted%20in%20multimodal%20large%20language%20models%0A%28MLLMs%29%20for%20integrating%20visual%20information%20into%20the%20language%20backbone.%20However%2C%0Ain%20applications%20with%20large%20visual%20inputs%2C%20such%20as%20video%20understanding%2C%0Aprocessing%20a%20large%20number%20of%20visual%20tokens%20in%20cross-attention%20layers%20leads%20to%0Ahigh%20memory%20demands%20and%20often%20necessitates%20distributed%20computation%20across%0Amultiple%20GPUs.%20Existing%20distributed%20attention%20mechanisms%20face%20significant%0Acommunication%20overheads%2C%20making%20cross-attention%20layers%20a%20critical%20bottleneck%0Afor%20efficient%20training%20and%20inference%20of%20MLLMs.%20To%20address%20this%2C%20we%20propose%0ALV-XAttn%2C%20a%20distributed%2C%20exact%20cross-attention%20mechanism%20with%20minimal%0Acommunication%20overhead.%20We%20observe%20that%20in%20applications%20involving%20large%20visual%0Ainputs%20the%20size%20of%20the%20query%20block%20is%20typically%20much%20smaller%20than%20that%20of%20the%0Akey-value%20blocks.%20Thus%2C%20in%20LV-XAttn%20we%20keep%20the%20large%20key-value%20blocks%20locally%0Aon%20each%20GPU%20and%20exchange%20smaller%20query%20blocks%20across%20GPUs.%20We%20also%20introduce%20an%0Aefficient%20activation%20recomputation%20technique%20enabling%20support%20for%20longer%20visual%0Acontext.%20We%20theoretically%20analyze%20the%20communication%20benefits%20of%20LV-XAttn%20and%0Ashow%20that%20it%20can%20achieve%20speedups%20for%20a%20wide%20range%20of%20models.%20Our%20evaluations%0Awith%20mPLUG-Owl3%20and%20OpenFlamingo%20models%20find%20that%20LV-XAttn%20achieves%20up%20to%0A5.58%24%5Ctimes%24%20end-to-end%20speedup%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLV-XAttn%253A%2520Distributed%2520Cross-Attention%2520for%2520Long%2520Visual%2520Inputs%2520in%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DTzu-Tao%2520Chang%2520and%2520Shivaram%2520Venkataraman%26entry.1292438233%3D%2520%2520Cross-attention%2520is%2520commonly%2520adopted%2520in%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520for%2520integrating%2520visual%2520information%2520into%2520the%2520language%2520backbone.%2520However%252C%250Ain%2520applications%2520with%2520large%2520visual%2520inputs%252C%2520such%2520as%2520video%2520understanding%252C%250Aprocessing%2520a%2520large%2520number%2520of%2520visual%2520tokens%2520in%2520cross-attention%2520layers%2520leads%2520to%250Ahigh%2520memory%2520demands%2520and%2520often%2520necessitates%2520distributed%2520computation%2520across%250Amultiple%2520GPUs.%2520Existing%2520distributed%2520attention%2520mechanisms%2520face%2520significant%250Acommunication%2520overheads%252C%2520making%2520cross-attention%2520layers%2520a%2520critical%2520bottleneck%250Afor%2520efficient%2520training%2520and%2520inference%2520of%2520MLLMs.%2520To%2520address%2520this%252C%2520we%2520propose%250ALV-XAttn%252C%2520a%2520distributed%252C%2520exact%2520cross-attention%2520mechanism%2520with%2520minimal%250Acommunication%2520overhead.%2520We%2520observe%2520that%2520in%2520applications%2520involving%2520large%2520visual%250Ainputs%2520the%2520size%2520of%2520the%2520query%2520block%2520is%2520typically%2520much%2520smaller%2520than%2520that%2520of%2520the%250Akey-value%2520blocks.%2520Thus%252C%2520in%2520LV-XAttn%2520we%2520keep%2520the%2520large%2520key-value%2520blocks%2520locally%250Aon%2520each%2520GPU%2520and%2520exchange%2520smaller%2520query%2520blocks%2520across%2520GPUs.%2520We%2520also%2520introduce%2520an%250Aefficient%2520activation%2520recomputation%2520technique%2520enabling%2520support%2520for%2520longer%2520visual%250Acontext.%2520We%2520theoretically%2520analyze%2520the%2520communication%2520benefits%2520of%2520LV-XAttn%2520and%250Ashow%2520that%2520it%2520can%2520achieve%2520speedups%2520for%2520a%2520wide%2520range%2520of%2520models.%2520Our%2520evaluations%250Awith%2520mPLUG-Owl3%2520and%2520OpenFlamingo%2520models%2520find%2520that%2520LV-XAttn%2520achieves%2520up%2520to%250A5.58%2524%255Ctimes%2524%2520end-to-end%2520speedup%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LV-XAttn%3A%20Distributed%20Cross-Attention%20for%20Long%20Visual%20Inputs%20in%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Tzu-Tao%20Chang%20and%20Shivaram%20Venkataraman&entry.1292438233=%20%20Cross-attention%20is%20commonly%20adopted%20in%20multimodal%20large%20language%20models%0A%28MLLMs%29%20for%20integrating%20visual%20information%20into%20the%20language%20backbone.%20However%2C%0Ain%20applications%20with%20large%20visual%20inputs%2C%20such%20as%20video%20understanding%2C%0Aprocessing%20a%20large%20number%20of%20visual%20tokens%20in%20cross-attention%20layers%20leads%20to%0Ahigh%20memory%20demands%20and%20often%20necessitates%20distributed%20computation%20across%0Amultiple%20GPUs.%20Existing%20distributed%20attention%20mechanisms%20face%20significant%0Acommunication%20overheads%2C%20making%20cross-attention%20layers%20a%20critical%20bottleneck%0Afor%20efficient%20training%20and%20inference%20of%20MLLMs.%20To%20address%20this%2C%20we%20propose%0ALV-XAttn%2C%20a%20distributed%2C%20exact%20cross-attention%20mechanism%20with%20minimal%0Acommunication%20overhead.%20We%20observe%20that%20in%20applications%20involving%20large%20visual%0Ainputs%20the%20size%20of%20the%20query%20block%20is%20typically%20much%20smaller%20than%20that%20of%20the%0Akey-value%20blocks.%20Thus%2C%20in%20LV-XAttn%20we%20keep%20the%20large%20key-value%20blocks%20locally%0Aon%20each%20GPU%20and%20exchange%20smaller%20query%20blocks%20across%20GPUs.%20We%20also%20introduce%20an%0Aefficient%20activation%20recomputation%20technique%20enabling%20support%20for%20longer%20visual%0Acontext.%20We%20theoretically%20analyze%20the%20communication%20benefits%20of%20LV-XAttn%20and%0Ashow%20that%20it%20can%20achieve%20speedups%20for%20a%20wide%20range%20of%20models.%20Our%20evaluations%0Awith%20mPLUG-Owl3%20and%20OpenFlamingo%20models%20find%20that%20LV-XAttn%20achieves%20up%20to%0A5.58%24%5Ctimes%24%20end-to-end%20speedup%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02406v1&entry.124074799=Read"},
{"title": "Learning Partial Graph Matching via Optimal Partial Transport", "author": "Gathika Ratnayaka and James Nichols and Qing Wang", "abstract": "  Partial graph matching extends traditional graph matching by allowing some\nnodes to remain unmatched, enabling applications in more complex scenarios.\nHowever, this flexibility introduces additional complexity, as both the subset\nof nodes to match and the optimal mapping must be determined. While recent\nstudies have explored deep learning techniques for partial graph matching, a\nsignificant limitation remains: the absence of an optimization objective that\nfully captures the problem's intrinsic nature while enabling efficient\nsolutions. In this paper, we propose a novel optimization framework for partial\ngraph matching, inspired by optimal partial transport. Our approach formulates\nan objective that enables partial assignments while incorporating matching\nbiases, using weighted total variation as the divergence function to guarantee\noptimal partial assignments. Our method can achieve efficient, exact solutions\nwithin cubic worst case time complexity. Our contributions are threefold: (i)\nwe introduce a novel optimization objective that balances matched and unmatched\nnodes; (ii) we establish a connection between partial graph matching and linear\nsum assignment problem, enabling efficient solutions; (iii) we propose a deep\ngraph matching architecture with a novel partial matching loss, providing an\nend-to-end solution. The empirical evaluations on standard graph matching\nbenchmarks demonstrate the efficacy of the proposed approach.\n", "link": "http://arxiv.org/abs/2410.16718v4", "date": "2025-02-04", "relevancy": 2.1362, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Partial%20Graph%20Matching%20via%20Optimal%20Partial%20Transport&body=Title%3A%20Learning%20Partial%20Graph%20Matching%20via%20Optimal%20Partial%20Transport%0AAuthor%3A%20Gathika%20Ratnayaka%20and%20James%20Nichols%20and%20Qing%20Wang%0AAbstract%3A%20%20%20Partial%20graph%20matching%20extends%20traditional%20graph%20matching%20by%20allowing%20some%0Anodes%20to%20remain%20unmatched%2C%20enabling%20applications%20in%20more%20complex%20scenarios.%0AHowever%2C%20this%20flexibility%20introduces%20additional%20complexity%2C%20as%20both%20the%20subset%0Aof%20nodes%20to%20match%20and%20the%20optimal%20mapping%20must%20be%20determined.%20While%20recent%0Astudies%20have%20explored%20deep%20learning%20techniques%20for%20partial%20graph%20matching%2C%20a%0Asignificant%20limitation%20remains%3A%20the%20absence%20of%20an%20optimization%20objective%20that%0Afully%20captures%20the%20problem%27s%20intrinsic%20nature%20while%20enabling%20efficient%0Asolutions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20optimization%20framework%20for%20partial%0Agraph%20matching%2C%20inspired%20by%20optimal%20partial%20transport.%20Our%20approach%20formulates%0Aan%20objective%20that%20enables%20partial%20assignments%20while%20incorporating%20matching%0Abiases%2C%20using%20weighted%20total%20variation%20as%20the%20divergence%20function%20to%20guarantee%0Aoptimal%20partial%20assignments.%20Our%20method%20can%20achieve%20efficient%2C%20exact%20solutions%0Awithin%20cubic%20worst%20case%20time%20complexity.%20Our%20contributions%20are%20threefold%3A%20%28i%29%0Awe%20introduce%20a%20novel%20optimization%20objective%20that%20balances%20matched%20and%20unmatched%0Anodes%3B%20%28ii%29%20we%20establish%20a%20connection%20between%20partial%20graph%20matching%20and%20linear%0Asum%20assignment%20problem%2C%20enabling%20efficient%20solutions%3B%20%28iii%29%20we%20propose%20a%20deep%0Agraph%20matching%20architecture%20with%20a%20novel%20partial%20matching%20loss%2C%20providing%20an%0Aend-to-end%20solution.%20The%20empirical%20evaluations%20on%20standard%20graph%20matching%0Abenchmarks%20demonstrate%20the%20efficacy%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16718v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Partial%2520Graph%2520Matching%2520via%2520Optimal%2520Partial%2520Transport%26entry.906535625%3DGathika%2520Ratnayaka%2520and%2520James%2520Nichols%2520and%2520Qing%2520Wang%26entry.1292438233%3D%2520%2520Partial%2520graph%2520matching%2520extends%2520traditional%2520graph%2520matching%2520by%2520allowing%2520some%250Anodes%2520to%2520remain%2520unmatched%252C%2520enabling%2520applications%2520in%2520more%2520complex%2520scenarios.%250AHowever%252C%2520this%2520flexibility%2520introduces%2520additional%2520complexity%252C%2520as%2520both%2520the%2520subset%250Aof%2520nodes%2520to%2520match%2520and%2520the%2520optimal%2520mapping%2520must%2520be%2520determined.%2520While%2520recent%250Astudies%2520have%2520explored%2520deep%2520learning%2520techniques%2520for%2520partial%2520graph%2520matching%252C%2520a%250Asignificant%2520limitation%2520remains%253A%2520the%2520absence%2520of%2520an%2520optimization%2520objective%2520that%250Afully%2520captures%2520the%2520problem%2527s%2520intrinsic%2520nature%2520while%2520enabling%2520efficient%250Asolutions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520optimization%2520framework%2520for%2520partial%250Agraph%2520matching%252C%2520inspired%2520by%2520optimal%2520partial%2520transport.%2520Our%2520approach%2520formulates%250Aan%2520objective%2520that%2520enables%2520partial%2520assignments%2520while%2520incorporating%2520matching%250Abiases%252C%2520using%2520weighted%2520total%2520variation%2520as%2520the%2520divergence%2520function%2520to%2520guarantee%250Aoptimal%2520partial%2520assignments.%2520Our%2520method%2520can%2520achieve%2520efficient%252C%2520exact%2520solutions%250Awithin%2520cubic%2520worst%2520case%2520time%2520complexity.%2520Our%2520contributions%2520are%2520threefold%253A%2520%2528i%2529%250Awe%2520introduce%2520a%2520novel%2520optimization%2520objective%2520that%2520balances%2520matched%2520and%2520unmatched%250Anodes%253B%2520%2528ii%2529%2520we%2520establish%2520a%2520connection%2520between%2520partial%2520graph%2520matching%2520and%2520linear%250Asum%2520assignment%2520problem%252C%2520enabling%2520efficient%2520solutions%253B%2520%2528iii%2529%2520we%2520propose%2520a%2520deep%250Agraph%2520matching%2520architecture%2520with%2520a%2520novel%2520partial%2520matching%2520loss%252C%2520providing%2520an%250Aend-to-end%2520solution.%2520The%2520empirical%2520evaluations%2520on%2520standard%2520graph%2520matching%250Abenchmarks%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16718v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Partial%20Graph%20Matching%20via%20Optimal%20Partial%20Transport&entry.906535625=Gathika%20Ratnayaka%20and%20James%20Nichols%20and%20Qing%20Wang&entry.1292438233=%20%20Partial%20graph%20matching%20extends%20traditional%20graph%20matching%20by%20allowing%20some%0Anodes%20to%20remain%20unmatched%2C%20enabling%20applications%20in%20more%20complex%20scenarios.%0AHowever%2C%20this%20flexibility%20introduces%20additional%20complexity%2C%20as%20both%20the%20subset%0Aof%20nodes%20to%20match%20and%20the%20optimal%20mapping%20must%20be%20determined.%20While%20recent%0Astudies%20have%20explored%20deep%20learning%20techniques%20for%20partial%20graph%20matching%2C%20a%0Asignificant%20limitation%20remains%3A%20the%20absence%20of%20an%20optimization%20objective%20that%0Afully%20captures%20the%20problem%27s%20intrinsic%20nature%20while%20enabling%20efficient%0Asolutions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20optimization%20framework%20for%20partial%0Agraph%20matching%2C%20inspired%20by%20optimal%20partial%20transport.%20Our%20approach%20formulates%0Aan%20objective%20that%20enables%20partial%20assignments%20while%20incorporating%20matching%0Abiases%2C%20using%20weighted%20total%20variation%20as%20the%20divergence%20function%20to%20guarantee%0Aoptimal%20partial%20assignments.%20Our%20method%20can%20achieve%20efficient%2C%20exact%20solutions%0Awithin%20cubic%20worst%20case%20time%20complexity.%20Our%20contributions%20are%20threefold%3A%20%28i%29%0Awe%20introduce%20a%20novel%20optimization%20objective%20that%20balances%20matched%20and%20unmatched%0Anodes%3B%20%28ii%29%20we%20establish%20a%20connection%20between%20partial%20graph%20matching%20and%20linear%0Asum%20assignment%20problem%2C%20enabling%20efficient%20solutions%3B%20%28iii%29%20we%20propose%20a%20deep%0Agraph%20matching%20architecture%20with%20a%20novel%20partial%20matching%20loss%2C%20providing%20an%0Aend-to-end%20solution.%20The%20empirical%20evaluations%20on%20standard%20graph%20matching%0Abenchmarks%20demonstrate%20the%20efficacy%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16718v4&entry.124074799=Read"},
{"title": "MIDI-GPT: A Controllable Generative Model for Computer-Assisted\n  Multitrack Music Composition", "author": "Philippe Pasquier and Jeff Ens and Nathan Fradet and Paul Triana and Davide Rizzotti and Jean-Baptiste Rolland and Maryam Safi", "abstract": "  We present and release MIDI-GPT, a generative system based on the Transformer\narchitecture that is designed for computer-assisted music composition\nworkflows. MIDI-GPT supports the infilling of musical material at the track and\nbar level, and can condition generation on attributes including: instrument\ntype, musical style, note density, polyphony level, and note duration. In order\nto integrate these features, we employ an alternative representation for\nmusical material, creating a time-ordered sequence of musical events for each\ntrack and concatenating several tracks into a single sequence, rather than\nusing a single time-ordered sequence where the musical events corresponding to\ndifferent tracks are interleaved. We also propose a variation of our\nrepresentation allowing for expressiveness. We present experimental results\nthat demonstrate that MIDI-GPT is able to consistently avoid duplicating the\nmusical material it was trained on, generate music that is stylistically\nsimilar to the training dataset, and that attribute controls allow enforcing\nvarious constraints on the generated material. We also outline several\nreal-world applications of MIDI-GPT, including collaborations with industry\npartners that explore the integration and evaluation of MIDI-GPT into\ncommercial products, as well as several artistic works produced using it.\n", "link": "http://arxiv.org/abs/2501.17011v2", "date": "2025-02-04", "relevancy": 2.1248, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5433}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5389}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition&body=Title%3A%20MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition%0AAuthor%3A%20Philippe%20Pasquier%20and%20Jeff%20Ens%20and%20Nathan%20Fradet%20and%20Paul%20Triana%20and%20Davide%20Rizzotti%20and%20Jean-Baptiste%20Rolland%20and%20Maryam%20Safi%0AAbstract%3A%20%20%20We%20present%20and%20release%20MIDI-GPT%2C%20a%20generative%20system%20based%20on%20the%20Transformer%0Aarchitecture%20that%20is%20designed%20for%20computer-assisted%20music%20composition%0Aworkflows.%20MIDI-GPT%20supports%20the%20infilling%20of%20musical%20material%20at%20the%20track%20and%0Abar%20level%2C%20and%20can%20condition%20generation%20on%20attributes%20including%3A%20instrument%0Atype%2C%20musical%20style%2C%20note%20density%2C%20polyphony%20level%2C%20and%20note%20duration.%20In%20order%0Ato%20integrate%20these%20features%2C%20we%20employ%20an%20alternative%20representation%20for%0Amusical%20material%2C%20creating%20a%20time-ordered%20sequence%20of%20musical%20events%20for%20each%0Atrack%20and%20concatenating%20several%20tracks%20into%20a%20single%20sequence%2C%20rather%20than%0Ausing%20a%20single%20time-ordered%20sequence%20where%20the%20musical%20events%20corresponding%20to%0Adifferent%20tracks%20are%20interleaved.%20We%20also%20propose%20a%20variation%20of%20our%0Arepresentation%20allowing%20for%20expressiveness.%20We%20present%20experimental%20results%0Athat%20demonstrate%20that%20MIDI-GPT%20is%20able%20to%20consistently%20avoid%20duplicating%20the%0Amusical%20material%20it%20was%20trained%20on%2C%20generate%20music%20that%20is%20stylistically%0Asimilar%20to%20the%20training%20dataset%2C%20and%20that%20attribute%20controls%20allow%20enforcing%0Avarious%20constraints%20on%20the%20generated%20material.%20We%20also%20outline%20several%0Areal-world%20applications%20of%20MIDI-GPT%2C%20including%20collaborations%20with%20industry%0Apartners%20that%20explore%20the%20integration%20and%20evaluation%20of%20MIDI-GPT%20into%0Acommercial%20products%2C%20as%20well%20as%20several%20artistic%20works%20produced%20using%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIDI-GPT%253A%2520A%2520Controllable%2520Generative%2520Model%2520for%2520Computer-Assisted%250A%2520%2520Multitrack%2520Music%2520Composition%26entry.906535625%3DPhilippe%2520Pasquier%2520and%2520Jeff%2520Ens%2520and%2520Nathan%2520Fradet%2520and%2520Paul%2520Triana%2520and%2520Davide%2520Rizzotti%2520and%2520Jean-Baptiste%2520Rolland%2520and%2520Maryam%2520Safi%26entry.1292438233%3D%2520%2520We%2520present%2520and%2520release%2520MIDI-GPT%252C%2520a%2520generative%2520system%2520based%2520on%2520the%2520Transformer%250Aarchitecture%2520that%2520is%2520designed%2520for%2520computer-assisted%2520music%2520composition%250Aworkflows.%2520MIDI-GPT%2520supports%2520the%2520infilling%2520of%2520musical%2520material%2520at%2520the%2520track%2520and%250Abar%2520level%252C%2520and%2520can%2520condition%2520generation%2520on%2520attributes%2520including%253A%2520instrument%250Atype%252C%2520musical%2520style%252C%2520note%2520density%252C%2520polyphony%2520level%252C%2520and%2520note%2520duration.%2520In%2520order%250Ato%2520integrate%2520these%2520features%252C%2520we%2520employ%2520an%2520alternative%2520representation%2520for%250Amusical%2520material%252C%2520creating%2520a%2520time-ordered%2520sequence%2520of%2520musical%2520events%2520for%2520each%250Atrack%2520and%2520concatenating%2520several%2520tracks%2520into%2520a%2520single%2520sequence%252C%2520rather%2520than%250Ausing%2520a%2520single%2520time-ordered%2520sequence%2520where%2520the%2520musical%2520events%2520corresponding%2520to%250Adifferent%2520tracks%2520are%2520interleaved.%2520We%2520also%2520propose%2520a%2520variation%2520of%2520our%250Arepresentation%2520allowing%2520for%2520expressiveness.%2520We%2520present%2520experimental%2520results%250Athat%2520demonstrate%2520that%2520MIDI-GPT%2520is%2520able%2520to%2520consistently%2520avoid%2520duplicating%2520the%250Amusical%2520material%2520it%2520was%2520trained%2520on%252C%2520generate%2520music%2520that%2520is%2520stylistically%250Asimilar%2520to%2520the%2520training%2520dataset%252C%2520and%2520that%2520attribute%2520controls%2520allow%2520enforcing%250Avarious%2520constraints%2520on%2520the%2520generated%2520material.%2520We%2520also%2520outline%2520several%250Areal-world%2520applications%2520of%2520MIDI-GPT%252C%2520including%2520collaborations%2520with%2520industry%250Apartners%2520that%2520explore%2520the%2520integration%2520and%2520evaluation%2520of%2520MIDI-GPT%2520into%250Acommercial%2520products%252C%2520as%2520well%2520as%2520several%2520artistic%2520works%2520produced%2520using%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition&entry.906535625=Philippe%20Pasquier%20and%20Jeff%20Ens%20and%20Nathan%20Fradet%20and%20Paul%20Triana%20and%20Davide%20Rizzotti%20and%20Jean-Baptiste%20Rolland%20and%20Maryam%20Safi&entry.1292438233=%20%20We%20present%20and%20release%20MIDI-GPT%2C%20a%20generative%20system%20based%20on%20the%20Transformer%0Aarchitecture%20that%20is%20designed%20for%20computer-assisted%20music%20composition%0Aworkflows.%20MIDI-GPT%20supports%20the%20infilling%20of%20musical%20material%20at%20the%20track%20and%0Abar%20level%2C%20and%20can%20condition%20generation%20on%20attributes%20including%3A%20instrument%0Atype%2C%20musical%20style%2C%20note%20density%2C%20polyphony%20level%2C%20and%20note%20duration.%20In%20order%0Ato%20integrate%20these%20features%2C%20we%20employ%20an%20alternative%20representation%20for%0Amusical%20material%2C%20creating%20a%20time-ordered%20sequence%20of%20musical%20events%20for%20each%0Atrack%20and%20concatenating%20several%20tracks%20into%20a%20single%20sequence%2C%20rather%20than%0Ausing%20a%20single%20time-ordered%20sequence%20where%20the%20musical%20events%20corresponding%20to%0Adifferent%20tracks%20are%20interleaved.%20We%20also%20propose%20a%20variation%20of%20our%0Arepresentation%20allowing%20for%20expressiveness.%20We%20present%20experimental%20results%0Athat%20demonstrate%20that%20MIDI-GPT%20is%20able%20to%20consistently%20avoid%20duplicating%20the%0Amusical%20material%20it%20was%20trained%20on%2C%20generate%20music%20that%20is%20stylistically%0Asimilar%20to%20the%20training%20dataset%2C%20and%20that%20attribute%20controls%20allow%20enforcing%0Avarious%20constraints%20on%20the%20generated%20material.%20We%20also%20outline%20several%0Areal-world%20applications%20of%20MIDI-GPT%2C%20including%20collaborations%20with%20industry%0Apartners%20that%20explore%20the%20integration%20and%20evaluation%20of%20MIDI-GPT%20into%0Acommercial%20products%2C%20as%20well%20as%20several%20artistic%20works%20produced%20using%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17011v2&entry.124074799=Read"},
{"title": "MAGNNET: Multi-Agent Graph Neural Network-based Efficient Task\n  Allocation for Autonomous Vehicles with Deep Reinforcement Learning", "author": "Lavanya Ratnabala and Aleksey Fedoseev and Robinroy Peter and Dzmitry Tsetserukou", "abstract": "  This paper addresses the challenge of decentralized task allocation within\nheterogeneous multi-agent systems operating under communication constraints. We\nintroduce a novel framework that integrates graph neural networks (GNNs) with a\ncentralized training and decentralized execution (CTDE) paradigm, further\nenhanced by a tailored Proximal Policy Optimization (PPO) algorithm for\nmulti-agent deep reinforcement learning (MARL). Our approach enables unmanned\naerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to dynamically\nallocate tasks efficiently without necessitating central coordination in a 3D\ngrid environment. The framework minimizes total travel time while\nsimultaneously avoiding conflicts in task assignments. For the cost calculation\nand routing, we employ reservation-based A* and R* path planners. Experimental\nresults revealed that our method achieves a high 92.5% conflict-free success\nrate, with only a 7.49% performance gap compared to the centralized Hungarian\nmethod, while outperforming the heuristic decentralized baseline based on\ngreedy approach. Additionally, the framework exhibits scalability with up to 20\nagents with allocation processing of 2.8 s and robustness in responding to\ndynamically generated tasks, underscoring its potential for real-world\napplications in complex multi-agent scenarios.\n", "link": "http://arxiv.org/abs/2502.02311v1", "date": "2025-02-04", "relevancy": 2.1241, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.539}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGNNET%3A%20Multi-Agent%20Graph%20Neural%20Network-based%20Efficient%20Task%0A%20%20Allocation%20for%20Autonomous%20Vehicles%20with%20Deep%20Reinforcement%20Learning&body=Title%3A%20MAGNNET%3A%20Multi-Agent%20Graph%20Neural%20Network-based%20Efficient%20Task%0A%20%20Allocation%20for%20Autonomous%20Vehicles%20with%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Lavanya%20Ratnabala%20and%20Aleksey%20Fedoseev%20and%20Robinroy%20Peter%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20decentralized%20task%20allocation%20within%0Aheterogeneous%20multi-agent%20systems%20operating%20under%20communication%20constraints.%20We%0Aintroduce%20a%20novel%20framework%20that%20integrates%20graph%20neural%20networks%20%28GNNs%29%20with%20a%0Acentralized%20training%20and%20decentralized%20execution%20%28CTDE%29%20paradigm%2C%20further%0Aenhanced%20by%20a%20tailored%20Proximal%20Policy%20Optimization%20%28PPO%29%20algorithm%20for%0Amulti-agent%20deep%20reinforcement%20learning%20%28MARL%29.%20Our%20approach%20enables%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20and%20unmanned%20ground%20vehicles%20%28UGVs%29%20to%20dynamically%0Aallocate%20tasks%20efficiently%20without%20necessitating%20central%20coordination%20in%20a%203D%0Agrid%20environment.%20The%20framework%20minimizes%20total%20travel%20time%20while%0Asimultaneously%20avoiding%20conflicts%20in%20task%20assignments.%20For%20the%20cost%20calculation%0Aand%20routing%2C%20we%20employ%20reservation-based%20A%2A%20and%20R%2A%20path%20planners.%20Experimental%0Aresults%20revealed%20that%20our%20method%20achieves%20a%20high%2092.5%25%20conflict-free%20success%0Arate%2C%20with%20only%20a%207.49%25%20performance%20gap%20compared%20to%20the%20centralized%20Hungarian%0Amethod%2C%20while%20outperforming%20the%20heuristic%20decentralized%20baseline%20based%20on%0Agreedy%20approach.%20Additionally%2C%20the%20framework%20exhibits%20scalability%20with%20up%20to%2020%0Aagents%20with%20allocation%20processing%20of%202.8%20s%20and%20robustness%20in%20responding%20to%0Adynamically%20generated%20tasks%2C%20underscoring%20its%20potential%20for%20real-world%0Aapplications%20in%20complex%20multi-agent%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGNNET%253A%2520Multi-Agent%2520Graph%2520Neural%2520Network-based%2520Efficient%2520Task%250A%2520%2520Allocation%2520for%2520Autonomous%2520Vehicles%2520with%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DLavanya%2520Ratnabala%2520and%2520Aleksey%2520Fedoseev%2520and%2520Robinroy%2520Peter%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520decentralized%2520task%2520allocation%2520within%250Aheterogeneous%2520multi-agent%2520systems%2520operating%2520under%2520communication%2520constraints.%2520We%250Aintroduce%2520a%2520novel%2520framework%2520that%2520integrates%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520with%2520a%250Acentralized%2520training%2520and%2520decentralized%2520execution%2520%2528CTDE%2529%2520paradigm%252C%2520further%250Aenhanced%2520by%2520a%2520tailored%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520algorithm%2520for%250Amulti-agent%2520deep%2520reinforcement%2520learning%2520%2528MARL%2529.%2520Our%2520approach%2520enables%2520unmanned%250Aaerial%2520vehicles%2520%2528UAVs%2529%2520and%2520unmanned%2520ground%2520vehicles%2520%2528UGVs%2529%2520to%2520dynamically%250Aallocate%2520tasks%2520efficiently%2520without%2520necessitating%2520central%2520coordination%2520in%2520a%25203D%250Agrid%2520environment.%2520The%2520framework%2520minimizes%2520total%2520travel%2520time%2520while%250Asimultaneously%2520avoiding%2520conflicts%2520in%2520task%2520assignments.%2520For%2520the%2520cost%2520calculation%250Aand%2520routing%252C%2520we%2520employ%2520reservation-based%2520A%252A%2520and%2520R%252A%2520path%2520planners.%2520Experimental%250Aresults%2520revealed%2520that%2520our%2520method%2520achieves%2520a%2520high%252092.5%2525%2520conflict-free%2520success%250Arate%252C%2520with%2520only%2520a%25207.49%2525%2520performance%2520gap%2520compared%2520to%2520the%2520centralized%2520Hungarian%250Amethod%252C%2520while%2520outperforming%2520the%2520heuristic%2520decentralized%2520baseline%2520based%2520on%250Agreedy%2520approach.%2520Additionally%252C%2520the%2520framework%2520exhibits%2520scalability%2520with%2520up%2520to%252020%250Aagents%2520with%2520allocation%2520processing%2520of%25202.8%2520s%2520and%2520robustness%2520in%2520responding%2520to%250Adynamically%2520generated%2520tasks%252C%2520underscoring%2520its%2520potential%2520for%2520real-world%250Aapplications%2520in%2520complex%2520multi-agent%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGNNET%3A%20Multi-Agent%20Graph%20Neural%20Network-based%20Efficient%20Task%0A%20%20Allocation%20for%20Autonomous%20Vehicles%20with%20Deep%20Reinforcement%20Learning&entry.906535625=Lavanya%20Ratnabala%20and%20Aleksey%20Fedoseev%20and%20Robinroy%20Peter%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20decentralized%20task%20allocation%20within%0Aheterogeneous%20multi-agent%20systems%20operating%20under%20communication%20constraints.%20We%0Aintroduce%20a%20novel%20framework%20that%20integrates%20graph%20neural%20networks%20%28GNNs%29%20with%20a%0Acentralized%20training%20and%20decentralized%20execution%20%28CTDE%29%20paradigm%2C%20further%0Aenhanced%20by%20a%20tailored%20Proximal%20Policy%20Optimization%20%28PPO%29%20algorithm%20for%0Amulti-agent%20deep%20reinforcement%20learning%20%28MARL%29.%20Our%20approach%20enables%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20and%20unmanned%20ground%20vehicles%20%28UGVs%29%20to%20dynamically%0Aallocate%20tasks%20efficiently%20without%20necessitating%20central%20coordination%20in%20a%203D%0Agrid%20environment.%20The%20framework%20minimizes%20total%20travel%20time%20while%0Asimultaneously%20avoiding%20conflicts%20in%20task%20assignments.%20For%20the%20cost%20calculation%0Aand%20routing%2C%20we%20employ%20reservation-based%20A%2A%20and%20R%2A%20path%20planners.%20Experimental%0Aresults%20revealed%20that%20our%20method%20achieves%20a%20high%2092.5%25%20conflict-free%20success%0Arate%2C%20with%20only%20a%207.49%25%20performance%20gap%20compared%20to%20the%20centralized%20Hungarian%0Amethod%2C%20while%20outperforming%20the%20heuristic%20decentralized%20baseline%20based%20on%0Agreedy%20approach.%20Additionally%2C%20the%20framework%20exhibits%20scalability%20with%20up%20to%2020%0Aagents%20with%20allocation%20processing%20of%202.8%20s%20and%20robustness%20in%20responding%20to%0Adynamically%20generated%20tasks%2C%20underscoring%20its%20potential%20for%20real-world%0Aapplications%20in%20complex%20multi-agent%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02311v1&entry.124074799=Read"},
{"title": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models", "author": "Jiangong Chen and Xiaoyi Wu and Tian Lan and Bin Li", "abstract": "  The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization.\n", "link": "http://arxiv.org/abs/2502.02441v1", "date": "2025-02-04", "relevancy": 2.1191, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMER%3A%20Crafting%20Interactive%20Extended%20Reality%20Worlds%20with%20JSON%20Data%0A%20%20Generated%20by%20Large%20Language%20Models&body=Title%3A%20LLMER%3A%20Crafting%20Interactive%20Extended%20Reality%20Worlds%20with%20JSON%20Data%0A%20%20Generated%20by%20Large%20Language%20Models%0AAuthor%3A%20Jiangong%20Chen%20and%20Xiaoyi%20Wu%20and%20Tian%20Lan%20and%20Bin%20Li%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%20with%20Extended%0AReality%20%28XR%29%20technologies%20offers%20the%20potential%20to%20build%20truly%20immersive%20XR%0Aenvironments%20that%20interact%20with%20human%20users%20through%20natural%20language%2C%20e.g.%2C%0Agenerating%20and%20animating%203D%20scenes%20from%20audio%20inputs.%20However%2C%20the%20complexity%0Aof%20XR%20environments%20makes%20it%20difficult%20to%20accurately%20extract%20relevant%20contextual%0Adata%20and%20scene/object%20parameters%20from%20an%20overwhelming%20volume%20of%20XR%20artifacts.%0AIt%20leads%20to%20not%20only%20increased%20costs%20with%20pay-per-use%20models%2C%20but%20also%20elevated%0Alevels%20of%20generation%20errors.%20Moreover%2C%20existing%20approaches%20focusing%20on%20coding%0Ascript%20generation%20are%20often%20prone%20to%20generation%20errors%2C%20resulting%20in%20flawed%20or%0Ainvalid%20scripts%2C%20application%20crashes%2C%20and%20ultimately%20a%20degraded%20user%0Aexperience.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20LLMER%2C%20a%20novel%20framework%0Athat%20creates%20interactive%20XR%20worlds%20using%20JSON%20data%20generated%20by%20LLMs.%20Unlike%0Aprior%20approaches%20focusing%20on%20coding%20script%20generation%2C%20LLMER%20translates%20natural%0Alanguage%20inputs%20into%20JSON%20data%2C%20significantly%20reducing%20the%20likelihood%20of%0Aapplication%20crashes%20and%20processing%20latency.%20It%20employs%20a%20multi-stage%20strategy%0Ato%20supply%20only%20the%20essential%20contextual%20information%20adapted%20to%20the%20user%27s%0Arequest%20and%20features%20multiple%20modules%20designed%20for%20various%20XR%20tasks.%20Our%0Apreliminary%20user%20study%20reveals%20the%20effectiveness%20of%20the%20proposed%20system%2C%20with%0Aover%2080%25%20reduction%20in%20consumed%20tokens%20and%20around%2060%25%20reduction%20in%20task%0Acompletion%20time%20compared%20to%20state-of-the-art%20approaches.%20The%20analysis%20of%20users%27%0Afeedback%20also%20illuminates%20a%20series%20of%20directions%20for%20further%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMER%253A%2520Crafting%2520Interactive%2520Extended%2520Reality%2520Worlds%2520with%2520JSON%2520Data%250A%2520%2520Generated%2520by%2520Large%2520Language%2520Models%26entry.906535625%3DJiangong%2520Chen%2520and%2520Xiaoyi%2520Wu%2520and%2520Tian%2520Lan%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520like%2520GPT-4%2520with%2520Extended%250AReality%2520%2528XR%2529%2520technologies%2520offers%2520the%2520potential%2520to%2520build%2520truly%2520immersive%2520XR%250Aenvironments%2520that%2520interact%2520with%2520human%2520users%2520through%2520natural%2520language%252C%2520e.g.%252C%250Agenerating%2520and%2520animating%25203D%2520scenes%2520from%2520audio%2520inputs.%2520However%252C%2520the%2520complexity%250Aof%2520XR%2520environments%2520makes%2520it%2520difficult%2520to%2520accurately%2520extract%2520relevant%2520contextual%250Adata%2520and%2520scene/object%2520parameters%2520from%2520an%2520overwhelming%2520volume%2520of%2520XR%2520artifacts.%250AIt%2520leads%2520to%2520not%2520only%2520increased%2520costs%2520with%2520pay-per-use%2520models%252C%2520but%2520also%2520elevated%250Alevels%2520of%2520generation%2520errors.%2520Moreover%252C%2520existing%2520approaches%2520focusing%2520on%2520coding%250Ascript%2520generation%2520are%2520often%2520prone%2520to%2520generation%2520errors%252C%2520resulting%2520in%2520flawed%2520or%250Ainvalid%2520scripts%252C%2520application%2520crashes%252C%2520and%2520ultimately%2520a%2520degraded%2520user%250Aexperience.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520LLMER%252C%2520a%2520novel%2520framework%250Athat%2520creates%2520interactive%2520XR%2520worlds%2520using%2520JSON%2520data%2520generated%2520by%2520LLMs.%2520Unlike%250Aprior%2520approaches%2520focusing%2520on%2520coding%2520script%2520generation%252C%2520LLMER%2520translates%2520natural%250Alanguage%2520inputs%2520into%2520JSON%2520data%252C%2520significantly%2520reducing%2520the%2520likelihood%2520of%250Aapplication%2520crashes%2520and%2520processing%2520latency.%2520It%2520employs%2520a%2520multi-stage%2520strategy%250Ato%2520supply%2520only%2520the%2520essential%2520contextual%2520information%2520adapted%2520to%2520the%2520user%2527s%250Arequest%2520and%2520features%2520multiple%2520modules%2520designed%2520for%2520various%2520XR%2520tasks.%2520Our%250Apreliminary%2520user%2520study%2520reveals%2520the%2520effectiveness%2520of%2520the%2520proposed%2520system%252C%2520with%250Aover%252080%2525%2520reduction%2520in%2520consumed%2520tokens%2520and%2520around%252060%2525%2520reduction%2520in%2520task%250Acompletion%2520time%2520compared%2520to%2520state-of-the-art%2520approaches.%2520The%2520analysis%2520of%2520users%2527%250Afeedback%2520also%2520illuminates%2520a%2520series%2520of%2520directions%2520for%2520further%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMER%3A%20Crafting%20Interactive%20Extended%20Reality%20Worlds%20with%20JSON%20Data%0A%20%20Generated%20by%20Large%20Language%20Models&entry.906535625=Jiangong%20Chen%20and%20Xiaoyi%20Wu%20and%20Tian%20Lan%20and%20Bin%20Li&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%20with%20Extended%0AReality%20%28XR%29%20technologies%20offers%20the%20potential%20to%20build%20truly%20immersive%20XR%0Aenvironments%20that%20interact%20with%20human%20users%20through%20natural%20language%2C%20e.g.%2C%0Agenerating%20and%20animating%203D%20scenes%20from%20audio%20inputs.%20However%2C%20the%20complexity%0Aof%20XR%20environments%20makes%20it%20difficult%20to%20accurately%20extract%20relevant%20contextual%0Adata%20and%20scene/object%20parameters%20from%20an%20overwhelming%20volume%20of%20XR%20artifacts.%0AIt%20leads%20to%20not%20only%20increased%20costs%20with%20pay-per-use%20models%2C%20but%20also%20elevated%0Alevels%20of%20generation%20errors.%20Moreover%2C%20existing%20approaches%20focusing%20on%20coding%0Ascript%20generation%20are%20often%20prone%20to%20generation%20errors%2C%20resulting%20in%20flawed%20or%0Ainvalid%20scripts%2C%20application%20crashes%2C%20and%20ultimately%20a%20degraded%20user%0Aexperience.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20LLMER%2C%20a%20novel%20framework%0Athat%20creates%20interactive%20XR%20worlds%20using%20JSON%20data%20generated%20by%20LLMs.%20Unlike%0Aprior%20approaches%20focusing%20on%20coding%20script%20generation%2C%20LLMER%20translates%20natural%0Alanguage%20inputs%20into%20JSON%20data%2C%20significantly%20reducing%20the%20likelihood%20of%0Aapplication%20crashes%20and%20processing%20latency.%20It%20employs%20a%20multi-stage%20strategy%0Ato%20supply%20only%20the%20essential%20contextual%20information%20adapted%20to%20the%20user%27s%0Arequest%20and%20features%20multiple%20modules%20designed%20for%20various%20XR%20tasks.%20Our%0Apreliminary%20user%20study%20reveals%20the%20effectiveness%20of%20the%20proposed%20system%2C%20with%0Aover%2080%25%20reduction%20in%20consumed%20tokens%20and%20around%2060%25%20reduction%20in%20task%0Acompletion%20time%20compared%20to%20state-of-the-art%20approaches.%20The%20analysis%20of%20users%27%0Afeedback%20also%20illuminates%20a%20series%20of%20directions%20for%20further%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02441v1&entry.124074799=Read"},
{"title": "Using Random Noise Equivariantly to Boost Graph Neural Networks\n  Universally", "author": "Xiyuan Wang and Muhan Zhang", "abstract": "  Recent advances in Graph Neural Networks (GNNs) have explored the potential\nof random noise as an input feature to enhance expressivity across diverse\ntasks. However, naively incorporating noise can degrade performance, while\narchitectures tailored to exploit noise for specific tasks excel yet lack broad\napplicability. This paper tackles these issues by laying down a theoretical\nframework that elucidates the increased sample complexity when introducing\nrandom noise into GNNs without careful design. We further propose Equivariant\nNoise GNN (ENGNN), a novel architecture that harnesses the symmetrical\nproperties of noise to mitigate sample complexity and bolster generalization.\nOur experiments demonstrate that using noise equivariantly significantly\nenhances performance on node-level, link-level, subgraph, and graph-level tasks\nand achieves comparable performance to models designed for specific tasks,\nthereby offering a general method to boost expressivity across various graph\ntasks.\n", "link": "http://arxiv.org/abs/2502.02479v1", "date": "2025-02-04", "relevancy": 2.1151, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5525}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.517}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Random%20Noise%20Equivariantly%20to%20Boost%20Graph%20Neural%20Networks%0A%20%20Universally&body=Title%3A%20Using%20Random%20Noise%20Equivariantly%20to%20Boost%20Graph%20Neural%20Networks%0A%20%20Universally%0AAuthor%3A%20Xiyuan%20Wang%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20explored%20the%20potential%0Aof%20random%20noise%20as%20an%20input%20feature%20to%20enhance%20expressivity%20across%20diverse%0Atasks.%20However%2C%20naively%20incorporating%20noise%20can%20degrade%20performance%2C%20while%0Aarchitectures%20tailored%20to%20exploit%20noise%20for%20specific%20tasks%20excel%20yet%20lack%20broad%0Aapplicability.%20This%20paper%20tackles%20these%20issues%20by%20laying%20down%20a%20theoretical%0Aframework%20that%20elucidates%20the%20increased%20sample%20complexity%20when%20introducing%0Arandom%20noise%20into%20GNNs%20without%20careful%20design.%20We%20further%20propose%20Equivariant%0ANoise%20GNN%20%28ENGNN%29%2C%20a%20novel%20architecture%20that%20harnesses%20the%20symmetrical%0Aproperties%20of%20noise%20to%20mitigate%20sample%20complexity%20and%20bolster%20generalization.%0AOur%20experiments%20demonstrate%20that%20using%20noise%20equivariantly%20significantly%0Aenhances%20performance%20on%20node-level%2C%20link-level%2C%20subgraph%2C%20and%20graph-level%20tasks%0Aand%20achieves%20comparable%20performance%20to%20models%20designed%20for%20specific%20tasks%2C%0Athereby%20offering%20a%20general%20method%20to%20boost%20expressivity%20across%20various%20graph%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Random%2520Noise%2520Equivariantly%2520to%2520Boost%2520Graph%2520Neural%2520Networks%250A%2520%2520Universally%26entry.906535625%3DXiyuan%2520Wang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520explored%2520the%2520potential%250Aof%2520random%2520noise%2520as%2520an%2520input%2520feature%2520to%2520enhance%2520expressivity%2520across%2520diverse%250Atasks.%2520However%252C%2520naively%2520incorporating%2520noise%2520can%2520degrade%2520performance%252C%2520while%250Aarchitectures%2520tailored%2520to%2520exploit%2520noise%2520for%2520specific%2520tasks%2520excel%2520yet%2520lack%2520broad%250Aapplicability.%2520This%2520paper%2520tackles%2520these%2520issues%2520by%2520laying%2520down%2520a%2520theoretical%250Aframework%2520that%2520elucidates%2520the%2520increased%2520sample%2520complexity%2520when%2520introducing%250Arandom%2520noise%2520into%2520GNNs%2520without%2520careful%2520design.%2520We%2520further%2520propose%2520Equivariant%250ANoise%2520GNN%2520%2528ENGNN%2529%252C%2520a%2520novel%2520architecture%2520that%2520harnesses%2520the%2520symmetrical%250Aproperties%2520of%2520noise%2520to%2520mitigate%2520sample%2520complexity%2520and%2520bolster%2520generalization.%250AOur%2520experiments%2520demonstrate%2520that%2520using%2520noise%2520equivariantly%2520significantly%250Aenhances%2520performance%2520on%2520node-level%252C%2520link-level%252C%2520subgraph%252C%2520and%2520graph-level%2520tasks%250Aand%2520achieves%2520comparable%2520performance%2520to%2520models%2520designed%2520for%2520specific%2520tasks%252C%250Athereby%2520offering%2520a%2520general%2520method%2520to%2520boost%2520expressivity%2520across%2520various%2520graph%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Random%20Noise%20Equivariantly%20to%20Boost%20Graph%20Neural%20Networks%0A%20%20Universally&entry.906535625=Xiyuan%20Wang%20and%20Muhan%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20explored%20the%20potential%0Aof%20random%20noise%20as%20an%20input%20feature%20to%20enhance%20expressivity%20across%20diverse%0Atasks.%20However%2C%20naively%20incorporating%20noise%20can%20degrade%20performance%2C%20while%0Aarchitectures%20tailored%20to%20exploit%20noise%20for%20specific%20tasks%20excel%20yet%20lack%20broad%0Aapplicability.%20This%20paper%20tackles%20these%20issues%20by%20laying%20down%20a%20theoretical%0Aframework%20that%20elucidates%20the%20increased%20sample%20complexity%20when%20introducing%0Arandom%20noise%20into%20GNNs%20without%20careful%20design.%20We%20further%20propose%20Equivariant%0ANoise%20GNN%20%28ENGNN%29%2C%20a%20novel%20architecture%20that%20harnesses%20the%20symmetrical%0Aproperties%20of%20noise%20to%20mitigate%20sample%20complexity%20and%20bolster%20generalization.%0AOur%20experiments%20demonstrate%20that%20using%20noise%20equivariantly%20significantly%0Aenhances%20performance%20on%20node-level%2C%20link-level%2C%20subgraph%2C%20and%20graph-level%20tasks%0Aand%20achieves%20comparable%20performance%20to%20models%20designed%20for%20specific%20tasks%2C%0Athereby%20offering%20a%20general%20method%20to%20boost%20expressivity%20across%20various%20graph%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02479v1&entry.124074799=Read"},
{"title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models", "author": "Kyungmin Lee and Xiaohang Li and Qifei Wang and Junfeng He and Junjie Ke and Ming-Hsuan Yang and Irfan Essa and Jinwoo Shin and Feng Yang and Yinxiao Li", "abstract": "  Aligning text-to-image (T2I) diffusion models with preference optimization is\nvaluable for human-annotated datasets, but the heavy cost of manual data\ncollection limits scalability. Using reward models offers an alternative,\nhowever, current preference optimization methods fall short in exploiting the\nrich information, as they only consider pairwise preference distribution.\nFurthermore, they lack generalization to multi-preference scenarios and\nstruggle to handle inconsistencies between rewards. To address this, we present\nCalibrated Preference Optimization (CaPO), a novel method to align T2I\ndiffusion models by incorporating the general preference from multiple reward\nmodels without human annotated data. The core of our approach involves a reward\ncalibration method to approximate the general preference by computing the\nexpected win-rate against the samples generated by the pretrained models.\nAdditionally, we propose a frontier-based pair selection method that\neffectively manages the multi-preference distribution by selecting pairs from\nPareto frontiers. Finally, we use regression loss to fine-tune diffusion models\nto match the difference between calibrated rewards of a selected pair.\nExperimental results show that CaPO consistently outperforms prior methods,\nsuch as Direct Preference Optimization (DPO), in both single and multi-reward\nsettings validated by evaluation on T2I benchmarks, including GenEval and\nT2I-Compbench.\n", "link": "http://arxiv.org/abs/2502.02588v1", "date": "2025-02-04", "relevancy": 2.1094, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5416}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Multi-Preference%20Optimization%20for%20Aligning%20Diffusion%20Models&body=Title%3A%20Calibrated%20Multi-Preference%20Optimization%20for%20Aligning%20Diffusion%20Models%0AAuthor%3A%20Kyungmin%20Lee%20and%20Xiaohang%20Li%20and%20Qifei%20Wang%20and%20Junfeng%20He%20and%20Junjie%20Ke%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Jinwoo%20Shin%20and%20Feng%20Yang%20and%20Yinxiao%20Li%0AAbstract%3A%20%20%20Aligning%20text-to-image%20%28T2I%29%20diffusion%20models%20with%20preference%20optimization%20is%0Avaluable%20for%20human-annotated%20datasets%2C%20but%20the%20heavy%20cost%20of%20manual%20data%0Acollection%20limits%20scalability.%20Using%20reward%20models%20offers%20an%20alternative%2C%0Ahowever%2C%20current%20preference%20optimization%20methods%20fall%20short%20in%20exploiting%20the%0Arich%20information%2C%20as%20they%20only%20consider%20pairwise%20preference%20distribution.%0AFurthermore%2C%20they%20lack%20generalization%20to%20multi-preference%20scenarios%20and%0Astruggle%20to%20handle%20inconsistencies%20between%20rewards.%20To%20address%20this%2C%20we%20present%0ACalibrated%20Preference%20Optimization%20%28CaPO%29%2C%20a%20novel%20method%20to%20align%20T2I%0Adiffusion%20models%20by%20incorporating%20the%20general%20preference%20from%20multiple%20reward%0Amodels%20without%20human%20annotated%20data.%20The%20core%20of%20our%20approach%20involves%20a%20reward%0Acalibration%20method%20to%20approximate%20the%20general%20preference%20by%20computing%20the%0Aexpected%20win-rate%20against%20the%20samples%20generated%20by%20the%20pretrained%20models.%0AAdditionally%2C%20we%20propose%20a%20frontier-based%20pair%20selection%20method%20that%0Aeffectively%20manages%20the%20multi-preference%20distribution%20by%20selecting%20pairs%20from%0APareto%20frontiers.%20Finally%2C%20we%20use%20regression%20loss%20to%20fine-tune%20diffusion%20models%0Ato%20match%20the%20difference%20between%20calibrated%20rewards%20of%20a%20selected%20pair.%0AExperimental%20results%20show%20that%20CaPO%20consistently%20outperforms%20prior%20methods%2C%0Asuch%20as%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20in%20both%20single%20and%20multi-reward%0Asettings%20validated%20by%20evaluation%20on%20T2I%20benchmarks%2C%20including%20GenEval%20and%0AT2I-Compbench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Multi-Preference%2520Optimization%2520for%2520Aligning%2520Diffusion%2520Models%26entry.906535625%3DKyungmin%2520Lee%2520and%2520Xiaohang%2520Li%2520and%2520Qifei%2520Wang%2520and%2520Junfeng%2520He%2520and%2520Junjie%2520Ke%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Irfan%2520Essa%2520and%2520Jinwoo%2520Shin%2520and%2520Feng%2520Yang%2520and%2520Yinxiao%2520Li%26entry.1292438233%3D%2520%2520Aligning%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520with%2520preference%2520optimization%2520is%250Avaluable%2520for%2520human-annotated%2520datasets%252C%2520but%2520the%2520heavy%2520cost%2520of%2520manual%2520data%250Acollection%2520limits%2520scalability.%2520Using%2520reward%2520models%2520offers%2520an%2520alternative%252C%250Ahowever%252C%2520current%2520preference%2520optimization%2520methods%2520fall%2520short%2520in%2520exploiting%2520the%250Arich%2520information%252C%2520as%2520they%2520only%2520consider%2520pairwise%2520preference%2520distribution.%250AFurthermore%252C%2520they%2520lack%2520generalization%2520to%2520multi-preference%2520scenarios%2520and%250Astruggle%2520to%2520handle%2520inconsistencies%2520between%2520rewards.%2520To%2520address%2520this%252C%2520we%2520present%250ACalibrated%2520Preference%2520Optimization%2520%2528CaPO%2529%252C%2520a%2520novel%2520method%2520to%2520align%2520T2I%250Adiffusion%2520models%2520by%2520incorporating%2520the%2520general%2520preference%2520from%2520multiple%2520reward%250Amodels%2520without%2520human%2520annotated%2520data.%2520The%2520core%2520of%2520our%2520approach%2520involves%2520a%2520reward%250Acalibration%2520method%2520to%2520approximate%2520the%2520general%2520preference%2520by%2520computing%2520the%250Aexpected%2520win-rate%2520against%2520the%2520samples%2520generated%2520by%2520the%2520pretrained%2520models.%250AAdditionally%252C%2520we%2520propose%2520a%2520frontier-based%2520pair%2520selection%2520method%2520that%250Aeffectively%2520manages%2520the%2520multi-preference%2520distribution%2520by%2520selecting%2520pairs%2520from%250APareto%2520frontiers.%2520Finally%252C%2520we%2520use%2520regression%2520loss%2520to%2520fine-tune%2520diffusion%2520models%250Ato%2520match%2520the%2520difference%2520between%2520calibrated%2520rewards%2520of%2520a%2520selected%2520pair.%250AExperimental%2520results%2520show%2520that%2520CaPO%2520consistently%2520outperforms%2520prior%2520methods%252C%250Asuch%2520as%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520in%2520both%2520single%2520and%2520multi-reward%250Asettings%2520validated%2520by%2520evaluation%2520on%2520T2I%2520benchmarks%252C%2520including%2520GenEval%2520and%250AT2I-Compbench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Multi-Preference%20Optimization%20for%20Aligning%20Diffusion%20Models&entry.906535625=Kyungmin%20Lee%20and%20Xiaohang%20Li%20and%20Qifei%20Wang%20and%20Junfeng%20He%20and%20Junjie%20Ke%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Jinwoo%20Shin%20and%20Feng%20Yang%20and%20Yinxiao%20Li&entry.1292438233=%20%20Aligning%20text-to-image%20%28T2I%29%20diffusion%20models%20with%20preference%20optimization%20is%0Avaluable%20for%20human-annotated%20datasets%2C%20but%20the%20heavy%20cost%20of%20manual%20data%0Acollection%20limits%20scalability.%20Using%20reward%20models%20offers%20an%20alternative%2C%0Ahowever%2C%20current%20preference%20optimization%20methods%20fall%20short%20in%20exploiting%20the%0Arich%20information%2C%20as%20they%20only%20consider%20pairwise%20preference%20distribution.%0AFurthermore%2C%20they%20lack%20generalization%20to%20multi-preference%20scenarios%20and%0Astruggle%20to%20handle%20inconsistencies%20between%20rewards.%20To%20address%20this%2C%20we%20present%0ACalibrated%20Preference%20Optimization%20%28CaPO%29%2C%20a%20novel%20method%20to%20align%20T2I%0Adiffusion%20models%20by%20incorporating%20the%20general%20preference%20from%20multiple%20reward%0Amodels%20without%20human%20annotated%20data.%20The%20core%20of%20our%20approach%20involves%20a%20reward%0Acalibration%20method%20to%20approximate%20the%20general%20preference%20by%20computing%20the%0Aexpected%20win-rate%20against%20the%20samples%20generated%20by%20the%20pretrained%20models.%0AAdditionally%2C%20we%20propose%20a%20frontier-based%20pair%20selection%20method%20that%0Aeffectively%20manages%20the%20multi-preference%20distribution%20by%20selecting%20pairs%20from%0APareto%20frontiers.%20Finally%2C%20we%20use%20regression%20loss%20to%20fine-tune%20diffusion%20models%0Ato%20match%20the%20difference%20between%20calibrated%20rewards%20of%20a%20selected%20pair.%0AExperimental%20results%20show%20that%20CaPO%20consistently%20outperforms%20prior%20methods%2C%0Asuch%20as%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20in%20both%20single%20and%20multi-reward%0Asettings%20validated%20by%20evaluation%20on%20T2I%20benchmarks%2C%20including%20GenEval%20and%0AT2I-Compbench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02588v1&entry.124074799=Read"},
{"title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning", "author": "Jianfeng Pan and Senyou Deng and Shaomang Huang", "abstract": "  Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results.\n", "link": "http://arxiv.org/abs/2502.02390v1", "date": "2025-02-04", "relevancy": 2.1057, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoAT%3A%20Chain-of-Associated-Thoughts%20Framework%20for%20Enhancing%20Large%0A%20%20Language%20Models%20Reasoning&body=Title%3A%20CoAT%3A%20Chain-of-Associated-Thoughts%20Framework%20for%20Enhancing%20Large%0A%20%20Language%20Models%20Reasoning%0AAuthor%3A%20Jianfeng%20Pan%20and%20Senyou%20Deng%20and%20Shaomang%20Huang%0AAbstract%3A%20%20%20Research%20on%20LLM%20technologies%20is%20rapidly%20emerging%2C%20with%20most%20of%20them%20employing%0Aa%20%27fast%20thinking%27%20approach%20to%20inference.%20Most%20LLMs%20generate%20the%20final%20result%0Abased%20solely%20on%20a%20single%20query%20and%20LLM%27s%20reasoning%20capabilities.%20However%2C%20with%0Athe%20advent%20of%20OpenAI-o1%2C%20%27slow%20thinking%27%20techniques%20have%20garnered%20increasing%0Aattention%20because%20its%20process%20is%20closer%20to%20the%20human%20thought%20process.%20Inspired%0Aby%20the%20human%20ability%20to%20constantly%20associate%20and%20replenish%20knowledge%20during%0Athinking%2C%20we%20developed%20the%20novel%20Chain-of-Associated-Thoughts%20%28CoAT%29%20framework%2C%0Awhich%20introduces%20an%20innovative%20synergy%20between%20the%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29%20algorithm%20and%20a%20dynamic%20mechanism%20for%20integrating%20new%20key%20information%2C%0Atermed%20%27associative%20memory%27.%20By%20combining%20the%20structured%20exploration%0Acapabilities%20of%20MCTS%20with%20the%20adaptive%20learning%20capacity%20of%20associative%20memory%2C%0ACoAT%20significantly%20expands%20the%20LLM%20search%20space%2C%20enabling%20our%20framework%20to%0Aexplore%20diverse%20reasoning%20pathways%20and%20dynamically%20update%20its%20knowledge%20base%20in%0Areal-time.%20This%20allows%20the%20framework%20to%20not%20only%20revisit%20and%20refine%20earlier%0Ainferences%20but%20also%20adaptively%20incorporate%20evolving%20information%2C%20ensuring%20that%0Athe%20final%20output%20is%20both%20accurate%20and%20comprehensive.%20To%20validate%20the%0Aeffectiveness%20of%20our%20framework%2C%20we%20conducted%20extensive%20experiments%20across%20a%0Arange%20of%20generative%20and%20reasoning%20tasks.%20These%20experiments%20demonstrated%20that%0Aour%20framework%20outperforms%20conventional%20inference%20processes%20on%20accuracy%2C%0Acoherence%2C%20and%20diversity.%20The%20framework%27s%20ability%20to%20iteratively%20expand%20its%0Asearch%20space%20while%20retaining%20contextually%20relevant%20information%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoAT%253A%2520Chain-of-Associated-Thoughts%2520Framework%2520for%2520Enhancing%2520Large%250A%2520%2520Language%2520Models%2520Reasoning%26entry.906535625%3DJianfeng%2520Pan%2520and%2520Senyou%2520Deng%2520and%2520Shaomang%2520Huang%26entry.1292438233%3D%2520%2520Research%2520on%2520LLM%2520technologies%2520is%2520rapidly%2520emerging%252C%2520with%2520most%2520of%2520them%2520employing%250Aa%2520%2527fast%2520thinking%2527%2520approach%2520to%2520inference.%2520Most%2520LLMs%2520generate%2520the%2520final%2520result%250Abased%2520solely%2520on%2520a%2520single%2520query%2520and%2520LLM%2527s%2520reasoning%2520capabilities.%2520However%252C%2520with%250Athe%2520advent%2520of%2520OpenAI-o1%252C%2520%2527slow%2520thinking%2527%2520techniques%2520have%2520garnered%2520increasing%250Aattention%2520because%2520its%2520process%2520is%2520closer%2520to%2520the%2520human%2520thought%2520process.%2520Inspired%250Aby%2520the%2520human%2520ability%2520to%2520constantly%2520associate%2520and%2520replenish%2520knowledge%2520during%250Athinking%252C%2520we%2520developed%2520the%2520novel%2520Chain-of-Associated-Thoughts%2520%2528CoAT%2529%2520framework%252C%250Awhich%2520introduces%2520an%2520innovative%2520synergy%2520between%2520the%2520Monte%2520Carlo%2520Tree%2520Search%250A%2528MCTS%2529%2520algorithm%2520and%2520a%2520dynamic%2520mechanism%2520for%2520integrating%2520new%2520key%2520information%252C%250Atermed%2520%2527associative%2520memory%2527.%2520By%2520combining%2520the%2520structured%2520exploration%250Acapabilities%2520of%2520MCTS%2520with%2520the%2520adaptive%2520learning%2520capacity%2520of%2520associative%2520memory%252C%250ACoAT%2520significantly%2520expands%2520the%2520LLM%2520search%2520space%252C%2520enabling%2520our%2520framework%2520to%250Aexplore%2520diverse%2520reasoning%2520pathways%2520and%2520dynamically%2520update%2520its%2520knowledge%2520base%2520in%250Areal-time.%2520This%2520allows%2520the%2520framework%2520to%2520not%2520only%2520revisit%2520and%2520refine%2520earlier%250Ainferences%2520but%2520also%2520adaptively%2520incorporate%2520evolving%2520information%252C%2520ensuring%2520that%250Athe%2520final%2520output%2520is%2520both%2520accurate%2520and%2520comprehensive.%2520To%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520framework%252C%2520we%2520conducted%2520extensive%2520experiments%2520across%2520a%250Arange%2520of%2520generative%2520and%2520reasoning%2520tasks.%2520These%2520experiments%2520demonstrated%2520that%250Aour%2520framework%2520outperforms%2520conventional%2520inference%2520processes%2520on%2520accuracy%252C%250Acoherence%252C%2520and%2520diversity.%2520The%2520framework%2527s%2520ability%2520to%2520iteratively%2520expand%2520its%250Asearch%2520space%2520while%2520retaining%2520contextually%2520relevant%2520information%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoAT%3A%20Chain-of-Associated-Thoughts%20Framework%20for%20Enhancing%20Large%0A%20%20Language%20Models%20Reasoning&entry.906535625=Jianfeng%20Pan%20and%20Senyou%20Deng%20and%20Shaomang%20Huang&entry.1292438233=%20%20Research%20on%20LLM%20technologies%20is%20rapidly%20emerging%2C%20with%20most%20of%20them%20employing%0Aa%20%27fast%20thinking%27%20approach%20to%20inference.%20Most%20LLMs%20generate%20the%20final%20result%0Abased%20solely%20on%20a%20single%20query%20and%20LLM%27s%20reasoning%20capabilities.%20However%2C%20with%0Athe%20advent%20of%20OpenAI-o1%2C%20%27slow%20thinking%27%20techniques%20have%20garnered%20increasing%0Aattention%20because%20its%20process%20is%20closer%20to%20the%20human%20thought%20process.%20Inspired%0Aby%20the%20human%20ability%20to%20constantly%20associate%20and%20replenish%20knowledge%20during%0Athinking%2C%20we%20developed%20the%20novel%20Chain-of-Associated-Thoughts%20%28CoAT%29%20framework%2C%0Awhich%20introduces%20an%20innovative%20synergy%20between%20the%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29%20algorithm%20and%20a%20dynamic%20mechanism%20for%20integrating%20new%20key%20information%2C%0Atermed%20%27associative%20memory%27.%20By%20combining%20the%20structured%20exploration%0Acapabilities%20of%20MCTS%20with%20the%20adaptive%20learning%20capacity%20of%20associative%20memory%2C%0ACoAT%20significantly%20expands%20the%20LLM%20search%20space%2C%20enabling%20our%20framework%20to%0Aexplore%20diverse%20reasoning%20pathways%20and%20dynamically%20update%20its%20knowledge%20base%20in%0Areal-time.%20This%20allows%20the%20framework%20to%20not%20only%20revisit%20and%20refine%20earlier%0Ainferences%20but%20also%20adaptively%20incorporate%20evolving%20information%2C%20ensuring%20that%0Athe%20final%20output%20is%20both%20accurate%20and%20comprehensive.%20To%20validate%20the%0Aeffectiveness%20of%20our%20framework%2C%20we%20conducted%20extensive%20experiments%20across%20a%0Arange%20of%20generative%20and%20reasoning%20tasks.%20These%20experiments%20demonstrated%20that%0Aour%20framework%20outperforms%20conventional%20inference%20processes%20on%20accuracy%2C%0Acoherence%2C%20and%20diversity.%20The%20framework%27s%20ability%20to%20iteratively%20expand%20its%0Asearch%20space%20while%20retaining%20contextually%20relevant%20information%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02390v1&entry.124074799=Read"},
{"title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents", "author": "Kevin Chen and Marco Cusumano-Towner and Brody Huval and Aleksei Petrenko and Jackson Hamburger and Vladlen Koltun and Philipp Kr\u00e4henb\u00fchl", "abstract": "  Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks.\n", "link": "http://arxiv.org/abs/2502.01600v2", "date": "2025-02-04", "relevancy": 2.1011, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5476}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20Long-Horizon%20Interactive%20LLM%20Agents&body=Title%3A%20Reinforcement%20Learning%20for%20Long-Horizon%20Interactive%20LLM%20Agents%0AAuthor%3A%20Kevin%20Chen%20and%20Marco%20Cusumano-Towner%20and%20Brody%20Huval%20and%20Aleksei%20Petrenko%20and%20Jackson%20Hamburger%20and%20Vladlen%20Koltun%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%0AAbstract%3A%20%20%20Interactive%20digital%20agents%20%28IDAs%29%20leverage%20APIs%20of%20stateful%20digital%0Aenvironments%20to%20perform%20tasks%20in%20response%20to%20user%20requests.%20While%20IDAs%20powered%0Aby%20instruction-tuned%20large%20language%20models%20%28LLMs%29%20can%20react%20to%20feedback%20from%0Ainterface%20invocations%20in%20multi-step%20exchanges%2C%20they%20have%20not%20been%20trained%20in%0Atheir%20respective%20digital%20environments.%20Prior%20methods%20accomplish%20less%20than%20half%0Aof%20tasks%20in%20sophisticated%20benchmarks%20such%20as%20AppWorld.%20We%20present%20a%0Areinforcement%20learning%20%28RL%29%20approach%20that%20trains%20IDAs%20directly%20in%20their%20target%0Aenvironments.%20We%20formalize%20this%20training%20as%20a%20partially%20observable%20Markov%0Adecision%20process%20and%20derive%20LOOP%2C%20a%20data-%20and%20memory-efficient%20variant%20of%0Aproximal%20policy%20optimization.%20LOOP%20uses%20no%20value%20network%20and%20maintains%20exactly%0Aone%20copy%20of%20the%20underlying%20LLM%20in%20memory%2C%20making%20its%20implementation%0Astraightforward%20and%20as%20memory-efficient%20as%20fine-tuning%20a%20single%20LLM.%20A%0A32-billion-parameter%20agent%20trained%20with%20LOOP%20in%20the%20AppWorld%20environment%0Aoutperforms%20the%20much%20larger%20OpenAI%20o1%20agent%20by%209%20percentage%20points%20%2815%25%0Arelative%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20reported%20application%20of%20RL%20to%0AIDAs%20that%20interact%20with%20a%20stateful%2C%20multi-domain%2C%20multi-app%20environment%20via%0Adirect%20API%20calls.%20Our%20analysis%20sheds%20light%20on%20the%20effectiveness%20of%20RL%20in%20this%0Aarea%2C%20showing%20that%20the%20agent%20learns%20to%20consult%20the%20API%20documentation%2C%20avoid%0Aunwarranted%20assumptions%2C%20minimize%20confabulation%2C%20and%20recover%20from%20setbacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520Long-Horizon%2520Interactive%2520LLM%2520Agents%26entry.906535625%3DKevin%2520Chen%2520and%2520Marco%2520Cusumano-Towner%2520and%2520Brody%2520Huval%2520and%2520Aleksei%2520Petrenko%2520and%2520Jackson%2520Hamburger%2520and%2520Vladlen%2520Koltun%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%26entry.1292438233%3D%2520%2520Interactive%2520digital%2520agents%2520%2528IDAs%2529%2520leverage%2520APIs%2520of%2520stateful%2520digital%250Aenvironments%2520to%2520perform%2520tasks%2520in%2520response%2520to%2520user%2520requests.%2520While%2520IDAs%2520powered%250Aby%2520instruction-tuned%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520react%2520to%2520feedback%2520from%250Ainterface%2520invocations%2520in%2520multi-step%2520exchanges%252C%2520they%2520have%2520not%2520been%2520trained%2520in%250Atheir%2520respective%2520digital%2520environments.%2520Prior%2520methods%2520accomplish%2520less%2520than%2520half%250Aof%2520tasks%2520in%2520sophisticated%2520benchmarks%2520such%2520as%2520AppWorld.%2520We%2520present%2520a%250Areinforcement%2520learning%2520%2528RL%2529%2520approach%2520that%2520trains%2520IDAs%2520directly%2520in%2520their%2520target%250Aenvironments.%2520We%2520formalize%2520this%2520training%2520as%2520a%2520partially%2520observable%2520Markov%250Adecision%2520process%2520and%2520derive%2520LOOP%252C%2520a%2520data-%2520and%2520memory-efficient%2520variant%2520of%250Aproximal%2520policy%2520optimization.%2520LOOP%2520uses%2520no%2520value%2520network%2520and%2520maintains%2520exactly%250Aone%2520copy%2520of%2520the%2520underlying%2520LLM%2520in%2520memory%252C%2520making%2520its%2520implementation%250Astraightforward%2520and%2520as%2520memory-efficient%2520as%2520fine-tuning%2520a%2520single%2520LLM.%2520A%250A32-billion-parameter%2520agent%2520trained%2520with%2520LOOP%2520in%2520the%2520AppWorld%2520environment%250Aoutperforms%2520the%2520much%2520larger%2520OpenAI%2520o1%2520agent%2520by%25209%2520percentage%2520points%2520%252815%2525%250Arelative%2529.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520reported%2520application%2520of%2520RL%2520to%250AIDAs%2520that%2520interact%2520with%2520a%2520stateful%252C%2520multi-domain%252C%2520multi-app%2520environment%2520via%250Adirect%2520API%2520calls.%2520Our%2520analysis%2520sheds%2520light%2520on%2520the%2520effectiveness%2520of%2520RL%2520in%2520this%250Aarea%252C%2520showing%2520that%2520the%2520agent%2520learns%2520to%2520consult%2520the%2520API%2520documentation%252C%2520avoid%250Aunwarranted%2520assumptions%252C%2520minimize%2520confabulation%252C%2520and%2520recover%2520from%2520setbacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20Long-Horizon%20Interactive%20LLM%20Agents&entry.906535625=Kevin%20Chen%20and%20Marco%20Cusumano-Towner%20and%20Brody%20Huval%20and%20Aleksei%20Petrenko%20and%20Jackson%20Hamburger%20and%20Vladlen%20Koltun%20and%20Philipp%20Kr%C3%A4henb%C3%BChl&entry.1292438233=%20%20Interactive%20digital%20agents%20%28IDAs%29%20leverage%20APIs%20of%20stateful%20digital%0Aenvironments%20to%20perform%20tasks%20in%20response%20to%20user%20requests.%20While%20IDAs%20powered%0Aby%20instruction-tuned%20large%20language%20models%20%28LLMs%29%20can%20react%20to%20feedback%20from%0Ainterface%20invocations%20in%20multi-step%20exchanges%2C%20they%20have%20not%20been%20trained%20in%0Atheir%20respective%20digital%20environments.%20Prior%20methods%20accomplish%20less%20than%20half%0Aof%20tasks%20in%20sophisticated%20benchmarks%20such%20as%20AppWorld.%20We%20present%20a%0Areinforcement%20learning%20%28RL%29%20approach%20that%20trains%20IDAs%20directly%20in%20their%20target%0Aenvironments.%20We%20formalize%20this%20training%20as%20a%20partially%20observable%20Markov%0Adecision%20process%20and%20derive%20LOOP%2C%20a%20data-%20and%20memory-efficient%20variant%20of%0Aproximal%20policy%20optimization.%20LOOP%20uses%20no%20value%20network%20and%20maintains%20exactly%0Aone%20copy%20of%20the%20underlying%20LLM%20in%20memory%2C%20making%20its%20implementation%0Astraightforward%20and%20as%20memory-efficient%20as%20fine-tuning%20a%20single%20LLM.%20A%0A32-billion-parameter%20agent%20trained%20with%20LOOP%20in%20the%20AppWorld%20environment%0Aoutperforms%20the%20much%20larger%20OpenAI%20o1%20agent%20by%209%20percentage%20points%20%2815%25%0Arelative%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20reported%20application%20of%20RL%20to%0AIDAs%20that%20interact%20with%20a%20stateful%2C%20multi-domain%2C%20multi-app%20environment%20via%0Adirect%20API%20calls.%20Our%20analysis%20sheds%20light%20on%20the%20effectiveness%20of%20RL%20in%20this%0Aarea%2C%20showing%20that%20the%20agent%20learns%20to%20consult%20the%20API%20documentation%2C%20avoid%0Aunwarranted%20assumptions%2C%20minimize%20confabulation%2C%20and%20recover%20from%20setbacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01600v2&entry.124074799=Read"},
{"title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions", "author": "Adam Ishay and Joohyung Lee", "abstract": "  Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.\n", "link": "http://arxiv.org/abs/2501.00830v2", "date": "2025-02-04", "relevancy": 2.1009, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%2BAL%3A%20Bridging%20Large%20Language%20Models%20and%20Action%20Languages%20for%20Complex%0A%20%20Reasoning%20about%20Actions&body=Title%3A%20LLM%2BAL%3A%20Bridging%20Large%20Language%20Models%20and%20Action%20Languages%20for%20Complex%0A%20%20Reasoning%20about%20Actions%0AAuthor%3A%20Adam%20Ishay%20and%20Joohyung%20Lee%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20various%0Aintelligent%20tasks%20but%20still%20struggle%20with%20complex%20action%20reasoning%20tasks%20that%0Arequire%20systematic%20search.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20method%20that%0Abridges%20the%20natural%20language%20understanding%20capabilities%20of%20LLMs%20with%20the%0Asymbolic%20reasoning%20strengths%20of%20action%20languages.%20Our%20approach%2C%20termed%0A%22LLM%2BAL%2C%22%20leverages%20the%20LLM%27s%20strengths%20in%20semantic%20parsing%20and%20commonsense%0Aknowledge%20generation%20alongside%20the%20action%20language%27s%20proficiency%20in%20automated%0Areasoning%20based%20on%20encoded%20knowledge.%20We%20compare%20LLM%2BAL%20against%0Astate-of-the-art%20LLMs%2C%20including%20ChatGPT-4%2C%20Claude%203%20Opus%2C%20Gemini%20Ultra%201.0%2C%0Aand%20o1-preview%2C%20using%20benchmarks%20for%20complex%20reasoning%20about%20actions.%20Our%0Afindings%20indicate%20that%2C%20although%20all%20methods%20exhibit%20errors%2C%20LLM%2BAL%2C%20with%0Arelatively%20minimal%20human%20corrections%2C%20consistently%20leads%20to%20correct%20answers%2C%0Awhereas%20standalone%20LLMs%20fail%20to%20improve%20even%20with%20human%20feedback.%20LLM%2BAL%20also%0Acontributes%20to%20automated%20generation%20of%20action%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%252BAL%253A%2520Bridging%2520Large%2520Language%2520Models%2520and%2520Action%2520Languages%2520for%2520Complex%250A%2520%2520Reasoning%2520about%2520Actions%26entry.906535625%3DAdam%2520Ishay%2520and%2520Joohyung%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%2520in%2520various%250Aintelligent%2520tasks%2520but%2520still%2520struggle%2520with%2520complex%2520action%2520reasoning%2520tasks%2520that%250Arequire%2520systematic%2520search.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520method%2520that%250Abridges%2520the%2520natural%2520language%2520understanding%2520capabilities%2520of%2520LLMs%2520with%2520the%250Asymbolic%2520reasoning%2520strengths%2520of%2520action%2520languages.%2520Our%2520approach%252C%2520termed%250A%2522LLM%252BAL%252C%2522%2520leverages%2520the%2520LLM%2527s%2520strengths%2520in%2520semantic%2520parsing%2520and%2520commonsense%250Aknowledge%2520generation%2520alongside%2520the%2520action%2520language%2527s%2520proficiency%2520in%2520automated%250Areasoning%2520based%2520on%2520encoded%2520knowledge.%2520We%2520compare%2520LLM%252BAL%2520against%250Astate-of-the-art%2520LLMs%252C%2520including%2520ChatGPT-4%252C%2520Claude%25203%2520Opus%252C%2520Gemini%2520Ultra%25201.0%252C%250Aand%2520o1-preview%252C%2520using%2520benchmarks%2520for%2520complex%2520reasoning%2520about%2520actions.%2520Our%250Afindings%2520indicate%2520that%252C%2520although%2520all%2520methods%2520exhibit%2520errors%252C%2520LLM%252BAL%252C%2520with%250Arelatively%2520minimal%2520human%2520corrections%252C%2520consistently%2520leads%2520to%2520correct%2520answers%252C%250Awhereas%2520standalone%2520LLMs%2520fail%2520to%2520improve%2520even%2520with%2520human%2520feedback.%2520LLM%252BAL%2520also%250Acontributes%2520to%2520automated%2520generation%2520of%2520action%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%2BAL%3A%20Bridging%20Large%20Language%20Models%20and%20Action%20Languages%20for%20Complex%0A%20%20Reasoning%20about%20Actions&entry.906535625=Adam%20Ishay%20and%20Joohyung%20Lee&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20various%0Aintelligent%20tasks%20but%20still%20struggle%20with%20complex%20action%20reasoning%20tasks%20that%0Arequire%20systematic%20search.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20method%20that%0Abridges%20the%20natural%20language%20understanding%20capabilities%20of%20LLMs%20with%20the%0Asymbolic%20reasoning%20strengths%20of%20action%20languages.%20Our%20approach%2C%20termed%0A%22LLM%2BAL%2C%22%20leverages%20the%20LLM%27s%20strengths%20in%20semantic%20parsing%20and%20commonsense%0Aknowledge%20generation%20alongside%20the%20action%20language%27s%20proficiency%20in%20automated%0Areasoning%20based%20on%20encoded%20knowledge.%20We%20compare%20LLM%2BAL%20against%0Astate-of-the-art%20LLMs%2C%20including%20ChatGPT-4%2C%20Claude%203%20Opus%2C%20Gemini%20Ultra%201.0%2C%0Aand%20o1-preview%2C%20using%20benchmarks%20for%20complex%20reasoning%20about%20actions.%20Our%0Afindings%20indicate%20that%2C%20although%20all%20methods%20exhibit%20errors%2C%20LLM%2BAL%2C%20with%0Arelatively%20minimal%20human%20corrections%2C%20consistently%20leads%20to%20correct%20answers%2C%0Awhereas%20standalone%20LLMs%20fail%20to%20improve%20even%20with%20human%20feedback.%20LLM%2BAL%20also%0Acontributes%20to%20automated%20generation%20of%20action%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00830v2&entry.124074799=Read"},
{"title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search", "author": "Zongyu Lin and Yao Tang and Xingcheng Yao and Da Yin and Ziniu Hu and Yizhou Sun and Kai-Wei Chang", "abstract": "  Language agents have become a promising solution to complex interactive\ntasks. One of the key ingredients to the success of language agents is the\nreward model on the trajectory of the agentic workflow, which provides valuable\nguidance during training or inference. However, due to the lack of annotations\nof intermediate interactions, most existing works use an outcome reward model\nto optimize policies across entire trajectories. This may lead to sub-optimal\npolicies and hinder the overall performance. To address this, we propose QLASS\n(Q-guided Language Agent Stepwise Search), to automatically generate\nannotations by estimating Q-values in a stepwise manner for open language\nagents. By introducing a reasoning tree and performing process reward modeling,\nQLASS provides effective intermediate guidance for each step. With the stepwise\nguidance, we propose a Q-guided generation strategy to enable language agents\nto better adapt to long-term value, resulting in significant performance\nimprovement during model inference on complex interactive agent tasks. Notably,\neven with almost half the annotated data, QLASS retains strong performance,\ndemonstrating its efficiency in handling limited supervision. We also\nempirically demonstrate that QLASS can lead to more effective decision making\nthrough qualitative analysis. We will release our code and data.\n", "link": "http://arxiv.org/abs/2502.02584v1", "date": "2025-02-04", "relevancy": 2.0962, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5367}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QLASS%3A%20Boosting%20Language%20Agent%20Inference%20via%20Q-Guided%20Stepwise%20Search&body=Title%3A%20QLASS%3A%20Boosting%20Language%20Agent%20Inference%20via%20Q-Guided%20Stepwise%20Search%0AAuthor%3A%20Zongyu%20Lin%20and%20Yao%20Tang%20and%20Xingcheng%20Yao%20and%20Da%20Yin%20and%20Ziniu%20Hu%20and%20Yizhou%20Sun%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Language%20agents%20have%20become%20a%20promising%20solution%20to%20complex%20interactive%0Atasks.%20One%20of%20the%20key%20ingredients%20to%20the%20success%20of%20language%20agents%20is%20the%0Areward%20model%20on%20the%20trajectory%20of%20the%20agentic%20workflow%2C%20which%20provides%20valuable%0Aguidance%20during%20training%20or%20inference.%20However%2C%20due%20to%20the%20lack%20of%20annotations%0Aof%20intermediate%20interactions%2C%20most%20existing%20works%20use%20an%20outcome%20reward%20model%0Ato%20optimize%20policies%20across%20entire%20trajectories.%20This%20may%20lead%20to%20sub-optimal%0Apolicies%20and%20hinder%20the%20overall%20performance.%20To%20address%20this%2C%20we%20propose%20QLASS%0A%28Q-guided%20Language%20Agent%20Stepwise%20Search%29%2C%20to%20automatically%20generate%0Aannotations%20by%20estimating%20Q-values%20in%20a%20stepwise%20manner%20for%20open%20language%0Aagents.%20By%20introducing%20a%20reasoning%20tree%20and%20performing%20process%20reward%20modeling%2C%0AQLASS%20provides%20effective%20intermediate%20guidance%20for%20each%20step.%20With%20the%20stepwise%0Aguidance%2C%20we%20propose%20a%20Q-guided%20generation%20strategy%20to%20enable%20language%20agents%0Ato%20better%20adapt%20to%20long-term%20value%2C%20resulting%20in%20significant%20performance%0Aimprovement%20during%20model%20inference%20on%20complex%20interactive%20agent%20tasks.%20Notably%2C%0Aeven%20with%20almost%20half%20the%20annotated%20data%2C%20QLASS%20retains%20strong%20performance%2C%0Ademonstrating%20its%20efficiency%20in%20handling%20limited%20supervision.%20We%20also%0Aempirically%20demonstrate%20that%20QLASS%20can%20lead%20to%20more%20effective%20decision%20making%0Athrough%20qualitative%20analysis.%20We%20will%20release%20our%20code%20and%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQLASS%253A%2520Boosting%2520Language%2520Agent%2520Inference%2520via%2520Q-Guided%2520Stepwise%2520Search%26entry.906535625%3DZongyu%2520Lin%2520and%2520Yao%2520Tang%2520and%2520Xingcheng%2520Yao%2520and%2520Da%2520Yin%2520and%2520Ziniu%2520Hu%2520and%2520Yizhou%2520Sun%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Language%2520agents%2520have%2520become%2520a%2520promising%2520solution%2520to%2520complex%2520interactive%250Atasks.%2520One%2520of%2520the%2520key%2520ingredients%2520to%2520the%2520success%2520of%2520language%2520agents%2520is%2520the%250Areward%2520model%2520on%2520the%2520trajectory%2520of%2520the%2520agentic%2520workflow%252C%2520which%2520provides%2520valuable%250Aguidance%2520during%2520training%2520or%2520inference.%2520However%252C%2520due%2520to%2520the%2520lack%2520of%2520annotations%250Aof%2520intermediate%2520interactions%252C%2520most%2520existing%2520works%2520use%2520an%2520outcome%2520reward%2520model%250Ato%2520optimize%2520policies%2520across%2520entire%2520trajectories.%2520This%2520may%2520lead%2520to%2520sub-optimal%250Apolicies%2520and%2520hinder%2520the%2520overall%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520QLASS%250A%2528Q-guided%2520Language%2520Agent%2520Stepwise%2520Search%2529%252C%2520to%2520automatically%2520generate%250Aannotations%2520by%2520estimating%2520Q-values%2520in%2520a%2520stepwise%2520manner%2520for%2520open%2520language%250Aagents.%2520By%2520introducing%2520a%2520reasoning%2520tree%2520and%2520performing%2520process%2520reward%2520modeling%252C%250AQLASS%2520provides%2520effective%2520intermediate%2520guidance%2520for%2520each%2520step.%2520With%2520the%2520stepwise%250Aguidance%252C%2520we%2520propose%2520a%2520Q-guided%2520generation%2520strategy%2520to%2520enable%2520language%2520agents%250Ato%2520better%2520adapt%2520to%2520long-term%2520value%252C%2520resulting%2520in%2520significant%2520performance%250Aimprovement%2520during%2520model%2520inference%2520on%2520complex%2520interactive%2520agent%2520tasks.%2520Notably%252C%250Aeven%2520with%2520almost%2520half%2520the%2520annotated%2520data%252C%2520QLASS%2520retains%2520strong%2520performance%252C%250Ademonstrating%2520its%2520efficiency%2520in%2520handling%2520limited%2520supervision.%2520We%2520also%250Aempirically%2520demonstrate%2520that%2520QLASS%2520can%2520lead%2520to%2520more%2520effective%2520decision%2520making%250Athrough%2520qualitative%2520analysis.%2520We%2520will%2520release%2520our%2520code%2520and%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QLASS%3A%20Boosting%20Language%20Agent%20Inference%20via%20Q-Guided%20Stepwise%20Search&entry.906535625=Zongyu%20Lin%20and%20Yao%20Tang%20and%20Xingcheng%20Yao%20and%20Da%20Yin%20and%20Ziniu%20Hu%20and%20Yizhou%20Sun%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Language%20agents%20have%20become%20a%20promising%20solution%20to%20complex%20interactive%0Atasks.%20One%20of%20the%20key%20ingredients%20to%20the%20success%20of%20language%20agents%20is%20the%0Areward%20model%20on%20the%20trajectory%20of%20the%20agentic%20workflow%2C%20which%20provides%20valuable%0Aguidance%20during%20training%20or%20inference.%20However%2C%20due%20to%20the%20lack%20of%20annotations%0Aof%20intermediate%20interactions%2C%20most%20existing%20works%20use%20an%20outcome%20reward%20model%0Ato%20optimize%20policies%20across%20entire%20trajectories.%20This%20may%20lead%20to%20sub-optimal%0Apolicies%20and%20hinder%20the%20overall%20performance.%20To%20address%20this%2C%20we%20propose%20QLASS%0A%28Q-guided%20Language%20Agent%20Stepwise%20Search%29%2C%20to%20automatically%20generate%0Aannotations%20by%20estimating%20Q-values%20in%20a%20stepwise%20manner%20for%20open%20language%0Aagents.%20By%20introducing%20a%20reasoning%20tree%20and%20performing%20process%20reward%20modeling%2C%0AQLASS%20provides%20effective%20intermediate%20guidance%20for%20each%20step.%20With%20the%20stepwise%0Aguidance%2C%20we%20propose%20a%20Q-guided%20generation%20strategy%20to%20enable%20language%20agents%0Ato%20better%20adapt%20to%20long-term%20value%2C%20resulting%20in%20significant%20performance%0Aimprovement%20during%20model%20inference%20on%20complex%20interactive%20agent%20tasks.%20Notably%2C%0Aeven%20with%20almost%20half%20the%20annotated%20data%2C%20QLASS%20retains%20strong%20performance%2C%0Ademonstrating%20its%20efficiency%20in%20handling%20limited%20supervision.%20We%20also%0Aempirically%20demonstrate%20that%20QLASS%20can%20lead%20to%20more%20effective%20decision%20making%0Athrough%20qualitative%20analysis.%20We%20will%20release%20our%20code%20and%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02584v1&entry.124074799=Read"},
{"title": "SelfFed: Self-Supervised Federated Learning for Data Heterogeneity and\n  Label Scarcity in Medical Images", "author": "Sunder Ali Khowaja and Kapal Dev and Syed Muhammad Anwar and Marius George Linguraru", "abstract": "  Self-supervised learning in the federated learning paradigm has been gaining\na lot of interest both in industry and research due to the collaborative\nlearning capability on unlabeled yet isolated data. However, self-supervised\nbased federated learning strategies suffer from performance degradation due to\nlabel scarcity and diverse data distributions, i.e., data heterogeneity. In\nthis paper, we propose the SelfFed framework for medical images to overcome\ndata heterogeneity and label scarcity issues. The first phase of the SelfFed\nframework helps to overcome the data heterogeneity issue by leveraging the\npre-training paradigm that performs augmentative modeling using Swin\nTransformer-based encoder in a decentralized manner. The label scarcity issue\nis addressed by fine-tuning paradigm that introduces a contrastive network and\na novel aggregation strategy. We perform our experimental analysis on publicly\navailable medical imaging datasets to show that SelfFed performs better when\ncompared to existing baselines and works. Our method achieves a maximum\nimprovement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID\ndatasets. Further, our proposed method outperforms existing baselines even when\ntrained on a few (10%) labeled instances.\n", "link": "http://arxiv.org/abs/2307.01514v3", "date": "2025-02-04", "relevancy": 2.0937, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5538}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfFed%3A%20Self-Supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20Medical%20Images&body=Title%3A%20SelfFed%3A%20Self-Supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20Medical%20Images%0AAuthor%3A%20Sunder%20Ali%20Khowaja%20and%20Kapal%20Dev%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru%0AAbstract%3A%20%20%20Self-supervised%20learning%20in%20the%20federated%20learning%20paradigm%20has%20been%20gaining%0Aa%20lot%20of%20interest%20both%20in%20industry%20and%20research%20due%20to%20the%20collaborative%0Alearning%20capability%20on%20unlabeled%20yet%20isolated%20data.%20However%2C%20self-supervised%0Abased%20federated%20learning%20strategies%20suffer%20from%20performance%20degradation%20due%20to%0Alabel%20scarcity%20and%20diverse%20data%20distributions%2C%20i.e.%2C%20data%20heterogeneity.%20In%0Athis%20paper%2C%20we%20propose%20the%20SelfFed%20framework%20for%20medical%20images%20to%20overcome%0Adata%20heterogeneity%20and%20label%20scarcity%20issues.%20The%20first%20phase%20of%20the%20SelfFed%0Aframework%20helps%20to%20overcome%20the%20data%20heterogeneity%20issue%20by%20leveraging%20the%0Apre-training%20paradigm%20that%20performs%20augmentative%20modeling%20using%20Swin%0ATransformer-based%20encoder%20in%20a%20decentralized%20manner.%20The%20label%20scarcity%20issue%0Ais%20addressed%20by%20fine-tuning%20paradigm%20that%20introduces%20a%20contrastive%20network%20and%0Aa%20novel%20aggregation%20strategy.%20We%20perform%20our%20experimental%20analysis%20on%20publicly%0Aavailable%20medical%20imaging%20datasets%20to%20show%20that%20SelfFed%20performs%20better%20when%0Acompared%20to%20existing%20baselines%20and%20works.%20Our%20method%20achieves%20a%20maximum%0Aimprovement%20of%208.8%25%20and%204.1%25%20on%20Retina%20and%20COVID-FL%20datasets%20on%20non-IID%0Adatasets.%20Further%2C%20our%20proposed%20method%20outperforms%20existing%20baselines%20even%20when%0Atrained%20on%20a%20few%20%2810%25%29%20labeled%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01514v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfFed%253A%2520Self-Supervised%2520Federated%2520Learning%2520for%2520Data%2520Heterogeneity%2520and%250A%2520%2520Label%2520Scarcity%2520in%2520Medical%2520Images%26entry.906535625%3DSunder%2520Ali%2520Khowaja%2520and%2520Kapal%2520Dev%2520and%2520Syed%2520Muhammad%2520Anwar%2520and%2520Marius%2520George%2520Linguraru%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520in%2520the%2520federated%2520learning%2520paradigm%2520has%2520been%2520gaining%250Aa%2520lot%2520of%2520interest%2520both%2520in%2520industry%2520and%2520research%2520due%2520to%2520the%2520collaborative%250Alearning%2520capability%2520on%2520unlabeled%2520yet%2520isolated%2520data.%2520However%252C%2520self-supervised%250Abased%2520federated%2520learning%2520strategies%2520suffer%2520from%2520performance%2520degradation%2520due%2520to%250Alabel%2520scarcity%2520and%2520diverse%2520data%2520distributions%252C%2520i.e.%252C%2520data%2520heterogeneity.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520SelfFed%2520framework%2520for%2520medical%2520images%2520to%2520overcome%250Adata%2520heterogeneity%2520and%2520label%2520scarcity%2520issues.%2520The%2520first%2520phase%2520of%2520the%2520SelfFed%250Aframework%2520helps%2520to%2520overcome%2520the%2520data%2520heterogeneity%2520issue%2520by%2520leveraging%2520the%250Apre-training%2520paradigm%2520that%2520performs%2520augmentative%2520modeling%2520using%2520Swin%250ATransformer-based%2520encoder%2520in%2520a%2520decentralized%2520manner.%2520The%2520label%2520scarcity%2520issue%250Ais%2520addressed%2520by%2520fine-tuning%2520paradigm%2520that%2520introduces%2520a%2520contrastive%2520network%2520and%250Aa%2520novel%2520aggregation%2520strategy.%2520We%2520perform%2520our%2520experimental%2520analysis%2520on%2520publicly%250Aavailable%2520medical%2520imaging%2520datasets%2520to%2520show%2520that%2520SelfFed%2520performs%2520better%2520when%250Acompared%2520to%2520existing%2520baselines%2520and%2520works.%2520Our%2520method%2520achieves%2520a%2520maximum%250Aimprovement%2520of%25208.8%2525%2520and%25204.1%2525%2520on%2520Retina%2520and%2520COVID-FL%2520datasets%2520on%2520non-IID%250Adatasets.%2520Further%252C%2520our%2520proposed%2520method%2520outperforms%2520existing%2520baselines%2520even%2520when%250Atrained%2520on%2520a%2520few%2520%252810%2525%2529%2520labeled%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01514v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfFed%3A%20Self-Supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20Medical%20Images&entry.906535625=Sunder%20Ali%20Khowaja%20and%20Kapal%20Dev%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru&entry.1292438233=%20%20Self-supervised%20learning%20in%20the%20federated%20learning%20paradigm%20has%20been%20gaining%0Aa%20lot%20of%20interest%20both%20in%20industry%20and%20research%20due%20to%20the%20collaborative%0Alearning%20capability%20on%20unlabeled%20yet%20isolated%20data.%20However%2C%20self-supervised%0Abased%20federated%20learning%20strategies%20suffer%20from%20performance%20degradation%20due%20to%0Alabel%20scarcity%20and%20diverse%20data%20distributions%2C%20i.e.%2C%20data%20heterogeneity.%20In%0Athis%20paper%2C%20we%20propose%20the%20SelfFed%20framework%20for%20medical%20images%20to%20overcome%0Adata%20heterogeneity%20and%20label%20scarcity%20issues.%20The%20first%20phase%20of%20the%20SelfFed%0Aframework%20helps%20to%20overcome%20the%20data%20heterogeneity%20issue%20by%20leveraging%20the%0Apre-training%20paradigm%20that%20performs%20augmentative%20modeling%20using%20Swin%0ATransformer-based%20encoder%20in%20a%20decentralized%20manner.%20The%20label%20scarcity%20issue%0Ais%20addressed%20by%20fine-tuning%20paradigm%20that%20introduces%20a%20contrastive%20network%20and%0Aa%20novel%20aggregation%20strategy.%20We%20perform%20our%20experimental%20analysis%20on%20publicly%0Aavailable%20medical%20imaging%20datasets%20to%20show%20that%20SelfFed%20performs%20better%20when%0Acompared%20to%20existing%20baselines%20and%20works.%20Our%20method%20achieves%20a%20maximum%0Aimprovement%20of%208.8%25%20and%204.1%25%20on%20Retina%20and%20COVID-FL%20datasets%20on%20non-IID%0Adatasets.%20Further%2C%20our%20proposed%20method%20outperforms%20existing%20baselines%20even%20when%0Atrained%20on%20a%20few%20%2810%25%29%20labeled%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01514v3&entry.124074799=Read"},
{"title": "Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian\n  Trajectory Prediction", "author": "Ruochen Li and Tanqiu Qiao and Stamos Katsigiannis and Zhanxing Zhu and Hubert P. H. Shum", "abstract": "  Pedestrian trajectory prediction aims to forecast future movements based on\nhistorical paths. Spatial-temporal (ST) methods often separately model spatial\ninteractions among pedestrians and temporal dependencies of individuals. They\noverlook the direct impacts of interactions among different pedestrians across\nvarious time steps (i.e., high-order cross-time interactions). This limits\ntheir ability to capture ST inter-dependencies and hinders prediction\nperformance. To address these limitations, we propose UniEdge with three major\ndesigns. Firstly, we introduce a unified ST graph data structure that\nsimplifies high-order cross-time interactions into first-order relationships,\nenabling the learning of ST inter-dependencies in a single step. This avoids\nthe information loss caused by multi-step aggregation. Secondly, traditional\nGNNs focus on aggregating pedestrian node features, neglecting the propagation\nof implicit interaction patterns encoded in edge features. We propose the\nEdge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph\nnetwork that jointly models explicit N2N social interactions among pedestrians\nand implicit E2E influence propagation across these interaction patterns.\nFinally, to overcome the limited receptive fields and challenges in capturing\nlong-range dependencies of auto-regressive architectures, we introduce a\ntransformer encoder-based predictor that enables global modeling of temporal\ncorrelation. UniEdge outperforms state-of-the-arts on multiple datasets,\nincluding ETH, UCY, and SDD.\n", "link": "http://arxiv.org/abs/2502.02504v1", "date": "2025-02-04", "relevancy": 2.0871, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5411}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5113}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Spatial-Temporal%20Edge-Enhanced%20Graph%20Networks%20for%20Pedestrian%0A%20%20Trajectory%20Prediction&body=Title%3A%20Unified%20Spatial-Temporal%20Edge-Enhanced%20Graph%20Networks%20for%20Pedestrian%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Ruochen%20Li%20and%20Tanqiu%20Qiao%20and%20Stamos%20Katsigiannis%20and%20Zhanxing%20Zhu%20and%20Hubert%20P.%20H.%20Shum%0AAbstract%3A%20%20%20Pedestrian%20trajectory%20prediction%20aims%20to%20forecast%20future%20movements%20based%20on%0Ahistorical%20paths.%20Spatial-temporal%20%28ST%29%20methods%20often%20separately%20model%20spatial%0Ainteractions%20among%20pedestrians%20and%20temporal%20dependencies%20of%20individuals.%20They%0Aoverlook%20the%20direct%20impacts%20of%20interactions%20among%20different%20pedestrians%20across%0Avarious%20time%20steps%20%28i.e.%2C%20high-order%20cross-time%20interactions%29.%20This%20limits%0Atheir%20ability%20to%20capture%20ST%20inter-dependencies%20and%20hinders%20prediction%0Aperformance.%20To%20address%20these%20limitations%2C%20we%20propose%20UniEdge%20with%20three%20major%0Adesigns.%20Firstly%2C%20we%20introduce%20a%20unified%20ST%20graph%20data%20structure%20that%0Asimplifies%20high-order%20cross-time%20interactions%20into%20first-order%20relationships%2C%0Aenabling%20the%20learning%20of%20ST%20inter-dependencies%20in%20a%20single%20step.%20This%20avoids%0Athe%20information%20loss%20caused%20by%20multi-step%20aggregation.%20Secondly%2C%20traditional%0AGNNs%20focus%20on%20aggregating%20pedestrian%20node%20features%2C%20neglecting%20the%20propagation%0Aof%20implicit%20interaction%20patterns%20encoded%20in%20edge%20features.%20We%20propose%20the%0AEdge-to-Edge-Node-to-Node%20Graph%20Convolution%20%28E2E-N2N-GCN%29%2C%20a%20novel%20dual-graph%0Anetwork%20that%20jointly%20models%20explicit%20N2N%20social%20interactions%20among%20pedestrians%0Aand%20implicit%20E2E%20influence%20propagation%20across%20these%20interaction%20patterns.%0AFinally%2C%20to%20overcome%20the%20limited%20receptive%20fields%20and%20challenges%20in%20capturing%0Along-range%20dependencies%20of%20auto-regressive%20architectures%2C%20we%20introduce%20a%0Atransformer%20encoder-based%20predictor%20that%20enables%20global%20modeling%20of%20temporal%0Acorrelation.%20UniEdge%20outperforms%20state-of-the-arts%20on%20multiple%20datasets%2C%0Aincluding%20ETH%2C%20UCY%2C%20and%20SDD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Spatial-Temporal%2520Edge-Enhanced%2520Graph%2520Networks%2520for%2520Pedestrian%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DRuochen%2520Li%2520and%2520Tanqiu%2520Qiao%2520and%2520Stamos%2520Katsigiannis%2520and%2520Zhanxing%2520Zhu%2520and%2520Hubert%2520P.%2520H.%2520Shum%26entry.1292438233%3D%2520%2520Pedestrian%2520trajectory%2520prediction%2520aims%2520to%2520forecast%2520future%2520movements%2520based%2520on%250Ahistorical%2520paths.%2520Spatial-temporal%2520%2528ST%2529%2520methods%2520often%2520separately%2520model%2520spatial%250Ainteractions%2520among%2520pedestrians%2520and%2520temporal%2520dependencies%2520of%2520individuals.%2520They%250Aoverlook%2520the%2520direct%2520impacts%2520of%2520interactions%2520among%2520different%2520pedestrians%2520across%250Avarious%2520time%2520steps%2520%2528i.e.%252C%2520high-order%2520cross-time%2520interactions%2529.%2520This%2520limits%250Atheir%2520ability%2520to%2520capture%2520ST%2520inter-dependencies%2520and%2520hinders%2520prediction%250Aperformance.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520UniEdge%2520with%2520three%2520major%250Adesigns.%2520Firstly%252C%2520we%2520introduce%2520a%2520unified%2520ST%2520graph%2520data%2520structure%2520that%250Asimplifies%2520high-order%2520cross-time%2520interactions%2520into%2520first-order%2520relationships%252C%250Aenabling%2520the%2520learning%2520of%2520ST%2520inter-dependencies%2520in%2520a%2520single%2520step.%2520This%2520avoids%250Athe%2520information%2520loss%2520caused%2520by%2520multi-step%2520aggregation.%2520Secondly%252C%2520traditional%250AGNNs%2520focus%2520on%2520aggregating%2520pedestrian%2520node%2520features%252C%2520neglecting%2520the%2520propagation%250Aof%2520implicit%2520interaction%2520patterns%2520encoded%2520in%2520edge%2520features.%2520We%2520propose%2520the%250AEdge-to-Edge-Node-to-Node%2520Graph%2520Convolution%2520%2528E2E-N2N-GCN%2529%252C%2520a%2520novel%2520dual-graph%250Anetwork%2520that%2520jointly%2520models%2520explicit%2520N2N%2520social%2520interactions%2520among%2520pedestrians%250Aand%2520implicit%2520E2E%2520influence%2520propagation%2520across%2520these%2520interaction%2520patterns.%250AFinally%252C%2520to%2520overcome%2520the%2520limited%2520receptive%2520fields%2520and%2520challenges%2520in%2520capturing%250Along-range%2520dependencies%2520of%2520auto-regressive%2520architectures%252C%2520we%2520introduce%2520a%250Atransformer%2520encoder-based%2520predictor%2520that%2520enables%2520global%2520modeling%2520of%2520temporal%250Acorrelation.%2520UniEdge%2520outperforms%2520state-of-the-arts%2520on%2520multiple%2520datasets%252C%250Aincluding%2520ETH%252C%2520UCY%252C%2520and%2520SDD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Spatial-Temporal%20Edge-Enhanced%20Graph%20Networks%20for%20Pedestrian%0A%20%20Trajectory%20Prediction&entry.906535625=Ruochen%20Li%20and%20Tanqiu%20Qiao%20and%20Stamos%20Katsigiannis%20and%20Zhanxing%20Zhu%20and%20Hubert%20P.%20H.%20Shum&entry.1292438233=%20%20Pedestrian%20trajectory%20prediction%20aims%20to%20forecast%20future%20movements%20based%20on%0Ahistorical%20paths.%20Spatial-temporal%20%28ST%29%20methods%20often%20separately%20model%20spatial%0Ainteractions%20among%20pedestrians%20and%20temporal%20dependencies%20of%20individuals.%20They%0Aoverlook%20the%20direct%20impacts%20of%20interactions%20among%20different%20pedestrians%20across%0Avarious%20time%20steps%20%28i.e.%2C%20high-order%20cross-time%20interactions%29.%20This%20limits%0Atheir%20ability%20to%20capture%20ST%20inter-dependencies%20and%20hinders%20prediction%0Aperformance.%20To%20address%20these%20limitations%2C%20we%20propose%20UniEdge%20with%20three%20major%0Adesigns.%20Firstly%2C%20we%20introduce%20a%20unified%20ST%20graph%20data%20structure%20that%0Asimplifies%20high-order%20cross-time%20interactions%20into%20first-order%20relationships%2C%0Aenabling%20the%20learning%20of%20ST%20inter-dependencies%20in%20a%20single%20step.%20This%20avoids%0Athe%20information%20loss%20caused%20by%20multi-step%20aggregation.%20Secondly%2C%20traditional%0AGNNs%20focus%20on%20aggregating%20pedestrian%20node%20features%2C%20neglecting%20the%20propagation%0Aof%20implicit%20interaction%20patterns%20encoded%20in%20edge%20features.%20We%20propose%20the%0AEdge-to-Edge-Node-to-Node%20Graph%20Convolution%20%28E2E-N2N-GCN%29%2C%20a%20novel%20dual-graph%0Anetwork%20that%20jointly%20models%20explicit%20N2N%20social%20interactions%20among%20pedestrians%0Aand%20implicit%20E2E%20influence%20propagation%20across%20these%20interaction%20patterns.%0AFinally%2C%20to%20overcome%20the%20limited%20receptive%20fields%20and%20challenges%20in%20capturing%0Along-range%20dependencies%20of%20auto-regressive%20architectures%2C%20we%20introduce%20a%0Atransformer%20encoder-based%20predictor%20that%20enables%20global%20modeling%20of%20temporal%0Acorrelation.%20UniEdge%20outperforms%20state-of-the-arts%20on%20multiple%20datasets%2C%0Aincluding%20ETH%2C%20UCY%2C%20and%20SDD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02504v1&entry.124074799=Read"},
{"title": "TransformDAS: Mapping \u03a6-OTDR Signals to Riemannian Manifold for\n  Robust Classification", "author": "Jiaju Kang and Puyu Han and Yang Chun and Xu Wang and Luqi Gong", "abstract": "  Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely\nused distributed fiber optic sensing system in engineering. Machine learning\nalgorithms for {\\Phi}-OTDR event classification require high volumes and\nquality of datasets; however, high-quality datasets are currently extremely\nscarce in the field, leading to a lack of robustness in models, which is\nmanifested by higher false alarm rates in real-world scenarios. One promising\napproach to address this issue is to augment existing data using generative\nmodels combined with a small amount of real-world data. We explored mapping\nboth {\\Phi}-OTDR features in a GAN-based generative pipeline and signal\nfeatures in a Transformer classifier to hyperbolic space to seek more effective\nmodel generalization. The results indicate that state-of-the-art models exhibit\nstronger generalization performance and lower false alarm rates in real-world\nscenarios when trained on augmented datasets. TransformDAS, in particular,\ndemonstrates the best classification performance, highlighting the benefits of\nRiemannian manifold mapping in {\\Phi}-OTDR data generation and model\nclassification.\n", "link": "http://arxiv.org/abs/2502.02428v1", "date": "2025-02-04", "relevancy": 2.0814, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5143}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransformDAS%3A%20Mapping%20%CE%A6-OTDR%20Signals%20to%20Riemannian%20Manifold%20for%0A%20%20Robust%20Classification&body=Title%3A%20TransformDAS%3A%20Mapping%20%CE%A6-OTDR%20Signals%20to%20Riemannian%20Manifold%20for%0A%20%20Robust%20Classification%0AAuthor%3A%20Jiaju%20Kang%20and%20Puyu%20Han%20and%20Yang%20Chun%20and%20Xu%20Wang%20and%20Luqi%20Gong%0AAbstract%3A%20%20%20Phase-sensitive%20optical%20time-domain%20reflectometry%20%28%7B%5CPhi%7D-OTDR%29%20is%20a%20widely%0Aused%20distributed%20fiber%20optic%20sensing%20system%20in%20engineering.%20Machine%20learning%0Aalgorithms%20for%20%7B%5CPhi%7D-OTDR%20event%20classification%20require%20high%20volumes%20and%0Aquality%20of%20datasets%3B%20however%2C%20high-quality%20datasets%20are%20currently%20extremely%0Ascarce%20in%20the%20field%2C%20leading%20to%20a%20lack%20of%20robustness%20in%20models%2C%20which%20is%0Amanifested%20by%20higher%20false%20alarm%20rates%20in%20real-world%20scenarios.%20One%20promising%0Aapproach%20to%20address%20this%20issue%20is%20to%20augment%20existing%20data%20using%20generative%0Amodels%20combined%20with%20a%20small%20amount%20of%20real-world%20data.%20We%20explored%20mapping%0Aboth%20%7B%5CPhi%7D-OTDR%20features%20in%20a%20GAN-based%20generative%20pipeline%20and%20signal%0Afeatures%20in%20a%20Transformer%20classifier%20to%20hyperbolic%20space%20to%20seek%20more%20effective%0Amodel%20generalization.%20The%20results%20indicate%20that%20state-of-the-art%20models%20exhibit%0Astronger%20generalization%20performance%20and%20lower%20false%20alarm%20rates%20in%20real-world%0Ascenarios%20when%20trained%20on%20augmented%20datasets.%20TransformDAS%2C%20in%20particular%2C%0Ademonstrates%20the%20best%20classification%20performance%2C%20highlighting%20the%20benefits%20of%0ARiemannian%20manifold%20mapping%20in%20%7B%5CPhi%7D-OTDR%20data%20generation%20and%20model%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformDAS%253A%2520Mapping%2520%25CE%25A6-OTDR%2520Signals%2520to%2520Riemannian%2520Manifold%2520for%250A%2520%2520Robust%2520Classification%26entry.906535625%3DJiaju%2520Kang%2520and%2520Puyu%2520Han%2520and%2520Yang%2520Chun%2520and%2520Xu%2520Wang%2520and%2520Luqi%2520Gong%26entry.1292438233%3D%2520%2520Phase-sensitive%2520optical%2520time-domain%2520reflectometry%2520%2528%257B%255CPhi%257D-OTDR%2529%2520is%2520a%2520widely%250Aused%2520distributed%2520fiber%2520optic%2520sensing%2520system%2520in%2520engineering.%2520Machine%2520learning%250Aalgorithms%2520for%2520%257B%255CPhi%257D-OTDR%2520event%2520classification%2520require%2520high%2520volumes%2520and%250Aquality%2520of%2520datasets%253B%2520however%252C%2520high-quality%2520datasets%2520are%2520currently%2520extremely%250Ascarce%2520in%2520the%2520field%252C%2520leading%2520to%2520a%2520lack%2520of%2520robustness%2520in%2520models%252C%2520which%2520is%250Amanifested%2520by%2520higher%2520false%2520alarm%2520rates%2520in%2520real-world%2520scenarios.%2520One%2520promising%250Aapproach%2520to%2520address%2520this%2520issue%2520is%2520to%2520augment%2520existing%2520data%2520using%2520generative%250Amodels%2520combined%2520with%2520a%2520small%2520amount%2520of%2520real-world%2520data.%2520We%2520explored%2520mapping%250Aboth%2520%257B%255CPhi%257D-OTDR%2520features%2520in%2520a%2520GAN-based%2520generative%2520pipeline%2520and%2520signal%250Afeatures%2520in%2520a%2520Transformer%2520classifier%2520to%2520hyperbolic%2520space%2520to%2520seek%2520more%2520effective%250Amodel%2520generalization.%2520The%2520results%2520indicate%2520that%2520state-of-the-art%2520models%2520exhibit%250Astronger%2520generalization%2520performance%2520and%2520lower%2520false%2520alarm%2520rates%2520in%2520real-world%250Ascenarios%2520when%2520trained%2520on%2520augmented%2520datasets.%2520TransformDAS%252C%2520in%2520particular%252C%250Ademonstrates%2520the%2520best%2520classification%2520performance%252C%2520highlighting%2520the%2520benefits%2520of%250ARiemannian%2520manifold%2520mapping%2520in%2520%257B%255CPhi%257D-OTDR%2520data%2520generation%2520and%2520model%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransformDAS%3A%20Mapping%20%CE%A6-OTDR%20Signals%20to%20Riemannian%20Manifold%20for%0A%20%20Robust%20Classification&entry.906535625=Jiaju%20Kang%20and%20Puyu%20Han%20and%20Yang%20Chun%20and%20Xu%20Wang%20and%20Luqi%20Gong&entry.1292438233=%20%20Phase-sensitive%20optical%20time-domain%20reflectometry%20%28%7B%5CPhi%7D-OTDR%29%20is%20a%20widely%0Aused%20distributed%20fiber%20optic%20sensing%20system%20in%20engineering.%20Machine%20learning%0Aalgorithms%20for%20%7B%5CPhi%7D-OTDR%20event%20classification%20require%20high%20volumes%20and%0Aquality%20of%20datasets%3B%20however%2C%20high-quality%20datasets%20are%20currently%20extremely%0Ascarce%20in%20the%20field%2C%20leading%20to%20a%20lack%20of%20robustness%20in%20models%2C%20which%20is%0Amanifested%20by%20higher%20false%20alarm%20rates%20in%20real-world%20scenarios.%20One%20promising%0Aapproach%20to%20address%20this%20issue%20is%20to%20augment%20existing%20data%20using%20generative%0Amodels%20combined%20with%20a%20small%20amount%20of%20real-world%20data.%20We%20explored%20mapping%0Aboth%20%7B%5CPhi%7D-OTDR%20features%20in%20a%20GAN-based%20generative%20pipeline%20and%20signal%0Afeatures%20in%20a%20Transformer%20classifier%20to%20hyperbolic%20space%20to%20seek%20more%20effective%0Amodel%20generalization.%20The%20results%20indicate%20that%20state-of-the-art%20models%20exhibit%0Astronger%20generalization%20performance%20and%20lower%20false%20alarm%20rates%20in%20real-world%0Ascenarios%20when%20trained%20on%20augmented%20datasets.%20TransformDAS%2C%20in%20particular%2C%0Ademonstrates%20the%20best%20classification%20performance%2C%20highlighting%20the%20benefits%20of%0ARiemannian%20manifold%20mapping%20in%20%7B%5CPhi%7D-OTDR%20data%20generation%20and%20model%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02428v1&entry.124074799=Read"},
{"title": "Connections between Schedule-Free Optimizers, AdEMAMix, and Accelerated\n  SGD Variants", "author": "Depen Morwani and Nikhil Vyas and Hanlin Zhang and Sham Kakade", "abstract": "  Recent advancements in deep learning optimization have introduced new\nalgorithms, such as Schedule-Free optimizers, AdEMAMix, MARS and Lion which\nmodify traditional momentum mechanisms. In a separate line of work, theoretical\nacceleration of stochastic gradient descent (SGD) in noise-dominated regime has\nbeen achieved by decoupling the momentum coefficient from the current\ngradient's weight. In this paper, we establish explicit connections between\nthese two lines of work. We substantiate our theoretical findings with\npreliminary experiments on a 150m language modeling task. We find that\nAdEMAMix, which most closely resembles accelerated versions of stochastic\ngradient descent, exhibits superior performance. Building on these insights, we\nintroduce a modification to AdEMAMix, termed Simplified-AdEMAMix, which\nmaintains the same performance as AdEMAMix across both large and small\nbatch-size settings while eliminating the need for two different momentum\nterms. The code for Simplified-AdEMAMix is available on the repository:\nhttps://github.com/DepenM/Simplified-AdEMAMix/.\n", "link": "http://arxiv.org/abs/2502.02431v1", "date": "2025-02-04", "relevancy": 2.078, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5397}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5067}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connections%20between%20Schedule-Free%20Optimizers%2C%20AdEMAMix%2C%20and%20Accelerated%0A%20%20SGD%20Variants&body=Title%3A%20Connections%20between%20Schedule-Free%20Optimizers%2C%20AdEMAMix%2C%20and%20Accelerated%0A%20%20SGD%20Variants%0AAuthor%3A%20Depen%20Morwani%20and%20Nikhil%20Vyas%20and%20Hanlin%20Zhang%20and%20Sham%20Kakade%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20optimization%20have%20introduced%20new%0Aalgorithms%2C%20such%20as%20Schedule-Free%20optimizers%2C%20AdEMAMix%2C%20MARS%20and%20Lion%20which%0Amodify%20traditional%20momentum%20mechanisms.%20In%20a%20separate%20line%20of%20work%2C%20theoretical%0Aacceleration%20of%20stochastic%20gradient%20descent%20%28SGD%29%20in%20noise-dominated%20regime%20has%0Abeen%20achieved%20by%20decoupling%20the%20momentum%20coefficient%20from%20the%20current%0Agradient%27s%20weight.%20In%20this%20paper%2C%20we%20establish%20explicit%20connections%20between%0Athese%20two%20lines%20of%20work.%20We%20substantiate%20our%20theoretical%20findings%20with%0Apreliminary%20experiments%20on%20a%20150m%20language%20modeling%20task.%20We%20find%20that%0AAdEMAMix%2C%20which%20most%20closely%20resembles%20accelerated%20versions%20of%20stochastic%0Agradient%20descent%2C%20exhibits%20superior%20performance.%20Building%20on%20these%20insights%2C%20we%0Aintroduce%20a%20modification%20to%20AdEMAMix%2C%20termed%20Simplified-AdEMAMix%2C%20which%0Amaintains%20the%20same%20performance%20as%20AdEMAMix%20across%20both%20large%20and%20small%0Abatch-size%20settings%20while%20eliminating%20the%20need%20for%20two%20different%20momentum%0Aterms.%20The%20code%20for%20Simplified-AdEMAMix%20is%20available%20on%20the%20repository%3A%0Ahttps%3A//github.com/DepenM/Simplified-AdEMAMix/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnections%2520between%2520Schedule-Free%2520Optimizers%252C%2520AdEMAMix%252C%2520and%2520Accelerated%250A%2520%2520SGD%2520Variants%26entry.906535625%3DDepen%2520Morwani%2520and%2520Nikhil%2520Vyas%2520and%2520Hanlin%2520Zhang%2520and%2520Sham%2520Kakade%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520optimization%2520have%2520introduced%2520new%250Aalgorithms%252C%2520such%2520as%2520Schedule-Free%2520optimizers%252C%2520AdEMAMix%252C%2520MARS%2520and%2520Lion%2520which%250Amodify%2520traditional%2520momentum%2520mechanisms.%2520In%2520a%2520separate%2520line%2520of%2520work%252C%2520theoretical%250Aacceleration%2520of%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520in%2520noise-dominated%2520regime%2520has%250Abeen%2520achieved%2520by%2520decoupling%2520the%2520momentum%2520coefficient%2520from%2520the%2520current%250Agradient%2527s%2520weight.%2520In%2520this%2520paper%252C%2520we%2520establish%2520explicit%2520connections%2520between%250Athese%2520two%2520lines%2520of%2520work.%2520We%2520substantiate%2520our%2520theoretical%2520findings%2520with%250Apreliminary%2520experiments%2520on%2520a%2520150m%2520language%2520modeling%2520task.%2520We%2520find%2520that%250AAdEMAMix%252C%2520which%2520most%2520closely%2520resembles%2520accelerated%2520versions%2520of%2520stochastic%250Agradient%2520descent%252C%2520exhibits%2520superior%2520performance.%2520Building%2520on%2520these%2520insights%252C%2520we%250Aintroduce%2520a%2520modification%2520to%2520AdEMAMix%252C%2520termed%2520Simplified-AdEMAMix%252C%2520which%250Amaintains%2520the%2520same%2520performance%2520as%2520AdEMAMix%2520across%2520both%2520large%2520and%2520small%250Abatch-size%2520settings%2520while%2520eliminating%2520the%2520need%2520for%2520two%2520different%2520momentum%250Aterms.%2520The%2520code%2520for%2520Simplified-AdEMAMix%2520is%2520available%2520on%2520the%2520repository%253A%250Ahttps%253A//github.com/DepenM/Simplified-AdEMAMix/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connections%20between%20Schedule-Free%20Optimizers%2C%20AdEMAMix%2C%20and%20Accelerated%0A%20%20SGD%20Variants&entry.906535625=Depen%20Morwani%20and%20Nikhil%20Vyas%20and%20Hanlin%20Zhang%20and%20Sham%20Kakade&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20optimization%20have%20introduced%20new%0Aalgorithms%2C%20such%20as%20Schedule-Free%20optimizers%2C%20AdEMAMix%2C%20MARS%20and%20Lion%20which%0Amodify%20traditional%20momentum%20mechanisms.%20In%20a%20separate%20line%20of%20work%2C%20theoretical%0Aacceleration%20of%20stochastic%20gradient%20descent%20%28SGD%29%20in%20noise-dominated%20regime%20has%0Abeen%20achieved%20by%20decoupling%20the%20momentum%20coefficient%20from%20the%20current%0Agradient%27s%20weight.%20In%20this%20paper%2C%20we%20establish%20explicit%20connections%20between%0Athese%20two%20lines%20of%20work.%20We%20substantiate%20our%20theoretical%20findings%20with%0Apreliminary%20experiments%20on%20a%20150m%20language%20modeling%20task.%20We%20find%20that%0AAdEMAMix%2C%20which%20most%20closely%20resembles%20accelerated%20versions%20of%20stochastic%0Agradient%20descent%2C%20exhibits%20superior%20performance.%20Building%20on%20these%20insights%2C%20we%0Aintroduce%20a%20modification%20to%20AdEMAMix%2C%20termed%20Simplified-AdEMAMix%2C%20which%0Amaintains%20the%20same%20performance%20as%20AdEMAMix%20across%20both%20large%20and%20small%0Abatch-size%20settings%20while%20eliminating%20the%20need%20for%20two%20different%20momentum%0Aterms.%20The%20code%20for%20Simplified-AdEMAMix%20is%20available%20on%20the%20repository%3A%0Ahttps%3A//github.com/DepenM/Simplified-AdEMAMix/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02431v1&entry.124074799=Read"},
{"title": "Brief analysis of DeepSeek R1 and it's implications for Generative AI", "author": "Sarah Mercer and Samuel Spillard and Daniel P. Martin", "abstract": "  In late January 2025, DeepSeek released their new reasoning model (DeepSeek\nR1); which was developed at a fraction of the cost yet remains competitive with\nOpenAI's models, despite the US's GPU export ban. This report discusses the\nmodel, and what its release means for the field of Generative AI more widely.\nWe briefly discuss other models released from China in recent weeks, their\nsimilarities; innovative use of Mixture of Experts (MoE), Reinforcement\nLearning (RL) and clever engineering appear to be key factors in the\ncapabilities of these models. This think piece has been written to a tight\ntime-scale, providing broad coverage of the topic, and serves as introductory\nmaterial for those looking to understand the model's technical advancements, as\nwell as it's place in the ecosystem. Several further areas of research are\nidentified.\n", "link": "http://arxiv.org/abs/2502.02523v1", "date": "2025-02-04", "relevancy": 2.0716, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5266}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5139}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20it%27s%20implications%20for%20Generative%20AI&body=Title%3A%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20it%27s%20implications%20for%20Generative%20AI%0AAuthor%3A%20Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin%0AAbstract%3A%20%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atime-scale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20it%27s%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrief%2520analysis%2520of%2520DeepSeek%2520R1%2520and%2520it%2527s%2520implications%2520for%2520Generative%2520AI%26entry.906535625%3DSarah%2520Mercer%2520and%2520Samuel%2520Spillard%2520and%2520Daniel%2520P.%2520Martin%26entry.1292438233%3D%2520%2520In%2520late%2520January%25202025%252C%2520DeepSeek%2520released%2520their%2520new%2520reasoning%2520model%2520%2528DeepSeek%250AR1%2529%253B%2520which%2520was%2520developed%2520at%2520a%2520fraction%2520of%2520the%2520cost%2520yet%2520remains%2520competitive%2520with%250AOpenAI%2527s%2520models%252C%2520despite%2520the%2520US%2527s%2520GPU%2520export%2520ban.%2520This%2520report%2520discusses%2520the%250Amodel%252C%2520and%2520what%2520its%2520release%2520means%2520for%2520the%2520field%2520of%2520Generative%2520AI%2520more%2520widely.%250AWe%2520briefly%2520discuss%2520other%2520models%2520released%2520from%2520China%2520in%2520recent%2520weeks%252C%2520their%250Asimilarities%253B%2520innovative%2520use%2520of%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520Reinforcement%250ALearning%2520%2528RL%2529%2520and%2520clever%2520engineering%2520appear%2520to%2520be%2520key%2520factors%2520in%2520the%250Acapabilities%2520of%2520these%2520models.%2520This%2520think%2520piece%2520has%2520been%2520written%2520to%2520a%2520tight%250Atime-scale%252C%2520providing%2520broad%2520coverage%2520of%2520the%2520topic%252C%2520and%2520serves%2520as%2520introductory%250Amaterial%2520for%2520those%2520looking%2520to%2520understand%2520the%2520model%2527s%2520technical%2520advancements%252C%2520as%250Awell%2520as%2520it%2527s%2520place%2520in%2520the%2520ecosystem.%2520Several%2520further%2520areas%2520of%2520research%2520are%250Aidentified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brief%20analysis%20of%20DeepSeek%20R1%20and%20it%27s%20implications%20for%20Generative%20AI&entry.906535625=Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin&entry.1292438233=%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atime-scale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20it%27s%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02523v1&entry.124074799=Read"},
{"title": "Foundation Model for Composite Materials and Microstructural Analysis", "author": "Ting-Ju Wei and Chuin-Shan Chen", "abstract": "  The rapid advancement of machine learning has unlocked numerous opportunities\nfor materials science, particularly in accelerating the design and analysis of\nmaterials. However, a significant challenge lies in the scarcity and high cost\nof obtaining high-quality materials datasets. While foundation models\npre-trained on large datasets have excelled in fields like natural language\nprocessing by leveraging latent features through transfer learning, their\napplication in materials science remains limited. Here, we present a foundation\nmodel specifically designed for composite materials. Pre-trained on a dataset\nof short-fiber composites to learn robust latent features, the model accurately\npredicts homogenized stiffness during transfer learning, even with limited\ntraining data. Additionally, our model effectively predicts the material's\nnonlinear behavior by transferring these learned features to an\nInteraction-based Material Network, which is a constitutive surrogate model.\nThese results demonstrate the potential of our foundation model to capture\ncomplex material behaviors. Our findings validate the feasibility and\neffectiveness of foundation models in composite materials. We anticipate\nextending this approach to more complex three-dimensional composite materials,\npolycrystalline materials, and beyond. Moreover, this framework enables\nhigh-accuracy predictions even when experimental data are scarce, paving the\nway for more efficient and cost-effective materials design and analysis.\n", "link": "http://arxiv.org/abs/2411.06565v2", "date": "2025-02-04", "relevancy": 2.0654, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model%20for%20Composite%20Materials%20and%20Microstructural%20Analysis&body=Title%3A%20Foundation%20Model%20for%20Composite%20Materials%20and%20Microstructural%20Analysis%0AAuthor%3A%20Ting-Ju%20Wei%20and%20Chuin-Shan%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20machine%20learning%20has%20unlocked%20numerous%20opportunities%0Afor%20materials%20science%2C%20particularly%20in%20accelerating%20the%20design%20and%20analysis%20of%0Amaterials.%20However%2C%20a%20significant%20challenge%20lies%20in%20the%20scarcity%20and%20high%20cost%0Aof%20obtaining%20high-quality%20materials%20datasets.%20While%20foundation%20models%0Apre-trained%20on%20large%20datasets%20have%20excelled%20in%20fields%20like%20natural%20language%0Aprocessing%20by%20leveraging%20latent%20features%20through%20transfer%20learning%2C%20their%0Aapplication%20in%20materials%20science%20remains%20limited.%20Here%2C%20we%20present%20a%20foundation%0Amodel%20specifically%20designed%20for%20composite%20materials.%20Pre-trained%20on%20a%20dataset%0Aof%20short-fiber%20composites%20to%20learn%20robust%20latent%20features%2C%20the%20model%20accurately%0Apredicts%20homogenized%20stiffness%20during%20transfer%20learning%2C%20even%20with%20limited%0Atraining%20data.%20Additionally%2C%20our%20model%20effectively%20predicts%20the%20material%27s%0Anonlinear%20behavior%20by%20transferring%20these%20learned%20features%20to%20an%0AInteraction-based%20Material%20Network%2C%20which%20is%20a%20constitutive%20surrogate%20model.%0AThese%20results%20demonstrate%20the%20potential%20of%20our%20foundation%20model%20to%20capture%0Acomplex%20material%20behaviors.%20Our%20findings%20validate%20the%20feasibility%20and%0Aeffectiveness%20of%20foundation%20models%20in%20composite%20materials.%20We%20anticipate%0Aextending%20this%20approach%20to%20more%20complex%20three-dimensional%20composite%20materials%2C%0Apolycrystalline%20materials%2C%20and%20beyond.%20Moreover%2C%20this%20framework%20enables%0Ahigh-accuracy%20predictions%20even%20when%20experimental%20data%20are%20scarce%2C%20paving%20the%0Away%20for%20more%20efficient%20and%20cost-effective%20materials%20design%20and%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model%2520for%2520Composite%2520Materials%2520and%2520Microstructural%2520Analysis%26entry.906535625%3DTing-Ju%2520Wei%2520and%2520Chuin-Shan%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520machine%2520learning%2520has%2520unlocked%2520numerous%2520opportunities%250Afor%2520materials%2520science%252C%2520particularly%2520in%2520accelerating%2520the%2520design%2520and%2520analysis%2520of%250Amaterials.%2520However%252C%2520a%2520significant%2520challenge%2520lies%2520in%2520the%2520scarcity%2520and%2520high%2520cost%250Aof%2520obtaining%2520high-quality%2520materials%2520datasets.%2520While%2520foundation%2520models%250Apre-trained%2520on%2520large%2520datasets%2520have%2520excelled%2520in%2520fields%2520like%2520natural%2520language%250Aprocessing%2520by%2520leveraging%2520latent%2520features%2520through%2520transfer%2520learning%252C%2520their%250Aapplication%2520in%2520materials%2520science%2520remains%2520limited.%2520Here%252C%2520we%2520present%2520a%2520foundation%250Amodel%2520specifically%2520designed%2520for%2520composite%2520materials.%2520Pre-trained%2520on%2520a%2520dataset%250Aof%2520short-fiber%2520composites%2520to%2520learn%2520robust%2520latent%2520features%252C%2520the%2520model%2520accurately%250Apredicts%2520homogenized%2520stiffness%2520during%2520transfer%2520learning%252C%2520even%2520with%2520limited%250Atraining%2520data.%2520Additionally%252C%2520our%2520model%2520effectively%2520predicts%2520the%2520material%2527s%250Anonlinear%2520behavior%2520by%2520transferring%2520these%2520learned%2520features%2520to%2520an%250AInteraction-based%2520Material%2520Network%252C%2520which%2520is%2520a%2520constitutive%2520surrogate%2520model.%250AThese%2520results%2520demonstrate%2520the%2520potential%2520of%2520our%2520foundation%2520model%2520to%2520capture%250Acomplex%2520material%2520behaviors.%2520Our%2520findings%2520validate%2520the%2520feasibility%2520and%250Aeffectiveness%2520of%2520foundation%2520models%2520in%2520composite%2520materials.%2520We%2520anticipate%250Aextending%2520this%2520approach%2520to%2520more%2520complex%2520three-dimensional%2520composite%2520materials%252C%250Apolycrystalline%2520materials%252C%2520and%2520beyond.%2520Moreover%252C%2520this%2520framework%2520enables%250Ahigh-accuracy%2520predictions%2520even%2520when%2520experimental%2520data%2520are%2520scarce%252C%2520paving%2520the%250Away%2520for%2520more%2520efficient%2520and%2520cost-effective%2520materials%2520design%2520and%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model%20for%20Composite%20Materials%20and%20Microstructural%20Analysis&entry.906535625=Ting-Ju%20Wei%20and%20Chuin-Shan%20Chen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20machine%20learning%20has%20unlocked%20numerous%20opportunities%0Afor%20materials%20science%2C%20particularly%20in%20accelerating%20the%20design%20and%20analysis%20of%0Amaterials.%20However%2C%20a%20significant%20challenge%20lies%20in%20the%20scarcity%20and%20high%20cost%0Aof%20obtaining%20high-quality%20materials%20datasets.%20While%20foundation%20models%0Apre-trained%20on%20large%20datasets%20have%20excelled%20in%20fields%20like%20natural%20language%0Aprocessing%20by%20leveraging%20latent%20features%20through%20transfer%20learning%2C%20their%0Aapplication%20in%20materials%20science%20remains%20limited.%20Here%2C%20we%20present%20a%20foundation%0Amodel%20specifically%20designed%20for%20composite%20materials.%20Pre-trained%20on%20a%20dataset%0Aof%20short-fiber%20composites%20to%20learn%20robust%20latent%20features%2C%20the%20model%20accurately%0Apredicts%20homogenized%20stiffness%20during%20transfer%20learning%2C%20even%20with%20limited%0Atraining%20data.%20Additionally%2C%20our%20model%20effectively%20predicts%20the%20material%27s%0Anonlinear%20behavior%20by%20transferring%20these%20learned%20features%20to%20an%0AInteraction-based%20Material%20Network%2C%20which%20is%20a%20constitutive%20surrogate%20model.%0AThese%20results%20demonstrate%20the%20potential%20of%20our%20foundation%20model%20to%20capture%0Acomplex%20material%20behaviors.%20Our%20findings%20validate%20the%20feasibility%20and%0Aeffectiveness%20of%20foundation%20models%20in%20composite%20materials.%20We%20anticipate%0Aextending%20this%20approach%20to%20more%20complex%20three-dimensional%20composite%20materials%2C%0Apolycrystalline%20materials%2C%20and%20beyond.%20Moreover%2C%20this%20framework%20enables%0Ahigh-accuracy%20predictions%20even%20when%20experimental%20data%20are%20scarce%2C%20paving%20the%0Away%20for%20more%20efficient%20and%20cost-effective%20materials%20design%20and%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06565v2&entry.124074799=Read"},
{"title": "A Null Space Compliance Approach for Maintaining Safety and Tracking\n  Performance in Human-Robot Interactions", "author": "Zi-Qi Yang and Miaomiao Wang and Mehrdad R. Kermani", "abstract": "  In recent years, the focus on developing robot manipulators has shifted\ntowards prioritizing safety in Human-Robot Interaction (HRI). Impedance control\nis a typical approach for interaction control in collaboration tasks. However,\nsuch a control approach has two main limitations: 1) the end-effector (EE)'s\nlimited compliance to adapt to unknown physical interactions, and 2) inability\nof the robot body to compliantly adapt to unknown physical interactions. In\nthis work, we present an approach to address these drawbacks. We introduce a\nmodified Cartesian impedance control method combined with a Dynamical System\n(DS)-based motion generator, aimed at enhancing the interaction capability of\nthe EE without compromising main task tracking performance. This approach\nenables human coworkers to interact with the EE on-the-fly, e.g. tool\nchangeover, after which the robot compliantly resumes its task. Additionally,\ncombining with a new null space impedance control method enables the robot body\nto exhibit compliant behaviour in response to interactions, avoiding serious\ninjuries from accidental contact while mitigating the impact on main task\ntracking performance. Finally, we prove the passivity of the system and\nvalidate the proposed approach through comprehensive comparative experiments on\na 7 Degree-of-Freedom (DOF) KUKA LWR IV+ robot.\n", "link": "http://arxiv.org/abs/2502.02443v1", "date": "2025-02-04", "relevancy": 1.7167, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5837}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Null%20Space%20Compliance%20Approach%20for%20Maintaining%20Safety%20and%20Tracking%0A%20%20Performance%20in%20Human-Robot%20Interactions&body=Title%3A%20A%20Null%20Space%20Compliance%20Approach%20for%20Maintaining%20Safety%20and%20Tracking%0A%20%20Performance%20in%20Human-Robot%20Interactions%0AAuthor%3A%20Zi-Qi%20Yang%20and%20Miaomiao%20Wang%20and%20Mehrdad%20R.%20Kermani%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20focus%20on%20developing%20robot%20manipulators%20has%20shifted%0Atowards%20prioritizing%20safety%20in%20Human-Robot%20Interaction%20%28HRI%29.%20Impedance%20control%0Ais%20a%20typical%20approach%20for%20interaction%20control%20in%20collaboration%20tasks.%20However%2C%0Asuch%20a%20control%20approach%20has%20two%20main%20limitations%3A%201%29%20the%20end-effector%20%28EE%29%27s%0Alimited%20compliance%20to%20adapt%20to%20unknown%20physical%20interactions%2C%20and%202%29%20inability%0Aof%20the%20robot%20body%20to%20compliantly%20adapt%20to%20unknown%20physical%20interactions.%20In%0Athis%20work%2C%20we%20present%20an%20approach%20to%20address%20these%20drawbacks.%20We%20introduce%20a%0Amodified%20Cartesian%20impedance%20control%20method%20combined%20with%20a%20Dynamical%20System%0A%28DS%29-based%20motion%20generator%2C%20aimed%20at%20enhancing%20the%20interaction%20capability%20of%0Athe%20EE%20without%20compromising%20main%20task%20tracking%20performance.%20This%20approach%0Aenables%20human%20coworkers%20to%20interact%20with%20the%20EE%20on-the-fly%2C%20e.g.%20tool%0Achangeover%2C%20after%20which%20the%20robot%20compliantly%20resumes%20its%20task.%20Additionally%2C%0Acombining%20with%20a%20new%20null%20space%20impedance%20control%20method%20enables%20the%20robot%20body%0Ato%20exhibit%20compliant%20behaviour%20in%20response%20to%20interactions%2C%20avoiding%20serious%0Ainjuries%20from%20accidental%20contact%20while%20mitigating%20the%20impact%20on%20main%20task%0Atracking%20performance.%20Finally%2C%20we%20prove%20the%20passivity%20of%20the%20system%20and%0Avalidate%20the%20proposed%20approach%20through%20comprehensive%20comparative%20experiments%20on%0Aa%207%20Degree-of-Freedom%20%28DOF%29%20KUKA%20LWR%20IV%2B%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Null%2520Space%2520Compliance%2520Approach%2520for%2520Maintaining%2520Safety%2520and%2520Tracking%250A%2520%2520Performance%2520in%2520Human-Robot%2520Interactions%26entry.906535625%3DZi-Qi%2520Yang%2520and%2520Miaomiao%2520Wang%2520and%2520Mehrdad%2520R.%2520Kermani%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520focus%2520on%2520developing%2520robot%2520manipulators%2520has%2520shifted%250Atowards%2520prioritizing%2520safety%2520in%2520Human-Robot%2520Interaction%2520%2528HRI%2529.%2520Impedance%2520control%250Ais%2520a%2520typical%2520approach%2520for%2520interaction%2520control%2520in%2520collaboration%2520tasks.%2520However%252C%250Asuch%2520a%2520control%2520approach%2520has%2520two%2520main%2520limitations%253A%25201%2529%2520the%2520end-effector%2520%2528EE%2529%2527s%250Alimited%2520compliance%2520to%2520adapt%2520to%2520unknown%2520physical%2520interactions%252C%2520and%25202%2529%2520inability%250Aof%2520the%2520robot%2520body%2520to%2520compliantly%2520adapt%2520to%2520unknown%2520physical%2520interactions.%2520In%250Athis%2520work%252C%2520we%2520present%2520an%2520approach%2520to%2520address%2520these%2520drawbacks.%2520We%2520introduce%2520a%250Amodified%2520Cartesian%2520impedance%2520control%2520method%2520combined%2520with%2520a%2520Dynamical%2520System%250A%2528DS%2529-based%2520motion%2520generator%252C%2520aimed%2520at%2520enhancing%2520the%2520interaction%2520capability%2520of%250Athe%2520EE%2520without%2520compromising%2520main%2520task%2520tracking%2520performance.%2520This%2520approach%250Aenables%2520human%2520coworkers%2520to%2520interact%2520with%2520the%2520EE%2520on-the-fly%252C%2520e.g.%2520tool%250Achangeover%252C%2520after%2520which%2520the%2520robot%2520compliantly%2520resumes%2520its%2520task.%2520Additionally%252C%250Acombining%2520with%2520a%2520new%2520null%2520space%2520impedance%2520control%2520method%2520enables%2520the%2520robot%2520body%250Ato%2520exhibit%2520compliant%2520behaviour%2520in%2520response%2520to%2520interactions%252C%2520avoiding%2520serious%250Ainjuries%2520from%2520accidental%2520contact%2520while%2520mitigating%2520the%2520impact%2520on%2520main%2520task%250Atracking%2520performance.%2520Finally%252C%2520we%2520prove%2520the%2520passivity%2520of%2520the%2520system%2520and%250Avalidate%2520the%2520proposed%2520approach%2520through%2520comprehensive%2520comparative%2520experiments%2520on%250Aa%25207%2520Degree-of-Freedom%2520%2528DOF%2529%2520KUKA%2520LWR%2520IV%252B%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Null%20Space%20Compliance%20Approach%20for%20Maintaining%20Safety%20and%20Tracking%0A%20%20Performance%20in%20Human-Robot%20Interactions&entry.906535625=Zi-Qi%20Yang%20and%20Miaomiao%20Wang%20and%20Mehrdad%20R.%20Kermani&entry.1292438233=%20%20In%20recent%20years%2C%20the%20focus%20on%20developing%20robot%20manipulators%20has%20shifted%0Atowards%20prioritizing%20safety%20in%20Human-Robot%20Interaction%20%28HRI%29.%20Impedance%20control%0Ais%20a%20typical%20approach%20for%20interaction%20control%20in%20collaboration%20tasks.%20However%2C%0Asuch%20a%20control%20approach%20has%20two%20main%20limitations%3A%201%29%20the%20end-effector%20%28EE%29%27s%0Alimited%20compliance%20to%20adapt%20to%20unknown%20physical%20interactions%2C%20and%202%29%20inability%0Aof%20the%20robot%20body%20to%20compliantly%20adapt%20to%20unknown%20physical%20interactions.%20In%0Athis%20work%2C%20we%20present%20an%20approach%20to%20address%20these%20drawbacks.%20We%20introduce%20a%0Amodified%20Cartesian%20impedance%20control%20method%20combined%20with%20a%20Dynamical%20System%0A%28DS%29-based%20motion%20generator%2C%20aimed%20at%20enhancing%20the%20interaction%20capability%20of%0Athe%20EE%20without%20compromising%20main%20task%20tracking%20performance.%20This%20approach%0Aenables%20human%20coworkers%20to%20interact%20with%20the%20EE%20on-the-fly%2C%20e.g.%20tool%0Achangeover%2C%20after%20which%20the%20robot%20compliantly%20resumes%20its%20task.%20Additionally%2C%0Acombining%20with%20a%20new%20null%20space%20impedance%20control%20method%20enables%20the%20robot%20body%0Ato%20exhibit%20compliant%20behaviour%20in%20response%20to%20interactions%2C%20avoiding%20serious%0Ainjuries%20from%20accidental%20contact%20while%20mitigating%20the%20impact%20on%20main%20task%0Atracking%20performance.%20Finally%2C%20we%20prove%20the%20passivity%20of%20the%20system%20and%0Avalidate%20the%20proposed%20approach%20through%20comprehensive%20comparative%20experiments%20on%0Aa%207%20Degree-of-Freedom%20%28DOF%29%20KUKA%20LWR%20IV%2B%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02443v1&entry.124074799=Read"},
{"title": "Addressing Label Shift in Distributed Learning via Entropy\n  Regularization", "author": "Zhiyuan Wu and Changkyu Choi and Xiangcheng Cao and Volkan Cevher and Ali Ramezani-Kebrya", "abstract": "  We address the challenge of minimizing true risk in multi-node distributed\nlearning. These systems are frequently exposed to both inter-node and\nintra-node label shifts, which present a critical obstacle to effectively\noptimizing model performance while ensuring that data remains confined to each\nnode. To tackle this, we propose the Versatile Robust Label Shift (VRLS)\nmethod, which enhances the maximum likelihood estimation of the test-to-train\nlabel density ratio. VRLS incorporates Shannon entropy-based regularization and\nadjusts the density ratio during training to better handle label shifts at the\ntest time. In multi-node learning environments, VRLS further extends its\ncapabilities by learning and adapting density ratios across nodes, effectively\nmitigating label shifts and improving overall model performance. Experiments\nconducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectiveness\nof VRLS, outperforming baselines by up to 20% in imbalanced settings. These\nresults highlight the significant improvements VRLS offers in addressing label\nshifts. Our theoretical analysis further supports this by establishing\nhigh-probability bounds on estimation errors.\n", "link": "http://arxiv.org/abs/2502.02544v1", "date": "2025-02-04", "relevancy": 1.5038, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5138}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Label%20Shift%20in%20Distributed%20Learning%20via%20Entropy%0A%20%20Regularization&body=Title%3A%20Addressing%20Label%20Shift%20in%20Distributed%20Learning%20via%20Entropy%0A%20%20Regularization%0AAuthor%3A%20Zhiyuan%20Wu%20and%20Changkyu%20Choi%20and%20Xiangcheng%20Cao%20and%20Volkan%20Cevher%20and%20Ali%20Ramezani-Kebrya%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20minimizing%20true%20risk%20in%20multi-node%20distributed%0Alearning.%20These%20systems%20are%20frequently%20exposed%20to%20both%20inter-node%20and%0Aintra-node%20label%20shifts%2C%20which%20present%20a%20critical%20obstacle%20to%20effectively%0Aoptimizing%20model%20performance%20while%20ensuring%20that%20data%20remains%20confined%20to%20each%0Anode.%20To%20tackle%20this%2C%20we%20propose%20the%20Versatile%20Robust%20Label%20Shift%20%28VRLS%29%0Amethod%2C%20which%20enhances%20the%20maximum%20likelihood%20estimation%20of%20the%20test-to-train%0Alabel%20density%20ratio.%20VRLS%20incorporates%20Shannon%20entropy-based%20regularization%20and%0Aadjusts%20the%20density%20ratio%20during%20training%20to%20better%20handle%20label%20shifts%20at%20the%0Atest%20time.%20In%20multi-node%20learning%20environments%2C%20VRLS%20further%20extends%20its%0Acapabilities%20by%20learning%20and%20adapting%20density%20ratios%20across%20nodes%2C%20effectively%0Amitigating%20label%20shifts%20and%20improving%20overall%20model%20performance.%20Experiments%0Aconducted%20on%20MNIST%2C%20Fashion%20MNIST%2C%20and%20CIFAR-10%20demonstrate%20the%20effectiveness%0Aof%20VRLS%2C%20outperforming%20baselines%20by%20up%20to%2020%25%20in%20imbalanced%20settings.%20These%0Aresults%20highlight%20the%20significant%20improvements%20VRLS%20offers%20in%20addressing%20label%0Ashifts.%20Our%20theoretical%20analysis%20further%20supports%20this%20by%20establishing%0Ahigh-probability%20bounds%20on%20estimation%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Label%2520Shift%2520in%2520Distributed%2520Learning%2520via%2520Entropy%250A%2520%2520Regularization%26entry.906535625%3DZhiyuan%2520Wu%2520and%2520Changkyu%2520Choi%2520and%2520Xiangcheng%2520Cao%2520and%2520Volkan%2520Cevher%2520and%2520Ali%2520Ramezani-Kebrya%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520minimizing%2520true%2520risk%2520in%2520multi-node%2520distributed%250Alearning.%2520These%2520systems%2520are%2520frequently%2520exposed%2520to%2520both%2520inter-node%2520and%250Aintra-node%2520label%2520shifts%252C%2520which%2520present%2520a%2520critical%2520obstacle%2520to%2520effectively%250Aoptimizing%2520model%2520performance%2520while%2520ensuring%2520that%2520data%2520remains%2520confined%2520to%2520each%250Anode.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520the%2520Versatile%2520Robust%2520Label%2520Shift%2520%2528VRLS%2529%250Amethod%252C%2520which%2520enhances%2520the%2520maximum%2520likelihood%2520estimation%2520of%2520the%2520test-to-train%250Alabel%2520density%2520ratio.%2520VRLS%2520incorporates%2520Shannon%2520entropy-based%2520regularization%2520and%250Aadjusts%2520the%2520density%2520ratio%2520during%2520training%2520to%2520better%2520handle%2520label%2520shifts%2520at%2520the%250Atest%2520time.%2520In%2520multi-node%2520learning%2520environments%252C%2520VRLS%2520further%2520extends%2520its%250Acapabilities%2520by%2520learning%2520and%2520adapting%2520density%2520ratios%2520across%2520nodes%252C%2520effectively%250Amitigating%2520label%2520shifts%2520and%2520improving%2520overall%2520model%2520performance.%2520Experiments%250Aconducted%2520on%2520MNIST%252C%2520Fashion%2520MNIST%252C%2520and%2520CIFAR-10%2520demonstrate%2520the%2520effectiveness%250Aof%2520VRLS%252C%2520outperforming%2520baselines%2520by%2520up%2520to%252020%2525%2520in%2520imbalanced%2520settings.%2520These%250Aresults%2520highlight%2520the%2520significant%2520improvements%2520VRLS%2520offers%2520in%2520addressing%2520label%250Ashifts.%2520Our%2520theoretical%2520analysis%2520further%2520supports%2520this%2520by%2520establishing%250Ahigh-probability%2520bounds%2520on%2520estimation%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Label%20Shift%20in%20Distributed%20Learning%20via%20Entropy%0A%20%20Regularization&entry.906535625=Zhiyuan%20Wu%20and%20Changkyu%20Choi%20and%20Xiangcheng%20Cao%20and%20Volkan%20Cevher%20and%20Ali%20Ramezani-Kebrya&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20minimizing%20true%20risk%20in%20multi-node%20distributed%0Alearning.%20These%20systems%20are%20frequently%20exposed%20to%20both%20inter-node%20and%0Aintra-node%20label%20shifts%2C%20which%20present%20a%20critical%20obstacle%20to%20effectively%0Aoptimizing%20model%20performance%20while%20ensuring%20that%20data%20remains%20confined%20to%20each%0Anode.%20To%20tackle%20this%2C%20we%20propose%20the%20Versatile%20Robust%20Label%20Shift%20%28VRLS%29%0Amethod%2C%20which%20enhances%20the%20maximum%20likelihood%20estimation%20of%20the%20test-to-train%0Alabel%20density%20ratio.%20VRLS%20incorporates%20Shannon%20entropy-based%20regularization%20and%0Aadjusts%20the%20density%20ratio%20during%20training%20to%20better%20handle%20label%20shifts%20at%20the%0Atest%20time.%20In%20multi-node%20learning%20environments%2C%20VRLS%20further%20extends%20its%0Acapabilities%20by%20learning%20and%20adapting%20density%20ratios%20across%20nodes%2C%20effectively%0Amitigating%20label%20shifts%20and%20improving%20overall%20model%20performance.%20Experiments%0Aconducted%20on%20MNIST%2C%20Fashion%20MNIST%2C%20and%20CIFAR-10%20demonstrate%20the%20effectiveness%0Aof%20VRLS%2C%20outperforming%20baselines%20by%20up%20to%2020%25%20in%20imbalanced%20settings.%20These%0Aresults%20highlight%20the%20significant%20improvements%20VRLS%20offers%20in%20addressing%20label%0Ashifts.%20Our%20theoretical%20analysis%20further%20supports%20this%20by%20establishing%0Ahigh-probability%20bounds%20on%20estimation%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02544v1&entry.124074799=Read"},
{"title": "Stable Port-Hamiltonian Neural Networks", "author": "Fabian J. Roth and Dominik K. Klein and Maximilian Kannapinn and Jan Peters and Oliver Weeger", "abstract": "  In recent years, nonlinear dynamic system identification using artificial\nneural networks has garnered attention due to its manifold potential\napplications in virtually all branches of science and engineering. However,\npurely data-driven approaches often struggle with extrapolation and may yield\nphysically implausible forecasts. Furthermore, the learned dynamics can exhibit\ninstabilities, making it difficult to apply such models safely and robustly.\nThis article proposes stable port-Hamiltonian neural networks, a machine\nlearning architecture that incorporates the physical biases of energy\nconservation or dissipation while guaranteeing global Lyapunov stability of the\nlearned dynamics. Evaluations with illustrative examples and real-world\nmeasurement data demonstrate the model's ability to generalize from sparse\ndata, outperforming purely data-driven approaches and avoiding instability\nissues. In addition, the model's potential for data-driven surrogate modeling\nis highlighted in application to multi-physics simulation data.\n", "link": "http://arxiv.org/abs/2502.02480v1", "date": "2025-02-04", "relevancy": 2.0005, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5305}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5159}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Port-Hamiltonian%20Neural%20Networks&body=Title%3A%20Stable%20Port-Hamiltonian%20Neural%20Networks%0AAuthor%3A%20Fabian%20J.%20Roth%20and%20Dominik%20K.%20Klein%20and%20Maximilian%20Kannapinn%20and%20Jan%20Peters%20and%20Oliver%20Weeger%0AAbstract%3A%20%20%20In%20recent%20years%2C%20nonlinear%20dynamic%20system%20identification%20using%20artificial%0Aneural%20networks%20has%20garnered%20attention%20due%20to%20its%20manifold%20potential%0Aapplications%20in%20virtually%20all%20branches%20of%20science%20and%20engineering.%20However%2C%0Apurely%20data-driven%20approaches%20often%20struggle%20with%20extrapolation%20and%20may%20yield%0Aphysically%20implausible%20forecasts.%20Furthermore%2C%20the%20learned%20dynamics%20can%20exhibit%0Ainstabilities%2C%20making%20it%20difficult%20to%20apply%20such%20models%20safely%20and%20robustly.%0AThis%20article%20proposes%20stable%20port-Hamiltonian%20neural%20networks%2C%20a%20machine%0Alearning%20architecture%20that%20incorporates%20the%20physical%20biases%20of%20energy%0Aconservation%20or%20dissipation%20while%20guaranteeing%20global%20Lyapunov%20stability%20of%20the%0Alearned%20dynamics.%20Evaluations%20with%20illustrative%20examples%20and%20real-world%0Ameasurement%20data%20demonstrate%20the%20model%27s%20ability%20to%20generalize%20from%20sparse%0Adata%2C%20outperforming%20purely%20data-driven%20approaches%20and%20avoiding%20instability%0Aissues.%20In%20addition%2C%20the%20model%27s%20potential%20for%20data-driven%20surrogate%20modeling%0Ais%20highlighted%20in%20application%20to%20multi-physics%20simulation%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Port-Hamiltonian%2520Neural%2520Networks%26entry.906535625%3DFabian%2520J.%2520Roth%2520and%2520Dominik%2520K.%2520Klein%2520and%2520Maximilian%2520Kannapinn%2520and%2520Jan%2520Peters%2520and%2520Oliver%2520Weeger%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520nonlinear%2520dynamic%2520system%2520identification%2520using%2520artificial%250Aneural%2520networks%2520has%2520garnered%2520attention%2520due%2520to%2520its%2520manifold%2520potential%250Aapplications%2520in%2520virtually%2520all%2520branches%2520of%2520science%2520and%2520engineering.%2520However%252C%250Apurely%2520data-driven%2520approaches%2520often%2520struggle%2520with%2520extrapolation%2520and%2520may%2520yield%250Aphysically%2520implausible%2520forecasts.%2520Furthermore%252C%2520the%2520learned%2520dynamics%2520can%2520exhibit%250Ainstabilities%252C%2520making%2520it%2520difficult%2520to%2520apply%2520such%2520models%2520safely%2520and%2520robustly.%250AThis%2520article%2520proposes%2520stable%2520port-Hamiltonian%2520neural%2520networks%252C%2520a%2520machine%250Alearning%2520architecture%2520that%2520incorporates%2520the%2520physical%2520biases%2520of%2520energy%250Aconservation%2520or%2520dissipation%2520while%2520guaranteeing%2520global%2520Lyapunov%2520stability%2520of%2520the%250Alearned%2520dynamics.%2520Evaluations%2520with%2520illustrative%2520examples%2520and%2520real-world%250Ameasurement%2520data%2520demonstrate%2520the%2520model%2527s%2520ability%2520to%2520generalize%2520from%2520sparse%250Adata%252C%2520outperforming%2520purely%2520data-driven%2520approaches%2520and%2520avoiding%2520instability%250Aissues.%2520In%2520addition%252C%2520the%2520model%2527s%2520potential%2520for%2520data-driven%2520surrogate%2520modeling%250Ais%2520highlighted%2520in%2520application%2520to%2520multi-physics%2520simulation%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Port-Hamiltonian%20Neural%20Networks&entry.906535625=Fabian%20J.%20Roth%20and%20Dominik%20K.%20Klein%20and%20Maximilian%20Kannapinn%20and%20Jan%20Peters%20and%20Oliver%20Weeger&entry.1292438233=%20%20In%20recent%20years%2C%20nonlinear%20dynamic%20system%20identification%20using%20artificial%0Aneural%20networks%20has%20garnered%20attention%20due%20to%20its%20manifold%20potential%0Aapplications%20in%20virtually%20all%20branches%20of%20science%20and%20engineering.%20However%2C%0Apurely%20data-driven%20approaches%20often%20struggle%20with%20extrapolation%20and%20may%20yield%0Aphysically%20implausible%20forecasts.%20Furthermore%2C%20the%20learned%20dynamics%20can%20exhibit%0Ainstabilities%2C%20making%20it%20difficult%20to%20apply%20such%20models%20safely%20and%20robustly.%0AThis%20article%20proposes%20stable%20port-Hamiltonian%20neural%20networks%2C%20a%20machine%0Alearning%20architecture%20that%20incorporates%20the%20physical%20biases%20of%20energy%0Aconservation%20or%20dissipation%20while%20guaranteeing%20global%20Lyapunov%20stability%20of%20the%0Alearned%20dynamics.%20Evaluations%20with%20illustrative%20examples%20and%20real-world%0Ameasurement%20data%20demonstrate%20the%20model%27s%20ability%20to%20generalize%20from%20sparse%0Adata%2C%20outperforming%20purely%20data-driven%20approaches%20and%20avoiding%20instability%0Aissues.%20In%20addition%2C%20the%20model%27s%20potential%20for%20data-driven%20surrogate%20modeling%0Ais%20highlighted%20in%20application%20to%20multi-physics%20simulation%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02480v1&entry.124074799=Read"},
{"title": "Event-Triggered Time-Varying Bayesian Optimization", "author": "Paul Brunzema and Alexander von Rohr and Friedrich Solowjow and Sebastian Trimpe", "abstract": "  We consider the problem of sequentially optimizing a time-varying objective\nfunction using time-varying Bayesian optimization (TVBO). Current approaches to\nTVBO require prior knowledge of a constant rate of change to cope with stale\ndata arising from time variations. However, in practice, the rate of change is\nusually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that\ntreats the optimization problem as static until it detects changes in the\nobjective function and then resets the dataset. This allows the algorithm to\nadapt online to realized temporal changes without the need for exact prior\nknowledge. The event trigger is based on probabilistic uniform error bounds\nused in Gaussian process regression. We derive regret bounds for adaptive\nresets without exact prior knowledge of the temporal changes and show in\nnumerical experiments that ET-GP-UCB outperforms competing GP-UCB algorithms on\nboth synthetic and real-world data. The results demonstrate that ET-GP-UCB is\nreadily applicable without extensive hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2208.10790v6", "date": "2025-02-04", "relevancy": 1.8101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4795}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Triggered%20Time-Varying%20Bayesian%20Optimization&body=Title%3A%20Event-Triggered%20Time-Varying%20Bayesian%20Optimization%0AAuthor%3A%20Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20sequentially%20optimizing%20a%20time-varying%20objective%0Afunction%20using%20time-varying%20Bayesian%20optimization%20%28TVBO%29.%20Current%20approaches%20to%0ATVBO%20require%20prior%20knowledge%20of%20a%20constant%20rate%20of%20change%20to%20cope%20with%20stale%0Adata%20arising%20from%20time%20variations.%20However%2C%20in%20practice%2C%20the%20rate%20of%20change%20is%0Ausually%20unknown.%20We%20propose%20an%20event-triggered%20algorithm%2C%20ET-GP-UCB%2C%20that%0Atreats%20the%20optimization%20problem%20as%20static%20until%20it%20detects%20changes%20in%20the%0Aobjective%20function%20and%20then%20resets%20the%20dataset.%20This%20allows%20the%20algorithm%20to%0Aadapt%20online%20to%20realized%20temporal%20changes%20without%20the%20need%20for%20exact%20prior%0Aknowledge.%20The%20event%20trigger%20is%20based%20on%20probabilistic%20uniform%20error%20bounds%0Aused%20in%20Gaussian%20process%20regression.%20We%20derive%20regret%20bounds%20for%20adaptive%0Aresets%20without%20exact%20prior%20knowledge%20of%20the%20temporal%20changes%20and%20show%20in%0Anumerical%20experiments%20that%20ET-GP-UCB%20outperforms%20competing%20GP-UCB%20algorithms%20on%0Aboth%20synthetic%20and%20real-world%20data.%20The%20results%20demonstrate%20that%20ET-GP-UCB%20is%0Areadily%20applicable%20without%20extensive%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.10790v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Triggered%2520Time-Varying%2520Bayesian%2520Optimization%26entry.906535625%3DPaul%2520Brunzema%2520and%2520Alexander%2520von%2520Rohr%2520and%2520Friedrich%2520Solowjow%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520sequentially%2520optimizing%2520a%2520time-varying%2520objective%250Afunction%2520using%2520time-varying%2520Bayesian%2520optimization%2520%2528TVBO%2529.%2520Current%2520approaches%2520to%250ATVBO%2520require%2520prior%2520knowledge%2520of%2520a%2520constant%2520rate%2520of%2520change%2520to%2520cope%2520with%2520stale%250Adata%2520arising%2520from%2520time%2520variations.%2520However%252C%2520in%2520practice%252C%2520the%2520rate%2520of%2520change%2520is%250Ausually%2520unknown.%2520We%2520propose%2520an%2520event-triggered%2520algorithm%252C%2520ET-GP-UCB%252C%2520that%250Atreats%2520the%2520optimization%2520problem%2520as%2520static%2520until%2520it%2520detects%2520changes%2520in%2520the%250Aobjective%2520function%2520and%2520then%2520resets%2520the%2520dataset.%2520This%2520allows%2520the%2520algorithm%2520to%250Aadapt%2520online%2520to%2520realized%2520temporal%2520changes%2520without%2520the%2520need%2520for%2520exact%2520prior%250Aknowledge.%2520The%2520event%2520trigger%2520is%2520based%2520on%2520probabilistic%2520uniform%2520error%2520bounds%250Aused%2520in%2520Gaussian%2520process%2520regression.%2520We%2520derive%2520regret%2520bounds%2520for%2520adaptive%250Aresets%2520without%2520exact%2520prior%2520knowledge%2520of%2520the%2520temporal%2520changes%2520and%2520show%2520in%250Anumerical%2520experiments%2520that%2520ET-GP-UCB%2520outperforms%2520competing%2520GP-UCB%2520algorithms%2520on%250Aboth%2520synthetic%2520and%2520real-world%2520data.%2520The%2520results%2520demonstrate%2520that%2520ET-GP-UCB%2520is%250Areadily%2520applicable%2520without%2520extensive%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.10790v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Triggered%20Time-Varying%20Bayesian%20Optimization&entry.906535625=Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20sequentially%20optimizing%20a%20time-varying%20objective%0Afunction%20using%20time-varying%20Bayesian%20optimization%20%28TVBO%29.%20Current%20approaches%20to%0ATVBO%20require%20prior%20knowledge%20of%20a%20constant%20rate%20of%20change%20to%20cope%20with%20stale%0Adata%20arising%20from%20time%20variations.%20However%2C%20in%20practice%2C%20the%20rate%20of%20change%20is%0Ausually%20unknown.%20We%20propose%20an%20event-triggered%20algorithm%2C%20ET-GP-UCB%2C%20that%0Atreats%20the%20optimization%20problem%20as%20static%20until%20it%20detects%20changes%20in%20the%0Aobjective%20function%20and%20then%20resets%20the%20dataset.%20This%20allows%20the%20algorithm%20to%0Aadapt%20online%20to%20realized%20temporal%20changes%20without%20the%20need%20for%20exact%20prior%0Aknowledge.%20The%20event%20trigger%20is%20based%20on%20probabilistic%20uniform%20error%20bounds%0Aused%20in%20Gaussian%20process%20regression.%20We%20derive%20regret%20bounds%20for%20adaptive%0Aresets%20without%20exact%20prior%20knowledge%20of%20the%20temporal%20changes%20and%20show%20in%0Anumerical%20experiments%20that%20ET-GP-UCB%20outperforms%20competing%20GP-UCB%20algorithms%20on%0Aboth%20synthetic%20and%20real-world%20data.%20The%20results%20demonstrate%20that%20ET-GP-UCB%20is%0Areadily%20applicable%20without%20extensive%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.10790v6&entry.124074799=Read"},
{"title": "Towards graph neural networks for provably solving convex optimization\n  problems", "author": "Chendi Qian and Christopher Morris", "abstract": "  Recently, message-passing graph neural networks (MPNNs) have shown potential\nfor solving combinatorial and continuous optimization problems due to their\nability to capture variable-constraint interactions. While existing approaches\nleverage MPNNs to approximate solutions or warm-start traditional solvers, they\noften lack guarantees for feasibility, particularly in convex optimization\nsettings. Here, we propose an iterative MPNN framework to solve convex\noptimization problems with provable feasibility guarantees. First, we\ndemonstrate that MPNNs can provably simulate standard interior-point methods\nfor solving quadratic problems with linear constraints, covering relevant\nproblems such as SVMs. Secondly, to ensure feasibility, we introduce a variant\nthat starts from a feasible point and iteratively restricts the search within\nthe feasible region. Experimental results show that our approach outperforms\nexisting neural baselines in solution quality and feasibility, generalizes well\nto unseen problem sizes, and, in some cases, achieves faster solution times\nthan state-of-the-art solvers such as Gurobi.\n", "link": "http://arxiv.org/abs/2502.02446v1", "date": "2025-02-04", "relevancy": 1.4278, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4711}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20graph%20neural%20networks%20for%20provably%20solving%20convex%20optimization%0A%20%20problems&body=Title%3A%20Towards%20graph%20neural%20networks%20for%20provably%20solving%20convex%20optimization%0A%20%20problems%0AAuthor%3A%20Chendi%20Qian%20and%20Christopher%20Morris%0AAbstract%3A%20%20%20Recently%2C%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20have%20shown%20potential%0Afor%20solving%20combinatorial%20and%20continuous%20optimization%20problems%20due%20to%20their%0Aability%20to%20capture%20variable-constraint%20interactions.%20While%20existing%20approaches%0Aleverage%20MPNNs%20to%20approximate%20solutions%20or%20warm-start%20traditional%20solvers%2C%20they%0Aoften%20lack%20guarantees%20for%20feasibility%2C%20particularly%20in%20convex%20optimization%0Asettings.%20Here%2C%20we%20propose%20an%20iterative%20MPNN%20framework%20to%20solve%20convex%0Aoptimization%20problems%20with%20provable%20feasibility%20guarantees.%20First%2C%20we%0Ademonstrate%20that%20MPNNs%20can%20provably%20simulate%20standard%20interior-point%20methods%0Afor%20solving%20quadratic%20problems%20with%20linear%20constraints%2C%20covering%20relevant%0Aproblems%20such%20as%20SVMs.%20Secondly%2C%20to%20ensure%20feasibility%2C%20we%20introduce%20a%20variant%0Athat%20starts%20from%20a%20feasible%20point%20and%20iteratively%20restricts%20the%20search%20within%0Athe%20feasible%20region.%20Experimental%20results%20show%20that%20our%20approach%20outperforms%0Aexisting%20neural%20baselines%20in%20solution%20quality%20and%20feasibility%2C%20generalizes%20well%0Ato%20unseen%20problem%20sizes%2C%20and%2C%20in%20some%20cases%2C%20achieves%20faster%20solution%20times%0Athan%20state-of-the-art%20solvers%20such%20as%20Gurobi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520graph%2520neural%2520networks%2520for%2520provably%2520solving%2520convex%2520optimization%250A%2520%2520problems%26entry.906535625%3DChendi%2520Qian%2520and%2520Christopher%2520Morris%26entry.1292438233%3D%2520%2520Recently%252C%2520message-passing%2520graph%2520neural%2520networks%2520%2528MPNNs%2529%2520have%2520shown%2520potential%250Afor%2520solving%2520combinatorial%2520and%2520continuous%2520optimization%2520problems%2520due%2520to%2520their%250Aability%2520to%2520capture%2520variable-constraint%2520interactions.%2520While%2520existing%2520approaches%250Aleverage%2520MPNNs%2520to%2520approximate%2520solutions%2520or%2520warm-start%2520traditional%2520solvers%252C%2520they%250Aoften%2520lack%2520guarantees%2520for%2520feasibility%252C%2520particularly%2520in%2520convex%2520optimization%250Asettings.%2520Here%252C%2520we%2520propose%2520an%2520iterative%2520MPNN%2520framework%2520to%2520solve%2520convex%250Aoptimization%2520problems%2520with%2520provable%2520feasibility%2520guarantees.%2520First%252C%2520we%250Ademonstrate%2520that%2520MPNNs%2520can%2520provably%2520simulate%2520standard%2520interior-point%2520methods%250Afor%2520solving%2520quadratic%2520problems%2520with%2520linear%2520constraints%252C%2520covering%2520relevant%250Aproblems%2520such%2520as%2520SVMs.%2520Secondly%252C%2520to%2520ensure%2520feasibility%252C%2520we%2520introduce%2520a%2520variant%250Athat%2520starts%2520from%2520a%2520feasible%2520point%2520and%2520iteratively%2520restricts%2520the%2520search%2520within%250Athe%2520feasible%2520region.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520outperforms%250Aexisting%2520neural%2520baselines%2520in%2520solution%2520quality%2520and%2520feasibility%252C%2520generalizes%2520well%250Ato%2520unseen%2520problem%2520sizes%252C%2520and%252C%2520in%2520some%2520cases%252C%2520achieves%2520faster%2520solution%2520times%250Athan%2520state-of-the-art%2520solvers%2520such%2520as%2520Gurobi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20graph%20neural%20networks%20for%20provably%20solving%20convex%20optimization%0A%20%20problems&entry.906535625=Chendi%20Qian%20and%20Christopher%20Morris&entry.1292438233=%20%20Recently%2C%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20have%20shown%20potential%0Afor%20solving%20combinatorial%20and%20continuous%20optimization%20problems%20due%20to%20their%0Aability%20to%20capture%20variable-constraint%20interactions.%20While%20existing%20approaches%0Aleverage%20MPNNs%20to%20approximate%20solutions%20or%20warm-start%20traditional%20solvers%2C%20they%0Aoften%20lack%20guarantees%20for%20feasibility%2C%20particularly%20in%20convex%20optimization%0Asettings.%20Here%2C%20we%20propose%20an%20iterative%20MPNN%20framework%20to%20solve%20convex%0Aoptimization%20problems%20with%20provable%20feasibility%20guarantees.%20First%2C%20we%0Ademonstrate%20that%20MPNNs%20can%20provably%20simulate%20standard%20interior-point%20methods%0Afor%20solving%20quadratic%20problems%20with%20linear%20constraints%2C%20covering%20relevant%0Aproblems%20such%20as%20SVMs.%20Secondly%2C%20to%20ensure%20feasibility%2C%20we%20introduce%20a%20variant%0Athat%20starts%20from%20a%20feasible%20point%20and%20iteratively%20restricts%20the%20search%20within%0Athe%20feasible%20region.%20Experimental%20results%20show%20that%20our%20approach%20outperforms%0Aexisting%20neural%20baselines%20in%20solution%20quality%20and%20feasibility%2C%20generalizes%20well%0Ato%20unseen%20problem%20sizes%2C%20and%2C%20in%20some%20cases%2C%20achieves%20faster%20solution%20times%0Athan%20state-of-the-art%20solvers%20such%20as%20Gurobi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02446v1&entry.124074799=Read"},
{"title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search", "author": "Maohao Shen and Guangtao Zeng and Zhenting Qi and Zhang-Wei Hong and Zhenfang Chen and Wei Lu and Gregory Wornell and Subhro Das and David Cox and Chuang Gan", "abstract": "  Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced.\n", "link": "http://arxiv.org/abs/2502.02508v1", "date": "2025-02-04", "relevancy": 1.979, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Satori%3A%20Reinforcement%20Learning%20with%20Chain-of-Action-Thought%20Enhances%20LLM%0A%20%20Reasoning%20via%20Autoregressive%20Search&body=Title%3A%20Satori%3A%20Reinforcement%20Learning%20with%20Chain-of-Action-Thought%20Enhances%20LLM%0A%20%20Reasoning%20via%20Autoregressive%20Search%0AAuthor%3A%20Maohao%20Shen%20and%20Guangtao%20Zeng%20and%20Zhenting%20Qi%20and%20Zhang-Wei%20Hong%20and%20Zhenfang%20Chen%20and%20Wei%20Lu%20and%20Gregory%20Wornell%20and%20Subhro%20Das%20and%20David%20Cox%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20across%20diverse%20domains.%20Recent%20studies%20have%20shown%20that%20increasing%0Atest-time%20computation%20enhances%20LLMs%27%20reasoning%20capabilities.%20This%20typically%0Ainvolves%20extensive%20sampling%20at%20inference%20time%20guided%20by%20an%20external%20LLM%0Averifier%2C%20resulting%20in%20a%20two-player%20system.%20Despite%20external%20guidance%2C%20the%0Aeffectiveness%20of%20this%20system%20demonstrates%20the%20potential%20of%20a%20single%20LLM%20to%0Atackle%20complex%20tasks.%20Thus%2C%20we%20pose%20a%20new%20research%20problem%3A%20Can%20we%20internalize%0Athe%20searching%20capabilities%20to%20fundamentally%20enhance%20the%20reasoning%20abilities%20of%0Aa%20single%20LLM%3F%20This%20work%20explores%20an%20orthogonal%20direction%20focusing%20on%0Apost-training%20LLMs%20for%20autoregressive%20searching%20%28i.e.%2C%20an%20extended%20reasoning%0Aprocess%20with%20self-reflection%20and%20self-exploration%20of%20new%20strategies%29.%20To%0Aachieve%20this%2C%20we%20propose%20the%20Chain-of-Action-Thought%20%28COAT%29%20reasoning%20and%20a%0Atwo-stage%20training%20paradigm%3A%201%29%20a%20small-scale%20format%20tuning%20stage%20to%0Ainternalize%20the%20COAT%20reasoning%20format%20and%202%29%20a%20large-scale%20self-improvement%0Astage%20leveraging%20reinforcement%20learning.%20Our%20approach%20results%20in%20Satori%2C%20a%207B%0ALLM%20trained%20on%20open-source%20models%20and%20data.%20Extensive%20empirical%20evaluations%0Ademonstrate%20that%20Satori%20achieves%20state-of-the-art%20performance%20on%20mathematical%0Areasoning%20benchmarks%20while%20exhibits%20strong%20generalization%20to%20out-of-domain%0Atasks.%20Code%2C%20data%2C%20and%20models%20will%20be%20fully%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSatori%253A%2520Reinforcement%2520Learning%2520with%2520Chain-of-Action-Thought%2520Enhances%2520LLM%250A%2520%2520Reasoning%2520via%2520Autoregressive%2520Search%26entry.906535625%3DMaohao%2520Shen%2520and%2520Guangtao%2520Zeng%2520and%2520Zhenting%2520Qi%2520and%2520Zhang-Wei%2520Hong%2520and%2520Zhenfang%2520Chen%2520and%2520Wei%2520Lu%2520and%2520Gregory%2520Wornell%2520and%2520Subhro%2520Das%2520and%2520David%2520Cox%2520and%2520Chuang%2520Gan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520reasoning%250Acapabilities%2520across%2520diverse%2520domains.%2520Recent%2520studies%2520have%2520shown%2520that%2520increasing%250Atest-time%2520computation%2520enhances%2520LLMs%2527%2520reasoning%2520capabilities.%2520This%2520typically%250Ainvolves%2520extensive%2520sampling%2520at%2520inference%2520time%2520guided%2520by%2520an%2520external%2520LLM%250Averifier%252C%2520resulting%2520in%2520a%2520two-player%2520system.%2520Despite%2520external%2520guidance%252C%2520the%250Aeffectiveness%2520of%2520this%2520system%2520demonstrates%2520the%2520potential%2520of%2520a%2520single%2520LLM%2520to%250Atackle%2520complex%2520tasks.%2520Thus%252C%2520we%2520pose%2520a%2520new%2520research%2520problem%253A%2520Can%2520we%2520internalize%250Athe%2520searching%2520capabilities%2520to%2520fundamentally%2520enhance%2520the%2520reasoning%2520abilities%2520of%250Aa%2520single%2520LLM%253F%2520This%2520work%2520explores%2520an%2520orthogonal%2520direction%2520focusing%2520on%250Apost-training%2520LLMs%2520for%2520autoregressive%2520searching%2520%2528i.e.%252C%2520an%2520extended%2520reasoning%250Aprocess%2520with%2520self-reflection%2520and%2520self-exploration%2520of%2520new%2520strategies%2529.%2520To%250Aachieve%2520this%252C%2520we%2520propose%2520the%2520Chain-of-Action-Thought%2520%2528COAT%2529%2520reasoning%2520and%2520a%250Atwo-stage%2520training%2520paradigm%253A%25201%2529%2520a%2520small-scale%2520format%2520tuning%2520stage%2520to%250Ainternalize%2520the%2520COAT%2520reasoning%2520format%2520and%25202%2529%2520a%2520large-scale%2520self-improvement%250Astage%2520leveraging%2520reinforcement%2520learning.%2520Our%2520approach%2520results%2520in%2520Satori%252C%2520a%25207B%250ALLM%2520trained%2520on%2520open-source%2520models%2520and%2520data.%2520Extensive%2520empirical%2520evaluations%250Ademonstrate%2520that%2520Satori%2520achieves%2520state-of-the-art%2520performance%2520on%2520mathematical%250Areasoning%2520benchmarks%2520while%2520exhibits%2520strong%2520generalization%2520to%2520out-of-domain%250Atasks.%2520Code%252C%2520data%252C%2520and%2520models%2520will%2520be%2520fully%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Satori%3A%20Reinforcement%20Learning%20with%20Chain-of-Action-Thought%20Enhances%20LLM%0A%20%20Reasoning%20via%20Autoregressive%20Search&entry.906535625=Maohao%20Shen%20and%20Guangtao%20Zeng%20and%20Zhenting%20Qi%20and%20Zhang-Wei%20Hong%20and%20Zhenfang%20Chen%20and%20Wei%20Lu%20and%20Gregory%20Wornell%20and%20Subhro%20Das%20and%20David%20Cox%20and%20Chuang%20Gan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20across%20diverse%20domains.%20Recent%20studies%20have%20shown%20that%20increasing%0Atest-time%20computation%20enhances%20LLMs%27%20reasoning%20capabilities.%20This%20typically%0Ainvolves%20extensive%20sampling%20at%20inference%20time%20guided%20by%20an%20external%20LLM%0Averifier%2C%20resulting%20in%20a%20two-player%20system.%20Despite%20external%20guidance%2C%20the%0Aeffectiveness%20of%20this%20system%20demonstrates%20the%20potential%20of%20a%20single%20LLM%20to%0Atackle%20complex%20tasks.%20Thus%2C%20we%20pose%20a%20new%20research%20problem%3A%20Can%20we%20internalize%0Athe%20searching%20capabilities%20to%20fundamentally%20enhance%20the%20reasoning%20abilities%20of%0Aa%20single%20LLM%3F%20This%20work%20explores%20an%20orthogonal%20direction%20focusing%20on%0Apost-training%20LLMs%20for%20autoregressive%20searching%20%28i.e.%2C%20an%20extended%20reasoning%0Aprocess%20with%20self-reflection%20and%20self-exploration%20of%20new%20strategies%29.%20To%0Aachieve%20this%2C%20we%20propose%20the%20Chain-of-Action-Thought%20%28COAT%29%20reasoning%20and%20a%0Atwo-stage%20training%20paradigm%3A%201%29%20a%20small-scale%20format%20tuning%20stage%20to%0Ainternalize%20the%20COAT%20reasoning%20format%20and%202%29%20a%20large-scale%20self-improvement%0Astage%20leveraging%20reinforcement%20learning.%20Our%20approach%20results%20in%20Satori%2C%20a%207B%0ALLM%20trained%20on%20open-source%20models%20and%20data.%20Extensive%20empirical%20evaluations%0Ademonstrate%20that%20Satori%20achieves%20state-of-the-art%20performance%20on%20mathematical%0Areasoning%20benchmarks%20while%20exhibits%20strong%20generalization%20to%20out-of-domain%0Atasks.%20Code%2C%20data%2C%20and%20models%20will%20be%20fully%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02508v1&entry.124074799=Read"},
{"title": "Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD", "author": "Arseniy Andreyev and Pierfrancesco Beneventano", "abstract": "  Recent findings by Cohen et al., 2021, demonstrate that when training neural\nnetworks with full-batch gradient descent with a step size of $\\eta$, the\nlargest eigenvalue $\\lambda_{\\max}$ of the full-batch Hessian consistently\nstabilizes at $\\lambda_{\\max} = 2/\\eta$. These results have significant\nimplications for convergence and generalization. This, however, is not the case\nof mini-batch stochastic gradient descent (SGD), limiting the broader\napplicability of its consequences. We show that SGD trains in a different\nregime we term Edge of Stochastic Stability (EoSS). In this regime, what\nstabilizes at $2/\\eta$ is *Batch Sharpness*: the expected directional curvature\nof mini-batch Hessians along their corresponding stochastic gradients. As a\nconsequence $\\lambda_{\\max}$--which is generally smaller than Batch\nSharpness--is suppressed, aligning with the long-standing empirical observation\nthat smaller batches and larger step sizes favor flatter minima. We further\ndiscuss implications for mathematical modeling of SGD trajectories.\n", "link": "http://arxiv.org/abs/2412.20553v3", "date": "2025-02-04", "relevancy": 1.8647, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4942}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4493}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20of%20Stochastic%20Stability%3A%20Revisiting%20the%20Edge%20of%20Stability%20for%20SGD&body=Title%3A%20Edge%20of%20Stochastic%20Stability%3A%20Revisiting%20the%20Edge%20of%20Stability%20for%20SGD%0AAuthor%3A%20Arseniy%20Andreyev%20and%20Pierfrancesco%20Beneventano%0AAbstract%3A%20%20%20Recent%20findings%20by%20Cohen%20et%20al.%2C%202021%2C%20demonstrate%20that%20when%20training%20neural%0Anetworks%20with%20full-batch%20gradient%20descent%20with%20a%20step%20size%20of%20%24%5Ceta%24%2C%20the%0Alargest%20eigenvalue%20%24%5Clambda_%7B%5Cmax%7D%24%20of%20the%20full-batch%20Hessian%20consistently%0Astabilizes%20at%20%24%5Clambda_%7B%5Cmax%7D%20%3D%202/%5Ceta%24.%20These%20results%20have%20significant%0Aimplications%20for%20convergence%20and%20generalization.%20This%2C%20however%2C%20is%20not%20the%20case%0Aof%20mini-batch%20stochastic%20gradient%20descent%20%28SGD%29%2C%20limiting%20the%20broader%0Aapplicability%20of%20its%20consequences.%20We%20show%20that%20SGD%20trains%20in%20a%20different%0Aregime%20we%20term%20Edge%20of%20Stochastic%20Stability%20%28EoSS%29.%20In%20this%20regime%2C%20what%0Astabilizes%20at%20%242/%5Ceta%24%20is%20%2ABatch%20Sharpness%2A%3A%20the%20expected%20directional%20curvature%0Aof%20mini-batch%20Hessians%20along%20their%20corresponding%20stochastic%20gradients.%20As%20a%0Aconsequence%20%24%5Clambda_%7B%5Cmax%7D%24--which%20is%20generally%20smaller%20than%20Batch%0ASharpness--is%20suppressed%2C%20aligning%20with%20the%20long-standing%20empirical%20observation%0Athat%20smaller%20batches%20and%20larger%20step%20sizes%20favor%20flatter%20minima.%20We%20further%0Adiscuss%20implications%20for%20mathematical%20modeling%20of%20SGD%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520of%2520Stochastic%2520Stability%253A%2520Revisiting%2520the%2520Edge%2520of%2520Stability%2520for%2520SGD%26entry.906535625%3DArseniy%2520Andreyev%2520and%2520Pierfrancesco%2520Beneventano%26entry.1292438233%3D%2520%2520Recent%2520findings%2520by%2520Cohen%2520et%2520al.%252C%25202021%252C%2520demonstrate%2520that%2520when%2520training%2520neural%250Anetworks%2520with%2520full-batch%2520gradient%2520descent%2520with%2520a%2520step%2520size%2520of%2520%2524%255Ceta%2524%252C%2520the%250Alargest%2520eigenvalue%2520%2524%255Clambda_%257B%255Cmax%257D%2524%2520of%2520the%2520full-batch%2520Hessian%2520consistently%250Astabilizes%2520at%2520%2524%255Clambda_%257B%255Cmax%257D%2520%253D%25202/%255Ceta%2524.%2520These%2520results%2520have%2520significant%250Aimplications%2520for%2520convergence%2520and%2520generalization.%2520This%252C%2520however%252C%2520is%2520not%2520the%2520case%250Aof%2520mini-batch%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520limiting%2520the%2520broader%250Aapplicability%2520of%2520its%2520consequences.%2520We%2520show%2520that%2520SGD%2520trains%2520in%2520a%2520different%250Aregime%2520we%2520term%2520Edge%2520of%2520Stochastic%2520Stability%2520%2528EoSS%2529.%2520In%2520this%2520regime%252C%2520what%250Astabilizes%2520at%2520%25242/%255Ceta%2524%2520is%2520%252ABatch%2520Sharpness%252A%253A%2520the%2520expected%2520directional%2520curvature%250Aof%2520mini-batch%2520Hessians%2520along%2520their%2520corresponding%2520stochastic%2520gradients.%2520As%2520a%250Aconsequence%2520%2524%255Clambda_%257B%255Cmax%257D%2524--which%2520is%2520generally%2520smaller%2520than%2520Batch%250ASharpness--is%2520suppressed%252C%2520aligning%2520with%2520the%2520long-standing%2520empirical%2520observation%250Athat%2520smaller%2520batches%2520and%2520larger%2520step%2520sizes%2520favor%2520flatter%2520minima.%2520We%2520further%250Adiscuss%2520implications%2520for%2520mathematical%2520modeling%2520of%2520SGD%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20of%20Stochastic%20Stability%3A%20Revisiting%20the%20Edge%20of%20Stability%20for%20SGD&entry.906535625=Arseniy%20Andreyev%20and%20Pierfrancesco%20Beneventano&entry.1292438233=%20%20Recent%20findings%20by%20Cohen%20et%20al.%2C%202021%2C%20demonstrate%20that%20when%20training%20neural%0Anetworks%20with%20full-batch%20gradient%20descent%20with%20a%20step%20size%20of%20%24%5Ceta%24%2C%20the%0Alargest%20eigenvalue%20%24%5Clambda_%7B%5Cmax%7D%24%20of%20the%20full-batch%20Hessian%20consistently%0Astabilizes%20at%20%24%5Clambda_%7B%5Cmax%7D%20%3D%202/%5Ceta%24.%20These%20results%20have%20significant%0Aimplications%20for%20convergence%20and%20generalization.%20This%2C%20however%2C%20is%20not%20the%20case%0Aof%20mini-batch%20stochastic%20gradient%20descent%20%28SGD%29%2C%20limiting%20the%20broader%0Aapplicability%20of%20its%20consequences.%20We%20show%20that%20SGD%20trains%20in%20a%20different%0Aregime%20we%20term%20Edge%20of%20Stochastic%20Stability%20%28EoSS%29.%20In%20this%20regime%2C%20what%0Astabilizes%20at%20%242/%5Ceta%24%20is%20%2ABatch%20Sharpness%2A%3A%20the%20expected%20directional%20curvature%0Aof%20mini-batch%20Hessians%20along%20their%20corresponding%20stochastic%20gradients.%20As%20a%0Aconsequence%20%24%5Clambda_%7B%5Cmax%7D%24--which%20is%20generally%20smaller%20than%20Batch%0ASharpness--is%20suppressed%2C%20aligning%20with%20the%20long-standing%20empirical%20observation%0Athat%20smaller%20batches%20and%20larger%20step%20sizes%20favor%20flatter%20minima.%20We%20further%0Adiscuss%20implications%20for%20mathematical%20modeling%20of%20SGD%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20553v3&entry.124074799=Read"},
{"title": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics", "author": "Kamer Ali Yuksel and Hassan Sawaf", "abstract": "  Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies.\n", "link": "http://arxiv.org/abs/2502.00029v2", "date": "2025-02-04", "relevancy": 1.3497, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaSharpe%3A%20LLM-Driven%20Discovery%20of%20Robust%20Risk-Adjusted%20Metrics&body=Title%3A%20AlphaSharpe%3A%20LLM-Driven%20Discovery%20of%20Robust%20Risk-Adjusted%20Metrics%0AAuthor%3A%20Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf%0AAbstract%3A%20%20%20Financial%20metrics%20like%20the%20Sharpe%20ratio%20are%20pivotal%20in%20evaluating%20investment%0Aperformance%20by%20balancing%20risk%20and%20return.%20However%2C%20traditional%20metrics%20often%0Astruggle%20with%20robustness%20and%20generalization%2C%20particularly%20in%20dynamic%20and%0Avolatile%20market%20conditions.%20This%20paper%20introduces%20AlphaSharpe%2C%20a%20novel%0Aframework%20leveraging%20large%20language%20models%20%28LLMs%29%20to%20iteratively%20evolve%20and%0Aoptimize%20financial%20metrics%20to%20discover%20enhanced%20risk-return%20metrics%20that%0Aoutperform%20traditional%20approaches%20in%20robustness%20and%20correlation%20with%20future%0Aperformance%20metrics%20by%20employing%20iterative%20crossover%2C%20mutation%2C%20and%20evaluation.%0AKey%20contributions%20of%20this%20work%20include%3A%20%281%29%20a%20novel%20use%20of%20LLMs%20to%20generate%20and%0Arefine%20financial%20metrics%20with%20implicit%20domain-specific%20knowledge%2C%20%282%29%20a%20scoring%0Amechanism%20to%20ensure%20that%20evolved%20metrics%20generalize%20effectively%20to%20unseen%20data%2C%0Aand%20%283%29%20an%20empirical%20demonstration%20of%203x%20predictive%20power%20for%20future%0Arisk-returns%2C%20and%202x%20portfolio%20performance.%20Experimental%20results%20in%20a%0Areal-world%20dataset%20highlight%20the%20superiority%20of%20discovered%20metrics%2C%20making%20them%0Ahighly%20relevant%20to%20portfolio%20managers%20and%20financial%20decision-makers.%20This%0Aframework%20not%20only%20addresses%20the%20limitations%20of%20existing%20metrics%20but%20also%0Ashowcases%20the%20potential%20of%20LLMs%20in%20advancing%20financial%20analytics%2C%20paving%20the%0Away%20for%20informed%20and%20robust%20investment%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaSharpe%253A%2520LLM-Driven%2520Discovery%2520of%2520Robust%2520Risk-Adjusted%2520Metrics%26entry.906535625%3DKamer%2520Ali%2520Yuksel%2520and%2520Hassan%2520Sawaf%26entry.1292438233%3D%2520%2520Financial%2520metrics%2520like%2520the%2520Sharpe%2520ratio%2520are%2520pivotal%2520in%2520evaluating%2520investment%250Aperformance%2520by%2520balancing%2520risk%2520and%2520return.%2520However%252C%2520traditional%2520metrics%2520often%250Astruggle%2520with%2520robustness%2520and%2520generalization%252C%2520particularly%2520in%2520dynamic%2520and%250Avolatile%2520market%2520conditions.%2520This%2520paper%2520introduces%2520AlphaSharpe%252C%2520a%2520novel%250Aframework%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520iteratively%2520evolve%2520and%250Aoptimize%2520financial%2520metrics%2520to%2520discover%2520enhanced%2520risk-return%2520metrics%2520that%250Aoutperform%2520traditional%2520approaches%2520in%2520robustness%2520and%2520correlation%2520with%2520future%250Aperformance%2520metrics%2520by%2520employing%2520iterative%2520crossover%252C%2520mutation%252C%2520and%2520evaluation.%250AKey%2520contributions%2520of%2520this%2520work%2520include%253A%2520%25281%2529%2520a%2520novel%2520use%2520of%2520LLMs%2520to%2520generate%2520and%250Arefine%2520financial%2520metrics%2520with%2520implicit%2520domain-specific%2520knowledge%252C%2520%25282%2529%2520a%2520scoring%250Amechanism%2520to%2520ensure%2520that%2520evolved%2520metrics%2520generalize%2520effectively%2520to%2520unseen%2520data%252C%250Aand%2520%25283%2529%2520an%2520empirical%2520demonstration%2520of%25203x%2520predictive%2520power%2520for%2520future%250Arisk-returns%252C%2520and%25202x%2520portfolio%2520performance.%2520Experimental%2520results%2520in%2520a%250Areal-world%2520dataset%2520highlight%2520the%2520superiority%2520of%2520discovered%2520metrics%252C%2520making%2520them%250Ahighly%2520relevant%2520to%2520portfolio%2520managers%2520and%2520financial%2520decision-makers.%2520This%250Aframework%2520not%2520only%2520addresses%2520the%2520limitations%2520of%2520existing%2520metrics%2520but%2520also%250Ashowcases%2520the%2520potential%2520of%2520LLMs%2520in%2520advancing%2520financial%2520analytics%252C%2520paving%2520the%250Away%2520for%2520informed%2520and%2520robust%2520investment%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaSharpe%3A%20LLM-Driven%20Discovery%20of%20Robust%20Risk-Adjusted%20Metrics&entry.906535625=Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf&entry.1292438233=%20%20Financial%20metrics%20like%20the%20Sharpe%20ratio%20are%20pivotal%20in%20evaluating%20investment%0Aperformance%20by%20balancing%20risk%20and%20return.%20However%2C%20traditional%20metrics%20often%0Astruggle%20with%20robustness%20and%20generalization%2C%20particularly%20in%20dynamic%20and%0Avolatile%20market%20conditions.%20This%20paper%20introduces%20AlphaSharpe%2C%20a%20novel%0Aframework%20leveraging%20large%20language%20models%20%28LLMs%29%20to%20iteratively%20evolve%20and%0Aoptimize%20financial%20metrics%20to%20discover%20enhanced%20risk-return%20metrics%20that%0Aoutperform%20traditional%20approaches%20in%20robustness%20and%20correlation%20with%20future%0Aperformance%20metrics%20by%20employing%20iterative%20crossover%2C%20mutation%2C%20and%20evaluation.%0AKey%20contributions%20of%20this%20work%20include%3A%20%281%29%20a%20novel%20use%20of%20LLMs%20to%20generate%20and%0Arefine%20financial%20metrics%20with%20implicit%20domain-specific%20knowledge%2C%20%282%29%20a%20scoring%0Amechanism%20to%20ensure%20that%20evolved%20metrics%20generalize%20effectively%20to%20unseen%20data%2C%0Aand%20%283%29%20an%20empirical%20demonstration%20of%203x%20predictive%20power%20for%20future%0Arisk-returns%2C%20and%202x%20portfolio%20performance.%20Experimental%20results%20in%20a%0Areal-world%20dataset%20highlight%20the%20superiority%20of%20discovered%20metrics%2C%20making%20them%0Ahighly%20relevant%20to%20portfolio%20managers%20and%20financial%20decision-makers.%20This%0Aframework%20not%20only%20addresses%20the%20limitations%20of%20existing%20metrics%20but%20also%0Ashowcases%20the%20potential%20of%20LLMs%20in%20advancing%20financial%20analytics%2C%20paving%20the%0Away%20for%20informed%20and%20robust%20investment%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00029v2&entry.124074799=Read"},
{"title": "A Tidal Current Speed Forecasting Model based on Multi-Periodicity\n  Learning", "author": "Tengfei Cheng and Yangdi Huang and Yunxuan Dong", "abstract": "  Tidal energy is one of the key components in increasing the penetration rate\nof renewable energy. The penetration of tidal energy in the electrical grid\ndepends on the accuracy of tidal current speed forecasting. Modeling\ninaccuracies hinder forecast accuracy. Previous research has primarily used\nphysical models to forecast tidal current speed. However, tidal current\nvariations influenced by the orbital periods of celestial bodies make accurate\nphysical modeling challenging. Researching the multi-periodicity of tides is\ncrucial for accurately forecasting tidal current speed. In this article, we\npropose the Wavelet-Enhanced Convolutional Network (WCN) to learn\nmulti-periodicity. The framework embeds intra-period and inter-period\nvariations of one-dimensional tidal current data into the rows and columns of a\ntwo-dimensional tensor. Then, the two-dimensional variations of the sequence\ncan be processed by convolutional kernels. We integrate a time-frequency\nanalysis method into the framework to further address local periodic features.\nAdditionally, to enhance the framework's stability, we optimize the framework's\nhyperparameters with the Tree-structured Parzen Estimator algorithm. The\nproposed framework avoids the lack of learning multi-periodicity. Compared with\nbenchmarks, the proposed framework reduces the mean absolute error and mean\nsquare error in 10-step forecasting by, at most, 90.36% and 97.56%,\nrespectively.\n", "link": "http://arxiv.org/abs/2410.09718v2", "date": "2025-02-04", "relevancy": 0.9635, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4761}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tidal%20Current%20Speed%20Forecasting%20Model%20based%20on%20Multi-Periodicity%0A%20%20Learning&body=Title%3A%20A%20Tidal%20Current%20Speed%20Forecasting%20Model%20based%20on%20Multi-Periodicity%0A%20%20Learning%0AAuthor%3A%20Tengfei%20Cheng%20and%20Yangdi%20Huang%20and%20Yunxuan%20Dong%0AAbstract%3A%20%20%20Tidal%20energy%20is%20one%20of%20the%20key%20components%20in%20increasing%20the%20penetration%20rate%0Aof%20renewable%20energy.%20The%20penetration%20of%20tidal%20energy%20in%20the%20electrical%20grid%0Adepends%20on%20the%20accuracy%20of%20tidal%20current%20speed%20forecasting.%20Modeling%0Ainaccuracies%20hinder%20forecast%20accuracy.%20Previous%20research%20has%20primarily%20used%0Aphysical%20models%20to%20forecast%20tidal%20current%20speed.%20However%2C%20tidal%20current%0Avariations%20influenced%20by%20the%20orbital%20periods%20of%20celestial%20bodies%20make%20accurate%0Aphysical%20modeling%20challenging.%20Researching%20the%20multi-periodicity%20of%20tides%20is%0Acrucial%20for%20accurately%20forecasting%20tidal%20current%20speed.%20In%20this%20article%2C%20we%0Apropose%20the%20Wavelet-Enhanced%20Convolutional%20Network%20%28WCN%29%20to%20learn%0Amulti-periodicity.%20The%20framework%20embeds%20intra-period%20and%20inter-period%0Avariations%20of%20one-dimensional%20tidal%20current%20data%20into%20the%20rows%20and%20columns%20of%20a%0Atwo-dimensional%20tensor.%20Then%2C%20the%20two-dimensional%20variations%20of%20the%20sequence%0Acan%20be%20processed%20by%20convolutional%20kernels.%20We%20integrate%20a%20time-frequency%0Aanalysis%20method%20into%20the%20framework%20to%20further%20address%20local%20periodic%20features.%0AAdditionally%2C%20to%20enhance%20the%20framework%27s%20stability%2C%20we%20optimize%20the%20framework%27s%0Ahyperparameters%20with%20the%20Tree-structured%20Parzen%20Estimator%20algorithm.%20The%0Aproposed%20framework%20avoids%20the%20lack%20of%20learning%20multi-periodicity.%20Compared%20with%0Abenchmarks%2C%20the%20proposed%20framework%20reduces%20the%20mean%20absolute%20error%20and%20mean%0Asquare%20error%20in%2010-step%20forecasting%20by%2C%20at%20most%2C%2090.36%25%20and%2097.56%25%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tidal%2520Current%2520Speed%2520Forecasting%2520Model%2520based%2520on%2520Multi-Periodicity%250A%2520%2520Learning%26entry.906535625%3DTengfei%2520Cheng%2520and%2520Yangdi%2520Huang%2520and%2520Yunxuan%2520Dong%26entry.1292438233%3D%2520%2520Tidal%2520energy%2520is%2520one%2520of%2520the%2520key%2520components%2520in%2520increasing%2520the%2520penetration%2520rate%250Aof%2520renewable%2520energy.%2520The%2520penetration%2520of%2520tidal%2520energy%2520in%2520the%2520electrical%2520grid%250Adepends%2520on%2520the%2520accuracy%2520of%2520tidal%2520current%2520speed%2520forecasting.%2520Modeling%250Ainaccuracies%2520hinder%2520forecast%2520accuracy.%2520Previous%2520research%2520has%2520primarily%2520used%250Aphysical%2520models%2520to%2520forecast%2520tidal%2520current%2520speed.%2520However%252C%2520tidal%2520current%250Avariations%2520influenced%2520by%2520the%2520orbital%2520periods%2520of%2520celestial%2520bodies%2520make%2520accurate%250Aphysical%2520modeling%2520challenging.%2520Researching%2520the%2520multi-periodicity%2520of%2520tides%2520is%250Acrucial%2520for%2520accurately%2520forecasting%2520tidal%2520current%2520speed.%2520In%2520this%2520article%252C%2520we%250Apropose%2520the%2520Wavelet-Enhanced%2520Convolutional%2520Network%2520%2528WCN%2529%2520to%2520learn%250Amulti-periodicity.%2520The%2520framework%2520embeds%2520intra-period%2520and%2520inter-period%250Avariations%2520of%2520one-dimensional%2520tidal%2520current%2520data%2520into%2520the%2520rows%2520and%2520columns%2520of%2520a%250Atwo-dimensional%2520tensor.%2520Then%252C%2520the%2520two-dimensional%2520variations%2520of%2520the%2520sequence%250Acan%2520be%2520processed%2520by%2520convolutional%2520kernels.%2520We%2520integrate%2520a%2520time-frequency%250Aanalysis%2520method%2520into%2520the%2520framework%2520to%2520further%2520address%2520local%2520periodic%2520features.%250AAdditionally%252C%2520to%2520enhance%2520the%2520framework%2527s%2520stability%252C%2520we%2520optimize%2520the%2520framework%2527s%250Ahyperparameters%2520with%2520the%2520Tree-structured%2520Parzen%2520Estimator%2520algorithm.%2520The%250Aproposed%2520framework%2520avoids%2520the%2520lack%2520of%2520learning%2520multi-periodicity.%2520Compared%2520with%250Abenchmarks%252C%2520the%2520proposed%2520framework%2520reduces%2520the%2520mean%2520absolute%2520error%2520and%2520mean%250Asquare%2520error%2520in%252010-step%2520forecasting%2520by%252C%2520at%2520most%252C%252090.36%2525%2520and%252097.56%2525%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tidal%20Current%20Speed%20Forecasting%20Model%20based%20on%20Multi-Periodicity%0A%20%20Learning&entry.906535625=Tengfei%20Cheng%20and%20Yangdi%20Huang%20and%20Yunxuan%20Dong&entry.1292438233=%20%20Tidal%20energy%20is%20one%20of%20the%20key%20components%20in%20increasing%20the%20penetration%20rate%0Aof%20renewable%20energy.%20The%20penetration%20of%20tidal%20energy%20in%20the%20electrical%20grid%0Adepends%20on%20the%20accuracy%20of%20tidal%20current%20speed%20forecasting.%20Modeling%0Ainaccuracies%20hinder%20forecast%20accuracy.%20Previous%20research%20has%20primarily%20used%0Aphysical%20models%20to%20forecast%20tidal%20current%20speed.%20However%2C%20tidal%20current%0Avariations%20influenced%20by%20the%20orbital%20periods%20of%20celestial%20bodies%20make%20accurate%0Aphysical%20modeling%20challenging.%20Researching%20the%20multi-periodicity%20of%20tides%20is%0Acrucial%20for%20accurately%20forecasting%20tidal%20current%20speed.%20In%20this%20article%2C%20we%0Apropose%20the%20Wavelet-Enhanced%20Convolutional%20Network%20%28WCN%29%20to%20learn%0Amulti-periodicity.%20The%20framework%20embeds%20intra-period%20and%20inter-period%0Avariations%20of%20one-dimensional%20tidal%20current%20data%20into%20the%20rows%20and%20columns%20of%20a%0Atwo-dimensional%20tensor.%20Then%2C%20the%20two-dimensional%20variations%20of%20the%20sequence%0Acan%20be%20processed%20by%20convolutional%20kernels.%20We%20integrate%20a%20time-frequency%0Aanalysis%20method%20into%20the%20framework%20to%20further%20address%20local%20periodic%20features.%0AAdditionally%2C%20to%20enhance%20the%20framework%27s%20stability%2C%20we%20optimize%20the%20framework%27s%0Ahyperparameters%20with%20the%20Tree-structured%20Parzen%20Estimator%20algorithm.%20The%0Aproposed%20framework%20avoids%20the%20lack%20of%20learning%20multi-periodicity.%20Compared%20with%0Abenchmarks%2C%20the%20proposed%20framework%20reduces%20the%20mean%20absolute%20error%20and%20mean%0Asquare%20error%20in%2010-step%20forecasting%20by%2C%20at%20most%2C%2090.36%25%20and%2097.56%25%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09718v2&entry.124074799=Read"},
{"title": "Adaptive Resource Allocation Optimization Using Large Language Models in\n  Dynamic Wireless Environments", "author": "Hyeonho Noh and Byonghyo Shim and Hyun Jong Yang", "abstract": "  Deep learning (DL) has made notable progress in addressing complex radio\naccess network control challenges that conventional analytic methods have\nstruggled to solve. However, DL has shown limitations in solving constrained\nNP-hard problems often encountered in network optimization, such as those\ninvolving quality of service (QoS) or discrete variables like user indices.\nCurrent solutions rely on domain-specific architectures or heuristic\ntechniques, and a general DL approach for constrained optimization remains\nundeveloped. Moreover, even minor changes in communication objectives demand\ntime-consuming retraining, limiting their adaptability to dynamic environments\nwhere task objectives, constraints, environmental factors, and communication\nscenarios frequently change. To address these challenges, we propose a large\nlanguage model for resource allocation optimizer (LLM-RAO), a novel approach\nthat harnesses the capabilities of LLMs to address the complex resource\nallocation problem while adhering to QoS constraints. By employing a\nprompt-based tuning strategy to flexibly convey ever-changing task descriptions\nand requirements to the LLM, LLM-RAO demonstrates robust performance and\nseamless adaptability in dynamic environments without requiring extensive\nretraining. Simulation results reveal that LLM-RAO achieves up to a 40%\nperformance enhancement compared to conventional DL methods and up to an $80$\\%\nimprovement over analytical approaches. Moreover, in scenarios with fluctuating\ncommunication objectives, LLM-RAO attains up to 2.9 times the performance of\ntraditional DL-based networks.\n", "link": "http://arxiv.org/abs/2502.02287v1", "date": "2025-02-04", "relevancy": 1.8655, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4681}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Resource%20Allocation%20Optimization%20Using%20Large%20Language%20Models%20in%0A%20%20Dynamic%20Wireless%20Environments&body=Title%3A%20Adaptive%20Resource%20Allocation%20Optimization%20Using%20Large%20Language%20Models%20in%0A%20%20Dynamic%20Wireless%20Environments%0AAuthor%3A%20Hyeonho%20Noh%20and%20Byonghyo%20Shim%20and%20Hyun%20Jong%20Yang%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20made%20notable%20progress%20in%20addressing%20complex%20radio%0Aaccess%20network%20control%20challenges%20that%20conventional%20analytic%20methods%20have%0Astruggled%20to%20solve.%20However%2C%20DL%20has%20shown%20limitations%20in%20solving%20constrained%0ANP-hard%20problems%20often%20encountered%20in%20network%20optimization%2C%20such%20as%20those%0Ainvolving%20quality%20of%20service%20%28QoS%29%20or%20discrete%20variables%20like%20user%20indices.%0ACurrent%20solutions%20rely%20on%20domain-specific%20architectures%20or%20heuristic%0Atechniques%2C%20and%20a%20general%20DL%20approach%20for%20constrained%20optimization%20remains%0Aundeveloped.%20Moreover%2C%20even%20minor%20changes%20in%20communication%20objectives%20demand%0Atime-consuming%20retraining%2C%20limiting%20their%20adaptability%20to%20dynamic%20environments%0Awhere%20task%20objectives%2C%20constraints%2C%20environmental%20factors%2C%20and%20communication%0Ascenarios%20frequently%20change.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20large%0Alanguage%20model%20for%20resource%20allocation%20optimizer%20%28LLM-RAO%29%2C%20a%20novel%20approach%0Athat%20harnesses%20the%20capabilities%20of%20LLMs%20to%20address%20the%20complex%20resource%0Aallocation%20problem%20while%20adhering%20to%20QoS%20constraints.%20By%20employing%20a%0Aprompt-based%20tuning%20strategy%20to%20flexibly%20convey%20ever-changing%20task%20descriptions%0Aand%20requirements%20to%20the%20LLM%2C%20LLM-RAO%20demonstrates%20robust%20performance%20and%0Aseamless%20adaptability%20in%20dynamic%20environments%20without%20requiring%20extensive%0Aretraining.%20Simulation%20results%20reveal%20that%20LLM-RAO%20achieves%20up%20to%20a%2040%25%0Aperformance%20enhancement%20compared%20to%20conventional%20DL%20methods%20and%20up%20to%20an%20%2480%24%5C%25%0Aimprovement%20over%20analytical%20approaches.%20Moreover%2C%20in%20scenarios%20with%20fluctuating%0Acommunication%20objectives%2C%20LLM-RAO%20attains%20up%20to%202.9%20times%20the%20performance%20of%0Atraditional%20DL-based%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Resource%2520Allocation%2520Optimization%2520Using%2520Large%2520Language%2520Models%2520in%250A%2520%2520Dynamic%2520Wireless%2520Environments%26entry.906535625%3DHyeonho%2520Noh%2520and%2520Byonghyo%2520Shim%2520and%2520Hyun%2520Jong%2520Yang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520made%2520notable%2520progress%2520in%2520addressing%2520complex%2520radio%250Aaccess%2520network%2520control%2520challenges%2520that%2520conventional%2520analytic%2520methods%2520have%250Astruggled%2520to%2520solve.%2520However%252C%2520DL%2520has%2520shown%2520limitations%2520in%2520solving%2520constrained%250ANP-hard%2520problems%2520often%2520encountered%2520in%2520network%2520optimization%252C%2520such%2520as%2520those%250Ainvolving%2520quality%2520of%2520service%2520%2528QoS%2529%2520or%2520discrete%2520variables%2520like%2520user%2520indices.%250ACurrent%2520solutions%2520rely%2520on%2520domain-specific%2520architectures%2520or%2520heuristic%250Atechniques%252C%2520and%2520a%2520general%2520DL%2520approach%2520for%2520constrained%2520optimization%2520remains%250Aundeveloped.%2520Moreover%252C%2520even%2520minor%2520changes%2520in%2520communication%2520objectives%2520demand%250Atime-consuming%2520retraining%252C%2520limiting%2520their%2520adaptability%2520to%2520dynamic%2520environments%250Awhere%2520task%2520objectives%252C%2520constraints%252C%2520environmental%2520factors%252C%2520and%2520communication%250Ascenarios%2520frequently%2520change.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520large%250Alanguage%2520model%2520for%2520resource%2520allocation%2520optimizer%2520%2528LLM-RAO%2529%252C%2520a%2520novel%2520approach%250Athat%2520harnesses%2520the%2520capabilities%2520of%2520LLMs%2520to%2520address%2520the%2520complex%2520resource%250Aallocation%2520problem%2520while%2520adhering%2520to%2520QoS%2520constraints.%2520By%2520employing%2520a%250Aprompt-based%2520tuning%2520strategy%2520to%2520flexibly%2520convey%2520ever-changing%2520task%2520descriptions%250Aand%2520requirements%2520to%2520the%2520LLM%252C%2520LLM-RAO%2520demonstrates%2520robust%2520performance%2520and%250Aseamless%2520adaptability%2520in%2520dynamic%2520environments%2520without%2520requiring%2520extensive%250Aretraining.%2520Simulation%2520results%2520reveal%2520that%2520LLM-RAO%2520achieves%2520up%2520to%2520a%252040%2525%250Aperformance%2520enhancement%2520compared%2520to%2520conventional%2520DL%2520methods%2520and%2520up%2520to%2520an%2520%252480%2524%255C%2525%250Aimprovement%2520over%2520analytical%2520approaches.%2520Moreover%252C%2520in%2520scenarios%2520with%2520fluctuating%250Acommunication%2520objectives%252C%2520LLM-RAO%2520attains%2520up%2520to%25202.9%2520times%2520the%2520performance%2520of%250Atraditional%2520DL-based%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Resource%20Allocation%20Optimization%20Using%20Large%20Language%20Models%20in%0A%20%20Dynamic%20Wireless%20Environments&entry.906535625=Hyeonho%20Noh%20and%20Byonghyo%20Shim%20and%20Hyun%20Jong%20Yang&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20made%20notable%20progress%20in%20addressing%20complex%20radio%0Aaccess%20network%20control%20challenges%20that%20conventional%20analytic%20methods%20have%0Astruggled%20to%20solve.%20However%2C%20DL%20has%20shown%20limitations%20in%20solving%20constrained%0ANP-hard%20problems%20often%20encountered%20in%20network%20optimization%2C%20such%20as%20those%0Ainvolving%20quality%20of%20service%20%28QoS%29%20or%20discrete%20variables%20like%20user%20indices.%0ACurrent%20solutions%20rely%20on%20domain-specific%20architectures%20or%20heuristic%0Atechniques%2C%20and%20a%20general%20DL%20approach%20for%20constrained%20optimization%20remains%0Aundeveloped.%20Moreover%2C%20even%20minor%20changes%20in%20communication%20objectives%20demand%0Atime-consuming%20retraining%2C%20limiting%20their%20adaptability%20to%20dynamic%20environments%0Awhere%20task%20objectives%2C%20constraints%2C%20environmental%20factors%2C%20and%20communication%0Ascenarios%20frequently%20change.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20large%0Alanguage%20model%20for%20resource%20allocation%20optimizer%20%28LLM-RAO%29%2C%20a%20novel%20approach%0Athat%20harnesses%20the%20capabilities%20of%20LLMs%20to%20address%20the%20complex%20resource%0Aallocation%20problem%20while%20adhering%20to%20QoS%20constraints.%20By%20employing%20a%0Aprompt-based%20tuning%20strategy%20to%20flexibly%20convey%20ever-changing%20task%20descriptions%0Aand%20requirements%20to%20the%20LLM%2C%20LLM-RAO%20demonstrates%20robust%20performance%20and%0Aseamless%20adaptability%20in%20dynamic%20environments%20without%20requiring%20extensive%0Aretraining.%20Simulation%20results%20reveal%20that%20LLM-RAO%20achieves%20up%20to%20a%2040%25%0Aperformance%20enhancement%20compared%20to%20conventional%20DL%20methods%20and%20up%20to%20an%20%2480%24%5C%25%0Aimprovement%20over%20analytical%20approaches.%20Moreover%2C%20in%20scenarios%20with%20fluctuating%0Acommunication%20objectives%2C%20LLM-RAO%20attains%20up%20to%202.9%20times%20the%20performance%20of%0Atraditional%20DL-based%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02287v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


