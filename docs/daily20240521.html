<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240520.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GGAvatar: Geometric Adjustment of Gaussian Head Avatar", "author": "Xinyang Li and Jiaxin Wang and Yixin Xuan and Gongxin Yao and Yu Pan", "abstract": "  We propose GGAvatar, a novel 3D avatar representation designed to robustly\nmodel dynamic head avatars with complex identities and deformations. GGAvatar\nemploys a coarse-to-fine structure, featuring two core modules: Neutral\nGaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian\nInitialization Module pairs Gaussian primitives with deformable triangular\nmeshes, employing an adaptive density control strategy to model the geometric\nstructure of the target subject with neutral expressions. Geometry Morph\nAdjuster introduces deformation bases for each Gaussian in global space,\ncreating fine-grained low-dimensional representations of deformation behaviors\nto address the Linear Blend Skinning formula's limitations effectively.\nExtensive experiments show that GGAvatar can produce high-fidelity renderings,\noutperforming state-of-the-art methods in visual quality and quantitative\nmetrics.\n", "link": "http://arxiv.org/abs/2405.11993v1", "date": "2024-05-20", "relevancy": 3.6808, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7951}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7951}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GGAvatar%3A%20Geometric%20Adjustment%20of%20Gaussian%20Head%20Avatar&body=Title%3A%20GGAvatar%3A%20Geometric%20Adjustment%20of%20Gaussian%20Head%20Avatar%0AAuthor%3A%20Xinyang%20Li%20and%20Jiaxin%20Wang%20and%20Yixin%20Xuan%20and%20Gongxin%20Yao%20and%20Yu%20Pan%0AAbstract%3A%20%20%20We%20propose%20GGAvatar%2C%20a%20novel%203D%20avatar%20representation%20designed%20to%20robustly%0Amodel%20dynamic%20head%20avatars%20with%20complex%20identities%20and%20deformations.%20GGAvatar%0Aemploys%20a%20coarse-to-fine%20structure%2C%20featuring%20two%20core%20modules%3A%20Neutral%0AGaussian%20Initialization%20Module%20and%20Geometry%20Morph%20Adjuster.%20Neutral%20Gaussian%0AInitialization%20Module%20pairs%20Gaussian%20primitives%20with%20deformable%20triangular%0Ameshes%2C%20employing%20an%20adaptive%20density%20control%20strategy%20to%20model%20the%20geometric%0Astructure%20of%20the%20target%20subject%20with%20neutral%20expressions.%20Geometry%20Morph%0AAdjuster%20introduces%20deformation%20bases%20for%20each%20Gaussian%20in%20global%20space%2C%0Acreating%20fine-grained%20low-dimensional%20representations%20of%20deformation%20behaviors%0Ato%20address%20the%20Linear%20Blend%20Skinning%20formula%27s%20limitations%20effectively.%0AExtensive%20experiments%20show%20that%20GGAvatar%20can%20produce%20high-fidelity%20renderings%2C%0Aoutperforming%20state-of-the-art%20methods%20in%20visual%20quality%20and%20quantitative%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGGAvatar%253A%2520Geometric%2520Adjustment%2520of%2520Gaussian%2520Head%2520Avatar%26entry.906535625%3DXinyang%2520Li%2520and%2520Jiaxin%2520Wang%2520and%2520Yixin%2520Xuan%2520and%2520Gongxin%2520Yao%2520and%2520Yu%2520Pan%26entry.1292438233%3D%2520%2520We%2520propose%2520GGAvatar%252C%2520a%2520novel%25203D%2520avatar%2520representation%2520designed%2520to%2520robustly%250Amodel%2520dynamic%2520head%2520avatars%2520with%2520complex%2520identities%2520and%2520deformations.%2520GGAvatar%250Aemploys%2520a%2520coarse-to-fine%2520structure%252C%2520featuring%2520two%2520core%2520modules%253A%2520Neutral%250AGaussian%2520Initialization%2520Module%2520and%2520Geometry%2520Morph%2520Adjuster.%2520Neutral%2520Gaussian%250AInitialization%2520Module%2520pairs%2520Gaussian%2520primitives%2520with%2520deformable%2520triangular%250Ameshes%252C%2520employing%2520an%2520adaptive%2520density%2520control%2520strategy%2520to%2520model%2520the%2520geometric%250Astructure%2520of%2520the%2520target%2520subject%2520with%2520neutral%2520expressions.%2520Geometry%2520Morph%250AAdjuster%2520introduces%2520deformation%2520bases%2520for%2520each%2520Gaussian%2520in%2520global%2520space%252C%250Acreating%2520fine-grained%2520low-dimensional%2520representations%2520of%2520deformation%2520behaviors%250Ato%2520address%2520the%2520Linear%2520Blend%2520Skinning%2520formula%2527s%2520limitations%2520effectively.%250AExtensive%2520experiments%2520show%2520that%2520GGAvatar%2520can%2520produce%2520high-fidelity%2520renderings%252C%250Aoutperforming%2520state-of-the-art%2520methods%2520in%2520visual%2520quality%2520and%2520quantitative%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GGAvatar%3A%20Geometric%20Adjustment%20of%20Gaussian%20Head%20Avatar&entry.906535625=Xinyang%20Li%20and%20Jiaxin%20Wang%20and%20Yixin%20Xuan%20and%20Gongxin%20Yao%20and%20Yu%20Pan&entry.1292438233=%20%20We%20propose%20GGAvatar%2C%20a%20novel%203D%20avatar%20representation%20designed%20to%20robustly%0Amodel%20dynamic%20head%20avatars%20with%20complex%20identities%20and%20deformations.%20GGAvatar%0Aemploys%20a%20coarse-to-fine%20structure%2C%20featuring%20two%20core%20modules%3A%20Neutral%0AGaussian%20Initialization%20Module%20and%20Geometry%20Morph%20Adjuster.%20Neutral%20Gaussian%0AInitialization%20Module%20pairs%20Gaussian%20primitives%20with%20deformable%20triangular%0Ameshes%2C%20employing%20an%20adaptive%20density%20control%20strategy%20to%20model%20the%20geometric%0Astructure%20of%20the%20target%20subject%20with%20neutral%20expressions.%20Geometry%20Morph%0AAdjuster%20introduces%20deformation%20bases%20for%20each%20Gaussian%20in%20global%20space%2C%0Acreating%20fine-grained%20low-dimensional%20representations%20of%20deformation%20behaviors%0Ato%20address%20the%20Linear%20Blend%20Skinning%20formula%27s%20limitations%20effectively.%0AExtensive%20experiments%20show%20that%20GGAvatar%20can%20produce%20high-fidelity%20renderings%2C%0Aoutperforming%20state-of-the-art%20methods%20in%20visual%20quality%20and%20quantitative%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11993v1&entry.124074799=Read"},
{"title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with\n  Anchor Gaussian Guided Texture Warping", "author": "Tianhao Wu and Jing Yang and Zhilin Guo and Jingyi Wan and Fangcheng Zhong and Cengiz Oztireli", "abstract": "  By equipping the most recent 3D Gaussian Splatting representation with head\n3D morphable models (3DMM), existing methods manage to create head avatars with\nhigh fidelity. However, most existing methods only reconstruct a head without\nthe body, substantially limiting their application scenarios. We found that\nnaively applying Gaussians to model the clothed chest and shoulders tends to\nresult in blurry reconstruction and noisy floaters under novel poses. This is\nbecause of the fundamental limitation of Gaussians and point clouds -- each\nGaussian or point can only have a single directional radiance without spatial\nvariance, therefore an unnecessarily large number of them is required to\nrepresent complicated spatially varying texture, even for simple geometry. In\ncontrast, we propose to model the body part with a neural texture that consists\nof coarse and pose-dependent fine colors. To properly render the body texture\nfor each view and pose without accurate geometry nor UV mapping, we optimize\nanother sparse set of Gaussians as anchors that constrain the neural warping\nfield that maps image plane coordinates to the texture space. We demonstrate\nthat Gaussian Head & Shoulders can fit the high-frequency details on the\nclothed upper body with high fidelity and potentially improve the accuracy and\nfidelity of the head region. We evaluate our method with casual phone-captured\nand internet videos and show our method archives superior reconstruction\nquality and robustness in both self and cross reenactment tasks. To fully\nutilize the efficient rendering speed of Gaussian splatting, we additionally\npropose an accelerated inference method of our trained model without\nMulti-Layer Perceptron (MLP) queries and reach a stable rendering speed of\naround 130 FPS for any subjects.\n", "link": "http://arxiv.org/abs/2405.12069v1", "date": "2024-05-20", "relevancy": 3.6791, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7599}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7599}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping&body=Title%3A%20Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping%0AAuthor%3A%20Tianhao%20Wu%20and%20Jing%20Yang%20and%20Zhilin%20Guo%20and%20Jingyi%20Wan%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20By%20equipping%20the%20most%20recent%203D%20Gaussian%20Splatting%20representation%20with%20head%0A3D%20morphable%20models%20%283DMM%29%2C%20existing%20methods%20manage%20to%20create%20head%20avatars%20with%0Ahigh%20fidelity.%20However%2C%20most%20existing%20methods%20only%20reconstruct%20a%20head%20without%0Athe%20body%2C%20substantially%20limiting%20their%20application%20scenarios.%20We%20found%20that%0Anaively%20applying%20Gaussians%20to%20model%20the%20clothed%20chest%20and%20shoulders%20tends%20to%0Aresult%20in%20blurry%20reconstruction%20and%20noisy%20floaters%20under%20novel%20poses.%20This%20is%0Abecause%20of%20the%20fundamental%20limitation%20of%20Gaussians%20and%20point%20clouds%20--%20each%0AGaussian%20or%20point%20can%20only%20have%20a%20single%20directional%20radiance%20without%20spatial%0Avariance%2C%20therefore%20an%20unnecessarily%20large%20number%20of%20them%20is%20required%20to%0Arepresent%20complicated%20spatially%20varying%20texture%2C%20even%20for%20simple%20geometry.%20In%0Acontrast%2C%20we%20propose%20to%20model%20the%20body%20part%20with%20a%20neural%20texture%20that%20consists%0Aof%20coarse%20and%20pose-dependent%20fine%20colors.%20To%20properly%20render%20the%20body%20texture%0Afor%20each%20view%20and%20pose%20without%20accurate%20geometry%20nor%20UV%20mapping%2C%20we%20optimize%0Aanother%20sparse%20set%20of%20Gaussians%20as%20anchors%20that%20constrain%20the%20neural%20warping%0Afield%20that%20maps%20image%20plane%20coordinates%20to%20the%20texture%20space.%20We%20demonstrate%0Athat%20Gaussian%20Head%20%26%20Shoulders%20can%20fit%20the%20high-frequency%20details%20on%20the%0Aclothed%20upper%20body%20with%20high%20fidelity%20and%20potentially%20improve%20the%20accuracy%20and%0Afidelity%20of%20the%20head%20region.%20We%20evaluate%20our%20method%20with%20casual%20phone-captured%0Aand%20internet%20videos%20and%20show%20our%20method%20archives%20superior%20reconstruction%0Aquality%20and%20robustness%20in%20both%20self%20and%20cross%20reenactment%20tasks.%20To%20fully%0Autilize%20the%20efficient%20rendering%20speed%20of%20Gaussian%20splatting%2C%20we%20additionally%0Apropose%20an%20accelerated%20inference%20method%20of%20our%20trained%20model%20without%0AMulti-Layer%20Perceptron%20%28MLP%29%20queries%20and%20reach%20a%20stable%20rendering%20speed%20of%0Aaround%20130%20FPS%20for%20any%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Head%2520%2526%2520Shoulders%253A%2520High%2520Fidelity%2520Neural%2520Upper%2520Body%2520Avatars%2520with%250A%2520%2520Anchor%2520Gaussian%2520Guided%2520Texture%2520Warping%26entry.906535625%3DTianhao%2520Wu%2520and%2520Jing%2520Yang%2520and%2520Zhilin%2520Guo%2520and%2520Jingyi%2520Wan%2520and%2520Fangcheng%2520Zhong%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520By%2520equipping%2520the%2520most%2520recent%25203D%2520Gaussian%2520Splatting%2520representation%2520with%2520head%250A3D%2520morphable%2520models%2520%25283DMM%2529%252C%2520existing%2520methods%2520manage%2520to%2520create%2520head%2520avatars%2520with%250Ahigh%2520fidelity.%2520However%252C%2520most%2520existing%2520methods%2520only%2520reconstruct%2520a%2520head%2520without%250Athe%2520body%252C%2520substantially%2520limiting%2520their%2520application%2520scenarios.%2520We%2520found%2520that%250Anaively%2520applying%2520Gaussians%2520to%2520model%2520the%2520clothed%2520chest%2520and%2520shoulders%2520tends%2520to%250Aresult%2520in%2520blurry%2520reconstruction%2520and%2520noisy%2520floaters%2520under%2520novel%2520poses.%2520This%2520is%250Abecause%2520of%2520the%2520fundamental%2520limitation%2520of%2520Gaussians%2520and%2520point%2520clouds%2520--%2520each%250AGaussian%2520or%2520point%2520can%2520only%2520have%2520a%2520single%2520directional%2520radiance%2520without%2520spatial%250Avariance%252C%2520therefore%2520an%2520unnecessarily%2520large%2520number%2520of%2520them%2520is%2520required%2520to%250Arepresent%2520complicated%2520spatially%2520varying%2520texture%252C%2520even%2520for%2520simple%2520geometry.%2520In%250Acontrast%252C%2520we%2520propose%2520to%2520model%2520the%2520body%2520part%2520with%2520a%2520neural%2520texture%2520that%2520consists%250Aof%2520coarse%2520and%2520pose-dependent%2520fine%2520colors.%2520To%2520properly%2520render%2520the%2520body%2520texture%250Afor%2520each%2520view%2520and%2520pose%2520without%2520accurate%2520geometry%2520nor%2520UV%2520mapping%252C%2520we%2520optimize%250Aanother%2520sparse%2520set%2520of%2520Gaussians%2520as%2520anchors%2520that%2520constrain%2520the%2520neural%2520warping%250Afield%2520that%2520maps%2520image%2520plane%2520coordinates%2520to%2520the%2520texture%2520space.%2520We%2520demonstrate%250Athat%2520Gaussian%2520Head%2520%2526%2520Shoulders%2520can%2520fit%2520the%2520high-frequency%2520details%2520on%2520the%250Aclothed%2520upper%2520body%2520with%2520high%2520fidelity%2520and%2520potentially%2520improve%2520the%2520accuracy%2520and%250Afidelity%2520of%2520the%2520head%2520region.%2520We%2520evaluate%2520our%2520method%2520with%2520casual%2520phone-captured%250Aand%2520internet%2520videos%2520and%2520show%2520our%2520method%2520archives%2520superior%2520reconstruction%250Aquality%2520and%2520robustness%2520in%2520both%2520self%2520and%2520cross%2520reenactment%2520tasks.%2520To%2520fully%250Autilize%2520the%2520efficient%2520rendering%2520speed%2520of%2520Gaussian%2520splatting%252C%2520we%2520additionally%250Apropose%2520an%2520accelerated%2520inference%2520method%2520of%2520our%2520trained%2520model%2520without%250AMulti-Layer%2520Perceptron%2520%2528MLP%2529%2520queries%2520and%2520reach%2520a%2520stable%2520rendering%2520speed%2520of%250Aaround%2520130%2520FPS%2520for%2520any%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping&entry.906535625=Tianhao%20Wu%20and%20Jing%20Yang%20and%20Zhilin%20Guo%20and%20Jingyi%20Wan%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20By%20equipping%20the%20most%20recent%203D%20Gaussian%20Splatting%20representation%20with%20head%0A3D%20morphable%20models%20%283DMM%29%2C%20existing%20methods%20manage%20to%20create%20head%20avatars%20with%0Ahigh%20fidelity.%20However%2C%20most%20existing%20methods%20only%20reconstruct%20a%20head%20without%0Athe%20body%2C%20substantially%20limiting%20their%20application%20scenarios.%20We%20found%20that%0Anaively%20applying%20Gaussians%20to%20model%20the%20clothed%20chest%20and%20shoulders%20tends%20to%0Aresult%20in%20blurry%20reconstruction%20and%20noisy%20floaters%20under%20novel%20poses.%20This%20is%0Abecause%20of%20the%20fundamental%20limitation%20of%20Gaussians%20and%20point%20clouds%20--%20each%0AGaussian%20or%20point%20can%20only%20have%20a%20single%20directional%20radiance%20without%20spatial%0Avariance%2C%20therefore%20an%20unnecessarily%20large%20number%20of%20them%20is%20required%20to%0Arepresent%20complicated%20spatially%20varying%20texture%2C%20even%20for%20simple%20geometry.%20In%0Acontrast%2C%20we%20propose%20to%20model%20the%20body%20part%20with%20a%20neural%20texture%20that%20consists%0Aof%20coarse%20and%20pose-dependent%20fine%20colors.%20To%20properly%20render%20the%20body%20texture%0Afor%20each%20view%20and%20pose%20without%20accurate%20geometry%20nor%20UV%20mapping%2C%20we%20optimize%0Aanother%20sparse%20set%20of%20Gaussians%20as%20anchors%20that%20constrain%20the%20neural%20warping%0Afield%20that%20maps%20image%20plane%20coordinates%20to%20the%20texture%20space.%20We%20demonstrate%0Athat%20Gaussian%20Head%20%26%20Shoulders%20can%20fit%20the%20high-frequency%20details%20on%20the%0Aclothed%20upper%20body%20with%20high%20fidelity%20and%20potentially%20improve%20the%20accuracy%20and%0Afidelity%20of%20the%20head%20region.%20We%20evaluate%20our%20method%20with%20casual%20phone-captured%0Aand%20internet%20videos%20and%20show%20our%20method%20archives%20superior%20reconstruction%0Aquality%20and%20robustness%20in%20both%20self%20and%20cross%20reenactment%20tasks.%20To%20fully%0Autilize%20the%20efficient%20rendering%20speed%20of%20Gaussian%20splatting%2C%20we%20additionally%0Apropose%20an%20accelerated%20inference%20method%20of%20our%20trained%20model%20without%0AMulti-Layer%20Perceptron%20%28MLP%29%20queries%20and%20reach%20a%20stable%20rendering%20speed%20of%0Aaround%20130%20FPS%20for%20any%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12069v1&entry.124074799=Read"},
{"title": "Fast Generalizable Gaussian Splatting Reconstruction from Multi-View\n  Stereo", "author": "Tianqi Liu and Guangcong Wang and Shoukang Hu and Liao Shen and Xinyi Ye and Yuhang Zang and Zhiguo Cao and Wei Li and Ziwei Liu", "abstract": "  We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.\n", "link": "http://arxiv.org/abs/2405.12218v1", "date": "2024-05-20", "relevancy": 3.3765, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7389}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6898}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%20Multi-View%0A%20%20Stereo&body=Title%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%20Multi-View%0A%20%20Stereo%0AAuthor%3A%20Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Generalizable%2520Gaussian%2520Splatting%2520Reconstruction%2520from%2520Multi-View%250A%2520%2520Stereo%26entry.906535625%3DTianqi%2520Liu%2520and%2520Guangcong%2520Wang%2520and%2520Shoukang%2520Hu%2520and%2520Liao%2520Shen%2520and%2520Xinyi%2520Ye%2520and%2520Yuhang%2520Zang%2520and%2520Zhiguo%2520Cao%2520and%2520Wei%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520MVSGaussian%252C%2520a%2520new%2520generalizable%25203D%2520Gaussian%2520representation%250Aapproach%2520derived%2520from%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520that%2520can%2520efficiently%2520reconstruct%250Aunseen%2520scenes.%2520Specifically%252C%25201%2529%2520we%2520leverage%2520MVS%2520to%2520encode%2520geometry-aware%250AGaussian%2520representations%2520and%2520decode%2520them%2520into%2520Gaussian%2520parameters.%25202%2529%2520To%250Afurther%2520enhance%2520performance%252C%2520we%2520propose%2520a%2520hybrid%2520Gaussian%2520rendering%2520that%250Aintegrates%2520an%2520efficient%2520volume%2520rendering%2520design%2520for%2520novel%2520view%2520synthesis.%25203%2529%2520To%250Asupport%2520fast%2520fine-tuning%2520for%2520specific%2520scenes%252C%2520we%2520introduce%2520a%2520multi-view%250Ageometric%2520consistent%2520aggregation%2520strategy%2520to%2520effectively%2520aggregate%2520the%2520point%250Aclouds%2520generated%2520by%2520the%2520generalizable%2520model%252C%2520serving%2520as%2520the%2520initialization%2520for%250Aper-scene%2520optimization.%2520Compared%2520with%2520previous%2520generalizable%2520NeRF-based%250Amethods%252C%2520which%2520typically%2520require%2520minutes%2520of%2520fine-tuning%2520and%2520seconds%2520of%250Arendering%2520per%2520image%252C%2520MVSGaussian%2520achieves%2520real-time%2520rendering%2520with%2520better%250Asynthesis%2520quality%2520for%2520each%2520scene.%2520Compared%2520with%2520the%2520vanilla%25203D-GS%252C%2520MVSGaussian%250Aachieves%2520better%2520view%2520synthesis%2520with%2520less%2520training%2520computational%2520cost.%2520Extensive%250Aexperiments%2520on%2520DTU%252C%2520Real%2520Forward-facing%252C%2520NeRF%2520Synthetic%252C%2520and%2520Tanks%2520and%2520Temples%250Adatasets%2520validate%2520that%2520MVSGaussian%2520attains%2520state-of-the-art%2520performance%2520with%250Aconvincing%2520generalizability%252C%2520real-time%2520rendering%2520speed%252C%2520and%2520fast%2520per-scene%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%20Multi-View%0A%20%20Stereo&entry.906535625=Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12218v1&entry.124074799=Read"},
{"title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror\n  Reflections", "author": "Jiayue Liu and Xiao Tang and Freeman Cheng and Roy Yang and Zhihao Li and Jianzhuang Liu and Yi Huang and Jiaqi Lin and Shiyong Liu and Xiaofei Wu and Songcen Xu and Chun Yuan", "abstract": "  3D Gaussian Splatting showcases notable advancements in photo-realistic and\nreal-time novel view synthesis. However, it faces challenges in modeling mirror\nreflections, which exhibit substantial appearance variations from different\nviewpoints. To tackle this problem, we present MirrorGaussian, the first method\nfor mirror scene reconstruction with real-time rendering based on 3D Gaussian\nSplatting. The key insight is grounded on the mirror symmetry between the\nreal-world space and the virtual mirror space. We introduce an intuitive\ndual-rendering strategy that enables differentiable rasterization of both the\nreal-world 3D Gaussians and the mirrored counterpart obtained by reflecting the\nformer about the mirror plane. All 3D Gaussians are jointly optimized with the\nmirror plane in an end-to-end framework. MirrorGaussian achieves high-quality\nand real-time rendering in scenes with mirrors, empowering scene editing like\nadding new mirrors and objects. Comprehensive experiments on multiple datasets\ndemonstrate that our approach significantly outperforms existing methods,\nachieving state-of-the-art results. Project page:\nhttps://mirror-gaussian.github.io/.\n", "link": "http://arxiv.org/abs/2405.11921v1", "date": "2024-05-20", "relevancy": 3.205, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6727}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6689}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MirrorGaussian%3A%20Reflecting%203D%20Gaussians%20for%20Reconstructing%20Mirror%0A%20%20Reflections&body=Title%3A%20MirrorGaussian%3A%20Reflecting%203D%20Gaussians%20for%20Reconstructing%20Mirror%0A%20%20Reflections%0AAuthor%3A%20Jiayue%20Liu%20and%20Xiao%20Tang%20and%20Freeman%20Cheng%20and%20Roy%20Yang%20and%20Zhihao%20Li%20and%20Jianzhuang%20Liu%20and%20Yi%20Huang%20and%20Jiaqi%20Lin%20and%20Shiyong%20Liu%20and%20Xiaofei%20Wu%20and%20Songcen%20Xu%20and%20Chun%20Yuan%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20showcases%20notable%20advancements%20in%20photo-realistic%20and%0Areal-time%20novel%20view%20synthesis.%20However%2C%20it%20faces%20challenges%20in%20modeling%20mirror%0Areflections%2C%20which%20exhibit%20substantial%20appearance%20variations%20from%20different%0Aviewpoints.%20To%20tackle%20this%20problem%2C%20we%20present%20MirrorGaussian%2C%20the%20first%20method%0Afor%20mirror%20scene%20reconstruction%20with%20real-time%20rendering%20based%20on%203D%20Gaussian%0ASplatting.%20The%20key%20insight%20is%20grounded%20on%20the%20mirror%20symmetry%20between%20the%0Areal-world%20space%20and%20the%20virtual%20mirror%20space.%20We%20introduce%20an%20intuitive%0Adual-rendering%20strategy%20that%20enables%20differentiable%20rasterization%20of%20both%20the%0Areal-world%203D%20Gaussians%20and%20the%20mirrored%20counterpart%20obtained%20by%20reflecting%20the%0Aformer%20about%20the%20mirror%20plane.%20All%203D%20Gaussians%20are%20jointly%20optimized%20with%20the%0Amirror%20plane%20in%20an%20end-to-end%20framework.%20MirrorGaussian%20achieves%20high-quality%0Aand%20real-time%20rendering%20in%20scenes%20with%20mirrors%2C%20empowering%20scene%20editing%20like%0Aadding%20new%20mirrors%20and%20objects.%20Comprehensive%20experiments%20on%20multiple%20datasets%0Ademonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%2C%0Aachieving%20state-of-the-art%20results.%20Project%20page%3A%0Ahttps%3A//mirror-gaussian.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirrorGaussian%253A%2520Reflecting%25203D%2520Gaussians%2520for%2520Reconstructing%2520Mirror%250A%2520%2520Reflections%26entry.906535625%3DJiayue%2520Liu%2520and%2520Xiao%2520Tang%2520and%2520Freeman%2520Cheng%2520and%2520Roy%2520Yang%2520and%2520Zhihao%2520Li%2520and%2520Jianzhuang%2520Liu%2520and%2520Yi%2520Huang%2520and%2520Jiaqi%2520Lin%2520and%2520Shiyong%2520Liu%2520and%2520Xiaofei%2520Wu%2520and%2520Songcen%2520Xu%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520showcases%2520notable%2520advancements%2520in%2520photo-realistic%2520and%250Areal-time%2520novel%2520view%2520synthesis.%2520However%252C%2520it%2520faces%2520challenges%2520in%2520modeling%2520mirror%250Areflections%252C%2520which%2520exhibit%2520substantial%2520appearance%2520variations%2520from%2520different%250Aviewpoints.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520present%2520MirrorGaussian%252C%2520the%2520first%2520method%250Afor%2520mirror%2520scene%2520reconstruction%2520with%2520real-time%2520rendering%2520based%2520on%25203D%2520Gaussian%250ASplatting.%2520The%2520key%2520insight%2520is%2520grounded%2520on%2520the%2520mirror%2520symmetry%2520between%2520the%250Areal-world%2520space%2520and%2520the%2520virtual%2520mirror%2520space.%2520We%2520introduce%2520an%2520intuitive%250Adual-rendering%2520strategy%2520that%2520enables%2520differentiable%2520rasterization%2520of%2520both%2520the%250Areal-world%25203D%2520Gaussians%2520and%2520the%2520mirrored%2520counterpart%2520obtained%2520by%2520reflecting%2520the%250Aformer%2520about%2520the%2520mirror%2520plane.%2520All%25203D%2520Gaussians%2520are%2520jointly%2520optimized%2520with%2520the%250Amirror%2520plane%2520in%2520an%2520end-to-end%2520framework.%2520MirrorGaussian%2520achieves%2520high-quality%250Aand%2520real-time%2520rendering%2520in%2520scenes%2520with%2520mirrors%252C%2520empowering%2520scene%2520editing%2520like%250Aadding%2520new%2520mirrors%2520and%2520objects.%2520Comprehensive%2520experiments%2520on%2520multiple%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520methods%252C%250Aachieving%2520state-of-the-art%2520results.%2520Project%2520page%253A%250Ahttps%253A//mirror-gaussian.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MirrorGaussian%3A%20Reflecting%203D%20Gaussians%20for%20Reconstructing%20Mirror%0A%20%20Reflections&entry.906535625=Jiayue%20Liu%20and%20Xiao%20Tang%20and%20Freeman%20Cheng%20and%20Roy%20Yang%20and%20Zhihao%20Li%20and%20Jianzhuang%20Liu%20and%20Yi%20Huang%20and%20Jiaqi%20Lin%20and%20Shiyong%20Liu%20and%20Xiaofei%20Wu%20and%20Songcen%20Xu%20and%20Chun%20Yuan&entry.1292438233=%20%203D%20Gaussian%20Splatting%20showcases%20notable%20advancements%20in%20photo-realistic%20and%0Areal-time%20novel%20view%20synthesis.%20However%2C%20it%20faces%20challenges%20in%20modeling%20mirror%0Areflections%2C%20which%20exhibit%20substantial%20appearance%20variations%20from%20different%0Aviewpoints.%20To%20tackle%20this%20problem%2C%20we%20present%20MirrorGaussian%2C%20the%20first%20method%0Afor%20mirror%20scene%20reconstruction%20with%20real-time%20rendering%20based%20on%203D%20Gaussian%0ASplatting.%20The%20key%20insight%20is%20grounded%20on%20the%20mirror%20symmetry%20between%20the%0Areal-world%20space%20and%20the%20virtual%20mirror%20space.%20We%20introduce%20an%20intuitive%0Adual-rendering%20strategy%20that%20enables%20differentiable%20rasterization%20of%20both%20the%0Areal-world%203D%20Gaussians%20and%20the%20mirrored%20counterpart%20obtained%20by%20reflecting%20the%0Aformer%20about%20the%20mirror%20plane.%20All%203D%20Gaussians%20are%20jointly%20optimized%20with%20the%0Amirror%20plane%20in%20an%20end-to-end%20framework.%20MirrorGaussian%20achieves%20high-quality%0Aand%20real-time%20rendering%20in%20scenes%20with%20mirrors%2C%20empowering%20scene%20editing%20like%0Aadding%20new%20mirrors%20and%20objects.%20Comprehensive%20experiments%20on%20multiple%20datasets%0Ademonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%2C%0Aachieving%20state-of-the-art%20results.%20Project%20page%3A%0Ahttps%3A//mirror-gaussian.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11921v1&entry.124074799=Read"},
{"title": "Efficient Data-driven Scene Simulation using Robotic Surgery Videos via\n  Physics-embedded 3D Gaussians", "author": "Zhenya Yang and Kai Chen and Yonghao Long and Qi Dou", "abstract": "  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n", "link": "http://arxiv.org/abs/2405.00956v2", "date": "2024-05-20", "relevancy": 3.0917, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6254}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians&body=Title%3A%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians%0AAuthor%3A%20Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou%0AAbstract%3A%20%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Data-driven%2520Scene%2520Simulation%2520using%2520Robotic%2520Surgery%2520Videos%2520via%250A%2520%2520Physics-embedded%25203D%2520Gaussians%26entry.906535625%3DZhenya%2520Yang%2520and%2520Kai%2520Chen%2520and%2520Yonghao%2520Long%2520and%2520Qi%2520Dou%26entry.1292438233%3D%2520%2520Surgical%2520scene%2520simulation%2520plays%2520a%2520crucial%2520role%2520in%2520surgical%2520education%2520and%250Asimulator-based%2520robot%2520learning.%2520Traditional%2520approaches%2520for%2520creating%2520these%250Aenvironments%2520with%2520surgical%2520scene%2520involve%2520a%2520labor-intensive%2520process%2520where%250Adesigners%2520hand-craft%2520tissues%2520models%2520with%2520textures%2520and%2520geometries%2520for%2520soft%2520body%250Asimulations.%2520This%2520manual%2520approach%2520is%2520not%2520only%2520time-consuming%2520but%2520also%2520limited%250Ain%2520the%2520scalability%2520and%2520realism.%2520In%2520contrast%252C%2520data-driven%2520simulation%2520offers%2520a%250Acompelling%2520alternative.%2520It%2520has%2520the%2520potential%2520to%2520automatically%2520reconstruct%25203D%250Asurgical%2520scenes%2520from%2520real-world%2520surgical%2520video%2520data%252C%2520followed%2520by%2520the%250Aapplication%2520of%2520soft%2520body%2520physics.%2520This%2520area%252C%2520however%252C%2520is%2520relatively%2520uncharted.%250AIn%2520our%2520research%252C%2520we%2520introduce%25203D%2520Gaussian%2520as%2520a%2520learnable%2520representation%2520for%250Asurgical%2520scene%252C%2520which%2520is%2520learned%2520from%2520stereo%2520endoscopic%2520video.%2520To%2520prevent%250Aover-fitting%2520and%2520ensure%2520the%2520geometrical%2520correctness%2520of%2520these%2520scenes%252C%2520we%250Aincorporate%2520depth%2520supervision%2520and%2520anisotropy%2520regularization%2520into%2520the%2520Gaussian%250Alearning%2520process.%2520Furthermore%252C%2520we%2520apply%2520the%2520Material%2520Point%2520Method%252C%2520which%2520is%250Aintegrated%2520with%2520physical%2520properties%252C%2520to%2520the%25203D%2520Gaussians%2520to%2520achieve%2520realistic%250Ascene%2520deformations.%2520Our%2520method%2520was%2520evaluated%2520on%2520our%2520collected%2520in-house%2520and%250Apublic%2520surgical%2520videos%2520datasets.%2520Results%2520show%2520that%2520it%2520can%2520reconstruct%2520and%250Asimulate%2520surgical%2520scenes%2520from%2520endoscopic%2520videos%2520efficiently-taking%2520only%2520a%2520few%250Aminutes%2520to%2520reconstruct%2520the%2520surgical%2520scene-and%2520produce%2520both%2520visually%2520and%250Aphysically%2520plausible%2520deformations%2520at%2520a%2520speed%2520approaching%2520real-time.%2520The%2520results%250Ademonstrate%2520great%2520potential%2520of%2520our%2520proposed%2520method%2520to%2520enhance%2520the%2520efficiency%250Aand%2520variety%2520of%2520simulations%2520available%2520for%2520surgical%2520education%2520and%2520robot%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians&entry.906535625=Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou&entry.1292438233=%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00956v2&entry.124074799=Read"},
{"title": "View-Consistent 3D Editing with Gaussian Splatting", "author": "Yuxuan Wang and Xuanyu Yi and Zike Wu and Na Zhao and Long Chen and Hanwang Zhang", "abstract": "  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n", "link": "http://arxiv.org/abs/2403.11868v4", "date": "2024-05-20", "relevancy": 3.0833, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6335}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6167}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20View-Consistent%203D%20Editing%20with%20Gaussian%20Splatting&body=Title%3A%20View-Consistent%203D%20Editing%20with%20Gaussian%20Splatting%0AAuthor%3A%20Yuxuan%20Wang%20and%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Na%20Zhao%20and%20Long%20Chen%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20The%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20editing%2C%0Aoffering%20efficient%2C%20high-fidelity%20rendering%20and%20enabling%20precise%20local%0Amanipulations.%20Currently%2C%20diffusion-based%202D%20editing%20models%20are%20harnessed%20to%0Amodify%20multi-view%20rendered%20images%2C%20which%20then%20guide%20the%20editing%20of%203DGS%20models.%0AHowever%2C%20this%20approach%20faces%20a%20critical%20issue%20of%20multi-view%20inconsistency%2C%0Awhere%20the%20guidance%20images%20exhibit%20significant%20discrepancies%20across%20views%2C%0Aleading%20to%20mode%20collapse%20and%20visual%20artifacts%20of%203DGS.%20To%20this%20end%2C%20we%0Aintroduce%20View-consistent%20Editing%20%28VcEdit%29%2C%20a%20novel%20framework%20that%20seamlessly%0Aincorporates%203DGS%20into%20image%20editing%20processes%2C%20ensuring%20multi-view%20consistency%0Ain%20edited%20guidance%20images%20and%20effectively%20mitigating%20mode%20collapse%20issues.%0AVcEdit%20employs%20two%20innovative%20consistency%20modules%3A%20the%20Cross-attention%0AConsistency%20Module%20and%20the%20Editing%20Consistency%20Module%2C%20both%20designed%20to%20reduce%0Ainconsistencies%20in%20edited%20images.%20By%20incorporating%20these%20consistency%20modules%0Ainto%20an%20iterative%20pattern%2C%20VcEdit%20proficiently%20resolves%20the%20issue%20of%20multi-view%0Ainconsistency%2C%20facilitating%20high-quality%203DGS%20editing%20across%20a%20diverse%20range%20of%0Ascenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11868v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DView-Consistent%25203D%2520Editing%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Xuanyu%2520Yi%2520and%2520Zike%2520Wu%2520and%2520Na%2520Zhao%2520and%2520Long%2520Chen%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520revolutionized%25203D%2520editing%252C%250Aoffering%2520efficient%252C%2520high-fidelity%2520rendering%2520and%2520enabling%2520precise%2520local%250Amanipulations.%2520Currently%252C%2520diffusion-based%25202D%2520editing%2520models%2520are%2520harnessed%2520to%250Amodify%2520multi-view%2520rendered%2520images%252C%2520which%2520then%2520guide%2520the%2520editing%2520of%25203DGS%2520models.%250AHowever%252C%2520this%2520approach%2520faces%2520a%2520critical%2520issue%2520of%2520multi-view%2520inconsistency%252C%250Awhere%2520the%2520guidance%2520images%2520exhibit%2520significant%2520discrepancies%2520across%2520views%252C%250Aleading%2520to%2520mode%2520collapse%2520and%2520visual%2520artifacts%2520of%25203DGS.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520View-consistent%2520Editing%2520%2528VcEdit%2529%252C%2520a%2520novel%2520framework%2520that%2520seamlessly%250Aincorporates%25203DGS%2520into%2520image%2520editing%2520processes%252C%2520ensuring%2520multi-view%2520consistency%250Ain%2520edited%2520guidance%2520images%2520and%2520effectively%2520mitigating%2520mode%2520collapse%2520issues.%250AVcEdit%2520employs%2520two%2520innovative%2520consistency%2520modules%253A%2520the%2520Cross-attention%250AConsistency%2520Module%2520and%2520the%2520Editing%2520Consistency%2520Module%252C%2520both%2520designed%2520to%2520reduce%250Ainconsistencies%2520in%2520edited%2520images.%2520By%2520incorporating%2520these%2520consistency%2520modules%250Ainto%2520an%2520iterative%2520pattern%252C%2520VcEdit%2520proficiently%2520resolves%2520the%2520issue%2520of%2520multi-view%250Ainconsistency%252C%2520facilitating%2520high-quality%25203DGS%2520editing%2520across%2520a%2520diverse%2520range%2520of%250Ascenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11868v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View-Consistent%203D%20Editing%20with%20Gaussian%20Splatting&entry.906535625=Yuxuan%20Wang%20and%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Na%20Zhao%20and%20Long%20Chen%20and%20Hanwang%20Zhang&entry.1292438233=%20%20The%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20editing%2C%0Aoffering%20efficient%2C%20high-fidelity%20rendering%20and%20enabling%20precise%20local%0Amanipulations.%20Currently%2C%20diffusion-based%202D%20editing%20models%20are%20harnessed%20to%0Amodify%20multi-view%20rendered%20images%2C%20which%20then%20guide%20the%20editing%20of%203DGS%20models.%0AHowever%2C%20this%20approach%20faces%20a%20critical%20issue%20of%20multi-view%20inconsistency%2C%0Awhere%20the%20guidance%20images%20exhibit%20significant%20discrepancies%20across%20views%2C%0Aleading%20to%20mode%20collapse%20and%20visual%20artifacts%20of%203DGS.%20To%20this%20end%2C%20we%0Aintroduce%20View-consistent%20Editing%20%28VcEdit%29%2C%20a%20novel%20framework%20that%20seamlessly%0Aincorporates%203DGS%20into%20image%20editing%20processes%2C%20ensuring%20multi-view%20consistency%0Ain%20edited%20guidance%20images%20and%20effectively%20mitigating%20mode%20collapse%20issues.%0AVcEdit%20employs%20two%20innovative%20consistency%20modules%3A%20the%20Cross-attention%0AConsistency%20Module%20and%20the%20Editing%20Consistency%20Module%2C%20both%20designed%20to%20reduce%0Ainconsistencies%20in%20edited%20images.%20By%20incorporating%20these%20consistency%20modules%0Ainto%20an%20iterative%20pattern%2C%20VcEdit%20proficiently%20resolves%20the%20issue%20of%20multi-view%0Ainconsistency%2C%20facilitating%20high-quality%203DGS%20editing%20across%20a%20diverse%20range%20of%0Ascenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11868v4&entry.124074799=Read"},
{"title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization", "author": "Jiawei Zhang and Jiahe Li and Xiaohan Yu and Lei Huang and Lin Gu and Jin Zheng and Xiao Bai", "abstract": "  3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D\nGaussians to represent a scene. With sparse training views, 3DGS easily suffers\nfrom overfitting, negatively impacting the reconstruction quality. This paper\nintroduces a new co-regularization perspective for improving sparse-view 3DGS.\nWhen training two 3D Gaussian radiance fields with the same sparse views of a\nscene, we observe that the two radiance fields exhibit \\textit{point\ndisagreement} and \\textit{rendering disagreement} that can unsupervisedly\npredict reconstruction quality, stemming from the sampling implementation in\ndensification. We further quantify the point disagreement and rendering\ndisagreement by evaluating the registration between Gaussians' point\nrepresentations and calculating differences in their rendered pixels. The\nempirical study demonstrates the negative correlation between the two\ndisagreements and accurate reconstruction, which allows us to identify\ninaccurate reconstruction without accessing ground-truth information. Based on\nthe study, we propose CoR-GS, which identifies and suppresses inaccurate\nreconstruction based on the two disagreements: (\\romannumeral1) Co-pruning\nconsiders Gaussians that exhibit high point disagreement in inaccurate\npositions and prunes them. (\\romannumeral2) Pseudo-view co-regularization\nconsiders pixels that exhibit high rendering disagreement are inaccurately\nrendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and\nBlender demonstrate that CoR-GS effectively regularizes the scene geometry,\nreconstructs the compact representations, and achieves state-of-the-art novel\nview synthesis quality under sparse training views.\n", "link": "http://arxiv.org/abs/2405.12110v1", "date": "2024-05-20", "relevancy": 2.9986, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6654}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6156}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization&body=Title%3A%20CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization%0AAuthor%3A%20Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lei%20Huang%20and%20Lin%20Gu%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20creates%20a%20radiance%20field%20consisting%20of%203D%0AGaussians%20to%20represent%20a%20scene.%20With%20sparse%20training%20views%2C%203DGS%20easily%20suffers%0Afrom%20overfitting%2C%20negatively%20impacting%20the%20reconstruction%20quality.%20This%20paper%0Aintroduces%20a%20new%20co-regularization%20perspective%20for%20improving%20sparse-view%203DGS.%0AWhen%20training%20two%203D%20Gaussian%20radiance%20fields%20with%20the%20same%20sparse%20views%20of%20a%0Ascene%2C%20we%20observe%20that%20the%20two%20radiance%20fields%20exhibit%20%5Ctextit%7Bpoint%0Adisagreement%7D%20and%20%5Ctextit%7Brendering%20disagreement%7D%20that%20can%20unsupervisedly%0Apredict%20reconstruction%20quality%2C%20stemming%20from%20the%20sampling%20implementation%20in%0Adensification.%20We%20further%20quantify%20the%20point%20disagreement%20and%20rendering%0Adisagreement%20by%20evaluating%20the%20registration%20between%20Gaussians%27%20point%0Arepresentations%20and%20calculating%20differences%20in%20their%20rendered%20pixels.%20The%0Aempirical%20study%20demonstrates%20the%20negative%20correlation%20between%20the%20two%0Adisagreements%20and%20accurate%20reconstruction%2C%20which%20allows%20us%20to%20identify%0Ainaccurate%20reconstruction%20without%20accessing%20ground-truth%20information.%20Based%20on%0Athe%20study%2C%20we%20propose%20CoR-GS%2C%20which%20identifies%20and%20suppresses%20inaccurate%0Areconstruction%20based%20on%20the%20two%20disagreements%3A%20%28%5Cromannumeral1%29%20Co-pruning%0Aconsiders%20Gaussians%20that%20exhibit%20high%20point%20disagreement%20in%20inaccurate%0Apositions%20and%20prunes%20them.%20%28%5Cromannumeral2%29%20Pseudo-view%20co-regularization%0Aconsiders%20pixels%20that%20exhibit%20high%20rendering%20disagreement%20are%20inaccurately%0Arendered%20and%20suppress%20the%20disagreement.%20Results%20on%20LLFF%2C%20Mip-NeRF360%2C%20DTU%2C%20and%0ABlender%20demonstrate%20that%20CoR-GS%20effectively%20regularizes%20the%20scene%20geometry%2C%0Areconstructs%20the%20compact%20representations%2C%20and%20achieves%20state-of-the-art%20novel%0Aview%20synthesis%20quality%20under%20sparse%20training%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoR-GS%253A%2520Sparse-View%25203D%2520Gaussian%2520Splatting%2520via%2520Co-Regularization%26entry.906535625%3DJiawei%2520Zhang%2520and%2520Jiahe%2520Li%2520and%2520Xiaohan%2520Yu%2520and%2520Lei%2520Huang%2520and%2520Lin%2520Gu%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520creates%2520a%2520radiance%2520field%2520consisting%2520of%25203D%250AGaussians%2520to%2520represent%2520a%2520scene.%2520With%2520sparse%2520training%2520views%252C%25203DGS%2520easily%2520suffers%250Afrom%2520overfitting%252C%2520negatively%2520impacting%2520the%2520reconstruction%2520quality.%2520This%2520paper%250Aintroduces%2520a%2520new%2520co-regularization%2520perspective%2520for%2520improving%2520sparse-view%25203DGS.%250AWhen%2520training%2520two%25203D%2520Gaussian%2520radiance%2520fields%2520with%2520the%2520same%2520sparse%2520views%2520of%2520a%250Ascene%252C%2520we%2520observe%2520that%2520the%2520two%2520radiance%2520fields%2520exhibit%2520%255Ctextit%257Bpoint%250Adisagreement%257D%2520and%2520%255Ctextit%257Brendering%2520disagreement%257D%2520that%2520can%2520unsupervisedly%250Apredict%2520reconstruction%2520quality%252C%2520stemming%2520from%2520the%2520sampling%2520implementation%2520in%250Adensification.%2520We%2520further%2520quantify%2520the%2520point%2520disagreement%2520and%2520rendering%250Adisagreement%2520by%2520evaluating%2520the%2520registration%2520between%2520Gaussians%2527%2520point%250Arepresentations%2520and%2520calculating%2520differences%2520in%2520their%2520rendered%2520pixels.%2520The%250Aempirical%2520study%2520demonstrates%2520the%2520negative%2520correlation%2520between%2520the%2520two%250Adisagreements%2520and%2520accurate%2520reconstruction%252C%2520which%2520allows%2520us%2520to%2520identify%250Ainaccurate%2520reconstruction%2520without%2520accessing%2520ground-truth%2520information.%2520Based%2520on%250Athe%2520study%252C%2520we%2520propose%2520CoR-GS%252C%2520which%2520identifies%2520and%2520suppresses%2520inaccurate%250Areconstruction%2520based%2520on%2520the%2520two%2520disagreements%253A%2520%2528%255Cromannumeral1%2529%2520Co-pruning%250Aconsiders%2520Gaussians%2520that%2520exhibit%2520high%2520point%2520disagreement%2520in%2520inaccurate%250Apositions%2520and%2520prunes%2520them.%2520%2528%255Cromannumeral2%2529%2520Pseudo-view%2520co-regularization%250Aconsiders%2520pixels%2520that%2520exhibit%2520high%2520rendering%2520disagreement%2520are%2520inaccurately%250Arendered%2520and%2520suppress%2520the%2520disagreement.%2520Results%2520on%2520LLFF%252C%2520Mip-NeRF360%252C%2520DTU%252C%2520and%250ABlender%2520demonstrate%2520that%2520CoR-GS%2520effectively%2520regularizes%2520the%2520scene%2520geometry%252C%250Areconstructs%2520the%2520compact%2520representations%252C%2520and%2520achieves%2520state-of-the-art%2520novel%250Aview%2520synthesis%2520quality%2520under%2520sparse%2520training%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization&entry.906535625=Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lei%20Huang%20and%20Lin%20Gu%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20creates%20a%20radiance%20field%20consisting%20of%203D%0AGaussians%20to%20represent%20a%20scene.%20With%20sparse%20training%20views%2C%203DGS%20easily%20suffers%0Afrom%20overfitting%2C%20negatively%20impacting%20the%20reconstruction%20quality.%20This%20paper%0Aintroduces%20a%20new%20co-regularization%20perspective%20for%20improving%20sparse-view%203DGS.%0AWhen%20training%20two%203D%20Gaussian%20radiance%20fields%20with%20the%20same%20sparse%20views%20of%20a%0Ascene%2C%20we%20observe%20that%20the%20two%20radiance%20fields%20exhibit%20%5Ctextit%7Bpoint%0Adisagreement%7D%20and%20%5Ctextit%7Brendering%20disagreement%7D%20that%20can%20unsupervisedly%0Apredict%20reconstruction%20quality%2C%20stemming%20from%20the%20sampling%20implementation%20in%0Adensification.%20We%20further%20quantify%20the%20point%20disagreement%20and%20rendering%0Adisagreement%20by%20evaluating%20the%20registration%20between%20Gaussians%27%20point%0Arepresentations%20and%20calculating%20differences%20in%20their%20rendered%20pixels.%20The%0Aempirical%20study%20demonstrates%20the%20negative%20correlation%20between%20the%20two%0Adisagreements%20and%20accurate%20reconstruction%2C%20which%20allows%20us%20to%20identify%0Ainaccurate%20reconstruction%20without%20accessing%20ground-truth%20information.%20Based%20on%0Athe%20study%2C%20we%20propose%20CoR-GS%2C%20which%20identifies%20and%20suppresses%20inaccurate%0Areconstruction%20based%20on%20the%20two%20disagreements%3A%20%28%5Cromannumeral1%29%20Co-pruning%0Aconsiders%20Gaussians%20that%20exhibit%20high%20point%20disagreement%20in%20inaccurate%0Apositions%20and%20prunes%20them.%20%28%5Cromannumeral2%29%20Pseudo-view%20co-regularization%0Aconsiders%20pixels%20that%20exhibit%20high%20rendering%20disagreement%20are%20inaccurately%0Arendered%20and%20suppress%20the%20disagreement.%20Results%20on%20LLFF%2C%20Mip-NeRF360%2C%20DTU%2C%20and%0ABlender%20demonstrate%20that%20CoR-GS%20effectively%20regularizes%20the%20scene%20geometry%2C%0Areconstructs%20the%20compact%20representations%2C%20and%20achieves%20state-of-the-art%20novel%0Aview%20synthesis%20quality%20under%20sparse%20training%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12110v1&entry.124074799=Read"},
{"title": "Multi-View Attentive Contextualization for Multi-View 3D Object\n  Detection", "author": "Xianpeng Liu and Ce Zheng and Ming Qian and Nan Xue and Chen Chen and Zhebin Zhang and Chen Li and Tianfu Wu", "abstract": "  We present Multi-View Attentive Contextualization (MvACon), a simple yet\neffective method for improving 2D-to-3D feature lifting in query-based\nmulti-view 3D (MV3D) object detection. Despite remarkable progress witnessed in\nthe field of query-based MV3D object detection, prior art often suffers from\neither the lack of exploiting high-resolution 2D features in dense\nattention-based lifting, due to high computational costs, or from\ninsufficiently dense grounding of 3D queries to multi-scale 2D features in\nsparse attention-based lifting. Our proposed MvACon hits the two birds with one\nstone using a representationally dense yet computationally sparse attentive\nfeature contextualization scheme that is agnostic to specific 2D-to-3D feature\nlifting approaches. In experiments, the proposed MvACon is thoroughly tested on\nthe nuScenes benchmark, using both the BEVFormer and its recent 3D deformable\nattention (DFA3D) variant, as well as the PETR, showing consistent detection\nperformance improvement, especially in enhancing performance in location,\norientation, and velocity prediction. It is also tested on the Waymo-mini\nbenchmark using BEVFormer with similar improvement. We qualitatively and\nquantitatively show that global cluster-based contexts effectively encode dense\nscene-level contexts for MV3D object detection. The promising results of our\nproposed MvACon reinforces the adage in computer vision -- ``(contextualized)\nfeature matters\".\n", "link": "http://arxiv.org/abs/2405.12200v1", "date": "2024-05-20", "relevancy": 2.9606, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Attentive%20Contextualization%20for%20Multi-View%203D%20Object%0A%20%20Detection&body=Title%3A%20Multi-View%20Attentive%20Contextualization%20for%20Multi-View%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Xianpeng%20Liu%20and%20Ce%20Zheng%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Chen%20Chen%20and%20Zhebin%20Zhang%20and%20Chen%20Li%20and%20Tianfu%20Wu%0AAbstract%3A%20%20%20We%20present%20Multi-View%20Attentive%20Contextualization%20%28MvACon%29%2C%20a%20simple%20yet%0Aeffective%20method%20for%20improving%202D-to-3D%20feature%20lifting%20in%20query-based%0Amulti-view%203D%20%28MV3D%29%20object%20detection.%20Despite%20remarkable%20progress%20witnessed%20in%0Athe%20field%20of%20query-based%20MV3D%20object%20detection%2C%20prior%20art%20often%20suffers%20from%0Aeither%20the%20lack%20of%20exploiting%20high-resolution%202D%20features%20in%20dense%0Aattention-based%20lifting%2C%20due%20to%20high%20computational%20costs%2C%20or%20from%0Ainsufficiently%20dense%20grounding%20of%203D%20queries%20to%20multi-scale%202D%20features%20in%0Asparse%20attention-based%20lifting.%20Our%20proposed%20MvACon%20hits%20the%20two%20birds%20with%20one%0Astone%20using%20a%20representationally%20dense%20yet%20computationally%20sparse%20attentive%0Afeature%20contextualization%20scheme%20that%20is%20agnostic%20to%20specific%202D-to-3D%20feature%0Alifting%20approaches.%20In%20experiments%2C%20the%20proposed%20MvACon%20is%20thoroughly%20tested%20on%0Athe%20nuScenes%20benchmark%2C%20using%20both%20the%20BEVFormer%20and%20its%20recent%203D%20deformable%0Aattention%20%28DFA3D%29%20variant%2C%20as%20well%20as%20the%20PETR%2C%20showing%20consistent%20detection%0Aperformance%20improvement%2C%20especially%20in%20enhancing%20performance%20in%20location%2C%0Aorientation%2C%20and%20velocity%20prediction.%20It%20is%20also%20tested%20on%20the%20Waymo-mini%0Abenchmark%20using%20BEVFormer%20with%20similar%20improvement.%20We%20qualitatively%20and%0Aquantitatively%20show%20that%20global%20cluster-based%20contexts%20effectively%20encode%20dense%0Ascene-level%20contexts%20for%20MV3D%20object%20detection.%20The%20promising%20results%20of%20our%0Aproposed%20MvACon%20reinforces%20the%20adage%20in%20computer%20vision%20--%20%60%60%28contextualized%29%0Afeature%20matters%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Attentive%2520Contextualization%2520for%2520Multi-View%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DXianpeng%2520Liu%2520and%2520Ce%2520Zheng%2520and%2520Ming%2520Qian%2520and%2520Nan%2520Xue%2520and%2520Chen%2520Chen%2520and%2520Zhebin%2520Zhang%2520and%2520Chen%2520Li%2520and%2520Tianfu%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520Multi-View%2520Attentive%2520Contextualization%2520%2528MvACon%2529%252C%2520a%2520simple%2520yet%250Aeffective%2520method%2520for%2520improving%25202D-to-3D%2520feature%2520lifting%2520in%2520query-based%250Amulti-view%25203D%2520%2528MV3D%2529%2520object%2520detection.%2520Despite%2520remarkable%2520progress%2520witnessed%2520in%250Athe%2520field%2520of%2520query-based%2520MV3D%2520object%2520detection%252C%2520prior%2520art%2520often%2520suffers%2520from%250Aeither%2520the%2520lack%2520of%2520exploiting%2520high-resolution%25202D%2520features%2520in%2520dense%250Aattention-based%2520lifting%252C%2520due%2520to%2520high%2520computational%2520costs%252C%2520or%2520from%250Ainsufficiently%2520dense%2520grounding%2520of%25203D%2520queries%2520to%2520multi-scale%25202D%2520features%2520in%250Asparse%2520attention-based%2520lifting.%2520Our%2520proposed%2520MvACon%2520hits%2520the%2520two%2520birds%2520with%2520one%250Astone%2520using%2520a%2520representationally%2520dense%2520yet%2520computationally%2520sparse%2520attentive%250Afeature%2520contextualization%2520scheme%2520that%2520is%2520agnostic%2520to%2520specific%25202D-to-3D%2520feature%250Alifting%2520approaches.%2520In%2520experiments%252C%2520the%2520proposed%2520MvACon%2520is%2520thoroughly%2520tested%2520on%250Athe%2520nuScenes%2520benchmark%252C%2520using%2520both%2520the%2520BEVFormer%2520and%2520its%2520recent%25203D%2520deformable%250Aattention%2520%2528DFA3D%2529%2520variant%252C%2520as%2520well%2520as%2520the%2520PETR%252C%2520showing%2520consistent%2520detection%250Aperformance%2520improvement%252C%2520especially%2520in%2520enhancing%2520performance%2520in%2520location%252C%250Aorientation%252C%2520and%2520velocity%2520prediction.%2520It%2520is%2520also%2520tested%2520on%2520the%2520Waymo-mini%250Abenchmark%2520using%2520BEVFormer%2520with%2520similar%2520improvement.%2520We%2520qualitatively%2520and%250Aquantitatively%2520show%2520that%2520global%2520cluster-based%2520contexts%2520effectively%2520encode%2520dense%250Ascene-level%2520contexts%2520for%2520MV3D%2520object%2520detection.%2520The%2520promising%2520results%2520of%2520our%250Aproposed%2520MvACon%2520reinforces%2520the%2520adage%2520in%2520computer%2520vision%2520--%2520%2560%2560%2528contextualized%2529%250Afeature%2520matters%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Attentive%20Contextualization%20for%20Multi-View%203D%20Object%0A%20%20Detection&entry.906535625=Xianpeng%20Liu%20and%20Ce%20Zheng%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Chen%20Chen%20and%20Zhebin%20Zhang%20and%20Chen%20Li%20and%20Tianfu%20Wu&entry.1292438233=%20%20We%20present%20Multi-View%20Attentive%20Contextualization%20%28MvACon%29%2C%20a%20simple%20yet%0Aeffective%20method%20for%20improving%202D-to-3D%20feature%20lifting%20in%20query-based%0Amulti-view%203D%20%28MV3D%29%20object%20detection.%20Despite%20remarkable%20progress%20witnessed%20in%0Athe%20field%20of%20query-based%20MV3D%20object%20detection%2C%20prior%20art%20often%20suffers%20from%0Aeither%20the%20lack%20of%20exploiting%20high-resolution%202D%20features%20in%20dense%0Aattention-based%20lifting%2C%20due%20to%20high%20computational%20costs%2C%20or%20from%0Ainsufficiently%20dense%20grounding%20of%203D%20queries%20to%20multi-scale%202D%20features%20in%0Asparse%20attention-based%20lifting.%20Our%20proposed%20MvACon%20hits%20the%20two%20birds%20with%20one%0Astone%20using%20a%20representationally%20dense%20yet%20computationally%20sparse%20attentive%0Afeature%20contextualization%20scheme%20that%20is%20agnostic%20to%20specific%202D-to-3D%20feature%0Alifting%20approaches.%20In%20experiments%2C%20the%20proposed%20MvACon%20is%20thoroughly%20tested%20on%0Athe%20nuScenes%20benchmark%2C%20using%20both%20the%20BEVFormer%20and%20its%20recent%203D%20deformable%0Aattention%20%28DFA3D%29%20variant%2C%20as%20well%20as%20the%20PETR%2C%20showing%20consistent%20detection%0Aperformance%20improvement%2C%20especially%20in%20enhancing%20performance%20in%20location%2C%0Aorientation%2C%20and%20velocity%20prediction.%20It%20is%20also%20tested%20on%20the%20Waymo-mini%0Abenchmark%20using%20BEVFormer%20with%20similar%20improvement.%20We%20qualitatively%20and%0Aquantitatively%20show%20that%20global%20cluster-based%20contexts%20effectively%20encode%20dense%0Ascene-level%20contexts%20for%20MV3D%20object%20detection.%20The%20promising%20results%20of%20our%0Aproposed%20MvACon%20reinforces%20the%20adage%20in%20computer%20vision%20--%20%60%60%28contextualized%29%0Afeature%20matters%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12200v1&entry.124074799=Read"},
{"title": "Depth Reconstruction with Neural Signed Distance Fields in Structured\n  Light Systems", "author": "Rukun Qiao and Hiroshi Kawasaki and Hongbin Zha", "abstract": "  We introduce a novel depth estimation technique for multi-frame structured\nlight setups using neural implicit representations of 3D space. Our approach\nemploys a neural signed distance field (SDF), trained through self-supervised\ndifferentiable rendering. Unlike passive vision, where joint estimation of\nradiance and geometry fields is necessary, we capitalize on known radiance\nfields from projected patterns in structured light systems. This enables\nisolated optimization of the geometry field, ensuring convergence and network\nefficacy with fixed device positioning. To enhance geometric fidelity, we\nincorporate an additional color loss based on object surfaces during training.\nReal-world experiments demonstrate our method's superiority in geometric\nperformance for few-shot scenarios, while achieving comparable results with\nincreased pattern availability.\n", "link": "http://arxiv.org/abs/2405.12006v1", "date": "2024-05-20", "relevancy": 2.8473, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.593}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5622}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Reconstruction%20with%20Neural%20Signed%20Distance%20Fields%20in%20Structured%0A%20%20Light%20Systems&body=Title%3A%20Depth%20Reconstruction%20with%20Neural%20Signed%20Distance%20Fields%20in%20Structured%0A%20%20Light%20Systems%0AAuthor%3A%20Rukun%20Qiao%20and%20Hiroshi%20Kawasaki%20and%20Hongbin%20Zha%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20depth%20estimation%20technique%20for%20multi-frame%20structured%0Alight%20setups%20using%20neural%20implicit%20representations%20of%203D%20space.%20Our%20approach%0Aemploys%20a%20neural%20signed%20distance%20field%20%28SDF%29%2C%20trained%20through%20self-supervised%0Adifferentiable%20rendering.%20Unlike%20passive%20vision%2C%20where%20joint%20estimation%20of%0Aradiance%20and%20geometry%20fields%20is%20necessary%2C%20we%20capitalize%20on%20known%20radiance%0Afields%20from%20projected%20patterns%20in%20structured%20light%20systems.%20This%20enables%0Aisolated%20optimization%20of%20the%20geometry%20field%2C%20ensuring%20convergence%20and%20network%0Aefficacy%20with%20fixed%20device%20positioning.%20To%20enhance%20geometric%20fidelity%2C%20we%0Aincorporate%20an%20additional%20color%20loss%20based%20on%20object%20surfaces%20during%20training.%0AReal-world%20experiments%20demonstrate%20our%20method%27s%20superiority%20in%20geometric%0Aperformance%20for%20few-shot%20scenarios%2C%20while%20achieving%20comparable%20results%20with%0Aincreased%20pattern%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Reconstruction%2520with%2520Neural%2520Signed%2520Distance%2520Fields%2520in%2520Structured%250A%2520%2520Light%2520Systems%26entry.906535625%3DRukun%2520Qiao%2520and%2520Hiroshi%2520Kawasaki%2520and%2520Hongbin%2520Zha%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520depth%2520estimation%2520technique%2520for%2520multi-frame%2520structured%250Alight%2520setups%2520using%2520neural%2520implicit%2520representations%2520of%25203D%2520space.%2520Our%2520approach%250Aemploys%2520a%2520neural%2520signed%2520distance%2520field%2520%2528SDF%2529%252C%2520trained%2520through%2520self-supervised%250Adifferentiable%2520rendering.%2520Unlike%2520passive%2520vision%252C%2520where%2520joint%2520estimation%2520of%250Aradiance%2520and%2520geometry%2520fields%2520is%2520necessary%252C%2520we%2520capitalize%2520on%2520known%2520radiance%250Afields%2520from%2520projected%2520patterns%2520in%2520structured%2520light%2520systems.%2520This%2520enables%250Aisolated%2520optimization%2520of%2520the%2520geometry%2520field%252C%2520ensuring%2520convergence%2520and%2520network%250Aefficacy%2520with%2520fixed%2520device%2520positioning.%2520To%2520enhance%2520geometric%2520fidelity%252C%2520we%250Aincorporate%2520an%2520additional%2520color%2520loss%2520based%2520on%2520object%2520surfaces%2520during%2520training.%250AReal-world%2520experiments%2520demonstrate%2520our%2520method%2527s%2520superiority%2520in%2520geometric%250Aperformance%2520for%2520few-shot%2520scenarios%252C%2520while%2520achieving%2520comparable%2520results%2520with%250Aincreased%2520pattern%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Reconstruction%20with%20Neural%20Signed%20Distance%20Fields%20in%20Structured%0A%20%20Light%20Systems&entry.906535625=Rukun%20Qiao%20and%20Hiroshi%20Kawasaki%20and%20Hongbin%20Zha&entry.1292438233=%20%20We%20introduce%20a%20novel%20depth%20estimation%20technique%20for%20multi-frame%20structured%0Alight%20setups%20using%20neural%20implicit%20representations%20of%203D%20space.%20Our%20approach%0Aemploys%20a%20neural%20signed%20distance%20field%20%28SDF%29%2C%20trained%20through%20self-supervised%0Adifferentiable%20rendering.%20Unlike%20passive%20vision%2C%20where%20joint%20estimation%20of%0Aradiance%20and%20geometry%20fields%20is%20necessary%2C%20we%20capitalize%20on%20known%20radiance%0Afields%20from%20projected%20patterns%20in%20structured%20light%20systems.%20This%20enables%0Aisolated%20optimization%20of%20the%20geometry%20field%2C%20ensuring%20convergence%20and%20network%0Aefficacy%20with%20fixed%20device%20positioning.%20To%20enhance%20geometric%20fidelity%2C%20we%0Aincorporate%20an%20additional%20color%20loss%20based%20on%20object%20surfaces%20during%20training.%0AReal-world%20experiments%20demonstrate%20our%20method%27s%20superiority%20in%20geometric%0Aperformance%20for%20few-shot%20scenarios%2C%20while%20achieving%20comparable%20results%20with%0Aincreased%20pattern%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12006v1&entry.124074799=Read"},
{"title": "GuidedRec: Guiding Ill-Posed Unsupervised Volumetric Recovery", "author": "Alexandre Cafaro and Amaury Leroy and Guillaume Beldjoudi and Pauline Maury and Charlotte Robert and Eric Deutsch and Vincent Gr\u00e9goire and Vincent Lepetit and Nikos Paragios", "abstract": "  We introduce a novel unsupervised approach to reconstructing a 3D volume from\nonly two planar projections that exploits a previous\\-ly-captured 3D volume of\nthe patient. Such volume is readily available in many important medical\nprocedures and previous methods already used such a volume. Earlier methods\nthat work by deforming this volume to match the projections typically fail when\nthe number of projections is very low as the alignment becomes\nunderconstrained. We show how to use a generative model of the volume\nstructures to constrain the deformation and obtain a correct estimate.\nMoreover, our method is not bounded to a specific sensor calibration and can be\napplied to new calibrations without retraining. We evaluate our approach on a\nchallenging dataset and show it outperforms state-of-the-art methods. As a\nresult, our method could be used in treatment scenarios such as surgery and\nradiotherapy while drastically reducing patient radiation exposure.\n", "link": "http://arxiv.org/abs/2405.11977v1", "date": "2024-05-20", "relevancy": 2.8304, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5742}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5742}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuidedRec%3A%20Guiding%20Ill-Posed%20Unsupervised%20Volumetric%20Recovery&body=Title%3A%20GuidedRec%3A%20Guiding%20Ill-Posed%20Unsupervised%20Volumetric%20Recovery%0AAuthor%3A%20Alexandre%20Cafaro%20and%20Amaury%20Leroy%20and%20Guillaume%20Beldjoudi%20and%20Pauline%20Maury%20and%20Charlotte%20Robert%20and%20Eric%20Deutsch%20and%20Vincent%20Gr%C3%A9goire%20and%20Vincent%20Lepetit%20and%20Nikos%20Paragios%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20unsupervised%20approach%20to%20reconstructing%20a%203D%20volume%20from%0Aonly%20two%20planar%20projections%20that%20exploits%20a%20previous%5C-ly-captured%203D%20volume%20of%0Athe%20patient.%20Such%20volume%20is%20readily%20available%20in%20many%20important%20medical%0Aprocedures%20and%20previous%20methods%20already%20used%20such%20a%20volume.%20Earlier%20methods%0Athat%20work%20by%20deforming%20this%20volume%20to%20match%20the%20projections%20typically%20fail%20when%0Athe%20number%20of%20projections%20is%20very%20low%20as%20the%20alignment%20becomes%0Aunderconstrained.%20We%20show%20how%20to%20use%20a%20generative%20model%20of%20the%20volume%0Astructures%20to%20constrain%20the%20deformation%20and%20obtain%20a%20correct%20estimate.%0AMoreover%2C%20our%20method%20is%20not%20bounded%20to%20a%20specific%20sensor%20calibration%20and%20can%20be%0Aapplied%20to%20new%20calibrations%20without%20retraining.%20We%20evaluate%20our%20approach%20on%20a%0Achallenging%20dataset%20and%20show%20it%20outperforms%20state-of-the-art%20methods.%20As%20a%0Aresult%2C%20our%20method%20could%20be%20used%20in%20treatment%20scenarios%20such%20as%20surgery%20and%0Aradiotherapy%20while%20drastically%20reducing%20patient%20radiation%20exposure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuidedRec%253A%2520Guiding%2520Ill-Posed%2520Unsupervised%2520Volumetric%2520Recovery%26entry.906535625%3DAlexandre%2520Cafaro%2520and%2520Amaury%2520Leroy%2520and%2520Guillaume%2520Beldjoudi%2520and%2520Pauline%2520Maury%2520and%2520Charlotte%2520Robert%2520and%2520Eric%2520Deutsch%2520and%2520Vincent%2520Gr%25C3%25A9goire%2520and%2520Vincent%2520Lepetit%2520and%2520Nikos%2520Paragios%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520unsupervised%2520approach%2520to%2520reconstructing%2520a%25203D%2520volume%2520from%250Aonly%2520two%2520planar%2520projections%2520that%2520exploits%2520a%2520previous%255C-ly-captured%25203D%2520volume%2520of%250Athe%2520patient.%2520Such%2520volume%2520is%2520readily%2520available%2520in%2520many%2520important%2520medical%250Aprocedures%2520and%2520previous%2520methods%2520already%2520used%2520such%2520a%2520volume.%2520Earlier%2520methods%250Athat%2520work%2520by%2520deforming%2520this%2520volume%2520to%2520match%2520the%2520projections%2520typically%2520fail%2520when%250Athe%2520number%2520of%2520projections%2520is%2520very%2520low%2520as%2520the%2520alignment%2520becomes%250Aunderconstrained.%2520We%2520show%2520how%2520to%2520use%2520a%2520generative%2520model%2520of%2520the%2520volume%250Astructures%2520to%2520constrain%2520the%2520deformation%2520and%2520obtain%2520a%2520correct%2520estimate.%250AMoreover%252C%2520our%2520method%2520is%2520not%2520bounded%2520to%2520a%2520specific%2520sensor%2520calibration%2520and%2520can%2520be%250Aapplied%2520to%2520new%2520calibrations%2520without%2520retraining.%2520We%2520evaluate%2520our%2520approach%2520on%2520a%250Achallenging%2520dataset%2520and%2520show%2520it%2520outperforms%2520state-of-the-art%2520methods.%2520As%2520a%250Aresult%252C%2520our%2520method%2520could%2520be%2520used%2520in%2520treatment%2520scenarios%2520such%2520as%2520surgery%2520and%250Aradiotherapy%2520while%2520drastically%2520reducing%2520patient%2520radiation%2520exposure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuidedRec%3A%20Guiding%20Ill-Posed%20Unsupervised%20Volumetric%20Recovery&entry.906535625=Alexandre%20Cafaro%20and%20Amaury%20Leroy%20and%20Guillaume%20Beldjoudi%20and%20Pauline%20Maury%20and%20Charlotte%20Robert%20and%20Eric%20Deutsch%20and%20Vincent%20Gr%C3%A9goire%20and%20Vincent%20Lepetit%20and%20Nikos%20Paragios&entry.1292438233=%20%20We%20introduce%20a%20novel%20unsupervised%20approach%20to%20reconstructing%20a%203D%20volume%20from%0Aonly%20two%20planar%20projections%20that%20exploits%20a%20previous%5C-ly-captured%203D%20volume%20of%0Athe%20patient.%20Such%20volume%20is%20readily%20available%20in%20many%20important%20medical%0Aprocedures%20and%20previous%20methods%20already%20used%20such%20a%20volume.%20Earlier%20methods%0Athat%20work%20by%20deforming%20this%20volume%20to%20match%20the%20projections%20typically%20fail%20when%0Athe%20number%20of%20projections%20is%20very%20low%20as%20the%20alignment%20becomes%0Aunderconstrained.%20We%20show%20how%20to%20use%20a%20generative%20model%20of%20the%20volume%0Astructures%20to%20constrain%20the%20deformation%20and%20obtain%20a%20correct%20estimate.%0AMoreover%2C%20our%20method%20is%20not%20bounded%20to%20a%20specific%20sensor%20calibration%20and%20can%20be%0Aapplied%20to%20new%20calibrations%20without%20retraining.%20We%20evaluate%20our%20approach%20on%20a%0Achallenging%20dataset%20and%20show%20it%20outperforms%20state-of-the-art%20methods.%20As%20a%0Aresult%2C%20our%20method%20could%20be%20used%20in%20treatment%20scenarios%20such%20as%20surgery%20and%0Aradiotherapy%20while%20drastically%20reducing%20patient%20radiation%20exposure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11977v1&entry.124074799=Read"},
{"title": "NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo", "author": "Fotios Logothetis and Ignas Budvytis and Roberto Cipolla", "abstract": "  In this work we present a novel multi-view photometric stereo (PS) method.\nLike many works in 3D reconstruction we are leveraging neural shape\nrepresentations and learnt renderers. However, our work differs from the\nstate-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we\nexplicity leverage per-pixel intensity renderings rather than relying mainly on\nestimated normals.\n  We model point light attenuation and explicitly raytrace cast shadows in\norder to best approximate each points incoming radiance. This is used as input\nto a fully neural material renderer that uses minimal prior assumptions and it\nis jointly optimised with the surface. Finally, estimated normal and\nsegmentation maps can also incorporated in order to maximise the surface\naccuracy.\n  Our method is among the first to outperform the classical approach of\nDiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at\napprox 1.5m distance away with approximate 400x400 resolution. Moreover, we\nshow robustness to poor normals in low light count scenario, achieving 0.27mm\nChamfer distance when pixel rendering is used instead of estimated normals.\n", "link": "http://arxiv.org/abs/2405.12057v1", "date": "2024-05-20", "relevancy": 2.7686, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo&body=Title%3A%20NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo%0AAuthor%3A%20Fotios%20Logothetis%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla%0AAbstract%3A%20%20%20In%20this%20work%20we%20present%20a%20novel%20multi-view%20photometric%20stereo%20%28PS%29%20method.%0ALike%20many%20works%20in%203D%20reconstruction%20we%20are%20leveraging%20neural%20shape%0Arepresentations%20and%20learnt%20renderers.%20However%2C%20our%20work%20differs%20from%20the%0Astate-of-the-art%20multi-view%20PS%20methods%20such%20as%20PS-NeRF%20or%20SuperNormal%20we%0Aexplicity%20leverage%20per-pixel%20intensity%20renderings%20rather%20than%20relying%20mainly%20on%0Aestimated%20normals.%0A%20%20We%20model%20point%20light%20attenuation%20and%20explicitly%20raytrace%20cast%20shadows%20in%0Aorder%20to%20best%20approximate%20each%20points%20incoming%20radiance.%20This%20is%20used%20as%20input%0Ato%20a%20fully%20neural%20material%20renderer%20that%20uses%20minimal%20prior%20assumptions%20and%20it%0Ais%20jointly%20optimised%20with%20the%20surface.%20Finally%2C%20estimated%20normal%20and%0Asegmentation%20maps%20can%20also%20incorporated%20in%20order%20to%20maximise%20the%20surface%0Aaccuracy.%0A%20%20Our%20method%20is%20among%20the%20first%20to%20outperform%20the%20classical%20approach%20of%0ADiLiGenT-MV%20and%20achieves%20average%200.2mm%20Chamfer%20distance%20for%20objects%20imaged%20at%0Aapprox%201.5m%20distance%20away%20with%20approximate%20400x400%20resolution.%20Moreover%2C%20we%0Ashow%20robustness%20to%20poor%20normals%20in%20low%20light%20count%20scenario%2C%20achieving%200.27mm%0AChamfer%20distance%20when%20pixel%20rendering%20is%20used%20instead%20of%20estimated%20normals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNPLMV-PS%253A%2520Neural%2520Point-Light%2520Multi-View%2520Photometric%2520Stereo%26entry.906535625%3DFotios%2520Logothetis%2520and%2520Ignas%2520Budvytis%2520and%2520Roberto%2520Cipolla%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520present%2520a%2520novel%2520multi-view%2520photometric%2520stereo%2520%2528PS%2529%2520method.%250ALike%2520many%2520works%2520in%25203D%2520reconstruction%2520we%2520are%2520leveraging%2520neural%2520shape%250Arepresentations%2520and%2520learnt%2520renderers.%2520However%252C%2520our%2520work%2520differs%2520from%2520the%250Astate-of-the-art%2520multi-view%2520PS%2520methods%2520such%2520as%2520PS-NeRF%2520or%2520SuperNormal%2520we%250Aexplicity%2520leverage%2520per-pixel%2520intensity%2520renderings%2520rather%2520than%2520relying%2520mainly%2520on%250Aestimated%2520normals.%250A%2520%2520We%2520model%2520point%2520light%2520attenuation%2520and%2520explicitly%2520raytrace%2520cast%2520shadows%2520in%250Aorder%2520to%2520best%2520approximate%2520each%2520points%2520incoming%2520radiance.%2520This%2520is%2520used%2520as%2520input%250Ato%2520a%2520fully%2520neural%2520material%2520renderer%2520that%2520uses%2520minimal%2520prior%2520assumptions%2520and%2520it%250Ais%2520jointly%2520optimised%2520with%2520the%2520surface.%2520Finally%252C%2520estimated%2520normal%2520and%250Asegmentation%2520maps%2520can%2520also%2520incorporated%2520in%2520order%2520to%2520maximise%2520the%2520surface%250Aaccuracy.%250A%2520%2520Our%2520method%2520is%2520among%2520the%2520first%2520to%2520outperform%2520the%2520classical%2520approach%2520of%250ADiLiGenT-MV%2520and%2520achieves%2520average%25200.2mm%2520Chamfer%2520distance%2520for%2520objects%2520imaged%2520at%250Aapprox%25201.5m%2520distance%2520away%2520with%2520approximate%2520400x400%2520resolution.%2520Moreover%252C%2520we%250Ashow%2520robustness%2520to%2520poor%2520normals%2520in%2520low%2520light%2520count%2520scenario%252C%2520achieving%25200.27mm%250AChamfer%2520distance%2520when%2520pixel%2520rendering%2520is%2520used%2520instead%2520of%2520estimated%2520normals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo&entry.906535625=Fotios%20Logothetis%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla&entry.1292438233=%20%20In%20this%20work%20we%20present%20a%20novel%20multi-view%20photometric%20stereo%20%28PS%29%20method.%0ALike%20many%20works%20in%203D%20reconstruction%20we%20are%20leveraging%20neural%20shape%0Arepresentations%20and%20learnt%20renderers.%20However%2C%20our%20work%20differs%20from%20the%0Astate-of-the-art%20multi-view%20PS%20methods%20such%20as%20PS-NeRF%20or%20SuperNormal%20we%0Aexplicity%20leverage%20per-pixel%20intensity%20renderings%20rather%20than%20relying%20mainly%20on%0Aestimated%20normals.%0A%20%20We%20model%20point%20light%20attenuation%20and%20explicitly%20raytrace%20cast%20shadows%20in%0Aorder%20to%20best%20approximate%20each%20points%20incoming%20radiance.%20This%20is%20used%20as%20input%0Ato%20a%20fully%20neural%20material%20renderer%20that%20uses%20minimal%20prior%20assumptions%20and%20it%0Ais%20jointly%20optimised%20with%20the%20surface.%20Finally%2C%20estimated%20normal%20and%0Asegmentation%20maps%20can%20also%20incorporated%20in%20order%20to%20maximise%20the%20surface%0Aaccuracy.%0A%20%20Our%20method%20is%20among%20the%20first%20to%20outperform%20the%20classical%20approach%20of%0ADiLiGenT-MV%20and%20achieves%20average%200.2mm%20Chamfer%20distance%20for%20objects%20imaged%20at%0Aapprox%201.5m%20distance%20away%20with%20approximate%20400x400%20resolution.%20Moreover%2C%20we%0Ashow%20robustness%20to%20poor%20normals%20in%20low%20light%20count%20scenario%2C%20achieving%200.27mm%0AChamfer%20distance%20when%20pixel%20rendering%20is%20used%20instead%20of%20estimated%20normals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12057v1&entry.124074799=Read"},
{"title": "AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements", "author": "Calvin Yeung and Kenjiro Ide and Keisuke Fujii", "abstract": "  Image understanding is a foundational task in computer vision, with recent\napplications emerging in soccer posture analysis. However, existing publicly\navailable datasets lack comprehensive information, notably in the form of\nposture sequences and 2D pose annotations. Moreover, current analysis models\noften rely on interpretable linear models (e.g., PCA and regression), limiting\ntheir capacity to capture non-linear spatiotemporal relationships in complex\nand diverse scenarios. To address these gaps, we introduce the 3D Shot Posture\n(3DSP) dataset in soccer broadcast videos, which represents the most extensive\nsports image dataset with 2D pose annotations to our knowledge. Additionally,\nwe present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear\napproach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,\na pipeline aimed at semi-automating 2D and 3D pose estimation and posture\nanalysis. While achieving full automation proved challenging, we provide a\nfoundational baseline, extending its utility beyond the scope of annotated\ndata. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present\nposture analysis results based on 3DSP. The dataset, code, and models are\navailable at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.\n", "link": "http://arxiv.org/abs/2405.12070v1", "date": "2024-05-20", "relevancy": 2.7237, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoSoccerPose%3A%20Automated%203D%20posture%20Analysis%20of%20Soccer%20Shot%20Movements&body=Title%3A%20AutoSoccerPose%3A%20Automated%203D%20posture%20Analysis%20of%20Soccer%20Shot%20Movements%0AAuthor%3A%20Calvin%20Yeung%20and%20Kenjiro%20Ide%20and%20Keisuke%20Fujii%0AAbstract%3A%20%20%20Image%20understanding%20is%20a%20foundational%20task%20in%20computer%20vision%2C%20with%20recent%0Aapplications%20emerging%20in%20soccer%20posture%20analysis.%20However%2C%20existing%20publicly%0Aavailable%20datasets%20lack%20comprehensive%20information%2C%20notably%20in%20the%20form%20of%0Aposture%20sequences%20and%202D%20pose%20annotations.%20Moreover%2C%20current%20analysis%20models%0Aoften%20rely%20on%20interpretable%20linear%20models%20%28e.g.%2C%20PCA%20and%20regression%29%2C%20limiting%0Atheir%20capacity%20to%20capture%20non-linear%20spatiotemporal%20relationships%20in%20complex%0Aand%20diverse%20scenarios.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%203D%20Shot%20Posture%0A%283DSP%29%20dataset%20in%20soccer%20broadcast%20videos%2C%20which%20represents%20the%20most%20extensive%0Asports%20image%20dataset%20with%202D%20pose%20annotations%20to%20our%20knowledge.%20Additionally%2C%0Awe%20present%20the%203DSP-GRAE%20%28Graph%20Recurrent%20AutoEncoder%29%20model%2C%20a%20non-linear%0Aapproach%20for%20embedding%20pose%20sequences.%20Furthermore%2C%20we%20propose%20AutoSoccerPose%2C%0Aa%20pipeline%20aimed%20at%20semi-automating%202D%20and%203D%20pose%20estimation%20and%20posture%0Aanalysis.%20While%20achieving%20full%20automation%20proved%20challenging%2C%20we%20provide%20a%0Afoundational%20baseline%2C%20extending%20its%20utility%20beyond%20the%20scope%20of%20annotated%0Adata.%20We%20validate%20AutoSoccerPose%20on%20SoccerNet%20and%203DSP%20datasets%2C%20and%20present%0Aposture%20analysis%20results%20based%20on%203DSP.%20The%20dataset%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/calvinyeungck/3D-Shot-Posture-Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoSoccerPose%253A%2520Automated%25203D%2520posture%2520Analysis%2520of%2520Soccer%2520Shot%2520Movements%26entry.906535625%3DCalvin%2520Yeung%2520and%2520Kenjiro%2520Ide%2520and%2520Keisuke%2520Fujii%26entry.1292438233%3D%2520%2520Image%2520understanding%2520is%2520a%2520foundational%2520task%2520in%2520computer%2520vision%252C%2520with%2520recent%250Aapplications%2520emerging%2520in%2520soccer%2520posture%2520analysis.%2520However%252C%2520existing%2520publicly%250Aavailable%2520datasets%2520lack%2520comprehensive%2520information%252C%2520notably%2520in%2520the%2520form%2520of%250Aposture%2520sequences%2520and%25202D%2520pose%2520annotations.%2520Moreover%252C%2520current%2520analysis%2520models%250Aoften%2520rely%2520on%2520interpretable%2520linear%2520models%2520%2528e.g.%252C%2520PCA%2520and%2520regression%2529%252C%2520limiting%250Atheir%2520capacity%2520to%2520capture%2520non-linear%2520spatiotemporal%2520relationships%2520in%2520complex%250Aand%2520diverse%2520scenarios.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520the%25203D%2520Shot%2520Posture%250A%25283DSP%2529%2520dataset%2520in%2520soccer%2520broadcast%2520videos%252C%2520which%2520represents%2520the%2520most%2520extensive%250Asports%2520image%2520dataset%2520with%25202D%2520pose%2520annotations%2520to%2520our%2520knowledge.%2520Additionally%252C%250Awe%2520present%2520the%25203DSP-GRAE%2520%2528Graph%2520Recurrent%2520AutoEncoder%2529%2520model%252C%2520a%2520non-linear%250Aapproach%2520for%2520embedding%2520pose%2520sequences.%2520Furthermore%252C%2520we%2520propose%2520AutoSoccerPose%252C%250Aa%2520pipeline%2520aimed%2520at%2520semi-automating%25202D%2520and%25203D%2520pose%2520estimation%2520and%2520posture%250Aanalysis.%2520While%2520achieving%2520full%2520automation%2520proved%2520challenging%252C%2520we%2520provide%2520a%250Afoundational%2520baseline%252C%2520extending%2520its%2520utility%2520beyond%2520the%2520scope%2520of%2520annotated%250Adata.%2520We%2520validate%2520AutoSoccerPose%2520on%2520SoccerNet%2520and%25203DSP%2520datasets%252C%2520and%2520present%250Aposture%2520analysis%2520results%2520based%2520on%25203DSP.%2520The%2520dataset%252C%2520code%252C%2520and%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/calvinyeungck/3D-Shot-Posture-Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoSoccerPose%3A%20Automated%203D%20posture%20Analysis%20of%20Soccer%20Shot%20Movements&entry.906535625=Calvin%20Yeung%20and%20Kenjiro%20Ide%20and%20Keisuke%20Fujii&entry.1292438233=%20%20Image%20understanding%20is%20a%20foundational%20task%20in%20computer%20vision%2C%20with%20recent%0Aapplications%20emerging%20in%20soccer%20posture%20analysis.%20However%2C%20existing%20publicly%0Aavailable%20datasets%20lack%20comprehensive%20information%2C%20notably%20in%20the%20form%20of%0Aposture%20sequences%20and%202D%20pose%20annotations.%20Moreover%2C%20current%20analysis%20models%0Aoften%20rely%20on%20interpretable%20linear%20models%20%28e.g.%2C%20PCA%20and%20regression%29%2C%20limiting%0Atheir%20capacity%20to%20capture%20non-linear%20spatiotemporal%20relationships%20in%20complex%0Aand%20diverse%20scenarios.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%203D%20Shot%20Posture%0A%283DSP%29%20dataset%20in%20soccer%20broadcast%20videos%2C%20which%20represents%20the%20most%20extensive%0Asports%20image%20dataset%20with%202D%20pose%20annotations%20to%20our%20knowledge.%20Additionally%2C%0Awe%20present%20the%203DSP-GRAE%20%28Graph%20Recurrent%20AutoEncoder%29%20model%2C%20a%20non-linear%0Aapproach%20for%20embedding%20pose%20sequences.%20Furthermore%2C%20we%20propose%20AutoSoccerPose%2C%0Aa%20pipeline%20aimed%20at%20semi-automating%202D%20and%203D%20pose%20estimation%20and%20posture%0Aanalysis.%20While%20achieving%20full%20automation%20proved%20challenging%2C%20we%20provide%20a%0Afoundational%20baseline%2C%20extending%20its%20utility%20beyond%20the%20scope%20of%20annotated%0Adata.%20We%20validate%20AutoSoccerPose%20on%20SoccerNet%20and%203DSP%20datasets%2C%20and%20present%0Aposture%20analysis%20results%20based%20on%203DSP.%20The%20dataset%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/calvinyeungck/3D-Shot-Posture-Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12070v1&entry.124074799=Read"},
{"title": "UAV-VisLoc: A Large-scale Dataset for UAV Visual Localization", "author": "Wenjia Xu and Yaxuan Yao and Jiaqi Cao and Zhiwei Wei and Chunbo Liu and Jiuniu Wang and Mugen Peng", "abstract": "  The application of unmanned aerial vehicles (UAV) has been widely extended\nrecently. It is crucial to ensure accurate latitude and longitude coordinates\nfor UAVs, especially when the global navigation satellite systems (GNSS) are\ndisrupted and unreliable. Existing visual localization methods achieve\nautonomous visual localization without error accumulation by matching the\nground-down view image of UAV with the ortho satellite maps. However,\ncollecting UAV ground-down view images across diverse locations is costly,\nleading to a scarcity of large-scale datasets for real-world scenarios.\nExisting datasets for UAV visual localization are often limited to small\ngeographic areas or are focused only on urban regions with distinct textures.\nTo address this, we define the UAV visual localization task by determining the\nUAV's real position coordinates on a large-scale satellite map based on the\ncaptured ground-down view. In this paper, we present a large-scale dataset,\nUAV-VisLoc, to facilitate the UAV visual localization task. This dataset\ncomprises images from diverse drones across 11 locations in China, capturing a\nrange of topographical features. The dataset features images from fixed-wing\ndrones and multi-terrain drones, captured at different altitudes and\norientations. Our dataset includes 6,742 drone images and 11 satellite maps,\nwith metadata such as latitude, longitude, altitude, and capture date. Our\ndataset is tailored to support both the training and testing of models by\nproviding a diverse and extensive data.\n", "link": "http://arxiv.org/abs/2405.11936v1", "date": "2024-05-20", "relevancy": 2.7146, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6173}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV-VisLoc%3A%20A%20Large-scale%20Dataset%20for%20UAV%20Visual%20Localization&body=Title%3A%20UAV-VisLoc%3A%20A%20Large-scale%20Dataset%20for%20UAV%20Visual%20Localization%0AAuthor%3A%20Wenjia%20Xu%20and%20Yaxuan%20Yao%20and%20Jiaqi%20Cao%20and%20Zhiwei%20Wei%20and%20Chunbo%20Liu%20and%20Jiuniu%20Wang%20and%20Mugen%20Peng%0AAbstract%3A%20%20%20The%20application%20of%20unmanned%20aerial%20vehicles%20%28UAV%29%20has%20been%20widely%20extended%0Arecently.%20It%20is%20crucial%20to%20ensure%20accurate%20latitude%20and%20longitude%20coordinates%0Afor%20UAVs%2C%20especially%20when%20the%20global%20navigation%20satellite%20systems%20%28GNSS%29%20are%0Adisrupted%20and%20unreliable.%20Existing%20visual%20localization%20methods%20achieve%0Aautonomous%20visual%20localization%20without%20error%20accumulation%20by%20matching%20the%0Aground-down%20view%20image%20of%20UAV%20with%20the%20ortho%20satellite%20maps.%20However%2C%0Acollecting%20UAV%20ground-down%20view%20images%20across%20diverse%20locations%20is%20costly%2C%0Aleading%20to%20a%20scarcity%20of%20large-scale%20datasets%20for%20real-world%20scenarios.%0AExisting%20datasets%20for%20UAV%20visual%20localization%20are%20often%20limited%20to%20small%0Ageographic%20areas%20or%20are%20focused%20only%20on%20urban%20regions%20with%20distinct%20textures.%0ATo%20address%20this%2C%20we%20define%20the%20UAV%20visual%20localization%20task%20by%20determining%20the%0AUAV%27s%20real%20position%20coordinates%20on%20a%20large-scale%20satellite%20map%20based%20on%20the%0Acaptured%20ground-down%20view.%20In%20this%20paper%2C%20we%20present%20a%20large-scale%20dataset%2C%0AUAV-VisLoc%2C%20to%20facilitate%20the%20UAV%20visual%20localization%20task.%20This%20dataset%0Acomprises%20images%20from%20diverse%20drones%20across%2011%20locations%20in%20China%2C%20capturing%20a%0Arange%20of%20topographical%20features.%20The%20dataset%20features%20images%20from%20fixed-wing%0Adrones%20and%20multi-terrain%20drones%2C%20captured%20at%20different%20altitudes%20and%0Aorientations.%20Our%20dataset%20includes%206%2C742%20drone%20images%20and%2011%20satellite%20maps%2C%0Awith%20metadata%20such%20as%20latitude%2C%20longitude%2C%20altitude%2C%20and%20capture%20date.%20Our%0Adataset%20is%20tailored%20to%20support%20both%20the%20training%20and%20testing%20of%20models%20by%0Aproviding%20a%20diverse%20and%20extensive%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV-VisLoc%253A%2520A%2520Large-scale%2520Dataset%2520for%2520UAV%2520Visual%2520Localization%26entry.906535625%3DWenjia%2520Xu%2520and%2520Yaxuan%2520Yao%2520and%2520Jiaqi%2520Cao%2520and%2520Zhiwei%2520Wei%2520and%2520Chunbo%2520Liu%2520and%2520Jiuniu%2520Wang%2520and%2520Mugen%2520Peng%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAV%2529%2520has%2520been%2520widely%2520extended%250Arecently.%2520It%2520is%2520crucial%2520to%2520ensure%2520accurate%2520latitude%2520and%2520longitude%2520coordinates%250Afor%2520UAVs%252C%2520especially%2520when%2520the%2520global%2520navigation%2520satellite%2520systems%2520%2528GNSS%2529%2520are%250Adisrupted%2520and%2520unreliable.%2520Existing%2520visual%2520localization%2520methods%2520achieve%250Aautonomous%2520visual%2520localization%2520without%2520error%2520accumulation%2520by%2520matching%2520the%250Aground-down%2520view%2520image%2520of%2520UAV%2520with%2520the%2520ortho%2520satellite%2520maps.%2520However%252C%250Acollecting%2520UAV%2520ground-down%2520view%2520images%2520across%2520diverse%2520locations%2520is%2520costly%252C%250Aleading%2520to%2520a%2520scarcity%2520of%2520large-scale%2520datasets%2520for%2520real-world%2520scenarios.%250AExisting%2520datasets%2520for%2520UAV%2520visual%2520localization%2520are%2520often%2520limited%2520to%2520small%250Ageographic%2520areas%2520or%2520are%2520focused%2520only%2520on%2520urban%2520regions%2520with%2520distinct%2520textures.%250ATo%2520address%2520this%252C%2520we%2520define%2520the%2520UAV%2520visual%2520localization%2520task%2520by%2520determining%2520the%250AUAV%2527s%2520real%2520position%2520coordinates%2520on%2520a%2520large-scale%2520satellite%2520map%2520based%2520on%2520the%250Acaptured%2520ground-down%2520view.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520large-scale%2520dataset%252C%250AUAV-VisLoc%252C%2520to%2520facilitate%2520the%2520UAV%2520visual%2520localization%2520task.%2520This%2520dataset%250Acomprises%2520images%2520from%2520diverse%2520drones%2520across%252011%2520locations%2520in%2520China%252C%2520capturing%2520a%250Arange%2520of%2520topographical%2520features.%2520The%2520dataset%2520features%2520images%2520from%2520fixed-wing%250Adrones%2520and%2520multi-terrain%2520drones%252C%2520captured%2520at%2520different%2520altitudes%2520and%250Aorientations.%2520Our%2520dataset%2520includes%25206%252C742%2520drone%2520images%2520and%252011%2520satellite%2520maps%252C%250Awith%2520metadata%2520such%2520as%2520latitude%252C%2520longitude%252C%2520altitude%252C%2520and%2520capture%2520date.%2520Our%250Adataset%2520is%2520tailored%2520to%2520support%2520both%2520the%2520training%2520and%2520testing%2520of%2520models%2520by%250Aproviding%2520a%2520diverse%2520and%2520extensive%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV-VisLoc%3A%20A%20Large-scale%20Dataset%20for%20UAV%20Visual%20Localization&entry.906535625=Wenjia%20Xu%20and%20Yaxuan%20Yao%20and%20Jiaqi%20Cao%20and%20Zhiwei%20Wei%20and%20Chunbo%20Liu%20and%20Jiuniu%20Wang%20and%20Mugen%20Peng&entry.1292438233=%20%20The%20application%20of%20unmanned%20aerial%20vehicles%20%28UAV%29%20has%20been%20widely%20extended%0Arecently.%20It%20is%20crucial%20to%20ensure%20accurate%20latitude%20and%20longitude%20coordinates%0Afor%20UAVs%2C%20especially%20when%20the%20global%20navigation%20satellite%20systems%20%28GNSS%29%20are%0Adisrupted%20and%20unreliable.%20Existing%20visual%20localization%20methods%20achieve%0Aautonomous%20visual%20localization%20without%20error%20accumulation%20by%20matching%20the%0Aground-down%20view%20image%20of%20UAV%20with%20the%20ortho%20satellite%20maps.%20However%2C%0Acollecting%20UAV%20ground-down%20view%20images%20across%20diverse%20locations%20is%20costly%2C%0Aleading%20to%20a%20scarcity%20of%20large-scale%20datasets%20for%20real-world%20scenarios.%0AExisting%20datasets%20for%20UAV%20visual%20localization%20are%20often%20limited%20to%20small%0Ageographic%20areas%20or%20are%20focused%20only%20on%20urban%20regions%20with%20distinct%20textures.%0ATo%20address%20this%2C%20we%20define%20the%20UAV%20visual%20localization%20task%20by%20determining%20the%0AUAV%27s%20real%20position%20coordinates%20on%20a%20large-scale%20satellite%20map%20based%20on%20the%0Acaptured%20ground-down%20view.%20In%20this%20paper%2C%20we%20present%20a%20large-scale%20dataset%2C%0AUAV-VisLoc%2C%20to%20facilitate%20the%20UAV%20visual%20localization%20task.%20This%20dataset%0Acomprises%20images%20from%20diverse%20drones%20across%2011%20locations%20in%20China%2C%20capturing%20a%0Arange%20of%20topographical%20features.%20The%20dataset%20features%20images%20from%20fixed-wing%0Adrones%20and%20multi-terrain%20drones%2C%20captured%20at%20different%20altitudes%20and%0Aorientations.%20Our%20dataset%20includes%206%2C742%20drone%20images%20and%2011%20satellite%20maps%2C%0Awith%20metadata%20such%20as%20latitude%2C%20longitude%2C%20altitude%2C%20and%20capture%20date.%20Our%0Adataset%20is%20tailored%20to%20support%20both%20the%20training%20and%20testing%20of%20models%20by%0Aproviding%20a%20diverse%20and%20extensive%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11936v1&entry.124074799=Read"},
{"title": "FashionEngine: Interactive 3D Human Generation and Editing via\n  Multimodal Controls", "author": "Tao Hu and Fangzhou Hong and Zhaoxi Chen and Ziwei Liu", "abstract": "  We present FashionEngine, an interactive 3D human generation and editing\nsystem that creates 3D digital humans via user-friendly multimodal controls\nsuch as natural languages, visual perceptions, and hand-drawing sketches.\nFashionEngine automates the 3D human production with three key components: 1) A\npre-trained 3D human diffusion model that learns to model 3D humans in a\nsemantic UV latent space from 2D image training data, which provides strong\npriors for diverse generation and editing tasks. 2) Multimodality-UV Space\nencoding the texture appearance, shape topology, and textual semantics of human\nclothing in a canonical UV-aligned space, which faithfully aligns the user\nmultimodal inputs with the implicit UV latent space for controllable 3D human\nediting. The multimodality-UV space is shared across different user inputs,\nsuch as texts, images, and sketches, which enables various joint multimodal\nediting tasks. 3) Multimodality-UV Aligned Sampler learns to sample\nhigh-quality and diverse 3D humans from the diffusion prior. Extensive\nexperiments validate FashionEngine's state-of-the-art performance for\nconditional generation/editing tasks. In addition, we present an interactive\nuser interface for our FashionEngine that enables both conditional and\nunconditional generation tasks, and editing tasks including pose/view/shape\ncontrol, text-, image-, and sketch-driven 3D human editing and 3D virtual\ntry-on, in a unified framework. Our project page is at:\nhttps://taohuumd.github.io/projects/FashionEngine.\n", "link": "http://arxiv.org/abs/2404.01655v3", "date": "2024-05-20", "relevancy": 2.6886, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7046}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6901}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FashionEngine%3A%20Interactive%203D%20Human%20Generation%20and%20Editing%20via%0A%20%20Multimodal%20Controls&body=Title%3A%20FashionEngine%3A%20Interactive%203D%20Human%20Generation%20and%20Editing%20via%0A%20%20Multimodal%20Controls%0AAuthor%3A%20Tao%20Hu%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20FashionEngine%2C%20an%20interactive%203D%20human%20generation%20and%20editing%0Asystem%20that%20creates%203D%20digital%20humans%20via%20user-friendly%20multimodal%20controls%0Asuch%20as%20natural%20languages%2C%20visual%20perceptions%2C%20and%20hand-drawing%20sketches.%0AFashionEngine%20automates%20the%203D%20human%20production%20with%20three%20key%20components%3A%201%29%20A%0Apre-trained%203D%20human%20diffusion%20model%20that%20learns%20to%20model%203D%20humans%20in%20a%0Asemantic%20UV%20latent%20space%20from%202D%20image%20training%20data%2C%20which%20provides%20strong%0Apriors%20for%20diverse%20generation%20and%20editing%20tasks.%202%29%20Multimodality-UV%20Space%0Aencoding%20the%20texture%20appearance%2C%20shape%20topology%2C%20and%20textual%20semantics%20of%20human%0Aclothing%20in%20a%20canonical%20UV-aligned%20space%2C%20which%20faithfully%20aligns%20the%20user%0Amultimodal%20inputs%20with%20the%20implicit%20UV%20latent%20space%20for%20controllable%203D%20human%0Aediting.%20The%20multimodality-UV%20space%20is%20shared%20across%20different%20user%20inputs%2C%0Asuch%20as%20texts%2C%20images%2C%20and%20sketches%2C%20which%20enables%20various%20joint%20multimodal%0Aediting%20tasks.%203%29%20Multimodality-UV%20Aligned%20Sampler%20learns%20to%20sample%0Ahigh-quality%20and%20diverse%203D%20humans%20from%20the%20diffusion%20prior.%20Extensive%0Aexperiments%20validate%20FashionEngine%27s%20state-of-the-art%20performance%20for%0Aconditional%20generation/editing%20tasks.%20In%20addition%2C%20we%20present%20an%20interactive%0Auser%20interface%20for%20our%20FashionEngine%20that%20enables%20both%20conditional%20and%0Aunconditional%20generation%20tasks%2C%20and%20editing%20tasks%20including%20pose/view/shape%0Acontrol%2C%20text-%2C%20image-%2C%20and%20sketch-driven%203D%20human%20editing%20and%203D%20virtual%0Atry-on%2C%20in%20a%20unified%20framework.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//taohuumd.github.io/projects/FashionEngine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01655v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionEngine%253A%2520Interactive%25203D%2520Human%2520Generation%2520and%2520Editing%2520via%250A%2520%2520Multimodal%2520Controls%26entry.906535625%3DTao%2520Hu%2520and%2520Fangzhou%2520Hong%2520and%2520Zhaoxi%2520Chen%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520FashionEngine%252C%2520an%2520interactive%25203D%2520human%2520generation%2520and%2520editing%250Asystem%2520that%2520creates%25203D%2520digital%2520humans%2520via%2520user-friendly%2520multimodal%2520controls%250Asuch%2520as%2520natural%2520languages%252C%2520visual%2520perceptions%252C%2520and%2520hand-drawing%2520sketches.%250AFashionEngine%2520automates%2520the%25203D%2520human%2520production%2520with%2520three%2520key%2520components%253A%25201%2529%2520A%250Apre-trained%25203D%2520human%2520diffusion%2520model%2520that%2520learns%2520to%2520model%25203D%2520humans%2520in%2520a%250Asemantic%2520UV%2520latent%2520space%2520from%25202D%2520image%2520training%2520data%252C%2520which%2520provides%2520strong%250Apriors%2520for%2520diverse%2520generation%2520and%2520editing%2520tasks.%25202%2529%2520Multimodality-UV%2520Space%250Aencoding%2520the%2520texture%2520appearance%252C%2520shape%2520topology%252C%2520and%2520textual%2520semantics%2520of%2520human%250Aclothing%2520in%2520a%2520canonical%2520UV-aligned%2520space%252C%2520which%2520faithfully%2520aligns%2520the%2520user%250Amultimodal%2520inputs%2520with%2520the%2520implicit%2520UV%2520latent%2520space%2520for%2520controllable%25203D%2520human%250Aediting.%2520The%2520multimodality-UV%2520space%2520is%2520shared%2520across%2520different%2520user%2520inputs%252C%250Asuch%2520as%2520texts%252C%2520images%252C%2520and%2520sketches%252C%2520which%2520enables%2520various%2520joint%2520multimodal%250Aediting%2520tasks.%25203%2529%2520Multimodality-UV%2520Aligned%2520Sampler%2520learns%2520to%2520sample%250Ahigh-quality%2520and%2520diverse%25203D%2520humans%2520from%2520the%2520diffusion%2520prior.%2520Extensive%250Aexperiments%2520validate%2520FashionEngine%2527s%2520state-of-the-art%2520performance%2520for%250Aconditional%2520generation/editing%2520tasks.%2520In%2520addition%252C%2520we%2520present%2520an%2520interactive%250Auser%2520interface%2520for%2520our%2520FashionEngine%2520that%2520enables%2520both%2520conditional%2520and%250Aunconditional%2520generation%2520tasks%252C%2520and%2520editing%2520tasks%2520including%2520pose/view/shape%250Acontrol%252C%2520text-%252C%2520image-%252C%2520and%2520sketch-driven%25203D%2520human%2520editing%2520and%25203D%2520virtual%250Atry-on%252C%2520in%2520a%2520unified%2520framework.%2520Our%2520project%2520page%2520is%2520at%253A%250Ahttps%253A//taohuumd.github.io/projects/FashionEngine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01655v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionEngine%3A%20Interactive%203D%20Human%20Generation%20and%20Editing%20via%0A%20%20Multimodal%20Controls&entry.906535625=Tao%20Hu%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20FashionEngine%2C%20an%20interactive%203D%20human%20generation%20and%20editing%0Asystem%20that%20creates%203D%20digital%20humans%20via%20user-friendly%20multimodal%20controls%0Asuch%20as%20natural%20languages%2C%20visual%20perceptions%2C%20and%20hand-drawing%20sketches.%0AFashionEngine%20automates%20the%203D%20human%20production%20with%20three%20key%20components%3A%201%29%20A%0Apre-trained%203D%20human%20diffusion%20model%20that%20learns%20to%20model%203D%20humans%20in%20a%0Asemantic%20UV%20latent%20space%20from%202D%20image%20training%20data%2C%20which%20provides%20strong%0Apriors%20for%20diverse%20generation%20and%20editing%20tasks.%202%29%20Multimodality-UV%20Space%0Aencoding%20the%20texture%20appearance%2C%20shape%20topology%2C%20and%20textual%20semantics%20of%20human%0Aclothing%20in%20a%20canonical%20UV-aligned%20space%2C%20which%20faithfully%20aligns%20the%20user%0Amultimodal%20inputs%20with%20the%20implicit%20UV%20latent%20space%20for%20controllable%203D%20human%0Aediting.%20The%20multimodality-UV%20space%20is%20shared%20across%20different%20user%20inputs%2C%0Asuch%20as%20texts%2C%20images%2C%20and%20sketches%2C%20which%20enables%20various%20joint%20multimodal%0Aediting%20tasks.%203%29%20Multimodality-UV%20Aligned%20Sampler%20learns%20to%20sample%0Ahigh-quality%20and%20diverse%203D%20humans%20from%20the%20diffusion%20prior.%20Extensive%0Aexperiments%20validate%20FashionEngine%27s%20state-of-the-art%20performance%20for%0Aconditional%20generation/editing%20tasks.%20In%20addition%2C%20we%20present%20an%20interactive%0Auser%20interface%20for%20our%20FashionEngine%20that%20enables%20both%20conditional%20and%0Aunconditional%20generation%20tasks%2C%20and%20editing%20tasks%20including%20pose/view/shape%0Acontrol%2C%20text-%2C%20image-%2C%20and%20sketch-driven%203D%20human%20editing%20and%203D%20virtual%0Atry-on%2C%20in%20a%20unified%20framework.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//taohuumd.github.io/projects/FashionEngine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01655v3&entry.124074799=Read"},
{"title": "Towards Principled Evaluations of Sparse Autoencoders for\n  Interpretability and Control", "author": "Aleksandar Makelov and George Lange and Neel Nanda", "abstract": "  Disentangling model activations into meaningful features is a central problem\nin interpretability. However, the absence of ground-truth for these features in\nrealistic scenarios makes validating recent approaches, such as sparse\ndictionary learning, elusive. To address this challenge, we propose a framework\nfor evaluating feature dictionaries in the context of specific tasks, by\ncomparing them against \\emph{supervised} feature dictionaries. First, we\ndemonstrate that supervised dictionaries achieve excellent approximation,\ncontrol, and interpretability of model computations on the task. Second, we use\nthe supervised dictionaries to develop and contextualize evaluations of\nunsupervised dictionaries along the same three axes.\n  We apply this framework to the indirect object identification (IOI) task\nusing GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or\nOpenWebText datasets. We find that these SAEs capture interpretable features\nfor the IOI task, but they are less successful than supervised features in\ncontrolling the model. Finally, we observe two qualitative phenomena in SAE\ntraining: feature occlusion (where a causally relevant concept is robustly\novershadowed by even slightly higher-magnitude ones in the learned features),\nand feature over-splitting (where binary features split into many smaller, less\ninterpretable features). We hope that our framework will provide a useful step\ntowards more objective and grounded evaluations of sparse dictionary learning\nmethods.\n", "link": "http://arxiv.org/abs/2405.08366v3", "date": "2024-05-20", "relevancy": 2.654, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5679}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Principled%20Evaluations%20of%20Sparse%20Autoencoders%20for%0A%20%20Interpretability%20and%20Control&body=Title%3A%20Towards%20Principled%20Evaluations%20of%20Sparse%20Autoencoders%20for%0A%20%20Interpretability%20and%20Control%0AAuthor%3A%20Aleksandar%20Makelov%20and%20George%20Lange%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Disentangling%20model%20activations%20into%20meaningful%20features%20is%20a%20central%20problem%0Ain%20interpretability.%20However%2C%20the%20absence%20of%20ground-truth%20for%20these%20features%20in%0Arealistic%20scenarios%20makes%20validating%20recent%20approaches%2C%20such%20as%20sparse%0Adictionary%20learning%2C%20elusive.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20framework%0Afor%20evaluating%20feature%20dictionaries%20in%20the%20context%20of%20specific%20tasks%2C%20by%0Acomparing%20them%20against%20%5Cemph%7Bsupervised%7D%20feature%20dictionaries.%20First%2C%20we%0Ademonstrate%20that%20supervised%20dictionaries%20achieve%20excellent%20approximation%2C%0Acontrol%2C%20and%20interpretability%20of%20model%20computations%20on%20the%20task.%20Second%2C%20we%20use%0Athe%20supervised%20dictionaries%20to%20develop%20and%20contextualize%20evaluations%20of%0Aunsupervised%20dictionaries%20along%20the%20same%20three%20axes.%0A%20%20We%20apply%20this%20framework%20to%20the%20indirect%20object%20identification%20%28IOI%29%20task%0Ausing%20GPT-2%20Small%2C%20with%20sparse%20autoencoders%20%28SAEs%29%20trained%20on%20either%20the%20IOI%20or%0AOpenWebText%20datasets.%20We%20find%20that%20these%20SAEs%20capture%20interpretable%20features%0Afor%20the%20IOI%20task%2C%20but%20they%20are%20less%20successful%20than%20supervised%20features%20in%0Acontrolling%20the%20model.%20Finally%2C%20we%20observe%20two%20qualitative%20phenomena%20in%20SAE%0Atraining%3A%20feature%20occlusion%20%28where%20a%20causally%20relevant%20concept%20is%20robustly%0Aovershadowed%20by%20even%20slightly%20higher-magnitude%20ones%20in%20the%20learned%20features%29%2C%0Aand%20feature%20over-splitting%20%28where%20binary%20features%20split%20into%20many%20smaller%2C%20less%0Ainterpretable%20features%29.%20We%20hope%20that%20our%20framework%20will%20provide%20a%20useful%20step%0Atowards%20more%20objective%20and%20grounded%20evaluations%20of%20sparse%20dictionary%20learning%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Principled%2520Evaluations%2520of%2520Sparse%2520Autoencoders%2520for%250A%2520%2520Interpretability%2520and%2520Control%26entry.906535625%3DAleksandar%2520Makelov%2520and%2520George%2520Lange%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Disentangling%2520model%2520activations%2520into%2520meaningful%2520features%2520is%2520a%2520central%2520problem%250Ain%2520interpretability.%2520However%252C%2520the%2520absence%2520of%2520ground-truth%2520for%2520these%2520features%2520in%250Arealistic%2520scenarios%2520makes%2520validating%2520recent%2520approaches%252C%2520such%2520as%2520sparse%250Adictionary%2520learning%252C%2520elusive.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520framework%250Afor%2520evaluating%2520feature%2520dictionaries%2520in%2520the%2520context%2520of%2520specific%2520tasks%252C%2520by%250Acomparing%2520them%2520against%2520%255Cemph%257Bsupervised%257D%2520feature%2520dictionaries.%2520First%252C%2520we%250Ademonstrate%2520that%2520supervised%2520dictionaries%2520achieve%2520excellent%2520approximation%252C%250Acontrol%252C%2520and%2520interpretability%2520of%2520model%2520computations%2520on%2520the%2520task.%2520Second%252C%2520we%2520use%250Athe%2520supervised%2520dictionaries%2520to%2520develop%2520and%2520contextualize%2520evaluations%2520of%250Aunsupervised%2520dictionaries%2520along%2520the%2520same%2520three%2520axes.%250A%2520%2520We%2520apply%2520this%2520framework%2520to%2520the%2520indirect%2520object%2520identification%2520%2528IOI%2529%2520task%250Ausing%2520GPT-2%2520Small%252C%2520with%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520trained%2520on%2520either%2520the%2520IOI%2520or%250AOpenWebText%2520datasets.%2520We%2520find%2520that%2520these%2520SAEs%2520capture%2520interpretable%2520features%250Afor%2520the%2520IOI%2520task%252C%2520but%2520they%2520are%2520less%2520successful%2520than%2520supervised%2520features%2520in%250Acontrolling%2520the%2520model.%2520Finally%252C%2520we%2520observe%2520two%2520qualitative%2520phenomena%2520in%2520SAE%250Atraining%253A%2520feature%2520occlusion%2520%2528where%2520a%2520causally%2520relevant%2520concept%2520is%2520robustly%250Aovershadowed%2520by%2520even%2520slightly%2520higher-magnitude%2520ones%2520in%2520the%2520learned%2520features%2529%252C%250Aand%2520feature%2520over-splitting%2520%2528where%2520binary%2520features%2520split%2520into%2520many%2520smaller%252C%2520less%250Ainterpretable%2520features%2529.%2520We%2520hope%2520that%2520our%2520framework%2520will%2520provide%2520a%2520useful%2520step%250Atowards%2520more%2520objective%2520and%2520grounded%2520evaluations%2520of%2520sparse%2520dictionary%2520learning%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Principled%20Evaluations%20of%20Sparse%20Autoencoders%20for%0A%20%20Interpretability%20and%20Control&entry.906535625=Aleksandar%20Makelov%20and%20George%20Lange%20and%20Neel%20Nanda&entry.1292438233=%20%20Disentangling%20model%20activations%20into%20meaningful%20features%20is%20a%20central%20problem%0Ain%20interpretability.%20However%2C%20the%20absence%20of%20ground-truth%20for%20these%20features%20in%0Arealistic%20scenarios%20makes%20validating%20recent%20approaches%2C%20such%20as%20sparse%0Adictionary%20learning%2C%20elusive.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20framework%0Afor%20evaluating%20feature%20dictionaries%20in%20the%20context%20of%20specific%20tasks%2C%20by%0Acomparing%20them%20against%20%5Cemph%7Bsupervised%7D%20feature%20dictionaries.%20First%2C%20we%0Ademonstrate%20that%20supervised%20dictionaries%20achieve%20excellent%20approximation%2C%0Acontrol%2C%20and%20interpretability%20of%20model%20computations%20on%20the%20task.%20Second%2C%20we%20use%0Athe%20supervised%20dictionaries%20to%20develop%20and%20contextualize%20evaluations%20of%0Aunsupervised%20dictionaries%20along%20the%20same%20three%20axes.%0A%20%20We%20apply%20this%20framework%20to%20the%20indirect%20object%20identification%20%28IOI%29%20task%0Ausing%20GPT-2%20Small%2C%20with%20sparse%20autoencoders%20%28SAEs%29%20trained%20on%20either%20the%20IOI%20or%0AOpenWebText%20datasets.%20We%20find%20that%20these%20SAEs%20capture%20interpretable%20features%0Afor%20the%20IOI%20task%2C%20but%20they%20are%20less%20successful%20than%20supervised%20features%20in%0Acontrolling%20the%20model.%20Finally%2C%20we%20observe%20two%20qualitative%20phenomena%20in%20SAE%0Atraining%3A%20feature%20occlusion%20%28where%20a%20causally%20relevant%20concept%20is%20robustly%0Aovershadowed%20by%20even%20slightly%20higher-magnitude%20ones%20in%20the%20learned%20features%29%2C%0Aand%20feature%20over-splitting%20%28where%20binary%20features%20split%20into%20many%20smaller%2C%20less%0Ainterpretable%20features%29.%20We%20hope%20that%20our%20framework%20will%20provide%20a%20useful%20step%0Atowards%20more%20objective%20and%20grounded%20evaluations%20of%20sparse%20dictionary%20learning%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08366v3&entry.124074799=Read"},
{"title": "Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models\n  Using Spatio-Temporal Slices", "author": "Nathaniel Cohen and Vladimir Kulikov and Matan Kleiner and Inbar Huberman-Spiegelglas and Tomer Michaeli", "abstract": "  Text-to-image (T2I) diffusion models achieve state-of-the-art results in\nimage synthesis and editing. However, leveraging such pretrained models for\nvideo editing is considered a major challenge. Many existing works attempt to\nenforce temporal consistency in the edited video through explicit\ncorrespondence mechanisms, either in pixel space or between deep features.\nThese methods, however, struggle with strong nonrigid motion. In this paper, we\nintroduce a fundamentally different approach, which is based on the observation\nthat spatiotemporal slices of natural videos exhibit similar characteristics to\nnatural images. Thus, the same T2I diffusion model that is normally used only\nas a prior on video frames, can also serve as a strong prior for enhancing\ntemporal consistency by applying it on spatiotemporal slices. Based on this\nobservation, we present Slicedit, a method for text-based video editing that\nutilizes a pretrained T2I diffusion model to process both spatial and\nspatiotemporal slices. Our method generates videos that retain the structure\nand motion of the original video while adhering to the target text. Through\nextensive experiments, we demonstrate Slicedit's ability to edit a wide range\nof real-world videos, confirming its clear advantages compared to existing\ncompeting methods. Webpage: https://matankleiner.github.io/slicedit/\n", "link": "http://arxiv.org/abs/2405.12211v1", "date": "2024-05-20", "relevancy": 2.5442, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7237}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6441}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slicedit%3A%20Zero-Shot%20Video%20Editing%20With%20Text-to-Image%20Diffusion%20Models%0A%20%20Using%20Spatio-Temporal%20Slices&body=Title%3A%20Slicedit%3A%20Zero-Shot%20Video%20Editing%20With%20Text-to-Image%20Diffusion%20Models%0A%20%20Using%20Spatio-Temporal%20Slices%0AAuthor%3A%20Nathaniel%20Cohen%20and%20Vladimir%20Kulikov%20and%20Matan%20Kleiner%20and%20Inbar%20Huberman-Spiegelglas%20and%20Tomer%20Michaeli%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20achieve%20state-of-the-art%20results%20in%0Aimage%20synthesis%20and%20editing.%20However%2C%20leveraging%20such%20pretrained%20models%20for%0Avideo%20editing%20is%20considered%20a%20major%20challenge.%20Many%20existing%20works%20attempt%20to%0Aenforce%20temporal%20consistency%20in%20the%20edited%20video%20through%20explicit%0Acorrespondence%20mechanisms%2C%20either%20in%20pixel%20space%20or%20between%20deep%20features.%0AThese%20methods%2C%20however%2C%20struggle%20with%20strong%20nonrigid%20motion.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20fundamentally%20different%20approach%2C%20which%20is%20based%20on%20the%20observation%0Athat%20spatiotemporal%20slices%20of%20natural%20videos%20exhibit%20similar%20characteristics%20to%0Anatural%20images.%20Thus%2C%20the%20same%20T2I%20diffusion%20model%20that%20is%20normally%20used%20only%0Aas%20a%20prior%20on%20video%20frames%2C%20can%20also%20serve%20as%20a%20strong%20prior%20for%20enhancing%0Atemporal%20consistency%20by%20applying%20it%20on%20spatiotemporal%20slices.%20Based%20on%20this%0Aobservation%2C%20we%20present%20Slicedit%2C%20a%20method%20for%20text-based%20video%20editing%20that%0Autilizes%20a%20pretrained%20T2I%20diffusion%20model%20to%20process%20both%20spatial%20and%0Aspatiotemporal%20slices.%20Our%20method%20generates%20videos%20that%20retain%20the%20structure%0Aand%20motion%20of%20the%20original%20video%20while%20adhering%20to%20the%20target%20text.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20Slicedit%27s%20ability%20to%20edit%20a%20wide%20range%0Aof%20real-world%20videos%2C%20confirming%20its%20clear%20advantages%20compared%20to%20existing%0Acompeting%20methods.%20Webpage%3A%20https%3A//matankleiner.github.io/slicedit/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlicedit%253A%2520Zero-Shot%2520Video%2520Editing%2520With%2520Text-to-Image%2520Diffusion%2520Models%250A%2520%2520Using%2520Spatio-Temporal%2520Slices%26entry.906535625%3DNathaniel%2520Cohen%2520and%2520Vladimir%2520Kulikov%2520and%2520Matan%2520Kleiner%2520and%2520Inbar%2520Huberman-Spiegelglas%2520and%2520Tomer%2520Michaeli%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520achieve%2520state-of-the-art%2520results%2520in%250Aimage%2520synthesis%2520and%2520editing.%2520However%252C%2520leveraging%2520such%2520pretrained%2520models%2520for%250Avideo%2520editing%2520is%2520considered%2520a%2520major%2520challenge.%2520Many%2520existing%2520works%2520attempt%2520to%250Aenforce%2520temporal%2520consistency%2520in%2520the%2520edited%2520video%2520through%2520explicit%250Acorrespondence%2520mechanisms%252C%2520either%2520in%2520pixel%2520space%2520or%2520between%2520deep%2520features.%250AThese%2520methods%252C%2520however%252C%2520struggle%2520with%2520strong%2520nonrigid%2520motion.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520fundamentally%2520different%2520approach%252C%2520which%2520is%2520based%2520on%2520the%2520observation%250Athat%2520spatiotemporal%2520slices%2520of%2520natural%2520videos%2520exhibit%2520similar%2520characteristics%2520to%250Anatural%2520images.%2520Thus%252C%2520the%2520same%2520T2I%2520diffusion%2520model%2520that%2520is%2520normally%2520used%2520only%250Aas%2520a%2520prior%2520on%2520video%2520frames%252C%2520can%2520also%2520serve%2520as%2520a%2520strong%2520prior%2520for%2520enhancing%250Atemporal%2520consistency%2520by%2520applying%2520it%2520on%2520spatiotemporal%2520slices.%2520Based%2520on%2520this%250Aobservation%252C%2520we%2520present%2520Slicedit%252C%2520a%2520method%2520for%2520text-based%2520video%2520editing%2520that%250Autilizes%2520a%2520pretrained%2520T2I%2520diffusion%2520model%2520to%2520process%2520both%2520spatial%2520and%250Aspatiotemporal%2520slices.%2520Our%2520method%2520generates%2520videos%2520that%2520retain%2520the%2520structure%250Aand%2520motion%2520of%2520the%2520original%2520video%2520while%2520adhering%2520to%2520the%2520target%2520text.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520Slicedit%2527s%2520ability%2520to%2520edit%2520a%2520wide%2520range%250Aof%2520real-world%2520videos%252C%2520confirming%2520its%2520clear%2520advantages%2520compared%2520to%2520existing%250Acompeting%2520methods.%2520Webpage%253A%2520https%253A//matankleiner.github.io/slicedit/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slicedit%3A%20Zero-Shot%20Video%20Editing%20With%20Text-to-Image%20Diffusion%20Models%0A%20%20Using%20Spatio-Temporal%20Slices&entry.906535625=Nathaniel%20Cohen%20and%20Vladimir%20Kulikov%20and%20Matan%20Kleiner%20and%20Inbar%20Huberman-Spiegelglas%20and%20Tomer%20Michaeli&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20achieve%20state-of-the-art%20results%20in%0Aimage%20synthesis%20and%20editing.%20However%2C%20leveraging%20such%20pretrained%20models%20for%0Avideo%20editing%20is%20considered%20a%20major%20challenge.%20Many%20existing%20works%20attempt%20to%0Aenforce%20temporal%20consistency%20in%20the%20edited%20video%20through%20explicit%0Acorrespondence%20mechanisms%2C%20either%20in%20pixel%20space%20or%20between%20deep%20features.%0AThese%20methods%2C%20however%2C%20struggle%20with%20strong%20nonrigid%20motion.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20fundamentally%20different%20approach%2C%20which%20is%20based%20on%20the%20observation%0Athat%20spatiotemporal%20slices%20of%20natural%20videos%20exhibit%20similar%20characteristics%20to%0Anatural%20images.%20Thus%2C%20the%20same%20T2I%20diffusion%20model%20that%20is%20normally%20used%20only%0Aas%20a%20prior%20on%20video%20frames%2C%20can%20also%20serve%20as%20a%20strong%20prior%20for%20enhancing%0Atemporal%20consistency%20by%20applying%20it%20on%20spatiotemporal%20slices.%20Based%20on%20this%0Aobservation%2C%20we%20present%20Slicedit%2C%20a%20method%20for%20text-based%20video%20editing%20that%0Autilizes%20a%20pretrained%20T2I%20diffusion%20model%20to%20process%20both%20spatial%20and%0Aspatiotemporal%20slices.%20Our%20method%20generates%20videos%20that%20retain%20the%20structure%0Aand%20motion%20of%20the%20original%20video%20while%20adhering%20to%20the%20target%20text.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20Slicedit%27s%20ability%20to%20edit%20a%20wide%20range%0Aof%20real-world%20videos%2C%20confirming%20its%20clear%20advantages%20compared%20to%20existing%0Acompeting%20methods.%20Webpage%3A%20https%3A//matankleiner.github.io/slicedit/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12211v1&entry.124074799=Read"},
{"title": "Distributed agency in second language learning and teaching through\n  generative AI", "author": "Robert Godwin-Jones", "abstract": "  Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.\n", "link": "http://arxiv.org/abs/2403.20216v2", "date": "2024-05-20", "relevancy": 2.5391, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5571}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4979}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&body=Title%3A%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI%0AAuthor%3A%20Robert%20Godwin-Jones%0AAbstract%3A%20%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520agency%2520in%2520second%2520language%2520learning%2520and%2520teaching%2520through%250A%2520%2520generative%2520AI%26entry.906535625%3DRobert%2520Godwin-Jones%26entry.1292438233%3D%2520%2520Generative%2520AI%2520offers%2520significant%2520opportunities%2520for%2520language%2520learning.%2520Tools%250Alike%2520ChatGPT%2520can%2520provide%2520informal%2520second%2520language%2520practice%2520through%2520chats%2520in%250Awritten%2520or%2520voice%2520forms%252C%2520with%2520the%2520learner%2520specifying%2520through%2520prompts%250Aconversational%2520parameters%2520such%2520as%2520proficiency%2520level%252C%2520language%2520register%252C%2520and%250Adiscussion%2520topics.%2520AI%2520can%2520be%2520instructed%2520to%2520give%2520corrective%2520feedback%252C%2520create%250Apractice%2520exercises%252C%2520or%2520develop%2520an%2520extended%2520study%2520plan.%2520Instructors%2520can%2520use%2520AI%250Ato%2520build%2520learning%2520and%2520assessment%2520materials%2520in%2520a%2520variety%2520of%2520media.%2520AI%2520is%2520likely%250Ato%2520make%2520immersive%2520technologies%2520more%2520powerful%2520and%2520versatile%252C%2520moving%2520away%2520from%250Ascripted%2520interactions.%2520For%2520both%2520learners%2520and%2520teachers%252C%2520it%2520is%2520important%2520to%250Aunderstand%2520the%2520limitations%2520of%2520AI%2520systems%2520that%2520arise%2520from%2520their%2520purely%250Astatistical%2520model%2520of%2520human%2520language%252C%2520which%2520limits%2520their%2520ability%2520to%2520deal%2520with%250Anuanced%2520social%2520and%2520cultural%2520aspects%2520of%2520language%2520use.%2520Additionally%252C%2520there%2520are%250Aethical%2520concerns%2520over%2520how%2520AI%2520systems%2520are%2520created%2520as%2520well%2520as%2520practical%250Aconstraints%2520in%2520their%2520use%252C%2520especially%2520for%2520less%2520privileged%2520populations.%2520The%2520power%250Aand%2520versatility%2520of%2520AI%2520tools%2520are%2520likely%2520to%2520turn%2520them%2520into%2520valuable%2520and%2520constant%250Acompanions%2520in%2520many%2520peoples%2520lives%2520%2528akin%2520to%2520smartphones%2529%252C%2520creating%2520a%2520close%250Aconnection%2520that%2520goes%2520beyond%2520simple%2520tool%2520use.%2520Ecological%2520theories%2520such%2520as%250Asociomaterialism%2520are%2520helpful%2520in%2520examining%2520the%2520shared%2520agency%2520that%2520develops%250Athrough%2520close%2520user-AI%2520interactions%252C%2520as%2520are%2520the%2520perspectives%2520on%2520human-object%250Arelations%2520from%2520Indigenous%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&entry.906535625=Robert%20Godwin-Jones&entry.1292438233=%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20216v2&entry.124074799=Read"},
{"title": "GIST: Generated Inputs Sets Transferability in Deep Learning", "author": "Florian Tambon and Foutse Khomh and Giuliano Antoniol", "abstract": "  To foster the verifiability and testability of Deep Neural Networks (DNN), an\nincreasing number of methods for test case generation techniques are being\ndeveloped.\n  When confronted with testing DNN models, the user can apply any existing test\ngeneration technique. However, it needs to do so for each technique and each\nDNN model under test, which can be expensive. Therefore, a paradigm shift could\nbenefit this testing process: rather than regenerating the test set\nindependently for each DNN model under test, we could transfer from existing\nDNN models.\n  This paper introduces GIST (Generated Inputs Sets Transferability), a novel\napproach for the efficient transfer of test sets. Given a property selected by\na user (e.g., neurons covered, faults), GIST enables the selection of good test\nsets from the point of view of this property among available test sets. This\nallows the user to recover similar properties on the transferred test sets as\nhe would have obtained by generating the test set from scratch with a test\ncases generation technique. Experimental results show that GIST can select\neffective test sets for the given property to transfer. Moreover, GIST scales\nbetter than reapplying test case generation techniques from scratch on DNN\nmodels under test.\n", "link": "http://arxiv.org/abs/2311.00801v3", "date": "2024-05-20", "relevancy": 2.5186, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5213}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5043}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIST%3A%20Generated%20Inputs%20Sets%20Transferability%20in%20Deep%20Learning&body=Title%3A%20GIST%3A%20Generated%20Inputs%20Sets%20Transferability%20in%20Deep%20Learning%0AAuthor%3A%20Florian%20Tambon%20and%20Foutse%20Khomh%20and%20Giuliano%20Antoniol%0AAbstract%3A%20%20%20To%20foster%20the%20verifiability%20and%20testability%20of%20Deep%20Neural%20Networks%20%28DNN%29%2C%20an%0Aincreasing%20number%20of%20methods%20for%20test%20case%20generation%20techniques%20are%20being%0Adeveloped.%0A%20%20When%20confronted%20with%20testing%20DNN%20models%2C%20the%20user%20can%20apply%20any%20existing%20test%0Ageneration%20technique.%20However%2C%20it%20needs%20to%20do%20so%20for%20each%20technique%20and%20each%0ADNN%20model%20under%20test%2C%20which%20can%20be%20expensive.%20Therefore%2C%20a%20paradigm%20shift%20could%0Abenefit%20this%20testing%20process%3A%20rather%20than%20regenerating%20the%20test%20set%0Aindependently%20for%20each%20DNN%20model%20under%20test%2C%20we%20could%20transfer%20from%20existing%0ADNN%20models.%0A%20%20This%20paper%20introduces%20GIST%20%28Generated%20Inputs%20Sets%20Transferability%29%2C%20a%20novel%0Aapproach%20for%20the%20efficient%20transfer%20of%20test%20sets.%20Given%20a%20property%20selected%20by%0Aa%20user%20%28e.g.%2C%20neurons%20covered%2C%20faults%29%2C%20GIST%20enables%20the%20selection%20of%20good%20test%0Asets%20from%20the%20point%20of%20view%20of%20this%20property%20among%20available%20test%20sets.%20This%0Aallows%20the%20user%20to%20recover%20similar%20properties%20on%20the%20transferred%20test%20sets%20as%0Ahe%20would%20have%20obtained%20by%20generating%20the%20test%20set%20from%20scratch%20with%20a%20test%0Acases%20generation%20technique.%20Experimental%20results%20show%20that%20GIST%20can%20select%0Aeffective%20test%20sets%20for%20the%20given%20property%20to%20transfer.%20Moreover%2C%20GIST%20scales%0Abetter%20than%20reapplying%20test%20case%20generation%20techniques%20from%20scratch%20on%20DNN%0Amodels%20under%20test.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00801v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIST%253A%2520Generated%2520Inputs%2520Sets%2520Transferability%2520in%2520Deep%2520Learning%26entry.906535625%3DFlorian%2520Tambon%2520and%2520Foutse%2520Khomh%2520and%2520Giuliano%2520Antoniol%26entry.1292438233%3D%2520%2520To%2520foster%2520the%2520verifiability%2520and%2520testability%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529%252C%2520an%250Aincreasing%2520number%2520of%2520methods%2520for%2520test%2520case%2520generation%2520techniques%2520are%2520being%250Adeveloped.%250A%2520%2520When%2520confronted%2520with%2520testing%2520DNN%2520models%252C%2520the%2520user%2520can%2520apply%2520any%2520existing%2520test%250Ageneration%2520technique.%2520However%252C%2520it%2520needs%2520to%2520do%2520so%2520for%2520each%2520technique%2520and%2520each%250ADNN%2520model%2520under%2520test%252C%2520which%2520can%2520be%2520expensive.%2520Therefore%252C%2520a%2520paradigm%2520shift%2520could%250Abenefit%2520this%2520testing%2520process%253A%2520rather%2520than%2520regenerating%2520the%2520test%2520set%250Aindependently%2520for%2520each%2520DNN%2520model%2520under%2520test%252C%2520we%2520could%2520transfer%2520from%2520existing%250ADNN%2520models.%250A%2520%2520This%2520paper%2520introduces%2520GIST%2520%2528Generated%2520Inputs%2520Sets%2520Transferability%2529%252C%2520a%2520novel%250Aapproach%2520for%2520the%2520efficient%2520transfer%2520of%2520test%2520sets.%2520Given%2520a%2520property%2520selected%2520by%250Aa%2520user%2520%2528e.g.%252C%2520neurons%2520covered%252C%2520faults%2529%252C%2520GIST%2520enables%2520the%2520selection%2520of%2520good%2520test%250Asets%2520from%2520the%2520point%2520of%2520view%2520of%2520this%2520property%2520among%2520available%2520test%2520sets.%2520This%250Aallows%2520the%2520user%2520to%2520recover%2520similar%2520properties%2520on%2520the%2520transferred%2520test%2520sets%2520as%250Ahe%2520would%2520have%2520obtained%2520by%2520generating%2520the%2520test%2520set%2520from%2520scratch%2520with%2520a%2520test%250Acases%2520generation%2520technique.%2520Experimental%2520results%2520show%2520that%2520GIST%2520can%2520select%250Aeffective%2520test%2520sets%2520for%2520the%2520given%2520property%2520to%2520transfer.%2520Moreover%252C%2520GIST%2520scales%250Abetter%2520than%2520reapplying%2520test%2520case%2520generation%2520techniques%2520from%2520scratch%2520on%2520DNN%250Amodels%2520under%2520test.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00801v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIST%3A%20Generated%20Inputs%20Sets%20Transferability%20in%20Deep%20Learning&entry.906535625=Florian%20Tambon%20and%20Foutse%20Khomh%20and%20Giuliano%20Antoniol&entry.1292438233=%20%20To%20foster%20the%20verifiability%20and%20testability%20of%20Deep%20Neural%20Networks%20%28DNN%29%2C%20an%0Aincreasing%20number%20of%20methods%20for%20test%20case%20generation%20techniques%20are%20being%0Adeveloped.%0A%20%20When%20confronted%20with%20testing%20DNN%20models%2C%20the%20user%20can%20apply%20any%20existing%20test%0Ageneration%20technique.%20However%2C%20it%20needs%20to%20do%20so%20for%20each%20technique%20and%20each%0ADNN%20model%20under%20test%2C%20which%20can%20be%20expensive.%20Therefore%2C%20a%20paradigm%20shift%20could%0Abenefit%20this%20testing%20process%3A%20rather%20than%20regenerating%20the%20test%20set%0Aindependently%20for%20each%20DNN%20model%20under%20test%2C%20we%20could%20transfer%20from%20existing%0ADNN%20models.%0A%20%20This%20paper%20introduces%20GIST%20%28Generated%20Inputs%20Sets%20Transferability%29%2C%20a%20novel%0Aapproach%20for%20the%20efficient%20transfer%20of%20test%20sets.%20Given%20a%20property%20selected%20by%0Aa%20user%20%28e.g.%2C%20neurons%20covered%2C%20faults%29%2C%20GIST%20enables%20the%20selection%20of%20good%20test%0Asets%20from%20the%20point%20of%20view%20of%20this%20property%20among%20available%20test%20sets.%20This%0Aallows%20the%20user%20to%20recover%20similar%20properties%20on%20the%20transferred%20test%20sets%20as%0Ahe%20would%20have%20obtained%20by%20generating%20the%20test%20set%20from%20scratch%20with%20a%20test%0Acases%20generation%20technique.%20Experimental%20results%20show%20that%20GIST%20can%20select%0Aeffective%20test%20sets%20for%20the%20given%20property%20to%20transfer.%20Moreover%2C%20GIST%20scales%0Abetter%20than%20reapplying%20test%20case%20generation%20techniques%20from%20scratch%20on%20DNN%0Amodels%20under%20test.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00801v3&entry.124074799=Read"},
{"title": "Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in\n  Dynamic Environments", "author": "Lukas Schmid and Marcus Abate and Yun Chang and Luca Carlone", "abstract": "  Perceiving and understanding highly dynamic and changing environments is a\ncrucial capability for robot autonomy. While large strides have been made\ntowards developing dynamic SLAM approaches that estimate the robot pose\naccurately, a lesser emphasis has been put on the construction of dense\nspatio-temporal representations of the robot environment. A detailed\nunderstanding of the scene and its evolution through time is crucial for\nlong-term robot autonomy and essential to tasks that require long-term\nreasoning, such as operating effectively in environments shared with humans and\nother agents and thus are subject to short and long-term dynamics. To address\nthis challenge, this work defines the Spatio-temporal Metric-semantic SLAM\n(SMS) problem, and presents a framework to factorize and solve it efficiently.\nWe show that the proposed factorization suggests a natural organization of a\nspatio-temporal perception system, where a fast process tracks short-term\ndynamics in an active temporal window, while a slower process reasons over\nlong-term changes in the environment using a factor graph formulation. We\nprovide an efficient implementation of the proposed spatio-temporal perception\napproach, that we call Khronos, and show that it unifies exiting\ninterpretations of short-term and long-term dynamics and is able to construct a\ndense spatio-temporal map in real-time. We provide simulated and real results,\nshowing that the spatio-temporal maps built by Khronos are an accurate\nreflection of a 3D scene over time and that Khronos outperforms baselines\nacross multiple metrics. We further validate our approach on two heterogeneous\nrobots in challenging, large-scale real-world environments.\n", "link": "http://arxiv.org/abs/2402.13817v2", "date": "2024-05-20", "relevancy": 2.4496, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Khronos%3A%20A%20Unified%20Approach%20for%20Spatio-Temporal%20Metric-Semantic%20SLAM%20in%0A%20%20Dynamic%20Environments&body=Title%3A%20Khronos%3A%20A%20Unified%20Approach%20for%20Spatio-Temporal%20Metric-Semantic%20SLAM%20in%0A%20%20Dynamic%20Environments%0AAuthor%3A%20Lukas%20Schmid%20and%20Marcus%20Abate%20and%20Yun%20Chang%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20Perceiving%20and%20understanding%20highly%20dynamic%20and%20changing%20environments%20is%20a%0Acrucial%20capability%20for%20robot%20autonomy.%20While%20large%20strides%20have%20been%20made%0Atowards%20developing%20dynamic%20SLAM%20approaches%20that%20estimate%20the%20robot%20pose%0Aaccurately%2C%20a%20lesser%20emphasis%20has%20been%20put%20on%20the%20construction%20of%20dense%0Aspatio-temporal%20representations%20of%20the%20robot%20environment.%20A%20detailed%0Aunderstanding%20of%20the%20scene%20and%20its%20evolution%20through%20time%20is%20crucial%20for%0Along-term%20robot%20autonomy%20and%20essential%20to%20tasks%20that%20require%20long-term%0Areasoning%2C%20such%20as%20operating%20effectively%20in%20environments%20shared%20with%20humans%20and%0Aother%20agents%20and%20thus%20are%20subject%20to%20short%20and%20long-term%20dynamics.%20To%20address%0Athis%20challenge%2C%20this%20work%20defines%20the%20Spatio-temporal%20Metric-semantic%20SLAM%0A%28SMS%29%20problem%2C%20and%20presents%20a%20framework%20to%20factorize%20and%20solve%20it%20efficiently.%0AWe%20show%20that%20the%20proposed%20factorization%20suggests%20a%20natural%20organization%20of%20a%0Aspatio-temporal%20perception%20system%2C%20where%20a%20fast%20process%20tracks%20short-term%0Adynamics%20in%20an%20active%20temporal%20window%2C%20while%20a%20slower%20process%20reasons%20over%0Along-term%20changes%20in%20the%20environment%20using%20a%20factor%20graph%20formulation.%20We%0Aprovide%20an%20efficient%20implementation%20of%20the%20proposed%20spatio-temporal%20perception%0Aapproach%2C%20that%20we%20call%20Khronos%2C%20and%20show%20that%20it%20unifies%20exiting%0Ainterpretations%20of%20short-term%20and%20long-term%20dynamics%20and%20is%20able%20to%20construct%20a%0Adense%20spatio-temporal%20map%20in%20real-time.%20We%20provide%20simulated%20and%20real%20results%2C%0Ashowing%20that%20the%20spatio-temporal%20maps%20built%20by%20Khronos%20are%20an%20accurate%0Areflection%20of%20a%203D%20scene%20over%20time%20and%20that%20Khronos%20outperforms%20baselines%0Aacross%20multiple%20metrics.%20We%20further%20validate%20our%20approach%20on%20two%20heterogeneous%0Arobots%20in%20challenging%2C%20large-scale%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKhronos%253A%2520A%2520Unified%2520Approach%2520for%2520Spatio-Temporal%2520Metric-Semantic%2520SLAM%2520in%250A%2520%2520Dynamic%2520Environments%26entry.906535625%3DLukas%2520Schmid%2520and%2520Marcus%2520Abate%2520and%2520Yun%2520Chang%2520and%2520Luca%2520Carlone%26entry.1292438233%3D%2520%2520Perceiving%2520and%2520understanding%2520highly%2520dynamic%2520and%2520changing%2520environments%2520is%2520a%250Acrucial%2520capability%2520for%2520robot%2520autonomy.%2520While%2520large%2520strides%2520have%2520been%2520made%250Atowards%2520developing%2520dynamic%2520SLAM%2520approaches%2520that%2520estimate%2520the%2520robot%2520pose%250Aaccurately%252C%2520a%2520lesser%2520emphasis%2520has%2520been%2520put%2520on%2520the%2520construction%2520of%2520dense%250Aspatio-temporal%2520representations%2520of%2520the%2520robot%2520environment.%2520A%2520detailed%250Aunderstanding%2520of%2520the%2520scene%2520and%2520its%2520evolution%2520through%2520time%2520is%2520crucial%2520for%250Along-term%2520robot%2520autonomy%2520and%2520essential%2520to%2520tasks%2520that%2520require%2520long-term%250Areasoning%252C%2520such%2520as%2520operating%2520effectively%2520in%2520environments%2520shared%2520with%2520humans%2520and%250Aother%2520agents%2520and%2520thus%2520are%2520subject%2520to%2520short%2520and%2520long-term%2520dynamics.%2520To%2520address%250Athis%2520challenge%252C%2520this%2520work%2520defines%2520the%2520Spatio-temporal%2520Metric-semantic%2520SLAM%250A%2528SMS%2529%2520problem%252C%2520and%2520presents%2520a%2520framework%2520to%2520factorize%2520and%2520solve%2520it%2520efficiently.%250AWe%2520show%2520that%2520the%2520proposed%2520factorization%2520suggests%2520a%2520natural%2520organization%2520of%2520a%250Aspatio-temporal%2520perception%2520system%252C%2520where%2520a%2520fast%2520process%2520tracks%2520short-term%250Adynamics%2520in%2520an%2520active%2520temporal%2520window%252C%2520while%2520a%2520slower%2520process%2520reasons%2520over%250Along-term%2520changes%2520in%2520the%2520environment%2520using%2520a%2520factor%2520graph%2520formulation.%2520We%250Aprovide%2520an%2520efficient%2520implementation%2520of%2520the%2520proposed%2520spatio-temporal%2520perception%250Aapproach%252C%2520that%2520we%2520call%2520Khronos%252C%2520and%2520show%2520that%2520it%2520unifies%2520exiting%250Ainterpretations%2520of%2520short-term%2520and%2520long-term%2520dynamics%2520and%2520is%2520able%2520to%2520construct%2520a%250Adense%2520spatio-temporal%2520map%2520in%2520real-time.%2520We%2520provide%2520simulated%2520and%2520real%2520results%252C%250Ashowing%2520that%2520the%2520spatio-temporal%2520maps%2520built%2520by%2520Khronos%2520are%2520an%2520accurate%250Areflection%2520of%2520a%25203D%2520scene%2520over%2520time%2520and%2520that%2520Khronos%2520outperforms%2520baselines%250Aacross%2520multiple%2520metrics.%2520We%2520further%2520validate%2520our%2520approach%2520on%2520two%2520heterogeneous%250Arobots%2520in%2520challenging%252C%2520large-scale%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Khronos%3A%20A%20Unified%20Approach%20for%20Spatio-Temporal%20Metric-Semantic%20SLAM%20in%0A%20%20Dynamic%20Environments&entry.906535625=Lukas%20Schmid%20and%20Marcus%20Abate%20and%20Yun%20Chang%20and%20Luca%20Carlone&entry.1292438233=%20%20Perceiving%20and%20understanding%20highly%20dynamic%20and%20changing%20environments%20is%20a%0Acrucial%20capability%20for%20robot%20autonomy.%20While%20large%20strides%20have%20been%20made%0Atowards%20developing%20dynamic%20SLAM%20approaches%20that%20estimate%20the%20robot%20pose%0Aaccurately%2C%20a%20lesser%20emphasis%20has%20been%20put%20on%20the%20construction%20of%20dense%0Aspatio-temporal%20representations%20of%20the%20robot%20environment.%20A%20detailed%0Aunderstanding%20of%20the%20scene%20and%20its%20evolution%20through%20time%20is%20crucial%20for%0Along-term%20robot%20autonomy%20and%20essential%20to%20tasks%20that%20require%20long-term%0Areasoning%2C%20such%20as%20operating%20effectively%20in%20environments%20shared%20with%20humans%20and%0Aother%20agents%20and%20thus%20are%20subject%20to%20short%20and%20long-term%20dynamics.%20To%20address%0Athis%20challenge%2C%20this%20work%20defines%20the%20Spatio-temporal%20Metric-semantic%20SLAM%0A%28SMS%29%20problem%2C%20and%20presents%20a%20framework%20to%20factorize%20and%20solve%20it%20efficiently.%0AWe%20show%20that%20the%20proposed%20factorization%20suggests%20a%20natural%20organization%20of%20a%0Aspatio-temporal%20perception%20system%2C%20where%20a%20fast%20process%20tracks%20short-term%0Adynamics%20in%20an%20active%20temporal%20window%2C%20while%20a%20slower%20process%20reasons%20over%0Along-term%20changes%20in%20the%20environment%20using%20a%20factor%20graph%20formulation.%20We%0Aprovide%20an%20efficient%20implementation%20of%20the%20proposed%20spatio-temporal%20perception%0Aapproach%2C%20that%20we%20call%20Khronos%2C%20and%20show%20that%20it%20unifies%20exiting%0Ainterpretations%20of%20short-term%20and%20long-term%20dynamics%20and%20is%20able%20to%20construct%20a%0Adense%20spatio-temporal%20map%20in%20real-time.%20We%20provide%20simulated%20and%20real%20results%2C%0Ashowing%20that%20the%20spatio-temporal%20maps%20built%20by%20Khronos%20are%20an%20accurate%0Areflection%20of%20a%203D%20scene%20over%20time%20and%20that%20Khronos%20outperforms%20baselines%0Aacross%20multiple%20metrics.%20We%20further%20validate%20our%20approach%20on%20two%20heterogeneous%0Arobots%20in%20challenging%2C%20large-scale%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13817v2&entry.124074799=Read"},
{"title": "Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov\n  Optimization Approach", "author": "Chung-Hsuan Hu and Zheng Chen and Erik G. Larsson", "abstract": "  Federated learning (FL) has received significant attention in recent years\nfor its advantages in efficient training of machine learning models across\ndistributed clients without disclosing user-sensitive data. Specifically, in\nfederated edge learning (FEEL) systems, the time-varying nature of wireless\nchannels introduces inevitable system dynamics in the communication process,\nthereby affecting training latency and energy consumption. In this work, we\nfurther consider a streaming data scenario where new training data samples are\nrandomly generated over time at edge devices. Our goal is to develop a dynamic\nscheduling and resource allocation algorithm to address the inherent randomness\nin data arrivals and resource availability under long-term energy constraints.\nTo achieve this, we formulate a stochastic network optimization problem and use\nthe Lyapunov drift-plus-penalty framework to obtain a dynamic resource\nmanagement design. Our proposed algorithm makes adaptive decisions on device\nscheduling, computational capacity adjustment, and allocation of bandwidth and\ntransmit power in every round. We provide convergence analysis for the\nconsidered setting with heterogeneous data and time-varying objective\nfunctions, which supports the rationale behind our proposed scheduling design.\nThe effectiveness of our scheme is verified through simulation results,\ndemonstrating improved learning performance and energy efficiency as compared\nto baseline schemes.\n", "link": "http://arxiv.org/abs/2405.12046v1", "date": "2024-05-20", "relevancy": 2.3961, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5173}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4656}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Federated%20Edge%20Learning%20with%20Streaming%20Data%3A%20A%20Lyapunov%0A%20%20Optimization%20Approach&body=Title%3A%20Energy-Efficient%20Federated%20Edge%20Learning%20with%20Streaming%20Data%3A%20A%20Lyapunov%0A%20%20Optimization%20Approach%0AAuthor%3A%20Chung-Hsuan%20Hu%20and%20Zheng%20Chen%20and%20Erik%20G.%20Larsson%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20received%20significant%20attention%20in%20recent%20years%0Afor%20its%20advantages%20in%20efficient%20training%20of%20machine%20learning%20models%20across%0Adistributed%20clients%20without%20disclosing%20user-sensitive%20data.%20Specifically%2C%20in%0Afederated%20edge%20learning%20%28FEEL%29%20systems%2C%20the%20time-varying%20nature%20of%20wireless%0Achannels%20introduces%20inevitable%20system%20dynamics%20in%20the%20communication%20process%2C%0Athereby%20affecting%20training%20latency%20and%20energy%20consumption.%20In%20this%20work%2C%20we%0Afurther%20consider%20a%20streaming%20data%20scenario%20where%20new%20training%20data%20samples%20are%0Arandomly%20generated%20over%20time%20at%20edge%20devices.%20Our%20goal%20is%20to%20develop%20a%20dynamic%0Ascheduling%20and%20resource%20allocation%20algorithm%20to%20address%20the%20inherent%20randomness%0Ain%20data%20arrivals%20and%20resource%20availability%20under%20long-term%20energy%20constraints.%0ATo%20achieve%20this%2C%20we%20formulate%20a%20stochastic%20network%20optimization%20problem%20and%20use%0Athe%20Lyapunov%20drift-plus-penalty%20framework%20to%20obtain%20a%20dynamic%20resource%0Amanagement%20design.%20Our%20proposed%20algorithm%20makes%20adaptive%20decisions%20on%20device%0Ascheduling%2C%20computational%20capacity%20adjustment%2C%20and%20allocation%20of%20bandwidth%20and%0Atransmit%20power%20in%20every%20round.%20We%20provide%20convergence%20analysis%20for%20the%0Aconsidered%20setting%20with%20heterogeneous%20data%20and%20time-varying%20objective%0Afunctions%2C%20which%20supports%20the%20rationale%20behind%20our%20proposed%20scheduling%20design.%0AThe%20effectiveness%20of%20our%20scheme%20is%20verified%20through%20simulation%20results%2C%0Ademonstrating%20improved%20learning%20performance%20and%20energy%20efficiency%20as%20compared%0Ato%20baseline%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Federated%2520Edge%2520Learning%2520with%2520Streaming%2520Data%253A%2520A%2520Lyapunov%250A%2520%2520Optimization%2520Approach%26entry.906535625%3DChung-Hsuan%2520Hu%2520and%2520Zheng%2520Chen%2520and%2520Erik%2520G.%2520Larsson%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520received%2520significant%2520attention%2520in%2520recent%2520years%250Afor%2520its%2520advantages%2520in%2520efficient%2520training%2520of%2520machine%2520learning%2520models%2520across%250Adistributed%2520clients%2520without%2520disclosing%2520user-sensitive%2520data.%2520Specifically%252C%2520in%250Afederated%2520edge%2520learning%2520%2528FEEL%2529%2520systems%252C%2520the%2520time-varying%2520nature%2520of%2520wireless%250Achannels%2520introduces%2520inevitable%2520system%2520dynamics%2520in%2520the%2520communication%2520process%252C%250Athereby%2520affecting%2520training%2520latency%2520and%2520energy%2520consumption.%2520In%2520this%2520work%252C%2520we%250Afurther%2520consider%2520a%2520streaming%2520data%2520scenario%2520where%2520new%2520training%2520data%2520samples%2520are%250Arandomly%2520generated%2520over%2520time%2520at%2520edge%2520devices.%2520Our%2520goal%2520is%2520to%2520develop%2520a%2520dynamic%250Ascheduling%2520and%2520resource%2520allocation%2520algorithm%2520to%2520address%2520the%2520inherent%2520randomness%250Ain%2520data%2520arrivals%2520and%2520resource%2520availability%2520under%2520long-term%2520energy%2520constraints.%250ATo%2520achieve%2520this%252C%2520we%2520formulate%2520a%2520stochastic%2520network%2520optimization%2520problem%2520and%2520use%250Athe%2520Lyapunov%2520drift-plus-penalty%2520framework%2520to%2520obtain%2520a%2520dynamic%2520resource%250Amanagement%2520design.%2520Our%2520proposed%2520algorithm%2520makes%2520adaptive%2520decisions%2520on%2520device%250Ascheduling%252C%2520computational%2520capacity%2520adjustment%252C%2520and%2520allocation%2520of%2520bandwidth%2520and%250Atransmit%2520power%2520in%2520every%2520round.%2520We%2520provide%2520convergence%2520analysis%2520for%2520the%250Aconsidered%2520setting%2520with%2520heterogeneous%2520data%2520and%2520time-varying%2520objective%250Afunctions%252C%2520which%2520supports%2520the%2520rationale%2520behind%2520our%2520proposed%2520scheduling%2520design.%250AThe%2520effectiveness%2520of%2520our%2520scheme%2520is%2520verified%2520through%2520simulation%2520results%252C%250Ademonstrating%2520improved%2520learning%2520performance%2520and%2520energy%2520efficiency%2520as%2520compared%250Ato%2520baseline%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Federated%20Edge%20Learning%20with%20Streaming%20Data%3A%20A%20Lyapunov%0A%20%20Optimization%20Approach&entry.906535625=Chung-Hsuan%20Hu%20and%20Zheng%20Chen%20and%20Erik%20G.%20Larsson&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20received%20significant%20attention%20in%20recent%20years%0Afor%20its%20advantages%20in%20efficient%20training%20of%20machine%20learning%20models%20across%0Adistributed%20clients%20without%20disclosing%20user-sensitive%20data.%20Specifically%2C%20in%0Afederated%20edge%20learning%20%28FEEL%29%20systems%2C%20the%20time-varying%20nature%20of%20wireless%0Achannels%20introduces%20inevitable%20system%20dynamics%20in%20the%20communication%20process%2C%0Athereby%20affecting%20training%20latency%20and%20energy%20consumption.%20In%20this%20work%2C%20we%0Afurther%20consider%20a%20streaming%20data%20scenario%20where%20new%20training%20data%20samples%20are%0Arandomly%20generated%20over%20time%20at%20edge%20devices.%20Our%20goal%20is%20to%20develop%20a%20dynamic%0Ascheduling%20and%20resource%20allocation%20algorithm%20to%20address%20the%20inherent%20randomness%0Ain%20data%20arrivals%20and%20resource%20availability%20under%20long-term%20energy%20constraints.%0ATo%20achieve%20this%2C%20we%20formulate%20a%20stochastic%20network%20optimization%20problem%20and%20use%0Athe%20Lyapunov%20drift-plus-penalty%20framework%20to%20obtain%20a%20dynamic%20resource%0Amanagement%20design.%20Our%20proposed%20algorithm%20makes%20adaptive%20decisions%20on%20device%0Ascheduling%2C%20computational%20capacity%20adjustment%2C%20and%20allocation%20of%20bandwidth%20and%0Atransmit%20power%20in%20every%20round.%20We%20provide%20convergence%20analysis%20for%20the%0Aconsidered%20setting%20with%20heterogeneous%20data%20and%20time-varying%20objective%0Afunctions%2C%20which%20supports%20the%20rationale%20behind%20our%20proposed%20scheduling%20design.%0AThe%20effectiveness%20of%20our%20scheme%20is%20verified%20through%20simulation%20results%2C%0Ademonstrating%20improved%20learning%20performance%20and%20energy%20efficiency%20as%20compared%0Ato%20baseline%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12046v1&entry.124074799=Read"},
{"title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models", "author": "Junhao Zheng and Shengjie Qiu and Qianli Ma", "abstract": "  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.\n", "link": "http://arxiv.org/abs/2312.07887v2", "date": "2024-05-20", "relevancy": 2.3812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4773}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models&body=Title%3A%20Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models%0AAuthor%3A%20Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Qianli%20Ma%0AAbstract%3A%20%20%20Incremental%20Learning%20%28IL%29%20has%20been%20a%20long-standing%20problem%20in%20both%20vision%20and%0ANatural%20Language%20Processing%20%28NLP%29%20communities.%20In%20recent%20years%2C%20as%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%20have%20achieved%20remarkable%20progress%20in%20various%20NLP%0Adownstream%20tasks%2C%20utilizing%20PLMs%20as%20backbones%20has%20become%20a%20common%20practice%20in%0Arecent%20research%20of%20IL%20in%20NLP.%20Most%20assume%20that%20catastrophic%20forgetting%20is%20the%0Abiggest%20obstacle%20to%20achieving%20superior%20IL%20performance%20and%20propose%20various%0Atechniques%20to%20overcome%20this%20issue.%20However%2C%20we%20find%20that%20this%20assumption%20is%0Aproblematic.%20Specifically%2C%20we%20revisit%20more%20than%2020%20methods%20on%20four%0Aclassification%20tasks%20%28Text%20Classification%2C%20Intent%20Classification%2C%20Relation%0AExtraction%2C%20and%20Named%20Entity%20Recognition%29%20under%20the%20two%20most%20popular%20IL%0Asettings%20%28Class-Incremental%20and%20Task-Incremental%29%20and%20reveal%20that%20most%20of%20them%0Aseverely%20underestimate%20the%20inherent%20anti-forgetting%20ability%20of%20PLMs.%20Based%20on%0Athe%20observation%2C%20we%20propose%20a%20frustratingly%20easy%20method%20called%20SEQ%2A%20for%20IL%20with%0APLMs.%20The%20results%20show%20that%20SEQ%2A%20has%20competitive%20or%20superior%20performance%0Acompared%20to%20state-of-the-art%20%28SOTA%29%20IL%20methods%20and%20requires%20considerably%20less%0Atrainable%20parameters%20and%20training%20time.%20These%20findings%20urge%20us%20to%20revisit%20the%0AIL%20with%20PLMs%20and%20encourage%20future%20studies%20to%20have%20a%20fundamental%20understanding%0Aof%20the%20catastrophic%20forgetting%20in%20PLMs.%20The%20data%2C%20code%20and%20scripts%20are%20publicly%0Aavailable%20at%0Ahttps%3A//github.com/zzz47zzz/pretrained-lm-for-incremental-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520or%2520Recall%253F%2520Revisiting%2520Incremental%2520Learning%2520with%2520Pre-trained%250A%2520%2520Language%2520Models%26entry.906535625%3DJunhao%2520Zheng%2520and%2520Shengjie%2520Qiu%2520and%2520Qianli%2520Ma%26entry.1292438233%3D%2520%2520Incremental%2520Learning%2520%2528IL%2529%2520has%2520been%2520a%2520long-standing%2520problem%2520in%2520both%2520vision%2520and%250ANatural%2520Language%2520Processing%2520%2528NLP%2529%2520communities.%2520In%2520recent%2520years%252C%2520as%2520Pre-trained%250ALanguage%2520Models%2520%2528PLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520various%2520NLP%250Adownstream%2520tasks%252C%2520utilizing%2520PLMs%2520as%2520backbones%2520has%2520become%2520a%2520common%2520practice%2520in%250Arecent%2520research%2520of%2520IL%2520in%2520NLP.%2520Most%2520assume%2520that%2520catastrophic%2520forgetting%2520is%2520the%250Abiggest%2520obstacle%2520to%2520achieving%2520superior%2520IL%2520performance%2520and%2520propose%2520various%250Atechniques%2520to%2520overcome%2520this%2520issue.%2520However%252C%2520we%2520find%2520that%2520this%2520assumption%2520is%250Aproblematic.%2520Specifically%252C%2520we%2520revisit%2520more%2520than%252020%2520methods%2520on%2520four%250Aclassification%2520tasks%2520%2528Text%2520Classification%252C%2520Intent%2520Classification%252C%2520Relation%250AExtraction%252C%2520and%2520Named%2520Entity%2520Recognition%2529%2520under%2520the%2520two%2520most%2520popular%2520IL%250Asettings%2520%2528Class-Incremental%2520and%2520Task-Incremental%2529%2520and%2520reveal%2520that%2520most%2520of%2520them%250Aseverely%2520underestimate%2520the%2520inherent%2520anti-forgetting%2520ability%2520of%2520PLMs.%2520Based%2520on%250Athe%2520observation%252C%2520we%2520propose%2520a%2520frustratingly%2520easy%2520method%2520called%2520SEQ%252A%2520for%2520IL%2520with%250APLMs.%2520The%2520results%2520show%2520that%2520SEQ%252A%2520has%2520competitive%2520or%2520superior%2520performance%250Acompared%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520IL%2520methods%2520and%2520requires%2520considerably%2520less%250Atrainable%2520parameters%2520and%2520training%2520time.%2520These%2520findings%2520urge%2520us%2520to%2520revisit%2520the%250AIL%2520with%2520PLMs%2520and%2520encourage%2520future%2520studies%2520to%2520have%2520a%2520fundamental%2520understanding%250Aof%2520the%2520catastrophic%2520forgetting%2520in%2520PLMs.%2520The%2520data%252C%2520code%2520and%2520scripts%2520are%2520publicly%250Aavailable%2520at%250Ahttps%253A//github.com/zzz47zzz/pretrained-lm-for-incremental-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models&entry.906535625=Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Qianli%20Ma&entry.1292438233=%20%20Incremental%20Learning%20%28IL%29%20has%20been%20a%20long-standing%20problem%20in%20both%20vision%20and%0ANatural%20Language%20Processing%20%28NLP%29%20communities.%20In%20recent%20years%2C%20as%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%20have%20achieved%20remarkable%20progress%20in%20various%20NLP%0Adownstream%20tasks%2C%20utilizing%20PLMs%20as%20backbones%20has%20become%20a%20common%20practice%20in%0Arecent%20research%20of%20IL%20in%20NLP.%20Most%20assume%20that%20catastrophic%20forgetting%20is%20the%0Abiggest%20obstacle%20to%20achieving%20superior%20IL%20performance%20and%20propose%20various%0Atechniques%20to%20overcome%20this%20issue.%20However%2C%20we%20find%20that%20this%20assumption%20is%0Aproblematic.%20Specifically%2C%20we%20revisit%20more%20than%2020%20methods%20on%20four%0Aclassification%20tasks%20%28Text%20Classification%2C%20Intent%20Classification%2C%20Relation%0AExtraction%2C%20and%20Named%20Entity%20Recognition%29%20under%20the%20two%20most%20popular%20IL%0Asettings%20%28Class-Incremental%20and%20Task-Incremental%29%20and%20reveal%20that%20most%20of%20them%0Aseverely%20underestimate%20the%20inherent%20anti-forgetting%20ability%20of%20PLMs.%20Based%20on%0Athe%20observation%2C%20we%20propose%20a%20frustratingly%20easy%20method%20called%20SEQ%2A%20for%20IL%20with%0APLMs.%20The%20results%20show%20that%20SEQ%2A%20has%20competitive%20or%20superior%20performance%0Acompared%20to%20state-of-the-art%20%28SOTA%29%20IL%20methods%20and%20requires%20considerably%20less%0Atrainable%20parameters%20and%20training%20time.%20These%20findings%20urge%20us%20to%20revisit%20the%0AIL%20with%20PLMs%20and%20encourage%20future%20studies%20to%20have%20a%20fundamental%20understanding%0Aof%20the%20catastrophic%20forgetting%20in%20PLMs.%20The%20data%2C%20code%20and%20scripts%20are%20publicly%0Aavailable%20at%0Ahttps%3A//github.com/zzz47zzz/pretrained-lm-for-incremental-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07887v2&entry.124074799=Read"},
{"title": "Print-N-Grip: A Disposable, Compliant, Scalable and One-Shot 3D-Printed\n  Multi-Fingered Robotic Hand", "author": "Alon Laron and Eran Sne and Yaron Perets and Avishai Sintov", "abstract": "  Robotic hands are an important tool for replacing humans in handling toxic or\nradioactive materials. However, these are usually highly expensive, and in many\ncases, once they are contaminated, they cannot be re-used. Some solutions cope\nwith this challenge by 3D printing parts of a tendon-based hand. However,\nfabrication requires additional assembly steps. Therefore, a novice user may\nhave difficulties fabricating a hand upon contamination of the previous one. We\npropose the Print-N-Grip (PNG) hand which is a tendon-based underactuated\nmechanism able to adapt to the shape of objects. The hand is fabricated through\none-shot 3D printing with no additional engineering effort, and can accommodate\na number of fingers as desired by the practitioner. Due to its low cost, the\nPNG hand can easily be detached from a universal base for disposing upon\ncontamination, and replaced by a newly printed one. In addition, the PNG hand\nis scalable such that one can effortlessly resize the computerized model and\nprint. We present the design of the PNG hand along with experiments to show the\ncapabilities and high durability of the hand.\n", "link": "http://arxiv.org/abs/2401.16463v2", "date": "2024-05-20", "relevancy": 2.3267, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5052}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4455}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Print-N-Grip%3A%20A%20Disposable%2C%20Compliant%2C%20Scalable%20and%20One-Shot%203D-Printed%0A%20%20Multi-Fingered%20Robotic%20Hand&body=Title%3A%20Print-N-Grip%3A%20A%20Disposable%2C%20Compliant%2C%20Scalable%20and%20One-Shot%203D-Printed%0A%20%20Multi-Fingered%20Robotic%20Hand%0AAuthor%3A%20Alon%20Laron%20and%20Eran%20Sne%20and%20Yaron%20Perets%20and%20Avishai%20Sintov%0AAbstract%3A%20%20%20Robotic%20hands%20are%20an%20important%20tool%20for%20replacing%20humans%20in%20handling%20toxic%20or%0Aradioactive%20materials.%20However%2C%20these%20are%20usually%20highly%20expensive%2C%20and%20in%20many%0Acases%2C%20once%20they%20are%20contaminated%2C%20they%20cannot%20be%20re-used.%20Some%20solutions%20cope%0Awith%20this%20challenge%20by%203D%20printing%20parts%20of%20a%20tendon-based%20hand.%20However%2C%0Afabrication%20requires%20additional%20assembly%20steps.%20Therefore%2C%20a%20novice%20user%20may%0Ahave%20difficulties%20fabricating%20a%20hand%20upon%20contamination%20of%20the%20previous%20one.%20We%0Apropose%20the%20Print-N-Grip%20%28PNG%29%20hand%20which%20is%20a%20tendon-based%20underactuated%0Amechanism%20able%20to%20adapt%20to%20the%20shape%20of%20objects.%20The%20hand%20is%20fabricated%20through%0Aone-shot%203D%20printing%20with%20no%20additional%20engineering%20effort%2C%20and%20can%20accommodate%0Aa%20number%20of%20fingers%20as%20desired%20by%20the%20practitioner.%20Due%20to%20its%20low%20cost%2C%20the%0APNG%20hand%20can%20easily%20be%20detached%20from%20a%20universal%20base%20for%20disposing%20upon%0Acontamination%2C%20and%20replaced%20by%20a%20newly%20printed%20one.%20In%20addition%2C%20the%20PNG%20hand%0Ais%20scalable%20such%20that%20one%20can%20effortlessly%20resize%20the%20computerized%20model%20and%0Aprint.%20We%20present%20the%20design%20of%20the%20PNG%20hand%20along%20with%20experiments%20to%20show%20the%0Acapabilities%20and%20high%20durability%20of%20the%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrint-N-Grip%253A%2520A%2520Disposable%252C%2520Compliant%252C%2520Scalable%2520and%2520One-Shot%25203D-Printed%250A%2520%2520Multi-Fingered%2520Robotic%2520Hand%26entry.906535625%3DAlon%2520Laron%2520and%2520Eran%2520Sne%2520and%2520Yaron%2520Perets%2520and%2520Avishai%2520Sintov%26entry.1292438233%3D%2520%2520Robotic%2520hands%2520are%2520an%2520important%2520tool%2520for%2520replacing%2520humans%2520in%2520handling%2520toxic%2520or%250Aradioactive%2520materials.%2520However%252C%2520these%2520are%2520usually%2520highly%2520expensive%252C%2520and%2520in%2520many%250Acases%252C%2520once%2520they%2520are%2520contaminated%252C%2520they%2520cannot%2520be%2520re-used.%2520Some%2520solutions%2520cope%250Awith%2520this%2520challenge%2520by%25203D%2520printing%2520parts%2520of%2520a%2520tendon-based%2520hand.%2520However%252C%250Afabrication%2520requires%2520additional%2520assembly%2520steps.%2520Therefore%252C%2520a%2520novice%2520user%2520may%250Ahave%2520difficulties%2520fabricating%2520a%2520hand%2520upon%2520contamination%2520of%2520the%2520previous%2520one.%2520We%250Apropose%2520the%2520Print-N-Grip%2520%2528PNG%2529%2520hand%2520which%2520is%2520a%2520tendon-based%2520underactuated%250Amechanism%2520able%2520to%2520adapt%2520to%2520the%2520shape%2520of%2520objects.%2520The%2520hand%2520is%2520fabricated%2520through%250Aone-shot%25203D%2520printing%2520with%2520no%2520additional%2520engineering%2520effort%252C%2520and%2520can%2520accommodate%250Aa%2520number%2520of%2520fingers%2520as%2520desired%2520by%2520the%2520practitioner.%2520Due%2520to%2520its%2520low%2520cost%252C%2520the%250APNG%2520hand%2520can%2520easily%2520be%2520detached%2520from%2520a%2520universal%2520base%2520for%2520disposing%2520upon%250Acontamination%252C%2520and%2520replaced%2520by%2520a%2520newly%2520printed%2520one.%2520In%2520addition%252C%2520the%2520PNG%2520hand%250Ais%2520scalable%2520such%2520that%2520one%2520can%2520effortlessly%2520resize%2520the%2520computerized%2520model%2520and%250Aprint.%2520We%2520present%2520the%2520design%2520of%2520the%2520PNG%2520hand%2520along%2520with%2520experiments%2520to%2520show%2520the%250Acapabilities%2520and%2520high%2520durability%2520of%2520the%2520hand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Print-N-Grip%3A%20A%20Disposable%2C%20Compliant%2C%20Scalable%20and%20One-Shot%203D-Printed%0A%20%20Multi-Fingered%20Robotic%20Hand&entry.906535625=Alon%20Laron%20and%20Eran%20Sne%20and%20Yaron%20Perets%20and%20Avishai%20Sintov&entry.1292438233=%20%20Robotic%20hands%20are%20an%20important%20tool%20for%20replacing%20humans%20in%20handling%20toxic%20or%0Aradioactive%20materials.%20However%2C%20these%20are%20usually%20highly%20expensive%2C%20and%20in%20many%0Acases%2C%20once%20they%20are%20contaminated%2C%20they%20cannot%20be%20re-used.%20Some%20solutions%20cope%0Awith%20this%20challenge%20by%203D%20printing%20parts%20of%20a%20tendon-based%20hand.%20However%2C%0Afabrication%20requires%20additional%20assembly%20steps.%20Therefore%2C%20a%20novice%20user%20may%0Ahave%20difficulties%20fabricating%20a%20hand%20upon%20contamination%20of%20the%20previous%20one.%20We%0Apropose%20the%20Print-N-Grip%20%28PNG%29%20hand%20which%20is%20a%20tendon-based%20underactuated%0Amechanism%20able%20to%20adapt%20to%20the%20shape%20of%20objects.%20The%20hand%20is%20fabricated%20through%0Aone-shot%203D%20printing%20with%20no%20additional%20engineering%20effort%2C%20and%20can%20accommodate%0Aa%20number%20of%20fingers%20as%20desired%20by%20the%20practitioner.%20Due%20to%20its%20low%20cost%2C%20the%0APNG%20hand%20can%20easily%20be%20detached%20from%20a%20universal%20base%20for%20disposing%20upon%0Acontamination%2C%20and%20replaced%20by%20a%20newly%20printed%20one.%20In%20addition%2C%20the%20PNG%20hand%0Ais%20scalable%20such%20that%20one%20can%20effortlessly%20resize%20the%20computerized%20model%20and%0Aprint.%20We%20present%20the%20design%20of%20the%20PNG%20hand%20along%20with%20experiments%20to%20show%20the%0Acapabilities%20and%20high%20durability%20of%20the%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16463v2&entry.124074799=Read"},
{"title": "Large Intestine 3D Shape Refinement Using Point Diffusion Models for\n  Digital Phantom Generation", "author": "Kaouther Mouheb and Mobina Ghojogh Nejad and Lavsen Dahal and Ehsan Samei and Kyle J. Lafata and W. Paul Segars and Joseph Y. Lo", "abstract": "  Accurate 3D modeling of human organs plays a crucial role in building\ncomputational phantoms for virtual imaging trials. However, generating\nanatomically plausible reconstructions of organ surfaces from computed\ntomography scans remains challenging for many structures in the human body.\nThis challenge is particularly evident when dealing with the large intestine.\nIn this study, we leverage recent advancements in geometric deep learning and\ndenoising diffusion probabilistic models to refine the segmentation results of\nthe large intestine. We begin by representing the organ as point clouds sampled\nfrom the surface of the 3D segmentation mask. Subsequently, we employ a\nhierarchical variational autoencoder to obtain global and local latent\nrepresentations of the organ's shape. We train two conditional denoising\ndiffusion models in the hierarchical latent space to perform shape refinement.\nTo further enhance our method, we incorporate a state-of-the-art surface\nreconstruction model, allowing us to generate smooth meshes from the obtained\ncomplete point clouds. Experimental results demonstrate the effectiveness of\nour approach in capturing both the global distribution of the organ's shape and\nits fine details. Our complete refinement pipeline demonstrates remarkable\nenhancements in surface representation compared to the initial segmentation,\nreducing the Chamfer distance by 70%, the Hausdorff distance by 32%, and the\nEarth Mover's distance by 6%. By combining geometric deep learning, denoising\ndiffusion models, and advanced surface reconstruction techniques, our proposed\nmethod offers a promising solution for accurately modeling the large\nintestine's surface and can easily be extended to other anatomical structures.\n", "link": "http://arxiv.org/abs/2309.08289v2", "date": "2024-05-20", "relevancy": 2.3208, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Intestine%203D%20Shape%20Refinement%20Using%20Point%20Diffusion%20Models%20for%0A%20%20Digital%20Phantom%20Generation&body=Title%3A%20Large%20Intestine%203D%20Shape%20Refinement%20Using%20Point%20Diffusion%20Models%20for%0A%20%20Digital%20Phantom%20Generation%0AAuthor%3A%20Kaouther%20Mouheb%20and%20Mobina%20Ghojogh%20Nejad%20and%20Lavsen%20Dahal%20and%20Ehsan%20Samei%20and%20Kyle%20J.%20Lafata%20and%20W.%20Paul%20Segars%20and%20Joseph%20Y.%20Lo%0AAbstract%3A%20%20%20Accurate%203D%20modeling%20of%20human%20organs%20plays%20a%20crucial%20role%20in%20building%0Acomputational%20phantoms%20for%20virtual%20imaging%20trials.%20However%2C%20generating%0Aanatomically%20plausible%20reconstructions%20of%20organ%20surfaces%20from%20computed%0Atomography%20scans%20remains%20challenging%20for%20many%20structures%20in%20the%20human%20body.%0AThis%20challenge%20is%20particularly%20evident%20when%20dealing%20with%20the%20large%20intestine.%0AIn%20this%20study%2C%20we%20leverage%20recent%20advancements%20in%20geometric%20deep%20learning%20and%0Adenoising%20diffusion%20probabilistic%20models%20to%20refine%20the%20segmentation%20results%20of%0Athe%20large%20intestine.%20We%20begin%20by%20representing%20the%20organ%20as%20point%20clouds%20sampled%0Afrom%20the%20surface%20of%20the%203D%20segmentation%20mask.%20Subsequently%2C%20we%20employ%20a%0Ahierarchical%20variational%20autoencoder%20to%20obtain%20global%20and%20local%20latent%0Arepresentations%20of%20the%20organ%27s%20shape.%20We%20train%20two%20conditional%20denoising%0Adiffusion%20models%20in%20the%20hierarchical%20latent%20space%20to%20perform%20shape%20refinement.%0ATo%20further%20enhance%20our%20method%2C%20we%20incorporate%20a%20state-of-the-art%20surface%0Areconstruction%20model%2C%20allowing%20us%20to%20generate%20smooth%20meshes%20from%20the%20obtained%0Acomplete%20point%20clouds.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20in%20capturing%20both%20the%20global%20distribution%20of%20the%20organ%27s%20shape%20and%0Aits%20fine%20details.%20Our%20complete%20refinement%20pipeline%20demonstrates%20remarkable%0Aenhancements%20in%20surface%20representation%20compared%20to%20the%20initial%20segmentation%2C%0Areducing%20the%20Chamfer%20distance%20by%2070%25%2C%20the%20Hausdorff%20distance%20by%2032%25%2C%20and%20the%0AEarth%20Mover%27s%20distance%20by%206%25.%20By%20combining%20geometric%20deep%20learning%2C%20denoising%0Adiffusion%20models%2C%20and%20advanced%20surface%20reconstruction%20techniques%2C%20our%20proposed%0Amethod%20offers%20a%20promising%20solution%20for%20accurately%20modeling%20the%20large%0Aintestine%27s%20surface%20and%20can%20easily%20be%20extended%20to%20other%20anatomical%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Intestine%25203D%2520Shape%2520Refinement%2520Using%2520Point%2520Diffusion%2520Models%2520for%250A%2520%2520Digital%2520Phantom%2520Generation%26entry.906535625%3DKaouther%2520Mouheb%2520and%2520Mobina%2520Ghojogh%2520Nejad%2520and%2520Lavsen%2520Dahal%2520and%2520Ehsan%2520Samei%2520and%2520Kyle%2520J.%2520Lafata%2520and%2520W.%2520Paul%2520Segars%2520and%2520Joseph%2520Y.%2520Lo%26entry.1292438233%3D%2520%2520Accurate%25203D%2520modeling%2520of%2520human%2520organs%2520plays%2520a%2520crucial%2520role%2520in%2520building%250Acomputational%2520phantoms%2520for%2520virtual%2520imaging%2520trials.%2520However%252C%2520generating%250Aanatomically%2520plausible%2520reconstructions%2520of%2520organ%2520surfaces%2520from%2520computed%250Atomography%2520scans%2520remains%2520challenging%2520for%2520many%2520structures%2520in%2520the%2520human%2520body.%250AThis%2520challenge%2520is%2520particularly%2520evident%2520when%2520dealing%2520with%2520the%2520large%2520intestine.%250AIn%2520this%2520study%252C%2520we%2520leverage%2520recent%2520advancements%2520in%2520geometric%2520deep%2520learning%2520and%250Adenoising%2520diffusion%2520probabilistic%2520models%2520to%2520refine%2520the%2520segmentation%2520results%2520of%250Athe%2520large%2520intestine.%2520We%2520begin%2520by%2520representing%2520the%2520organ%2520as%2520point%2520clouds%2520sampled%250Afrom%2520the%2520surface%2520of%2520the%25203D%2520segmentation%2520mask.%2520Subsequently%252C%2520we%2520employ%2520a%250Ahierarchical%2520variational%2520autoencoder%2520to%2520obtain%2520global%2520and%2520local%2520latent%250Arepresentations%2520of%2520the%2520organ%2527s%2520shape.%2520We%2520train%2520two%2520conditional%2520denoising%250Adiffusion%2520models%2520in%2520the%2520hierarchical%2520latent%2520space%2520to%2520perform%2520shape%2520refinement.%250ATo%2520further%2520enhance%2520our%2520method%252C%2520we%2520incorporate%2520a%2520state-of-the-art%2520surface%250Areconstruction%2520model%252C%2520allowing%2520us%2520to%2520generate%2520smooth%2520meshes%2520from%2520the%2520obtained%250Acomplete%2520point%2520clouds.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520in%2520capturing%2520both%2520the%2520global%2520distribution%2520of%2520the%2520organ%2527s%2520shape%2520and%250Aits%2520fine%2520details.%2520Our%2520complete%2520refinement%2520pipeline%2520demonstrates%2520remarkable%250Aenhancements%2520in%2520surface%2520representation%2520compared%2520to%2520the%2520initial%2520segmentation%252C%250Areducing%2520the%2520Chamfer%2520distance%2520by%252070%2525%252C%2520the%2520Hausdorff%2520distance%2520by%252032%2525%252C%2520and%2520the%250AEarth%2520Mover%2527s%2520distance%2520by%25206%2525.%2520By%2520combining%2520geometric%2520deep%2520learning%252C%2520denoising%250Adiffusion%2520models%252C%2520and%2520advanced%2520surface%2520reconstruction%2520techniques%252C%2520our%2520proposed%250Amethod%2520offers%2520a%2520promising%2520solution%2520for%2520accurately%2520modeling%2520the%2520large%250Aintestine%2527s%2520surface%2520and%2520can%2520easily%2520be%2520extended%2520to%2520other%2520anatomical%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Intestine%203D%20Shape%20Refinement%20Using%20Point%20Diffusion%20Models%20for%0A%20%20Digital%20Phantom%20Generation&entry.906535625=Kaouther%20Mouheb%20and%20Mobina%20Ghojogh%20Nejad%20and%20Lavsen%20Dahal%20and%20Ehsan%20Samei%20and%20Kyle%20J.%20Lafata%20and%20W.%20Paul%20Segars%20and%20Joseph%20Y.%20Lo&entry.1292438233=%20%20Accurate%203D%20modeling%20of%20human%20organs%20plays%20a%20crucial%20role%20in%20building%0Acomputational%20phantoms%20for%20virtual%20imaging%20trials.%20However%2C%20generating%0Aanatomically%20plausible%20reconstructions%20of%20organ%20surfaces%20from%20computed%0Atomography%20scans%20remains%20challenging%20for%20many%20structures%20in%20the%20human%20body.%0AThis%20challenge%20is%20particularly%20evident%20when%20dealing%20with%20the%20large%20intestine.%0AIn%20this%20study%2C%20we%20leverage%20recent%20advancements%20in%20geometric%20deep%20learning%20and%0Adenoising%20diffusion%20probabilistic%20models%20to%20refine%20the%20segmentation%20results%20of%0Athe%20large%20intestine.%20We%20begin%20by%20representing%20the%20organ%20as%20point%20clouds%20sampled%0Afrom%20the%20surface%20of%20the%203D%20segmentation%20mask.%20Subsequently%2C%20we%20employ%20a%0Ahierarchical%20variational%20autoencoder%20to%20obtain%20global%20and%20local%20latent%0Arepresentations%20of%20the%20organ%27s%20shape.%20We%20train%20two%20conditional%20denoising%0Adiffusion%20models%20in%20the%20hierarchical%20latent%20space%20to%20perform%20shape%20refinement.%0ATo%20further%20enhance%20our%20method%2C%20we%20incorporate%20a%20state-of-the-art%20surface%0Areconstruction%20model%2C%20allowing%20us%20to%20generate%20smooth%20meshes%20from%20the%20obtained%0Acomplete%20point%20clouds.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20in%20capturing%20both%20the%20global%20distribution%20of%20the%20organ%27s%20shape%20and%0Aits%20fine%20details.%20Our%20complete%20refinement%20pipeline%20demonstrates%20remarkable%0Aenhancements%20in%20surface%20representation%20compared%20to%20the%20initial%20segmentation%2C%0Areducing%20the%20Chamfer%20distance%20by%2070%25%2C%20the%20Hausdorff%20distance%20by%2032%25%2C%20and%20the%0AEarth%20Mover%27s%20distance%20by%206%25.%20By%20combining%20geometric%20deep%20learning%2C%20denoising%0Adiffusion%20models%2C%20and%20advanced%20surface%20reconstruction%20techniques%2C%20our%20proposed%0Amethod%20offers%20a%20promising%20solution%20for%20accurately%20modeling%20the%20large%0Aintestine%27s%20surface%20and%20can%20easily%20be%20extended%20to%20other%20anatomical%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08289v2&entry.124074799=Read"},
{"title": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models", "author": "Yiqing Xu and Jiayuan Mao and Yilun Du and Tomas Loz\u00e1no-P\u00e9rez and Leslie Pack Kaebling and David Hsu", "abstract": "  This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.\n", "link": "http://arxiv.org/abs/2405.11928v1", "date": "2024-05-20", "relevancy": 2.3125, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6008}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models&body=Title%3A%20%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models%0AAuthor%3A%20Yiqing%20Xu%20and%20Jiayuan%20Mao%20and%20Yilun%20Du%20and%20Tomas%20Loz%C3%A1no-P%C3%A9rez%20and%20Leslie%20Pack%20Kaebling%20and%20David%20Hsu%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20challenge%20of%20developing%20robots%20capable%20of%0Aunderstanding%20under-specified%20instructions%20for%20creating%20functional%20object%0Aarrangements%2C%20such%20as%20%22set%20up%20a%20dining%20table%20for%20two%22%3B%20previous%20arrangement%0Aapproaches%20have%20focused%20on%20much%20more%20explicit%20instructions%2C%20such%20as%20%22put%20object%0AA%20on%20the%20table.%22%20We%20introduce%20a%20framework%2C%20SetItUp%2C%20for%20learning%20to%20interpret%0Aunder-specified%20instructions.%20SetItUp%20takes%20a%20small%20number%20of%20training%20examples%0Aand%20a%20human-crafted%20program%20sketch%20to%20uncover%20arrangement%20rules%20for%20specific%0Ascene%20types.%20By%20leveraging%20an%20intermediate%20graph-like%20representation%20of%0Aabstract%20spatial%20relationships%20among%20objects%2C%20SetItUp%20decomposes%20the%0Aarrangement%20problem%20into%20two%20subproblems%3A%20i%29%20learning%20the%20arrangement%20patterns%0Afrom%20limited%20data%20and%20ii%29%20grounding%20these%20abstract%20relationships%20into%20object%0Aposes.%20SetItUp%20leverages%20large%20language%20models%20%28LLMs%29%20to%20propose%20the%20abstract%0Aspatial%20relationships%20among%20objects%20in%20novel%20scenes%20as%20the%20constraints%20to%20be%0Asatisfied%3B%20then%2C%20it%20composes%20a%20library%20of%20diffusion%20models%20associated%20with%0Athese%20abstract%20relationships%20to%20find%20object%20poses%20that%20satisfy%20the%20constraints.%0AWe%20validate%20our%20framework%20on%20a%20dataset%20comprising%20study%20desks%2C%20dining%20tables%2C%0Aand%20coffee%20tables%2C%20with%20the%20results%20showing%20superior%20performance%20in%20generating%0Aphysically%20plausible%2C%20functional%2C%20and%20aesthetically%20pleasing%20object%0Aarrangements%20compared%20to%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Set%2520It%2520Up%2521%2522%253A%2520Functional%2520Object%2520Arrangement%2520with%2520Compositional%250A%2520%2520Generative%2520Models%26entry.906535625%3DYiqing%2520Xu%2520and%2520Jiayuan%2520Mao%2520and%2520Yilun%2520Du%2520and%2520Tomas%2520Loz%25C3%25A1no-P%25C3%25A9rez%2520and%2520Leslie%2520Pack%2520Kaebling%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520challenge%2520of%2520developing%2520robots%2520capable%2520of%250Aunderstanding%2520under-specified%2520instructions%2520for%2520creating%2520functional%2520object%250Aarrangements%252C%2520such%2520as%2520%2522set%2520up%2520a%2520dining%2520table%2520for%2520two%2522%253B%2520previous%2520arrangement%250Aapproaches%2520have%2520focused%2520on%2520much%2520more%2520explicit%2520instructions%252C%2520such%2520as%2520%2522put%2520object%250AA%2520on%2520the%2520table.%2522%2520We%2520introduce%2520a%2520framework%252C%2520SetItUp%252C%2520for%2520learning%2520to%2520interpret%250Aunder-specified%2520instructions.%2520SetItUp%2520takes%2520a%2520small%2520number%2520of%2520training%2520examples%250Aand%2520a%2520human-crafted%2520program%2520sketch%2520to%2520uncover%2520arrangement%2520rules%2520for%2520specific%250Ascene%2520types.%2520By%2520leveraging%2520an%2520intermediate%2520graph-like%2520representation%2520of%250Aabstract%2520spatial%2520relationships%2520among%2520objects%252C%2520SetItUp%2520decomposes%2520the%250Aarrangement%2520problem%2520into%2520two%2520subproblems%253A%2520i%2529%2520learning%2520the%2520arrangement%2520patterns%250Afrom%2520limited%2520data%2520and%2520ii%2529%2520grounding%2520these%2520abstract%2520relationships%2520into%2520object%250Aposes.%2520SetItUp%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520propose%2520the%2520abstract%250Aspatial%2520relationships%2520among%2520objects%2520in%2520novel%2520scenes%2520as%2520the%2520constraints%2520to%2520be%250Asatisfied%253B%2520then%252C%2520it%2520composes%2520a%2520library%2520of%2520diffusion%2520models%2520associated%2520with%250Athese%2520abstract%2520relationships%2520to%2520find%2520object%2520poses%2520that%2520satisfy%2520the%2520constraints.%250AWe%2520validate%2520our%2520framework%2520on%2520a%2520dataset%2520comprising%2520study%2520desks%252C%2520dining%2520tables%252C%250Aand%2520coffee%2520tables%252C%2520with%2520the%2520results%2520showing%2520superior%2520performance%2520in%2520generating%250Aphysically%2520plausible%252C%2520functional%252C%2520and%2520aesthetically%2520pleasing%2520object%250Aarrangements%2520compared%2520to%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models&entry.906535625=Yiqing%20Xu%20and%20Jiayuan%20Mao%20and%20Yilun%20Du%20and%20Tomas%20Loz%C3%A1no-P%C3%A9rez%20and%20Leslie%20Pack%20Kaebling%20and%20David%20Hsu&entry.1292438233=%20%20This%20paper%20studies%20the%20challenge%20of%20developing%20robots%20capable%20of%0Aunderstanding%20under-specified%20instructions%20for%20creating%20functional%20object%0Aarrangements%2C%20such%20as%20%22set%20up%20a%20dining%20table%20for%20two%22%3B%20previous%20arrangement%0Aapproaches%20have%20focused%20on%20much%20more%20explicit%20instructions%2C%20such%20as%20%22put%20object%0AA%20on%20the%20table.%22%20We%20introduce%20a%20framework%2C%20SetItUp%2C%20for%20learning%20to%20interpret%0Aunder-specified%20instructions.%20SetItUp%20takes%20a%20small%20number%20of%20training%20examples%0Aand%20a%20human-crafted%20program%20sketch%20to%20uncover%20arrangement%20rules%20for%20specific%0Ascene%20types.%20By%20leveraging%20an%20intermediate%20graph-like%20representation%20of%0Aabstract%20spatial%20relationships%20among%20objects%2C%20SetItUp%20decomposes%20the%0Aarrangement%20problem%20into%20two%20subproblems%3A%20i%29%20learning%20the%20arrangement%20patterns%0Afrom%20limited%20data%20and%20ii%29%20grounding%20these%20abstract%20relationships%20into%20object%0Aposes.%20SetItUp%20leverages%20large%20language%20models%20%28LLMs%29%20to%20propose%20the%20abstract%0Aspatial%20relationships%20among%20objects%20in%20novel%20scenes%20as%20the%20constraints%20to%20be%0Asatisfied%3B%20then%2C%20it%20composes%20a%20library%20of%20diffusion%20models%20associated%20with%0Athese%20abstract%20relationships%20to%20find%20object%20poses%20that%20satisfy%20the%20constraints.%0AWe%20validate%20our%20framework%20on%20a%20dataset%20comprising%20study%20desks%2C%20dining%20tables%2C%0Aand%20coffee%20tables%2C%20with%20the%20results%20showing%20superior%20performance%20in%20generating%0Aphysically%20plausible%2C%20functional%2C%20and%20aesthetically%20pleasing%20object%0Aarrangements%20compared%20to%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11928v1&entry.124074799=Read"},
{"title": "Perception Without Vision for Trajectory Prediction: Ego Vehicle\n  Dynamics as Scene Representation for Efficient Active Learning in Autonomous\n  Driving", "author": "Ross Greer and Mohan Trivedi", "abstract": "  This study investigates the use of trajectory and dynamic state information\nfor efficient data curation in autonomous driving machine learning tasks. We\npropose methods for clustering trajectory-states and sampling strategies in an\nactive learning framework, aiming to reduce annotation and data costs while\nmaintaining model performance. Our approach leverages trajectory information to\nguide data selection, promoting diversity in the training data. We demonstrate\nthe effectiveness of our methods on the trajectory prediction task using the\nnuScenes dataset, showing consistent performance gains over random sampling\nacross different data pool sizes, and even reaching sub-baseline displacement\nerrors at just 50% of the data cost. Our results suggest that sampling typical\ndata initially helps overcome the ''cold start problem,'' while introducing\nnovelty becomes more beneficial as the training pool size increases. By\nintegrating trajectory-state-informed active learning, we demonstrate that more\nefficient and robust autonomous driving systems are possible and practical\nusing low-cost data curation strategies.\n", "link": "http://arxiv.org/abs/2405.09049v2", "date": "2024-05-20", "relevancy": 2.2982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5978}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Without%20Vision%20for%20Trajectory%20Prediction%3A%20Ego%20Vehicle%0A%20%20Dynamics%20as%20Scene%20Representation%20for%20Efficient%20Active%20Learning%20in%20Autonomous%0A%20%20Driving&body=Title%3A%20Perception%20Without%20Vision%20for%20Trajectory%20Prediction%3A%20Ego%20Vehicle%0A%20%20Dynamics%20as%20Scene%20Representation%20for%20Efficient%20Active%20Learning%20in%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Ross%20Greer%20and%20Mohan%20Trivedi%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20use%20of%20trajectory%20and%20dynamic%20state%20information%0Afor%20efficient%20data%20curation%20in%20autonomous%20driving%20machine%20learning%20tasks.%20We%0Apropose%20methods%20for%20clustering%20trajectory-states%20and%20sampling%20strategies%20in%20an%0Aactive%20learning%20framework%2C%20aiming%20to%20reduce%20annotation%20and%20data%20costs%20while%0Amaintaining%20model%20performance.%20Our%20approach%20leverages%20trajectory%20information%20to%0Aguide%20data%20selection%2C%20promoting%20diversity%20in%20the%20training%20data.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20methods%20on%20the%20trajectory%20prediction%20task%20using%20the%0AnuScenes%20dataset%2C%20showing%20consistent%20performance%20gains%20over%20random%20sampling%0Aacross%20different%20data%20pool%20sizes%2C%20and%20even%20reaching%20sub-baseline%20displacement%0Aerrors%20at%20just%2050%25%20of%20the%20data%20cost.%20Our%20results%20suggest%20that%20sampling%20typical%0Adata%20initially%20helps%20overcome%20the%20%27%27cold%20start%20problem%2C%27%27%20while%20introducing%0Anovelty%20becomes%20more%20beneficial%20as%20the%20training%20pool%20size%20increases.%20By%0Aintegrating%20trajectory-state-informed%20active%20learning%2C%20we%20demonstrate%20that%20more%0Aefficient%20and%20robust%20autonomous%20driving%20systems%20are%20possible%20and%20practical%0Ausing%20low-cost%20data%20curation%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Without%2520Vision%2520for%2520Trajectory%2520Prediction%253A%2520Ego%2520Vehicle%250A%2520%2520Dynamics%2520as%2520Scene%2520Representation%2520for%2520Efficient%2520Active%2520Learning%2520in%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DRoss%2520Greer%2520and%2520Mohan%2520Trivedi%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520use%2520of%2520trajectory%2520and%2520dynamic%2520state%2520information%250Afor%2520efficient%2520data%2520curation%2520in%2520autonomous%2520driving%2520machine%2520learning%2520tasks.%2520We%250Apropose%2520methods%2520for%2520clustering%2520trajectory-states%2520and%2520sampling%2520strategies%2520in%2520an%250Aactive%2520learning%2520framework%252C%2520aiming%2520to%2520reduce%2520annotation%2520and%2520data%2520costs%2520while%250Amaintaining%2520model%2520performance.%2520Our%2520approach%2520leverages%2520trajectory%2520information%2520to%250Aguide%2520data%2520selection%252C%2520promoting%2520diversity%2520in%2520the%2520training%2520data.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520methods%2520on%2520the%2520trajectory%2520prediction%2520task%2520using%2520the%250AnuScenes%2520dataset%252C%2520showing%2520consistent%2520performance%2520gains%2520over%2520random%2520sampling%250Aacross%2520different%2520data%2520pool%2520sizes%252C%2520and%2520even%2520reaching%2520sub-baseline%2520displacement%250Aerrors%2520at%2520just%252050%2525%2520of%2520the%2520data%2520cost.%2520Our%2520results%2520suggest%2520that%2520sampling%2520typical%250Adata%2520initially%2520helps%2520overcome%2520the%2520%2527%2527cold%2520start%2520problem%252C%2527%2527%2520while%2520introducing%250Anovelty%2520becomes%2520more%2520beneficial%2520as%2520the%2520training%2520pool%2520size%2520increases.%2520By%250Aintegrating%2520trajectory-state-informed%2520active%2520learning%252C%2520we%2520demonstrate%2520that%2520more%250Aefficient%2520and%2520robust%2520autonomous%2520driving%2520systems%2520are%2520possible%2520and%2520practical%250Ausing%2520low-cost%2520data%2520curation%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Without%20Vision%20for%20Trajectory%20Prediction%3A%20Ego%20Vehicle%0A%20%20Dynamics%20as%20Scene%20Representation%20for%20Efficient%20Active%20Learning%20in%20Autonomous%0A%20%20Driving&entry.906535625=Ross%20Greer%20and%20Mohan%20Trivedi&entry.1292438233=%20%20This%20study%20investigates%20the%20use%20of%20trajectory%20and%20dynamic%20state%20information%0Afor%20efficient%20data%20curation%20in%20autonomous%20driving%20machine%20learning%20tasks.%20We%0Apropose%20methods%20for%20clustering%20trajectory-states%20and%20sampling%20strategies%20in%20an%0Aactive%20learning%20framework%2C%20aiming%20to%20reduce%20annotation%20and%20data%20costs%20while%0Amaintaining%20model%20performance.%20Our%20approach%20leverages%20trajectory%20information%20to%0Aguide%20data%20selection%2C%20promoting%20diversity%20in%20the%20training%20data.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20methods%20on%20the%20trajectory%20prediction%20task%20using%20the%0AnuScenes%20dataset%2C%20showing%20consistent%20performance%20gains%20over%20random%20sampling%0Aacross%20different%20data%20pool%20sizes%2C%20and%20even%20reaching%20sub-baseline%20displacement%0Aerrors%20at%20just%2050%25%20of%20the%20data%20cost.%20Our%20results%20suggest%20that%20sampling%20typical%0Adata%20initially%20helps%20overcome%20the%20%27%27cold%20start%20problem%2C%27%27%20while%20introducing%0Anovelty%20becomes%20more%20beneficial%20as%20the%20training%20pool%20size%20increases.%20By%0Aintegrating%20trajectory-state-informed%20active%20learning%2C%20we%20demonstrate%20that%20more%0Aefficient%20and%20robust%20autonomous%20driving%20systems%20are%20possible%20and%20practical%0Ausing%20low-cost%20data%20curation%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09049v2&entry.124074799=Read"},
{"title": "Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points,\n  Saddle Escaping, and Network Embedding", "author": "Zhengqing Wu and Berfin Simsek and Francois Ged", "abstract": "  In this paper, we investigate the loss landscape of one-hidden-layer neural\nnetworks with ReLU-like activation functions trained with the empirical squared\nloss. As the activation function is non-differentiable, it is so far unclear\nhow to completely characterize the stationary points. We propose the conditions\nfor stationarity that apply to both non-differentiable and differentiable\ncases. Additionally, we show that, if a stationary point does not contain\n\"escape neurons\", which are defined with first-order conditions, then it must\nbe a local minimum. Moreover, for the scalar-output case, the presence of an\nescape neuron guarantees that the stationary point is not a local minimum. Our\nresults refine the description of the saddle-to-saddle training process\nstarting from infinitesimally small (vanishing) initialization for shallow\nReLU-like networks, linking saddle escaping directly with the parameter changes\nof escape neurons. Moreover, we are also able to fully discuss how network\nembedding, which is to instantiate a narrower network within a wider network,\nreshapes the stationary points.\n", "link": "http://arxiv.org/abs/2402.05626v2", "date": "2024-05-20", "relevancy": 2.2699, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding&body=Title%3A%20Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding%0AAuthor%3A%20Zhengqing%20Wu%20and%20Berfin%20Simsek%20and%20Francois%20Ged%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20loss%20landscape%20of%20one-hidden-layer%20neural%0Anetworks%20with%20ReLU-like%20activation%20functions%20trained%20with%20the%20empirical%20squared%0Aloss.%20As%20the%20activation%20function%20is%20non-differentiable%2C%20it%20is%20so%20far%20unclear%0Ahow%20to%20completely%20characterize%20the%20stationary%20points.%20We%20propose%20the%20conditions%0Afor%20stationarity%20that%20apply%20to%20both%20non-differentiable%20and%20differentiable%0Acases.%20Additionally%2C%20we%20show%20that%2C%20if%20a%20stationary%20point%20does%20not%20contain%0A%22escape%20neurons%22%2C%20which%20are%20defined%20with%20first-order%20conditions%2C%20then%20it%20must%0Abe%20a%20local%20minimum.%20Moreover%2C%20for%20the%20scalar-output%20case%2C%20the%20presence%20of%20an%0Aescape%20neuron%20guarantees%20that%20the%20stationary%20point%20is%20not%20a%20local%20minimum.%20Our%0Aresults%20refine%20the%20description%20of%20the%20saddle-to-saddle%20training%20process%0Astarting%20from%20infinitesimally%20small%20%28vanishing%29%20initialization%20for%20shallow%0AReLU-like%20networks%2C%20linking%20saddle%20escaping%20directly%20with%20the%20parameter%20changes%0Aof%20escape%20neurons.%20Moreover%2C%20we%20are%20also%20able%20to%20fully%20discuss%20how%20network%0Aembedding%2C%20which%20is%20to%20instantiate%20a%20narrower%20network%20within%20a%20wider%20network%2C%0Areshapes%20the%20stationary%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoss%2520Landscape%2520of%2520Shallow%2520ReLU-like%2520Neural%2520Networks%253A%2520Stationary%2520Points%252C%250A%2520%2520Saddle%2520Escaping%252C%2520and%2520Network%2520Embedding%26entry.906535625%3DZhengqing%2520Wu%2520and%2520Berfin%2520Simsek%2520and%2520Francois%2520Ged%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520loss%2520landscape%2520of%2520one-hidden-layer%2520neural%250Anetworks%2520with%2520ReLU-like%2520activation%2520functions%2520trained%2520with%2520the%2520empirical%2520squared%250Aloss.%2520As%2520the%2520activation%2520function%2520is%2520non-differentiable%252C%2520it%2520is%2520so%2520far%2520unclear%250Ahow%2520to%2520completely%2520characterize%2520the%2520stationary%2520points.%2520We%2520propose%2520the%2520conditions%250Afor%2520stationarity%2520that%2520apply%2520to%2520both%2520non-differentiable%2520and%2520differentiable%250Acases.%2520Additionally%252C%2520we%2520show%2520that%252C%2520if%2520a%2520stationary%2520point%2520does%2520not%2520contain%250A%2522escape%2520neurons%2522%252C%2520which%2520are%2520defined%2520with%2520first-order%2520conditions%252C%2520then%2520it%2520must%250Abe%2520a%2520local%2520minimum.%2520Moreover%252C%2520for%2520the%2520scalar-output%2520case%252C%2520the%2520presence%2520of%2520an%250Aescape%2520neuron%2520guarantees%2520that%2520the%2520stationary%2520point%2520is%2520not%2520a%2520local%2520minimum.%2520Our%250Aresults%2520refine%2520the%2520description%2520of%2520the%2520saddle-to-saddle%2520training%2520process%250Astarting%2520from%2520infinitesimally%2520small%2520%2528vanishing%2529%2520initialization%2520for%2520shallow%250AReLU-like%2520networks%252C%2520linking%2520saddle%2520escaping%2520directly%2520with%2520the%2520parameter%2520changes%250Aof%2520escape%2520neurons.%2520Moreover%252C%2520we%2520are%2520also%2520able%2520to%2520fully%2520discuss%2520how%2520network%250Aembedding%252C%2520which%2520is%2520to%2520instantiate%2520a%2520narrower%2520network%2520within%2520a%2520wider%2520network%252C%250Areshapes%2520the%2520stationary%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding&entry.906535625=Zhengqing%20Wu%20and%20Berfin%20Simsek%20and%20Francois%20Ged&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20loss%20landscape%20of%20one-hidden-layer%20neural%0Anetworks%20with%20ReLU-like%20activation%20functions%20trained%20with%20the%20empirical%20squared%0Aloss.%20As%20the%20activation%20function%20is%20non-differentiable%2C%20it%20is%20so%20far%20unclear%0Ahow%20to%20completely%20characterize%20the%20stationary%20points.%20We%20propose%20the%20conditions%0Afor%20stationarity%20that%20apply%20to%20both%20non-differentiable%20and%20differentiable%0Acases.%20Additionally%2C%20we%20show%20that%2C%20if%20a%20stationary%20point%20does%20not%20contain%0A%22escape%20neurons%22%2C%20which%20are%20defined%20with%20first-order%20conditions%2C%20then%20it%20must%0Abe%20a%20local%20minimum.%20Moreover%2C%20for%20the%20scalar-output%20case%2C%20the%20presence%20of%20an%0Aescape%20neuron%20guarantees%20that%20the%20stationary%20point%20is%20not%20a%20local%20minimum.%20Our%0Aresults%20refine%20the%20description%20of%20the%20saddle-to-saddle%20training%20process%0Astarting%20from%20infinitesimally%20small%20%28vanishing%29%20initialization%20for%20shallow%0AReLU-like%20networks%2C%20linking%20saddle%20escaping%20directly%20with%20the%20parameter%20changes%0Aof%20escape%20neurons.%20Moreover%2C%20we%20are%20also%20able%20to%20fully%20discuss%20how%20network%0Aembedding%2C%20which%20is%20to%20instantiate%20a%20narrower%20network%20within%20a%20wider%20network%2C%0Areshapes%20the%20stationary%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05626v2&entry.124074799=Read"},
{"title": "DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling", "author": "Linqi Zhou and Andy Shih and Chenlin Meng and Stefano Ermon", "abstract": "  Recent methods such as Score Distillation Sampling (SDS) and Variational\nScore Distillation (VSD) using 2D diffusion models for text-to-3D generation\nhave demonstrated impressive generation quality. However, the long generation\ntime of such algorithms significantly degrades the user experience. To tackle\nthis problem, we propose DreamPropeller, a drop-in acceleration algorithm that\ncan be wrapped around any existing text-to-3D generation pipeline based on\nscore distillation. Our framework generalizes Picard iterations, a classical\nalgorithm for parallel sampling an ODE path, and can account for non-ODE paths\nsuch as momentum-based gradient updates and changes in dimensions during the\noptimization process as in many cases of 3D generation. We show that our\nalgorithm trades parallel compute for wallclock time and empirically achieves\nup to 4.7x speedup with a negligible drop in generation quality for all tested\nframeworks.\n", "link": "http://arxiv.org/abs/2311.17082v3", "date": "2024-05-20", "relevancy": 2.2409, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5614}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamPropeller%3A%20Supercharge%20Text-to-3D%20Generation%20with%20Parallel%20Sampling&body=Title%3A%20DreamPropeller%3A%20Supercharge%20Text-to-3D%20Generation%20with%20Parallel%20Sampling%0AAuthor%3A%20Linqi%20Zhou%20and%20Andy%20Shih%20and%20Chenlin%20Meng%20and%20Stefano%20Ermon%0AAbstract%3A%20%20%20Recent%20methods%20such%20as%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20Variational%0AScore%20Distillation%20%28VSD%29%20using%202D%20diffusion%20models%20for%20text-to-3D%20generation%0Ahave%20demonstrated%20impressive%20generation%20quality.%20However%2C%20the%20long%20generation%0Atime%20of%20such%20algorithms%20significantly%20degrades%20the%20user%20experience.%20To%20tackle%0Athis%20problem%2C%20we%20propose%20DreamPropeller%2C%20a%20drop-in%20acceleration%20algorithm%20that%0Acan%20be%20wrapped%20around%20any%20existing%20text-to-3D%20generation%20pipeline%20based%20on%0Ascore%20distillation.%20Our%20framework%20generalizes%20Picard%20iterations%2C%20a%20classical%0Aalgorithm%20for%20parallel%20sampling%20an%20ODE%20path%2C%20and%20can%20account%20for%20non-ODE%20paths%0Asuch%20as%20momentum-based%20gradient%20updates%20and%20changes%20in%20dimensions%20during%20the%0Aoptimization%20process%20as%20in%20many%20cases%20of%203D%20generation.%20We%20show%20that%20our%0Aalgorithm%20trades%20parallel%20compute%20for%20wallclock%20time%20and%20empirically%20achieves%0Aup%20to%204.7x%20speedup%20with%20a%20negligible%20drop%20in%20generation%20quality%20for%20all%20tested%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamPropeller%253A%2520Supercharge%2520Text-to-3D%2520Generation%2520with%2520Parallel%2520Sampling%26entry.906535625%3DLinqi%2520Zhou%2520and%2520Andy%2520Shih%2520and%2520Chenlin%2520Meng%2520and%2520Stefano%2520Ermon%26entry.1292438233%3D%2520%2520Recent%2520methods%2520such%2520as%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520and%2520Variational%250AScore%2520Distillation%2520%2528VSD%2529%2520using%25202D%2520diffusion%2520models%2520for%2520text-to-3D%2520generation%250Ahave%2520demonstrated%2520impressive%2520generation%2520quality.%2520However%252C%2520the%2520long%2520generation%250Atime%2520of%2520such%2520algorithms%2520significantly%2520degrades%2520the%2520user%2520experience.%2520To%2520tackle%250Athis%2520problem%252C%2520we%2520propose%2520DreamPropeller%252C%2520a%2520drop-in%2520acceleration%2520algorithm%2520that%250Acan%2520be%2520wrapped%2520around%2520any%2520existing%2520text-to-3D%2520generation%2520pipeline%2520based%2520on%250Ascore%2520distillation.%2520Our%2520framework%2520generalizes%2520Picard%2520iterations%252C%2520a%2520classical%250Aalgorithm%2520for%2520parallel%2520sampling%2520an%2520ODE%2520path%252C%2520and%2520can%2520account%2520for%2520non-ODE%2520paths%250Asuch%2520as%2520momentum-based%2520gradient%2520updates%2520and%2520changes%2520in%2520dimensions%2520during%2520the%250Aoptimization%2520process%2520as%2520in%2520many%2520cases%2520of%25203D%2520generation.%2520We%2520show%2520that%2520our%250Aalgorithm%2520trades%2520parallel%2520compute%2520for%2520wallclock%2520time%2520and%2520empirically%2520achieves%250Aup%2520to%25204.7x%2520speedup%2520with%2520a%2520negligible%2520drop%2520in%2520generation%2520quality%2520for%2520all%2520tested%250Aframeworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamPropeller%3A%20Supercharge%20Text-to-3D%20Generation%20with%20Parallel%20Sampling&entry.906535625=Linqi%20Zhou%20and%20Andy%20Shih%20and%20Chenlin%20Meng%20and%20Stefano%20Ermon&entry.1292438233=%20%20Recent%20methods%20such%20as%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20Variational%0AScore%20Distillation%20%28VSD%29%20using%202D%20diffusion%20models%20for%20text-to-3D%20generation%0Ahave%20demonstrated%20impressive%20generation%20quality.%20However%2C%20the%20long%20generation%0Atime%20of%20such%20algorithms%20significantly%20degrades%20the%20user%20experience.%20To%20tackle%0Athis%20problem%2C%20we%20propose%20DreamPropeller%2C%20a%20drop-in%20acceleration%20algorithm%20that%0Acan%20be%20wrapped%20around%20any%20existing%20text-to-3D%20generation%20pipeline%20based%20on%0Ascore%20distillation.%20Our%20framework%20generalizes%20Picard%20iterations%2C%20a%20classical%0Aalgorithm%20for%20parallel%20sampling%20an%20ODE%20path%2C%20and%20can%20account%20for%20non-ODE%20paths%0Asuch%20as%20momentum-based%20gradient%20updates%20and%20changes%20in%20dimensions%20during%20the%0Aoptimization%20process%20as%20in%20many%20cases%20of%203D%20generation.%20We%20show%20that%20our%0Aalgorithm%20trades%20parallel%20compute%20for%20wallclock%20time%20and%20empirically%20achieves%0Aup%20to%204.7x%20speedup%20with%20a%20negligible%20drop%20in%20generation%20quality%20for%20all%20tested%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17082v3&entry.124074799=Read"},
{"title": "Special Characters Attack: Toward Scalable Training Data Extraction From\n  Large Language Models", "author": "Yang Bai and Ge Pei and Jindong Gu and Yong Yang and Xingjun Ma", "abstract": "  Large language models (LLMs) have achieved remarkable performance on a wide\nrange of tasks. However, recent studies have shown that LLMs can memorize\ntraining data and simple repeated tokens can trick the model to leak the data.\nIn this paper, we take a step further and show that certain special characters\nor their combinations with English letters are stronger memory triggers,\nleading to more severe data leakage. The intuition is that, since LLMs are\ntrained with massive data that contains a substantial amount of special\ncharacters (e.g. structural symbols {, } of JSON files, and @, # in emails and\nonline posts), the model may memorize the co-occurrence between these special\ncharacters and the raw texts. This motivates us to propose a simple but\neffective Special Characters Attack (SCA) to induce training data leakage. Our\nexperiments verify the high effectiveness of SCA against state-of-the-art LLMs:\nthey can leak diverse training data, such as code corpus, web pages, and\npersonally identifiable information, and sometimes generate non-stop outputs as\na byproduct. We further show that the composition of the training data corpus\ncan be revealed by inspecting the leaked data -- one crucial piece of\ninformation for pre-training high-performance LLMs. Our work can help\nunderstand the sensitivity of LLMs to special characters and identify potential\nareas for improvement.\n", "link": "http://arxiv.org/abs/2405.05990v2", "date": "2024-05-20", "relevancy": 2.2338, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4552}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Special%20Characters%20Attack%3A%20Toward%20Scalable%20Training%20Data%20Extraction%20From%0A%20%20Large%20Language%20Models&body=Title%3A%20Special%20Characters%20Attack%3A%20Toward%20Scalable%20Training%20Data%20Extraction%20From%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yang%20Bai%20and%20Ge%20Pei%20and%20Jindong%20Gu%20and%20Yong%20Yang%20and%20Xingjun%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20a%20wide%0Arange%20of%20tasks.%20However%2C%20recent%20studies%20have%20shown%20that%20LLMs%20can%20memorize%0Atraining%20data%20and%20simple%20repeated%20tokens%20can%20trick%20the%20model%20to%20leak%20the%20data.%0AIn%20this%20paper%2C%20we%20take%20a%20step%20further%20and%20show%20that%20certain%20special%20characters%0Aor%20their%20combinations%20with%20English%20letters%20are%20stronger%20memory%20triggers%2C%0Aleading%20to%20more%20severe%20data%20leakage.%20The%20intuition%20is%20that%2C%20since%20LLMs%20are%0Atrained%20with%20massive%20data%20that%20contains%20a%20substantial%20amount%20of%20special%0Acharacters%20%28e.g.%20structural%20symbols%20%7B%2C%20%7D%20of%20JSON%20files%2C%20and%20%40%2C%20%23%20in%20emails%20and%0Aonline%20posts%29%2C%20the%20model%20may%20memorize%20the%20co-occurrence%20between%20these%20special%0Acharacters%20and%20the%20raw%20texts.%20This%20motivates%20us%20to%20propose%20a%20simple%20but%0Aeffective%20Special%20Characters%20Attack%20%28SCA%29%20to%20induce%20training%20data%20leakage.%20Our%0Aexperiments%20verify%20the%20high%20effectiveness%20of%20SCA%20against%20state-of-the-art%20LLMs%3A%0Athey%20can%20leak%20diverse%20training%20data%2C%20such%20as%20code%20corpus%2C%20web%20pages%2C%20and%0Apersonally%20identifiable%20information%2C%20and%20sometimes%20generate%20non-stop%20outputs%20as%0Aa%20byproduct.%20We%20further%20show%20that%20the%20composition%20of%20the%20training%20data%20corpus%0Acan%20be%20revealed%20by%20inspecting%20the%20leaked%20data%20--%20one%20crucial%20piece%20of%0Ainformation%20for%20pre-training%20high-performance%20LLMs.%20Our%20work%20can%20help%0Aunderstand%20the%20sensitivity%20of%20LLMs%20to%20special%20characters%20and%20identify%20potential%0Aareas%20for%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecial%2520Characters%2520Attack%253A%2520Toward%2520Scalable%2520Training%2520Data%2520Extraction%2520From%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYang%2520Bai%2520and%2520Ge%2520Pei%2520and%2520Jindong%2520Gu%2520and%2520Yong%2520Yang%2520and%2520Xingjun%2520Ma%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520on%2520a%2520wide%250Arange%2520of%2520tasks.%2520However%252C%2520recent%2520studies%2520have%2520shown%2520that%2520LLMs%2520can%2520memorize%250Atraining%2520data%2520and%2520simple%2520repeated%2520tokens%2520can%2520trick%2520the%2520model%2520to%2520leak%2520the%2520data.%250AIn%2520this%2520paper%252C%2520we%2520take%2520a%2520step%2520further%2520and%2520show%2520that%2520certain%2520special%2520characters%250Aor%2520their%2520combinations%2520with%2520English%2520letters%2520are%2520stronger%2520memory%2520triggers%252C%250Aleading%2520to%2520more%2520severe%2520data%2520leakage.%2520The%2520intuition%2520is%2520that%252C%2520since%2520LLMs%2520are%250Atrained%2520with%2520massive%2520data%2520that%2520contains%2520a%2520substantial%2520amount%2520of%2520special%250Acharacters%2520%2528e.g.%2520structural%2520symbols%2520%257B%252C%2520%257D%2520of%2520JSON%2520files%252C%2520and%2520%2540%252C%2520%2523%2520in%2520emails%2520and%250Aonline%2520posts%2529%252C%2520the%2520model%2520may%2520memorize%2520the%2520co-occurrence%2520between%2520these%2520special%250Acharacters%2520and%2520the%2520raw%2520texts.%2520This%2520motivates%2520us%2520to%2520propose%2520a%2520simple%2520but%250Aeffective%2520Special%2520Characters%2520Attack%2520%2528SCA%2529%2520to%2520induce%2520training%2520data%2520leakage.%2520Our%250Aexperiments%2520verify%2520the%2520high%2520effectiveness%2520of%2520SCA%2520against%2520state-of-the-art%2520LLMs%253A%250Athey%2520can%2520leak%2520diverse%2520training%2520data%252C%2520such%2520as%2520code%2520corpus%252C%2520web%2520pages%252C%2520and%250Apersonally%2520identifiable%2520information%252C%2520and%2520sometimes%2520generate%2520non-stop%2520outputs%2520as%250Aa%2520byproduct.%2520We%2520further%2520show%2520that%2520the%2520composition%2520of%2520the%2520training%2520data%2520corpus%250Acan%2520be%2520revealed%2520by%2520inspecting%2520the%2520leaked%2520data%2520--%2520one%2520crucial%2520piece%2520of%250Ainformation%2520for%2520pre-training%2520high-performance%2520LLMs.%2520Our%2520work%2520can%2520help%250Aunderstand%2520the%2520sensitivity%2520of%2520LLMs%2520to%2520special%2520characters%2520and%2520identify%2520potential%250Aareas%2520for%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Special%20Characters%20Attack%3A%20Toward%20Scalable%20Training%20Data%20Extraction%20From%0A%20%20Large%20Language%20Models&entry.906535625=Yang%20Bai%20and%20Ge%20Pei%20and%20Jindong%20Gu%20and%20Yong%20Yang%20and%20Xingjun%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20a%20wide%0Arange%20of%20tasks.%20However%2C%20recent%20studies%20have%20shown%20that%20LLMs%20can%20memorize%0Atraining%20data%20and%20simple%20repeated%20tokens%20can%20trick%20the%20model%20to%20leak%20the%20data.%0AIn%20this%20paper%2C%20we%20take%20a%20step%20further%20and%20show%20that%20certain%20special%20characters%0Aor%20their%20combinations%20with%20English%20letters%20are%20stronger%20memory%20triggers%2C%0Aleading%20to%20more%20severe%20data%20leakage.%20The%20intuition%20is%20that%2C%20since%20LLMs%20are%0Atrained%20with%20massive%20data%20that%20contains%20a%20substantial%20amount%20of%20special%0Acharacters%20%28e.g.%20structural%20symbols%20%7B%2C%20%7D%20of%20JSON%20files%2C%20and%20%40%2C%20%23%20in%20emails%20and%0Aonline%20posts%29%2C%20the%20model%20may%20memorize%20the%20co-occurrence%20between%20these%20special%0Acharacters%20and%20the%20raw%20texts.%20This%20motivates%20us%20to%20propose%20a%20simple%20but%0Aeffective%20Special%20Characters%20Attack%20%28SCA%29%20to%20induce%20training%20data%20leakage.%20Our%0Aexperiments%20verify%20the%20high%20effectiveness%20of%20SCA%20against%20state-of-the-art%20LLMs%3A%0Athey%20can%20leak%20diverse%20training%20data%2C%20such%20as%20code%20corpus%2C%20web%20pages%2C%20and%0Apersonally%20identifiable%20information%2C%20and%20sometimes%20generate%20non-stop%20outputs%20as%0Aa%20byproduct.%20We%20further%20show%20that%20the%20composition%20of%20the%20training%20data%20corpus%0Acan%20be%20revealed%20by%20inspecting%20the%20leaked%20data%20--%20one%20crucial%20piece%20of%0Ainformation%20for%20pre-training%20high-performance%20LLMs.%20Our%20work%20can%20help%0Aunderstand%20the%20sensitivity%20of%20LLMs%20to%20special%20characters%20and%20identify%20potential%0Aareas%20for%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05990v2&entry.124074799=Read"},
{"title": "Text-to-Vector Generation with Neural Path Representation", "author": "Peiying Zhang and Nanxuan Zhao and Jing Liao", "abstract": "  Vector graphics are widely used in digital art and highly favored by\ndesigners due to their scalability and layer-wise properties. However, the\nprocess of creating and editing vector graphics requires creativity and design\nexpertise, making it a time-consuming task. Recent advancements in\ntext-to-vector (T2V) generation have aimed to make this process more\naccessible. However, existing T2V methods directly optimize control points of\nvector graphics paths, often resulting in intersecting or jagged paths due to\nthe lack of geometry constraints. To overcome these limitations, we propose a\nnovel neural path representation by designing a dual-branch Variational\nAutoencoder (VAE) that learns the path latent space from both sequence and\nimage modalities. By optimizing the combination of neural paths, we can\nincorporate geometric constraints while preserving expressivity in generated\nSVGs. Furthermore, we introduce a two-stage path optimization method to improve\nthe visual and topological quality of generated SVGs. In the first stage, a\npre-trained text-to-image diffusion model guides the initial generation of\ncomplex vector graphics through the Variational Score Distillation (VSD)\nprocess. In the second stage, we refine the graphics using a layer-wise image\nvectorization strategy to achieve clearer elements and structure. We\ndemonstrate the effectiveness of our method through extensive experiments and\nshowcase various applications. The project page is\nhttps://intchous.github.io/T2V-NPR.\n", "link": "http://arxiv.org/abs/2405.10317v2", "date": "2024-05-20", "relevancy": 2.1962, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5529}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation&body=Title%3A%20Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation%0AAuthor%3A%20Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Jing%20Liao%0AAbstract%3A%20%20%20Vector%20graphics%20are%20widely%20used%20in%20digital%20art%20and%20highly%20favored%20by%0Adesigners%20due%20to%20their%20scalability%20and%20layer-wise%20properties.%20However%2C%20the%0Aprocess%20of%20creating%20and%20editing%20vector%20graphics%20requires%20creativity%20and%20design%0Aexpertise%2C%20making%20it%20a%20time-consuming%20task.%20Recent%20advancements%20in%0Atext-to-vector%20%28T2V%29%20generation%20have%20aimed%20to%20make%20this%20process%20more%0Aaccessible.%20However%2C%20existing%20T2V%20methods%20directly%20optimize%20control%20points%20of%0Avector%20graphics%20paths%2C%20often%20resulting%20in%20intersecting%20or%20jagged%20paths%20due%20to%0Athe%20lack%20of%20geometry%20constraints.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20neural%20path%20representation%20by%20designing%20a%20dual-branch%20Variational%0AAutoencoder%20%28VAE%29%20that%20learns%20the%20path%20latent%20space%20from%20both%20sequence%20and%0Aimage%20modalities.%20By%20optimizing%20the%20combination%20of%20neural%20paths%2C%20we%20can%0Aincorporate%20geometric%20constraints%20while%20preserving%20expressivity%20in%20generated%0ASVGs.%20Furthermore%2C%20we%20introduce%20a%20two-stage%20path%20optimization%20method%20to%20improve%0Athe%20visual%20and%20topological%20quality%20of%20generated%20SVGs.%20In%20the%20first%20stage%2C%20a%0Apre-trained%20text-to-image%20diffusion%20model%20guides%20the%20initial%20generation%20of%0Acomplex%20vector%20graphics%20through%20the%20Variational%20Score%20Distillation%20%28VSD%29%0Aprocess.%20In%20the%20second%20stage%2C%20we%20refine%20the%20graphics%20using%20a%20layer-wise%20image%0Avectorization%20strategy%20to%20achieve%20clearer%20elements%20and%20structure.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%20experiments%20and%0Ashowcase%20various%20applications.%20The%20project%20page%20is%0Ahttps%3A//intchous.github.io/T2V-NPR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Vector%2520Generation%2520with%2520Neural%2520Path%2520Representation%26entry.906535625%3DPeiying%2520Zhang%2520and%2520Nanxuan%2520Zhao%2520and%2520Jing%2520Liao%26entry.1292438233%3D%2520%2520Vector%2520graphics%2520are%2520widely%2520used%2520in%2520digital%2520art%2520and%2520highly%2520favored%2520by%250Adesigners%2520due%2520to%2520their%2520scalability%2520and%2520layer-wise%2520properties.%2520However%252C%2520the%250Aprocess%2520of%2520creating%2520and%2520editing%2520vector%2520graphics%2520requires%2520creativity%2520and%2520design%250Aexpertise%252C%2520making%2520it%2520a%2520time-consuming%2520task.%2520Recent%2520advancements%2520in%250Atext-to-vector%2520%2528T2V%2529%2520generation%2520have%2520aimed%2520to%2520make%2520this%2520process%2520more%250Aaccessible.%2520However%252C%2520existing%2520T2V%2520methods%2520directly%2520optimize%2520control%2520points%2520of%250Avector%2520graphics%2520paths%252C%2520often%2520resulting%2520in%2520intersecting%2520or%2520jagged%2520paths%2520due%2520to%250Athe%2520lack%2520of%2520geometry%2520constraints.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520neural%2520path%2520representation%2520by%2520designing%2520a%2520dual-branch%2520Variational%250AAutoencoder%2520%2528VAE%2529%2520that%2520learns%2520the%2520path%2520latent%2520space%2520from%2520both%2520sequence%2520and%250Aimage%2520modalities.%2520By%2520optimizing%2520the%2520combination%2520of%2520neural%2520paths%252C%2520we%2520can%250Aincorporate%2520geometric%2520constraints%2520while%2520preserving%2520expressivity%2520in%2520generated%250ASVGs.%2520Furthermore%252C%2520we%2520introduce%2520a%2520two-stage%2520path%2520optimization%2520method%2520to%2520improve%250Athe%2520visual%2520and%2520topological%2520quality%2520of%2520generated%2520SVGs.%2520In%2520the%2520first%2520stage%252C%2520a%250Apre-trained%2520text-to-image%2520diffusion%2520model%2520guides%2520the%2520initial%2520generation%2520of%250Acomplex%2520vector%2520graphics%2520through%2520the%2520Variational%2520Score%2520Distillation%2520%2528VSD%2529%250Aprocess.%2520In%2520the%2520second%2520stage%252C%2520we%2520refine%2520the%2520graphics%2520using%2520a%2520layer-wise%2520image%250Avectorization%2520strategy%2520to%2520achieve%2520clearer%2520elements%2520and%2520structure.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%2520extensive%2520experiments%2520and%250Ashowcase%2520various%2520applications.%2520The%2520project%2520page%2520is%250Ahttps%253A//intchous.github.io/T2V-NPR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation&entry.906535625=Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Jing%20Liao&entry.1292438233=%20%20Vector%20graphics%20are%20widely%20used%20in%20digital%20art%20and%20highly%20favored%20by%0Adesigners%20due%20to%20their%20scalability%20and%20layer-wise%20properties.%20However%2C%20the%0Aprocess%20of%20creating%20and%20editing%20vector%20graphics%20requires%20creativity%20and%20design%0Aexpertise%2C%20making%20it%20a%20time-consuming%20task.%20Recent%20advancements%20in%0Atext-to-vector%20%28T2V%29%20generation%20have%20aimed%20to%20make%20this%20process%20more%0Aaccessible.%20However%2C%20existing%20T2V%20methods%20directly%20optimize%20control%20points%20of%0Avector%20graphics%20paths%2C%20often%20resulting%20in%20intersecting%20or%20jagged%20paths%20due%20to%0Athe%20lack%20of%20geometry%20constraints.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20neural%20path%20representation%20by%20designing%20a%20dual-branch%20Variational%0AAutoencoder%20%28VAE%29%20that%20learns%20the%20path%20latent%20space%20from%20both%20sequence%20and%0Aimage%20modalities.%20By%20optimizing%20the%20combination%20of%20neural%20paths%2C%20we%20can%0Aincorporate%20geometric%20constraints%20while%20preserving%20expressivity%20in%20generated%0ASVGs.%20Furthermore%2C%20we%20introduce%20a%20two-stage%20path%20optimization%20method%20to%20improve%0Athe%20visual%20and%20topological%20quality%20of%20generated%20SVGs.%20In%20the%20first%20stage%2C%20a%0Apre-trained%20text-to-image%20diffusion%20model%20guides%20the%20initial%20generation%20of%0Acomplex%20vector%20graphics%20through%20the%20Variational%20Score%20Distillation%20%28VSD%29%0Aprocess.%20In%20the%20second%20stage%2C%20we%20refine%20the%20graphics%20using%20a%20layer-wise%20image%0Avectorization%20strategy%20to%20achieve%20clearer%20elements%20and%20structure.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%20experiments%20and%0Ashowcase%20various%20applications.%20The%20project%20page%20is%0Ahttps%3A//intchous.github.io/T2V-NPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10317v2&entry.124074799=Read"},
{"title": "Images that Sound: Composing Images and Sounds on a Single Canvas", "author": "Ziyang Chen and Daniel Geng and Andrew Owens", "abstract": "  Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these spectrograms images that sound. Our\napproach is simple and zero-shot, and it leverages pre-trained text-to-image\nand text-to-spectrogram diffusion models that operate in a shared latent space.\nDuring the reverse process, we denoise noisy latents with both the audio and\nimage diffusion models in parallel, resulting in a sample that is likely under\nboth models. Through quantitative evaluations and perceptual studies, we find\nthat our method successfully generates spectrograms that align with a desired\naudio prompt while also taking the visual appearance of a desired image prompt.\nPlease see our project page for video results:\nhttps://ificl.github.io/images-that-sound/\n", "link": "http://arxiv.org/abs/2405.12221v1", "date": "2024-05-20", "relevancy": 2.1773, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5717}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5284}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Images%20that%20Sound%3A%20Composing%20Images%20and%20Sounds%20on%20a%20Single%20Canvas&body=Title%3A%20Images%20that%20Sound%3A%20Composing%20Images%20and%20Sounds%20on%20a%20Single%20Canvas%0AAuthor%3A%20Ziyang%20Chen%20and%20Daniel%20Geng%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20Spectrograms%20are%202D%20representations%20of%20sound%20that%20look%20very%20different%20from%0Athe%20images%20found%20in%20our%20visual%20world.%20And%20natural%20images%2C%20when%20played%20as%0Aspectrograms%2C%20make%20unnatural%20sounds.%20In%20this%20paper%2C%20we%20show%20that%20it%20is%20possible%0Ato%20synthesize%20spectrograms%20that%20simultaneously%20look%20like%20natural%20images%20and%0Asound%20like%20natural%20audio.%20We%20call%20these%20spectrograms%20images%20that%20sound.%20Our%0Aapproach%20is%20simple%20and%20zero-shot%2C%20and%20it%20leverages%20pre-trained%20text-to-image%0Aand%20text-to-spectrogram%20diffusion%20models%20that%20operate%20in%20a%20shared%20latent%20space.%0ADuring%20the%20reverse%20process%2C%20we%20denoise%20noisy%20latents%20with%20both%20the%20audio%20and%0Aimage%20diffusion%20models%20in%20parallel%2C%20resulting%20in%20a%20sample%20that%20is%20likely%20under%0Aboth%20models.%20Through%20quantitative%20evaluations%20and%20perceptual%20studies%2C%20we%20find%0Athat%20our%20method%20successfully%20generates%20spectrograms%20that%20align%20with%20a%20desired%0Aaudio%20prompt%20while%20also%20taking%20the%20visual%20appearance%20of%20a%20desired%20image%20prompt.%0APlease%20see%20our%20project%20page%20for%20video%20results%3A%0Ahttps%3A//ificl.github.io/images-that-sound/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImages%2520that%2520Sound%253A%2520Composing%2520Images%2520and%2520Sounds%2520on%2520a%2520Single%2520Canvas%26entry.906535625%3DZiyang%2520Chen%2520and%2520Daniel%2520Geng%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520Spectrograms%2520are%25202D%2520representations%2520of%2520sound%2520that%2520look%2520very%2520different%2520from%250Athe%2520images%2520found%2520in%2520our%2520visual%2520world.%2520And%2520natural%2520images%252C%2520when%2520played%2520as%250Aspectrograms%252C%2520make%2520unnatural%2520sounds.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520it%2520is%2520possible%250Ato%2520synthesize%2520spectrograms%2520that%2520simultaneously%2520look%2520like%2520natural%2520images%2520and%250Asound%2520like%2520natural%2520audio.%2520We%2520call%2520these%2520spectrograms%2520images%2520that%2520sound.%2520Our%250Aapproach%2520is%2520simple%2520and%2520zero-shot%252C%2520and%2520it%2520leverages%2520pre-trained%2520text-to-image%250Aand%2520text-to-spectrogram%2520diffusion%2520models%2520that%2520operate%2520in%2520a%2520shared%2520latent%2520space.%250ADuring%2520the%2520reverse%2520process%252C%2520we%2520denoise%2520noisy%2520latents%2520with%2520both%2520the%2520audio%2520and%250Aimage%2520diffusion%2520models%2520in%2520parallel%252C%2520resulting%2520in%2520a%2520sample%2520that%2520is%2520likely%2520under%250Aboth%2520models.%2520Through%2520quantitative%2520evaluations%2520and%2520perceptual%2520studies%252C%2520we%2520find%250Athat%2520our%2520method%2520successfully%2520generates%2520spectrograms%2520that%2520align%2520with%2520a%2520desired%250Aaudio%2520prompt%2520while%2520also%2520taking%2520the%2520visual%2520appearance%2520of%2520a%2520desired%2520image%2520prompt.%250APlease%2520see%2520our%2520project%2520page%2520for%2520video%2520results%253A%250Ahttps%253A//ificl.github.io/images-that-sound/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Images%20that%20Sound%3A%20Composing%20Images%20and%20Sounds%20on%20a%20Single%20Canvas&entry.906535625=Ziyang%20Chen%20and%20Daniel%20Geng%20and%20Andrew%20Owens&entry.1292438233=%20%20Spectrograms%20are%202D%20representations%20of%20sound%20that%20look%20very%20different%20from%0Athe%20images%20found%20in%20our%20visual%20world.%20And%20natural%20images%2C%20when%20played%20as%0Aspectrograms%2C%20make%20unnatural%20sounds.%20In%20this%20paper%2C%20we%20show%20that%20it%20is%20possible%0Ato%20synthesize%20spectrograms%20that%20simultaneously%20look%20like%20natural%20images%20and%0Asound%20like%20natural%20audio.%20We%20call%20these%20spectrograms%20images%20that%20sound.%20Our%0Aapproach%20is%20simple%20and%20zero-shot%2C%20and%20it%20leverages%20pre-trained%20text-to-image%0Aand%20text-to-spectrogram%20diffusion%20models%20that%20operate%20in%20a%20shared%20latent%20space.%0ADuring%20the%20reverse%20process%2C%20we%20denoise%20noisy%20latents%20with%20both%20the%20audio%20and%0Aimage%20diffusion%20models%20in%20parallel%2C%20resulting%20in%20a%20sample%20that%20is%20likely%20under%0Aboth%20models.%20Through%20quantitative%20evaluations%20and%20perceptual%20studies%2C%20we%20find%0Athat%20our%20method%20successfully%20generates%20spectrograms%20that%20align%20with%20a%20desired%0Aaudio%20prompt%20while%20also%20taking%20the%20visual%20appearance%20of%20a%20desired%20image%20prompt.%0APlease%20see%20our%20project%20page%20for%20video%20results%3A%0Ahttps%3A//ificl.github.io/images-that-sound/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12221v1&entry.124074799=Read"},
{"title": "State of the Practice for Medical Imaging Software", "author": "W. Spencer Smith and Ao Dong and Jacques Carette and Michael D. Noseworthy", "abstract": "  We selected 29 medical imaging projects from 48 candidates, assessed 10\nsoftware qualities by answering 108 questions for each software project, and\ninterviewed 8 of the 29 development teams. Based on the quantitative data, we\nranked the MI software with the Analytic Hierarchy Process (AHP). The four\ntop-ranked software products are 3D Slicer, ImageJ, Fiji, and OHIF Viewer.\nGenerally, MI software is in a healthy state as shown by the following: we\nobserved 88% of the documentation artifacts recommended by research software\ndevelopment guidelines, 100% of MI projects use version control tools, and\ndevelopers appear to use the common quasi-agile research software development\nprocess. However, the current state of the practice deviates from the existing\nguidelines because of the rarity of some recommended artifacts, low usage of\ncontinuous integration (17% of the projects), low use of unit testing (about\n50% of projects), and room for improvement with documentation (six of nine\ndevelopers felt their documentation was not clear enough). From interviewing\nthe developers, we identified five pain points and two qualities of potential\nconcern: lack of development time, lack of funding, technology hurdles,\nensuring correctness, usability, maintainability, and reproducibility. The\ninterviewees proposed strategies to improve the state of the practice, to\naddress the identified pain points, and to improve software quality. Combining\ntheir ideas with ours, we have the following list of recommendations: increase\ndocumentation, increase testing by enriching datasets, increase continuous\nintegration usage, move to web applications, employ linters, use peer reviews,\ndesign for change, add assurance cases, and incorporate a \"Generate All Things\"\napproach.\n", "link": "http://arxiv.org/abs/2405.12171v1", "date": "2024-05-20", "relevancy": 2.1743, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4503}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20of%20the%20Practice%20for%20Medical%20Imaging%20Software&body=Title%3A%20State%20of%20the%20Practice%20for%20Medical%20Imaging%20Software%0AAuthor%3A%20W.%20Spencer%20Smith%20and%20Ao%20Dong%20and%20Jacques%20Carette%20and%20Michael%20D.%20Noseworthy%0AAbstract%3A%20%20%20We%20selected%2029%20medical%20imaging%20projects%20from%2048%20candidates%2C%20assessed%2010%0Asoftware%20qualities%20by%20answering%20108%20questions%20for%20each%20software%20project%2C%20and%0Ainterviewed%208%20of%20the%2029%20development%20teams.%20Based%20on%20the%20quantitative%20data%2C%20we%0Aranked%20the%20MI%20software%20with%20the%20Analytic%20Hierarchy%20Process%20%28AHP%29.%20The%20four%0Atop-ranked%20software%20products%20are%203D%20Slicer%2C%20ImageJ%2C%20Fiji%2C%20and%20OHIF%20Viewer.%0AGenerally%2C%20MI%20software%20is%20in%20a%20healthy%20state%20as%20shown%20by%20the%20following%3A%20we%0Aobserved%2088%25%20of%20the%20documentation%20artifacts%20recommended%20by%20research%20software%0Adevelopment%20guidelines%2C%20100%25%20of%20MI%20projects%20use%20version%20control%20tools%2C%20and%0Adevelopers%20appear%20to%20use%20the%20common%20quasi-agile%20research%20software%20development%0Aprocess.%20However%2C%20the%20current%20state%20of%20the%20practice%20deviates%20from%20the%20existing%0Aguidelines%20because%20of%20the%20rarity%20of%20some%20recommended%20artifacts%2C%20low%20usage%20of%0Acontinuous%20integration%20%2817%25%20of%20the%20projects%29%2C%20low%20use%20of%20unit%20testing%20%28about%0A50%25%20of%20projects%29%2C%20and%20room%20for%20improvement%20with%20documentation%20%28six%20of%20nine%0Adevelopers%20felt%20their%20documentation%20was%20not%20clear%20enough%29.%20From%20interviewing%0Athe%20developers%2C%20we%20identified%20five%20pain%20points%20and%20two%20qualities%20of%20potential%0Aconcern%3A%20lack%20of%20development%20time%2C%20lack%20of%20funding%2C%20technology%20hurdles%2C%0Aensuring%20correctness%2C%20usability%2C%20maintainability%2C%20and%20reproducibility.%20The%0Ainterviewees%20proposed%20strategies%20to%20improve%20the%20state%20of%20the%20practice%2C%20to%0Aaddress%20the%20identified%20pain%20points%2C%20and%20to%20improve%20software%20quality.%20Combining%0Atheir%20ideas%20with%20ours%2C%20we%20have%20the%20following%20list%20of%20recommendations%3A%20increase%0Adocumentation%2C%20increase%20testing%20by%20enriching%20datasets%2C%20increase%20continuous%0Aintegration%20usage%2C%20move%20to%20web%20applications%2C%20employ%20linters%2C%20use%20peer%20reviews%2C%0Adesign%20for%20change%2C%20add%20assurance%20cases%2C%20and%20incorporate%20a%20%22Generate%20All%20Things%22%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520of%2520the%2520Practice%2520for%2520Medical%2520Imaging%2520Software%26entry.906535625%3DW.%2520Spencer%2520Smith%2520and%2520Ao%2520Dong%2520and%2520Jacques%2520Carette%2520and%2520Michael%2520D.%2520Noseworthy%26entry.1292438233%3D%2520%2520We%2520selected%252029%2520medical%2520imaging%2520projects%2520from%252048%2520candidates%252C%2520assessed%252010%250Asoftware%2520qualities%2520by%2520answering%2520108%2520questions%2520for%2520each%2520software%2520project%252C%2520and%250Ainterviewed%25208%2520of%2520the%252029%2520development%2520teams.%2520Based%2520on%2520the%2520quantitative%2520data%252C%2520we%250Aranked%2520the%2520MI%2520software%2520with%2520the%2520Analytic%2520Hierarchy%2520Process%2520%2528AHP%2529.%2520The%2520four%250Atop-ranked%2520software%2520products%2520are%25203D%2520Slicer%252C%2520ImageJ%252C%2520Fiji%252C%2520and%2520OHIF%2520Viewer.%250AGenerally%252C%2520MI%2520software%2520is%2520in%2520a%2520healthy%2520state%2520as%2520shown%2520by%2520the%2520following%253A%2520we%250Aobserved%252088%2525%2520of%2520the%2520documentation%2520artifacts%2520recommended%2520by%2520research%2520software%250Adevelopment%2520guidelines%252C%2520100%2525%2520of%2520MI%2520projects%2520use%2520version%2520control%2520tools%252C%2520and%250Adevelopers%2520appear%2520to%2520use%2520the%2520common%2520quasi-agile%2520research%2520software%2520development%250Aprocess.%2520However%252C%2520the%2520current%2520state%2520of%2520the%2520practice%2520deviates%2520from%2520the%2520existing%250Aguidelines%2520because%2520of%2520the%2520rarity%2520of%2520some%2520recommended%2520artifacts%252C%2520low%2520usage%2520of%250Acontinuous%2520integration%2520%252817%2525%2520of%2520the%2520projects%2529%252C%2520low%2520use%2520of%2520unit%2520testing%2520%2528about%250A50%2525%2520of%2520projects%2529%252C%2520and%2520room%2520for%2520improvement%2520with%2520documentation%2520%2528six%2520of%2520nine%250Adevelopers%2520felt%2520their%2520documentation%2520was%2520not%2520clear%2520enough%2529.%2520From%2520interviewing%250Athe%2520developers%252C%2520we%2520identified%2520five%2520pain%2520points%2520and%2520two%2520qualities%2520of%2520potential%250Aconcern%253A%2520lack%2520of%2520development%2520time%252C%2520lack%2520of%2520funding%252C%2520technology%2520hurdles%252C%250Aensuring%2520correctness%252C%2520usability%252C%2520maintainability%252C%2520and%2520reproducibility.%2520The%250Ainterviewees%2520proposed%2520strategies%2520to%2520improve%2520the%2520state%2520of%2520the%2520practice%252C%2520to%250Aaddress%2520the%2520identified%2520pain%2520points%252C%2520and%2520to%2520improve%2520software%2520quality.%2520Combining%250Atheir%2520ideas%2520with%2520ours%252C%2520we%2520have%2520the%2520following%2520list%2520of%2520recommendations%253A%2520increase%250Adocumentation%252C%2520increase%2520testing%2520by%2520enriching%2520datasets%252C%2520increase%2520continuous%250Aintegration%2520usage%252C%2520move%2520to%2520web%2520applications%252C%2520employ%2520linters%252C%2520use%2520peer%2520reviews%252C%250Adesign%2520for%2520change%252C%2520add%2520assurance%2520cases%252C%2520and%2520incorporate%2520a%2520%2522Generate%2520All%2520Things%2522%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20of%20the%20Practice%20for%20Medical%20Imaging%20Software&entry.906535625=W.%20Spencer%20Smith%20and%20Ao%20Dong%20and%20Jacques%20Carette%20and%20Michael%20D.%20Noseworthy&entry.1292438233=%20%20We%20selected%2029%20medical%20imaging%20projects%20from%2048%20candidates%2C%20assessed%2010%0Asoftware%20qualities%20by%20answering%20108%20questions%20for%20each%20software%20project%2C%20and%0Ainterviewed%208%20of%20the%2029%20development%20teams.%20Based%20on%20the%20quantitative%20data%2C%20we%0Aranked%20the%20MI%20software%20with%20the%20Analytic%20Hierarchy%20Process%20%28AHP%29.%20The%20four%0Atop-ranked%20software%20products%20are%203D%20Slicer%2C%20ImageJ%2C%20Fiji%2C%20and%20OHIF%20Viewer.%0AGenerally%2C%20MI%20software%20is%20in%20a%20healthy%20state%20as%20shown%20by%20the%20following%3A%20we%0Aobserved%2088%25%20of%20the%20documentation%20artifacts%20recommended%20by%20research%20software%0Adevelopment%20guidelines%2C%20100%25%20of%20MI%20projects%20use%20version%20control%20tools%2C%20and%0Adevelopers%20appear%20to%20use%20the%20common%20quasi-agile%20research%20software%20development%0Aprocess.%20However%2C%20the%20current%20state%20of%20the%20practice%20deviates%20from%20the%20existing%0Aguidelines%20because%20of%20the%20rarity%20of%20some%20recommended%20artifacts%2C%20low%20usage%20of%0Acontinuous%20integration%20%2817%25%20of%20the%20projects%29%2C%20low%20use%20of%20unit%20testing%20%28about%0A50%25%20of%20projects%29%2C%20and%20room%20for%20improvement%20with%20documentation%20%28six%20of%20nine%0Adevelopers%20felt%20their%20documentation%20was%20not%20clear%20enough%29.%20From%20interviewing%0Athe%20developers%2C%20we%20identified%20five%20pain%20points%20and%20two%20qualities%20of%20potential%0Aconcern%3A%20lack%20of%20development%20time%2C%20lack%20of%20funding%2C%20technology%20hurdles%2C%0Aensuring%20correctness%2C%20usability%2C%20maintainability%2C%20and%20reproducibility.%20The%0Ainterviewees%20proposed%20strategies%20to%20improve%20the%20state%20of%20the%20practice%2C%20to%0Aaddress%20the%20identified%20pain%20points%2C%20and%20to%20improve%20software%20quality.%20Combining%0Atheir%20ideas%20with%20ours%2C%20we%20have%20the%20following%20list%20of%20recommendations%3A%20increase%0Adocumentation%2C%20increase%20testing%20by%20enriching%20datasets%2C%20increase%20continuous%0Aintegration%20usage%2C%20move%20to%20web%20applications%2C%20employ%20linters%2C%20use%20peer%20reviews%2C%0Adesign%20for%20change%2C%20add%20assurance%20cases%2C%20and%20incorporate%20a%20%22Generate%20All%20Things%22%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12171v1&entry.124074799=Read"},
{"title": "DIRECT: Deep Active Learning under Imbalance and Label Noise", "author": "Shyam Nuggehalli and Jifan Zhang and Lalit Jain and Robert Nowak", "abstract": "  Class imbalance is a prevalent issue in real world machine learning\napplications, often leading to poor performance in rare and minority classes.\nWith an abundance of wild unlabeled data, active learning is perhaps the most\neffective technique in solving the problem at its root -- collecting a more\nbalanced and informative set of labeled examples during annotation. Label noise\nis another common issue in data annotation jobs, which is especially\nchallenging for active learning methods. In this work, we conduct the first\nstudy of active learning under both class imbalance and label noise. We propose\na novel algorithm that robustly identifies the class separation threshold and\nannotates the most uncertain examples that are closest from it. Through a novel\nreduction to one-dimensional active learning, our algorithm DIRECT is able to\nleverage the classic active learning literature to address issues such as batch\nlabeling and tolerance towards label noise. We present extensive experiments on\nimbalanced datasets with and without label noise. Our results demonstrate that\nDIRECT can save more than 60% of the annotation budget compared to state-of-art\nactive learning algorithms and more than 80% of annotation budget compared to\nrandom sampling.\n", "link": "http://arxiv.org/abs/2312.09196v3", "date": "2024-05-20", "relevancy": 2.1244, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5714}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIRECT%3A%20Deep%20Active%20Learning%20under%20Imbalance%20and%20Label%20Noise&body=Title%3A%20DIRECT%3A%20Deep%20Active%20Learning%20under%20Imbalance%20and%20Label%20Noise%0AAuthor%3A%20Shyam%20Nuggehalli%20and%20Jifan%20Zhang%20and%20Lalit%20Jain%20and%20Robert%20Nowak%0AAbstract%3A%20%20%20Class%20imbalance%20is%20a%20prevalent%20issue%20in%20real%20world%20machine%20learning%0Aapplications%2C%20often%20leading%20to%20poor%20performance%20in%20rare%20and%20minority%20classes.%0AWith%20an%20abundance%20of%20wild%20unlabeled%20data%2C%20active%20learning%20is%20perhaps%20the%20most%0Aeffective%20technique%20in%20solving%20the%20problem%20at%20its%20root%20--%20collecting%20a%20more%0Abalanced%20and%20informative%20set%20of%20labeled%20examples%20during%20annotation.%20Label%20noise%0Ais%20another%20common%20issue%20in%20data%20annotation%20jobs%2C%20which%20is%20especially%0Achallenging%20for%20active%20learning%20methods.%20In%20this%20work%2C%20we%20conduct%20the%20first%0Astudy%20of%20active%20learning%20under%20both%20class%20imbalance%20and%20label%20noise.%20We%20propose%0Aa%20novel%20algorithm%20that%20robustly%20identifies%20the%20class%20separation%20threshold%20and%0Aannotates%20the%20most%20uncertain%20examples%20that%20are%20closest%20from%20it.%20Through%20a%20novel%0Areduction%20to%20one-dimensional%20active%20learning%2C%20our%20algorithm%20DIRECT%20is%20able%20to%0Aleverage%20the%20classic%20active%20learning%20literature%20to%20address%20issues%20such%20as%20batch%0Alabeling%20and%20tolerance%20towards%20label%20noise.%20We%20present%20extensive%20experiments%20on%0Aimbalanced%20datasets%20with%20and%20without%20label%20noise.%20Our%20results%20demonstrate%20that%0ADIRECT%20can%20save%20more%20than%2060%25%20of%20the%20annotation%20budget%20compared%20to%20state-of-art%0Aactive%20learning%20algorithms%20and%20more%20than%2080%25%20of%20annotation%20budget%20compared%20to%0Arandom%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIRECT%253A%2520Deep%2520Active%2520Learning%2520under%2520Imbalance%2520and%2520Label%2520Noise%26entry.906535625%3DShyam%2520Nuggehalli%2520and%2520Jifan%2520Zhang%2520and%2520Lalit%2520Jain%2520and%2520Robert%2520Nowak%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520is%2520a%2520prevalent%2520issue%2520in%2520real%2520world%2520machine%2520learning%250Aapplications%252C%2520often%2520leading%2520to%2520poor%2520performance%2520in%2520rare%2520and%2520minority%2520classes.%250AWith%2520an%2520abundance%2520of%2520wild%2520unlabeled%2520data%252C%2520active%2520learning%2520is%2520perhaps%2520the%2520most%250Aeffective%2520technique%2520in%2520solving%2520the%2520problem%2520at%2520its%2520root%2520--%2520collecting%2520a%2520more%250Abalanced%2520and%2520informative%2520set%2520of%2520labeled%2520examples%2520during%2520annotation.%2520Label%2520noise%250Ais%2520another%2520common%2520issue%2520in%2520data%2520annotation%2520jobs%252C%2520which%2520is%2520especially%250Achallenging%2520for%2520active%2520learning%2520methods.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%250Astudy%2520of%2520active%2520learning%2520under%2520both%2520class%2520imbalance%2520and%2520label%2520noise.%2520We%2520propose%250Aa%2520novel%2520algorithm%2520that%2520robustly%2520identifies%2520the%2520class%2520separation%2520threshold%2520and%250Aannotates%2520the%2520most%2520uncertain%2520examples%2520that%2520are%2520closest%2520from%2520it.%2520Through%2520a%2520novel%250Areduction%2520to%2520one-dimensional%2520active%2520learning%252C%2520our%2520algorithm%2520DIRECT%2520is%2520able%2520to%250Aleverage%2520the%2520classic%2520active%2520learning%2520literature%2520to%2520address%2520issues%2520such%2520as%2520batch%250Alabeling%2520and%2520tolerance%2520towards%2520label%2520noise.%2520We%2520present%2520extensive%2520experiments%2520on%250Aimbalanced%2520datasets%2520with%2520and%2520without%2520label%2520noise.%2520Our%2520results%2520demonstrate%2520that%250ADIRECT%2520can%2520save%2520more%2520than%252060%2525%2520of%2520the%2520annotation%2520budget%2520compared%2520to%2520state-of-art%250Aactive%2520learning%2520algorithms%2520and%2520more%2520than%252080%2525%2520of%2520annotation%2520budget%2520compared%2520to%250Arandom%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIRECT%3A%20Deep%20Active%20Learning%20under%20Imbalance%20and%20Label%20Noise&entry.906535625=Shyam%20Nuggehalli%20and%20Jifan%20Zhang%20and%20Lalit%20Jain%20and%20Robert%20Nowak&entry.1292438233=%20%20Class%20imbalance%20is%20a%20prevalent%20issue%20in%20real%20world%20machine%20learning%0Aapplications%2C%20often%20leading%20to%20poor%20performance%20in%20rare%20and%20minority%20classes.%0AWith%20an%20abundance%20of%20wild%20unlabeled%20data%2C%20active%20learning%20is%20perhaps%20the%20most%0Aeffective%20technique%20in%20solving%20the%20problem%20at%20its%20root%20--%20collecting%20a%20more%0Abalanced%20and%20informative%20set%20of%20labeled%20examples%20during%20annotation.%20Label%20noise%0Ais%20another%20common%20issue%20in%20data%20annotation%20jobs%2C%20which%20is%20especially%0Achallenging%20for%20active%20learning%20methods.%20In%20this%20work%2C%20we%20conduct%20the%20first%0Astudy%20of%20active%20learning%20under%20both%20class%20imbalance%20and%20label%20noise.%20We%20propose%0Aa%20novel%20algorithm%20that%20robustly%20identifies%20the%20class%20separation%20threshold%20and%0Aannotates%20the%20most%20uncertain%20examples%20that%20are%20closest%20from%20it.%20Through%20a%20novel%0Areduction%20to%20one-dimensional%20active%20learning%2C%20our%20algorithm%20DIRECT%20is%20able%20to%0Aleverage%20the%20classic%20active%20learning%20literature%20to%20address%20issues%20such%20as%20batch%0Alabeling%20and%20tolerance%20towards%20label%20noise.%20We%20present%20extensive%20experiments%20on%0Aimbalanced%20datasets%20with%20and%20without%20label%20noise.%20Our%20results%20demonstrate%20that%0ADIRECT%20can%20save%20more%20than%2060%25%20of%20the%20annotation%20budget%20compared%20to%20state-of-art%0Aactive%20learning%20algorithms%20and%20more%20than%2080%25%20of%20annotation%20budget%20compared%20to%0Arandom%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09196v3&entry.124074799=Read"},
{"title": "Data Augmentation for Text-based Person Retrieval Using Large Language\n  Models", "author": "Zheng Li and Lijia Si and Caili Guo and Yang Yang and Qiushi Cao", "abstract": "  Text-based Person Retrieval (TPR) aims to retrieve person images that match\nthe description given a text query. The performance improvement of the TPR\nmodel relies on high-quality data for supervised training. However, it is\ndifficult to construct a large-scale, high-quality TPR dataset due to expensive\nannotation and privacy protection. Recently, Large Language Models (LLMs) have\napproached or even surpassed human performance on many NLP tasks, creating the\npossibility to expand high-quality TPR datasets. This paper proposes an\nLLM-based Data Augmentation (LLM-DA) method for TPR. LLM-DA uses LLMs to\nrewrite the text in the current TPR dataset, achieving high-quality expansion\nof the dataset concisely and efficiently. These rewritten texts are able to\nincrease the diversity of vocabulary and sentence structure while retaining the\noriginal key concepts and semantic information. In order to alleviate the\nhallucinations of LLMs, LLM-DA introduces a Text Faithfulness Filter (TFF) to\nfilter out unfaithful rewritten text. To balance the contributions of original\ntext and augmented text, a Balanced Sampling Strategy (BSS) is proposed to\ncontrol the proportion of original text and augmented text used for training.\nLLM-DA is a plug-and-play method that can be easily integrated into various TPR\nmodels. Comprehensive experiments on three TPR benchmarks show that LLM-DA can\nimprove the retrieval performance of current TPR models.\n", "link": "http://arxiv.org/abs/2405.11971v1", "date": "2024-05-20", "relevancy": 2.1191, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Text-based%20Person%20Retrieval%20Using%20Large%20Language%0A%20%20Models&body=Title%3A%20Data%20Augmentation%20for%20Text-based%20Person%20Retrieval%20Using%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zheng%20Li%20and%20Lijia%20Si%20and%20Caili%20Guo%20and%20Yang%20Yang%20and%20Qiushi%20Cao%0AAbstract%3A%20%20%20Text-based%20Person%20Retrieval%20%28TPR%29%20aims%20to%20retrieve%20person%20images%20that%20match%0Athe%20description%20given%20a%20text%20query.%20The%20performance%20improvement%20of%20the%20TPR%0Amodel%20relies%20on%20high-quality%20data%20for%20supervised%20training.%20However%2C%20it%20is%0Adifficult%20to%20construct%20a%20large-scale%2C%20high-quality%20TPR%20dataset%20due%20to%20expensive%0Aannotation%20and%20privacy%20protection.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%0Aapproached%20or%20even%20surpassed%20human%20performance%20on%20many%20NLP%20tasks%2C%20creating%20the%0Apossibility%20to%20expand%20high-quality%20TPR%20datasets.%20This%20paper%20proposes%20an%0ALLM-based%20Data%20Augmentation%20%28LLM-DA%29%20method%20for%20TPR.%20LLM-DA%20uses%20LLMs%20to%0Arewrite%20the%20text%20in%20the%20current%20TPR%20dataset%2C%20achieving%20high-quality%20expansion%0Aof%20the%20dataset%20concisely%20and%20efficiently.%20These%20rewritten%20texts%20are%20able%20to%0Aincrease%20the%20diversity%20of%20vocabulary%20and%20sentence%20structure%20while%20retaining%20the%0Aoriginal%20key%20concepts%20and%20semantic%20information.%20In%20order%20to%20alleviate%20the%0Ahallucinations%20of%20LLMs%2C%20LLM-DA%20introduces%20a%20Text%20Faithfulness%20Filter%20%28TFF%29%20to%0Afilter%20out%20unfaithful%20rewritten%20text.%20To%20balance%20the%20contributions%20of%20original%0Atext%20and%20augmented%20text%2C%20a%20Balanced%20Sampling%20Strategy%20%28BSS%29%20is%20proposed%20to%0Acontrol%20the%20proportion%20of%20original%20text%20and%20augmented%20text%20used%20for%20training.%0ALLM-DA%20is%20a%20plug-and-play%20method%20that%20can%20be%20easily%20integrated%20into%20various%20TPR%0Amodels.%20Comprehensive%20experiments%20on%20three%20TPR%20benchmarks%20show%20that%20LLM-DA%20can%0Aimprove%20the%20retrieval%20performance%20of%20current%20TPR%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520for%2520Text-based%2520Person%2520Retrieval%2520Using%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZheng%2520Li%2520and%2520Lijia%2520Si%2520and%2520Caili%2520Guo%2520and%2520Yang%2520Yang%2520and%2520Qiushi%2520Cao%26entry.1292438233%3D%2520%2520Text-based%2520Person%2520Retrieval%2520%2528TPR%2529%2520aims%2520to%2520retrieve%2520person%2520images%2520that%2520match%250Athe%2520description%2520given%2520a%2520text%2520query.%2520The%2520performance%2520improvement%2520of%2520the%2520TPR%250Amodel%2520relies%2520on%2520high-quality%2520data%2520for%2520supervised%2520training.%2520However%252C%2520it%2520is%250Adifficult%2520to%2520construct%2520a%2520large-scale%252C%2520high-quality%2520TPR%2520dataset%2520due%2520to%2520expensive%250Aannotation%2520and%2520privacy%2520protection.%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Aapproached%2520or%2520even%2520surpassed%2520human%2520performance%2520on%2520many%2520NLP%2520tasks%252C%2520creating%2520the%250Apossibility%2520to%2520expand%2520high-quality%2520TPR%2520datasets.%2520This%2520paper%2520proposes%2520an%250ALLM-based%2520Data%2520Augmentation%2520%2528LLM-DA%2529%2520method%2520for%2520TPR.%2520LLM-DA%2520uses%2520LLMs%2520to%250Arewrite%2520the%2520text%2520in%2520the%2520current%2520TPR%2520dataset%252C%2520achieving%2520high-quality%2520expansion%250Aof%2520the%2520dataset%2520concisely%2520and%2520efficiently.%2520These%2520rewritten%2520texts%2520are%2520able%2520to%250Aincrease%2520the%2520diversity%2520of%2520vocabulary%2520and%2520sentence%2520structure%2520while%2520retaining%2520the%250Aoriginal%2520key%2520concepts%2520and%2520semantic%2520information.%2520In%2520order%2520to%2520alleviate%2520the%250Ahallucinations%2520of%2520LLMs%252C%2520LLM-DA%2520introduces%2520a%2520Text%2520Faithfulness%2520Filter%2520%2528TFF%2529%2520to%250Afilter%2520out%2520unfaithful%2520rewritten%2520text.%2520To%2520balance%2520the%2520contributions%2520of%2520original%250Atext%2520and%2520augmented%2520text%252C%2520a%2520Balanced%2520Sampling%2520Strategy%2520%2528BSS%2529%2520is%2520proposed%2520to%250Acontrol%2520the%2520proportion%2520of%2520original%2520text%2520and%2520augmented%2520text%2520used%2520for%2520training.%250ALLM-DA%2520is%2520a%2520plug-and-play%2520method%2520that%2520can%2520be%2520easily%2520integrated%2520into%2520various%2520TPR%250Amodels.%2520Comprehensive%2520experiments%2520on%2520three%2520TPR%2520benchmarks%2520show%2520that%2520LLM-DA%2520can%250Aimprove%2520the%2520retrieval%2520performance%2520of%2520current%2520TPR%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Text-based%20Person%20Retrieval%20Using%20Large%20Language%0A%20%20Models&entry.906535625=Zheng%20Li%20and%20Lijia%20Si%20and%20Caili%20Guo%20and%20Yang%20Yang%20and%20Qiushi%20Cao&entry.1292438233=%20%20Text-based%20Person%20Retrieval%20%28TPR%29%20aims%20to%20retrieve%20person%20images%20that%20match%0Athe%20description%20given%20a%20text%20query.%20The%20performance%20improvement%20of%20the%20TPR%0Amodel%20relies%20on%20high-quality%20data%20for%20supervised%20training.%20However%2C%20it%20is%0Adifficult%20to%20construct%20a%20large-scale%2C%20high-quality%20TPR%20dataset%20due%20to%20expensive%0Aannotation%20and%20privacy%20protection.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%0Aapproached%20or%20even%20surpassed%20human%20performance%20on%20many%20NLP%20tasks%2C%20creating%20the%0Apossibility%20to%20expand%20high-quality%20TPR%20datasets.%20This%20paper%20proposes%20an%0ALLM-based%20Data%20Augmentation%20%28LLM-DA%29%20method%20for%20TPR.%20LLM-DA%20uses%20LLMs%20to%0Arewrite%20the%20text%20in%20the%20current%20TPR%20dataset%2C%20achieving%20high-quality%20expansion%0Aof%20the%20dataset%20concisely%20and%20efficiently.%20These%20rewritten%20texts%20are%20able%20to%0Aincrease%20the%20diversity%20of%20vocabulary%20and%20sentence%20structure%20while%20retaining%20the%0Aoriginal%20key%20concepts%20and%20semantic%20information.%20In%20order%20to%20alleviate%20the%0Ahallucinations%20of%20LLMs%2C%20LLM-DA%20introduces%20a%20Text%20Faithfulness%20Filter%20%28TFF%29%20to%0Afilter%20out%20unfaithful%20rewritten%20text.%20To%20balance%20the%20contributions%20of%20original%0Atext%20and%20augmented%20text%2C%20a%20Balanced%20Sampling%20Strategy%20%28BSS%29%20is%20proposed%20to%0Acontrol%20the%20proportion%20of%20original%20text%20and%20augmented%20text%20used%20for%20training.%0ALLM-DA%20is%20a%20plug-and-play%20method%20that%20can%20be%20easily%20integrated%20into%20various%20TPR%0Amodels.%20Comprehensive%20experiments%20on%20three%20TPR%20benchmarks%20show%20that%20LLM-DA%20can%0Aimprove%20the%20retrieval%20performance%20of%20current%20TPR%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11971v1&entry.124074799=Read"},
{"title": "An Active Learning Framework with a Class Balancing Strategy for Time\n  Series Classification", "author": "Shemonto Das", "abstract": "  Training machine learning models for classification tasks often requires\nlabeling numerous samples, which is costly and time-consuming, especially in\ntime series analysis. This research investigates Active Learning (AL)\nstrategies to reduce the amount of labeled data needed for effective time\nseries classification. Traditional AL techniques cannot control the selection\nof instances per class for labeling, leading to potential bias in\nclassification performance and instance selection, particularly in imbalanced\ntime series datasets. To address this, we propose a novel class-balancing\ninstance selection algorithm integrated with standard AL strategies. Our\napproach aims to select more instances from classes with fewer labeled\nexamples, thereby addressing imbalance in time series datasets. We demonstrate\nthe effectiveness of our AL framework in selecting informative data samples for\ntwo distinct domains of tactile texture recognition and industrial fault\ndetection. In robotics, our method achieves high-performance texture\ncategorization while significantly reducing labeled training data requirements\nto 70%. We also evaluate the impact of different sliding window time intervals\non robotic texture classification using AL strategies. In synthetic fiber\nmanufacturing, we adapt AL techniques to address the challenge of fault\nclassification, aiming to minimize data annotation cost and time for\nindustries. We also address real-life class imbalances in the multiclass\nindustrial anomalous dataset using our class-balancing instance algorithm\nintegrated with AL strategies. Overall, this thesis highlights the potential of\nour AL framework across these two distinct domains.\n", "link": "http://arxiv.org/abs/2405.12122v1", "date": "2024-05-20", "relevancy": 2.1161, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5222}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Active%20Learning%20Framework%20with%20a%20Class%20Balancing%20Strategy%20for%20Time%0A%20%20Series%20Classification&body=Title%3A%20An%20Active%20Learning%20Framework%20with%20a%20Class%20Balancing%20Strategy%20for%20Time%0A%20%20Series%20Classification%0AAuthor%3A%20Shemonto%20Das%0AAbstract%3A%20%20%20Training%20machine%20learning%20models%20for%20classification%20tasks%20often%20requires%0Alabeling%20numerous%20samples%2C%20which%20is%20costly%20and%20time-consuming%2C%20especially%20in%0Atime%20series%20analysis.%20This%20research%20investigates%20Active%20Learning%20%28AL%29%0Astrategies%20to%20reduce%20the%20amount%20of%20labeled%20data%20needed%20for%20effective%20time%0Aseries%20classification.%20Traditional%20AL%20techniques%20cannot%20control%20the%20selection%0Aof%20instances%20per%20class%20for%20labeling%2C%20leading%20to%20potential%20bias%20in%0Aclassification%20performance%20and%20instance%20selection%2C%20particularly%20in%20imbalanced%0Atime%20series%20datasets.%20To%20address%20this%2C%20we%20propose%20a%20novel%20class-balancing%0Ainstance%20selection%20algorithm%20integrated%20with%20standard%20AL%20strategies.%20Our%0Aapproach%20aims%20to%20select%20more%20instances%20from%20classes%20with%20fewer%20labeled%0Aexamples%2C%20thereby%20addressing%20imbalance%20in%20time%20series%20datasets.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20AL%20framework%20in%20selecting%20informative%20data%20samples%20for%0Atwo%20distinct%20domains%20of%20tactile%20texture%20recognition%20and%20industrial%20fault%0Adetection.%20In%20robotics%2C%20our%20method%20achieves%20high-performance%20texture%0Acategorization%20while%20significantly%20reducing%20labeled%20training%20data%20requirements%0Ato%2070%25.%20We%20also%20evaluate%20the%20impact%20of%20different%20sliding%20window%20time%20intervals%0Aon%20robotic%20texture%20classification%20using%20AL%20strategies.%20In%20synthetic%20fiber%0Amanufacturing%2C%20we%20adapt%20AL%20techniques%20to%20address%20the%20challenge%20of%20fault%0Aclassification%2C%20aiming%20to%20minimize%20data%20annotation%20cost%20and%20time%20for%0Aindustries.%20We%20also%20address%20real-life%20class%20imbalances%20in%20the%20multiclass%0Aindustrial%20anomalous%20dataset%20using%20our%20class-balancing%20instance%20algorithm%0Aintegrated%20with%20AL%20strategies.%20Overall%2C%20this%20thesis%20highlights%20the%20potential%20of%0Aour%20AL%20framework%20across%20these%20two%20distinct%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Active%2520Learning%2520Framework%2520with%2520a%2520Class%2520Balancing%2520Strategy%2520for%2520Time%250A%2520%2520Series%2520Classification%26entry.906535625%3DShemonto%2520Das%26entry.1292438233%3D%2520%2520Training%2520machine%2520learning%2520models%2520for%2520classification%2520tasks%2520often%2520requires%250Alabeling%2520numerous%2520samples%252C%2520which%2520is%2520costly%2520and%2520time-consuming%252C%2520especially%2520in%250Atime%2520series%2520analysis.%2520This%2520research%2520investigates%2520Active%2520Learning%2520%2528AL%2529%250Astrategies%2520to%2520reduce%2520the%2520amount%2520of%2520labeled%2520data%2520needed%2520for%2520effective%2520time%250Aseries%2520classification.%2520Traditional%2520AL%2520techniques%2520cannot%2520control%2520the%2520selection%250Aof%2520instances%2520per%2520class%2520for%2520labeling%252C%2520leading%2520to%2520potential%2520bias%2520in%250Aclassification%2520performance%2520and%2520instance%2520selection%252C%2520particularly%2520in%2520imbalanced%250Atime%2520series%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520class-balancing%250Ainstance%2520selection%2520algorithm%2520integrated%2520with%2520standard%2520AL%2520strategies.%2520Our%250Aapproach%2520aims%2520to%2520select%2520more%2520instances%2520from%2520classes%2520with%2520fewer%2520labeled%250Aexamples%252C%2520thereby%2520addressing%2520imbalance%2520in%2520time%2520series%2520datasets.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520AL%2520framework%2520in%2520selecting%2520informative%2520data%2520samples%2520for%250Atwo%2520distinct%2520domains%2520of%2520tactile%2520texture%2520recognition%2520and%2520industrial%2520fault%250Adetection.%2520In%2520robotics%252C%2520our%2520method%2520achieves%2520high-performance%2520texture%250Acategorization%2520while%2520significantly%2520reducing%2520labeled%2520training%2520data%2520requirements%250Ato%252070%2525.%2520We%2520also%2520evaluate%2520the%2520impact%2520of%2520different%2520sliding%2520window%2520time%2520intervals%250Aon%2520robotic%2520texture%2520classification%2520using%2520AL%2520strategies.%2520In%2520synthetic%2520fiber%250Amanufacturing%252C%2520we%2520adapt%2520AL%2520techniques%2520to%2520address%2520the%2520challenge%2520of%2520fault%250Aclassification%252C%2520aiming%2520to%2520minimize%2520data%2520annotation%2520cost%2520and%2520time%2520for%250Aindustries.%2520We%2520also%2520address%2520real-life%2520class%2520imbalances%2520in%2520the%2520multiclass%250Aindustrial%2520anomalous%2520dataset%2520using%2520our%2520class-balancing%2520instance%2520algorithm%250Aintegrated%2520with%2520AL%2520strategies.%2520Overall%252C%2520this%2520thesis%2520highlights%2520the%2520potential%2520of%250Aour%2520AL%2520framework%2520across%2520these%2520two%2520distinct%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Active%20Learning%20Framework%20with%20a%20Class%20Balancing%20Strategy%20for%20Time%0A%20%20Series%20Classification&entry.906535625=Shemonto%20Das&entry.1292438233=%20%20Training%20machine%20learning%20models%20for%20classification%20tasks%20often%20requires%0Alabeling%20numerous%20samples%2C%20which%20is%20costly%20and%20time-consuming%2C%20especially%20in%0Atime%20series%20analysis.%20This%20research%20investigates%20Active%20Learning%20%28AL%29%0Astrategies%20to%20reduce%20the%20amount%20of%20labeled%20data%20needed%20for%20effective%20time%0Aseries%20classification.%20Traditional%20AL%20techniques%20cannot%20control%20the%20selection%0Aof%20instances%20per%20class%20for%20labeling%2C%20leading%20to%20potential%20bias%20in%0Aclassification%20performance%20and%20instance%20selection%2C%20particularly%20in%20imbalanced%0Atime%20series%20datasets.%20To%20address%20this%2C%20we%20propose%20a%20novel%20class-balancing%0Ainstance%20selection%20algorithm%20integrated%20with%20standard%20AL%20strategies.%20Our%0Aapproach%20aims%20to%20select%20more%20instances%20from%20classes%20with%20fewer%20labeled%0Aexamples%2C%20thereby%20addressing%20imbalance%20in%20time%20series%20datasets.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20AL%20framework%20in%20selecting%20informative%20data%20samples%20for%0Atwo%20distinct%20domains%20of%20tactile%20texture%20recognition%20and%20industrial%20fault%0Adetection.%20In%20robotics%2C%20our%20method%20achieves%20high-performance%20texture%0Acategorization%20while%20significantly%20reducing%20labeled%20training%20data%20requirements%0Ato%2070%25.%20We%20also%20evaluate%20the%20impact%20of%20different%20sliding%20window%20time%20intervals%0Aon%20robotic%20texture%20classification%20using%20AL%20strategies.%20In%20synthetic%20fiber%0Amanufacturing%2C%20we%20adapt%20AL%20techniques%20to%20address%20the%20challenge%20of%20fault%0Aclassification%2C%20aiming%20to%20minimize%20data%20annotation%20cost%20and%20time%20for%0Aindustries.%20We%20also%20address%20real-life%20class%20imbalances%20in%20the%20multiclass%0Aindustrial%20anomalous%20dataset%20using%20our%20class-balancing%20instance%20algorithm%0Aintegrated%20with%20AL%20strategies.%20Overall%2C%20this%20thesis%20highlights%20the%20potential%20of%0Aour%20AL%20framework%20across%20these%20two%20distinct%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12122v1&entry.124074799=Read"},
{"title": "Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model\n  for Hyperspectral Image Classification", "author": "Weilian Zhou and Sei-Ichiro Kamata and Haipeng Wang and Man-Sing Wong and  Huiying and  Hou", "abstract": "  Hyperspectral image (HSI) classification is pivotal in the remote sensing\n(RS) field, particularly with the advancement of deep learning techniques.\nSequential models, adapted from the natural language processing (NLP) field\nsuch as Recurrent Neural Networks (RNNs) and Transformers, have been tailored\nto this task, offering a unique viewpoint. However, several challenges persist\n1) RNNs struggle with centric feature aggregation and are sensitive to\ninterfering pixels, 2) Transformers require significant computational resources\nand often underperform with limited HSI training samples, and 3) Current\nscanning methods for converting images into sequence-data are simplistic and\ninefficient. In response, this study introduces the innovative Mamba-in-Mamba\n(MiM) architecture for HSI classification, the first attempt of deploying State\nSpace Model (SSM) in this task. The MiM model includes 1) A novel centralized\nMamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2)\nA Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask\n(GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for\nenhanced feature generation and concentration, and 3) A Weighted MCS Fusion\n(WMF) module coupled with a Multi-Scale Loss Design to improve decoding\nefficiency. Experimental results from three public HSI datasets with fixed and\ndisjoint training-testing samples demonstrate that our method outperforms\nexisting baselines and state-of-the-art approaches, highlighting its efficacy\nand potential in HSI applications.\n", "link": "http://arxiv.org/abs/2405.12003v1", "date": "2024-05-20", "relevancy": 2.11, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5451}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5342}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-in-Mamba%3A%20Centralized%20Mamba-Cross-Scan%20in%20Tokenized%20Mamba%20Model%0A%20%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Mamba-in-Mamba%3A%20Centralized%20Mamba-Cross-Scan%20in%20Tokenized%20Mamba%20Model%0A%20%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Weilian%20Zhou%20and%20Sei-Ichiro%20Kamata%20and%20Haipeng%20Wang%20and%20Man-Sing%20Wong%20and%20%20Huiying%20and%20%20Hou%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20is%20pivotal%20in%20the%20remote%20sensing%0A%28RS%29%20field%2C%20particularly%20with%20the%20advancement%20of%20deep%20learning%20techniques.%0ASequential%20models%2C%20adapted%20from%20the%20natural%20language%20processing%20%28NLP%29%20field%0Asuch%20as%20Recurrent%20Neural%20Networks%20%28RNNs%29%20and%20Transformers%2C%20have%20been%20tailored%0Ato%20this%20task%2C%20offering%20a%20unique%20viewpoint.%20However%2C%20several%20challenges%20persist%0A1%29%20RNNs%20struggle%20with%20centric%20feature%20aggregation%20and%20are%20sensitive%20to%0Ainterfering%20pixels%2C%202%29%20Transformers%20require%20significant%20computational%20resources%0Aand%20often%20underperform%20with%20limited%20HSI%20training%20samples%2C%20and%203%29%20Current%0Ascanning%20methods%20for%20converting%20images%20into%20sequence-data%20are%20simplistic%20and%0Ainefficient.%20In%20response%2C%20this%20study%20introduces%20the%20innovative%20Mamba-in-Mamba%0A%28MiM%29%20architecture%20for%20HSI%20classification%2C%20the%20first%20attempt%20of%20deploying%20State%0ASpace%20Model%20%28SSM%29%20in%20this%20task.%20The%20MiM%20model%20includes%201%29%20A%20novel%20centralized%0AMamba-Cross-Scan%20%28MCS%29%20mechanism%20for%20transforming%20images%20into%20sequence-data%2C%202%29%0AA%20Tokenized%20Mamba%20%28T-Mamba%29%20encoder%20that%20incorporates%20a%20Gaussian%20Decay%20Mask%0A%28GDM%29%2C%20a%20Semantic%20Token%20Learner%20%28STL%29%2C%20and%20a%20Semantic%20Token%20Fuser%20%28STF%29%20for%0Aenhanced%20feature%20generation%20and%20concentration%2C%20and%203%29%20A%20Weighted%20MCS%20Fusion%0A%28WMF%29%20module%20coupled%20with%20a%20Multi-Scale%20Loss%20Design%20to%20improve%20decoding%0Aefficiency.%20Experimental%20results%20from%20three%20public%20HSI%20datasets%20with%20fixed%20and%0Adisjoint%20training-testing%20samples%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20baselines%20and%20state-of-the-art%20approaches%2C%20highlighting%20its%20efficacy%0Aand%20potential%20in%20HSI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-in-Mamba%253A%2520Centralized%2520Mamba-Cross-Scan%2520in%2520Tokenized%2520Mamba%2520Model%250A%2520%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DWeilian%2520Zhou%2520and%2520Sei-Ichiro%2520Kamata%2520and%2520Haipeng%2520Wang%2520and%2520Man-Sing%2520Wong%2520and%2520%2520Huiying%2520and%2520%2520Hou%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520is%2520pivotal%2520in%2520the%2520remote%2520sensing%250A%2528RS%2529%2520field%252C%2520particularly%2520with%2520the%2520advancement%2520of%2520deep%2520learning%2520techniques.%250ASequential%2520models%252C%2520adapted%2520from%2520the%2520natural%2520language%2520processing%2520%2528NLP%2529%2520field%250Asuch%2520as%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520and%2520Transformers%252C%2520have%2520been%2520tailored%250Ato%2520this%2520task%252C%2520offering%2520a%2520unique%2520viewpoint.%2520However%252C%2520several%2520challenges%2520persist%250A1%2529%2520RNNs%2520struggle%2520with%2520centric%2520feature%2520aggregation%2520and%2520are%2520sensitive%2520to%250Ainterfering%2520pixels%252C%25202%2529%2520Transformers%2520require%2520significant%2520computational%2520resources%250Aand%2520often%2520underperform%2520with%2520limited%2520HSI%2520training%2520samples%252C%2520and%25203%2529%2520Current%250Ascanning%2520methods%2520for%2520converting%2520images%2520into%2520sequence-data%2520are%2520simplistic%2520and%250Ainefficient.%2520In%2520response%252C%2520this%2520study%2520introduces%2520the%2520innovative%2520Mamba-in-Mamba%250A%2528MiM%2529%2520architecture%2520for%2520HSI%2520classification%252C%2520the%2520first%2520attempt%2520of%2520deploying%2520State%250ASpace%2520Model%2520%2528SSM%2529%2520in%2520this%2520task.%2520The%2520MiM%2520model%2520includes%25201%2529%2520A%2520novel%2520centralized%250AMamba-Cross-Scan%2520%2528MCS%2529%2520mechanism%2520for%2520transforming%2520images%2520into%2520sequence-data%252C%25202%2529%250AA%2520Tokenized%2520Mamba%2520%2528T-Mamba%2529%2520encoder%2520that%2520incorporates%2520a%2520Gaussian%2520Decay%2520Mask%250A%2528GDM%2529%252C%2520a%2520Semantic%2520Token%2520Learner%2520%2528STL%2529%252C%2520and%2520a%2520Semantic%2520Token%2520Fuser%2520%2528STF%2529%2520for%250Aenhanced%2520feature%2520generation%2520and%2520concentration%252C%2520and%25203%2529%2520A%2520Weighted%2520MCS%2520Fusion%250A%2528WMF%2529%2520module%2520coupled%2520with%2520a%2520Multi-Scale%2520Loss%2520Design%2520to%2520improve%2520decoding%250Aefficiency.%2520Experimental%2520results%2520from%2520three%2520public%2520HSI%2520datasets%2520with%2520fixed%2520and%250Adisjoint%2520training-testing%2520samples%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aexisting%2520baselines%2520and%2520state-of-the-art%2520approaches%252C%2520highlighting%2520its%2520efficacy%250Aand%2520potential%2520in%2520HSI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-in-Mamba%3A%20Centralized%20Mamba-Cross-Scan%20in%20Tokenized%20Mamba%20Model%0A%20%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Weilian%20Zhou%20and%20Sei-Ichiro%20Kamata%20and%20Haipeng%20Wang%20and%20Man-Sing%20Wong%20and%20%20Huiying%20and%20%20Hou&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20is%20pivotal%20in%20the%20remote%20sensing%0A%28RS%29%20field%2C%20particularly%20with%20the%20advancement%20of%20deep%20learning%20techniques.%0ASequential%20models%2C%20adapted%20from%20the%20natural%20language%20processing%20%28NLP%29%20field%0Asuch%20as%20Recurrent%20Neural%20Networks%20%28RNNs%29%20and%20Transformers%2C%20have%20been%20tailored%0Ato%20this%20task%2C%20offering%20a%20unique%20viewpoint.%20However%2C%20several%20challenges%20persist%0A1%29%20RNNs%20struggle%20with%20centric%20feature%20aggregation%20and%20are%20sensitive%20to%0Ainterfering%20pixels%2C%202%29%20Transformers%20require%20significant%20computational%20resources%0Aand%20often%20underperform%20with%20limited%20HSI%20training%20samples%2C%20and%203%29%20Current%0Ascanning%20methods%20for%20converting%20images%20into%20sequence-data%20are%20simplistic%20and%0Ainefficient.%20In%20response%2C%20this%20study%20introduces%20the%20innovative%20Mamba-in-Mamba%0A%28MiM%29%20architecture%20for%20HSI%20classification%2C%20the%20first%20attempt%20of%20deploying%20State%0ASpace%20Model%20%28SSM%29%20in%20this%20task.%20The%20MiM%20model%20includes%201%29%20A%20novel%20centralized%0AMamba-Cross-Scan%20%28MCS%29%20mechanism%20for%20transforming%20images%20into%20sequence-data%2C%202%29%0AA%20Tokenized%20Mamba%20%28T-Mamba%29%20encoder%20that%20incorporates%20a%20Gaussian%20Decay%20Mask%0A%28GDM%29%2C%20a%20Semantic%20Token%20Learner%20%28STL%29%2C%20and%20a%20Semantic%20Token%20Fuser%20%28STF%29%20for%0Aenhanced%20feature%20generation%20and%20concentration%2C%20and%203%29%20A%20Weighted%20MCS%20Fusion%0A%28WMF%29%20module%20coupled%20with%20a%20Multi-Scale%20Loss%20Design%20to%20improve%20decoding%0Aefficiency.%20Experimental%20results%20from%20three%20public%20HSI%20datasets%20with%20fixed%20and%0Adisjoint%20training-testing%20samples%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20baselines%20and%20state-of-the-art%20approaches%2C%20highlighting%20its%20efficacy%0Aand%20potential%20in%20HSI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12003v1&entry.124074799=Read"},
{"title": "KI-PMF: Knowledge Integrated Plausible Motion Forecasting", "author": "Abhishek Vivekanandan and Ahmed Abouelazm and Philip Sch\u00f6rner and J. Marius Z\u00f6llner", "abstract": "  Accurately forecasting the motion of traffic actors is crucial for the\ndeployment of autonomous vehicles at a large scale. Current trajectory\nforecasting approaches primarily concentrate on optimizing a loss function with\na specific metric, which can result in predictions that do not adhere to\nphysical laws or violate external constraints. Our objective is to incorporate\nexplicit knowledge priors that allow a network to forecast future trajectories\nin compliance with both the kinematic constraints of a vehicle and the geometry\nof the driving environment. To achieve this, we introduce a non-parametric\npruning layer and attention layers to integrate the defined knowledge priors.\nOur proposed method is designed to ensure reachability guarantees for traffic\nactors in both complex and dynamic situations. By conditioning the network to\nfollow physical laws, we can obtain accurate and safe predictions, essential\nfor maintaining autonomous vehicles' safety and efficiency in real-world\nsettings.In summary, this paper presents concepts that prevent off-road\npredictions for safe and reliable motion forecasting by incorporating knowledge\npriors into the training process.\n", "link": "http://arxiv.org/abs/2310.12007v2", "date": "2024-05-20", "relevancy": 2.1072, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting&body=Title%3A%20KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting%0AAuthor%3A%20Abhishek%20Vivekanandan%20and%20Ahmed%20Abouelazm%20and%20Philip%20Sch%C3%B6rner%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Accurately%20forecasting%20the%20motion%20of%20traffic%20actors%20is%20crucial%20for%20the%0Adeployment%20of%20autonomous%20vehicles%20at%20a%20large%20scale.%20Current%20trajectory%0Aforecasting%20approaches%20primarily%20concentrate%20on%20optimizing%20a%20loss%20function%20with%0Aa%20specific%20metric%2C%20which%20can%20result%20in%20predictions%20that%20do%20not%20adhere%20to%0Aphysical%20laws%20or%20violate%20external%20constraints.%20Our%20objective%20is%20to%20incorporate%0Aexplicit%20knowledge%20priors%20that%20allow%20a%20network%20to%20forecast%20future%20trajectories%0Ain%20compliance%20with%20both%20the%20kinematic%20constraints%20of%20a%20vehicle%20and%20the%20geometry%0Aof%20the%20driving%20environment.%20To%20achieve%20this%2C%20we%20introduce%20a%20non-parametric%0Apruning%20layer%20and%20attention%20layers%20to%20integrate%20the%20defined%20knowledge%20priors.%0AOur%20proposed%20method%20is%20designed%20to%20ensure%20reachability%20guarantees%20for%20traffic%0Aactors%20in%20both%20complex%20and%20dynamic%20situations.%20By%20conditioning%20the%20network%20to%0Afollow%20physical%20laws%2C%20we%20can%20obtain%20accurate%20and%20safe%20predictions%2C%20essential%0Afor%20maintaining%20autonomous%20vehicles%27%20safety%20and%20efficiency%20in%20real-world%0Asettings.In%20summary%2C%20this%20paper%20presents%20concepts%20that%20prevent%20off-road%0Apredictions%20for%20safe%20and%20reliable%20motion%20forecasting%20by%20incorporating%20knowledge%0Apriors%20into%20the%20training%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKI-PMF%253A%2520Knowledge%2520Integrated%2520Plausible%2520Motion%2520Forecasting%26entry.906535625%3DAbhishek%2520Vivekanandan%2520and%2520Ahmed%2520Abouelazm%2520and%2520Philip%2520Sch%25C3%25B6rner%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Accurately%2520forecasting%2520the%2520motion%2520of%2520traffic%2520actors%2520is%2520crucial%2520for%2520the%250Adeployment%2520of%2520autonomous%2520vehicles%2520at%2520a%2520large%2520scale.%2520Current%2520trajectory%250Aforecasting%2520approaches%2520primarily%2520concentrate%2520on%2520optimizing%2520a%2520loss%2520function%2520with%250Aa%2520specific%2520metric%252C%2520which%2520can%2520result%2520in%2520predictions%2520that%2520do%2520not%2520adhere%2520to%250Aphysical%2520laws%2520or%2520violate%2520external%2520constraints.%2520Our%2520objective%2520is%2520to%2520incorporate%250Aexplicit%2520knowledge%2520priors%2520that%2520allow%2520a%2520network%2520to%2520forecast%2520future%2520trajectories%250Ain%2520compliance%2520with%2520both%2520the%2520kinematic%2520constraints%2520of%2520a%2520vehicle%2520and%2520the%2520geometry%250Aof%2520the%2520driving%2520environment.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520non-parametric%250Apruning%2520layer%2520and%2520attention%2520layers%2520to%2520integrate%2520the%2520defined%2520knowledge%2520priors.%250AOur%2520proposed%2520method%2520is%2520designed%2520to%2520ensure%2520reachability%2520guarantees%2520for%2520traffic%250Aactors%2520in%2520both%2520complex%2520and%2520dynamic%2520situations.%2520By%2520conditioning%2520the%2520network%2520to%250Afollow%2520physical%2520laws%252C%2520we%2520can%2520obtain%2520accurate%2520and%2520safe%2520predictions%252C%2520essential%250Afor%2520maintaining%2520autonomous%2520vehicles%2527%2520safety%2520and%2520efficiency%2520in%2520real-world%250Asettings.In%2520summary%252C%2520this%2520paper%2520presents%2520concepts%2520that%2520prevent%2520off-road%250Apredictions%2520for%2520safe%2520and%2520reliable%2520motion%2520forecasting%2520by%2520incorporating%2520knowledge%250Apriors%2520into%2520the%2520training%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting&entry.906535625=Abhishek%20Vivekanandan%20and%20Ahmed%20Abouelazm%20and%20Philip%20Sch%C3%B6rner%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Accurately%20forecasting%20the%20motion%20of%20traffic%20actors%20is%20crucial%20for%20the%0Adeployment%20of%20autonomous%20vehicles%20at%20a%20large%20scale.%20Current%20trajectory%0Aforecasting%20approaches%20primarily%20concentrate%20on%20optimizing%20a%20loss%20function%20with%0Aa%20specific%20metric%2C%20which%20can%20result%20in%20predictions%20that%20do%20not%20adhere%20to%0Aphysical%20laws%20or%20violate%20external%20constraints.%20Our%20objective%20is%20to%20incorporate%0Aexplicit%20knowledge%20priors%20that%20allow%20a%20network%20to%20forecast%20future%20trajectories%0Ain%20compliance%20with%20both%20the%20kinematic%20constraints%20of%20a%20vehicle%20and%20the%20geometry%0Aof%20the%20driving%20environment.%20To%20achieve%20this%2C%20we%20introduce%20a%20non-parametric%0Apruning%20layer%20and%20attention%20layers%20to%20integrate%20the%20defined%20knowledge%20priors.%0AOur%20proposed%20method%20is%20designed%20to%20ensure%20reachability%20guarantees%20for%20traffic%0Aactors%20in%20both%20complex%20and%20dynamic%20situations.%20By%20conditioning%20the%20network%20to%0Afollow%20physical%20laws%2C%20we%20can%20obtain%20accurate%20and%20safe%20predictions%2C%20essential%0Afor%20maintaining%20autonomous%20vehicles%27%20safety%20and%20efficiency%20in%20real-world%0Asettings.In%20summary%2C%20this%20paper%20presents%20concepts%20that%20prevent%20off-road%0Apredictions%20for%20safe%20and%20reliable%20motion%20forecasting%20by%20incorporating%20knowledge%0Apriors%20into%20the%20training%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12007v2&entry.124074799=Read"},
{"title": "ASLseg: Adapting SAM in the Loop for Semi-supervised Liver Tumor\n  Segmentation", "author": "Shiyun Chen and Li Lin and Pujin Cheng and Xiaoying Tang", "abstract": "  Liver tumor segmentation is essential for computer-aided diagnosis, surgical\nplanning, and prognosis evaluation. However, obtaining and maintaining a\nlarge-scale dataset with dense annotations is challenging. Semi-Supervised\nLearning (SSL) is a common technique to address these challenges. Recently,\nSegment Anything Model (SAM) has shown promising performance in some medical\nimage segmentation tasks, but it performs poorly for liver tumor segmentation.\nIn this paper, we propose a novel semi-supervised framework, named ASLseg,\nwhich can effectively adapt the SAM to the SSL setting and combine both\ndomain-specific and general knowledge of liver tumors. Specifically, the\nsegmentation model trained with a specific SSL paradigm provides the generated\npseudo-labels as prompts to the fine-tuned SAM. An adaptation network is then\nused to refine the SAM-predictions and generate higher-quality pseudo-labels.\nFinally, the reliable pseudo-labels are selected to expand the labeled set for\niterative training. Extensive experiments on the LiTS dataset demonstrate\noverwhelming performance of our ASLseg.\n", "link": "http://arxiv.org/abs/2312.07969v2", "date": "2024-05-20", "relevancy": 2.0966, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASLseg%3A%20Adapting%20SAM%20in%20the%20Loop%20for%20Semi-supervised%20Liver%20Tumor%0A%20%20Segmentation&body=Title%3A%20ASLseg%3A%20Adapting%20SAM%20in%20the%20Loop%20for%20Semi-supervised%20Liver%20Tumor%0A%20%20Segmentation%0AAuthor%3A%20Shiyun%20Chen%20and%20Li%20Lin%20and%20Pujin%20Cheng%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20Liver%20tumor%20segmentation%20is%20essential%20for%20computer-aided%20diagnosis%2C%20surgical%0Aplanning%2C%20and%20prognosis%20evaluation.%20However%2C%20obtaining%20and%20maintaining%20a%0Alarge-scale%20dataset%20with%20dense%20annotations%20is%20challenging.%20Semi-Supervised%0ALearning%20%28SSL%29%20is%20a%20common%20technique%20to%20address%20these%20challenges.%20Recently%2C%0ASegment%20Anything%20Model%20%28SAM%29%20has%20shown%20promising%20performance%20in%20some%20medical%0Aimage%20segmentation%20tasks%2C%20but%20it%20performs%20poorly%20for%20liver%20tumor%20segmentation.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20semi-supervised%20framework%2C%20named%20ASLseg%2C%0Awhich%20can%20effectively%20adapt%20the%20SAM%20to%20the%20SSL%20setting%20and%20combine%20both%0Adomain-specific%20and%20general%20knowledge%20of%20liver%20tumors.%20Specifically%2C%20the%0Asegmentation%20model%20trained%20with%20a%20specific%20SSL%20paradigm%20provides%20the%20generated%0Apseudo-labels%20as%20prompts%20to%20the%20fine-tuned%20SAM.%20An%20adaptation%20network%20is%20then%0Aused%20to%20refine%20the%20SAM-predictions%20and%20generate%20higher-quality%20pseudo-labels.%0AFinally%2C%20the%20reliable%20pseudo-labels%20are%20selected%20to%20expand%20the%20labeled%20set%20for%0Aiterative%20training.%20Extensive%20experiments%20on%20the%20LiTS%20dataset%20demonstrate%0Aoverwhelming%20performance%20of%20our%20ASLseg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASLseg%253A%2520Adapting%2520SAM%2520in%2520the%2520Loop%2520for%2520Semi-supervised%2520Liver%2520Tumor%250A%2520%2520Segmentation%26entry.906535625%3DShiyun%2520Chen%2520and%2520Li%2520Lin%2520and%2520Pujin%2520Cheng%2520and%2520Xiaoying%2520Tang%26entry.1292438233%3D%2520%2520Liver%2520tumor%2520segmentation%2520is%2520essential%2520for%2520computer-aided%2520diagnosis%252C%2520surgical%250Aplanning%252C%2520and%2520prognosis%2520evaluation.%2520However%252C%2520obtaining%2520and%2520maintaining%2520a%250Alarge-scale%2520dataset%2520with%2520dense%2520annotations%2520is%2520challenging.%2520Semi-Supervised%250ALearning%2520%2528SSL%2529%2520is%2520a%2520common%2520technique%2520to%2520address%2520these%2520challenges.%2520Recently%252C%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520shown%2520promising%2520performance%2520in%2520some%2520medical%250Aimage%2520segmentation%2520tasks%252C%2520but%2520it%2520performs%2520poorly%2520for%2520liver%2520tumor%2520segmentation.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520semi-supervised%2520framework%252C%2520named%2520ASLseg%252C%250Awhich%2520can%2520effectively%2520adapt%2520the%2520SAM%2520to%2520the%2520SSL%2520setting%2520and%2520combine%2520both%250Adomain-specific%2520and%2520general%2520knowledge%2520of%2520liver%2520tumors.%2520Specifically%252C%2520the%250Asegmentation%2520model%2520trained%2520with%2520a%2520specific%2520SSL%2520paradigm%2520provides%2520the%2520generated%250Apseudo-labels%2520as%2520prompts%2520to%2520the%2520fine-tuned%2520SAM.%2520An%2520adaptation%2520network%2520is%2520then%250Aused%2520to%2520refine%2520the%2520SAM-predictions%2520and%2520generate%2520higher-quality%2520pseudo-labels.%250AFinally%252C%2520the%2520reliable%2520pseudo-labels%2520are%2520selected%2520to%2520expand%2520the%2520labeled%2520set%2520for%250Aiterative%2520training.%2520Extensive%2520experiments%2520on%2520the%2520LiTS%2520dataset%2520demonstrate%250Aoverwhelming%2520performance%2520of%2520our%2520ASLseg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASLseg%3A%20Adapting%20SAM%20in%20the%20Loop%20for%20Semi-supervised%20Liver%20Tumor%0A%20%20Segmentation&entry.906535625=Shiyun%20Chen%20and%20Li%20Lin%20and%20Pujin%20Cheng%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Liver%20tumor%20segmentation%20is%20essential%20for%20computer-aided%20diagnosis%2C%20surgical%0Aplanning%2C%20and%20prognosis%20evaluation.%20However%2C%20obtaining%20and%20maintaining%20a%0Alarge-scale%20dataset%20with%20dense%20annotations%20is%20challenging.%20Semi-Supervised%0ALearning%20%28SSL%29%20is%20a%20common%20technique%20to%20address%20these%20challenges.%20Recently%2C%0ASegment%20Anything%20Model%20%28SAM%29%20has%20shown%20promising%20performance%20in%20some%20medical%0Aimage%20segmentation%20tasks%2C%20but%20it%20performs%20poorly%20for%20liver%20tumor%20segmentation.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20semi-supervised%20framework%2C%20named%20ASLseg%2C%0Awhich%20can%20effectively%20adapt%20the%20SAM%20to%20the%20SSL%20setting%20and%20combine%20both%0Adomain-specific%20and%20general%20knowledge%20of%20liver%20tumors.%20Specifically%2C%20the%0Asegmentation%20model%20trained%20with%20a%20specific%20SSL%20paradigm%20provides%20the%20generated%0Apseudo-labels%20as%20prompts%20to%20the%20fine-tuned%20SAM.%20An%20adaptation%20network%20is%20then%0Aused%20to%20refine%20the%20SAM-predictions%20and%20generate%20higher-quality%20pseudo-labels.%0AFinally%2C%20the%20reliable%20pseudo-labels%20are%20selected%20to%20expand%20the%20labeled%20set%20for%0Aiterative%20training.%20Extensive%20experiments%20on%20the%20LiTS%20dataset%20demonstrate%0Aoverwhelming%20performance%20of%20our%20ASLseg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07969v2&entry.124074799=Read"},
{"title": "Infrared Image Super-Resolution via Lightweight Information Split\n  Network", "author": "Shijie Liu and Kang Yan and Feiwei Qin and Changmiao Wang and Ruiquan Ge and Kai Zhang and Jie Huang and Yong Peng and Jin Cao", "abstract": "  Single image super-resolution (SR) is an established pixel-level vision task\naimed at reconstructing a high-resolution image from its degraded\nlow-resolution counterpart. Despite the notable advancements achieved by\nleveraging deep neural networks for SR, most existing deep learning\narchitectures feature an extensive number of layers, leading to high\ncomputational complexity and substantial memory demands. These issues become\nparticularly pronounced in the context of infrared image SR, where infrared\ndevices often have stringent storage and computational constraints. To mitigate\nthese challenges, we introduce a novel, efficient, and precise single infrared\nimage SR model, termed the Lightweight Information Split Network (LISN). The\nLISN comprises four main components: shallow feature extraction, deep feature\nextraction, dense feature fusion, and high-resolution infrared image\nreconstruction. A key innovation within this model is the introduction of the\nLightweight Information Split Block (LISB) for deep feature extraction. The\nLISB employs a sequential process to extract hierarchical features, which are\nthen aggregated based on the relevance of the features under consideration. By\nintegrating channel splitting and shift operations, the LISB successfully\nstrikes an optimal balance between enhanced SR performance and a lightweight\nframework. Comprehensive experimental evaluations reveal that the proposed LISN\nachieves superior performance over contemporary state-of-the-art methods in\nterms of both SR quality and model complexity, affirming its efficacy for\npractical deployment in resource-constrained infrared imaging applications.\n", "link": "http://arxiv.org/abs/2405.10561v2", "date": "2024-05-20", "relevancy": 2.0965, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5453}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5301}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrared%20Image%20Super-Resolution%20via%20Lightweight%20Information%20Split%0A%20%20Network&body=Title%3A%20Infrared%20Image%20Super-Resolution%20via%20Lightweight%20Information%20Split%0A%20%20Network%0AAuthor%3A%20Shijie%20Liu%20and%20Kang%20Yan%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Ruiquan%20Ge%20and%20Kai%20Zhang%20and%20Jie%20Huang%20and%20Yong%20Peng%20and%20Jin%20Cao%0AAbstract%3A%20%20%20Single%20image%20super-resolution%20%28SR%29%20is%20an%20established%20pixel-level%20vision%20task%0Aaimed%20at%20reconstructing%20a%20high-resolution%20image%20from%20its%20degraded%0Alow-resolution%20counterpart.%20Despite%20the%20notable%20advancements%20achieved%20by%0Aleveraging%20deep%20neural%20networks%20for%20SR%2C%20most%20existing%20deep%20learning%0Aarchitectures%20feature%20an%20extensive%20number%20of%20layers%2C%20leading%20to%20high%0Acomputational%20complexity%20and%20substantial%20memory%20demands.%20These%20issues%20become%0Aparticularly%20pronounced%20in%20the%20context%20of%20infrared%20image%20SR%2C%20where%20infrared%0Adevices%20often%20have%20stringent%20storage%20and%20computational%20constraints.%20To%20mitigate%0Athese%20challenges%2C%20we%20introduce%20a%20novel%2C%20efficient%2C%20and%20precise%20single%20infrared%0Aimage%20SR%20model%2C%20termed%20the%20Lightweight%20Information%20Split%20Network%20%28LISN%29.%20The%0ALISN%20comprises%20four%20main%20components%3A%20shallow%20feature%20extraction%2C%20deep%20feature%0Aextraction%2C%20dense%20feature%20fusion%2C%20and%20high-resolution%20infrared%20image%0Areconstruction.%20A%20key%20innovation%20within%20this%20model%20is%20the%20introduction%20of%20the%0ALightweight%20Information%20Split%20Block%20%28LISB%29%20for%20deep%20feature%20extraction.%20The%0ALISB%20employs%20a%20sequential%20process%20to%20extract%20hierarchical%20features%2C%20which%20are%0Athen%20aggregated%20based%20on%20the%20relevance%20of%20the%20features%20under%20consideration.%20By%0Aintegrating%20channel%20splitting%20and%20shift%20operations%2C%20the%20LISB%20successfully%0Astrikes%20an%20optimal%20balance%20between%20enhanced%20SR%20performance%20and%20a%20lightweight%0Aframework.%20Comprehensive%20experimental%20evaluations%20reveal%20that%20the%20proposed%20LISN%0Aachieves%20superior%20performance%20over%20contemporary%20state-of-the-art%20methods%20in%0Aterms%20of%20both%20SR%20quality%20and%20model%20complexity%2C%20affirming%20its%20efficacy%20for%0Apractical%20deployment%20in%20resource-constrained%20infrared%20imaging%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrared%2520Image%2520Super-Resolution%2520via%2520Lightweight%2520Information%2520Split%250A%2520%2520Network%26entry.906535625%3DShijie%2520Liu%2520and%2520Kang%2520Yan%2520and%2520Feiwei%2520Qin%2520and%2520Changmiao%2520Wang%2520and%2520Ruiquan%2520Ge%2520and%2520Kai%2520Zhang%2520and%2520Jie%2520Huang%2520and%2520Yong%2520Peng%2520and%2520Jin%2520Cao%26entry.1292438233%3D%2520%2520Single%2520image%2520super-resolution%2520%2528SR%2529%2520is%2520an%2520established%2520pixel-level%2520vision%2520task%250Aaimed%2520at%2520reconstructing%2520a%2520high-resolution%2520image%2520from%2520its%2520degraded%250Alow-resolution%2520counterpart.%2520Despite%2520the%2520notable%2520advancements%2520achieved%2520by%250Aleveraging%2520deep%2520neural%2520networks%2520for%2520SR%252C%2520most%2520existing%2520deep%2520learning%250Aarchitectures%2520feature%2520an%2520extensive%2520number%2520of%2520layers%252C%2520leading%2520to%2520high%250Acomputational%2520complexity%2520and%2520substantial%2520memory%2520demands.%2520These%2520issues%2520become%250Aparticularly%2520pronounced%2520in%2520the%2520context%2520of%2520infrared%2520image%2520SR%252C%2520where%2520infrared%250Adevices%2520often%2520have%2520stringent%2520storage%2520and%2520computational%2520constraints.%2520To%2520mitigate%250Athese%2520challenges%252C%2520we%2520introduce%2520a%2520novel%252C%2520efficient%252C%2520and%2520precise%2520single%2520infrared%250Aimage%2520SR%2520model%252C%2520termed%2520the%2520Lightweight%2520Information%2520Split%2520Network%2520%2528LISN%2529.%2520The%250ALISN%2520comprises%2520four%2520main%2520components%253A%2520shallow%2520feature%2520extraction%252C%2520deep%2520feature%250Aextraction%252C%2520dense%2520feature%2520fusion%252C%2520and%2520high-resolution%2520infrared%2520image%250Areconstruction.%2520A%2520key%2520innovation%2520within%2520this%2520model%2520is%2520the%2520introduction%2520of%2520the%250ALightweight%2520Information%2520Split%2520Block%2520%2528LISB%2529%2520for%2520deep%2520feature%2520extraction.%2520The%250ALISB%2520employs%2520a%2520sequential%2520process%2520to%2520extract%2520hierarchical%2520features%252C%2520which%2520are%250Athen%2520aggregated%2520based%2520on%2520the%2520relevance%2520of%2520the%2520features%2520under%2520consideration.%2520By%250Aintegrating%2520channel%2520splitting%2520and%2520shift%2520operations%252C%2520the%2520LISB%2520successfully%250Astrikes%2520an%2520optimal%2520balance%2520between%2520enhanced%2520SR%2520performance%2520and%2520a%2520lightweight%250Aframework.%2520Comprehensive%2520experimental%2520evaluations%2520reveal%2520that%2520the%2520proposed%2520LISN%250Aachieves%2520superior%2520performance%2520over%2520contemporary%2520state-of-the-art%2520methods%2520in%250Aterms%2520of%2520both%2520SR%2520quality%2520and%2520model%2520complexity%252C%2520affirming%2520its%2520efficacy%2520for%250Apractical%2520deployment%2520in%2520resource-constrained%2520infrared%2520imaging%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrared%20Image%20Super-Resolution%20via%20Lightweight%20Information%20Split%0A%20%20Network&entry.906535625=Shijie%20Liu%20and%20Kang%20Yan%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Ruiquan%20Ge%20and%20Kai%20Zhang%20and%20Jie%20Huang%20and%20Yong%20Peng%20and%20Jin%20Cao&entry.1292438233=%20%20Single%20image%20super-resolution%20%28SR%29%20is%20an%20established%20pixel-level%20vision%20task%0Aaimed%20at%20reconstructing%20a%20high-resolution%20image%20from%20its%20degraded%0Alow-resolution%20counterpart.%20Despite%20the%20notable%20advancements%20achieved%20by%0Aleveraging%20deep%20neural%20networks%20for%20SR%2C%20most%20existing%20deep%20learning%0Aarchitectures%20feature%20an%20extensive%20number%20of%20layers%2C%20leading%20to%20high%0Acomputational%20complexity%20and%20substantial%20memory%20demands.%20These%20issues%20become%0Aparticularly%20pronounced%20in%20the%20context%20of%20infrared%20image%20SR%2C%20where%20infrared%0Adevices%20often%20have%20stringent%20storage%20and%20computational%20constraints.%20To%20mitigate%0Athese%20challenges%2C%20we%20introduce%20a%20novel%2C%20efficient%2C%20and%20precise%20single%20infrared%0Aimage%20SR%20model%2C%20termed%20the%20Lightweight%20Information%20Split%20Network%20%28LISN%29.%20The%0ALISN%20comprises%20four%20main%20components%3A%20shallow%20feature%20extraction%2C%20deep%20feature%0Aextraction%2C%20dense%20feature%20fusion%2C%20and%20high-resolution%20infrared%20image%0Areconstruction.%20A%20key%20innovation%20within%20this%20model%20is%20the%20introduction%20of%20the%0ALightweight%20Information%20Split%20Block%20%28LISB%29%20for%20deep%20feature%20extraction.%20The%0ALISB%20employs%20a%20sequential%20process%20to%20extract%20hierarchical%20features%2C%20which%20are%0Athen%20aggregated%20based%20on%20the%20relevance%20of%20the%20features%20under%20consideration.%20By%0Aintegrating%20channel%20splitting%20and%20shift%20operations%2C%20the%20LISB%20successfully%0Astrikes%20an%20optimal%20balance%20between%20enhanced%20SR%20performance%20and%20a%20lightweight%0Aframework.%20Comprehensive%20experimental%20evaluations%20reveal%20that%20the%20proposed%20LISN%0Aachieves%20superior%20performance%20over%20contemporary%20state-of-the-art%20methods%20in%0Aterms%20of%20both%20SR%20quality%20and%20model%20complexity%2C%20affirming%20its%20efficacy%20for%0Apractical%20deployment%20in%20resource-constrained%20infrared%20imaging%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10561v2&entry.124074799=Read"},
{"title": "MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering", "author": "Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and Yanjie Wang and Yuliang Liu and Hao Liu and Xiang Bai and Can Huang", "abstract": "  Text-Centric Visual Question Answering (TEC-VQA) in its proper format not\nonly facilitates human-machine interaction in text-centric visual environments\nbut also serves as a de facto gold proxy to evaluate AI models in the domain of\ntext-centric scene understanding. However, most TEC-VQA benchmarks have focused\non high-resource languages like English and Chinese. Despite pioneering works\nto expand multilingual QA pairs in non-text-centric VQA datasets using\ntranslation engines, the translation-based protocol encounters a substantial\n``Visual-textual misalignment'' problem when applied to TEC-VQA. Specifically,\nit prioritizes the text in question-answer pairs while disregarding the visual\ntext present in images. Furthermore, it does not adequately tackle challenges\nrelated to nuanced meaning, contextual distortion, language bias, and\nquestion-type diversity. In this work, we address the task of multilingual\nTEC-VQA and provide a benchmark with high-quality human expert annotations in 9\ndiverse languages, called MTVQA. To our knowledge, MTVQA is the first\nmultilingual TEC-VQA benchmark to provide human expert annotations for\ntext-centric scenarios. Further, by evaluating several state-of-the-art\nMultimodal Large Language Models (MLLMs), including GPT-4V, on our MTVQA\ndataset, it is evident that there is still room for performance improvement,\nunderscoring the value of our dataset. We hope this dataset will provide\nresearchers with fresh perspectives and inspiration within the community. The\nMTVQA dataset will be available at\nhttps://huggingface.co/datasets/ByteDance/MTVQA.\n", "link": "http://arxiv.org/abs/2405.11985v1", "date": "2024-05-20", "relevancy": 2.0937, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5212}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTVQA%3A%20Benchmarking%20Multilingual%20Text-Centric%20Visual%20Question%20Answering&body=Title%3A%20MTVQA%3A%20Benchmarking%20Multilingual%20Text-Centric%20Visual%20Question%20Answering%0AAuthor%3A%20Jingqun%20Tang%20and%20Qi%20Liu%20and%20Yongjie%20Ye%20and%20Jinghui%20Lu%20and%20Shu%20Wei%20and%20Chunhui%20Lin%20and%20Wanqing%20Li%20and%20Mohamad%20Fitri%20Faiz%20Bin%20Mahmood%20and%20Hao%20Feng%20and%20Zhen%20Zhao%20and%20Yanjie%20Wang%20and%20Yuliang%20Liu%20and%20Hao%20Liu%20and%20Xiang%20Bai%20and%20Can%20Huang%0AAbstract%3A%20%20%20Text-Centric%20Visual%20Question%20Answering%20%28TEC-VQA%29%20in%20its%20proper%20format%20not%0Aonly%20facilitates%20human-machine%20interaction%20in%20text-centric%20visual%20environments%0Abut%20also%20serves%20as%20a%20de%20facto%20gold%20proxy%20to%20evaluate%20AI%20models%20in%20the%20domain%20of%0Atext-centric%20scene%20understanding.%20However%2C%20most%20TEC-VQA%20benchmarks%20have%20focused%0Aon%20high-resource%20languages%20like%20English%20and%20Chinese.%20Despite%20pioneering%20works%0Ato%20expand%20multilingual%20QA%20pairs%20in%20non-text-centric%20VQA%20datasets%20using%0Atranslation%20engines%2C%20the%20translation-based%20protocol%20encounters%20a%20substantial%0A%60%60Visual-textual%20misalignment%27%27%20problem%20when%20applied%20to%20TEC-VQA.%20Specifically%2C%0Ait%20prioritizes%20the%20text%20in%20question-answer%20pairs%20while%20disregarding%20the%20visual%0Atext%20present%20in%20images.%20Furthermore%2C%20it%20does%20not%20adequately%20tackle%20challenges%0Arelated%20to%20nuanced%20meaning%2C%20contextual%20distortion%2C%20language%20bias%2C%20and%0Aquestion-type%20diversity.%20In%20this%20work%2C%20we%20address%20the%20task%20of%20multilingual%0ATEC-VQA%20and%20provide%20a%20benchmark%20with%20high-quality%20human%20expert%20annotations%20in%209%0Adiverse%20languages%2C%20called%20MTVQA.%20To%20our%20knowledge%2C%20MTVQA%20is%20the%20first%0Amultilingual%20TEC-VQA%20benchmark%20to%20provide%20human%20expert%20annotations%20for%0Atext-centric%20scenarios.%20Further%2C%20by%20evaluating%20several%20state-of-the-art%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20including%20GPT-4V%2C%20on%20our%20MTVQA%0Adataset%2C%20it%20is%20evident%20that%20there%20is%20still%20room%20for%20performance%20improvement%2C%0Aunderscoring%20the%20value%20of%20our%20dataset.%20We%20hope%20this%20dataset%20will%20provide%0Aresearchers%20with%20fresh%20perspectives%20and%20inspiration%20within%20the%20community.%20The%0AMTVQA%20dataset%20will%20be%20available%20at%0Ahttps%3A//huggingface.co/datasets/ByteDance/MTVQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTVQA%253A%2520Benchmarking%2520Multilingual%2520Text-Centric%2520Visual%2520Question%2520Answering%26entry.906535625%3DJingqun%2520Tang%2520and%2520Qi%2520Liu%2520and%2520Yongjie%2520Ye%2520and%2520Jinghui%2520Lu%2520and%2520Shu%2520Wei%2520and%2520Chunhui%2520Lin%2520and%2520Wanqing%2520Li%2520and%2520Mohamad%2520Fitri%2520Faiz%2520Bin%2520Mahmood%2520and%2520Hao%2520Feng%2520and%2520Zhen%2520Zhao%2520and%2520Yanjie%2520Wang%2520and%2520Yuliang%2520Liu%2520and%2520Hao%2520Liu%2520and%2520Xiang%2520Bai%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520Text-Centric%2520Visual%2520Question%2520Answering%2520%2528TEC-VQA%2529%2520in%2520its%2520proper%2520format%2520not%250Aonly%2520facilitates%2520human-machine%2520interaction%2520in%2520text-centric%2520visual%2520environments%250Abut%2520also%2520serves%2520as%2520a%2520de%2520facto%2520gold%2520proxy%2520to%2520evaluate%2520AI%2520models%2520in%2520the%2520domain%2520of%250Atext-centric%2520scene%2520understanding.%2520However%252C%2520most%2520TEC-VQA%2520benchmarks%2520have%2520focused%250Aon%2520high-resource%2520languages%2520like%2520English%2520and%2520Chinese.%2520Despite%2520pioneering%2520works%250Ato%2520expand%2520multilingual%2520QA%2520pairs%2520in%2520non-text-centric%2520VQA%2520datasets%2520using%250Atranslation%2520engines%252C%2520the%2520translation-based%2520protocol%2520encounters%2520a%2520substantial%250A%2560%2560Visual-textual%2520misalignment%2527%2527%2520problem%2520when%2520applied%2520to%2520TEC-VQA.%2520Specifically%252C%250Ait%2520prioritizes%2520the%2520text%2520in%2520question-answer%2520pairs%2520while%2520disregarding%2520the%2520visual%250Atext%2520present%2520in%2520images.%2520Furthermore%252C%2520it%2520does%2520not%2520adequately%2520tackle%2520challenges%250Arelated%2520to%2520nuanced%2520meaning%252C%2520contextual%2520distortion%252C%2520language%2520bias%252C%2520and%250Aquestion-type%2520diversity.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520task%2520of%2520multilingual%250ATEC-VQA%2520and%2520provide%2520a%2520benchmark%2520with%2520high-quality%2520human%2520expert%2520annotations%2520in%25209%250Adiverse%2520languages%252C%2520called%2520MTVQA.%2520To%2520our%2520knowledge%252C%2520MTVQA%2520is%2520the%2520first%250Amultilingual%2520TEC-VQA%2520benchmark%2520to%2520provide%2520human%2520expert%2520annotations%2520for%250Atext-centric%2520scenarios.%2520Further%252C%2520by%2520evaluating%2520several%2520state-of-the-art%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520including%2520GPT-4V%252C%2520on%2520our%2520MTVQA%250Adataset%252C%2520it%2520is%2520evident%2520that%2520there%2520is%2520still%2520room%2520for%2520performance%2520improvement%252C%250Aunderscoring%2520the%2520value%2520of%2520our%2520dataset.%2520We%2520hope%2520this%2520dataset%2520will%2520provide%250Aresearchers%2520with%2520fresh%2520perspectives%2520and%2520inspiration%2520within%2520the%2520community.%2520The%250AMTVQA%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/ByteDance/MTVQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTVQA%3A%20Benchmarking%20Multilingual%20Text-Centric%20Visual%20Question%20Answering&entry.906535625=Jingqun%20Tang%20and%20Qi%20Liu%20and%20Yongjie%20Ye%20and%20Jinghui%20Lu%20and%20Shu%20Wei%20and%20Chunhui%20Lin%20and%20Wanqing%20Li%20and%20Mohamad%20Fitri%20Faiz%20Bin%20Mahmood%20and%20Hao%20Feng%20and%20Zhen%20Zhao%20and%20Yanjie%20Wang%20and%20Yuliang%20Liu%20and%20Hao%20Liu%20and%20Xiang%20Bai%20and%20Can%20Huang&entry.1292438233=%20%20Text-Centric%20Visual%20Question%20Answering%20%28TEC-VQA%29%20in%20its%20proper%20format%20not%0Aonly%20facilitates%20human-machine%20interaction%20in%20text-centric%20visual%20environments%0Abut%20also%20serves%20as%20a%20de%20facto%20gold%20proxy%20to%20evaluate%20AI%20models%20in%20the%20domain%20of%0Atext-centric%20scene%20understanding.%20However%2C%20most%20TEC-VQA%20benchmarks%20have%20focused%0Aon%20high-resource%20languages%20like%20English%20and%20Chinese.%20Despite%20pioneering%20works%0Ato%20expand%20multilingual%20QA%20pairs%20in%20non-text-centric%20VQA%20datasets%20using%0Atranslation%20engines%2C%20the%20translation-based%20protocol%20encounters%20a%20substantial%0A%60%60Visual-textual%20misalignment%27%27%20problem%20when%20applied%20to%20TEC-VQA.%20Specifically%2C%0Ait%20prioritizes%20the%20text%20in%20question-answer%20pairs%20while%20disregarding%20the%20visual%0Atext%20present%20in%20images.%20Furthermore%2C%20it%20does%20not%20adequately%20tackle%20challenges%0Arelated%20to%20nuanced%20meaning%2C%20contextual%20distortion%2C%20language%20bias%2C%20and%0Aquestion-type%20diversity.%20In%20this%20work%2C%20we%20address%20the%20task%20of%20multilingual%0ATEC-VQA%20and%20provide%20a%20benchmark%20with%20high-quality%20human%20expert%20annotations%20in%209%0Adiverse%20languages%2C%20called%20MTVQA.%20To%20our%20knowledge%2C%20MTVQA%20is%20the%20first%0Amultilingual%20TEC-VQA%20benchmark%20to%20provide%20human%20expert%20annotations%20for%0Atext-centric%20scenarios.%20Further%2C%20by%20evaluating%20several%20state-of-the-art%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20including%20GPT-4V%2C%20on%20our%20MTVQA%0Adataset%2C%20it%20is%20evident%20that%20there%20is%20still%20room%20for%20performance%20improvement%2C%0Aunderscoring%20the%20value%20of%20our%20dataset.%20We%20hope%20this%20dataset%20will%20provide%0Aresearchers%20with%20fresh%20perspectives%20and%20inspiration%20within%20the%20community.%20The%0AMTVQA%20dataset%20will%20be%20available%20at%0Ahttps%3A//huggingface.co/datasets/ByteDance/MTVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11985v1&entry.124074799=Read"},
{"title": "Data Contamination Calibration for Black-box LLMs", "author": "Wentao Ye and Jiaqi Hu and Liyao Li and Haobo Wang and Gang Chen and Junbo Zhao", "abstract": "  The rapid advancements of Large Language Models (LLMs) tightly associate with\nthe expansion of the training data size. However, the unchecked\nultra-large-scale training sets introduce a series of potential risks like data\ncontamination, i.e. the benchmark data is used for training. In this work, we\npropose a holistic method named Polarized Augment Calibration (PAC) along with\na new to-be-released dataset to detect the contaminated data and diminish the\ncontamination effect. PAC extends the popular MIA (Membership Inference Attack)\n-- from machine learning community -- by forming a more global target at\ndetecting training data to Clarify invisible training data. As a pioneering\nwork, PAC is very much plug-and-play that can be integrated with most (if not\nall) current white- and black-box LLMs. By extensive experiments, PAC\noutperforms existing methods by at least 4.5%, towards data contamination\ndetection on more 4 dataset formats, with more than 10 base LLMs. Besides, our\napplication in real-world scenarios highlights the prominent presence of\ncontamination and related issues.\n", "link": "http://arxiv.org/abs/2405.11930v1", "date": "2024-05-20", "relevancy": 2.0929, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5236}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Contamination%20Calibration%20for%20Black-box%20LLMs&body=Title%3A%20Data%20Contamination%20Calibration%20for%20Black-box%20LLMs%0AAuthor%3A%20Wentao%20Ye%20and%20Jiaqi%20Hu%20and%20Liyao%20Li%20and%20Haobo%20Wang%20and%20Gang%20Chen%20and%20Junbo%20Zhao%0AAbstract%3A%20%20%20The%20rapid%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%20tightly%20associate%20with%0Athe%20expansion%20of%20the%20training%20data%20size.%20However%2C%20the%20unchecked%0Aultra-large-scale%20training%20sets%20introduce%20a%20series%20of%20potential%20risks%20like%20data%0Acontamination%2C%20i.e.%20the%20benchmark%20data%20is%20used%20for%20training.%20In%20this%20work%2C%20we%0Apropose%20a%20holistic%20method%20named%20Polarized%20Augment%20Calibration%20%28PAC%29%20along%20with%0Aa%20new%20to-be-released%20dataset%20to%20detect%20the%20contaminated%20data%20and%20diminish%20the%0Acontamination%20effect.%20PAC%20extends%20the%20popular%20MIA%20%28Membership%20Inference%20Attack%29%0A--%20from%20machine%20learning%20community%20--%20by%20forming%20a%20more%20global%20target%20at%0Adetecting%20training%20data%20to%20Clarify%20invisible%20training%20data.%20As%20a%20pioneering%0Awork%2C%20PAC%20is%20very%20much%20plug-and-play%20that%20can%20be%20integrated%20with%20most%20%28if%20not%0Aall%29%20current%20white-%20and%20black-box%20LLMs.%20By%20extensive%20experiments%2C%20PAC%0Aoutperforms%20existing%20methods%20by%20at%20least%204.5%25%2C%20towards%20data%20contamination%0Adetection%20on%20more%204%20dataset%20formats%2C%20with%20more%20than%2010%20base%20LLMs.%20Besides%2C%20our%0Aapplication%20in%20real-world%20scenarios%20highlights%20the%20prominent%20presence%20of%0Acontamination%20and%20related%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Contamination%2520Calibration%2520for%2520Black-box%2520LLMs%26entry.906535625%3DWentao%2520Ye%2520and%2520Jiaqi%2520Hu%2520and%2520Liyao%2520Li%2520and%2520Haobo%2520Wang%2520and%2520Gang%2520Chen%2520and%2520Junbo%2520Zhao%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520tightly%2520associate%2520with%250Athe%2520expansion%2520of%2520the%2520training%2520data%2520size.%2520However%252C%2520the%2520unchecked%250Aultra-large-scale%2520training%2520sets%2520introduce%2520a%2520series%2520of%2520potential%2520risks%2520like%2520data%250Acontamination%252C%2520i.e.%2520the%2520benchmark%2520data%2520is%2520used%2520for%2520training.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520holistic%2520method%2520named%2520Polarized%2520Augment%2520Calibration%2520%2528PAC%2529%2520along%2520with%250Aa%2520new%2520to-be-released%2520dataset%2520to%2520detect%2520the%2520contaminated%2520data%2520and%2520diminish%2520the%250Acontamination%2520effect.%2520PAC%2520extends%2520the%2520popular%2520MIA%2520%2528Membership%2520Inference%2520Attack%2529%250A--%2520from%2520machine%2520learning%2520community%2520--%2520by%2520forming%2520a%2520more%2520global%2520target%2520at%250Adetecting%2520training%2520data%2520to%2520Clarify%2520invisible%2520training%2520data.%2520As%2520a%2520pioneering%250Awork%252C%2520PAC%2520is%2520very%2520much%2520plug-and-play%2520that%2520can%2520be%2520integrated%2520with%2520most%2520%2528if%2520not%250Aall%2529%2520current%2520white-%2520and%2520black-box%2520LLMs.%2520By%2520extensive%2520experiments%252C%2520PAC%250Aoutperforms%2520existing%2520methods%2520by%2520at%2520least%25204.5%2525%252C%2520towards%2520data%2520contamination%250Adetection%2520on%2520more%25204%2520dataset%2520formats%252C%2520with%2520more%2520than%252010%2520base%2520LLMs.%2520Besides%252C%2520our%250Aapplication%2520in%2520real-world%2520scenarios%2520highlights%2520the%2520prominent%2520presence%2520of%250Acontamination%2520and%2520related%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Contamination%20Calibration%20for%20Black-box%20LLMs&entry.906535625=Wentao%20Ye%20and%20Jiaqi%20Hu%20and%20Liyao%20Li%20and%20Haobo%20Wang%20and%20Gang%20Chen%20and%20Junbo%20Zhao&entry.1292438233=%20%20The%20rapid%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%20tightly%20associate%20with%0Athe%20expansion%20of%20the%20training%20data%20size.%20However%2C%20the%20unchecked%0Aultra-large-scale%20training%20sets%20introduce%20a%20series%20of%20potential%20risks%20like%20data%0Acontamination%2C%20i.e.%20the%20benchmark%20data%20is%20used%20for%20training.%20In%20this%20work%2C%20we%0Apropose%20a%20holistic%20method%20named%20Polarized%20Augment%20Calibration%20%28PAC%29%20along%20with%0Aa%20new%20to-be-released%20dataset%20to%20detect%20the%20contaminated%20data%20and%20diminish%20the%0Acontamination%20effect.%20PAC%20extends%20the%20popular%20MIA%20%28Membership%20Inference%20Attack%29%0A--%20from%20machine%20learning%20community%20--%20by%20forming%20a%20more%20global%20target%20at%0Adetecting%20training%20data%20to%20Clarify%20invisible%20training%20data.%20As%20a%20pioneering%0Awork%2C%20PAC%20is%20very%20much%20plug-and-play%20that%20can%20be%20integrated%20with%20most%20%28if%20not%0Aall%29%20current%20white-%20and%20black-box%20LLMs.%20By%20extensive%20experiments%2C%20PAC%0Aoutperforms%20existing%20methods%20by%20at%20least%204.5%25%2C%20towards%20data%20contamination%0Adetection%20on%20more%204%20dataset%20formats%2C%20with%20more%20than%2010%20base%20LLMs.%20Besides%2C%20our%0Aapplication%20in%20real-world%20scenarios%20highlights%20the%20prominent%20presence%20of%0Acontamination%20and%20related%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11930v1&entry.124074799=Read"},
{"title": "Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy\n  Learning", "author": "Xuecheng Niu and Akinori Ito and Takashi Nose", "abstract": "  Training task-oriented dialog agents based on reinforcement learning is\ntime-consuming and requires a large number of interactions with real users. How\nto grasp dialog policy within limited dialog experiences remains an obstacle\nthat makes the agent training process less efficient. In addition, most\nprevious frameworks start training by randomly choosing training samples, which\ndiffers from the human learning method and hurts the efficiency and stability\nof training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a\ncuriosity-driven curriculum learning framework based on a state-of-the-art\nmodel-based reinforcement learning dialog model, Deep Dyna-Q (DDQ).\nFurthermore, we designed learning schedules for SC-DDQ and DDQ, respectively,\nfollowing two opposite training strategies: classic curriculum learning and its\nreverse version. Our results show that by introducing scheduled learning and\ncuriosity, the new framework leads to a significant improvement over the DDQ\nand Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum\nlearning was not always effective. Specifically, according to the experimental\nresults, the easy-first and difficult-first strategies are more suitable for\nSC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled\nactions to depict action exploration and found that training strategies with\nhigh entropy in the first stage and low entropy in the last stage lead to\nbetter performance.\n", "link": "http://arxiv.org/abs/2402.00085v2", "date": "2024-05-20", "relevancy": 2.0748, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5369}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scheduled%20Curiosity-Deep%20Dyna-Q%3A%20Efficient%20Exploration%20for%20Dialog%20Policy%0A%20%20Learning&body=Title%3A%20Scheduled%20Curiosity-Deep%20Dyna-Q%3A%20Efficient%20Exploration%20for%20Dialog%20Policy%0A%20%20Learning%0AAuthor%3A%20Xuecheng%20Niu%20and%20Akinori%20Ito%20and%20Takashi%20Nose%0AAbstract%3A%20%20%20Training%20task-oriented%20dialog%20agents%20based%20on%20reinforcement%20learning%20is%0Atime-consuming%20and%20requires%20a%20large%20number%20of%20interactions%20with%20real%20users.%20How%0Ato%20grasp%20dialog%20policy%20within%20limited%20dialog%20experiences%20remains%20an%20obstacle%0Athat%20makes%20the%20agent%20training%20process%20less%20efficient.%20In%20addition%2C%20most%0Aprevious%20frameworks%20start%20training%20by%20randomly%20choosing%20training%20samples%2C%20which%0Adiffers%20from%20the%20human%20learning%20method%20and%20hurts%20the%20efficiency%20and%20stability%0Aof%20training.%20Therefore%2C%20we%20propose%20Scheduled%20Curiosity-Deep%20Dyna-Q%20%28SC-DDQ%29%2C%20a%0Acuriosity-driven%20curriculum%20learning%20framework%20based%20on%20a%20state-of-the-art%0Amodel-based%20reinforcement%20learning%20dialog%20model%2C%20Deep%20Dyna-Q%20%28DDQ%29.%0AFurthermore%2C%20we%20designed%20learning%20schedules%20for%20SC-DDQ%20and%20DDQ%2C%20respectively%2C%0Afollowing%20two%20opposite%20training%20strategies%3A%20classic%20curriculum%20learning%20and%20its%0Areverse%20version.%20Our%20results%20show%20that%20by%20introducing%20scheduled%20learning%20and%0Acuriosity%2C%20the%20new%20framework%20leads%20to%20a%20significant%20improvement%20over%20the%20DDQ%0Aand%20Deep%20Q-learning%28DQN%29.%20Surprisingly%2C%20we%20found%20that%20traditional%20curriculum%0Alearning%20was%20not%20always%20effective.%20Specifically%2C%20according%20to%20the%20experimental%0Aresults%2C%20the%20easy-first%20and%20difficult-first%20strategies%20are%20more%20suitable%20for%0ASC-DDQ%20and%20DDQ.%20To%20analyze%20our%20results%2C%20we%20adopted%20the%20entropy%20of%20sampled%0Aactions%20to%20depict%20action%20exploration%20and%20found%20that%20training%20strategies%20with%0Ahigh%20entropy%20in%20the%20first%20stage%20and%20low%20entropy%20in%20the%20last%20stage%20lead%20to%0Abetter%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScheduled%2520Curiosity-Deep%2520Dyna-Q%253A%2520Efficient%2520Exploration%2520for%2520Dialog%2520Policy%250A%2520%2520Learning%26entry.906535625%3DXuecheng%2520Niu%2520and%2520Akinori%2520Ito%2520and%2520Takashi%2520Nose%26entry.1292438233%3D%2520%2520Training%2520task-oriented%2520dialog%2520agents%2520based%2520on%2520reinforcement%2520learning%2520is%250Atime-consuming%2520and%2520requires%2520a%2520large%2520number%2520of%2520interactions%2520with%2520real%2520users.%2520How%250Ato%2520grasp%2520dialog%2520policy%2520within%2520limited%2520dialog%2520experiences%2520remains%2520an%2520obstacle%250Athat%2520makes%2520the%2520agent%2520training%2520process%2520less%2520efficient.%2520In%2520addition%252C%2520most%250Aprevious%2520frameworks%2520start%2520training%2520by%2520randomly%2520choosing%2520training%2520samples%252C%2520which%250Adiffers%2520from%2520the%2520human%2520learning%2520method%2520and%2520hurts%2520the%2520efficiency%2520and%2520stability%250Aof%2520training.%2520Therefore%252C%2520we%2520propose%2520Scheduled%2520Curiosity-Deep%2520Dyna-Q%2520%2528SC-DDQ%2529%252C%2520a%250Acuriosity-driven%2520curriculum%2520learning%2520framework%2520based%2520on%2520a%2520state-of-the-art%250Amodel-based%2520reinforcement%2520learning%2520dialog%2520model%252C%2520Deep%2520Dyna-Q%2520%2528DDQ%2529.%250AFurthermore%252C%2520we%2520designed%2520learning%2520schedules%2520for%2520SC-DDQ%2520and%2520DDQ%252C%2520respectively%252C%250Afollowing%2520two%2520opposite%2520training%2520strategies%253A%2520classic%2520curriculum%2520learning%2520and%2520its%250Areverse%2520version.%2520Our%2520results%2520show%2520that%2520by%2520introducing%2520scheduled%2520learning%2520and%250Acuriosity%252C%2520the%2520new%2520framework%2520leads%2520to%2520a%2520significant%2520improvement%2520over%2520the%2520DDQ%250Aand%2520Deep%2520Q-learning%2528DQN%2529.%2520Surprisingly%252C%2520we%2520found%2520that%2520traditional%2520curriculum%250Alearning%2520was%2520not%2520always%2520effective.%2520Specifically%252C%2520according%2520to%2520the%2520experimental%250Aresults%252C%2520the%2520easy-first%2520and%2520difficult-first%2520strategies%2520are%2520more%2520suitable%2520for%250ASC-DDQ%2520and%2520DDQ.%2520To%2520analyze%2520our%2520results%252C%2520we%2520adopted%2520the%2520entropy%2520of%2520sampled%250Aactions%2520to%2520depict%2520action%2520exploration%2520and%2520found%2520that%2520training%2520strategies%2520with%250Ahigh%2520entropy%2520in%2520the%2520first%2520stage%2520and%2520low%2520entropy%2520in%2520the%2520last%2520stage%2520lead%2520to%250Abetter%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scheduled%20Curiosity-Deep%20Dyna-Q%3A%20Efficient%20Exploration%20for%20Dialog%20Policy%0A%20%20Learning&entry.906535625=Xuecheng%20Niu%20and%20Akinori%20Ito%20and%20Takashi%20Nose&entry.1292438233=%20%20Training%20task-oriented%20dialog%20agents%20based%20on%20reinforcement%20learning%20is%0Atime-consuming%20and%20requires%20a%20large%20number%20of%20interactions%20with%20real%20users.%20How%0Ato%20grasp%20dialog%20policy%20within%20limited%20dialog%20experiences%20remains%20an%20obstacle%0Athat%20makes%20the%20agent%20training%20process%20less%20efficient.%20In%20addition%2C%20most%0Aprevious%20frameworks%20start%20training%20by%20randomly%20choosing%20training%20samples%2C%20which%0Adiffers%20from%20the%20human%20learning%20method%20and%20hurts%20the%20efficiency%20and%20stability%0Aof%20training.%20Therefore%2C%20we%20propose%20Scheduled%20Curiosity-Deep%20Dyna-Q%20%28SC-DDQ%29%2C%20a%0Acuriosity-driven%20curriculum%20learning%20framework%20based%20on%20a%20state-of-the-art%0Amodel-based%20reinforcement%20learning%20dialog%20model%2C%20Deep%20Dyna-Q%20%28DDQ%29.%0AFurthermore%2C%20we%20designed%20learning%20schedules%20for%20SC-DDQ%20and%20DDQ%2C%20respectively%2C%0Afollowing%20two%20opposite%20training%20strategies%3A%20classic%20curriculum%20learning%20and%20its%0Areverse%20version.%20Our%20results%20show%20that%20by%20introducing%20scheduled%20learning%20and%0Acuriosity%2C%20the%20new%20framework%20leads%20to%20a%20significant%20improvement%20over%20the%20DDQ%0Aand%20Deep%20Q-learning%28DQN%29.%20Surprisingly%2C%20we%20found%20that%20traditional%20curriculum%0Alearning%20was%20not%20always%20effective.%20Specifically%2C%20according%20to%20the%20experimental%0Aresults%2C%20the%20easy-first%20and%20difficult-first%20strategies%20are%20more%20suitable%20for%0ASC-DDQ%20and%20DDQ.%20To%20analyze%20our%20results%2C%20we%20adopted%20the%20entropy%20of%20sampled%0Aactions%20to%20depict%20action%20exploration%20and%20found%20that%20training%20strategies%20with%0Ahigh%20entropy%20in%20the%20first%20stage%20and%20low%20entropy%20in%20the%20last%20stage%20lead%20to%0Abetter%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00085v2&entry.124074799=Read"},
{"title": "Continuous Sign Language Recognition with Adapted Conformer via\n  Unsupervised Pretraining", "author": "Neena Aloysius and Geetha M and Prema Nedungadi", "abstract": "  Conventional Deep Learning frameworks for continuous sign language\nrecognition (CSLR) are comprised of a single or multi-modal feature extractor,\na sequence-learning module, and a decoder for outputting the glosses. The\nsequence learning module is a crucial part wherein transformers have\ndemonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the\nresearch progress in the field of Natural Language Processing and Speech\nRecognition, a rapid introduction of various transformer variants is observed.\nHowever, in the realm of sign language, experimentation in the sequence\nlearning component is limited. In this work, the state-of-the-art Conformer\nmodel for Speech Recognition is adapted for CSLR and the proposed model is\ntermed ConSignformer. This marks the first instance of employing Conformer for\na vision-based task. ConSignformer has bimodal pipeline of CNN as feature\nextractor and Conformer for sequence learning. For improved context learning we\nalso introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA\ninto the model, it becomes more adept at learning and utilizing complex\nrelationships within the data. To further enhance the Conformer model,\nunsupervised pretraining called Regressional Feature Extraction is conducted on\na curated sign language dataset. The pretrained Conformer is then fine-tuned\nfor the downstream recognition task. The experimental results confirm the\neffectiveness of the adopted pretraining strategy and demonstrate how CMRA\ncontributes to the recognition process. Remarkably, leveraging a\nConformer-based backbone, our model achieves state-of-the-art performance on\nthe benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.\n", "link": "http://arxiv.org/abs/2405.12018v1", "date": "2024-05-20", "relevancy": 2.0724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5409}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Sign%20Language%20Recognition%20with%20Adapted%20Conformer%20via%0A%20%20Unsupervised%20Pretraining&body=Title%3A%20Continuous%20Sign%20Language%20Recognition%20with%20Adapted%20Conformer%20via%0A%20%20Unsupervised%20Pretraining%0AAuthor%3A%20Neena%20Aloysius%20and%20Geetha%20M%20and%20Prema%20Nedungadi%0AAbstract%3A%20%20%20Conventional%20Deep%20Learning%20frameworks%20for%20continuous%20sign%20language%0Arecognition%20%28CSLR%29%20are%20comprised%20of%20a%20single%20or%20multi-modal%20feature%20extractor%2C%0Aa%20sequence-learning%20module%2C%20and%20a%20decoder%20for%20outputting%20the%20glosses.%20The%0Asequence%20learning%20module%20is%20a%20crucial%20part%20wherein%20transformers%20have%0Ademonstrated%20their%20efficacy%20in%20the%20sequence-to-sequence%20tasks.%20Analyzing%20the%0Aresearch%20progress%20in%20the%20field%20of%20Natural%20Language%20Processing%20and%20Speech%0ARecognition%2C%20a%20rapid%20introduction%20of%20various%20transformer%20variants%20is%20observed.%0AHowever%2C%20in%20the%20realm%20of%20sign%20language%2C%20experimentation%20in%20the%20sequence%0Alearning%20component%20is%20limited.%20In%20this%20work%2C%20the%20state-of-the-art%20Conformer%0Amodel%20for%20Speech%20Recognition%20is%20adapted%20for%20CSLR%20and%20the%20proposed%20model%20is%0Atermed%20ConSignformer.%20This%20marks%20the%20first%20instance%20of%20employing%20Conformer%20for%0Aa%20vision-based%20task.%20ConSignformer%20has%20bimodal%20pipeline%20of%20CNN%20as%20feature%0Aextractor%20and%20Conformer%20for%20sequence%20learning.%20For%20improved%20context%20learning%20we%0Aalso%20introduce%20Cross-Modal%20Relative%20Attention%20%28CMRA%29.%20By%20incorporating%20CMRA%0Ainto%20the%20model%2C%20it%20becomes%20more%20adept%20at%20learning%20and%20utilizing%20complex%0Arelationships%20within%20the%20data.%20To%20further%20enhance%20the%20Conformer%20model%2C%0Aunsupervised%20pretraining%20called%20Regressional%20Feature%20Extraction%20is%20conducted%20on%0Aa%20curated%20sign%20language%20dataset.%20The%20pretrained%20Conformer%20is%20then%20fine-tuned%0Afor%20the%20downstream%20recognition%20task.%20The%20experimental%20results%20confirm%20the%0Aeffectiveness%20of%20the%20adopted%20pretraining%20strategy%20and%20demonstrate%20how%20CMRA%0Acontributes%20to%20the%20recognition%20process.%20Remarkably%2C%20leveraging%20a%0AConformer-based%20backbone%2C%20our%20model%20achieves%20state-of-the-art%20performance%20on%0Athe%20benchmark%20datasets%3A%20PHOENIX-2014%20and%20PHOENIX-2014T.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Sign%2520Language%2520Recognition%2520with%2520Adapted%2520Conformer%2520via%250A%2520%2520Unsupervised%2520Pretraining%26entry.906535625%3DNeena%2520Aloysius%2520and%2520Geetha%2520M%2520and%2520Prema%2520Nedungadi%26entry.1292438233%3D%2520%2520Conventional%2520Deep%2520Learning%2520frameworks%2520for%2520continuous%2520sign%2520language%250Arecognition%2520%2528CSLR%2529%2520are%2520comprised%2520of%2520a%2520single%2520or%2520multi-modal%2520feature%2520extractor%252C%250Aa%2520sequence-learning%2520module%252C%2520and%2520a%2520decoder%2520for%2520outputting%2520the%2520glosses.%2520The%250Asequence%2520learning%2520module%2520is%2520a%2520crucial%2520part%2520wherein%2520transformers%2520have%250Ademonstrated%2520their%2520efficacy%2520in%2520the%2520sequence-to-sequence%2520tasks.%2520Analyzing%2520the%250Aresearch%2520progress%2520in%2520the%2520field%2520of%2520Natural%2520Language%2520Processing%2520and%2520Speech%250ARecognition%252C%2520a%2520rapid%2520introduction%2520of%2520various%2520transformer%2520variants%2520is%2520observed.%250AHowever%252C%2520in%2520the%2520realm%2520of%2520sign%2520language%252C%2520experimentation%2520in%2520the%2520sequence%250Alearning%2520component%2520is%2520limited.%2520In%2520this%2520work%252C%2520the%2520state-of-the-art%2520Conformer%250Amodel%2520for%2520Speech%2520Recognition%2520is%2520adapted%2520for%2520CSLR%2520and%2520the%2520proposed%2520model%2520is%250Atermed%2520ConSignformer.%2520This%2520marks%2520the%2520first%2520instance%2520of%2520employing%2520Conformer%2520for%250Aa%2520vision-based%2520task.%2520ConSignformer%2520has%2520bimodal%2520pipeline%2520of%2520CNN%2520as%2520feature%250Aextractor%2520and%2520Conformer%2520for%2520sequence%2520learning.%2520For%2520improved%2520context%2520learning%2520we%250Aalso%2520introduce%2520Cross-Modal%2520Relative%2520Attention%2520%2528CMRA%2529.%2520By%2520incorporating%2520CMRA%250Ainto%2520the%2520model%252C%2520it%2520becomes%2520more%2520adept%2520at%2520learning%2520and%2520utilizing%2520complex%250Arelationships%2520within%2520the%2520data.%2520To%2520further%2520enhance%2520the%2520Conformer%2520model%252C%250Aunsupervised%2520pretraining%2520called%2520Regressional%2520Feature%2520Extraction%2520is%2520conducted%2520on%250Aa%2520curated%2520sign%2520language%2520dataset.%2520The%2520pretrained%2520Conformer%2520is%2520then%2520fine-tuned%250Afor%2520the%2520downstream%2520recognition%2520task.%2520The%2520experimental%2520results%2520confirm%2520the%250Aeffectiveness%2520of%2520the%2520adopted%2520pretraining%2520strategy%2520and%2520demonstrate%2520how%2520CMRA%250Acontributes%2520to%2520the%2520recognition%2520process.%2520Remarkably%252C%2520leveraging%2520a%250AConformer-based%2520backbone%252C%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%250Athe%2520benchmark%2520datasets%253A%2520PHOENIX-2014%2520and%2520PHOENIX-2014T.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Sign%20Language%20Recognition%20with%20Adapted%20Conformer%20via%0A%20%20Unsupervised%20Pretraining&entry.906535625=Neena%20Aloysius%20and%20Geetha%20M%20and%20Prema%20Nedungadi&entry.1292438233=%20%20Conventional%20Deep%20Learning%20frameworks%20for%20continuous%20sign%20language%0Arecognition%20%28CSLR%29%20are%20comprised%20of%20a%20single%20or%20multi-modal%20feature%20extractor%2C%0Aa%20sequence-learning%20module%2C%20and%20a%20decoder%20for%20outputting%20the%20glosses.%20The%0Asequence%20learning%20module%20is%20a%20crucial%20part%20wherein%20transformers%20have%0Ademonstrated%20their%20efficacy%20in%20the%20sequence-to-sequence%20tasks.%20Analyzing%20the%0Aresearch%20progress%20in%20the%20field%20of%20Natural%20Language%20Processing%20and%20Speech%0ARecognition%2C%20a%20rapid%20introduction%20of%20various%20transformer%20variants%20is%20observed.%0AHowever%2C%20in%20the%20realm%20of%20sign%20language%2C%20experimentation%20in%20the%20sequence%0Alearning%20component%20is%20limited.%20In%20this%20work%2C%20the%20state-of-the-art%20Conformer%0Amodel%20for%20Speech%20Recognition%20is%20adapted%20for%20CSLR%20and%20the%20proposed%20model%20is%0Atermed%20ConSignformer.%20This%20marks%20the%20first%20instance%20of%20employing%20Conformer%20for%0Aa%20vision-based%20task.%20ConSignformer%20has%20bimodal%20pipeline%20of%20CNN%20as%20feature%0Aextractor%20and%20Conformer%20for%20sequence%20learning.%20For%20improved%20context%20learning%20we%0Aalso%20introduce%20Cross-Modal%20Relative%20Attention%20%28CMRA%29.%20By%20incorporating%20CMRA%0Ainto%20the%20model%2C%20it%20becomes%20more%20adept%20at%20learning%20and%20utilizing%20complex%0Arelationships%20within%20the%20data.%20To%20further%20enhance%20the%20Conformer%20model%2C%0Aunsupervised%20pretraining%20called%20Regressional%20Feature%20Extraction%20is%20conducted%20on%0Aa%20curated%20sign%20language%20dataset.%20The%20pretrained%20Conformer%20is%20then%20fine-tuned%0Afor%20the%20downstream%20recognition%20task.%20The%20experimental%20results%20confirm%20the%0Aeffectiveness%20of%20the%20adopted%20pretraining%20strategy%20and%20demonstrate%20how%20CMRA%0Acontributes%20to%20the%20recognition%20process.%20Remarkably%2C%20leveraging%20a%0AConformer-based%20backbone%2C%20our%20model%20achieves%20state-of-the-art%20performance%20on%0Athe%20benchmark%20datasets%3A%20PHOENIX-2014%20and%20PHOENIX-2014T.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12018v1&entry.124074799=Read"},
{"title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware\n  Loss Prior for Arbitrary-scale Super-resolution", "author": "Xihaier Luo and Xiaoning Qian and Byung-Jun Yoon", "abstract": "  In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.\n", "link": "http://arxiv.org/abs/2405.12202v1", "date": "2024-05-20", "relevancy": 2.0675, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5503}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Neural%20Operator%20Transformer%20with%20Learnable%20Frequency-aware%0A%20%20Loss%20Prior%20for%20Arbitrary-scale%20Super-resolution&body=Title%3A%20Hierarchical%20Neural%20Operator%20Transformer%20with%20Learnable%20Frequency-aware%0A%20%20Loss%20Prior%20for%20Arbitrary-scale%20Super-resolution%0AAuthor%3A%20Xihaier%20Luo%20and%20Xiaoning%20Qian%20and%20Byung-Jun%20Yoon%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20an%20arbitrary-scale%20super-resolution%20%28SR%29%20method%20to%0Aenhance%20the%20resolution%20of%20scientific%20data%2C%20which%20often%20involves%20complex%0Achallenges%20such%20as%20continuity%2C%20multi-scale%20physics%2C%20and%20the%20intricacies%20of%0Ahigh-frequency%20signals.%20Grounded%20in%20operator%20learning%2C%20the%20proposed%20method%20is%0Aresolution-invariant.%20The%20core%20of%20our%20model%20is%20a%20hierarchical%20neural%20operator%0Athat%20leverages%20a%20Galerkin-type%20self-attention%20mechanism%2C%20enabling%20efficient%0Alearning%20of%20mappings%20between%20function%20spaces.%20Sinc%20filters%20are%20used%20to%0Afacilitate%20the%20information%20transfer%20across%20different%20levels%20in%20the%20hierarchy%2C%0Athereby%20ensuring%20representation%20equivalence%20in%20the%20proposed%20neural%20operator.%0AAdditionally%2C%20we%20introduce%20a%20learnable%20prior%20structure%20that%20is%20derived%20from%20the%0Aspectral%20resizing%20of%20the%20input%20data.%20This%20loss%20prior%20is%20model-agnostic%20and%20is%0Adesigned%20to%20dynamically%20adjust%20the%20weighting%20of%20pixel%20contributions%2C%20thereby%0Abalancing%20gradients%20effectively%20across%20the%20model.%20We%20conduct%20extensive%0Aexperiments%20on%20diverse%20datasets%20from%20different%20domains%20and%20demonstrate%0Aconsistent%20improvements%20compared%20to%20strong%20baselines%2C%20which%20consist%20of%20various%0Astate-of-the-art%20SR%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Neural%2520Operator%2520Transformer%2520with%2520Learnable%2520Frequency-aware%250A%2520%2520Loss%2520Prior%2520for%2520Arbitrary-scale%2520Super-resolution%26entry.906535625%3DXihaier%2520Luo%2520and%2520Xiaoning%2520Qian%2520and%2520Byung-Jun%2520Yoon%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520arbitrary-scale%2520super-resolution%2520%2528SR%2529%2520method%2520to%250Aenhance%2520the%2520resolution%2520of%2520scientific%2520data%252C%2520which%2520often%2520involves%2520complex%250Achallenges%2520such%2520as%2520continuity%252C%2520multi-scale%2520physics%252C%2520and%2520the%2520intricacies%2520of%250Ahigh-frequency%2520signals.%2520Grounded%2520in%2520operator%2520learning%252C%2520the%2520proposed%2520method%2520is%250Aresolution-invariant.%2520The%2520core%2520of%2520our%2520model%2520is%2520a%2520hierarchical%2520neural%2520operator%250Athat%2520leverages%2520a%2520Galerkin-type%2520self-attention%2520mechanism%252C%2520enabling%2520efficient%250Alearning%2520of%2520mappings%2520between%2520function%2520spaces.%2520Sinc%2520filters%2520are%2520used%2520to%250Afacilitate%2520the%2520information%2520transfer%2520across%2520different%2520levels%2520in%2520the%2520hierarchy%252C%250Athereby%2520ensuring%2520representation%2520equivalence%2520in%2520the%2520proposed%2520neural%2520operator.%250AAdditionally%252C%2520we%2520introduce%2520a%2520learnable%2520prior%2520structure%2520that%2520is%2520derived%2520from%2520the%250Aspectral%2520resizing%2520of%2520the%2520input%2520data.%2520This%2520loss%2520prior%2520is%2520model-agnostic%2520and%2520is%250Adesigned%2520to%2520dynamically%2520adjust%2520the%2520weighting%2520of%2520pixel%2520contributions%252C%2520thereby%250Abalancing%2520gradients%2520effectively%2520across%2520the%2520model.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520diverse%2520datasets%2520from%2520different%2520domains%2520and%2520demonstrate%250Aconsistent%2520improvements%2520compared%2520to%2520strong%2520baselines%252C%2520which%2520consist%2520of%2520various%250Astate-of-the-art%2520SR%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Neural%20Operator%20Transformer%20with%20Learnable%20Frequency-aware%0A%20%20Loss%20Prior%20for%20Arbitrary-scale%20Super-resolution&entry.906535625=Xihaier%20Luo%20and%20Xiaoning%20Qian%20and%20Byung-Jun%20Yoon&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20an%20arbitrary-scale%20super-resolution%20%28SR%29%20method%20to%0Aenhance%20the%20resolution%20of%20scientific%20data%2C%20which%20often%20involves%20complex%0Achallenges%20such%20as%20continuity%2C%20multi-scale%20physics%2C%20and%20the%20intricacies%20of%0Ahigh-frequency%20signals.%20Grounded%20in%20operator%20learning%2C%20the%20proposed%20method%20is%0Aresolution-invariant.%20The%20core%20of%20our%20model%20is%20a%20hierarchical%20neural%20operator%0Athat%20leverages%20a%20Galerkin-type%20self-attention%20mechanism%2C%20enabling%20efficient%0Alearning%20of%20mappings%20between%20function%20spaces.%20Sinc%20filters%20are%20used%20to%0Afacilitate%20the%20information%20transfer%20across%20different%20levels%20in%20the%20hierarchy%2C%0Athereby%20ensuring%20representation%20equivalence%20in%20the%20proposed%20neural%20operator.%0AAdditionally%2C%20we%20introduce%20a%20learnable%20prior%20structure%20that%20is%20derived%20from%20the%0Aspectral%20resizing%20of%20the%20input%20data.%20This%20loss%20prior%20is%20model-agnostic%20and%20is%0Adesigned%20to%20dynamically%20adjust%20the%20weighting%20of%20pixel%20contributions%2C%20thereby%0Abalancing%20gradients%20effectively%20across%20the%20model.%20We%20conduct%20extensive%0Aexperiments%20on%20diverse%20datasets%20from%20different%20domains%20and%20demonstrate%0Aconsistent%20improvements%20compared%20to%20strong%20baselines%2C%20which%20consist%20of%20various%0Astate-of-the-art%20SR%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12202v1&entry.124074799=Read"},
{"title": "Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP\n  for CNN Interpretability", "author": "Vaibhav Dhore and Achintya Bhat and Viraj Nerlekar and Kashyap Chavhan and Aniket Umare", "abstract": "  We present a new technique that explains the output of a CNN-based model\nusing a combination of GradCAM and LRP methods. Both of these methods produce\nvisual explanations by highlighting input regions that are important for\npredictions. In the new method, the explanation produced by GradCAM is first\nprocessed to remove noises. The processed output is then multiplied elementwise\nwith the output of LRP. Finally, a Gaussian blur is applied on the product. We\ncompared the proposed method with GradCAM and LRP on the metrics of\nFaithfulness, Robustness, Complexity, Localisation and Randomisation. It was\nobserved that this method performs better on Complexity than both GradCAM and\nLRP and is better than atleast one of them in the other metrics.\n", "link": "http://arxiv.org/abs/2405.12175v1", "date": "2024-05-20", "relevancy": 2.0645, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Explainable%20AI%3A%20A%20Hybrid%20Approach%20Combining%20GradCAM%20and%20LRP%0A%20%20for%20CNN%20Interpretability&body=Title%3A%20Enhancing%20Explainable%20AI%3A%20A%20Hybrid%20Approach%20Combining%20GradCAM%20and%20LRP%0A%20%20for%20CNN%20Interpretability%0AAuthor%3A%20Vaibhav%20Dhore%20and%20Achintya%20Bhat%20and%20Viraj%20Nerlekar%20and%20Kashyap%20Chavhan%20and%20Aniket%20Umare%0AAbstract%3A%20%20%20We%20present%20a%20new%20technique%20that%20explains%20the%20output%20of%20a%20CNN-based%20model%0Ausing%20a%20combination%20of%20GradCAM%20and%20LRP%20methods.%20Both%20of%20these%20methods%20produce%0Avisual%20explanations%20by%20highlighting%20input%20regions%20that%20are%20important%20for%0Apredictions.%20In%20the%20new%20method%2C%20the%20explanation%20produced%20by%20GradCAM%20is%20first%0Aprocessed%20to%20remove%20noises.%20The%20processed%20output%20is%20then%20multiplied%20elementwise%0Awith%20the%20output%20of%20LRP.%20Finally%2C%20a%20Gaussian%20blur%20is%20applied%20on%20the%20product.%20We%0Acompared%20the%20proposed%20method%20with%20GradCAM%20and%20LRP%20on%20the%20metrics%20of%0AFaithfulness%2C%20Robustness%2C%20Complexity%2C%20Localisation%20and%20Randomisation.%20It%20was%0Aobserved%20that%20this%20method%20performs%20better%20on%20Complexity%20than%20both%20GradCAM%20and%0ALRP%20and%20is%20better%20than%20atleast%20one%20of%20them%20in%20the%20other%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Explainable%2520AI%253A%2520A%2520Hybrid%2520Approach%2520Combining%2520GradCAM%2520and%2520LRP%250A%2520%2520for%2520CNN%2520Interpretability%26entry.906535625%3DVaibhav%2520Dhore%2520and%2520Achintya%2520Bhat%2520and%2520Viraj%2520Nerlekar%2520and%2520Kashyap%2520Chavhan%2520and%2520Aniket%2520Umare%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520technique%2520that%2520explains%2520the%2520output%2520of%2520a%2520CNN-based%2520model%250Ausing%2520a%2520combination%2520of%2520GradCAM%2520and%2520LRP%2520methods.%2520Both%2520of%2520these%2520methods%2520produce%250Avisual%2520explanations%2520by%2520highlighting%2520input%2520regions%2520that%2520are%2520important%2520for%250Apredictions.%2520In%2520the%2520new%2520method%252C%2520the%2520explanation%2520produced%2520by%2520GradCAM%2520is%2520first%250Aprocessed%2520to%2520remove%2520noises.%2520The%2520processed%2520output%2520is%2520then%2520multiplied%2520elementwise%250Awith%2520the%2520output%2520of%2520LRP.%2520Finally%252C%2520a%2520Gaussian%2520blur%2520is%2520applied%2520on%2520the%2520product.%2520We%250Acompared%2520the%2520proposed%2520method%2520with%2520GradCAM%2520and%2520LRP%2520on%2520the%2520metrics%2520of%250AFaithfulness%252C%2520Robustness%252C%2520Complexity%252C%2520Localisation%2520and%2520Randomisation.%2520It%2520was%250Aobserved%2520that%2520this%2520method%2520performs%2520better%2520on%2520Complexity%2520than%2520both%2520GradCAM%2520and%250ALRP%2520and%2520is%2520better%2520than%2520atleast%2520one%2520of%2520them%2520in%2520the%2520other%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Explainable%20AI%3A%20A%20Hybrid%20Approach%20Combining%20GradCAM%20and%20LRP%0A%20%20for%20CNN%20Interpretability&entry.906535625=Vaibhav%20Dhore%20and%20Achintya%20Bhat%20and%20Viraj%20Nerlekar%20and%20Kashyap%20Chavhan%20and%20Aniket%20Umare&entry.1292438233=%20%20We%20present%20a%20new%20technique%20that%20explains%20the%20output%20of%20a%20CNN-based%20model%0Ausing%20a%20combination%20of%20GradCAM%20and%20LRP%20methods.%20Both%20of%20these%20methods%20produce%0Avisual%20explanations%20by%20highlighting%20input%20regions%20that%20are%20important%20for%0Apredictions.%20In%20the%20new%20method%2C%20the%20explanation%20produced%20by%20GradCAM%20is%20first%0Aprocessed%20to%20remove%20noises.%20The%20processed%20output%20is%20then%20multiplied%20elementwise%0Awith%20the%20output%20of%20LRP.%20Finally%2C%20a%20Gaussian%20blur%20is%20applied%20on%20the%20product.%20We%0Acompared%20the%20proposed%20method%20with%20GradCAM%20and%20LRP%20on%20the%20metrics%20of%0AFaithfulness%2C%20Robustness%2C%20Complexity%2C%20Localisation%20and%20Randomisation.%20It%20was%0Aobserved%20that%20this%20method%20performs%20better%20on%20Complexity%20than%20both%20GradCAM%20and%0ALRP%20and%20is%20better%20than%20atleast%20one%20of%20them%20in%20the%20other%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12175v1&entry.124074799=Read"},
{"title": "PatchAD: A Lightweight Patch-based MLP-Mixer for Time Series Anomaly\n  Detection", "author": "Zhijie Zhong and Zhiwen Yu and Yiyuan Yang and Weizheng Wang and Kaixiang Yang", "abstract": "  Anomaly detection in time series analysis is a pivotal task, yet it poses the\nchallenge of discerning normal and abnormal patterns in label-deficient\nscenarios. While prior studies have largely employed reconstruction-based\napproaches, which limits the models' representational capacities. Moreover,\nexisting deep learning-based methods are not sufficiently lightweight.\nAddressing these issues, we present PatchAD, our novel, highly efficient\nmultiscale patch-based MLP-Mixer architecture that utilizes contrastive\nlearning for representation extraction and anomaly detection. With its four\ndistinct MLP Mixers and innovative dual project constraint module, PatchAD\nmitigates potential model degradation and offers a lightweight solution,\nrequiring only $3.2$MB. Its efficacy is demonstrated by state-of-the-art\nresults across $9$ datasets sourced from different application scenarios,\noutperforming over $30$ comparative algorithms. PatchAD significantly improves\nthe classical F1 score by $50.5\\%$, the Aff-F1 score by $7.8\\%$, and the AUC by\n$10.0\\%$. The code is publicly available.\n\\url{https://github.com/EmorZz1G/PatchAD}\n", "link": "http://arxiv.org/abs/2401.09793v4", "date": "2024-05-20", "relevancy": 2.0625, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5296}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5076}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection&body=Title%3A%20PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Zhijie%20Zhong%20and%20Zhiwen%20Yu%20and%20Yiyuan%20Yang%20and%20Weizheng%20Wang%20and%20Kaixiang%20Yang%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20time%20series%20analysis%20is%20a%20pivotal%20task%2C%20yet%20it%20poses%20the%0Achallenge%20of%20discerning%20normal%20and%20abnormal%20patterns%20in%20label-deficient%0Ascenarios.%20While%20prior%20studies%20have%20largely%20employed%20reconstruction-based%0Aapproaches%2C%20which%20limits%20the%20models%27%20representational%20capacities.%20Moreover%2C%0Aexisting%20deep%20learning-based%20methods%20are%20not%20sufficiently%20lightweight.%0AAddressing%20these%20issues%2C%20we%20present%20PatchAD%2C%20our%20novel%2C%20highly%20efficient%0Amultiscale%20patch-based%20MLP-Mixer%20architecture%20that%20utilizes%20contrastive%0Alearning%20for%20representation%20extraction%20and%20anomaly%20detection.%20With%20its%20four%0Adistinct%20MLP%20Mixers%20and%20innovative%20dual%20project%20constraint%20module%2C%20PatchAD%0Amitigates%20potential%20model%20degradation%20and%20offers%20a%20lightweight%20solution%2C%0Arequiring%20only%20%243.2%24MB.%20Its%20efficacy%20is%20demonstrated%20by%20state-of-the-art%0Aresults%20across%20%249%24%20datasets%20sourced%20from%20different%20application%20scenarios%2C%0Aoutperforming%20over%20%2430%24%20comparative%20algorithms.%20PatchAD%20significantly%20improves%0Athe%20classical%20F1%20score%20by%20%2450.5%5C%25%24%2C%20the%20Aff-F1%20score%20by%20%247.8%5C%25%24%2C%20and%20the%20AUC%20by%0A%2410.0%5C%25%24.%20The%20code%20is%20publicly%20available.%0A%5Curl%7Bhttps%3A//github.com/EmorZz1G/PatchAD%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09793v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchAD%253A%2520A%2520Lightweight%2520Patch-based%2520MLP-Mixer%2520for%2520Time%2520Series%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DZhijie%2520Zhong%2520and%2520Zhiwen%2520Yu%2520and%2520Yiyuan%2520Yang%2520and%2520Weizheng%2520Wang%2520and%2520Kaixiang%2520Yang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520time%2520series%2520analysis%2520is%2520a%2520pivotal%2520task%252C%2520yet%2520it%2520poses%2520the%250Achallenge%2520of%2520discerning%2520normal%2520and%2520abnormal%2520patterns%2520in%2520label-deficient%250Ascenarios.%2520While%2520prior%2520studies%2520have%2520largely%2520employed%2520reconstruction-based%250Aapproaches%252C%2520which%2520limits%2520the%2520models%2527%2520representational%2520capacities.%2520Moreover%252C%250Aexisting%2520deep%2520learning-based%2520methods%2520are%2520not%2520sufficiently%2520lightweight.%250AAddressing%2520these%2520issues%252C%2520we%2520present%2520PatchAD%252C%2520our%2520novel%252C%2520highly%2520efficient%250Amultiscale%2520patch-based%2520MLP-Mixer%2520architecture%2520that%2520utilizes%2520contrastive%250Alearning%2520for%2520representation%2520extraction%2520and%2520anomaly%2520detection.%2520With%2520its%2520four%250Adistinct%2520MLP%2520Mixers%2520and%2520innovative%2520dual%2520project%2520constraint%2520module%252C%2520PatchAD%250Amitigates%2520potential%2520model%2520degradation%2520and%2520offers%2520a%2520lightweight%2520solution%252C%250Arequiring%2520only%2520%25243.2%2524MB.%2520Its%2520efficacy%2520is%2520demonstrated%2520by%2520state-of-the-art%250Aresults%2520across%2520%25249%2524%2520datasets%2520sourced%2520from%2520different%2520application%2520scenarios%252C%250Aoutperforming%2520over%2520%252430%2524%2520comparative%2520algorithms.%2520PatchAD%2520significantly%2520improves%250Athe%2520classical%2520F1%2520score%2520by%2520%252450.5%255C%2525%2524%252C%2520the%2520Aff-F1%2520score%2520by%2520%25247.8%255C%2525%2524%252C%2520and%2520the%2520AUC%2520by%250A%252410.0%255C%2525%2524.%2520The%2520code%2520is%2520publicly%2520available.%250A%255Curl%257Bhttps%253A//github.com/EmorZz1G/PatchAD%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09793v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection&entry.906535625=Zhijie%20Zhong%20and%20Zhiwen%20Yu%20and%20Yiyuan%20Yang%20and%20Weizheng%20Wang%20and%20Kaixiang%20Yang&entry.1292438233=%20%20Anomaly%20detection%20in%20time%20series%20analysis%20is%20a%20pivotal%20task%2C%20yet%20it%20poses%20the%0Achallenge%20of%20discerning%20normal%20and%20abnormal%20patterns%20in%20label-deficient%0Ascenarios.%20While%20prior%20studies%20have%20largely%20employed%20reconstruction-based%0Aapproaches%2C%20which%20limits%20the%20models%27%20representational%20capacities.%20Moreover%2C%0Aexisting%20deep%20learning-based%20methods%20are%20not%20sufficiently%20lightweight.%0AAddressing%20these%20issues%2C%20we%20present%20PatchAD%2C%20our%20novel%2C%20highly%20efficient%0Amultiscale%20patch-based%20MLP-Mixer%20architecture%20that%20utilizes%20contrastive%0Alearning%20for%20representation%20extraction%20and%20anomaly%20detection.%20With%20its%20four%0Adistinct%20MLP%20Mixers%20and%20innovative%20dual%20project%20constraint%20module%2C%20PatchAD%0Amitigates%20potential%20model%20degradation%20and%20offers%20a%20lightweight%20solution%2C%0Arequiring%20only%20%243.2%24MB.%20Its%20efficacy%20is%20demonstrated%20by%20state-of-the-art%0Aresults%20across%20%249%24%20datasets%20sourced%20from%20different%20application%20scenarios%2C%0Aoutperforming%20over%20%2430%24%20comparative%20algorithms.%20PatchAD%20significantly%20improves%0Athe%20classical%20F1%20score%20by%20%2450.5%5C%25%24%2C%20the%20Aff-F1%20score%20by%20%247.8%5C%25%24%2C%20and%20the%20AUC%20by%0A%2410.0%5C%25%24.%20The%20code%20is%20publicly%20available.%0A%5Curl%7Bhttps%3A//github.com/EmorZz1G/PatchAD%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09793v4&entry.124074799=Read"},
{"title": "Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays", "author": "Zhichao Sun and Yuliang Gu and Yepeng Liu and Zerui Zhang and Zhou Zhao and Yongchao Xu", "abstract": "  Anomaly detection in chest X-rays is a critical task. Most methods mainly\nmodel the distribution of normal images, and then regard significant deviation\nfrom normal distribution as anomaly. Recently, CLIP-based methods, pre-trained\non a large number of medical images, have shown impressive performance on\nzero/few-shot downstream tasks. In this paper, we aim to explore the potential\nof CLIP-based methods for anomaly detection in chest X-rays. Considering the\ndiscrepancy between the CLIP pre-training data and the task-specific data, we\npropose a position-guided prompt learning method. Specifically, inspired by the\nfact that experts diagnose chest X-rays by carefully examining distinct lung\nregions, we propose learnable position-guided text and image prompts to adapt\nthe task data to the frozen pre-trained CLIP-based model. To enhance the\nmodel's discriminative capability, we propose a novel structure-preserving\nanomaly synthesis method within chest x-rays during the training process.\nExtensive experiments on three datasets demonstrate that our proposed method\noutperforms some state-of-the-art methods. The code of our implementation is\navailable at https://github.com/sunzc-sunny/PPAD.\n", "link": "http://arxiv.org/abs/2405.11976v1", "date": "2024-05-20", "relevancy": 2.053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5045}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position-Guided%20Prompt%20Learning%20for%20Anomaly%20Detection%20in%20Chest%20X-Rays&body=Title%3A%20Position-Guided%20Prompt%20Learning%20for%20Anomaly%20Detection%20in%20Chest%20X-Rays%0AAuthor%3A%20Zhichao%20Sun%20and%20Yuliang%20Gu%20and%20Yepeng%20Liu%20and%20Zerui%20Zhang%20and%20Zhou%20Zhao%20and%20Yongchao%20Xu%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20chest%20X-rays%20is%20a%20critical%20task.%20Most%20methods%20mainly%0Amodel%20the%20distribution%20of%20normal%20images%2C%20and%20then%20regard%20significant%20deviation%0Afrom%20normal%20distribution%20as%20anomaly.%20Recently%2C%20CLIP-based%20methods%2C%20pre-trained%0Aon%20a%20large%20number%20of%20medical%20images%2C%20have%20shown%20impressive%20performance%20on%0Azero/few-shot%20downstream%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%20explore%20the%20potential%0Aof%20CLIP-based%20methods%20for%20anomaly%20detection%20in%20chest%20X-rays.%20Considering%20the%0Adiscrepancy%20between%20the%20CLIP%20pre-training%20data%20and%20the%20task-specific%20data%2C%20we%0Apropose%20a%20position-guided%20prompt%20learning%20method.%20Specifically%2C%20inspired%20by%20the%0Afact%20that%20experts%20diagnose%20chest%20X-rays%20by%20carefully%20examining%20distinct%20lung%0Aregions%2C%20we%20propose%20learnable%20position-guided%20text%20and%20image%20prompts%20to%20adapt%0Athe%20task%20data%20to%20the%20frozen%20pre-trained%20CLIP-based%20model.%20To%20enhance%20the%0Amodel%27s%20discriminative%20capability%2C%20we%20propose%20a%20novel%20structure-preserving%0Aanomaly%20synthesis%20method%20within%20chest%20x-rays%20during%20the%20training%20process.%0AExtensive%20experiments%20on%20three%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aoutperforms%20some%20state-of-the-art%20methods.%20The%20code%20of%20our%20implementation%20is%0Aavailable%20at%20https%3A//github.com/sunzc-sunny/PPAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition-Guided%2520Prompt%2520Learning%2520for%2520Anomaly%2520Detection%2520in%2520Chest%2520X-Rays%26entry.906535625%3DZhichao%2520Sun%2520and%2520Yuliang%2520Gu%2520and%2520Yepeng%2520Liu%2520and%2520Zerui%2520Zhang%2520and%2520Zhou%2520Zhao%2520and%2520Yongchao%2520Xu%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520chest%2520X-rays%2520is%2520a%2520critical%2520task.%2520Most%2520methods%2520mainly%250Amodel%2520the%2520distribution%2520of%2520normal%2520images%252C%2520and%2520then%2520regard%2520significant%2520deviation%250Afrom%2520normal%2520distribution%2520as%2520anomaly.%2520Recently%252C%2520CLIP-based%2520methods%252C%2520pre-trained%250Aon%2520a%2520large%2520number%2520of%2520medical%2520images%252C%2520have%2520shown%2520impressive%2520performance%2520on%250Azero/few-shot%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520explore%2520the%2520potential%250Aof%2520CLIP-based%2520methods%2520for%2520anomaly%2520detection%2520in%2520chest%2520X-rays.%2520Considering%2520the%250Adiscrepancy%2520between%2520the%2520CLIP%2520pre-training%2520data%2520and%2520the%2520task-specific%2520data%252C%2520we%250Apropose%2520a%2520position-guided%2520prompt%2520learning%2520method.%2520Specifically%252C%2520inspired%2520by%2520the%250Afact%2520that%2520experts%2520diagnose%2520chest%2520X-rays%2520by%2520carefully%2520examining%2520distinct%2520lung%250Aregions%252C%2520we%2520propose%2520learnable%2520position-guided%2520text%2520and%2520image%2520prompts%2520to%2520adapt%250Athe%2520task%2520data%2520to%2520the%2520frozen%2520pre-trained%2520CLIP-based%2520model.%2520To%2520enhance%2520the%250Amodel%2527s%2520discriminative%2520capability%252C%2520we%2520propose%2520a%2520novel%2520structure-preserving%250Aanomaly%2520synthesis%2520method%2520within%2520chest%2520x-rays%2520during%2520the%2520training%2520process.%250AExtensive%2520experiments%2520on%2520three%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520method%250Aoutperforms%2520some%2520state-of-the-art%2520methods.%2520The%2520code%2520of%2520our%2520implementation%2520is%250Aavailable%2520at%2520https%253A//github.com/sunzc-sunny/PPAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position-Guided%20Prompt%20Learning%20for%20Anomaly%20Detection%20in%20Chest%20X-Rays&entry.906535625=Zhichao%20Sun%20and%20Yuliang%20Gu%20and%20Yepeng%20Liu%20and%20Zerui%20Zhang%20and%20Zhou%20Zhao%20and%20Yongchao%20Xu&entry.1292438233=%20%20Anomaly%20detection%20in%20chest%20X-rays%20is%20a%20critical%20task.%20Most%20methods%20mainly%0Amodel%20the%20distribution%20of%20normal%20images%2C%20and%20then%20regard%20significant%20deviation%0Afrom%20normal%20distribution%20as%20anomaly.%20Recently%2C%20CLIP-based%20methods%2C%20pre-trained%0Aon%20a%20large%20number%20of%20medical%20images%2C%20have%20shown%20impressive%20performance%20on%0Azero/few-shot%20downstream%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%20explore%20the%20potential%0Aof%20CLIP-based%20methods%20for%20anomaly%20detection%20in%20chest%20X-rays.%20Considering%20the%0Adiscrepancy%20between%20the%20CLIP%20pre-training%20data%20and%20the%20task-specific%20data%2C%20we%0Apropose%20a%20position-guided%20prompt%20learning%20method.%20Specifically%2C%20inspired%20by%20the%0Afact%20that%20experts%20diagnose%20chest%20X-rays%20by%20carefully%20examining%20distinct%20lung%0Aregions%2C%20we%20propose%20learnable%20position-guided%20text%20and%20image%20prompts%20to%20adapt%0Athe%20task%20data%20to%20the%20frozen%20pre-trained%20CLIP-based%20model.%20To%20enhance%20the%0Amodel%27s%20discriminative%20capability%2C%20we%20propose%20a%20novel%20structure-preserving%0Aanomaly%20synthesis%20method%20within%20chest%20x-rays%20during%20the%20training%20process.%0AExtensive%20experiments%20on%20three%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aoutperforms%20some%20state-of-the-art%20methods.%20The%20code%20of%20our%20implementation%20is%0Aavailable%20at%20https%3A//github.com/sunzc-sunny/PPAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11976v1&entry.124074799=Read"},
{"title": "UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal\n  Prediction", "author": "Yuan Yuan and Jingtao Ding and Jie Feng and Depeng Jin and Yong Li", "abstract": "  Urban spatio-temporal prediction is crucial for informed decision-making,\nsuch as transportation management, resource optimization, and urban planning.\nAlthough pretrained foundation models for natural languages have experienced\nremarkable breakthroughs, wherein one general-purpose model can tackle multiple\ntasks across various domains, urban spatio-temporal modeling lags behind.\nExisting approaches for urban prediction are usually tailored for specific\nspatio-temporal scenarios, requiring task-specific model designs and extensive\nin-domain training data. In this work, we propose a universal model, UniST, for\nurban spatio-temporal prediction. Drawing inspiration from large language\nmodels, UniST achieves success through: (i) flexibility towards diverse\nspatio-temporal data characteristics, (ii) effective generative pre-training\nwith elaborated masking strategies to capture complex spatio-temporal\nrelationships, (iii) spatio-temporal knowledge-guided prompts that align and\nleverage intrinsic and shared knowledge across scenarios. These designs\ntogether unlock the potential of a one-for-all model for spatio-temporal\nprediction with powerful generalization capability. Extensive experiments on 15\ncities and 6 domains demonstrate the universality of UniST in advancing\nstate-of-the-art prediction performance, especially in few-shot and zero-shot\nscenarios.\n", "link": "http://arxiv.org/abs/2402.11838v2", "date": "2024-05-20", "relevancy": 2.0489, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5175}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5134}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniST%3A%20A%20Prompt-Empowered%20Universal%20Model%20for%20Urban%20Spatio-Temporal%0A%20%20Prediction&body=Title%3A%20UniST%3A%20A%20Prompt-Empowered%20Universal%20Model%20for%20Urban%20Spatio-Temporal%0A%20%20Prediction%0AAuthor%3A%20Yuan%20Yuan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Depeng%20Jin%20and%20Yong%20Li%0AAbstract%3A%20%20%20Urban%20spatio-temporal%20prediction%20is%20crucial%20for%20informed%20decision-making%2C%0Asuch%20as%20transportation%20management%2C%20resource%20optimization%2C%20and%20urban%20planning.%0AAlthough%20pretrained%20foundation%20models%20for%20natural%20languages%20have%20experienced%0Aremarkable%20breakthroughs%2C%20wherein%20one%20general-purpose%20model%20can%20tackle%20multiple%0Atasks%20across%20various%20domains%2C%20urban%20spatio-temporal%20modeling%20lags%20behind.%0AExisting%20approaches%20for%20urban%20prediction%20are%20usually%20tailored%20for%20specific%0Aspatio-temporal%20scenarios%2C%20requiring%20task-specific%20model%20designs%20and%20extensive%0Ain-domain%20training%20data.%20In%20this%20work%2C%20we%20propose%20a%20universal%20model%2C%20UniST%2C%20for%0Aurban%20spatio-temporal%20prediction.%20Drawing%20inspiration%20from%20large%20language%0Amodels%2C%20UniST%20achieves%20success%20through%3A%20%28i%29%20flexibility%20towards%20diverse%0Aspatio-temporal%20data%20characteristics%2C%20%28ii%29%20effective%20generative%20pre-training%0Awith%20elaborated%20masking%20strategies%20to%20capture%20complex%20spatio-temporal%0Arelationships%2C%20%28iii%29%20spatio-temporal%20knowledge-guided%20prompts%20that%20align%20and%0Aleverage%20intrinsic%20and%20shared%20knowledge%20across%20scenarios.%20These%20designs%0Atogether%20unlock%20the%20potential%20of%20a%20one-for-all%20model%20for%20spatio-temporal%0Aprediction%20with%20powerful%20generalization%20capability.%20Extensive%20experiments%20on%2015%0Acities%20and%206%20domains%20demonstrate%20the%20universality%20of%20UniST%20in%20advancing%0Astate-of-the-art%20prediction%20performance%2C%20especially%20in%20few-shot%20and%20zero-shot%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniST%253A%2520A%2520Prompt-Empowered%2520Universal%2520Model%2520for%2520Urban%2520Spatio-Temporal%250A%2520%2520Prediction%26entry.906535625%3DYuan%2520Yuan%2520and%2520Jingtao%2520Ding%2520and%2520Jie%2520Feng%2520and%2520Depeng%2520Jin%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Urban%2520spatio-temporal%2520prediction%2520is%2520crucial%2520for%2520informed%2520decision-making%252C%250Asuch%2520as%2520transportation%2520management%252C%2520resource%2520optimization%252C%2520and%2520urban%2520planning.%250AAlthough%2520pretrained%2520foundation%2520models%2520for%2520natural%2520languages%2520have%2520experienced%250Aremarkable%2520breakthroughs%252C%2520wherein%2520one%2520general-purpose%2520model%2520can%2520tackle%2520multiple%250Atasks%2520across%2520various%2520domains%252C%2520urban%2520spatio-temporal%2520modeling%2520lags%2520behind.%250AExisting%2520approaches%2520for%2520urban%2520prediction%2520are%2520usually%2520tailored%2520for%2520specific%250Aspatio-temporal%2520scenarios%252C%2520requiring%2520task-specific%2520model%2520designs%2520and%2520extensive%250Ain-domain%2520training%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520universal%2520model%252C%2520UniST%252C%2520for%250Aurban%2520spatio-temporal%2520prediction.%2520Drawing%2520inspiration%2520from%2520large%2520language%250Amodels%252C%2520UniST%2520achieves%2520success%2520through%253A%2520%2528i%2529%2520flexibility%2520towards%2520diverse%250Aspatio-temporal%2520data%2520characteristics%252C%2520%2528ii%2529%2520effective%2520generative%2520pre-training%250Awith%2520elaborated%2520masking%2520strategies%2520to%2520capture%2520complex%2520spatio-temporal%250Arelationships%252C%2520%2528iii%2529%2520spatio-temporal%2520knowledge-guided%2520prompts%2520that%2520align%2520and%250Aleverage%2520intrinsic%2520and%2520shared%2520knowledge%2520across%2520scenarios.%2520These%2520designs%250Atogether%2520unlock%2520the%2520potential%2520of%2520a%2520one-for-all%2520model%2520for%2520spatio-temporal%250Aprediction%2520with%2520powerful%2520generalization%2520capability.%2520Extensive%2520experiments%2520on%252015%250Acities%2520and%25206%2520domains%2520demonstrate%2520the%2520universality%2520of%2520UniST%2520in%2520advancing%250Astate-of-the-art%2520prediction%2520performance%252C%2520especially%2520in%2520few-shot%2520and%2520zero-shot%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniST%3A%20A%20Prompt-Empowered%20Universal%20Model%20for%20Urban%20Spatio-Temporal%0A%20%20Prediction&entry.906535625=Yuan%20Yuan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Depeng%20Jin%20and%20Yong%20Li&entry.1292438233=%20%20Urban%20spatio-temporal%20prediction%20is%20crucial%20for%20informed%20decision-making%2C%0Asuch%20as%20transportation%20management%2C%20resource%20optimization%2C%20and%20urban%20planning.%0AAlthough%20pretrained%20foundation%20models%20for%20natural%20languages%20have%20experienced%0Aremarkable%20breakthroughs%2C%20wherein%20one%20general-purpose%20model%20can%20tackle%20multiple%0Atasks%20across%20various%20domains%2C%20urban%20spatio-temporal%20modeling%20lags%20behind.%0AExisting%20approaches%20for%20urban%20prediction%20are%20usually%20tailored%20for%20specific%0Aspatio-temporal%20scenarios%2C%20requiring%20task-specific%20model%20designs%20and%20extensive%0Ain-domain%20training%20data.%20In%20this%20work%2C%20we%20propose%20a%20universal%20model%2C%20UniST%2C%20for%0Aurban%20spatio-temporal%20prediction.%20Drawing%20inspiration%20from%20large%20language%0Amodels%2C%20UniST%20achieves%20success%20through%3A%20%28i%29%20flexibility%20towards%20diverse%0Aspatio-temporal%20data%20characteristics%2C%20%28ii%29%20effective%20generative%20pre-training%0Awith%20elaborated%20masking%20strategies%20to%20capture%20complex%20spatio-temporal%0Arelationships%2C%20%28iii%29%20spatio-temporal%20knowledge-guided%20prompts%20that%20align%20and%0Aleverage%20intrinsic%20and%20shared%20knowledge%20across%20scenarios.%20These%20designs%0Atogether%20unlock%20the%20potential%20of%20a%20one-for-all%20model%20for%20spatio-temporal%0Aprediction%20with%20powerful%20generalization%20capability.%20Extensive%20experiments%20on%2015%0Acities%20and%206%20domains%20demonstrate%20the%20universality%20of%20UniST%20in%20advancing%0Astate-of-the-art%20prediction%20performance%2C%20especially%20in%20few-shot%20and%20zero-shot%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11838v2&entry.124074799=Read"},
{"title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity", "author": "Diego Sanmartin", "abstract": "  Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.\n", "link": "http://arxiv.org/abs/2405.12035v1", "date": "2024-05-20", "relevancy": 2.0484, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5043}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KG-RAG%3A%20Bridging%20the%20Gap%20Between%20Knowledge%20and%20Creativity&body=Title%3A%20KG-RAG%3A%20Bridging%20the%20Gap%20Between%20Knowledge%20and%20Creativity%0AAuthor%3A%20Diego%20Sanmartin%0AAbstract%3A%20%20%20Ensuring%20factual%20accuracy%20while%20maintaining%20the%20creative%20capabilities%20of%0ALarge%20Language%20Model%20Agents%20%28LMAs%29%20poses%20significant%20challenges%20in%20the%0Adevelopment%20of%20intelligent%20agent%20systems.%20LMAs%20face%20prevalent%20issues%20such%20as%0Ainformation%20hallucinations%2C%20catastrophic%20forgetting%2C%20and%20limitations%20in%0Aprocessing%20long%20contexts%20when%20dealing%20with%20knowledge-intensive%20tasks.%20This%0Apaper%20introduces%20a%20KG-RAG%20%28Knowledge%20Graph-Retrieval%20Augmented%20Generation%29%0Apipeline%2C%20a%20novel%20framework%20designed%20to%20enhance%20the%20knowledge%20capabilities%20of%0ALMAs%20by%20integrating%20structured%20Knowledge%20Graphs%20%28KGs%29%20with%20the%20functionalities%0Aof%20LLMs%2C%20thereby%20significantly%20reducing%20the%20reliance%20on%20the%20latent%20knowledge%20of%0ALLMs.%20The%20KG-RAG%20pipeline%20constructs%20a%20KG%20from%20unstructured%20text%20and%20then%0Aperforms%20information%20retrieval%20over%20the%20newly%20created%20graph%20to%20perform%20KGQA%0A%28Knowledge%20Graph%20Question%20Answering%29.%20The%20retrieval%20methodology%20leverages%20a%0Anovel%20algorithm%20called%20Chain%20of%20Explorations%20%28CoE%29%20which%20benefits%20from%20LLMs%0Areasoning%20to%20explore%20nodes%20and%20relationships%20within%20the%20KG%20sequentially.%0APreliminary%20experiments%20on%20the%20ComplexWebQuestions%20dataset%20demonstrate%20notable%0Aimprovements%20in%20the%20reduction%20of%20hallucinated%20content%20and%20suggest%20a%20promising%0Apath%20toward%20developing%20intelligent%20systems%20adept%20at%20handling%0Aknowledge-intensive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKG-RAG%253A%2520Bridging%2520the%2520Gap%2520Between%2520Knowledge%2520and%2520Creativity%26entry.906535625%3DDiego%2520Sanmartin%26entry.1292438233%3D%2520%2520Ensuring%2520factual%2520accuracy%2520while%2520maintaining%2520the%2520creative%2520capabilities%2520of%250ALarge%2520Language%2520Model%2520Agents%2520%2528LMAs%2529%2520poses%2520significant%2520challenges%2520in%2520the%250Adevelopment%2520of%2520intelligent%2520agent%2520systems.%2520LMAs%2520face%2520prevalent%2520issues%2520such%2520as%250Ainformation%2520hallucinations%252C%2520catastrophic%2520forgetting%252C%2520and%2520limitations%2520in%250Aprocessing%2520long%2520contexts%2520when%2520dealing%2520with%2520knowledge-intensive%2520tasks.%2520This%250Apaper%2520introduces%2520a%2520KG-RAG%2520%2528Knowledge%2520Graph-Retrieval%2520Augmented%2520Generation%2529%250Apipeline%252C%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520the%2520knowledge%2520capabilities%2520of%250ALMAs%2520by%2520integrating%2520structured%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520with%2520the%2520functionalities%250Aof%2520LLMs%252C%2520thereby%2520significantly%2520reducing%2520the%2520reliance%2520on%2520the%2520latent%2520knowledge%2520of%250ALLMs.%2520The%2520KG-RAG%2520pipeline%2520constructs%2520a%2520KG%2520from%2520unstructured%2520text%2520and%2520then%250Aperforms%2520information%2520retrieval%2520over%2520the%2520newly%2520created%2520graph%2520to%2520perform%2520KGQA%250A%2528Knowledge%2520Graph%2520Question%2520Answering%2529.%2520The%2520retrieval%2520methodology%2520leverages%2520a%250Anovel%2520algorithm%2520called%2520Chain%2520of%2520Explorations%2520%2528CoE%2529%2520which%2520benefits%2520from%2520LLMs%250Areasoning%2520to%2520explore%2520nodes%2520and%2520relationships%2520within%2520the%2520KG%2520sequentially.%250APreliminary%2520experiments%2520on%2520the%2520ComplexWebQuestions%2520dataset%2520demonstrate%2520notable%250Aimprovements%2520in%2520the%2520reduction%2520of%2520hallucinated%2520content%2520and%2520suggest%2520a%2520promising%250Apath%2520toward%2520developing%2520intelligent%2520systems%2520adept%2520at%2520handling%250Aknowledge-intensive%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KG-RAG%3A%20Bridging%20the%20Gap%20Between%20Knowledge%20and%20Creativity&entry.906535625=Diego%20Sanmartin&entry.1292438233=%20%20Ensuring%20factual%20accuracy%20while%20maintaining%20the%20creative%20capabilities%20of%0ALarge%20Language%20Model%20Agents%20%28LMAs%29%20poses%20significant%20challenges%20in%20the%0Adevelopment%20of%20intelligent%20agent%20systems.%20LMAs%20face%20prevalent%20issues%20such%20as%0Ainformation%20hallucinations%2C%20catastrophic%20forgetting%2C%20and%20limitations%20in%0Aprocessing%20long%20contexts%20when%20dealing%20with%20knowledge-intensive%20tasks.%20This%0Apaper%20introduces%20a%20KG-RAG%20%28Knowledge%20Graph-Retrieval%20Augmented%20Generation%29%0Apipeline%2C%20a%20novel%20framework%20designed%20to%20enhance%20the%20knowledge%20capabilities%20of%0ALMAs%20by%20integrating%20structured%20Knowledge%20Graphs%20%28KGs%29%20with%20the%20functionalities%0Aof%20LLMs%2C%20thereby%20significantly%20reducing%20the%20reliance%20on%20the%20latent%20knowledge%20of%0ALLMs.%20The%20KG-RAG%20pipeline%20constructs%20a%20KG%20from%20unstructured%20text%20and%20then%0Aperforms%20information%20retrieval%20over%20the%20newly%20created%20graph%20to%20perform%20KGQA%0A%28Knowledge%20Graph%20Question%20Answering%29.%20The%20retrieval%20methodology%20leverages%20a%0Anovel%20algorithm%20called%20Chain%20of%20Explorations%20%28CoE%29%20which%20benefits%20from%20LLMs%0Areasoning%20to%20explore%20nodes%20and%20relationships%20within%20the%20KG%20sequentially.%0APreliminary%20experiments%20on%20the%20ComplexWebQuestions%20dataset%20demonstrate%20notable%0Aimprovements%20in%20the%20reduction%20of%20hallucinated%20content%20and%20suggest%20a%20promising%0Apath%20toward%20developing%20intelligent%20systems%20adept%20at%20handling%0Aknowledge-intensive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12035v1&entry.124074799=Read"},
{"title": "Learning Top-k Subtask Planning Tree based on Discriminative\n  Representation Pre-training for Decision Making", "author": "Jingqing Ruan and Kaishen Wang and Qingyang Zhang and Dengpeng Xing and Bo Xu", "abstract": "  Many complicated real-world tasks can be broken down into smaller, more\nmanageable parts, and planning with prior knowledge extracted from these\nsimplified pieces is crucial for humans to make accurate decisions. However,\nreplicating this process remains a challenge for AI agents and naturally raises\ntwo questions: How to extract discriminative knowledge representation from\npriors? How to develop a rational plan to decompose complex problems? Most\nexisting representation learning methods employing a single encoder structure\nare fragile and sensitive to complex and diverse dynamics. To address this\nissue, we introduce a multiple-encoder and individual-predictor regime to learn\ntask-essential representations from sufficient data for simple subtasks.\nMultiple encoders can extract adequate task-relevant dynamics without\nconfusion, and the shared predictor can discriminate the task characteristics.\nWe also use the attention mechanism to generate a top-k subtask planning tree,\nwhich customizes subtask execution plans in guiding complex decisions on unseen\ntasks. This process enables forward-looking and globality by flexibly adjusting\nthe depth and width of the planning tree. Empirical results on a challenging\nplatform composed of some basic simple tasks and combinatorially rich synthetic\ntasks consistently outperform some competitive baselines and demonstrate the\nbenefits of our design.\n", "link": "http://arxiv.org/abs/2312.11027v2", "date": "2024-05-20", "relevancy": 2.0478, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Top-k%20Subtask%20Planning%20Tree%20based%20on%20Discriminative%0A%20%20Representation%20Pre-training%20for%20Decision%20Making&body=Title%3A%20Learning%20Top-k%20Subtask%20Planning%20Tree%20based%20on%20Discriminative%0A%20%20Representation%20Pre-training%20for%20Decision%20Making%0AAuthor%3A%20Jingqing%20Ruan%20and%20Kaishen%20Wang%20and%20Qingyang%20Zhang%20and%20Dengpeng%20Xing%20and%20Bo%20Xu%0AAbstract%3A%20%20%20Many%20complicated%20real-world%20tasks%20can%20be%20broken%20down%20into%20smaller%2C%20more%0Amanageable%20parts%2C%20and%20planning%20with%20prior%20knowledge%20extracted%20from%20these%0Asimplified%20pieces%20is%20crucial%20for%20humans%20to%20make%20accurate%20decisions.%20However%2C%0Areplicating%20this%20process%20remains%20a%20challenge%20for%20AI%20agents%20and%20naturally%20raises%0Atwo%20questions%3A%20How%20to%20extract%20discriminative%20knowledge%20representation%20from%0Apriors%3F%20How%20to%20develop%20a%20rational%20plan%20to%20decompose%20complex%20problems%3F%20Most%0Aexisting%20representation%20learning%20methods%20employing%20a%20single%20encoder%20structure%0Aare%20fragile%20and%20sensitive%20to%20complex%20and%20diverse%20dynamics.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20multiple-encoder%20and%20individual-predictor%20regime%20to%20learn%0Atask-essential%20representations%20from%20sufficient%20data%20for%20simple%20subtasks.%0AMultiple%20encoders%20can%20extract%20adequate%20task-relevant%20dynamics%20without%0Aconfusion%2C%20and%20the%20shared%20predictor%20can%20discriminate%20the%20task%20characteristics.%0AWe%20also%20use%20the%20attention%20mechanism%20to%20generate%20a%20top-k%20subtask%20planning%20tree%2C%0Awhich%20customizes%20subtask%20execution%20plans%20in%20guiding%20complex%20decisions%20on%20unseen%0Atasks.%20This%20process%20enables%20forward-looking%20and%20globality%20by%20flexibly%20adjusting%0Athe%20depth%20and%20width%20of%20the%20planning%20tree.%20Empirical%20results%20on%20a%20challenging%0Aplatform%20composed%20of%20some%20basic%20simple%20tasks%20and%20combinatorially%20rich%20synthetic%0Atasks%20consistently%20outperform%20some%20competitive%20baselines%20and%20demonstrate%20the%0Abenefits%20of%20our%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Top-k%2520Subtask%2520Planning%2520Tree%2520based%2520on%2520Discriminative%250A%2520%2520Representation%2520Pre-training%2520for%2520Decision%2520Making%26entry.906535625%3DJingqing%2520Ruan%2520and%2520Kaishen%2520Wang%2520and%2520Qingyang%2520Zhang%2520and%2520Dengpeng%2520Xing%2520and%2520Bo%2520Xu%26entry.1292438233%3D%2520%2520Many%2520complicated%2520real-world%2520tasks%2520can%2520be%2520broken%2520down%2520into%2520smaller%252C%2520more%250Amanageable%2520parts%252C%2520and%2520planning%2520with%2520prior%2520knowledge%2520extracted%2520from%2520these%250Asimplified%2520pieces%2520is%2520crucial%2520for%2520humans%2520to%2520make%2520accurate%2520decisions.%2520However%252C%250Areplicating%2520this%2520process%2520remains%2520a%2520challenge%2520for%2520AI%2520agents%2520and%2520naturally%2520raises%250Atwo%2520questions%253A%2520How%2520to%2520extract%2520discriminative%2520knowledge%2520representation%2520from%250Apriors%253F%2520How%2520to%2520develop%2520a%2520rational%2520plan%2520to%2520decompose%2520complex%2520problems%253F%2520Most%250Aexisting%2520representation%2520learning%2520methods%2520employing%2520a%2520single%2520encoder%2520structure%250Aare%2520fragile%2520and%2520sensitive%2520to%2520complex%2520and%2520diverse%2520dynamics.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520a%2520multiple-encoder%2520and%2520individual-predictor%2520regime%2520to%2520learn%250Atask-essential%2520representations%2520from%2520sufficient%2520data%2520for%2520simple%2520subtasks.%250AMultiple%2520encoders%2520can%2520extract%2520adequate%2520task-relevant%2520dynamics%2520without%250Aconfusion%252C%2520and%2520the%2520shared%2520predictor%2520can%2520discriminate%2520the%2520task%2520characteristics.%250AWe%2520also%2520use%2520the%2520attention%2520mechanism%2520to%2520generate%2520a%2520top-k%2520subtask%2520planning%2520tree%252C%250Awhich%2520customizes%2520subtask%2520execution%2520plans%2520in%2520guiding%2520complex%2520decisions%2520on%2520unseen%250Atasks.%2520This%2520process%2520enables%2520forward-looking%2520and%2520globality%2520by%2520flexibly%2520adjusting%250Athe%2520depth%2520and%2520width%2520of%2520the%2520planning%2520tree.%2520Empirical%2520results%2520on%2520a%2520challenging%250Aplatform%2520composed%2520of%2520some%2520basic%2520simple%2520tasks%2520and%2520combinatorially%2520rich%2520synthetic%250Atasks%2520consistently%2520outperform%2520some%2520competitive%2520baselines%2520and%2520demonstrate%2520the%250Abenefits%2520of%2520our%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Top-k%20Subtask%20Planning%20Tree%20based%20on%20Discriminative%0A%20%20Representation%20Pre-training%20for%20Decision%20Making&entry.906535625=Jingqing%20Ruan%20and%20Kaishen%20Wang%20and%20Qingyang%20Zhang%20and%20Dengpeng%20Xing%20and%20Bo%20Xu&entry.1292438233=%20%20Many%20complicated%20real-world%20tasks%20can%20be%20broken%20down%20into%20smaller%2C%20more%0Amanageable%20parts%2C%20and%20planning%20with%20prior%20knowledge%20extracted%20from%20these%0Asimplified%20pieces%20is%20crucial%20for%20humans%20to%20make%20accurate%20decisions.%20However%2C%0Areplicating%20this%20process%20remains%20a%20challenge%20for%20AI%20agents%20and%20naturally%20raises%0Atwo%20questions%3A%20How%20to%20extract%20discriminative%20knowledge%20representation%20from%0Apriors%3F%20How%20to%20develop%20a%20rational%20plan%20to%20decompose%20complex%20problems%3F%20Most%0Aexisting%20representation%20learning%20methods%20employing%20a%20single%20encoder%20structure%0Aare%20fragile%20and%20sensitive%20to%20complex%20and%20diverse%20dynamics.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20multiple-encoder%20and%20individual-predictor%20regime%20to%20learn%0Atask-essential%20representations%20from%20sufficient%20data%20for%20simple%20subtasks.%0AMultiple%20encoders%20can%20extract%20adequate%20task-relevant%20dynamics%20without%0Aconfusion%2C%20and%20the%20shared%20predictor%20can%20discriminate%20the%20task%20characteristics.%0AWe%20also%20use%20the%20attention%20mechanism%20to%20generate%20a%20top-k%20subtask%20planning%20tree%2C%0Awhich%20customizes%20subtask%20execution%20plans%20in%20guiding%20complex%20decisions%20on%20unseen%0Atasks.%20This%20process%20enables%20forward-looking%20and%20globality%20by%20flexibly%20adjusting%0Athe%20depth%20and%20width%20of%20the%20planning%20tree.%20Empirical%20results%20on%20a%20challenging%0Aplatform%20composed%20of%20some%20basic%20simple%20tasks%20and%20combinatorially%20rich%20synthetic%0Atasks%20consistently%20outperform%20some%20competitive%20baselines%20and%20demonstrate%20the%0Abenefits%20of%20our%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11027v2&entry.124074799=Read"},
{"title": "Distinguished In Uniform: Self Attention Vs. Virtual Nodes", "author": "Eran Rosenbluth and Jan T\u00f6nshoff and Martin Ritzert and Berke Kisin and Martin Grohe", "abstract": "  Graph Transformers (GTs) such as SAN and GPS are graph processing models that\ncombine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were\nshown to be universal function approximators, with two reservations: 1. The\ninitial node features must be augmented with certain positional encodings. 2.\nThe approximation is non-uniform: Graphs of different sizes may require a\ndifferent approximating network.\n  We first clarify that this form of universality is not unique to GTs: Using\nthe same positional encodings, also pure MPGNNs and even 2-layer MLPs are\nnon-uniform universal approximators. We then consider uniform expressivity: The\ntarget function is to be approximated by a single network for graphs of all\nsizes. There, we compare GTs to the more efficient MPGNN + Virtual Node\narchitecture. The essential difference between the two model definitions is in\ntheir global computation method -- Self-Attention Vs Virtual Node. We prove\nthat none of the models is a uniform-universal approximator, before proving our\nmain result: Neither model's uniform expressivity subsumes the other's. We\ndemonstrate the theory with experiments on synthetic data. We further augment\nour study with real-world datasets, observing mixed results which indicate no\nclear ranking in practice as well.\n", "link": "http://arxiv.org/abs/2405.11951v1", "date": "2024-05-20", "relevancy": 2.0461, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5266}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5133}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distinguished%20In%20Uniform%3A%20Self%20Attention%20Vs.%20Virtual%20Nodes&body=Title%3A%20Distinguished%20In%20Uniform%3A%20Self%20Attention%20Vs.%20Virtual%20Nodes%0AAuthor%3A%20Eran%20Rosenbluth%20and%20Jan%20T%C3%B6nshoff%20and%20Martin%20Ritzert%20and%20Berke%20Kisin%20and%20Martin%20Grohe%0AAbstract%3A%20%20%20Graph%20Transformers%20%28GTs%29%20such%20as%20SAN%20and%20GPS%20are%20graph%20processing%20models%20that%0Acombine%20Message-Passing%20GNNs%20%28MPGNNs%29%20with%20global%20Self-Attention.%20They%20were%0Ashown%20to%20be%20universal%20function%20approximators%2C%20with%20two%20reservations%3A%201.%20The%0Ainitial%20node%20features%20must%20be%20augmented%20with%20certain%20positional%20encodings.%202.%0AThe%20approximation%20is%20non-uniform%3A%20Graphs%20of%20different%20sizes%20may%20require%20a%0Adifferent%20approximating%20network.%0A%20%20We%20first%20clarify%20that%20this%20form%20of%20universality%20is%20not%20unique%20to%20GTs%3A%20Using%0Athe%20same%20positional%20encodings%2C%20also%20pure%20MPGNNs%20and%20even%202-layer%20MLPs%20are%0Anon-uniform%20universal%20approximators.%20We%20then%20consider%20uniform%20expressivity%3A%20The%0Atarget%20function%20is%20to%20be%20approximated%20by%20a%20single%20network%20for%20graphs%20of%20all%0Asizes.%20There%2C%20we%20compare%20GTs%20to%20the%20more%20efficient%20MPGNN%20%2B%20Virtual%20Node%0Aarchitecture.%20The%20essential%20difference%20between%20the%20two%20model%20definitions%20is%20in%0Atheir%20global%20computation%20method%20--%20Self-Attention%20Vs%20Virtual%20Node.%20We%20prove%0Athat%20none%20of%20the%20models%20is%20a%20uniform-universal%20approximator%2C%20before%20proving%20our%0Amain%20result%3A%20Neither%20model%27s%20uniform%20expressivity%20subsumes%20the%20other%27s.%20We%0Ademonstrate%20the%20theory%20with%20experiments%20on%20synthetic%20data.%20We%20further%20augment%0Aour%20study%20with%20real-world%20datasets%2C%20observing%20mixed%20results%20which%20indicate%20no%0Aclear%20ranking%20in%20practice%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistinguished%2520In%2520Uniform%253A%2520Self%2520Attention%2520Vs.%2520Virtual%2520Nodes%26entry.906535625%3DEran%2520Rosenbluth%2520and%2520Jan%2520T%25C3%25B6nshoff%2520and%2520Martin%2520Ritzert%2520and%2520Berke%2520Kisin%2520and%2520Martin%2520Grohe%26entry.1292438233%3D%2520%2520Graph%2520Transformers%2520%2528GTs%2529%2520such%2520as%2520SAN%2520and%2520GPS%2520are%2520graph%2520processing%2520models%2520that%250Acombine%2520Message-Passing%2520GNNs%2520%2528MPGNNs%2529%2520with%2520global%2520Self-Attention.%2520They%2520were%250Ashown%2520to%2520be%2520universal%2520function%2520approximators%252C%2520with%2520two%2520reservations%253A%25201.%2520The%250Ainitial%2520node%2520features%2520must%2520be%2520augmented%2520with%2520certain%2520positional%2520encodings.%25202.%250AThe%2520approximation%2520is%2520non-uniform%253A%2520Graphs%2520of%2520different%2520sizes%2520may%2520require%2520a%250Adifferent%2520approximating%2520network.%250A%2520%2520We%2520first%2520clarify%2520that%2520this%2520form%2520of%2520universality%2520is%2520not%2520unique%2520to%2520GTs%253A%2520Using%250Athe%2520same%2520positional%2520encodings%252C%2520also%2520pure%2520MPGNNs%2520and%2520even%25202-layer%2520MLPs%2520are%250Anon-uniform%2520universal%2520approximators.%2520We%2520then%2520consider%2520uniform%2520expressivity%253A%2520The%250Atarget%2520function%2520is%2520to%2520be%2520approximated%2520by%2520a%2520single%2520network%2520for%2520graphs%2520of%2520all%250Asizes.%2520There%252C%2520we%2520compare%2520GTs%2520to%2520the%2520more%2520efficient%2520MPGNN%2520%252B%2520Virtual%2520Node%250Aarchitecture.%2520The%2520essential%2520difference%2520between%2520the%2520two%2520model%2520definitions%2520is%2520in%250Atheir%2520global%2520computation%2520method%2520--%2520Self-Attention%2520Vs%2520Virtual%2520Node.%2520We%2520prove%250Athat%2520none%2520of%2520the%2520models%2520is%2520a%2520uniform-universal%2520approximator%252C%2520before%2520proving%2520our%250Amain%2520result%253A%2520Neither%2520model%2527s%2520uniform%2520expressivity%2520subsumes%2520the%2520other%2527s.%2520We%250Ademonstrate%2520the%2520theory%2520with%2520experiments%2520on%2520synthetic%2520data.%2520We%2520further%2520augment%250Aour%2520study%2520with%2520real-world%2520datasets%252C%2520observing%2520mixed%2520results%2520which%2520indicate%2520no%250Aclear%2520ranking%2520in%2520practice%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distinguished%20In%20Uniform%3A%20Self%20Attention%20Vs.%20Virtual%20Nodes&entry.906535625=Eran%20Rosenbluth%20and%20Jan%20T%C3%B6nshoff%20and%20Martin%20Ritzert%20and%20Berke%20Kisin%20and%20Martin%20Grohe&entry.1292438233=%20%20Graph%20Transformers%20%28GTs%29%20such%20as%20SAN%20and%20GPS%20are%20graph%20processing%20models%20that%0Acombine%20Message-Passing%20GNNs%20%28MPGNNs%29%20with%20global%20Self-Attention.%20They%20were%0Ashown%20to%20be%20universal%20function%20approximators%2C%20with%20two%20reservations%3A%201.%20The%0Ainitial%20node%20features%20must%20be%20augmented%20with%20certain%20positional%20encodings.%202.%0AThe%20approximation%20is%20non-uniform%3A%20Graphs%20of%20different%20sizes%20may%20require%20a%0Adifferent%20approximating%20network.%0A%20%20We%20first%20clarify%20that%20this%20form%20of%20universality%20is%20not%20unique%20to%20GTs%3A%20Using%0Athe%20same%20positional%20encodings%2C%20also%20pure%20MPGNNs%20and%20even%202-layer%20MLPs%20are%0Anon-uniform%20universal%20approximators.%20We%20then%20consider%20uniform%20expressivity%3A%20The%0Atarget%20function%20is%20to%20be%20approximated%20by%20a%20single%20network%20for%20graphs%20of%20all%0Asizes.%20There%2C%20we%20compare%20GTs%20to%20the%20more%20efficient%20MPGNN%20%2B%20Virtual%20Node%0Aarchitecture.%20The%20essential%20difference%20between%20the%20two%20model%20definitions%20is%20in%0Atheir%20global%20computation%20method%20--%20Self-Attention%20Vs%20Virtual%20Node.%20We%20prove%0Athat%20none%20of%20the%20models%20is%20a%20uniform-universal%20approximator%2C%20before%20proving%20our%0Amain%20result%3A%20Neither%20model%27s%20uniform%20expressivity%20subsumes%20the%20other%27s.%20We%0Ademonstrate%20the%20theory%20with%20experiments%20on%20synthetic%20data.%20We%20further%20augment%0Aour%20study%20with%20real-world%20datasets%2C%20observing%20mixed%20results%20which%20indicate%20no%0Aclear%20ranking%20in%20practice%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11951v1&entry.124074799=Read"},
{"title": "CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly\n  Supervised Audio-Visual Video Parsing", "author": "Faegheh Sardari and Armin Mustafa and Philip J. B. Jackson and Adrian Hilton", "abstract": "  Weakly supervised audio-visual video parsing (AVVP) methods aim to detect\naudible-only, visible-only, and audible-visible events using only video-level\nlabels. Existing approaches tackle this by leveraging unimodal and cross-modal\ncontexts. However, we argue that while cross-modal learning is beneficial for\ndetecting audible-visible events, in the weakly supervised scenario, it\nnegatively impacts unaligned audible or visible events by introducing\nirrelevant modality information. In this paper, we propose CoLeaF, a novel\nlearning framework that optimizes the integration of cross-modal context in the\nembedding space such that the network explicitly learns to combine cross-modal\ninformation for audible-visible events while filtering them out for unaligned\nevents. Additionally, as videos often involve complex class relationships,\nmodelling them improves performance. However, this introduces extra\ncomputational costs into the network. Our framework is designed to leverage\ncross-class relationships during training without incurring additional\ncomputations at inference. Furthermore, we propose new metrics to better\nevaluate a method's capabilities in performing AVVP. Our extensive experiments\ndemonstrate that CoLeaF significantly improves the state-of-the-art results by\nan average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets,\nrespectively.\n", "link": "http://arxiv.org/abs/2405.10690v2", "date": "2024-05-20", "relevancy": 2.0153, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing&body=Title%3A%20CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton%0AAbstract%3A%20%20%20Weakly%20supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20methods%20aim%20to%20detect%0Aaudible-only%2C%20visible-only%2C%20and%20audible-visible%20events%20using%20only%20video-level%0Alabels.%20Existing%20approaches%20tackle%20this%20by%20leveraging%20unimodal%20and%20cross-modal%0Acontexts.%20However%2C%20we%20argue%20that%20while%20cross-modal%20learning%20is%20beneficial%20for%0Adetecting%20audible-visible%20events%2C%20in%20the%20weakly%20supervised%20scenario%2C%20it%0Anegatively%20impacts%20unaligned%20audible%20or%20visible%20events%20by%20introducing%0Airrelevant%20modality%20information.%20In%20this%20paper%2C%20we%20propose%20CoLeaF%2C%20a%20novel%0Alearning%20framework%20that%20optimizes%20the%20integration%20of%20cross-modal%20context%20in%20the%0Aembedding%20space%20such%20that%20the%20network%20explicitly%20learns%20to%20combine%20cross-modal%0Ainformation%20for%20audible-visible%20events%20while%20filtering%20them%20out%20for%20unaligned%0Aevents.%20Additionally%2C%20as%20videos%20often%20involve%20complex%20class%20relationships%2C%0Amodelling%20them%20improves%20performance.%20However%2C%20this%20introduces%20extra%0Acomputational%20costs%20into%20the%20network.%20Our%20framework%20is%20designed%20to%20leverage%0Across-class%20relationships%20during%20training%20without%20incurring%20additional%0Acomputations%20at%20inference.%20Furthermore%2C%20we%20propose%20new%20metrics%20to%20better%0Aevaluate%20a%20method%27s%20capabilities%20in%20performing%20AVVP.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoLeaF%20significantly%20improves%20the%20state-of-the-art%20results%20by%0Aan%20average%20of%201.9%25%20and%202.4%25%20F-score%20on%20the%20LLP%20and%20UnAV-100%20datasets%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoLeaF%253A%2520A%2520Contrastive-Collaborative%2520Learning%2520Framework%2520for%2520Weakly%250A%2520%2520Supervised%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DFaegheh%2520Sardari%2520and%2520Armin%2520Mustafa%2520and%2520Philip%2520J.%2520B.%2520Jackson%2520and%2520Adrian%2520Hilton%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520audio-visual%2520video%2520parsing%2520%2528AVVP%2529%2520methods%2520aim%2520to%2520detect%250Aaudible-only%252C%2520visible-only%252C%2520and%2520audible-visible%2520events%2520using%2520only%2520video-level%250Alabels.%2520Existing%2520approaches%2520tackle%2520this%2520by%2520leveraging%2520unimodal%2520and%2520cross-modal%250Acontexts.%2520However%252C%2520we%2520argue%2520that%2520while%2520cross-modal%2520learning%2520is%2520beneficial%2520for%250Adetecting%2520audible-visible%2520events%252C%2520in%2520the%2520weakly%2520supervised%2520scenario%252C%2520it%250Anegatively%2520impacts%2520unaligned%2520audible%2520or%2520visible%2520events%2520by%2520introducing%250Airrelevant%2520modality%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CoLeaF%252C%2520a%2520novel%250Alearning%2520framework%2520that%2520optimizes%2520the%2520integration%2520of%2520cross-modal%2520context%2520in%2520the%250Aembedding%2520space%2520such%2520that%2520the%2520network%2520explicitly%2520learns%2520to%2520combine%2520cross-modal%250Ainformation%2520for%2520audible-visible%2520events%2520while%2520filtering%2520them%2520out%2520for%2520unaligned%250Aevents.%2520Additionally%252C%2520as%2520videos%2520often%2520involve%2520complex%2520class%2520relationships%252C%250Amodelling%2520them%2520improves%2520performance.%2520However%252C%2520this%2520introduces%2520extra%250Acomputational%2520costs%2520into%2520the%2520network.%2520Our%2520framework%2520is%2520designed%2520to%2520leverage%250Across-class%2520relationships%2520during%2520training%2520without%2520incurring%2520additional%250Acomputations%2520at%2520inference.%2520Furthermore%252C%2520we%2520propose%2520new%2520metrics%2520to%2520better%250Aevaluate%2520a%2520method%2527s%2520capabilities%2520in%2520performing%2520AVVP.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520that%2520CoLeaF%2520significantly%2520improves%2520the%2520state-of-the-art%2520results%2520by%250Aan%2520average%2520of%25201.9%2525%2520and%25202.4%2525%2520F-score%2520on%2520the%2520LLP%2520and%2520UnAV-100%2520datasets%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing&entry.906535625=Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton&entry.1292438233=%20%20Weakly%20supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20methods%20aim%20to%20detect%0Aaudible-only%2C%20visible-only%2C%20and%20audible-visible%20events%20using%20only%20video-level%0Alabels.%20Existing%20approaches%20tackle%20this%20by%20leveraging%20unimodal%20and%20cross-modal%0Acontexts.%20However%2C%20we%20argue%20that%20while%20cross-modal%20learning%20is%20beneficial%20for%0Adetecting%20audible-visible%20events%2C%20in%20the%20weakly%20supervised%20scenario%2C%20it%0Anegatively%20impacts%20unaligned%20audible%20or%20visible%20events%20by%20introducing%0Airrelevant%20modality%20information.%20In%20this%20paper%2C%20we%20propose%20CoLeaF%2C%20a%20novel%0Alearning%20framework%20that%20optimizes%20the%20integration%20of%20cross-modal%20context%20in%20the%0Aembedding%20space%20such%20that%20the%20network%20explicitly%20learns%20to%20combine%20cross-modal%0Ainformation%20for%20audible-visible%20events%20while%20filtering%20them%20out%20for%20unaligned%0Aevents.%20Additionally%2C%20as%20videos%20often%20involve%20complex%20class%20relationships%2C%0Amodelling%20them%20improves%20performance.%20However%2C%20this%20introduces%20extra%0Acomputational%20costs%20into%20the%20network.%20Our%20framework%20is%20designed%20to%20leverage%0Across-class%20relationships%20during%20training%20without%20incurring%20additional%0Acomputations%20at%20inference.%20Furthermore%2C%20we%20propose%20new%20metrics%20to%20better%0Aevaluate%20a%20method%27s%20capabilities%20in%20performing%20AVVP.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoLeaF%20significantly%20improves%20the%20state-of-the-art%20results%20by%0Aan%20average%20of%201.9%25%20and%202.4%25%20F-score%20on%20the%20LLP%20and%20UnAV-100%20datasets%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10690v2&entry.124074799=Read"},
{"title": "Parallelization of the K-Means Algorithm with Applications to Big Data\n  Clustering", "author": "Ashish Srivastava and Mohammed Nawfal", "abstract": "  The K-Means clustering using LLoyd's algorithm is an iterative approach to\npartition the given dataset into K different clusters. The algorithm assigns\neach point to the cluster based on the following objective function\n  \\[\\ \\min \\Sigma_{i=1}^{n}||x_i-\\mu_{x_i}||^2\\] The serial algorithm involves\niterative steps where we compute the distance of each datapoint from the\ncentroids and assign the datapoint to the nearest centroid. This approach is\nessentially known as the expectation-maximization step. Clustering involves\nextensive computations to calculate distances at each iteration, which\nincreases as the number of data points increases. This provides scope for\nparallelism. However, we must ensure that in a parallel process, each thread\nhas access to the updated centroid value and no racing condition exists on any\ncentroid values. We will compare two different approaches in this project. The\nfirst approach is an OpenMP flat synchronous method where all processes are run\nin parallel, and we use synchronization to ensure safe updates of clusters. The\nsecond approach we adopt is a GPU based parallelization approach using OpenACC\nwherein we will try to make use of GPU architecture to parallelize chunks of\nthe algorithm to observe decreased computation time. We will analyze metrics\nsuch as speed up, efficiency,time taken with varying data points, and number of\nprocesses to compare the two approaches and understand the relative performance\nimprovement we can get.\n", "link": "http://arxiv.org/abs/2405.12052v1", "date": "2024-05-20", "relevancy": 2.0147, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.402}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallelization%20of%20the%20K-Means%20Algorithm%20with%20Applications%20to%20Big%20Data%0A%20%20Clustering&body=Title%3A%20Parallelization%20of%20the%20K-Means%20Algorithm%20with%20Applications%20to%20Big%20Data%0A%20%20Clustering%0AAuthor%3A%20Ashish%20Srivastava%20and%20Mohammed%20Nawfal%0AAbstract%3A%20%20%20The%20K-Means%20clustering%20using%20LLoyd%27s%20algorithm%20is%20an%20iterative%20approach%20to%0Apartition%20the%20given%20dataset%20into%20K%20different%20clusters.%20The%20algorithm%20assigns%0Aeach%20point%20to%20the%20cluster%20based%20on%20the%20following%20objective%20function%0A%20%20%5C%5B%5C%20%5Cmin%20%5CSigma_%7Bi%3D1%7D%5E%7Bn%7D%7C%7Cx_i-%5Cmu_%7Bx_i%7D%7C%7C%5E2%5C%5D%20The%20serial%20algorithm%20involves%0Aiterative%20steps%20where%20we%20compute%20the%20distance%20of%20each%20datapoint%20from%20the%0Acentroids%20and%20assign%20the%20datapoint%20to%20the%20nearest%20centroid.%20This%20approach%20is%0Aessentially%20known%20as%20the%20expectation-maximization%20step.%20Clustering%20involves%0Aextensive%20computations%20to%20calculate%20distances%20at%20each%20iteration%2C%20which%0Aincreases%20as%20the%20number%20of%20data%20points%20increases.%20This%20provides%20scope%20for%0Aparallelism.%20However%2C%20we%20must%20ensure%20that%20in%20a%20parallel%20process%2C%20each%20thread%0Ahas%20access%20to%20the%20updated%20centroid%20value%20and%20no%20racing%20condition%20exists%20on%20any%0Acentroid%20values.%20We%20will%20compare%20two%20different%20approaches%20in%20this%20project.%20The%0Afirst%20approach%20is%20an%20OpenMP%20flat%20synchronous%20method%20where%20all%20processes%20are%20run%0Ain%20parallel%2C%20and%20we%20use%20synchronization%20to%20ensure%20safe%20updates%20of%20clusters.%20The%0Asecond%20approach%20we%20adopt%20is%20a%20GPU%20based%20parallelization%20approach%20using%20OpenACC%0Awherein%20we%20will%20try%20to%20make%20use%20of%20GPU%20architecture%20to%20parallelize%20chunks%20of%0Athe%20algorithm%20to%20observe%20decreased%20computation%20time.%20We%20will%20analyze%20metrics%0Asuch%20as%20speed%20up%2C%20efficiency%2Ctime%20taken%20with%20varying%20data%20points%2C%20and%20number%20of%0Aprocesses%20to%20compare%20the%20two%20approaches%20and%20understand%20the%20relative%20performance%0Aimprovement%20we%20can%20get.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelization%2520of%2520the%2520K-Means%2520Algorithm%2520with%2520Applications%2520to%2520Big%2520Data%250A%2520%2520Clustering%26entry.906535625%3DAshish%2520Srivastava%2520and%2520Mohammed%2520Nawfal%26entry.1292438233%3D%2520%2520The%2520K-Means%2520clustering%2520using%2520LLoyd%2527s%2520algorithm%2520is%2520an%2520iterative%2520approach%2520to%250Apartition%2520the%2520given%2520dataset%2520into%2520K%2520different%2520clusters.%2520The%2520algorithm%2520assigns%250Aeach%2520point%2520to%2520the%2520cluster%2520based%2520on%2520the%2520following%2520objective%2520function%250A%2520%2520%255C%255B%255C%2520%255Cmin%2520%255CSigma_%257Bi%253D1%257D%255E%257Bn%257D%257C%257Cx_i-%255Cmu_%257Bx_i%257D%257C%257C%255E2%255C%255D%2520The%2520serial%2520algorithm%2520involves%250Aiterative%2520steps%2520where%2520we%2520compute%2520the%2520distance%2520of%2520each%2520datapoint%2520from%2520the%250Acentroids%2520and%2520assign%2520the%2520datapoint%2520to%2520the%2520nearest%2520centroid.%2520This%2520approach%2520is%250Aessentially%2520known%2520as%2520the%2520expectation-maximization%2520step.%2520Clustering%2520involves%250Aextensive%2520computations%2520to%2520calculate%2520distances%2520at%2520each%2520iteration%252C%2520which%250Aincreases%2520as%2520the%2520number%2520of%2520data%2520points%2520increases.%2520This%2520provides%2520scope%2520for%250Aparallelism.%2520However%252C%2520we%2520must%2520ensure%2520that%2520in%2520a%2520parallel%2520process%252C%2520each%2520thread%250Ahas%2520access%2520to%2520the%2520updated%2520centroid%2520value%2520and%2520no%2520racing%2520condition%2520exists%2520on%2520any%250Acentroid%2520values.%2520We%2520will%2520compare%2520two%2520different%2520approaches%2520in%2520this%2520project.%2520The%250Afirst%2520approach%2520is%2520an%2520OpenMP%2520flat%2520synchronous%2520method%2520where%2520all%2520processes%2520are%2520run%250Ain%2520parallel%252C%2520and%2520we%2520use%2520synchronization%2520to%2520ensure%2520safe%2520updates%2520of%2520clusters.%2520The%250Asecond%2520approach%2520we%2520adopt%2520is%2520a%2520GPU%2520based%2520parallelization%2520approach%2520using%2520OpenACC%250Awherein%2520we%2520will%2520try%2520to%2520make%2520use%2520of%2520GPU%2520architecture%2520to%2520parallelize%2520chunks%2520of%250Athe%2520algorithm%2520to%2520observe%2520decreased%2520computation%2520time.%2520We%2520will%2520analyze%2520metrics%250Asuch%2520as%2520speed%2520up%252C%2520efficiency%252Ctime%2520taken%2520with%2520varying%2520data%2520points%252C%2520and%2520number%2520of%250Aprocesses%2520to%2520compare%2520the%2520two%2520approaches%2520and%2520understand%2520the%2520relative%2520performance%250Aimprovement%2520we%2520can%2520get.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelization%20of%20the%20K-Means%20Algorithm%20with%20Applications%20to%20Big%20Data%0A%20%20Clustering&entry.906535625=Ashish%20Srivastava%20and%20Mohammed%20Nawfal&entry.1292438233=%20%20The%20K-Means%20clustering%20using%20LLoyd%27s%20algorithm%20is%20an%20iterative%20approach%20to%0Apartition%20the%20given%20dataset%20into%20K%20different%20clusters.%20The%20algorithm%20assigns%0Aeach%20point%20to%20the%20cluster%20based%20on%20the%20following%20objective%20function%0A%20%20%5C%5B%5C%20%5Cmin%20%5CSigma_%7Bi%3D1%7D%5E%7Bn%7D%7C%7Cx_i-%5Cmu_%7Bx_i%7D%7C%7C%5E2%5C%5D%20The%20serial%20algorithm%20involves%0Aiterative%20steps%20where%20we%20compute%20the%20distance%20of%20each%20datapoint%20from%20the%0Acentroids%20and%20assign%20the%20datapoint%20to%20the%20nearest%20centroid.%20This%20approach%20is%0Aessentially%20known%20as%20the%20expectation-maximization%20step.%20Clustering%20involves%0Aextensive%20computations%20to%20calculate%20distances%20at%20each%20iteration%2C%20which%0Aincreases%20as%20the%20number%20of%20data%20points%20increases.%20This%20provides%20scope%20for%0Aparallelism.%20However%2C%20we%20must%20ensure%20that%20in%20a%20parallel%20process%2C%20each%20thread%0Ahas%20access%20to%20the%20updated%20centroid%20value%20and%20no%20racing%20condition%20exists%20on%20any%0Acentroid%20values.%20We%20will%20compare%20two%20different%20approaches%20in%20this%20project.%20The%0Afirst%20approach%20is%20an%20OpenMP%20flat%20synchronous%20method%20where%20all%20processes%20are%20run%0Ain%20parallel%2C%20and%20we%20use%20synchronization%20to%20ensure%20safe%20updates%20of%20clusters.%20The%0Asecond%20approach%20we%20adopt%20is%20a%20GPU%20based%20parallelization%20approach%20using%20OpenACC%0Awherein%20we%20will%20try%20to%20make%20use%20of%20GPU%20architecture%20to%20parallelize%20chunks%20of%0Athe%20algorithm%20to%20observe%20decreased%20computation%20time.%20We%20will%20analyze%20metrics%0Asuch%20as%20speed%20up%2C%20efficiency%2Ctime%20taken%20with%20varying%20data%20points%2C%20and%20number%20of%0Aprocesses%20to%20compare%20the%20two%20approaches%20and%20understand%20the%20relative%20performance%0Aimprovement%20we%20can%20get.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12052v1&entry.124074799=Read"},
{"title": "Stable Attractors for Neural networks classification via Ordinary\n  Differential Equations (SA-nODE)", "author": "Raffaele Marino and Lorenzo Giambagli and Lorenzo Chicchi and Lorenzo Buffoni and Duccio Fanelli", "abstract": "  A novel approach for supervised classification is presented which sits at the\nintersection of machine learning and dynamical systems theory. At variance with\nother methodologies that employ ordinary differential equations for\nclassification purposes, the untrained model is a priori constructed to\naccommodate for a set of pre-assigned stationary stable attractors. Classifying\namounts to steer the dynamics towards one of the planted attractors, depending\non the specificity of the processed item supplied as an input. Asymptotically\nthe system will hence converge on a specific point of the explored\nmulti-dimensional space, flagging the category of the object to be eventually\nclassified. Working in this context, the inherent ability to perform\nclassification, as acquired ex post by the trained model, is ultimately\nreflected in the shaped basin of attractions associated to each of the target\nstable attractors. The performance of the proposed method is here challenged\nagainst simple toy models crafted for the purpose, as well as by resorting to\nwell established reference standards. Although this method does not reach the\nperformance of state-of-the-art deep learning algorithms, it illustrates that\ncontinuous dynamical systems with closed analytical interaction terms can serve\nas high-performance classifiers.\n", "link": "http://arxiv.org/abs/2311.10387v2", "date": "2024-05-20", "relevancy": 2.0147, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Attractors%20for%20Neural%20networks%20classification%20via%20Ordinary%0A%20%20Differential%20Equations%20%28SA-nODE%29&body=Title%3A%20Stable%20Attractors%20for%20Neural%20networks%20classification%20via%20Ordinary%0A%20%20Differential%20Equations%20%28SA-nODE%29%0AAuthor%3A%20Raffaele%20Marino%20and%20Lorenzo%20Giambagli%20and%20Lorenzo%20Chicchi%20and%20Lorenzo%20Buffoni%20and%20Duccio%20Fanelli%0AAbstract%3A%20%20%20A%20novel%20approach%20for%20supervised%20classification%20is%20presented%20which%20sits%20at%20the%0Aintersection%20of%20machine%20learning%20and%20dynamical%20systems%20theory.%20At%20variance%20with%0Aother%20methodologies%20that%20employ%20ordinary%20differential%20equations%20for%0Aclassification%20purposes%2C%20the%20untrained%20model%20is%20a%20priori%20constructed%20to%0Aaccommodate%20for%20a%20set%20of%20pre-assigned%20stationary%20stable%20attractors.%20Classifying%0Aamounts%20to%20steer%20the%20dynamics%20towards%20one%20of%20the%20planted%20attractors%2C%20depending%0Aon%20the%20specificity%20of%20the%20processed%20item%20supplied%20as%20an%20input.%20Asymptotically%0Athe%20system%20will%20hence%20converge%20on%20a%20specific%20point%20of%20the%20explored%0Amulti-dimensional%20space%2C%20flagging%20the%20category%20of%20the%20object%20to%20be%20eventually%0Aclassified.%20Working%20in%20this%20context%2C%20the%20inherent%20ability%20to%20perform%0Aclassification%2C%20as%20acquired%20ex%20post%20by%20the%20trained%20model%2C%20is%20ultimately%0Areflected%20in%20the%20shaped%20basin%20of%20attractions%20associated%20to%20each%20of%20the%20target%0Astable%20attractors.%20The%20performance%20of%20the%20proposed%20method%20is%20here%20challenged%0Aagainst%20simple%20toy%20models%20crafted%20for%20the%20purpose%2C%20as%20well%20as%20by%20resorting%20to%0Awell%20established%20reference%20standards.%20Although%20this%20method%20does%20not%20reach%20the%0Aperformance%20of%20state-of-the-art%20deep%20learning%20algorithms%2C%20it%20illustrates%20that%0Acontinuous%20dynamical%20systems%20with%20closed%20analytical%20interaction%20terms%20can%20serve%0Aas%20high-performance%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Attractors%2520for%2520Neural%2520networks%2520classification%2520via%2520Ordinary%250A%2520%2520Differential%2520Equations%2520%2528SA-nODE%2529%26entry.906535625%3DRaffaele%2520Marino%2520and%2520Lorenzo%2520Giambagli%2520and%2520Lorenzo%2520Chicchi%2520and%2520Lorenzo%2520Buffoni%2520and%2520Duccio%2520Fanelli%26entry.1292438233%3D%2520%2520A%2520novel%2520approach%2520for%2520supervised%2520classification%2520is%2520presented%2520which%2520sits%2520at%2520the%250Aintersection%2520of%2520machine%2520learning%2520and%2520dynamical%2520systems%2520theory.%2520At%2520variance%2520with%250Aother%2520methodologies%2520that%2520employ%2520ordinary%2520differential%2520equations%2520for%250Aclassification%2520purposes%252C%2520the%2520untrained%2520model%2520is%2520a%2520priori%2520constructed%2520to%250Aaccommodate%2520for%2520a%2520set%2520of%2520pre-assigned%2520stationary%2520stable%2520attractors.%2520Classifying%250Aamounts%2520to%2520steer%2520the%2520dynamics%2520towards%2520one%2520of%2520the%2520planted%2520attractors%252C%2520depending%250Aon%2520the%2520specificity%2520of%2520the%2520processed%2520item%2520supplied%2520as%2520an%2520input.%2520Asymptotically%250Athe%2520system%2520will%2520hence%2520converge%2520on%2520a%2520specific%2520point%2520of%2520the%2520explored%250Amulti-dimensional%2520space%252C%2520flagging%2520the%2520category%2520of%2520the%2520object%2520to%2520be%2520eventually%250Aclassified.%2520Working%2520in%2520this%2520context%252C%2520the%2520inherent%2520ability%2520to%2520perform%250Aclassification%252C%2520as%2520acquired%2520ex%2520post%2520by%2520the%2520trained%2520model%252C%2520is%2520ultimately%250Areflected%2520in%2520the%2520shaped%2520basin%2520of%2520attractions%2520associated%2520to%2520each%2520of%2520the%2520target%250Astable%2520attractors.%2520The%2520performance%2520of%2520the%2520proposed%2520method%2520is%2520here%2520challenged%250Aagainst%2520simple%2520toy%2520models%2520crafted%2520for%2520the%2520purpose%252C%2520as%2520well%2520as%2520by%2520resorting%2520to%250Awell%2520established%2520reference%2520standards.%2520Although%2520this%2520method%2520does%2520not%2520reach%2520the%250Aperformance%2520of%2520state-of-the-art%2520deep%2520learning%2520algorithms%252C%2520it%2520illustrates%2520that%250Acontinuous%2520dynamical%2520systems%2520with%2520closed%2520analytical%2520interaction%2520terms%2520can%2520serve%250Aas%2520high-performance%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Attractors%20for%20Neural%20networks%20classification%20via%20Ordinary%0A%20%20Differential%20Equations%20%28SA-nODE%29&entry.906535625=Raffaele%20Marino%20and%20Lorenzo%20Giambagli%20and%20Lorenzo%20Chicchi%20and%20Lorenzo%20Buffoni%20and%20Duccio%20Fanelli&entry.1292438233=%20%20A%20novel%20approach%20for%20supervised%20classification%20is%20presented%20which%20sits%20at%20the%0Aintersection%20of%20machine%20learning%20and%20dynamical%20systems%20theory.%20At%20variance%20with%0Aother%20methodologies%20that%20employ%20ordinary%20differential%20equations%20for%0Aclassification%20purposes%2C%20the%20untrained%20model%20is%20a%20priori%20constructed%20to%0Aaccommodate%20for%20a%20set%20of%20pre-assigned%20stationary%20stable%20attractors.%20Classifying%0Aamounts%20to%20steer%20the%20dynamics%20towards%20one%20of%20the%20planted%20attractors%2C%20depending%0Aon%20the%20specificity%20of%20the%20processed%20item%20supplied%20as%20an%20input.%20Asymptotically%0Athe%20system%20will%20hence%20converge%20on%20a%20specific%20point%20of%20the%20explored%0Amulti-dimensional%20space%2C%20flagging%20the%20category%20of%20the%20object%20to%20be%20eventually%0Aclassified.%20Working%20in%20this%20context%2C%20the%20inherent%20ability%20to%20perform%0Aclassification%2C%20as%20acquired%20ex%20post%20by%20the%20trained%20model%2C%20is%20ultimately%0Areflected%20in%20the%20shaped%20basin%20of%20attractions%20associated%20to%20each%20of%20the%20target%0Astable%20attractors.%20The%20performance%20of%20the%20proposed%20method%20is%20here%20challenged%0Aagainst%20simple%20toy%20models%20crafted%20for%20the%20purpose%2C%20as%20well%20as%20by%20resorting%20to%0Awell%20established%20reference%20standards.%20Although%20this%20method%20does%20not%20reach%20the%0Aperformance%20of%20state-of-the-art%20deep%20learning%20algorithms%2C%20it%20illustrates%20that%0Acontinuous%20dynamical%20systems%20with%20closed%20analytical%20interaction%20terms%20can%20serve%0Aas%20high-performance%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10387v2&entry.124074799=Read"},
{"title": "Triple-CFN: Restructuring Concept Spaces for Enhancing Abstract\n  Reasoning Process", "author": "Ruizhuo Song and Beiming Yuan", "abstract": "  Abstract reasoning poses significant challenges to artificial intelligence\nalgorithms, demanding a cognitive ability beyond that required for perceptual\ntasks. In this study, we introduce the Cross-Feature Network (CFN), a novel\nframework designed to separately extract concepts and features from images.\nThis framework utilizes the responses of features to concepts as\nrepresentations for reasoning, particularly in addressing the Bongard-Logo\nproblem. By integrating an Expectation-Maximization process between the\nextracted concepts and features within the CFN, we have achieved notable\nresults, albeit with certain limitations. To overcome these limitations, we\npropose the Triple-CFN, an efficient model that maximizes feature extraction\nfrom images and demonstrates effectiveness in both the Bongard-Logo and Raven's\nProgressive Matrices (RPM) problems. Furthermore, we introduce Meta Triple-CFN,\nan advanced version of Triple-CFN, which explicitly constructs a concept space\ntailored for RPM problems. This ensures high accuracy of reasoning and\ninterpretability of the concepts involved. Overall, this work explores\ninnovative network designs for abstract reasoning, thereby advancing the\nfrontiers of machine intelligence.\n", "link": "http://arxiv.org/abs/2403.03190v8", "date": "2024-05-20", "relevancy": 2.009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5015}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triple-CFN%3A%20Restructuring%20Concept%20Spaces%20for%20Enhancing%20Abstract%0A%20%20Reasoning%20Process&body=Title%3A%20Triple-CFN%3A%20Restructuring%20Concept%20Spaces%20for%20Enhancing%20Abstract%0A%20%20Reasoning%20Process%0AAuthor%3A%20Ruizhuo%20Song%20and%20Beiming%20Yuan%0AAbstract%3A%20%20%20Abstract%20reasoning%20poses%20significant%20challenges%20to%20artificial%20intelligence%0Aalgorithms%2C%20demanding%20a%20cognitive%20ability%20beyond%20that%20required%20for%20perceptual%0Atasks.%20In%20this%20study%2C%20we%20introduce%20the%20Cross-Feature%20Network%20%28CFN%29%2C%20a%20novel%0Aframework%20designed%20to%20separately%20extract%20concepts%20and%20features%20from%20images.%0AThis%20framework%20utilizes%20the%20responses%20of%20features%20to%20concepts%20as%0Arepresentations%20for%20reasoning%2C%20particularly%20in%20addressing%20the%20Bongard-Logo%0Aproblem.%20By%20integrating%20an%20Expectation-Maximization%20process%20between%20the%0Aextracted%20concepts%20and%20features%20within%20the%20CFN%2C%20we%20have%20achieved%20notable%0Aresults%2C%20albeit%20with%20certain%20limitations.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20the%20Triple-CFN%2C%20an%20efficient%20model%20that%20maximizes%20feature%20extraction%0Afrom%20images%20and%20demonstrates%20effectiveness%20in%20both%20the%20Bongard-Logo%20and%20Raven%27s%0AProgressive%20Matrices%20%28RPM%29%20problems.%20Furthermore%2C%20we%20introduce%20Meta%20Triple-CFN%2C%0Aan%20advanced%20version%20of%20Triple-CFN%2C%20which%20explicitly%20constructs%20a%20concept%20space%0Atailored%20for%20RPM%20problems.%20This%20ensures%20high%20accuracy%20of%20reasoning%20and%0Ainterpretability%20of%20the%20concepts%20involved.%20Overall%2C%20this%20work%20explores%0Ainnovative%20network%20designs%20for%20abstract%20reasoning%2C%20thereby%20advancing%20the%0Afrontiers%20of%20machine%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03190v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriple-CFN%253A%2520Restructuring%2520Concept%2520Spaces%2520for%2520Enhancing%2520Abstract%250A%2520%2520Reasoning%2520Process%26entry.906535625%3DRuizhuo%2520Song%2520and%2520Beiming%2520Yuan%26entry.1292438233%3D%2520%2520Abstract%2520reasoning%2520poses%2520significant%2520challenges%2520to%2520artificial%2520intelligence%250Aalgorithms%252C%2520demanding%2520a%2520cognitive%2520ability%2520beyond%2520that%2520required%2520for%2520perceptual%250Atasks.%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520Cross-Feature%2520Network%2520%2528CFN%2529%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520separately%2520extract%2520concepts%2520and%2520features%2520from%2520images.%250AThis%2520framework%2520utilizes%2520the%2520responses%2520of%2520features%2520to%2520concepts%2520as%250Arepresentations%2520for%2520reasoning%252C%2520particularly%2520in%2520addressing%2520the%2520Bongard-Logo%250Aproblem.%2520By%2520integrating%2520an%2520Expectation-Maximization%2520process%2520between%2520the%250Aextracted%2520concepts%2520and%2520features%2520within%2520the%2520CFN%252C%2520we%2520have%2520achieved%2520notable%250Aresults%252C%2520albeit%2520with%2520certain%2520limitations.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520the%2520Triple-CFN%252C%2520an%2520efficient%2520model%2520that%2520maximizes%2520feature%2520extraction%250Afrom%2520images%2520and%2520demonstrates%2520effectiveness%2520in%2520both%2520the%2520Bongard-Logo%2520and%2520Raven%2527s%250AProgressive%2520Matrices%2520%2528RPM%2529%2520problems.%2520Furthermore%252C%2520we%2520introduce%2520Meta%2520Triple-CFN%252C%250Aan%2520advanced%2520version%2520of%2520Triple-CFN%252C%2520which%2520explicitly%2520constructs%2520a%2520concept%2520space%250Atailored%2520for%2520RPM%2520problems.%2520This%2520ensures%2520high%2520accuracy%2520of%2520reasoning%2520and%250Ainterpretability%2520of%2520the%2520concepts%2520involved.%2520Overall%252C%2520this%2520work%2520explores%250Ainnovative%2520network%2520designs%2520for%2520abstract%2520reasoning%252C%2520thereby%2520advancing%2520the%250Afrontiers%2520of%2520machine%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03190v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triple-CFN%3A%20Restructuring%20Concept%20Spaces%20for%20Enhancing%20Abstract%0A%20%20Reasoning%20Process&entry.906535625=Ruizhuo%20Song%20and%20Beiming%20Yuan&entry.1292438233=%20%20Abstract%20reasoning%20poses%20significant%20challenges%20to%20artificial%20intelligence%0Aalgorithms%2C%20demanding%20a%20cognitive%20ability%20beyond%20that%20required%20for%20perceptual%0Atasks.%20In%20this%20study%2C%20we%20introduce%20the%20Cross-Feature%20Network%20%28CFN%29%2C%20a%20novel%0Aframework%20designed%20to%20separately%20extract%20concepts%20and%20features%20from%20images.%0AThis%20framework%20utilizes%20the%20responses%20of%20features%20to%20concepts%20as%0Arepresentations%20for%20reasoning%2C%20particularly%20in%20addressing%20the%20Bongard-Logo%0Aproblem.%20By%20integrating%20an%20Expectation-Maximization%20process%20between%20the%0Aextracted%20concepts%20and%20features%20within%20the%20CFN%2C%20we%20have%20achieved%20notable%0Aresults%2C%20albeit%20with%20certain%20limitations.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20the%20Triple-CFN%2C%20an%20efficient%20model%20that%20maximizes%20feature%20extraction%0Afrom%20images%20and%20demonstrates%20effectiveness%20in%20both%20the%20Bongard-Logo%20and%20Raven%27s%0AProgressive%20Matrices%20%28RPM%29%20problems.%20Furthermore%2C%20we%20introduce%20Meta%20Triple-CFN%2C%0Aan%20advanced%20version%20of%20Triple-CFN%2C%20which%20explicitly%20constructs%20a%20concept%20space%0Atailored%20for%20RPM%20problems.%20This%20ensures%20high%20accuracy%20of%20reasoning%20and%0Ainterpretability%20of%20the%20concepts%20involved.%20Overall%2C%20this%20work%20explores%0Ainnovative%20network%20designs%20for%20abstract%20reasoning%2C%20thereby%20advancing%20the%0Afrontiers%20of%20machine%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03190v8&entry.124074799=Read"},
{"title": "Boosting Fair Classifier Generalization through Adaptive Priority\n  Reweighing", "author": "Zhihao Hu and Yiran Xu and Mengnan Du and Jindong Gu and Xinmei Tian and Fengxiang He", "abstract": "  With the increasing penetration of machine learning applications in critical\ndecision-making areas, calls for algorithmic fairness are more prominent.\nAlthough there have been various modalities to improve algorithmic fairness\nthrough learning with fairness constraints, their performance does not\ngeneralize well in the test set. A performance-promising fair algorithm with\nbetter generalizability is needed. This paper proposes a novel adaptive\nreweighing method to eliminate the impact of the distribution shifts between\ntraining and test data on model generalizability. Most previous reweighing\nmethods propose to assign a unified weight for each (sub)group. Rather, our\nmethod granularly models the distance from the sample predictions to the\ndecision boundary. Our adaptive reweighing method prioritizes samples closer to\nthe decision boundary and assigns a higher weight to improve the\ngeneralizability of fair classifiers. Extensive experiments are performed to\nvalidate the generalizability of our adaptive priority reweighing method for\naccuracy and fairness measures (i.e., equal opportunity, equalized odds, and\ndemographic parity) in tabular benchmarks. We also highlight the performance of\nour method in improving the fairness of language and vision models. The code is\navailable at https://github.com/che2198/APW.\n", "link": "http://arxiv.org/abs/2309.08375v3", "date": "2024-05-20", "relevancy": 2.0086, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5192}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Fair%20Classifier%20Generalization%20through%20Adaptive%20Priority%0A%20%20Reweighing&body=Title%3A%20Boosting%20Fair%20Classifier%20Generalization%20through%20Adaptive%20Priority%0A%20%20Reweighing%0AAuthor%3A%20Zhihao%20Hu%20and%20Yiran%20Xu%20and%20Mengnan%20Du%20and%20Jindong%20Gu%20and%20Xinmei%20Tian%20and%20Fengxiang%20He%0AAbstract%3A%20%20%20With%20the%20increasing%20penetration%20of%20machine%20learning%20applications%20in%20critical%0Adecision-making%20areas%2C%20calls%20for%20algorithmic%20fairness%20are%20more%20prominent.%0AAlthough%20there%20have%20been%20various%20modalities%20to%20improve%20algorithmic%20fairness%0Athrough%20learning%20with%20fairness%20constraints%2C%20their%20performance%20does%20not%0Ageneralize%20well%20in%20the%20test%20set.%20A%20performance-promising%20fair%20algorithm%20with%0Abetter%20generalizability%20is%20needed.%20This%20paper%20proposes%20a%20novel%20adaptive%0Areweighing%20method%20to%20eliminate%20the%20impact%20of%20the%20distribution%20shifts%20between%0Atraining%20and%20test%20data%20on%20model%20generalizability.%20Most%20previous%20reweighing%0Amethods%20propose%20to%20assign%20a%20unified%20weight%20for%20each%20%28sub%29group.%20Rather%2C%20our%0Amethod%20granularly%20models%20the%20distance%20from%20the%20sample%20predictions%20to%20the%0Adecision%20boundary.%20Our%20adaptive%20reweighing%20method%20prioritizes%20samples%20closer%20to%0Athe%20decision%20boundary%20and%20assigns%20a%20higher%20weight%20to%20improve%20the%0Ageneralizability%20of%20fair%20classifiers.%20Extensive%20experiments%20are%20performed%20to%0Avalidate%20the%20generalizability%20of%20our%20adaptive%20priority%20reweighing%20method%20for%0Aaccuracy%20and%20fairness%20measures%20%28i.e.%2C%20equal%20opportunity%2C%20equalized%20odds%2C%20and%0Ademographic%20parity%29%20in%20tabular%20benchmarks.%20We%20also%20highlight%20the%20performance%20of%0Aour%20method%20in%20improving%20the%20fairness%20of%20language%20and%20vision%20models.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/che2198/APW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08375v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Fair%2520Classifier%2520Generalization%2520through%2520Adaptive%2520Priority%250A%2520%2520Reweighing%26entry.906535625%3DZhihao%2520Hu%2520and%2520Yiran%2520Xu%2520and%2520Mengnan%2520Du%2520and%2520Jindong%2520Gu%2520and%2520Xinmei%2520Tian%2520and%2520Fengxiang%2520He%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520penetration%2520of%2520machine%2520learning%2520applications%2520in%2520critical%250Adecision-making%2520areas%252C%2520calls%2520for%2520algorithmic%2520fairness%2520are%2520more%2520prominent.%250AAlthough%2520there%2520have%2520been%2520various%2520modalities%2520to%2520improve%2520algorithmic%2520fairness%250Athrough%2520learning%2520with%2520fairness%2520constraints%252C%2520their%2520performance%2520does%2520not%250Ageneralize%2520well%2520in%2520the%2520test%2520set.%2520A%2520performance-promising%2520fair%2520algorithm%2520with%250Abetter%2520generalizability%2520is%2520needed.%2520This%2520paper%2520proposes%2520a%2520novel%2520adaptive%250Areweighing%2520method%2520to%2520eliminate%2520the%2520impact%2520of%2520the%2520distribution%2520shifts%2520between%250Atraining%2520and%2520test%2520data%2520on%2520model%2520generalizability.%2520Most%2520previous%2520reweighing%250Amethods%2520propose%2520to%2520assign%2520a%2520unified%2520weight%2520for%2520each%2520%2528sub%2529group.%2520Rather%252C%2520our%250Amethod%2520granularly%2520models%2520the%2520distance%2520from%2520the%2520sample%2520predictions%2520to%2520the%250Adecision%2520boundary.%2520Our%2520adaptive%2520reweighing%2520method%2520prioritizes%2520samples%2520closer%2520to%250Athe%2520decision%2520boundary%2520and%2520assigns%2520a%2520higher%2520weight%2520to%2520improve%2520the%250Ageneralizability%2520of%2520fair%2520classifiers.%2520Extensive%2520experiments%2520are%2520performed%2520to%250Avalidate%2520the%2520generalizability%2520of%2520our%2520adaptive%2520priority%2520reweighing%2520method%2520for%250Aaccuracy%2520and%2520fairness%2520measures%2520%2528i.e.%252C%2520equal%2520opportunity%252C%2520equalized%2520odds%252C%2520and%250Ademographic%2520parity%2529%2520in%2520tabular%2520benchmarks.%2520We%2520also%2520highlight%2520the%2520performance%2520of%250Aour%2520method%2520in%2520improving%2520the%2520fairness%2520of%2520language%2520and%2520vision%2520models.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/che2198/APW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08375v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Fair%20Classifier%20Generalization%20through%20Adaptive%20Priority%0A%20%20Reweighing&entry.906535625=Zhihao%20Hu%20and%20Yiran%20Xu%20and%20Mengnan%20Du%20and%20Jindong%20Gu%20and%20Xinmei%20Tian%20and%20Fengxiang%20He&entry.1292438233=%20%20With%20the%20increasing%20penetration%20of%20machine%20learning%20applications%20in%20critical%0Adecision-making%20areas%2C%20calls%20for%20algorithmic%20fairness%20are%20more%20prominent.%0AAlthough%20there%20have%20been%20various%20modalities%20to%20improve%20algorithmic%20fairness%0Athrough%20learning%20with%20fairness%20constraints%2C%20their%20performance%20does%20not%0Ageneralize%20well%20in%20the%20test%20set.%20A%20performance-promising%20fair%20algorithm%20with%0Abetter%20generalizability%20is%20needed.%20This%20paper%20proposes%20a%20novel%20adaptive%0Areweighing%20method%20to%20eliminate%20the%20impact%20of%20the%20distribution%20shifts%20between%0Atraining%20and%20test%20data%20on%20model%20generalizability.%20Most%20previous%20reweighing%0Amethods%20propose%20to%20assign%20a%20unified%20weight%20for%20each%20%28sub%29group.%20Rather%2C%20our%0Amethod%20granularly%20models%20the%20distance%20from%20the%20sample%20predictions%20to%20the%0Adecision%20boundary.%20Our%20adaptive%20reweighing%20method%20prioritizes%20samples%20closer%20to%0Athe%20decision%20boundary%20and%20assigns%20a%20higher%20weight%20to%20improve%20the%0Ageneralizability%20of%20fair%20classifiers.%20Extensive%20experiments%20are%20performed%20to%0Avalidate%20the%20generalizability%20of%20our%20adaptive%20priority%20reweighing%20method%20for%0Aaccuracy%20and%20fairness%20measures%20%28i.e.%2C%20equal%20opportunity%2C%20equalized%20odds%2C%20and%0Ademographic%20parity%29%20in%20tabular%20benchmarks.%20We%20also%20highlight%20the%20performance%20of%0Aour%20method%20in%20improving%20the%20fairness%20of%20language%20and%20vision%20models.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/che2198/APW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08375v3&entry.124074799=Read"},
{"title": "Adaptive Extraction Network for Multivariate Long Sequence Time-Series\n  Forecasting", "author": "Dandan Zhang and Yun Wang", "abstract": "  Models employing CNN architecture have made significant progress in\nmultivariate long sequence time-series forecasting (MLSTF), particularly in\nmodeling local time series characteristics. However, during the MLSTF process,\nextracting the global time series patterns and understanding the correlations\namong different variables are highly significant. To address this challenge, we\nintroduce multi-resolution convolution and deformable convolution operations.\nBy enlarging the receptive field using convolution kernels with different\ndilation factors to capture temporal correlation information across different\nresolutions, and adaptively adjusting the sampling positions through additional\noffset vectors, we enhance the network's ability to capture correlated features\nbetween variables. Building upon this, we propose ATVCNet, an adaptive\ntemporal-variable convolutional network designed to effectively model the\nlocal/global temporal dependencies and inter-variable dependencies of\nmultivariate time series. Specifically, extracting and fusing time series\nfeatures at different resolutions, captures both local contextual information\nand global patterns in the time series. The designed inter-variable feature\nadaptive extraction module captures the correlation among different variables\nin the time series. We evaluated the performance of ATVCNet across eight\nreal-world datasets. The results indicate that ATVCNet achieved a performance\nimprovement of approximately 63.4% over state-of-the-art MLSTF models.\n", "link": "http://arxiv.org/abs/2405.12038v1", "date": "2024-05-20", "relevancy": 2.0077, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5119}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Extraction%20Network%20for%20Multivariate%20Long%20Sequence%20Time-Series%0A%20%20Forecasting&body=Title%3A%20Adaptive%20Extraction%20Network%20for%20Multivariate%20Long%20Sequence%20Time-Series%0A%20%20Forecasting%0AAuthor%3A%20Dandan%20Zhang%20and%20Yun%20Wang%0AAbstract%3A%20%20%20Models%20employing%20CNN%20architecture%20have%20made%20significant%20progress%20in%0Amultivariate%20long%20sequence%20time-series%20forecasting%20%28MLSTF%29%2C%20particularly%20in%0Amodeling%20local%20time%20series%20characteristics.%20However%2C%20during%20the%20MLSTF%20process%2C%0Aextracting%20the%20global%20time%20series%20patterns%20and%20understanding%20the%20correlations%0Aamong%20different%20variables%20are%20highly%20significant.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20multi-resolution%20convolution%20and%20deformable%20convolution%20operations.%0ABy%20enlarging%20the%20receptive%20field%20using%20convolution%20kernels%20with%20different%0Adilation%20factors%20to%20capture%20temporal%20correlation%20information%20across%20different%0Aresolutions%2C%20and%20adaptively%20adjusting%20the%20sampling%20positions%20through%20additional%0Aoffset%20vectors%2C%20we%20enhance%20the%20network%27s%20ability%20to%20capture%20correlated%20features%0Abetween%20variables.%20Building%20upon%20this%2C%20we%20propose%20ATVCNet%2C%20an%20adaptive%0Atemporal-variable%20convolutional%20network%20designed%20to%20effectively%20model%20the%0Alocal/global%20temporal%20dependencies%20and%20inter-variable%20dependencies%20of%0Amultivariate%20time%20series.%20Specifically%2C%20extracting%20and%20fusing%20time%20series%0Afeatures%20at%20different%20resolutions%2C%20captures%20both%20local%20contextual%20information%0Aand%20global%20patterns%20in%20the%20time%20series.%20The%20designed%20inter-variable%20feature%0Aadaptive%20extraction%20module%20captures%20the%20correlation%20among%20different%20variables%0Ain%20the%20time%20series.%20We%20evaluated%20the%20performance%20of%20ATVCNet%20across%20eight%0Areal-world%20datasets.%20The%20results%20indicate%20that%20ATVCNet%20achieved%20a%20performance%0Aimprovement%20of%20approximately%2063.4%25%20over%20state-of-the-art%20MLSTF%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Extraction%2520Network%2520for%2520Multivariate%2520Long%2520Sequence%2520Time-Series%250A%2520%2520Forecasting%26entry.906535625%3DDandan%2520Zhang%2520and%2520Yun%2520Wang%26entry.1292438233%3D%2520%2520Models%2520employing%2520CNN%2520architecture%2520have%2520made%2520significant%2520progress%2520in%250Amultivariate%2520long%2520sequence%2520time-series%2520forecasting%2520%2528MLSTF%2529%252C%2520particularly%2520in%250Amodeling%2520local%2520time%2520series%2520characteristics.%2520However%252C%2520during%2520the%2520MLSTF%2520process%252C%250Aextracting%2520the%2520global%2520time%2520series%2520patterns%2520and%2520understanding%2520the%2520correlations%250Aamong%2520different%2520variables%2520are%2520highly%2520significant.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520multi-resolution%2520convolution%2520and%2520deformable%2520convolution%2520operations.%250ABy%2520enlarging%2520the%2520receptive%2520field%2520using%2520convolution%2520kernels%2520with%2520different%250Adilation%2520factors%2520to%2520capture%2520temporal%2520correlation%2520information%2520across%2520different%250Aresolutions%252C%2520and%2520adaptively%2520adjusting%2520the%2520sampling%2520positions%2520through%2520additional%250Aoffset%2520vectors%252C%2520we%2520enhance%2520the%2520network%2527s%2520ability%2520to%2520capture%2520correlated%2520features%250Abetween%2520variables.%2520Building%2520upon%2520this%252C%2520we%2520propose%2520ATVCNet%252C%2520an%2520adaptive%250Atemporal-variable%2520convolutional%2520network%2520designed%2520to%2520effectively%2520model%2520the%250Alocal/global%2520temporal%2520dependencies%2520and%2520inter-variable%2520dependencies%2520of%250Amultivariate%2520time%2520series.%2520Specifically%252C%2520extracting%2520and%2520fusing%2520time%2520series%250Afeatures%2520at%2520different%2520resolutions%252C%2520captures%2520both%2520local%2520contextual%2520information%250Aand%2520global%2520patterns%2520in%2520the%2520time%2520series.%2520The%2520designed%2520inter-variable%2520feature%250Aadaptive%2520extraction%2520module%2520captures%2520the%2520correlation%2520among%2520different%2520variables%250Ain%2520the%2520time%2520series.%2520We%2520evaluated%2520the%2520performance%2520of%2520ATVCNet%2520across%2520eight%250Areal-world%2520datasets.%2520The%2520results%2520indicate%2520that%2520ATVCNet%2520achieved%2520a%2520performance%250Aimprovement%2520of%2520approximately%252063.4%2525%2520over%2520state-of-the-art%2520MLSTF%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Extraction%20Network%20for%20Multivariate%20Long%20Sequence%20Time-Series%0A%20%20Forecasting&entry.906535625=Dandan%20Zhang%20and%20Yun%20Wang&entry.1292438233=%20%20Models%20employing%20CNN%20architecture%20have%20made%20significant%20progress%20in%0Amultivariate%20long%20sequence%20time-series%20forecasting%20%28MLSTF%29%2C%20particularly%20in%0Amodeling%20local%20time%20series%20characteristics.%20However%2C%20during%20the%20MLSTF%20process%2C%0Aextracting%20the%20global%20time%20series%20patterns%20and%20understanding%20the%20correlations%0Aamong%20different%20variables%20are%20highly%20significant.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20multi-resolution%20convolution%20and%20deformable%20convolution%20operations.%0ABy%20enlarging%20the%20receptive%20field%20using%20convolution%20kernels%20with%20different%0Adilation%20factors%20to%20capture%20temporal%20correlation%20information%20across%20different%0Aresolutions%2C%20and%20adaptively%20adjusting%20the%20sampling%20positions%20through%20additional%0Aoffset%20vectors%2C%20we%20enhance%20the%20network%27s%20ability%20to%20capture%20correlated%20features%0Abetween%20variables.%20Building%20upon%20this%2C%20we%20propose%20ATVCNet%2C%20an%20adaptive%0Atemporal-variable%20convolutional%20network%20designed%20to%20effectively%20model%20the%0Alocal/global%20temporal%20dependencies%20and%20inter-variable%20dependencies%20of%0Amultivariate%20time%20series.%20Specifically%2C%20extracting%20and%20fusing%20time%20series%0Afeatures%20at%20different%20resolutions%2C%20captures%20both%20local%20contextual%20information%0Aand%20global%20patterns%20in%20the%20time%20series.%20The%20designed%20inter-variable%20feature%0Aadaptive%20extraction%20module%20captures%20the%20correlation%20among%20different%20variables%0Ain%20the%20time%20series.%20We%20evaluated%20the%20performance%20of%20ATVCNet%20across%20eight%0Areal-world%20datasets.%20The%20results%20indicate%20that%20ATVCNet%20achieved%20a%20performance%0Aimprovement%20of%20approximately%2063.4%25%20over%20state-of-the-art%20MLSTF%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12038v1&entry.124074799=Read"},
{"title": "Few Shot Semantic Segmentation: a review of methodologies, benchmarks,\n  and open challenges", "author": "Nico Catalano and Matteo Matteucci", "abstract": "  Semantic segmentation, vital for applications ranging from autonomous driving\nto robotics, faces significant challenges in domains where collecting large\nannotated datasets is difficult or prohibitively expensive. In such contexts,\nsuch as medicine and agriculture, the scarcity of training images hampers\nprogress.\n  Introducing Few-Shot Semantic Segmentation, a novel task in computer vision,\nwhich aims at designing models capable of segmenting new semantic classes with\nonly a few examples. This paper consists of a comprehensive survey of Few-Shot\nSemantic Segmentation, tracing its evolution and exploring various model\ndesigns, from the more popular conditional and prototypical networks to the\nmore niche latent space optimization methods, presenting also the new\nopportunities offered by recent foundational models. Through a chronological\nnarrative, we dissect influential trends and methodologies, providing insights\ninto their strengths and limitations. A temporal timeline offers a visual\nroadmap, marking key milestones in the field's progression.\n  Complemented by quantitative analyses on benchmark datasets and qualitative\nshowcases of seminal works, this survey equips readers with a deep\nunderstanding of the topic. By elucidating current challenges, state-of-the-art\nmodels, and prospects, we aid researchers and practitioners in navigating the\nintricacies of Few-Shot Semantic Segmentation and provide ground for future\ndevelopment.\n", "link": "http://arxiv.org/abs/2304.05832v2", "date": "2024-05-20", "relevancy": 1.9879, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5024}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few%20Shot%20Semantic%20Segmentation%3A%20a%20review%20of%20methodologies%2C%20benchmarks%2C%0A%20%20and%20open%20challenges&body=Title%3A%20Few%20Shot%20Semantic%20Segmentation%3A%20a%20review%20of%20methodologies%2C%20benchmarks%2C%0A%20%20and%20open%20challenges%0AAuthor%3A%20Nico%20Catalano%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20Semantic%20segmentation%2C%20vital%20for%20applications%20ranging%20from%20autonomous%20driving%0Ato%20robotics%2C%20faces%20significant%20challenges%20in%20domains%20where%20collecting%20large%0Aannotated%20datasets%20is%20difficult%20or%20prohibitively%20expensive.%20In%20such%20contexts%2C%0Asuch%20as%20medicine%20and%20agriculture%2C%20the%20scarcity%20of%20training%20images%20hampers%0Aprogress.%0A%20%20Introducing%20Few-Shot%20Semantic%20Segmentation%2C%20a%20novel%20task%20in%20computer%20vision%2C%0Awhich%20aims%20at%20designing%20models%20capable%20of%20segmenting%20new%20semantic%20classes%20with%0Aonly%20a%20few%20examples.%20This%20paper%20consists%20of%20a%20comprehensive%20survey%20of%20Few-Shot%0ASemantic%20Segmentation%2C%20tracing%20its%20evolution%20and%20exploring%20various%20model%0Adesigns%2C%20from%20the%20more%20popular%20conditional%20and%20prototypical%20networks%20to%20the%0Amore%20niche%20latent%20space%20optimization%20methods%2C%20presenting%20also%20the%20new%0Aopportunities%20offered%20by%20recent%20foundational%20models.%20Through%20a%20chronological%0Anarrative%2C%20we%20dissect%20influential%20trends%20and%20methodologies%2C%20providing%20insights%0Ainto%20their%20strengths%20and%20limitations.%20A%20temporal%20timeline%20offers%20a%20visual%0Aroadmap%2C%20marking%20key%20milestones%20in%20the%20field%27s%20progression.%0A%20%20Complemented%20by%20quantitative%20analyses%20on%20benchmark%20datasets%20and%20qualitative%0Ashowcases%20of%20seminal%20works%2C%20this%20survey%20equips%20readers%20with%20a%20deep%0Aunderstanding%20of%20the%20topic.%20By%20elucidating%20current%20challenges%2C%20state-of-the-art%0Amodels%2C%20and%20prospects%2C%20we%20aid%20researchers%20and%20practitioners%20in%20navigating%20the%0Aintricacies%20of%20Few-Shot%20Semantic%20Segmentation%20and%20provide%20ground%20for%20future%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.05832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew%2520Shot%2520Semantic%2520Segmentation%253A%2520a%2520review%2520of%2520methodologies%252C%2520benchmarks%252C%250A%2520%2520and%2520open%2520challenges%26entry.906535625%3DNico%2520Catalano%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%252C%2520vital%2520for%2520applications%2520ranging%2520from%2520autonomous%2520driving%250Ato%2520robotics%252C%2520faces%2520significant%2520challenges%2520in%2520domains%2520where%2520collecting%2520large%250Aannotated%2520datasets%2520is%2520difficult%2520or%2520prohibitively%2520expensive.%2520In%2520such%2520contexts%252C%250Asuch%2520as%2520medicine%2520and%2520agriculture%252C%2520the%2520scarcity%2520of%2520training%2520images%2520hampers%250Aprogress.%250A%2520%2520Introducing%2520Few-Shot%2520Semantic%2520Segmentation%252C%2520a%2520novel%2520task%2520in%2520computer%2520vision%252C%250Awhich%2520aims%2520at%2520designing%2520models%2520capable%2520of%2520segmenting%2520new%2520semantic%2520classes%2520with%250Aonly%2520a%2520few%2520examples.%2520This%2520paper%2520consists%2520of%2520a%2520comprehensive%2520survey%2520of%2520Few-Shot%250ASemantic%2520Segmentation%252C%2520tracing%2520its%2520evolution%2520and%2520exploring%2520various%2520model%250Adesigns%252C%2520from%2520the%2520more%2520popular%2520conditional%2520and%2520prototypical%2520networks%2520to%2520the%250Amore%2520niche%2520latent%2520space%2520optimization%2520methods%252C%2520presenting%2520also%2520the%2520new%250Aopportunities%2520offered%2520by%2520recent%2520foundational%2520models.%2520Through%2520a%2520chronological%250Anarrative%252C%2520we%2520dissect%2520influential%2520trends%2520and%2520methodologies%252C%2520providing%2520insights%250Ainto%2520their%2520strengths%2520and%2520limitations.%2520A%2520temporal%2520timeline%2520offers%2520a%2520visual%250Aroadmap%252C%2520marking%2520key%2520milestones%2520in%2520the%2520field%2527s%2520progression.%250A%2520%2520Complemented%2520by%2520quantitative%2520analyses%2520on%2520benchmark%2520datasets%2520and%2520qualitative%250Ashowcases%2520of%2520seminal%2520works%252C%2520this%2520survey%2520equips%2520readers%2520with%2520a%2520deep%250Aunderstanding%2520of%2520the%2520topic.%2520By%2520elucidating%2520current%2520challenges%252C%2520state-of-the-art%250Amodels%252C%2520and%2520prospects%252C%2520we%2520aid%2520researchers%2520and%2520practitioners%2520in%2520navigating%2520the%250Aintricacies%2520of%2520Few-Shot%2520Semantic%2520Segmentation%2520and%2520provide%2520ground%2520for%2520future%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.05832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few%20Shot%20Semantic%20Segmentation%3A%20a%20review%20of%20methodologies%2C%20benchmarks%2C%0A%20%20and%20open%20challenges&entry.906535625=Nico%20Catalano%20and%20Matteo%20Matteucci&entry.1292438233=%20%20Semantic%20segmentation%2C%20vital%20for%20applications%20ranging%20from%20autonomous%20driving%0Ato%20robotics%2C%20faces%20significant%20challenges%20in%20domains%20where%20collecting%20large%0Aannotated%20datasets%20is%20difficult%20or%20prohibitively%20expensive.%20In%20such%20contexts%2C%0Asuch%20as%20medicine%20and%20agriculture%2C%20the%20scarcity%20of%20training%20images%20hampers%0Aprogress.%0A%20%20Introducing%20Few-Shot%20Semantic%20Segmentation%2C%20a%20novel%20task%20in%20computer%20vision%2C%0Awhich%20aims%20at%20designing%20models%20capable%20of%20segmenting%20new%20semantic%20classes%20with%0Aonly%20a%20few%20examples.%20This%20paper%20consists%20of%20a%20comprehensive%20survey%20of%20Few-Shot%0ASemantic%20Segmentation%2C%20tracing%20its%20evolution%20and%20exploring%20various%20model%0Adesigns%2C%20from%20the%20more%20popular%20conditional%20and%20prototypical%20networks%20to%20the%0Amore%20niche%20latent%20space%20optimization%20methods%2C%20presenting%20also%20the%20new%0Aopportunities%20offered%20by%20recent%20foundational%20models.%20Through%20a%20chronological%0Anarrative%2C%20we%20dissect%20influential%20trends%20and%20methodologies%2C%20providing%20insights%0Ainto%20their%20strengths%20and%20limitations.%20A%20temporal%20timeline%20offers%20a%20visual%0Aroadmap%2C%20marking%20key%20milestones%20in%20the%20field%27s%20progression.%0A%20%20Complemented%20by%20quantitative%20analyses%20on%20benchmark%20datasets%20and%20qualitative%0Ashowcases%20of%20seminal%20works%2C%20this%20survey%20equips%20readers%20with%20a%20deep%0Aunderstanding%20of%20the%20topic.%20By%20elucidating%20current%20challenges%2C%20state-of-the-art%0Amodels%2C%20and%20prospects%2C%20we%20aid%20researchers%20and%20practitioners%20in%20navigating%20the%0Aintricacies%20of%20Few-Shot%20Semantic%20Segmentation%20and%20provide%20ground%20for%20future%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05832v2&entry.124074799=Read"},
{"title": "A Framework for Inference Inspired by Human Memory Mechanisms", "author": "Xiangyu Zeng and Jie Lin and Piao Hu and Ruizheng Huang and Zhicheng Zhang", "abstract": "  How humans and machines make sense of current inputs for relation reasoning\nand question-answering while putting the perceived information into context of\nour past memories, has been a challenging conundrum in cognitive science and\nartificial intelligence. Inspired by human brain's memory system and cognitive\narchitectures, we propose a PMI framework that consists of perception, memory\nand inference components. Notably, the memory module comprises working and\nlong-term memory, with the latter endowed with a higher-order structure to\nretain extensive and complex relational knowledge and experience. Through a\ndifferentiable competitive write access, current perceptions update working\nmemory, which is later merged with long-term memory via outer product\nassociations, reducing information conflicts and averting memory overflow. In\nthe inference module, relevant information is retrieved from two separate\nmemory origins and associatively integrated to attain a more comprehensive and\nprecise interpretation of current perceptions. We exploratively apply our PMI\nto improve prevailing Transformers and CNN models on question-answering tasks\nlike bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateral\ntriangles, language modeling and image classification tasks, and in each case,\nour PMI enhancements consistently outshine their original counterparts\nsignificantly. Visualization analyses reveal that relational memory\nconsolidation, along with the interaction and integration of information from\ndiverse memory sources, substantially contributes to the model effectiveness on\ninference tasks.\n", "link": "http://arxiv.org/abs/2310.09297v2", "date": "2024-05-20", "relevancy": 1.9816, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5259}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Inference%20Inspired%20by%20Human%20Memory%20Mechanisms&body=Title%3A%20A%20Framework%20for%20Inference%20Inspired%20by%20Human%20Memory%20Mechanisms%0AAuthor%3A%20Xiangyu%20Zeng%20and%20Jie%20Lin%20and%20Piao%20Hu%20and%20Ruizheng%20Huang%20and%20Zhicheng%20Zhang%0AAbstract%3A%20%20%20How%20humans%20and%20machines%20make%20sense%20of%20current%20inputs%20for%20relation%20reasoning%0Aand%20question-answering%20while%20putting%20the%20perceived%20information%20into%20context%20of%0Aour%20past%20memories%2C%20has%20been%20a%20challenging%20conundrum%20in%20cognitive%20science%20and%0Aartificial%20intelligence.%20Inspired%20by%20human%20brain%27s%20memory%20system%20and%20cognitive%0Aarchitectures%2C%20we%20propose%20a%20PMI%20framework%20that%20consists%20of%20perception%2C%20memory%0Aand%20inference%20components.%20Notably%2C%20the%20memory%20module%20comprises%20working%20and%0Along-term%20memory%2C%20with%20the%20latter%20endowed%20with%20a%20higher-order%20structure%20to%0Aretain%20extensive%20and%20complex%20relational%20knowledge%20and%20experience.%20Through%20a%0Adifferentiable%20competitive%20write%20access%2C%20current%20perceptions%20update%20working%0Amemory%2C%20which%20is%20later%20merged%20with%20long-term%20memory%20via%20outer%20product%0Aassociations%2C%20reducing%20information%20conflicts%20and%20averting%20memory%20overflow.%20In%0Athe%20inference%20module%2C%20relevant%20information%20is%20retrieved%20from%20two%20separate%0Amemory%20origins%20and%20associatively%20integrated%20to%20attain%20a%20more%20comprehensive%20and%0Aprecise%20interpretation%20of%20current%20perceptions.%20We%20exploratively%20apply%20our%20PMI%0Ato%20improve%20prevailing%20Transformers%20and%20CNN%20models%20on%20question-answering%20tasks%0Alike%20bAbI-20k%20and%20Sort-of-CLEVR%20datasets%2C%20as%20well%20as%20detecting%20equilateral%0Atriangles%2C%20language%20modeling%20and%20image%20classification%20tasks%2C%20and%20in%20each%20case%2C%0Aour%20PMI%20enhancements%20consistently%20outshine%20their%20original%20counterparts%0Asignificantly.%20Visualization%20analyses%20reveal%20that%20relational%20memory%0Aconsolidation%2C%20along%20with%20the%20interaction%20and%20integration%20of%20information%20from%0Adiverse%20memory%20sources%2C%20substantially%20contributes%20to%20the%20model%20effectiveness%20on%0Ainference%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Inference%2520Inspired%2520by%2520Human%2520Memory%2520Mechanisms%26entry.906535625%3DXiangyu%2520Zeng%2520and%2520Jie%2520Lin%2520and%2520Piao%2520Hu%2520and%2520Ruizheng%2520Huang%2520and%2520Zhicheng%2520Zhang%26entry.1292438233%3D%2520%2520How%2520humans%2520and%2520machines%2520make%2520sense%2520of%2520current%2520inputs%2520for%2520relation%2520reasoning%250Aand%2520question-answering%2520while%2520putting%2520the%2520perceived%2520information%2520into%2520context%2520of%250Aour%2520past%2520memories%252C%2520has%2520been%2520a%2520challenging%2520conundrum%2520in%2520cognitive%2520science%2520and%250Aartificial%2520intelligence.%2520Inspired%2520by%2520human%2520brain%2527s%2520memory%2520system%2520and%2520cognitive%250Aarchitectures%252C%2520we%2520propose%2520a%2520PMI%2520framework%2520that%2520consists%2520of%2520perception%252C%2520memory%250Aand%2520inference%2520components.%2520Notably%252C%2520the%2520memory%2520module%2520comprises%2520working%2520and%250Along-term%2520memory%252C%2520with%2520the%2520latter%2520endowed%2520with%2520a%2520higher-order%2520structure%2520to%250Aretain%2520extensive%2520and%2520complex%2520relational%2520knowledge%2520and%2520experience.%2520Through%2520a%250Adifferentiable%2520competitive%2520write%2520access%252C%2520current%2520perceptions%2520update%2520working%250Amemory%252C%2520which%2520is%2520later%2520merged%2520with%2520long-term%2520memory%2520via%2520outer%2520product%250Aassociations%252C%2520reducing%2520information%2520conflicts%2520and%2520averting%2520memory%2520overflow.%2520In%250Athe%2520inference%2520module%252C%2520relevant%2520information%2520is%2520retrieved%2520from%2520two%2520separate%250Amemory%2520origins%2520and%2520associatively%2520integrated%2520to%2520attain%2520a%2520more%2520comprehensive%2520and%250Aprecise%2520interpretation%2520of%2520current%2520perceptions.%2520We%2520exploratively%2520apply%2520our%2520PMI%250Ato%2520improve%2520prevailing%2520Transformers%2520and%2520CNN%2520models%2520on%2520question-answering%2520tasks%250Alike%2520bAbI-20k%2520and%2520Sort-of-CLEVR%2520datasets%252C%2520as%2520well%2520as%2520detecting%2520equilateral%250Atriangles%252C%2520language%2520modeling%2520and%2520image%2520classification%2520tasks%252C%2520and%2520in%2520each%2520case%252C%250Aour%2520PMI%2520enhancements%2520consistently%2520outshine%2520their%2520original%2520counterparts%250Asignificantly.%2520Visualization%2520analyses%2520reveal%2520that%2520relational%2520memory%250Aconsolidation%252C%2520along%2520with%2520the%2520interaction%2520and%2520integration%2520of%2520information%2520from%250Adiverse%2520memory%2520sources%252C%2520substantially%2520contributes%2520to%2520the%2520model%2520effectiveness%2520on%250Ainference%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Inference%20Inspired%20by%20Human%20Memory%20Mechanisms&entry.906535625=Xiangyu%20Zeng%20and%20Jie%20Lin%20and%20Piao%20Hu%20and%20Ruizheng%20Huang%20and%20Zhicheng%20Zhang&entry.1292438233=%20%20How%20humans%20and%20machines%20make%20sense%20of%20current%20inputs%20for%20relation%20reasoning%0Aand%20question-answering%20while%20putting%20the%20perceived%20information%20into%20context%20of%0Aour%20past%20memories%2C%20has%20been%20a%20challenging%20conundrum%20in%20cognitive%20science%20and%0Aartificial%20intelligence.%20Inspired%20by%20human%20brain%27s%20memory%20system%20and%20cognitive%0Aarchitectures%2C%20we%20propose%20a%20PMI%20framework%20that%20consists%20of%20perception%2C%20memory%0Aand%20inference%20components.%20Notably%2C%20the%20memory%20module%20comprises%20working%20and%0Along-term%20memory%2C%20with%20the%20latter%20endowed%20with%20a%20higher-order%20structure%20to%0Aretain%20extensive%20and%20complex%20relational%20knowledge%20and%20experience.%20Through%20a%0Adifferentiable%20competitive%20write%20access%2C%20current%20perceptions%20update%20working%0Amemory%2C%20which%20is%20later%20merged%20with%20long-term%20memory%20via%20outer%20product%0Aassociations%2C%20reducing%20information%20conflicts%20and%20averting%20memory%20overflow.%20In%0Athe%20inference%20module%2C%20relevant%20information%20is%20retrieved%20from%20two%20separate%0Amemory%20origins%20and%20associatively%20integrated%20to%20attain%20a%20more%20comprehensive%20and%0Aprecise%20interpretation%20of%20current%20perceptions.%20We%20exploratively%20apply%20our%20PMI%0Ato%20improve%20prevailing%20Transformers%20and%20CNN%20models%20on%20question-answering%20tasks%0Alike%20bAbI-20k%20and%20Sort-of-CLEVR%20datasets%2C%20as%20well%20as%20detecting%20equilateral%0Atriangles%2C%20language%20modeling%20and%20image%20classification%20tasks%2C%20and%20in%20each%20case%2C%0Aour%20PMI%20enhancements%20consistently%20outshine%20their%20original%20counterparts%0Asignificantly.%20Visualization%20analyses%20reveal%20that%20relational%20memory%0Aconsolidation%2C%20along%20with%20the%20interaction%20and%20integration%20of%20information%20from%0Adiverse%20memory%20sources%2C%20substantially%20contributes%20to%20the%20model%20effectiveness%20on%0Ainference%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09297v2&entry.124074799=Read"},
{"title": "MambaOut: Do We Really Need Mamba for Vision?", "author": "Weihao Yu and Xinchao Wang", "abstract": "  Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named MambaOut through stacking Mamba blocks while\nremoving their core token mixer, SSM. Experimental results strongly support our\nhypotheses. Specifically, our MambaOut model surpasses all visual Mamba models\non ImageNet image classification, indicating that Mamba is indeed unnecessary\nfor this task. As for detection and segmentation, MambaOut cannot match the\nperformance of state-of-the-art visual Mamba models, demonstrating the\npotential of Mamba for long-sequence visual tasks. The code is available at\nhttps://github.com/yuweihao/MambaOut\n", "link": "http://arxiv.org/abs/2405.07992v3", "date": "2024-05-20", "relevancy": 1.9809, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4931}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F&body=Title%3A%20MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F%0AAuthor%3A%20Weihao%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Mamba%2C%20an%20architecture%20with%20RNN-like%20token%20mixer%20of%20state%20space%20model%20%28SSM%29%2C%0Awas%20recently%20introduced%20to%20address%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%20and%20subsequently%20applied%20to%20vision%20tasks.%20Nevertheless%2C%20the%0Aperformance%20of%20Mamba%20for%20vision%20is%20often%20underwhelming%20when%20compared%20with%0Aconvolutional%20and%20attention-based%20models.%20In%20this%20paper%2C%20we%20delve%20into%20the%0Aessence%20of%20Mamba%2C%20and%20conceptually%20conclude%20that%20Mamba%20is%20ideally%20suited%20for%0Atasks%20with%20long-sequence%20and%20autoregressive%20characteristics.%20For%20vision%20tasks%2C%0Aas%20image%20classification%20does%20not%20align%20with%20either%20characteristic%2C%20we%0Ahypothesize%20that%20Mamba%20is%20not%20necessary%20for%20this%20task%3B%20Detection%20and%0Asegmentation%20tasks%20are%20also%20not%20autoregressive%2C%20yet%20they%20adhere%20to%20the%0Along-sequence%20characteristic%2C%20so%20we%20believe%20it%20is%20still%20worthwhile%20to%20explore%0AMamba%27s%20potential%20for%20these%20tasks.%20To%20empirically%20verify%20our%20hypotheses%2C%20we%0Aconstruct%20a%20series%20of%20models%20named%20MambaOut%20through%20stacking%20Mamba%20blocks%20while%0Aremoving%20their%20core%20token%20mixer%2C%20SSM.%20Experimental%20results%20strongly%20support%20our%0Ahypotheses.%20Specifically%2C%20our%20MambaOut%20model%20surpasses%20all%20visual%20Mamba%20models%0Aon%20ImageNet%20image%20classification%2C%20indicating%20that%20Mamba%20is%20indeed%20unnecessary%0Afor%20this%20task.%20As%20for%20detection%20and%20segmentation%2C%20MambaOut%20cannot%20match%20the%0Aperformance%20of%20state-of-the-art%20visual%20Mamba%20models%2C%20demonstrating%20the%0Apotential%20of%20Mamba%20for%20long-sequence%20visual%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yuweihao/MambaOut%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07992v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaOut%253A%2520Do%2520We%2520Really%2520Need%2520Mamba%2520for%2520Vision%253F%26entry.906535625%3DWeihao%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Mamba%252C%2520an%2520architecture%2520with%2520RNN-like%2520token%2520mixer%2520of%2520state%2520space%2520model%2520%2528SSM%2529%252C%250Awas%2520recently%2520introduced%2520to%2520address%2520the%2520quadratic%2520complexity%2520of%2520the%2520attention%250Amechanism%2520and%2520subsequently%2520applied%2520to%2520vision%2520tasks.%2520Nevertheless%252C%2520the%250Aperformance%2520of%2520Mamba%2520for%2520vision%2520is%2520often%2520underwhelming%2520when%2520compared%2520with%250Aconvolutional%2520and%2520attention-based%2520models.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%250Aessence%2520of%2520Mamba%252C%2520and%2520conceptually%2520conclude%2520that%2520Mamba%2520is%2520ideally%2520suited%2520for%250Atasks%2520with%2520long-sequence%2520and%2520autoregressive%2520characteristics.%2520For%2520vision%2520tasks%252C%250Aas%2520image%2520classification%2520does%2520not%2520align%2520with%2520either%2520characteristic%252C%2520we%250Ahypothesize%2520that%2520Mamba%2520is%2520not%2520necessary%2520for%2520this%2520task%253B%2520Detection%2520and%250Asegmentation%2520tasks%2520are%2520also%2520not%2520autoregressive%252C%2520yet%2520they%2520adhere%2520to%2520the%250Along-sequence%2520characteristic%252C%2520so%2520we%2520believe%2520it%2520is%2520still%2520worthwhile%2520to%2520explore%250AMamba%2527s%2520potential%2520for%2520these%2520tasks.%2520To%2520empirically%2520verify%2520our%2520hypotheses%252C%2520we%250Aconstruct%2520a%2520series%2520of%2520models%2520named%2520MambaOut%2520through%2520stacking%2520Mamba%2520blocks%2520while%250Aremoving%2520their%2520core%2520token%2520mixer%252C%2520SSM.%2520Experimental%2520results%2520strongly%2520support%2520our%250Ahypotheses.%2520Specifically%252C%2520our%2520MambaOut%2520model%2520surpasses%2520all%2520visual%2520Mamba%2520models%250Aon%2520ImageNet%2520image%2520classification%252C%2520indicating%2520that%2520Mamba%2520is%2520indeed%2520unnecessary%250Afor%2520this%2520task.%2520As%2520for%2520detection%2520and%2520segmentation%252C%2520MambaOut%2520cannot%2520match%2520the%250Aperformance%2520of%2520state-of-the-art%2520visual%2520Mamba%2520models%252C%2520demonstrating%2520the%250Apotential%2520of%2520Mamba%2520for%2520long-sequence%2520visual%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yuweihao/MambaOut%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07992v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F&entry.906535625=Weihao%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Mamba%2C%20an%20architecture%20with%20RNN-like%20token%20mixer%20of%20state%20space%20model%20%28SSM%29%2C%0Awas%20recently%20introduced%20to%20address%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%20and%20subsequently%20applied%20to%20vision%20tasks.%20Nevertheless%2C%20the%0Aperformance%20of%20Mamba%20for%20vision%20is%20often%20underwhelming%20when%20compared%20with%0Aconvolutional%20and%20attention-based%20models.%20In%20this%20paper%2C%20we%20delve%20into%20the%0Aessence%20of%20Mamba%2C%20and%20conceptually%20conclude%20that%20Mamba%20is%20ideally%20suited%20for%0Atasks%20with%20long-sequence%20and%20autoregressive%20characteristics.%20For%20vision%20tasks%2C%0Aas%20image%20classification%20does%20not%20align%20with%20either%20characteristic%2C%20we%0Ahypothesize%20that%20Mamba%20is%20not%20necessary%20for%20this%20task%3B%20Detection%20and%0Asegmentation%20tasks%20are%20also%20not%20autoregressive%2C%20yet%20they%20adhere%20to%20the%0Along-sequence%20characteristic%2C%20so%20we%20believe%20it%20is%20still%20worthwhile%20to%20explore%0AMamba%27s%20potential%20for%20these%20tasks.%20To%20empirically%20verify%20our%20hypotheses%2C%20we%0Aconstruct%20a%20series%20of%20models%20named%20MambaOut%20through%20stacking%20Mamba%20blocks%20while%0Aremoving%20their%20core%20token%20mixer%2C%20SSM.%20Experimental%20results%20strongly%20support%20our%0Ahypotheses.%20Specifically%2C%20our%20MambaOut%20model%20surpasses%20all%20visual%20Mamba%20models%0Aon%20ImageNet%20image%20classification%2C%20indicating%20that%20Mamba%20is%20indeed%20unnecessary%0Afor%20this%20task.%20As%20for%20detection%20and%20segmentation%2C%20MambaOut%20cannot%20match%20the%0Aperformance%20of%20state-of-the-art%20visual%20Mamba%20models%2C%20demonstrating%20the%0Apotential%20of%20Mamba%20for%20long-sequence%20visual%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yuweihao/MambaOut%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07992v3&entry.124074799=Read"},
{"title": "Building Temporal Kernels with Orthogonal Polynomials", "author": "Yan Ru Pei and Olivier Coenen", "abstract": "  We introduce a class of models named PLEIADES (PoLynomial Expansion In\nAdaptive Distributed Event-based Systems), which contains temporal convolution\nkernels generated from orthogonal polynomial basis functions. We focus on\ninterfacing these networks with event-based data to perform online\nspatiotemporal classification and detection with low latency. By virtue of\nusing structured temporal kernels and event-based data, we have the freedom to\nvary the sample rate of the data along with the discretization step-size of the\nnetwork without additional finetuning. We experimented with three event-based\nbenchmarks and obtained state-of-the-art results on all three by large margins\nwith significantly smaller memory and compute costs. We achieved: 1) 99.59%\naccuracy with 192K parameters on the DVS128 hand gesture recognition dataset\nand 100% with a small additional output filter; 2) 99.58% test accuracy with\n277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with\n576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.\n", "link": "http://arxiv.org/abs/2405.12179v1", "date": "2024-05-20", "relevancy": 1.9735, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5139}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5055}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Temporal%20Kernels%20with%20Orthogonal%20Polynomials&body=Title%3A%20Building%20Temporal%20Kernels%20with%20Orthogonal%20Polynomials%0AAuthor%3A%20Yan%20Ru%20Pei%20and%20Olivier%20Coenen%0AAbstract%3A%20%20%20We%20introduce%20a%20class%20of%20models%20named%20PLEIADES%20%28PoLynomial%20Expansion%20In%0AAdaptive%20Distributed%20Event-based%20Systems%29%2C%20which%20contains%20temporal%20convolution%0Akernels%20generated%20from%20orthogonal%20polynomial%20basis%20functions.%20We%20focus%20on%0Ainterfacing%20these%20networks%20with%20event-based%20data%20to%20perform%20online%0Aspatiotemporal%20classification%20and%20detection%20with%20low%20latency.%20By%20virtue%20of%0Ausing%20structured%20temporal%20kernels%20and%20event-based%20data%2C%20we%20have%20the%20freedom%20to%0Avary%20the%20sample%20rate%20of%20the%20data%20along%20with%20the%20discretization%20step-size%20of%20the%0Anetwork%20without%20additional%20finetuning.%20We%20experimented%20with%20three%20event-based%0Abenchmarks%20and%20obtained%20state-of-the-art%20results%20on%20all%20three%20by%20large%20margins%0Awith%20significantly%20smaller%20memory%20and%20compute%20costs.%20We%20achieved%3A%201%29%2099.59%25%0Aaccuracy%20with%20192K%20parameters%20on%20the%20DVS128%20hand%20gesture%20recognition%20dataset%0Aand%20100%25%20with%20a%20small%20additional%20output%20filter%3B%202%29%2099.58%25%20test%20accuracy%20with%0A277K%20parameters%20on%20the%20AIS%202024%20eye%20tracking%20challenge%3B%20and%203%29%200.556%20mAP%20with%0A576k%20parameters%20on%20the%20PROPHESEE%201%20Megapixel%20Automotive%20Detection%20Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Temporal%2520Kernels%2520with%2520Orthogonal%2520Polynomials%26entry.906535625%3DYan%2520Ru%2520Pei%2520and%2520Olivier%2520Coenen%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520class%2520of%2520models%2520named%2520PLEIADES%2520%2528PoLynomial%2520Expansion%2520In%250AAdaptive%2520Distributed%2520Event-based%2520Systems%2529%252C%2520which%2520contains%2520temporal%2520convolution%250Akernels%2520generated%2520from%2520orthogonal%2520polynomial%2520basis%2520functions.%2520We%2520focus%2520on%250Ainterfacing%2520these%2520networks%2520with%2520event-based%2520data%2520to%2520perform%2520online%250Aspatiotemporal%2520classification%2520and%2520detection%2520with%2520low%2520latency.%2520By%2520virtue%2520of%250Ausing%2520structured%2520temporal%2520kernels%2520and%2520event-based%2520data%252C%2520we%2520have%2520the%2520freedom%2520to%250Avary%2520the%2520sample%2520rate%2520of%2520the%2520data%2520along%2520with%2520the%2520discretization%2520step-size%2520of%2520the%250Anetwork%2520without%2520additional%2520finetuning.%2520We%2520experimented%2520with%2520three%2520event-based%250Abenchmarks%2520and%2520obtained%2520state-of-the-art%2520results%2520on%2520all%2520three%2520by%2520large%2520margins%250Awith%2520significantly%2520smaller%2520memory%2520and%2520compute%2520costs.%2520We%2520achieved%253A%25201%2529%252099.59%2525%250Aaccuracy%2520with%2520192K%2520parameters%2520on%2520the%2520DVS128%2520hand%2520gesture%2520recognition%2520dataset%250Aand%2520100%2525%2520with%2520a%2520small%2520additional%2520output%2520filter%253B%25202%2529%252099.58%2525%2520test%2520accuracy%2520with%250A277K%2520parameters%2520on%2520the%2520AIS%25202024%2520eye%2520tracking%2520challenge%253B%2520and%25203%2529%25200.556%2520mAP%2520with%250A576k%2520parameters%2520on%2520the%2520PROPHESEE%25201%2520Megapixel%2520Automotive%2520Detection%2520Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Temporal%20Kernels%20with%20Orthogonal%20Polynomials&entry.906535625=Yan%20Ru%20Pei%20and%20Olivier%20Coenen&entry.1292438233=%20%20We%20introduce%20a%20class%20of%20models%20named%20PLEIADES%20%28PoLynomial%20Expansion%20In%0AAdaptive%20Distributed%20Event-based%20Systems%29%2C%20which%20contains%20temporal%20convolution%0Akernels%20generated%20from%20orthogonal%20polynomial%20basis%20functions.%20We%20focus%20on%0Ainterfacing%20these%20networks%20with%20event-based%20data%20to%20perform%20online%0Aspatiotemporal%20classification%20and%20detection%20with%20low%20latency.%20By%20virtue%20of%0Ausing%20structured%20temporal%20kernels%20and%20event-based%20data%2C%20we%20have%20the%20freedom%20to%0Avary%20the%20sample%20rate%20of%20the%20data%20along%20with%20the%20discretization%20step-size%20of%20the%0Anetwork%20without%20additional%20finetuning.%20We%20experimented%20with%20three%20event-based%0Abenchmarks%20and%20obtained%20state-of-the-art%20results%20on%20all%20three%20by%20large%20margins%0Awith%20significantly%20smaller%20memory%20and%20compute%20costs.%20We%20achieved%3A%201%29%2099.59%25%0Aaccuracy%20with%20192K%20parameters%20on%20the%20DVS128%20hand%20gesture%20recognition%20dataset%0Aand%20100%25%20with%20a%20small%20additional%20output%20filter%3B%202%29%2099.58%25%20test%20accuracy%20with%0A277K%20parameters%20on%20the%20AIS%202024%20eye%20tracking%20challenge%3B%20and%203%29%200.556%20mAP%20with%0A576k%20parameters%20on%20the%20PROPHESEE%201%20Megapixel%20Automotive%20Detection%20Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12179v1&entry.124074799=Read"},
{"title": "Alzheimer's Magnetic Resonance Imaging Classification Using Deep and\n  Meta-Learning Models", "author": "Nida Nasir and Muneeb Ahmed and Neda Afreen and Mustafa Sameer", "abstract": "  Deep learning, a cutting-edge machine learning approach, outperforms\ntraditional machine learning in identifying intricate structures in complex\nhigh-dimensional data, particularly in the domain of healthcare. This study\nfocuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's\ndisease (AD) by leveraging deep learning techniques characterized by\nstate-of-the-art CNNs. Brain imaging techniques such as MRI have enabled the\nmeasurement of pathophysiological brain changes related to Alzheimer's disease.\nAlzheimer's disease is the leading cause of dementia in the elderly, and it is\nan irreversible brain illness that causes gradual cognitive function disorder.\nIn this paper, we train some benchmark deep models individually for the\napproach of the solution and later use an ensembling approach to combine the\neffect of multiple CNNs towards the observation of higher recall and accuracy.\nHere, the model's effectiveness is evaluated using various methods, including\nstacking, majority voting, and the combination of models with high recall\nvalues. The majority voting performs better than the alternative modelling\napproach as the majority voting approach typically reduces the variance in the\npredictions. We report a test accuracy of 90% with a precision score of 0.90\nand a recall score of 0.89 in our proposed approach. In future, this study can\nbe extended to incorporate other types of medical data, including signals,\nimages, and other data. The same or alternative datasets can be used with\nadditional classifiers, neural networks, and AI techniques to enhance\nAlzheimer's detection.\n", "link": "http://arxiv.org/abs/2405.12126v1", "date": "2024-05-20", "relevancy": 1.9608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4928}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alzheimer%27s%20Magnetic%20Resonance%20Imaging%20Classification%20Using%20Deep%20and%0A%20%20Meta-Learning%20Models&body=Title%3A%20Alzheimer%27s%20Magnetic%20Resonance%20Imaging%20Classification%20Using%20Deep%20and%0A%20%20Meta-Learning%20Models%0AAuthor%3A%20Nida%20Nasir%20and%20Muneeb%20Ahmed%20and%20Neda%20Afreen%20and%20Mustafa%20Sameer%0AAbstract%3A%20%20%20Deep%20learning%2C%20a%20cutting-edge%20machine%20learning%20approach%2C%20outperforms%0Atraditional%20machine%20learning%20in%20identifying%20intricate%20structures%20in%20complex%0Ahigh-dimensional%20data%2C%20particularly%20in%20the%20domain%20of%20healthcare.%20This%20study%0Afocuses%20on%20classifying%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20data%20for%20Alzheimer%27s%0Adisease%20%28AD%29%20by%20leveraging%20deep%20learning%20techniques%20characterized%20by%0Astate-of-the-art%20CNNs.%20Brain%20imaging%20techniques%20such%20as%20MRI%20have%20enabled%20the%0Ameasurement%20of%20pathophysiological%20brain%20changes%20related%20to%20Alzheimer%27s%20disease.%0AAlzheimer%27s%20disease%20is%20the%20leading%20cause%20of%20dementia%20in%20the%20elderly%2C%20and%20it%20is%0Aan%20irreversible%20brain%20illness%20that%20causes%20gradual%20cognitive%20function%20disorder.%0AIn%20this%20paper%2C%20we%20train%20some%20benchmark%20deep%20models%20individually%20for%20the%0Aapproach%20of%20the%20solution%20and%20later%20use%20an%20ensembling%20approach%20to%20combine%20the%0Aeffect%20of%20multiple%20CNNs%20towards%20the%20observation%20of%20higher%20recall%20and%20accuracy.%0AHere%2C%20the%20model%27s%20effectiveness%20is%20evaluated%20using%20various%20methods%2C%20including%0Astacking%2C%20majority%20voting%2C%20and%20the%20combination%20of%20models%20with%20high%20recall%0Avalues.%20The%20majority%20voting%20performs%20better%20than%20the%20alternative%20modelling%0Aapproach%20as%20the%20majority%20voting%20approach%20typically%20reduces%20the%20variance%20in%20the%0Apredictions.%20We%20report%20a%20test%20accuracy%20of%2090%25%20with%20a%20precision%20score%20of%200.90%0Aand%20a%20recall%20score%20of%200.89%20in%20our%20proposed%20approach.%20In%20future%2C%20this%20study%20can%0Abe%20extended%20to%20incorporate%20other%20types%20of%20medical%20data%2C%20including%20signals%2C%0Aimages%2C%20and%20other%20data.%20The%20same%20or%20alternative%20datasets%20can%20be%20used%20with%0Aadditional%20classifiers%2C%20neural%20networks%2C%20and%20AI%20techniques%20to%20enhance%0AAlzheimer%27s%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlzheimer%2527s%2520Magnetic%2520Resonance%2520Imaging%2520Classification%2520Using%2520Deep%2520and%250A%2520%2520Meta-Learning%2520Models%26entry.906535625%3DNida%2520Nasir%2520and%2520Muneeb%2520Ahmed%2520and%2520Neda%2520Afreen%2520and%2520Mustafa%2520Sameer%26entry.1292438233%3D%2520%2520Deep%2520learning%252C%2520a%2520cutting-edge%2520machine%2520learning%2520approach%252C%2520outperforms%250Atraditional%2520machine%2520learning%2520in%2520identifying%2520intricate%2520structures%2520in%2520complex%250Ahigh-dimensional%2520data%252C%2520particularly%2520in%2520the%2520domain%2520of%2520healthcare.%2520This%2520study%250Afocuses%2520on%2520classifying%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520data%2520for%2520Alzheimer%2527s%250Adisease%2520%2528AD%2529%2520by%2520leveraging%2520deep%2520learning%2520techniques%2520characterized%2520by%250Astate-of-the-art%2520CNNs.%2520Brain%2520imaging%2520techniques%2520such%2520as%2520MRI%2520have%2520enabled%2520the%250Ameasurement%2520of%2520pathophysiological%2520brain%2520changes%2520related%2520to%2520Alzheimer%2527s%2520disease.%250AAlzheimer%2527s%2520disease%2520is%2520the%2520leading%2520cause%2520of%2520dementia%2520in%2520the%2520elderly%252C%2520and%2520it%2520is%250Aan%2520irreversible%2520brain%2520illness%2520that%2520causes%2520gradual%2520cognitive%2520function%2520disorder.%250AIn%2520this%2520paper%252C%2520we%2520train%2520some%2520benchmark%2520deep%2520models%2520individually%2520for%2520the%250Aapproach%2520of%2520the%2520solution%2520and%2520later%2520use%2520an%2520ensembling%2520approach%2520to%2520combine%2520the%250Aeffect%2520of%2520multiple%2520CNNs%2520towards%2520the%2520observation%2520of%2520higher%2520recall%2520and%2520accuracy.%250AHere%252C%2520the%2520model%2527s%2520effectiveness%2520is%2520evaluated%2520using%2520various%2520methods%252C%2520including%250Astacking%252C%2520majority%2520voting%252C%2520and%2520the%2520combination%2520of%2520models%2520with%2520high%2520recall%250Avalues.%2520The%2520majority%2520voting%2520performs%2520better%2520than%2520the%2520alternative%2520modelling%250Aapproach%2520as%2520the%2520majority%2520voting%2520approach%2520typically%2520reduces%2520the%2520variance%2520in%2520the%250Apredictions.%2520We%2520report%2520a%2520test%2520accuracy%2520of%252090%2525%2520with%2520a%2520precision%2520score%2520of%25200.90%250Aand%2520a%2520recall%2520score%2520of%25200.89%2520in%2520our%2520proposed%2520approach.%2520In%2520future%252C%2520this%2520study%2520can%250Abe%2520extended%2520to%2520incorporate%2520other%2520types%2520of%2520medical%2520data%252C%2520including%2520signals%252C%250Aimages%252C%2520and%2520other%2520data.%2520The%2520same%2520or%2520alternative%2520datasets%2520can%2520be%2520used%2520with%250Aadditional%2520classifiers%252C%2520neural%2520networks%252C%2520and%2520AI%2520techniques%2520to%2520enhance%250AAlzheimer%2527s%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alzheimer%27s%20Magnetic%20Resonance%20Imaging%20Classification%20Using%20Deep%20and%0A%20%20Meta-Learning%20Models&entry.906535625=Nida%20Nasir%20and%20Muneeb%20Ahmed%20and%20Neda%20Afreen%20and%20Mustafa%20Sameer&entry.1292438233=%20%20Deep%20learning%2C%20a%20cutting-edge%20machine%20learning%20approach%2C%20outperforms%0Atraditional%20machine%20learning%20in%20identifying%20intricate%20structures%20in%20complex%0Ahigh-dimensional%20data%2C%20particularly%20in%20the%20domain%20of%20healthcare.%20This%20study%0Afocuses%20on%20classifying%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20data%20for%20Alzheimer%27s%0Adisease%20%28AD%29%20by%20leveraging%20deep%20learning%20techniques%20characterized%20by%0Astate-of-the-art%20CNNs.%20Brain%20imaging%20techniques%20such%20as%20MRI%20have%20enabled%20the%0Ameasurement%20of%20pathophysiological%20brain%20changes%20related%20to%20Alzheimer%27s%20disease.%0AAlzheimer%27s%20disease%20is%20the%20leading%20cause%20of%20dementia%20in%20the%20elderly%2C%20and%20it%20is%0Aan%20irreversible%20brain%20illness%20that%20causes%20gradual%20cognitive%20function%20disorder.%0AIn%20this%20paper%2C%20we%20train%20some%20benchmark%20deep%20models%20individually%20for%20the%0Aapproach%20of%20the%20solution%20and%20later%20use%20an%20ensembling%20approach%20to%20combine%20the%0Aeffect%20of%20multiple%20CNNs%20towards%20the%20observation%20of%20higher%20recall%20and%20accuracy.%0AHere%2C%20the%20model%27s%20effectiveness%20is%20evaluated%20using%20various%20methods%2C%20including%0Astacking%2C%20majority%20voting%2C%20and%20the%20combination%20of%20models%20with%20high%20recall%0Avalues.%20The%20majority%20voting%20performs%20better%20than%20the%20alternative%20modelling%0Aapproach%20as%20the%20majority%20voting%20approach%20typically%20reduces%20the%20variance%20in%20the%0Apredictions.%20We%20report%20a%20test%20accuracy%20of%2090%25%20with%20a%20precision%20score%20of%200.90%0Aand%20a%20recall%20score%20of%200.89%20in%20our%20proposed%20approach.%20In%20future%2C%20this%20study%20can%0Abe%20extended%20to%20incorporate%20other%20types%20of%20medical%20data%2C%20including%20signals%2C%0Aimages%2C%20and%20other%20data.%20The%20same%20or%20alternative%20datasets%20can%20be%20used%20with%0Aadditional%20classifiers%2C%20neural%20networks%2C%20and%20AI%20techniques%20to%20enhance%0AAlzheimer%27s%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12126v1&entry.124074799=Read"},
{"title": "Engineered Ordinary Differential Equations as Classification Algorithm\n  (EODECA): thorough characterization and testing", "author": "Raffaele Marino and Lorenzo Buffoni and Lorenzo Chicchi and Lorenzo Giambagli and Duccio Fanelli", "abstract": "  EODECA (Engineered Ordinary Differential Equations as Classification\nAlgorithm) is a novel approach at the intersection of machine learning and\ndynamical systems theory, presenting a unique framework for classification\ntasks [1]. This method stands out with its dynamical system structure,\nutilizing ordinary differential equations (ODEs) to efficiently handle complex\nclassification challenges. The paper delves into EODECA's dynamical properties,\nemphasizing its resilience against random perturbations and robust performance\nacross various classification scenarios. Notably, EODECA's design incorporates\nthe ability to embed stable attractors in the phase space, enhancing\nreliability and allowing for reversible dynamics. In this paper, we carry out a\ncomprehensive analysis by expanding on the work [1], and employing a Euler\ndiscretization scheme. In particular, we evaluate EODECA's performance across\nfive distinct classification problems, examining its adaptability and\nefficiency. Significantly, we demonstrate EODECA's effectiveness on the MNIST\nand Fashion MNIST datasets, achieving impressive accuracies of $98.06\\%$ and\n$88.21\\%$, respectively. These results are comparable to those of a multi-layer\nperceptron (MLP), underscoring EODECA's potential in complex data processing\ntasks. We further explore the model's learning journey, assessing its evolution\nin both pre and post training environments and highlighting its ability to\nnavigate towards stable attractors. The study also investigates the\ninvertibility of EODECA, shedding light on its decision-making processes and\ninternal workings. This paper presents a significant step towards a more\ntransparent and robust machine learning paradigm, bridging the gap between\nmachine learning algorithms and dynamical systems methodologies.\n", "link": "http://arxiv.org/abs/2312.14681v2", "date": "2024-05-20", "relevancy": 1.9329, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Engineered%20Ordinary%20Differential%20Equations%20as%20Classification%20Algorithm%0A%20%20%28EODECA%29%3A%20thorough%20characterization%20and%20testing&body=Title%3A%20Engineered%20Ordinary%20Differential%20Equations%20as%20Classification%20Algorithm%0A%20%20%28EODECA%29%3A%20thorough%20characterization%20and%20testing%0AAuthor%3A%20Raffaele%20Marino%20and%20Lorenzo%20Buffoni%20and%20Lorenzo%20Chicchi%20and%20Lorenzo%20Giambagli%20and%20Duccio%20Fanelli%0AAbstract%3A%20%20%20EODECA%20%28Engineered%20Ordinary%20Differential%20Equations%20as%20Classification%0AAlgorithm%29%20is%20a%20novel%20approach%20at%20the%20intersection%20of%20machine%20learning%20and%0Adynamical%20systems%20theory%2C%20presenting%20a%20unique%20framework%20for%20classification%0Atasks%20%5B1%5D.%20This%20method%20stands%20out%20with%20its%20dynamical%20system%20structure%2C%0Autilizing%20ordinary%20differential%20equations%20%28ODEs%29%20to%20efficiently%20handle%20complex%0Aclassification%20challenges.%20The%20paper%20delves%20into%20EODECA%27s%20dynamical%20properties%2C%0Aemphasizing%20its%20resilience%20against%20random%20perturbations%20and%20robust%20performance%0Aacross%20various%20classification%20scenarios.%20Notably%2C%20EODECA%27s%20design%20incorporates%0Athe%20ability%20to%20embed%20stable%20attractors%20in%20the%20phase%20space%2C%20enhancing%0Areliability%20and%20allowing%20for%20reversible%20dynamics.%20In%20this%20paper%2C%20we%20carry%20out%20a%0Acomprehensive%20analysis%20by%20expanding%20on%20the%20work%20%5B1%5D%2C%20and%20employing%20a%20Euler%0Adiscretization%20scheme.%20In%20particular%2C%20we%20evaluate%20EODECA%27s%20performance%20across%0Afive%20distinct%20classification%20problems%2C%20examining%20its%20adaptability%20and%0Aefficiency.%20Significantly%2C%20we%20demonstrate%20EODECA%27s%20effectiveness%20on%20the%20MNIST%0Aand%20Fashion%20MNIST%20datasets%2C%20achieving%20impressive%20accuracies%20of%20%2498.06%5C%25%24%20and%0A%2488.21%5C%25%24%2C%20respectively.%20These%20results%20are%20comparable%20to%20those%20of%20a%20multi-layer%0Aperceptron%20%28MLP%29%2C%20underscoring%20EODECA%27s%20potential%20in%20complex%20data%20processing%0Atasks.%20We%20further%20explore%20the%20model%27s%20learning%20journey%2C%20assessing%20its%20evolution%0Ain%20both%20pre%20and%20post%20training%20environments%20and%20highlighting%20its%20ability%20to%0Anavigate%20towards%20stable%20attractors.%20The%20study%20also%20investigates%20the%0Ainvertibility%20of%20EODECA%2C%20shedding%20light%20on%20its%20decision-making%20processes%20and%0Ainternal%20workings.%20This%20paper%20presents%20a%20significant%20step%20towards%20a%20more%0Atransparent%20and%20robust%20machine%20learning%20paradigm%2C%20bridging%20the%20gap%20between%0Amachine%20learning%20algorithms%20and%20dynamical%20systems%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngineered%2520Ordinary%2520Differential%2520Equations%2520as%2520Classification%2520Algorithm%250A%2520%2520%2528EODECA%2529%253A%2520thorough%2520characterization%2520and%2520testing%26entry.906535625%3DRaffaele%2520Marino%2520and%2520Lorenzo%2520Buffoni%2520and%2520Lorenzo%2520Chicchi%2520and%2520Lorenzo%2520Giambagli%2520and%2520Duccio%2520Fanelli%26entry.1292438233%3D%2520%2520EODECA%2520%2528Engineered%2520Ordinary%2520Differential%2520Equations%2520as%2520Classification%250AAlgorithm%2529%2520is%2520a%2520novel%2520approach%2520at%2520the%2520intersection%2520of%2520machine%2520learning%2520and%250Adynamical%2520systems%2520theory%252C%2520presenting%2520a%2520unique%2520framework%2520for%2520classification%250Atasks%2520%255B1%255D.%2520This%2520method%2520stands%2520out%2520with%2520its%2520dynamical%2520system%2520structure%252C%250Autilizing%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%2520to%2520efficiently%2520handle%2520complex%250Aclassification%2520challenges.%2520The%2520paper%2520delves%2520into%2520EODECA%2527s%2520dynamical%2520properties%252C%250Aemphasizing%2520its%2520resilience%2520against%2520random%2520perturbations%2520and%2520robust%2520performance%250Aacross%2520various%2520classification%2520scenarios.%2520Notably%252C%2520EODECA%2527s%2520design%2520incorporates%250Athe%2520ability%2520to%2520embed%2520stable%2520attractors%2520in%2520the%2520phase%2520space%252C%2520enhancing%250Areliability%2520and%2520allowing%2520for%2520reversible%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520carry%2520out%2520a%250Acomprehensive%2520analysis%2520by%2520expanding%2520on%2520the%2520work%2520%255B1%255D%252C%2520and%2520employing%2520a%2520Euler%250Adiscretization%2520scheme.%2520In%2520particular%252C%2520we%2520evaluate%2520EODECA%2527s%2520performance%2520across%250Afive%2520distinct%2520classification%2520problems%252C%2520examining%2520its%2520adaptability%2520and%250Aefficiency.%2520Significantly%252C%2520we%2520demonstrate%2520EODECA%2527s%2520effectiveness%2520on%2520the%2520MNIST%250Aand%2520Fashion%2520MNIST%2520datasets%252C%2520achieving%2520impressive%2520accuracies%2520of%2520%252498.06%255C%2525%2524%2520and%250A%252488.21%255C%2525%2524%252C%2520respectively.%2520These%2520results%2520are%2520comparable%2520to%2520those%2520of%2520a%2520multi-layer%250Aperceptron%2520%2528MLP%2529%252C%2520underscoring%2520EODECA%2527s%2520potential%2520in%2520complex%2520data%2520processing%250Atasks.%2520We%2520further%2520explore%2520the%2520model%2527s%2520learning%2520journey%252C%2520assessing%2520its%2520evolution%250Ain%2520both%2520pre%2520and%2520post%2520training%2520environments%2520and%2520highlighting%2520its%2520ability%2520to%250Anavigate%2520towards%2520stable%2520attractors.%2520The%2520study%2520also%2520investigates%2520the%250Ainvertibility%2520of%2520EODECA%252C%2520shedding%2520light%2520on%2520its%2520decision-making%2520processes%2520and%250Ainternal%2520workings.%2520This%2520paper%2520presents%2520a%2520significant%2520step%2520towards%2520a%2520more%250Atransparent%2520and%2520robust%2520machine%2520learning%2520paradigm%252C%2520bridging%2520the%2520gap%2520between%250Amachine%2520learning%2520algorithms%2520and%2520dynamical%2520systems%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Engineered%20Ordinary%20Differential%20Equations%20as%20Classification%20Algorithm%0A%20%20%28EODECA%29%3A%20thorough%20characterization%20and%20testing&entry.906535625=Raffaele%20Marino%20and%20Lorenzo%20Buffoni%20and%20Lorenzo%20Chicchi%20and%20Lorenzo%20Giambagli%20and%20Duccio%20Fanelli&entry.1292438233=%20%20EODECA%20%28Engineered%20Ordinary%20Differential%20Equations%20as%20Classification%0AAlgorithm%29%20is%20a%20novel%20approach%20at%20the%20intersection%20of%20machine%20learning%20and%0Adynamical%20systems%20theory%2C%20presenting%20a%20unique%20framework%20for%20classification%0Atasks%20%5B1%5D.%20This%20method%20stands%20out%20with%20its%20dynamical%20system%20structure%2C%0Autilizing%20ordinary%20differential%20equations%20%28ODEs%29%20to%20efficiently%20handle%20complex%0Aclassification%20challenges.%20The%20paper%20delves%20into%20EODECA%27s%20dynamical%20properties%2C%0Aemphasizing%20its%20resilience%20against%20random%20perturbations%20and%20robust%20performance%0Aacross%20various%20classification%20scenarios.%20Notably%2C%20EODECA%27s%20design%20incorporates%0Athe%20ability%20to%20embed%20stable%20attractors%20in%20the%20phase%20space%2C%20enhancing%0Areliability%20and%20allowing%20for%20reversible%20dynamics.%20In%20this%20paper%2C%20we%20carry%20out%20a%0Acomprehensive%20analysis%20by%20expanding%20on%20the%20work%20%5B1%5D%2C%20and%20employing%20a%20Euler%0Adiscretization%20scheme.%20In%20particular%2C%20we%20evaluate%20EODECA%27s%20performance%20across%0Afive%20distinct%20classification%20problems%2C%20examining%20its%20adaptability%20and%0Aefficiency.%20Significantly%2C%20we%20demonstrate%20EODECA%27s%20effectiveness%20on%20the%20MNIST%0Aand%20Fashion%20MNIST%20datasets%2C%20achieving%20impressive%20accuracies%20of%20%2498.06%5C%25%24%20and%0A%2488.21%5C%25%24%2C%20respectively.%20These%20results%20are%20comparable%20to%20those%20of%20a%20multi-layer%0Aperceptron%20%28MLP%29%2C%20underscoring%20EODECA%27s%20potential%20in%20complex%20data%20processing%0Atasks.%20We%20further%20explore%20the%20model%27s%20learning%20journey%2C%20assessing%20its%20evolution%0Ain%20both%20pre%20and%20post%20training%20environments%20and%20highlighting%20its%20ability%20to%0Anavigate%20towards%20stable%20attractors.%20The%20study%20also%20investigates%20the%0Ainvertibility%20of%20EODECA%2C%20shedding%20light%20on%20its%20decision-making%20processes%20and%0Ainternal%20workings.%20This%20paper%20presents%20a%20significant%20step%20towards%20a%20more%0Atransparent%20and%20robust%20machine%20learning%20paradigm%2C%20bridging%20the%20gap%20between%0Amachine%20learning%20algorithms%20and%20dynamical%20systems%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14681v2&entry.124074799=Read"},
{"title": "A Comprehensive Study of Machine Learning Techniques for Log-Based\n  Anomaly Detection", "author": "Shan Ali and Chaima Boufaied and Domenico Bianculli and Paula Branco and Lionel Briand", "abstract": "  Growth in system complexity increases the need for automated techniques\ndedicated to different log analysis tasks such as Log-based Anomaly Detection\n(LAD). The latter has been widely addressed in the literature, mostly by means\nof a variety of deep learning techniques.\n  Despite their many advantages, that focus on deep learning techniques is\nsomewhat arbitrary as traditional Machine Learning (ML) techniques may perform\nwell in many cases, depending on the context and datasets. In the same vein,\nsemi-supervised techniques deserve the same attention as supervised techniques\nsince the former have clear practical advantages. Further, current evaluations\nmostly rely on the assessment of detection accuracy. However, this is not\nenough to decide whether or not a specific ML technique is suitable to address\nthe LAD problem in a given context. Other aspects to consider include training\nand prediction times as well as the sensitivity to hyperparameter tuning, which\nin practice matters to engineers. In this paper, we present a comprehensive\nempirical study, in which we evaluate supervised and semi-supervised,\ntraditional and deep ML techniques w.r.t. four evaluation criteria: detection\naccuracy, time performance, sensitivity of detection accuracy and time\nperformance to hyperparameter tuning. The experimental results show that\nsupervised traditional and deep ML techniques fare similarly in terms of their\ndetection accuracy and prediction time. Moreover, overall, sensitivity analysis\nto hyperparameter tuning w.r.t. detection accuracy shows that supervised\ntraditional ML techniques are less sensitive than deep learning techniques.\nFurther, semi-supervised techniques yield significantly worse detection\naccuracy than supervised techniques.\n", "link": "http://arxiv.org/abs/2307.16714v2", "date": "2024-05-20", "relevancy": 1.9309, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4867}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Study%20of%20Machine%20Learning%20Techniques%20for%20Log-Based%0A%20%20Anomaly%20Detection&body=Title%3A%20A%20Comprehensive%20Study%20of%20Machine%20Learning%20Techniques%20for%20Log-Based%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Shan%20Ali%20and%20Chaima%20Boufaied%20and%20Domenico%20Bianculli%20and%20Paula%20Branco%20and%20Lionel%20Briand%0AAbstract%3A%20%20%20Growth%20in%20system%20complexity%20increases%20the%20need%20for%20automated%20techniques%0Adedicated%20to%20different%20log%20analysis%20tasks%20such%20as%20Log-based%20Anomaly%20Detection%0A%28LAD%29.%20The%20latter%20has%20been%20widely%20addressed%20in%20the%20literature%2C%20mostly%20by%20means%0Aof%20a%20variety%20of%20deep%20learning%20techniques.%0A%20%20Despite%20their%20many%20advantages%2C%20that%20focus%20on%20deep%20learning%20techniques%20is%0Asomewhat%20arbitrary%20as%20traditional%20Machine%20Learning%20%28ML%29%20techniques%20may%20perform%0Awell%20in%20many%20cases%2C%20depending%20on%20the%20context%20and%20datasets.%20In%20the%20same%20vein%2C%0Asemi-supervised%20techniques%20deserve%20the%20same%20attention%20as%20supervised%20techniques%0Asince%20the%20former%20have%20clear%20practical%20advantages.%20Further%2C%20current%20evaluations%0Amostly%20rely%20on%20the%20assessment%20of%20detection%20accuracy.%20However%2C%20this%20is%20not%0Aenough%20to%20decide%20whether%20or%20not%20a%20specific%20ML%20technique%20is%20suitable%20to%20address%0Athe%20LAD%20problem%20in%20a%20given%20context.%20Other%20aspects%20to%20consider%20include%20training%0Aand%20prediction%20times%20as%20well%20as%20the%20sensitivity%20to%20hyperparameter%20tuning%2C%20which%0Ain%20practice%20matters%20to%20engineers.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Aempirical%20study%2C%20in%20which%20we%20evaluate%20supervised%20and%20semi-supervised%2C%0Atraditional%20and%20deep%20ML%20techniques%20w.r.t.%20four%20evaluation%20criteria%3A%20detection%0Aaccuracy%2C%20time%20performance%2C%20sensitivity%20of%20detection%20accuracy%20and%20time%0Aperformance%20to%20hyperparameter%20tuning.%20The%20experimental%20results%20show%20that%0Asupervised%20traditional%20and%20deep%20ML%20techniques%20fare%20similarly%20in%20terms%20of%20their%0Adetection%20accuracy%20and%20prediction%20time.%20Moreover%2C%20overall%2C%20sensitivity%20analysis%0Ato%20hyperparameter%20tuning%20w.r.t.%20detection%20accuracy%20shows%20that%20supervised%0Atraditional%20ML%20techniques%20are%20less%20sensitive%20than%20deep%20learning%20techniques.%0AFurther%2C%20semi-supervised%20techniques%20yield%20significantly%20worse%20detection%0Aaccuracy%20than%20supervised%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Study%2520of%2520Machine%2520Learning%2520Techniques%2520for%2520Log-Based%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DShan%2520Ali%2520and%2520Chaima%2520Boufaied%2520and%2520Domenico%2520Bianculli%2520and%2520Paula%2520Branco%2520and%2520Lionel%2520Briand%26entry.1292438233%3D%2520%2520Growth%2520in%2520system%2520complexity%2520increases%2520the%2520need%2520for%2520automated%2520techniques%250Adedicated%2520to%2520different%2520log%2520analysis%2520tasks%2520such%2520as%2520Log-based%2520Anomaly%2520Detection%250A%2528LAD%2529.%2520The%2520latter%2520has%2520been%2520widely%2520addressed%2520in%2520the%2520literature%252C%2520mostly%2520by%2520means%250Aof%2520a%2520variety%2520of%2520deep%2520learning%2520techniques.%250A%2520%2520Despite%2520their%2520many%2520advantages%252C%2520that%2520focus%2520on%2520deep%2520learning%2520techniques%2520is%250Asomewhat%2520arbitrary%2520as%2520traditional%2520Machine%2520Learning%2520%2528ML%2529%2520techniques%2520may%2520perform%250Awell%2520in%2520many%2520cases%252C%2520depending%2520on%2520the%2520context%2520and%2520datasets.%2520In%2520the%2520same%2520vein%252C%250Asemi-supervised%2520techniques%2520deserve%2520the%2520same%2520attention%2520as%2520supervised%2520techniques%250Asince%2520the%2520former%2520have%2520clear%2520practical%2520advantages.%2520Further%252C%2520current%2520evaluations%250Amostly%2520rely%2520on%2520the%2520assessment%2520of%2520detection%2520accuracy.%2520However%252C%2520this%2520is%2520not%250Aenough%2520to%2520decide%2520whether%2520or%2520not%2520a%2520specific%2520ML%2520technique%2520is%2520suitable%2520to%2520address%250Athe%2520LAD%2520problem%2520in%2520a%2520given%2520context.%2520Other%2520aspects%2520to%2520consider%2520include%2520training%250Aand%2520prediction%2520times%2520as%2520well%2520as%2520the%2520sensitivity%2520to%2520hyperparameter%2520tuning%252C%2520which%250Ain%2520practice%2520matters%2520to%2520engineers.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%250Aempirical%2520study%252C%2520in%2520which%2520we%2520evaluate%2520supervised%2520and%2520semi-supervised%252C%250Atraditional%2520and%2520deep%2520ML%2520techniques%2520w.r.t.%2520four%2520evaluation%2520criteria%253A%2520detection%250Aaccuracy%252C%2520time%2520performance%252C%2520sensitivity%2520of%2520detection%2520accuracy%2520and%2520time%250Aperformance%2520to%2520hyperparameter%2520tuning.%2520The%2520experimental%2520results%2520show%2520that%250Asupervised%2520traditional%2520and%2520deep%2520ML%2520techniques%2520fare%2520similarly%2520in%2520terms%2520of%2520their%250Adetection%2520accuracy%2520and%2520prediction%2520time.%2520Moreover%252C%2520overall%252C%2520sensitivity%2520analysis%250Ato%2520hyperparameter%2520tuning%2520w.r.t.%2520detection%2520accuracy%2520shows%2520that%2520supervised%250Atraditional%2520ML%2520techniques%2520are%2520less%2520sensitive%2520than%2520deep%2520learning%2520techniques.%250AFurther%252C%2520semi-supervised%2520techniques%2520yield%2520significantly%2520worse%2520detection%250Aaccuracy%2520than%2520supervised%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Study%20of%20Machine%20Learning%20Techniques%20for%20Log-Based%0A%20%20Anomaly%20Detection&entry.906535625=Shan%20Ali%20and%20Chaima%20Boufaied%20and%20Domenico%20Bianculli%20and%20Paula%20Branco%20and%20Lionel%20Briand&entry.1292438233=%20%20Growth%20in%20system%20complexity%20increases%20the%20need%20for%20automated%20techniques%0Adedicated%20to%20different%20log%20analysis%20tasks%20such%20as%20Log-based%20Anomaly%20Detection%0A%28LAD%29.%20The%20latter%20has%20been%20widely%20addressed%20in%20the%20literature%2C%20mostly%20by%20means%0Aof%20a%20variety%20of%20deep%20learning%20techniques.%0A%20%20Despite%20their%20many%20advantages%2C%20that%20focus%20on%20deep%20learning%20techniques%20is%0Asomewhat%20arbitrary%20as%20traditional%20Machine%20Learning%20%28ML%29%20techniques%20may%20perform%0Awell%20in%20many%20cases%2C%20depending%20on%20the%20context%20and%20datasets.%20In%20the%20same%20vein%2C%0Asemi-supervised%20techniques%20deserve%20the%20same%20attention%20as%20supervised%20techniques%0Asince%20the%20former%20have%20clear%20practical%20advantages.%20Further%2C%20current%20evaluations%0Amostly%20rely%20on%20the%20assessment%20of%20detection%20accuracy.%20However%2C%20this%20is%20not%0Aenough%20to%20decide%20whether%20or%20not%20a%20specific%20ML%20technique%20is%20suitable%20to%20address%0Athe%20LAD%20problem%20in%20a%20given%20context.%20Other%20aspects%20to%20consider%20include%20training%0Aand%20prediction%20times%20as%20well%20as%20the%20sensitivity%20to%20hyperparameter%20tuning%2C%20which%0Ain%20practice%20matters%20to%20engineers.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Aempirical%20study%2C%20in%20which%20we%20evaluate%20supervised%20and%20semi-supervised%2C%0Atraditional%20and%20deep%20ML%20techniques%20w.r.t.%20four%20evaluation%20criteria%3A%20detection%0Aaccuracy%2C%20time%20performance%2C%20sensitivity%20of%20detection%20accuracy%20and%20time%0Aperformance%20to%20hyperparameter%20tuning.%20The%20experimental%20results%20show%20that%0Asupervised%20traditional%20and%20deep%20ML%20techniques%20fare%20similarly%20in%20terms%20of%20their%0Adetection%20accuracy%20and%20prediction%20time.%20Moreover%2C%20overall%2C%20sensitivity%20analysis%0Ato%20hyperparameter%20tuning%20w.r.t.%20detection%20accuracy%20shows%20that%20supervised%0Atraditional%20ML%20techniques%20are%20less%20sensitive%20than%20deep%20learning%20techniques.%0AFurther%2C%20semi-supervised%20techniques%20yield%20significantly%20worse%20detection%0Aaccuracy%20than%20supervised%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16714v2&entry.124074799=Read"},
{"title": "A review on the use of large language models as virtual tutors", "author": "Silvia Garc\u00eda-M\u00e9ndez and Francisco de Arriba-P\u00e9rez and Mar\u00eda del Carmen Somoza-L\u00f3pez", "abstract": "  Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.\n", "link": "http://arxiv.org/abs/2405.11983v1", "date": "2024-05-20", "relevancy": 1.9218, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors&body=Title%3A%20A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors%0AAuthor%3A%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Mar%C3%ADa%20del%20Carmen%20Somoza-L%C3%B3pez%0AAbstract%3A%20%20%20Transformer%20architectures%20contribute%20to%20managing%20long-term%20dependencies%20for%0ANatural%20Language%20Processing%2C%20representing%20one%20of%20the%20most%20recent%20changes%20in%20the%0Afield.%20These%20architectures%20are%20the%20basis%20of%20the%20innovative%2C%20cutting-edge%20Large%0ALanguage%20Models%20%28LLMs%29%20that%20have%20produced%20a%20huge%20buzz%20in%20several%20fields%20and%0Aindustrial%20sectors%2C%20among%20the%20ones%20education%20stands%20out.%20Accordingly%2C%20these%0Agenerative%20Artificial%20Intelligence-based%20solutions%20have%20directed%20the%20change%20in%0Atechniques%20and%20the%20evolution%20in%20educational%20methods%20and%20contents%2C%20along%20with%0Anetwork%20infrastructure%2C%20towards%20high-quality%20learning.%20Given%20the%20popularity%20of%0ALLMs%2C%20this%20review%20seeks%20to%20provide%20a%20comprehensive%20overview%20of%20those%20solutions%0Adesigned%20specifically%20to%20generate%20and%20evaluate%20educational%20materials%20and%20which%0Ainvolve%20students%20and%20teachers%20in%20their%20design%20or%20experimental%20plan.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20review%20of%20educational%20applications%20%28e.g.%2C%0Astudent%20assessment%29%20of%20LLMs.%20As%20expected%2C%20the%20most%20common%20role%20of%20these%20systems%0Ais%20as%20virtual%20tutors%20for%20automatic%20question%20generation.%20Moreover%2C%20the%20most%0Apopular%20models%20are%20GTP-3%20and%20BERT.%20However%2C%20due%20to%20the%20continuous%20launch%20of%20new%0Agenerative%20models%2C%20new%20works%20are%20expected%20to%20be%20published%20shortly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520review%2520on%2520the%2520use%2520of%2520large%2520language%2520models%2520as%2520virtual%2520tutors%26entry.906535625%3DSilvia%2520Garc%25C3%25ADa-M%25C3%25A9ndez%2520and%2520Francisco%2520de%2520Arriba-P%25C3%25A9rez%2520and%2520Mar%25C3%25ADa%2520del%2520Carmen%2520Somoza-L%25C3%25B3pez%26entry.1292438233%3D%2520%2520Transformer%2520architectures%2520contribute%2520to%2520managing%2520long-term%2520dependencies%2520for%250ANatural%2520Language%2520Processing%252C%2520representing%2520one%2520of%2520the%2520most%2520recent%2520changes%2520in%2520the%250Afield.%2520These%2520architectures%2520are%2520the%2520basis%2520of%2520the%2520innovative%252C%2520cutting-edge%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520that%2520have%2520produced%2520a%2520huge%2520buzz%2520in%2520several%2520fields%2520and%250Aindustrial%2520sectors%252C%2520among%2520the%2520ones%2520education%2520stands%2520out.%2520Accordingly%252C%2520these%250Agenerative%2520Artificial%2520Intelligence-based%2520solutions%2520have%2520directed%2520the%2520change%2520in%250Atechniques%2520and%2520the%2520evolution%2520in%2520educational%2520methods%2520and%2520contents%252C%2520along%2520with%250Anetwork%2520infrastructure%252C%2520towards%2520high-quality%2520learning.%2520Given%2520the%2520popularity%2520of%250ALLMs%252C%2520this%2520review%2520seeks%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520those%2520solutions%250Adesigned%2520specifically%2520to%2520generate%2520and%2520evaluate%2520educational%2520materials%2520and%2520which%250Ainvolve%2520students%2520and%2520teachers%2520in%2520their%2520design%2520or%2520experimental%2520plan.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520review%2520of%2520educational%2520applications%2520%2528e.g.%252C%250Astudent%2520assessment%2529%2520of%2520LLMs.%2520As%2520expected%252C%2520the%2520most%2520common%2520role%2520of%2520these%2520systems%250Ais%2520as%2520virtual%2520tutors%2520for%2520automatic%2520question%2520generation.%2520Moreover%252C%2520the%2520most%250Apopular%2520models%2520are%2520GTP-3%2520and%2520BERT.%2520However%252C%2520due%2520to%2520the%2520continuous%2520launch%2520of%2520new%250Agenerative%2520models%252C%2520new%2520works%2520are%2520expected%2520to%2520be%2520published%2520shortly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors&entry.906535625=Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Mar%C3%ADa%20del%20Carmen%20Somoza-L%C3%B3pez&entry.1292438233=%20%20Transformer%20architectures%20contribute%20to%20managing%20long-term%20dependencies%20for%0ANatural%20Language%20Processing%2C%20representing%20one%20of%20the%20most%20recent%20changes%20in%20the%0Afield.%20These%20architectures%20are%20the%20basis%20of%20the%20innovative%2C%20cutting-edge%20Large%0ALanguage%20Models%20%28LLMs%29%20that%20have%20produced%20a%20huge%20buzz%20in%20several%20fields%20and%0Aindustrial%20sectors%2C%20among%20the%20ones%20education%20stands%20out.%20Accordingly%2C%20these%0Agenerative%20Artificial%20Intelligence-based%20solutions%20have%20directed%20the%20change%20in%0Atechniques%20and%20the%20evolution%20in%20educational%20methods%20and%20contents%2C%20along%20with%0Anetwork%20infrastructure%2C%20towards%20high-quality%20learning.%20Given%20the%20popularity%20of%0ALLMs%2C%20this%20review%20seeks%20to%20provide%20a%20comprehensive%20overview%20of%20those%20solutions%0Adesigned%20specifically%20to%20generate%20and%20evaluate%20educational%20materials%20and%20which%0Ainvolve%20students%20and%20teachers%20in%20their%20design%20or%20experimental%20plan.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20review%20of%20educational%20applications%20%28e.g.%2C%0Astudent%20assessment%29%20of%20LLMs.%20As%20expected%2C%20the%20most%20common%20role%20of%20these%20systems%0Ais%20as%20virtual%20tutors%20for%20automatic%20question%20generation.%20Moreover%2C%20the%20most%0Apopular%20models%20are%20GTP-3%20and%20BERT.%20However%2C%20due%20to%20the%20continuous%20launch%20of%20new%0Agenerative%20models%2C%20new%20works%20are%20expected%20to%20be%20published%20shortly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11983v1&entry.124074799=Read"},
{"title": "EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri\n  using X-ray CT", "author": "Stephen Parsons and C. Seth Parker and Christy Chapman and Mami Hayashida and W. Brent Seales", "abstract": "  We present a complete software pipeline for revealing the hidden texts of the\nHerculaneum papyri using X-ray CT images. This enhanced virtual unwrapping\npipeline combines machine learning with a novel geometric framework linking 3D\nand 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset\nrepresenting two decades of research effort on this problem. EduceLab-Scrolls\ncontains a set of volumetric X-ray CT images of both small fragments and\nintact, rolled scrolls. The dataset also contains 2D image labels that are used\nin the supervised training of an ink detection model. Labeling is enabled by\naligning spectral photography of scroll fragments with X-ray CT images of the\nsame fragments, thus creating a machine-learnable mapping between image spaces\nand modalities. This alignment permits supervised learning for the detection of\n\"invisible\" carbon ink in X-ray CT, a task that is \"impossible\" even for human\nexpert labelers. To our knowledge, this is the first aligned dataset of its\nkind and is the largest dataset ever released in the heritage domain. Our\nmethod is capable of revealing accurate lines of text on scroll fragments with\nknown ground truth. Revealed text is verified using visual confirmation,\nquantitative image metrics, and scholarly review. EduceLab-Scrolls has also\nenabled the discovery, for the first time, of hidden texts from the Herculaneum\npapyri, which we present here. We anticipate that the EduceLab-Scrolls dataset\nwill generate more textual discovery as research continues.\n", "link": "http://arxiv.org/abs/2304.02084v4", "date": "2024-05-20", "relevancy": 1.9139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4762}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EduceLab-Scrolls%3A%20Verifiable%20Recovery%20of%20Text%20from%20Herculaneum%20Papyri%0A%20%20using%20X-ray%20CT&body=Title%3A%20EduceLab-Scrolls%3A%20Verifiable%20Recovery%20of%20Text%20from%20Herculaneum%20Papyri%0A%20%20using%20X-ray%20CT%0AAuthor%3A%20Stephen%20Parsons%20and%20C.%20Seth%20Parker%20and%20Christy%20Chapman%20and%20Mami%20Hayashida%20and%20W.%20Brent%20Seales%0AAbstract%3A%20%20%20We%20present%20a%20complete%20software%20pipeline%20for%20revealing%20the%20hidden%20texts%20of%20the%0AHerculaneum%20papyri%20using%20X-ray%20CT%20images.%20This%20enhanced%20virtual%20unwrapping%0Apipeline%20combines%20machine%20learning%20with%20a%20novel%20geometric%20framework%20linking%203D%0Aand%202D%20images.%20We%20also%20present%20EduceLab-Scrolls%2C%20a%20comprehensive%20open%20dataset%0Arepresenting%20two%20decades%20of%20research%20effort%20on%20this%20problem.%20EduceLab-Scrolls%0Acontains%20a%20set%20of%20volumetric%20X-ray%20CT%20images%20of%20both%20small%20fragments%20and%0Aintact%2C%20rolled%20scrolls.%20The%20dataset%20also%20contains%202D%20image%20labels%20that%20are%20used%0Ain%20the%20supervised%20training%20of%20an%20ink%20detection%20model.%20Labeling%20is%20enabled%20by%0Aaligning%20spectral%20photography%20of%20scroll%20fragments%20with%20X-ray%20CT%20images%20of%20the%0Asame%20fragments%2C%20thus%20creating%20a%20machine-learnable%20mapping%20between%20image%20spaces%0Aand%20modalities.%20This%20alignment%20permits%20supervised%20learning%20for%20the%20detection%20of%0A%22invisible%22%20carbon%20ink%20in%20X-ray%20CT%2C%20a%20task%20that%20is%20%22impossible%22%20even%20for%20human%0Aexpert%20labelers.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20aligned%20dataset%20of%20its%0Akind%20and%20is%20the%20largest%20dataset%20ever%20released%20in%20the%20heritage%20domain.%20Our%0Amethod%20is%20capable%20of%20revealing%20accurate%20lines%20of%20text%20on%20scroll%20fragments%20with%0Aknown%20ground%20truth.%20Revealed%20text%20is%20verified%20using%20visual%20confirmation%2C%0Aquantitative%20image%20metrics%2C%20and%20scholarly%20review.%20EduceLab-Scrolls%20has%20also%0Aenabled%20the%20discovery%2C%20for%20the%20first%20time%2C%20of%20hidden%20texts%20from%20the%20Herculaneum%0Apapyri%2C%20which%20we%20present%20here.%20We%20anticipate%20that%20the%20EduceLab-Scrolls%20dataset%0Awill%20generate%20more%20textual%20discovery%20as%20research%20continues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02084v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEduceLab-Scrolls%253A%2520Verifiable%2520Recovery%2520of%2520Text%2520from%2520Herculaneum%2520Papyri%250A%2520%2520using%2520X-ray%2520CT%26entry.906535625%3DStephen%2520Parsons%2520and%2520C.%2520Seth%2520Parker%2520and%2520Christy%2520Chapman%2520and%2520Mami%2520Hayashida%2520and%2520W.%2520Brent%2520Seales%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520complete%2520software%2520pipeline%2520for%2520revealing%2520the%2520hidden%2520texts%2520of%2520the%250AHerculaneum%2520papyri%2520using%2520X-ray%2520CT%2520images.%2520This%2520enhanced%2520virtual%2520unwrapping%250Apipeline%2520combines%2520machine%2520learning%2520with%2520a%2520novel%2520geometric%2520framework%2520linking%25203D%250Aand%25202D%2520images.%2520We%2520also%2520present%2520EduceLab-Scrolls%252C%2520a%2520comprehensive%2520open%2520dataset%250Arepresenting%2520two%2520decades%2520of%2520research%2520effort%2520on%2520this%2520problem.%2520EduceLab-Scrolls%250Acontains%2520a%2520set%2520of%2520volumetric%2520X-ray%2520CT%2520images%2520of%2520both%2520small%2520fragments%2520and%250Aintact%252C%2520rolled%2520scrolls.%2520The%2520dataset%2520also%2520contains%25202D%2520image%2520labels%2520that%2520are%2520used%250Ain%2520the%2520supervised%2520training%2520of%2520an%2520ink%2520detection%2520model.%2520Labeling%2520is%2520enabled%2520by%250Aaligning%2520spectral%2520photography%2520of%2520scroll%2520fragments%2520with%2520X-ray%2520CT%2520images%2520of%2520the%250Asame%2520fragments%252C%2520thus%2520creating%2520a%2520machine-learnable%2520mapping%2520between%2520image%2520spaces%250Aand%2520modalities.%2520This%2520alignment%2520permits%2520supervised%2520learning%2520for%2520the%2520detection%2520of%250A%2522invisible%2522%2520carbon%2520ink%2520in%2520X-ray%2520CT%252C%2520a%2520task%2520that%2520is%2520%2522impossible%2522%2520even%2520for%2520human%250Aexpert%2520labelers.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520aligned%2520dataset%2520of%2520its%250Akind%2520and%2520is%2520the%2520largest%2520dataset%2520ever%2520released%2520in%2520the%2520heritage%2520domain.%2520Our%250Amethod%2520is%2520capable%2520of%2520revealing%2520accurate%2520lines%2520of%2520text%2520on%2520scroll%2520fragments%2520with%250Aknown%2520ground%2520truth.%2520Revealed%2520text%2520is%2520verified%2520using%2520visual%2520confirmation%252C%250Aquantitative%2520image%2520metrics%252C%2520and%2520scholarly%2520review.%2520EduceLab-Scrolls%2520has%2520also%250Aenabled%2520the%2520discovery%252C%2520for%2520the%2520first%2520time%252C%2520of%2520hidden%2520texts%2520from%2520the%2520Herculaneum%250Apapyri%252C%2520which%2520we%2520present%2520here.%2520We%2520anticipate%2520that%2520the%2520EduceLab-Scrolls%2520dataset%250Awill%2520generate%2520more%2520textual%2520discovery%2520as%2520research%2520continues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02084v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EduceLab-Scrolls%3A%20Verifiable%20Recovery%20of%20Text%20from%20Herculaneum%20Papyri%0A%20%20using%20X-ray%20CT&entry.906535625=Stephen%20Parsons%20and%20C.%20Seth%20Parker%20and%20Christy%20Chapman%20and%20Mami%20Hayashida%20and%20W.%20Brent%20Seales&entry.1292438233=%20%20We%20present%20a%20complete%20software%20pipeline%20for%20revealing%20the%20hidden%20texts%20of%20the%0AHerculaneum%20papyri%20using%20X-ray%20CT%20images.%20This%20enhanced%20virtual%20unwrapping%0Apipeline%20combines%20machine%20learning%20with%20a%20novel%20geometric%20framework%20linking%203D%0Aand%202D%20images.%20We%20also%20present%20EduceLab-Scrolls%2C%20a%20comprehensive%20open%20dataset%0Arepresenting%20two%20decades%20of%20research%20effort%20on%20this%20problem.%20EduceLab-Scrolls%0Acontains%20a%20set%20of%20volumetric%20X-ray%20CT%20images%20of%20both%20small%20fragments%20and%0Aintact%2C%20rolled%20scrolls.%20The%20dataset%20also%20contains%202D%20image%20labels%20that%20are%20used%0Ain%20the%20supervised%20training%20of%20an%20ink%20detection%20model.%20Labeling%20is%20enabled%20by%0Aaligning%20spectral%20photography%20of%20scroll%20fragments%20with%20X-ray%20CT%20images%20of%20the%0Asame%20fragments%2C%20thus%20creating%20a%20machine-learnable%20mapping%20between%20image%20spaces%0Aand%20modalities.%20This%20alignment%20permits%20supervised%20learning%20for%20the%20detection%20of%0A%22invisible%22%20carbon%20ink%20in%20X-ray%20CT%2C%20a%20task%20that%20is%20%22impossible%22%20even%20for%20human%0Aexpert%20labelers.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20aligned%20dataset%20of%20its%0Akind%20and%20is%20the%20largest%20dataset%20ever%20released%20in%20the%20heritage%20domain.%20Our%0Amethod%20is%20capable%20of%20revealing%20accurate%20lines%20of%20text%20on%20scroll%20fragments%20with%0Aknown%20ground%20truth.%20Revealed%20text%20is%20verified%20using%20visual%20confirmation%2C%0Aquantitative%20image%20metrics%2C%20and%20scholarly%20review.%20EduceLab-Scrolls%20has%20also%0Aenabled%20the%20discovery%2C%20for%20the%20first%20time%2C%20of%20hidden%20texts%20from%20the%20Herculaneum%0Apapyri%2C%20which%20we%20present%20here.%20We%20anticipate%20that%20the%20EduceLab-Scrolls%20dataset%0Awill%20generate%20more%20textual%20discovery%20as%20research%20continues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02084v4&entry.124074799=Read"},
{"title": "Bangladeshi Native Vehicle Detection in Wild", "author": "Bipin Saha and Md. Johirul Islam and Shaikh Khaled Mostaque and Aditya Bhowmik and Tapodhir Karmakar Taton and Md. Nakib Hayat Chowdhury and Mamun Bin Ibne Reaz", "abstract": "  The success of autonomous navigation relies on robust and precise vehicle\nrecognition, hindered by the scarcity of region-specific vehicle detection\ndatasets, impeding the development of context-aware systems. To advance\nterrestrial object detection research, this paper proposes a native vehicle\ndetection dataset for the most commonly appeared vehicle classes in Bangladesh.\n17 distinct vehicle classes have been taken into account, with fully annotated\n81542 instances of 17326 images. Each image width is set to at least 1280px.\nThe dataset's average vehicle bounding box-to-image ratio is 4.7036. This\nBangladesh Native Vehicle Dataset (BNVD) has accounted for several\ngeographical, illumination, variety of vehicle sizes, and orientations to be\nmore robust on surprised scenarios. In the context of examining the BNVD\ndataset, this work provides a thorough assessment with four successive You Only\nLook Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's\neffectiveness is methodically evaluated and contrasted with other vehicle\ndatasets already in use. The BNVD dataset exhibits mean average precision(mAP)\nat 50% intersection over union (IoU) is 0.848 corresponding precision and\nrecall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643\nat an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset\nserves as a reliable representation of vehicle distribution and presents\nconsiderable complexities.\n", "link": "http://arxiv.org/abs/2405.12150v1", "date": "2024-05-20", "relevancy": 1.8979, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bangladeshi%20Native%20Vehicle%20Detection%20in%20Wild&body=Title%3A%20Bangladeshi%20Native%20Vehicle%20Detection%20in%20Wild%0AAuthor%3A%20Bipin%20Saha%20and%20Md.%20Johirul%20Islam%20and%20Shaikh%20Khaled%20Mostaque%20and%20Aditya%20Bhowmik%20and%20Tapodhir%20Karmakar%20Taton%20and%20Md.%20Nakib%20Hayat%20Chowdhury%20and%20Mamun%20Bin%20Ibne%20Reaz%0AAbstract%3A%20%20%20The%20success%20of%20autonomous%20navigation%20relies%20on%20robust%20and%20precise%20vehicle%0Arecognition%2C%20hindered%20by%20the%20scarcity%20of%20region-specific%20vehicle%20detection%0Adatasets%2C%20impeding%20the%20development%20of%20context-aware%20systems.%20To%20advance%0Aterrestrial%20object%20detection%20research%2C%20this%20paper%20proposes%20a%20native%20vehicle%0Adetection%20dataset%20for%20the%20most%20commonly%20appeared%20vehicle%20classes%20in%20Bangladesh.%0A17%20distinct%20vehicle%20classes%20have%20been%20taken%20into%20account%2C%20with%20fully%20annotated%0A81542%20instances%20of%2017326%20images.%20Each%20image%20width%20is%20set%20to%20at%20least%201280px.%0AThe%20dataset%27s%20average%20vehicle%20bounding%20box-to-image%20ratio%20is%204.7036.%20This%0ABangladesh%20Native%20Vehicle%20Dataset%20%28BNVD%29%20has%20accounted%20for%20several%0Ageographical%2C%20illumination%2C%20variety%20of%20vehicle%20sizes%2C%20and%20orientations%20to%20be%0Amore%20robust%20on%20surprised%20scenarios.%20In%20the%20context%20of%20examining%20the%20BNVD%0Adataset%2C%20this%20work%20provides%20a%20thorough%20assessment%20with%20four%20successive%20You%20Only%0ALook%20Once%20%28YOLO%29%20models%2C%20namely%20YOLO%20v5%2C%20v6%2C%20v7%2C%20and%20v8.%20These%20dataset%27s%0Aeffectiveness%20is%20methodically%20evaluated%20and%20contrasted%20with%20other%20vehicle%0Adatasets%20already%20in%20use.%20The%20BNVD%20dataset%20exhibits%20mean%20average%20precision%28mAP%29%0Aat%2050%25%20intersection%20over%20union%20%28IoU%29%20is%200.848%20corresponding%20precision%20and%0Arecall%20values%20of%200.841%20and%200.774.%20The%20research%20findings%20indicate%20a%20mAP%20of%200.643%0Aat%20an%20IoU%20range%20of%200.5%20to%200.95.%20The%20experiments%20show%20that%20the%20BNVD%20dataset%0Aserves%20as%20a%20reliable%20representation%20of%20vehicle%20distribution%20and%20presents%0Aconsiderable%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBangladeshi%2520Native%2520Vehicle%2520Detection%2520in%2520Wild%26entry.906535625%3DBipin%2520Saha%2520and%2520Md.%2520Johirul%2520Islam%2520and%2520Shaikh%2520Khaled%2520Mostaque%2520and%2520Aditya%2520Bhowmik%2520and%2520Tapodhir%2520Karmakar%2520Taton%2520and%2520Md.%2520Nakib%2520Hayat%2520Chowdhury%2520and%2520Mamun%2520Bin%2520Ibne%2520Reaz%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520autonomous%2520navigation%2520relies%2520on%2520robust%2520and%2520precise%2520vehicle%250Arecognition%252C%2520hindered%2520by%2520the%2520scarcity%2520of%2520region-specific%2520vehicle%2520detection%250Adatasets%252C%2520impeding%2520the%2520development%2520of%2520context-aware%2520systems.%2520To%2520advance%250Aterrestrial%2520object%2520detection%2520research%252C%2520this%2520paper%2520proposes%2520a%2520native%2520vehicle%250Adetection%2520dataset%2520for%2520the%2520most%2520commonly%2520appeared%2520vehicle%2520classes%2520in%2520Bangladesh.%250A17%2520distinct%2520vehicle%2520classes%2520have%2520been%2520taken%2520into%2520account%252C%2520with%2520fully%2520annotated%250A81542%2520instances%2520of%252017326%2520images.%2520Each%2520image%2520width%2520is%2520set%2520to%2520at%2520least%25201280px.%250AThe%2520dataset%2527s%2520average%2520vehicle%2520bounding%2520box-to-image%2520ratio%2520is%25204.7036.%2520This%250ABangladesh%2520Native%2520Vehicle%2520Dataset%2520%2528BNVD%2529%2520has%2520accounted%2520for%2520several%250Ageographical%252C%2520illumination%252C%2520variety%2520of%2520vehicle%2520sizes%252C%2520and%2520orientations%2520to%2520be%250Amore%2520robust%2520on%2520surprised%2520scenarios.%2520In%2520the%2520context%2520of%2520examining%2520the%2520BNVD%250Adataset%252C%2520this%2520work%2520provides%2520a%2520thorough%2520assessment%2520with%2520four%2520successive%2520You%2520Only%250ALook%2520Once%2520%2528YOLO%2529%2520models%252C%2520namely%2520YOLO%2520v5%252C%2520v6%252C%2520v7%252C%2520and%2520v8.%2520These%2520dataset%2527s%250Aeffectiveness%2520is%2520methodically%2520evaluated%2520and%2520contrasted%2520with%2520other%2520vehicle%250Adatasets%2520already%2520in%2520use.%2520The%2520BNVD%2520dataset%2520exhibits%2520mean%2520average%2520precision%2528mAP%2529%250Aat%252050%2525%2520intersection%2520over%2520union%2520%2528IoU%2529%2520is%25200.848%2520corresponding%2520precision%2520and%250Arecall%2520values%2520of%25200.841%2520and%25200.774.%2520The%2520research%2520findings%2520indicate%2520a%2520mAP%2520of%25200.643%250Aat%2520an%2520IoU%2520range%2520of%25200.5%2520to%25200.95.%2520The%2520experiments%2520show%2520that%2520the%2520BNVD%2520dataset%250Aserves%2520as%2520a%2520reliable%2520representation%2520of%2520vehicle%2520distribution%2520and%2520presents%250Aconsiderable%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bangladeshi%20Native%20Vehicle%20Detection%20in%20Wild&entry.906535625=Bipin%20Saha%20and%20Md.%20Johirul%20Islam%20and%20Shaikh%20Khaled%20Mostaque%20and%20Aditya%20Bhowmik%20and%20Tapodhir%20Karmakar%20Taton%20and%20Md.%20Nakib%20Hayat%20Chowdhury%20and%20Mamun%20Bin%20Ibne%20Reaz&entry.1292438233=%20%20The%20success%20of%20autonomous%20navigation%20relies%20on%20robust%20and%20precise%20vehicle%0Arecognition%2C%20hindered%20by%20the%20scarcity%20of%20region-specific%20vehicle%20detection%0Adatasets%2C%20impeding%20the%20development%20of%20context-aware%20systems.%20To%20advance%0Aterrestrial%20object%20detection%20research%2C%20this%20paper%20proposes%20a%20native%20vehicle%0Adetection%20dataset%20for%20the%20most%20commonly%20appeared%20vehicle%20classes%20in%20Bangladesh.%0A17%20distinct%20vehicle%20classes%20have%20been%20taken%20into%20account%2C%20with%20fully%20annotated%0A81542%20instances%20of%2017326%20images.%20Each%20image%20width%20is%20set%20to%20at%20least%201280px.%0AThe%20dataset%27s%20average%20vehicle%20bounding%20box-to-image%20ratio%20is%204.7036.%20This%0ABangladesh%20Native%20Vehicle%20Dataset%20%28BNVD%29%20has%20accounted%20for%20several%0Ageographical%2C%20illumination%2C%20variety%20of%20vehicle%20sizes%2C%20and%20orientations%20to%20be%0Amore%20robust%20on%20surprised%20scenarios.%20In%20the%20context%20of%20examining%20the%20BNVD%0Adataset%2C%20this%20work%20provides%20a%20thorough%20assessment%20with%20four%20successive%20You%20Only%0ALook%20Once%20%28YOLO%29%20models%2C%20namely%20YOLO%20v5%2C%20v6%2C%20v7%2C%20and%20v8.%20These%20dataset%27s%0Aeffectiveness%20is%20methodically%20evaluated%20and%20contrasted%20with%20other%20vehicle%0Adatasets%20already%20in%20use.%20The%20BNVD%20dataset%20exhibits%20mean%20average%20precision%28mAP%29%0Aat%2050%25%20intersection%20over%20union%20%28IoU%29%20is%200.848%20corresponding%20precision%20and%0Arecall%20values%20of%200.841%20and%200.774.%20The%20research%20findings%20indicate%20a%20mAP%20of%200.643%0Aat%20an%20IoU%20range%20of%200.5%20to%200.95.%20The%20experiments%20show%20that%20the%20BNVD%20dataset%0Aserves%20as%20a%20reliable%20representation%20of%20vehicle%20distribution%20and%20presents%0Aconsiderable%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12150v1&entry.124074799=Read"},
{"title": "Non-autoregressive Generative Models for Reranking Recommendation", "author": "Yuxin Ren and Qiya Yang and Yichun Wu and Wei Xu and Yalong Wang and Zhiqiang Zhang", "abstract": "  Contemporary recommendation systems are designed to meet users' needs by\ndelivering tailored lists of items that align with their specific demands or\ninterests. In a multi-stage recommendation system, reranking plays a crucial\nrole by modeling the intra-list correlations among items. The key challenge of\nreranking lies in the exploration of optimal sequences within the combinatorial\nspace of permutations. Recent research proposes a generator-evaluator learning\nparadigm, where the generator generates multiple feasible sequences and the\nevaluator picks out the best sequence based on the estimated listwise score.\nThe generator is of vital importance, and generative models are well-suited for\nthe generator function. Current generative models employ an autoregressive\nstrategy for sequence generation. However, deploying autoregressive models in\nreal-time industrial systems is challenging. To address these issues, we\npropose a Non-AutoRegressive generative model for reranking Recommendation\n(NAR4Rec) designed to enhance efficiency and effectiveness. To tackle\nchallenges such as sparse training samples and dynamic candidates, we introduce\na matching model. Considering the diverse nature of user feedback, we employ a\nsequence-level unlikelihood training objective to differentiate feasible\nsequences from unfeasible ones. Additionally, to overcome the lack of\ndependency modeling in non-autoregressive models regarding target items, we\nintroduce contrastive decoding to capture correlations among these items.\nExtensive offline experiments validate the superior performance of NAR4Rec over\nstate-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec\nsignificantly enhances the user experience. Furthermore, NAR4Rec has been fully\ndeployed in a popular video app Kuaishou with over 300 million daily active\nusers.\n", "link": "http://arxiv.org/abs/2402.06871v2", "date": "2024-05-20", "relevancy": 1.8835, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation&body=Title%3A%20Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation%0AAuthor%3A%20Yuxin%20Ren%20and%20Qiya%20Yang%20and%20Yichun%20Wu%20and%20Wei%20Xu%20and%20Yalong%20Wang%20and%20Zhiqiang%20Zhang%0AAbstract%3A%20%20%20Contemporary%20recommendation%20systems%20are%20designed%20to%20meet%20users%27%20needs%20by%0Adelivering%20tailored%20lists%20of%20items%20that%20align%20with%20their%20specific%20demands%20or%0Ainterests.%20In%20a%20multi-stage%20recommendation%20system%2C%20reranking%20plays%20a%20crucial%0Arole%20by%20modeling%20the%20intra-list%20correlations%20among%20items.%20The%20key%20challenge%20of%0Areranking%20lies%20in%20the%20exploration%20of%20optimal%20sequences%20within%20the%20combinatorial%0Aspace%20of%20permutations.%20Recent%20research%20proposes%20a%20generator-evaluator%20learning%0Aparadigm%2C%20where%20the%20generator%20generates%20multiple%20feasible%20sequences%20and%20the%0Aevaluator%20picks%20out%20the%20best%20sequence%20based%20on%20the%20estimated%20listwise%20score.%0AThe%20generator%20is%20of%20vital%20importance%2C%20and%20generative%20models%20are%20well-suited%20for%0Athe%20generator%20function.%20Current%20generative%20models%20employ%20an%20autoregressive%0Astrategy%20for%20sequence%20generation.%20However%2C%20deploying%20autoregressive%20models%20in%0Areal-time%20industrial%20systems%20is%20challenging.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Non-AutoRegressive%20generative%20model%20for%20reranking%20Recommendation%0A%28NAR4Rec%29%20designed%20to%20enhance%20efficiency%20and%20effectiveness.%20To%20tackle%0Achallenges%20such%20as%20sparse%20training%20samples%20and%20dynamic%20candidates%2C%20we%20introduce%0Aa%20matching%20model.%20Considering%20the%20diverse%20nature%20of%20user%20feedback%2C%20we%20employ%20a%0Asequence-level%20unlikelihood%20training%20objective%20to%20differentiate%20feasible%0Asequences%20from%20unfeasible%20ones.%20Additionally%2C%20to%20overcome%20the%20lack%20of%0Adependency%20modeling%20in%20non-autoregressive%20models%20regarding%20target%20items%2C%20we%0Aintroduce%20contrastive%20decoding%20to%20capture%20correlations%20among%20these%20items.%0AExtensive%20offline%20experiments%20validate%20the%20superior%20performance%20of%20NAR4Rec%20over%0Astate-of-the-art%20reranking%20methods.%20Online%20A/B%20tests%20reveal%20that%20NAR4Rec%0Asignificantly%20enhances%20the%20user%20experience.%20Furthermore%2C%20NAR4Rec%20has%20been%20fully%0Adeployed%20in%20a%20popular%20video%20app%20Kuaishou%20with%20over%20300%20million%20daily%20active%0Ausers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-autoregressive%2520Generative%2520Models%2520for%2520Reranking%2520Recommendation%26entry.906535625%3DYuxin%2520Ren%2520and%2520Qiya%2520Yang%2520and%2520Yichun%2520Wu%2520and%2520Wei%2520Xu%2520and%2520Yalong%2520Wang%2520and%2520Zhiqiang%2520Zhang%26entry.1292438233%3D%2520%2520Contemporary%2520recommendation%2520systems%2520are%2520designed%2520to%2520meet%2520users%2527%2520needs%2520by%250Adelivering%2520tailored%2520lists%2520of%2520items%2520that%2520align%2520with%2520their%2520specific%2520demands%2520or%250Ainterests.%2520In%2520a%2520multi-stage%2520recommendation%2520system%252C%2520reranking%2520plays%2520a%2520crucial%250Arole%2520by%2520modeling%2520the%2520intra-list%2520correlations%2520among%2520items.%2520The%2520key%2520challenge%2520of%250Areranking%2520lies%2520in%2520the%2520exploration%2520of%2520optimal%2520sequences%2520within%2520the%2520combinatorial%250Aspace%2520of%2520permutations.%2520Recent%2520research%2520proposes%2520a%2520generator-evaluator%2520learning%250Aparadigm%252C%2520where%2520the%2520generator%2520generates%2520multiple%2520feasible%2520sequences%2520and%2520the%250Aevaluator%2520picks%2520out%2520the%2520best%2520sequence%2520based%2520on%2520the%2520estimated%2520listwise%2520score.%250AThe%2520generator%2520is%2520of%2520vital%2520importance%252C%2520and%2520generative%2520models%2520are%2520well-suited%2520for%250Athe%2520generator%2520function.%2520Current%2520generative%2520models%2520employ%2520an%2520autoregressive%250Astrategy%2520for%2520sequence%2520generation.%2520However%252C%2520deploying%2520autoregressive%2520models%2520in%250Areal-time%2520industrial%2520systems%2520is%2520challenging.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520Non-AutoRegressive%2520generative%2520model%2520for%2520reranking%2520Recommendation%250A%2528NAR4Rec%2529%2520designed%2520to%2520enhance%2520efficiency%2520and%2520effectiveness.%2520To%2520tackle%250Achallenges%2520such%2520as%2520sparse%2520training%2520samples%2520and%2520dynamic%2520candidates%252C%2520we%2520introduce%250Aa%2520matching%2520model.%2520Considering%2520the%2520diverse%2520nature%2520of%2520user%2520feedback%252C%2520we%2520employ%2520a%250Asequence-level%2520unlikelihood%2520training%2520objective%2520to%2520differentiate%2520feasible%250Asequences%2520from%2520unfeasible%2520ones.%2520Additionally%252C%2520to%2520overcome%2520the%2520lack%2520of%250Adependency%2520modeling%2520in%2520non-autoregressive%2520models%2520regarding%2520target%2520items%252C%2520we%250Aintroduce%2520contrastive%2520decoding%2520to%2520capture%2520correlations%2520among%2520these%2520items.%250AExtensive%2520offline%2520experiments%2520validate%2520the%2520superior%2520performance%2520of%2520NAR4Rec%2520over%250Astate-of-the-art%2520reranking%2520methods.%2520Online%2520A/B%2520tests%2520reveal%2520that%2520NAR4Rec%250Asignificantly%2520enhances%2520the%2520user%2520experience.%2520Furthermore%252C%2520NAR4Rec%2520has%2520been%2520fully%250Adeployed%2520in%2520a%2520popular%2520video%2520app%2520Kuaishou%2520with%2520over%2520300%2520million%2520daily%2520active%250Ausers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation&entry.906535625=Yuxin%20Ren%20and%20Qiya%20Yang%20and%20Yichun%20Wu%20and%20Wei%20Xu%20and%20Yalong%20Wang%20and%20Zhiqiang%20Zhang&entry.1292438233=%20%20Contemporary%20recommendation%20systems%20are%20designed%20to%20meet%20users%27%20needs%20by%0Adelivering%20tailored%20lists%20of%20items%20that%20align%20with%20their%20specific%20demands%20or%0Ainterests.%20In%20a%20multi-stage%20recommendation%20system%2C%20reranking%20plays%20a%20crucial%0Arole%20by%20modeling%20the%20intra-list%20correlations%20among%20items.%20The%20key%20challenge%20of%0Areranking%20lies%20in%20the%20exploration%20of%20optimal%20sequences%20within%20the%20combinatorial%0Aspace%20of%20permutations.%20Recent%20research%20proposes%20a%20generator-evaluator%20learning%0Aparadigm%2C%20where%20the%20generator%20generates%20multiple%20feasible%20sequences%20and%20the%0Aevaluator%20picks%20out%20the%20best%20sequence%20based%20on%20the%20estimated%20listwise%20score.%0AThe%20generator%20is%20of%20vital%20importance%2C%20and%20generative%20models%20are%20well-suited%20for%0Athe%20generator%20function.%20Current%20generative%20models%20employ%20an%20autoregressive%0Astrategy%20for%20sequence%20generation.%20However%2C%20deploying%20autoregressive%20models%20in%0Areal-time%20industrial%20systems%20is%20challenging.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Non-AutoRegressive%20generative%20model%20for%20reranking%20Recommendation%0A%28NAR4Rec%29%20designed%20to%20enhance%20efficiency%20and%20effectiveness.%20To%20tackle%0Achallenges%20such%20as%20sparse%20training%20samples%20and%20dynamic%20candidates%2C%20we%20introduce%0Aa%20matching%20model.%20Considering%20the%20diverse%20nature%20of%20user%20feedback%2C%20we%20employ%20a%0Asequence-level%20unlikelihood%20training%20objective%20to%20differentiate%20feasible%0Asequences%20from%20unfeasible%20ones.%20Additionally%2C%20to%20overcome%20the%20lack%20of%0Adependency%20modeling%20in%20non-autoregressive%20models%20regarding%20target%20items%2C%20we%0Aintroduce%20contrastive%20decoding%20to%20capture%20correlations%20among%20these%20items.%0AExtensive%20offline%20experiments%20validate%20the%20superior%20performance%20of%20NAR4Rec%20over%0Astate-of-the-art%20reranking%20methods.%20Online%20A/B%20tests%20reveal%20that%20NAR4Rec%0Asignificantly%20enhances%20the%20user%20experience.%20Furthermore%2C%20NAR4Rec%20has%20been%20fully%0Adeployed%20in%20a%20popular%20video%20app%20Kuaishou%20with%20over%20300%20million%20daily%20active%0Ausers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06871v2&entry.124074799=Read"},
{"title": "PECAN: A Deterministic Certified Defense Against Backdoor Attacks", "author": "Yuhao Zhang and Aws Albarghouthi and Loris D'Antoni", "abstract": "  Neural networks are vulnerable to backdoor poisoning attacks, where the\nattackers maliciously poison the training set and insert triggers into the test\ninput to change the prediction of the victim model. Existing defenses for\nbackdoor attacks either provide no formal guarantees or come with\nexpensive-to-compute and ineffective probabilistic guarantees. We present\nPECAN, an efficient and certified approach for defending against backdoor\nattacks. The key insight powering PECAN is to apply off-the-shelf test-time\nevasion certification techniques on a set of neural networks trained on\ndisjoint partitions of the data. We evaluate PECAN on image classification and\nmalware detection datasets. Our results demonstrate that PECAN can (1)\nsignificantly outperform the state-of-the-art certified backdoor defense, both\nin defense strength and efficiency, and (2) on real back-door attacks, PECAN\ncan reduce attack success rate by order of magnitude when compared to a range\nof baselines from the literature.\n", "link": "http://arxiv.org/abs/2301.11824v4", "date": "2024-05-20", "relevancy": 1.8815, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4944}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4566}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PECAN%3A%20A%20Deterministic%20Certified%20Defense%20Against%20Backdoor%20Attacks&body=Title%3A%20PECAN%3A%20A%20Deterministic%20Certified%20Defense%20Against%20Backdoor%20Attacks%0AAuthor%3A%20Yuhao%20Zhang%20and%20Aws%20Albarghouthi%20and%20Loris%20D%27Antoni%0AAbstract%3A%20%20%20Neural%20networks%20are%20vulnerable%20to%20backdoor%20poisoning%20attacks%2C%20where%20the%0Aattackers%20maliciously%20poison%20the%20training%20set%20and%20insert%20triggers%20into%20the%20test%0Ainput%20to%20change%20the%20prediction%20of%20the%20victim%20model.%20Existing%20defenses%20for%0Abackdoor%20attacks%20either%20provide%20no%20formal%20guarantees%20or%20come%20with%0Aexpensive-to-compute%20and%20ineffective%20probabilistic%20guarantees.%20We%20present%0APECAN%2C%20an%20efficient%20and%20certified%20approach%20for%20defending%20against%20backdoor%0Aattacks.%20The%20key%20insight%20powering%20PECAN%20is%20to%20apply%20off-the-shelf%20test-time%0Aevasion%20certification%20techniques%20on%20a%20set%20of%20neural%20networks%20trained%20on%0Adisjoint%20partitions%20of%20the%20data.%20We%20evaluate%20PECAN%20on%20image%20classification%20and%0Amalware%20detection%20datasets.%20Our%20results%20demonstrate%20that%20PECAN%20can%20%281%29%0Asignificantly%20outperform%20the%20state-of-the-art%20certified%20backdoor%20defense%2C%20both%0Ain%20defense%20strength%20and%20efficiency%2C%20and%20%282%29%20on%20real%20back-door%20attacks%2C%20PECAN%0Acan%20reduce%20attack%20success%20rate%20by%20order%20of%20magnitude%20when%20compared%20to%20a%20range%0Aof%20baselines%20from%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.11824v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPECAN%253A%2520A%2520Deterministic%2520Certified%2520Defense%2520Against%2520Backdoor%2520Attacks%26entry.906535625%3DYuhao%2520Zhang%2520and%2520Aws%2520Albarghouthi%2520and%2520Loris%2520D%2527Antoni%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520vulnerable%2520to%2520backdoor%2520poisoning%2520attacks%252C%2520where%2520the%250Aattackers%2520maliciously%2520poison%2520the%2520training%2520set%2520and%2520insert%2520triggers%2520into%2520the%2520test%250Ainput%2520to%2520change%2520the%2520prediction%2520of%2520the%2520victim%2520model.%2520Existing%2520defenses%2520for%250Abackdoor%2520attacks%2520either%2520provide%2520no%2520formal%2520guarantees%2520or%2520come%2520with%250Aexpensive-to-compute%2520and%2520ineffective%2520probabilistic%2520guarantees.%2520We%2520present%250APECAN%252C%2520an%2520efficient%2520and%2520certified%2520approach%2520for%2520defending%2520against%2520backdoor%250Aattacks.%2520The%2520key%2520insight%2520powering%2520PECAN%2520is%2520to%2520apply%2520off-the-shelf%2520test-time%250Aevasion%2520certification%2520techniques%2520on%2520a%2520set%2520of%2520neural%2520networks%2520trained%2520on%250Adisjoint%2520partitions%2520of%2520the%2520data.%2520We%2520evaluate%2520PECAN%2520on%2520image%2520classification%2520and%250Amalware%2520detection%2520datasets.%2520Our%2520results%2520demonstrate%2520that%2520PECAN%2520can%2520%25281%2529%250Asignificantly%2520outperform%2520the%2520state-of-the-art%2520certified%2520backdoor%2520defense%252C%2520both%250Ain%2520defense%2520strength%2520and%2520efficiency%252C%2520and%2520%25282%2529%2520on%2520real%2520back-door%2520attacks%252C%2520PECAN%250Acan%2520reduce%2520attack%2520success%2520rate%2520by%2520order%2520of%2520magnitude%2520when%2520compared%2520to%2520a%2520range%250Aof%2520baselines%2520from%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.11824v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PECAN%3A%20A%20Deterministic%20Certified%20Defense%20Against%20Backdoor%20Attacks&entry.906535625=Yuhao%20Zhang%20and%20Aws%20Albarghouthi%20and%20Loris%20D%27Antoni&entry.1292438233=%20%20Neural%20networks%20are%20vulnerable%20to%20backdoor%20poisoning%20attacks%2C%20where%20the%0Aattackers%20maliciously%20poison%20the%20training%20set%20and%20insert%20triggers%20into%20the%20test%0Ainput%20to%20change%20the%20prediction%20of%20the%20victim%20model.%20Existing%20defenses%20for%0Abackdoor%20attacks%20either%20provide%20no%20formal%20guarantees%20or%20come%20with%0Aexpensive-to-compute%20and%20ineffective%20probabilistic%20guarantees.%20We%20present%0APECAN%2C%20an%20efficient%20and%20certified%20approach%20for%20defending%20against%20backdoor%0Aattacks.%20The%20key%20insight%20powering%20PECAN%20is%20to%20apply%20off-the-shelf%20test-time%0Aevasion%20certification%20techniques%20on%20a%20set%20of%20neural%20networks%20trained%20on%0Adisjoint%20partitions%20of%20the%20data.%20We%20evaluate%20PECAN%20on%20image%20classification%20and%0Amalware%20detection%20datasets.%20Our%20results%20demonstrate%20that%20PECAN%20can%20%281%29%0Asignificantly%20outperform%20the%20state-of-the-art%20certified%20backdoor%20defense%2C%20both%0Ain%20defense%20strength%20and%20efficiency%2C%20and%20%282%29%20on%20real%20back-door%20attacks%2C%20PECAN%0Acan%20reduce%20attack%20success%20rate%20by%20order%20of%20magnitude%20when%20compared%20to%20a%20range%0Aof%20baselines%20from%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.11824v4&entry.124074799=Read"},
{"title": "A New Baseline Assumption of Integated Gradients Based on Shaply value", "author": "Shuyang Liu and Zixuan Chen and Ge Shi and Ji Wang and Changjie Fan and Yu Xiong and Runze Wu Yujing Hu and Ze Ji and Yang Gao", "abstract": "  Efforts to decode deep neural networks (DNNs) often involve mapping their\npredictions back to the input features. Among these methods, Integrated\nGradients (IG) has emerged as a significant technique. The selection of\nappropriate baselines in IG is crucial for crafting meaningful and unbiased\nexplanations of model predictions in diverse settings. The standard approach of\nutilizing a single baseline, however, is frequently inadequate, prompting the\nneed for multiple baselines. Leveraging the natural link between IG and the\nAumann-Shapley Value, we provide a novel outlook on baseline design.\nTheoretically, we demonstrate that under certain assumptions, a collection of\nbaselines aligns with the coalitions described by the Shapley Value. Building\non this insight, we develop a new baseline method called Shapley Integrated\nGradients (SIG), which uses proportional sampling to mirror the Shapley Value\ncomputation process. Simulations conducted in GridWorld validate that SIG\neffectively emulates the distribution of Shapley Values. Moreover, empirical\ntests on various image processing tasks show that SIG surpasses traditional IG\nbaseline methods by offering more precise estimates of feature contributions,\nproviding consistent explanations across different applications, and ensuring\nadaptability to diverse data types with negligible additional computational\ndemand.\n", "link": "http://arxiv.org/abs/2310.04821v3", "date": "2024-05-20", "relevancy": 1.8806, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4584}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Baseline%20Assumption%20of%20Integated%20Gradients%20Based%20on%20Shaply%20value&body=Title%3A%20A%20New%20Baseline%20Assumption%20of%20Integated%20Gradients%20Based%20on%20Shaply%20value%0AAuthor%3A%20Shuyang%20Liu%20and%20Zixuan%20Chen%20and%20Ge%20Shi%20and%20Ji%20Wang%20and%20Changjie%20Fan%20and%20Yu%20Xiong%20and%20Runze%20Wu%20Yujing%20Hu%20and%20Ze%20Ji%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Efforts%20to%20decode%20deep%20neural%20networks%20%28DNNs%29%20often%20involve%20mapping%20their%0Apredictions%20back%20to%20the%20input%20features.%20Among%20these%20methods%2C%20Integrated%0AGradients%20%28IG%29%20has%20emerged%20as%20a%20significant%20technique.%20The%20selection%20of%0Aappropriate%20baselines%20in%20IG%20is%20crucial%20for%20crafting%20meaningful%20and%20unbiased%0Aexplanations%20of%20model%20predictions%20in%20diverse%20settings.%20The%20standard%20approach%20of%0Autilizing%20a%20single%20baseline%2C%20however%2C%20is%20frequently%20inadequate%2C%20prompting%20the%0Aneed%20for%20multiple%20baselines.%20Leveraging%20the%20natural%20link%20between%20IG%20and%20the%0AAumann-Shapley%20Value%2C%20we%20provide%20a%20novel%20outlook%20on%20baseline%20design.%0ATheoretically%2C%20we%20demonstrate%20that%20under%20certain%20assumptions%2C%20a%20collection%20of%0Abaselines%20aligns%20with%20the%20coalitions%20described%20by%20the%20Shapley%20Value.%20Building%0Aon%20this%20insight%2C%20we%20develop%20a%20new%20baseline%20method%20called%20Shapley%20Integrated%0AGradients%20%28SIG%29%2C%20which%20uses%20proportional%20sampling%20to%20mirror%20the%20Shapley%20Value%0Acomputation%20process.%20Simulations%20conducted%20in%20GridWorld%20validate%20that%20SIG%0Aeffectively%20emulates%20the%20distribution%20of%20Shapley%20Values.%20Moreover%2C%20empirical%0Atests%20on%20various%20image%20processing%20tasks%20show%20that%20SIG%20surpasses%20traditional%20IG%0Abaseline%20methods%20by%20offering%20more%20precise%20estimates%20of%20feature%20contributions%2C%0Aproviding%20consistent%20explanations%20across%20different%20applications%2C%20and%20ensuring%0Aadaptability%20to%20diverse%20data%20types%20with%20negligible%20additional%20computational%0Ademand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Baseline%2520Assumption%2520of%2520Integated%2520Gradients%2520Based%2520on%2520Shaply%2520value%26entry.906535625%3DShuyang%2520Liu%2520and%2520Zixuan%2520Chen%2520and%2520Ge%2520Shi%2520and%2520Ji%2520Wang%2520and%2520Changjie%2520Fan%2520and%2520Yu%2520Xiong%2520and%2520Runze%2520Wu%2520Yujing%2520Hu%2520and%2520Ze%2520Ji%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Efforts%2520to%2520decode%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520often%2520involve%2520mapping%2520their%250Apredictions%2520back%2520to%2520the%2520input%2520features.%2520Among%2520these%2520methods%252C%2520Integrated%250AGradients%2520%2528IG%2529%2520has%2520emerged%2520as%2520a%2520significant%2520technique.%2520The%2520selection%2520of%250Aappropriate%2520baselines%2520in%2520IG%2520is%2520crucial%2520for%2520crafting%2520meaningful%2520and%2520unbiased%250Aexplanations%2520of%2520model%2520predictions%2520in%2520diverse%2520settings.%2520The%2520standard%2520approach%2520of%250Autilizing%2520a%2520single%2520baseline%252C%2520however%252C%2520is%2520frequently%2520inadequate%252C%2520prompting%2520the%250Aneed%2520for%2520multiple%2520baselines.%2520Leveraging%2520the%2520natural%2520link%2520between%2520IG%2520and%2520the%250AAumann-Shapley%2520Value%252C%2520we%2520provide%2520a%2520novel%2520outlook%2520on%2520baseline%2520design.%250ATheoretically%252C%2520we%2520demonstrate%2520that%2520under%2520certain%2520assumptions%252C%2520a%2520collection%2520of%250Abaselines%2520aligns%2520with%2520the%2520coalitions%2520described%2520by%2520the%2520Shapley%2520Value.%2520Building%250Aon%2520this%2520insight%252C%2520we%2520develop%2520a%2520new%2520baseline%2520method%2520called%2520Shapley%2520Integrated%250AGradients%2520%2528SIG%2529%252C%2520which%2520uses%2520proportional%2520sampling%2520to%2520mirror%2520the%2520Shapley%2520Value%250Acomputation%2520process.%2520Simulations%2520conducted%2520in%2520GridWorld%2520validate%2520that%2520SIG%250Aeffectively%2520emulates%2520the%2520distribution%2520of%2520Shapley%2520Values.%2520Moreover%252C%2520empirical%250Atests%2520on%2520various%2520image%2520processing%2520tasks%2520show%2520that%2520SIG%2520surpasses%2520traditional%2520IG%250Abaseline%2520methods%2520by%2520offering%2520more%2520precise%2520estimates%2520of%2520feature%2520contributions%252C%250Aproviding%2520consistent%2520explanations%2520across%2520different%2520applications%252C%2520and%2520ensuring%250Aadaptability%2520to%2520diverse%2520data%2520types%2520with%2520negligible%2520additional%2520computational%250Ademand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Baseline%20Assumption%20of%20Integated%20Gradients%20Based%20on%20Shaply%20value&entry.906535625=Shuyang%20Liu%20and%20Zixuan%20Chen%20and%20Ge%20Shi%20and%20Ji%20Wang%20and%20Changjie%20Fan%20and%20Yu%20Xiong%20and%20Runze%20Wu%20Yujing%20Hu%20and%20Ze%20Ji%20and%20Yang%20Gao&entry.1292438233=%20%20Efforts%20to%20decode%20deep%20neural%20networks%20%28DNNs%29%20often%20involve%20mapping%20their%0Apredictions%20back%20to%20the%20input%20features.%20Among%20these%20methods%2C%20Integrated%0AGradients%20%28IG%29%20has%20emerged%20as%20a%20significant%20technique.%20The%20selection%20of%0Aappropriate%20baselines%20in%20IG%20is%20crucial%20for%20crafting%20meaningful%20and%20unbiased%0Aexplanations%20of%20model%20predictions%20in%20diverse%20settings.%20The%20standard%20approach%20of%0Autilizing%20a%20single%20baseline%2C%20however%2C%20is%20frequently%20inadequate%2C%20prompting%20the%0Aneed%20for%20multiple%20baselines.%20Leveraging%20the%20natural%20link%20between%20IG%20and%20the%0AAumann-Shapley%20Value%2C%20we%20provide%20a%20novel%20outlook%20on%20baseline%20design.%0ATheoretically%2C%20we%20demonstrate%20that%20under%20certain%20assumptions%2C%20a%20collection%20of%0Abaselines%20aligns%20with%20the%20coalitions%20described%20by%20the%20Shapley%20Value.%20Building%0Aon%20this%20insight%2C%20we%20develop%20a%20new%20baseline%20method%20called%20Shapley%20Integrated%0AGradients%20%28SIG%29%2C%20which%20uses%20proportional%20sampling%20to%20mirror%20the%20Shapley%20Value%0Acomputation%20process.%20Simulations%20conducted%20in%20GridWorld%20validate%20that%20SIG%0Aeffectively%20emulates%20the%20distribution%20of%20Shapley%20Values.%20Moreover%2C%20empirical%0Atests%20on%20various%20image%20processing%20tasks%20show%20that%20SIG%20surpasses%20traditional%20IG%0Abaseline%20methods%20by%20offering%20more%20precise%20estimates%20of%20feature%20contributions%2C%0Aproviding%20consistent%20explanations%20across%20different%20applications%2C%20and%20ensuring%0Aadaptability%20to%20diverse%20data%20types%20with%20negligible%20additional%20computational%0Ademand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04821v3&entry.124074799=Read"},
{"title": "Pre-Training on Large-Scale Generated Docking Conformations with\n  HelixDock to Unlock the Potential of Protein-ligand Structure Prediction\n  Models", "author": "Lihang Liu and Shanzhuo Zhang and Donglong He and Xianbin Ye and Jingbo Zhou and Xiaonan Zhang and Yaoyao Jiang and Weiming Diao and Hang Yin and Hua Chai and Fan Wang and Jingzhou He and Liang Zheng and Yonghui Li and Xiaomin Fang", "abstract": "  Protein-ligand structure prediction is an essential task in drug discovery,\npredicting the binding interactions between small molecules (ligands) and\ntarget proteins (receptors). Recent advances have incorporated deep learning\ntechniques to improve the accuracy of protein-ligand structure prediction.\nNevertheless, the experimental validation of docking conformations remains\ncostly, it raises concerns regarding the generalizability of these deep\nlearning-based methods due to the limited training data. In this work, we show\nthat by pre-training on a large-scale docking conformation generated by\ntraditional physics-based docking tools and then fine-tuning with a limited set\nof experimentally validated receptor-ligand complexes, we can obtain a\nprotein-ligand structure prediction model with outstanding performance.\nSpecifically, this process involved the generation of 100 million docking\nconformations for protein-ligand pairings, an endeavor consuming roughly 1\nmillion CPU core days. The proposed model, HelixDock, aims to acquire the\nphysical knowledge encapsulated by the physics-based docking tools during the\npre-training phase. HelixDock has been rigorously benchmarked against both\nphysics-based and deep learning-based baselines, demonstrating its exceptional\nprecision and robust transferability in predicting binding confirmation. In\naddition, our investigation reveals the scaling laws governing pre-trained\nprotein-ligand structure prediction models, indicating a consistent enhancement\nin performance with increases in model parameters and the volume of\npre-training data. Moreover, we applied HelixDock to several drug\ndiscovery-related tasks to validate its practical utility. HelixDock\ndemonstrates outstanding capabilities on both cross-docking and structure-based\nvirtual screening benchmarks.\n", "link": "http://arxiv.org/abs/2310.13913v3", "date": "2024-05-20", "relevancy": 1.8763, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4675}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Training%20on%20Large-Scale%20Generated%20Docking%20Conformations%20with%0A%20%20HelixDock%20to%20Unlock%20the%20Potential%20of%20Protein-ligand%20Structure%20Prediction%0A%20%20Models&body=Title%3A%20Pre-Training%20on%20Large-Scale%20Generated%20Docking%20Conformations%20with%0A%20%20HelixDock%20to%20Unlock%20the%20Potential%20of%20Protein-ligand%20Structure%20Prediction%0A%20%20Models%0AAuthor%3A%20Lihang%20Liu%20and%20Shanzhuo%20Zhang%20and%20Donglong%20He%20and%20Xianbin%20Ye%20and%20Jingbo%20Zhou%20and%20Xiaonan%20Zhang%20and%20Yaoyao%20Jiang%20and%20Weiming%20Diao%20and%20Hang%20Yin%20and%20Hua%20Chai%20and%20Fan%20Wang%20and%20Jingzhou%20He%20and%20Liang%20Zheng%20and%20Yonghui%20Li%20and%20Xiaomin%20Fang%0AAbstract%3A%20%20%20Protein-ligand%20structure%20prediction%20is%20an%20essential%20task%20in%20drug%20discovery%2C%0Apredicting%20the%20binding%20interactions%20between%20small%20molecules%20%28ligands%29%20and%0Atarget%20proteins%20%28receptors%29.%20Recent%20advances%20have%20incorporated%20deep%20learning%0Atechniques%20to%20improve%20the%20accuracy%20of%20protein-ligand%20structure%20prediction.%0ANevertheless%2C%20the%20experimental%20validation%20of%20docking%20conformations%20remains%0Acostly%2C%20it%20raises%20concerns%20regarding%20the%20generalizability%20of%20these%20deep%0Alearning-based%20methods%20due%20to%20the%20limited%20training%20data.%20In%20this%20work%2C%20we%20show%0Athat%20by%20pre-training%20on%20a%20large-scale%20docking%20conformation%20generated%20by%0Atraditional%20physics-based%20docking%20tools%20and%20then%20fine-tuning%20with%20a%20limited%20set%0Aof%20experimentally%20validated%20receptor-ligand%20complexes%2C%20we%20can%20obtain%20a%0Aprotein-ligand%20structure%20prediction%20model%20with%20outstanding%20performance.%0ASpecifically%2C%20this%20process%20involved%20the%20generation%20of%20100%20million%20docking%0Aconformations%20for%20protein-ligand%20pairings%2C%20an%20endeavor%20consuming%20roughly%201%0Amillion%20CPU%20core%20days.%20The%20proposed%20model%2C%20HelixDock%2C%20aims%20to%20acquire%20the%0Aphysical%20knowledge%20encapsulated%20by%20the%20physics-based%20docking%20tools%20during%20the%0Apre-training%20phase.%20HelixDock%20has%20been%20rigorously%20benchmarked%20against%20both%0Aphysics-based%20and%20deep%20learning-based%20baselines%2C%20demonstrating%20its%20exceptional%0Aprecision%20and%20robust%20transferability%20in%20predicting%20binding%20confirmation.%20In%0Aaddition%2C%20our%20investigation%20reveals%20the%20scaling%20laws%20governing%20pre-trained%0Aprotein-ligand%20structure%20prediction%20models%2C%20indicating%20a%20consistent%20enhancement%0Ain%20performance%20with%20increases%20in%20model%20parameters%20and%20the%20volume%20of%0Apre-training%20data.%20Moreover%2C%20we%20applied%20HelixDock%20to%20several%20drug%0Adiscovery-related%20tasks%20to%20validate%20its%20practical%20utility.%20HelixDock%0Ademonstrates%20outstanding%20capabilities%20on%20both%20cross-docking%20and%20structure-based%0Avirtual%20screening%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13913v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Training%2520on%2520Large-Scale%2520Generated%2520Docking%2520Conformations%2520with%250A%2520%2520HelixDock%2520to%2520Unlock%2520the%2520Potential%2520of%2520Protein-ligand%2520Structure%2520Prediction%250A%2520%2520Models%26entry.906535625%3DLihang%2520Liu%2520and%2520Shanzhuo%2520Zhang%2520and%2520Donglong%2520He%2520and%2520Xianbin%2520Ye%2520and%2520Jingbo%2520Zhou%2520and%2520Xiaonan%2520Zhang%2520and%2520Yaoyao%2520Jiang%2520and%2520Weiming%2520Diao%2520and%2520Hang%2520Yin%2520and%2520Hua%2520Chai%2520and%2520Fan%2520Wang%2520and%2520Jingzhou%2520He%2520and%2520Liang%2520Zheng%2520and%2520Yonghui%2520Li%2520and%2520Xiaomin%2520Fang%26entry.1292438233%3D%2520%2520Protein-ligand%2520structure%2520prediction%2520is%2520an%2520essential%2520task%2520in%2520drug%2520discovery%252C%250Apredicting%2520the%2520binding%2520interactions%2520between%2520small%2520molecules%2520%2528ligands%2529%2520and%250Atarget%2520proteins%2520%2528receptors%2529.%2520Recent%2520advances%2520have%2520incorporated%2520deep%2520learning%250Atechniques%2520to%2520improve%2520the%2520accuracy%2520of%2520protein-ligand%2520structure%2520prediction.%250ANevertheless%252C%2520the%2520experimental%2520validation%2520of%2520docking%2520conformations%2520remains%250Acostly%252C%2520it%2520raises%2520concerns%2520regarding%2520the%2520generalizability%2520of%2520these%2520deep%250Alearning-based%2520methods%2520due%2520to%2520the%2520limited%2520training%2520data.%2520In%2520this%2520work%252C%2520we%2520show%250Athat%2520by%2520pre-training%2520on%2520a%2520large-scale%2520docking%2520conformation%2520generated%2520by%250Atraditional%2520physics-based%2520docking%2520tools%2520and%2520then%2520fine-tuning%2520with%2520a%2520limited%2520set%250Aof%2520experimentally%2520validated%2520receptor-ligand%2520complexes%252C%2520we%2520can%2520obtain%2520a%250Aprotein-ligand%2520structure%2520prediction%2520model%2520with%2520outstanding%2520performance.%250ASpecifically%252C%2520this%2520process%2520involved%2520the%2520generation%2520of%2520100%2520million%2520docking%250Aconformations%2520for%2520protein-ligand%2520pairings%252C%2520an%2520endeavor%2520consuming%2520roughly%25201%250Amillion%2520CPU%2520core%2520days.%2520The%2520proposed%2520model%252C%2520HelixDock%252C%2520aims%2520to%2520acquire%2520the%250Aphysical%2520knowledge%2520encapsulated%2520by%2520the%2520physics-based%2520docking%2520tools%2520during%2520the%250Apre-training%2520phase.%2520HelixDock%2520has%2520been%2520rigorously%2520benchmarked%2520against%2520both%250Aphysics-based%2520and%2520deep%2520learning-based%2520baselines%252C%2520demonstrating%2520its%2520exceptional%250Aprecision%2520and%2520robust%2520transferability%2520in%2520predicting%2520binding%2520confirmation.%2520In%250Aaddition%252C%2520our%2520investigation%2520reveals%2520the%2520scaling%2520laws%2520governing%2520pre-trained%250Aprotein-ligand%2520structure%2520prediction%2520models%252C%2520indicating%2520a%2520consistent%2520enhancement%250Ain%2520performance%2520with%2520increases%2520in%2520model%2520parameters%2520and%2520the%2520volume%2520of%250Apre-training%2520data.%2520Moreover%252C%2520we%2520applied%2520HelixDock%2520to%2520several%2520drug%250Adiscovery-related%2520tasks%2520to%2520validate%2520its%2520practical%2520utility.%2520HelixDock%250Ademonstrates%2520outstanding%2520capabilities%2520on%2520both%2520cross-docking%2520and%2520structure-based%250Avirtual%2520screening%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13913v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Training%20on%20Large-Scale%20Generated%20Docking%20Conformations%20with%0A%20%20HelixDock%20to%20Unlock%20the%20Potential%20of%20Protein-ligand%20Structure%20Prediction%0A%20%20Models&entry.906535625=Lihang%20Liu%20and%20Shanzhuo%20Zhang%20and%20Donglong%20He%20and%20Xianbin%20Ye%20and%20Jingbo%20Zhou%20and%20Xiaonan%20Zhang%20and%20Yaoyao%20Jiang%20and%20Weiming%20Diao%20and%20Hang%20Yin%20and%20Hua%20Chai%20and%20Fan%20Wang%20and%20Jingzhou%20He%20and%20Liang%20Zheng%20and%20Yonghui%20Li%20and%20Xiaomin%20Fang&entry.1292438233=%20%20Protein-ligand%20structure%20prediction%20is%20an%20essential%20task%20in%20drug%20discovery%2C%0Apredicting%20the%20binding%20interactions%20between%20small%20molecules%20%28ligands%29%20and%0Atarget%20proteins%20%28receptors%29.%20Recent%20advances%20have%20incorporated%20deep%20learning%0Atechniques%20to%20improve%20the%20accuracy%20of%20protein-ligand%20structure%20prediction.%0ANevertheless%2C%20the%20experimental%20validation%20of%20docking%20conformations%20remains%0Acostly%2C%20it%20raises%20concerns%20regarding%20the%20generalizability%20of%20these%20deep%0Alearning-based%20methods%20due%20to%20the%20limited%20training%20data.%20In%20this%20work%2C%20we%20show%0Athat%20by%20pre-training%20on%20a%20large-scale%20docking%20conformation%20generated%20by%0Atraditional%20physics-based%20docking%20tools%20and%20then%20fine-tuning%20with%20a%20limited%20set%0Aof%20experimentally%20validated%20receptor-ligand%20complexes%2C%20we%20can%20obtain%20a%0Aprotein-ligand%20structure%20prediction%20model%20with%20outstanding%20performance.%0ASpecifically%2C%20this%20process%20involved%20the%20generation%20of%20100%20million%20docking%0Aconformations%20for%20protein-ligand%20pairings%2C%20an%20endeavor%20consuming%20roughly%201%0Amillion%20CPU%20core%20days.%20The%20proposed%20model%2C%20HelixDock%2C%20aims%20to%20acquire%20the%0Aphysical%20knowledge%20encapsulated%20by%20the%20physics-based%20docking%20tools%20during%20the%0Apre-training%20phase.%20HelixDock%20has%20been%20rigorously%20benchmarked%20against%20both%0Aphysics-based%20and%20deep%20learning-based%20baselines%2C%20demonstrating%20its%20exceptional%0Aprecision%20and%20robust%20transferability%20in%20predicting%20binding%20confirmation.%20In%0Aaddition%2C%20our%20investigation%20reveals%20the%20scaling%20laws%20governing%20pre-trained%0Aprotein-ligand%20structure%20prediction%20models%2C%20indicating%20a%20consistent%20enhancement%0Ain%20performance%20with%20increases%20in%20model%20parameters%20and%20the%20volume%20of%0Apre-training%20data.%20Moreover%2C%20we%20applied%20HelixDock%20to%20several%20drug%0Adiscovery-related%20tasks%20to%20validate%20its%20practical%20utility.%20HelixDock%0Ademonstrates%20outstanding%20capabilities%20on%20both%20cross-docking%20and%20structure-based%0Avirtual%20screening%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13913v3&entry.124074799=Read"},
{"title": "An Analysis of Quantile Temporal-Difference Learning", "author": "Mark Rowland and R\u00e9mi Munos and Mohammad Gheshlaghi Azar and Yunhao Tang and Georg Ostrovski and Anna Harutyunyan and Karl Tuyls and Marc G. Bellemare and Will Dabney", "abstract": "  We analyse quantile temporal-difference learning (QTD), a distributional\nreinforcement learning algorithm that has proven to be a key component in\nseveral successful large-scale applications of reinforcement learning. Despite\nthese empirical successes, a theoretical understanding of QTD has proven\nelusive until now. Unlike classical TD learning, which can be analysed with\nstandard stochastic approximation tools, QTD updates do not approximate\ncontraction mappings, are highly non-linear, and may have multiple fixed\npoints. The core result of this paper is a proof of convergence to the fixed\npoints of a related family of dynamic programming procedures with probability\n1, putting QTD on firm theoretical footing. The proof establishes connections\nbetween QTD and non-linear differential inclusions through stochastic\napproximation theory and non-smooth analysis.\n", "link": "http://arxiv.org/abs/2301.04462v3", "date": "2024-05-20", "relevancy": 1.8707, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4885}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Quantile%20Temporal-Difference%20Learning&body=Title%3A%20An%20Analysis%20of%20Quantile%20Temporal-Difference%20Learning%0AAuthor%3A%20Mark%20Rowland%20and%20R%C3%A9mi%20Munos%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Yunhao%20Tang%20and%20Georg%20Ostrovski%20and%20Anna%20Harutyunyan%20and%20Karl%20Tuyls%20and%20Marc%20G.%20Bellemare%20and%20Will%20Dabney%0AAbstract%3A%20%20%20We%20analyse%20quantile%20temporal-difference%20learning%20%28QTD%29%2C%20a%20distributional%0Areinforcement%20learning%20algorithm%20that%20has%20proven%20to%20be%20a%20key%20component%20in%0Aseveral%20successful%20large-scale%20applications%20of%20reinforcement%20learning.%20Despite%0Athese%20empirical%20successes%2C%20a%20theoretical%20understanding%20of%20QTD%20has%20proven%0Aelusive%20until%20now.%20Unlike%20classical%20TD%20learning%2C%20which%20can%20be%20analysed%20with%0Astandard%20stochastic%20approximation%20tools%2C%20QTD%20updates%20do%20not%20approximate%0Acontraction%20mappings%2C%20are%20highly%20non-linear%2C%20and%20may%20have%20multiple%20fixed%0Apoints.%20The%20core%20result%20of%20this%20paper%20is%20a%20proof%20of%20convergence%20to%20the%20fixed%0Apoints%20of%20a%20related%20family%20of%20dynamic%20programming%20procedures%20with%20probability%0A1%2C%20putting%20QTD%20on%20firm%20theoretical%20footing.%20The%20proof%20establishes%20connections%0Abetween%20QTD%20and%20non-linear%20differential%20inclusions%20through%20stochastic%0Aapproximation%20theory%20and%20non-smooth%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04462v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Analysis%2520of%2520Quantile%2520Temporal-Difference%2520Learning%26entry.906535625%3DMark%2520Rowland%2520and%2520R%25C3%25A9mi%2520Munos%2520and%2520Mohammad%2520Gheshlaghi%2520Azar%2520and%2520Yunhao%2520Tang%2520and%2520Georg%2520Ostrovski%2520and%2520Anna%2520Harutyunyan%2520and%2520Karl%2520Tuyls%2520and%2520Marc%2520G.%2520Bellemare%2520and%2520Will%2520Dabney%26entry.1292438233%3D%2520%2520We%2520analyse%2520quantile%2520temporal-difference%2520learning%2520%2528QTD%2529%252C%2520a%2520distributional%250Areinforcement%2520learning%2520algorithm%2520that%2520has%2520proven%2520to%2520be%2520a%2520key%2520component%2520in%250Aseveral%2520successful%2520large-scale%2520applications%2520of%2520reinforcement%2520learning.%2520Despite%250Athese%2520empirical%2520successes%252C%2520a%2520theoretical%2520understanding%2520of%2520QTD%2520has%2520proven%250Aelusive%2520until%2520now.%2520Unlike%2520classical%2520TD%2520learning%252C%2520which%2520can%2520be%2520analysed%2520with%250Astandard%2520stochastic%2520approximation%2520tools%252C%2520QTD%2520updates%2520do%2520not%2520approximate%250Acontraction%2520mappings%252C%2520are%2520highly%2520non-linear%252C%2520and%2520may%2520have%2520multiple%2520fixed%250Apoints.%2520The%2520core%2520result%2520of%2520this%2520paper%2520is%2520a%2520proof%2520of%2520convergence%2520to%2520the%2520fixed%250Apoints%2520of%2520a%2520related%2520family%2520of%2520dynamic%2520programming%2520procedures%2520with%2520probability%250A1%252C%2520putting%2520QTD%2520on%2520firm%2520theoretical%2520footing.%2520The%2520proof%2520establishes%2520connections%250Abetween%2520QTD%2520and%2520non-linear%2520differential%2520inclusions%2520through%2520stochastic%250Aapproximation%2520theory%2520and%2520non-smooth%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04462v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Quantile%20Temporal-Difference%20Learning&entry.906535625=Mark%20Rowland%20and%20R%C3%A9mi%20Munos%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Yunhao%20Tang%20and%20Georg%20Ostrovski%20and%20Anna%20Harutyunyan%20and%20Karl%20Tuyls%20and%20Marc%20G.%20Bellemare%20and%20Will%20Dabney&entry.1292438233=%20%20We%20analyse%20quantile%20temporal-difference%20learning%20%28QTD%29%2C%20a%20distributional%0Areinforcement%20learning%20algorithm%20that%20has%20proven%20to%20be%20a%20key%20component%20in%0Aseveral%20successful%20large-scale%20applications%20of%20reinforcement%20learning.%20Despite%0Athese%20empirical%20successes%2C%20a%20theoretical%20understanding%20of%20QTD%20has%20proven%0Aelusive%20until%20now.%20Unlike%20classical%20TD%20learning%2C%20which%20can%20be%20analysed%20with%0Astandard%20stochastic%20approximation%20tools%2C%20QTD%20updates%20do%20not%20approximate%0Acontraction%20mappings%2C%20are%20highly%20non-linear%2C%20and%20may%20have%20multiple%20fixed%0Apoints.%20The%20core%20result%20of%20this%20paper%20is%20a%20proof%20of%20convergence%20to%20the%20fixed%0Apoints%20of%20a%20related%20family%20of%20dynamic%20programming%20procedures%20with%20probability%0A1%2C%20putting%20QTD%20on%20firm%20theoretical%20footing.%20The%20proof%20establishes%20connections%0Abetween%20QTD%20and%20non-linear%20differential%20inclusions%20through%20stochastic%0Aapproximation%20theory%20and%20non-smooth%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04462v3&entry.124074799=Read"},
{"title": "Multi-order Graph Clustering with Adaptive Node-level Weight Learning", "author": "Ye Liu and Xuelei Lin and Yejia Chen and Reynold Cheng", "abstract": "  Current graph clustering methods emphasize individual node and edge con\nnections, while ignoring higher-order organization at the level of motif. Re\ncently, higher-order graph clustering approaches have been designed by motif\nbased hypergraphs. However, these approaches often suffer from hypergraph\nfragmentation issue seriously, which degrades the clustering performance\ngreatly. Moreover, real-world graphs usually contain diverse motifs, with nodes\nparticipating in multiple motifs. A key challenge is how to achieve precise\nclustering results by integrating information from multiple motifs at the node\nlevel. In this paper, we propose a multi-order graph clustering model (MOGC) to\nintegrate multiple higher-order structures and edge connections at node level.\nMOGC employs an adaptive weight learning mechanism to au tomatically adjust the\ncontributions of different motifs for each node. This not only tackles\nhypergraph fragmentation issue but enhances clustering accuracy. MOGC is\nefficiently solved by an alternating minimization algo rithm. Experiments on\nseven real-world datasets illustrate the effectiveness of MOGC.\n", "link": "http://arxiv.org/abs/2405.12183v1", "date": "2024-05-20", "relevancy": 1.864, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.457}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-order%20Graph%20Clustering%20with%20Adaptive%20Node-level%20Weight%20Learning&body=Title%3A%20Multi-order%20Graph%20Clustering%20with%20Adaptive%20Node-level%20Weight%20Learning%0AAuthor%3A%20Ye%20Liu%20and%20Xuelei%20Lin%20and%20Yejia%20Chen%20and%20Reynold%20Cheng%0AAbstract%3A%20%20%20Current%20graph%20clustering%20methods%20emphasize%20individual%20node%20and%20edge%20con%0Anections%2C%20while%20ignoring%20higher-order%20organization%20at%20the%20level%20of%20motif.%20Re%0Acently%2C%20higher-order%20graph%20clustering%20approaches%20have%20been%20designed%20by%20motif%0Abased%20hypergraphs.%20However%2C%20these%20approaches%20often%20suffer%20from%20hypergraph%0Afragmentation%20issue%20seriously%2C%20which%20degrades%20the%20clustering%20performance%0Agreatly.%20Moreover%2C%20real-world%20graphs%20usually%20contain%20diverse%20motifs%2C%20with%20nodes%0Aparticipating%20in%20multiple%20motifs.%20A%20key%20challenge%20is%20how%20to%20achieve%20precise%0Aclustering%20results%20by%20integrating%20information%20from%20multiple%20motifs%20at%20the%20node%0Alevel.%20In%20this%20paper%2C%20we%20propose%20a%20multi-order%20graph%20clustering%20model%20%28MOGC%29%20to%0Aintegrate%20multiple%20higher-order%20structures%20and%20edge%20connections%20at%20node%20level.%0AMOGC%20employs%20an%20adaptive%20weight%20learning%20mechanism%20to%20au%20tomatically%20adjust%20the%0Acontributions%20of%20different%20motifs%20for%20each%20node.%20This%20not%20only%20tackles%0Ahypergraph%20fragmentation%20issue%20but%20enhances%20clustering%20accuracy.%20MOGC%20is%0Aefficiently%20solved%20by%20an%20alternating%20minimization%20algo%20rithm.%20Experiments%20on%0Aseven%20real-world%20datasets%20illustrate%20the%20effectiveness%20of%20MOGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-order%2520Graph%2520Clustering%2520with%2520Adaptive%2520Node-level%2520Weight%2520Learning%26entry.906535625%3DYe%2520Liu%2520and%2520Xuelei%2520Lin%2520and%2520Yejia%2520Chen%2520and%2520Reynold%2520Cheng%26entry.1292438233%3D%2520%2520Current%2520graph%2520clustering%2520methods%2520emphasize%2520individual%2520node%2520and%2520edge%2520con%250Anections%252C%2520while%2520ignoring%2520higher-order%2520organization%2520at%2520the%2520level%2520of%2520motif.%2520Re%250Acently%252C%2520higher-order%2520graph%2520clustering%2520approaches%2520have%2520been%2520designed%2520by%2520motif%250Abased%2520hypergraphs.%2520However%252C%2520these%2520approaches%2520often%2520suffer%2520from%2520hypergraph%250Afragmentation%2520issue%2520seriously%252C%2520which%2520degrades%2520the%2520clustering%2520performance%250Agreatly.%2520Moreover%252C%2520real-world%2520graphs%2520usually%2520contain%2520diverse%2520motifs%252C%2520with%2520nodes%250Aparticipating%2520in%2520multiple%2520motifs.%2520A%2520key%2520challenge%2520is%2520how%2520to%2520achieve%2520precise%250Aclustering%2520results%2520by%2520integrating%2520information%2520from%2520multiple%2520motifs%2520at%2520the%2520node%250Alevel.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-order%2520graph%2520clustering%2520model%2520%2528MOGC%2529%2520to%250Aintegrate%2520multiple%2520higher-order%2520structures%2520and%2520edge%2520connections%2520at%2520node%2520level.%250AMOGC%2520employs%2520an%2520adaptive%2520weight%2520learning%2520mechanism%2520to%2520au%2520tomatically%2520adjust%2520the%250Acontributions%2520of%2520different%2520motifs%2520for%2520each%2520node.%2520This%2520not%2520only%2520tackles%250Ahypergraph%2520fragmentation%2520issue%2520but%2520enhances%2520clustering%2520accuracy.%2520MOGC%2520is%250Aefficiently%2520solved%2520by%2520an%2520alternating%2520minimization%2520algo%2520rithm.%2520Experiments%2520on%250Aseven%2520real-world%2520datasets%2520illustrate%2520the%2520effectiveness%2520of%2520MOGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-order%20Graph%20Clustering%20with%20Adaptive%20Node-level%20Weight%20Learning&entry.906535625=Ye%20Liu%20and%20Xuelei%20Lin%20and%20Yejia%20Chen%20and%20Reynold%20Cheng&entry.1292438233=%20%20Current%20graph%20clustering%20methods%20emphasize%20individual%20node%20and%20edge%20con%0Anections%2C%20while%20ignoring%20higher-order%20organization%20at%20the%20level%20of%20motif.%20Re%0Acently%2C%20higher-order%20graph%20clustering%20approaches%20have%20been%20designed%20by%20motif%0Abased%20hypergraphs.%20However%2C%20these%20approaches%20often%20suffer%20from%20hypergraph%0Afragmentation%20issue%20seriously%2C%20which%20degrades%20the%20clustering%20performance%0Agreatly.%20Moreover%2C%20real-world%20graphs%20usually%20contain%20diverse%20motifs%2C%20with%20nodes%0Aparticipating%20in%20multiple%20motifs.%20A%20key%20challenge%20is%20how%20to%20achieve%20precise%0Aclustering%20results%20by%20integrating%20information%20from%20multiple%20motifs%20at%20the%20node%0Alevel.%20In%20this%20paper%2C%20we%20propose%20a%20multi-order%20graph%20clustering%20model%20%28MOGC%29%20to%0Aintegrate%20multiple%20higher-order%20structures%20and%20edge%20connections%20at%20node%20level.%0AMOGC%20employs%20an%20adaptive%20weight%20learning%20mechanism%20to%20au%20tomatically%20adjust%20the%0Acontributions%20of%20different%20motifs%20for%20each%20node.%20This%20not%20only%20tackles%0Ahypergraph%20fragmentation%20issue%20but%20enhances%20clustering%20accuracy.%20MOGC%20is%0Aefficiently%20solved%20by%20an%20alternating%20minimization%20algo%20rithm.%20Experiments%20on%0Aseven%20real-world%20datasets%20illustrate%20the%20effectiveness%20of%20MOGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12183v1&entry.124074799=Read"},
{"title": "PATE: Proximity-Aware Time series anomaly Evaluation", "author": "Ramin Ghorbani and Marcel J. T. Reinders and David M. J. Tax", "abstract": "  Evaluating anomaly detection algorithms in time series data is critical as\ninaccuracies can lead to flawed decision-making in various domains where\nreal-time analytics and data-driven strategies are essential. Traditional\nperformance metrics assume iid data and fail to capture the complex temporal\ndynamics and specific characteristics of time series anomalies, such as early\nand delayed detections. We introduce Proximity-Aware Time series anomaly\nEvaluation (PATE), a novel evaluation metric that incorporates the temporal\nrelationship between prediction and anomaly intervals. PATE uses\nproximity-based weighting considering buffer zones around anomaly intervals,\nenabling a more detailed and informed assessment of a detection. Using these\nweights, PATE computes a weighted version of the area under the Precision and\nRecall curve. Our experiments with synthetic and real-world datasets show the\nsuperiority of PATE in providing more sensible and accurate evaluations than\nother evaluation metrics. We also tested several state-of-the-art anomaly\ndetectors across various benchmark datasets using the PATE evaluation scheme.\nThe results show that a common metric like Point-Adjusted F1 Score fails to\ncharacterize the detection performances well, and that PATE is able to provide\na more fair model comparison. By introducing PATE, we redefine the\nunderstanding of model efficacy that steers future studies toward developing\nmore effective and accurate detection models.\n", "link": "http://arxiv.org/abs/2405.12096v1", "date": "2024-05-20", "relevancy": 1.8635, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4736}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PATE%3A%20Proximity-Aware%20Time%20series%20anomaly%20Evaluation&body=Title%3A%20PATE%3A%20Proximity-Aware%20Time%20series%20anomaly%20Evaluation%0AAuthor%3A%20Ramin%20Ghorbani%20and%20Marcel%20J.%20T.%20Reinders%20and%20David%20M.%20J.%20Tax%0AAbstract%3A%20%20%20Evaluating%20anomaly%20detection%20algorithms%20in%20time%20series%20data%20is%20critical%20as%0Ainaccuracies%20can%20lead%20to%20flawed%20decision-making%20in%20various%20domains%20where%0Areal-time%20analytics%20and%20data-driven%20strategies%20are%20essential.%20Traditional%0Aperformance%20metrics%20assume%20iid%20data%20and%20fail%20to%20capture%20the%20complex%20temporal%0Adynamics%20and%20specific%20characteristics%20of%20time%20series%20anomalies%2C%20such%20as%20early%0Aand%20delayed%20detections.%20We%20introduce%20Proximity-Aware%20Time%20series%20anomaly%0AEvaluation%20%28PATE%29%2C%20a%20novel%20evaluation%20metric%20that%20incorporates%20the%20temporal%0Arelationship%20between%20prediction%20and%20anomaly%20intervals.%20PATE%20uses%0Aproximity-based%20weighting%20considering%20buffer%20zones%20around%20anomaly%20intervals%2C%0Aenabling%20a%20more%20detailed%20and%20informed%20assessment%20of%20a%20detection.%20Using%20these%0Aweights%2C%20PATE%20computes%20a%20weighted%20version%20of%20the%20area%20under%20the%20Precision%20and%0ARecall%20curve.%20Our%20experiments%20with%20synthetic%20and%20real-world%20datasets%20show%20the%0Asuperiority%20of%20PATE%20in%20providing%20more%20sensible%20and%20accurate%20evaluations%20than%0Aother%20evaluation%20metrics.%20We%20also%20tested%20several%20state-of-the-art%20anomaly%0Adetectors%20across%20various%20benchmark%20datasets%20using%20the%20PATE%20evaluation%20scheme.%0AThe%20results%20show%20that%20a%20common%20metric%20like%20Point-Adjusted%20F1%20Score%20fails%20to%0Acharacterize%20the%20detection%20performances%20well%2C%20and%20that%20PATE%20is%20able%20to%20provide%0Aa%20more%20fair%20model%20comparison.%20By%20introducing%20PATE%2C%20we%20redefine%20the%0Aunderstanding%20of%20model%20efficacy%20that%20steers%20future%20studies%20toward%20developing%0Amore%20effective%20and%20accurate%20detection%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPATE%253A%2520Proximity-Aware%2520Time%2520series%2520anomaly%2520Evaluation%26entry.906535625%3DRamin%2520Ghorbani%2520and%2520Marcel%2520J.%2520T.%2520Reinders%2520and%2520David%2520M.%2520J.%2520Tax%26entry.1292438233%3D%2520%2520Evaluating%2520anomaly%2520detection%2520algorithms%2520in%2520time%2520series%2520data%2520is%2520critical%2520as%250Ainaccuracies%2520can%2520lead%2520to%2520flawed%2520decision-making%2520in%2520various%2520domains%2520where%250Areal-time%2520analytics%2520and%2520data-driven%2520strategies%2520are%2520essential.%2520Traditional%250Aperformance%2520metrics%2520assume%2520iid%2520data%2520and%2520fail%2520to%2520capture%2520the%2520complex%2520temporal%250Adynamics%2520and%2520specific%2520characteristics%2520of%2520time%2520series%2520anomalies%252C%2520such%2520as%2520early%250Aand%2520delayed%2520detections.%2520We%2520introduce%2520Proximity-Aware%2520Time%2520series%2520anomaly%250AEvaluation%2520%2528PATE%2529%252C%2520a%2520novel%2520evaluation%2520metric%2520that%2520incorporates%2520the%2520temporal%250Arelationship%2520between%2520prediction%2520and%2520anomaly%2520intervals.%2520PATE%2520uses%250Aproximity-based%2520weighting%2520considering%2520buffer%2520zones%2520around%2520anomaly%2520intervals%252C%250Aenabling%2520a%2520more%2520detailed%2520and%2520informed%2520assessment%2520of%2520a%2520detection.%2520Using%2520these%250Aweights%252C%2520PATE%2520computes%2520a%2520weighted%2520version%2520of%2520the%2520area%2520under%2520the%2520Precision%2520and%250ARecall%2520curve.%2520Our%2520experiments%2520with%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520the%250Asuperiority%2520of%2520PATE%2520in%2520providing%2520more%2520sensible%2520and%2520accurate%2520evaluations%2520than%250Aother%2520evaluation%2520metrics.%2520We%2520also%2520tested%2520several%2520state-of-the-art%2520anomaly%250Adetectors%2520across%2520various%2520benchmark%2520datasets%2520using%2520the%2520PATE%2520evaluation%2520scheme.%250AThe%2520results%2520show%2520that%2520a%2520common%2520metric%2520like%2520Point-Adjusted%2520F1%2520Score%2520fails%2520to%250Acharacterize%2520the%2520detection%2520performances%2520well%252C%2520and%2520that%2520PATE%2520is%2520able%2520to%2520provide%250Aa%2520more%2520fair%2520model%2520comparison.%2520By%2520introducing%2520PATE%252C%2520we%2520redefine%2520the%250Aunderstanding%2520of%2520model%2520efficacy%2520that%2520steers%2520future%2520studies%2520toward%2520developing%250Amore%2520effective%2520and%2520accurate%2520detection%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PATE%3A%20Proximity-Aware%20Time%20series%20anomaly%20Evaluation&entry.906535625=Ramin%20Ghorbani%20and%20Marcel%20J.%20T.%20Reinders%20and%20David%20M.%20J.%20Tax&entry.1292438233=%20%20Evaluating%20anomaly%20detection%20algorithms%20in%20time%20series%20data%20is%20critical%20as%0Ainaccuracies%20can%20lead%20to%20flawed%20decision-making%20in%20various%20domains%20where%0Areal-time%20analytics%20and%20data-driven%20strategies%20are%20essential.%20Traditional%0Aperformance%20metrics%20assume%20iid%20data%20and%20fail%20to%20capture%20the%20complex%20temporal%0Adynamics%20and%20specific%20characteristics%20of%20time%20series%20anomalies%2C%20such%20as%20early%0Aand%20delayed%20detections.%20We%20introduce%20Proximity-Aware%20Time%20series%20anomaly%0AEvaluation%20%28PATE%29%2C%20a%20novel%20evaluation%20metric%20that%20incorporates%20the%20temporal%0Arelationship%20between%20prediction%20and%20anomaly%20intervals.%20PATE%20uses%0Aproximity-based%20weighting%20considering%20buffer%20zones%20around%20anomaly%20intervals%2C%0Aenabling%20a%20more%20detailed%20and%20informed%20assessment%20of%20a%20detection.%20Using%20these%0Aweights%2C%20PATE%20computes%20a%20weighted%20version%20of%20the%20area%20under%20the%20Precision%20and%0ARecall%20curve.%20Our%20experiments%20with%20synthetic%20and%20real-world%20datasets%20show%20the%0Asuperiority%20of%20PATE%20in%20providing%20more%20sensible%20and%20accurate%20evaluations%20than%0Aother%20evaluation%20metrics.%20We%20also%20tested%20several%20state-of-the-art%20anomaly%0Adetectors%20across%20various%20benchmark%20datasets%20using%20the%20PATE%20evaluation%20scheme.%0AThe%20results%20show%20that%20a%20common%20metric%20like%20Point-Adjusted%20F1%20Score%20fails%20to%0Acharacterize%20the%20detection%20performances%20well%2C%20and%20that%20PATE%20is%20able%20to%20provide%0Aa%20more%20fair%20model%20comparison.%20By%20introducing%20PATE%2C%20we%20redefine%20the%0Aunderstanding%20of%20model%20efficacy%20that%20steers%20future%20studies%20toward%20developing%0Amore%20effective%20and%20accurate%20detection%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12096v1&entry.124074799=Read"},
{"title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API\n  LLMs", "author": "Kinjal Basu and Ibrahim Abdelaziz and Subhajit Chaudhury and Soham Dan and Maxwell Crouse and Asim Munawar and Sadhana Kumaravel and Vinod Muthusamy and Pavan Kapanipathi and Luis A. Lastras", "abstract": "  There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.\n", "link": "http://arxiv.org/abs/2402.15491v2", "date": "2024-05-20", "relevancy": 1.837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4743}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20API-BLEND%3A%20A%20Comprehensive%20Corpora%20for%20Training%20and%20Benchmarking%20API%0A%20%20LLMs&body=Title%3A%20API-BLEND%3A%20A%20Comprehensive%20Corpora%20for%20Training%20and%20Benchmarking%20API%0A%20%20LLMs%0AAuthor%3A%20Kinjal%20Basu%20and%20Ibrahim%20Abdelaziz%20and%20Subhajit%20Chaudhury%20and%20Soham%20Dan%20and%20Maxwell%20Crouse%20and%20Asim%20Munawar%20and%20Sadhana%20Kumaravel%20and%20Vinod%20Muthusamy%20and%20Pavan%20Kapanipathi%20and%20Luis%20A.%20Lastras%0AAbstract%3A%20%20%20There%20is%20a%20growing%20need%20for%20Large%20Language%20Models%20%28LLMs%29%20to%20effectively%20use%0Atools%20and%20external%20Application%20Programming%20Interfaces%20%28APIs%29%20to%20plan%20and%0Acomplete%20tasks.%20As%20such%2C%20there%20is%20tremendous%20interest%20in%20methods%20that%20can%0Aacquire%20sufficient%20quantities%20of%20train%20and%20test%20data%20that%20involve%20calls%20to%0Atools%20/%20APIs.%20Two%20lines%20of%20research%20have%20emerged%20as%20the%20predominant%20strategies%0Afor%20addressing%20this%20challenge.%20The%20first%20has%20focused%20on%20synthetic%20data%0Ageneration%20techniques%2C%20while%20the%20second%20has%20involved%20curating%20task-adjacent%0Adatasets%20which%20can%20be%20transformed%20into%20API%20/%20Tool-based%20tasks.%20In%20this%20paper%2C%0Awe%20focus%20on%20the%20task%20of%20identifying%2C%20curating%2C%20and%20transforming%20existing%0Adatasets%20and%2C%20in%20turn%2C%20introduce%20API-BLEND%2C%20a%20large%20corpora%20for%20training%20and%0Asystematic%20testing%20of%20tool-augmented%20LLMs.%20The%20datasets%20mimic%20real-world%0Ascenarios%20involving%20API-tasks%20such%20as%20API%20/%20tool%20detection%2C%20slot%20filling%2C%20and%0Asequencing%20of%20the%20detected%20APIs.%20We%20demonstrate%20the%20utility%20of%20the%20API-BLEND%0Adataset%20for%20both%20training%20and%20benchmarking%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPI-BLEND%253A%2520A%2520Comprehensive%2520Corpora%2520for%2520Training%2520and%2520Benchmarking%2520API%250A%2520%2520LLMs%26entry.906535625%3DKinjal%2520Basu%2520and%2520Ibrahim%2520Abdelaziz%2520and%2520Subhajit%2520Chaudhury%2520and%2520Soham%2520Dan%2520and%2520Maxwell%2520Crouse%2520and%2520Asim%2520Munawar%2520and%2520Sadhana%2520Kumaravel%2520and%2520Vinod%2520Muthusamy%2520and%2520Pavan%2520Kapanipathi%2520and%2520Luis%2520A.%2520Lastras%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520growing%2520need%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520effectively%2520use%250Atools%2520and%2520external%2520Application%2520Programming%2520Interfaces%2520%2528APIs%2529%2520to%2520plan%2520and%250Acomplete%2520tasks.%2520As%2520such%252C%2520there%2520is%2520tremendous%2520interest%2520in%2520methods%2520that%2520can%250Aacquire%2520sufficient%2520quantities%2520of%2520train%2520and%2520test%2520data%2520that%2520involve%2520calls%2520to%250Atools%2520/%2520APIs.%2520Two%2520lines%2520of%2520research%2520have%2520emerged%2520as%2520the%2520predominant%2520strategies%250Afor%2520addressing%2520this%2520challenge.%2520The%2520first%2520has%2520focused%2520on%2520synthetic%2520data%250Ageneration%2520techniques%252C%2520while%2520the%2520second%2520has%2520involved%2520curating%2520task-adjacent%250Adatasets%2520which%2520can%2520be%2520transformed%2520into%2520API%2520/%2520Tool-based%2520tasks.%2520In%2520this%2520paper%252C%250Awe%2520focus%2520on%2520the%2520task%2520of%2520identifying%252C%2520curating%252C%2520and%2520transforming%2520existing%250Adatasets%2520and%252C%2520in%2520turn%252C%2520introduce%2520API-BLEND%252C%2520a%2520large%2520corpora%2520for%2520training%2520and%250Asystematic%2520testing%2520of%2520tool-augmented%2520LLMs.%2520The%2520datasets%2520mimic%2520real-world%250Ascenarios%2520involving%2520API-tasks%2520such%2520as%2520API%2520/%2520tool%2520detection%252C%2520slot%2520filling%252C%2520and%250Asequencing%2520of%2520the%2520detected%2520APIs.%2520We%2520demonstrate%2520the%2520utility%2520of%2520the%2520API-BLEND%250Adataset%2520for%2520both%2520training%2520and%2520benchmarking%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=API-BLEND%3A%20A%20Comprehensive%20Corpora%20for%20Training%20and%20Benchmarking%20API%0A%20%20LLMs&entry.906535625=Kinjal%20Basu%20and%20Ibrahim%20Abdelaziz%20and%20Subhajit%20Chaudhury%20and%20Soham%20Dan%20and%20Maxwell%20Crouse%20and%20Asim%20Munawar%20and%20Sadhana%20Kumaravel%20and%20Vinod%20Muthusamy%20and%20Pavan%20Kapanipathi%20and%20Luis%20A.%20Lastras&entry.1292438233=%20%20There%20is%20a%20growing%20need%20for%20Large%20Language%20Models%20%28LLMs%29%20to%20effectively%20use%0Atools%20and%20external%20Application%20Programming%20Interfaces%20%28APIs%29%20to%20plan%20and%0Acomplete%20tasks.%20As%20such%2C%20there%20is%20tremendous%20interest%20in%20methods%20that%20can%0Aacquire%20sufficient%20quantities%20of%20train%20and%20test%20data%20that%20involve%20calls%20to%0Atools%20/%20APIs.%20Two%20lines%20of%20research%20have%20emerged%20as%20the%20predominant%20strategies%0Afor%20addressing%20this%20challenge.%20The%20first%20has%20focused%20on%20synthetic%20data%0Ageneration%20techniques%2C%20while%20the%20second%20has%20involved%20curating%20task-adjacent%0Adatasets%20which%20can%20be%20transformed%20into%20API%20/%20Tool-based%20tasks.%20In%20this%20paper%2C%0Awe%20focus%20on%20the%20task%20of%20identifying%2C%20curating%2C%20and%20transforming%20existing%0Adatasets%20and%2C%20in%20turn%2C%20introduce%20API-BLEND%2C%20a%20large%20corpora%20for%20training%20and%0Asystematic%20testing%20of%20tool-augmented%20LLMs.%20The%20datasets%20mimic%20real-world%0Ascenarios%20involving%20API-tasks%20such%20as%20API%20/%20tool%20detection%2C%20slot%20filling%2C%20and%0Asequencing%20of%20the%20detected%20APIs.%20We%20demonstrate%20the%20utility%20of%20the%20API-BLEND%0Adataset%20for%20both%20training%20and%20benchmarking%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15491v2&entry.124074799=Read"},
{"title": "Variational Inference: Posterior Threshold Improves Network Clustering\n  Accuracy in Sparse Regimes", "author": "Xuezhen Li and Can M. Le", "abstract": "  Variational inference has been widely used in machine learning literature to\nfit various Bayesian models. In network analysis, this method has been\nsuccessfully applied to solve the community detection problems. Although these\nresults are promising, their theoretical support is only for relatively dense\nnetworks, an assumption that may not hold for real networks. In addition, it\nhas been shown recently that the variational loss surface has many saddle\npoints, which may severely affect its performance, especially when applied to\nsparse networks. This paper proposes a simple way to improve the variational\ninference method by hard thresholding the posterior of the community assignment\nafter each iteration. Using a random initialization that correlates with the\ntrue community assignment, we show that the proposed method converges and can\naccurately recover the true community labels, even when the average node degree\nof the network is bounded. Extensive numerical study further confirms the\nadvantage of the proposed method over the classical variational inference and\nanother state-of-the-art algorithm.\n", "link": "http://arxiv.org/abs/2301.04771v2", "date": "2024-05-20", "relevancy": 1.8325, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4602}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Inference%3A%20Posterior%20Threshold%20Improves%20Network%20Clustering%0A%20%20Accuracy%20in%20Sparse%20Regimes&body=Title%3A%20Variational%20Inference%3A%20Posterior%20Threshold%20Improves%20Network%20Clustering%0A%20%20Accuracy%20in%20Sparse%20Regimes%0AAuthor%3A%20Xuezhen%20Li%20and%20Can%20M.%20Le%0AAbstract%3A%20%20%20Variational%20inference%20has%20been%20widely%20used%20in%20machine%20learning%20literature%20to%0Afit%20various%20Bayesian%20models.%20In%20network%20analysis%2C%20this%20method%20has%20been%0Asuccessfully%20applied%20to%20solve%20the%20community%20detection%20problems.%20Although%20these%0Aresults%20are%20promising%2C%20their%20theoretical%20support%20is%20only%20for%20relatively%20dense%0Anetworks%2C%20an%20assumption%20that%20may%20not%20hold%20for%20real%20networks.%20In%20addition%2C%20it%0Ahas%20been%20shown%20recently%20that%20the%20variational%20loss%20surface%20has%20many%20saddle%0Apoints%2C%20which%20may%20severely%20affect%20its%20performance%2C%20especially%20when%20applied%20to%0Asparse%20networks.%20This%20paper%20proposes%20a%20simple%20way%20to%20improve%20the%20variational%0Ainference%20method%20by%20hard%20thresholding%20the%20posterior%20of%20the%20community%20assignment%0Aafter%20each%20iteration.%20Using%20a%20random%20initialization%20that%20correlates%20with%20the%0Atrue%20community%20assignment%2C%20we%20show%20that%20the%20proposed%20method%20converges%20and%20can%0Aaccurately%20recover%20the%20true%20community%20labels%2C%20even%20when%20the%20average%20node%20degree%0Aof%20the%20network%20is%20bounded.%20Extensive%20numerical%20study%20further%20confirms%20the%0Aadvantage%20of%20the%20proposed%20method%20over%20the%20classical%20variational%20inference%20and%0Aanother%20state-of-the-art%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Inference%253A%2520Posterior%2520Threshold%2520Improves%2520Network%2520Clustering%250A%2520%2520Accuracy%2520in%2520Sparse%2520Regimes%26entry.906535625%3DXuezhen%2520Li%2520and%2520Can%2520M.%2520Le%26entry.1292438233%3D%2520%2520Variational%2520inference%2520has%2520been%2520widely%2520used%2520in%2520machine%2520learning%2520literature%2520to%250Afit%2520various%2520Bayesian%2520models.%2520In%2520network%2520analysis%252C%2520this%2520method%2520has%2520been%250Asuccessfully%2520applied%2520to%2520solve%2520the%2520community%2520detection%2520problems.%2520Although%2520these%250Aresults%2520are%2520promising%252C%2520their%2520theoretical%2520support%2520is%2520only%2520for%2520relatively%2520dense%250Anetworks%252C%2520an%2520assumption%2520that%2520may%2520not%2520hold%2520for%2520real%2520networks.%2520In%2520addition%252C%2520it%250Ahas%2520been%2520shown%2520recently%2520that%2520the%2520variational%2520loss%2520surface%2520has%2520many%2520saddle%250Apoints%252C%2520which%2520may%2520severely%2520affect%2520its%2520performance%252C%2520especially%2520when%2520applied%2520to%250Asparse%2520networks.%2520This%2520paper%2520proposes%2520a%2520simple%2520way%2520to%2520improve%2520the%2520variational%250Ainference%2520method%2520by%2520hard%2520thresholding%2520the%2520posterior%2520of%2520the%2520community%2520assignment%250Aafter%2520each%2520iteration.%2520Using%2520a%2520random%2520initialization%2520that%2520correlates%2520with%2520the%250Atrue%2520community%2520assignment%252C%2520we%2520show%2520that%2520the%2520proposed%2520method%2520converges%2520and%2520can%250Aaccurately%2520recover%2520the%2520true%2520community%2520labels%252C%2520even%2520when%2520the%2520average%2520node%2520degree%250Aof%2520the%2520network%2520is%2520bounded.%2520Extensive%2520numerical%2520study%2520further%2520confirms%2520the%250Aadvantage%2520of%2520the%2520proposed%2520method%2520over%2520the%2520classical%2520variational%2520inference%2520and%250Aanother%2520state-of-the-art%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Inference%3A%20Posterior%20Threshold%20Improves%20Network%20Clustering%0A%20%20Accuracy%20in%20Sparse%20Regimes&entry.906535625=Xuezhen%20Li%20and%20Can%20M.%20Le&entry.1292438233=%20%20Variational%20inference%20has%20been%20widely%20used%20in%20machine%20learning%20literature%20to%0Afit%20various%20Bayesian%20models.%20In%20network%20analysis%2C%20this%20method%20has%20been%0Asuccessfully%20applied%20to%20solve%20the%20community%20detection%20problems.%20Although%20these%0Aresults%20are%20promising%2C%20their%20theoretical%20support%20is%20only%20for%20relatively%20dense%0Anetworks%2C%20an%20assumption%20that%20may%20not%20hold%20for%20real%20networks.%20In%20addition%2C%20it%0Ahas%20been%20shown%20recently%20that%20the%20variational%20loss%20surface%20has%20many%20saddle%0Apoints%2C%20which%20may%20severely%20affect%20its%20performance%2C%20especially%20when%20applied%20to%0Asparse%20networks.%20This%20paper%20proposes%20a%20simple%20way%20to%20improve%20the%20variational%0Ainference%20method%20by%20hard%20thresholding%20the%20posterior%20of%20the%20community%20assignment%0Aafter%20each%20iteration.%20Using%20a%20random%20initialization%20that%20correlates%20with%20the%0Atrue%20community%20assignment%2C%20we%20show%20that%20the%20proposed%20method%20converges%20and%20can%0Aaccurately%20recover%20the%20true%20community%20labels%2C%20even%20when%20the%20average%20node%20degree%0Aof%20the%20network%20is%20bounded.%20Extensive%20numerical%20study%20further%20confirms%20the%0Aadvantage%20of%20the%20proposed%20method%20over%20the%20classical%20variational%20inference%20and%0Aanother%20state-of-the-art%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04771v2&entry.124074799=Read"},
{"title": "Continual Learning of Diffusion Models with Generative Distillation", "author": "Sergi Masip and Pau Rodriguez and Tinne Tuytelaars and Gido M. van de Ven", "abstract": "  Diffusion models are powerful generative models that achieve state-of-the-art\nperformance in image synthesis. However, training them demands substantial\namounts of data and computational resources. Continual learning would allow for\nincrementally learning new tasks and accumulating knowledge, thus enabling the\nreuse of trained models for further learning. One potentially suitable\ncontinual learning approach is generative replay, where a copy of a generative\nmodel trained on previous tasks produces synthetic data that are interleaved\nwith data from the current task. However, standard generative replay applied to\ndiffusion models results in a catastrophic loss in denoising capabilities. In\nthis paper, we propose generative distillation, an approach that distils the\nentire reverse process of a diffusion model. We demonstrate that our approach\nsubstantially improves the continual learning performance of generative replay\nwith only a modest increase in the computational costs.\n", "link": "http://arxiv.org/abs/2311.14028v2", "date": "2024-05-20", "relevancy": 1.8172, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6273}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.611}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20of%20Diffusion%20Models%20with%20Generative%20Distillation&body=Title%3A%20Continual%20Learning%20of%20Diffusion%20Models%20with%20Generative%20Distillation%0AAuthor%3A%20Sergi%20Masip%20and%20Pau%20Rodriguez%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven%0AAbstract%3A%20%20%20Diffusion%20models%20are%20powerful%20generative%20models%20that%20achieve%20state-of-the-art%0Aperformance%20in%20image%20synthesis.%20However%2C%20training%20them%20demands%20substantial%0Aamounts%20of%20data%20and%20computational%20resources.%20Continual%20learning%20would%20allow%20for%0Aincrementally%20learning%20new%20tasks%20and%20accumulating%20knowledge%2C%20thus%20enabling%20the%0Areuse%20of%20trained%20models%20for%20further%20learning.%20One%20potentially%20suitable%0Acontinual%20learning%20approach%20is%20generative%20replay%2C%20where%20a%20copy%20of%20a%20generative%0Amodel%20trained%20on%20previous%20tasks%20produces%20synthetic%20data%20that%20are%20interleaved%0Awith%20data%20from%20the%20current%20task.%20However%2C%20standard%20generative%20replay%20applied%20to%0Adiffusion%20models%20results%20in%20a%20catastrophic%20loss%20in%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20generative%20distillation%2C%20an%20approach%20that%20distils%20the%0Aentire%20reverse%20process%20of%20a%20diffusion%20model.%20We%20demonstrate%20that%20our%20approach%0Asubstantially%20improves%20the%20continual%20learning%20performance%20of%20generative%20replay%0Awith%20only%20a%20modest%20increase%20in%20the%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520of%2520Diffusion%2520Models%2520with%2520Generative%2520Distillation%26entry.906535625%3DSergi%2520Masip%2520and%2520Pau%2520Rodriguez%2520and%2520Tinne%2520Tuytelaars%2520and%2520Gido%2520M.%2520van%2520de%2520Ven%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520are%2520powerful%2520generative%2520models%2520that%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520image%2520synthesis.%2520However%252C%2520training%2520them%2520demands%2520substantial%250Aamounts%2520of%2520data%2520and%2520computational%2520resources.%2520Continual%2520learning%2520would%2520allow%2520for%250Aincrementally%2520learning%2520new%2520tasks%2520and%2520accumulating%2520knowledge%252C%2520thus%2520enabling%2520the%250Areuse%2520of%2520trained%2520models%2520for%2520further%2520learning.%2520One%2520potentially%2520suitable%250Acontinual%2520learning%2520approach%2520is%2520generative%2520replay%252C%2520where%2520a%2520copy%2520of%2520a%2520generative%250Amodel%2520trained%2520on%2520previous%2520tasks%2520produces%2520synthetic%2520data%2520that%2520are%2520interleaved%250Awith%2520data%2520from%2520the%2520current%2520task.%2520However%252C%2520standard%2520generative%2520replay%2520applied%2520to%250Adiffusion%2520models%2520results%2520in%2520a%2520catastrophic%2520loss%2520in%2520denoising%2520capabilities.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520generative%2520distillation%252C%2520an%2520approach%2520that%2520distils%2520the%250Aentire%2520reverse%2520process%2520of%2520a%2520diffusion%2520model.%2520We%2520demonstrate%2520that%2520our%2520approach%250Asubstantially%2520improves%2520the%2520continual%2520learning%2520performance%2520of%2520generative%2520replay%250Awith%2520only%2520a%2520modest%2520increase%2520in%2520the%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20of%20Diffusion%20Models%20with%20Generative%20Distillation&entry.906535625=Sergi%20Masip%20and%20Pau%20Rodriguez%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven&entry.1292438233=%20%20Diffusion%20models%20are%20powerful%20generative%20models%20that%20achieve%20state-of-the-art%0Aperformance%20in%20image%20synthesis.%20However%2C%20training%20them%20demands%20substantial%0Aamounts%20of%20data%20and%20computational%20resources.%20Continual%20learning%20would%20allow%20for%0Aincrementally%20learning%20new%20tasks%20and%20accumulating%20knowledge%2C%20thus%20enabling%20the%0Areuse%20of%20trained%20models%20for%20further%20learning.%20One%20potentially%20suitable%0Acontinual%20learning%20approach%20is%20generative%20replay%2C%20where%20a%20copy%20of%20a%20generative%0Amodel%20trained%20on%20previous%20tasks%20produces%20synthetic%20data%20that%20are%20interleaved%0Awith%20data%20from%20the%20current%20task.%20However%2C%20standard%20generative%20replay%20applied%20to%0Adiffusion%20models%20results%20in%20a%20catastrophic%20loss%20in%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20generative%20distillation%2C%20an%20approach%20that%20distils%20the%0Aentire%20reverse%20process%20of%20a%20diffusion%20model.%20We%20demonstrate%20that%20our%20approach%0Asubstantially%20improves%20the%20continual%20learning%20performance%20of%20generative%20replay%0Awith%20only%20a%20modest%20increase%20in%20the%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14028v2&entry.124074799=Read"},
{"title": "Sheet Music Transformer ++: End-to-End Full-Page Optical Music\n  Recognition for Pianoform Sheet Music", "author": "Antonio R\u00edos-Vila and Jorge Calvo-Zaragoza and David Rizo and Thierry Paquet", "abstract": "  Optical Music Recognition is a field that has progressed significantly,\nbringing accurate systems that transcribe effectively music scores into digital\nformats. Despite this, there are still several limitations that hinder OMR from\nachieving its full potential. Specifically, state of the art OMR still depends\non multi-stage pipelines for performing full-page transcription, as well as it\nhas only been demonstrated in monophonic cases, leaving behind very relevant\nengravings. In this work, we present the Sheet Music Transformer++, an\nend-to-end model that is able to transcribe full-page polyphonic music scores\nwithout the need of a previous Layout Analysis step. This is done thanks to an\nextensive curriculum learning-based pretraining with synthetic data generation.\nWe conduct several experiments on a full-page extension of a public polyphonic\ntranscription dataset. The experimental outcomes confirm that the model is\ncompetent at transcribing full-page pianoform scores, marking a noteworthy\nmilestone in end-to-end OMR transcription.\n", "link": "http://arxiv.org/abs/2405.12105v1", "date": "2024-05-20", "relevancy": 1.8148, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4934}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sheet%20Music%20Transformer%20%2B%2B%3A%20End-to-End%20Full-Page%20Optical%20Music%0A%20%20Recognition%20for%20Pianoform%20Sheet%20Music&body=Title%3A%20Sheet%20Music%20Transformer%20%2B%2B%3A%20End-to-End%20Full-Page%20Optical%20Music%0A%20%20Recognition%20for%20Pianoform%20Sheet%20Music%0AAuthor%3A%20Antonio%20R%C3%ADos-Vila%20and%20Jorge%20Calvo-Zaragoza%20and%20David%20Rizo%20and%20Thierry%20Paquet%0AAbstract%3A%20%20%20Optical%20Music%20Recognition%20is%20a%20field%20that%20has%20progressed%20significantly%2C%0Abringing%20accurate%20systems%20that%20transcribe%20effectively%20music%20scores%20into%20digital%0Aformats.%20Despite%20this%2C%20there%20are%20still%20several%20limitations%20that%20hinder%20OMR%20from%0Aachieving%20its%20full%20potential.%20Specifically%2C%20state%20of%20the%20art%20OMR%20still%20depends%0Aon%20multi-stage%20pipelines%20for%20performing%20full-page%20transcription%2C%20as%20well%20as%20it%0Ahas%20only%20been%20demonstrated%20in%20monophonic%20cases%2C%20leaving%20behind%20very%20relevant%0Aengravings.%20In%20this%20work%2C%20we%20present%20the%20Sheet%20Music%20Transformer%2B%2B%2C%20an%0Aend-to-end%20model%20that%20is%20able%20to%20transcribe%20full-page%20polyphonic%20music%20scores%0Awithout%20the%20need%20of%20a%20previous%20Layout%20Analysis%20step.%20This%20is%20done%20thanks%20to%20an%0Aextensive%20curriculum%20learning-based%20pretraining%20with%20synthetic%20data%20generation.%0AWe%20conduct%20several%20experiments%20on%20a%20full-page%20extension%20of%20a%20public%20polyphonic%0Atranscription%20dataset.%20The%20experimental%20outcomes%20confirm%20that%20the%20model%20is%0Acompetent%20at%20transcribing%20full-page%20pianoform%20scores%2C%20marking%20a%20noteworthy%0Amilestone%20in%20end-to-end%20OMR%20transcription.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSheet%2520Music%2520Transformer%2520%252B%252B%253A%2520End-to-End%2520Full-Page%2520Optical%2520Music%250A%2520%2520Recognition%2520for%2520Pianoform%2520Sheet%2520Music%26entry.906535625%3DAntonio%2520R%25C3%25ADos-Vila%2520and%2520Jorge%2520Calvo-Zaragoza%2520and%2520David%2520Rizo%2520and%2520Thierry%2520Paquet%26entry.1292438233%3D%2520%2520Optical%2520Music%2520Recognition%2520is%2520a%2520field%2520that%2520has%2520progressed%2520significantly%252C%250Abringing%2520accurate%2520systems%2520that%2520transcribe%2520effectively%2520music%2520scores%2520into%2520digital%250Aformats.%2520Despite%2520this%252C%2520there%2520are%2520still%2520several%2520limitations%2520that%2520hinder%2520OMR%2520from%250Aachieving%2520its%2520full%2520potential.%2520Specifically%252C%2520state%2520of%2520the%2520art%2520OMR%2520still%2520depends%250Aon%2520multi-stage%2520pipelines%2520for%2520performing%2520full-page%2520transcription%252C%2520as%2520well%2520as%2520it%250Ahas%2520only%2520been%2520demonstrated%2520in%2520monophonic%2520cases%252C%2520leaving%2520behind%2520very%2520relevant%250Aengravings.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520Sheet%2520Music%2520Transformer%252B%252B%252C%2520an%250Aend-to-end%2520model%2520that%2520is%2520able%2520to%2520transcribe%2520full-page%2520polyphonic%2520music%2520scores%250Awithout%2520the%2520need%2520of%2520a%2520previous%2520Layout%2520Analysis%2520step.%2520This%2520is%2520done%2520thanks%2520to%2520an%250Aextensive%2520curriculum%2520learning-based%2520pretraining%2520with%2520synthetic%2520data%2520generation.%250AWe%2520conduct%2520several%2520experiments%2520on%2520a%2520full-page%2520extension%2520of%2520a%2520public%2520polyphonic%250Atranscription%2520dataset.%2520The%2520experimental%2520outcomes%2520confirm%2520that%2520the%2520model%2520is%250Acompetent%2520at%2520transcribing%2520full-page%2520pianoform%2520scores%252C%2520marking%2520a%2520noteworthy%250Amilestone%2520in%2520end-to-end%2520OMR%2520transcription.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sheet%20Music%20Transformer%20%2B%2B%3A%20End-to-End%20Full-Page%20Optical%20Music%0A%20%20Recognition%20for%20Pianoform%20Sheet%20Music&entry.906535625=Antonio%20R%C3%ADos-Vila%20and%20Jorge%20Calvo-Zaragoza%20and%20David%20Rizo%20and%20Thierry%20Paquet&entry.1292438233=%20%20Optical%20Music%20Recognition%20is%20a%20field%20that%20has%20progressed%20significantly%2C%0Abringing%20accurate%20systems%20that%20transcribe%20effectively%20music%20scores%20into%20digital%0Aformats.%20Despite%20this%2C%20there%20are%20still%20several%20limitations%20that%20hinder%20OMR%20from%0Aachieving%20its%20full%20potential.%20Specifically%2C%20state%20of%20the%20art%20OMR%20still%20depends%0Aon%20multi-stage%20pipelines%20for%20performing%20full-page%20transcription%2C%20as%20well%20as%20it%0Ahas%20only%20been%20demonstrated%20in%20monophonic%20cases%2C%20leaving%20behind%20very%20relevant%0Aengravings.%20In%20this%20work%2C%20we%20present%20the%20Sheet%20Music%20Transformer%2B%2B%2C%20an%0Aend-to-end%20model%20that%20is%20able%20to%20transcribe%20full-page%20polyphonic%20music%20scores%0Awithout%20the%20need%20of%20a%20previous%20Layout%20Analysis%20step.%20This%20is%20done%20thanks%20to%20an%0Aextensive%20curriculum%20learning-based%20pretraining%20with%20synthetic%20data%20generation.%0AWe%20conduct%20several%20experiments%20on%20a%20full-page%20extension%20of%20a%20public%20polyphonic%0Atranscription%20dataset.%20The%20experimental%20outcomes%20confirm%20that%20the%20model%20is%0Acompetent%20at%20transcribing%20full-page%20pianoform%20scores%2C%20marking%20a%20noteworthy%0Amilestone%20in%20end-to-end%20OMR%20transcription.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12105v1&entry.124074799=Read"},
{"title": "Accelerating Relative Entropy Coding with Space Partitioning", "author": "Jiajun He and Gergely Flamich and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Relative entropy coding (REC) algorithms encode a random sample following a\ntarget distribution $Q$, using a coding distribution $P$ shared between the\nsender and receiver. Sadly, general REC algorithms suffer from prohibitive\nencoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster\nalgorithms are limited to very specific settings. This work addresses this\nissue by introducing a REC scheme utilizing space partitioning to reduce\nruntime in practical scenarios. We provide theoretical analyses of our method\nand demonstrate its effectiveness with both toy examples and practical\napplications. Notably, our method successfully handles REC tasks with\n$D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and\nreduces the compression rate by approximately 5-15\\% in VAE-based lossless\ncompression on MNIST and INR-based lossy compression on CIFAR-10 compared to\nprevious methods, significantly improving the practicality of REC for neural\ncompression.\n", "link": "http://arxiv.org/abs/2405.12203v1", "date": "2024-05-20", "relevancy": 1.8111, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4557}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.454}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Relative%20Entropy%20Coding%20with%20Space%20Partitioning&body=Title%3A%20Accelerating%20Relative%20Entropy%20Coding%20with%20Space%20Partitioning%0AAuthor%3A%20Jiajun%20He%20and%20Gergely%20Flamich%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Relative%20entropy%20coding%20%28REC%29%20algorithms%20encode%20a%20random%20sample%20following%20a%0Atarget%20distribution%20%24Q%24%2C%20using%20a%20coding%20distribution%20%24P%24%20shared%20between%20the%0Asender%20and%20receiver.%20Sadly%2C%20general%20REC%20algorithms%20suffer%20from%20prohibitive%0Aencoding%20times%2C%20at%20least%20on%20the%20order%20of%20%242%5E%7BD_%7B%5Ctext%7BKL%7D%7D%5BQ%7C%7CP%5D%7D%24%2C%20and%20faster%0Aalgorithms%20are%20limited%20to%20very%20specific%20settings.%20This%20work%20addresses%20this%0Aissue%20by%20introducing%20a%20REC%20scheme%20utilizing%20space%20partitioning%20to%20reduce%0Aruntime%20in%20practical%20scenarios.%20We%20provide%20theoretical%20analyses%20of%20our%20method%0Aand%20demonstrate%20its%20effectiveness%20with%20both%20toy%20examples%20and%20practical%0Aapplications.%20Notably%2C%20our%20method%20successfully%20handles%20REC%20tasks%20with%0A%24D_%7B%5Ctext%7BKL%7D%7D%5BQ%7C%7CP%5D%24%20about%20three%20times%20what%20previous%20methods%20can%20manage%20and%0Areduces%20the%20compression%20rate%20by%20approximately%205-15%5C%25%20in%20VAE-based%20lossless%0Acompression%20on%20MNIST%20and%20INR-based%20lossy%20compression%20on%20CIFAR-10%20compared%20to%0Aprevious%20methods%2C%20significantly%20improving%20the%20practicality%20of%20REC%20for%20neural%0Acompression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Relative%2520Entropy%2520Coding%2520with%2520Space%2520Partitioning%26entry.906535625%3DJiajun%2520He%2520and%2520Gergely%2520Flamich%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Relative%2520entropy%2520coding%2520%2528REC%2529%2520algorithms%2520encode%2520a%2520random%2520sample%2520following%2520a%250Atarget%2520distribution%2520%2524Q%2524%252C%2520using%2520a%2520coding%2520distribution%2520%2524P%2524%2520shared%2520between%2520the%250Asender%2520and%2520receiver.%2520Sadly%252C%2520general%2520REC%2520algorithms%2520suffer%2520from%2520prohibitive%250Aencoding%2520times%252C%2520at%2520least%2520on%2520the%2520order%2520of%2520%25242%255E%257BD_%257B%255Ctext%257BKL%257D%257D%255BQ%257C%257CP%255D%257D%2524%252C%2520and%2520faster%250Aalgorithms%2520are%2520limited%2520to%2520very%2520specific%2520settings.%2520This%2520work%2520addresses%2520this%250Aissue%2520by%2520introducing%2520a%2520REC%2520scheme%2520utilizing%2520space%2520partitioning%2520to%2520reduce%250Aruntime%2520in%2520practical%2520scenarios.%2520We%2520provide%2520theoretical%2520analyses%2520of%2520our%2520method%250Aand%2520demonstrate%2520its%2520effectiveness%2520with%2520both%2520toy%2520examples%2520and%2520practical%250Aapplications.%2520Notably%252C%2520our%2520method%2520successfully%2520handles%2520REC%2520tasks%2520with%250A%2524D_%257B%255Ctext%257BKL%257D%257D%255BQ%257C%257CP%255D%2524%2520about%2520three%2520times%2520what%2520previous%2520methods%2520can%2520manage%2520and%250Areduces%2520the%2520compression%2520rate%2520by%2520approximately%25205-15%255C%2525%2520in%2520VAE-based%2520lossless%250Acompression%2520on%2520MNIST%2520and%2520INR-based%2520lossy%2520compression%2520on%2520CIFAR-10%2520compared%2520to%250Aprevious%2520methods%252C%2520significantly%2520improving%2520the%2520practicality%2520of%2520REC%2520for%2520neural%250Acompression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Relative%20Entropy%20Coding%20with%20Space%20Partitioning&entry.906535625=Jiajun%20He%20and%20Gergely%20Flamich%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Relative%20entropy%20coding%20%28REC%29%20algorithms%20encode%20a%20random%20sample%20following%20a%0Atarget%20distribution%20%24Q%24%2C%20using%20a%20coding%20distribution%20%24P%24%20shared%20between%20the%0Asender%20and%20receiver.%20Sadly%2C%20general%20REC%20algorithms%20suffer%20from%20prohibitive%0Aencoding%20times%2C%20at%20least%20on%20the%20order%20of%20%242%5E%7BD_%7B%5Ctext%7BKL%7D%7D%5BQ%7C%7CP%5D%7D%24%2C%20and%20faster%0Aalgorithms%20are%20limited%20to%20very%20specific%20settings.%20This%20work%20addresses%20this%0Aissue%20by%20introducing%20a%20REC%20scheme%20utilizing%20space%20partitioning%20to%20reduce%0Aruntime%20in%20practical%20scenarios.%20We%20provide%20theoretical%20analyses%20of%20our%20method%0Aand%20demonstrate%20its%20effectiveness%20with%20both%20toy%20examples%20and%20practical%0Aapplications.%20Notably%2C%20our%20method%20successfully%20handles%20REC%20tasks%20with%0A%24D_%7B%5Ctext%7BKL%7D%7D%5BQ%7C%7CP%5D%24%20about%20three%20times%20what%20previous%20methods%20can%20manage%20and%0Areduces%20the%20compression%20rate%20by%20approximately%205-15%5C%25%20in%20VAE-based%20lossless%0Acompression%20on%20MNIST%20and%20INR-based%20lossy%20compression%20on%20CIFAR-10%20compared%20to%0Aprevious%20methods%2C%20significantly%20improving%20the%20practicality%20of%20REC%20for%20neural%0Acompression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12203v1&entry.124074799=Read"},
{"title": "SM-DTW: Stability Modulated Dynamic Time Warping for signature\n  verification", "author": "Antonio Parziale and Moises Diaz and Miguel A. Ferrer and Angelo Marcelli", "abstract": "  Building upon findings in computational model of handwriting learning and\nexecution, we introduce the concept of stability to explain the difference\nbetween the actual movements performed during multiple execution of the\nsubject's signature, and conjecture that the most stable parts of the signature\nshould play a paramount role in evaluating the similarity between a questioned\nsignature and the reference ones during signature verification. We then\nintroduce the Stability Modulated Dynamic Time Warping algorithm for\nincorporating the stability regions, i.e. the most similar parts between two\nsignatures, into the distance measure between a pair of signatures computed by\nthe Dynamic Time Warping for signature verification. Experiments were conducted\non two datasets largely adopted for performance evaluation. Experimental\nresults show that the proposed algorithm improves the performance of the\nbaseline system and compares favourably with other top performing signature\nverification systems.\n", "link": "http://arxiv.org/abs/2405.11978v1", "date": "2024-05-20", "relevancy": 1.7945, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4605}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4443}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SM-DTW%3A%20Stability%20Modulated%20Dynamic%20Time%20Warping%20for%20signature%0A%20%20verification&body=Title%3A%20SM-DTW%3A%20Stability%20Modulated%20Dynamic%20Time%20Warping%20for%20signature%0A%20%20verification%0AAuthor%3A%20Antonio%20Parziale%20and%20Moises%20Diaz%20and%20Miguel%20A.%20Ferrer%20and%20Angelo%20Marcelli%0AAbstract%3A%20%20%20Building%20upon%20findings%20in%20computational%20model%20of%20handwriting%20learning%20and%0Aexecution%2C%20we%20introduce%20the%20concept%20of%20stability%20to%20explain%20the%20difference%0Abetween%20the%20actual%20movements%20performed%20during%20multiple%20execution%20of%20the%0Asubject%27s%20signature%2C%20and%20conjecture%20that%20the%20most%20stable%20parts%20of%20the%20signature%0Ashould%20play%20a%20paramount%20role%20in%20evaluating%20the%20similarity%20between%20a%20questioned%0Asignature%20and%20the%20reference%20ones%20during%20signature%20verification.%20We%20then%0Aintroduce%20the%20Stability%20Modulated%20Dynamic%20Time%20Warping%20algorithm%20for%0Aincorporating%20the%20stability%20regions%2C%20i.e.%20the%20most%20similar%20parts%20between%20two%0Asignatures%2C%20into%20the%20distance%20measure%20between%20a%20pair%20of%20signatures%20computed%20by%0Athe%20Dynamic%20Time%20Warping%20for%20signature%20verification.%20Experiments%20were%20conducted%0Aon%20two%20datasets%20largely%20adopted%20for%20performance%20evaluation.%20Experimental%0Aresults%20show%20that%20the%20proposed%20algorithm%20improves%20the%20performance%20of%20the%0Abaseline%20system%20and%20compares%20favourably%20with%20other%20top%20performing%20signature%0Averification%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSM-DTW%253A%2520Stability%2520Modulated%2520Dynamic%2520Time%2520Warping%2520for%2520signature%250A%2520%2520verification%26entry.906535625%3DAntonio%2520Parziale%2520and%2520Moises%2520Diaz%2520and%2520Miguel%2520A.%2520Ferrer%2520and%2520Angelo%2520Marcelli%26entry.1292438233%3D%2520%2520Building%2520upon%2520findings%2520in%2520computational%2520model%2520of%2520handwriting%2520learning%2520and%250Aexecution%252C%2520we%2520introduce%2520the%2520concept%2520of%2520stability%2520to%2520explain%2520the%2520difference%250Abetween%2520the%2520actual%2520movements%2520performed%2520during%2520multiple%2520execution%2520of%2520the%250Asubject%2527s%2520signature%252C%2520and%2520conjecture%2520that%2520the%2520most%2520stable%2520parts%2520of%2520the%2520signature%250Ashould%2520play%2520a%2520paramount%2520role%2520in%2520evaluating%2520the%2520similarity%2520between%2520a%2520questioned%250Asignature%2520and%2520the%2520reference%2520ones%2520during%2520signature%2520verification.%2520We%2520then%250Aintroduce%2520the%2520Stability%2520Modulated%2520Dynamic%2520Time%2520Warping%2520algorithm%2520for%250Aincorporating%2520the%2520stability%2520regions%252C%2520i.e.%2520the%2520most%2520similar%2520parts%2520between%2520two%250Asignatures%252C%2520into%2520the%2520distance%2520measure%2520between%2520a%2520pair%2520of%2520signatures%2520computed%2520by%250Athe%2520Dynamic%2520Time%2520Warping%2520for%2520signature%2520verification.%2520Experiments%2520were%2520conducted%250Aon%2520two%2520datasets%2520largely%2520adopted%2520for%2520performance%2520evaluation.%2520Experimental%250Aresults%2520show%2520that%2520the%2520proposed%2520algorithm%2520improves%2520the%2520performance%2520of%2520the%250Abaseline%2520system%2520and%2520compares%2520favourably%2520with%2520other%2520top%2520performing%2520signature%250Averification%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SM-DTW%3A%20Stability%20Modulated%20Dynamic%20Time%20Warping%20for%20signature%0A%20%20verification&entry.906535625=Antonio%20Parziale%20and%20Moises%20Diaz%20and%20Miguel%20A.%20Ferrer%20and%20Angelo%20Marcelli&entry.1292438233=%20%20Building%20upon%20findings%20in%20computational%20model%20of%20handwriting%20learning%20and%0Aexecution%2C%20we%20introduce%20the%20concept%20of%20stability%20to%20explain%20the%20difference%0Abetween%20the%20actual%20movements%20performed%20during%20multiple%20execution%20of%20the%0Asubject%27s%20signature%2C%20and%20conjecture%20that%20the%20most%20stable%20parts%20of%20the%20signature%0Ashould%20play%20a%20paramount%20role%20in%20evaluating%20the%20similarity%20between%20a%20questioned%0Asignature%20and%20the%20reference%20ones%20during%20signature%20verification.%20We%20then%0Aintroduce%20the%20Stability%20Modulated%20Dynamic%20Time%20Warping%20algorithm%20for%0Aincorporating%20the%20stability%20regions%2C%20i.e.%20the%20most%20similar%20parts%20between%20two%0Asignatures%2C%20into%20the%20distance%20measure%20between%20a%20pair%20of%20signatures%20computed%20by%0Athe%20Dynamic%20Time%20Warping%20for%20signature%20verification.%20Experiments%20were%20conducted%0Aon%20two%20datasets%20largely%20adopted%20for%20performance%20evaluation.%20Experimental%0Aresults%20show%20that%20the%20proposed%20algorithm%20improves%20the%20performance%20of%20the%0Abaseline%20system%20and%20compares%20favourably%20with%20other%20top%20performing%20signature%0Averification%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11978v1&entry.124074799=Read"},
{"title": "Effective Clustering on Large Attributed Bipartite Graphs", "author": "Renchi Yang and Yidu Wu and Xiaoyang Lin and Qichen Wang and Tsz Nam Chan and Jieming Shi", "abstract": "  Attributed bipartite graphs (ABGs) are an expressive data model for\ndescribing the interactions between two sets of heterogeneous nodes that are\nassociated with rich attributes, such as customer-product purchase networks and\nauthor-paper authorship graphs. Partitioning the target node set in such graphs\ninto k disjoint clusters (referred to as k-ABGC) finds widespread use in\nvarious domains, including social network analysis, recommendation systems,\ninformation retrieval, and bioinformatics. However, the majority of existing\nsolutions towards k-ABGC either overlook attribute information or fail to\ncapture bipartite graph structures accurately, engendering severely compromised\nresult quality. The severity of these issues is accentuated in real ABGs, which\noften encompass millions of nodes and a sheer volume of attribute data,\nrendering effective k-ABGC over such graphs highly challenging.\n  In this paper, we propose TPO, an effective and efficient approach to k-ABGC\nthat achieves superb clustering performance on multiple real datasets. TPO\nobtains high clustering quality through two major contributions: (i) a novel\nformulation and transformation of the k-ABGC problem based on multi-scale\nattribute affinity specialized for capturing attribute affinities between nodes\nwith the consideration of their multi-hop connections in ABGs, and (ii) a\nhighly efficient solver that includes a suite of carefully-crafted\noptimizations for sidestepping explicit affinity matrix construction and\nfacilitating faster convergence. Extensive experiments, comparing TPO against\n19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO\nmeasured against ground-truth labels. Moreover, compared to the state of the\narts, TPO is often more than 40x faster over both small and large ABGs.\n", "link": "http://arxiv.org/abs/2405.11922v1", "date": "2024-05-20", "relevancy": 1.7921, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4557}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4427}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Clustering%20on%20Large%20Attributed%20Bipartite%20Graphs&body=Title%3A%20Effective%20Clustering%20on%20Large%20Attributed%20Bipartite%20Graphs%0AAuthor%3A%20Renchi%20Yang%20and%20Yidu%20Wu%20and%20Xiaoyang%20Lin%20and%20Qichen%20Wang%20and%20Tsz%20Nam%20Chan%20and%20Jieming%20Shi%0AAbstract%3A%20%20%20Attributed%20bipartite%20graphs%20%28ABGs%29%20are%20an%20expressive%20data%20model%20for%0Adescribing%20the%20interactions%20between%20two%20sets%20of%20heterogeneous%20nodes%20that%20are%0Aassociated%20with%20rich%20attributes%2C%20such%20as%20customer-product%20purchase%20networks%20and%0Aauthor-paper%20authorship%20graphs.%20Partitioning%20the%20target%20node%20set%20in%20such%20graphs%0Ainto%20k%20disjoint%20clusters%20%28referred%20to%20as%20k-ABGC%29%20finds%20widespread%20use%20in%0Avarious%20domains%2C%20including%20social%20network%20analysis%2C%20recommendation%20systems%2C%0Ainformation%20retrieval%2C%20and%20bioinformatics.%20However%2C%20the%20majority%20of%20existing%0Asolutions%20towards%20k-ABGC%20either%20overlook%20attribute%20information%20or%20fail%20to%0Acapture%20bipartite%20graph%20structures%20accurately%2C%20engendering%20severely%20compromised%0Aresult%20quality.%20The%20severity%20of%20these%20issues%20is%20accentuated%20in%20real%20ABGs%2C%20which%0Aoften%20encompass%20millions%20of%20nodes%20and%20a%20sheer%20volume%20of%20attribute%20data%2C%0Arendering%20effective%20k-ABGC%20over%20such%20graphs%20highly%20challenging.%0A%20%20In%20this%20paper%2C%20we%20propose%20TPO%2C%20an%20effective%20and%20efficient%20approach%20to%20k-ABGC%0Athat%20achieves%20superb%20clustering%20performance%20on%20multiple%20real%20datasets.%20TPO%0Aobtains%20high%20clustering%20quality%20through%20two%20major%20contributions%3A%20%28i%29%20a%20novel%0Aformulation%20and%20transformation%20of%20the%20k-ABGC%20problem%20based%20on%20multi-scale%0Aattribute%20affinity%20specialized%20for%20capturing%20attribute%20affinities%20between%20nodes%0Awith%20the%20consideration%20of%20their%20multi-hop%20connections%20in%20ABGs%2C%20and%20%28ii%29%20a%0Ahighly%20efficient%20solver%20that%20includes%20a%20suite%20of%20carefully-crafted%0Aoptimizations%20for%20sidestepping%20explicit%20affinity%20matrix%20construction%20and%0Afacilitating%20faster%20convergence.%20Extensive%20experiments%2C%20comparing%20TPO%20against%0A19%20baselines%20over%205%20real%20ABGs%2C%20showcase%20the%20superior%20clustering%20quality%20of%20TPO%0Ameasured%20against%20ground-truth%20labels.%20Moreover%2C%20compared%20to%20the%20state%20of%20the%0Aarts%2C%20TPO%20is%20often%20more%20than%2040x%20faster%20over%20both%20small%20and%20large%20ABGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Clustering%2520on%2520Large%2520Attributed%2520Bipartite%2520Graphs%26entry.906535625%3DRenchi%2520Yang%2520and%2520Yidu%2520Wu%2520and%2520Xiaoyang%2520Lin%2520and%2520Qichen%2520Wang%2520and%2520Tsz%2520Nam%2520Chan%2520and%2520Jieming%2520Shi%26entry.1292438233%3D%2520%2520Attributed%2520bipartite%2520graphs%2520%2528ABGs%2529%2520are%2520an%2520expressive%2520data%2520model%2520for%250Adescribing%2520the%2520interactions%2520between%2520two%2520sets%2520of%2520heterogeneous%2520nodes%2520that%2520are%250Aassociated%2520with%2520rich%2520attributes%252C%2520such%2520as%2520customer-product%2520purchase%2520networks%2520and%250Aauthor-paper%2520authorship%2520graphs.%2520Partitioning%2520the%2520target%2520node%2520set%2520in%2520such%2520graphs%250Ainto%2520k%2520disjoint%2520clusters%2520%2528referred%2520to%2520as%2520k-ABGC%2529%2520finds%2520widespread%2520use%2520in%250Avarious%2520domains%252C%2520including%2520social%2520network%2520analysis%252C%2520recommendation%2520systems%252C%250Ainformation%2520retrieval%252C%2520and%2520bioinformatics.%2520However%252C%2520the%2520majority%2520of%2520existing%250Asolutions%2520towards%2520k-ABGC%2520either%2520overlook%2520attribute%2520information%2520or%2520fail%2520to%250Acapture%2520bipartite%2520graph%2520structures%2520accurately%252C%2520engendering%2520severely%2520compromised%250Aresult%2520quality.%2520The%2520severity%2520of%2520these%2520issues%2520is%2520accentuated%2520in%2520real%2520ABGs%252C%2520which%250Aoften%2520encompass%2520millions%2520of%2520nodes%2520and%2520a%2520sheer%2520volume%2520of%2520attribute%2520data%252C%250Arendering%2520effective%2520k-ABGC%2520over%2520such%2520graphs%2520highly%2520challenging.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520TPO%252C%2520an%2520effective%2520and%2520efficient%2520approach%2520to%2520k-ABGC%250Athat%2520achieves%2520superb%2520clustering%2520performance%2520on%2520multiple%2520real%2520datasets.%2520TPO%250Aobtains%2520high%2520clustering%2520quality%2520through%2520two%2520major%2520contributions%253A%2520%2528i%2529%2520a%2520novel%250Aformulation%2520and%2520transformation%2520of%2520the%2520k-ABGC%2520problem%2520based%2520on%2520multi-scale%250Aattribute%2520affinity%2520specialized%2520for%2520capturing%2520attribute%2520affinities%2520between%2520nodes%250Awith%2520the%2520consideration%2520of%2520their%2520multi-hop%2520connections%2520in%2520ABGs%252C%2520and%2520%2528ii%2529%2520a%250Ahighly%2520efficient%2520solver%2520that%2520includes%2520a%2520suite%2520of%2520carefully-crafted%250Aoptimizations%2520for%2520sidestepping%2520explicit%2520affinity%2520matrix%2520construction%2520and%250Afacilitating%2520faster%2520convergence.%2520Extensive%2520experiments%252C%2520comparing%2520TPO%2520against%250A19%2520baselines%2520over%25205%2520real%2520ABGs%252C%2520showcase%2520the%2520superior%2520clustering%2520quality%2520of%2520TPO%250Ameasured%2520against%2520ground-truth%2520labels.%2520Moreover%252C%2520compared%2520to%2520the%2520state%2520of%2520the%250Aarts%252C%2520TPO%2520is%2520often%2520more%2520than%252040x%2520faster%2520over%2520both%2520small%2520and%2520large%2520ABGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Clustering%20on%20Large%20Attributed%20Bipartite%20Graphs&entry.906535625=Renchi%20Yang%20and%20Yidu%20Wu%20and%20Xiaoyang%20Lin%20and%20Qichen%20Wang%20and%20Tsz%20Nam%20Chan%20and%20Jieming%20Shi&entry.1292438233=%20%20Attributed%20bipartite%20graphs%20%28ABGs%29%20are%20an%20expressive%20data%20model%20for%0Adescribing%20the%20interactions%20between%20two%20sets%20of%20heterogeneous%20nodes%20that%20are%0Aassociated%20with%20rich%20attributes%2C%20such%20as%20customer-product%20purchase%20networks%20and%0Aauthor-paper%20authorship%20graphs.%20Partitioning%20the%20target%20node%20set%20in%20such%20graphs%0Ainto%20k%20disjoint%20clusters%20%28referred%20to%20as%20k-ABGC%29%20finds%20widespread%20use%20in%0Avarious%20domains%2C%20including%20social%20network%20analysis%2C%20recommendation%20systems%2C%0Ainformation%20retrieval%2C%20and%20bioinformatics.%20However%2C%20the%20majority%20of%20existing%0Asolutions%20towards%20k-ABGC%20either%20overlook%20attribute%20information%20or%20fail%20to%0Acapture%20bipartite%20graph%20structures%20accurately%2C%20engendering%20severely%20compromised%0Aresult%20quality.%20The%20severity%20of%20these%20issues%20is%20accentuated%20in%20real%20ABGs%2C%20which%0Aoften%20encompass%20millions%20of%20nodes%20and%20a%20sheer%20volume%20of%20attribute%20data%2C%0Arendering%20effective%20k-ABGC%20over%20such%20graphs%20highly%20challenging.%0A%20%20In%20this%20paper%2C%20we%20propose%20TPO%2C%20an%20effective%20and%20efficient%20approach%20to%20k-ABGC%0Athat%20achieves%20superb%20clustering%20performance%20on%20multiple%20real%20datasets.%20TPO%0Aobtains%20high%20clustering%20quality%20through%20two%20major%20contributions%3A%20%28i%29%20a%20novel%0Aformulation%20and%20transformation%20of%20the%20k-ABGC%20problem%20based%20on%20multi-scale%0Aattribute%20affinity%20specialized%20for%20capturing%20attribute%20affinities%20between%20nodes%0Awith%20the%20consideration%20of%20their%20multi-hop%20connections%20in%20ABGs%2C%20and%20%28ii%29%20a%0Ahighly%20efficient%20solver%20that%20includes%20a%20suite%20of%20carefully-crafted%0Aoptimizations%20for%20sidestepping%20explicit%20affinity%20matrix%20construction%20and%0Afacilitating%20faster%20convergence.%20Extensive%20experiments%2C%20comparing%20TPO%20against%0A19%20baselines%20over%205%20real%20ABGs%2C%20showcase%20the%20superior%20clustering%20quality%20of%20TPO%0Ameasured%20against%20ground-truth%20labels.%20Moreover%2C%20compared%20to%20the%20state%20of%20the%0Aarts%2C%20TPO%20is%20often%20more%20than%2040x%20faster%20over%20both%20small%20and%20large%20ABGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11922v1&entry.124074799=Read"},
{"title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning", "author": "Ting Jiang and Shaohan Huang and Shengyue Luo and Zihan Zhang and Haizhen Huang and Furu Wei and Weiwei Deng and Feng Sun and Qi Zhang and Deqing Wang and Fuzhen Zhuang", "abstract": "  Low-rank adaptation is a popular parameter-efficient fine-tuning method for\nlarge language models. In this paper, we analyze the impact of low-rank\nupdating, as implemented in LoRA. Our findings suggest that the low-rank\nupdating mechanism may limit the ability of LLMs to effectively learn and\nmemorize new knowledge. Inspired by this observation, we propose a new method\ncalled MoRA, which employs a square matrix to achieve high-rank updating while\nmaintaining the same number of trainable parameters. To achieve it, we\nintroduce the corresponding non-parameter operators to reduce the input\ndimension and increase the output dimension for the square matrix. Furthermore,\nthese operators ensure that the weight can be merged back into LLMs, which\nmakes our method can be deployed like LoRA. We perform a comprehensive\nevaluation of our method across five tasks: instruction tuning, mathematical\nreasoning, continual pretraining, memory and pretraining. Our method\noutperforms LoRA on memory-intensive tasks and achieves comparable performance\non other tasks.\n", "link": "http://arxiv.org/abs/2405.12130v1", "date": "2024-05-20", "relevancy": 1.7902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRA%3A%20High-Rank%20Updating%20for%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20MoRA%3A%20High-Rank%20Updating%20for%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Ting%20Jiang%20and%20Shaohan%20Huang%20and%20Shengyue%20Luo%20and%20Zihan%20Zhang%20and%20Haizhen%20Huang%20and%20Furu%20Wei%20and%20Weiwei%20Deng%20and%20Feng%20Sun%20and%20Qi%20Zhang%20and%20Deqing%20Wang%20and%20Fuzhen%20Zhuang%0AAbstract%3A%20%20%20Low-rank%20adaptation%20is%20a%20popular%20parameter-efficient%20fine-tuning%20method%20for%0Alarge%20language%20models.%20In%20this%20paper%2C%20we%20analyze%20the%20impact%20of%20low-rank%0Aupdating%2C%20as%20implemented%20in%20LoRA.%20Our%20findings%20suggest%20that%20the%20low-rank%0Aupdating%20mechanism%20may%20limit%20the%20ability%20of%20LLMs%20to%20effectively%20learn%20and%0Amemorize%20new%20knowledge.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20new%20method%0Acalled%20MoRA%2C%20which%20employs%20a%20square%20matrix%20to%20achieve%20high-rank%20updating%20while%0Amaintaining%20the%20same%20number%20of%20trainable%20parameters.%20To%20achieve%20it%2C%20we%0Aintroduce%20the%20corresponding%20non-parameter%20operators%20to%20reduce%20the%20input%0Adimension%20and%20increase%20the%20output%20dimension%20for%20the%20square%20matrix.%20Furthermore%2C%0Athese%20operators%20ensure%20that%20the%20weight%20can%20be%20merged%20back%20into%20LLMs%2C%20which%0Amakes%20our%20method%20can%20be%20deployed%20like%20LoRA.%20We%20perform%20a%20comprehensive%0Aevaluation%20of%20our%20method%20across%20five%20tasks%3A%20instruction%20tuning%2C%20mathematical%0Areasoning%2C%20continual%20pretraining%2C%20memory%20and%20pretraining.%20Our%20method%0Aoutperforms%20LoRA%20on%20memory-intensive%20tasks%20and%20achieves%20comparable%20performance%0Aon%20other%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRA%253A%2520High-Rank%2520Updating%2520for%2520Parameter-Efficient%2520Fine-Tuning%26entry.906535625%3DTing%2520Jiang%2520and%2520Shaohan%2520Huang%2520and%2520Shengyue%2520Luo%2520and%2520Zihan%2520Zhang%2520and%2520Haizhen%2520Huang%2520and%2520Furu%2520Wei%2520and%2520Weiwei%2520Deng%2520and%2520Feng%2520Sun%2520and%2520Qi%2520Zhang%2520and%2520Deqing%2520Wang%2520and%2520Fuzhen%2520Zhuang%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520is%2520a%2520popular%2520parameter-efficient%2520fine-tuning%2520method%2520for%250Alarge%2520language%2520models.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520impact%2520of%2520low-rank%250Aupdating%252C%2520as%2520implemented%2520in%2520LoRA.%2520Our%2520findings%2520suggest%2520that%2520the%2520low-rank%250Aupdating%2520mechanism%2520may%2520limit%2520the%2520ability%2520of%2520LLMs%2520to%2520effectively%2520learn%2520and%250Amemorize%2520new%2520knowledge.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520new%2520method%250Acalled%2520MoRA%252C%2520which%2520employs%2520a%2520square%2520matrix%2520to%2520achieve%2520high-rank%2520updating%2520while%250Amaintaining%2520the%2520same%2520number%2520of%2520trainable%2520parameters.%2520To%2520achieve%2520it%252C%2520we%250Aintroduce%2520the%2520corresponding%2520non-parameter%2520operators%2520to%2520reduce%2520the%2520input%250Adimension%2520and%2520increase%2520the%2520output%2520dimension%2520for%2520the%2520square%2520matrix.%2520Furthermore%252C%250Athese%2520operators%2520ensure%2520that%2520the%2520weight%2520can%2520be%2520merged%2520back%2520into%2520LLMs%252C%2520which%250Amakes%2520our%2520method%2520can%2520be%2520deployed%2520like%2520LoRA.%2520We%2520perform%2520a%2520comprehensive%250Aevaluation%2520of%2520our%2520method%2520across%2520five%2520tasks%253A%2520instruction%2520tuning%252C%2520mathematical%250Areasoning%252C%2520continual%2520pretraining%252C%2520memory%2520and%2520pretraining.%2520Our%2520method%250Aoutperforms%2520LoRA%2520on%2520memory-intensive%2520tasks%2520and%2520achieves%2520comparable%2520performance%250Aon%2520other%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRA%3A%20High-Rank%20Updating%20for%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Ting%20Jiang%20and%20Shaohan%20Huang%20and%20Shengyue%20Luo%20and%20Zihan%20Zhang%20and%20Haizhen%20Huang%20and%20Furu%20Wei%20and%20Weiwei%20Deng%20and%20Feng%20Sun%20and%20Qi%20Zhang%20and%20Deqing%20Wang%20and%20Fuzhen%20Zhuang&entry.1292438233=%20%20Low-rank%20adaptation%20is%20a%20popular%20parameter-efficient%20fine-tuning%20method%20for%0Alarge%20language%20models.%20In%20this%20paper%2C%20we%20analyze%20the%20impact%20of%20low-rank%0Aupdating%2C%20as%20implemented%20in%20LoRA.%20Our%20findings%20suggest%20that%20the%20low-rank%0Aupdating%20mechanism%20may%20limit%20the%20ability%20of%20LLMs%20to%20effectively%20learn%20and%0Amemorize%20new%20knowledge.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20new%20method%0Acalled%20MoRA%2C%20which%20employs%20a%20square%20matrix%20to%20achieve%20high-rank%20updating%20while%0Amaintaining%20the%20same%20number%20of%20trainable%20parameters.%20To%20achieve%20it%2C%20we%0Aintroduce%20the%20corresponding%20non-parameter%20operators%20to%20reduce%20the%20input%0Adimension%20and%20increase%20the%20output%20dimension%20for%20the%20square%20matrix.%20Furthermore%2C%0Athese%20operators%20ensure%20that%20the%20weight%20can%20be%20merged%20back%20into%20LLMs%2C%20which%0Amakes%20our%20method%20can%20be%20deployed%20like%20LoRA.%20We%20perform%20a%20comprehensive%0Aevaluation%20of%20our%20method%20across%20five%20tasks%3A%20instruction%20tuning%2C%20mathematical%0Areasoning%2C%20continual%20pretraining%2C%20memory%20and%20pretraining.%20Our%20method%0Aoutperforms%20LoRA%20on%20memory-intensive%20tasks%20and%20achieves%20comparable%20performance%0Aon%20other%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12130v1&entry.124074799=Read"},
{"title": "Optimistic Query Routing in Clustering-based Approximate Maximum Inner\n  Product Search", "author": "Sebastian Bruch and Aditya Krishnan and Franco Maria Nardini", "abstract": "  Clustering-based nearest neighbor search is a simple yet effective method in\nwhich data points are partitioned into geometric shards to form an index, and\nonly a few shards are searched during query processing to find an approximate\nset of top-$k$ vectors. Even though the search efficacy is heavily influenced\nby the algorithm that identifies the set of shards to probe, it has received\nlittle attention in the literature. This work attempts to bridge that gap by\nstudying the problem of routing in clustering-based maximum inner product\nsearch (MIPS). We begin by unpacking existing routing protocols and notice the\nsurprising contribution of optimism. We then take a page from the sequential\ndecision making literature and formalize that insight following the principle\nof ``optimism in the face of uncertainty.'' In particular, we present a new\nframework that incorporates the moments of the distribution of inner products\nwithin each shard to optimistically estimate the maximum inner product. We then\npresent a simple instance of our algorithm that uses only the first two moments\nto reach the same accuracy as state-of-the-art routers such as \\scann by\nprobing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our\nalgorithm is also space-efficient: we design a sketch of the second moment\nwhose size is independent of the number of points and in practice requires\nstoring only $O(1)$ additional vectors per shard.\n", "link": "http://arxiv.org/abs/2405.12207v1", "date": "2024-05-20", "relevancy": 1.7484, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4494}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Query%20Routing%20in%20Clustering-based%20Approximate%20Maximum%20Inner%0A%20%20Product%20Search&body=Title%3A%20Optimistic%20Query%20Routing%20in%20Clustering-based%20Approximate%20Maximum%20Inner%0A%20%20Product%20Search%0AAuthor%3A%20Sebastian%20Bruch%20and%20Aditya%20Krishnan%20and%20Franco%20Maria%20Nardini%0AAbstract%3A%20%20%20Clustering-based%20nearest%20neighbor%20search%20is%20a%20simple%20yet%20effective%20method%20in%0Awhich%20data%20points%20are%20partitioned%20into%20geometric%20shards%20to%20form%20an%20index%2C%20and%0Aonly%20a%20few%20shards%20are%20searched%20during%20query%20processing%20to%20find%20an%20approximate%0Aset%20of%20top-%24k%24%20vectors.%20Even%20though%20the%20search%20efficacy%20is%20heavily%20influenced%0Aby%20the%20algorithm%20that%20identifies%20the%20set%20of%20shards%20to%20probe%2C%20it%20has%20received%0Alittle%20attention%20in%20the%20literature.%20This%20work%20attempts%20to%20bridge%20that%20gap%20by%0Astudying%20the%20problem%20of%20routing%20in%20clustering-based%20maximum%20inner%20product%0Asearch%20%28MIPS%29.%20We%20begin%20by%20unpacking%20existing%20routing%20protocols%20and%20notice%20the%0Asurprising%20contribution%20of%20optimism.%20We%20then%20take%20a%20page%20from%20the%20sequential%0Adecision%20making%20literature%20and%20formalize%20that%20insight%20following%20the%20principle%0Aof%20%60%60optimism%20in%20the%20face%20of%20uncertainty.%27%27%20In%20particular%2C%20we%20present%20a%20new%0Aframework%20that%20incorporates%20the%20moments%20of%20the%20distribution%20of%20inner%20products%0Awithin%20each%20shard%20to%20optimistically%20estimate%20the%20maximum%20inner%20product.%20We%20then%0Apresent%20a%20simple%20instance%20of%20our%20algorithm%20that%20uses%20only%20the%20first%20two%20moments%0Ato%20reach%20the%20same%20accuracy%20as%20state-of-the-art%20routers%20such%20as%20%5Cscann%20by%0Aprobing%20up%20to%20%2450%25%24%20fewer%20points%20on%20a%20suite%20of%20benchmark%20MIPS%20datasets.%20Our%0Aalgorithm%20is%20also%20space-efficient%3A%20we%20design%20a%20sketch%20of%20the%20second%20moment%0Awhose%20size%20is%20independent%20of%20the%20number%20of%20points%20and%20in%20practice%20requires%0Astoring%20only%20%24O%281%29%24%20additional%20vectors%20per%20shard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Query%2520Routing%2520in%2520Clustering-based%2520Approximate%2520Maximum%2520Inner%250A%2520%2520Product%2520Search%26entry.906535625%3DSebastian%2520Bruch%2520and%2520Aditya%2520Krishnan%2520and%2520Franco%2520Maria%2520Nardini%26entry.1292438233%3D%2520%2520Clustering-based%2520nearest%2520neighbor%2520search%2520is%2520a%2520simple%2520yet%2520effective%2520method%2520in%250Awhich%2520data%2520points%2520are%2520partitioned%2520into%2520geometric%2520shards%2520to%2520form%2520an%2520index%252C%2520and%250Aonly%2520a%2520few%2520shards%2520are%2520searched%2520during%2520query%2520processing%2520to%2520find%2520an%2520approximate%250Aset%2520of%2520top-%2524k%2524%2520vectors.%2520Even%2520though%2520the%2520search%2520efficacy%2520is%2520heavily%2520influenced%250Aby%2520the%2520algorithm%2520that%2520identifies%2520the%2520set%2520of%2520shards%2520to%2520probe%252C%2520it%2520has%2520received%250Alittle%2520attention%2520in%2520the%2520literature.%2520This%2520work%2520attempts%2520to%2520bridge%2520that%2520gap%2520by%250Astudying%2520the%2520problem%2520of%2520routing%2520in%2520clustering-based%2520maximum%2520inner%2520product%250Asearch%2520%2528MIPS%2529.%2520We%2520begin%2520by%2520unpacking%2520existing%2520routing%2520protocols%2520and%2520notice%2520the%250Asurprising%2520contribution%2520of%2520optimism.%2520We%2520then%2520take%2520a%2520page%2520from%2520the%2520sequential%250Adecision%2520making%2520literature%2520and%2520formalize%2520that%2520insight%2520following%2520the%2520principle%250Aof%2520%2560%2560optimism%2520in%2520the%2520face%2520of%2520uncertainty.%2527%2527%2520In%2520particular%252C%2520we%2520present%2520a%2520new%250Aframework%2520that%2520incorporates%2520the%2520moments%2520of%2520the%2520distribution%2520of%2520inner%2520products%250Awithin%2520each%2520shard%2520to%2520optimistically%2520estimate%2520the%2520maximum%2520inner%2520product.%2520We%2520then%250Apresent%2520a%2520simple%2520instance%2520of%2520our%2520algorithm%2520that%2520uses%2520only%2520the%2520first%2520two%2520moments%250Ato%2520reach%2520the%2520same%2520accuracy%2520as%2520state-of-the-art%2520routers%2520such%2520as%2520%255Cscann%2520by%250Aprobing%2520up%2520to%2520%252450%2525%2524%2520fewer%2520points%2520on%2520a%2520suite%2520of%2520benchmark%2520MIPS%2520datasets.%2520Our%250Aalgorithm%2520is%2520also%2520space-efficient%253A%2520we%2520design%2520a%2520sketch%2520of%2520the%2520second%2520moment%250Awhose%2520size%2520is%2520independent%2520of%2520the%2520number%2520of%2520points%2520and%2520in%2520practice%2520requires%250Astoring%2520only%2520%2524O%25281%2529%2524%2520additional%2520vectors%2520per%2520shard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Query%20Routing%20in%20Clustering-based%20Approximate%20Maximum%20Inner%0A%20%20Product%20Search&entry.906535625=Sebastian%20Bruch%20and%20Aditya%20Krishnan%20and%20Franco%20Maria%20Nardini&entry.1292438233=%20%20Clustering-based%20nearest%20neighbor%20search%20is%20a%20simple%20yet%20effective%20method%20in%0Awhich%20data%20points%20are%20partitioned%20into%20geometric%20shards%20to%20form%20an%20index%2C%20and%0Aonly%20a%20few%20shards%20are%20searched%20during%20query%20processing%20to%20find%20an%20approximate%0Aset%20of%20top-%24k%24%20vectors.%20Even%20though%20the%20search%20efficacy%20is%20heavily%20influenced%0Aby%20the%20algorithm%20that%20identifies%20the%20set%20of%20shards%20to%20probe%2C%20it%20has%20received%0Alittle%20attention%20in%20the%20literature.%20This%20work%20attempts%20to%20bridge%20that%20gap%20by%0Astudying%20the%20problem%20of%20routing%20in%20clustering-based%20maximum%20inner%20product%0Asearch%20%28MIPS%29.%20We%20begin%20by%20unpacking%20existing%20routing%20protocols%20and%20notice%20the%0Asurprising%20contribution%20of%20optimism.%20We%20then%20take%20a%20page%20from%20the%20sequential%0Adecision%20making%20literature%20and%20formalize%20that%20insight%20following%20the%20principle%0Aof%20%60%60optimism%20in%20the%20face%20of%20uncertainty.%27%27%20In%20particular%2C%20we%20present%20a%20new%0Aframework%20that%20incorporates%20the%20moments%20of%20the%20distribution%20of%20inner%20products%0Awithin%20each%20shard%20to%20optimistically%20estimate%20the%20maximum%20inner%20product.%20We%20then%0Apresent%20a%20simple%20instance%20of%20our%20algorithm%20that%20uses%20only%20the%20first%20two%20moments%0Ato%20reach%20the%20same%20accuracy%20as%20state-of-the-art%20routers%20such%20as%20%5Cscann%20by%0Aprobing%20up%20to%20%2450%25%24%20fewer%20points%20on%20a%20suite%20of%20benchmark%20MIPS%20datasets.%20Our%0Aalgorithm%20is%20also%20space-efficient%3A%20we%20design%20a%20sketch%20of%20the%20second%20moment%0Awhose%20size%20is%20independent%20of%20the%20number%20of%20points%20and%20in%20practice%20requires%0Astoring%20only%20%24O%281%29%24%20additional%20vectors%20per%20shard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12207v1&entry.124074799=Read"},
{"title": "A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer\n  using LSTM, BiLSTM, CNN, GRU, and GloVe", "author": "Sanad Aburass and Osama Dorgham and Jamil Al Shaqsi", "abstract": "  In our study, we introduce a novel hybrid ensemble model that synergistically\ncombines LSTM, BiLSTM, CNN, GRU, and GloVe embeddings for the classification of\ngene mutations in cancer. This model was rigorously tested using Kaggle's\nPersonalized Medicine: Redefining Cancer Treatment dataset, demonstrating\nexceptional performance across all evaluation metrics. Notably, our approach\nachieved a training accuracy of 80.6%, precision of 81.6%, recall of 80.6%, and\nan F1 score of 83.1%, alongside a significantly reduced Mean Squared Error\n(MSE) of 2.596. These results surpass those of advanced transformer models and\ntheir ensembles, showcasing our model's superior capability in handling the\ncomplexities of gene mutation classification. The accuracy and efficiency of\ngene mutation classification are paramount in the era of precision medicine,\nwhere tailored treatment plans based on individual genetic profiles can\ndramatically improve patient outcomes and save lives. Our model's remarkable\nperformance highlights its potential in enhancing the precision of cancer\ndiagnoses and treatments, thereby contributing significantly to the advancement\nof personalized healthcare.\n", "link": "http://arxiv.org/abs/2307.14361v3", "date": "2024-05-20", "relevancy": 1.7429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4447}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4351}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Machine%20Learning%20Model%20for%20Classifying%20Gene%20Mutations%20in%20Cancer%0A%20%20using%20LSTM%2C%20BiLSTM%2C%20CNN%2C%20GRU%2C%20and%20GloVe&body=Title%3A%20A%20Hybrid%20Machine%20Learning%20Model%20for%20Classifying%20Gene%20Mutations%20in%20Cancer%0A%20%20using%20LSTM%2C%20BiLSTM%2C%20CNN%2C%20GRU%2C%20and%20GloVe%0AAuthor%3A%20Sanad%20Aburass%20and%20Osama%20Dorgham%20and%20Jamil%20Al%20Shaqsi%0AAbstract%3A%20%20%20In%20our%20study%2C%20we%20introduce%20a%20novel%20hybrid%20ensemble%20model%20that%20synergistically%0Acombines%20LSTM%2C%20BiLSTM%2C%20CNN%2C%20GRU%2C%20and%20GloVe%20embeddings%20for%20the%20classification%20of%0Agene%20mutations%20in%20cancer.%20This%20model%20was%20rigorously%20tested%20using%20Kaggle%27s%0APersonalized%20Medicine%3A%20Redefining%20Cancer%20Treatment%20dataset%2C%20demonstrating%0Aexceptional%20performance%20across%20all%20evaluation%20metrics.%20Notably%2C%20our%20approach%0Aachieved%20a%20training%20accuracy%20of%2080.6%25%2C%20precision%20of%2081.6%25%2C%20recall%20of%2080.6%25%2C%20and%0Aan%20F1%20score%20of%2083.1%25%2C%20alongside%20a%20significantly%20reduced%20Mean%20Squared%20Error%0A%28MSE%29%20of%202.596.%20These%20results%20surpass%20those%20of%20advanced%20transformer%20models%20and%0Atheir%20ensembles%2C%20showcasing%20our%20model%27s%20superior%20capability%20in%20handling%20the%0Acomplexities%20of%20gene%20mutation%20classification.%20The%20accuracy%20and%20efficiency%20of%0Agene%20mutation%20classification%20are%20paramount%20in%20the%20era%20of%20precision%20medicine%2C%0Awhere%20tailored%20treatment%20plans%20based%20on%20individual%20genetic%20profiles%20can%0Adramatically%20improve%20patient%20outcomes%20and%20save%20lives.%20Our%20model%27s%20remarkable%0Aperformance%20highlights%20its%20potential%20in%20enhancing%20the%20precision%20of%20cancer%0Adiagnoses%20and%20treatments%2C%20thereby%20contributing%20significantly%20to%20the%20advancement%0Aof%20personalized%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14361v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Machine%2520Learning%2520Model%2520for%2520Classifying%2520Gene%2520Mutations%2520in%2520Cancer%250A%2520%2520using%2520LSTM%252C%2520BiLSTM%252C%2520CNN%252C%2520GRU%252C%2520and%2520GloVe%26entry.906535625%3DSanad%2520Aburass%2520and%2520Osama%2520Dorgham%2520and%2520Jamil%2520Al%2520Shaqsi%26entry.1292438233%3D%2520%2520In%2520our%2520study%252C%2520we%2520introduce%2520a%2520novel%2520hybrid%2520ensemble%2520model%2520that%2520synergistically%250Acombines%2520LSTM%252C%2520BiLSTM%252C%2520CNN%252C%2520GRU%252C%2520and%2520GloVe%2520embeddings%2520for%2520the%2520classification%2520of%250Agene%2520mutations%2520in%2520cancer.%2520This%2520model%2520was%2520rigorously%2520tested%2520using%2520Kaggle%2527s%250APersonalized%2520Medicine%253A%2520Redefining%2520Cancer%2520Treatment%2520dataset%252C%2520demonstrating%250Aexceptional%2520performance%2520across%2520all%2520evaluation%2520metrics.%2520Notably%252C%2520our%2520approach%250Aachieved%2520a%2520training%2520accuracy%2520of%252080.6%2525%252C%2520precision%2520of%252081.6%2525%252C%2520recall%2520of%252080.6%2525%252C%2520and%250Aan%2520F1%2520score%2520of%252083.1%2525%252C%2520alongside%2520a%2520significantly%2520reduced%2520Mean%2520Squared%2520Error%250A%2528MSE%2529%2520of%25202.596.%2520These%2520results%2520surpass%2520those%2520of%2520advanced%2520transformer%2520models%2520and%250Atheir%2520ensembles%252C%2520showcasing%2520our%2520model%2527s%2520superior%2520capability%2520in%2520handling%2520the%250Acomplexities%2520of%2520gene%2520mutation%2520classification.%2520The%2520accuracy%2520and%2520efficiency%2520of%250Agene%2520mutation%2520classification%2520are%2520paramount%2520in%2520the%2520era%2520of%2520precision%2520medicine%252C%250Awhere%2520tailored%2520treatment%2520plans%2520based%2520on%2520individual%2520genetic%2520profiles%2520can%250Adramatically%2520improve%2520patient%2520outcomes%2520and%2520save%2520lives.%2520Our%2520model%2527s%2520remarkable%250Aperformance%2520highlights%2520its%2520potential%2520in%2520enhancing%2520the%2520precision%2520of%2520cancer%250Adiagnoses%2520and%2520treatments%252C%2520thereby%2520contributing%2520significantly%2520to%2520the%2520advancement%250Aof%2520personalized%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14361v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Machine%20Learning%20Model%20for%20Classifying%20Gene%20Mutations%20in%20Cancer%0A%20%20using%20LSTM%2C%20BiLSTM%2C%20CNN%2C%20GRU%2C%20and%20GloVe&entry.906535625=Sanad%20Aburass%20and%20Osama%20Dorgham%20and%20Jamil%20Al%20Shaqsi&entry.1292438233=%20%20In%20our%20study%2C%20we%20introduce%20a%20novel%20hybrid%20ensemble%20model%20that%20synergistically%0Acombines%20LSTM%2C%20BiLSTM%2C%20CNN%2C%20GRU%2C%20and%20GloVe%20embeddings%20for%20the%20classification%20of%0Agene%20mutations%20in%20cancer.%20This%20model%20was%20rigorously%20tested%20using%20Kaggle%27s%0APersonalized%20Medicine%3A%20Redefining%20Cancer%20Treatment%20dataset%2C%20demonstrating%0Aexceptional%20performance%20across%20all%20evaluation%20metrics.%20Notably%2C%20our%20approach%0Aachieved%20a%20training%20accuracy%20of%2080.6%25%2C%20precision%20of%2081.6%25%2C%20recall%20of%2080.6%25%2C%20and%0Aan%20F1%20score%20of%2083.1%25%2C%20alongside%20a%20significantly%20reduced%20Mean%20Squared%20Error%0A%28MSE%29%20of%202.596.%20These%20results%20surpass%20those%20of%20advanced%20transformer%20models%20and%0Atheir%20ensembles%2C%20showcasing%20our%20model%27s%20superior%20capability%20in%20handling%20the%0Acomplexities%20of%20gene%20mutation%20classification.%20The%20accuracy%20and%20efficiency%20of%0Agene%20mutation%20classification%20are%20paramount%20in%20the%20era%20of%20precision%20medicine%2C%0Awhere%20tailored%20treatment%20plans%20based%20on%20individual%20genetic%20profiles%20can%0Adramatically%20improve%20patient%20outcomes%20and%20save%20lives.%20Our%20model%27s%20remarkable%0Aperformance%20highlights%20its%20potential%20in%20enhancing%20the%20precision%20of%20cancer%0Adiagnoses%20and%20treatments%2C%20thereby%20contributing%20significantly%20to%20the%20advancement%0Aof%20personalized%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14361v3&entry.124074799=Read"},
{"title": "On the Communication Complexity of Decentralized Bilevel Optimization", "author": "Yihan Zhang and My T. Thai and Jie Wu and Hongchang Gao", "abstract": "  Decentralized bilevel optimization has been actively studied in the past few\nyears since it has widespread applications in machine learning. However,\nexisting algorithms suffer from large communication complexity caused by the\nestimation of stochastic hypergradient, limiting their application to\nreal-world tasks. To address this issue, we develop a novel decentralized\nstochastic bilevel gradient descent algorithm under the heterogeneous setting,\nwhich enjoys a small communication cost in each round and a small number of\ncommunication rounds. As such, it can achieve a much better communication\ncomplexity than existing algorithms without any strong assumptions regarding\nheterogeneity. To the best of our knowledge, this is the first stochastic\nalgorithm achieving these theoretical results under the heterogeneous setting.\nAt last, the experimental results confirm the efficacy of our algorithm.\n", "link": "http://arxiv.org/abs/2311.11342v3", "date": "2024-05-20", "relevancy": 1.7279, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4371}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4333}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Communication%20Complexity%20of%20Decentralized%20Bilevel%20Optimization&body=Title%3A%20On%20the%20Communication%20Complexity%20of%20Decentralized%20Bilevel%20Optimization%0AAuthor%3A%20Yihan%20Zhang%20and%20My%20T.%20Thai%20and%20Jie%20Wu%20and%20Hongchang%20Gao%0AAbstract%3A%20%20%20Decentralized%20bilevel%20optimization%20has%20been%20actively%20studied%20in%20the%20past%20few%0Ayears%20since%20it%20has%20widespread%20applications%20in%20machine%20learning.%20However%2C%0Aexisting%20algorithms%20suffer%20from%20large%20communication%20complexity%20caused%20by%20the%0Aestimation%20of%20stochastic%20hypergradient%2C%20limiting%20their%20application%20to%0Areal-world%20tasks.%20To%20address%20this%20issue%2C%20we%20develop%20a%20novel%20decentralized%0Astochastic%20bilevel%20gradient%20descent%20algorithm%20under%20the%20heterogeneous%20setting%2C%0Awhich%20enjoys%20a%20small%20communication%20cost%20in%20each%20round%20and%20a%20small%20number%20of%0Acommunication%20rounds.%20As%20such%2C%20it%20can%20achieve%20a%20much%20better%20communication%0Acomplexity%20than%20existing%20algorithms%20without%20any%20strong%20assumptions%20regarding%0Aheterogeneity.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20stochastic%0Aalgorithm%20achieving%20these%20theoretical%20results%20under%20the%20heterogeneous%20setting.%0AAt%20last%2C%20the%20experimental%20results%20confirm%20the%20efficacy%20of%20our%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11342v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Communication%2520Complexity%2520of%2520Decentralized%2520Bilevel%2520Optimization%26entry.906535625%3DYihan%2520Zhang%2520and%2520My%2520T.%2520Thai%2520and%2520Jie%2520Wu%2520and%2520Hongchang%2520Gao%26entry.1292438233%3D%2520%2520Decentralized%2520bilevel%2520optimization%2520has%2520been%2520actively%2520studied%2520in%2520the%2520past%2520few%250Ayears%2520since%2520it%2520has%2520widespread%2520applications%2520in%2520machine%2520learning.%2520However%252C%250Aexisting%2520algorithms%2520suffer%2520from%2520large%2520communication%2520complexity%2520caused%2520by%2520the%250Aestimation%2520of%2520stochastic%2520hypergradient%252C%2520limiting%2520their%2520application%2520to%250Areal-world%2520tasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520a%2520novel%2520decentralized%250Astochastic%2520bilevel%2520gradient%2520descent%2520algorithm%2520under%2520the%2520heterogeneous%2520setting%252C%250Awhich%2520enjoys%2520a%2520small%2520communication%2520cost%2520in%2520each%2520round%2520and%2520a%2520small%2520number%2520of%250Acommunication%2520rounds.%2520As%2520such%252C%2520it%2520can%2520achieve%2520a%2520much%2520better%2520communication%250Acomplexity%2520than%2520existing%2520algorithms%2520without%2520any%2520strong%2520assumptions%2520regarding%250Aheterogeneity.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520stochastic%250Aalgorithm%2520achieving%2520these%2520theoretical%2520results%2520under%2520the%2520heterogeneous%2520setting.%250AAt%2520last%252C%2520the%2520experimental%2520results%2520confirm%2520the%2520efficacy%2520of%2520our%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11342v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Communication%20Complexity%20of%20Decentralized%20Bilevel%20Optimization&entry.906535625=Yihan%20Zhang%20and%20My%20T.%20Thai%20and%20Jie%20Wu%20and%20Hongchang%20Gao&entry.1292438233=%20%20Decentralized%20bilevel%20optimization%20has%20been%20actively%20studied%20in%20the%20past%20few%0Ayears%20since%20it%20has%20widespread%20applications%20in%20machine%20learning.%20However%2C%0Aexisting%20algorithms%20suffer%20from%20large%20communication%20complexity%20caused%20by%20the%0Aestimation%20of%20stochastic%20hypergradient%2C%20limiting%20their%20application%20to%0Areal-world%20tasks.%20To%20address%20this%20issue%2C%20we%20develop%20a%20novel%20decentralized%0Astochastic%20bilevel%20gradient%20descent%20algorithm%20under%20the%20heterogeneous%20setting%2C%0Awhich%20enjoys%20a%20small%20communication%20cost%20in%20each%20round%20and%20a%20small%20number%20of%0Acommunication%20rounds.%20As%20such%2C%20it%20can%20achieve%20a%20much%20better%20communication%0Acomplexity%20than%20existing%20algorithms%20without%20any%20strong%20assumptions%20regarding%0Aheterogeneity.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20stochastic%0Aalgorithm%20achieving%20these%20theoretical%20results%20under%20the%20heterogeneous%20setting.%0AAt%20last%2C%20the%20experimental%20results%20confirm%20the%20efficacy%20of%20our%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11342v3&entry.124074799=Read"},
{"title": "Using Unsupervised Learning to Explore Robot-Pedestrian Interactions in\n  Urban Environments", "author": "Sebastian Zug and Georg J\u00e4ger and Norman Seyffer and Martin Plank and Gero Licht and Felix Wilhelm Siebert", "abstract": "  This study identifies a gap in data-driven approaches to robot-centric\npedestrian interactions and proposes a corresponding pipeline. The pipeline\nutilizes unsupervised learning techniques to identify patterns in interaction\ndata of urban environments, specifically focusing on conflict scenarios.\nAnalyzed features include the robot's and pedestrian's speed and contextual\nparameters such as proximity to intersections. They are extracted and reduced\nin dimensionality using Principal Component Analysis (PCA). Finally, K-means\nclustering is employed to uncover underlying patterns in the interaction data.\nA use case application of the pipeline is presented, utilizing real-world robot\nmission data from a mid-sized German city. The results indicate the need for\nenriching interaction representations with contextual information to enable\nfine-grained analysis and reasoning. Nevertheless, they also highlight the need\nfor expanding the data set and incorporating additional contextual factors to\nenhance the robots situational awareness and interaction quality.\n", "link": "http://arxiv.org/abs/2405.12098v1", "date": "2024-05-20", "relevancy": 1.7184, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Unsupervised%20Learning%20to%20Explore%20Robot-Pedestrian%20Interactions%20in%0A%20%20Urban%20Environments&body=Title%3A%20Using%20Unsupervised%20Learning%20to%20Explore%20Robot-Pedestrian%20Interactions%20in%0A%20%20Urban%20Environments%0AAuthor%3A%20Sebastian%20Zug%20and%20Georg%20J%C3%A4ger%20and%20Norman%20Seyffer%20and%20Martin%20Plank%20and%20Gero%20Licht%20and%20Felix%20Wilhelm%20Siebert%0AAbstract%3A%20%20%20This%20study%20identifies%20a%20gap%20in%20data-driven%20approaches%20to%20robot-centric%0Apedestrian%20interactions%20and%20proposes%20a%20corresponding%20pipeline.%20The%20pipeline%0Autilizes%20unsupervised%20learning%20techniques%20to%20identify%20patterns%20in%20interaction%0Adata%20of%20urban%20environments%2C%20specifically%20focusing%20on%20conflict%20scenarios.%0AAnalyzed%20features%20include%20the%20robot%27s%20and%20pedestrian%27s%20speed%20and%20contextual%0Aparameters%20such%20as%20proximity%20to%20intersections.%20They%20are%20extracted%20and%20reduced%0Ain%20dimensionality%20using%20Principal%20Component%20Analysis%20%28PCA%29.%20Finally%2C%20K-means%0Aclustering%20is%20employed%20to%20uncover%20underlying%20patterns%20in%20the%20interaction%20data.%0AA%20use%20case%20application%20of%20the%20pipeline%20is%20presented%2C%20utilizing%20real-world%20robot%0Amission%20data%20from%20a%20mid-sized%20German%20city.%20The%20results%20indicate%20the%20need%20for%0Aenriching%20interaction%20representations%20with%20contextual%20information%20to%20enable%0Afine-grained%20analysis%20and%20reasoning.%20Nevertheless%2C%20they%20also%20highlight%20the%20need%0Afor%20expanding%20the%20data%20set%20and%20incorporating%20additional%20contextual%20factors%20to%0Aenhance%20the%20robots%20situational%20awareness%20and%20interaction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Unsupervised%2520Learning%2520to%2520Explore%2520Robot-Pedestrian%2520Interactions%2520in%250A%2520%2520Urban%2520Environments%26entry.906535625%3DSebastian%2520Zug%2520and%2520Georg%2520J%25C3%25A4ger%2520and%2520Norman%2520Seyffer%2520and%2520Martin%2520Plank%2520and%2520Gero%2520Licht%2520and%2520Felix%2520Wilhelm%2520Siebert%26entry.1292438233%3D%2520%2520This%2520study%2520identifies%2520a%2520gap%2520in%2520data-driven%2520approaches%2520to%2520robot-centric%250Apedestrian%2520interactions%2520and%2520proposes%2520a%2520corresponding%2520pipeline.%2520The%2520pipeline%250Autilizes%2520unsupervised%2520learning%2520techniques%2520to%2520identify%2520patterns%2520in%2520interaction%250Adata%2520of%2520urban%2520environments%252C%2520specifically%2520focusing%2520on%2520conflict%2520scenarios.%250AAnalyzed%2520features%2520include%2520the%2520robot%2527s%2520and%2520pedestrian%2527s%2520speed%2520and%2520contextual%250Aparameters%2520such%2520as%2520proximity%2520to%2520intersections.%2520They%2520are%2520extracted%2520and%2520reduced%250Ain%2520dimensionality%2520using%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529.%2520Finally%252C%2520K-means%250Aclustering%2520is%2520employed%2520to%2520uncover%2520underlying%2520patterns%2520in%2520the%2520interaction%2520data.%250AA%2520use%2520case%2520application%2520of%2520the%2520pipeline%2520is%2520presented%252C%2520utilizing%2520real-world%2520robot%250Amission%2520data%2520from%2520a%2520mid-sized%2520German%2520city.%2520The%2520results%2520indicate%2520the%2520need%2520for%250Aenriching%2520interaction%2520representations%2520with%2520contextual%2520information%2520to%2520enable%250Afine-grained%2520analysis%2520and%2520reasoning.%2520Nevertheless%252C%2520they%2520also%2520highlight%2520the%2520need%250Afor%2520expanding%2520the%2520data%2520set%2520and%2520incorporating%2520additional%2520contextual%2520factors%2520to%250Aenhance%2520the%2520robots%2520situational%2520awareness%2520and%2520interaction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Unsupervised%20Learning%20to%20Explore%20Robot-Pedestrian%20Interactions%20in%0A%20%20Urban%20Environments&entry.906535625=Sebastian%20Zug%20and%20Georg%20J%C3%A4ger%20and%20Norman%20Seyffer%20and%20Martin%20Plank%20and%20Gero%20Licht%20and%20Felix%20Wilhelm%20Siebert&entry.1292438233=%20%20This%20study%20identifies%20a%20gap%20in%20data-driven%20approaches%20to%20robot-centric%0Apedestrian%20interactions%20and%20proposes%20a%20corresponding%20pipeline.%20The%20pipeline%0Autilizes%20unsupervised%20learning%20techniques%20to%20identify%20patterns%20in%20interaction%0Adata%20of%20urban%20environments%2C%20specifically%20focusing%20on%20conflict%20scenarios.%0AAnalyzed%20features%20include%20the%20robot%27s%20and%20pedestrian%27s%20speed%20and%20contextual%0Aparameters%20such%20as%20proximity%20to%20intersections.%20They%20are%20extracted%20and%20reduced%0Ain%20dimensionality%20using%20Principal%20Component%20Analysis%20%28PCA%29.%20Finally%2C%20K-means%0Aclustering%20is%20employed%20to%20uncover%20underlying%20patterns%20in%20the%20interaction%20data.%0AA%20use%20case%20application%20of%20the%20pipeline%20is%20presented%2C%20utilizing%20real-world%20robot%0Amission%20data%20from%20a%20mid-sized%20German%20city.%20The%20results%20indicate%20the%20need%20for%0Aenriching%20interaction%20representations%20with%20contextual%20information%20to%20enable%0Afine-grained%20analysis%20and%20reasoning.%20Nevertheless%2C%20they%20also%20highlight%20the%20need%0Afor%20expanding%20the%20data%20set%20and%20incorporating%20additional%20contextual%20factors%20to%0Aenhance%20the%20robots%20situational%20awareness%20and%20interaction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12098v1&entry.124074799=Read"},
{"title": "Learning Force Control for Legged Manipulation", "author": "Tifanny Portela and Gabriel B. Margolis and Yandong Ji and Pulkit Agrawal", "abstract": "  Controlling contact forces during interactions is critical for locomotion and\nmanipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded\nin many contact-rich problems, current RL methods achieve forceful interactions\nimplicitly without explicitly regulating forces. We propose a method for\ntraining RL policies for direct force control without requiring access to force\nsensing. We showcase our method on a whole-body control platform of a quadruped\nrobot with an arm. Such force control enables us to perform gravity\ncompensation and impedance control, unlocking compliant whole-body\nmanipulation. The learned whole-body controller with variable compliance makes\nit intuitive for humans to teleoperate the robot by only commanding the\nmanipulator, and the robot's body adjusts automatically to achieve the desired\nposition and force. Consequently, a human teleoperator can easily demonstrate a\nwide variety of loco-manipulation tasks. To the best of our knowledge, we\nprovide the first deployment of learned whole-body force control in legged\nmanipulators, paving the way for more versatile and adaptable legged robots.\n", "link": "http://arxiv.org/abs/2405.01402v2", "date": "2024-05-20", "relevancy": 1.683, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5879}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5586}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Force%20Control%20for%20Legged%20Manipulation&body=Title%3A%20Learning%20Force%20Control%20for%20Legged%20Manipulation%0AAuthor%3A%20Tifanny%20Portela%20and%20Gabriel%20B.%20Margolis%20and%20Yandong%20Ji%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Controlling%20contact%20forces%20during%20interactions%20is%20critical%20for%20locomotion%20and%0Amanipulation%20tasks.%20While%20sim-to-real%20reinforcement%20learning%20%28RL%29%20has%20succeeded%0Ain%20many%20contact-rich%20problems%2C%20current%20RL%20methods%20achieve%20forceful%20interactions%0Aimplicitly%20without%20explicitly%20regulating%20forces.%20We%20propose%20a%20method%20for%0Atraining%20RL%20policies%20for%20direct%20force%20control%20without%20requiring%20access%20to%20force%0Asensing.%20We%20showcase%20our%20method%20on%20a%20whole-body%20control%20platform%20of%20a%20quadruped%0Arobot%20with%20an%20arm.%20Such%20force%20control%20enables%20us%20to%20perform%20gravity%0Acompensation%20and%20impedance%20control%2C%20unlocking%20compliant%20whole-body%0Amanipulation.%20The%20learned%20whole-body%20controller%20with%20variable%20compliance%20makes%0Ait%20intuitive%20for%20humans%20to%20teleoperate%20the%20robot%20by%20only%20commanding%20the%0Amanipulator%2C%20and%20the%20robot%27s%20body%20adjusts%20automatically%20to%20achieve%20the%20desired%0Aposition%20and%20force.%20Consequently%2C%20a%20human%20teleoperator%20can%20easily%20demonstrate%20a%0Awide%20variety%20of%20loco-manipulation%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%0Aprovide%20the%20first%20deployment%20of%20learned%20whole-body%20force%20control%20in%20legged%0Amanipulators%2C%20paving%20the%20way%20for%20more%20versatile%20and%20adaptable%20legged%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Force%2520Control%2520for%2520Legged%2520Manipulation%26entry.906535625%3DTifanny%2520Portela%2520and%2520Gabriel%2520B.%2520Margolis%2520and%2520Yandong%2520Ji%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Controlling%2520contact%2520forces%2520during%2520interactions%2520is%2520critical%2520for%2520locomotion%2520and%250Amanipulation%2520tasks.%2520While%2520sim-to-real%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520succeeded%250Ain%2520many%2520contact-rich%2520problems%252C%2520current%2520RL%2520methods%2520achieve%2520forceful%2520interactions%250Aimplicitly%2520without%2520explicitly%2520regulating%2520forces.%2520We%2520propose%2520a%2520method%2520for%250Atraining%2520RL%2520policies%2520for%2520direct%2520force%2520control%2520without%2520requiring%2520access%2520to%2520force%250Asensing.%2520We%2520showcase%2520our%2520method%2520on%2520a%2520whole-body%2520control%2520platform%2520of%2520a%2520quadruped%250Arobot%2520with%2520an%2520arm.%2520Such%2520force%2520control%2520enables%2520us%2520to%2520perform%2520gravity%250Acompensation%2520and%2520impedance%2520control%252C%2520unlocking%2520compliant%2520whole-body%250Amanipulation.%2520The%2520learned%2520whole-body%2520controller%2520with%2520variable%2520compliance%2520makes%250Ait%2520intuitive%2520for%2520humans%2520to%2520teleoperate%2520the%2520robot%2520by%2520only%2520commanding%2520the%250Amanipulator%252C%2520and%2520the%2520robot%2527s%2520body%2520adjusts%2520automatically%2520to%2520achieve%2520the%2520desired%250Aposition%2520and%2520force.%2520Consequently%252C%2520a%2520human%2520teleoperator%2520can%2520easily%2520demonstrate%2520a%250Awide%2520variety%2520of%2520loco-manipulation%2520tasks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%250Aprovide%2520the%2520first%2520deployment%2520of%2520learned%2520whole-body%2520force%2520control%2520in%2520legged%250Amanipulators%252C%2520paving%2520the%2520way%2520for%2520more%2520versatile%2520and%2520adaptable%2520legged%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Force%20Control%20for%20Legged%20Manipulation&entry.906535625=Tifanny%20Portela%20and%20Gabriel%20B.%20Margolis%20and%20Yandong%20Ji%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Controlling%20contact%20forces%20during%20interactions%20is%20critical%20for%20locomotion%20and%0Amanipulation%20tasks.%20While%20sim-to-real%20reinforcement%20learning%20%28RL%29%20has%20succeeded%0Ain%20many%20contact-rich%20problems%2C%20current%20RL%20methods%20achieve%20forceful%20interactions%0Aimplicitly%20without%20explicitly%20regulating%20forces.%20We%20propose%20a%20method%20for%0Atraining%20RL%20policies%20for%20direct%20force%20control%20without%20requiring%20access%20to%20force%0Asensing.%20We%20showcase%20our%20method%20on%20a%20whole-body%20control%20platform%20of%20a%20quadruped%0Arobot%20with%20an%20arm.%20Such%20force%20control%20enables%20us%20to%20perform%20gravity%0Acompensation%20and%20impedance%20control%2C%20unlocking%20compliant%20whole-body%0Amanipulation.%20The%20learned%20whole-body%20controller%20with%20variable%20compliance%20makes%0Ait%20intuitive%20for%20humans%20to%20teleoperate%20the%20robot%20by%20only%20commanding%20the%0Amanipulator%2C%20and%20the%20robot%27s%20body%20adjusts%20automatically%20to%20achieve%20the%20desired%0Aposition%20and%20force.%20Consequently%2C%20a%20human%20teleoperator%20can%20easily%20demonstrate%20a%0Awide%20variety%20of%20loco-manipulation%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%0Aprovide%20the%20first%20deployment%20of%20learned%20whole-body%20force%20control%20in%20legged%0Amanipulators%2C%20paving%20the%20way%20for%20more%20versatile%20and%20adaptable%20legged%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01402v2&entry.124074799=Read"},
{"title": "DREAM: Decentralized Real-time Asynchronous Probabilistic Trajectory\n  Planning for Collision-free Multi-Robot Navigation in Cluttered Environments", "author": "Bask\u0131n \u015eenba\u015flar and Gaurav S. Sukhatme", "abstract": "  Collision-free navigation in cluttered environments with static and dynamic\nobstacles is essential for many multi-robot tasks. Dynamic obstacles may also\nbe interactive, i.e., their behavior varies based on the behavior of other\nentities. We propose a novel representation for interactive behavior of dynamic\nobstacles and a decentralized real-time multi-robot trajectory planning\nalgorithm allowing inter-robot collision avoidance as well as static and\ndynamic obstacle avoidance. Our planner simulates the behavior of dynamic\nobstacles, accounting for interactivity. We account for the perception\ninaccuracy of static and prediction inaccuracy of dynamic obstacles. We handle\nasynchronous planning between teammates and message delays, drops, and\nre-orderings. We evaluate our algorithm in simulations using 25400 random cases\nand compare it against three state-of-the-art baselines using 2100 random\ncases. Our algorithm achieves up to 1.68x success rate using as low as 0.28x\ntime in single-robot, and up to 2.15x success rate using as low as 0.36x time\nin multi-robot cases compared to the best baseline. We implement our planner on\nreal quadrotors to show its real-world applicability.\n", "link": "http://arxiv.org/abs/2307.15887v2", "date": "2024-05-20", "relevancy": 1.6758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5842}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5641}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DREAM%3A%20Decentralized%20Real-time%20Asynchronous%20Probabilistic%20Trajectory%0A%20%20Planning%20for%20Collision-free%20Multi-Robot%20Navigation%20in%20Cluttered%20Environments&body=Title%3A%20DREAM%3A%20Decentralized%20Real-time%20Asynchronous%20Probabilistic%20Trajectory%0A%20%20Planning%20for%20Collision-free%20Multi-Robot%20Navigation%20in%20Cluttered%20Environments%0AAuthor%3A%20Bask%C4%B1n%20%C5%9Eenba%C5%9Flar%20and%20Gaurav%20S.%20Sukhatme%0AAbstract%3A%20%20%20Collision-free%20navigation%20in%20cluttered%20environments%20with%20static%20and%20dynamic%0Aobstacles%20is%20essential%20for%20many%20multi-robot%20tasks.%20Dynamic%20obstacles%20may%20also%0Abe%20interactive%2C%20i.e.%2C%20their%20behavior%20varies%20based%20on%20the%20behavior%20of%20other%0Aentities.%20We%20propose%20a%20novel%20representation%20for%20interactive%20behavior%20of%20dynamic%0Aobstacles%20and%20a%20decentralized%20real-time%20multi-robot%20trajectory%20planning%0Aalgorithm%20allowing%20inter-robot%20collision%20avoidance%20as%20well%20as%20static%20and%0Adynamic%20obstacle%20avoidance.%20Our%20planner%20simulates%20the%20behavior%20of%20dynamic%0Aobstacles%2C%20accounting%20for%20interactivity.%20We%20account%20for%20the%20perception%0Ainaccuracy%20of%20static%20and%20prediction%20inaccuracy%20of%20dynamic%20obstacles.%20We%20handle%0Aasynchronous%20planning%20between%20teammates%20and%20message%20delays%2C%20drops%2C%20and%0Are-orderings.%20We%20evaluate%20our%20algorithm%20in%20simulations%20using%2025400%20random%20cases%0Aand%20compare%20it%20against%20three%20state-of-the-art%20baselines%20using%202100%20random%0Acases.%20Our%20algorithm%20achieves%20up%20to%201.68x%20success%20rate%20using%20as%20low%20as%200.28x%0Atime%20in%20single-robot%2C%20and%20up%20to%202.15x%20success%20rate%20using%20as%20low%20as%200.36x%20time%0Ain%20multi-robot%20cases%20compared%20to%20the%20best%20baseline.%20We%20implement%20our%20planner%20on%0Areal%20quadrotors%20to%20show%20its%20real-world%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDREAM%253A%2520Decentralized%2520Real-time%2520Asynchronous%2520Probabilistic%2520Trajectory%250A%2520%2520Planning%2520for%2520Collision-free%2520Multi-Robot%2520Navigation%2520in%2520Cluttered%2520Environments%26entry.906535625%3DBask%25C4%25B1n%2520%25C5%259Eenba%25C5%259Flar%2520and%2520Gaurav%2520S.%2520Sukhatme%26entry.1292438233%3D%2520%2520Collision-free%2520navigation%2520in%2520cluttered%2520environments%2520with%2520static%2520and%2520dynamic%250Aobstacles%2520is%2520essential%2520for%2520many%2520multi-robot%2520tasks.%2520Dynamic%2520obstacles%2520may%2520also%250Abe%2520interactive%252C%2520i.e.%252C%2520their%2520behavior%2520varies%2520based%2520on%2520the%2520behavior%2520of%2520other%250Aentities.%2520We%2520propose%2520a%2520novel%2520representation%2520for%2520interactive%2520behavior%2520of%2520dynamic%250Aobstacles%2520and%2520a%2520decentralized%2520real-time%2520multi-robot%2520trajectory%2520planning%250Aalgorithm%2520allowing%2520inter-robot%2520collision%2520avoidance%2520as%2520well%2520as%2520static%2520and%250Adynamic%2520obstacle%2520avoidance.%2520Our%2520planner%2520simulates%2520the%2520behavior%2520of%2520dynamic%250Aobstacles%252C%2520accounting%2520for%2520interactivity.%2520We%2520account%2520for%2520the%2520perception%250Ainaccuracy%2520of%2520static%2520and%2520prediction%2520inaccuracy%2520of%2520dynamic%2520obstacles.%2520We%2520handle%250Aasynchronous%2520planning%2520between%2520teammates%2520and%2520message%2520delays%252C%2520drops%252C%2520and%250Are-orderings.%2520We%2520evaluate%2520our%2520algorithm%2520in%2520simulations%2520using%252025400%2520random%2520cases%250Aand%2520compare%2520it%2520against%2520three%2520state-of-the-art%2520baselines%2520using%25202100%2520random%250Acases.%2520Our%2520algorithm%2520achieves%2520up%2520to%25201.68x%2520success%2520rate%2520using%2520as%2520low%2520as%25200.28x%250Atime%2520in%2520single-robot%252C%2520and%2520up%2520to%25202.15x%2520success%2520rate%2520using%2520as%2520low%2520as%25200.36x%2520time%250Ain%2520multi-robot%2520cases%2520compared%2520to%2520the%2520best%2520baseline.%2520We%2520implement%2520our%2520planner%2520on%250Areal%2520quadrotors%2520to%2520show%2520its%2520real-world%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DREAM%3A%20Decentralized%20Real-time%20Asynchronous%20Probabilistic%20Trajectory%0A%20%20Planning%20for%20Collision-free%20Multi-Robot%20Navigation%20in%20Cluttered%20Environments&entry.906535625=Bask%C4%B1n%20%C5%9Eenba%C5%9Flar%20and%20Gaurav%20S.%20Sukhatme&entry.1292438233=%20%20Collision-free%20navigation%20in%20cluttered%20environments%20with%20static%20and%20dynamic%0Aobstacles%20is%20essential%20for%20many%20multi-robot%20tasks.%20Dynamic%20obstacles%20may%20also%0Abe%20interactive%2C%20i.e.%2C%20their%20behavior%20varies%20based%20on%20the%20behavior%20of%20other%0Aentities.%20We%20propose%20a%20novel%20representation%20for%20interactive%20behavior%20of%20dynamic%0Aobstacles%20and%20a%20decentralized%20real-time%20multi-robot%20trajectory%20planning%0Aalgorithm%20allowing%20inter-robot%20collision%20avoidance%20as%20well%20as%20static%20and%0Adynamic%20obstacle%20avoidance.%20Our%20planner%20simulates%20the%20behavior%20of%20dynamic%0Aobstacles%2C%20accounting%20for%20interactivity.%20We%20account%20for%20the%20perception%0Ainaccuracy%20of%20static%20and%20prediction%20inaccuracy%20of%20dynamic%20obstacles.%20We%20handle%0Aasynchronous%20planning%20between%20teammates%20and%20message%20delays%2C%20drops%2C%20and%0Are-orderings.%20We%20evaluate%20our%20algorithm%20in%20simulations%20using%2025400%20random%20cases%0Aand%20compare%20it%20against%20three%20state-of-the-art%20baselines%20using%202100%20random%0Acases.%20Our%20algorithm%20achieves%20up%20to%201.68x%20success%20rate%20using%20as%20low%20as%200.28x%0Atime%20in%20single-robot%2C%20and%20up%20to%202.15x%20success%20rate%20using%20as%20low%20as%200.36x%20time%0Ain%20multi-robot%20cases%20compared%20to%20the%20best%20baseline.%20We%20implement%20our%20planner%20on%0Areal%20quadrotors%20to%20show%20its%20real-world%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15887v2&entry.124074799=Read"},
{"title": "Recommender Algorithm for Supporting Self-Management of CVD Risk Factors\n  in an Adult Population at Home", "author": "Tatiana V. Afanasieva and Pavel V. Platov and Anastasia I. Medvedeva", "abstract": "  One of the new trends in the development of recommendation algorithms is the\ndissemination of their capabilities to support the population in managing their\nhealth. This article focuses on the problem of improving the effectiveness of\ncardiovascular diseases (CVD) prevention, since CVD is the leading cause of\ndeath worldwide. To address this issue, a knowledge-based recommendation\nalgorithm was proposed to support self-management of CVD risk factors in adults\nat home. The proposed algorithm is based on the original multidimensional\nrecommendation model and on a new user profile model, which includes predictive\nassessments of CVD health in addition to its current ones as outlined in\nofficial guidelines. The main feature of the proposed algorithm is the\ncombination of rule-based logic with the capabilities of a large language model\nin generating human-like text for explanatory component of multidimensional\nrecommendation. The verification and evaluation of the proposed algorithm\nshowed the usefulness of the proposed recommendation algorithm for supporting\nadults in self-management of their CVD risk factors at home. As follows from\nthe comparison with similar knowledge-based recommendation algorithms, the\nproposed algorithm evaluates a larger number of CVD risk factors and has a\ngreater information and semantic capacity of the generated recommendations.\n", "link": "http://arxiv.org/abs/2405.11967v1", "date": "2024-05-20", "relevancy": 1.6701, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4215}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recommender%20Algorithm%20for%20Supporting%20Self-Management%20of%20CVD%20Risk%20Factors%0A%20%20in%20an%20Adult%20Population%20at%20Home&body=Title%3A%20Recommender%20Algorithm%20for%20Supporting%20Self-Management%20of%20CVD%20Risk%20Factors%0A%20%20in%20an%20Adult%20Population%20at%20Home%0AAuthor%3A%20Tatiana%20V.%20Afanasieva%20and%20Pavel%20V.%20Platov%20and%20Anastasia%20I.%20Medvedeva%0AAbstract%3A%20%20%20One%20of%20the%20new%20trends%20in%20the%20development%20of%20recommendation%20algorithms%20is%20the%0Adissemination%20of%20their%20capabilities%20to%20support%20the%20population%20in%20managing%20their%0Ahealth.%20This%20article%20focuses%20on%20the%20problem%20of%20improving%20the%20effectiveness%20of%0Acardiovascular%20diseases%20%28CVD%29%20prevention%2C%20since%20CVD%20is%20the%20leading%20cause%20of%0Adeath%20worldwide.%20To%20address%20this%20issue%2C%20a%20knowledge-based%20recommendation%0Aalgorithm%20was%20proposed%20to%20support%20self-management%20of%20CVD%20risk%20factors%20in%20adults%0Aat%20home.%20The%20proposed%20algorithm%20is%20based%20on%20the%20original%20multidimensional%0Arecommendation%20model%20and%20on%20a%20new%20user%20profile%20model%2C%20which%20includes%20predictive%0Aassessments%20of%20CVD%20health%20in%20addition%20to%20its%20current%20ones%20as%20outlined%20in%0Aofficial%20guidelines.%20The%20main%20feature%20of%20the%20proposed%20algorithm%20is%20the%0Acombination%20of%20rule-based%20logic%20with%20the%20capabilities%20of%20a%20large%20language%20model%0Ain%20generating%20human-like%20text%20for%20explanatory%20component%20of%20multidimensional%0Arecommendation.%20The%20verification%20and%20evaluation%20of%20the%20proposed%20algorithm%0Ashowed%20the%20usefulness%20of%20the%20proposed%20recommendation%20algorithm%20for%20supporting%0Aadults%20in%20self-management%20of%20their%20CVD%20risk%20factors%20at%20home.%20As%20follows%20from%0Athe%20comparison%20with%20similar%20knowledge-based%20recommendation%20algorithms%2C%20the%0Aproposed%20algorithm%20evaluates%20a%20larger%20number%20of%20CVD%20risk%20factors%20and%20has%20a%0Agreater%20information%20and%20semantic%20capacity%20of%20the%20generated%20recommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecommender%2520Algorithm%2520for%2520Supporting%2520Self-Management%2520of%2520CVD%2520Risk%2520Factors%250A%2520%2520in%2520an%2520Adult%2520Population%2520at%2520Home%26entry.906535625%3DTatiana%2520V.%2520Afanasieva%2520and%2520Pavel%2520V.%2520Platov%2520and%2520Anastasia%2520I.%2520Medvedeva%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520new%2520trends%2520in%2520the%2520development%2520of%2520recommendation%2520algorithms%2520is%2520the%250Adissemination%2520of%2520their%2520capabilities%2520to%2520support%2520the%2520population%2520in%2520managing%2520their%250Ahealth.%2520This%2520article%2520focuses%2520on%2520the%2520problem%2520of%2520improving%2520the%2520effectiveness%2520of%250Acardiovascular%2520diseases%2520%2528CVD%2529%2520prevention%252C%2520since%2520CVD%2520is%2520the%2520leading%2520cause%2520of%250Adeath%2520worldwide.%2520To%2520address%2520this%2520issue%252C%2520a%2520knowledge-based%2520recommendation%250Aalgorithm%2520was%2520proposed%2520to%2520support%2520self-management%2520of%2520CVD%2520risk%2520factors%2520in%2520adults%250Aat%2520home.%2520The%2520proposed%2520algorithm%2520is%2520based%2520on%2520the%2520original%2520multidimensional%250Arecommendation%2520model%2520and%2520on%2520a%2520new%2520user%2520profile%2520model%252C%2520which%2520includes%2520predictive%250Aassessments%2520of%2520CVD%2520health%2520in%2520addition%2520to%2520its%2520current%2520ones%2520as%2520outlined%2520in%250Aofficial%2520guidelines.%2520The%2520main%2520feature%2520of%2520the%2520proposed%2520algorithm%2520is%2520the%250Acombination%2520of%2520rule-based%2520logic%2520with%2520the%2520capabilities%2520of%2520a%2520large%2520language%2520model%250Ain%2520generating%2520human-like%2520text%2520for%2520explanatory%2520component%2520of%2520multidimensional%250Arecommendation.%2520The%2520verification%2520and%2520evaluation%2520of%2520the%2520proposed%2520algorithm%250Ashowed%2520the%2520usefulness%2520of%2520the%2520proposed%2520recommendation%2520algorithm%2520for%2520supporting%250Aadults%2520in%2520self-management%2520of%2520their%2520CVD%2520risk%2520factors%2520at%2520home.%2520As%2520follows%2520from%250Athe%2520comparison%2520with%2520similar%2520knowledge-based%2520recommendation%2520algorithms%252C%2520the%250Aproposed%2520algorithm%2520evaluates%2520a%2520larger%2520number%2520of%2520CVD%2520risk%2520factors%2520and%2520has%2520a%250Agreater%2520information%2520and%2520semantic%2520capacity%2520of%2520the%2520generated%2520recommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recommender%20Algorithm%20for%20Supporting%20Self-Management%20of%20CVD%20Risk%20Factors%0A%20%20in%20an%20Adult%20Population%20at%20Home&entry.906535625=Tatiana%20V.%20Afanasieva%20and%20Pavel%20V.%20Platov%20and%20Anastasia%20I.%20Medvedeva&entry.1292438233=%20%20One%20of%20the%20new%20trends%20in%20the%20development%20of%20recommendation%20algorithms%20is%20the%0Adissemination%20of%20their%20capabilities%20to%20support%20the%20population%20in%20managing%20their%0Ahealth.%20This%20article%20focuses%20on%20the%20problem%20of%20improving%20the%20effectiveness%20of%0Acardiovascular%20diseases%20%28CVD%29%20prevention%2C%20since%20CVD%20is%20the%20leading%20cause%20of%0Adeath%20worldwide.%20To%20address%20this%20issue%2C%20a%20knowledge-based%20recommendation%0Aalgorithm%20was%20proposed%20to%20support%20self-management%20of%20CVD%20risk%20factors%20in%20adults%0Aat%20home.%20The%20proposed%20algorithm%20is%20based%20on%20the%20original%20multidimensional%0Arecommendation%20model%20and%20on%20a%20new%20user%20profile%20model%2C%20which%20includes%20predictive%0Aassessments%20of%20CVD%20health%20in%20addition%20to%20its%20current%20ones%20as%20outlined%20in%0Aofficial%20guidelines.%20The%20main%20feature%20of%20the%20proposed%20algorithm%20is%20the%0Acombination%20of%20rule-based%20logic%20with%20the%20capabilities%20of%20a%20large%20language%20model%0Ain%20generating%20human-like%20text%20for%20explanatory%20component%20of%20multidimensional%0Arecommendation.%20The%20verification%20and%20evaluation%20of%20the%20proposed%20algorithm%0Ashowed%20the%20usefulness%20of%20the%20proposed%20recommendation%20algorithm%20for%20supporting%0Aadults%20in%20self-management%20of%20their%20CVD%20risk%20factors%20at%20home.%20As%20follows%20from%0Athe%20comparison%20with%20similar%20knowledge-based%20recommendation%20algorithms%2C%20the%0Aproposed%20algorithm%20evaluates%20a%20larger%20number%20of%20CVD%20risk%20factors%20and%20has%20a%0Agreater%20information%20and%20semantic%20capacity%20of%20the%20generated%20recommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11967v1&entry.124074799=Read"},
{"title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of\n  In-Context Learning", "author": "Guanglin Zhou and Zhongyi Han and Shiming Chen and Biwei Huang and Liming Zhu and Salman Khan and Xin Gao and Lina Yao", "abstract": "  Recent studies indicate that large multimodal models (LMMs) are highly robust\nagainst natural distribution shifts, often surpassing previous baselines.\nDespite this, domain-specific adaptation is still necessary, particularly in\nspecialized areas like healthcare. Due to the impracticality of fine-tuning\nLMMs given their vast parameter space, this work investigates in-context\nlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. We\nfind that the success of ICL heavily relies on the choice of demonstration,\nmirroring challenges seen in large language models but introducing unique\ncomplexities for LMMs facing distribution shifts. Our study addresses this by\nevaluating an unsupervised ICL method, TopKNearestPR, which selects in-context\nexamples through a nearest example search based on feature similarity. We\nuncover that its effectiveness is limited by the deficiencies of pre-trained\nvision encoders under distribution shift scenarios. To address these\nchallenges, we propose InvariantSelectPR, a novel method leveraging\nClass-conditioned Contrastive Invariance (CCI) for more robust demonstration\nselection. Specifically, CCI enhances pre-trained vision encoders by improving\ntheir discriminative capabilities across different classes and ensuring\ninvariance to domain-specific variations. This enhancement allows the encoders\nto effectively identify and retrieve the most informative examples, which are\nthen used to guide LMMs in adapting to new query samples under varying\ndistributions. Our experiments show that InvariantSelectPR substantially\nimproves the adaptability of LMMs, achieving significant performance gains on\nbenchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on\nCamelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the\nbaseline zero-shot performance.\n", "link": "http://arxiv.org/abs/2405.12217v1", "date": "2024-05-20", "relevancy": 1.6618, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5723}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Large%20Multimodal%20Models%20to%20Distribution%20Shifts%3A%20The%20Role%20of%0A%20%20In-Context%20Learning&body=Title%3A%20Adapting%20Large%20Multimodal%20Models%20to%20Distribution%20Shifts%3A%20The%20Role%20of%0A%20%20In-Context%20Learning%0AAuthor%3A%20Guanglin%20Zhou%20and%20Zhongyi%20Han%20and%20Shiming%20Chen%20and%20Biwei%20Huang%20and%20Liming%20Zhu%20and%20Salman%20Khan%20and%20Xin%20Gao%20and%20Lina%20Yao%0AAbstract%3A%20%20%20Recent%20studies%20indicate%20that%20large%20multimodal%20models%20%28LMMs%29%20are%20highly%20robust%0Aagainst%20natural%20distribution%20shifts%2C%20often%20surpassing%20previous%20baselines.%0ADespite%20this%2C%20domain-specific%20adaptation%20is%20still%20necessary%2C%20particularly%20in%0Aspecialized%20areas%20like%20healthcare.%20Due%20to%20the%20impracticality%20of%20fine-tuning%0ALMMs%20given%20their%20vast%20parameter%20space%2C%20this%20work%20investigates%20in-context%0Alearning%20%28ICL%29%20as%20an%20effective%20alternative%20for%20enhancing%20LMMs%27%20adaptability.%20We%0Afind%20that%20the%20success%20of%20ICL%20heavily%20relies%20on%20the%20choice%20of%20demonstration%2C%0Amirroring%20challenges%20seen%20in%20large%20language%20models%20but%20introducing%20unique%0Acomplexities%20for%20LMMs%20facing%20distribution%20shifts.%20Our%20study%20addresses%20this%20by%0Aevaluating%20an%20unsupervised%20ICL%20method%2C%20TopKNearestPR%2C%20which%20selects%20in-context%0Aexamples%20through%20a%20nearest%20example%20search%20based%20on%20feature%20similarity.%20We%0Auncover%20that%20its%20effectiveness%20is%20limited%20by%20the%20deficiencies%20of%20pre-trained%0Avision%20encoders%20under%20distribution%20shift%20scenarios.%20To%20address%20these%0Achallenges%2C%20we%20propose%20InvariantSelectPR%2C%20a%20novel%20method%20leveraging%0AClass-conditioned%20Contrastive%20Invariance%20%28CCI%29%20for%20more%20robust%20demonstration%0Aselection.%20Specifically%2C%20CCI%20enhances%20pre-trained%20vision%20encoders%20by%20improving%0Atheir%20discriminative%20capabilities%20across%20different%20classes%20and%20ensuring%0Ainvariance%20to%20domain-specific%20variations.%20This%20enhancement%20allows%20the%20encoders%0Ato%20effectively%20identify%20and%20retrieve%20the%20most%20informative%20examples%2C%20which%20are%0Athen%20used%20to%20guide%20LMMs%20in%20adapting%20to%20new%20query%20samples%20under%20varying%0Adistributions.%20Our%20experiments%20show%20that%20InvariantSelectPR%20substantially%0Aimproves%20the%20adaptability%20of%20LMMs%2C%20achieving%20significant%20performance%20gains%20on%0Abenchmark%20datasets%2C%20with%20a%2034.2%25%24%5Cuparrow%24%20accuracy%20increase%20in%207-shot%20on%0ACamelyon17%20and%2016.9%25%24%5Cuparrow%24%20increase%20in%207-shot%20on%20HAM10000%20compared%20to%20the%0Abaseline%20zero-shot%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Large%2520Multimodal%2520Models%2520to%2520Distribution%2520Shifts%253A%2520The%2520Role%2520of%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DGuanglin%2520Zhou%2520and%2520Zhongyi%2520Han%2520and%2520Shiming%2520Chen%2520and%2520Biwei%2520Huang%2520and%2520Liming%2520Zhu%2520and%2520Salman%2520Khan%2520and%2520Xin%2520Gao%2520and%2520Lina%2520Yao%26entry.1292438233%3D%2520%2520Recent%2520studies%2520indicate%2520that%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520are%2520highly%2520robust%250Aagainst%2520natural%2520distribution%2520shifts%252C%2520often%2520surpassing%2520previous%2520baselines.%250ADespite%2520this%252C%2520domain-specific%2520adaptation%2520is%2520still%2520necessary%252C%2520particularly%2520in%250Aspecialized%2520areas%2520like%2520healthcare.%2520Due%2520to%2520the%2520impracticality%2520of%2520fine-tuning%250ALMMs%2520given%2520their%2520vast%2520parameter%2520space%252C%2520this%2520work%2520investigates%2520in-context%250Alearning%2520%2528ICL%2529%2520as%2520an%2520effective%2520alternative%2520for%2520enhancing%2520LMMs%2527%2520adaptability.%2520We%250Afind%2520that%2520the%2520success%2520of%2520ICL%2520heavily%2520relies%2520on%2520the%2520choice%2520of%2520demonstration%252C%250Amirroring%2520challenges%2520seen%2520in%2520large%2520language%2520models%2520but%2520introducing%2520unique%250Acomplexities%2520for%2520LMMs%2520facing%2520distribution%2520shifts.%2520Our%2520study%2520addresses%2520this%2520by%250Aevaluating%2520an%2520unsupervised%2520ICL%2520method%252C%2520TopKNearestPR%252C%2520which%2520selects%2520in-context%250Aexamples%2520through%2520a%2520nearest%2520example%2520search%2520based%2520on%2520feature%2520similarity.%2520We%250Auncover%2520that%2520its%2520effectiveness%2520is%2520limited%2520by%2520the%2520deficiencies%2520of%2520pre-trained%250Avision%2520encoders%2520under%2520distribution%2520shift%2520scenarios.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520InvariantSelectPR%252C%2520a%2520novel%2520method%2520leveraging%250AClass-conditioned%2520Contrastive%2520Invariance%2520%2528CCI%2529%2520for%2520more%2520robust%2520demonstration%250Aselection.%2520Specifically%252C%2520CCI%2520enhances%2520pre-trained%2520vision%2520encoders%2520by%2520improving%250Atheir%2520discriminative%2520capabilities%2520across%2520different%2520classes%2520and%2520ensuring%250Ainvariance%2520to%2520domain-specific%2520variations.%2520This%2520enhancement%2520allows%2520the%2520encoders%250Ato%2520effectively%2520identify%2520and%2520retrieve%2520the%2520most%2520informative%2520examples%252C%2520which%2520are%250Athen%2520used%2520to%2520guide%2520LMMs%2520in%2520adapting%2520to%2520new%2520query%2520samples%2520under%2520varying%250Adistributions.%2520Our%2520experiments%2520show%2520that%2520InvariantSelectPR%2520substantially%250Aimproves%2520the%2520adaptability%2520of%2520LMMs%252C%2520achieving%2520significant%2520performance%2520gains%2520on%250Abenchmark%2520datasets%252C%2520with%2520a%252034.2%2525%2524%255Cuparrow%2524%2520accuracy%2520increase%2520in%25207-shot%2520on%250ACamelyon17%2520and%252016.9%2525%2524%255Cuparrow%2524%2520increase%2520in%25207-shot%2520on%2520HAM10000%2520compared%2520to%2520the%250Abaseline%2520zero-shot%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Large%20Multimodal%20Models%20to%20Distribution%20Shifts%3A%20The%20Role%20of%0A%20%20In-Context%20Learning&entry.906535625=Guanglin%20Zhou%20and%20Zhongyi%20Han%20and%20Shiming%20Chen%20and%20Biwei%20Huang%20and%20Liming%20Zhu%20and%20Salman%20Khan%20and%20Xin%20Gao%20and%20Lina%20Yao&entry.1292438233=%20%20Recent%20studies%20indicate%20that%20large%20multimodal%20models%20%28LMMs%29%20are%20highly%20robust%0Aagainst%20natural%20distribution%20shifts%2C%20often%20surpassing%20previous%20baselines.%0ADespite%20this%2C%20domain-specific%20adaptation%20is%20still%20necessary%2C%20particularly%20in%0Aspecialized%20areas%20like%20healthcare.%20Due%20to%20the%20impracticality%20of%20fine-tuning%0ALMMs%20given%20their%20vast%20parameter%20space%2C%20this%20work%20investigates%20in-context%0Alearning%20%28ICL%29%20as%20an%20effective%20alternative%20for%20enhancing%20LMMs%27%20adaptability.%20We%0Afind%20that%20the%20success%20of%20ICL%20heavily%20relies%20on%20the%20choice%20of%20demonstration%2C%0Amirroring%20challenges%20seen%20in%20large%20language%20models%20but%20introducing%20unique%0Acomplexities%20for%20LMMs%20facing%20distribution%20shifts.%20Our%20study%20addresses%20this%20by%0Aevaluating%20an%20unsupervised%20ICL%20method%2C%20TopKNearestPR%2C%20which%20selects%20in-context%0Aexamples%20through%20a%20nearest%20example%20search%20based%20on%20feature%20similarity.%20We%0Auncover%20that%20its%20effectiveness%20is%20limited%20by%20the%20deficiencies%20of%20pre-trained%0Avision%20encoders%20under%20distribution%20shift%20scenarios.%20To%20address%20these%0Achallenges%2C%20we%20propose%20InvariantSelectPR%2C%20a%20novel%20method%20leveraging%0AClass-conditioned%20Contrastive%20Invariance%20%28CCI%29%20for%20more%20robust%20demonstration%0Aselection.%20Specifically%2C%20CCI%20enhances%20pre-trained%20vision%20encoders%20by%20improving%0Atheir%20discriminative%20capabilities%20across%20different%20classes%20and%20ensuring%0Ainvariance%20to%20domain-specific%20variations.%20This%20enhancement%20allows%20the%20encoders%0Ato%20effectively%20identify%20and%20retrieve%20the%20most%20informative%20examples%2C%20which%20are%0Athen%20used%20to%20guide%20LMMs%20in%20adapting%20to%20new%20query%20samples%20under%20varying%0Adistributions.%20Our%20experiments%20show%20that%20InvariantSelectPR%20substantially%0Aimproves%20the%20adaptability%20of%20LMMs%2C%20achieving%20significant%20performance%20gains%20on%0Abenchmark%20datasets%2C%20with%20a%2034.2%25%24%5Cuparrow%24%20accuracy%20increase%20in%207-shot%20on%0ACamelyon17%20and%2016.9%25%24%5Cuparrow%24%20increase%20in%207-shot%20on%20HAM10000%20compared%20to%20the%0Abaseline%20zero-shot%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12217v1&entry.124074799=Read"},
{"title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM", "author": "Xuchen Li and Xiaokun Feng and Shiyu Hu and Meiqi Wu and Dailing Zhang and Jing Zhang and Kaiqi Huang", "abstract": "  Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding.\n", "link": "http://arxiv.org/abs/2405.12139v1", "date": "2024-05-20", "relevancy": 1.6587, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM&body=Title%3A%20DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM%0AAuthor%3A%20Xuchen%20Li%20and%20Xiaokun%20Feng%20and%20Shiyu%20Hu%20and%20Meiqi%20Wu%20and%20Dailing%20Zhang%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20single%20object%20tracking%20%28SOT%29%20by%0Aintegrating%20natural%20language%20descriptions%20from%20a%20video%2C%20for%20the%20precise%0Atracking%20of%20a%20specified%20object.%20By%20leveraging%20high-level%20semantic%20information%2C%0AVLT%20guides%20object%20tracking%2C%20alleviating%20the%20constraints%20associated%20with%20relying%0Aon%20a%20visual%20modality.%20Nevertheless%2C%20most%20VLT%20benchmarks%20are%20annotated%20in%20a%0Asingle%20granularity%20and%20lack%20a%20coherent%20semantic%20framework%20to%20provide%20scientific%0Aguidance.%20Moreover%2C%20coordinating%20human%20annotators%20for%20high-quality%20annotations%0Ais%20laborious%20and%20time-consuming.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADTLLM-VLT%2C%20which%20automatically%20generates%20extensive%20and%20multi-granularity%20text%0Ato%20enhance%20environmental%20diversity.%20%281%29%20DTLLM-VLT%20generates%20scientific%20and%0Amulti-granularity%20text%20descriptions%20using%20a%20cohesive%20prompt%20framework.%20Its%0Asuccinct%20and%20highly%20adaptable%20design%20allows%20seamless%20integration%20into%20various%0Avisual%20tracking%20benchmarks.%20%282%29%20We%20select%20three%20prominent%20benchmarks%20to%20deploy%0Aour%20approach%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20We%20offer%20four%20granularity%20combinations%20for%20these%20benchmarks%2C%0Aconsidering%20the%20extent%20and%20density%20of%20semantic%20information%2C%20thereby%20showcasing%0Athe%20practicality%20and%20versatility%20of%20DTLLM-VLT.%20%283%29%20We%20conduct%20comparative%0Aexperiments%20on%20VLT%20benchmarks%20with%20different%20text%20granularities%2C%20evaluating%20and%0Aanalyzing%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance.%20Conclusionally%2C%0Athis%20work%20leverages%20LLM%20to%20provide%20multi-granularity%20semantic%20information%20for%0AVLT%20task%20from%20efficient%20and%20diverse%20perspectives%2C%20enabling%20fine-grained%0Aevaluation%20of%20multi-modal%20trackers.%20In%20the%20future%2C%20we%20believe%20this%20work%20can%20be%0Aextended%20to%20more%20datasets%20to%20support%20vision%20datasets%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTLLM-VLT%253A%2520Diverse%2520Text%2520Generation%2520for%2520Visual%2520Language%2520Tracking%2520Based%2520on%250A%2520%2520LLM%26entry.906535625%3DXuchen%2520Li%2520and%2520Xiaokun%2520Feng%2520and%2520Shiyu%2520Hu%2520and%2520Meiqi%2520Wu%2520and%2520Dailing%2520Zhang%2520and%2520Jing%2520Zhang%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Visual%2520Language%2520Tracking%2520%2528VLT%2529%2520enhances%2520single%2520object%2520tracking%2520%2528SOT%2529%2520by%250Aintegrating%2520natural%2520language%2520descriptions%2520from%2520a%2520video%252C%2520for%2520the%2520precise%250Atracking%2520of%2520a%2520specified%2520object.%2520By%2520leveraging%2520high-level%2520semantic%2520information%252C%250AVLT%2520guides%2520object%2520tracking%252C%2520alleviating%2520the%2520constraints%2520associated%2520with%2520relying%250Aon%2520a%2520visual%2520modality.%2520Nevertheless%252C%2520most%2520VLT%2520benchmarks%2520are%2520annotated%2520in%2520a%250Asingle%2520granularity%2520and%2520lack%2520a%2520coherent%2520semantic%2520framework%2520to%2520provide%2520scientific%250Aguidance.%2520Moreover%252C%2520coordinating%2520human%2520annotators%2520for%2520high-quality%2520annotations%250Ais%2520laborious%2520and%2520time-consuming.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ADTLLM-VLT%252C%2520which%2520automatically%2520generates%2520extensive%2520and%2520multi-granularity%2520text%250Ato%2520enhance%2520environmental%2520diversity.%2520%25281%2529%2520DTLLM-VLT%2520generates%2520scientific%2520and%250Amulti-granularity%2520text%2520descriptions%2520using%2520a%2520cohesive%2520prompt%2520framework.%2520Its%250Asuccinct%2520and%2520highly%2520adaptable%2520design%2520allows%2520seamless%2520integration%2520into%2520various%250Avisual%2520tracking%2520benchmarks.%2520%25282%2529%2520We%2520select%2520three%2520prominent%2520benchmarks%2520to%2520deploy%250Aour%2520approach%253A%2520short-term%2520tracking%252C%2520long-term%2520tracking%252C%2520and%2520global%2520instance%250Atracking.%2520We%2520offer%2520four%2520granularity%2520combinations%2520for%2520these%2520benchmarks%252C%250Aconsidering%2520the%2520extent%2520and%2520density%2520of%2520semantic%2520information%252C%2520thereby%2520showcasing%250Athe%2520practicality%2520and%2520versatility%2520of%2520DTLLM-VLT.%2520%25283%2529%2520We%2520conduct%2520comparative%250Aexperiments%2520on%2520VLT%2520benchmarks%2520with%2520different%2520text%2520granularities%252C%2520evaluating%2520and%250Aanalyzing%2520the%2520impact%2520of%2520diverse%2520text%2520on%2520tracking%2520performance.%2520Conclusionally%252C%250Athis%2520work%2520leverages%2520LLM%2520to%2520provide%2520multi-granularity%2520semantic%2520information%2520for%250AVLT%2520task%2520from%2520efficient%2520and%2520diverse%2520perspectives%252C%2520enabling%2520fine-grained%250Aevaluation%2520of%2520multi-modal%2520trackers.%2520In%2520the%2520future%252C%2520we%2520believe%2520this%2520work%2520can%2520be%250Aextended%2520to%2520more%2520datasets%2520to%2520support%2520vision%2520datasets%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM&entry.906535625=Xuchen%20Li%20and%20Xiaokun%20Feng%20and%20Shiyu%20Hu%20and%20Meiqi%20Wu%20and%20Dailing%20Zhang%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20single%20object%20tracking%20%28SOT%29%20by%0Aintegrating%20natural%20language%20descriptions%20from%20a%20video%2C%20for%20the%20precise%0Atracking%20of%20a%20specified%20object.%20By%20leveraging%20high-level%20semantic%20information%2C%0AVLT%20guides%20object%20tracking%2C%20alleviating%20the%20constraints%20associated%20with%20relying%0Aon%20a%20visual%20modality.%20Nevertheless%2C%20most%20VLT%20benchmarks%20are%20annotated%20in%20a%0Asingle%20granularity%20and%20lack%20a%20coherent%20semantic%20framework%20to%20provide%20scientific%0Aguidance.%20Moreover%2C%20coordinating%20human%20annotators%20for%20high-quality%20annotations%0Ais%20laborious%20and%20time-consuming.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADTLLM-VLT%2C%20which%20automatically%20generates%20extensive%20and%20multi-granularity%20text%0Ato%20enhance%20environmental%20diversity.%20%281%29%20DTLLM-VLT%20generates%20scientific%20and%0Amulti-granularity%20text%20descriptions%20using%20a%20cohesive%20prompt%20framework.%20Its%0Asuccinct%20and%20highly%20adaptable%20design%20allows%20seamless%20integration%20into%20various%0Avisual%20tracking%20benchmarks.%20%282%29%20We%20select%20three%20prominent%20benchmarks%20to%20deploy%0Aour%20approach%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20We%20offer%20four%20granularity%20combinations%20for%20these%20benchmarks%2C%0Aconsidering%20the%20extent%20and%20density%20of%20semantic%20information%2C%20thereby%20showcasing%0Athe%20practicality%20and%20versatility%20of%20DTLLM-VLT.%20%283%29%20We%20conduct%20comparative%0Aexperiments%20on%20VLT%20benchmarks%20with%20different%20text%20granularities%2C%20evaluating%20and%0Aanalyzing%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance.%20Conclusionally%2C%0Athis%20work%20leverages%20LLM%20to%20provide%20multi-granularity%20semantic%20information%20for%0AVLT%20task%20from%20efficient%20and%20diverse%20perspectives%2C%20enabling%20fine-grained%0Aevaluation%20of%20multi-modal%20trackers.%20In%20the%20future%2C%20we%20believe%20this%20work%20can%20be%0Aextended%20to%20more%20datasets%20to%20support%20vision%20datasets%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12139v1&entry.124074799=Read"},
{"title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving\n  Machine Translation", "author": "Kamil Guttmann and Miko\u0142aj Pokrywka and Adrian Charkiewicz and Artur Nowakowski", "abstract": "  This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.\n", "link": "http://arxiv.org/abs/2405.11937v1", "date": "2024-05-20", "relevancy": 1.3881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4548}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chasing%20COMET%3A%20Leveraging%20Minimum%20Bayes%20Risk%20Decoding%20for%20Self-Improving%0A%20%20Machine%20Translation&body=Title%3A%20Chasing%20COMET%3A%20Leveraging%20Minimum%20Bayes%20Risk%20Decoding%20for%20Self-Improving%0A%20%20Machine%20Translation%0AAuthor%3A%20Kamil%20Guttmann%20and%20Miko%C5%82aj%20Pokrywka%20and%20Adrian%20Charkiewicz%20and%20Artur%20Nowakowski%0AAbstract%3A%20%20%20This%20paper%20explores%20Minimum%20Bayes%20Risk%20%28MBR%29%20decoding%20for%20self-improvement%20in%0Amachine%20translation%20%28MT%29%2C%20particularly%20for%20domain%20adaptation%20and%20low-resource%0Alanguages.%20We%20implement%20the%20self-improvement%20process%20by%20fine-tuning%20the%20model%0Aon%20its%20MBR-decoded%20forward%20translations.%20By%20employing%20COMET%20as%20the%20MBR%20utility%0Ametric%2C%20we%20aim%20to%20achieve%20the%20reranking%20of%20translations%20that%20better%20aligns%20with%0Ahuman%20preferences.%20The%20paper%20explores%20the%20iterative%20application%20of%20this%0Aapproach%20and%20the%20potential%20need%20for%20language-specific%20MBR%20utility%20metrics.%20The%0Aresults%20demonstrate%20significant%20enhancements%20in%20translation%20quality%20for%20all%0Aexamined%20language%20pairs%2C%20including%20successful%20application%20to%20domain-adapted%0Amodels%20and%20generalisation%20to%20low-resource%20settings.%20This%20highlights%20the%0Apotential%20of%20COMET-guided%20MBR%20for%20efficient%20MT%20self-improvement%20in%20various%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChasing%2520COMET%253A%2520Leveraging%2520Minimum%2520Bayes%2520Risk%2520Decoding%2520for%2520Self-Improving%250A%2520%2520Machine%2520Translation%26entry.906535625%3DKamil%2520Guttmann%2520and%2520Miko%25C5%2582aj%2520Pokrywka%2520and%2520Adrian%2520Charkiewicz%2520and%2520Artur%2520Nowakowski%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520Minimum%2520Bayes%2520Risk%2520%2528MBR%2529%2520decoding%2520for%2520self-improvement%2520in%250Amachine%2520translation%2520%2528MT%2529%252C%2520particularly%2520for%2520domain%2520adaptation%2520and%2520low-resource%250Alanguages.%2520We%2520implement%2520the%2520self-improvement%2520process%2520by%2520fine-tuning%2520the%2520model%250Aon%2520its%2520MBR-decoded%2520forward%2520translations.%2520By%2520employing%2520COMET%2520as%2520the%2520MBR%2520utility%250Ametric%252C%2520we%2520aim%2520to%2520achieve%2520the%2520reranking%2520of%2520translations%2520that%2520better%2520aligns%2520with%250Ahuman%2520preferences.%2520The%2520paper%2520explores%2520the%2520iterative%2520application%2520of%2520this%250Aapproach%2520and%2520the%2520potential%2520need%2520for%2520language-specific%2520MBR%2520utility%2520metrics.%2520The%250Aresults%2520demonstrate%2520significant%2520enhancements%2520in%2520translation%2520quality%2520for%2520all%250Aexamined%2520language%2520pairs%252C%2520including%2520successful%2520application%2520to%2520domain-adapted%250Amodels%2520and%2520generalisation%2520to%2520low-resource%2520settings.%2520This%2520highlights%2520the%250Apotential%2520of%2520COMET-guided%2520MBR%2520for%2520efficient%2520MT%2520self-improvement%2520in%2520various%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chasing%20COMET%3A%20Leveraging%20Minimum%20Bayes%20Risk%20Decoding%20for%20Self-Improving%0A%20%20Machine%20Translation&entry.906535625=Kamil%20Guttmann%20and%20Miko%C5%82aj%20Pokrywka%20and%20Adrian%20Charkiewicz%20and%20Artur%20Nowakowski&entry.1292438233=%20%20This%20paper%20explores%20Minimum%20Bayes%20Risk%20%28MBR%29%20decoding%20for%20self-improvement%20in%0Amachine%20translation%20%28MT%29%2C%20particularly%20for%20domain%20adaptation%20and%20low-resource%0Alanguages.%20We%20implement%20the%20self-improvement%20process%20by%20fine-tuning%20the%20model%0Aon%20its%20MBR-decoded%20forward%20translations.%20By%20employing%20COMET%20as%20the%20MBR%20utility%0Ametric%2C%20we%20aim%20to%20achieve%20the%20reranking%20of%20translations%20that%20better%20aligns%20with%0Ahuman%20preferences.%20The%20paper%20explores%20the%20iterative%20application%20of%20this%0Aapproach%20and%20the%20potential%20need%20for%20language-specific%20MBR%20utility%20metrics.%20The%0Aresults%20demonstrate%20significant%20enhancements%20in%20translation%20quality%20for%20all%0Aexamined%20language%20pairs%2C%20including%20successful%20application%20to%20domain-adapted%0Amodels%20and%20generalisation%20to%20low-resource%20settings.%20This%20highlights%20the%0Apotential%20of%20COMET-guided%20MBR%20for%20efficient%20MT%20self-improvement%20in%20various%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11937v1&entry.124074799=Read"},
{"title": "Nonequilbrium physics of generative diffusion models", "author": "Zhendong Yu and Haiping Huang", "abstract": "  Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interest from industrial application,\nbut a complete picture about inherent mechanisms is still lacking. In this\npaper, we provide a transparent physics analysis of the diffusion models,\nderiving the fluctuation theorem, entropy production, Franz-Parisi potential to\nunderstand the intrinsic phase transitions discovered recently. Our analysis is\nrooted in non-equlibrium physics and concepts from equilibrium physics, i.e.,\ntreating both forward and backward dynamics as a Langevin dynamics, and\ntreating the reverse diffusion generative process as a statistical inference,\nwhere the time-dependent state variables serve as quenched disorder studied in\nspin glass theory. This unified principle is expected to guide machine learning\npractitioners to design better algorithms and theoretical physicists to link\nthe machine learning to non-equilibrium thermodynamics.\n", "link": "http://arxiv.org/abs/2405.11932v1", "date": "2024-05-20", "relevancy": 1.5329, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5427}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5135}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonequilbrium%20physics%20of%20generative%20diffusion%20models&body=Title%3A%20Nonequilbrium%20physics%20of%20generative%20diffusion%20models%0AAuthor%3A%20Zhendong%20Yu%20and%20Haiping%20Huang%0AAbstract%3A%20%20%20Generative%20diffusion%20models%20apply%20the%20concept%20of%20Langevin%20dynamics%20in%20physics%0Ato%20machine%20leaning%2C%20attracting%20a%20lot%20of%20interest%20from%20industrial%20application%2C%0Abut%20a%20complete%20picture%20about%20inherent%20mechanisms%20is%20still%20lacking.%20In%20this%0Apaper%2C%20we%20provide%20a%20transparent%20physics%20analysis%20of%20the%20diffusion%20models%2C%0Aderiving%20the%20fluctuation%20theorem%2C%20entropy%20production%2C%20Franz-Parisi%20potential%20to%0Aunderstand%20the%20intrinsic%20phase%20transitions%20discovered%20recently.%20Our%20analysis%20is%0Arooted%20in%20non-equlibrium%20physics%20and%20concepts%20from%20equilibrium%20physics%2C%20i.e.%2C%0Atreating%20both%20forward%20and%20backward%20dynamics%20as%20a%20Langevin%20dynamics%2C%20and%0Atreating%20the%20reverse%20diffusion%20generative%20process%20as%20a%20statistical%20inference%2C%0Awhere%20the%20time-dependent%20state%20variables%20serve%20as%20quenched%20disorder%20studied%20in%0Aspin%20glass%20theory.%20This%20unified%20principle%20is%20expected%20to%20guide%20machine%20learning%0Apractitioners%20to%20design%20better%20algorithms%20and%20theoretical%20physicists%20to%20link%0Athe%20machine%20learning%20to%20non-equilibrium%20thermodynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonequilbrium%2520physics%2520of%2520generative%2520diffusion%2520models%26entry.906535625%3DZhendong%2520Yu%2520and%2520Haiping%2520Huang%26entry.1292438233%3D%2520%2520Generative%2520diffusion%2520models%2520apply%2520the%2520concept%2520of%2520Langevin%2520dynamics%2520in%2520physics%250Ato%2520machine%2520leaning%252C%2520attracting%2520a%2520lot%2520of%2520interest%2520from%2520industrial%2520application%252C%250Abut%2520a%2520complete%2520picture%2520about%2520inherent%2520mechanisms%2520is%2520still%2520lacking.%2520In%2520this%250Apaper%252C%2520we%2520provide%2520a%2520transparent%2520physics%2520analysis%2520of%2520the%2520diffusion%2520models%252C%250Aderiving%2520the%2520fluctuation%2520theorem%252C%2520entropy%2520production%252C%2520Franz-Parisi%2520potential%2520to%250Aunderstand%2520the%2520intrinsic%2520phase%2520transitions%2520discovered%2520recently.%2520Our%2520analysis%2520is%250Arooted%2520in%2520non-equlibrium%2520physics%2520and%2520concepts%2520from%2520equilibrium%2520physics%252C%2520i.e.%252C%250Atreating%2520both%2520forward%2520and%2520backward%2520dynamics%2520as%2520a%2520Langevin%2520dynamics%252C%2520and%250Atreating%2520the%2520reverse%2520diffusion%2520generative%2520process%2520as%2520a%2520statistical%2520inference%252C%250Awhere%2520the%2520time-dependent%2520state%2520variables%2520serve%2520as%2520quenched%2520disorder%2520studied%2520in%250Aspin%2520glass%2520theory.%2520This%2520unified%2520principle%2520is%2520expected%2520to%2520guide%2520machine%2520learning%250Apractitioners%2520to%2520design%2520better%2520algorithms%2520and%2520theoretical%2520physicists%2520to%2520link%250Athe%2520machine%2520learning%2520to%2520non-equilibrium%2520thermodynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonequilbrium%20physics%20of%20generative%20diffusion%20models&entry.906535625=Zhendong%20Yu%20and%20Haiping%20Huang&entry.1292438233=%20%20Generative%20diffusion%20models%20apply%20the%20concept%20of%20Langevin%20dynamics%20in%20physics%0Ato%20machine%20leaning%2C%20attracting%20a%20lot%20of%20interest%20from%20industrial%20application%2C%0Abut%20a%20complete%20picture%20about%20inherent%20mechanisms%20is%20still%20lacking.%20In%20this%0Apaper%2C%20we%20provide%20a%20transparent%20physics%20analysis%20of%20the%20diffusion%20models%2C%0Aderiving%20the%20fluctuation%20theorem%2C%20entropy%20production%2C%20Franz-Parisi%20potential%20to%0Aunderstand%20the%20intrinsic%20phase%20transitions%20discovered%20recently.%20Our%20analysis%20is%0Arooted%20in%20non-equlibrium%20physics%20and%20concepts%20from%20equilibrium%20physics%2C%20i.e.%2C%0Atreating%20both%20forward%20and%20backward%20dynamics%20as%20a%20Langevin%20dynamics%2C%20and%0Atreating%20the%20reverse%20diffusion%20generative%20process%20as%20a%20statistical%20inference%2C%0Awhere%20the%20time-dependent%20state%20variables%20serve%20as%20quenched%20disorder%20studied%20in%0Aspin%20glass%20theory.%20This%20unified%20principle%20is%20expected%20to%20guide%20machine%20learning%0Apractitioners%20to%20design%20better%20algorithms%20and%20theoretical%20physicists%20to%20link%0Athe%20machine%20learning%20to%20non-equilibrium%20thermodynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11932v1&entry.124074799=Read"},
{"title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical\n  Problem Solving", "author": "Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Rezende and Yoshua Bengio and Michael Mozer and Sanjeev Arora", "abstract": "  Metacognitive knowledge refers to humans' intuitive knowledge of their own\nthinking and reasoning processes. Today's best LLMs clearly possess some\nreasoning processes. The paper gives evidence that they also have metacognitive\nknowledge, including ability to name skills and procedures to apply given a\ntask. We explore this primarily in context of math reasoning, developing a\nprompt-guided interaction procedure to get a powerful LLM to assign sensible\nskill labels to math questions, followed by having it perform semantic\nclustering to obtain coarser families of skill labels. These coarse skill\nlabels look interpretable to humans.\n  To validate that these skill labels are meaningful and relevant to the LLM's\nreasoning processes we perform the following experiments. (a) We ask GPT-4 to\nassign skill labels to training questions in math datasets GSM8K and MATH. (b)\nWhen using an LLM to solve the test questions, we present it with the full list\nof skill labels and ask it to identify the skill needed. Then it is presented\nwith randomly selected exemplar solved questions associated with that skill\nlabel. This improves accuracy on GSM8k and MATH for several strong LLMs,\nincluding code-assisted models. The methodology presented is domain-agnostic,\neven though this article applies it to math problems.\n", "link": "http://arxiv.org/abs/2405.12205v1", "date": "2024-05-20", "relevancy": 1.405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metacognitive%20Capabilities%20of%20LLMs%3A%20An%20Exploration%20in%20Mathematical%0A%20%20Problem%20Solving&body=Title%3A%20Metacognitive%20Capabilities%20of%20LLMs%3A%20An%20Exploration%20in%20Mathematical%0A%20%20Problem%20Solving%0AAuthor%3A%20Aniket%20Didolkar%20and%20Anirudh%20Goyal%20and%20Nan%20Rosemary%20Ke%20and%20Siyuan%20Guo%20and%20Michal%20Valko%20and%20Timothy%20Lillicrap%20and%20Danilo%20Rezende%20and%20Yoshua%20Bengio%20and%20Michael%20Mozer%20and%20Sanjeev%20Arora%0AAbstract%3A%20%20%20Metacognitive%20knowledge%20refers%20to%20humans%27%20intuitive%20knowledge%20of%20their%20own%0Athinking%20and%20reasoning%20processes.%20Today%27s%20best%20LLMs%20clearly%20possess%20some%0Areasoning%20processes.%20The%20paper%20gives%20evidence%20that%20they%20also%20have%20metacognitive%0Aknowledge%2C%20including%20ability%20to%20name%20skills%20and%20procedures%20to%20apply%20given%20a%0Atask.%20We%20explore%20this%20primarily%20in%20context%20of%20math%20reasoning%2C%20developing%20a%0Aprompt-guided%20interaction%20procedure%20to%20get%20a%20powerful%20LLM%20to%20assign%20sensible%0Askill%20labels%20to%20math%20questions%2C%20followed%20by%20having%20it%20perform%20semantic%0Aclustering%20to%20obtain%20coarser%20families%20of%20skill%20labels.%20These%20coarse%20skill%0Alabels%20look%20interpretable%20to%20humans.%0A%20%20To%20validate%20that%20these%20skill%20labels%20are%20meaningful%20and%20relevant%20to%20the%20LLM%27s%0Areasoning%20processes%20we%20perform%20the%20following%20experiments.%20%28a%29%20We%20ask%20GPT-4%20to%0Aassign%20skill%20labels%20to%20training%20questions%20in%20math%20datasets%20GSM8K%20and%20MATH.%20%28b%29%0AWhen%20using%20an%20LLM%20to%20solve%20the%20test%20questions%2C%20we%20present%20it%20with%20the%20full%20list%0Aof%20skill%20labels%20and%20ask%20it%20to%20identify%20the%20skill%20needed.%20Then%20it%20is%20presented%0Awith%20randomly%20selected%20exemplar%20solved%20questions%20associated%20with%20that%20skill%0Alabel.%20This%20improves%20accuracy%20on%20GSM8k%20and%20MATH%20for%20several%20strong%20LLMs%2C%0Aincluding%20code-assisted%20models.%20The%20methodology%20presented%20is%20domain-agnostic%2C%0Aeven%20though%20this%20article%20applies%20it%20to%20math%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetacognitive%2520Capabilities%2520of%2520LLMs%253A%2520An%2520Exploration%2520in%2520Mathematical%250A%2520%2520Problem%2520Solving%26entry.906535625%3DAniket%2520Didolkar%2520and%2520Anirudh%2520Goyal%2520and%2520Nan%2520Rosemary%2520Ke%2520and%2520Siyuan%2520Guo%2520and%2520Michal%2520Valko%2520and%2520Timothy%2520Lillicrap%2520and%2520Danilo%2520Rezende%2520and%2520Yoshua%2520Bengio%2520and%2520Michael%2520Mozer%2520and%2520Sanjeev%2520Arora%26entry.1292438233%3D%2520%2520Metacognitive%2520knowledge%2520refers%2520to%2520humans%2527%2520intuitive%2520knowledge%2520of%2520their%2520own%250Athinking%2520and%2520reasoning%2520processes.%2520Today%2527s%2520best%2520LLMs%2520clearly%2520possess%2520some%250Areasoning%2520processes.%2520The%2520paper%2520gives%2520evidence%2520that%2520they%2520also%2520have%2520metacognitive%250Aknowledge%252C%2520including%2520ability%2520to%2520name%2520skills%2520and%2520procedures%2520to%2520apply%2520given%2520a%250Atask.%2520We%2520explore%2520this%2520primarily%2520in%2520context%2520of%2520math%2520reasoning%252C%2520developing%2520a%250Aprompt-guided%2520interaction%2520procedure%2520to%2520get%2520a%2520powerful%2520LLM%2520to%2520assign%2520sensible%250Askill%2520labels%2520to%2520math%2520questions%252C%2520followed%2520by%2520having%2520it%2520perform%2520semantic%250Aclustering%2520to%2520obtain%2520coarser%2520families%2520of%2520skill%2520labels.%2520These%2520coarse%2520skill%250Alabels%2520look%2520interpretable%2520to%2520humans.%250A%2520%2520To%2520validate%2520that%2520these%2520skill%2520labels%2520are%2520meaningful%2520and%2520relevant%2520to%2520the%2520LLM%2527s%250Areasoning%2520processes%2520we%2520perform%2520the%2520following%2520experiments.%2520%2528a%2529%2520We%2520ask%2520GPT-4%2520to%250Aassign%2520skill%2520labels%2520to%2520training%2520questions%2520in%2520math%2520datasets%2520GSM8K%2520and%2520MATH.%2520%2528b%2529%250AWhen%2520using%2520an%2520LLM%2520to%2520solve%2520the%2520test%2520questions%252C%2520we%2520present%2520it%2520with%2520the%2520full%2520list%250Aof%2520skill%2520labels%2520and%2520ask%2520it%2520to%2520identify%2520the%2520skill%2520needed.%2520Then%2520it%2520is%2520presented%250Awith%2520randomly%2520selected%2520exemplar%2520solved%2520questions%2520associated%2520with%2520that%2520skill%250Alabel.%2520This%2520improves%2520accuracy%2520on%2520GSM8k%2520and%2520MATH%2520for%2520several%2520strong%2520LLMs%252C%250Aincluding%2520code-assisted%2520models.%2520The%2520methodology%2520presented%2520is%2520domain-agnostic%252C%250Aeven%2520though%2520this%2520article%2520applies%2520it%2520to%2520math%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metacognitive%20Capabilities%20of%20LLMs%3A%20An%20Exploration%20in%20Mathematical%0A%20%20Problem%20Solving&entry.906535625=Aniket%20Didolkar%20and%20Anirudh%20Goyal%20and%20Nan%20Rosemary%20Ke%20and%20Siyuan%20Guo%20and%20Michal%20Valko%20and%20Timothy%20Lillicrap%20and%20Danilo%20Rezende%20and%20Yoshua%20Bengio%20and%20Michael%20Mozer%20and%20Sanjeev%20Arora&entry.1292438233=%20%20Metacognitive%20knowledge%20refers%20to%20humans%27%20intuitive%20knowledge%20of%20their%20own%0Athinking%20and%20reasoning%20processes.%20Today%27s%20best%20LLMs%20clearly%20possess%20some%0Areasoning%20processes.%20The%20paper%20gives%20evidence%20that%20they%20also%20have%20metacognitive%0Aknowledge%2C%20including%20ability%20to%20name%20skills%20and%20procedures%20to%20apply%20given%20a%0Atask.%20We%20explore%20this%20primarily%20in%20context%20of%20math%20reasoning%2C%20developing%20a%0Aprompt-guided%20interaction%20procedure%20to%20get%20a%20powerful%20LLM%20to%20assign%20sensible%0Askill%20labels%20to%20math%20questions%2C%20followed%20by%20having%20it%20perform%20semantic%0Aclustering%20to%20obtain%20coarser%20families%20of%20skill%20labels.%20These%20coarse%20skill%0Alabels%20look%20interpretable%20to%20humans.%0A%20%20To%20validate%20that%20these%20skill%20labels%20are%20meaningful%20and%20relevant%20to%20the%20LLM%27s%0Areasoning%20processes%20we%20perform%20the%20following%20experiments.%20%28a%29%20We%20ask%20GPT-4%20to%0Aassign%20skill%20labels%20to%20training%20questions%20in%20math%20datasets%20GSM8K%20and%20MATH.%20%28b%29%0AWhen%20using%20an%20LLM%20to%20solve%20the%20test%20questions%2C%20we%20present%20it%20with%20the%20full%20list%0Aof%20skill%20labels%20and%20ask%20it%20to%20identify%20the%20skill%20needed.%20Then%20it%20is%20presented%0Awith%20randomly%20selected%20exemplar%20solved%20questions%20associated%20with%20that%20skill%0Alabel.%20This%20improves%20accuracy%20on%20GSM8k%20and%20MATH%20for%20several%20strong%20LLMs%2C%0Aincluding%20code-assisted%20models.%20The%20methodology%20presented%20is%20domain-agnostic%2C%0Aeven%20though%20this%20article%20applies%20it%20to%20math%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12205v1&entry.124074799=Read"},
{"title": "Eliciting Problem Specifications via Large Language Models", "author": "Robert E. Wray and James R. Kirk and John E. Laird", "abstract": "  Cognitive systems generally require a human to translate a problem definition\ninto some specification that the cognitive system can use to attempt to solve\nthe problem or perform the task. In this paper, we illustrate that large\nlanguage models (LLMs) can be utilized to map a problem class, defined in\nnatural language, into a semi-formal specification that can then be utilized by\nan existing reasoning and learning system to solve instances from the problem\nclass. We present the design of LLM-enabled cognitive task analyst agent(s).\nImplemented with LLM agents, this system produces a definition of problem\nspaces for tasks specified in natural language. LLM prompts are derived from\nthe definition of problem spaces in the AI literature and general\nproblem-solving strategies (Polya's How to Solve It). A cognitive system can\nthen use the problem-space specification, applying domain-general problem\nsolving strategies (\"weak methods\" such as search), to solve multiple instances\nof problems from the problem class. This result, while preliminary, suggests\nthe potential for speeding cognitive systems research via disintermediation of\nproblem formulation while also retaining core capabilities of cognitive\nsystems, such as robust inference and online learning.\n", "link": "http://arxiv.org/abs/2405.12147v1", "date": "2024-05-20", "relevancy": 0.9673, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliciting%20Problem%20Specifications%20via%20Large%20Language%20Models&body=Title%3A%20Eliciting%20Problem%20Specifications%20via%20Large%20Language%20Models%0AAuthor%3A%20Robert%20E.%20Wray%20and%20James%20R.%20Kirk%20and%20John%20E.%20Laird%0AAbstract%3A%20%20%20Cognitive%20systems%20generally%20require%20a%20human%20to%20translate%20a%20problem%20definition%0Ainto%20some%20specification%20that%20the%20cognitive%20system%20can%20use%20to%20attempt%20to%20solve%0Athe%20problem%20or%20perform%20the%20task.%20In%20this%20paper%2C%20we%20illustrate%20that%20large%0Alanguage%20models%20%28LLMs%29%20can%20be%20utilized%20to%20map%20a%20problem%20class%2C%20defined%20in%0Anatural%20language%2C%20into%20a%20semi-formal%20specification%20that%20can%20then%20be%20utilized%20by%0Aan%20existing%20reasoning%20and%20learning%20system%20to%20solve%20instances%20from%20the%20problem%0Aclass.%20We%20present%20the%20design%20of%20LLM-enabled%20cognitive%20task%20analyst%20agent%28s%29.%0AImplemented%20with%20LLM%20agents%2C%20this%20system%20produces%20a%20definition%20of%20problem%0Aspaces%20for%20tasks%20specified%20in%20natural%20language.%20LLM%20prompts%20are%20derived%20from%0Athe%20definition%20of%20problem%20spaces%20in%20the%20AI%20literature%20and%20general%0Aproblem-solving%20strategies%20%28Polya%27s%20How%20to%20Solve%20It%29.%20A%20cognitive%20system%20can%0Athen%20use%20the%20problem-space%20specification%2C%20applying%20domain-general%20problem%0Asolving%20strategies%20%28%22weak%20methods%22%20such%20as%20search%29%2C%20to%20solve%20multiple%20instances%0Aof%20problems%20from%20the%20problem%20class.%20This%20result%2C%20while%20preliminary%2C%20suggests%0Athe%20potential%20for%20speeding%20cognitive%20systems%20research%20via%20disintermediation%20of%0Aproblem%20formulation%20while%20also%20retaining%20core%20capabilities%20of%20cognitive%0Asystems%2C%20such%20as%20robust%20inference%20and%20online%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliciting%2520Problem%2520Specifications%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DRobert%2520E.%2520Wray%2520and%2520James%2520R.%2520Kirk%2520and%2520John%2520E.%2520Laird%26entry.1292438233%3D%2520%2520Cognitive%2520systems%2520generally%2520require%2520a%2520human%2520to%2520translate%2520a%2520problem%2520definition%250Ainto%2520some%2520specification%2520that%2520the%2520cognitive%2520system%2520can%2520use%2520to%2520attempt%2520to%2520solve%250Athe%2520problem%2520or%2520perform%2520the%2520task.%2520In%2520this%2520paper%252C%2520we%2520illustrate%2520that%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520can%2520be%2520utilized%2520to%2520map%2520a%2520problem%2520class%252C%2520defined%2520in%250Anatural%2520language%252C%2520into%2520a%2520semi-formal%2520specification%2520that%2520can%2520then%2520be%2520utilized%2520by%250Aan%2520existing%2520reasoning%2520and%2520learning%2520system%2520to%2520solve%2520instances%2520from%2520the%2520problem%250Aclass.%2520We%2520present%2520the%2520design%2520of%2520LLM-enabled%2520cognitive%2520task%2520analyst%2520agent%2528s%2529.%250AImplemented%2520with%2520LLM%2520agents%252C%2520this%2520system%2520produces%2520a%2520definition%2520of%2520problem%250Aspaces%2520for%2520tasks%2520specified%2520in%2520natural%2520language.%2520LLM%2520prompts%2520are%2520derived%2520from%250Athe%2520definition%2520of%2520problem%2520spaces%2520in%2520the%2520AI%2520literature%2520and%2520general%250Aproblem-solving%2520strategies%2520%2528Polya%2527s%2520How%2520to%2520Solve%2520It%2529.%2520A%2520cognitive%2520system%2520can%250Athen%2520use%2520the%2520problem-space%2520specification%252C%2520applying%2520domain-general%2520problem%250Asolving%2520strategies%2520%2528%2522weak%2520methods%2522%2520such%2520as%2520search%2529%252C%2520to%2520solve%2520multiple%2520instances%250Aof%2520problems%2520from%2520the%2520problem%2520class.%2520This%2520result%252C%2520while%2520preliminary%252C%2520suggests%250Athe%2520potential%2520for%2520speeding%2520cognitive%2520systems%2520research%2520via%2520disintermediation%2520of%250Aproblem%2520formulation%2520while%2520also%2520retaining%2520core%2520capabilities%2520of%2520cognitive%250Asystems%252C%2520such%2520as%2520robust%2520inference%2520and%2520online%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliciting%20Problem%20Specifications%20via%20Large%20Language%20Models&entry.906535625=Robert%20E.%20Wray%20and%20James%20R.%20Kirk%20and%20John%20E.%20Laird&entry.1292438233=%20%20Cognitive%20systems%20generally%20require%20a%20human%20to%20translate%20a%20problem%20definition%0Ainto%20some%20specification%20that%20the%20cognitive%20system%20can%20use%20to%20attempt%20to%20solve%0Athe%20problem%20or%20perform%20the%20task.%20In%20this%20paper%2C%20we%20illustrate%20that%20large%0Alanguage%20models%20%28LLMs%29%20can%20be%20utilized%20to%20map%20a%20problem%20class%2C%20defined%20in%0Anatural%20language%2C%20into%20a%20semi-formal%20specification%20that%20can%20then%20be%20utilized%20by%0Aan%20existing%20reasoning%20and%20learning%20system%20to%20solve%20instances%20from%20the%20problem%0Aclass.%20We%20present%20the%20design%20of%20LLM-enabled%20cognitive%20task%20analyst%20agent%28s%29.%0AImplemented%20with%20LLM%20agents%2C%20this%20system%20produces%20a%20definition%20of%20problem%0Aspaces%20for%20tasks%20specified%20in%20natural%20language.%20LLM%20prompts%20are%20derived%20from%0Athe%20definition%20of%20problem%20spaces%20in%20the%20AI%20literature%20and%20general%0Aproblem-solving%20strategies%20%28Polya%27s%20How%20to%20Solve%20It%29.%20A%20cognitive%20system%20can%0Athen%20use%20the%20problem-space%20specification%2C%20applying%20domain-general%20problem%0Asolving%20strategies%20%28%22weak%20methods%22%20such%20as%20search%29%2C%20to%20solve%20multiple%20instances%0Aof%20problems%20from%20the%20problem%20class.%20This%20result%2C%20while%20preliminary%2C%20suggests%0Athe%20potential%20for%20speeding%20cognitive%20systems%20research%20via%20disintermediation%20of%0Aproblem%20formulation%20while%20also%20retaining%20core%20capabilities%20of%20cognitive%0Asystems%2C%20such%20as%20robust%20inference%20and%20online%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12147v1&entry.124074799=Read"},
{"title": "Noise-tolerant learnability of shallow quantum circuits from statistics\n  and the cost of quantum pseudorandomness", "author": "Chirag Wadhwa and Mina Doosti", "abstract": "  This work studies the learnability of unknown quantum circuits in the near\nterm. We prove the natural robustness of quantum statistical queries for\nlearning quantum processes and provide an efficient way to benchmark various\nclasses of noise from statistics, which gives us a powerful framework for\ndeveloping noise-tolerant algorithms. We adapt a learning algorithm for\nconstant-depth quantum circuits to the quantum statistical query setting with a\nsmall overhead in the query complexity. We prove average-case lower bounds for\nlearning random quantum circuits of logarithmic and higher depths within\ndiamond distance with statistical queries. Additionally, we show the hardness\nof the quantum threshold search problem from quantum statistical queries and\ndiscuss its implications for the learnability of shallow quantum circuits.\nFinally, we prove that pseudorandom unitaries (PRUs) cannot be constructed\nusing circuits of constant depth by constructing an efficient distinguisher and\nproving a new variation of the quantum no-free lunch theorem.\n", "link": "http://arxiv.org/abs/2405.12085v1", "date": "2024-05-20", "relevancy": 1.5874, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4111}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4095}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise-tolerant%20learnability%20of%20shallow%20quantum%20circuits%20from%20statistics%0A%20%20and%20the%20cost%20of%20quantum%20pseudorandomness&body=Title%3A%20Noise-tolerant%20learnability%20of%20shallow%20quantum%20circuits%20from%20statistics%0A%20%20and%20the%20cost%20of%20quantum%20pseudorandomness%0AAuthor%3A%20Chirag%20Wadhwa%20and%20Mina%20Doosti%0AAbstract%3A%20%20%20This%20work%20studies%20the%20learnability%20of%20unknown%20quantum%20circuits%20in%20the%20near%0Aterm.%20We%20prove%20the%20natural%20robustness%20of%20quantum%20statistical%20queries%20for%0Alearning%20quantum%20processes%20and%20provide%20an%20efficient%20way%20to%20benchmark%20various%0Aclasses%20of%20noise%20from%20statistics%2C%20which%20gives%20us%20a%20powerful%20framework%20for%0Adeveloping%20noise-tolerant%20algorithms.%20We%20adapt%20a%20learning%20algorithm%20for%0Aconstant-depth%20quantum%20circuits%20to%20the%20quantum%20statistical%20query%20setting%20with%20a%0Asmall%20overhead%20in%20the%20query%20complexity.%20We%20prove%20average-case%20lower%20bounds%20for%0Alearning%20random%20quantum%20circuits%20of%20logarithmic%20and%20higher%20depths%20within%0Adiamond%20distance%20with%20statistical%20queries.%20Additionally%2C%20we%20show%20the%20hardness%0Aof%20the%20quantum%20threshold%20search%20problem%20from%20quantum%20statistical%20queries%20and%0Adiscuss%20its%20implications%20for%20the%20learnability%20of%20shallow%20quantum%20circuits.%0AFinally%2C%20we%20prove%20that%20pseudorandom%20unitaries%20%28PRUs%29%20cannot%20be%20constructed%0Ausing%20circuits%20of%20constant%20depth%20by%20constructing%20an%20efficient%20distinguisher%20and%0Aproving%20a%20new%20variation%20of%20the%20quantum%20no-free%20lunch%20theorem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise-tolerant%2520learnability%2520of%2520shallow%2520quantum%2520circuits%2520from%2520statistics%250A%2520%2520and%2520the%2520cost%2520of%2520quantum%2520pseudorandomness%26entry.906535625%3DChirag%2520Wadhwa%2520and%2520Mina%2520Doosti%26entry.1292438233%3D%2520%2520This%2520work%2520studies%2520the%2520learnability%2520of%2520unknown%2520quantum%2520circuits%2520in%2520the%2520near%250Aterm.%2520We%2520prove%2520the%2520natural%2520robustness%2520of%2520quantum%2520statistical%2520queries%2520for%250Alearning%2520quantum%2520processes%2520and%2520provide%2520an%2520efficient%2520way%2520to%2520benchmark%2520various%250Aclasses%2520of%2520noise%2520from%2520statistics%252C%2520which%2520gives%2520us%2520a%2520powerful%2520framework%2520for%250Adeveloping%2520noise-tolerant%2520algorithms.%2520We%2520adapt%2520a%2520learning%2520algorithm%2520for%250Aconstant-depth%2520quantum%2520circuits%2520to%2520the%2520quantum%2520statistical%2520query%2520setting%2520with%2520a%250Asmall%2520overhead%2520in%2520the%2520query%2520complexity.%2520We%2520prove%2520average-case%2520lower%2520bounds%2520for%250Alearning%2520random%2520quantum%2520circuits%2520of%2520logarithmic%2520and%2520higher%2520depths%2520within%250Adiamond%2520distance%2520with%2520statistical%2520queries.%2520Additionally%252C%2520we%2520show%2520the%2520hardness%250Aof%2520the%2520quantum%2520threshold%2520search%2520problem%2520from%2520quantum%2520statistical%2520queries%2520and%250Adiscuss%2520its%2520implications%2520for%2520the%2520learnability%2520of%2520shallow%2520quantum%2520circuits.%250AFinally%252C%2520we%2520prove%2520that%2520pseudorandom%2520unitaries%2520%2528PRUs%2529%2520cannot%2520be%2520constructed%250Ausing%2520circuits%2520of%2520constant%2520depth%2520by%2520constructing%2520an%2520efficient%2520distinguisher%2520and%250Aproving%2520a%2520new%2520variation%2520of%2520the%2520quantum%2520no-free%2520lunch%2520theorem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-tolerant%20learnability%20of%20shallow%20quantum%20circuits%20from%20statistics%0A%20%20and%20the%20cost%20of%20quantum%20pseudorandomness&entry.906535625=Chirag%20Wadhwa%20and%20Mina%20Doosti&entry.1292438233=%20%20This%20work%20studies%20the%20learnability%20of%20unknown%20quantum%20circuits%20in%20the%20near%0Aterm.%20We%20prove%20the%20natural%20robustness%20of%20quantum%20statistical%20queries%20for%0Alearning%20quantum%20processes%20and%20provide%20an%20efficient%20way%20to%20benchmark%20various%0Aclasses%20of%20noise%20from%20statistics%2C%20which%20gives%20us%20a%20powerful%20framework%20for%0Adeveloping%20noise-tolerant%20algorithms.%20We%20adapt%20a%20learning%20algorithm%20for%0Aconstant-depth%20quantum%20circuits%20to%20the%20quantum%20statistical%20query%20setting%20with%20a%0Asmall%20overhead%20in%20the%20query%20complexity.%20We%20prove%20average-case%20lower%20bounds%20for%0Alearning%20random%20quantum%20circuits%20of%20logarithmic%20and%20higher%20depths%20within%0Adiamond%20distance%20with%20statistical%20queries.%20Additionally%2C%20we%20show%20the%20hardness%0Aof%20the%20quantum%20threshold%20search%20problem%20from%20quantum%20statistical%20queries%20and%0Adiscuss%20its%20implications%20for%20the%20learnability%20of%20shallow%20quantum%20circuits.%0AFinally%2C%20we%20prove%20that%20pseudorandom%20unitaries%20%28PRUs%29%20cannot%20be%20constructed%0Ausing%20circuits%20of%20constant%20depth%20by%20constructing%20an%20efficient%20distinguisher%20and%0Aproving%20a%20new%20variation%20of%20the%20quantum%20no-free%20lunch%20theorem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12085v1&entry.124074799=Read"},
{"title": "RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing", "author": "Borna Barahimi and Hakam Singh and Hina Tabassum and Omer Waqar and Mohammad Omer", "abstract": "  WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere\ncommunication devices to sensing instruments, leveraging Channel State\nInformation (CSI) extraction capabilities. Nevertheless, resource-constrained\nIoT devices and the intricacies of deep neural networks necessitate\ntransmitting CSI to cloud servers for sensing. Although feasible, this leads to\nconsiderable communication overhead. In this context, this paper develops a\nnovel Real-time Sensing and Compression Network (RSCNet) which enables sensing\nwith compressed CSI; thereby reducing the communication overheads. RSCNet\nfacilitates optimization across CSI windows composed of a few CSI frames. Once\ntransmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to\nharness data from prior windows, thus bolstering both the sensing accuracy and\nCSI reconstruction. RSCNet adeptly balances the trade-off between CSI\ncompression and sensing precision, thus streamlining real-time cloud-based WiFi\nsensing with reduced communication costs. Numerical findings demonstrate the\ngains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensing\naccuracy of 97.4% with minimal CSI reconstruction error. Numerical results also\nshow a computational analysis of the proposed RSCNet as a function of the\nnumber of CSI frames.\n", "link": "http://arxiv.org/abs/2402.04888v2", "date": "2024-05-20", "relevancy": 1.4188, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4927}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSCNet%3A%20Dynamic%20CSI%20Compression%20for%20Cloud-based%20WiFi%20Sensing&body=Title%3A%20RSCNet%3A%20Dynamic%20CSI%20Compression%20for%20Cloud-based%20WiFi%20Sensing%0AAuthor%3A%20Borna%20Barahimi%20and%20Hakam%20Singh%20and%20Hina%20Tabassum%20and%20Omer%20Waqar%20and%20Mohammad%20Omer%0AAbstract%3A%20%20%20WiFi-enabled%20Internet-of-Things%20%28IoT%29%20devices%20are%20evolving%20from%20mere%0Acommunication%20devices%20to%20sensing%20instruments%2C%20leveraging%20Channel%20State%0AInformation%20%28CSI%29%20extraction%20capabilities.%20Nevertheless%2C%20resource-constrained%0AIoT%20devices%20and%20the%20intricacies%20of%20deep%20neural%20networks%20necessitate%0Atransmitting%20CSI%20to%20cloud%20servers%20for%20sensing.%20Although%20feasible%2C%20this%20leads%20to%0Aconsiderable%20communication%20overhead.%20In%20this%20context%2C%20this%20paper%20develops%20a%0Anovel%20Real-time%20Sensing%20and%20Compression%20Network%20%28RSCNet%29%20which%20enables%20sensing%0Awith%20compressed%20CSI%3B%20thereby%20reducing%20the%20communication%20overheads.%20RSCNet%0Afacilitates%20optimization%20across%20CSI%20windows%20composed%20of%20a%20few%20CSI%20frames.%20Once%0Atransmitted%20to%20cloud%20servers%2C%20it%20employs%20Long%20Short-Term%20Memory%20%28LSTM%29%20units%20to%0Aharness%20data%20from%20prior%20windows%2C%20thus%20bolstering%20both%20the%20sensing%20accuracy%20and%0ACSI%20reconstruction.%20RSCNet%20adeptly%20balances%20the%20trade-off%20between%20CSI%0Acompression%20and%20sensing%20precision%2C%20thus%20streamlining%20real-time%20cloud-based%20WiFi%0Asensing%20with%20reduced%20communication%20costs.%20Numerical%20findings%20demonstrate%20the%0Agains%20of%20RSCNet%20over%20the%20existing%20benchmarks%20like%20SenseFi%2C%20showcasing%20a%20sensing%0Aaccuracy%20of%2097.4%25%20with%20minimal%20CSI%20reconstruction%20error.%20Numerical%20results%20also%0Ashow%20a%20computational%20analysis%20of%20the%20proposed%20RSCNet%20as%20a%20function%20of%20the%0Anumber%20of%20CSI%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSCNet%253A%2520Dynamic%2520CSI%2520Compression%2520for%2520Cloud-based%2520WiFi%2520Sensing%26entry.906535625%3DBorna%2520Barahimi%2520and%2520Hakam%2520Singh%2520and%2520Hina%2520Tabassum%2520and%2520Omer%2520Waqar%2520and%2520Mohammad%2520Omer%26entry.1292438233%3D%2520%2520WiFi-enabled%2520Internet-of-Things%2520%2528IoT%2529%2520devices%2520are%2520evolving%2520from%2520mere%250Acommunication%2520devices%2520to%2520sensing%2520instruments%252C%2520leveraging%2520Channel%2520State%250AInformation%2520%2528CSI%2529%2520extraction%2520capabilities.%2520Nevertheless%252C%2520resource-constrained%250AIoT%2520devices%2520and%2520the%2520intricacies%2520of%2520deep%2520neural%2520networks%2520necessitate%250Atransmitting%2520CSI%2520to%2520cloud%2520servers%2520for%2520sensing.%2520Although%2520feasible%252C%2520this%2520leads%2520to%250Aconsiderable%2520communication%2520overhead.%2520In%2520this%2520context%252C%2520this%2520paper%2520develops%2520a%250Anovel%2520Real-time%2520Sensing%2520and%2520Compression%2520Network%2520%2528RSCNet%2529%2520which%2520enables%2520sensing%250Awith%2520compressed%2520CSI%253B%2520thereby%2520reducing%2520the%2520communication%2520overheads.%2520RSCNet%250Afacilitates%2520optimization%2520across%2520CSI%2520windows%2520composed%2520of%2520a%2520few%2520CSI%2520frames.%2520Once%250Atransmitted%2520to%2520cloud%2520servers%252C%2520it%2520employs%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520units%2520to%250Aharness%2520data%2520from%2520prior%2520windows%252C%2520thus%2520bolstering%2520both%2520the%2520sensing%2520accuracy%2520and%250ACSI%2520reconstruction.%2520RSCNet%2520adeptly%2520balances%2520the%2520trade-off%2520between%2520CSI%250Acompression%2520and%2520sensing%2520precision%252C%2520thus%2520streamlining%2520real-time%2520cloud-based%2520WiFi%250Asensing%2520with%2520reduced%2520communication%2520costs.%2520Numerical%2520findings%2520demonstrate%2520the%250Agains%2520of%2520RSCNet%2520over%2520the%2520existing%2520benchmarks%2520like%2520SenseFi%252C%2520showcasing%2520a%2520sensing%250Aaccuracy%2520of%252097.4%2525%2520with%2520minimal%2520CSI%2520reconstruction%2520error.%2520Numerical%2520results%2520also%250Ashow%2520a%2520computational%2520analysis%2520of%2520the%2520proposed%2520RSCNet%2520as%2520a%2520function%2520of%2520the%250Anumber%2520of%2520CSI%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSCNet%3A%20Dynamic%20CSI%20Compression%20for%20Cloud-based%20WiFi%20Sensing&entry.906535625=Borna%20Barahimi%20and%20Hakam%20Singh%20and%20Hina%20Tabassum%20and%20Omer%20Waqar%20and%20Mohammad%20Omer&entry.1292438233=%20%20WiFi-enabled%20Internet-of-Things%20%28IoT%29%20devices%20are%20evolving%20from%20mere%0Acommunication%20devices%20to%20sensing%20instruments%2C%20leveraging%20Channel%20State%0AInformation%20%28CSI%29%20extraction%20capabilities.%20Nevertheless%2C%20resource-constrained%0AIoT%20devices%20and%20the%20intricacies%20of%20deep%20neural%20networks%20necessitate%0Atransmitting%20CSI%20to%20cloud%20servers%20for%20sensing.%20Although%20feasible%2C%20this%20leads%20to%0Aconsiderable%20communication%20overhead.%20In%20this%20context%2C%20this%20paper%20develops%20a%0Anovel%20Real-time%20Sensing%20and%20Compression%20Network%20%28RSCNet%29%20which%20enables%20sensing%0Awith%20compressed%20CSI%3B%20thereby%20reducing%20the%20communication%20overheads.%20RSCNet%0Afacilitates%20optimization%20across%20CSI%20windows%20composed%20of%20a%20few%20CSI%20frames.%20Once%0Atransmitted%20to%20cloud%20servers%2C%20it%20employs%20Long%20Short-Term%20Memory%20%28LSTM%29%20units%20to%0Aharness%20data%20from%20prior%20windows%2C%20thus%20bolstering%20both%20the%20sensing%20accuracy%20and%0ACSI%20reconstruction.%20RSCNet%20adeptly%20balances%20the%20trade-off%20between%20CSI%0Acompression%20and%20sensing%20precision%2C%20thus%20streamlining%20real-time%20cloud-based%20WiFi%0Asensing%20with%20reduced%20communication%20costs.%20Numerical%20findings%20demonstrate%20the%0Agains%20of%20RSCNet%20over%20the%20existing%20benchmarks%20like%20SenseFi%2C%20showcasing%20a%20sensing%0Aaccuracy%20of%2097.4%25%20with%20minimal%20CSI%20reconstruction%20error.%20Numerical%20results%20also%0Ashow%20a%20computational%20analysis%20of%20the%20proposed%20RSCNet%20as%20a%20function%20of%20the%0Anumber%20of%20CSI%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04888v2&entry.124074799=Read"},
{"title": "On Efficient and Statistical Quality Estimation for Data Annotation", "author": "Jan-Christoph Klie and Rahul Nair and Juan Haladjian and Marc Kirchner", "abstract": "  Annotated datasets are an essential ingredient to train, evaluate, compare\nand productionalize supervised machine learning models. It is therefore\nimperative that annotations are of high quality. For their creation, good\nquality management and thereby reliable quality estimates are needed. Then, if\nquality is insufficient during the annotation process, rectifying measures can\nbe taken to improve it. Quality estimation is often performed by having experts\nmanually label instances as correct or incorrect. But checking all annotated\ninstances tends to be expensive. Therefore, in practice, usually only subsets\nare inspected; sizes are chosen mostly without justification or regard to\nstatistical power and more often than not, are relatively small. Basing\nestimates on small sample sizes, however, can lead to imprecise values for the\nerror rate. Using unnecessarily large sample sizes costs money that could be\nbetter spent, for instance on more annotations. Therefore, we first describe in\ndetail how to use confidence intervals for finding the minimal sample size\nneeded to estimate the annotation error rate. Then, we propose applying\nacceptance sampling as an alternative to error rate estimation We show that\nacceptance sampling can reduce the required sample sizes up to 50% while\nproviding the same statistical guarantees.\n", "link": "http://arxiv.org/abs/2405.11919v1", "date": "2024-05-20", "relevancy": 1.3407, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4239}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Efficient%20and%20Statistical%20Quality%20Estimation%20for%20Data%20Annotation&body=Title%3A%20On%20Efficient%20and%20Statistical%20Quality%20Estimation%20for%20Data%20Annotation%0AAuthor%3A%20Jan-Christoph%20Klie%20and%20Rahul%20Nair%20and%20Juan%20Haladjian%20and%20Marc%20Kirchner%0AAbstract%3A%20%20%20Annotated%20datasets%20are%20an%20essential%20ingredient%20to%20train%2C%20evaluate%2C%20compare%0Aand%20productionalize%20supervised%20machine%20learning%20models.%20It%20is%20therefore%0Aimperative%20that%20annotations%20are%20of%20high%20quality.%20For%20their%20creation%2C%20good%0Aquality%20management%20and%20thereby%20reliable%20quality%20estimates%20are%20needed.%20Then%2C%20if%0Aquality%20is%20insufficient%20during%20the%20annotation%20process%2C%20rectifying%20measures%20can%0Abe%20taken%20to%20improve%20it.%20Quality%20estimation%20is%20often%20performed%20by%20having%20experts%0Amanually%20label%20instances%20as%20correct%20or%20incorrect.%20But%20checking%20all%20annotated%0Ainstances%20tends%20to%20be%20expensive.%20Therefore%2C%20in%20practice%2C%20usually%20only%20subsets%0Aare%20inspected%3B%20sizes%20are%20chosen%20mostly%20without%20justification%20or%20regard%20to%0Astatistical%20power%20and%20more%20often%20than%20not%2C%20are%20relatively%20small.%20Basing%0Aestimates%20on%20small%20sample%20sizes%2C%20however%2C%20can%20lead%20to%20imprecise%20values%20for%20the%0Aerror%20rate.%20Using%20unnecessarily%20large%20sample%20sizes%20costs%20money%20that%20could%20be%0Abetter%20spent%2C%20for%20instance%20on%20more%20annotations.%20Therefore%2C%20we%20first%20describe%20in%0Adetail%20how%20to%20use%20confidence%20intervals%20for%20finding%20the%20minimal%20sample%20size%0Aneeded%20to%20estimate%20the%20annotation%20error%20rate.%20Then%2C%20we%20propose%20applying%0Aacceptance%20sampling%20as%20an%20alternative%20to%20error%20rate%20estimation%20We%20show%20that%0Aacceptance%20sampling%20can%20reduce%20the%20required%20sample%20sizes%20up%20to%2050%25%20while%0Aproviding%20the%20same%20statistical%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Efficient%2520and%2520Statistical%2520Quality%2520Estimation%2520for%2520Data%2520Annotation%26entry.906535625%3DJan-Christoph%2520Klie%2520and%2520Rahul%2520Nair%2520and%2520Juan%2520Haladjian%2520and%2520Marc%2520Kirchner%26entry.1292438233%3D%2520%2520Annotated%2520datasets%2520are%2520an%2520essential%2520ingredient%2520to%2520train%252C%2520evaluate%252C%2520compare%250Aand%2520productionalize%2520supervised%2520machine%2520learning%2520models.%2520It%2520is%2520therefore%250Aimperative%2520that%2520annotations%2520are%2520of%2520high%2520quality.%2520For%2520their%2520creation%252C%2520good%250Aquality%2520management%2520and%2520thereby%2520reliable%2520quality%2520estimates%2520are%2520needed.%2520Then%252C%2520if%250Aquality%2520is%2520insufficient%2520during%2520the%2520annotation%2520process%252C%2520rectifying%2520measures%2520can%250Abe%2520taken%2520to%2520improve%2520it.%2520Quality%2520estimation%2520is%2520often%2520performed%2520by%2520having%2520experts%250Amanually%2520label%2520instances%2520as%2520correct%2520or%2520incorrect.%2520But%2520checking%2520all%2520annotated%250Ainstances%2520tends%2520to%2520be%2520expensive.%2520Therefore%252C%2520in%2520practice%252C%2520usually%2520only%2520subsets%250Aare%2520inspected%253B%2520sizes%2520are%2520chosen%2520mostly%2520without%2520justification%2520or%2520regard%2520to%250Astatistical%2520power%2520and%2520more%2520often%2520than%2520not%252C%2520are%2520relatively%2520small.%2520Basing%250Aestimates%2520on%2520small%2520sample%2520sizes%252C%2520however%252C%2520can%2520lead%2520to%2520imprecise%2520values%2520for%2520the%250Aerror%2520rate.%2520Using%2520unnecessarily%2520large%2520sample%2520sizes%2520costs%2520money%2520that%2520could%2520be%250Abetter%2520spent%252C%2520for%2520instance%2520on%2520more%2520annotations.%2520Therefore%252C%2520we%2520first%2520describe%2520in%250Adetail%2520how%2520to%2520use%2520confidence%2520intervals%2520for%2520finding%2520the%2520minimal%2520sample%2520size%250Aneeded%2520to%2520estimate%2520the%2520annotation%2520error%2520rate.%2520Then%252C%2520we%2520propose%2520applying%250Aacceptance%2520sampling%2520as%2520an%2520alternative%2520to%2520error%2520rate%2520estimation%2520We%2520show%2520that%250Aacceptance%2520sampling%2520can%2520reduce%2520the%2520required%2520sample%2520sizes%2520up%2520to%252050%2525%2520while%250Aproviding%2520the%2520same%2520statistical%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Efficient%20and%20Statistical%20Quality%20Estimation%20for%20Data%20Annotation&entry.906535625=Jan-Christoph%20Klie%20and%20Rahul%20Nair%20and%20Juan%20Haladjian%20and%20Marc%20Kirchner&entry.1292438233=%20%20Annotated%20datasets%20are%20an%20essential%20ingredient%20to%20train%2C%20evaluate%2C%20compare%0Aand%20productionalize%20supervised%20machine%20learning%20models.%20It%20is%20therefore%0Aimperative%20that%20annotations%20are%20of%20high%20quality.%20For%20their%20creation%2C%20good%0Aquality%20management%20and%20thereby%20reliable%20quality%20estimates%20are%20needed.%20Then%2C%20if%0Aquality%20is%20insufficient%20during%20the%20annotation%20process%2C%20rectifying%20measures%20can%0Abe%20taken%20to%20improve%20it.%20Quality%20estimation%20is%20often%20performed%20by%20having%20experts%0Amanually%20label%20instances%20as%20correct%20or%20incorrect.%20But%20checking%20all%20annotated%0Ainstances%20tends%20to%20be%20expensive.%20Therefore%2C%20in%20practice%2C%20usually%20only%20subsets%0Aare%20inspected%3B%20sizes%20are%20chosen%20mostly%20without%20justification%20or%20regard%20to%0Astatistical%20power%20and%20more%20often%20than%20not%2C%20are%20relatively%20small.%20Basing%0Aestimates%20on%20small%20sample%20sizes%2C%20however%2C%20can%20lead%20to%20imprecise%20values%20for%20the%0Aerror%20rate.%20Using%20unnecessarily%20large%20sample%20sizes%20costs%20money%20that%20could%20be%0Abetter%20spent%2C%20for%20instance%20on%20more%20annotations.%20Therefore%2C%20we%20first%20describe%20in%0Adetail%20how%20to%20use%20confidence%20intervals%20for%20finding%20the%20minimal%20sample%20size%0Aneeded%20to%20estimate%20the%20annotation%20error%20rate.%20Then%2C%20we%20propose%20applying%0Aacceptance%20sampling%20as%20an%20alternative%20to%20error%20rate%20estimation%20We%20show%20that%0Aacceptance%20sampling%20can%20reduce%20the%20required%20sample%20sizes%20up%20to%2050%25%20while%0Aproviding%20the%20same%20statistical%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11919v1&entry.124074799=Read"},
{"title": "RulE: Knowledge Graph Reasoning with Rule Embedding", "author": "Xiaojuan Tang and Song-Chun Zhu and Yitao Liang and Muhan Zhang", "abstract": "  Knowledge graph (KG) reasoning is an important problem for knowledge graphs.\nIn this paper, we propose a novel and principled framework called \\textbf{RulE}\n(stands for {Rul}e {E}mbedding) to effectively leverage logical rules to\nenhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE\nlearns rule embeddings from existing triplets and first-order {rules} by\njointly representing \\textbf{entities}, \\textbf{relations} and \\textbf{logical\nrules} in a unified embedding space. Based on the learned rule embeddings, a\nconfidence score can be calculated for each rule, reflecting its consistency\nwith the observed triplets. This allows us to perform logical rule inference in\na soft way, thus alleviating the brittleness of logic. On the other hand, RulE\ninjects prior logical rule information into the embedding space, enriching and\nregularizing the entity/relation embeddings. This makes KGE alone perform\nbetter too. RulE is conceptually simple and empirically effective. We conduct\nextensive experiments to verify each component of RulE. Results on multiple\nbenchmarks reveal that our model outperforms the majority of existing\nembedding-based and rule-based approaches.\n", "link": "http://arxiv.org/abs/2210.14905v3", "date": "2024-05-20", "relevancy": 0.9032, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RulE%3A%20Knowledge%20Graph%20Reasoning%20with%20Rule%20Embedding&body=Title%3A%20RulE%3A%20Knowledge%20Graph%20Reasoning%20with%20Rule%20Embedding%0AAuthor%3A%20Xiaojuan%20Tang%20and%20Song-Chun%20Zhu%20and%20Yitao%20Liang%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Knowledge%20graph%20%28KG%29%20reasoning%20is%20an%20important%20problem%20for%20knowledge%20graphs.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20and%20principled%20framework%20called%20%5Ctextbf%7BRulE%7D%0A%28stands%20for%20%7BRul%7De%20%7BE%7Dmbedding%29%20to%20effectively%20leverage%20logical%20rules%20to%0Aenhance%20KG%20reasoning.%20Unlike%20knowledge%20graph%20embedding%20%28KGE%29%20methods%2C%20RulE%0Alearns%20rule%20embeddings%20from%20existing%20triplets%20and%20first-order%20%7Brules%7D%20by%0Ajointly%20representing%20%5Ctextbf%7Bentities%7D%2C%20%5Ctextbf%7Brelations%7D%20and%20%5Ctextbf%7Blogical%0Arules%7D%20in%20a%20unified%20embedding%20space.%20Based%20on%20the%20learned%20rule%20embeddings%2C%20a%0Aconfidence%20score%20can%20be%20calculated%20for%20each%20rule%2C%20reflecting%20its%20consistency%0Awith%20the%20observed%20triplets.%20This%20allows%20us%20to%20perform%20logical%20rule%20inference%20in%0Aa%20soft%20way%2C%20thus%20alleviating%20the%20brittleness%20of%20logic.%20On%20the%20other%20hand%2C%20RulE%0Ainjects%20prior%20logical%20rule%20information%20into%20the%20embedding%20space%2C%20enriching%20and%0Aregularizing%20the%20entity/relation%20embeddings.%20This%20makes%20KGE%20alone%20perform%0Abetter%20too.%20RulE%20is%20conceptually%20simple%20and%20empirically%20effective.%20We%20conduct%0Aextensive%20experiments%20to%20verify%20each%20component%20of%20RulE.%20Results%20on%20multiple%0Abenchmarks%20reveal%20that%20our%20model%20outperforms%20the%20majority%20of%20existing%0Aembedding-based%20and%20rule-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.14905v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRulE%253A%2520Knowledge%2520Graph%2520Reasoning%2520with%2520Rule%2520Embedding%26entry.906535625%3DXiaojuan%2520Tang%2520and%2520Song-Chun%2520Zhu%2520and%2520Yitao%2520Liang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520%2528KG%2529%2520reasoning%2520is%2520an%2520important%2520problem%2520for%2520knowledge%2520graphs.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520principled%2520framework%2520called%2520%255Ctextbf%257BRulE%257D%250A%2528stands%2520for%2520%257BRul%257De%2520%257BE%257Dmbedding%2529%2520to%2520effectively%2520leverage%2520logical%2520rules%2520to%250Aenhance%2520KG%2520reasoning.%2520Unlike%2520knowledge%2520graph%2520embedding%2520%2528KGE%2529%2520methods%252C%2520RulE%250Alearns%2520rule%2520embeddings%2520from%2520existing%2520triplets%2520and%2520first-order%2520%257Brules%257D%2520by%250Ajointly%2520representing%2520%255Ctextbf%257Bentities%257D%252C%2520%255Ctextbf%257Brelations%257D%2520and%2520%255Ctextbf%257Blogical%250Arules%257D%2520in%2520a%2520unified%2520embedding%2520space.%2520Based%2520on%2520the%2520learned%2520rule%2520embeddings%252C%2520a%250Aconfidence%2520score%2520can%2520be%2520calculated%2520for%2520each%2520rule%252C%2520reflecting%2520its%2520consistency%250Awith%2520the%2520observed%2520triplets.%2520This%2520allows%2520us%2520to%2520perform%2520logical%2520rule%2520inference%2520in%250Aa%2520soft%2520way%252C%2520thus%2520alleviating%2520the%2520brittleness%2520of%2520logic.%2520On%2520the%2520other%2520hand%252C%2520RulE%250Ainjects%2520prior%2520logical%2520rule%2520information%2520into%2520the%2520embedding%2520space%252C%2520enriching%2520and%250Aregularizing%2520the%2520entity/relation%2520embeddings.%2520This%2520makes%2520KGE%2520alone%2520perform%250Abetter%2520too.%2520RulE%2520is%2520conceptually%2520simple%2520and%2520empirically%2520effective.%2520We%2520conduct%250Aextensive%2520experiments%2520to%2520verify%2520each%2520component%2520of%2520RulE.%2520Results%2520on%2520multiple%250Abenchmarks%2520reveal%2520that%2520our%2520model%2520outperforms%2520the%2520majority%2520of%2520existing%250Aembedding-based%2520and%2520rule-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.14905v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RulE%3A%20Knowledge%20Graph%20Reasoning%20with%20Rule%20Embedding&entry.906535625=Xiaojuan%20Tang%20and%20Song-Chun%20Zhu%20and%20Yitao%20Liang%20and%20Muhan%20Zhang&entry.1292438233=%20%20Knowledge%20graph%20%28KG%29%20reasoning%20is%20an%20important%20problem%20for%20knowledge%20graphs.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20and%20principled%20framework%20called%20%5Ctextbf%7BRulE%7D%0A%28stands%20for%20%7BRul%7De%20%7BE%7Dmbedding%29%20to%20effectively%20leverage%20logical%20rules%20to%0Aenhance%20KG%20reasoning.%20Unlike%20knowledge%20graph%20embedding%20%28KGE%29%20methods%2C%20RulE%0Alearns%20rule%20embeddings%20from%20existing%20triplets%20and%20first-order%20%7Brules%7D%20by%0Ajointly%20representing%20%5Ctextbf%7Bentities%7D%2C%20%5Ctextbf%7Brelations%7D%20and%20%5Ctextbf%7Blogical%0Arules%7D%20in%20a%20unified%20embedding%20space.%20Based%20on%20the%20learned%20rule%20embeddings%2C%20a%0Aconfidence%20score%20can%20be%20calculated%20for%20each%20rule%2C%20reflecting%20its%20consistency%0Awith%20the%20observed%20triplets.%20This%20allows%20us%20to%20perform%20logical%20rule%20inference%20in%0Aa%20soft%20way%2C%20thus%20alleviating%20the%20brittleness%20of%20logic.%20On%20the%20other%20hand%2C%20RulE%0Ainjects%20prior%20logical%20rule%20information%20into%20the%20embedding%20space%2C%20enriching%20and%0Aregularizing%20the%20entity/relation%20embeddings.%20This%20makes%20KGE%20alone%20perform%0Abetter%20too.%20RulE%20is%20conceptually%20simple%20and%20empirically%20effective.%20We%20conduct%0Aextensive%20experiments%20to%20verify%20each%20component%20of%20RulE.%20Results%20on%20multiple%0Abenchmarks%20reveal%20that%20our%20model%20outperforms%20the%20majority%20of%20existing%0Aembedding-based%20and%20rule-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.14905v3&entry.124074799=Read"},
{"title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models\n  for Lay Summarization of Scientific Articles", "author": "Tabea M. G. Pakull and Hendrik Damm and Ahmad Idrissi-Yaghir and Henning Sch\u00e4fer and Peter A. Horn and Christoph M. Friedrich", "abstract": "  This paper details the efforts of the WisPerMed team in the BioLaySumm2024\nShared Task on automatic lay summarization in the biomedical domain, aimed at\nmaking scientific publications accessible to non-specialists. Large language\nmodels (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned\nand employed to create lay summaries from complex scientific texts. The\nsummarization performance was enhanced through various approaches, including\ninstruction tuning, few-shot learning, and prompt variations tailored to\nincorporate specific context information. The experiments demonstrated that\nfine-tuning generally led to the best performance across most evaluated\nmetrics. Few-shot learning notably improved the models' ability to generate\nrelevant and factually accurate texts, particularly when using a well-crafted\nprompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize\nthe selection of text outputs based on readability and factuality metrics was\ndeveloped. Out of 54 participants, the WisPerMed team reached the 4th place,\nmeasured by readability, factuality, and relevance. Determined by the overall\nscore, our approach improved upon the baseline by approx. 5.5 percentage points\nand was only approx 1.5 percentage points behind the first place.\n", "link": "http://arxiv.org/abs/2405.11950v1", "date": "2024-05-20", "relevancy": 1.3699, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WisPerMed%20at%20BioLaySumm%3A%20Adapting%20Autoregressive%20Large%20Language%20Models%0A%20%20for%20Lay%20Summarization%20of%20Scientific%20Articles&body=Title%3A%20WisPerMed%20at%20BioLaySumm%3A%20Adapting%20Autoregressive%20Large%20Language%20Models%0A%20%20for%20Lay%20Summarization%20of%20Scientific%20Articles%0AAuthor%3A%20Tabea%20M.%20G.%20Pakull%20and%20Hendrik%20Damm%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Peter%20A.%20Horn%20and%20Christoph%20M.%20Friedrich%0AAbstract%3A%20%20%20This%20paper%20details%20the%20efforts%20of%20the%20WisPerMed%20team%20in%20the%20BioLaySumm2024%0AShared%20Task%20on%20automatic%20lay%20summarization%20in%20the%20biomedical%20domain%2C%20aimed%20at%0Amaking%20scientific%20publications%20accessible%20to%20non-specialists.%20Large%20language%0Amodels%20%28LLMs%29%2C%20specifically%20the%20BioMistral%20and%20Llama3%20models%2C%20were%20fine-tuned%0Aand%20employed%20to%20create%20lay%20summaries%20from%20complex%20scientific%20texts.%20The%0Asummarization%20performance%20was%20enhanced%20through%20various%20approaches%2C%20including%0Ainstruction%20tuning%2C%20few-shot%20learning%2C%20and%20prompt%20variations%20tailored%20to%0Aincorporate%20specific%20context%20information.%20The%20experiments%20demonstrated%20that%0Afine-tuning%20generally%20led%20to%20the%20best%20performance%20across%20most%20evaluated%0Ametrics.%20Few-shot%20learning%20notably%20improved%20the%20models%27%20ability%20to%20generate%0Arelevant%20and%20factually%20accurate%20texts%2C%20particularly%20when%20using%20a%20well-crafted%0Aprompt.%20Additionally%2C%20a%20Dynamic%20Expert%20Selection%20%28DES%29%20mechanism%20to%20optimize%0Athe%20selection%20of%20text%20outputs%20based%20on%20readability%20and%20factuality%20metrics%20was%0Adeveloped.%20Out%20of%2054%20participants%2C%20the%20WisPerMed%20team%20reached%20the%204th%20place%2C%0Ameasured%20by%20readability%2C%20factuality%2C%20and%20relevance.%20Determined%20by%20the%20overall%0Ascore%2C%20our%20approach%20improved%20upon%20the%20baseline%20by%20approx.%205.5%20percentage%20points%0Aand%20was%20only%20approx%201.5%20percentage%20points%20behind%20the%20first%20place.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWisPerMed%2520at%2520BioLaySumm%253A%2520Adapting%2520Autoregressive%2520Large%2520Language%2520Models%250A%2520%2520for%2520Lay%2520Summarization%2520of%2520Scientific%2520Articles%26entry.906535625%3DTabea%2520M.%2520G.%2520Pakull%2520and%2520Hendrik%2520Damm%2520and%2520Ahmad%2520Idrissi-Yaghir%2520and%2520Henning%2520Sch%25C3%25A4fer%2520and%2520Peter%2520A.%2520Horn%2520and%2520Christoph%2520M.%2520Friedrich%26entry.1292438233%3D%2520%2520This%2520paper%2520details%2520the%2520efforts%2520of%2520the%2520WisPerMed%2520team%2520in%2520the%2520BioLaySumm2024%250AShared%2520Task%2520on%2520automatic%2520lay%2520summarization%2520in%2520the%2520biomedical%2520domain%252C%2520aimed%2520at%250Amaking%2520scientific%2520publications%2520accessible%2520to%2520non-specialists.%2520Large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520specifically%2520the%2520BioMistral%2520and%2520Llama3%2520models%252C%2520were%2520fine-tuned%250Aand%2520employed%2520to%2520create%2520lay%2520summaries%2520from%2520complex%2520scientific%2520texts.%2520The%250Asummarization%2520performance%2520was%2520enhanced%2520through%2520various%2520approaches%252C%2520including%250Ainstruction%2520tuning%252C%2520few-shot%2520learning%252C%2520and%2520prompt%2520variations%2520tailored%2520to%250Aincorporate%2520specific%2520context%2520information.%2520The%2520experiments%2520demonstrated%2520that%250Afine-tuning%2520generally%2520led%2520to%2520the%2520best%2520performance%2520across%2520most%2520evaluated%250Ametrics.%2520Few-shot%2520learning%2520notably%2520improved%2520the%2520models%2527%2520ability%2520to%2520generate%250Arelevant%2520and%2520factually%2520accurate%2520texts%252C%2520particularly%2520when%2520using%2520a%2520well-crafted%250Aprompt.%2520Additionally%252C%2520a%2520Dynamic%2520Expert%2520Selection%2520%2528DES%2529%2520mechanism%2520to%2520optimize%250Athe%2520selection%2520of%2520text%2520outputs%2520based%2520on%2520readability%2520and%2520factuality%2520metrics%2520was%250Adeveloped.%2520Out%2520of%252054%2520participants%252C%2520the%2520WisPerMed%2520team%2520reached%2520the%25204th%2520place%252C%250Ameasured%2520by%2520readability%252C%2520factuality%252C%2520and%2520relevance.%2520Determined%2520by%2520the%2520overall%250Ascore%252C%2520our%2520approach%2520improved%2520upon%2520the%2520baseline%2520by%2520approx.%25205.5%2520percentage%2520points%250Aand%2520was%2520only%2520approx%25201.5%2520percentage%2520points%2520behind%2520the%2520first%2520place.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WisPerMed%20at%20BioLaySumm%3A%20Adapting%20Autoregressive%20Large%20Language%20Models%0A%20%20for%20Lay%20Summarization%20of%20Scientific%20Articles&entry.906535625=Tabea%20M.%20G.%20Pakull%20and%20Hendrik%20Damm%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Peter%20A.%20Horn%20and%20Christoph%20M.%20Friedrich&entry.1292438233=%20%20This%20paper%20details%20the%20efforts%20of%20the%20WisPerMed%20team%20in%20the%20BioLaySumm2024%0AShared%20Task%20on%20automatic%20lay%20summarization%20in%20the%20biomedical%20domain%2C%20aimed%20at%0Amaking%20scientific%20publications%20accessible%20to%20non-specialists.%20Large%20language%0Amodels%20%28LLMs%29%2C%20specifically%20the%20BioMistral%20and%20Llama3%20models%2C%20were%20fine-tuned%0Aand%20employed%20to%20create%20lay%20summaries%20from%20complex%20scientific%20texts.%20The%0Asummarization%20performance%20was%20enhanced%20through%20various%20approaches%2C%20including%0Ainstruction%20tuning%2C%20few-shot%20learning%2C%20and%20prompt%20variations%20tailored%20to%0Aincorporate%20specific%20context%20information.%20The%20experiments%20demonstrated%20that%0Afine-tuning%20generally%20led%20to%20the%20best%20performance%20across%20most%20evaluated%0Ametrics.%20Few-shot%20learning%20notably%20improved%20the%20models%27%20ability%20to%20generate%0Arelevant%20and%20factually%20accurate%20texts%2C%20particularly%20when%20using%20a%20well-crafted%0Aprompt.%20Additionally%2C%20a%20Dynamic%20Expert%20Selection%20%28DES%29%20mechanism%20to%20optimize%0Athe%20selection%20of%20text%20outputs%20based%20on%20readability%20and%20factuality%20metrics%20was%0Adeveloped.%20Out%20of%2054%20participants%2C%20the%20WisPerMed%20team%20reached%20the%204th%20place%2C%0Ameasured%20by%20readability%2C%20factuality%2C%20and%20relevance.%20Determined%20by%20the%20overall%0Ascore%2C%20our%20approach%20improved%20upon%20the%20baseline%20by%20approx.%205.5%20percentage%20points%0Aand%20was%20only%20approx%201.5%20percentage%20points%20behind%20the%20first%20place.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11950v1&entry.124074799=Read"},
{"title": "A Temporal Stochastic Bias Correction using a Machine Learning Attention\n  model", "author": "Omer Nivron and Damon J. Wischik and Mathieu Vrac and Emily Shuckburgh and Alex T. Archibald", "abstract": "  Climate models are biased with respect to real-world observations. They\nusually need to be adjusted before being used in impact studies. The suite of\nstatistical methods that enable such adjustments is called bias correction\n(BC). However, BC methods currently struggle to adjust temporal biases. Because\nthey mostly disregard the dependence between consecutive time points. As a\nresult, climate statistics with long-range temporal properties, such as\nheatwave duration and frequency, cannot be corrected accurately. This makes it\nmore difficult to produce reliable impact studies on such climate statistics.\nThis paper offers a novel BC methodology to correct temporal biases. This is\nmade possible by rethinking the philosophy behind BC. We will introduce BC as a\ntime-indexed regression task with stochastic outputs. Rethinking BC enables us\nto adapt state-of-the-art machine learning (ML) attention models and thereby\nlearn different types of biases, including temporal asynchronicities. With a\ncase study of heatwave duration statistics in Abuja, Nigeria, and Tokyo, Japan,\nwe show more accurate results than current climate model outputs and\nalternative BC methods.\n", "link": "http://arxiv.org/abs/2402.14169v4", "date": "2024-05-20", "relevancy": 0.9195, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4691}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Temporal%20Stochastic%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%0A%20%20model&body=Title%3A%20A%20Temporal%20Stochastic%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%0A%20%20model%0AAuthor%3A%20Omer%20Nivron%20and%20Damon%20J.%20Wischik%20and%20Mathieu%20Vrac%20and%20Emily%20Shuckburgh%20and%20Alex%20T.%20Archibald%0AAbstract%3A%20%20%20Climate%20models%20are%20biased%20with%20respect%20to%20real-world%20observations.%20They%0Ausually%20need%20to%20be%20adjusted%20before%20being%20used%20in%20impact%20studies.%20The%20suite%20of%0Astatistical%20methods%20that%20enable%20such%20adjustments%20is%20called%20bias%20correction%0A%28BC%29.%20However%2C%20BC%20methods%20currently%20struggle%20to%20adjust%20temporal%20biases.%20Because%0Athey%20mostly%20disregard%20the%20dependence%20between%20consecutive%20time%20points.%20As%20a%0Aresult%2C%20climate%20statistics%20with%20long-range%20temporal%20properties%2C%20such%20as%0Aheatwave%20duration%20and%20frequency%2C%20cannot%20be%20corrected%20accurately.%20This%20makes%20it%0Amore%20difficult%20to%20produce%20reliable%20impact%20studies%20on%20such%20climate%20statistics.%0AThis%20paper%20offers%20a%20novel%20BC%20methodology%20to%20correct%20temporal%20biases.%20This%20is%0Amade%20possible%20by%20rethinking%20the%20philosophy%20behind%20BC.%20We%20will%20introduce%20BC%20as%20a%0Atime-indexed%20regression%20task%20with%20stochastic%20outputs.%20Rethinking%20BC%20enables%20us%0Ato%20adapt%20state-of-the-art%20machine%20learning%20%28ML%29%20attention%20models%20and%20thereby%0Alearn%20different%20types%20of%20biases%2C%20including%20temporal%20asynchronicities.%20With%20a%0Acase%20study%20of%20heatwave%20duration%20statistics%20in%20Abuja%2C%20Nigeria%2C%20and%20Tokyo%2C%20Japan%2C%0Awe%20show%20more%20accurate%20results%20than%20current%20climate%20model%20outputs%20and%0Aalternative%20BC%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14169v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Temporal%2520Stochastic%2520Bias%2520Correction%2520using%2520a%2520Machine%2520Learning%2520Attention%250A%2520%2520model%26entry.906535625%3DOmer%2520Nivron%2520and%2520Damon%2520J.%2520Wischik%2520and%2520Mathieu%2520Vrac%2520and%2520Emily%2520Shuckburgh%2520and%2520Alex%2520T.%2520Archibald%26entry.1292438233%3D%2520%2520Climate%2520models%2520are%2520biased%2520with%2520respect%2520to%2520real-world%2520observations.%2520They%250Ausually%2520need%2520to%2520be%2520adjusted%2520before%2520being%2520used%2520in%2520impact%2520studies.%2520The%2520suite%2520of%250Astatistical%2520methods%2520that%2520enable%2520such%2520adjustments%2520is%2520called%2520bias%2520correction%250A%2528BC%2529.%2520However%252C%2520BC%2520methods%2520currently%2520struggle%2520to%2520adjust%2520temporal%2520biases.%2520Because%250Athey%2520mostly%2520disregard%2520the%2520dependence%2520between%2520consecutive%2520time%2520points.%2520As%2520a%250Aresult%252C%2520climate%2520statistics%2520with%2520long-range%2520temporal%2520properties%252C%2520such%2520as%250Aheatwave%2520duration%2520and%2520frequency%252C%2520cannot%2520be%2520corrected%2520accurately.%2520This%2520makes%2520it%250Amore%2520difficult%2520to%2520produce%2520reliable%2520impact%2520studies%2520on%2520such%2520climate%2520statistics.%250AThis%2520paper%2520offers%2520a%2520novel%2520BC%2520methodology%2520to%2520correct%2520temporal%2520biases.%2520This%2520is%250Amade%2520possible%2520by%2520rethinking%2520the%2520philosophy%2520behind%2520BC.%2520We%2520will%2520introduce%2520BC%2520as%2520a%250Atime-indexed%2520regression%2520task%2520with%2520stochastic%2520outputs.%2520Rethinking%2520BC%2520enables%2520us%250Ato%2520adapt%2520state-of-the-art%2520machine%2520learning%2520%2528ML%2529%2520attention%2520models%2520and%2520thereby%250Alearn%2520different%2520types%2520of%2520biases%252C%2520including%2520temporal%2520asynchronicities.%2520With%2520a%250Acase%2520study%2520of%2520heatwave%2520duration%2520statistics%2520in%2520Abuja%252C%2520Nigeria%252C%2520and%2520Tokyo%252C%2520Japan%252C%250Awe%2520show%2520more%2520accurate%2520results%2520than%2520current%2520climate%2520model%2520outputs%2520and%250Aalternative%2520BC%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14169v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Temporal%20Stochastic%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%0A%20%20model&entry.906535625=Omer%20Nivron%20and%20Damon%20J.%20Wischik%20and%20Mathieu%20Vrac%20and%20Emily%20Shuckburgh%20and%20Alex%20T.%20Archibald&entry.1292438233=%20%20Climate%20models%20are%20biased%20with%20respect%20to%20real-world%20observations.%20They%0Ausually%20need%20to%20be%20adjusted%20before%20being%20used%20in%20impact%20studies.%20The%20suite%20of%0Astatistical%20methods%20that%20enable%20such%20adjustments%20is%20called%20bias%20correction%0A%28BC%29.%20However%2C%20BC%20methods%20currently%20struggle%20to%20adjust%20temporal%20biases.%20Because%0Athey%20mostly%20disregard%20the%20dependence%20between%20consecutive%20time%20points.%20As%20a%0Aresult%2C%20climate%20statistics%20with%20long-range%20temporal%20properties%2C%20such%20as%0Aheatwave%20duration%20and%20frequency%2C%20cannot%20be%20corrected%20accurately.%20This%20makes%20it%0Amore%20difficult%20to%20produce%20reliable%20impact%20studies%20on%20such%20climate%20statistics.%0AThis%20paper%20offers%20a%20novel%20BC%20methodology%20to%20correct%20temporal%20biases.%20This%20is%0Amade%20possible%20by%20rethinking%20the%20philosophy%20behind%20BC.%20We%20will%20introduce%20BC%20as%20a%0Atime-indexed%20regression%20task%20with%20stochastic%20outputs.%20Rethinking%20BC%20enables%20us%0Ato%20adapt%20state-of-the-art%20machine%20learning%20%28ML%29%20attention%20models%20and%20thereby%0Alearn%20different%20types%20of%20biases%2C%20including%20temporal%20asynchronicities.%20With%20a%0Acase%20study%20of%20heatwave%20duration%20statistics%20in%20Abuja%2C%20Nigeria%2C%20and%20Tokyo%2C%20Japan%2C%0Awe%20show%20more%20accurate%20results%20than%20current%20climate%20model%20outputs%20and%0Aalternative%20BC%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14169v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


