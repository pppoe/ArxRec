<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241010.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Generalizable and Animatable Gaussian Head Avatar", "author": "Xuangeng Chu and Tatsuya Harada", "abstract": "  In this paper, we propose Generalizable and Animatable Gaussian head Avatar\n(GAGAvatar) for one-shot animatable head avatar reconstruction. Existing\nmethods rely on neural radiance fields, leading to heavy rendering consumption\nand low reenactment speeds. To address these limitations, we generate the\nparameters of 3D Gaussians from a single image in a single forward pass. The\nkey innovation of our work is the proposed dual-lifting method, which produces\nhigh-fidelity 3D Gaussians that capture identity and facial details.\nAdditionally, we leverage global image features and the 3D morphable model to\nconstruct 3D Gaussians for controlling expressions. After training, our model\ncan reconstruct unseen identities without specific optimizations and perform\nreenactment rendering at real-time speeds. Experiments show that our method\nexhibits superior performance compared to previous methods in terms of\nreconstruction quality and expression accuracy. We believe our method can\nestablish new benchmarks for future research and advance applications of\ndigital avatars. Code and demos are available\nhttps://github.com/xg-chu/GAGAvatar.\n", "link": "http://arxiv.org/abs/2410.07971v1", "date": "2024-10-10", "relevancy": 3.7689, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7976}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7976}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20and%20Animatable%20Gaussian%20Head%20Avatar&body=Title%3A%20Generalizable%20and%20Animatable%20Gaussian%20Head%20Avatar%0AAuthor%3A%20Xuangeng%20Chu%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Generalizable%20and%20Animatable%20Gaussian%20head%20Avatar%0A%28GAGAvatar%29%20for%20one-shot%20animatable%20head%20avatar%20reconstruction.%20Existing%0Amethods%20rely%20on%20neural%20radiance%20fields%2C%20leading%20to%20heavy%20rendering%20consumption%0Aand%20low%20reenactment%20speeds.%20To%20address%20these%20limitations%2C%20we%20generate%20the%0Aparameters%20of%203D%20Gaussians%20from%20a%20single%20image%20in%20a%20single%20forward%20pass.%20The%0Akey%20innovation%20of%20our%20work%20is%20the%20proposed%20dual-lifting%20method%2C%20which%20produces%0Ahigh-fidelity%203D%20Gaussians%20that%20capture%20identity%20and%20facial%20details.%0AAdditionally%2C%20we%20leverage%20global%20image%20features%20and%20the%203D%20morphable%20model%20to%0Aconstruct%203D%20Gaussians%20for%20controlling%20expressions.%20After%20training%2C%20our%20model%0Acan%20reconstruct%20unseen%20identities%20without%20specific%20optimizations%20and%20perform%0Areenactment%20rendering%20at%20real-time%20speeds.%20Experiments%20show%20that%20our%20method%0Aexhibits%20superior%20performance%20compared%20to%20previous%20methods%20in%20terms%20of%0Areconstruction%20quality%20and%20expression%20accuracy.%20We%20believe%20our%20method%20can%0Aestablish%20new%20benchmarks%20for%20future%20research%20and%20advance%20applications%20of%0Adigital%20avatars.%20Code%20and%20demos%20are%20available%0Ahttps%3A//github.com/xg-chu/GAGAvatar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520and%2520Animatable%2520Gaussian%2520Head%2520Avatar%26entry.906535625%3DXuangeng%2520Chu%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Generalizable%2520and%2520Animatable%2520Gaussian%2520head%2520Avatar%250A%2528GAGAvatar%2529%2520for%2520one-shot%2520animatable%2520head%2520avatar%2520reconstruction.%2520Existing%250Amethods%2520rely%2520on%2520neural%2520radiance%2520fields%252C%2520leading%2520to%2520heavy%2520rendering%2520consumption%250Aand%2520low%2520reenactment%2520speeds.%2520To%2520address%2520these%2520limitations%252C%2520we%2520generate%2520the%250Aparameters%2520of%25203D%2520Gaussians%2520from%2520a%2520single%2520image%2520in%2520a%2520single%2520forward%2520pass.%2520The%250Akey%2520innovation%2520of%2520our%2520work%2520is%2520the%2520proposed%2520dual-lifting%2520method%252C%2520which%2520produces%250Ahigh-fidelity%25203D%2520Gaussians%2520that%2520capture%2520identity%2520and%2520facial%2520details.%250AAdditionally%252C%2520we%2520leverage%2520global%2520image%2520features%2520and%2520the%25203D%2520morphable%2520model%2520to%250Aconstruct%25203D%2520Gaussians%2520for%2520controlling%2520expressions.%2520After%2520training%252C%2520our%2520model%250Acan%2520reconstruct%2520unseen%2520identities%2520without%2520specific%2520optimizations%2520and%2520perform%250Areenactment%2520rendering%2520at%2520real-time%2520speeds.%2520Experiments%2520show%2520that%2520our%2520method%250Aexhibits%2520superior%2520performance%2520compared%2520to%2520previous%2520methods%2520in%2520terms%2520of%250Areconstruction%2520quality%2520and%2520expression%2520accuracy.%2520We%2520believe%2520our%2520method%2520can%250Aestablish%2520new%2520benchmarks%2520for%2520future%2520research%2520and%2520advance%2520applications%2520of%250Adigital%2520avatars.%2520Code%2520and%2520demos%2520are%2520available%250Ahttps%253A//github.com/xg-chu/GAGAvatar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20and%20Animatable%20Gaussian%20Head%20Avatar&entry.906535625=Xuangeng%20Chu%20and%20Tatsuya%20Harada&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Generalizable%20and%20Animatable%20Gaussian%20head%20Avatar%0A%28GAGAvatar%29%20for%20one-shot%20animatable%20head%20avatar%20reconstruction.%20Existing%0Amethods%20rely%20on%20neural%20radiance%20fields%2C%20leading%20to%20heavy%20rendering%20consumption%0Aand%20low%20reenactment%20speeds.%20To%20address%20these%20limitations%2C%20we%20generate%20the%0Aparameters%20of%203D%20Gaussians%20from%20a%20single%20image%20in%20a%20single%20forward%20pass.%20The%0Akey%20innovation%20of%20our%20work%20is%20the%20proposed%20dual-lifting%20method%2C%20which%20produces%0Ahigh-fidelity%203D%20Gaussians%20that%20capture%20identity%20and%20facial%20details.%0AAdditionally%2C%20we%20leverage%20global%20image%20features%20and%20the%203D%20morphable%20model%20to%0Aconstruct%203D%20Gaussians%20for%20controlling%20expressions.%20After%20training%2C%20our%20model%0Acan%20reconstruct%20unseen%20identities%20without%20specific%20optimizations%20and%20perform%0Areenactment%20rendering%20at%20real-time%20speeds.%20Experiments%20show%20that%20our%20method%0Aexhibits%20superior%20performance%20compared%20to%20previous%20methods%20in%20terms%20of%0Areconstruction%20quality%20and%20expression%20accuracy.%20We%20believe%20our%20method%20can%0Aestablish%20new%20benchmarks%20for%20future%20research%20and%20advance%20applications%20of%0Adigital%20avatars.%20Code%20and%20demos%20are%20available%0Ahttps%3A//github.com/xg-chu/GAGAvatar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07971v1&entry.124074799=Read"},
{"title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera", "author": "Jian Huang and Chengrui Dong and Peidong Liu", "abstract": "  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n", "link": "http://arxiv.org/abs/2410.08107v1", "date": "2024-10-10", "relevancy": 3.4506, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7431}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.678}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera&body=Title%3A%20IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera%0AAuthor%3A%20Jian%20Huang%20and%20Chengrui%20Dong%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20and%20explicit%203D%20Gaussian%20Splatting%20%283D-GS%29%20for%0Anovel%20view%20synthesis%20have%20achieved%20remarkable%20progress%20with%20frame-based%20camera%0A%28e.g.%20RGB%20and%20RGB-D%20cameras%29%20recently.%20Compared%20to%20frame-based%20camera%2C%20a%20novel%0Atype%20of%20bio-inspired%20visual%20sensor%2C%20i.e.%20event%20camera%2C%20has%20demonstrated%0Aadvantages%20in%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20low%20power%0Aconsumption%20and%20low%20latency.%20Due%20to%20its%20unique%20asynchronous%20and%20irregular%20data%0Acapturing%20process%2C%20limited%20work%20has%20been%20proposed%20to%20apply%20neural%0Arepresentation%20or%203D%20Gaussian%20splatting%20for%20an%20event%20camera.%20In%20this%20work%2C%20we%0Apresent%20IncEventGS%2C%20an%20incremental%203D%20Gaussian%20Splatting%20reconstruction%0Aalgorithm%20with%20a%20single%20event%20camera.%20To%20recover%20the%203D%20scene%20representation%0Aincrementally%2C%20we%20exploit%20the%20tracking%20and%20mapping%20paradigm%20of%20conventional%0ASLAM%20pipelines%20for%20IncEventGS.%20Given%20the%20incoming%20event%20stream%2C%20the%20tracker%0Afirstly%20estimates%20an%20initial%20camera%20motion%20based%20on%20prior%20reconstructed%203D-GS%0Ascene%20representation.%20The%20mapper%20then%20jointly%20refines%20both%20the%203D%20scene%0Arepresentation%20and%20camera%20motion%20based%20on%20the%20previously%20estimated%20motion%0Atrajectory%20from%20the%20tracker.%20The%20experimental%20results%20demonstrate%20that%0AIncEventGS%20delivers%20superior%20performance%20compared%20to%20prior%20NeRF-based%20methods%0Aand%20other%20related%20baselines%2C%20even%20we%20do%20not%20have%20the%20ground-truth%20camera%20poses.%0AFurthermore%2C%20our%20method%20can%20also%20deliver%20better%20performance%20compared%20to%0Astate-of-the-art%20event%20visual%20odometry%20methods%20in%20terms%20of%20camera%20motion%0Aestimation.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/wu-cvgl/IncEventGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncEventGS%253A%2520Pose-Free%2520Gaussian%2520Splatting%2520from%2520a%2520Single%2520Event%2520Camera%26entry.906535625%3DJian%2520Huang%2520and%2520Chengrui%2520Dong%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representation%2520and%2520explicit%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520for%250Anovel%2520view%2520synthesis%2520have%2520achieved%2520remarkable%2520progress%2520with%2520frame-based%2520camera%250A%2528e.g.%2520RGB%2520and%2520RGB-D%2520cameras%2529%2520recently.%2520Compared%2520to%2520frame-based%2520camera%252C%2520a%2520novel%250Atype%2520of%2520bio-inspired%2520visual%2520sensor%252C%2520i.e.%2520event%2520camera%252C%2520has%2520demonstrated%250Aadvantages%2520in%2520high%2520temporal%2520resolution%252C%2520high%2520dynamic%2520range%252C%2520low%2520power%250Aconsumption%2520and%2520low%2520latency.%2520Due%2520to%2520its%2520unique%2520asynchronous%2520and%2520irregular%2520data%250Acapturing%2520process%252C%2520limited%2520work%2520has%2520been%2520proposed%2520to%2520apply%2520neural%250Arepresentation%2520or%25203D%2520Gaussian%2520splatting%2520for%2520an%2520event%2520camera.%2520In%2520this%2520work%252C%2520we%250Apresent%2520IncEventGS%252C%2520an%2520incremental%25203D%2520Gaussian%2520Splatting%2520reconstruction%250Aalgorithm%2520with%2520a%2520single%2520event%2520camera.%2520To%2520recover%2520the%25203D%2520scene%2520representation%250Aincrementally%252C%2520we%2520exploit%2520the%2520tracking%2520and%2520mapping%2520paradigm%2520of%2520conventional%250ASLAM%2520pipelines%2520for%2520IncEventGS.%2520Given%2520the%2520incoming%2520event%2520stream%252C%2520the%2520tracker%250Afirstly%2520estimates%2520an%2520initial%2520camera%2520motion%2520based%2520on%2520prior%2520reconstructed%25203D-GS%250Ascene%2520representation.%2520The%2520mapper%2520then%2520jointly%2520refines%2520both%2520the%25203D%2520scene%250Arepresentation%2520and%2520camera%2520motion%2520based%2520on%2520the%2520previously%2520estimated%2520motion%250Atrajectory%2520from%2520the%2520tracker.%2520The%2520experimental%2520results%2520demonstrate%2520that%250AIncEventGS%2520delivers%2520superior%2520performance%2520compared%2520to%2520prior%2520NeRF-based%2520methods%250Aand%2520other%2520related%2520baselines%252C%2520even%2520we%2520do%2520not%2520have%2520the%2520ground-truth%2520camera%2520poses.%250AFurthermore%252C%2520our%2520method%2520can%2520also%2520deliver%2520better%2520performance%2520compared%2520to%250Astate-of-the-art%2520event%2520visual%2520odometry%2520methods%2520in%2520terms%2520of%2520camera%2520motion%250Aestimation.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/wu-cvgl/IncEventGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera&entry.906535625=Jian%20Huang%20and%20Chengrui%20Dong%20and%20Peidong%20Liu&entry.1292438233=%20%20Implicit%20neural%20representation%20and%20explicit%203D%20Gaussian%20Splatting%20%283D-GS%29%20for%0Anovel%20view%20synthesis%20have%20achieved%20remarkable%20progress%20with%20frame-based%20camera%0A%28e.g.%20RGB%20and%20RGB-D%20cameras%29%20recently.%20Compared%20to%20frame-based%20camera%2C%20a%20novel%0Atype%20of%20bio-inspired%20visual%20sensor%2C%20i.e.%20event%20camera%2C%20has%20demonstrated%0Aadvantages%20in%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20low%20power%0Aconsumption%20and%20low%20latency.%20Due%20to%20its%20unique%20asynchronous%20and%20irregular%20data%0Acapturing%20process%2C%20limited%20work%20has%20been%20proposed%20to%20apply%20neural%0Arepresentation%20or%203D%20Gaussian%20splatting%20for%20an%20event%20camera.%20In%20this%20work%2C%20we%0Apresent%20IncEventGS%2C%20an%20incremental%203D%20Gaussian%20Splatting%20reconstruction%0Aalgorithm%20with%20a%20single%20event%20camera.%20To%20recover%20the%203D%20scene%20representation%0Aincrementally%2C%20we%20exploit%20the%20tracking%20and%20mapping%20paradigm%20of%20conventional%0ASLAM%20pipelines%20for%20IncEventGS.%20Given%20the%20incoming%20event%20stream%2C%20the%20tracker%0Afirstly%20estimates%20an%20initial%20camera%20motion%20based%20on%20prior%20reconstructed%203D-GS%0Ascene%20representation.%20The%20mapper%20then%20jointly%20refines%20both%20the%203D%20scene%0Arepresentation%20and%20camera%20motion%20based%20on%20the%20previously%20estimated%20motion%0Atrajectory%20from%20the%20tracker.%20The%20experimental%20results%20demonstrate%20that%0AIncEventGS%20delivers%20superior%20performance%20compared%20to%20prior%20NeRF-based%20methods%0Aand%20other%20related%20baselines%2C%20even%20we%20do%20not%20have%20the%20ground-truth%20camera%20poses.%0AFurthermore%2C%20our%20method%20can%20also%20deliver%20better%20performance%20compared%20to%0Astate-of-the-art%20event%20visual%20odometry%20methods%20in%20terms%20of%20camera%20motion%0Aestimation.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/wu-cvgl/IncEventGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08107v1&entry.124074799=Read"},
{"title": "6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric\n  Rendering", "author": "Zhongpai Gao and Benjamin Planche and Meng Zheng and Anwesa Choudhuri and Terrence Chen and Ziyan Wu", "abstract": "  Novel view synthesis has advanced significantly with the development of\nneural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,\nachieving high quality without compromising real-time rendering remains\nchallenging, particularly for physically-based ray tracing with view-dependent\neffects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D\nspatial-angular representation to better incorporate view-dependent effects,\nbut the Gaussian representation and control scheme are sub-optimal. In this\npaper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),\nwhich enhances color and opacity representations and leverages the additional\ndirectional information in the 6D space for optimized Gaussian control. Our\napproach is fully compatible with the 3DGS framework and significantly improves\nreal-time radiance field rendering by better modeling view-dependent effects\nand fine details. Experiments demonstrate that 6DGS significantly outperforms\n3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction\nof 66.5% Gaussian points compared to 3DGS. The project page is:\nhttps://gaozhongpai.github.io/6dgs/\n", "link": "http://arxiv.org/abs/2410.04974v2", "date": "2024-10-10", "relevancy": 3.373, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7093}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.659}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%206DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering&body=Title%3A%206DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering%0AAuthor%3A%20Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Ziyan%20Wu%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20has%20advanced%20significantly%20with%20the%20development%20of%0Aneural%20radiance%20fields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29.%20However%2C%0Aachieving%20high%20quality%20without%20compromising%20real-time%20rendering%20remains%0Achallenging%2C%20particularly%20for%20physically-based%20ray%20tracing%20with%20view-dependent%0Aeffects.%20Recently%2C%20N-dimensional%20Gaussians%20%28N-DG%29%20introduced%20a%206D%0Aspatial-angular%20representation%20to%20better%20incorporate%20view-dependent%20effects%2C%0Abut%20the%20Gaussian%20representation%20and%20control%20scheme%20are%20sub-optimal.%20In%20this%0Apaper%2C%20we%20revisit%206D%20Gaussians%20and%20introduce%206D%20Gaussian%20Splatting%20%286DGS%29%2C%0Awhich%20enhances%20color%20and%20opacity%20representations%20and%20leverages%20the%20additional%0Adirectional%20information%20in%20the%206D%20space%20for%20optimized%20Gaussian%20control.%20Our%0Aapproach%20is%20fully%20compatible%20with%20the%203DGS%20framework%20and%20significantly%20improves%0Areal-time%20radiance%20field%20rendering%20by%20better%20modeling%20view-dependent%20effects%0Aand%20fine%20details.%20Experiments%20demonstrate%20that%206DGS%20significantly%20outperforms%0A3DGS%20and%20N-DG%2C%20achieving%20up%20to%20a%2015.73%20dB%20improvement%20in%20PSNR%20with%20a%20reduction%0Aof%2066.5%25%20Gaussian%20points%20compared%20to%203DGS.%20The%20project%20page%20is%3A%0Ahttps%3A//gaozhongpai.github.io/6dgs/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D6DGS%253A%2520Enhanced%2520Direction-Aware%2520Gaussian%2520Splatting%2520for%2520Volumetric%250A%2520%2520Rendering%26entry.906535625%3DZhongpai%2520Gao%2520and%2520Benjamin%2520Planche%2520and%2520Meng%2520Zheng%2520and%2520Anwesa%2520Choudhuri%2520and%2520Terrence%2520Chen%2520and%2520Ziyan%2520Wu%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520has%2520advanced%2520significantly%2520with%2520the%2520development%2520of%250Aneural%2520radiance%2520fields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529.%2520However%252C%250Aachieving%2520high%2520quality%2520without%2520compromising%2520real-time%2520rendering%2520remains%250Achallenging%252C%2520particularly%2520for%2520physically-based%2520ray%2520tracing%2520with%2520view-dependent%250Aeffects.%2520Recently%252C%2520N-dimensional%2520Gaussians%2520%2528N-DG%2529%2520introduced%2520a%25206D%250Aspatial-angular%2520representation%2520to%2520better%2520incorporate%2520view-dependent%2520effects%252C%250Abut%2520the%2520Gaussian%2520representation%2520and%2520control%2520scheme%2520are%2520sub-optimal.%2520In%2520this%250Apaper%252C%2520we%2520revisit%25206D%2520Gaussians%2520and%2520introduce%25206D%2520Gaussian%2520Splatting%2520%25286DGS%2529%252C%250Awhich%2520enhances%2520color%2520and%2520opacity%2520representations%2520and%2520leverages%2520the%2520additional%250Adirectional%2520information%2520in%2520the%25206D%2520space%2520for%2520optimized%2520Gaussian%2520control.%2520Our%250Aapproach%2520is%2520fully%2520compatible%2520with%2520the%25203DGS%2520framework%2520and%2520significantly%2520improves%250Areal-time%2520radiance%2520field%2520rendering%2520by%2520better%2520modeling%2520view-dependent%2520effects%250Aand%2520fine%2520details.%2520Experiments%2520demonstrate%2520that%25206DGS%2520significantly%2520outperforms%250A3DGS%2520and%2520N-DG%252C%2520achieving%2520up%2520to%2520a%252015.73%2520dB%2520improvement%2520in%2520PSNR%2520with%2520a%2520reduction%250Aof%252066.5%2525%2520Gaussian%2520points%2520compared%2520to%25203DGS.%2520The%2520project%2520page%2520is%253A%250Ahttps%253A//gaozhongpai.github.io/6dgs/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=6DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering&entry.906535625=Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Ziyan%20Wu&entry.1292438233=%20%20Novel%20view%20synthesis%20has%20advanced%20significantly%20with%20the%20development%20of%0Aneural%20radiance%20fields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29.%20However%2C%0Aachieving%20high%20quality%20without%20compromising%20real-time%20rendering%20remains%0Achallenging%2C%20particularly%20for%20physically-based%20ray%20tracing%20with%20view-dependent%0Aeffects.%20Recently%2C%20N-dimensional%20Gaussians%20%28N-DG%29%20introduced%20a%206D%0Aspatial-angular%20representation%20to%20better%20incorporate%20view-dependent%20effects%2C%0Abut%20the%20Gaussian%20representation%20and%20control%20scheme%20are%20sub-optimal.%20In%20this%0Apaper%2C%20we%20revisit%206D%20Gaussians%20and%20introduce%206D%20Gaussian%20Splatting%20%286DGS%29%2C%0Awhich%20enhances%20color%20and%20opacity%20representations%20and%20leverages%20the%20additional%0Adirectional%20information%20in%20the%206D%20space%20for%20optimized%20Gaussian%20control.%20Our%0Aapproach%20is%20fully%20compatible%20with%20the%203DGS%20framework%20and%20significantly%20improves%0Areal-time%20radiance%20field%20rendering%20by%20better%20modeling%20view-dependent%20effects%0Aand%20fine%20details.%20Experiments%20demonstrate%20that%206DGS%20significantly%20outperforms%0A3DGS%20and%20N-DG%2C%20achieving%20up%20to%20a%2015.73%20dB%20improvement%20in%20PSNR%20with%20a%20reduction%0Aof%2066.5%25%20Gaussian%20points%20compared%20to%203DGS.%20The%20project%20page%20is%3A%0Ahttps%3A//gaozhongpai.github.io/6dgs/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04974v2&entry.124074799=Read"},
{"title": "Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid\n  Transparency", "author": "Florian Hahlbohm and Fabian Friederichs and Tim Weyrich and Linus Franke and Moritz Kappel and Susana Castillo and Marc Stamminger and Martin Eisemann and Marcus Magnor", "abstract": "  3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both\nfor inverse rendering as well as real-time exploration of scenes. In these\napplications, coherence across camera frames and multiple views is crucial, be\nit for robust convergence of a scene reconstruction or for artifact-free\nfly-throughs. Recent work started mitigating artifacts that break multi-view\ncoherence, including popping artifacts due to inconsistent transparency sorting\nand perspective-correct outlines of (2D) splats. At the same time, real-time\nrequirements forced such implementations to accept compromises in how\ntransparency of large assemblies of 3D Gaussians is resolved, in turn breaking\ncoherence in other ways. In our work, we aim at achieving maximum coherence, by\nrendering fully perspective-correct 3D Gaussians while using a high-quality\napproximation of accurate blending, hybrid transparency, on a per-pixel level,\nin order to retain real-time frame rates. Our fast and perspectively accurate\napproach for evaluation of 3D Gaussians does not require matrix inversions,\nthereby ensuring numerical stability and eliminating the need for special\nhandling of degenerate splats, and the hybrid transparency formulation for\nblending maintains similar quality as fully resolved per-pixel transparencies\nat a fraction of the rendering costs. We further show that each of these two\ncomponents can be independently integrated into Gaussian splatting systems. In\ncombination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster\noptimization, and equal or better image quality with fewer rendering artifacts\ncompared to traditional 3DGS on common benchmarks.\n", "link": "http://arxiv.org/abs/2410.08129v1", "date": "2024-10-10", "relevancy": 3.3481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7163}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.651}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency&body=Title%3A%20Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency%0AAuthor%3A%20Florian%20Hahlbohm%20and%20Fabian%20Friederichs%20and%20Tim%20Weyrich%20and%20Linus%20Franke%20and%20Moritz%20Kappel%20and%20Susana%20Castillo%20and%20Marc%20Stamminger%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor%0AAbstract%3A%20%20%203D%20Gaussian%20Splats%20%283DGS%29%20have%20proven%20a%20versatile%20rendering%20primitive%2C%20both%0Afor%20inverse%20rendering%20as%20well%20as%20real-time%20exploration%20of%20scenes.%20In%20these%0Aapplications%2C%20coherence%20across%20camera%20frames%20and%20multiple%20views%20is%20crucial%2C%20be%0Ait%20for%20robust%20convergence%20of%20a%20scene%20reconstruction%20or%20for%20artifact-free%0Afly-throughs.%20Recent%20work%20started%20mitigating%20artifacts%20that%20break%20multi-view%0Acoherence%2C%20including%20popping%20artifacts%20due%20to%20inconsistent%20transparency%20sorting%0Aand%20perspective-correct%20outlines%20of%20%282D%29%20splats.%20At%20the%20same%20time%2C%20real-time%0Arequirements%20forced%20such%20implementations%20to%20accept%20compromises%20in%20how%0Atransparency%20of%20large%20assemblies%20of%203D%20Gaussians%20is%20resolved%2C%20in%20turn%20breaking%0Acoherence%20in%20other%20ways.%20In%20our%20work%2C%20we%20aim%20at%20achieving%20maximum%20coherence%2C%20by%0Arendering%20fully%20perspective-correct%203D%20Gaussians%20while%20using%20a%20high-quality%0Aapproximation%20of%20accurate%20blending%2C%20hybrid%20transparency%2C%20on%20a%20per-pixel%20level%2C%0Ain%20order%20to%20retain%20real-time%20frame%20rates.%20Our%20fast%20and%20perspectively%20accurate%0Aapproach%20for%20evaluation%20of%203D%20Gaussians%20does%20not%20require%20matrix%20inversions%2C%0Athereby%20ensuring%20numerical%20stability%20and%20eliminating%20the%20need%20for%20special%0Ahandling%20of%20degenerate%20splats%2C%20and%20the%20hybrid%20transparency%20formulation%20for%0Ablending%20maintains%20similar%20quality%20as%20fully%20resolved%20per-pixel%20transparencies%0Aat%20a%20fraction%20of%20the%20rendering%20costs.%20We%20further%20show%20that%20each%20of%20these%20two%0Acomponents%20can%20be%20independently%20integrated%20into%20Gaussian%20splatting%20systems.%20In%0Acombination%2C%20they%20achieve%20up%20to%202%24%5Ctimes%24%20higher%20frame%20rates%2C%202%24%5Ctimes%24%20faster%0Aoptimization%2C%20and%20equal%20or%20better%20image%20quality%20with%20fewer%20rendering%20artifacts%0Acompared%20to%20traditional%203DGS%20on%20common%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Perspective-Correct%25203D%2520Gaussian%2520Splatting%2520Using%2520Hybrid%250A%2520%2520Transparency%26entry.906535625%3DFlorian%2520Hahlbohm%2520and%2520Fabian%2520Friederichs%2520and%2520Tim%2520Weyrich%2520and%2520Linus%2520Franke%2520and%2520Moritz%2520Kappel%2520and%2520Susana%2520Castillo%2520and%2520Marc%2520Stamminger%2520and%2520Martin%2520Eisemann%2520and%2520Marcus%2520Magnor%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splats%2520%25283DGS%2529%2520have%2520proven%2520a%2520versatile%2520rendering%2520primitive%252C%2520both%250Afor%2520inverse%2520rendering%2520as%2520well%2520as%2520real-time%2520exploration%2520of%2520scenes.%2520In%2520these%250Aapplications%252C%2520coherence%2520across%2520camera%2520frames%2520and%2520multiple%2520views%2520is%2520crucial%252C%2520be%250Ait%2520for%2520robust%2520convergence%2520of%2520a%2520scene%2520reconstruction%2520or%2520for%2520artifact-free%250Afly-throughs.%2520Recent%2520work%2520started%2520mitigating%2520artifacts%2520that%2520break%2520multi-view%250Acoherence%252C%2520including%2520popping%2520artifacts%2520due%2520to%2520inconsistent%2520transparency%2520sorting%250Aand%2520perspective-correct%2520outlines%2520of%2520%25282D%2529%2520splats.%2520At%2520the%2520same%2520time%252C%2520real-time%250Arequirements%2520forced%2520such%2520implementations%2520to%2520accept%2520compromises%2520in%2520how%250Atransparency%2520of%2520large%2520assemblies%2520of%25203D%2520Gaussians%2520is%2520resolved%252C%2520in%2520turn%2520breaking%250Acoherence%2520in%2520other%2520ways.%2520In%2520our%2520work%252C%2520we%2520aim%2520at%2520achieving%2520maximum%2520coherence%252C%2520by%250Arendering%2520fully%2520perspective-correct%25203D%2520Gaussians%2520while%2520using%2520a%2520high-quality%250Aapproximation%2520of%2520accurate%2520blending%252C%2520hybrid%2520transparency%252C%2520on%2520a%2520per-pixel%2520level%252C%250Ain%2520order%2520to%2520retain%2520real-time%2520frame%2520rates.%2520Our%2520fast%2520and%2520perspectively%2520accurate%250Aapproach%2520for%2520evaluation%2520of%25203D%2520Gaussians%2520does%2520not%2520require%2520matrix%2520inversions%252C%250Athereby%2520ensuring%2520numerical%2520stability%2520and%2520eliminating%2520the%2520need%2520for%2520special%250Ahandling%2520of%2520degenerate%2520splats%252C%2520and%2520the%2520hybrid%2520transparency%2520formulation%2520for%250Ablending%2520maintains%2520similar%2520quality%2520as%2520fully%2520resolved%2520per-pixel%2520transparencies%250Aat%2520a%2520fraction%2520of%2520the%2520rendering%2520costs.%2520We%2520further%2520show%2520that%2520each%2520of%2520these%2520two%250Acomponents%2520can%2520be%2520independently%2520integrated%2520into%2520Gaussian%2520splatting%2520systems.%2520In%250Acombination%252C%2520they%2520achieve%2520up%2520to%25202%2524%255Ctimes%2524%2520higher%2520frame%2520rates%252C%25202%2524%255Ctimes%2524%2520faster%250Aoptimization%252C%2520and%2520equal%2520or%2520better%2520image%2520quality%2520with%2520fewer%2520rendering%2520artifacts%250Acompared%2520to%2520traditional%25203DGS%2520on%2520common%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency&entry.906535625=Florian%20Hahlbohm%20and%20Fabian%20Friederichs%20and%20Tim%20Weyrich%20and%20Linus%20Franke%20and%20Moritz%20Kappel%20and%20Susana%20Castillo%20and%20Marc%20Stamminger%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor&entry.1292438233=%20%203D%20Gaussian%20Splats%20%283DGS%29%20have%20proven%20a%20versatile%20rendering%20primitive%2C%20both%0Afor%20inverse%20rendering%20as%20well%20as%20real-time%20exploration%20of%20scenes.%20In%20these%0Aapplications%2C%20coherence%20across%20camera%20frames%20and%20multiple%20views%20is%20crucial%2C%20be%0Ait%20for%20robust%20convergence%20of%20a%20scene%20reconstruction%20or%20for%20artifact-free%0Afly-throughs.%20Recent%20work%20started%20mitigating%20artifacts%20that%20break%20multi-view%0Acoherence%2C%20including%20popping%20artifacts%20due%20to%20inconsistent%20transparency%20sorting%0Aand%20perspective-correct%20outlines%20of%20%282D%29%20splats.%20At%20the%20same%20time%2C%20real-time%0Arequirements%20forced%20such%20implementations%20to%20accept%20compromises%20in%20how%0Atransparency%20of%20large%20assemblies%20of%203D%20Gaussians%20is%20resolved%2C%20in%20turn%20breaking%0Acoherence%20in%20other%20ways.%20In%20our%20work%2C%20we%20aim%20at%20achieving%20maximum%20coherence%2C%20by%0Arendering%20fully%20perspective-correct%203D%20Gaussians%20while%20using%20a%20high-quality%0Aapproximation%20of%20accurate%20blending%2C%20hybrid%20transparency%2C%20on%20a%20per-pixel%20level%2C%0Ain%20order%20to%20retain%20real-time%20frame%20rates.%20Our%20fast%20and%20perspectively%20accurate%0Aapproach%20for%20evaluation%20of%203D%20Gaussians%20does%20not%20require%20matrix%20inversions%2C%0Athereby%20ensuring%20numerical%20stability%20and%20eliminating%20the%20need%20for%20special%0Ahandling%20of%20degenerate%20splats%2C%20and%20the%20hybrid%20transparency%20formulation%20for%0Ablending%20maintains%20similar%20quality%20as%20fully%20resolved%20per-pixel%20transparencies%0Aat%20a%20fraction%20of%20the%20rendering%20costs.%20We%20further%20show%20that%20each%20of%20these%20two%0Acomponents%20can%20be%20independently%20integrated%20into%20Gaussian%20splatting%20systems.%20In%0Acombination%2C%20they%20achieve%20up%20to%202%24%5Ctimes%24%20higher%20frame%20rates%2C%202%24%5Ctimes%24%20faster%0Aoptimization%2C%20and%20equal%20or%20better%20image%20quality%20with%20fewer%20rendering%20artifacts%0Acompared%20to%20traditional%203DGS%20on%20common%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08129v1&entry.124074799=Read"},
{"title": "Fast Feedforward 3D Gaussian Splatting Compression", "author": "Yihang Chen and Qianyi Wu and Mengyao Li and Weiyao Lin and Mehrtash Harandi and Jianfei Cai", "abstract": "  With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity\nrendering for novel view synthesis, storage requirements pose challenges for\ntheir widespread adoption. Although various compression techniques have been\nproposed, previous art suffers from a common limitation: for any existing 3DGS,\nper-scene optimization is needed to achieve compression, making the compression\nsluggish and slow. To address this issue, we introduce Fast Compression of 3D\nGaussian Splatting (FCGS), an optimization-free model that can compress 3DGS\nrepresentations rapidly in a single feed-forward pass, which significantly\nreduces compression time from minutes to seconds. To enhance compression\nefficiency, we propose a multi-path entropy module that assigns Gaussian\nattributes to different entropy constraint paths for balance between size and\nfidelity. We also carefully design both inter- and intra-Gaussian context\nmodels to remove redundancies among the unstructured Gaussian blobs. Overall,\nFCGS achieves a compression ratio of over 20X while maintaining fidelity,\nsurpassing most per-scene SOTA optimization-based methods. Our code is\navailable at: https://github.com/YihangChen-ee/FCGS.\n", "link": "http://arxiv.org/abs/2410.08017v1", "date": "2024-10-10", "relevancy": 3.3314, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6972}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6723}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Feedforward%203D%20Gaussian%20Splatting%20Compression&body=Title%3A%20Fast%20Feedforward%203D%20Gaussian%20Splatting%20Compression%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Mengyao%20Li%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20With%203D%20Gaussian%20Splatting%20%283DGS%29%20advancing%20real-time%20and%20high-fidelity%0Arendering%20for%20novel%20view%20synthesis%2C%20storage%20requirements%20pose%20challenges%20for%0Atheir%20widespread%20adoption.%20Although%20various%20compression%20techniques%20have%20been%0Aproposed%2C%20previous%20art%20suffers%20from%20a%20common%20limitation%3A%20for%20any%20existing%203DGS%2C%0Aper-scene%20optimization%20is%20needed%20to%20achieve%20compression%2C%20making%20the%20compression%0Asluggish%20and%20slow.%20To%20address%20this%20issue%2C%20we%20introduce%20Fast%20Compression%20of%203D%0AGaussian%20Splatting%20%28FCGS%29%2C%20an%20optimization-free%20model%20that%20can%20compress%203DGS%0Arepresentations%20rapidly%20in%20a%20single%20feed-forward%20pass%2C%20which%20significantly%0Areduces%20compression%20time%20from%20minutes%20to%20seconds.%20To%20enhance%20compression%0Aefficiency%2C%20we%20propose%20a%20multi-path%20entropy%20module%20that%20assigns%20Gaussian%0Aattributes%20to%20different%20entropy%20constraint%20paths%20for%20balance%20between%20size%20and%0Afidelity.%20We%20also%20carefully%20design%20both%20inter-%20and%20intra-Gaussian%20context%0Amodels%20to%20remove%20redundancies%20among%20the%20unstructured%20Gaussian%20blobs.%20Overall%2C%0AFCGS%20achieves%20a%20compression%20ratio%20of%20over%2020X%20while%20maintaining%20fidelity%2C%0Asurpassing%20most%20per-scene%20SOTA%20optimization-based%20methods.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/YihangChen-ee/FCGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Feedforward%25203D%2520Gaussian%2520Splatting%2520Compression%26entry.906535625%3DYihang%2520Chen%2520and%2520Qianyi%2520Wu%2520and%2520Mengyao%2520Li%2520and%2520Weiyao%2520Lin%2520and%2520Mehrtash%2520Harandi%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520With%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520advancing%2520real-time%2520and%2520high-fidelity%250Arendering%2520for%2520novel%2520view%2520synthesis%252C%2520storage%2520requirements%2520pose%2520challenges%2520for%250Atheir%2520widespread%2520adoption.%2520Although%2520various%2520compression%2520techniques%2520have%2520been%250Aproposed%252C%2520previous%2520art%2520suffers%2520from%2520a%2520common%2520limitation%253A%2520for%2520any%2520existing%25203DGS%252C%250Aper-scene%2520optimization%2520is%2520needed%2520to%2520achieve%2520compression%252C%2520making%2520the%2520compression%250Asluggish%2520and%2520slow.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Fast%2520Compression%2520of%25203D%250AGaussian%2520Splatting%2520%2528FCGS%2529%252C%2520an%2520optimization-free%2520model%2520that%2520can%2520compress%25203DGS%250Arepresentations%2520rapidly%2520in%2520a%2520single%2520feed-forward%2520pass%252C%2520which%2520significantly%250Areduces%2520compression%2520time%2520from%2520minutes%2520to%2520seconds.%2520To%2520enhance%2520compression%250Aefficiency%252C%2520we%2520propose%2520a%2520multi-path%2520entropy%2520module%2520that%2520assigns%2520Gaussian%250Aattributes%2520to%2520different%2520entropy%2520constraint%2520paths%2520for%2520balance%2520between%2520size%2520and%250Afidelity.%2520We%2520also%2520carefully%2520design%2520both%2520inter-%2520and%2520intra-Gaussian%2520context%250Amodels%2520to%2520remove%2520redundancies%2520among%2520the%2520unstructured%2520Gaussian%2520blobs.%2520Overall%252C%250AFCGS%2520achieves%2520a%2520compression%2520ratio%2520of%2520over%252020X%2520while%2520maintaining%2520fidelity%252C%250Asurpassing%2520most%2520per-scene%2520SOTA%2520optimization-based%2520methods.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/YihangChen-ee/FCGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Feedforward%203D%20Gaussian%20Splatting%20Compression&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Mengyao%20Li%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai&entry.1292438233=%20%20With%203D%20Gaussian%20Splatting%20%283DGS%29%20advancing%20real-time%20and%20high-fidelity%0Arendering%20for%20novel%20view%20synthesis%2C%20storage%20requirements%20pose%20challenges%20for%0Atheir%20widespread%20adoption.%20Although%20various%20compression%20techniques%20have%20been%0Aproposed%2C%20previous%20art%20suffers%20from%20a%20common%20limitation%3A%20for%20any%20existing%203DGS%2C%0Aper-scene%20optimization%20is%20needed%20to%20achieve%20compression%2C%20making%20the%20compression%0Asluggish%20and%20slow.%20To%20address%20this%20issue%2C%20we%20introduce%20Fast%20Compression%20of%203D%0AGaussian%20Splatting%20%28FCGS%29%2C%20an%20optimization-free%20model%20that%20can%20compress%203DGS%0Arepresentations%20rapidly%20in%20a%20single%20feed-forward%20pass%2C%20which%20significantly%0Areduces%20compression%20time%20from%20minutes%20to%20seconds.%20To%20enhance%20compression%0Aefficiency%2C%20we%20propose%20a%20multi-path%20entropy%20module%20that%20assigns%20Gaussian%0Aattributes%20to%20different%20entropy%20constraint%20paths%20for%20balance%20between%20size%20and%0Afidelity.%20We%20also%20carefully%20design%20both%20inter-%20and%20intra-Gaussian%20context%0Amodels%20to%20remove%20redundancies%20among%20the%20unstructured%20Gaussian%20blobs.%20Overall%2C%0AFCGS%20achieves%20a%20compression%20ratio%20of%20over%2020X%20while%20maintaining%20fidelity%2C%0Asurpassing%20most%20per-scene%20SOTA%20optimization-based%20methods.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/YihangChen-ee/FCGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08017v1&entry.124074799=Read"},
{"title": "A transition towards virtual representations of visual scenes", "author": "Am\u00e9rico Pereira and Pedro Carvalho and Lu\u00eds C\u00f4rte-Real", "abstract": "  Visual scene understanding is a fundamental task in computer vision that aims\nto extract meaningful information from visual data. It traditionally involves\ndisjoint and specialized algorithms for different tasks that are tailored for\nspecific application scenarios. This can be cumbersome when designing complex\nsystems that include processing of visual and semantic data extracted from\nvisual scenes, which is even more noticeable nowadays with the influx of\napplications for virtual or augmented reality. When designing a system that\nemploys automatic visual scene understanding to enable a precise and\nsemantically coherent description of the underlying scene, which can be used to\nfuel a visualization component with 3D virtual synthesis, the lack of\nflexibility and unified frameworks become more prominent. To alleviate this\nissue and its inherent problems, we propose an architecture that addresses the\nchallenges of visual scene understanding and description towards a 3D virtual\nsynthesis that enables an adaptable, unified and coherent solution.\nFurthermore, we expose how our proposition can be of use into multiple\napplication areas. Additionally, we also present a proof of concept system that\nemploys our architecture to further prove its usability in practice.\n", "link": "http://arxiv.org/abs/2410.07987v1", "date": "2024-10-10", "relevancy": 3.1894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20transition%20towards%20virtual%20representations%20of%20visual%20scenes&body=Title%3A%20A%20transition%20towards%20virtual%20representations%20of%20visual%20scenes%0AAuthor%3A%20Am%C3%A9rico%20Pereira%20and%20Pedro%20Carvalho%20and%20Lu%C3%ADs%20C%C3%B4rte-Real%0AAbstract%3A%20%20%20Visual%20scene%20understanding%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20aims%0Ato%20extract%20meaningful%20information%20from%20visual%20data.%20It%20traditionally%20involves%0Adisjoint%20and%20specialized%20algorithms%20for%20different%20tasks%20that%20are%20tailored%20for%0Aspecific%20application%20scenarios.%20This%20can%20be%20cumbersome%20when%20designing%20complex%0Asystems%20that%20include%20processing%20of%20visual%20and%20semantic%20data%20extracted%20from%0Avisual%20scenes%2C%20which%20is%20even%20more%20noticeable%20nowadays%20with%20the%20influx%20of%0Aapplications%20for%20virtual%20or%20augmented%20reality.%20When%20designing%20a%20system%20that%0Aemploys%20automatic%20visual%20scene%20understanding%20to%20enable%20a%20precise%20and%0Asemantically%20coherent%20description%20of%20the%20underlying%20scene%2C%20which%20can%20be%20used%20to%0Afuel%20a%20visualization%20component%20with%203D%20virtual%20synthesis%2C%20the%20lack%20of%0Aflexibility%20and%20unified%20frameworks%20become%20more%20prominent.%20To%20alleviate%20this%0Aissue%20and%20its%20inherent%20problems%2C%20we%20propose%20an%20architecture%20that%20addresses%20the%0Achallenges%20of%20visual%20scene%20understanding%20and%20description%20towards%20a%203D%20virtual%0Asynthesis%20that%20enables%20an%20adaptable%2C%20unified%20and%20coherent%20solution.%0AFurthermore%2C%20we%20expose%20how%20our%20proposition%20can%20be%20of%20use%20into%20multiple%0Aapplication%20areas.%20Additionally%2C%20we%20also%20present%20a%20proof%20of%20concept%20system%20that%0Aemploys%20our%20architecture%20to%20further%20prove%20its%20usability%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520transition%2520towards%2520virtual%2520representations%2520of%2520visual%2520scenes%26entry.906535625%3DAm%25C3%25A9rico%2520Pereira%2520and%2520Pedro%2520Carvalho%2520and%2520Lu%25C3%25ADs%2520C%25C3%25B4rte-Real%26entry.1292438233%3D%2520%2520Visual%2520scene%2520understanding%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%2520that%2520aims%250Ato%2520extract%2520meaningful%2520information%2520from%2520visual%2520data.%2520It%2520traditionally%2520involves%250Adisjoint%2520and%2520specialized%2520algorithms%2520for%2520different%2520tasks%2520that%2520are%2520tailored%2520for%250Aspecific%2520application%2520scenarios.%2520This%2520can%2520be%2520cumbersome%2520when%2520designing%2520complex%250Asystems%2520that%2520include%2520processing%2520of%2520visual%2520and%2520semantic%2520data%2520extracted%2520from%250Avisual%2520scenes%252C%2520which%2520is%2520even%2520more%2520noticeable%2520nowadays%2520with%2520the%2520influx%2520of%250Aapplications%2520for%2520virtual%2520or%2520augmented%2520reality.%2520When%2520designing%2520a%2520system%2520that%250Aemploys%2520automatic%2520visual%2520scene%2520understanding%2520to%2520enable%2520a%2520precise%2520and%250Asemantically%2520coherent%2520description%2520of%2520the%2520underlying%2520scene%252C%2520which%2520can%2520be%2520used%2520to%250Afuel%2520a%2520visualization%2520component%2520with%25203D%2520virtual%2520synthesis%252C%2520the%2520lack%2520of%250Aflexibility%2520and%2520unified%2520frameworks%2520become%2520more%2520prominent.%2520To%2520alleviate%2520this%250Aissue%2520and%2520its%2520inherent%2520problems%252C%2520we%2520propose%2520an%2520architecture%2520that%2520addresses%2520the%250Achallenges%2520of%2520visual%2520scene%2520understanding%2520and%2520description%2520towards%2520a%25203D%2520virtual%250Asynthesis%2520that%2520enables%2520an%2520adaptable%252C%2520unified%2520and%2520coherent%2520solution.%250AFurthermore%252C%2520we%2520expose%2520how%2520our%2520proposition%2520can%2520be%2520of%2520use%2520into%2520multiple%250Aapplication%2520areas.%2520Additionally%252C%2520we%2520also%2520present%2520a%2520proof%2520of%2520concept%2520system%2520that%250Aemploys%2520our%2520architecture%2520to%2520further%2520prove%2520its%2520usability%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20transition%20towards%20virtual%20representations%20of%20visual%20scenes&entry.906535625=Am%C3%A9rico%20Pereira%20and%20Pedro%20Carvalho%20and%20Lu%C3%ADs%20C%C3%B4rte-Real&entry.1292438233=%20%20Visual%20scene%20understanding%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20aims%0Ato%20extract%20meaningful%20information%20from%20visual%20data.%20It%20traditionally%20involves%0Adisjoint%20and%20specialized%20algorithms%20for%20different%20tasks%20that%20are%20tailored%20for%0Aspecific%20application%20scenarios.%20This%20can%20be%20cumbersome%20when%20designing%20complex%0Asystems%20that%20include%20processing%20of%20visual%20and%20semantic%20data%20extracted%20from%0Avisual%20scenes%2C%20which%20is%20even%20more%20noticeable%20nowadays%20with%20the%20influx%20of%0Aapplications%20for%20virtual%20or%20augmented%20reality.%20When%20designing%20a%20system%20that%0Aemploys%20automatic%20visual%20scene%20understanding%20to%20enable%20a%20precise%20and%0Asemantically%20coherent%20description%20of%20the%20underlying%20scene%2C%20which%20can%20be%20used%20to%0Afuel%20a%20visualization%20component%20with%203D%20virtual%20synthesis%2C%20the%20lack%20of%0Aflexibility%20and%20unified%20frameworks%20become%20more%20prominent.%20To%20alleviate%20this%0Aissue%20and%20its%20inherent%20problems%2C%20we%20propose%20an%20architecture%20that%20addresses%20the%0Achallenges%20of%20visual%20scene%20understanding%20and%20description%20towards%20a%203D%20virtual%0Asynthesis%20that%20enables%20an%20adaptable%2C%20unified%20and%20coherent%20solution.%0AFurthermore%2C%20we%20expose%20how%20our%20proposition%20can%20be%20of%20use%20into%20multiple%0Aapplication%20areas.%20Additionally%2C%20we%20also%20present%20a%20proof%20of%20concept%20system%20that%0Aemploys%20our%20architecture%20to%20further%20prove%20its%20usability%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07987v1&entry.124074799=Read"},
{"title": "RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS\n  Generative Model from a Single Image", "author": "Xiaoxue Chen and Jv Zheng and Hao Huang and Haoran Xu and Weihao Gu and Kangliang Chen and He xiang and Huan-ang Gao and Hao Zhao and Guyue Zhou and Yaqin Zhang", "abstract": "  The generation of high-quality 3D car assets is essential for various\napplications, including video games, autonomous driving, and virtual reality.\nCurrent 3D generation methods utilizing NeRF or 3D-GS as representations for 3D\nobjects, generate a Lambertian object under fixed lighting and lack separated\nmodelings for material and global illumination. As a result, the generated\nassets are unsuitable for relighting under varying lighting conditions,\nlimiting their applicability in downstream tasks. To address this challenge, we\npropose a novel relightable 3D object generative framework that automates the\ncreation of 3D car assets, enabling the swift and accurate reconstruction of a\nvehicle's geometry, texture, and material properties from a single input image.\nOur approach begins with introducing a large-scale synthetic car dataset\ncomprising over 1,000 high-precision 3D vehicle models. We represent 3D objects\nusing global illumination and relightable 3D Gaussian primitives integrating\nwith BRDF parameters. Building on this representation, we introduce a\nfeed-forward model that takes images as input and outputs both relightable 3D\nGaussians and global illumination parameters. Experimental results demonstrate\nthat our method produces photorealistic 3D car assets that can be seamlessly\nintegrated into road scenes with different illuminations, which offers\nsubstantial practical benefits for industrial applications.\n", "link": "http://arxiv.org/abs/2410.08181v1", "date": "2024-10-10", "relevancy": 3.1391, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6479}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6257}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGM%3A%20Reconstructing%20High-fidelity%203D%20Car%20Assets%20with%20Relightable%203D-GS%0A%20%20Generative%20Model%20from%20a%20Single%20Image&body=Title%3A%20RGM%3A%20Reconstructing%20High-fidelity%203D%20Car%20Assets%20with%20Relightable%203D-GS%0A%20%20Generative%20Model%20from%20a%20Single%20Image%0AAuthor%3A%20Xiaoxue%20Chen%20and%20Jv%20Zheng%20and%20Hao%20Huang%20and%20Haoran%20Xu%20and%20Weihao%20Gu%20and%20Kangliang%20Chen%20and%20He%20xiang%20and%20Huan-ang%20Gao%20and%20Hao%20Zhao%20and%20Guyue%20Zhou%20and%20Yaqin%20Zhang%0AAbstract%3A%20%20%20The%20generation%20of%20high-quality%203D%20car%20assets%20is%20essential%20for%20various%0Aapplications%2C%20including%20video%20games%2C%20autonomous%20driving%2C%20and%20virtual%20reality.%0ACurrent%203D%20generation%20methods%20utilizing%20NeRF%20or%203D-GS%20as%20representations%20for%203D%0Aobjects%2C%20generate%20a%20Lambertian%20object%20under%20fixed%20lighting%20and%20lack%20separated%0Amodelings%20for%20material%20and%20global%20illumination.%20As%20a%20result%2C%20the%20generated%0Aassets%20are%20unsuitable%20for%20relighting%20under%20varying%20lighting%20conditions%2C%0Alimiting%20their%20applicability%20in%20downstream%20tasks.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20relightable%203D%20object%20generative%20framework%20that%20automates%20the%0Acreation%20of%203D%20car%20assets%2C%20enabling%20the%20swift%20and%20accurate%20reconstruction%20of%20a%0Avehicle%27s%20geometry%2C%20texture%2C%20and%20material%20properties%20from%20a%20single%20input%20image.%0AOur%20approach%20begins%20with%20introducing%20a%20large-scale%20synthetic%20car%20dataset%0Acomprising%20over%201%2C000%20high-precision%203D%20vehicle%20models.%20We%20represent%203D%20objects%0Ausing%20global%20illumination%20and%20relightable%203D%20Gaussian%20primitives%20integrating%0Awith%20BRDF%20parameters.%20Building%20on%20this%20representation%2C%20we%20introduce%20a%0Afeed-forward%20model%20that%20takes%20images%20as%20input%20and%20outputs%20both%20relightable%203D%0AGaussians%20and%20global%20illumination%20parameters.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20produces%20photorealistic%203D%20car%20assets%20that%20can%20be%20seamlessly%0Aintegrated%20into%20road%20scenes%20with%20different%20illuminations%2C%20which%20offers%0Asubstantial%20practical%20benefits%20for%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGM%253A%2520Reconstructing%2520High-fidelity%25203D%2520Car%2520Assets%2520with%2520Relightable%25203D-GS%250A%2520%2520Generative%2520Model%2520from%2520a%2520Single%2520Image%26entry.906535625%3DXiaoxue%2520Chen%2520and%2520Jv%2520Zheng%2520and%2520Hao%2520Huang%2520and%2520Haoran%2520Xu%2520and%2520Weihao%2520Gu%2520and%2520Kangliang%2520Chen%2520and%2520He%2520xiang%2520and%2520Huan-ang%2520Gao%2520and%2520Hao%2520Zhao%2520and%2520Guyue%2520Zhou%2520and%2520Yaqin%2520Zhang%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520high-quality%25203D%2520car%2520assets%2520is%2520essential%2520for%2520various%250Aapplications%252C%2520including%2520video%2520games%252C%2520autonomous%2520driving%252C%2520and%2520virtual%2520reality.%250ACurrent%25203D%2520generation%2520methods%2520utilizing%2520NeRF%2520or%25203D-GS%2520as%2520representations%2520for%25203D%250Aobjects%252C%2520generate%2520a%2520Lambertian%2520object%2520under%2520fixed%2520lighting%2520and%2520lack%2520separated%250Amodelings%2520for%2520material%2520and%2520global%2520illumination.%2520As%2520a%2520result%252C%2520the%2520generated%250Aassets%2520are%2520unsuitable%2520for%2520relighting%2520under%2520varying%2520lighting%2520conditions%252C%250Alimiting%2520their%2520applicability%2520in%2520downstream%2520tasks.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520novel%2520relightable%25203D%2520object%2520generative%2520framework%2520that%2520automates%2520the%250Acreation%2520of%25203D%2520car%2520assets%252C%2520enabling%2520the%2520swift%2520and%2520accurate%2520reconstruction%2520of%2520a%250Avehicle%2527s%2520geometry%252C%2520texture%252C%2520and%2520material%2520properties%2520from%2520a%2520single%2520input%2520image.%250AOur%2520approach%2520begins%2520with%2520introducing%2520a%2520large-scale%2520synthetic%2520car%2520dataset%250Acomprising%2520over%25201%252C000%2520high-precision%25203D%2520vehicle%2520models.%2520We%2520represent%25203D%2520objects%250Ausing%2520global%2520illumination%2520and%2520relightable%25203D%2520Gaussian%2520primitives%2520integrating%250Awith%2520BRDF%2520parameters.%2520Building%2520on%2520this%2520representation%252C%2520we%2520introduce%2520a%250Afeed-forward%2520model%2520that%2520takes%2520images%2520as%2520input%2520and%2520outputs%2520both%2520relightable%25203D%250AGaussians%2520and%2520global%2520illumination%2520parameters.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520produces%2520photorealistic%25203D%2520car%2520assets%2520that%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520road%2520scenes%2520with%2520different%2520illuminations%252C%2520which%2520offers%250Asubstantial%2520practical%2520benefits%2520for%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGM%3A%20Reconstructing%20High-fidelity%203D%20Car%20Assets%20with%20Relightable%203D-GS%0A%20%20Generative%20Model%20from%20a%20Single%20Image&entry.906535625=Xiaoxue%20Chen%20and%20Jv%20Zheng%20and%20Hao%20Huang%20and%20Haoran%20Xu%20and%20Weihao%20Gu%20and%20Kangliang%20Chen%20and%20He%20xiang%20and%20Huan-ang%20Gao%20and%20Hao%20Zhao%20and%20Guyue%20Zhou%20and%20Yaqin%20Zhang&entry.1292438233=%20%20The%20generation%20of%20high-quality%203D%20car%20assets%20is%20essential%20for%20various%0Aapplications%2C%20including%20video%20games%2C%20autonomous%20driving%2C%20and%20virtual%20reality.%0ACurrent%203D%20generation%20methods%20utilizing%20NeRF%20or%203D-GS%20as%20representations%20for%203D%0Aobjects%2C%20generate%20a%20Lambertian%20object%20under%20fixed%20lighting%20and%20lack%20separated%0Amodelings%20for%20material%20and%20global%20illumination.%20As%20a%20result%2C%20the%20generated%0Aassets%20are%20unsuitable%20for%20relighting%20under%20varying%20lighting%20conditions%2C%0Alimiting%20their%20applicability%20in%20downstream%20tasks.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20relightable%203D%20object%20generative%20framework%20that%20automates%20the%0Acreation%20of%203D%20car%20assets%2C%20enabling%20the%20swift%20and%20accurate%20reconstruction%20of%20a%0Avehicle%27s%20geometry%2C%20texture%2C%20and%20material%20properties%20from%20a%20single%20input%20image.%0AOur%20approach%20begins%20with%20introducing%20a%20large-scale%20synthetic%20car%20dataset%0Acomprising%20over%201%2C000%20high-precision%203D%20vehicle%20models.%20We%20represent%203D%20objects%0Ausing%20global%20illumination%20and%20relightable%203D%20Gaussian%20primitives%20integrating%0Awith%20BRDF%20parameters.%20Building%20on%20this%20representation%2C%20we%20introduce%20a%0Afeed-forward%20model%20that%20takes%20images%20as%20input%20and%20outputs%20both%20relightable%203D%0AGaussians%20and%20global%20illumination%20parameters.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20produces%20photorealistic%203D%20car%20assets%20that%20can%20be%20seamlessly%0Aintegrated%20into%20road%20scenes%20with%20different%20illuminations%2C%20which%20offers%0Asubstantial%20practical%20benefits%20for%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08181v1&entry.124074799=Read"},
{"title": "UV-free Texture Generation with Denoising and Geodesic Heat Diffusions", "author": "Simone Foti and Stefanos Zafeiriou and Tolga Birdal", "abstract": "  Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.\n", "link": "http://arxiv.org/abs/2408.16762v2", "date": "2024-10-10", "relevancy": 3.1336, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.647}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6166}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions&body=Title%3A%20UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions%0AAuthor%3A%20Simone%20Foti%20and%20Stefanos%20Zafeiriou%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Seams%2C%20distortions%2C%20wasted%20UV%20space%2C%20vertex-duplication%2C%20and%20varying%0Aresolution%20over%20the%20surface%20are%20the%20most%20prominent%20issues%20of%20the%20standard%0AUV-based%20texturing%20of%20meshes.%20These%20issues%20are%20particularly%20acute%20when%0Aautomatic%20UV-unwrapping%20techniques%20are%20used.%20For%20this%20reason%2C%20instead%20of%0Agenerating%20textures%20in%20automatically%20generated%20UV-planes%20like%20most%0Astate-of-the-art%20methods%2C%20we%20propose%20to%20represent%20textures%20as%20coloured%0Apoint-clouds%20whose%20colours%20are%20generated%20by%20a%20denoising%20diffusion%20probabilistic%0Amodel%20constrained%20to%20operate%20on%20the%20surface%20of%203D%20objects.%20Our%20sampling%20and%0Aresolution%20agnostic%20generative%20model%20heavily%20relies%20on%20heat%20diffusion%20over%20the%0Asurface%20of%20the%20meshes%20for%20spatial%20communication%20between%20points.%20To%20enable%0Aprocessing%20of%20arbitrarily%20sampled%20point-cloud%20textures%20and%20ensure%20long-distance%0Atexture%20consistency%20we%20introduce%20a%20fast%20re-sampling%20of%20the%20mesh%20spectral%0Aproperties%20used%20during%20the%20heat%20diffusion%20and%20introduce%20a%20novel%0Aheat-diffusion-based%20self-attention%20mechanism.%20Our%20code%20and%20pre-trained%20models%0Aare%20available%20at%20github.com/simofoti/UV3-TeD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUV-free%2520Texture%2520Generation%2520with%2520Denoising%2520and%2520Geodesic%2520Heat%2520Diffusions%26entry.906535625%3DSimone%2520Foti%2520and%2520Stefanos%2520Zafeiriou%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Seams%252C%2520distortions%252C%2520wasted%2520UV%2520space%252C%2520vertex-duplication%252C%2520and%2520varying%250Aresolution%2520over%2520the%2520surface%2520are%2520the%2520most%2520prominent%2520issues%2520of%2520the%2520standard%250AUV-based%2520texturing%2520of%2520meshes.%2520These%2520issues%2520are%2520particularly%2520acute%2520when%250Aautomatic%2520UV-unwrapping%2520techniques%2520are%2520used.%2520For%2520this%2520reason%252C%2520instead%2520of%250Agenerating%2520textures%2520in%2520automatically%2520generated%2520UV-planes%2520like%2520most%250Astate-of-the-art%2520methods%252C%2520we%2520propose%2520to%2520represent%2520textures%2520as%2520coloured%250Apoint-clouds%2520whose%2520colours%2520are%2520generated%2520by%2520a%2520denoising%2520diffusion%2520probabilistic%250Amodel%2520constrained%2520to%2520operate%2520on%2520the%2520surface%2520of%25203D%2520objects.%2520Our%2520sampling%2520and%250Aresolution%2520agnostic%2520generative%2520model%2520heavily%2520relies%2520on%2520heat%2520diffusion%2520over%2520the%250Asurface%2520of%2520the%2520meshes%2520for%2520spatial%2520communication%2520between%2520points.%2520To%2520enable%250Aprocessing%2520of%2520arbitrarily%2520sampled%2520point-cloud%2520textures%2520and%2520ensure%2520long-distance%250Atexture%2520consistency%2520we%2520introduce%2520a%2520fast%2520re-sampling%2520of%2520the%2520mesh%2520spectral%250Aproperties%2520used%2520during%2520the%2520heat%2520diffusion%2520and%2520introduce%2520a%2520novel%250Aheat-diffusion-based%2520self-attention%2520mechanism.%2520Our%2520code%2520and%2520pre-trained%2520models%250Aare%2520available%2520at%2520github.com/simofoti/UV3-TeD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions&entry.906535625=Simone%20Foti%20and%20Stefanos%20Zafeiriou%20and%20Tolga%20Birdal&entry.1292438233=%20%20Seams%2C%20distortions%2C%20wasted%20UV%20space%2C%20vertex-duplication%2C%20and%20varying%0Aresolution%20over%20the%20surface%20are%20the%20most%20prominent%20issues%20of%20the%20standard%0AUV-based%20texturing%20of%20meshes.%20These%20issues%20are%20particularly%20acute%20when%0Aautomatic%20UV-unwrapping%20techniques%20are%20used.%20For%20this%20reason%2C%20instead%20of%0Agenerating%20textures%20in%20automatically%20generated%20UV-planes%20like%20most%0Astate-of-the-art%20methods%2C%20we%20propose%20to%20represent%20textures%20as%20coloured%0Apoint-clouds%20whose%20colours%20are%20generated%20by%20a%20denoising%20diffusion%20probabilistic%0Amodel%20constrained%20to%20operate%20on%20the%20surface%20of%203D%20objects.%20Our%20sampling%20and%0Aresolution%20agnostic%20generative%20model%20heavily%20relies%20on%20heat%20diffusion%20over%20the%0Asurface%20of%20the%20meshes%20for%20spatial%20communication%20between%20points.%20To%20enable%0Aprocessing%20of%20arbitrarily%20sampled%20point-cloud%20textures%20and%20ensure%20long-distance%0Atexture%20consistency%20we%20introduce%20a%20fast%20re-sampling%20of%20the%20mesh%20spectral%0Aproperties%20used%20during%20the%20heat%20diffusion%20and%20introduce%20a%20novel%0Aheat-diffusion-based%20self-attention%20mechanism.%20Our%20code%20and%20pre-trained%20models%0Aare%20available%20at%20github.com/simofoti/UV3-TeD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16762v2&entry.124074799=Read"},
{"title": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Models", "author": "Simon Schrodi and David T. Hoffmann and Max Argus and Volker Fischer and Thomas Brox", "abstract": "  Contrastive vision-language models (VLMs), like CLIP, have gained popularity\nfor their versatile applicability to various downstream tasks. Despite their\nsuccesses in some tasks, like zero-shot object recognition, they perform\nsurprisingly poor on other tasks, like attribute recognition. Previous work has\nattributed these challenges to the modality gap, a separation of image and text\nin the shared representation space, and to a bias towards objects over other\nfactors, such as attributes. In this analysis paper, we investigate both\nphenomena thoroughly. We evaluated off-the-shelf VLMs and find that while the\ngap's influence on performance is typically overshadowed by other factors, we\nfind indications that closing the gap indeed leads to improvements. Moreover,\nwe find that, contrary to intuition, only few embedding dimensions drive the\ngap and that the embedding spaces are differently organized. To allow for a\nclean study of object bias, we introduce a definition and a corresponding\nmeasure of it. Equipped with this tool, we find that object bias does not lead\nto worse performance on other concepts, such as attributes per se. However, why\ndo both phenomena, modality gap and object bias, emerge in the first place? To\nanswer this fundamental question and uncover some of the inner workings of\ncontrastive VLMs, we conducted experiments that allowed us to control the\namount of shared information between the modalities. These experiments revealed\nthat the driving factor behind both the modality gap and the object bias, is an\ninformation imbalance between images and captions, and unveiled an intriguing\nconnection between the modality gap and entropy of the logits.\n", "link": "http://arxiv.org/abs/2404.07983v2", "date": "2024-10-10", "relevancy": 3.0699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6181}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Models&body=Title%3A%20Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Models%0AAuthor%3A%20Simon%20Schrodi%20and%20David%20T.%20Hoffmann%20and%20Max%20Argus%20and%20Volker%20Fischer%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Contrastive%20vision-language%20models%20%28VLMs%29%2C%20like%20CLIP%2C%20have%20gained%20popularity%0Afor%20their%20versatile%20applicability%20to%20various%20downstream%20tasks.%20Despite%20their%0Asuccesses%20in%20some%20tasks%2C%20like%20zero-shot%20object%20recognition%2C%20they%20perform%0Asurprisingly%20poor%20on%20other%20tasks%2C%20like%20attribute%20recognition.%20Previous%20work%20has%0Aattributed%20these%20challenges%20to%20the%20modality%20gap%2C%20a%20separation%20of%20image%20and%20text%0Ain%20the%20shared%20representation%20space%2C%20and%20to%20a%20bias%20towards%20objects%20over%20other%0Afactors%2C%20such%20as%20attributes.%20In%20this%20analysis%20paper%2C%20we%20investigate%20both%0Aphenomena%20thoroughly.%20We%20evaluated%20off-the-shelf%20VLMs%20and%20find%20that%20while%20the%0Agap%27s%20influence%20on%20performance%20is%20typically%20overshadowed%20by%20other%20factors%2C%20we%0Afind%20indications%20that%20closing%20the%20gap%20indeed%20leads%20to%20improvements.%20Moreover%2C%0Awe%20find%20that%2C%20contrary%20to%20intuition%2C%20only%20few%20embedding%20dimensions%20drive%20the%0Agap%20and%20that%20the%20embedding%20spaces%20are%20differently%20organized.%20To%20allow%20for%20a%0Aclean%20study%20of%20object%20bias%2C%20we%20introduce%20a%20definition%20and%20a%20corresponding%0Ameasure%20of%20it.%20Equipped%20with%20this%20tool%2C%20we%20find%20that%20object%20bias%20does%20not%20lead%0Ato%20worse%20performance%20on%20other%20concepts%2C%20such%20as%20attributes%20per%20se.%20However%2C%20why%0Ado%20both%20phenomena%2C%20modality%20gap%20and%20object%20bias%2C%20emerge%20in%20the%20first%20place%3F%20To%0Aanswer%20this%20fundamental%20question%20and%20uncover%20some%20of%20the%20inner%20workings%20of%0Acontrastive%20VLMs%2C%20we%20conducted%20experiments%20that%20allowed%20us%20to%20control%20the%0Aamount%20of%20shared%20information%20between%20the%20modalities.%20These%20experiments%20revealed%0Athat%20the%20driving%20factor%20behind%20both%20the%20modality%20gap%20and%20the%20object%20bias%2C%20is%20an%0Ainformation%20imbalance%20between%20images%20and%20captions%2C%20and%20unveiled%20an%20intriguing%0Aconnection%20between%20the%20modality%20gap%20and%20entropy%20of%20the%20logits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Effects%252C%2520One%2520Trigger%253A%2520On%2520the%2520Modality%2520Gap%252C%2520Object%2520Bias%252C%2520and%250A%2520%2520Information%2520Imbalance%2520in%2520Contrastive%2520Vision-Language%2520Models%26entry.906535625%3DSimon%2520Schrodi%2520and%2520David%2520T.%2520Hoffmann%2520and%2520Max%2520Argus%2520and%2520Volker%2520Fischer%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Contrastive%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520like%2520CLIP%252C%2520have%2520gained%2520popularity%250Afor%2520their%2520versatile%2520applicability%2520to%2520various%2520downstream%2520tasks.%2520Despite%2520their%250Asuccesses%2520in%2520some%2520tasks%252C%2520like%2520zero-shot%2520object%2520recognition%252C%2520they%2520perform%250Asurprisingly%2520poor%2520on%2520other%2520tasks%252C%2520like%2520attribute%2520recognition.%2520Previous%2520work%2520has%250Aattributed%2520these%2520challenges%2520to%2520the%2520modality%2520gap%252C%2520a%2520separation%2520of%2520image%2520and%2520text%250Ain%2520the%2520shared%2520representation%2520space%252C%2520and%2520to%2520a%2520bias%2520towards%2520objects%2520over%2520other%250Afactors%252C%2520such%2520as%2520attributes.%2520In%2520this%2520analysis%2520paper%252C%2520we%2520investigate%2520both%250Aphenomena%2520thoroughly.%2520We%2520evaluated%2520off-the-shelf%2520VLMs%2520and%2520find%2520that%2520while%2520the%250Agap%2527s%2520influence%2520on%2520performance%2520is%2520typically%2520overshadowed%2520by%2520other%2520factors%252C%2520we%250Afind%2520indications%2520that%2520closing%2520the%2520gap%2520indeed%2520leads%2520to%2520improvements.%2520Moreover%252C%250Awe%2520find%2520that%252C%2520contrary%2520to%2520intuition%252C%2520only%2520few%2520embedding%2520dimensions%2520drive%2520the%250Agap%2520and%2520that%2520the%2520embedding%2520spaces%2520are%2520differently%2520organized.%2520To%2520allow%2520for%2520a%250Aclean%2520study%2520of%2520object%2520bias%252C%2520we%2520introduce%2520a%2520definition%2520and%2520a%2520corresponding%250Ameasure%2520of%2520it.%2520Equipped%2520with%2520this%2520tool%252C%2520we%2520find%2520that%2520object%2520bias%2520does%2520not%2520lead%250Ato%2520worse%2520performance%2520on%2520other%2520concepts%252C%2520such%2520as%2520attributes%2520per%2520se.%2520However%252C%2520why%250Ado%2520both%2520phenomena%252C%2520modality%2520gap%2520and%2520object%2520bias%252C%2520emerge%2520in%2520the%2520first%2520place%253F%2520To%250Aanswer%2520this%2520fundamental%2520question%2520and%2520uncover%2520some%2520of%2520the%2520inner%2520workings%2520of%250Acontrastive%2520VLMs%252C%2520we%2520conducted%2520experiments%2520that%2520allowed%2520us%2520to%2520control%2520the%250Aamount%2520of%2520shared%2520information%2520between%2520the%2520modalities.%2520These%2520experiments%2520revealed%250Athat%2520the%2520driving%2520factor%2520behind%2520both%2520the%2520modality%2520gap%2520and%2520the%2520object%2520bias%252C%2520is%2520an%250Ainformation%2520imbalance%2520between%2520images%2520and%2520captions%252C%2520and%2520unveiled%2520an%2520intriguing%250Aconnection%2520between%2520the%2520modality%2520gap%2520and%2520entropy%2520of%2520the%2520logits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Effects%2C%20One%20Trigger%3A%20On%20the%20Modality%20Gap%2C%20Object%20Bias%2C%20and%0A%20%20Information%20Imbalance%20in%20Contrastive%20Vision-Language%20Models&entry.906535625=Simon%20Schrodi%20and%20David%20T.%20Hoffmann%20and%20Max%20Argus%20and%20Volker%20Fischer%20and%20Thomas%20Brox&entry.1292438233=%20%20Contrastive%20vision-language%20models%20%28VLMs%29%2C%20like%20CLIP%2C%20have%20gained%20popularity%0Afor%20their%20versatile%20applicability%20to%20various%20downstream%20tasks.%20Despite%20their%0Asuccesses%20in%20some%20tasks%2C%20like%20zero-shot%20object%20recognition%2C%20they%20perform%0Asurprisingly%20poor%20on%20other%20tasks%2C%20like%20attribute%20recognition.%20Previous%20work%20has%0Aattributed%20these%20challenges%20to%20the%20modality%20gap%2C%20a%20separation%20of%20image%20and%20text%0Ain%20the%20shared%20representation%20space%2C%20and%20to%20a%20bias%20towards%20objects%20over%20other%0Afactors%2C%20such%20as%20attributes.%20In%20this%20analysis%20paper%2C%20we%20investigate%20both%0Aphenomena%20thoroughly.%20We%20evaluated%20off-the-shelf%20VLMs%20and%20find%20that%20while%20the%0Agap%27s%20influence%20on%20performance%20is%20typically%20overshadowed%20by%20other%20factors%2C%20we%0Afind%20indications%20that%20closing%20the%20gap%20indeed%20leads%20to%20improvements.%20Moreover%2C%0Awe%20find%20that%2C%20contrary%20to%20intuition%2C%20only%20few%20embedding%20dimensions%20drive%20the%0Agap%20and%20that%20the%20embedding%20spaces%20are%20differently%20organized.%20To%20allow%20for%20a%0Aclean%20study%20of%20object%20bias%2C%20we%20introduce%20a%20definition%20and%20a%20corresponding%0Ameasure%20of%20it.%20Equipped%20with%20this%20tool%2C%20we%20find%20that%20object%20bias%20does%20not%20lead%0Ato%20worse%20performance%20on%20other%20concepts%2C%20such%20as%20attributes%20per%20se.%20However%2C%20why%0Ado%20both%20phenomena%2C%20modality%20gap%20and%20object%20bias%2C%20emerge%20in%20the%20first%20place%3F%20To%0Aanswer%20this%20fundamental%20question%20and%20uncover%20some%20of%20the%20inner%20workings%20of%0Acontrastive%20VLMs%2C%20we%20conducted%20experiments%20that%20allowed%20us%20to%20control%20the%0Aamount%20of%20shared%20information%20between%20the%20modalities.%20These%20experiments%20revealed%0Athat%20the%20driving%20factor%20behind%20both%20the%20modality%20gap%20and%20the%20object%20bias%2C%20is%20an%0Ainformation%20imbalance%20between%20images%20and%20captions%2C%20and%20unveiled%20an%20intriguing%0Aconnection%20between%20the%20modality%20gap%20and%20entropy%20of%20the%20logits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07983v2&entry.124074799=Read"},
{"title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training", "author": "Gen Luo and Xue Yang and Wenhan Dou and Zhaokai Wang and Jifeng Dai and Yu Qiao and Xizhou Zhu", "abstract": "  The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%.\n", "link": "http://arxiv.org/abs/2410.08202v1", "date": "2024-10-10", "relevancy": 3.0292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mono-InternVL%3A%20Pushing%20the%20Boundaries%20of%20Monolithic%20Multimodal%20Large%0A%20%20Language%20Models%20with%20Endogenous%20Visual%20Pre-training&body=Title%3A%20Mono-InternVL%3A%20Pushing%20the%20Boundaries%20of%20Monolithic%20Multimodal%20Large%0A%20%20Language%20Models%20with%20Endogenous%20Visual%20Pre-training%0AAuthor%3A%20Gen%20Luo%20and%20Xue%20Yang%20and%20Wenhan%20Dou%20and%20Zhaokai%20Wang%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Xizhou%20Zhu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20an%20influx%20of%0Aefforts%20to%20extend%20their%20capabilities%20to%20multimodal%20tasks.%20Among%20them%2C%20growing%0Aattention%20has%20been%20focused%20on%20monolithic%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20that%20integrate%20visual%20encoding%20and%20language%20decoding%20into%20a%20single%20LLM.%0ADespite%20the%20structural%20simplicity%20and%20deployment-friendliness%2C%20training%20a%0Amonolithic%20MLLM%20with%20promising%20performance%20still%20remains%20challenging.%20In%0Aparticular%2C%20the%20popular%20approaches%20adopt%20continuous%20pre-training%20to%20extend%20a%0Apre-trained%20LLM%20to%20a%20monolithic%20MLLM%2C%20which%20suffers%20from%20catastrophic%0Aforgetting%20and%20leads%20to%20performance%20degeneration.%20In%20this%20paper%2C%20we%20aim%20to%0Aovercome%20this%20limitation%20from%20the%20perspective%20of%20delta%20tuning.%20Specifically%2C%0Aour%20core%20idea%20is%20to%20embed%20visual%20parameters%20into%20a%20pre-trained%20LLM%2C%20thereby%0Aincrementally%20learning%20visual%20knowledge%20from%20massive%20data%20via%20delta%20tuning%2C%0Ai.e.%2C%20freezing%20the%20LLM%20when%20optimizing%20the%20visual%20parameters.%20Based%20on%20this%0Aprinciple%2C%20we%20present%20Mono-InternVL%2C%20a%20novel%20monolithic%20MLLM%20that%20seamlessly%0Aintegrates%20a%20set%20of%20visual%20experts%20via%20a%20multimodal%20mixture-of-experts%0Astructure.%20Moreover%2C%20we%20propose%20an%20innovative%20pre-training%20strategy%20to%20maximize%0Athe%20visual%20capability%20of%20Mono-InternVL%2C%20namely%20Endogenous%20Visual%20Pre-training%0A%28EViP%29.%20In%20particular%2C%20EViP%20is%20designed%20as%20a%20progressive%20learning%20process%20for%0Avisual%20experts%2C%20which%20aims%20to%20fully%20exploit%20the%20visual%20knowledge%20from%20noisy%0Adata%20to%20high-quality%20data.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%0Aexperiments%20on%2016%20benchmarks.%20Experimental%20results%20not%20only%20validate%20the%0Asuperior%20performance%20of%20Mono-InternVL%20compared%20to%20the%20state-of-the-art%20MLLM%20on%0A6%20multimodal%20benchmarks%2C%20e.g.%2C%20%2B113%20points%20over%20InternVL-1.5%20on%20OCRBench%2C%20but%0Aalso%20confirm%20its%20better%20deployment%20efficiency%2C%20with%20first%20token%20latency%20reduced%0Aby%20up%20to%2067%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMono-InternVL%253A%2520Pushing%2520the%2520Boundaries%2520of%2520Monolithic%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%2520with%2520Endogenous%2520Visual%2520Pre-training%26entry.906535625%3DGen%2520Luo%2520and%2520Xue%2520Yang%2520and%2520Wenhan%2520Dou%2520and%2520Zhaokai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Yu%2520Qiao%2520and%2520Xizhou%2520Zhu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520led%2520to%2520an%2520influx%2520of%250Aefforts%2520to%2520extend%2520their%2520capabilities%2520to%2520multimodal%2520tasks.%2520Among%2520them%252C%2520growing%250Aattention%2520has%2520been%2520focused%2520on%2520monolithic%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520that%2520integrate%2520visual%2520encoding%2520and%2520language%2520decoding%2520into%2520a%2520single%2520LLM.%250ADespite%2520the%2520structural%2520simplicity%2520and%2520deployment-friendliness%252C%2520training%2520a%250Amonolithic%2520MLLM%2520with%2520promising%2520performance%2520still%2520remains%2520challenging.%2520In%250Aparticular%252C%2520the%2520popular%2520approaches%2520adopt%2520continuous%2520pre-training%2520to%2520extend%2520a%250Apre-trained%2520LLM%2520to%2520a%2520monolithic%2520MLLM%252C%2520which%2520suffers%2520from%2520catastrophic%250Aforgetting%2520and%2520leads%2520to%2520performance%2520degeneration.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Aovercome%2520this%2520limitation%2520from%2520the%2520perspective%2520of%2520delta%2520tuning.%2520Specifically%252C%250Aour%2520core%2520idea%2520is%2520to%2520embed%2520visual%2520parameters%2520into%2520a%2520pre-trained%2520LLM%252C%2520thereby%250Aincrementally%2520learning%2520visual%2520knowledge%2520from%2520massive%2520data%2520via%2520delta%2520tuning%252C%250Ai.e.%252C%2520freezing%2520the%2520LLM%2520when%2520optimizing%2520the%2520visual%2520parameters.%2520Based%2520on%2520this%250Aprinciple%252C%2520we%2520present%2520Mono-InternVL%252C%2520a%2520novel%2520monolithic%2520MLLM%2520that%2520seamlessly%250Aintegrates%2520a%2520set%2520of%2520visual%2520experts%2520via%2520a%2520multimodal%2520mixture-of-experts%250Astructure.%2520Moreover%252C%2520we%2520propose%2520an%2520innovative%2520pre-training%2520strategy%2520to%2520maximize%250Athe%2520visual%2520capability%2520of%2520Mono-InternVL%252C%2520namely%2520Endogenous%2520Visual%2520Pre-training%250A%2528EViP%2529.%2520In%2520particular%252C%2520EViP%2520is%2520designed%2520as%2520a%2520progressive%2520learning%2520process%2520for%250Avisual%2520experts%252C%2520which%2520aims%2520to%2520fully%2520exploit%2520the%2520visual%2520knowledge%2520from%2520noisy%250Adata%2520to%2520high-quality%2520data.%2520To%2520validate%2520our%2520approach%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520on%252016%2520benchmarks.%2520Experimental%2520results%2520not%2520only%2520validate%2520the%250Asuperior%2520performance%2520of%2520Mono-InternVL%2520compared%2520to%2520the%2520state-of-the-art%2520MLLM%2520on%250A6%2520multimodal%2520benchmarks%252C%2520e.g.%252C%2520%252B113%2520points%2520over%2520InternVL-1.5%2520on%2520OCRBench%252C%2520but%250Aalso%2520confirm%2520its%2520better%2520deployment%2520efficiency%252C%2520with%2520first%2520token%2520latency%2520reduced%250Aby%2520up%2520to%252067%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mono-InternVL%3A%20Pushing%20the%20Boundaries%20of%20Monolithic%20Multimodal%20Large%0A%20%20Language%20Models%20with%20Endogenous%20Visual%20Pre-training&entry.906535625=Gen%20Luo%20and%20Xue%20Yang%20and%20Wenhan%20Dou%20and%20Zhaokai%20Wang%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Xizhou%20Zhu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20an%20influx%20of%0Aefforts%20to%20extend%20their%20capabilities%20to%20multimodal%20tasks.%20Among%20them%2C%20growing%0Aattention%20has%20been%20focused%20on%20monolithic%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20that%20integrate%20visual%20encoding%20and%20language%20decoding%20into%20a%20single%20LLM.%0ADespite%20the%20structural%20simplicity%20and%20deployment-friendliness%2C%20training%20a%0Amonolithic%20MLLM%20with%20promising%20performance%20still%20remains%20challenging.%20In%0Aparticular%2C%20the%20popular%20approaches%20adopt%20continuous%20pre-training%20to%20extend%20a%0Apre-trained%20LLM%20to%20a%20monolithic%20MLLM%2C%20which%20suffers%20from%20catastrophic%0Aforgetting%20and%20leads%20to%20performance%20degeneration.%20In%20this%20paper%2C%20we%20aim%20to%0Aovercome%20this%20limitation%20from%20the%20perspective%20of%20delta%20tuning.%20Specifically%2C%0Aour%20core%20idea%20is%20to%20embed%20visual%20parameters%20into%20a%20pre-trained%20LLM%2C%20thereby%0Aincrementally%20learning%20visual%20knowledge%20from%20massive%20data%20via%20delta%20tuning%2C%0Ai.e.%2C%20freezing%20the%20LLM%20when%20optimizing%20the%20visual%20parameters.%20Based%20on%20this%0Aprinciple%2C%20we%20present%20Mono-InternVL%2C%20a%20novel%20monolithic%20MLLM%20that%20seamlessly%0Aintegrates%20a%20set%20of%20visual%20experts%20via%20a%20multimodal%20mixture-of-experts%0Astructure.%20Moreover%2C%20we%20propose%20an%20innovative%20pre-training%20strategy%20to%20maximize%0Athe%20visual%20capability%20of%20Mono-InternVL%2C%20namely%20Endogenous%20Visual%20Pre-training%0A%28EViP%29.%20In%20particular%2C%20EViP%20is%20designed%20as%20a%20progressive%20learning%20process%20for%0Avisual%20experts%2C%20which%20aims%20to%20fully%20exploit%20the%20visual%20knowledge%20from%20noisy%0Adata%20to%20high-quality%20data.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%0Aexperiments%20on%2016%20benchmarks.%20Experimental%20results%20not%20only%20validate%20the%0Asuperior%20performance%20of%20Mono-InternVL%20compared%20to%20the%20state-of-the-art%20MLLM%20on%0A6%20multimodal%20benchmarks%2C%20e.g.%2C%20%2B113%20points%20over%20InternVL-1.5%20on%20OCRBench%2C%20but%0Aalso%20confirm%20its%20better%20deployment%20efficiency%2C%20with%20first%20token%20latency%20reduced%0Aby%20up%20to%2067%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08202v1&entry.124074799=Read"},
{"title": "Morphing Tokens Draw Strong Masked Image Models", "author": "Taekyung Kim and Byeongho Heo and Dongyoon Han", "abstract": "  Masked image modeling (MIM) has emerged as a promising approach for training\nVision Transformers (ViTs). The essence of MIM lies in the token-wise\nprediction of masked tokens, which aims to predict targets tokenized from\nimages or generated by pre-trained models like vision-language models. While\nusing tokenizers or pre-trained models are plausible MIM targets, they often\noffer spatially inconsistent targets even for neighboring tokens, complicating\nmodels to learn unified and discriminative representations. Our pilot study\nidentifies spatial inconsistencies and suggests that resolving them can\naccelerate representation learning. Building upon this insight, we introduce a\nnovel self-supervision signal called Dynamic Token Morphing (DTM), which\ndynamically aggregates contextually related tokens to yield contextualized\ntargets, thereby mitigating spatial inconsistency. DTM is compatible with\nvarious SSL frameworks; we showcase improved MIM results by employing DTM,\nbarely introducing extra training costs. Our method facilitates training by\nusing consistent targets, resulting in 1) faster training and 2) reduced\nlosses. Experiments on ImageNet-1K and ADE20K demonstrate the superiority of\nour method compared with state-of-the-art, complex MIM methods. Furthermore,\nthe comparative evaluation of the iNaturalists and fine-grained visual\nclassification datasets further validates the transferability of our method on\nvarious downstream tasks. Code is available at https://github.com/naver-ai/dtm\n", "link": "http://arxiv.org/abs/2401.00254v3", "date": "2024-10-10", "relevancy": 3.0007, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6031}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models&body=Title%3A%20Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models%0AAuthor%3A%20Taekyung%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han%0AAbstract%3A%20%20%20Masked%20image%20modeling%20%28MIM%29%20has%20emerged%20as%20a%20promising%20approach%20for%20training%0AVision%20Transformers%20%28ViTs%29.%20The%20essence%20of%20MIM%20lies%20in%20the%20token-wise%0Aprediction%20of%20masked%20tokens%2C%20which%20aims%20to%20predict%20targets%20tokenized%20from%0Aimages%20or%20generated%20by%20pre-trained%20models%20like%20vision-language%20models.%20While%0Ausing%20tokenizers%20or%20pre-trained%20models%20are%20plausible%20MIM%20targets%2C%20they%20often%0Aoffer%20spatially%20inconsistent%20targets%20even%20for%20neighboring%20tokens%2C%20complicating%0Amodels%20to%20learn%20unified%20and%20discriminative%20representations.%20Our%20pilot%20study%0Aidentifies%20spatial%20inconsistencies%20and%20suggests%20that%20resolving%20them%20can%0Aaccelerate%20representation%20learning.%20Building%20upon%20this%20insight%2C%20we%20introduce%20a%0Anovel%20self-supervision%20signal%20called%20Dynamic%20Token%20Morphing%20%28DTM%29%2C%20which%0Adynamically%20aggregates%20contextually%20related%20tokens%20to%20yield%20contextualized%0Atargets%2C%20thereby%20mitigating%20spatial%20inconsistency.%20DTM%20is%20compatible%20with%0Avarious%20SSL%20frameworks%3B%20we%20showcase%20improved%20MIM%20results%20by%20employing%20DTM%2C%0Abarely%20introducing%20extra%20training%20costs.%20Our%20method%20facilitates%20training%20by%0Ausing%20consistent%20targets%2C%20resulting%20in%201%29%20faster%20training%20and%202%29%20reduced%0Alosses.%20Experiments%20on%20ImageNet-1K%20and%20ADE20K%20demonstrate%20the%20superiority%20of%0Aour%20method%20compared%20with%20state-of-the-art%2C%20complex%20MIM%20methods.%20Furthermore%2C%0Athe%20comparative%20evaluation%20of%20the%20iNaturalists%20and%20fine-grained%20visual%0Aclassification%20datasets%20further%20validates%20the%20transferability%20of%20our%20method%20on%0Avarious%20downstream%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/naver-ai/dtm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphing%2520Tokens%2520Draw%2520Strong%2520Masked%2520Image%2520Models%26entry.906535625%3DTaekyung%2520Kim%2520and%2520Byeongho%2520Heo%2520and%2520Dongyoon%2520Han%26entry.1292438233%3D%2520%2520Masked%2520image%2520modeling%2520%2528MIM%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520training%250AVision%2520Transformers%2520%2528ViTs%2529.%2520The%2520essence%2520of%2520MIM%2520lies%2520in%2520the%2520token-wise%250Aprediction%2520of%2520masked%2520tokens%252C%2520which%2520aims%2520to%2520predict%2520targets%2520tokenized%2520from%250Aimages%2520or%2520generated%2520by%2520pre-trained%2520models%2520like%2520vision-language%2520models.%2520While%250Ausing%2520tokenizers%2520or%2520pre-trained%2520models%2520are%2520plausible%2520MIM%2520targets%252C%2520they%2520often%250Aoffer%2520spatially%2520inconsistent%2520targets%2520even%2520for%2520neighboring%2520tokens%252C%2520complicating%250Amodels%2520to%2520learn%2520unified%2520and%2520discriminative%2520representations.%2520Our%2520pilot%2520study%250Aidentifies%2520spatial%2520inconsistencies%2520and%2520suggests%2520that%2520resolving%2520them%2520can%250Aaccelerate%2520representation%2520learning.%2520Building%2520upon%2520this%2520insight%252C%2520we%2520introduce%2520a%250Anovel%2520self-supervision%2520signal%2520called%2520Dynamic%2520Token%2520Morphing%2520%2528DTM%2529%252C%2520which%250Adynamically%2520aggregates%2520contextually%2520related%2520tokens%2520to%2520yield%2520contextualized%250Atargets%252C%2520thereby%2520mitigating%2520spatial%2520inconsistency.%2520DTM%2520is%2520compatible%2520with%250Avarious%2520SSL%2520frameworks%253B%2520we%2520showcase%2520improved%2520MIM%2520results%2520by%2520employing%2520DTM%252C%250Abarely%2520introducing%2520extra%2520training%2520costs.%2520Our%2520method%2520facilitates%2520training%2520by%250Ausing%2520consistent%2520targets%252C%2520resulting%2520in%25201%2529%2520faster%2520training%2520and%25202%2529%2520reduced%250Alosses.%2520Experiments%2520on%2520ImageNet-1K%2520and%2520ADE20K%2520demonstrate%2520the%2520superiority%2520of%250Aour%2520method%2520compared%2520with%2520state-of-the-art%252C%2520complex%2520MIM%2520methods.%2520Furthermore%252C%250Athe%2520comparative%2520evaluation%2520of%2520the%2520iNaturalists%2520and%2520fine-grained%2520visual%250Aclassification%2520datasets%2520further%2520validates%2520the%2520transferability%2520of%2520our%2520method%2520on%250Avarious%2520downstream%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/naver-ai/dtm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models&entry.906535625=Taekyung%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han&entry.1292438233=%20%20Masked%20image%20modeling%20%28MIM%29%20has%20emerged%20as%20a%20promising%20approach%20for%20training%0AVision%20Transformers%20%28ViTs%29.%20The%20essence%20of%20MIM%20lies%20in%20the%20token-wise%0Aprediction%20of%20masked%20tokens%2C%20which%20aims%20to%20predict%20targets%20tokenized%20from%0Aimages%20or%20generated%20by%20pre-trained%20models%20like%20vision-language%20models.%20While%0Ausing%20tokenizers%20or%20pre-trained%20models%20are%20plausible%20MIM%20targets%2C%20they%20often%0Aoffer%20spatially%20inconsistent%20targets%20even%20for%20neighboring%20tokens%2C%20complicating%0Amodels%20to%20learn%20unified%20and%20discriminative%20representations.%20Our%20pilot%20study%0Aidentifies%20spatial%20inconsistencies%20and%20suggests%20that%20resolving%20them%20can%0Aaccelerate%20representation%20learning.%20Building%20upon%20this%20insight%2C%20we%20introduce%20a%0Anovel%20self-supervision%20signal%20called%20Dynamic%20Token%20Morphing%20%28DTM%29%2C%20which%0Adynamically%20aggregates%20contextually%20related%20tokens%20to%20yield%20contextualized%0Atargets%2C%20thereby%20mitigating%20spatial%20inconsistency.%20DTM%20is%20compatible%20with%0Avarious%20SSL%20frameworks%3B%20we%20showcase%20improved%20MIM%20results%20by%20employing%20DTM%2C%0Abarely%20introducing%20extra%20training%20costs.%20Our%20method%20facilitates%20training%20by%0Ausing%20consistent%20targets%2C%20resulting%20in%201%29%20faster%20training%20and%202%29%20reduced%0Alosses.%20Experiments%20on%20ImageNet-1K%20and%20ADE20K%20demonstrate%20the%20superiority%20of%0Aour%20method%20compared%20with%20state-of-the-art%2C%20complex%20MIM%20methods.%20Furthermore%2C%0Athe%20comparative%20evaluation%20of%20the%20iNaturalists%20and%20fine-grained%20visual%0Aclassification%20datasets%20further%20validates%20the%20transferability%20of%20our%20method%20on%0Avarious%20downstream%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/naver-ai/dtm%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00254v3&entry.124074799=Read"},
{"title": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs", "author": "Xiaoyuan Liu and Wenxuan Wang and Youliang Yuan and Jen-tse Huang and Qiuzhi Liu and Pinjia He and Zhaopeng Tu", "abstract": "  This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available.\n", "link": "http://arxiv.org/abs/2410.08145v1", "date": "2024-10-10", "relevancy": 2.9903, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6111}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insight%20Over%20Sight%3F%20Exploring%20the%20Vision-Knowledge%20Conflicts%20in%0A%20%20Multimodal%20LLMs&body=Title%3A%20Insight%20Over%20Sight%3F%20Exploring%20the%20Vision-Knowledge%20Conflicts%20in%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Youliang%20Yuan%20and%20Jen-tse%20Huang%20and%20Qiuzhi%20Liu%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20problem%20of%20commonsense-level%20vision-knowledge%0Aconflict%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20where%20visual%20information%0Acontradicts%20model%27s%20internal%20commonsense%20knowledge%20%28see%20Figure%201%29.%20To%20study%0Athis%20issue%2C%20we%20introduce%20an%20automated%20pipeline%2C%20augmented%20with%0Ahuman-in-the-loop%20quality%20control%2C%20to%20establish%20a%20benchmark%20aimed%20at%20simulating%0Aand%20assessing%20the%20conflicts%20in%20MLLMs.%20Utilizing%20this%20pipeline%2C%20we%20have%20crafted%0Aa%20diagnostic%20benchmark%20comprising%20374%20original%20images%20and%201%2C122%20high-quality%0Aquestion-answer%20%28QA%29%20pairs.%20This%20benchmark%20covers%20two%20types%20of%20conflict%20target%0Aand%20three%20question%20difficulty%20levels%2C%20providing%20a%20thorough%20assessment%20tool.%0AThrough%20this%20benchmark%2C%20we%20evaluate%20the%20conflict-resolution%20capabilities%20of%0Anine%20representative%20MLLMs%20across%20various%20model%20families%20and%20find%20a%20noticeable%0Aover-reliance%20on%20textual%20queries.%20Drawing%20on%20these%20findings%2C%20we%20propose%20a%20novel%0Aprompting%20strategy%2C%20%22Focus-on-Vision%22%20%28FoV%29%2C%20which%20markedly%20enhances%20MLLMs%27%0Aability%20to%20favor%20visual%20data%20over%20conflicting%20textual%20knowledge.%20Our%20detailed%0Aanalysis%20and%20the%20newly%20proposed%20strategy%20significantly%20advance%20the%0Aunderstanding%20and%20mitigating%20of%20vision-knowledge%20conflicts%20in%20MLLMs.%20The%20data%0Aand%20code%20are%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsight%2520Over%2520Sight%253F%2520Exploring%2520the%2520Vision-Knowledge%2520Conflicts%2520in%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DXiaoyuan%2520Liu%2520and%2520Wenxuan%2520Wang%2520and%2520Youliang%2520Yuan%2520and%2520Jen-tse%2520Huang%2520and%2520Qiuzhi%2520Liu%2520and%2520Pinjia%2520He%2520and%2520Zhaopeng%2520Tu%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520problem%2520of%2520commonsense-level%2520vision-knowledge%250Aconflict%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520where%2520visual%2520information%250Acontradicts%2520model%2527s%2520internal%2520commonsense%2520knowledge%2520%2528see%2520Figure%25201%2529.%2520To%2520study%250Athis%2520issue%252C%2520we%2520introduce%2520an%2520automated%2520pipeline%252C%2520augmented%2520with%250Ahuman-in-the-loop%2520quality%2520control%252C%2520to%2520establish%2520a%2520benchmark%2520aimed%2520at%2520simulating%250Aand%2520assessing%2520the%2520conflicts%2520in%2520MLLMs.%2520Utilizing%2520this%2520pipeline%252C%2520we%2520have%2520crafted%250Aa%2520diagnostic%2520benchmark%2520comprising%2520374%2520original%2520images%2520and%25201%252C122%2520high-quality%250Aquestion-answer%2520%2528QA%2529%2520pairs.%2520This%2520benchmark%2520covers%2520two%2520types%2520of%2520conflict%2520target%250Aand%2520three%2520question%2520difficulty%2520levels%252C%2520providing%2520a%2520thorough%2520assessment%2520tool.%250AThrough%2520this%2520benchmark%252C%2520we%2520evaluate%2520the%2520conflict-resolution%2520capabilities%2520of%250Anine%2520representative%2520MLLMs%2520across%2520various%2520model%2520families%2520and%2520find%2520a%2520noticeable%250Aover-reliance%2520on%2520textual%2520queries.%2520Drawing%2520on%2520these%2520findings%252C%2520we%2520propose%2520a%2520novel%250Aprompting%2520strategy%252C%2520%2522Focus-on-Vision%2522%2520%2528FoV%2529%252C%2520which%2520markedly%2520enhances%2520MLLMs%2527%250Aability%2520to%2520favor%2520visual%2520data%2520over%2520conflicting%2520textual%2520knowledge.%2520Our%2520detailed%250Aanalysis%2520and%2520the%2520newly%2520proposed%2520strategy%2520significantly%2520advance%2520the%250Aunderstanding%2520and%2520mitigating%2520of%2520vision-knowledge%2520conflicts%2520in%2520MLLMs.%2520The%2520data%250Aand%2520code%2520are%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight%20Over%20Sight%3F%20Exploring%20the%20Vision-Knowledge%20Conflicts%20in%0A%20%20Multimodal%20LLMs&entry.906535625=Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Youliang%20Yuan%20and%20Jen-tse%20Huang%20and%20Qiuzhi%20Liu%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu&entry.1292438233=%20%20This%20paper%20explores%20the%20problem%20of%20commonsense-level%20vision-knowledge%0Aconflict%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20where%20visual%20information%0Acontradicts%20model%27s%20internal%20commonsense%20knowledge%20%28see%20Figure%201%29.%20To%20study%0Athis%20issue%2C%20we%20introduce%20an%20automated%20pipeline%2C%20augmented%20with%0Ahuman-in-the-loop%20quality%20control%2C%20to%20establish%20a%20benchmark%20aimed%20at%20simulating%0Aand%20assessing%20the%20conflicts%20in%20MLLMs.%20Utilizing%20this%20pipeline%2C%20we%20have%20crafted%0Aa%20diagnostic%20benchmark%20comprising%20374%20original%20images%20and%201%2C122%20high-quality%0Aquestion-answer%20%28QA%29%20pairs.%20This%20benchmark%20covers%20two%20types%20of%20conflict%20target%0Aand%20three%20question%20difficulty%20levels%2C%20providing%20a%20thorough%20assessment%20tool.%0AThrough%20this%20benchmark%2C%20we%20evaluate%20the%20conflict-resolution%20capabilities%20of%0Anine%20representative%20MLLMs%20across%20various%20model%20families%20and%20find%20a%20noticeable%0Aover-reliance%20on%20textual%20queries.%20Drawing%20on%20these%20findings%2C%20we%20propose%20a%20novel%0Aprompting%20strategy%2C%20%22Focus-on-Vision%22%20%28FoV%29%2C%20which%20markedly%20enhances%20MLLMs%27%0Aability%20to%20favor%20visual%20data%20over%20conflicting%20textual%20knowledge.%20Our%20detailed%0Aanalysis%20and%20the%20newly%20proposed%20strategy%20significantly%20advance%20the%0Aunderstanding%20and%20mitigating%20of%20vision-knowledge%20conflicts%20in%20MLLMs.%20The%20data%0Aand%20code%20are%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08145v1&entry.124074799=Read"},
{"title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation", "author": "Haoyi Zhu and Honghui Yang and Yating Wang and Jiange Yang and Limin Wang and Tong He", "abstract": "  In this paper, we introduce SPA, a novel representation learning framework\nthat emphasizes the importance of 3D spatial awareness in embodied AI. Our\napproach leverages differentiable neural rendering on multi-view images to\nendow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.\nWe present the most comprehensive evaluation of embodied representation\nlearning to date, covering 268 tasks across 8 simulators with diverse policies\nin both single-task and language-conditioned multi-task scenarios. The results\nare compelling: SPA consistently outperforms more than 10 state-of-the-art\nrepresentation methods, including those specifically designed for embodied AI,\nvision-centric tasks, and multi-modal applications, while using less training\ndata. Furthermore, we conduct a series of real-world experiments to confirm its\neffectiveness in practical scenarios. These results highlight the critical role\nof 3D spatial awareness for embodied representation learning. Our strongest\nmodel takes more than 6000 GPU hours to train and we are committed to\nopen-sourcing all code and model weights to foster future research in embodied\nrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.\n", "link": "http://arxiv.org/abs/2410.08208v1", "date": "2024-10-10", "relevancy": 2.9836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.617}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPA%3A%203D%20Spatial-Awareness%20Enables%20Effective%20Embodied%20Representation&body=Title%3A%20SPA%3A%203D%20Spatial-Awareness%20Enables%20Effective%20Embodied%20Representation%0AAuthor%3A%20Haoyi%20Zhu%20and%20Honghui%20Yang%20and%20Yating%20Wang%20and%20Jiange%20Yang%20and%20Limin%20Wang%20and%20Tong%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SPA%2C%20a%20novel%20representation%20learning%20framework%0Athat%20emphasizes%20the%20importance%20of%203D%20spatial%20awareness%20in%20embodied%20AI.%20Our%0Aapproach%20leverages%20differentiable%20neural%20rendering%20on%20multi-view%20images%20to%0Aendow%20a%20vanilla%20Vision%20Transformer%20%28ViT%29%20with%20intrinsic%20spatial%20understanding.%0AWe%20present%20the%20most%20comprehensive%20evaluation%20of%20embodied%20representation%0Alearning%20to%20date%2C%20covering%20268%20tasks%20across%208%20simulators%20with%20diverse%20policies%0Ain%20both%20single-task%20and%20language-conditioned%20multi-task%20scenarios.%20The%20results%0Aare%20compelling%3A%20SPA%20consistently%20outperforms%20more%20than%2010%20state-of-the-art%0Arepresentation%20methods%2C%20including%20those%20specifically%20designed%20for%20embodied%20AI%2C%0Avision-centric%20tasks%2C%20and%20multi-modal%20applications%2C%20while%20using%20less%20training%0Adata.%20Furthermore%2C%20we%20conduct%20a%20series%20of%20real-world%20experiments%20to%20confirm%20its%0Aeffectiveness%20in%20practical%20scenarios.%20These%20results%20highlight%20the%20critical%20role%0Aof%203D%20spatial%20awareness%20for%20embodied%20representation%20learning.%20Our%20strongest%0Amodel%20takes%20more%20than%206000%20GPU%20hours%20to%20train%20and%20we%20are%20committed%20to%0Aopen-sourcing%20all%20code%20and%20model%20weights%20to%20foster%20future%20research%20in%20embodied%0Arepresentation%20learning.%20Project%20Page%3A%20https%3A//haoyizhu.github.io/spa/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPA%253A%25203D%2520Spatial-Awareness%2520Enables%2520Effective%2520Embodied%2520Representation%26entry.906535625%3DHaoyi%2520Zhu%2520and%2520Honghui%2520Yang%2520and%2520Yating%2520Wang%2520and%2520Jiange%2520Yang%2520and%2520Limin%2520Wang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SPA%252C%2520a%2520novel%2520representation%2520learning%2520framework%250Athat%2520emphasizes%2520the%2520importance%2520of%25203D%2520spatial%2520awareness%2520in%2520embodied%2520AI.%2520Our%250Aapproach%2520leverages%2520differentiable%2520neural%2520rendering%2520on%2520multi-view%2520images%2520to%250Aendow%2520a%2520vanilla%2520Vision%2520Transformer%2520%2528ViT%2529%2520with%2520intrinsic%2520spatial%2520understanding.%250AWe%2520present%2520the%2520most%2520comprehensive%2520evaluation%2520of%2520embodied%2520representation%250Alearning%2520to%2520date%252C%2520covering%2520268%2520tasks%2520across%25208%2520simulators%2520with%2520diverse%2520policies%250Ain%2520both%2520single-task%2520and%2520language-conditioned%2520multi-task%2520scenarios.%2520The%2520results%250Aare%2520compelling%253A%2520SPA%2520consistently%2520outperforms%2520more%2520than%252010%2520state-of-the-art%250Arepresentation%2520methods%252C%2520including%2520those%2520specifically%2520designed%2520for%2520embodied%2520AI%252C%250Avision-centric%2520tasks%252C%2520and%2520multi-modal%2520applications%252C%2520while%2520using%2520less%2520training%250Adata.%2520Furthermore%252C%2520we%2520conduct%2520a%2520series%2520of%2520real-world%2520experiments%2520to%2520confirm%2520its%250Aeffectiveness%2520in%2520practical%2520scenarios.%2520These%2520results%2520highlight%2520the%2520critical%2520role%250Aof%25203D%2520spatial%2520awareness%2520for%2520embodied%2520representation%2520learning.%2520Our%2520strongest%250Amodel%2520takes%2520more%2520than%25206000%2520GPU%2520hours%2520to%2520train%2520and%2520we%2520are%2520committed%2520to%250Aopen-sourcing%2520all%2520code%2520and%2520model%2520weights%2520to%2520foster%2520future%2520research%2520in%2520embodied%250Arepresentation%2520learning.%2520Project%2520Page%253A%2520https%253A//haoyizhu.github.io/spa/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPA%3A%203D%20Spatial-Awareness%20Enables%20Effective%20Embodied%20Representation&entry.906535625=Haoyi%20Zhu%20and%20Honghui%20Yang%20and%20Yating%20Wang%20and%20Jiange%20Yang%20and%20Limin%20Wang%20and%20Tong%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SPA%2C%20a%20novel%20representation%20learning%20framework%0Athat%20emphasizes%20the%20importance%20of%203D%20spatial%20awareness%20in%20embodied%20AI.%20Our%0Aapproach%20leverages%20differentiable%20neural%20rendering%20on%20multi-view%20images%20to%0Aendow%20a%20vanilla%20Vision%20Transformer%20%28ViT%29%20with%20intrinsic%20spatial%20understanding.%0AWe%20present%20the%20most%20comprehensive%20evaluation%20of%20embodied%20representation%0Alearning%20to%20date%2C%20covering%20268%20tasks%20across%208%20simulators%20with%20diverse%20policies%0Ain%20both%20single-task%20and%20language-conditioned%20multi-task%20scenarios.%20The%20results%0Aare%20compelling%3A%20SPA%20consistently%20outperforms%20more%20than%2010%20state-of-the-art%0Arepresentation%20methods%2C%20including%20those%20specifically%20designed%20for%20embodied%20AI%2C%0Avision-centric%20tasks%2C%20and%20multi-modal%20applications%2C%20while%20using%20less%20training%0Adata.%20Furthermore%2C%20we%20conduct%20a%20series%20of%20real-world%20experiments%20to%20confirm%20its%0Aeffectiveness%20in%20practical%20scenarios.%20These%20results%20highlight%20the%20critical%20role%0Aof%203D%20spatial%20awareness%20for%20embodied%20representation%20learning.%20Our%20strongest%0Amodel%20takes%20more%20than%206000%20GPU%20hours%20to%20train%20and%20we%20are%20committed%20to%0Aopen-sourcing%20all%20code%20and%20model%20weights%20to%20foster%20future%20research%20in%20embodied%0Arepresentation%20learning.%20Project%20Page%3A%20https%3A//haoyizhu.github.io/spa/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08208v1&entry.124074799=Read"},
{"title": "OpenDAS: Open-Vocabulary Domain Adaptation for Segmentation", "author": "Gonca Yilmaz and Songyou Peng and Marc Pollefeys and Francis Engelmann and Hermann Blum", "abstract": "  Recently, Vision-Language Models (VLMs) have advanced segmentation techniques\nby shifting from the traditional segmentation of a closed-set of predefined\nobject classes to open-vocabulary segmentation (OVS), allowing users to segment\nnovel classes and concepts unseen during training of the segmentation model.\nHowever, this flexibility comes with a trade-off: fully-supervised closed-set\nmethods still outperform OVS methods on base classes, that is on classes on\nwhich they have been explicitly trained. This is due to the lack of\npixel-aligned training masks for VLMs (which are trained on image-caption\npairs), and the absence of domain-specific knowledge, such as autonomous\ndriving. Therefore, we propose the task of open-vocabulary domain adaptation to\ninfuse domain-specific knowledge into VLMs while preserving their\nopen-vocabulary nature. By doing so, we achieve improved performance in base\nand novel classes. Existing VLM adaptation methods improve performance on base\n(training) queries, but fail to fully preserve the open-set capabilities of\nVLMs on novel queries. To address this shortcoming, we combine\nparameter-efficient prompt tuning with a triplet-loss-based training strategy\nthat uses auxiliary negative queries. Notably, our approach is the only\nparameter-efficient method that consistently surpasses the original VLM on\nnovel classes. Our adapted VLMs can seamlessly be integrated into existing OVS\npipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D\nsegmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for\nopen-vocabulary 3D instance segmentation without other changes.\n", "link": "http://arxiv.org/abs/2405.20141v2", "date": "2024-10-10", "relevancy": 2.9689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDAS%3A%20Open-Vocabulary%20Domain%20Adaptation%20for%20Segmentation&body=Title%3A%20OpenDAS%3A%20Open-Vocabulary%20Domain%20Adaptation%20for%20Segmentation%0AAuthor%3A%20Gonca%20Yilmaz%20and%20Songyou%20Peng%20and%20Marc%20Pollefeys%20and%20Francis%20Engelmann%20and%20Hermann%20Blum%0AAbstract%3A%20%20%20Recently%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20advanced%20segmentation%20techniques%0Aby%20shifting%20from%20the%20traditional%20segmentation%20of%20a%20closed-set%20of%20predefined%0Aobject%20classes%20to%20open-vocabulary%20segmentation%20%28OVS%29%2C%20allowing%20users%20to%20segment%0Anovel%20classes%20and%20concepts%20unseen%20during%20training%20of%20the%20segmentation%20model.%0AHowever%2C%20this%20flexibility%20comes%20with%20a%20trade-off%3A%20fully-supervised%20closed-set%0Amethods%20still%20outperform%20OVS%20methods%20on%20base%20classes%2C%20that%20is%20on%20classes%20on%0Awhich%20they%20have%20been%20explicitly%20trained.%20This%20is%20due%20to%20the%20lack%20of%0Apixel-aligned%20training%20masks%20for%20VLMs%20%28which%20are%20trained%20on%20image-caption%0Apairs%29%2C%20and%20the%20absence%20of%20domain-specific%20knowledge%2C%20such%20as%20autonomous%0Adriving.%20Therefore%2C%20we%20propose%20the%20task%20of%20open-vocabulary%20domain%20adaptation%20to%0Ainfuse%20domain-specific%20knowledge%20into%20VLMs%20while%20preserving%20their%0Aopen-vocabulary%20nature.%20By%20doing%20so%2C%20we%20achieve%20improved%20performance%20in%20base%0Aand%20novel%20classes.%20Existing%20VLM%20adaptation%20methods%20improve%20performance%20on%20base%0A%28training%29%20queries%2C%20but%20fail%20to%20fully%20preserve%20the%20open-set%20capabilities%20of%0AVLMs%20on%20novel%20queries.%20To%20address%20this%20shortcoming%2C%20we%20combine%0Aparameter-efficient%20prompt%20tuning%20with%20a%20triplet-loss-based%20training%20strategy%0Athat%20uses%20auxiliary%20negative%20queries.%20Notably%2C%20our%20approach%20is%20the%20only%0Aparameter-efficient%20method%20that%20consistently%20surpasses%20the%20original%20VLM%20on%0Anovel%20classes.%20Our%20adapted%20VLMs%20can%20seamlessly%20be%20integrated%20into%20existing%20OVS%0Apipelines%2C%20e.g.%2C%20improving%20OVSeg%20by%20%2B6.0%25%20mIoU%20on%20ADE20K%20for%20open-vocabulary%202D%0Asegmentation%2C%20and%20OpenMask3D%20by%20%2B4.1%25%20AP%20on%20ScanNet%2B%2B%20Offices%20for%0Aopen-vocabulary%203D%20instance%20segmentation%20without%20other%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDAS%253A%2520Open-Vocabulary%2520Domain%2520Adaptation%2520for%2520Segmentation%26entry.906535625%3DGonca%2520Yilmaz%2520and%2520Songyou%2520Peng%2520and%2520Marc%2520Pollefeys%2520and%2520Francis%2520Engelmann%2520and%2520Hermann%2520Blum%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520advanced%2520segmentation%2520techniques%250Aby%2520shifting%2520from%2520the%2520traditional%2520segmentation%2520of%2520a%2520closed-set%2520of%2520predefined%250Aobject%2520classes%2520to%2520open-vocabulary%2520segmentation%2520%2528OVS%2529%252C%2520allowing%2520users%2520to%2520segment%250Anovel%2520classes%2520and%2520concepts%2520unseen%2520during%2520training%2520of%2520the%2520segmentation%2520model.%250AHowever%252C%2520this%2520flexibility%2520comes%2520with%2520a%2520trade-off%253A%2520fully-supervised%2520closed-set%250Amethods%2520still%2520outperform%2520OVS%2520methods%2520on%2520base%2520classes%252C%2520that%2520is%2520on%2520classes%2520on%250Awhich%2520they%2520have%2520been%2520explicitly%2520trained.%2520This%2520is%2520due%2520to%2520the%2520lack%2520of%250Apixel-aligned%2520training%2520masks%2520for%2520VLMs%2520%2528which%2520are%2520trained%2520on%2520image-caption%250Apairs%2529%252C%2520and%2520the%2520absence%2520of%2520domain-specific%2520knowledge%252C%2520such%2520as%2520autonomous%250Adriving.%2520Therefore%252C%2520we%2520propose%2520the%2520task%2520of%2520open-vocabulary%2520domain%2520adaptation%2520to%250Ainfuse%2520domain-specific%2520knowledge%2520into%2520VLMs%2520while%2520preserving%2520their%250Aopen-vocabulary%2520nature.%2520By%2520doing%2520so%252C%2520we%2520achieve%2520improved%2520performance%2520in%2520base%250Aand%2520novel%2520classes.%2520Existing%2520VLM%2520adaptation%2520methods%2520improve%2520performance%2520on%2520base%250A%2528training%2529%2520queries%252C%2520but%2520fail%2520to%2520fully%2520preserve%2520the%2520open-set%2520capabilities%2520of%250AVLMs%2520on%2520novel%2520queries.%2520To%2520address%2520this%2520shortcoming%252C%2520we%2520combine%250Aparameter-efficient%2520prompt%2520tuning%2520with%2520a%2520triplet-loss-based%2520training%2520strategy%250Athat%2520uses%2520auxiliary%2520negative%2520queries.%2520Notably%252C%2520our%2520approach%2520is%2520the%2520only%250Aparameter-efficient%2520method%2520that%2520consistently%2520surpasses%2520the%2520original%2520VLM%2520on%250Anovel%2520classes.%2520Our%2520adapted%2520VLMs%2520can%2520seamlessly%2520be%2520integrated%2520into%2520existing%2520OVS%250Apipelines%252C%2520e.g.%252C%2520improving%2520OVSeg%2520by%2520%252B6.0%2525%2520mIoU%2520on%2520ADE20K%2520for%2520open-vocabulary%25202D%250Asegmentation%252C%2520and%2520OpenMask3D%2520by%2520%252B4.1%2525%2520AP%2520on%2520ScanNet%252B%252B%2520Offices%2520for%250Aopen-vocabulary%25203D%2520instance%2520segmentation%2520without%2520other%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDAS%3A%20Open-Vocabulary%20Domain%20Adaptation%20for%20Segmentation&entry.906535625=Gonca%20Yilmaz%20and%20Songyou%20Peng%20and%20Marc%20Pollefeys%20and%20Francis%20Engelmann%20and%20Hermann%20Blum&entry.1292438233=%20%20Recently%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20advanced%20segmentation%20techniques%0Aby%20shifting%20from%20the%20traditional%20segmentation%20of%20a%20closed-set%20of%20predefined%0Aobject%20classes%20to%20open-vocabulary%20segmentation%20%28OVS%29%2C%20allowing%20users%20to%20segment%0Anovel%20classes%20and%20concepts%20unseen%20during%20training%20of%20the%20segmentation%20model.%0AHowever%2C%20this%20flexibility%20comes%20with%20a%20trade-off%3A%20fully-supervised%20closed-set%0Amethods%20still%20outperform%20OVS%20methods%20on%20base%20classes%2C%20that%20is%20on%20classes%20on%0Awhich%20they%20have%20been%20explicitly%20trained.%20This%20is%20due%20to%20the%20lack%20of%0Apixel-aligned%20training%20masks%20for%20VLMs%20%28which%20are%20trained%20on%20image-caption%0Apairs%29%2C%20and%20the%20absence%20of%20domain-specific%20knowledge%2C%20such%20as%20autonomous%0Adriving.%20Therefore%2C%20we%20propose%20the%20task%20of%20open-vocabulary%20domain%20adaptation%20to%0Ainfuse%20domain-specific%20knowledge%20into%20VLMs%20while%20preserving%20their%0Aopen-vocabulary%20nature.%20By%20doing%20so%2C%20we%20achieve%20improved%20performance%20in%20base%0Aand%20novel%20classes.%20Existing%20VLM%20adaptation%20methods%20improve%20performance%20on%20base%0A%28training%29%20queries%2C%20but%20fail%20to%20fully%20preserve%20the%20open-set%20capabilities%20of%0AVLMs%20on%20novel%20queries.%20To%20address%20this%20shortcoming%2C%20we%20combine%0Aparameter-efficient%20prompt%20tuning%20with%20a%20triplet-loss-based%20training%20strategy%0Athat%20uses%20auxiliary%20negative%20queries.%20Notably%2C%20our%20approach%20is%20the%20only%0Aparameter-efficient%20method%20that%20consistently%20surpasses%20the%20original%20VLM%20on%0Anovel%20classes.%20Our%20adapted%20VLMs%20can%20seamlessly%20be%20integrated%20into%20existing%20OVS%0Apipelines%2C%20e.g.%2C%20improving%20OVSeg%20by%20%2B6.0%25%20mIoU%20on%20ADE20K%20for%20open-vocabulary%202D%0Asegmentation%2C%20and%20OpenMask3D%20by%20%2B4.1%25%20AP%20on%20ScanNet%2B%2B%20Offices%20for%0Aopen-vocabulary%203D%20instance%20segmentation%20without%20other%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20141v2&entry.124074799=Read"},
{"title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting", "author": "Jiahao Lu and Yifan Zhang and Qiuhong Shen and Xinchao Wang and Shuicheng Yan", "abstract": "  3D Gaussian splatting (3DGS), known for its groundbreaking performance and\nefficiency, has become a dominant 3D representation and brought progress to\nmany 3D vision tasks. However, in this work, we reveal a significant security\nvulnerability that has been largely overlooked in 3DGS: the computation cost of\ntraining 3DGS could be maliciously tampered by poisoning the input data. By\ndeveloping an attack named Poison-splat, we reveal a novel attack surface where\nthe adversary can poison the input images to drastically increase the\ncomputation memory and time needed for 3DGS training, pushing the algorithm\ntowards its worst computation complexity. In extreme cases, the attack can even\nconsume all allocable memory, leading to a Denial-of-Service (DoS) that\ndisrupts servers, resulting in practical damages to real-world 3DGS service\nvendors. Such a computation cost attack is achieved by addressing a bi-level\noptimization problem through three tailored strategies: attack objective\napproximation, proxy model rendering, and optional constrained optimization.\nThese strategies not only ensure the effectiveness of our attack but also make\nit difficult to defend with simple defensive measures. We hope the revelation\nof this novel attack surface can spark attention to this crucial yet overlooked\nvulnerability of 3DGS systems.\n", "link": "http://arxiv.org/abs/2410.08190v1", "date": "2024-10-10", "relevancy": 2.9481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6104}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5845}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poison-splat%3A%20Computation%20Cost%20Attack%20on%203D%20Gaussian%20Splatting&body=Title%3A%20Poison-splat%3A%20Computation%20Cost%20Attack%20on%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jiahao%20Lu%20and%20Yifan%20Zhang%20and%20Qiuhong%20Shen%20and%20Xinchao%20Wang%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%2C%20known%20for%20its%20groundbreaking%20performance%20and%0Aefficiency%2C%20has%20become%20a%20dominant%203D%20representation%20and%20brought%20progress%20to%0Amany%203D%20vision%20tasks.%20However%2C%20in%20this%20work%2C%20we%20reveal%20a%20significant%20security%0Avulnerability%20that%20has%20been%20largely%20overlooked%20in%203DGS%3A%20the%20computation%20cost%20of%0Atraining%203DGS%20could%20be%20maliciously%20tampered%20by%20poisoning%20the%20input%20data.%20By%0Adeveloping%20an%20attack%20named%20Poison-splat%2C%20we%20reveal%20a%20novel%20attack%20surface%20where%0Athe%20adversary%20can%20poison%20the%20input%20images%20to%20drastically%20increase%20the%0Acomputation%20memory%20and%20time%20needed%20for%203DGS%20training%2C%20pushing%20the%20algorithm%0Atowards%20its%20worst%20computation%20complexity.%20In%20extreme%20cases%2C%20the%20attack%20can%20even%0Aconsume%20all%20allocable%20memory%2C%20leading%20to%20a%20Denial-of-Service%20%28DoS%29%20that%0Adisrupts%20servers%2C%20resulting%20in%20practical%20damages%20to%20real-world%203DGS%20service%0Avendors.%20Such%20a%20computation%20cost%20attack%20is%20achieved%20by%20addressing%20a%20bi-level%0Aoptimization%20problem%20through%20three%20tailored%20strategies%3A%20attack%20objective%0Aapproximation%2C%20proxy%20model%20rendering%2C%20and%20optional%20constrained%20optimization.%0AThese%20strategies%20not%20only%20ensure%20the%20effectiveness%20of%20our%20attack%20but%20also%20make%0Ait%20difficult%20to%20defend%20with%20simple%20defensive%20measures.%20We%20hope%20the%20revelation%0Aof%20this%20novel%20attack%20surface%20can%20spark%20attention%20to%20this%20crucial%20yet%20overlooked%0Avulnerability%20of%203DGS%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoison-splat%253A%2520Computation%2520Cost%2520Attack%2520on%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJiahao%2520Lu%2520and%2520Yifan%2520Zhang%2520and%2520Qiuhong%2520Shen%2520and%2520Xinchao%2520Wang%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%252C%2520known%2520for%2520its%2520groundbreaking%2520performance%2520and%250Aefficiency%252C%2520has%2520become%2520a%2520dominant%25203D%2520representation%2520and%2520brought%2520progress%2520to%250Amany%25203D%2520vision%2520tasks.%2520However%252C%2520in%2520this%2520work%252C%2520we%2520reveal%2520a%2520significant%2520security%250Avulnerability%2520that%2520has%2520been%2520largely%2520overlooked%2520in%25203DGS%253A%2520the%2520computation%2520cost%2520of%250Atraining%25203DGS%2520could%2520be%2520maliciously%2520tampered%2520by%2520poisoning%2520the%2520input%2520data.%2520By%250Adeveloping%2520an%2520attack%2520named%2520Poison-splat%252C%2520we%2520reveal%2520a%2520novel%2520attack%2520surface%2520where%250Athe%2520adversary%2520can%2520poison%2520the%2520input%2520images%2520to%2520drastically%2520increase%2520the%250Acomputation%2520memory%2520and%2520time%2520needed%2520for%25203DGS%2520training%252C%2520pushing%2520the%2520algorithm%250Atowards%2520its%2520worst%2520computation%2520complexity.%2520In%2520extreme%2520cases%252C%2520the%2520attack%2520can%2520even%250Aconsume%2520all%2520allocable%2520memory%252C%2520leading%2520to%2520a%2520Denial-of-Service%2520%2528DoS%2529%2520that%250Adisrupts%2520servers%252C%2520resulting%2520in%2520practical%2520damages%2520to%2520real-world%25203DGS%2520service%250Avendors.%2520Such%2520a%2520computation%2520cost%2520attack%2520is%2520achieved%2520by%2520addressing%2520a%2520bi-level%250Aoptimization%2520problem%2520through%2520three%2520tailored%2520strategies%253A%2520attack%2520objective%250Aapproximation%252C%2520proxy%2520model%2520rendering%252C%2520and%2520optional%2520constrained%2520optimization.%250AThese%2520strategies%2520not%2520only%2520ensure%2520the%2520effectiveness%2520of%2520our%2520attack%2520but%2520also%2520make%250Ait%2520difficult%2520to%2520defend%2520with%2520simple%2520defensive%2520measures.%2520We%2520hope%2520the%2520revelation%250Aof%2520this%2520novel%2520attack%2520surface%2520can%2520spark%2520attention%2520to%2520this%2520crucial%2520yet%2520overlooked%250Avulnerability%2520of%25203DGS%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poison-splat%3A%20Computation%20Cost%20Attack%20on%203D%20Gaussian%20Splatting&entry.906535625=Jiahao%20Lu%20and%20Yifan%20Zhang%20and%20Qiuhong%20Shen%20and%20Xinchao%20Wang%20and%20Shuicheng%20Yan&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%2C%20known%20for%20its%20groundbreaking%20performance%20and%0Aefficiency%2C%20has%20become%20a%20dominant%203D%20representation%20and%20brought%20progress%20to%0Amany%203D%20vision%20tasks.%20However%2C%20in%20this%20work%2C%20we%20reveal%20a%20significant%20security%0Avulnerability%20that%20has%20been%20largely%20overlooked%20in%203DGS%3A%20the%20computation%20cost%20of%0Atraining%203DGS%20could%20be%20maliciously%20tampered%20by%20poisoning%20the%20input%20data.%20By%0Adeveloping%20an%20attack%20named%20Poison-splat%2C%20we%20reveal%20a%20novel%20attack%20surface%20where%0Athe%20adversary%20can%20poison%20the%20input%20images%20to%20drastically%20increase%20the%0Acomputation%20memory%20and%20time%20needed%20for%203DGS%20training%2C%20pushing%20the%20algorithm%0Atowards%20its%20worst%20computation%20complexity.%20In%20extreme%20cases%2C%20the%20attack%20can%20even%0Aconsume%20all%20allocable%20memory%2C%20leading%20to%20a%20Denial-of-Service%20%28DoS%29%20that%0Adisrupts%20servers%2C%20resulting%20in%20practical%20damages%20to%20real-world%203DGS%20service%0Avendors.%20Such%20a%20computation%20cost%20attack%20is%20achieved%20by%20addressing%20a%20bi-level%0Aoptimization%20problem%20through%20three%20tailored%20strategies%3A%20attack%20objective%0Aapproximation%2C%20proxy%20model%20rendering%2C%20and%20optional%20constrained%20optimization.%0AThese%20strategies%20not%20only%20ensure%20the%20effectiveness%20of%20our%20attack%20but%20also%20make%0Ait%20difficult%20to%20defend%20with%20simple%20defensive%20measures.%20We%20hope%20the%20revelation%0Aof%20this%20novel%20attack%20surface%20can%20spark%20attention%20to%20this%20crucial%20yet%20overlooked%0Avulnerability%20of%203DGS%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08190v1&entry.124074799=Read"},
{"title": "OneRef: Unified One-tower Expression Grounding and Segmentation with\n  Mask Referring Modeling", "author": "Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu", "abstract": "  Constrained by the separate encoding of vision and language, existing\ngrounding and referring segmentation works heavily rely on bulky\nTransformer-based fusion en-/decoders and a variety of early-stage interaction\ntechnologies. Simultaneously, the current mask visual language modeling (MVLM)\nfails to capture the nuanced referential relationship between image-text in\nreferring tasks. In this paper, we propose OneRef, a minimalist referring\nframework built on the modality-shared one-tower transformer that unifies the\nvisual and linguistic feature spaces. To modeling the referential relationship,\nwe introduce a novel MVLM paradigm called Mask Referring Modeling (MRefM),\nwhich encompasses both referring-aware mask image modeling and referring-aware\nmask language modeling. Both modules not only reconstruct modality-related\ncontent but also cross-modal referring content. Within MRefM, we propose a\nreferring-aware dynamic image masking strategy that is aware of the referred\nregion rather than relying on fixed ratios or generic random masking schemes.\nBy leveraging the unified visual language feature space and incorporating\nMRefM's ability to model the referential relations, our approach enables direct\nregression of the referring results without resorting to various complex\ntechniques. Our method consistently surpasses existing approaches and achieves\nSoTA performance on both grounding and segmentation tasks, providing valuable\ninsights for future research. Our code and models are available at\nhttps://github.com/linhuixiao/OneRef.\n", "link": "http://arxiv.org/abs/2410.08021v1", "date": "2024-10-10", "relevancy": 2.8652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling&body=Title%3A%20OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling%0AAuthor%3A%20Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20Constrained%20by%20the%20separate%20encoding%20of%20vision%20and%20language%2C%20existing%0Agrounding%20and%20referring%20segmentation%20works%20heavily%20rely%20on%20bulky%0ATransformer-based%20fusion%20en-/decoders%20and%20a%20variety%20of%20early-stage%20interaction%0Atechnologies.%20Simultaneously%2C%20the%20current%20mask%20visual%20language%20modeling%20%28MVLM%29%0Afails%20to%20capture%20the%20nuanced%20referential%20relationship%20between%20image-text%20in%0Areferring%20tasks.%20In%20this%20paper%2C%20we%20propose%20OneRef%2C%20a%20minimalist%20referring%0Aframework%20built%20on%20the%20modality-shared%20one-tower%20transformer%20that%20unifies%20the%0Avisual%20and%20linguistic%20feature%20spaces.%20To%20modeling%20the%20referential%20relationship%2C%0Awe%20introduce%20a%20novel%20MVLM%20paradigm%20called%20Mask%20Referring%20Modeling%20%28MRefM%29%2C%0Awhich%20encompasses%20both%20referring-aware%20mask%20image%20modeling%20and%20referring-aware%0Amask%20language%20modeling.%20Both%20modules%20not%20only%20reconstruct%20modality-related%0Acontent%20but%20also%20cross-modal%20referring%20content.%20Within%20MRefM%2C%20we%20propose%20a%0Areferring-aware%20dynamic%20image%20masking%20strategy%20that%20is%20aware%20of%20the%20referred%0Aregion%20rather%20than%20relying%20on%20fixed%20ratios%20or%20generic%20random%20masking%20schemes.%0ABy%20leveraging%20the%20unified%20visual%20language%20feature%20space%20and%20incorporating%0AMRefM%27s%20ability%20to%20model%20the%20referential%20relations%2C%20our%20approach%20enables%20direct%0Aregression%20of%20the%20referring%20results%20without%20resorting%20to%20various%20complex%0Atechniques.%20Our%20method%20consistently%20surpasses%20existing%20approaches%20and%20achieves%0ASoTA%20performance%20on%20both%20grounding%20and%20segmentation%20tasks%2C%20providing%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/linhuixiao/OneRef.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneRef%253A%2520Unified%2520One-tower%2520Expression%2520Grounding%2520and%2520Segmentation%2520with%250A%2520%2520Mask%2520Referring%2520Modeling%26entry.906535625%3DLinhui%2520Xiao%2520and%2520Xiaoshan%2520Yang%2520and%2520Fang%2520Peng%2520and%2520Yaowei%2520Wang%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520Constrained%2520by%2520the%2520separate%2520encoding%2520of%2520vision%2520and%2520language%252C%2520existing%250Agrounding%2520and%2520referring%2520segmentation%2520works%2520heavily%2520rely%2520on%2520bulky%250ATransformer-based%2520fusion%2520en-/decoders%2520and%2520a%2520variety%2520of%2520early-stage%2520interaction%250Atechnologies.%2520Simultaneously%252C%2520the%2520current%2520mask%2520visual%2520language%2520modeling%2520%2528MVLM%2529%250Afails%2520to%2520capture%2520the%2520nuanced%2520referential%2520relationship%2520between%2520image-text%2520in%250Areferring%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OneRef%252C%2520a%2520minimalist%2520referring%250Aframework%2520built%2520on%2520the%2520modality-shared%2520one-tower%2520transformer%2520that%2520unifies%2520the%250Avisual%2520and%2520linguistic%2520feature%2520spaces.%2520To%2520modeling%2520the%2520referential%2520relationship%252C%250Awe%2520introduce%2520a%2520novel%2520MVLM%2520paradigm%2520called%2520Mask%2520Referring%2520Modeling%2520%2528MRefM%2529%252C%250Awhich%2520encompasses%2520both%2520referring-aware%2520mask%2520image%2520modeling%2520and%2520referring-aware%250Amask%2520language%2520modeling.%2520Both%2520modules%2520not%2520only%2520reconstruct%2520modality-related%250Acontent%2520but%2520also%2520cross-modal%2520referring%2520content.%2520Within%2520MRefM%252C%2520we%2520propose%2520a%250Areferring-aware%2520dynamic%2520image%2520masking%2520strategy%2520that%2520is%2520aware%2520of%2520the%2520referred%250Aregion%2520rather%2520than%2520relying%2520on%2520fixed%2520ratios%2520or%2520generic%2520random%2520masking%2520schemes.%250ABy%2520leveraging%2520the%2520unified%2520visual%2520language%2520feature%2520space%2520and%2520incorporating%250AMRefM%2527s%2520ability%2520to%2520model%2520the%2520referential%2520relations%252C%2520our%2520approach%2520enables%2520direct%250Aregression%2520of%2520the%2520referring%2520results%2520without%2520resorting%2520to%2520various%2520complex%250Atechniques.%2520Our%2520method%2520consistently%2520surpasses%2520existing%2520approaches%2520and%2520achieves%250ASoTA%2520performance%2520on%2520both%2520grounding%2520and%2520segmentation%2520tasks%252C%2520providing%2520valuable%250Ainsights%2520for%2520future%2520research.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/linhuixiao/OneRef.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling&entry.906535625=Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu&entry.1292438233=%20%20Constrained%20by%20the%20separate%20encoding%20of%20vision%20and%20language%2C%20existing%0Agrounding%20and%20referring%20segmentation%20works%20heavily%20rely%20on%20bulky%0ATransformer-based%20fusion%20en-/decoders%20and%20a%20variety%20of%20early-stage%20interaction%0Atechnologies.%20Simultaneously%2C%20the%20current%20mask%20visual%20language%20modeling%20%28MVLM%29%0Afails%20to%20capture%20the%20nuanced%20referential%20relationship%20between%20image-text%20in%0Areferring%20tasks.%20In%20this%20paper%2C%20we%20propose%20OneRef%2C%20a%20minimalist%20referring%0Aframework%20built%20on%20the%20modality-shared%20one-tower%20transformer%20that%20unifies%20the%0Avisual%20and%20linguistic%20feature%20spaces.%20To%20modeling%20the%20referential%20relationship%2C%0Awe%20introduce%20a%20novel%20MVLM%20paradigm%20called%20Mask%20Referring%20Modeling%20%28MRefM%29%2C%0Awhich%20encompasses%20both%20referring-aware%20mask%20image%20modeling%20and%20referring-aware%0Amask%20language%20modeling.%20Both%20modules%20not%20only%20reconstruct%20modality-related%0Acontent%20but%20also%20cross-modal%20referring%20content.%20Within%20MRefM%2C%20we%20propose%20a%0Areferring-aware%20dynamic%20image%20masking%20strategy%20that%20is%20aware%20of%20the%20referred%0Aregion%20rather%20than%20relying%20on%20fixed%20ratios%20or%20generic%20random%20masking%20schemes.%0ABy%20leveraging%20the%20unified%20visual%20language%20feature%20space%20and%20incorporating%0AMRefM%27s%20ability%20to%20model%20the%20referential%20relations%2C%20our%20approach%20enables%20direct%0Aregression%20of%20the%20referring%20results%20without%20resorting%20to%20various%20complex%0Atechniques.%20Our%20method%20consistently%20surpasses%20existing%20approaches%20and%20achieves%0ASoTA%20performance%20on%20both%20grounding%20and%20segmentation%20tasks%2C%20providing%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/linhuixiao/OneRef.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08021v1&entry.124074799=Read"},
{"title": "Interactive4D: Interactive 4D LiDAR Segmentation", "author": "Ilya Fradlin and Idil Esen Zulfikar and Kadir Yilmaz and Theodora Kontogianni and Bastian Leibe", "abstract": "  Interactive segmentation has an important role in facilitating the annotation\nprocess of future LiDAR datasets. Existing approaches sequentially segment\nindividual objects at each LiDAR scan, repeating the process throughout the\nentire sequence, which is redundant and ineffective. In this work, we propose\ninteractive 4D segmentation, a new paradigm that allows segmenting multiple\nobjects on multiple LiDAR scans simultaneously, and Interactive4D, the first\ninteractive 4D segmentation model that segments multiple objects on\nsuperimposed consecutive LiDAR scans in a single iteration by utilizing the\nsequential nature of LiDAR data. While performing interactive segmentation, our\nmodel leverages the entire space-time volume, leading to more efficient\nsegmentation. Operating on the 4D volume, it directly provides consistent\ninstance IDs over time and also simplifies tracking annotations. Moreover, we\nshow that click simulations are crucial for successful model training on LiDAR\npoint clouds. To this end, we design a click simulation strategy that is better\nsuited for the characteristics of LiDAR data. To demonstrate its accuracy and\neffectiveness, we evaluate Interactive4D on multiple LiDAR datasets, where\nInteractive4D achieves a new state-of-the-art by a large margin. Upon\nacceptance, we will publicly release the code and models at\nhttps://vision.rwth-aachen.de/Interactive4D.\n", "link": "http://arxiv.org/abs/2410.08206v1", "date": "2024-10-10", "relevancy": 2.8538, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive4D%3A%20Interactive%204D%20LiDAR%20Segmentation&body=Title%3A%20Interactive4D%3A%20Interactive%204D%20LiDAR%20Segmentation%0AAuthor%3A%20Ilya%20Fradlin%20and%20Idil%20Esen%20Zulfikar%20and%20Kadir%20Yilmaz%20and%20Theodora%20Kontogianni%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%20Interactive%20segmentation%20has%20an%20important%20role%20in%20facilitating%20the%20annotation%0Aprocess%20of%20future%20LiDAR%20datasets.%20Existing%20approaches%20sequentially%20segment%0Aindividual%20objects%20at%20each%20LiDAR%20scan%2C%20repeating%20the%20process%20throughout%20the%0Aentire%20sequence%2C%20which%20is%20redundant%20and%20ineffective.%20In%20this%20work%2C%20we%20propose%0Ainteractive%204D%20segmentation%2C%20a%20new%20paradigm%20that%20allows%20segmenting%20multiple%0Aobjects%20on%20multiple%20LiDAR%20scans%20simultaneously%2C%20and%20Interactive4D%2C%20the%20first%0Ainteractive%204D%20segmentation%20model%20that%20segments%20multiple%20objects%20on%0Asuperimposed%20consecutive%20LiDAR%20scans%20in%20a%20single%20iteration%20by%20utilizing%20the%0Asequential%20nature%20of%20LiDAR%20data.%20While%20performing%20interactive%20segmentation%2C%20our%0Amodel%20leverages%20the%20entire%20space-time%20volume%2C%20leading%20to%20more%20efficient%0Asegmentation.%20Operating%20on%20the%204D%20volume%2C%20it%20directly%20provides%20consistent%0Ainstance%20IDs%20over%20time%20and%20also%20simplifies%20tracking%20annotations.%20Moreover%2C%20we%0Ashow%20that%20click%20simulations%20are%20crucial%20for%20successful%20model%20training%20on%20LiDAR%0Apoint%20clouds.%20To%20this%20end%2C%20we%20design%20a%20click%20simulation%20strategy%20that%20is%20better%0Asuited%20for%20the%20characteristics%20of%20LiDAR%20data.%20To%20demonstrate%20its%20accuracy%20and%0Aeffectiveness%2C%20we%20evaluate%20Interactive4D%20on%20multiple%20LiDAR%20datasets%2C%20where%0AInteractive4D%20achieves%20a%20new%20state-of-the-art%20by%20a%20large%20margin.%20Upon%0Aacceptance%2C%20we%20will%20publicly%20release%20the%20code%20and%20models%20at%0Ahttps%3A//vision.rwth-aachen.de/Interactive4D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive4D%253A%2520Interactive%25204D%2520LiDAR%2520Segmentation%26entry.906535625%3DIlya%2520Fradlin%2520and%2520Idil%2520Esen%2520Zulfikar%2520and%2520Kadir%2520Yilmaz%2520and%2520Theodora%2520Kontogianni%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%2520Interactive%2520segmentation%2520has%2520an%2520important%2520role%2520in%2520facilitating%2520the%2520annotation%250Aprocess%2520of%2520future%2520LiDAR%2520datasets.%2520Existing%2520approaches%2520sequentially%2520segment%250Aindividual%2520objects%2520at%2520each%2520LiDAR%2520scan%252C%2520repeating%2520the%2520process%2520throughout%2520the%250Aentire%2520sequence%252C%2520which%2520is%2520redundant%2520and%2520ineffective.%2520In%2520this%2520work%252C%2520we%2520propose%250Ainteractive%25204D%2520segmentation%252C%2520a%2520new%2520paradigm%2520that%2520allows%2520segmenting%2520multiple%250Aobjects%2520on%2520multiple%2520LiDAR%2520scans%2520simultaneously%252C%2520and%2520Interactive4D%252C%2520the%2520first%250Ainteractive%25204D%2520segmentation%2520model%2520that%2520segments%2520multiple%2520objects%2520on%250Asuperimposed%2520consecutive%2520LiDAR%2520scans%2520in%2520a%2520single%2520iteration%2520by%2520utilizing%2520the%250Asequential%2520nature%2520of%2520LiDAR%2520data.%2520While%2520performing%2520interactive%2520segmentation%252C%2520our%250Amodel%2520leverages%2520the%2520entire%2520space-time%2520volume%252C%2520leading%2520to%2520more%2520efficient%250Asegmentation.%2520Operating%2520on%2520the%25204D%2520volume%252C%2520it%2520directly%2520provides%2520consistent%250Ainstance%2520IDs%2520over%2520time%2520and%2520also%2520simplifies%2520tracking%2520annotations.%2520Moreover%252C%2520we%250Ashow%2520that%2520click%2520simulations%2520are%2520crucial%2520for%2520successful%2520model%2520training%2520on%2520LiDAR%250Apoint%2520clouds.%2520To%2520this%2520end%252C%2520we%2520design%2520a%2520click%2520simulation%2520strategy%2520that%2520is%2520better%250Asuited%2520for%2520the%2520characteristics%2520of%2520LiDAR%2520data.%2520To%2520demonstrate%2520its%2520accuracy%2520and%250Aeffectiveness%252C%2520we%2520evaluate%2520Interactive4D%2520on%2520multiple%2520LiDAR%2520datasets%252C%2520where%250AInteractive4D%2520achieves%2520a%2520new%2520state-of-the-art%2520by%2520a%2520large%2520margin.%2520Upon%250Aacceptance%252C%2520we%2520will%2520publicly%2520release%2520the%2520code%2520and%2520models%2520at%250Ahttps%253A//vision.rwth-aachen.de/Interactive4D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive4D%3A%20Interactive%204D%20LiDAR%20Segmentation&entry.906535625=Ilya%20Fradlin%20and%20Idil%20Esen%20Zulfikar%20and%20Kadir%20Yilmaz%20and%20Theodora%20Kontogianni%20and%20Bastian%20Leibe&entry.1292438233=%20%20Interactive%20segmentation%20has%20an%20important%20role%20in%20facilitating%20the%20annotation%0Aprocess%20of%20future%20LiDAR%20datasets.%20Existing%20approaches%20sequentially%20segment%0Aindividual%20objects%20at%20each%20LiDAR%20scan%2C%20repeating%20the%20process%20throughout%20the%0Aentire%20sequence%2C%20which%20is%20redundant%20and%20ineffective.%20In%20this%20work%2C%20we%20propose%0Ainteractive%204D%20segmentation%2C%20a%20new%20paradigm%20that%20allows%20segmenting%20multiple%0Aobjects%20on%20multiple%20LiDAR%20scans%20simultaneously%2C%20and%20Interactive4D%2C%20the%20first%0Ainteractive%204D%20segmentation%20model%20that%20segments%20multiple%20objects%20on%0Asuperimposed%20consecutive%20LiDAR%20scans%20in%20a%20single%20iteration%20by%20utilizing%20the%0Asequential%20nature%20of%20LiDAR%20data.%20While%20performing%20interactive%20segmentation%2C%20our%0Amodel%20leverages%20the%20entire%20space-time%20volume%2C%20leading%20to%20more%20efficient%0Asegmentation.%20Operating%20on%20the%204D%20volume%2C%20it%20directly%20provides%20consistent%0Ainstance%20IDs%20over%20time%20and%20also%20simplifies%20tracking%20annotations.%20Moreover%2C%20we%0Ashow%20that%20click%20simulations%20are%20crucial%20for%20successful%20model%20training%20on%20LiDAR%0Apoint%20clouds.%20To%20this%20end%2C%20we%20design%20a%20click%20simulation%20strategy%20that%20is%20better%0Asuited%20for%20the%20characteristics%20of%20LiDAR%20data.%20To%20demonstrate%20its%20accuracy%20and%0Aeffectiveness%2C%20we%20evaluate%20Interactive4D%20on%20multiple%20LiDAR%20datasets%2C%20where%0AInteractive4D%20achieves%20a%20new%20state-of-the-art%20by%20a%20large%20margin.%20Upon%0Aacceptance%2C%20we%20will%20publicly%20release%20the%20code%20and%20models%20at%0Ahttps%3A//vision.rwth-aachen.de/Interactive4D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08206v1&entry.124074799=Read"},
{"title": "UW-SDF: Exploiting Hybrid Geometric Priors for Neural SDF Reconstruction\n  from Underwater Multi-view Monocular Images", "author": "Zeyu Chen and Jingyi Tang and Gu Wang and Shengquan Li and Xinghui Li and Xiangyang Ji and Xiu Li", "abstract": "  Due to the unique characteristics of underwater environments, accurate 3D\nreconstruction of underwater objects poses a challenging problem in tasks such\nas underwater exploration and mapping. Traditional methods that rely on\nmultiple sensor data for 3D reconstruction are time-consuming and face\nchallenges in data acquisition in underwater scenarios. We propose UW-SDF, a\nframework for reconstructing target objects from multi-view underwater images\nbased on neural SDF. We introduce hybrid geometric priors to optimize the\nreconstruction process, markedly enhancing the quality and efficiency of neural\nSDF reconstruction. Additionally, to address the challenge of segmentation\nconsistency in multi-view images, we propose a novel few-shot multi-view target\nsegmentation strategy using the general-purpose segmentation model (SAM),\nenabling rapid automatic segmentation of unseen objects. Through extensive\nqualitative and quantitative experiments on diverse datasets, we demonstrate\nthat our proposed method outperforms the traditional underwater 3D\nreconstruction method and other neural rendering approaches in the field of\nunderwater 3D reconstruction.\n", "link": "http://arxiv.org/abs/2410.08092v1", "date": "2024-10-10", "relevancy": 2.8533, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5804}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5681}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UW-SDF%3A%20Exploiting%20Hybrid%20Geometric%20Priors%20for%20Neural%20SDF%20Reconstruction%0A%20%20from%20Underwater%20Multi-view%20Monocular%20Images&body=Title%3A%20UW-SDF%3A%20Exploiting%20Hybrid%20Geometric%20Priors%20for%20Neural%20SDF%20Reconstruction%0A%20%20from%20Underwater%20Multi-view%20Monocular%20Images%0AAuthor%3A%20Zeyu%20Chen%20and%20Jingyi%20Tang%20and%20Gu%20Wang%20and%20Shengquan%20Li%20and%20Xinghui%20Li%20and%20Xiangyang%20Ji%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Due%20to%20the%20unique%20characteristics%20of%20underwater%20environments%2C%20accurate%203D%0Areconstruction%20of%20underwater%20objects%20poses%20a%20challenging%20problem%20in%20tasks%20such%0Aas%20underwater%20exploration%20and%20mapping.%20Traditional%20methods%20that%20rely%20on%0Amultiple%20sensor%20data%20for%203D%20reconstruction%20are%20time-consuming%20and%20face%0Achallenges%20in%20data%20acquisition%20in%20underwater%20scenarios.%20We%20propose%20UW-SDF%2C%20a%0Aframework%20for%20reconstructing%20target%20objects%20from%20multi-view%20underwater%20images%0Abased%20on%20neural%20SDF.%20We%20introduce%20hybrid%20geometric%20priors%20to%20optimize%20the%0Areconstruction%20process%2C%20markedly%20enhancing%20the%20quality%20and%20efficiency%20of%20neural%0ASDF%20reconstruction.%20Additionally%2C%20to%20address%20the%20challenge%20of%20segmentation%0Aconsistency%20in%20multi-view%20images%2C%20we%20propose%20a%20novel%20few-shot%20multi-view%20target%0Asegmentation%20strategy%20using%20the%20general-purpose%20segmentation%20model%20%28SAM%29%2C%0Aenabling%20rapid%20automatic%20segmentation%20of%20unseen%20objects.%20Through%20extensive%0Aqualitative%20and%20quantitative%20experiments%20on%20diverse%20datasets%2C%20we%20demonstrate%0Athat%20our%20proposed%20method%20outperforms%20the%20traditional%20underwater%203D%0Areconstruction%20method%20and%20other%20neural%20rendering%20approaches%20in%20the%20field%20of%0Aunderwater%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUW-SDF%253A%2520Exploiting%2520Hybrid%2520Geometric%2520Priors%2520for%2520Neural%2520SDF%2520Reconstruction%250A%2520%2520from%2520Underwater%2520Multi-view%2520Monocular%2520Images%26entry.906535625%3DZeyu%2520Chen%2520and%2520Jingyi%2520Tang%2520and%2520Gu%2520Wang%2520and%2520Shengquan%2520Li%2520and%2520Xinghui%2520Li%2520and%2520Xiangyang%2520Ji%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520unique%2520characteristics%2520of%2520underwater%2520environments%252C%2520accurate%25203D%250Areconstruction%2520of%2520underwater%2520objects%2520poses%2520a%2520challenging%2520problem%2520in%2520tasks%2520such%250Aas%2520underwater%2520exploration%2520and%2520mapping.%2520Traditional%2520methods%2520that%2520rely%2520on%250Amultiple%2520sensor%2520data%2520for%25203D%2520reconstruction%2520are%2520time-consuming%2520and%2520face%250Achallenges%2520in%2520data%2520acquisition%2520in%2520underwater%2520scenarios.%2520We%2520propose%2520UW-SDF%252C%2520a%250Aframework%2520for%2520reconstructing%2520target%2520objects%2520from%2520multi-view%2520underwater%2520images%250Abased%2520on%2520neural%2520SDF.%2520We%2520introduce%2520hybrid%2520geometric%2520priors%2520to%2520optimize%2520the%250Areconstruction%2520process%252C%2520markedly%2520enhancing%2520the%2520quality%2520and%2520efficiency%2520of%2520neural%250ASDF%2520reconstruction.%2520Additionally%252C%2520to%2520address%2520the%2520challenge%2520of%2520segmentation%250Aconsistency%2520in%2520multi-view%2520images%252C%2520we%2520propose%2520a%2520novel%2520few-shot%2520multi-view%2520target%250Asegmentation%2520strategy%2520using%2520the%2520general-purpose%2520segmentation%2520model%2520%2528SAM%2529%252C%250Aenabling%2520rapid%2520automatic%2520segmentation%2520of%2520unseen%2520objects.%2520Through%2520extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520on%2520diverse%2520datasets%252C%2520we%2520demonstrate%250Athat%2520our%2520proposed%2520method%2520outperforms%2520the%2520traditional%2520underwater%25203D%250Areconstruction%2520method%2520and%2520other%2520neural%2520rendering%2520approaches%2520in%2520the%2520field%2520of%250Aunderwater%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UW-SDF%3A%20Exploiting%20Hybrid%20Geometric%20Priors%20for%20Neural%20SDF%20Reconstruction%0A%20%20from%20Underwater%20Multi-view%20Monocular%20Images&entry.906535625=Zeyu%20Chen%20and%20Jingyi%20Tang%20and%20Gu%20Wang%20and%20Shengquan%20Li%20and%20Xinghui%20Li%20and%20Xiangyang%20Ji%20and%20Xiu%20Li&entry.1292438233=%20%20Due%20to%20the%20unique%20characteristics%20of%20underwater%20environments%2C%20accurate%203D%0Areconstruction%20of%20underwater%20objects%20poses%20a%20challenging%20problem%20in%20tasks%20such%0Aas%20underwater%20exploration%20and%20mapping.%20Traditional%20methods%20that%20rely%20on%0Amultiple%20sensor%20data%20for%203D%20reconstruction%20are%20time-consuming%20and%20face%0Achallenges%20in%20data%20acquisition%20in%20underwater%20scenarios.%20We%20propose%20UW-SDF%2C%20a%0Aframework%20for%20reconstructing%20target%20objects%20from%20multi-view%20underwater%20images%0Abased%20on%20neural%20SDF.%20We%20introduce%20hybrid%20geometric%20priors%20to%20optimize%20the%0Areconstruction%20process%2C%20markedly%20enhancing%20the%20quality%20and%20efficiency%20of%20neural%0ASDF%20reconstruction.%20Additionally%2C%20to%20address%20the%20challenge%20of%20segmentation%0Aconsistency%20in%20multi-view%20images%2C%20we%20propose%20a%20novel%20few-shot%20multi-view%20target%0Asegmentation%20strategy%20using%20the%20general-purpose%20segmentation%20model%20%28SAM%29%2C%0Aenabling%20rapid%20automatic%20segmentation%20of%20unseen%20objects.%20Through%20extensive%0Aqualitative%20and%20quantitative%20experiments%20on%20diverse%20datasets%2C%20we%20demonstrate%0Athat%20our%20proposed%20method%20outperforms%20the%20traditional%20underwater%203D%0Areconstruction%20method%20and%20other%20neural%20rendering%20approaches%20in%20the%20field%20of%0Aunderwater%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08092v1&entry.124074799=Read"},
{"title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal\n  Models", "author": "Wenbo Hu and Jia-Chen Gu and Zi-Yi Dou and Mohsen Fayyaz and Pan Lu and Kai-Wei Chang and Nanyun Peng", "abstract": "  Existing multimodal retrieval benchmarks primarily focus on evaluating\nwhether models can retrieve and utilize external textual knowledge for question\nanswering. However, there are scenarios where retrieving visual information is\neither more beneficial or easier to access than textual data. In this paper, we\nintroduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in\nwhich we systematically identify and categorize scenarios where visually\naugmented knowledge is better than textual knowledge, for instance, more images\nfrom varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353\nhuman-annotated multiple-choice questions across 9 distinct scenarios. With\nMRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large\nvision-language models (LVLMs). Our results show that all LVLMs exhibit greater\nimprovements when augmented with images compared to textual knowledge,\nconfirming that MRAG-Bench is vision-centric. Additionally, we conduct\nextensive analysis with MRAG-Bench, which offers valuable insights into\nretrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces\nchallenges in effectively leveraging retrieved knowledge, achieving only a\n5.82% improvement with ground-truth information, in contrast to a 33.16%\nimprovement observed in human participants. These findings highlight the\nimportance of MRAG-Bench in encouraging the community to enhance LVLMs' ability\nto utilize retrieved visual knowledge more effectively.\n", "link": "http://arxiv.org/abs/2410.08182v1", "date": "2024-10-10", "relevancy": 2.8474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5766}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRAG-Bench%3A%20Vision-Centric%20Evaluation%20for%20Retrieval-Augmented%20Multimodal%0A%20%20Models&body=Title%3A%20MRAG-Bench%3A%20Vision-Centric%20Evaluation%20for%20Retrieval-Augmented%20Multimodal%0A%20%20Models%0AAuthor%3A%20Wenbo%20Hu%20and%20Jia-Chen%20Gu%20and%20Zi-Yi%20Dou%20and%20Mohsen%20Fayyaz%20and%20Pan%20Lu%20and%20Kai-Wei%20Chang%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20Existing%20multimodal%20retrieval%20benchmarks%20primarily%20focus%20on%20evaluating%0Awhether%20models%20can%20retrieve%20and%20utilize%20external%20textual%20knowledge%20for%20question%0Aanswering.%20However%2C%20there%20are%20scenarios%20where%20retrieving%20visual%20information%20is%0Aeither%20more%20beneficial%20or%20easier%20to%20access%20than%20textual%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20multimodal%20retrieval-augmented%20generation%20benchmark%2C%20MRAG-Bench%2C%20in%0Awhich%20we%20systematically%20identify%20and%20categorize%20scenarios%20where%20visually%0Aaugmented%20knowledge%20is%20better%20than%20textual%20knowledge%2C%20for%20instance%2C%20more%20images%0Afrom%20varying%20viewpoints.%20MRAG-Bench%20consists%20of%2016%2C130%20images%20and%201%2C353%0Ahuman-annotated%20multiple-choice%20questions%20across%209%20distinct%20scenarios.%20With%0AMRAG-Bench%2C%20we%20conduct%20an%20evaluation%20of%2010%20open-source%20and%204%20proprietary%20large%0Avision-language%20models%20%28LVLMs%29.%20Our%20results%20show%20that%20all%20LVLMs%20exhibit%20greater%0Aimprovements%20when%20augmented%20with%20images%20compared%20to%20textual%20knowledge%2C%0Aconfirming%20that%20MRAG-Bench%20is%20vision-centric.%20Additionally%2C%20we%20conduct%0Aextensive%20analysis%20with%20MRAG-Bench%2C%20which%20offers%20valuable%20insights%20into%0Aretrieval-augmented%20LVLMs.%20Notably%2C%20the%20top-performing%20model%2C%20GPT-4o%2C%20faces%0Achallenges%20in%20effectively%20leveraging%20retrieved%20knowledge%2C%20achieving%20only%20a%0A5.82%25%20improvement%20with%20ground-truth%20information%2C%20in%20contrast%20to%20a%2033.16%25%0Aimprovement%20observed%20in%20human%20participants.%20These%20findings%20highlight%20the%0Aimportance%20of%20MRAG-Bench%20in%20encouraging%20the%20community%20to%20enhance%20LVLMs%27%20ability%0Ato%20utilize%20retrieved%20visual%20knowledge%20more%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRAG-Bench%253A%2520Vision-Centric%2520Evaluation%2520for%2520Retrieval-Augmented%2520Multimodal%250A%2520%2520Models%26entry.906535625%3DWenbo%2520Hu%2520and%2520Jia-Chen%2520Gu%2520and%2520Zi-Yi%2520Dou%2520and%2520Mohsen%2520Fayyaz%2520and%2520Pan%2520Lu%2520and%2520Kai-Wei%2520Chang%2520and%2520Nanyun%2520Peng%26entry.1292438233%3D%2520%2520Existing%2520multimodal%2520retrieval%2520benchmarks%2520primarily%2520focus%2520on%2520evaluating%250Awhether%2520models%2520can%2520retrieve%2520and%2520utilize%2520external%2520textual%2520knowledge%2520for%2520question%250Aanswering.%2520However%252C%2520there%2520are%2520scenarios%2520where%2520retrieving%2520visual%2520information%2520is%250Aeither%2520more%2520beneficial%2520or%2520easier%2520to%2520access%2520than%2520textual%2520data.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520multimodal%2520retrieval-augmented%2520generation%2520benchmark%252C%2520MRAG-Bench%252C%2520in%250Awhich%2520we%2520systematically%2520identify%2520and%2520categorize%2520scenarios%2520where%2520visually%250Aaugmented%2520knowledge%2520is%2520better%2520than%2520textual%2520knowledge%252C%2520for%2520instance%252C%2520more%2520images%250Afrom%2520varying%2520viewpoints.%2520MRAG-Bench%2520consists%2520of%252016%252C130%2520images%2520and%25201%252C353%250Ahuman-annotated%2520multiple-choice%2520questions%2520across%25209%2520distinct%2520scenarios.%2520With%250AMRAG-Bench%252C%2520we%2520conduct%2520an%2520evaluation%2520of%252010%2520open-source%2520and%25204%2520proprietary%2520large%250Avision-language%2520models%2520%2528LVLMs%2529.%2520Our%2520results%2520show%2520that%2520all%2520LVLMs%2520exhibit%2520greater%250Aimprovements%2520when%2520augmented%2520with%2520images%2520compared%2520to%2520textual%2520knowledge%252C%250Aconfirming%2520that%2520MRAG-Bench%2520is%2520vision-centric.%2520Additionally%252C%2520we%2520conduct%250Aextensive%2520analysis%2520with%2520MRAG-Bench%252C%2520which%2520offers%2520valuable%2520insights%2520into%250Aretrieval-augmented%2520LVLMs.%2520Notably%252C%2520the%2520top-performing%2520model%252C%2520GPT-4o%252C%2520faces%250Achallenges%2520in%2520effectively%2520leveraging%2520retrieved%2520knowledge%252C%2520achieving%2520only%2520a%250A5.82%2525%2520improvement%2520with%2520ground-truth%2520information%252C%2520in%2520contrast%2520to%2520a%252033.16%2525%250Aimprovement%2520observed%2520in%2520human%2520participants.%2520These%2520findings%2520highlight%2520the%250Aimportance%2520of%2520MRAG-Bench%2520in%2520encouraging%2520the%2520community%2520to%2520enhance%2520LVLMs%2527%2520ability%250Ato%2520utilize%2520retrieved%2520visual%2520knowledge%2520more%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRAG-Bench%3A%20Vision-Centric%20Evaluation%20for%20Retrieval-Augmented%20Multimodal%0A%20%20Models&entry.906535625=Wenbo%20Hu%20and%20Jia-Chen%20Gu%20and%20Zi-Yi%20Dou%20and%20Mohsen%20Fayyaz%20and%20Pan%20Lu%20and%20Kai-Wei%20Chang%20and%20Nanyun%20Peng&entry.1292438233=%20%20Existing%20multimodal%20retrieval%20benchmarks%20primarily%20focus%20on%20evaluating%0Awhether%20models%20can%20retrieve%20and%20utilize%20external%20textual%20knowledge%20for%20question%0Aanswering.%20However%2C%20there%20are%20scenarios%20where%20retrieving%20visual%20information%20is%0Aeither%20more%20beneficial%20or%20easier%20to%20access%20than%20textual%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20multimodal%20retrieval-augmented%20generation%20benchmark%2C%20MRAG-Bench%2C%20in%0Awhich%20we%20systematically%20identify%20and%20categorize%20scenarios%20where%20visually%0Aaugmented%20knowledge%20is%20better%20than%20textual%20knowledge%2C%20for%20instance%2C%20more%20images%0Afrom%20varying%20viewpoints.%20MRAG-Bench%20consists%20of%2016%2C130%20images%20and%201%2C353%0Ahuman-annotated%20multiple-choice%20questions%20across%209%20distinct%20scenarios.%20With%0AMRAG-Bench%2C%20we%20conduct%20an%20evaluation%20of%2010%20open-source%20and%204%20proprietary%20large%0Avision-language%20models%20%28LVLMs%29.%20Our%20results%20show%20that%20all%20LVLMs%20exhibit%20greater%0Aimprovements%20when%20augmented%20with%20images%20compared%20to%20textual%20knowledge%2C%0Aconfirming%20that%20MRAG-Bench%20is%20vision-centric.%20Additionally%2C%20we%20conduct%0Aextensive%20analysis%20with%20MRAG-Bench%2C%20which%20offers%20valuable%20insights%20into%0Aretrieval-augmented%20LVLMs.%20Notably%2C%20the%20top-performing%20model%2C%20GPT-4o%2C%20faces%0Achallenges%20in%20effectively%20leveraging%20retrieved%20knowledge%2C%20achieving%20only%20a%0A5.82%25%20improvement%20with%20ground-truth%20information%2C%20in%20contrast%20to%20a%2033.16%25%0Aimprovement%20observed%20in%20human%20participants.%20These%20findings%20highlight%20the%0Aimportance%20of%20MRAG-Bench%20in%20encouraging%20the%20community%20to%20enhance%20LVLMs%27%20ability%0Ato%20utilize%20retrieved%20visual%20knowledge%20more%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08182v1&entry.124074799=Read"},
{"title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images", "author": "Sina Malakouti and Aysan Aghazadeh and Ashmit Khandelwal and Adriana Kovashka", "abstract": "  Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.\n", "link": "http://arxiv.org/abs/2409.10719v2", "date": "2024-10-10", "relevancy": 2.8173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20VLMs%27%20Reasoning%20About%20Persuasive%20Atypical%20Images&body=Title%3A%20Benchmarking%20VLMs%27%20Reasoning%20About%20Persuasive%20Atypical%20Images%0AAuthor%3A%20Sina%20Malakouti%20and%20Aysan%20Aghazadeh%20and%20Ashmit%20Khandelwal%20and%20Adriana%20Kovashka%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20have%20shown%20strong%20zero-shot%20generalization%0Aacross%20various%20tasks%2C%20especially%20when%20integrated%20with%20large%20language%20models%0A%28LLMs%29.%20However%2C%20their%20ability%20to%20comprehend%20rhetorical%20and%20persuasive%20visual%0Amedia%2C%20such%20as%20advertisements%2C%20remains%20understudied.%20Ads%20often%20employ%20atypical%0Aimagery%2C%20using%20surprising%20object%20juxtapositions%20to%20convey%20shared%20properties.%0AFor%20example%2C%20Fig.%201%20%28e%29%20shows%20a%20beer%20with%20a%20feather-like%20texture.%20This%20requires%0Aadvanced%20reasoning%20to%20deduce%20that%20this%20atypical%20representation%20signifies%20the%0Abeer%27s%20lightness.%20We%20introduce%20three%20novel%20tasks%2C%20Multi-label%20Atypicality%0AClassification%2C%20Atypicality%20Statement%20Retrieval%2C%20and%20Aypical%20Object%0ARecognition%2C%20to%20benchmark%20VLMs%27%20understanding%20of%20atypicality%20in%20persuasive%0Aimages.%20We%20evaluate%20how%20well%20VLMs%20use%20atypicality%20to%20infer%20an%20ad%27s%20message%20and%0Atest%20their%20reasoning%20abilities%20by%20employing%20semantically%20challenging%20negatives.%0AFinally%2C%20we%20pioneer%20atypicality-aware%20verbalization%20by%20extracting%20comprehensive%0Aimage%20descriptions%20sensitive%20to%20atypical%20elements.%20Our%20findings%20reveal%20that%3A%0A%281%29%20VLMs%20lack%20advanced%20reasoning%20capabilities%20compared%20to%20LLMs%3B%20%282%29%20simple%2C%0Aeffective%20strategies%20can%20extract%20atypicality-aware%20information%2C%20leading%20to%0Acomprehensive%20image%20verbalization%3B%20%283%29%20atypicality%20aids%20persuasive%0Aadvertisement%20understanding.%20Code%20and%20data%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520VLMs%2527%2520Reasoning%2520About%2520Persuasive%2520Atypical%2520Images%26entry.906535625%3DSina%2520Malakouti%2520and%2520Aysan%2520Aghazadeh%2520and%2520Ashmit%2520Khandelwal%2520and%2520Adriana%2520Kovashka%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520zero-shot%2520generalization%250Aacross%2520various%2520tasks%252C%2520especially%2520when%2520integrated%2520with%2520large%2520language%2520models%250A%2528LLMs%2529.%2520However%252C%2520their%2520ability%2520to%2520comprehend%2520rhetorical%2520and%2520persuasive%2520visual%250Amedia%252C%2520such%2520as%2520advertisements%252C%2520remains%2520understudied.%2520Ads%2520often%2520employ%2520atypical%250Aimagery%252C%2520using%2520surprising%2520object%2520juxtapositions%2520to%2520convey%2520shared%2520properties.%250AFor%2520example%252C%2520Fig.%25201%2520%2528e%2529%2520shows%2520a%2520beer%2520with%2520a%2520feather-like%2520texture.%2520This%2520requires%250Aadvanced%2520reasoning%2520to%2520deduce%2520that%2520this%2520atypical%2520representation%2520signifies%2520the%250Abeer%2527s%2520lightness.%2520We%2520introduce%2520three%2520novel%2520tasks%252C%2520Multi-label%2520Atypicality%250AClassification%252C%2520Atypicality%2520Statement%2520Retrieval%252C%2520and%2520Aypical%2520Object%250ARecognition%252C%2520to%2520benchmark%2520VLMs%2527%2520understanding%2520of%2520atypicality%2520in%2520persuasive%250Aimages.%2520We%2520evaluate%2520how%2520well%2520VLMs%2520use%2520atypicality%2520to%2520infer%2520an%2520ad%2527s%2520message%2520and%250Atest%2520their%2520reasoning%2520abilities%2520by%2520employing%2520semantically%2520challenging%2520negatives.%250AFinally%252C%2520we%2520pioneer%2520atypicality-aware%2520verbalization%2520by%2520extracting%2520comprehensive%250Aimage%2520descriptions%2520sensitive%2520to%2520atypical%2520elements.%2520Our%2520findings%2520reveal%2520that%253A%250A%25281%2529%2520VLMs%2520lack%2520advanced%2520reasoning%2520capabilities%2520compared%2520to%2520LLMs%253B%2520%25282%2529%2520simple%252C%250Aeffective%2520strategies%2520can%2520extract%2520atypicality-aware%2520information%252C%2520leading%2520to%250Acomprehensive%2520image%2520verbalization%253B%2520%25283%2529%2520atypicality%2520aids%2520persuasive%250Aadvertisement%2520understanding.%2520Code%2520and%2520data%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20VLMs%27%20Reasoning%20About%20Persuasive%20Atypical%20Images&entry.906535625=Sina%20Malakouti%20and%20Aysan%20Aghazadeh%20and%20Ashmit%20Khandelwal%20and%20Adriana%20Kovashka&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20have%20shown%20strong%20zero-shot%20generalization%0Aacross%20various%20tasks%2C%20especially%20when%20integrated%20with%20large%20language%20models%0A%28LLMs%29.%20However%2C%20their%20ability%20to%20comprehend%20rhetorical%20and%20persuasive%20visual%0Amedia%2C%20such%20as%20advertisements%2C%20remains%20understudied.%20Ads%20often%20employ%20atypical%0Aimagery%2C%20using%20surprising%20object%20juxtapositions%20to%20convey%20shared%20properties.%0AFor%20example%2C%20Fig.%201%20%28e%29%20shows%20a%20beer%20with%20a%20feather-like%20texture.%20This%20requires%0Aadvanced%20reasoning%20to%20deduce%20that%20this%20atypical%20representation%20signifies%20the%0Abeer%27s%20lightness.%20We%20introduce%20three%20novel%20tasks%2C%20Multi-label%20Atypicality%0AClassification%2C%20Atypicality%20Statement%20Retrieval%2C%20and%20Aypical%20Object%0ARecognition%2C%20to%20benchmark%20VLMs%27%20understanding%20of%20atypicality%20in%20persuasive%0Aimages.%20We%20evaluate%20how%20well%20VLMs%20use%20atypicality%20to%20infer%20an%20ad%27s%20message%20and%0Atest%20their%20reasoning%20abilities%20by%20employing%20semantically%20challenging%20negatives.%0AFinally%2C%20we%20pioneer%20atypicality-aware%20verbalization%20by%20extracting%20comprehensive%0Aimage%20descriptions%20sensitive%20to%20atypical%20elements.%20Our%20findings%20reveal%20that%3A%0A%281%29%20VLMs%20lack%20advanced%20reasoning%20capabilities%20compared%20to%20LLMs%3B%20%282%29%20simple%2C%0Aeffective%20strategies%20can%20extract%20atypicality-aware%20information%2C%20leading%20to%0Acomprehensive%20image%20verbalization%3B%20%283%29%20atypicality%20aids%20persuasive%0Aadvertisement%20understanding.%20Code%20and%20data%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10719v2&entry.124074799=Read"},
{"title": "MolMix: A Simple Yet Effective Baseline for Multimodal Molecular\n  Representation Learning", "author": "Andrei Manolache and Dragos Tantaru and Mathias Niepert", "abstract": "  In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.\n", "link": "http://arxiv.org/abs/2410.07981v1", "date": "2024-10-10", "relevancy": 2.8073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolMix%3A%20A%20Simple%20Yet%20Effective%20Baseline%20for%20Multimodal%20Molecular%0A%20%20Representation%20Learning&body=Title%3A%20MolMix%3A%20A%20Simple%20Yet%20Effective%20Baseline%20for%20Multimodal%20Molecular%0A%20%20Representation%20Learning%0AAuthor%3A%20Andrei%20Manolache%20and%20Dragos%20Tantaru%20and%20Mathias%20Niepert%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20simple%20transformer-based%20baseline%20for%20multimodal%0Amolecular%20representation%20learning%2C%20integrating%20three%20distinct%20modalities%3A%0ASMILES%20strings%2C%202D%20graph%20representations%2C%20and%203D%20conformers%20of%20molecules.%20A%20key%0Aaspect%20of%20our%20approach%20is%20the%20aggregation%20of%203D%20conformers%2C%20allowing%20the%20model%0Ato%20account%20for%20the%20fact%20that%20molecules%20can%20adopt%20multiple%20conformations-an%0Aimportant%20factor%20for%20accurate%20molecular%20representation.%20The%20tokens%20for%20each%0Amodality%20are%20extracted%20using%20modality-specific%20encoders%3A%20a%20transformer%20for%0ASMILES%20strings%2C%20a%20message-passing%20neural%20network%20for%202D%20graphs%2C%20and%20an%0Aequivariant%20neural%20network%20for%203D%20conformers.%20The%20flexibility%20and%20modularity%20of%0Athis%20framework%20enable%20easy%20adaptation%20and%20replacement%20of%20these%20encoders%2C%20making%0Athe%20model%20highly%20versatile%20for%20different%20molecular%20tasks.%20The%20extracted%20tokens%0Aare%20then%20combined%20into%20a%20unified%20multimodal%20sequence%2C%20which%20is%20processed%20by%20a%0Adownstream%20transformer%20for%20prediction%20tasks.%20To%20efficiently%20scale%20our%20model%20for%0Alarge%20multimodal%20datasets%2C%20we%20utilize%20Flash%20Attention%202%20and%20bfloat16%20precision.%0ADespite%20its%20simplicity%2C%20our%20approach%20achieves%20state-of-the-art%20results%20across%0Amultiple%20datasets%2C%20demonstrating%20its%20effectiveness%20as%20a%20strong%20baseline%20for%0Amultimodal%20molecular%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolMix%253A%2520A%2520Simple%2520Yet%2520Effective%2520Baseline%2520for%2520Multimodal%2520Molecular%250A%2520%2520Representation%2520Learning%26entry.906535625%3DAndrei%2520Manolache%2520and%2520Dragos%2520Tantaru%2520and%2520Mathias%2520Niepert%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520transformer-based%2520baseline%2520for%2520multimodal%250Amolecular%2520representation%2520learning%252C%2520integrating%2520three%2520distinct%2520modalities%253A%250ASMILES%2520strings%252C%25202D%2520graph%2520representations%252C%2520and%25203D%2520conformers%2520of%2520molecules.%2520A%2520key%250Aaspect%2520of%2520our%2520approach%2520is%2520the%2520aggregation%2520of%25203D%2520conformers%252C%2520allowing%2520the%2520model%250Ato%2520account%2520for%2520the%2520fact%2520that%2520molecules%2520can%2520adopt%2520multiple%2520conformations-an%250Aimportant%2520factor%2520for%2520accurate%2520molecular%2520representation.%2520The%2520tokens%2520for%2520each%250Amodality%2520are%2520extracted%2520using%2520modality-specific%2520encoders%253A%2520a%2520transformer%2520for%250ASMILES%2520strings%252C%2520a%2520message-passing%2520neural%2520network%2520for%25202D%2520graphs%252C%2520and%2520an%250Aequivariant%2520neural%2520network%2520for%25203D%2520conformers.%2520The%2520flexibility%2520and%2520modularity%2520of%250Athis%2520framework%2520enable%2520easy%2520adaptation%2520and%2520replacement%2520of%2520these%2520encoders%252C%2520making%250Athe%2520model%2520highly%2520versatile%2520for%2520different%2520molecular%2520tasks.%2520The%2520extracted%2520tokens%250Aare%2520then%2520combined%2520into%2520a%2520unified%2520multimodal%2520sequence%252C%2520which%2520is%2520processed%2520by%2520a%250Adownstream%2520transformer%2520for%2520prediction%2520tasks.%2520To%2520efficiently%2520scale%2520our%2520model%2520for%250Alarge%2520multimodal%2520datasets%252C%2520we%2520utilize%2520Flash%2520Attention%25202%2520and%2520bfloat16%2520precision.%250ADespite%2520its%2520simplicity%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520across%250Amultiple%2520datasets%252C%2520demonstrating%2520its%2520effectiveness%2520as%2520a%2520strong%2520baseline%2520for%250Amultimodal%2520molecular%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolMix%3A%20A%20Simple%20Yet%20Effective%20Baseline%20for%20Multimodal%20Molecular%0A%20%20Representation%20Learning&entry.906535625=Andrei%20Manolache%20and%20Dragos%20Tantaru%20and%20Mathias%20Niepert&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20simple%20transformer-based%20baseline%20for%20multimodal%0Amolecular%20representation%20learning%2C%20integrating%20three%20distinct%20modalities%3A%0ASMILES%20strings%2C%202D%20graph%20representations%2C%20and%203D%20conformers%20of%20molecules.%20A%20key%0Aaspect%20of%20our%20approach%20is%20the%20aggregation%20of%203D%20conformers%2C%20allowing%20the%20model%0Ato%20account%20for%20the%20fact%20that%20molecules%20can%20adopt%20multiple%20conformations-an%0Aimportant%20factor%20for%20accurate%20molecular%20representation.%20The%20tokens%20for%20each%0Amodality%20are%20extracted%20using%20modality-specific%20encoders%3A%20a%20transformer%20for%0ASMILES%20strings%2C%20a%20message-passing%20neural%20network%20for%202D%20graphs%2C%20and%20an%0Aequivariant%20neural%20network%20for%203D%20conformers.%20The%20flexibility%20and%20modularity%20of%0Athis%20framework%20enable%20easy%20adaptation%20and%20replacement%20of%20these%20encoders%2C%20making%0Athe%20model%20highly%20versatile%20for%20different%20molecular%20tasks.%20The%20extracted%20tokens%0Aare%20then%20combined%20into%20a%20unified%20multimodal%20sequence%2C%20which%20is%20processed%20by%20a%0Adownstream%20transformer%20for%20prediction%20tasks.%20To%20efficiently%20scale%20our%20model%20for%0Alarge%20multimodal%20datasets%2C%20we%20utilize%20Flash%20Attention%202%20and%20bfloat16%20precision.%0ADespite%20its%20simplicity%2C%20our%20approach%20achieves%20state-of-the-art%20results%20across%0Amultiple%20datasets%2C%20demonstrating%20its%20effectiveness%20as%20a%20strong%20baseline%20for%0Amultimodal%20molecular%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07981v1&entry.124074799=Read"},
{"title": "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human\n  with Animatable Garments", "author": "Yifan Zhan and Qingtian Zhu and Muyao Niu and Mingze Ma and Jiancheng Zhao and Zhihang Zhong and Xiao Sun and Yu Qiao and Yinqiang Zheng", "abstract": "  In this paper, we highlight a critical yet often overlooked factor in most 3D\nhuman tasks, namely modeling humans with complex garments. It is known that the\nparameterized formulation of SMPL is able to fit human skin; while complex\ngarments, e.g., hand-held objects and loose-fitting garments, are difficult to\nget modeled within the unified framework, since their movements are usually\ndecoupled with the human body. To enhance the capability of SMPL skeleton in\nresponse to this situation, we propose a modular growth strategy that enables\nthe joint tree of the skeleton to expand adaptively. Specifically, our method,\ncalled ToMiE, consists of parent joints localization and external joints\noptimization. For parent joints localization, we employ a gradient-based\napproach guided by both LBS blending weights and motion kernels. Once the\nexternal joints are obtained, we proceed to optimize their transformations in\nSE(3) across different frames, enabling rendering and explicit animation. ToMiE\nmanages to outperform other methods across various cases with garments, not\nonly in rendering quality but also by offering free animation of grown joints,\nthereby enhancing the expressive ability of SMPL skeleton for a broader range\nof applications.\n", "link": "http://arxiv.org/abs/2410.08082v1", "date": "2024-10-10", "relevancy": 2.7622, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5882}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToMiE%3A%20Towards%20Modular%20Growth%20in%20Enhanced%20SMPL%20Skeleton%20for%203D%20Human%0A%20%20with%20Animatable%20Garments&body=Title%3A%20ToMiE%3A%20Towards%20Modular%20Growth%20in%20Enhanced%20SMPL%20Skeleton%20for%203D%20Human%0A%20%20with%20Animatable%20Garments%0AAuthor%3A%20Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Muyao%20Niu%20and%20Mingze%20Ma%20and%20Jiancheng%20Zhao%20and%20Zhihang%20Zhong%20and%20Xiao%20Sun%20and%20Yu%20Qiao%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20highlight%20a%20critical%20yet%20often%20overlooked%20factor%20in%20most%203D%0Ahuman%20tasks%2C%20namely%20modeling%20humans%20with%20complex%20garments.%20It%20is%20known%20that%20the%0Aparameterized%20formulation%20of%20SMPL%20is%20able%20to%20fit%20human%20skin%3B%20while%20complex%0Agarments%2C%20e.g.%2C%20hand-held%20objects%20and%20loose-fitting%20garments%2C%20are%20difficult%20to%0Aget%20modeled%20within%20the%20unified%20framework%2C%20since%20their%20movements%20are%20usually%0Adecoupled%20with%20the%20human%20body.%20To%20enhance%20the%20capability%20of%20SMPL%20skeleton%20in%0Aresponse%20to%20this%20situation%2C%20we%20propose%20a%20modular%20growth%20strategy%20that%20enables%0Athe%20joint%20tree%20of%20the%20skeleton%20to%20expand%20adaptively.%20Specifically%2C%20our%20method%2C%0Acalled%20ToMiE%2C%20consists%20of%20parent%20joints%20localization%20and%20external%20joints%0Aoptimization.%20For%20parent%20joints%20localization%2C%20we%20employ%20a%20gradient-based%0Aapproach%20guided%20by%20both%20LBS%20blending%20weights%20and%20motion%20kernels.%20Once%20the%0Aexternal%20joints%20are%20obtained%2C%20we%20proceed%20to%20optimize%20their%20transformations%20in%0ASE%283%29%20across%20different%20frames%2C%20enabling%20rendering%20and%20explicit%20animation.%20ToMiE%0Amanages%20to%20outperform%20other%20methods%20across%20various%20cases%20with%20garments%2C%20not%0Aonly%20in%20rendering%20quality%20but%20also%20by%20offering%20free%20animation%20of%20grown%20joints%2C%0Athereby%20enhancing%20the%20expressive%20ability%20of%20SMPL%20skeleton%20for%20a%20broader%20range%0Aof%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToMiE%253A%2520Towards%2520Modular%2520Growth%2520in%2520Enhanced%2520SMPL%2520Skeleton%2520for%25203D%2520Human%250A%2520%2520with%2520Animatable%2520Garments%26entry.906535625%3DYifan%2520Zhan%2520and%2520Qingtian%2520Zhu%2520and%2520Muyao%2520Niu%2520and%2520Mingze%2520Ma%2520and%2520Jiancheng%2520Zhao%2520and%2520Zhihang%2520Zhong%2520and%2520Xiao%2520Sun%2520and%2520Yu%2520Qiao%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520highlight%2520a%2520critical%2520yet%2520often%2520overlooked%2520factor%2520in%2520most%25203D%250Ahuman%2520tasks%252C%2520namely%2520modeling%2520humans%2520with%2520complex%2520garments.%2520It%2520is%2520known%2520that%2520the%250Aparameterized%2520formulation%2520of%2520SMPL%2520is%2520able%2520to%2520fit%2520human%2520skin%253B%2520while%2520complex%250Agarments%252C%2520e.g.%252C%2520hand-held%2520objects%2520and%2520loose-fitting%2520garments%252C%2520are%2520difficult%2520to%250Aget%2520modeled%2520within%2520the%2520unified%2520framework%252C%2520since%2520their%2520movements%2520are%2520usually%250Adecoupled%2520with%2520the%2520human%2520body.%2520To%2520enhance%2520the%2520capability%2520of%2520SMPL%2520skeleton%2520in%250Aresponse%2520to%2520this%2520situation%252C%2520we%2520propose%2520a%2520modular%2520growth%2520strategy%2520that%2520enables%250Athe%2520joint%2520tree%2520of%2520the%2520skeleton%2520to%2520expand%2520adaptively.%2520Specifically%252C%2520our%2520method%252C%250Acalled%2520ToMiE%252C%2520consists%2520of%2520parent%2520joints%2520localization%2520and%2520external%2520joints%250Aoptimization.%2520For%2520parent%2520joints%2520localization%252C%2520we%2520employ%2520a%2520gradient-based%250Aapproach%2520guided%2520by%2520both%2520LBS%2520blending%2520weights%2520and%2520motion%2520kernels.%2520Once%2520the%250Aexternal%2520joints%2520are%2520obtained%252C%2520we%2520proceed%2520to%2520optimize%2520their%2520transformations%2520in%250ASE%25283%2529%2520across%2520different%2520frames%252C%2520enabling%2520rendering%2520and%2520explicit%2520animation.%2520ToMiE%250Amanages%2520to%2520outperform%2520other%2520methods%2520across%2520various%2520cases%2520with%2520garments%252C%2520not%250Aonly%2520in%2520rendering%2520quality%2520but%2520also%2520by%2520offering%2520free%2520animation%2520of%2520grown%2520joints%252C%250Athereby%2520enhancing%2520the%2520expressive%2520ability%2520of%2520SMPL%2520skeleton%2520for%2520a%2520broader%2520range%250Aof%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToMiE%3A%20Towards%20Modular%20Growth%20in%20Enhanced%20SMPL%20Skeleton%20for%203D%20Human%0A%20%20with%20Animatable%20Garments&entry.906535625=Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Muyao%20Niu%20and%20Mingze%20Ma%20and%20Jiancheng%20Zhao%20and%20Zhihang%20Zhong%20and%20Xiao%20Sun%20and%20Yu%20Qiao%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20highlight%20a%20critical%20yet%20often%20overlooked%20factor%20in%20most%203D%0Ahuman%20tasks%2C%20namely%20modeling%20humans%20with%20complex%20garments.%20It%20is%20known%20that%20the%0Aparameterized%20formulation%20of%20SMPL%20is%20able%20to%20fit%20human%20skin%3B%20while%20complex%0Agarments%2C%20e.g.%2C%20hand-held%20objects%20and%20loose-fitting%20garments%2C%20are%20difficult%20to%0Aget%20modeled%20within%20the%20unified%20framework%2C%20since%20their%20movements%20are%20usually%0Adecoupled%20with%20the%20human%20body.%20To%20enhance%20the%20capability%20of%20SMPL%20skeleton%20in%0Aresponse%20to%20this%20situation%2C%20we%20propose%20a%20modular%20growth%20strategy%20that%20enables%0Athe%20joint%20tree%20of%20the%20skeleton%20to%20expand%20adaptively.%20Specifically%2C%20our%20method%2C%0Acalled%20ToMiE%2C%20consists%20of%20parent%20joints%20localization%20and%20external%20joints%0Aoptimization.%20For%20parent%20joints%20localization%2C%20we%20employ%20a%20gradient-based%0Aapproach%20guided%20by%20both%20LBS%20blending%20weights%20and%20motion%20kernels.%20Once%20the%0Aexternal%20joints%20are%20obtained%2C%20we%20proceed%20to%20optimize%20their%20transformations%20in%0ASE%283%29%20across%20different%20frames%2C%20enabling%20rendering%20and%20explicit%20animation.%20ToMiE%0Amanages%20to%20outperform%20other%20methods%20across%20various%20cases%20with%20garments%2C%20not%0Aonly%20in%20rendering%20quality%20but%20also%20by%20offering%20free%20animation%20of%20grown%20joints%2C%0Athereby%20enhancing%20the%20expressive%20ability%20of%20SMPL%20skeleton%20for%20a%20broader%20range%0Aof%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08082v1&entry.124074799=Read"},
{"title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "author": "Kate Sanders and Nathaniel Weir and Benjamin Van Durme", "abstract": "  It is challenging for models to understand complex, multimodal content such\nas television clips, and this is in part because video-language models often\nrely on single-modality reasoning and lack interpretability. To combat these\nissues we propose TV-TREES, the first multimodal entailment tree generator.\nTV-TREES serves as an approach to video understanding that promotes\ninterpretable joint-modality reasoning by searching for trees of entailment\nrelationships between simple text-video evidence and higher-level conclusions\nthat prove question-answer pairs. We also introduce the task of multimodal\nentailment tree generation to evaluate reasoning quality. Our method's\nperformance on the challenging TVQA benchmark demonstrates interpretable,\nstate-of-the-art zero-shot performance on full clips, illustrating that\nmultimodal entailment tree generation can be a best-of-both-worlds alternative\nto black-box systems.\n", "link": "http://arxiv.org/abs/2402.19467v4", "date": "2024-10-10", "relevancy": 2.7611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV-TREES%3A%20Multimodal%20Entailment%20Trees%20for%20Neuro-Symbolic%20Video%20Reasoning&body=Title%3A%20TV-TREES%3A%20Multimodal%20Entailment%20Trees%20for%20Neuro-Symbolic%20Video%20Reasoning%0AAuthor%3A%20Kate%20Sanders%20and%20Nathaniel%20Weir%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20It%20is%20challenging%20for%20models%20to%20understand%20complex%2C%20multimodal%20content%20such%0Aas%20television%20clips%2C%20and%20this%20is%20in%20part%20because%20video-language%20models%20often%0Arely%20on%20single-modality%20reasoning%20and%20lack%20interpretability.%20To%20combat%20these%0Aissues%20we%20propose%20TV-TREES%2C%20the%20first%20multimodal%20entailment%20tree%20generator.%0ATV-TREES%20serves%20as%20an%20approach%20to%20video%20understanding%20that%20promotes%0Ainterpretable%20joint-modality%20reasoning%20by%20searching%20for%20trees%20of%20entailment%0Arelationships%20between%20simple%20text-video%20evidence%20and%20higher-level%20conclusions%0Athat%20prove%20question-answer%20pairs.%20We%20also%20introduce%20the%20task%20of%20multimodal%0Aentailment%20tree%20generation%20to%20evaluate%20reasoning%20quality.%20Our%20method%27s%0Aperformance%20on%20the%20challenging%20TVQA%20benchmark%20demonstrates%20interpretable%2C%0Astate-of-the-art%20zero-shot%20performance%20on%20full%20clips%2C%20illustrating%20that%0Amultimodal%20entailment%20tree%20generation%20can%20be%20a%20best-of-both-worlds%20alternative%0Ato%20black-box%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19467v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV-TREES%253A%2520Multimodal%2520Entailment%2520Trees%2520for%2520Neuro-Symbolic%2520Video%2520Reasoning%26entry.906535625%3DKate%2520Sanders%2520and%2520Nathaniel%2520Weir%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520It%2520is%2520challenging%2520for%2520models%2520to%2520understand%2520complex%252C%2520multimodal%2520content%2520such%250Aas%2520television%2520clips%252C%2520and%2520this%2520is%2520in%2520part%2520because%2520video-language%2520models%2520often%250Arely%2520on%2520single-modality%2520reasoning%2520and%2520lack%2520interpretability.%2520To%2520combat%2520these%250Aissues%2520we%2520propose%2520TV-TREES%252C%2520the%2520first%2520multimodal%2520entailment%2520tree%2520generator.%250ATV-TREES%2520serves%2520as%2520an%2520approach%2520to%2520video%2520understanding%2520that%2520promotes%250Ainterpretable%2520joint-modality%2520reasoning%2520by%2520searching%2520for%2520trees%2520of%2520entailment%250Arelationships%2520between%2520simple%2520text-video%2520evidence%2520and%2520higher-level%2520conclusions%250Athat%2520prove%2520question-answer%2520pairs.%2520We%2520also%2520introduce%2520the%2520task%2520of%2520multimodal%250Aentailment%2520tree%2520generation%2520to%2520evaluate%2520reasoning%2520quality.%2520Our%2520method%2527s%250Aperformance%2520on%2520the%2520challenging%2520TVQA%2520benchmark%2520demonstrates%2520interpretable%252C%250Astate-of-the-art%2520zero-shot%2520performance%2520on%2520full%2520clips%252C%2520illustrating%2520that%250Amultimodal%2520entailment%2520tree%2520generation%2520can%2520be%2520a%2520best-of-both-worlds%2520alternative%250Ato%2520black-box%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19467v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV-TREES%3A%20Multimodal%20Entailment%20Trees%20for%20Neuro-Symbolic%20Video%20Reasoning&entry.906535625=Kate%20Sanders%20and%20Nathaniel%20Weir%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20It%20is%20challenging%20for%20models%20to%20understand%20complex%2C%20multimodal%20content%20such%0Aas%20television%20clips%2C%20and%20this%20is%20in%20part%20because%20video-language%20models%20often%0Arely%20on%20single-modality%20reasoning%20and%20lack%20interpretability.%20To%20combat%20these%0Aissues%20we%20propose%20TV-TREES%2C%20the%20first%20multimodal%20entailment%20tree%20generator.%0ATV-TREES%20serves%20as%20an%20approach%20to%20video%20understanding%20that%20promotes%0Ainterpretable%20joint-modality%20reasoning%20by%20searching%20for%20trees%20of%20entailment%0Arelationships%20between%20simple%20text-video%20evidence%20and%20higher-level%20conclusions%0Athat%20prove%20question-answer%20pairs.%20We%20also%20introduce%20the%20task%20of%20multimodal%0Aentailment%20tree%20generation%20to%20evaluate%20reasoning%20quality.%20Our%20method%27s%0Aperformance%20on%20the%20challenging%20TVQA%20benchmark%20demonstrates%20interpretable%2C%0Astate-of-the-art%20zero-shot%20performance%20on%20full%20clips%2C%20illustrating%20that%0Amultimodal%20entailment%20tree%20generation%20can%20be%20a%20best-of-both-worlds%20alternative%0Ato%20black-box%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19467v4&entry.124074799=Read"},
{"title": "A Lightweight Target-Driven Network of Stereo Matching for Inland\n  Waterways", "author": "Jing Su and Yiqing Zhou and Yu Zhang and Chao Wang and Yi Wei", "abstract": "  Stereo matching for inland waterways is one of the key technologies for the\nautonomous navigation of Unmanned Surface Vehicles (USVs), which involves\ndividing the stereo images into reference images and target images for\npixel-level matching. However, due to the challenges of the inland waterway\nenvironment, such as blurred textures, large spatial scales, and computational\nresource constraints of the USVs platform, the participation of geometric\nfeatures from the target image is required for efficient target-driven\nmatching. Based on this target-driven concept, we propose a lightweight\ntarget-driven stereo matching neural network, named LTNet. Specifically, a\nlightweight and efficient 4D cost volume, named the Geometry Target Volume\n(GTV), is designed to fully utilize the geometric information of target\nfeatures by employing the shifted target features as the filtered feature\nvolume. Subsequently, to address the substantial texture interference and\nobject occlusions present in the waterway environment, a Left-Right Consistency\nRefinement (LRR) module is proposed. The \\text{LRR} utilizes the pixel-level\ndifferences in left and right disparities to introduce soft constraints,\nthereby enhancing the accuracy of predictions during the intermediate stages of\nthe network. Moreover, knowledge distillation is utilized to enhance the\ngeneralization capability of lightweight models on the USVInland dataset.\nFurthermore, a new large-scale benchmark, named Spring, is utilized to validate\nthe applicability of LTNet across various scenarios. In experiments on the\naforementioned two datasets, LTNet achieves competitive results, with only 3.7M\nparameters. The code is available at https://github.com/Open-YiQingZhou/LTNet .\n", "link": "http://arxiv.org/abs/2410.07915v1", "date": "2024-10-10", "relevancy": 2.7559, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5426}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Target-Driven%20Network%20of%20Stereo%20Matching%20for%20Inland%0A%20%20Waterways&body=Title%3A%20A%20Lightweight%20Target-Driven%20Network%20of%20Stereo%20Matching%20for%20Inland%0A%20%20Waterways%0AAuthor%3A%20Jing%20Su%20and%20Yiqing%20Zhou%20and%20Yu%20Zhang%20and%20Chao%20Wang%20and%20Yi%20Wei%0AAbstract%3A%20%20%20Stereo%20matching%20for%20inland%20waterways%20is%20one%20of%20the%20key%20technologies%20for%20the%0Aautonomous%20navigation%20of%20Unmanned%20Surface%20Vehicles%20%28USVs%29%2C%20which%20involves%0Adividing%20the%20stereo%20images%20into%20reference%20images%20and%20target%20images%20for%0Apixel-level%20matching.%20However%2C%20due%20to%20the%20challenges%20of%20the%20inland%20waterway%0Aenvironment%2C%20such%20as%20blurred%20textures%2C%20large%20spatial%20scales%2C%20and%20computational%0Aresource%20constraints%20of%20the%20USVs%20platform%2C%20the%20participation%20of%20geometric%0Afeatures%20from%20the%20target%20image%20is%20required%20for%20efficient%20target-driven%0Amatching.%20Based%20on%20this%20target-driven%20concept%2C%20we%20propose%20a%20lightweight%0Atarget-driven%20stereo%20matching%20neural%20network%2C%20named%20LTNet.%20Specifically%2C%20a%0Alightweight%20and%20efficient%204D%20cost%20volume%2C%20named%20the%20Geometry%20Target%20Volume%0A%28GTV%29%2C%20is%20designed%20to%20fully%20utilize%20the%20geometric%20information%20of%20target%0Afeatures%20by%20employing%20the%20shifted%20target%20features%20as%20the%20filtered%20feature%0Avolume.%20Subsequently%2C%20to%20address%20the%20substantial%20texture%20interference%20and%0Aobject%20occlusions%20present%20in%20the%20waterway%20environment%2C%20a%20Left-Right%20Consistency%0ARefinement%20%28LRR%29%20module%20is%20proposed.%20The%20%5Ctext%7BLRR%7D%20utilizes%20the%20pixel-level%0Adifferences%20in%20left%20and%20right%20disparities%20to%20introduce%20soft%20constraints%2C%0Athereby%20enhancing%20the%20accuracy%20of%20predictions%20during%20the%20intermediate%20stages%20of%0Athe%20network.%20Moreover%2C%20knowledge%20distillation%20is%20utilized%20to%20enhance%20the%0Ageneralization%20capability%20of%20lightweight%20models%20on%20the%20USVInland%20dataset.%0AFurthermore%2C%20a%20new%20large-scale%20benchmark%2C%20named%20Spring%2C%20is%20utilized%20to%20validate%0Athe%20applicability%20of%20LTNet%20across%20various%20scenarios.%20In%20experiments%20on%20the%0Aaforementioned%20two%20datasets%2C%20LTNet%20achieves%20competitive%20results%2C%20with%20only%203.7M%0Aparameters.%20The%20code%20is%20available%20at%20https%3A//github.com/Open-YiQingZhou/LTNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Target-Driven%2520Network%2520of%2520Stereo%2520Matching%2520for%2520Inland%250A%2520%2520Waterways%26entry.906535625%3DJing%2520Su%2520and%2520Yiqing%2520Zhou%2520and%2520Yu%2520Zhang%2520and%2520Chao%2520Wang%2520and%2520Yi%2520Wei%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520for%2520inland%2520waterways%2520is%2520one%2520of%2520the%2520key%2520technologies%2520for%2520the%250Aautonomous%2520navigation%2520of%2520Unmanned%2520Surface%2520Vehicles%2520%2528USVs%2529%252C%2520which%2520involves%250Adividing%2520the%2520stereo%2520images%2520into%2520reference%2520images%2520and%2520target%2520images%2520for%250Apixel-level%2520matching.%2520However%252C%2520due%2520to%2520the%2520challenges%2520of%2520the%2520inland%2520waterway%250Aenvironment%252C%2520such%2520as%2520blurred%2520textures%252C%2520large%2520spatial%2520scales%252C%2520and%2520computational%250Aresource%2520constraints%2520of%2520the%2520USVs%2520platform%252C%2520the%2520participation%2520of%2520geometric%250Afeatures%2520from%2520the%2520target%2520image%2520is%2520required%2520for%2520efficient%2520target-driven%250Amatching.%2520Based%2520on%2520this%2520target-driven%2520concept%252C%2520we%2520propose%2520a%2520lightweight%250Atarget-driven%2520stereo%2520matching%2520neural%2520network%252C%2520named%2520LTNet.%2520Specifically%252C%2520a%250Alightweight%2520and%2520efficient%25204D%2520cost%2520volume%252C%2520named%2520the%2520Geometry%2520Target%2520Volume%250A%2528GTV%2529%252C%2520is%2520designed%2520to%2520fully%2520utilize%2520the%2520geometric%2520information%2520of%2520target%250Afeatures%2520by%2520employing%2520the%2520shifted%2520target%2520features%2520as%2520the%2520filtered%2520feature%250Avolume.%2520Subsequently%252C%2520to%2520address%2520the%2520substantial%2520texture%2520interference%2520and%250Aobject%2520occlusions%2520present%2520in%2520the%2520waterway%2520environment%252C%2520a%2520Left-Right%2520Consistency%250ARefinement%2520%2528LRR%2529%2520module%2520is%2520proposed.%2520The%2520%255Ctext%257BLRR%257D%2520utilizes%2520the%2520pixel-level%250Adifferences%2520in%2520left%2520and%2520right%2520disparities%2520to%2520introduce%2520soft%2520constraints%252C%250Athereby%2520enhancing%2520the%2520accuracy%2520of%2520predictions%2520during%2520the%2520intermediate%2520stages%2520of%250Athe%2520network.%2520Moreover%252C%2520knowledge%2520distillation%2520is%2520utilized%2520to%2520enhance%2520the%250Ageneralization%2520capability%2520of%2520lightweight%2520models%2520on%2520the%2520USVInland%2520dataset.%250AFurthermore%252C%2520a%2520new%2520large-scale%2520benchmark%252C%2520named%2520Spring%252C%2520is%2520utilized%2520to%2520validate%250Athe%2520applicability%2520of%2520LTNet%2520across%2520various%2520scenarios.%2520In%2520experiments%2520on%2520the%250Aaforementioned%2520two%2520datasets%252C%2520LTNet%2520achieves%2520competitive%2520results%252C%2520with%2520only%25203.7M%250Aparameters.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Open-YiQingZhou/LTNet%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Target-Driven%20Network%20of%20Stereo%20Matching%20for%20Inland%0A%20%20Waterways&entry.906535625=Jing%20Su%20and%20Yiqing%20Zhou%20and%20Yu%20Zhang%20and%20Chao%20Wang%20and%20Yi%20Wei&entry.1292438233=%20%20Stereo%20matching%20for%20inland%20waterways%20is%20one%20of%20the%20key%20technologies%20for%20the%0Aautonomous%20navigation%20of%20Unmanned%20Surface%20Vehicles%20%28USVs%29%2C%20which%20involves%0Adividing%20the%20stereo%20images%20into%20reference%20images%20and%20target%20images%20for%0Apixel-level%20matching.%20However%2C%20due%20to%20the%20challenges%20of%20the%20inland%20waterway%0Aenvironment%2C%20such%20as%20blurred%20textures%2C%20large%20spatial%20scales%2C%20and%20computational%0Aresource%20constraints%20of%20the%20USVs%20platform%2C%20the%20participation%20of%20geometric%0Afeatures%20from%20the%20target%20image%20is%20required%20for%20efficient%20target-driven%0Amatching.%20Based%20on%20this%20target-driven%20concept%2C%20we%20propose%20a%20lightweight%0Atarget-driven%20stereo%20matching%20neural%20network%2C%20named%20LTNet.%20Specifically%2C%20a%0Alightweight%20and%20efficient%204D%20cost%20volume%2C%20named%20the%20Geometry%20Target%20Volume%0A%28GTV%29%2C%20is%20designed%20to%20fully%20utilize%20the%20geometric%20information%20of%20target%0Afeatures%20by%20employing%20the%20shifted%20target%20features%20as%20the%20filtered%20feature%0Avolume.%20Subsequently%2C%20to%20address%20the%20substantial%20texture%20interference%20and%0Aobject%20occlusions%20present%20in%20the%20waterway%20environment%2C%20a%20Left-Right%20Consistency%0ARefinement%20%28LRR%29%20module%20is%20proposed.%20The%20%5Ctext%7BLRR%7D%20utilizes%20the%20pixel-level%0Adifferences%20in%20left%20and%20right%20disparities%20to%20introduce%20soft%20constraints%2C%0Athereby%20enhancing%20the%20accuracy%20of%20predictions%20during%20the%20intermediate%20stages%20of%0Athe%20network.%20Moreover%2C%20knowledge%20distillation%20is%20utilized%20to%20enhance%20the%0Ageneralization%20capability%20of%20lightweight%20models%20on%20the%20USVInland%20dataset.%0AFurthermore%2C%20a%20new%20large-scale%20benchmark%2C%20named%20Spring%2C%20is%20utilized%20to%20validate%0Athe%20applicability%20of%20LTNet%20across%20various%20scenarios.%20In%20experiments%20on%20the%0Aaforementioned%20two%20datasets%2C%20LTNet%20achieves%20competitive%20results%2C%20with%20only%203.7M%0Aparameters.%20The%20code%20is%20available%20at%20https%3A//github.com/Open-YiQingZhou/LTNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07915v1&entry.124074799=Read"},
{"title": "Distribution Guidance Network for Weakly Supervised Point Cloud Semantic\n  Segmentation", "author": "Zhiyi Pan and Wei Gao and Shan Liu and Ge Li", "abstract": "  Despite alleviating the dependence on dense annotations inherent to fully\nsupervised methods, weakly supervised point cloud semantic segmentation suffers\nfrom inadequate supervision signals. In response to this challenge, we\nintroduce a novel perspective that imparts auxiliary constraints by regulating\nthe feature space under weak supervision. Our initial investigation identifies\nwhich distributions accurately characterize the feature space, subsequently\nleveraging this priori to guide the alignment of the weakly supervised\nembeddings. Specifically, we analyze the superiority of the mixture of von\nMises-Fisher distributions (moVMF) among several common distribution\ncandidates. Accordingly, we develop a Distribution Guidance Network (DGNet),\nwhich comprises a weakly supervised learning branch and a distribution\nalignment branch. Leveraging reliable clustering initialization derived from\nthe weakly supervised learning branch, the distribution alignment branch\nalternately updates the parameters of the moVMF and the network, ensuring\nalignment with the moVMF-defined latent space. Extensive experiments validate\nthe rationality and effectiveness of our distribution choice and network\ndesign. Consequently, DGNet achieves state-of-the-art performance under\nmultiple datasets and various weakly supervised settings.\n", "link": "http://arxiv.org/abs/2410.08091v1", "date": "2024-10-10", "relevancy": 2.7409, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5686}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5391}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution%20Guidance%20Network%20for%20Weakly%20Supervised%20Point%20Cloud%20Semantic%0A%20%20Segmentation&body=Title%3A%20Distribution%20Guidance%20Network%20for%20Weakly%20Supervised%20Point%20Cloud%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Zhiyi%20Pan%20and%20Wei%20Gao%20and%20Shan%20Liu%20and%20Ge%20Li%0AAbstract%3A%20%20%20Despite%20alleviating%20the%20dependence%20on%20dense%20annotations%20inherent%20to%20fully%0Asupervised%20methods%2C%20weakly%20supervised%20point%20cloud%20semantic%20segmentation%20suffers%0Afrom%20inadequate%20supervision%20signals.%20In%20response%20to%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20perspective%20that%20imparts%20auxiliary%20constraints%20by%20regulating%0Athe%20feature%20space%20under%20weak%20supervision.%20Our%20initial%20investigation%20identifies%0Awhich%20distributions%20accurately%20characterize%20the%20feature%20space%2C%20subsequently%0Aleveraging%20this%20priori%20to%20guide%20the%20alignment%20of%20the%20weakly%20supervised%0Aembeddings.%20Specifically%2C%20we%20analyze%20the%20superiority%20of%20the%20mixture%20of%20von%0AMises-Fisher%20distributions%20%28moVMF%29%20among%20several%20common%20distribution%0Acandidates.%20Accordingly%2C%20we%20develop%20a%20Distribution%20Guidance%20Network%20%28DGNet%29%2C%0Awhich%20comprises%20a%20weakly%20supervised%20learning%20branch%20and%20a%20distribution%0Aalignment%20branch.%20Leveraging%20reliable%20clustering%20initialization%20derived%20from%0Athe%20weakly%20supervised%20learning%20branch%2C%20the%20distribution%20alignment%20branch%0Aalternately%20updates%20the%20parameters%20of%20the%20moVMF%20and%20the%20network%2C%20ensuring%0Aalignment%20with%20the%20moVMF-defined%20latent%20space.%20Extensive%20experiments%20validate%0Athe%20rationality%20and%20effectiveness%20of%20our%20distribution%20choice%20and%20network%0Adesign.%20Consequently%2C%20DGNet%20achieves%20state-of-the-art%20performance%20under%0Amultiple%20datasets%20and%20various%20weakly%20supervised%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution%2520Guidance%2520Network%2520for%2520Weakly%2520Supervised%2520Point%2520Cloud%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DZhiyi%2520Pan%2520and%2520Wei%2520Gao%2520and%2520Shan%2520Liu%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Despite%2520alleviating%2520the%2520dependence%2520on%2520dense%2520annotations%2520inherent%2520to%2520fully%250Asupervised%2520methods%252C%2520weakly%2520supervised%2520point%2520cloud%2520semantic%2520segmentation%2520suffers%250Afrom%2520inadequate%2520supervision%2520signals.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%250Aintroduce%2520a%2520novel%2520perspective%2520that%2520imparts%2520auxiliary%2520constraints%2520by%2520regulating%250Athe%2520feature%2520space%2520under%2520weak%2520supervision.%2520Our%2520initial%2520investigation%2520identifies%250Awhich%2520distributions%2520accurately%2520characterize%2520the%2520feature%2520space%252C%2520subsequently%250Aleveraging%2520this%2520priori%2520to%2520guide%2520the%2520alignment%2520of%2520the%2520weakly%2520supervised%250Aembeddings.%2520Specifically%252C%2520we%2520analyze%2520the%2520superiority%2520of%2520the%2520mixture%2520of%2520von%250AMises-Fisher%2520distributions%2520%2528moVMF%2529%2520among%2520several%2520common%2520distribution%250Acandidates.%2520Accordingly%252C%2520we%2520develop%2520a%2520Distribution%2520Guidance%2520Network%2520%2528DGNet%2529%252C%250Awhich%2520comprises%2520a%2520weakly%2520supervised%2520learning%2520branch%2520and%2520a%2520distribution%250Aalignment%2520branch.%2520Leveraging%2520reliable%2520clustering%2520initialization%2520derived%2520from%250Athe%2520weakly%2520supervised%2520learning%2520branch%252C%2520the%2520distribution%2520alignment%2520branch%250Aalternately%2520updates%2520the%2520parameters%2520of%2520the%2520moVMF%2520and%2520the%2520network%252C%2520ensuring%250Aalignment%2520with%2520the%2520moVMF-defined%2520latent%2520space.%2520Extensive%2520experiments%2520validate%250Athe%2520rationality%2520and%2520effectiveness%2520of%2520our%2520distribution%2520choice%2520and%2520network%250Adesign.%2520Consequently%252C%2520DGNet%2520achieves%2520state-of-the-art%2520performance%2520under%250Amultiple%2520datasets%2520and%2520various%2520weakly%2520supervised%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution%20Guidance%20Network%20for%20Weakly%20Supervised%20Point%20Cloud%20Semantic%0A%20%20Segmentation&entry.906535625=Zhiyi%20Pan%20and%20Wei%20Gao%20and%20Shan%20Liu%20and%20Ge%20Li&entry.1292438233=%20%20Despite%20alleviating%20the%20dependence%20on%20dense%20annotations%20inherent%20to%20fully%0Asupervised%20methods%2C%20weakly%20supervised%20point%20cloud%20semantic%20segmentation%20suffers%0Afrom%20inadequate%20supervision%20signals.%20In%20response%20to%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20perspective%20that%20imparts%20auxiliary%20constraints%20by%20regulating%0Athe%20feature%20space%20under%20weak%20supervision.%20Our%20initial%20investigation%20identifies%0Awhich%20distributions%20accurately%20characterize%20the%20feature%20space%2C%20subsequently%0Aleveraging%20this%20priori%20to%20guide%20the%20alignment%20of%20the%20weakly%20supervised%0Aembeddings.%20Specifically%2C%20we%20analyze%20the%20superiority%20of%20the%20mixture%20of%20von%0AMises-Fisher%20distributions%20%28moVMF%29%20among%20several%20common%20distribution%0Acandidates.%20Accordingly%2C%20we%20develop%20a%20Distribution%20Guidance%20Network%20%28DGNet%29%2C%0Awhich%20comprises%20a%20weakly%20supervised%20learning%20branch%20and%20a%20distribution%0Aalignment%20branch.%20Leveraging%20reliable%20clustering%20initialization%20derived%20from%0Athe%20weakly%20supervised%20learning%20branch%2C%20the%20distribution%20alignment%20branch%0Aalternately%20updates%20the%20parameters%20of%20the%20moVMF%20and%20the%20network%2C%20ensuring%0Aalignment%20with%20the%20moVMF-defined%20latent%20space.%20Extensive%20experiments%20validate%0Athe%20rationality%20and%20effectiveness%20of%20our%20distribution%20choice%20and%20network%0Adesign.%20Consequently%2C%20DGNet%20achieves%20state-of-the-art%20performance%20under%0Amultiple%20datasets%20and%20various%20weakly%20supervised%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08091v1&entry.124074799=Read"},
{"title": "DragTraffic: Interactive and Controllable Traffic Scene Generation for\n  Autonomous Driving", "author": "Sheng Wang and Ge Sun and Fulong Ma and Tianshuai Hu and Qiang Qin and Yongkang Song and Lei Zhu and Junwei Liang", "abstract": "  Evaluating and training autonomous driving systems require diverse and\nscalable corner cases. However, most existing scene generation methods lack\ncontrollability, accuracy, and versatility, resulting in unsatisfactory\ngeneration results. Inspired by DragGAN in image generation, we propose\nDragTraffic, a generalized, interactive, and controllable traffic scene\ngeneration framework based on conditional diffusion. DragTraffic enables\nnon-experts to generate a variety of realistic driving scenarios for different\ntypes of traffic agents through an adaptive mixture expert architecture. We\nemploy a regression model to provide a general initial solution and a\nrefinement process based on the conditional diffusion model to ensure\ndiversity. User-customized context is introduced through cross-attention to\nensure high controllability. Experiments on a real-world driving dataset show\nthat DragTraffic outperforms existing methods in terms of authenticity,\ndiversity, and freedom. Demo videos and code are available at\nhttps://chantsss.github.io/Dragtraffic/.\n", "link": "http://arxiv.org/abs/2404.12624v2", "date": "2024-10-10", "relevancy": 2.722, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5432}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragTraffic%3A%20Interactive%20and%20Controllable%20Traffic%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20DragTraffic%3A%20Interactive%20and%20Controllable%20Traffic%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Sheng%20Wang%20and%20Ge%20Sun%20and%20Fulong%20Ma%20and%20Tianshuai%20Hu%20and%20Qiang%20Qin%20and%20Yongkang%20Song%20and%20Lei%20Zhu%20and%20Junwei%20Liang%0AAbstract%3A%20%20%20Evaluating%20and%20training%20autonomous%20driving%20systems%20require%20diverse%20and%0Ascalable%20corner%20cases.%20However%2C%20most%20existing%20scene%20generation%20methods%20lack%0Acontrollability%2C%20accuracy%2C%20and%20versatility%2C%20resulting%20in%20unsatisfactory%0Ageneration%20results.%20Inspired%20by%20DragGAN%20in%20image%20generation%2C%20we%20propose%0ADragTraffic%2C%20a%20generalized%2C%20interactive%2C%20and%20controllable%20traffic%20scene%0Ageneration%20framework%20based%20on%20conditional%20diffusion.%20DragTraffic%20enables%0Anon-experts%20to%20generate%20a%20variety%20of%20realistic%20driving%20scenarios%20for%20different%0Atypes%20of%20traffic%20agents%20through%20an%20adaptive%20mixture%20expert%20architecture.%20We%0Aemploy%20a%20regression%20model%20to%20provide%20a%20general%20initial%20solution%20and%20a%0Arefinement%20process%20based%20on%20the%20conditional%20diffusion%20model%20to%20ensure%0Adiversity.%20User-customized%20context%20is%20introduced%20through%20cross-attention%20to%0Aensure%20high%20controllability.%20Experiments%20on%20a%20real-world%20driving%20dataset%20show%0Athat%20DragTraffic%20outperforms%20existing%20methods%20in%20terms%20of%20authenticity%2C%0Adiversity%2C%20and%20freedom.%20Demo%20videos%20and%20code%20are%20available%20at%0Ahttps%3A//chantsss.github.io/Dragtraffic/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragTraffic%253A%2520Interactive%2520and%2520Controllable%2520Traffic%2520Scene%2520Generation%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DSheng%2520Wang%2520and%2520Ge%2520Sun%2520and%2520Fulong%2520Ma%2520and%2520Tianshuai%2520Hu%2520and%2520Qiang%2520Qin%2520and%2520Yongkang%2520Song%2520and%2520Lei%2520Zhu%2520and%2520Junwei%2520Liang%26entry.1292438233%3D%2520%2520Evaluating%2520and%2520training%2520autonomous%2520driving%2520systems%2520require%2520diverse%2520and%250Ascalable%2520corner%2520cases.%2520However%252C%2520most%2520existing%2520scene%2520generation%2520methods%2520lack%250Acontrollability%252C%2520accuracy%252C%2520and%2520versatility%252C%2520resulting%2520in%2520unsatisfactory%250Ageneration%2520results.%2520Inspired%2520by%2520DragGAN%2520in%2520image%2520generation%252C%2520we%2520propose%250ADragTraffic%252C%2520a%2520generalized%252C%2520interactive%252C%2520and%2520controllable%2520traffic%2520scene%250Ageneration%2520framework%2520based%2520on%2520conditional%2520diffusion.%2520DragTraffic%2520enables%250Anon-experts%2520to%2520generate%2520a%2520variety%2520of%2520realistic%2520driving%2520scenarios%2520for%2520different%250Atypes%2520of%2520traffic%2520agents%2520through%2520an%2520adaptive%2520mixture%2520expert%2520architecture.%2520We%250Aemploy%2520a%2520regression%2520model%2520to%2520provide%2520a%2520general%2520initial%2520solution%2520and%2520a%250Arefinement%2520process%2520based%2520on%2520the%2520conditional%2520diffusion%2520model%2520to%2520ensure%250Adiversity.%2520User-customized%2520context%2520is%2520introduced%2520through%2520cross-attention%2520to%250Aensure%2520high%2520controllability.%2520Experiments%2520on%2520a%2520real-world%2520driving%2520dataset%2520show%250Athat%2520DragTraffic%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520authenticity%252C%250Adiversity%252C%2520and%2520freedom.%2520Demo%2520videos%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//chantsss.github.io/Dragtraffic/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragTraffic%3A%20Interactive%20and%20Controllable%20Traffic%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&entry.906535625=Sheng%20Wang%20and%20Ge%20Sun%20and%20Fulong%20Ma%20and%20Tianshuai%20Hu%20and%20Qiang%20Qin%20and%20Yongkang%20Song%20and%20Lei%20Zhu%20and%20Junwei%20Liang&entry.1292438233=%20%20Evaluating%20and%20training%20autonomous%20driving%20systems%20require%20diverse%20and%0Ascalable%20corner%20cases.%20However%2C%20most%20existing%20scene%20generation%20methods%20lack%0Acontrollability%2C%20accuracy%2C%20and%20versatility%2C%20resulting%20in%20unsatisfactory%0Ageneration%20results.%20Inspired%20by%20DragGAN%20in%20image%20generation%2C%20we%20propose%0ADragTraffic%2C%20a%20generalized%2C%20interactive%2C%20and%20controllable%20traffic%20scene%0Ageneration%20framework%20based%20on%20conditional%20diffusion.%20DragTraffic%20enables%0Anon-experts%20to%20generate%20a%20variety%20of%20realistic%20driving%20scenarios%20for%20different%0Atypes%20of%20traffic%20agents%20through%20an%20adaptive%20mixture%20expert%20architecture.%20We%0Aemploy%20a%20regression%20model%20to%20provide%20a%20general%20initial%20solution%20and%20a%0Arefinement%20process%20based%20on%20the%20conditional%20diffusion%20model%20to%20ensure%0Adiversity.%20User-customized%20context%20is%20introduced%20through%20cross-attention%20to%0Aensure%20high%20controllability.%20Experiments%20on%20a%20real-world%20driving%20dataset%20show%0Athat%20DragTraffic%20outperforms%20existing%20methods%20in%20terms%20of%20authenticity%2C%0Adiversity%2C%20and%20freedom.%20Demo%20videos%20and%20code%20are%20available%20at%0Ahttps%3A//chantsss.github.io/Dragtraffic/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12624v2&entry.124074799=Read"},
{"title": "Contrastive Learning Via Equivariant Representation", "author": "Sifan Song and Jinfeng Wang and Qiaochu Zhao and Xiang Li and Dufan Wu and Angelos Stefanidis and Jionglong Su and S. Kevin Zhou and Quanzheng Li", "abstract": "  Invariant Contrastive Learning (ICL) methods have achieved impressive\nperformance across various domains. However, the absence of latent space\nrepresentation for distortion (augmentation)-related information in the latent\nspace makes ICL sub-optimal regarding training efficiency and robustness in\ndownstream tasks. Recent studies suggest that introducing equivariance into\nContrastive Learning (CL) can improve overall performance. In this paper, we\nrevisit the roles of augmentation strategies and equivariance in improving CL's\nefficacy. We propose CLeVER (Contrastive Learning Via Equivariant\nRepresentation), a novel equivariant contrastive learning framework compatible\nwith augmentation strategies of arbitrary complexity for various mainstream CL\nbackbone models. Experimental results demonstrate that CLeVER effectively\nextracts and incorporates equivariant information from practical natural\nimages, thereby improving the training efficiency and robustness of baseline\nmodels in downstream tasks and achieving state-of-the-art (SOTA) performance.\nMoreover, we find that leveraging equivariant information extracted by CLeVER\nsimultaneously enhances rotational invariance and sensitivity across\nexperimental tasks, and helps stabilize the framework when handling complex\naugmentations, particularly for models with small-scale backbones.\n", "link": "http://arxiv.org/abs/2406.00262v2", "date": "2024-10-10", "relevancy": 2.6654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20Via%20Equivariant%20Representation&body=Title%3A%20Contrastive%20Learning%20Via%20Equivariant%20Representation%0AAuthor%3A%20Sifan%20Song%20and%20Jinfeng%20Wang%20and%20Qiaochu%20Zhao%20and%20Xiang%20Li%20and%20Dufan%20Wu%20and%20Angelos%20Stefanidis%20and%20Jionglong%20Su%20and%20S.%20Kevin%20Zhou%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20Invariant%20Contrastive%20Learning%20%28ICL%29%20methods%20have%20achieved%20impressive%0Aperformance%20across%20various%20domains.%20However%2C%20the%20absence%20of%20latent%20space%0Arepresentation%20for%20distortion%20%28augmentation%29-related%20information%20in%20the%20latent%0Aspace%20makes%20ICL%20sub-optimal%20regarding%20training%20efficiency%20and%20robustness%20in%0Adownstream%20tasks.%20Recent%20studies%20suggest%20that%20introducing%20equivariance%20into%0AContrastive%20Learning%20%28CL%29%20can%20improve%20overall%20performance.%20In%20this%20paper%2C%20we%0Arevisit%20the%20roles%20of%20augmentation%20strategies%20and%20equivariance%20in%20improving%20CL%27s%0Aefficacy.%20We%20propose%20CLeVER%20%28Contrastive%20Learning%20Via%20Equivariant%0ARepresentation%29%2C%20a%20novel%20equivariant%20contrastive%20learning%20framework%20compatible%0Awith%20augmentation%20strategies%20of%20arbitrary%20complexity%20for%20various%20mainstream%20CL%0Abackbone%20models.%20Experimental%20results%20demonstrate%20that%20CLeVER%20effectively%0Aextracts%20and%20incorporates%20equivariant%20information%20from%20practical%20natural%0Aimages%2C%20thereby%20improving%20the%20training%20efficiency%20and%20robustness%20of%20baseline%0Amodels%20in%20downstream%20tasks%20and%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0AMoreover%2C%20we%20find%20that%20leveraging%20equivariant%20information%20extracted%20by%20CLeVER%0Asimultaneously%20enhances%20rotational%20invariance%20and%20sensitivity%20across%0Aexperimental%20tasks%2C%20and%20helps%20stabilize%20the%20framework%20when%20handling%20complex%0Aaugmentations%2C%20particularly%20for%20models%20with%20small-scale%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520Via%2520Equivariant%2520Representation%26entry.906535625%3DSifan%2520Song%2520and%2520Jinfeng%2520Wang%2520and%2520Qiaochu%2520Zhao%2520and%2520Xiang%2520Li%2520and%2520Dufan%2520Wu%2520and%2520Angelos%2520Stefanidis%2520and%2520Jionglong%2520Su%2520and%2520S.%2520Kevin%2520Zhou%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520Invariant%2520Contrastive%2520Learning%2520%2528ICL%2529%2520methods%2520have%2520achieved%2520impressive%250Aperformance%2520across%2520various%2520domains.%2520However%252C%2520the%2520absence%2520of%2520latent%2520space%250Arepresentation%2520for%2520distortion%2520%2528augmentation%2529-related%2520information%2520in%2520the%2520latent%250Aspace%2520makes%2520ICL%2520sub-optimal%2520regarding%2520training%2520efficiency%2520and%2520robustness%2520in%250Adownstream%2520tasks.%2520Recent%2520studies%2520suggest%2520that%2520introducing%2520equivariance%2520into%250AContrastive%2520Learning%2520%2528CL%2529%2520can%2520improve%2520overall%2520performance.%2520In%2520this%2520paper%252C%2520we%250Arevisit%2520the%2520roles%2520of%2520augmentation%2520strategies%2520and%2520equivariance%2520in%2520improving%2520CL%2527s%250Aefficacy.%2520We%2520propose%2520CLeVER%2520%2528Contrastive%2520Learning%2520Via%2520Equivariant%250ARepresentation%2529%252C%2520a%2520novel%2520equivariant%2520contrastive%2520learning%2520framework%2520compatible%250Awith%2520augmentation%2520strategies%2520of%2520arbitrary%2520complexity%2520for%2520various%2520mainstream%2520CL%250Abackbone%2520models.%2520Experimental%2520results%2520demonstrate%2520that%2520CLeVER%2520effectively%250Aextracts%2520and%2520incorporates%2520equivariant%2520information%2520from%2520practical%2520natural%250Aimages%252C%2520thereby%2520improving%2520the%2520training%2520efficiency%2520and%2520robustness%2520of%2520baseline%250Amodels%2520in%2520downstream%2520tasks%2520and%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%250AMoreover%252C%2520we%2520find%2520that%2520leveraging%2520equivariant%2520information%2520extracted%2520by%2520CLeVER%250Asimultaneously%2520enhances%2520rotational%2520invariance%2520and%2520sensitivity%2520across%250Aexperimental%2520tasks%252C%2520and%2520helps%2520stabilize%2520the%2520framework%2520when%2520handling%2520complex%250Aaugmentations%252C%2520particularly%2520for%2520models%2520with%2520small-scale%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20Via%20Equivariant%20Representation&entry.906535625=Sifan%20Song%20and%20Jinfeng%20Wang%20and%20Qiaochu%20Zhao%20and%20Xiang%20Li%20and%20Dufan%20Wu%20and%20Angelos%20Stefanidis%20and%20Jionglong%20Su%20and%20S.%20Kevin%20Zhou%20and%20Quanzheng%20Li&entry.1292438233=%20%20Invariant%20Contrastive%20Learning%20%28ICL%29%20methods%20have%20achieved%20impressive%0Aperformance%20across%20various%20domains.%20However%2C%20the%20absence%20of%20latent%20space%0Arepresentation%20for%20distortion%20%28augmentation%29-related%20information%20in%20the%20latent%0Aspace%20makes%20ICL%20sub-optimal%20regarding%20training%20efficiency%20and%20robustness%20in%0Adownstream%20tasks.%20Recent%20studies%20suggest%20that%20introducing%20equivariance%20into%0AContrastive%20Learning%20%28CL%29%20can%20improve%20overall%20performance.%20In%20this%20paper%2C%20we%0Arevisit%20the%20roles%20of%20augmentation%20strategies%20and%20equivariance%20in%20improving%20CL%27s%0Aefficacy.%20We%20propose%20CLeVER%20%28Contrastive%20Learning%20Via%20Equivariant%0ARepresentation%29%2C%20a%20novel%20equivariant%20contrastive%20learning%20framework%20compatible%0Awith%20augmentation%20strategies%20of%20arbitrary%20complexity%20for%20various%20mainstream%20CL%0Abackbone%20models.%20Experimental%20results%20demonstrate%20that%20CLeVER%20effectively%0Aextracts%20and%20incorporates%20equivariant%20information%20from%20practical%20natural%0Aimages%2C%20thereby%20improving%20the%20training%20efficiency%20and%20robustness%20of%20baseline%0Amodels%20in%20downstream%20tasks%20and%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0AMoreover%2C%20we%20find%20that%20leveraging%20equivariant%20information%20extracted%20by%20CLeVER%0Asimultaneously%20enhances%20rotational%20invariance%20and%20sensitivity%20across%0Aexperimental%20tasks%2C%20and%20helps%20stabilize%20the%20framework%20when%20handling%20complex%0Aaugmentations%2C%20particularly%20for%20models%20with%20small-scale%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00262v2&entry.124074799=Read"},
{"title": "RayEmb: Arbitrary Landmark Detection in X-Ray Images Using Ray Embedding\n  Subspace", "author": "Pragyan Shrestha and Chun Xie and Yuichi Yoshii and Itaru Kitahara", "abstract": "  Intra-operative 2D-3D registration of X-ray images with pre-operatively\nacquired CT scans is a crucial procedure in orthopedic surgeries. Anatomical\nlandmarks pre-annotated in the CT volume can be detected in X-ray images to\nestablish 2D-3D correspondences, which are then utilized for registration.\nHowever, registration often fails in certain view angles due to poor landmark\nvisibility. We propose a novel method to address this issue by detecting\narbitrary landmark points in X-ray images. Our approach represents 3D points as\ndistinct subspaces, formed by feature vectors (referred to as ray embeddings)\ncorresponding to intersecting rays. Establishing 2D-3D correspondences then\nbecomes a task of finding ray embeddings that are close to a given subspace,\nessentially performing an intersection test. Unlike conventional methods for\nlandmark estimation, our approach eliminates the need for manually annotating\nfixed landmarks. We trained our model using the synthetic images generated from\nCTPelvic1K CLINIC dataset, which contains 103 CT volumes, and evaluated it on\nthe DeepFluoro dataset, comprising real X-ray images. Experimental results\ndemonstrate the superiority of our method over conventional methods. The code\nis available at https://github.com/Pragyanstha/rayemb.\n", "link": "http://arxiv.org/abs/2410.08152v1", "date": "2024-10-10", "relevancy": 2.6642, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RayEmb%3A%20Arbitrary%20Landmark%20Detection%20in%20X-Ray%20Images%20Using%20Ray%20Embedding%0A%20%20Subspace&body=Title%3A%20RayEmb%3A%20Arbitrary%20Landmark%20Detection%20in%20X-Ray%20Images%20Using%20Ray%20Embedding%0A%20%20Subspace%0AAuthor%3A%20Pragyan%20Shrestha%20and%20Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara%0AAbstract%3A%20%20%20Intra-operative%202D-3D%20registration%20of%20X-ray%20images%20with%20pre-operatively%0Aacquired%20CT%20scans%20is%20a%20crucial%20procedure%20in%20orthopedic%20surgeries.%20Anatomical%0Alandmarks%20pre-annotated%20in%20the%20CT%20volume%20can%20be%20detected%20in%20X-ray%20images%20to%0Aestablish%202D-3D%20correspondences%2C%20which%20are%20then%20utilized%20for%20registration.%0AHowever%2C%20registration%20often%20fails%20in%20certain%20view%20angles%20due%20to%20poor%20landmark%0Avisibility.%20We%20propose%20a%20novel%20method%20to%20address%20this%20issue%20by%20detecting%0Aarbitrary%20landmark%20points%20in%20X-ray%20images.%20Our%20approach%20represents%203D%20points%20as%0Adistinct%20subspaces%2C%20formed%20by%20feature%20vectors%20%28referred%20to%20as%20ray%20embeddings%29%0Acorresponding%20to%20intersecting%20rays.%20Establishing%202D-3D%20correspondences%20then%0Abecomes%20a%20task%20of%20finding%20ray%20embeddings%20that%20are%20close%20to%20a%20given%20subspace%2C%0Aessentially%20performing%20an%20intersection%20test.%20Unlike%20conventional%20methods%20for%0Alandmark%20estimation%2C%20our%20approach%20eliminates%20the%20need%20for%20manually%20annotating%0Afixed%20landmarks.%20We%20trained%20our%20model%20using%20the%20synthetic%20images%20generated%20from%0ACTPelvic1K%20CLINIC%20dataset%2C%20which%20contains%20103%20CT%20volumes%2C%20and%20evaluated%20it%20on%0Athe%20DeepFluoro%20dataset%2C%20comprising%20real%20X-ray%20images.%20Experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20conventional%20methods.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Pragyanstha/rayemb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRayEmb%253A%2520Arbitrary%2520Landmark%2520Detection%2520in%2520X-Ray%2520Images%2520Using%2520Ray%2520Embedding%250A%2520%2520Subspace%26entry.906535625%3DPragyan%2520Shrestha%2520and%2520Chun%2520Xie%2520and%2520Yuichi%2520Yoshii%2520and%2520Itaru%2520Kitahara%26entry.1292438233%3D%2520%2520Intra-operative%25202D-3D%2520registration%2520of%2520X-ray%2520images%2520with%2520pre-operatively%250Aacquired%2520CT%2520scans%2520is%2520a%2520crucial%2520procedure%2520in%2520orthopedic%2520surgeries.%2520Anatomical%250Alandmarks%2520pre-annotated%2520in%2520the%2520CT%2520volume%2520can%2520be%2520detected%2520in%2520X-ray%2520images%2520to%250Aestablish%25202D-3D%2520correspondences%252C%2520which%2520are%2520then%2520utilized%2520for%2520registration.%250AHowever%252C%2520registration%2520often%2520fails%2520in%2520certain%2520view%2520angles%2520due%2520to%2520poor%2520landmark%250Avisibility.%2520We%2520propose%2520a%2520novel%2520method%2520to%2520address%2520this%2520issue%2520by%2520detecting%250Aarbitrary%2520landmark%2520points%2520in%2520X-ray%2520images.%2520Our%2520approach%2520represents%25203D%2520points%2520as%250Adistinct%2520subspaces%252C%2520formed%2520by%2520feature%2520vectors%2520%2528referred%2520to%2520as%2520ray%2520embeddings%2529%250Acorresponding%2520to%2520intersecting%2520rays.%2520Establishing%25202D-3D%2520correspondences%2520then%250Abecomes%2520a%2520task%2520of%2520finding%2520ray%2520embeddings%2520that%2520are%2520close%2520to%2520a%2520given%2520subspace%252C%250Aessentially%2520performing%2520an%2520intersection%2520test.%2520Unlike%2520conventional%2520methods%2520for%250Alandmark%2520estimation%252C%2520our%2520approach%2520eliminates%2520the%2520need%2520for%2520manually%2520annotating%250Afixed%2520landmarks.%2520We%2520trained%2520our%2520model%2520using%2520the%2520synthetic%2520images%2520generated%2520from%250ACTPelvic1K%2520CLINIC%2520dataset%252C%2520which%2520contains%2520103%2520CT%2520volumes%252C%2520and%2520evaluated%2520it%2520on%250Athe%2520DeepFluoro%2520dataset%252C%2520comprising%2520real%2520X-ray%2520images.%2520Experimental%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520conventional%2520methods.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/Pragyanstha/rayemb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RayEmb%3A%20Arbitrary%20Landmark%20Detection%20in%20X-Ray%20Images%20Using%20Ray%20Embedding%0A%20%20Subspace&entry.906535625=Pragyan%20Shrestha%20and%20Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara&entry.1292438233=%20%20Intra-operative%202D-3D%20registration%20of%20X-ray%20images%20with%20pre-operatively%0Aacquired%20CT%20scans%20is%20a%20crucial%20procedure%20in%20orthopedic%20surgeries.%20Anatomical%0Alandmarks%20pre-annotated%20in%20the%20CT%20volume%20can%20be%20detected%20in%20X-ray%20images%20to%0Aestablish%202D-3D%20correspondences%2C%20which%20are%20then%20utilized%20for%20registration.%0AHowever%2C%20registration%20often%20fails%20in%20certain%20view%20angles%20due%20to%20poor%20landmark%0Avisibility.%20We%20propose%20a%20novel%20method%20to%20address%20this%20issue%20by%20detecting%0Aarbitrary%20landmark%20points%20in%20X-ray%20images.%20Our%20approach%20represents%203D%20points%20as%0Adistinct%20subspaces%2C%20formed%20by%20feature%20vectors%20%28referred%20to%20as%20ray%20embeddings%29%0Acorresponding%20to%20intersecting%20rays.%20Establishing%202D-3D%20correspondences%20then%0Abecomes%20a%20task%20of%20finding%20ray%20embeddings%20that%20are%20close%20to%20a%20given%20subspace%2C%0Aessentially%20performing%20an%20intersection%20test.%20Unlike%20conventional%20methods%20for%0Alandmark%20estimation%2C%20our%20approach%20eliminates%20the%20need%20for%20manually%20annotating%0Afixed%20landmarks.%20We%20trained%20our%20model%20using%20the%20synthetic%20images%20generated%20from%0ACTPelvic1K%20CLINIC%20dataset%2C%20which%20contains%20103%20CT%20volumes%2C%20and%20evaluated%20it%20on%0Athe%20DeepFluoro%20dataset%2C%20comprising%20real%20X-ray%20images.%20Experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20conventional%20methods.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Pragyanstha/rayemb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08152v1&entry.124074799=Read"},
{"title": "Parameter-Efficient Fine-Tuning in Spectral Domain for Point Cloud\n  Learning", "author": "Dingkang Liang and Tianrui Feng and Xin Zhou and Yumeng Zhang and Zhikang Zou and Xiang Bai", "abstract": "  Recently, leveraging pre-training techniques to enhance point cloud models\nhas become a hot research topic. However, existing approaches typically require\nfull fine-tuning of pre-trained models to achieve satisfied performance on\ndownstream tasks, accompanying storage-intensive and computationally demanding.\nTo address this issue, we propose a novel Parameter-Efficient Fine-Tuning\n(PEFT) method for point cloud, called PointGST (Point cloud Graph Spectral\nTuning). PointGST freezes the pre-trained model and introduces a lightweight,\ntrainable Point Cloud Spectral Adapter (PCSA) to fine-tune parameters in the\nspectral domain. The core idea is built on two observations: 1) The inner\ntokens from frozen models might present confusion in the spatial domain; 2)\nTask-specific intrinsic information is important for transferring the general\nknowledge to the downstream task. Specifically, PointGST transfers the point\ntokens from the spatial domain to the spectral domain, effectively\nde-correlating confusion among tokens via using orthogonal components for\nseparating. Moreover, the generated spectral basis involves intrinsic\ninformation about the downstream point clouds, enabling more targeted tuning.\nAs a result, PointGST facilitates the efficient transfer of general knowledge\nto downstream tasks while significantly reducing training costs. Extensive\nexperiments on challenging point cloud datasets across various tasks\ndemonstrate that PointGST not only outperforms its fully fine-tuning\ncounterpart but also significantly reduces trainable parameters, making it a\npromising solution for efficient point cloud learning. It improves upon a solid\nbaseline by +2.28%, 1.16%, and 2.78%, resulting in 99.48%, 97.76%, and 96.18%\non the ScanObjNN OBJ BG, OBJ OBLY, and PB T50 RS datasets, respectively. This\nadvancement establishes a new state-of-the-art, using only 0.67% of the\ntrainable parameters.\n", "link": "http://arxiv.org/abs/2410.08114v1", "date": "2024-10-10", "relevancy": 2.623, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5531}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5109}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Fine-Tuning%20in%20Spectral%20Domain%20for%20Point%20Cloud%0A%20%20Learning&body=Title%3A%20Parameter-Efficient%20Fine-Tuning%20in%20Spectral%20Domain%20for%20Point%20Cloud%0A%20%20Learning%0AAuthor%3A%20Dingkang%20Liang%20and%20Tianrui%20Feng%20and%20Xin%20Zhou%20and%20Yumeng%20Zhang%20and%20Zhikang%20Zou%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Recently%2C%20leveraging%20pre-training%20techniques%20to%20enhance%20point%20cloud%20models%0Ahas%20become%20a%20hot%20research%20topic.%20However%2C%20existing%20approaches%20typically%20require%0Afull%20fine-tuning%20of%20pre-trained%20models%20to%20achieve%20satisfied%20performance%20on%0Adownstream%20tasks%2C%20accompanying%20storage-intensive%20and%20computationally%20demanding.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20method%20for%20point%20cloud%2C%20called%20PointGST%20%28Point%20cloud%20Graph%20Spectral%0ATuning%29.%20PointGST%20freezes%20the%20pre-trained%20model%20and%20introduces%20a%20lightweight%2C%0Atrainable%20Point%20Cloud%20Spectral%20Adapter%20%28PCSA%29%20to%20fine-tune%20parameters%20in%20the%0Aspectral%20domain.%20The%20core%20idea%20is%20built%20on%20two%20observations%3A%201%29%20The%20inner%0Atokens%20from%20frozen%20models%20might%20present%20confusion%20in%20the%20spatial%20domain%3B%202%29%0ATask-specific%20intrinsic%20information%20is%20important%20for%20transferring%20the%20general%0Aknowledge%20to%20the%20downstream%20task.%20Specifically%2C%20PointGST%20transfers%20the%20point%0Atokens%20from%20the%20spatial%20domain%20to%20the%20spectral%20domain%2C%20effectively%0Ade-correlating%20confusion%20among%20tokens%20via%20using%20orthogonal%20components%20for%0Aseparating.%20Moreover%2C%20the%20generated%20spectral%20basis%20involves%20intrinsic%0Ainformation%20about%20the%20downstream%20point%20clouds%2C%20enabling%20more%20targeted%20tuning.%0AAs%20a%20result%2C%20PointGST%20facilitates%20the%20efficient%20transfer%20of%20general%20knowledge%0Ato%20downstream%20tasks%20while%20significantly%20reducing%20training%20costs.%20Extensive%0Aexperiments%20on%20challenging%20point%20cloud%20datasets%20across%20various%20tasks%0Ademonstrate%20that%20PointGST%20not%20only%20outperforms%20its%20fully%20fine-tuning%0Acounterpart%20but%20also%20significantly%20reduces%20trainable%20parameters%2C%20making%20it%20a%0Apromising%20solution%20for%20efficient%20point%20cloud%20learning.%20It%20improves%20upon%20a%20solid%0Abaseline%20by%20%2B2.28%25%2C%201.16%25%2C%20and%202.78%25%2C%20resulting%20in%2099.48%25%2C%2097.76%25%2C%20and%2096.18%25%0Aon%20the%20ScanObjNN%20OBJ%20BG%2C%20OBJ%20OBLY%2C%20and%20PB%20T50%20RS%20datasets%2C%20respectively.%20This%0Aadvancement%20establishes%20a%20new%20state-of-the-art%2C%20using%20only%200.67%25%20of%20the%0Atrainable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Fine-Tuning%2520in%2520Spectral%2520Domain%2520for%2520Point%2520Cloud%250A%2520%2520Learning%26entry.906535625%3DDingkang%2520Liang%2520and%2520Tianrui%2520Feng%2520and%2520Xin%2520Zhou%2520and%2520Yumeng%2520Zhang%2520and%2520Zhikang%2520Zou%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Recently%252C%2520leveraging%2520pre-training%2520techniques%2520to%2520enhance%2520point%2520cloud%2520models%250Ahas%2520become%2520a%2520hot%2520research%2520topic.%2520However%252C%2520existing%2520approaches%2520typically%2520require%250Afull%2520fine-tuning%2520of%2520pre-trained%2520models%2520to%2520achieve%2520satisfied%2520performance%2520on%250Adownstream%2520tasks%252C%2520accompanying%2520storage-intensive%2520and%2520computationally%2520demanding.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520Parameter-Efficient%2520Fine-Tuning%250A%2528PEFT%2529%2520method%2520for%2520point%2520cloud%252C%2520called%2520PointGST%2520%2528Point%2520cloud%2520Graph%2520Spectral%250ATuning%2529.%2520PointGST%2520freezes%2520the%2520pre-trained%2520model%2520and%2520introduces%2520a%2520lightweight%252C%250Atrainable%2520Point%2520Cloud%2520Spectral%2520Adapter%2520%2528PCSA%2529%2520to%2520fine-tune%2520parameters%2520in%2520the%250Aspectral%2520domain.%2520The%2520core%2520idea%2520is%2520built%2520on%2520two%2520observations%253A%25201%2529%2520The%2520inner%250Atokens%2520from%2520frozen%2520models%2520might%2520present%2520confusion%2520in%2520the%2520spatial%2520domain%253B%25202%2529%250ATask-specific%2520intrinsic%2520information%2520is%2520important%2520for%2520transferring%2520the%2520general%250Aknowledge%2520to%2520the%2520downstream%2520task.%2520Specifically%252C%2520PointGST%2520transfers%2520the%2520point%250Atokens%2520from%2520the%2520spatial%2520domain%2520to%2520the%2520spectral%2520domain%252C%2520effectively%250Ade-correlating%2520confusion%2520among%2520tokens%2520via%2520using%2520orthogonal%2520components%2520for%250Aseparating.%2520Moreover%252C%2520the%2520generated%2520spectral%2520basis%2520involves%2520intrinsic%250Ainformation%2520about%2520the%2520downstream%2520point%2520clouds%252C%2520enabling%2520more%2520targeted%2520tuning.%250AAs%2520a%2520result%252C%2520PointGST%2520facilitates%2520the%2520efficient%2520transfer%2520of%2520general%2520knowledge%250Ato%2520downstream%2520tasks%2520while%2520significantly%2520reducing%2520training%2520costs.%2520Extensive%250Aexperiments%2520on%2520challenging%2520point%2520cloud%2520datasets%2520across%2520various%2520tasks%250Ademonstrate%2520that%2520PointGST%2520not%2520only%2520outperforms%2520its%2520fully%2520fine-tuning%250Acounterpart%2520but%2520also%2520significantly%2520reduces%2520trainable%2520parameters%252C%2520making%2520it%2520a%250Apromising%2520solution%2520for%2520efficient%2520point%2520cloud%2520learning.%2520It%2520improves%2520upon%2520a%2520solid%250Abaseline%2520by%2520%252B2.28%2525%252C%25201.16%2525%252C%2520and%25202.78%2525%252C%2520resulting%2520in%252099.48%2525%252C%252097.76%2525%252C%2520and%252096.18%2525%250Aon%2520the%2520ScanObjNN%2520OBJ%2520BG%252C%2520OBJ%2520OBLY%252C%2520and%2520PB%2520T50%2520RS%2520datasets%252C%2520respectively.%2520This%250Aadvancement%2520establishes%2520a%2520new%2520state-of-the-art%252C%2520using%2520only%25200.67%2525%2520of%2520the%250Atrainable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Fine-Tuning%20in%20Spectral%20Domain%20for%20Point%20Cloud%0A%20%20Learning&entry.906535625=Dingkang%20Liang%20and%20Tianrui%20Feng%20and%20Xin%20Zhou%20and%20Yumeng%20Zhang%20and%20Zhikang%20Zou%20and%20Xiang%20Bai&entry.1292438233=%20%20Recently%2C%20leveraging%20pre-training%20techniques%20to%20enhance%20point%20cloud%20models%0Ahas%20become%20a%20hot%20research%20topic.%20However%2C%20existing%20approaches%20typically%20require%0Afull%20fine-tuning%20of%20pre-trained%20models%20to%20achieve%20satisfied%20performance%20on%0Adownstream%20tasks%2C%20accompanying%20storage-intensive%20and%20computationally%20demanding.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20method%20for%20point%20cloud%2C%20called%20PointGST%20%28Point%20cloud%20Graph%20Spectral%0ATuning%29.%20PointGST%20freezes%20the%20pre-trained%20model%20and%20introduces%20a%20lightweight%2C%0Atrainable%20Point%20Cloud%20Spectral%20Adapter%20%28PCSA%29%20to%20fine-tune%20parameters%20in%20the%0Aspectral%20domain.%20The%20core%20idea%20is%20built%20on%20two%20observations%3A%201%29%20The%20inner%0Atokens%20from%20frozen%20models%20might%20present%20confusion%20in%20the%20spatial%20domain%3B%202%29%0ATask-specific%20intrinsic%20information%20is%20important%20for%20transferring%20the%20general%0Aknowledge%20to%20the%20downstream%20task.%20Specifically%2C%20PointGST%20transfers%20the%20point%0Atokens%20from%20the%20spatial%20domain%20to%20the%20spectral%20domain%2C%20effectively%0Ade-correlating%20confusion%20among%20tokens%20via%20using%20orthogonal%20components%20for%0Aseparating.%20Moreover%2C%20the%20generated%20spectral%20basis%20involves%20intrinsic%0Ainformation%20about%20the%20downstream%20point%20clouds%2C%20enabling%20more%20targeted%20tuning.%0AAs%20a%20result%2C%20PointGST%20facilitates%20the%20efficient%20transfer%20of%20general%20knowledge%0Ato%20downstream%20tasks%20while%20significantly%20reducing%20training%20costs.%20Extensive%0Aexperiments%20on%20challenging%20point%20cloud%20datasets%20across%20various%20tasks%0Ademonstrate%20that%20PointGST%20not%20only%20outperforms%20its%20fully%20fine-tuning%0Acounterpart%20but%20also%20significantly%20reduces%20trainable%20parameters%2C%20making%20it%20a%0Apromising%20solution%20for%20efficient%20point%20cloud%20learning.%20It%20improves%20upon%20a%20solid%0Abaseline%20by%20%2B2.28%25%2C%201.16%25%2C%20and%202.78%25%2C%20resulting%20in%2099.48%25%2C%2097.76%25%2C%20and%2096.18%25%0Aon%20the%20ScanObjNN%20OBJ%20BG%2C%20OBJ%20OBLY%2C%20and%20PB%20T50%20RS%20datasets%2C%20respectively.%20This%0Aadvancement%20establishes%20a%20new%20state-of-the-art%2C%20using%20only%200.67%25%20of%20the%0Atrainable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08114v1&entry.124074799=Read"},
{"title": "HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly", "author": "Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen", "abstract": "  There have been many benchmarks for evaluating long-context language models\n(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack\n(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate\nto the diverse downstream applications of LCLMs, and the inconsistency further\ncomplicates model comparison. We investigate the underlying reasons behind\ncurrent practices and find that existing benchmarks often provide noisy signals\ndue to low coverage of applications, insufficient lengths, unreliable metrics,\nand incompatibility with base models. In this work, we present HELMET (How to\nEvaluate Long-context Models Effectively and Thoroughly), a comprehensive\nbenchmark encompassing seven diverse, application-centric categories. We also\naddress many issues in previous benchmarks by adding controllable lengths up to\n128k tokens, model-based evaluation for reliable metrics, and few-shot\nprompting for robustly evaluating base models. Consequently, we demonstrate\nthat HELMET offers more reliable and consistent rankings of frontier LCLMs.\nThrough a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks\nlike NIAH are not good predictors of downstream performance; (2) the diverse\ncategories in HELMET exhibit distinct trends and low correlation with each\nother; and (3) while most LCLMs achieve perfect NIAH scores, open-source models\nsignificantly lag behind closed ones when the task requires full-context\nreasoning or following complex instructions -- the gap widens with increased\nlengths. Finally, we recommend using our RAG tasks for fast model development,\nas they are easy to run and more predictive of other downstream performance;\nultimately, we advocate for a holistic evaluation across diverse tasks.\n", "link": "http://arxiv.org/abs/2410.02694v2", "date": "2024-10-10", "relevancy": 2.591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HELMET%3A%20How%20to%20Evaluate%20Long-Context%20Language%20Models%20Effectively%20and%0A%20%20Thoroughly&body=Title%3A%20HELMET%3A%20How%20to%20Evaluate%20Long-Context%20Language%20Models%20Effectively%20and%0A%20%20Thoroughly%0AAuthor%3A%20Howard%20Yen%20and%20Tianyu%20Gao%20and%20Minmin%20Hou%20and%20Ke%20Ding%20and%20Daniel%20Fleischer%20and%20Peter%20Izsak%20and%20Moshe%20Wasserblat%20and%20Danqi%20Chen%0AAbstract%3A%20%20%20There%20have%20been%20many%20benchmarks%20for%20evaluating%20long-context%20language%20models%0A%28LCLMs%29%2C%20but%20developers%20often%20rely%20on%20synthetic%20tasks%20like%20needle-in-a-haystack%0A%28NIAH%29%20or%20arbitrary%20subsets%20of%20tasks.%20It%20remains%20unclear%20whether%20they%20translate%0Ato%20the%20diverse%20downstream%20applications%20of%20LCLMs%2C%20and%20the%20inconsistency%20further%0Acomplicates%20model%20comparison.%20We%20investigate%20the%20underlying%20reasons%20behind%0Acurrent%20practices%20and%20find%20that%20existing%20benchmarks%20often%20provide%20noisy%20signals%0Adue%20to%20low%20coverage%20of%20applications%2C%20insufficient%20lengths%2C%20unreliable%20metrics%2C%0Aand%20incompatibility%20with%20base%20models.%20In%20this%20work%2C%20we%20present%20HELMET%20%28How%20to%0AEvaluate%20Long-context%20Models%20Effectively%20and%20Thoroughly%29%2C%20a%20comprehensive%0Abenchmark%20encompassing%20seven%20diverse%2C%20application-centric%20categories.%20We%20also%0Aaddress%20many%20issues%20in%20previous%20benchmarks%20by%20adding%20controllable%20lengths%20up%20to%0A128k%20tokens%2C%20model-based%20evaluation%20for%20reliable%20metrics%2C%20and%20few-shot%0Aprompting%20for%20robustly%20evaluating%20base%20models.%20Consequently%2C%20we%20demonstrate%0Athat%20HELMET%20offers%20more%20reliable%20and%20consistent%20rankings%20of%20frontier%20LCLMs.%0AThrough%20a%20comprehensive%20study%20of%2051%20LCLMs%2C%20we%20find%20that%20%281%29%20synthetic%20tasks%0Alike%20NIAH%20are%20not%20good%20predictors%20of%20downstream%20performance%3B%20%282%29%20the%20diverse%0Acategories%20in%20HELMET%20exhibit%20distinct%20trends%20and%20low%20correlation%20with%20each%0Aother%3B%20and%20%283%29%20while%20most%20LCLMs%20achieve%20perfect%20NIAH%20scores%2C%20open-source%20models%0Asignificantly%20lag%20behind%20closed%20ones%20when%20the%20task%20requires%20full-context%0Areasoning%20or%20following%20complex%20instructions%20--%20the%20gap%20widens%20with%20increased%0Alengths.%20Finally%2C%20we%20recommend%20using%20our%20RAG%20tasks%20for%20fast%20model%20development%2C%0Aas%20they%20are%20easy%20to%20run%20and%20more%20predictive%20of%20other%20downstream%20performance%3B%0Aultimately%2C%20we%20advocate%20for%20a%20holistic%20evaluation%20across%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHELMET%253A%2520How%2520to%2520Evaluate%2520Long-Context%2520Language%2520Models%2520Effectively%2520and%250A%2520%2520Thoroughly%26entry.906535625%3DHoward%2520Yen%2520and%2520Tianyu%2520Gao%2520and%2520Minmin%2520Hou%2520and%2520Ke%2520Ding%2520and%2520Daniel%2520Fleischer%2520and%2520Peter%2520Izsak%2520and%2520Moshe%2520Wasserblat%2520and%2520Danqi%2520Chen%26entry.1292438233%3D%2520%2520There%2520have%2520been%2520many%2520benchmarks%2520for%2520evaluating%2520long-context%2520language%2520models%250A%2528LCLMs%2529%252C%2520but%2520developers%2520often%2520rely%2520on%2520synthetic%2520tasks%2520like%2520needle-in-a-haystack%250A%2528NIAH%2529%2520or%2520arbitrary%2520subsets%2520of%2520tasks.%2520It%2520remains%2520unclear%2520whether%2520they%2520translate%250Ato%2520the%2520diverse%2520downstream%2520applications%2520of%2520LCLMs%252C%2520and%2520the%2520inconsistency%2520further%250Acomplicates%2520model%2520comparison.%2520We%2520investigate%2520the%2520underlying%2520reasons%2520behind%250Acurrent%2520practices%2520and%2520find%2520that%2520existing%2520benchmarks%2520often%2520provide%2520noisy%2520signals%250Adue%2520to%2520low%2520coverage%2520of%2520applications%252C%2520insufficient%2520lengths%252C%2520unreliable%2520metrics%252C%250Aand%2520incompatibility%2520with%2520base%2520models.%2520In%2520this%2520work%252C%2520we%2520present%2520HELMET%2520%2528How%2520to%250AEvaluate%2520Long-context%2520Models%2520Effectively%2520and%2520Thoroughly%2529%252C%2520a%2520comprehensive%250Abenchmark%2520encompassing%2520seven%2520diverse%252C%2520application-centric%2520categories.%2520We%2520also%250Aaddress%2520many%2520issues%2520in%2520previous%2520benchmarks%2520by%2520adding%2520controllable%2520lengths%2520up%2520to%250A128k%2520tokens%252C%2520model-based%2520evaluation%2520for%2520reliable%2520metrics%252C%2520and%2520few-shot%250Aprompting%2520for%2520robustly%2520evaluating%2520base%2520models.%2520Consequently%252C%2520we%2520demonstrate%250Athat%2520HELMET%2520offers%2520more%2520reliable%2520and%2520consistent%2520rankings%2520of%2520frontier%2520LCLMs.%250AThrough%2520a%2520comprehensive%2520study%2520of%252051%2520LCLMs%252C%2520we%2520find%2520that%2520%25281%2529%2520synthetic%2520tasks%250Alike%2520NIAH%2520are%2520not%2520good%2520predictors%2520of%2520downstream%2520performance%253B%2520%25282%2529%2520the%2520diverse%250Acategories%2520in%2520HELMET%2520exhibit%2520distinct%2520trends%2520and%2520low%2520correlation%2520with%2520each%250Aother%253B%2520and%2520%25283%2529%2520while%2520most%2520LCLMs%2520achieve%2520perfect%2520NIAH%2520scores%252C%2520open-source%2520models%250Asignificantly%2520lag%2520behind%2520closed%2520ones%2520when%2520the%2520task%2520requires%2520full-context%250Areasoning%2520or%2520following%2520complex%2520instructions%2520--%2520the%2520gap%2520widens%2520with%2520increased%250Alengths.%2520Finally%252C%2520we%2520recommend%2520using%2520our%2520RAG%2520tasks%2520for%2520fast%2520model%2520development%252C%250Aas%2520they%2520are%2520easy%2520to%2520run%2520and%2520more%2520predictive%2520of%2520other%2520downstream%2520performance%253B%250Aultimately%252C%2520we%2520advocate%2520for%2520a%2520holistic%2520evaluation%2520across%2520diverse%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HELMET%3A%20How%20to%20Evaluate%20Long-Context%20Language%20Models%20Effectively%20and%0A%20%20Thoroughly&entry.906535625=Howard%20Yen%20and%20Tianyu%20Gao%20and%20Minmin%20Hou%20and%20Ke%20Ding%20and%20Daniel%20Fleischer%20and%20Peter%20Izsak%20and%20Moshe%20Wasserblat%20and%20Danqi%20Chen&entry.1292438233=%20%20There%20have%20been%20many%20benchmarks%20for%20evaluating%20long-context%20language%20models%0A%28LCLMs%29%2C%20but%20developers%20often%20rely%20on%20synthetic%20tasks%20like%20needle-in-a-haystack%0A%28NIAH%29%20or%20arbitrary%20subsets%20of%20tasks.%20It%20remains%20unclear%20whether%20they%20translate%0Ato%20the%20diverse%20downstream%20applications%20of%20LCLMs%2C%20and%20the%20inconsistency%20further%0Acomplicates%20model%20comparison.%20We%20investigate%20the%20underlying%20reasons%20behind%0Acurrent%20practices%20and%20find%20that%20existing%20benchmarks%20often%20provide%20noisy%20signals%0Adue%20to%20low%20coverage%20of%20applications%2C%20insufficient%20lengths%2C%20unreliable%20metrics%2C%0Aand%20incompatibility%20with%20base%20models.%20In%20this%20work%2C%20we%20present%20HELMET%20%28How%20to%0AEvaluate%20Long-context%20Models%20Effectively%20and%20Thoroughly%29%2C%20a%20comprehensive%0Abenchmark%20encompassing%20seven%20diverse%2C%20application-centric%20categories.%20We%20also%0Aaddress%20many%20issues%20in%20previous%20benchmarks%20by%20adding%20controllable%20lengths%20up%20to%0A128k%20tokens%2C%20model-based%20evaluation%20for%20reliable%20metrics%2C%20and%20few-shot%0Aprompting%20for%20robustly%20evaluating%20base%20models.%20Consequently%2C%20we%20demonstrate%0Athat%20HELMET%20offers%20more%20reliable%20and%20consistent%20rankings%20of%20frontier%20LCLMs.%0AThrough%20a%20comprehensive%20study%20of%2051%20LCLMs%2C%20we%20find%20that%20%281%29%20synthetic%20tasks%0Alike%20NIAH%20are%20not%20good%20predictors%20of%20downstream%20performance%3B%20%282%29%20the%20diverse%0Acategories%20in%20HELMET%20exhibit%20distinct%20trends%20and%20low%20correlation%20with%20each%0Aother%3B%20and%20%283%29%20while%20most%20LCLMs%20achieve%20perfect%20NIAH%20scores%2C%20open-source%20models%0Asignificantly%20lag%20behind%20closed%20ones%20when%20the%20task%20requires%20full-context%0Areasoning%20or%20following%20complex%20instructions%20--%20the%20gap%20widens%20with%20increased%0Alengths.%20Finally%2C%20we%20recommend%20using%20our%20RAG%20tasks%20for%20fast%20model%20development%2C%0Aas%20they%20are%20easy%20to%20run%20and%20more%20predictive%20of%20other%20downstream%20performance%3B%0Aultimately%2C%20we%20advocate%20for%20a%20holistic%20evaluation%20across%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02694v2&entry.124074799=Read"},
{"title": "Asynchronous Graph Generator", "author": "Christopher P. Ley and Felipe Tobar", "abstract": "  We introduce the asynchronous graph generator (AGG), a novel graph attention\nnetwork for imputation and prediction of multi-channel time series. Free from\nrecurrent components or assumptions about temporal/spatial regularity, AGG\nencodes measurements, timestamps and channel-specific features directly in the\nnodes via learnable embeddings. Through an attention mechanism, these\nembeddings allow for discovering expressive relationships among the variables\nof interest in the form of a homogeneous graph. Once trained, AGG performs\nimputation by \\emph{conditional attention generation}, i.e., by creating a new\nnode conditioned on given timestamps and channel specification. The proposed\nAGG is compared to related methods in the literature and its performance is\nanalysed from a data augmentation perspective. Our experiments reveal that AGG\nachieved state-of-the-art results in time series imputation, classification and\nprediction for the benchmark datasets \\emph{Beijing Air Quality},\n\\emph{PhysioNet ICU 2012} and \\emph{UCI localisation}, outperforming other\nrecent attention-based networks.\n", "link": "http://arxiv.org/abs/2309.17335v3", "date": "2024-10-10", "relevancy": 2.5853, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5152}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Graph%20Generator&body=Title%3A%20Asynchronous%20Graph%20Generator%0AAuthor%3A%20Christopher%20P.%20Ley%20and%20Felipe%20Tobar%0AAbstract%3A%20%20%20We%20introduce%20the%20asynchronous%20graph%20generator%20%28AGG%29%2C%20a%20novel%20graph%20attention%0Anetwork%20for%20imputation%20and%20prediction%20of%20multi-channel%20time%20series.%20Free%20from%0Arecurrent%20components%20or%20assumptions%20about%20temporal/spatial%20regularity%2C%20AGG%0Aencodes%20measurements%2C%20timestamps%20and%20channel-specific%20features%20directly%20in%20the%0Anodes%20via%20learnable%20embeddings.%20Through%20an%20attention%20mechanism%2C%20these%0Aembeddings%20allow%20for%20discovering%20expressive%20relationships%20among%20the%20variables%0Aof%20interest%20in%20the%20form%20of%20a%20homogeneous%20graph.%20Once%20trained%2C%20AGG%20performs%0Aimputation%20by%20%5Cemph%7Bconditional%20attention%20generation%7D%2C%20i.e.%2C%20by%20creating%20a%20new%0Anode%20conditioned%20on%20given%20timestamps%20and%20channel%20specification.%20The%20proposed%0AAGG%20is%20compared%20to%20related%20methods%20in%20the%20literature%20and%20its%20performance%20is%0Aanalysed%20from%20a%20data%20augmentation%20perspective.%20Our%20experiments%20reveal%20that%20AGG%0Aachieved%20state-of-the-art%20results%20in%20time%20series%20imputation%2C%20classification%20and%0Aprediction%20for%20the%20benchmark%20datasets%20%5Cemph%7BBeijing%20Air%20Quality%7D%2C%0A%5Cemph%7BPhysioNet%20ICU%202012%7D%20and%20%5Cemph%7BUCI%20localisation%7D%2C%20outperforming%20other%0Arecent%20attention-based%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Graph%2520Generator%26entry.906535625%3DChristopher%2520P.%2520Ley%2520and%2520Felipe%2520Tobar%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520asynchronous%2520graph%2520generator%2520%2528AGG%2529%252C%2520a%2520novel%2520graph%2520attention%250Anetwork%2520for%2520imputation%2520and%2520prediction%2520of%2520multi-channel%2520time%2520series.%2520Free%2520from%250Arecurrent%2520components%2520or%2520assumptions%2520about%2520temporal/spatial%2520regularity%252C%2520AGG%250Aencodes%2520measurements%252C%2520timestamps%2520and%2520channel-specific%2520features%2520directly%2520in%2520the%250Anodes%2520via%2520learnable%2520embeddings.%2520Through%2520an%2520attention%2520mechanism%252C%2520these%250Aembeddings%2520allow%2520for%2520discovering%2520expressive%2520relationships%2520among%2520the%2520variables%250Aof%2520interest%2520in%2520the%2520form%2520of%2520a%2520homogeneous%2520graph.%2520Once%2520trained%252C%2520AGG%2520performs%250Aimputation%2520by%2520%255Cemph%257Bconditional%2520attention%2520generation%257D%252C%2520i.e.%252C%2520by%2520creating%2520a%2520new%250Anode%2520conditioned%2520on%2520given%2520timestamps%2520and%2520channel%2520specification.%2520The%2520proposed%250AAGG%2520is%2520compared%2520to%2520related%2520methods%2520in%2520the%2520literature%2520and%2520its%2520performance%2520is%250Aanalysed%2520from%2520a%2520data%2520augmentation%2520perspective.%2520Our%2520experiments%2520reveal%2520that%2520AGG%250Aachieved%2520state-of-the-art%2520results%2520in%2520time%2520series%2520imputation%252C%2520classification%2520and%250Aprediction%2520for%2520the%2520benchmark%2520datasets%2520%255Cemph%257BBeijing%2520Air%2520Quality%257D%252C%250A%255Cemph%257BPhysioNet%2520ICU%25202012%257D%2520and%2520%255Cemph%257BUCI%2520localisation%257D%252C%2520outperforming%2520other%250Arecent%2520attention-based%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Graph%20Generator&entry.906535625=Christopher%20P.%20Ley%20and%20Felipe%20Tobar&entry.1292438233=%20%20We%20introduce%20the%20asynchronous%20graph%20generator%20%28AGG%29%2C%20a%20novel%20graph%20attention%0Anetwork%20for%20imputation%20and%20prediction%20of%20multi-channel%20time%20series.%20Free%20from%0Arecurrent%20components%20or%20assumptions%20about%20temporal/spatial%20regularity%2C%20AGG%0Aencodes%20measurements%2C%20timestamps%20and%20channel-specific%20features%20directly%20in%20the%0Anodes%20via%20learnable%20embeddings.%20Through%20an%20attention%20mechanism%2C%20these%0Aembeddings%20allow%20for%20discovering%20expressive%20relationships%20among%20the%20variables%0Aof%20interest%20in%20the%20form%20of%20a%20homogeneous%20graph.%20Once%20trained%2C%20AGG%20performs%0Aimputation%20by%20%5Cemph%7Bconditional%20attention%20generation%7D%2C%20i.e.%2C%20by%20creating%20a%20new%0Anode%20conditioned%20on%20given%20timestamps%20and%20channel%20specification.%20The%20proposed%0AAGG%20is%20compared%20to%20related%20methods%20in%20the%20literature%20and%20its%20performance%20is%0Aanalysed%20from%20a%20data%20augmentation%20perspective.%20Our%20experiments%20reveal%20that%20AGG%0Aachieved%20state-of-the-art%20results%20in%20time%20series%20imputation%2C%20classification%20and%0Aprediction%20for%20the%20benchmark%20datasets%20%5Cemph%7BBeijing%20Air%20Quality%7D%2C%0A%5Cemph%7BPhysioNet%20ICU%202012%7D%20and%20%5Cemph%7BUCI%20localisation%7D%2C%20outperforming%20other%0Arecent%20attention-based%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17335v3&entry.124074799=Read"},
{"title": "Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic\n  Labeling using Foundation Models", "author": "Mohamad Al Mdfaa and Raghad Salameh and Sergey Zagoruyko and Gonzalo Ferrer", "abstract": "  In the field of robotics and computer vision, efficient and accurate semantic\nmapping remains a significant challenge due to the growing demand for\nintelligent machines that can comprehend and interact with complex\nenvironments. Conventional panoptic mapping methods, however, are limited by\npredefined semantic classes, thus making them ineffective for handling novel or\nunforeseen objects. In response to this limitation, we introduce the Unified\nPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in\nfoundation models to enable real-time, on-demand label generation using natural\nlanguage prompts. By incorporating a dynamic labeling strategy into traditional\npanoptic mapping techniques, UPPM provides significant improvements in\nadaptability and versatility while maintaining high performance levels in map\nreconstruction. We demonstrate our approach on real-world and simulated\ndatasets. Results show that UPPM can accurately reconstruct scenes and segment\nobjects while generating rich semantic labels through natural language\ninteractions. A series of ablation experiments validated the advantages of\nfoundation model-based labeling over fixed label sets.\n", "link": "http://arxiv.org/abs/2405.02162v3", "date": "2024-10-10", "relevancy": 2.5822, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6948}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&body=Title%3A%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models%0AAuthor%3A%20Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20In%20the%20field%20of%20robotics%20and%20computer%20vision%2C%20efficient%20and%20accurate%20semantic%0Amapping%20remains%20a%20significant%20challenge%20due%20to%20the%20growing%20demand%20for%0Aintelligent%20machines%20that%20can%20comprehend%20and%20interact%20with%20complex%0Aenvironments.%20Conventional%20panoptic%20mapping%20methods%2C%20however%2C%20are%20limited%20by%0Apredefined%20semantic%20classes%2C%20thus%20making%20them%20ineffective%20for%20handling%20novel%20or%0Aunforeseen%20objects.%20In%20response%20to%20this%20limitation%2C%20we%20introduce%20the%20Unified%0APromptable%20Panoptic%20Mapping%20%28UPPM%29%20method.%20UPPM%20utilizes%20recent%20advances%20in%0Afoundation%20models%20to%20enable%20real-time%2C%20on-demand%20label%20generation%20using%20natural%0Alanguage%20prompts.%20By%20incorporating%20a%20dynamic%20labeling%20strategy%20into%20traditional%0Apanoptic%20mapping%20techniques%2C%20UPPM%20provides%20significant%20improvements%20in%0Aadaptability%20and%20versatility%20while%20maintaining%20high%20performance%20levels%20in%20map%0Areconstruction.%20We%20demonstrate%20our%20approach%20on%20real-world%20and%20simulated%0Adatasets.%20Results%20show%20that%20UPPM%20can%20accurately%20reconstruct%20scenes%20and%20segment%0Aobjects%20while%20generating%20rich%20semantic%20labels%20through%20natural%20language%0Ainteractions.%20A%20series%20of%20ablation%20experiments%20validated%20the%20advantages%20of%0Afoundation%20model-based%20labeling%20over%20fixed%20label%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02162v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Unseen%253A%2520Unified%2520Promptable%2520Panoptic%2520Mapping%2520with%2520Dynamic%250A%2520%2520Labeling%2520using%2520Foundation%2520Models%26entry.906535625%3DMohamad%2520Al%2520Mdfaa%2520and%2520Raghad%2520Salameh%2520and%2520Sergey%2520Zagoruyko%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520robotics%2520and%2520computer%2520vision%252C%2520efficient%2520and%2520accurate%2520semantic%250Amapping%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520growing%2520demand%2520for%250Aintelligent%2520machines%2520that%2520can%2520comprehend%2520and%2520interact%2520with%2520complex%250Aenvironments.%2520Conventional%2520panoptic%2520mapping%2520methods%252C%2520however%252C%2520are%2520limited%2520by%250Apredefined%2520semantic%2520classes%252C%2520thus%2520making%2520them%2520ineffective%2520for%2520handling%2520novel%2520or%250Aunforeseen%2520objects.%2520In%2520response%2520to%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Unified%250APromptable%2520Panoptic%2520Mapping%2520%2528UPPM%2529%2520method.%2520UPPM%2520utilizes%2520recent%2520advances%2520in%250Afoundation%2520models%2520to%2520enable%2520real-time%252C%2520on-demand%2520label%2520generation%2520using%2520natural%250Alanguage%2520prompts.%2520By%2520incorporating%2520a%2520dynamic%2520labeling%2520strategy%2520into%2520traditional%250Apanoptic%2520mapping%2520techniques%252C%2520UPPM%2520provides%2520significant%2520improvements%2520in%250Aadaptability%2520and%2520versatility%2520while%2520maintaining%2520high%2520performance%2520levels%2520in%2520map%250Areconstruction.%2520We%2520demonstrate%2520our%2520approach%2520on%2520real-world%2520and%2520simulated%250Adatasets.%2520Results%2520show%2520that%2520UPPM%2520can%2520accurately%2520reconstruct%2520scenes%2520and%2520segment%250Aobjects%2520while%2520generating%2520rich%2520semantic%2520labels%2520through%2520natural%2520language%250Ainteractions.%2520A%2520series%2520of%2520ablation%2520experiments%2520validated%2520the%2520advantages%2520of%250Afoundation%2520model-based%2520labeling%2520over%2520fixed%2520label%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02162v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&entry.906535625=Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20In%20the%20field%20of%20robotics%20and%20computer%20vision%2C%20efficient%20and%20accurate%20semantic%0Amapping%20remains%20a%20significant%20challenge%20due%20to%20the%20growing%20demand%20for%0Aintelligent%20machines%20that%20can%20comprehend%20and%20interact%20with%20complex%0Aenvironments.%20Conventional%20panoptic%20mapping%20methods%2C%20however%2C%20are%20limited%20by%0Apredefined%20semantic%20classes%2C%20thus%20making%20them%20ineffective%20for%20handling%20novel%20or%0Aunforeseen%20objects.%20In%20response%20to%20this%20limitation%2C%20we%20introduce%20the%20Unified%0APromptable%20Panoptic%20Mapping%20%28UPPM%29%20method.%20UPPM%20utilizes%20recent%20advances%20in%0Afoundation%20models%20to%20enable%20real-time%2C%20on-demand%20label%20generation%20using%20natural%0Alanguage%20prompts.%20By%20incorporating%20a%20dynamic%20labeling%20strategy%20into%20traditional%0Apanoptic%20mapping%20techniques%2C%20UPPM%20provides%20significant%20improvements%20in%0Aadaptability%20and%20versatility%20while%20maintaining%20high%20performance%20levels%20in%20map%0Areconstruction.%20We%20demonstrate%20our%20approach%20on%20real-world%20and%20simulated%0Adatasets.%20Results%20show%20that%20UPPM%20can%20accurately%20reconstruct%20scenes%20and%20segment%0Aobjects%20while%20generating%20rich%20semantic%20labels%20through%20natural%20language%0Ainteractions.%20A%20series%20of%20ablation%20experiments%20validated%20the%20advantages%20of%0Afoundation%20model-based%20labeling%20over%20fixed%20label%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02162v3&entry.124074799=Read"},
{"title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models", "author": "Wenting Tan and Dongxiao Chen and Jieting Xue and Zihao Wang and Taijie Chen", "abstract": "  Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting.\n", "link": "http://arxiv.org/abs/2410.08068v1", "date": "2024-10-10", "relevancy": 2.5566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching-Inspired%20Integrated%20Prompting%20Framework%3A%20A%20Novel%20Approach%20for%0A%20%20Enhancing%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Teaching-Inspired%20Integrated%20Prompting%20Framework%3A%20A%20Novel%20Approach%20for%0A%20%20Enhancing%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Wenting%20Tan%20and%20Dongxiao%20Chen%20and%20Jieting%20Xue%20and%20Zihao%20Wang%20and%20Taijie%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20impressive%20performance%20across%20various%0Adomains%20but%20still%20struggle%20with%20arithmetic%20reasoning%20tasks.%20Recent%20work%20shows%0Athe%20effectiveness%20of%20prompt%20design%20methods%20in%20enhancing%20reasoning%20capabilities.%0AHowever%2C%20these%20approaches%20overlook%20crucial%20requirements%20for%20prior%20knowledge%20of%0Aspecific%20concepts%2C%20theorems%2C%20and%20tricks%20to%20tackle%20most%20arithmetic%20reasoning%0Aproblems%20successfully.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20and%20effective%0ATeaching-Inspired%20Integrated%20Framework%2C%20which%20emulates%20the%20instructional%0Aprocess%20of%20a%20teacher%20guiding%20students.%20This%20method%20equips%20LLMs%20with%20essential%0Aconcepts%2C%20relevant%20theorems%2C%20and%20similar%20problems%20with%20analogous%20solution%0Aapproaches%2C%20facilitating%20the%20enhancement%20of%20reasoning%20abilities.%20Additionally%2C%0Awe%20introduce%20two%20new%20Chinese%20datasets%2C%20MathMC%20and%20MathToF%2C%20both%20with%20detailed%0Aexplanations%20and%20answers.%20Experiments%20are%20conducted%20on%20nine%20benchmarks%20which%0Ademonstrates%20that%20our%20approach%20improves%20the%20reasoning%20accuracy%20of%20LLMs.%20With%0AGPT-4%20and%20our%20framework%2C%20we%20achieve%20new%20state-of-the-art%20performance%20on%20four%0Amath%20benchmarks%20%28AddSub%2C%20SVAMP%2C%20Math23K%20and%20AQuA%29%20with%20accuracies%20of%2098.2%25%0A%28%2B3.3%25%29%2C%2093.9%25%20%28%2B0.2%25%29%2C%2094.3%25%20%28%2B7.2%25%29%20and%2081.1%25%20%28%2B1.2%25%29.%20Our%20data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/SallyTan13/Teaching-Inspired-Prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching-Inspired%2520Integrated%2520Prompting%2520Framework%253A%2520A%2520Novel%2520Approach%2520for%250A%2520%2520Enhancing%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DWenting%2520Tan%2520and%2520Dongxiao%2520Chen%2520and%2520Jieting%2520Xue%2520and%2520Zihao%2520Wang%2520and%2520Taijie%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520impressive%2520performance%2520across%2520various%250Adomains%2520but%2520still%2520struggle%2520with%2520arithmetic%2520reasoning%2520tasks.%2520Recent%2520work%2520shows%250Athe%2520effectiveness%2520of%2520prompt%2520design%2520methods%2520in%2520enhancing%2520reasoning%2520capabilities.%250AHowever%252C%2520these%2520approaches%2520overlook%2520crucial%2520requirements%2520for%2520prior%2520knowledge%2520of%250Aspecific%2520concepts%252C%2520theorems%252C%2520and%2520tricks%2520to%2520tackle%2520most%2520arithmetic%2520reasoning%250Aproblems%2520successfully.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%250ATeaching-Inspired%2520Integrated%2520Framework%252C%2520which%2520emulates%2520the%2520instructional%250Aprocess%2520of%2520a%2520teacher%2520guiding%2520students.%2520This%2520method%2520equips%2520LLMs%2520with%2520essential%250Aconcepts%252C%2520relevant%2520theorems%252C%2520and%2520similar%2520problems%2520with%2520analogous%2520solution%250Aapproaches%252C%2520facilitating%2520the%2520enhancement%2520of%2520reasoning%2520abilities.%2520Additionally%252C%250Awe%2520introduce%2520two%2520new%2520Chinese%2520datasets%252C%2520MathMC%2520and%2520MathToF%252C%2520both%2520with%2520detailed%250Aexplanations%2520and%2520answers.%2520Experiments%2520are%2520conducted%2520on%2520nine%2520benchmarks%2520which%250Ademonstrates%2520that%2520our%2520approach%2520improves%2520the%2520reasoning%2520accuracy%2520of%2520LLMs.%2520With%250AGPT-4%2520and%2520our%2520framework%252C%2520we%2520achieve%2520new%2520state-of-the-art%2520performance%2520on%2520four%250Amath%2520benchmarks%2520%2528AddSub%252C%2520SVAMP%252C%2520Math23K%2520and%2520AQuA%2529%2520with%2520accuracies%2520of%252098.2%2525%250A%2528%252B3.3%2525%2529%252C%252093.9%2525%2520%2528%252B0.2%2525%2529%252C%252094.3%2525%2520%2528%252B7.2%2525%2529%2520and%252081.1%2525%2520%2528%252B1.2%2525%2529.%2520Our%2520data%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/SallyTan13/Teaching-Inspired-Prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching-Inspired%20Integrated%20Prompting%20Framework%3A%20A%20Novel%20Approach%20for%0A%20%20Enhancing%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Wenting%20Tan%20and%20Dongxiao%20Chen%20and%20Jieting%20Xue%20and%20Zihao%20Wang%20and%20Taijie%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20impressive%20performance%20across%20various%0Adomains%20but%20still%20struggle%20with%20arithmetic%20reasoning%20tasks.%20Recent%20work%20shows%0Athe%20effectiveness%20of%20prompt%20design%20methods%20in%20enhancing%20reasoning%20capabilities.%0AHowever%2C%20these%20approaches%20overlook%20crucial%20requirements%20for%20prior%20knowledge%20of%0Aspecific%20concepts%2C%20theorems%2C%20and%20tricks%20to%20tackle%20most%20arithmetic%20reasoning%0Aproblems%20successfully.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20and%20effective%0ATeaching-Inspired%20Integrated%20Framework%2C%20which%20emulates%20the%20instructional%0Aprocess%20of%20a%20teacher%20guiding%20students.%20This%20method%20equips%20LLMs%20with%20essential%0Aconcepts%2C%20relevant%20theorems%2C%20and%20similar%20problems%20with%20analogous%20solution%0Aapproaches%2C%20facilitating%20the%20enhancement%20of%20reasoning%20abilities.%20Additionally%2C%0Awe%20introduce%20two%20new%20Chinese%20datasets%2C%20MathMC%20and%20MathToF%2C%20both%20with%20detailed%0Aexplanations%20and%20answers.%20Experiments%20are%20conducted%20on%20nine%20benchmarks%20which%0Ademonstrates%20that%20our%20approach%20improves%20the%20reasoning%20accuracy%20of%20LLMs.%20With%0AGPT-4%20and%20our%20framework%2C%20we%20achieve%20new%20state-of-the-art%20performance%20on%20four%0Amath%20benchmarks%20%28AddSub%2C%20SVAMP%2C%20Math23K%20and%20AQuA%29%20with%20accuracies%20of%2098.2%25%0A%28%2B3.3%25%29%2C%2093.9%25%20%28%2B0.2%25%29%2C%2094.3%25%20%28%2B7.2%25%29%20and%2081.1%25%20%28%2B1.2%25%29.%20Our%20data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/SallyTan13/Teaching-Inspired-Prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08068v1&entry.124074799=Read"},
{"title": "Deep Learning-based Accelerated MR Cholangiopancreatography without\n  Fully-sampled Data", "author": "Jinho Kim and Marcel Dominik Nickel and Florian Knoll", "abstract": "  The purpose of this study was to accelerate MR cholangiopancreatography\n(MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and\n0.55T. A total of 35 healthy volunteers underwent conventional two-fold\naccelerated MRCP scans at field strengths of 3T and 0.55T. We trained DL\nreconstructions using two different training strategies, supervised (SV) and\nself-supervised (SSV), with retrospectively six-fold undersampled data obtained\nat 3T. We then evaluated the DL reconstructions against standard techniques,\nparallel imaging (PI) and compressed sensing (CS), focusing on peak\nsignal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics. We\nalso tested DL reconstructions in a prospectively accelerated scenario to\nreflect real-world clinical applications and evaluated their adaptability to\nMRCP at 0.55T. Both DL reconstructions demonstrated a remarkable reduction in\naverage acquisition time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T.\nIn both retrospective and prospective undersampling scenarios, PSNR and SSIM of\nDL reconstructions were higher than those of PI and CS. At the same time, DL\nreconstructions preserved the image quality of undersampled data, including\nsharpness and the visibility of hepatobiliary ducts. In addition, both DL\napproaches produced high-quality reconstructions at 0.55T. In summary, DL\nreconstructions trained for highly accelerated MRCP enabled a reduction in\nacquisition time by a factor of 2.4/3.0 at 3T/0.55T while maintaining the image\nquality of conventional acquisition.\n", "link": "http://arxiv.org/abs/2405.03732v2", "date": "2024-10-10", "relevancy": 2.555, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5122}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data&body=Title%3A%20Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data%0AAuthor%3A%20Jinho%20Kim%20and%20Marcel%20Dominik%20Nickel%20and%20Florian%20Knoll%0AAbstract%3A%20%20%20The%20purpose%20of%20this%20study%20was%20to%20accelerate%20MR%20cholangiopancreatography%0A%28MRCP%29%20acquisitions%20using%20deep%20learning-based%20%28DL%29%20reconstruction%20at%203T%20and%0A0.55T.%20A%20total%20of%2035%20healthy%20volunteers%20underwent%20conventional%20two-fold%0Aaccelerated%20MRCP%20scans%20at%20field%20strengths%20of%203T%20and%200.55T.%20We%20trained%20DL%0Areconstructions%20using%20two%20different%20training%20strategies%2C%20supervised%20%28SV%29%20and%0Aself-supervised%20%28SSV%29%2C%20with%20retrospectively%20six-fold%20undersampled%20data%20obtained%0Aat%203T.%20We%20then%20evaluated%20the%20DL%20reconstructions%20against%20standard%20techniques%2C%0Aparallel%20imaging%20%28PI%29%20and%20compressed%20sensing%20%28CS%29%2C%20focusing%20on%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20and%20structural%20similarity%20%28SSIM%29%20as%20metrics.%20We%0Aalso%20tested%20DL%20reconstructions%20in%20a%20prospectively%20accelerated%20scenario%20to%0Areflect%20real-world%20clinical%20applications%20and%20evaluated%20their%20adaptability%20to%0AMRCP%20at%200.55T.%20Both%20DL%20reconstructions%20demonstrated%20a%20remarkable%20reduction%20in%0Aaverage%20acquisition%20time%20from%20599/542%20to%20255/180%20seconds%20for%20MRCP%20at%203T/0.55T.%0AIn%20both%20retrospective%20and%20prospective%20undersampling%20scenarios%2C%20PSNR%20and%20SSIM%20of%0ADL%20reconstructions%20were%20higher%20than%20those%20of%20PI%20and%20CS.%20At%20the%20same%20time%2C%20DL%0Areconstructions%20preserved%20the%20image%20quality%20of%20undersampled%20data%2C%20including%0Asharpness%20and%20the%20visibility%20of%20hepatobiliary%20ducts.%20In%20addition%2C%20both%20DL%0Aapproaches%20produced%20high-quality%20reconstructions%20at%200.55T.%20In%20summary%2C%20DL%0Areconstructions%20trained%20for%20highly%20accelerated%20MRCP%20enabled%20a%20reduction%20in%0Aacquisition%20time%20by%20a%20factor%20of%202.4/3.0%20at%203T/0.55T%20while%20maintaining%20the%20image%0Aquality%20of%20conventional%20acquisition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Accelerated%2520MR%2520Cholangiopancreatography%2520without%250A%2520%2520Fully-sampled%2520Data%26entry.906535625%3DJinho%2520Kim%2520and%2520Marcel%2520Dominik%2520Nickel%2520and%2520Florian%2520Knoll%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520this%2520study%2520was%2520to%2520accelerate%2520MR%2520cholangiopancreatography%250A%2528MRCP%2529%2520acquisitions%2520using%2520deep%2520learning-based%2520%2528DL%2529%2520reconstruction%2520at%25203T%2520and%250A0.55T.%2520A%2520total%2520of%252035%2520healthy%2520volunteers%2520underwent%2520conventional%2520two-fold%250Aaccelerated%2520MRCP%2520scans%2520at%2520field%2520strengths%2520of%25203T%2520and%25200.55T.%2520We%2520trained%2520DL%250Areconstructions%2520using%2520two%2520different%2520training%2520strategies%252C%2520supervised%2520%2528SV%2529%2520and%250Aself-supervised%2520%2528SSV%2529%252C%2520with%2520retrospectively%2520six-fold%2520undersampled%2520data%2520obtained%250Aat%25203T.%2520We%2520then%2520evaluated%2520the%2520DL%2520reconstructions%2520against%2520standard%2520techniques%252C%250Aparallel%2520imaging%2520%2528PI%2529%2520and%2520compressed%2520sensing%2520%2528CS%2529%252C%2520focusing%2520on%2520peak%250Asignal-to-noise%2520ratio%2520%2528PSNR%2529%2520and%2520structural%2520similarity%2520%2528SSIM%2529%2520as%2520metrics.%2520We%250Aalso%2520tested%2520DL%2520reconstructions%2520in%2520a%2520prospectively%2520accelerated%2520scenario%2520to%250Areflect%2520real-world%2520clinical%2520applications%2520and%2520evaluated%2520their%2520adaptability%2520to%250AMRCP%2520at%25200.55T.%2520Both%2520DL%2520reconstructions%2520demonstrated%2520a%2520remarkable%2520reduction%2520in%250Aaverage%2520acquisition%2520time%2520from%2520599/542%2520to%2520255/180%2520seconds%2520for%2520MRCP%2520at%25203T/0.55T.%250AIn%2520both%2520retrospective%2520and%2520prospective%2520undersampling%2520scenarios%252C%2520PSNR%2520and%2520SSIM%2520of%250ADL%2520reconstructions%2520were%2520higher%2520than%2520those%2520of%2520PI%2520and%2520CS.%2520At%2520the%2520same%2520time%252C%2520DL%250Areconstructions%2520preserved%2520the%2520image%2520quality%2520of%2520undersampled%2520data%252C%2520including%250Asharpness%2520and%2520the%2520visibility%2520of%2520hepatobiliary%2520ducts.%2520In%2520addition%252C%2520both%2520DL%250Aapproaches%2520produced%2520high-quality%2520reconstructions%2520at%25200.55T.%2520In%2520summary%252C%2520DL%250Areconstructions%2520trained%2520for%2520highly%2520accelerated%2520MRCP%2520enabled%2520a%2520reduction%2520in%250Aacquisition%2520time%2520by%2520a%2520factor%2520of%25202.4/3.0%2520at%25203T/0.55T%2520while%2520maintaining%2520the%2520image%250Aquality%2520of%2520conventional%2520acquisition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data&entry.906535625=Jinho%20Kim%20and%20Marcel%20Dominik%20Nickel%20and%20Florian%20Knoll&entry.1292438233=%20%20The%20purpose%20of%20this%20study%20was%20to%20accelerate%20MR%20cholangiopancreatography%0A%28MRCP%29%20acquisitions%20using%20deep%20learning-based%20%28DL%29%20reconstruction%20at%203T%20and%0A0.55T.%20A%20total%20of%2035%20healthy%20volunteers%20underwent%20conventional%20two-fold%0Aaccelerated%20MRCP%20scans%20at%20field%20strengths%20of%203T%20and%200.55T.%20We%20trained%20DL%0Areconstructions%20using%20two%20different%20training%20strategies%2C%20supervised%20%28SV%29%20and%0Aself-supervised%20%28SSV%29%2C%20with%20retrospectively%20six-fold%20undersampled%20data%20obtained%0Aat%203T.%20We%20then%20evaluated%20the%20DL%20reconstructions%20against%20standard%20techniques%2C%0Aparallel%20imaging%20%28PI%29%20and%20compressed%20sensing%20%28CS%29%2C%20focusing%20on%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20and%20structural%20similarity%20%28SSIM%29%20as%20metrics.%20We%0Aalso%20tested%20DL%20reconstructions%20in%20a%20prospectively%20accelerated%20scenario%20to%0Areflect%20real-world%20clinical%20applications%20and%20evaluated%20their%20adaptability%20to%0AMRCP%20at%200.55T.%20Both%20DL%20reconstructions%20demonstrated%20a%20remarkable%20reduction%20in%0Aaverage%20acquisition%20time%20from%20599/542%20to%20255/180%20seconds%20for%20MRCP%20at%203T/0.55T.%0AIn%20both%20retrospective%20and%20prospective%20undersampling%20scenarios%2C%20PSNR%20and%20SSIM%20of%0ADL%20reconstructions%20were%20higher%20than%20those%20of%20PI%20and%20CS.%20At%20the%20same%20time%2C%20DL%0Areconstructions%20preserved%20the%20image%20quality%20of%20undersampled%20data%2C%20including%0Asharpness%20and%20the%20visibility%20of%20hepatobiliary%20ducts.%20In%20addition%2C%20both%20DL%0Aapproaches%20produced%20high-quality%20reconstructions%20at%200.55T.%20In%20summary%2C%20DL%0Areconstructions%20trained%20for%20highly%20accelerated%20MRCP%20enabled%20a%20reduction%20in%0Aacquisition%20time%20by%20a%20factor%20of%202.4/3.0%20at%203T/0.55T%20while%20maintaining%20the%20image%0Aquality%20of%20conventional%20acquisition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03732v2&entry.124074799=Read"},
{"title": "Sparse Attention Decomposition Applied to Circuit Tracing", "author": "Gabriel Franco and Mark Crovella", "abstract": "  Many papers have shown that attention heads work in conjunction with each\nother to perform complex tasks. It's frequently assumed that communication\nbetween attention heads is via the addition of specific features to token\nresiduals. In this work we seek to isolate and identify the features used to\neffect communication and coordination among attention heads in GPT-2 small. Our\nkey leverage on the problem is to show that these features are very often\nsparsely coded in the singular vectors of attention head matrices. We\ncharacterize the dimensionality and occurrence of these signals across the\nattention heads in GPT-2 small when used for the Indirect Object Identification\n(IOI) task. The sparse encoding of signals, as provided by attention head\nsingular vectors, allows for efficient separation of signals from the residual\nbackground and straightforward identification of communication paths between\nattention heads. We explore the effectiveness of this approach by tracing\nportions of the circuits used in the IOI task. Our traces reveal considerable\ndetail not present in previous studies, shedding light on the nature of\nredundant paths present in GPT-2. And our traces go beyond previous work by\nidentifying features used to communicate between attention heads when\nperforming IOI.\n", "link": "http://arxiv.org/abs/2410.00340v2", "date": "2024-10-10", "relevancy": 2.5518, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5651}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4858}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Attention%20Decomposition%20Applied%20to%20Circuit%20Tracing&body=Title%3A%20Sparse%20Attention%20Decomposition%20Applied%20to%20Circuit%20Tracing%0AAuthor%3A%20Gabriel%20Franco%20and%20Mark%20Crovella%0AAbstract%3A%20%20%20Many%20papers%20have%20shown%20that%20attention%20heads%20work%20in%20conjunction%20with%20each%0Aother%20to%20perform%20complex%20tasks.%20It%27s%20frequently%20assumed%20that%20communication%0Abetween%20attention%20heads%20is%20via%20the%20addition%20of%20specific%20features%20to%20token%0Aresiduals.%20In%20this%20work%20we%20seek%20to%20isolate%20and%20identify%20the%20features%20used%20to%0Aeffect%20communication%20and%20coordination%20among%20attention%20heads%20in%20GPT-2%20small.%20Our%0Akey%20leverage%20on%20the%20problem%20is%20to%20show%20that%20these%20features%20are%20very%20often%0Asparsely%20coded%20in%20the%20singular%20vectors%20of%20attention%20head%20matrices.%20We%0Acharacterize%20the%20dimensionality%20and%20occurrence%20of%20these%20signals%20across%20the%0Aattention%20heads%20in%20GPT-2%20small%20when%20used%20for%20the%20Indirect%20Object%20Identification%0A%28IOI%29%20task.%20The%20sparse%20encoding%20of%20signals%2C%20as%20provided%20by%20attention%20head%0Asingular%20vectors%2C%20allows%20for%20efficient%20separation%20of%20signals%20from%20the%20residual%0Abackground%20and%20straightforward%20identification%20of%20communication%20paths%20between%0Aattention%20heads.%20We%20explore%20the%20effectiveness%20of%20this%20approach%20by%20tracing%0Aportions%20of%20the%20circuits%20used%20in%20the%20IOI%20task.%20Our%20traces%20reveal%20considerable%0Adetail%20not%20present%20in%20previous%20studies%2C%20shedding%20light%20on%20the%20nature%20of%0Aredundant%20paths%20present%20in%20GPT-2.%20And%20our%20traces%20go%20beyond%20previous%20work%20by%0Aidentifying%20features%20used%20to%20communicate%20between%20attention%20heads%20when%0Aperforming%20IOI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Attention%2520Decomposition%2520Applied%2520to%2520Circuit%2520Tracing%26entry.906535625%3DGabriel%2520Franco%2520and%2520Mark%2520Crovella%26entry.1292438233%3D%2520%2520Many%2520papers%2520have%2520shown%2520that%2520attention%2520heads%2520work%2520in%2520conjunction%2520with%2520each%250Aother%2520to%2520perform%2520complex%2520tasks.%2520It%2527s%2520frequently%2520assumed%2520that%2520communication%250Abetween%2520attention%2520heads%2520is%2520via%2520the%2520addition%2520of%2520specific%2520features%2520to%2520token%250Aresiduals.%2520In%2520this%2520work%2520we%2520seek%2520to%2520isolate%2520and%2520identify%2520the%2520features%2520used%2520to%250Aeffect%2520communication%2520and%2520coordination%2520among%2520attention%2520heads%2520in%2520GPT-2%2520small.%2520Our%250Akey%2520leverage%2520on%2520the%2520problem%2520is%2520to%2520show%2520that%2520these%2520features%2520are%2520very%2520often%250Asparsely%2520coded%2520in%2520the%2520singular%2520vectors%2520of%2520attention%2520head%2520matrices.%2520We%250Acharacterize%2520the%2520dimensionality%2520and%2520occurrence%2520of%2520these%2520signals%2520across%2520the%250Aattention%2520heads%2520in%2520GPT-2%2520small%2520when%2520used%2520for%2520the%2520Indirect%2520Object%2520Identification%250A%2528IOI%2529%2520task.%2520The%2520sparse%2520encoding%2520of%2520signals%252C%2520as%2520provided%2520by%2520attention%2520head%250Asingular%2520vectors%252C%2520allows%2520for%2520efficient%2520separation%2520of%2520signals%2520from%2520the%2520residual%250Abackground%2520and%2520straightforward%2520identification%2520of%2520communication%2520paths%2520between%250Aattention%2520heads.%2520We%2520explore%2520the%2520effectiveness%2520of%2520this%2520approach%2520by%2520tracing%250Aportions%2520of%2520the%2520circuits%2520used%2520in%2520the%2520IOI%2520task.%2520Our%2520traces%2520reveal%2520considerable%250Adetail%2520not%2520present%2520in%2520previous%2520studies%252C%2520shedding%2520light%2520on%2520the%2520nature%2520of%250Aredundant%2520paths%2520present%2520in%2520GPT-2.%2520And%2520our%2520traces%2520go%2520beyond%2520previous%2520work%2520by%250Aidentifying%2520features%2520used%2520to%2520communicate%2520between%2520attention%2520heads%2520when%250Aperforming%2520IOI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Attention%20Decomposition%20Applied%20to%20Circuit%20Tracing&entry.906535625=Gabriel%20Franco%20and%20Mark%20Crovella&entry.1292438233=%20%20Many%20papers%20have%20shown%20that%20attention%20heads%20work%20in%20conjunction%20with%20each%0Aother%20to%20perform%20complex%20tasks.%20It%27s%20frequently%20assumed%20that%20communication%0Abetween%20attention%20heads%20is%20via%20the%20addition%20of%20specific%20features%20to%20token%0Aresiduals.%20In%20this%20work%20we%20seek%20to%20isolate%20and%20identify%20the%20features%20used%20to%0Aeffect%20communication%20and%20coordination%20among%20attention%20heads%20in%20GPT-2%20small.%20Our%0Akey%20leverage%20on%20the%20problem%20is%20to%20show%20that%20these%20features%20are%20very%20often%0Asparsely%20coded%20in%20the%20singular%20vectors%20of%20attention%20head%20matrices.%20We%0Acharacterize%20the%20dimensionality%20and%20occurrence%20of%20these%20signals%20across%20the%0Aattention%20heads%20in%20GPT-2%20small%20when%20used%20for%20the%20Indirect%20Object%20Identification%0A%28IOI%29%20task.%20The%20sparse%20encoding%20of%20signals%2C%20as%20provided%20by%20attention%20head%0Asingular%20vectors%2C%20allows%20for%20efficient%20separation%20of%20signals%20from%20the%20residual%0Abackground%20and%20straightforward%20identification%20of%20communication%20paths%20between%0Aattention%20heads.%20We%20explore%20the%20effectiveness%20of%20this%20approach%20by%20tracing%0Aportions%20of%20the%20circuits%20used%20in%20the%20IOI%20task.%20Our%20traces%20reveal%20considerable%0Adetail%20not%20present%20in%20previous%20studies%2C%20shedding%20light%20on%20the%20nature%20of%0Aredundant%20paths%20present%20in%20GPT-2.%20And%20our%20traces%20go%20beyond%20previous%20work%20by%0Aidentifying%20features%20used%20to%20communicate%20between%20attention%20heads%20when%0Aperforming%20IOI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00340v2&entry.124074799=Read"},
{"title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning", "author": "Yuan Sui and Jiaru Zou and Mengyu Zhou and Xinyi He and Lun Du and Shi Han and Dongmei Zhang", "abstract": "  Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing.\n", "link": "http://arxiv.org/abs/2312.09039v3", "date": "2024-10-10", "relevancy": 2.5371, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAP4LLM%3A%20Table%20Provider%20on%20Sampling%2C%20Augmenting%2C%20and%20Packing%0A%20%20Semi-structured%20Data%20for%20Large%20Language%20Model%20Reasoning&body=Title%3A%20TAP4LLM%3A%20Table%20Provider%20on%20Sampling%2C%20Augmenting%2C%20and%20Packing%0A%20%20Semi-structured%20Data%20for%20Large%20Language%20Model%20Reasoning%0AAuthor%3A%20Yuan%20Sui%20and%20Jiaru%20Zou%20and%20Mengyu%20Zhou%20and%20Xinyi%20He%20and%20Lun%20Du%20and%20Shi%20Han%20and%20Dongmei%20Zhang%0AAbstract%3A%20%20%20Table%20reasoning%20tasks%20have%20shown%20remarkable%20progress%20with%20the%20development%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20which%20involve%20interpreting%20and%20drawing%0Aconclusions%20from%20tabular%20data%20based%20on%20natural%20language%20%28NL%29%20questions.%0AExisting%20solutions%20mainly%20tested%20on%20smaller%20tables%20face%20scalability%20issues%20and%0Astruggle%20with%20complex%20queries%20due%20to%20incomplete%20or%20dispersed%20data%20across%0Adifferent%20table%20sections.%20To%20alleviate%20these%20challenges%2C%20we%20propose%20TAP4LLM%20as%0Aa%20versatile%20pre-processor%20suite%20for%20leveraging%20LLMs%20in%20table-based%20tasks%0Aeffectively.%20It%20covers%20several%20distinct%20components%3A%20%281%29%20table%20sampling%20to%0Adecompose%20large%20tables%20into%20manageable%20sub-tables%20based%20on%20query%20semantics%2C%20%282%29%0Atable%20augmentation%20to%20enhance%20tables%20with%20additional%20knowledge%20from%20external%0Asources%20or%20models%2C%20and%20%283%29%20table%20packing%20%26%20serialization%20to%20convert%20tables%20into%0Avarious%20formats%20suitable%20for%20LLMs%27%20understanding.%20In%20each%20module%2C%20we%20design%20and%0Acompare%20several%20common%20methods%20under%20various%20usage%20scenarios%2C%20aiming%20to%20shed%0Alight%20on%20the%20best%20practices%20for%20leveraging%20LLMs%20for%20table-reasoning%20tasks.%20Our%0Aexperiments%20show%20that%20our%20method%20improves%20LLMs%27%20reasoning%20capabilities%20in%0Avarious%20tabular%20tasks%20and%20enhances%20the%20interaction%20between%20LLMs%20and%20tabular%0Adata%20by%20employing%20effective%20pre-processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAP4LLM%253A%2520Table%2520Provider%2520on%2520Sampling%252C%2520Augmenting%252C%2520and%2520Packing%250A%2520%2520Semi-structured%2520Data%2520for%2520Large%2520Language%2520Model%2520Reasoning%26entry.906535625%3DYuan%2520Sui%2520and%2520Jiaru%2520Zou%2520and%2520Mengyu%2520Zhou%2520and%2520Xinyi%2520He%2520and%2520Lun%2520Du%2520and%2520Shi%2520Han%2520and%2520Dongmei%2520Zhang%26entry.1292438233%3D%2520%2520Table%2520reasoning%2520tasks%2520have%2520shown%2520remarkable%2520progress%2520with%2520the%2520development%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520involve%2520interpreting%2520and%2520drawing%250Aconclusions%2520from%2520tabular%2520data%2520based%2520on%2520natural%2520language%2520%2528NL%2529%2520questions.%250AExisting%2520solutions%2520mainly%2520tested%2520on%2520smaller%2520tables%2520face%2520scalability%2520issues%2520and%250Astruggle%2520with%2520complex%2520queries%2520due%2520to%2520incomplete%2520or%2520dispersed%2520data%2520across%250Adifferent%2520table%2520sections.%2520To%2520alleviate%2520these%2520challenges%252C%2520we%2520propose%2520TAP4LLM%2520as%250Aa%2520versatile%2520pre-processor%2520suite%2520for%2520leveraging%2520LLMs%2520in%2520table-based%2520tasks%250Aeffectively.%2520It%2520covers%2520several%2520distinct%2520components%253A%2520%25281%2529%2520table%2520sampling%2520to%250Adecompose%2520large%2520tables%2520into%2520manageable%2520sub-tables%2520based%2520on%2520query%2520semantics%252C%2520%25282%2529%250Atable%2520augmentation%2520to%2520enhance%2520tables%2520with%2520additional%2520knowledge%2520from%2520external%250Asources%2520or%2520models%252C%2520and%2520%25283%2529%2520table%2520packing%2520%2526%2520serialization%2520to%2520convert%2520tables%2520into%250Avarious%2520formats%2520suitable%2520for%2520LLMs%2527%2520understanding.%2520In%2520each%2520module%252C%2520we%2520design%2520and%250Acompare%2520several%2520common%2520methods%2520under%2520various%2520usage%2520scenarios%252C%2520aiming%2520to%2520shed%250Alight%2520on%2520the%2520best%2520practices%2520for%2520leveraging%2520LLMs%2520for%2520table-reasoning%2520tasks.%2520Our%250Aexperiments%2520show%2520that%2520our%2520method%2520improves%2520LLMs%2527%2520reasoning%2520capabilities%2520in%250Avarious%2520tabular%2520tasks%2520and%2520enhances%2520the%2520interaction%2520between%2520LLMs%2520and%2520tabular%250Adata%2520by%2520employing%2520effective%2520pre-processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAP4LLM%3A%20Table%20Provider%20on%20Sampling%2C%20Augmenting%2C%20and%20Packing%0A%20%20Semi-structured%20Data%20for%20Large%20Language%20Model%20Reasoning&entry.906535625=Yuan%20Sui%20and%20Jiaru%20Zou%20and%20Mengyu%20Zhou%20and%20Xinyi%20He%20and%20Lun%20Du%20and%20Shi%20Han%20and%20Dongmei%20Zhang&entry.1292438233=%20%20Table%20reasoning%20tasks%20have%20shown%20remarkable%20progress%20with%20the%20development%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20which%20involve%20interpreting%20and%20drawing%0Aconclusions%20from%20tabular%20data%20based%20on%20natural%20language%20%28NL%29%20questions.%0AExisting%20solutions%20mainly%20tested%20on%20smaller%20tables%20face%20scalability%20issues%20and%0Astruggle%20with%20complex%20queries%20due%20to%20incomplete%20or%20dispersed%20data%20across%0Adifferent%20table%20sections.%20To%20alleviate%20these%20challenges%2C%20we%20propose%20TAP4LLM%20as%0Aa%20versatile%20pre-processor%20suite%20for%20leveraging%20LLMs%20in%20table-based%20tasks%0Aeffectively.%20It%20covers%20several%20distinct%20components%3A%20%281%29%20table%20sampling%20to%0Adecompose%20large%20tables%20into%20manageable%20sub-tables%20based%20on%20query%20semantics%2C%20%282%29%0Atable%20augmentation%20to%20enhance%20tables%20with%20additional%20knowledge%20from%20external%0Asources%20or%20models%2C%20and%20%283%29%20table%20packing%20%26%20serialization%20to%20convert%20tables%20into%0Avarious%20formats%20suitable%20for%20LLMs%27%20understanding.%20In%20each%20module%2C%20we%20design%20and%0Acompare%20several%20common%20methods%20under%20various%20usage%20scenarios%2C%20aiming%20to%20shed%0Alight%20on%20the%20best%20practices%20for%20leveraging%20LLMs%20for%20table-reasoning%20tasks.%20Our%0Aexperiments%20show%20that%20our%20method%20improves%20LLMs%27%20reasoning%20capabilities%20in%0Avarious%20tabular%20tasks%20and%20enhances%20the%20interaction%20between%20LLMs%20and%20tabular%0Adata%20by%20employing%20effective%20pre-processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09039v3&entry.124074799=Read"},
{"title": "The Dawn of Video Generation: Preliminary Explorations with SORA-like\n  Models", "author": "Ailing Zeng and Yuhang Yang and Weidong Chen and Wei Liu", "abstract": "  High-quality video generation, encompassing text-to-video (T2V),\nimage-to-video (I2V), and video-to-video (V2V) generation, holds considerable\nsignificance in content creation to benefit anyone express their inherent\ncreativity in new ways and world simulation to modeling and understanding the\nworld. Models like SORA have advanced generating videos with higher resolution,\nmore natural motion, better vision-language alignment, and increased\ncontrollability, particularly for long video sequences. These improvements have\nbeen driven by the evolution of model architectures, shifting from UNet to more\nscalable and parameter-rich DiT models, along with large-scale data expansion\nand refined training strategies. However, despite the emergence of DiT-based\nclosed-source and open-source models, a comprehensive investigation into their\ncapabilities and limitations remains lacking. Furthermore, the rapid\ndevelopment has made it challenging for recent benchmarks to fully cover\nSORA-like models and recognize their significant advancements. Additionally,\nevaluation metrics often fail to align with human preferences.\n", "link": "http://arxiv.org/abs/2410.05227v2", "date": "2024-10-10", "relevancy": 2.5215, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6333}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models&body=Title%3A%20The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models%0AAuthor%3A%20Ailing%20Zeng%20and%20Yuhang%20Yang%20and%20Weidong%20Chen%20and%20Wei%20Liu%0AAbstract%3A%20%20%20High-quality%20video%20generation%2C%20encompassing%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20video-to-video%20%28V2V%29%20generation%2C%20holds%20considerable%0Asignificance%20in%20content%20creation%20to%20benefit%20anyone%20express%20their%20inherent%0Acreativity%20in%20new%20ways%20and%20world%20simulation%20to%20modeling%20and%20understanding%20the%0Aworld.%20Models%20like%20SORA%20have%20advanced%20generating%20videos%20with%20higher%20resolution%2C%0Amore%20natural%20motion%2C%20better%20vision-language%20alignment%2C%20and%20increased%0Acontrollability%2C%20particularly%20for%20long%20video%20sequences.%20These%20improvements%20have%0Abeen%20driven%20by%20the%20evolution%20of%20model%20architectures%2C%20shifting%20from%20UNet%20to%20more%0Ascalable%20and%20parameter-rich%20DiT%20models%2C%20along%20with%20large-scale%20data%20expansion%0Aand%20refined%20training%20strategies.%20However%2C%20despite%20the%20emergence%20of%20DiT-based%0Aclosed-source%20and%20open-source%20models%2C%20a%20comprehensive%20investigation%20into%20their%0Acapabilities%20and%20limitations%20remains%20lacking.%20Furthermore%2C%20the%20rapid%0Adevelopment%20has%20made%20it%20challenging%20for%20recent%20benchmarks%20to%20fully%20cover%0ASORA-like%20models%20and%20recognize%20their%20significant%20advancements.%20Additionally%2C%0Aevaluation%20metrics%20often%20fail%20to%20align%20with%20human%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dawn%2520of%2520Video%2520Generation%253A%2520Preliminary%2520Explorations%2520with%2520SORA-like%250A%2520%2520Models%26entry.906535625%3DAiling%2520Zeng%2520and%2520Yuhang%2520Yang%2520and%2520Weidong%2520Chen%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520High-quality%2520video%2520generation%252C%2520encompassing%2520text-to-video%2520%2528T2V%2529%252C%250Aimage-to-video%2520%2528I2V%2529%252C%2520and%2520video-to-video%2520%2528V2V%2529%2520generation%252C%2520holds%2520considerable%250Asignificance%2520in%2520content%2520creation%2520to%2520benefit%2520anyone%2520express%2520their%2520inherent%250Acreativity%2520in%2520new%2520ways%2520and%2520world%2520simulation%2520to%2520modeling%2520and%2520understanding%2520the%250Aworld.%2520Models%2520like%2520SORA%2520have%2520advanced%2520generating%2520videos%2520with%2520higher%2520resolution%252C%250Amore%2520natural%2520motion%252C%2520better%2520vision-language%2520alignment%252C%2520and%2520increased%250Acontrollability%252C%2520particularly%2520for%2520long%2520video%2520sequences.%2520These%2520improvements%2520have%250Abeen%2520driven%2520by%2520the%2520evolution%2520of%2520model%2520architectures%252C%2520shifting%2520from%2520UNet%2520to%2520more%250Ascalable%2520and%2520parameter-rich%2520DiT%2520models%252C%2520along%2520with%2520large-scale%2520data%2520expansion%250Aand%2520refined%2520training%2520strategies.%2520However%252C%2520despite%2520the%2520emergence%2520of%2520DiT-based%250Aclosed-source%2520and%2520open-source%2520models%252C%2520a%2520comprehensive%2520investigation%2520into%2520their%250Acapabilities%2520and%2520limitations%2520remains%2520lacking.%2520Furthermore%252C%2520the%2520rapid%250Adevelopment%2520has%2520made%2520it%2520challenging%2520for%2520recent%2520benchmarks%2520to%2520fully%2520cover%250ASORA-like%2520models%2520and%2520recognize%2520their%2520significant%2520advancements.%2520Additionally%252C%250Aevaluation%2520metrics%2520often%2520fail%2520to%2520align%2520with%2520human%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models&entry.906535625=Ailing%20Zeng%20and%20Yuhang%20Yang%20and%20Weidong%20Chen%20and%20Wei%20Liu&entry.1292438233=%20%20High-quality%20video%20generation%2C%20encompassing%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20video-to-video%20%28V2V%29%20generation%2C%20holds%20considerable%0Asignificance%20in%20content%20creation%20to%20benefit%20anyone%20express%20their%20inherent%0Acreativity%20in%20new%20ways%20and%20world%20simulation%20to%20modeling%20and%20understanding%20the%0Aworld.%20Models%20like%20SORA%20have%20advanced%20generating%20videos%20with%20higher%20resolution%2C%0Amore%20natural%20motion%2C%20better%20vision-language%20alignment%2C%20and%20increased%0Acontrollability%2C%20particularly%20for%20long%20video%20sequences.%20These%20improvements%20have%0Abeen%20driven%20by%20the%20evolution%20of%20model%20architectures%2C%20shifting%20from%20UNet%20to%20more%0Ascalable%20and%20parameter-rich%20DiT%20models%2C%20along%20with%20large-scale%20data%20expansion%0Aand%20refined%20training%20strategies.%20However%2C%20despite%20the%20emergence%20of%20DiT-based%0Aclosed-source%20and%20open-source%20models%2C%20a%20comprehensive%20investigation%20into%20their%0Acapabilities%20and%20limitations%20remains%20lacking.%20Furthermore%2C%20the%20rapid%0Adevelopment%20has%20made%20it%20challenging%20for%20recent%20benchmarks%20to%20fully%20cover%0ASORA-like%20models%20and%20recognize%20their%20significant%20advancements.%20Additionally%2C%0Aevaluation%20metrics%20often%20fail%20to%20align%20with%20human%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05227v2&entry.124074799=Read"},
{"title": "Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection", "author": "Moirangthem Tiken Singh and Rabinder Kumar Prasad and Gurumayum Robert Michael and N K Kaphungkui and N. Hemarjit Singh", "abstract": "  The digital revolution has significantly impacted financial transactions,\nleading to a notable increase in credit card usage. However, this convenience\ncomes with a trade-off: a substantial rise in fraudulent activities.\nTraditional machine learning methods for fraud detection often struggle to\ncapture the inherent interconnectedness within financial data. This paper\nproposes a novel approach for credit card fraud detection that leverages Graph\nNeural Networks (GNNs) with attention mechanisms applied to heterogeneous graph\nrepresentations of financial data. Unlike homogeneous graphs, heterogeneous\ngraphs capture intricate relationships between various entities in the\nfinancial ecosystem, such as cardholders, merchants, and transactions,\nproviding a richer and more comprehensive data representation for fraud\nanalysis. To address the inherent class imbalance in fraud data, where genuine\ntransactions significantly outnumber fraudulent ones, the proposed approach\nintegrates an autoencoder. This autoencoder, trained on genuine transactions,\nlearns a latent representation and flags deviations during reconstruction as\npotential fraud. This research investigates two key questions: (1) How\neffectively can a GNN with an attention mechanism detect and prevent credit\ncard fraud when applied to a heterogeneous graph? (2) How does the efficacy of\nthe autoencoder with attention approach compare to traditional methods? The\nresults are promising, demonstrating that the proposed model outperforms\nbenchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PR\nof 0.89 and an F1-score of 0.81. This research significantly advances fraud\ndetection systems and the overall security of financial transactions by\nleveraging GNNs with attention mechanisms and addressing class imbalance\nthrough an autoencoder.\n", "link": "http://arxiv.org/abs/2410.08121v1", "date": "2024-10-10", "relevancy": 2.4858, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5163}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4952}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Graph%20Auto-Encoder%20for%20CreditCard%20Fraud%20Detection&body=Title%3A%20Heterogeneous%20Graph%20Auto-Encoder%20for%20CreditCard%20Fraud%20Detection%0AAuthor%3A%20Moirangthem%20Tiken%20Singh%20and%20Rabinder%20Kumar%20Prasad%20and%20Gurumayum%20Robert%20Michael%20and%20N%20K%20Kaphungkui%20and%20N.%20Hemarjit%20Singh%0AAbstract%3A%20%20%20The%20digital%20revolution%20has%20significantly%20impacted%20financial%20transactions%2C%0Aleading%20to%20a%20notable%20increase%20in%20credit%20card%20usage.%20However%2C%20this%20convenience%0Acomes%20with%20a%20trade-off%3A%20a%20substantial%20rise%20in%20fraudulent%20activities.%0ATraditional%20machine%20learning%20methods%20for%20fraud%20detection%20often%20struggle%20to%0Acapture%20the%20inherent%20interconnectedness%20within%20financial%20data.%20This%20paper%0Aproposes%20a%20novel%20approach%20for%20credit%20card%20fraud%20detection%20that%20leverages%20Graph%0ANeural%20Networks%20%28GNNs%29%20with%20attention%20mechanisms%20applied%20to%20heterogeneous%20graph%0Arepresentations%20of%20financial%20data.%20Unlike%20homogeneous%20graphs%2C%20heterogeneous%0Agraphs%20capture%20intricate%20relationships%20between%20various%20entities%20in%20the%0Afinancial%20ecosystem%2C%20such%20as%20cardholders%2C%20merchants%2C%20and%20transactions%2C%0Aproviding%20a%20richer%20and%20more%20comprehensive%20data%20representation%20for%20fraud%0Aanalysis.%20To%20address%20the%20inherent%20class%20imbalance%20in%20fraud%20data%2C%20where%20genuine%0Atransactions%20significantly%20outnumber%20fraudulent%20ones%2C%20the%20proposed%20approach%0Aintegrates%20an%20autoencoder.%20This%20autoencoder%2C%20trained%20on%20genuine%20transactions%2C%0Alearns%20a%20latent%20representation%20and%20flags%20deviations%20during%20reconstruction%20as%0Apotential%20fraud.%20This%20research%20investigates%20two%20key%20questions%3A%20%281%29%20How%0Aeffectively%20can%20a%20GNN%20with%20an%20attention%20mechanism%20detect%20and%20prevent%20credit%0Acard%20fraud%20when%20applied%20to%20a%20heterogeneous%20graph%3F%20%282%29%20How%20does%20the%20efficacy%20of%0Athe%20autoencoder%20with%20attention%20approach%20compare%20to%20traditional%20methods%3F%20The%0Aresults%20are%20promising%2C%20demonstrating%20that%20the%20proposed%20model%20outperforms%0Abenchmark%20algorithms%20such%20as%20Graph%20Sage%20and%20FI-GRL%2C%20achieving%20a%20superior%20AUC-PR%0Aof%200.89%20and%20an%20F1-score%20of%200.81.%20This%20research%20significantly%20advances%20fraud%0Adetection%20systems%20and%20the%20overall%20security%20of%20financial%20transactions%20by%0Aleveraging%20GNNs%20with%20attention%20mechanisms%20and%20addressing%20class%20imbalance%0Athrough%20an%20autoencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Graph%2520Auto-Encoder%2520for%2520CreditCard%2520Fraud%2520Detection%26entry.906535625%3DMoirangthem%2520Tiken%2520Singh%2520and%2520Rabinder%2520Kumar%2520Prasad%2520and%2520Gurumayum%2520Robert%2520Michael%2520and%2520N%2520K%2520Kaphungkui%2520and%2520N.%2520Hemarjit%2520Singh%26entry.1292438233%3D%2520%2520The%2520digital%2520revolution%2520has%2520significantly%2520impacted%2520financial%2520transactions%252C%250Aleading%2520to%2520a%2520notable%2520increase%2520in%2520credit%2520card%2520usage.%2520However%252C%2520this%2520convenience%250Acomes%2520with%2520a%2520trade-off%253A%2520a%2520substantial%2520rise%2520in%2520fraudulent%2520activities.%250ATraditional%2520machine%2520learning%2520methods%2520for%2520fraud%2520detection%2520often%2520struggle%2520to%250Acapture%2520the%2520inherent%2520interconnectedness%2520within%2520financial%2520data.%2520This%2520paper%250Aproposes%2520a%2520novel%2520approach%2520for%2520credit%2520card%2520fraud%2520detection%2520that%2520leverages%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520with%2520attention%2520mechanisms%2520applied%2520to%2520heterogeneous%2520graph%250Arepresentations%2520of%2520financial%2520data.%2520Unlike%2520homogeneous%2520graphs%252C%2520heterogeneous%250Agraphs%2520capture%2520intricate%2520relationships%2520between%2520various%2520entities%2520in%2520the%250Afinancial%2520ecosystem%252C%2520such%2520as%2520cardholders%252C%2520merchants%252C%2520and%2520transactions%252C%250Aproviding%2520a%2520richer%2520and%2520more%2520comprehensive%2520data%2520representation%2520for%2520fraud%250Aanalysis.%2520To%2520address%2520the%2520inherent%2520class%2520imbalance%2520in%2520fraud%2520data%252C%2520where%2520genuine%250Atransactions%2520significantly%2520outnumber%2520fraudulent%2520ones%252C%2520the%2520proposed%2520approach%250Aintegrates%2520an%2520autoencoder.%2520This%2520autoencoder%252C%2520trained%2520on%2520genuine%2520transactions%252C%250Alearns%2520a%2520latent%2520representation%2520and%2520flags%2520deviations%2520during%2520reconstruction%2520as%250Apotential%2520fraud.%2520This%2520research%2520investigates%2520two%2520key%2520questions%253A%2520%25281%2529%2520How%250Aeffectively%2520can%2520a%2520GNN%2520with%2520an%2520attention%2520mechanism%2520detect%2520and%2520prevent%2520credit%250Acard%2520fraud%2520when%2520applied%2520to%2520a%2520heterogeneous%2520graph%253F%2520%25282%2529%2520How%2520does%2520the%2520efficacy%2520of%250Athe%2520autoencoder%2520with%2520attention%2520approach%2520compare%2520to%2520traditional%2520methods%253F%2520The%250Aresults%2520are%2520promising%252C%2520demonstrating%2520that%2520the%2520proposed%2520model%2520outperforms%250Abenchmark%2520algorithms%2520such%2520as%2520Graph%2520Sage%2520and%2520FI-GRL%252C%2520achieving%2520a%2520superior%2520AUC-PR%250Aof%25200.89%2520and%2520an%2520F1-score%2520of%25200.81.%2520This%2520research%2520significantly%2520advances%2520fraud%250Adetection%2520systems%2520and%2520the%2520overall%2520security%2520of%2520financial%2520transactions%2520by%250Aleveraging%2520GNNs%2520with%2520attention%2520mechanisms%2520and%2520addressing%2520class%2520imbalance%250Athrough%2520an%2520autoencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Graph%20Auto-Encoder%20for%20CreditCard%20Fraud%20Detection&entry.906535625=Moirangthem%20Tiken%20Singh%20and%20Rabinder%20Kumar%20Prasad%20and%20Gurumayum%20Robert%20Michael%20and%20N%20K%20Kaphungkui%20and%20N.%20Hemarjit%20Singh&entry.1292438233=%20%20The%20digital%20revolution%20has%20significantly%20impacted%20financial%20transactions%2C%0Aleading%20to%20a%20notable%20increase%20in%20credit%20card%20usage.%20However%2C%20this%20convenience%0Acomes%20with%20a%20trade-off%3A%20a%20substantial%20rise%20in%20fraudulent%20activities.%0ATraditional%20machine%20learning%20methods%20for%20fraud%20detection%20often%20struggle%20to%0Acapture%20the%20inherent%20interconnectedness%20within%20financial%20data.%20This%20paper%0Aproposes%20a%20novel%20approach%20for%20credit%20card%20fraud%20detection%20that%20leverages%20Graph%0ANeural%20Networks%20%28GNNs%29%20with%20attention%20mechanisms%20applied%20to%20heterogeneous%20graph%0Arepresentations%20of%20financial%20data.%20Unlike%20homogeneous%20graphs%2C%20heterogeneous%0Agraphs%20capture%20intricate%20relationships%20between%20various%20entities%20in%20the%0Afinancial%20ecosystem%2C%20such%20as%20cardholders%2C%20merchants%2C%20and%20transactions%2C%0Aproviding%20a%20richer%20and%20more%20comprehensive%20data%20representation%20for%20fraud%0Aanalysis.%20To%20address%20the%20inherent%20class%20imbalance%20in%20fraud%20data%2C%20where%20genuine%0Atransactions%20significantly%20outnumber%20fraudulent%20ones%2C%20the%20proposed%20approach%0Aintegrates%20an%20autoencoder.%20This%20autoencoder%2C%20trained%20on%20genuine%20transactions%2C%0Alearns%20a%20latent%20representation%20and%20flags%20deviations%20during%20reconstruction%20as%0Apotential%20fraud.%20This%20research%20investigates%20two%20key%20questions%3A%20%281%29%20How%0Aeffectively%20can%20a%20GNN%20with%20an%20attention%20mechanism%20detect%20and%20prevent%20credit%0Acard%20fraud%20when%20applied%20to%20a%20heterogeneous%20graph%3F%20%282%29%20How%20does%20the%20efficacy%20of%0Athe%20autoencoder%20with%20attention%20approach%20compare%20to%20traditional%20methods%3F%20The%0Aresults%20are%20promising%2C%20demonstrating%20that%20the%20proposed%20model%20outperforms%0Abenchmark%20algorithms%20such%20as%20Graph%20Sage%20and%20FI-GRL%2C%20achieving%20a%20superior%20AUC-PR%0Aof%200.89%20and%20an%20F1-score%20of%200.81.%20This%20research%20significantly%20advances%20fraud%0Adetection%20systems%20and%20the%20overall%20security%20of%20financial%20transactions%20by%0Aleveraging%20GNNs%20with%20attention%20mechanisms%20and%20addressing%20class%20imbalance%0Athrough%20an%20autoencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08121v1&entry.124074799=Read"},
{"title": "Less is More: High-value Data Selection for Visual Instruction Tuning", "author": "Zikang Liu and Kun Zhou and Wayne Xin Zhao and Dawei Gao and Yaliang Li and Ji-Rong Wen", "abstract": "  Visual instruction tuning is the key to building large vision language\nmodels~(LVLMs), which can greatly improve the task generalization and solving\ncapabilities by learning a mixture of instruction data from diverse visual\ntasks. Previous work mostly collects multiple existing visual instruction\ndatasets via heuristic ways for training (even more than a million\ninstructions), which may introduce data redundancy and enlarge the training\ncost. To investigate this issue, we conduct a series of empirical studies,\nwhich reveal a significant redundancy within the visual instruction datasets,\nand show that greatly reducing the amount of instructions from several tasks\neven do not affect the performance. Based on the findings, we propose a\nhigh-value data selection approach TIVE, to eliminate redundancy within the\nvisual instruction data and reduce the training cost. In TIVE, we first\nestimate the instance influence score on its corresponding task, and the task\ndifficulty score, based on the gradient-based influence functions. Then, we\nleverage the two kinds of scores to determine the task proportion within the\nselected visual instruction subset, and select high-value instances for each\ntask, respectively. Experiments on various LVLMs show that our approach using\nonly about 15% data can achieve comparable average performance to the full-data\nfine-tuned model across eight benchmarks, even surpassing it on four of the\nbenchmarks. Our code and data will be publicly released.\n", "link": "http://arxiv.org/abs/2403.09559v4", "date": "2024-10-10", "relevancy": 2.4797, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20High-value%20Data%20Selection%20for%20Visual%20Instruction%20Tuning&body=Title%3A%20Less%20is%20More%3A%20High-value%20Data%20Selection%20for%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Zikang%20Liu%20and%20Kun%20Zhou%20and%20Wayne%20Xin%20Zhao%20and%20Dawei%20Gao%20and%20Yaliang%20Li%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Visual%20instruction%20tuning%20is%20the%20key%20to%20building%20large%20vision%20language%0Amodels~%28LVLMs%29%2C%20which%20can%20greatly%20improve%20the%20task%20generalization%20and%20solving%0Acapabilities%20by%20learning%20a%20mixture%20of%20instruction%20data%20from%20diverse%20visual%0Atasks.%20Previous%20work%20mostly%20collects%20multiple%20existing%20visual%20instruction%0Adatasets%20via%20heuristic%20ways%20for%20training%20%28even%20more%20than%20a%20million%0Ainstructions%29%2C%20which%20may%20introduce%20data%20redundancy%20and%20enlarge%20the%20training%0Acost.%20To%20investigate%20this%20issue%2C%20we%20conduct%20a%20series%20of%20empirical%20studies%2C%0Awhich%20reveal%20a%20significant%20redundancy%20within%20the%20visual%20instruction%20datasets%2C%0Aand%20show%20that%20greatly%20reducing%20the%20amount%20of%20instructions%20from%20several%20tasks%0Aeven%20do%20not%20affect%20the%20performance.%20Based%20on%20the%20findings%2C%20we%20propose%20a%0Ahigh-value%20data%20selection%20approach%20TIVE%2C%20to%20eliminate%20redundancy%20within%20the%0Avisual%20instruction%20data%20and%20reduce%20the%20training%20cost.%20In%20TIVE%2C%20we%20first%0Aestimate%20the%20instance%20influence%20score%20on%20its%20corresponding%20task%2C%20and%20the%20task%0Adifficulty%20score%2C%20based%20on%20the%20gradient-based%20influence%20functions.%20Then%2C%20we%0Aleverage%20the%20two%20kinds%20of%20scores%20to%20determine%20the%20task%20proportion%20within%20the%0Aselected%20visual%20instruction%20subset%2C%20and%20select%20high-value%20instances%20for%20each%0Atask%2C%20respectively.%20Experiments%20on%20various%20LVLMs%20show%20that%20our%20approach%20using%0Aonly%20about%2015%25%20data%20can%20achieve%20comparable%20average%20performance%20to%20the%20full-data%0Afine-tuned%20model%20across%20eight%20benchmarks%2C%20even%20surpassing%20it%20on%20four%20of%20the%0Abenchmarks.%20Our%20code%20and%20data%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09559v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520High-value%2520Data%2520Selection%2520for%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DZikang%2520Liu%2520and%2520Kun%2520Zhou%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Dawei%2520Gao%2520and%2520Yaliang%2520Li%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Visual%2520instruction%2520tuning%2520is%2520the%2520key%2520to%2520building%2520large%2520vision%2520language%250Amodels~%2528LVLMs%2529%252C%2520which%2520can%2520greatly%2520improve%2520the%2520task%2520generalization%2520and%2520solving%250Acapabilities%2520by%2520learning%2520a%2520mixture%2520of%2520instruction%2520data%2520from%2520diverse%2520visual%250Atasks.%2520Previous%2520work%2520mostly%2520collects%2520multiple%2520existing%2520visual%2520instruction%250Adatasets%2520via%2520heuristic%2520ways%2520for%2520training%2520%2528even%2520more%2520than%2520a%2520million%250Ainstructions%2529%252C%2520which%2520may%2520introduce%2520data%2520redundancy%2520and%2520enlarge%2520the%2520training%250Acost.%2520To%2520investigate%2520this%2520issue%252C%2520we%2520conduct%2520a%2520series%2520of%2520empirical%2520studies%252C%250Awhich%2520reveal%2520a%2520significant%2520redundancy%2520within%2520the%2520visual%2520instruction%2520datasets%252C%250Aand%2520show%2520that%2520greatly%2520reducing%2520the%2520amount%2520of%2520instructions%2520from%2520several%2520tasks%250Aeven%2520do%2520not%2520affect%2520the%2520performance.%2520Based%2520on%2520the%2520findings%252C%2520we%2520propose%2520a%250Ahigh-value%2520data%2520selection%2520approach%2520TIVE%252C%2520to%2520eliminate%2520redundancy%2520within%2520the%250Avisual%2520instruction%2520data%2520and%2520reduce%2520the%2520training%2520cost.%2520In%2520TIVE%252C%2520we%2520first%250Aestimate%2520the%2520instance%2520influence%2520score%2520on%2520its%2520corresponding%2520task%252C%2520and%2520the%2520task%250Adifficulty%2520score%252C%2520based%2520on%2520the%2520gradient-based%2520influence%2520functions.%2520Then%252C%2520we%250Aleverage%2520the%2520two%2520kinds%2520of%2520scores%2520to%2520determine%2520the%2520task%2520proportion%2520within%2520the%250Aselected%2520visual%2520instruction%2520subset%252C%2520and%2520select%2520high-value%2520instances%2520for%2520each%250Atask%252C%2520respectively.%2520Experiments%2520on%2520various%2520LVLMs%2520show%2520that%2520our%2520approach%2520using%250Aonly%2520about%252015%2525%2520data%2520can%2520achieve%2520comparable%2520average%2520performance%2520to%2520the%2520full-data%250Afine-tuned%2520model%2520across%2520eight%2520benchmarks%252C%2520even%2520surpassing%2520it%2520on%2520four%2520of%2520the%250Abenchmarks.%2520Our%2520code%2520and%2520data%2520will%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09559v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20High-value%20Data%20Selection%20for%20Visual%20Instruction%20Tuning&entry.906535625=Zikang%20Liu%20and%20Kun%20Zhou%20and%20Wayne%20Xin%20Zhao%20and%20Dawei%20Gao%20and%20Yaliang%20Li%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Visual%20instruction%20tuning%20is%20the%20key%20to%20building%20large%20vision%20language%0Amodels~%28LVLMs%29%2C%20which%20can%20greatly%20improve%20the%20task%20generalization%20and%20solving%0Acapabilities%20by%20learning%20a%20mixture%20of%20instruction%20data%20from%20diverse%20visual%0Atasks.%20Previous%20work%20mostly%20collects%20multiple%20existing%20visual%20instruction%0Adatasets%20via%20heuristic%20ways%20for%20training%20%28even%20more%20than%20a%20million%0Ainstructions%29%2C%20which%20may%20introduce%20data%20redundancy%20and%20enlarge%20the%20training%0Acost.%20To%20investigate%20this%20issue%2C%20we%20conduct%20a%20series%20of%20empirical%20studies%2C%0Awhich%20reveal%20a%20significant%20redundancy%20within%20the%20visual%20instruction%20datasets%2C%0Aand%20show%20that%20greatly%20reducing%20the%20amount%20of%20instructions%20from%20several%20tasks%0Aeven%20do%20not%20affect%20the%20performance.%20Based%20on%20the%20findings%2C%20we%20propose%20a%0Ahigh-value%20data%20selection%20approach%20TIVE%2C%20to%20eliminate%20redundancy%20within%20the%0Avisual%20instruction%20data%20and%20reduce%20the%20training%20cost.%20In%20TIVE%2C%20we%20first%0Aestimate%20the%20instance%20influence%20score%20on%20its%20corresponding%20task%2C%20and%20the%20task%0Adifficulty%20score%2C%20based%20on%20the%20gradient-based%20influence%20functions.%20Then%2C%20we%0Aleverage%20the%20two%20kinds%20of%20scores%20to%20determine%20the%20task%20proportion%20within%20the%0Aselected%20visual%20instruction%20subset%2C%20and%20select%20high-value%20instances%20for%20each%0Atask%2C%20respectively.%20Experiments%20on%20various%20LVLMs%20show%20that%20our%20approach%20using%0Aonly%20about%2015%25%20data%20can%20achieve%20comparable%20average%20performance%20to%20the%20full-data%0Afine-tuned%20model%20across%20eight%20benchmarks%2C%20even%20surpassing%20it%20on%20four%20of%20the%0Abenchmarks.%20Our%20code%20and%20data%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09559v4&entry.124074799=Read"},
{"title": "PaliGemma: A versatile 3B VLM for transfer", "author": "Lucas Beyer and Andreas Steiner and Andr\u00e9 Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bo\u0161njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai", "abstract": "  PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.\n", "link": "http://arxiv.org/abs/2407.07726v2", "date": "2024-10-10", "relevancy": 2.4754, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer&body=Title%3A%20PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer%0AAuthor%3A%20Lucas%20Beyer%20and%20Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%20and%20Xiao%20Wang%20and%20Daniel%20Salz%20and%20Maxim%20Neumann%20and%20Ibrahim%20Alabdulmohsin%20and%20Michael%20Tschannen%20and%20Emanuele%20Bugliarello%20and%20Thomas%20Unterthiner%20and%20Daniel%20Keysers%20and%20Skanda%20Koppula%20and%20Fangyu%20Liu%20and%20Adam%20Grycner%20and%20Alexey%20Gritsenko%20and%20Neil%20Houlsby%20and%20Manoj%20Kumar%20and%20Keran%20Rong%20and%20Julian%20Eisenschlos%20and%20Rishabh%20Kabra%20and%20Matthias%20Bauer%20and%20Matko%20Bo%C5%A1njak%20and%20Xi%20Chen%20and%20Matthias%20Minderer%20and%20Paul%20Voigtlaender%20and%20Ioana%20Bica%20and%20Ivana%20Balazevic%20and%20Joan%20Puigcerver%20and%20Pinelopi%20Papalampidi%20and%20Olivier%20Henaff%20and%20Xi%20Xiong%20and%20Radu%20Soricut%20and%20Jeremiah%20Harmsen%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20PaliGemma%20is%20an%20open%20Vision-Language%20Model%20%28VLM%29%20that%20is%20based%20on%20the%0ASigLIP-So400m%20vision%20encoder%20and%20the%20Gemma-2B%20language%20model.%20It%20is%20trained%20to%0Abe%20a%20versatile%20and%20broadly%20knowledgeable%20base%20model%20that%20is%20effective%20to%0Atransfer.%20It%20achieves%20strong%20performance%20on%20a%20wide%20variety%20of%20open-world%20tasks.%0AWe%20evaluate%20PaliGemma%20on%20almost%2040%20diverse%20tasks%20including%20standard%20VLM%0Abenchmarks%2C%20but%20also%20more%20specialized%20tasks%20such%20as%20remote-sensing%20and%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaliGemma%253A%2520A%2520versatile%25203B%2520VLM%2520for%2520transfer%26entry.906535625%3DLucas%2520Beyer%2520and%2520Andreas%2520Steiner%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Alexander%2520Kolesnikov%2520and%2520Xiao%2520Wang%2520and%2520Daniel%2520Salz%2520and%2520Maxim%2520Neumann%2520and%2520Ibrahim%2520Alabdulmohsin%2520and%2520Michael%2520Tschannen%2520and%2520Emanuele%2520Bugliarello%2520and%2520Thomas%2520Unterthiner%2520and%2520Daniel%2520Keysers%2520and%2520Skanda%2520Koppula%2520and%2520Fangyu%2520Liu%2520and%2520Adam%2520Grycner%2520and%2520Alexey%2520Gritsenko%2520and%2520Neil%2520Houlsby%2520and%2520Manoj%2520Kumar%2520and%2520Keran%2520Rong%2520and%2520Julian%2520Eisenschlos%2520and%2520Rishabh%2520Kabra%2520and%2520Matthias%2520Bauer%2520and%2520Matko%2520Bo%25C5%25A1njak%2520and%2520Xi%2520Chen%2520and%2520Matthias%2520Minderer%2520and%2520Paul%2520Voigtlaender%2520and%2520Ioana%2520Bica%2520and%2520Ivana%2520Balazevic%2520and%2520Joan%2520Puigcerver%2520and%2520Pinelopi%2520Papalampidi%2520and%2520Olivier%2520Henaff%2520and%2520Xi%2520Xiong%2520and%2520Radu%2520Soricut%2520and%2520Jeremiah%2520Harmsen%2520and%2520Xiaohua%2520Zhai%26entry.1292438233%3D%2520%2520PaliGemma%2520is%2520an%2520open%2520Vision-Language%2520Model%2520%2528VLM%2529%2520that%2520is%2520based%2520on%2520the%250ASigLIP-So400m%2520vision%2520encoder%2520and%2520the%2520Gemma-2B%2520language%2520model.%2520It%2520is%2520trained%2520to%250Abe%2520a%2520versatile%2520and%2520broadly%2520knowledgeable%2520base%2520model%2520that%2520is%2520effective%2520to%250Atransfer.%2520It%2520achieves%2520strong%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520open-world%2520tasks.%250AWe%2520evaluate%2520PaliGemma%2520on%2520almost%252040%2520diverse%2520tasks%2520including%2520standard%2520VLM%250Abenchmarks%252C%2520but%2520also%2520more%2520specialized%2520tasks%2520such%2520as%2520remote-sensing%2520and%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaliGemma%3A%20A%20versatile%203B%20VLM%20for%20transfer&entry.906535625=Lucas%20Beyer%20and%20Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%20and%20Xiao%20Wang%20and%20Daniel%20Salz%20and%20Maxim%20Neumann%20and%20Ibrahim%20Alabdulmohsin%20and%20Michael%20Tschannen%20and%20Emanuele%20Bugliarello%20and%20Thomas%20Unterthiner%20and%20Daniel%20Keysers%20and%20Skanda%20Koppula%20and%20Fangyu%20Liu%20and%20Adam%20Grycner%20and%20Alexey%20Gritsenko%20and%20Neil%20Houlsby%20and%20Manoj%20Kumar%20and%20Keran%20Rong%20and%20Julian%20Eisenschlos%20and%20Rishabh%20Kabra%20and%20Matthias%20Bauer%20and%20Matko%20Bo%C5%A1njak%20and%20Xi%20Chen%20and%20Matthias%20Minderer%20and%20Paul%20Voigtlaender%20and%20Ioana%20Bica%20and%20Ivana%20Balazevic%20and%20Joan%20Puigcerver%20and%20Pinelopi%20Papalampidi%20and%20Olivier%20Henaff%20and%20Xi%20Xiong%20and%20Radu%20Soricut%20and%20Jeremiah%20Harmsen%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20PaliGemma%20is%20an%20open%20Vision-Language%20Model%20%28VLM%29%20that%20is%20based%20on%20the%0ASigLIP-So400m%20vision%20encoder%20and%20the%20Gemma-2B%20language%20model.%20It%20is%20trained%20to%0Abe%20a%20versatile%20and%20broadly%20knowledgeable%20base%20model%20that%20is%20effective%20to%0Atransfer.%20It%20achieves%20strong%20performance%20on%20a%20wide%20variety%20of%20open-world%20tasks.%0AWe%20evaluate%20PaliGemma%20on%20almost%2040%20diverse%20tasks%20including%20standard%20VLM%0Abenchmarks%2C%20but%20also%20more%20specialized%20tasks%20such%20as%20remote-sensing%20and%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07726v2&entry.124074799=Read"},
{"title": "MOMENT: A Family of Open Time-series Foundation Models", "author": "Mononito Goswami and Konrad Szafer and Arjun Choudhry and Yifu Cai and Shuo Li and Artur Dubrawski", "abstract": "  We introduce MOMENT, a family of open-source foundation models for\ngeneral-purpose time series analysis. Pre-training large models on time series\ndata is challenging due to (1) the absence of a large and cohesive public time\nseries repository, and (2) diverse time series characteristics which make\nmulti-dataset training onerous. Additionally, (3) experimental benchmarks to\nevaluate these models, especially in scenarios with limited resources, time,\nand supervision, are still in their nascent stages. To address these\nchallenges, we compile a large and diverse collection of public time series,\ncalled the Time series Pile, and systematically tackle time series-specific\nchallenges to unlock large-scale multi-dataset pre-training. Finally, we build\non recent work to design a benchmark to evaluate time series foundation models\non diverse tasks and datasets in limited supervision settings. Experiments on\nthis benchmark demonstrate the effectiveness of our pre-trained models with\nminimal data and task-specific fine-tuning. Finally, we present several\ninteresting empirical observations about large pre-trained time series models.\nPre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile\n(AutonLab/Timeseries-PILE) are available on Huggingface.\n", "link": "http://arxiv.org/abs/2402.03885v3", "date": "2024-10-10", "relevancy": 2.4282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOMENT%3A%20A%20Family%20of%20Open%20Time-series%20Foundation%20Models&body=Title%3A%20MOMENT%3A%20A%20Family%20of%20Open%20Time-series%20Foundation%20Models%0AAuthor%3A%20Mononito%20Goswami%20and%20Konrad%20Szafer%20and%20Arjun%20Choudhry%20and%20Yifu%20Cai%20and%20Shuo%20Li%20and%20Artur%20Dubrawski%0AAbstract%3A%20%20%20We%20introduce%20MOMENT%2C%20a%20family%20of%20open-source%20foundation%20models%20for%0Ageneral-purpose%20time%20series%20analysis.%20Pre-training%20large%20models%20on%20time%20series%0Adata%20is%20challenging%20due%20to%20%281%29%20the%20absence%20of%20a%20large%20and%20cohesive%20public%20time%0Aseries%20repository%2C%20and%20%282%29%20diverse%20time%20series%20characteristics%20which%20make%0Amulti-dataset%20training%20onerous.%20Additionally%2C%20%283%29%20experimental%20benchmarks%20to%0Aevaluate%20these%20models%2C%20especially%20in%20scenarios%20with%20limited%20resources%2C%20time%2C%0Aand%20supervision%2C%20are%20still%20in%20their%20nascent%20stages.%20To%20address%20these%0Achallenges%2C%20we%20compile%20a%20large%20and%20diverse%20collection%20of%20public%20time%20series%2C%0Acalled%20the%20Time%20series%20Pile%2C%20and%20systematically%20tackle%20time%20series-specific%0Achallenges%20to%20unlock%20large-scale%20multi-dataset%20pre-training.%20Finally%2C%20we%20build%0Aon%20recent%20work%20to%20design%20a%20benchmark%20to%20evaluate%20time%20series%20foundation%20models%0Aon%20diverse%20tasks%20and%20datasets%20in%20limited%20supervision%20settings.%20Experiments%20on%0Athis%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20pre-trained%20models%20with%0Aminimal%20data%20and%20task-specific%20fine-tuning.%20Finally%2C%20we%20present%20several%0Ainteresting%20empirical%20observations%20about%20large%20pre-trained%20time%20series%20models.%0APre-trained%20models%20%28AutonLab/MOMENT-1-large%29%20and%20Time%20Series%20Pile%0A%28AutonLab/Timeseries-PILE%29%20are%20available%20on%20Huggingface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOMENT%253A%2520A%2520Family%2520of%2520Open%2520Time-series%2520Foundation%2520Models%26entry.906535625%3DMononito%2520Goswami%2520and%2520Konrad%2520Szafer%2520and%2520Arjun%2520Choudhry%2520and%2520Yifu%2520Cai%2520and%2520Shuo%2520Li%2520and%2520Artur%2520Dubrawski%26entry.1292438233%3D%2520%2520We%2520introduce%2520MOMENT%252C%2520a%2520family%2520of%2520open-source%2520foundation%2520models%2520for%250Ageneral-purpose%2520time%2520series%2520analysis.%2520Pre-training%2520large%2520models%2520on%2520time%2520series%250Adata%2520is%2520challenging%2520due%2520to%2520%25281%2529%2520the%2520absence%2520of%2520a%2520large%2520and%2520cohesive%2520public%2520time%250Aseries%2520repository%252C%2520and%2520%25282%2529%2520diverse%2520time%2520series%2520characteristics%2520which%2520make%250Amulti-dataset%2520training%2520onerous.%2520Additionally%252C%2520%25283%2529%2520experimental%2520benchmarks%2520to%250Aevaluate%2520these%2520models%252C%2520especially%2520in%2520scenarios%2520with%2520limited%2520resources%252C%2520time%252C%250Aand%2520supervision%252C%2520are%2520still%2520in%2520their%2520nascent%2520stages.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520compile%2520a%2520large%2520and%2520diverse%2520collection%2520of%2520public%2520time%2520series%252C%250Acalled%2520the%2520Time%2520series%2520Pile%252C%2520and%2520systematically%2520tackle%2520time%2520series-specific%250Achallenges%2520to%2520unlock%2520large-scale%2520multi-dataset%2520pre-training.%2520Finally%252C%2520we%2520build%250Aon%2520recent%2520work%2520to%2520design%2520a%2520benchmark%2520to%2520evaluate%2520time%2520series%2520foundation%2520models%250Aon%2520diverse%2520tasks%2520and%2520datasets%2520in%2520limited%2520supervision%2520settings.%2520Experiments%2520on%250Athis%2520benchmark%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520pre-trained%2520models%2520with%250Aminimal%2520data%2520and%2520task-specific%2520fine-tuning.%2520Finally%252C%2520we%2520present%2520several%250Ainteresting%2520empirical%2520observations%2520about%2520large%2520pre-trained%2520time%2520series%2520models.%250APre-trained%2520models%2520%2528AutonLab/MOMENT-1-large%2529%2520and%2520Time%2520Series%2520Pile%250A%2528AutonLab/Timeseries-PILE%2529%2520are%2520available%2520on%2520Huggingface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOMENT%3A%20A%20Family%20of%20Open%20Time-series%20Foundation%20Models&entry.906535625=Mononito%20Goswami%20and%20Konrad%20Szafer%20and%20Arjun%20Choudhry%20and%20Yifu%20Cai%20and%20Shuo%20Li%20and%20Artur%20Dubrawski&entry.1292438233=%20%20We%20introduce%20MOMENT%2C%20a%20family%20of%20open-source%20foundation%20models%20for%0Ageneral-purpose%20time%20series%20analysis.%20Pre-training%20large%20models%20on%20time%20series%0Adata%20is%20challenging%20due%20to%20%281%29%20the%20absence%20of%20a%20large%20and%20cohesive%20public%20time%0Aseries%20repository%2C%20and%20%282%29%20diverse%20time%20series%20characteristics%20which%20make%0Amulti-dataset%20training%20onerous.%20Additionally%2C%20%283%29%20experimental%20benchmarks%20to%0Aevaluate%20these%20models%2C%20especially%20in%20scenarios%20with%20limited%20resources%2C%20time%2C%0Aand%20supervision%2C%20are%20still%20in%20their%20nascent%20stages.%20To%20address%20these%0Achallenges%2C%20we%20compile%20a%20large%20and%20diverse%20collection%20of%20public%20time%20series%2C%0Acalled%20the%20Time%20series%20Pile%2C%20and%20systematically%20tackle%20time%20series-specific%0Achallenges%20to%20unlock%20large-scale%20multi-dataset%20pre-training.%20Finally%2C%20we%20build%0Aon%20recent%20work%20to%20design%20a%20benchmark%20to%20evaluate%20time%20series%20foundation%20models%0Aon%20diverse%20tasks%20and%20datasets%20in%20limited%20supervision%20settings.%20Experiments%20on%0Athis%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20pre-trained%20models%20with%0Aminimal%20data%20and%20task-specific%20fine-tuning.%20Finally%2C%20we%20present%20several%0Ainteresting%20empirical%20observations%20about%20large%20pre-trained%20time%20series%20models.%0APre-trained%20models%20%28AutonLab/MOMENT-1-large%29%20and%20Time%20Series%20Pile%0A%28AutonLab/Timeseries-PILE%29%20are%20available%20on%20Huggingface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03885v3&entry.124074799=Read"},
{"title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object\n  Navigation", "author": "Hang Yin and Xiuwei Xu and Zhenyu Wu and Jie Zhou and Jiwen Lu", "abstract": "  In this paper, we propose a new framework for zero-shot object navigation.\nExisting zero-shot object navigation methods prompt LLM with the text of\nspatially closed objects, which lacks enough scene context for in-depth\nreasoning. To better preserve the information of environment and fully exploit\nthe reasoning ability of LLM, we propose to represent the observed scene with\n3D scene graph. The scene graph encodes the relationships between objects,\ngroups and rooms with a LLM-friendly structure, for which we design a\nhierarchical chain-of-thought prompt to help LLM reason the goal location\naccording to scene context by traversing the nodes and edges. Moreover, benefit\nfrom the scene graph representation, we further design a re-perception\nmechanism to empower the object navigation framework with the ability to\ncorrect perception error. We conduct extensive experiments on MP3D, HM3D and\nRoboTHOR environments, where SG-Nav surpasses previous state-of-the-art\nzero-shot methods by more than 10% SR on all benchmarks, while the decision\nprocess is explainable. To the best of our knowledge, SG-Nav is the first\nzero-shot method that achieves even higher performance than supervised object\nnavigation methods on the challenging MP3D benchmark.\n", "link": "http://arxiv.org/abs/2410.08189v1", "date": "2024-10-10", "relevancy": 2.4232, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-Nav%3A%20Online%203D%20Scene%20Graph%20Prompting%20for%20LLM-based%20Zero-shot%20Object%0A%20%20Navigation&body=Title%3A%20SG-Nav%3A%20Online%203D%20Scene%20Graph%20Prompting%20for%20LLM-based%20Zero-shot%20Object%0A%20%20Navigation%0AAuthor%3A%20Hang%20Yin%20and%20Xiuwei%20Xu%20and%20Zhenyu%20Wu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20for%20zero-shot%20object%20navigation.%0AExisting%20zero-shot%20object%20navigation%20methods%20prompt%20LLM%20with%20the%20text%20of%0Aspatially%20closed%20objects%2C%20which%20lacks%20enough%20scene%20context%20for%20in-depth%0Areasoning.%20To%20better%20preserve%20the%20information%20of%20environment%20and%20fully%20exploit%0Athe%20reasoning%20ability%20of%20LLM%2C%20we%20propose%20to%20represent%20the%20observed%20scene%20with%0A3D%20scene%20graph.%20The%20scene%20graph%20encodes%20the%20relationships%20between%20objects%2C%0Agroups%20and%20rooms%20with%20a%20LLM-friendly%20structure%2C%20for%20which%20we%20design%20a%0Ahierarchical%20chain-of-thought%20prompt%20to%20help%20LLM%20reason%20the%20goal%20location%0Aaccording%20to%20scene%20context%20by%20traversing%20the%20nodes%20and%20edges.%20Moreover%2C%20benefit%0Afrom%20the%20scene%20graph%20representation%2C%20we%20further%20design%20a%20re-perception%0Amechanism%20to%20empower%20the%20object%20navigation%20framework%20with%20the%20ability%20to%0Acorrect%20perception%20error.%20We%20conduct%20extensive%20experiments%20on%20MP3D%2C%20HM3D%20and%0ARoboTHOR%20environments%2C%20where%20SG-Nav%20surpasses%20previous%20state-of-the-art%0Azero-shot%20methods%20by%20more%20than%2010%25%20SR%20on%20all%20benchmarks%2C%20while%20the%20decision%0Aprocess%20is%20explainable.%20To%20the%20best%20of%20our%20knowledge%2C%20SG-Nav%20is%20the%20first%0Azero-shot%20method%20that%20achieves%20even%20higher%20performance%20than%20supervised%20object%0Anavigation%20methods%20on%20the%20challenging%20MP3D%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-Nav%253A%2520Online%25203D%2520Scene%2520Graph%2520Prompting%2520for%2520LLM-based%2520Zero-shot%2520Object%250A%2520%2520Navigation%26entry.906535625%3DHang%2520Yin%2520and%2520Xiuwei%2520Xu%2520and%2520Zhenyu%2520Wu%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520framework%2520for%2520zero-shot%2520object%2520navigation.%250AExisting%2520zero-shot%2520object%2520navigation%2520methods%2520prompt%2520LLM%2520with%2520the%2520text%2520of%250Aspatially%2520closed%2520objects%252C%2520which%2520lacks%2520enough%2520scene%2520context%2520for%2520in-depth%250Areasoning.%2520To%2520better%2520preserve%2520the%2520information%2520of%2520environment%2520and%2520fully%2520exploit%250Athe%2520reasoning%2520ability%2520of%2520LLM%252C%2520we%2520propose%2520to%2520represent%2520the%2520observed%2520scene%2520with%250A3D%2520scene%2520graph.%2520The%2520scene%2520graph%2520encodes%2520the%2520relationships%2520between%2520objects%252C%250Agroups%2520and%2520rooms%2520with%2520a%2520LLM-friendly%2520structure%252C%2520for%2520which%2520we%2520design%2520a%250Ahierarchical%2520chain-of-thought%2520prompt%2520to%2520help%2520LLM%2520reason%2520the%2520goal%2520location%250Aaccording%2520to%2520scene%2520context%2520by%2520traversing%2520the%2520nodes%2520and%2520edges.%2520Moreover%252C%2520benefit%250Afrom%2520the%2520scene%2520graph%2520representation%252C%2520we%2520further%2520design%2520a%2520re-perception%250Amechanism%2520to%2520empower%2520the%2520object%2520navigation%2520framework%2520with%2520the%2520ability%2520to%250Acorrect%2520perception%2520error.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520MP3D%252C%2520HM3D%2520and%250ARoboTHOR%2520environments%252C%2520where%2520SG-Nav%2520surpasses%2520previous%2520state-of-the-art%250Azero-shot%2520methods%2520by%2520more%2520than%252010%2525%2520SR%2520on%2520all%2520benchmarks%252C%2520while%2520the%2520decision%250Aprocess%2520is%2520explainable.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520SG-Nav%2520is%2520the%2520first%250Azero-shot%2520method%2520that%2520achieves%2520even%2520higher%2520performance%2520than%2520supervised%2520object%250Anavigation%2520methods%2520on%2520the%2520challenging%2520MP3D%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-Nav%3A%20Online%203D%20Scene%20Graph%20Prompting%20for%20LLM-based%20Zero-shot%20Object%0A%20%20Navigation&entry.906535625=Hang%20Yin%20and%20Xiuwei%20Xu%20and%20Zhenyu%20Wu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20for%20zero-shot%20object%20navigation.%0AExisting%20zero-shot%20object%20navigation%20methods%20prompt%20LLM%20with%20the%20text%20of%0Aspatially%20closed%20objects%2C%20which%20lacks%20enough%20scene%20context%20for%20in-depth%0Areasoning.%20To%20better%20preserve%20the%20information%20of%20environment%20and%20fully%20exploit%0Athe%20reasoning%20ability%20of%20LLM%2C%20we%20propose%20to%20represent%20the%20observed%20scene%20with%0A3D%20scene%20graph.%20The%20scene%20graph%20encodes%20the%20relationships%20between%20objects%2C%0Agroups%20and%20rooms%20with%20a%20LLM-friendly%20structure%2C%20for%20which%20we%20design%20a%0Ahierarchical%20chain-of-thought%20prompt%20to%20help%20LLM%20reason%20the%20goal%20location%0Aaccording%20to%20scene%20context%20by%20traversing%20the%20nodes%20and%20edges.%20Moreover%2C%20benefit%0Afrom%20the%20scene%20graph%20representation%2C%20we%20further%20design%20a%20re-perception%0Amechanism%20to%20empower%20the%20object%20navigation%20framework%20with%20the%20ability%20to%0Acorrect%20perception%20error.%20We%20conduct%20extensive%20experiments%20on%20MP3D%2C%20HM3D%20and%0ARoboTHOR%20environments%2C%20where%20SG-Nav%20surpasses%20previous%20state-of-the-art%0Azero-shot%20methods%20by%20more%20than%2010%25%20SR%20on%20all%20benchmarks%2C%20while%20the%20decision%0Aprocess%20is%20explainable.%20To%20the%20best%20of%20our%20knowledge%2C%20SG-Nav%20is%20the%20first%0Azero-shot%20method%20that%20achieves%20even%20higher%20performance%20than%20supervised%20object%0Anavigation%20methods%20on%20the%20challenging%20MP3D%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08189v1&entry.124074799=Read"},
{"title": "Multimodal Perception System for Real Open Environment", "author": "Yuyang Sha", "abstract": "  This paper presents a novel multimodal perception system for a real open\nenvironment. The proposed system includes an embedded computation platform,\ncameras, ultrasonic sensors, GPS, and IMU devices. Unlike the traditional\nframeworks, our system integrates multiple sensors with advanced computer\nvision algorithms to help users walk outside reliably. The system can\nefficiently complete various tasks, including navigating to specific locations,\npassing through obstacle regions, and crossing intersections. Specifically, we\nalso use ultrasonic sensors and depth cameras to enhance obstacle avoidance\nperformance. The path planning module is designed to find the locally optimal\nroute based on various feedback and the user's current state. To evaluate the\nperformance of the proposed system, we design several experiments under\ndifferent scenarios. The results show that the system can help users walk\nefficiently and independently in complex situations.\n", "link": "http://arxiv.org/abs/2410.07926v1", "date": "2024-10-10", "relevancy": 2.4071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6375}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5984}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Perception%20System%20for%20Real%20Open%20Environment&body=Title%3A%20Multimodal%20Perception%20System%20for%20Real%20Open%20Environment%0AAuthor%3A%20Yuyang%20Sha%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20multimodal%20perception%20system%20for%20a%20real%20open%0Aenvironment.%20The%20proposed%20system%20includes%20an%20embedded%20computation%20platform%2C%0Acameras%2C%20ultrasonic%20sensors%2C%20GPS%2C%20and%20IMU%20devices.%20Unlike%20the%20traditional%0Aframeworks%2C%20our%20system%20integrates%20multiple%20sensors%20with%20advanced%20computer%0Avision%20algorithms%20to%20help%20users%20walk%20outside%20reliably.%20The%20system%20can%0Aefficiently%20complete%20various%20tasks%2C%20including%20navigating%20to%20specific%20locations%2C%0Apassing%20through%20obstacle%20regions%2C%20and%20crossing%20intersections.%20Specifically%2C%20we%0Aalso%20use%20ultrasonic%20sensors%20and%20depth%20cameras%20to%20enhance%20obstacle%20avoidance%0Aperformance.%20The%20path%20planning%20module%20is%20designed%20to%20find%20the%20locally%20optimal%0Aroute%20based%20on%20various%20feedback%20and%20the%20user%27s%20current%20state.%20To%20evaluate%20the%0Aperformance%20of%20the%20proposed%20system%2C%20we%20design%20several%20experiments%20under%0Adifferent%20scenarios.%20The%20results%20show%20that%20the%20system%20can%20help%20users%20walk%0Aefficiently%20and%20independently%20in%20complex%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Perception%2520System%2520for%2520Real%2520Open%2520Environment%26entry.906535625%3DYuyang%2520Sha%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520multimodal%2520perception%2520system%2520for%2520a%2520real%2520open%250Aenvironment.%2520The%2520proposed%2520system%2520includes%2520an%2520embedded%2520computation%2520platform%252C%250Acameras%252C%2520ultrasonic%2520sensors%252C%2520GPS%252C%2520and%2520IMU%2520devices.%2520Unlike%2520the%2520traditional%250Aframeworks%252C%2520our%2520system%2520integrates%2520multiple%2520sensors%2520with%2520advanced%2520computer%250Avision%2520algorithms%2520to%2520help%2520users%2520walk%2520outside%2520reliably.%2520The%2520system%2520can%250Aefficiently%2520complete%2520various%2520tasks%252C%2520including%2520navigating%2520to%2520specific%2520locations%252C%250Apassing%2520through%2520obstacle%2520regions%252C%2520and%2520crossing%2520intersections.%2520Specifically%252C%2520we%250Aalso%2520use%2520ultrasonic%2520sensors%2520and%2520depth%2520cameras%2520to%2520enhance%2520obstacle%2520avoidance%250Aperformance.%2520The%2520path%2520planning%2520module%2520is%2520designed%2520to%2520find%2520the%2520locally%2520optimal%250Aroute%2520based%2520on%2520various%2520feedback%2520and%2520the%2520user%2527s%2520current%2520state.%2520To%2520evaluate%2520the%250Aperformance%2520of%2520the%2520proposed%2520system%252C%2520we%2520design%2520several%2520experiments%2520under%250Adifferent%2520scenarios.%2520The%2520results%2520show%2520that%2520the%2520system%2520can%2520help%2520users%2520walk%250Aefficiently%2520and%2520independently%2520in%2520complex%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Perception%20System%20for%20Real%20Open%20Environment&entry.906535625=Yuyang%20Sha&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20multimodal%20perception%20system%20for%20a%20real%20open%0Aenvironment.%20The%20proposed%20system%20includes%20an%20embedded%20computation%20platform%2C%0Acameras%2C%20ultrasonic%20sensors%2C%20GPS%2C%20and%20IMU%20devices.%20Unlike%20the%20traditional%0Aframeworks%2C%20our%20system%20integrates%20multiple%20sensors%20with%20advanced%20computer%0Avision%20algorithms%20to%20help%20users%20walk%20outside%20reliably.%20The%20system%20can%0Aefficiently%20complete%20various%20tasks%2C%20including%20navigating%20to%20specific%20locations%2C%0Apassing%20through%20obstacle%20regions%2C%20and%20crossing%20intersections.%20Specifically%2C%20we%0Aalso%20use%20ultrasonic%20sensors%20and%20depth%20cameras%20to%20enhance%20obstacle%20avoidance%0Aperformance.%20The%20path%20planning%20module%20is%20designed%20to%20find%20the%20locally%20optimal%0Aroute%20based%20on%20various%20feedback%20and%20the%20user%27s%20current%20state.%20To%20evaluate%20the%0Aperformance%20of%20the%20proposed%20system%2C%20we%20design%20several%20experiments%20under%0Adifferent%20scenarios.%20The%20results%20show%20that%20the%20system%20can%20help%20users%20walk%0Aefficiently%20and%20independently%20in%20complex%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07926v1&entry.124074799=Read"},
{"title": "MathCoder2: Better Math Reasoning from Continued Pretraining on\n  Model-translated Mathematical Code", "author": "Zimu Lu and Aojun Zhou and Ke Wang and Houxing Ren and Weikang Shi and Junting Pan and Mingjie Zhan and Hongsheng Li", "abstract": "  Code has been shown to be effective in enhancing the mathematical reasoning\nabilities of large language models due to its precision and accuracy. Previous\nworks involving continued mathematical pretraining often include code that\nutilizes math-related packages, which are primarily designed for fields such as\nengineering, machine learning, signal processing, or module testing, rather\nthan being directly focused on mathematical reasoning. In this paper, we\nintroduce a novel method for generating mathematical code accompanied with\ncorresponding reasoning steps for continued pretraining. Our approach begins\nwith the construction of a high-quality mathematical continued pretraining\ndataset by incorporating math-related web data, code using mathematical\npackages, math textbooks, and synthetic data. Next, we construct reasoning\nsteps by extracting LaTeX expressions, the conditions needed for the\nexpressions, and the results of the expressions from the previously collected\ndataset. Based on this extracted information, we generate corresponding code to\naccurately capture the mathematical reasoning process. Appending the generated\ncode to each reasoning step results in data consisting of paired natural\nlanguage reasoning steps and their corresponding code. Combining this data with\nthe original dataset results in a 19.2B-token high-performing mathematical\npretraining corpus, which we name MathCode-Pile. Training several popular base\nmodels with this corpus significantly improves their mathematical abilities,\nleading to the creation of the MathCoder2 family of models. All of our data\nprocessing and training code is open-sourced, ensuring full transparency and\neasy reproducibility of the entire data collection and training pipeline. The\ncode is released at https://github.com/mathllm/MathCoder2 .\n", "link": "http://arxiv.org/abs/2410.08196v1", "date": "2024-10-10", "relevancy": 2.4035, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathCoder2%3A%20Better%20Math%20Reasoning%20from%20Continued%20Pretraining%20on%0A%20%20Model-translated%20Mathematical%20Code&body=Title%3A%20MathCoder2%3A%20Better%20Math%20Reasoning%20from%20Continued%20Pretraining%20on%0A%20%20Model-translated%20Mathematical%20Code%0AAuthor%3A%20Zimu%20Lu%20and%20Aojun%20Zhou%20and%20Ke%20Wang%20and%20Houxing%20Ren%20and%20Weikang%20Shi%20and%20Junting%20Pan%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Code%20has%20been%20shown%20to%20be%20effective%20in%20enhancing%20the%20mathematical%20reasoning%0Aabilities%20of%20large%20language%20models%20due%20to%20its%20precision%20and%20accuracy.%20Previous%0Aworks%20involving%20continued%20mathematical%20pretraining%20often%20include%20code%20that%0Autilizes%20math-related%20packages%2C%20which%20are%20primarily%20designed%20for%20fields%20such%20as%0Aengineering%2C%20machine%20learning%2C%20signal%20processing%2C%20or%20module%20testing%2C%20rather%0Athan%20being%20directly%20focused%20on%20mathematical%20reasoning.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20method%20for%20generating%20mathematical%20code%20accompanied%20with%0Acorresponding%20reasoning%20steps%20for%20continued%20pretraining.%20Our%20approach%20begins%0Awith%20the%20construction%20of%20a%20high-quality%20mathematical%20continued%20pretraining%0Adataset%20by%20incorporating%20math-related%20web%20data%2C%20code%20using%20mathematical%0Apackages%2C%20math%20textbooks%2C%20and%20synthetic%20data.%20Next%2C%20we%20construct%20reasoning%0Asteps%20by%20extracting%20LaTeX%20expressions%2C%20the%20conditions%20needed%20for%20the%0Aexpressions%2C%20and%20the%20results%20of%20the%20expressions%20from%20the%20previously%20collected%0Adataset.%20Based%20on%20this%20extracted%20information%2C%20we%20generate%20corresponding%20code%20to%0Aaccurately%20capture%20the%20mathematical%20reasoning%20process.%20Appending%20the%20generated%0Acode%20to%20each%20reasoning%20step%20results%20in%20data%20consisting%20of%20paired%20natural%0Alanguage%20reasoning%20steps%20and%20their%20corresponding%20code.%20Combining%20this%20data%20with%0Athe%20original%20dataset%20results%20in%20a%2019.2B-token%20high-performing%20mathematical%0Apretraining%20corpus%2C%20which%20we%20name%20MathCode-Pile.%20Training%20several%20popular%20base%0Amodels%20with%20this%20corpus%20significantly%20improves%20their%20mathematical%20abilities%2C%0Aleading%20to%20the%20creation%20of%20the%20MathCoder2%20family%20of%20models.%20All%20of%20our%20data%0Aprocessing%20and%20training%20code%20is%20open-sourced%2C%20ensuring%20full%20transparency%20and%0Aeasy%20reproducibility%20of%20the%20entire%20data%20collection%20and%20training%20pipeline.%20The%0Acode%20is%20released%20at%20https%3A//github.com/mathllm/MathCoder2%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathCoder2%253A%2520Better%2520Math%2520Reasoning%2520from%2520Continued%2520Pretraining%2520on%250A%2520%2520Model-translated%2520Mathematical%2520Code%26entry.906535625%3DZimu%2520Lu%2520and%2520Aojun%2520Zhou%2520and%2520Ke%2520Wang%2520and%2520Houxing%2520Ren%2520and%2520Weikang%2520Shi%2520and%2520Junting%2520Pan%2520and%2520Mingjie%2520Zhan%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Code%2520has%2520been%2520shown%2520to%2520be%2520effective%2520in%2520enhancing%2520the%2520mathematical%2520reasoning%250Aabilities%2520of%2520large%2520language%2520models%2520due%2520to%2520its%2520precision%2520and%2520accuracy.%2520Previous%250Aworks%2520involving%2520continued%2520mathematical%2520pretraining%2520often%2520include%2520code%2520that%250Autilizes%2520math-related%2520packages%252C%2520which%2520are%2520primarily%2520designed%2520for%2520fields%2520such%2520as%250Aengineering%252C%2520machine%2520learning%252C%2520signal%2520processing%252C%2520or%2520module%2520testing%252C%2520rather%250Athan%2520being%2520directly%2520focused%2520on%2520mathematical%2520reasoning.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520method%2520for%2520generating%2520mathematical%2520code%2520accompanied%2520with%250Acorresponding%2520reasoning%2520steps%2520for%2520continued%2520pretraining.%2520Our%2520approach%2520begins%250Awith%2520the%2520construction%2520of%2520a%2520high-quality%2520mathematical%2520continued%2520pretraining%250Adataset%2520by%2520incorporating%2520math-related%2520web%2520data%252C%2520code%2520using%2520mathematical%250Apackages%252C%2520math%2520textbooks%252C%2520and%2520synthetic%2520data.%2520Next%252C%2520we%2520construct%2520reasoning%250Asteps%2520by%2520extracting%2520LaTeX%2520expressions%252C%2520the%2520conditions%2520needed%2520for%2520the%250Aexpressions%252C%2520and%2520the%2520results%2520of%2520the%2520expressions%2520from%2520the%2520previously%2520collected%250Adataset.%2520Based%2520on%2520this%2520extracted%2520information%252C%2520we%2520generate%2520corresponding%2520code%2520to%250Aaccurately%2520capture%2520the%2520mathematical%2520reasoning%2520process.%2520Appending%2520the%2520generated%250Acode%2520to%2520each%2520reasoning%2520step%2520results%2520in%2520data%2520consisting%2520of%2520paired%2520natural%250Alanguage%2520reasoning%2520steps%2520and%2520their%2520corresponding%2520code.%2520Combining%2520this%2520data%2520with%250Athe%2520original%2520dataset%2520results%2520in%2520a%252019.2B-token%2520high-performing%2520mathematical%250Apretraining%2520corpus%252C%2520which%2520we%2520name%2520MathCode-Pile.%2520Training%2520several%2520popular%2520base%250Amodels%2520with%2520this%2520corpus%2520significantly%2520improves%2520their%2520mathematical%2520abilities%252C%250Aleading%2520to%2520the%2520creation%2520of%2520the%2520MathCoder2%2520family%2520of%2520models.%2520All%2520of%2520our%2520data%250Aprocessing%2520and%2520training%2520code%2520is%2520open-sourced%252C%2520ensuring%2520full%2520transparency%2520and%250Aeasy%2520reproducibility%2520of%2520the%2520entire%2520data%2520collection%2520and%2520training%2520pipeline.%2520The%250Acode%2520is%2520released%2520at%2520https%253A//github.com/mathllm/MathCoder2%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathCoder2%3A%20Better%20Math%20Reasoning%20from%20Continued%20Pretraining%20on%0A%20%20Model-translated%20Mathematical%20Code&entry.906535625=Zimu%20Lu%20and%20Aojun%20Zhou%20and%20Ke%20Wang%20and%20Houxing%20Ren%20and%20Weikang%20Shi%20and%20Junting%20Pan%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li&entry.1292438233=%20%20Code%20has%20been%20shown%20to%20be%20effective%20in%20enhancing%20the%20mathematical%20reasoning%0Aabilities%20of%20large%20language%20models%20due%20to%20its%20precision%20and%20accuracy.%20Previous%0Aworks%20involving%20continued%20mathematical%20pretraining%20often%20include%20code%20that%0Autilizes%20math-related%20packages%2C%20which%20are%20primarily%20designed%20for%20fields%20such%20as%0Aengineering%2C%20machine%20learning%2C%20signal%20processing%2C%20or%20module%20testing%2C%20rather%0Athan%20being%20directly%20focused%20on%20mathematical%20reasoning.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20method%20for%20generating%20mathematical%20code%20accompanied%20with%0Acorresponding%20reasoning%20steps%20for%20continued%20pretraining.%20Our%20approach%20begins%0Awith%20the%20construction%20of%20a%20high-quality%20mathematical%20continued%20pretraining%0Adataset%20by%20incorporating%20math-related%20web%20data%2C%20code%20using%20mathematical%0Apackages%2C%20math%20textbooks%2C%20and%20synthetic%20data.%20Next%2C%20we%20construct%20reasoning%0Asteps%20by%20extracting%20LaTeX%20expressions%2C%20the%20conditions%20needed%20for%20the%0Aexpressions%2C%20and%20the%20results%20of%20the%20expressions%20from%20the%20previously%20collected%0Adataset.%20Based%20on%20this%20extracted%20information%2C%20we%20generate%20corresponding%20code%20to%0Aaccurately%20capture%20the%20mathematical%20reasoning%20process.%20Appending%20the%20generated%0Acode%20to%20each%20reasoning%20step%20results%20in%20data%20consisting%20of%20paired%20natural%0Alanguage%20reasoning%20steps%20and%20their%20corresponding%20code.%20Combining%20this%20data%20with%0Athe%20original%20dataset%20results%20in%20a%2019.2B-token%20high-performing%20mathematical%0Apretraining%20corpus%2C%20which%20we%20name%20MathCode-Pile.%20Training%20several%20popular%20base%0Amodels%20with%20this%20corpus%20significantly%20improves%20their%20mathematical%20abilities%2C%0Aleading%20to%20the%20creation%20of%20the%20MathCoder2%20family%20of%20models.%20All%20of%20our%20data%0Aprocessing%20and%20training%20code%20is%20open-sourced%2C%20ensuring%20full%20transparency%20and%0Aeasy%20reproducibility%20of%20the%20entire%20data%20collection%20and%20training%20pipeline.%20The%0Acode%20is%20released%20at%20https%3A//github.com/mathllm/MathCoder2%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08196v1&entry.124074799=Read"},
{"title": "RegionGrasp: A Novel Task for Contact Region Controllable Hand Grasp\n  Generation", "author": "Yilin Wang and Chuan Guo and Li Cheng and Hai Jiang", "abstract": "  Can machine automatically generate multiple distinct and natural hand grasps,\ngiven specific contact region of an object in 3D? This motivates us to consider\na novel task of \\textit{Region Controllable Hand Grasp Generation\n(RegionGrasp)}, as follows: given as input a 3D object, together with its\nspecific surface area selected as the intended contact region, to generate a\ndiverse set of plausible hand grasps of the object, where the thumb finger tip\ntouches the object surface on the contact region. To address this task,\nRegionGrasp-CVAE is proposed, which consists of two main parts. First, to\nenable contact region-awareness, we propose ConditionNet as the condition\nencoder that includes in it a transformer-backboned object encoder, O-Enc; a\npretraining strategy is adopted by O-Enc, where the point patches of object\nsurface are randomly masked off and subsequently restored, to further capture\nsurface geometric information of the object. Second, to realize interaction\nawareness, HOINet is introduced to encode hand-object interaction features by\nentangling high-level hand features with embedded object features through\ngeometric-aware multi-head cross attention. Empirical evaluations demonstrate\nthe effectiveness of our approach qualitatively and quantitatively where it is\nshown to compare favorably with respect to the state of the art methods.\n", "link": "http://arxiv.org/abs/2410.07995v1", "date": "2024-10-10", "relevancy": 2.387, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6825}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5534}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RegionGrasp%3A%20A%20Novel%20Task%20for%20Contact%20Region%20Controllable%20Hand%20Grasp%0A%20%20Generation&body=Title%3A%20RegionGrasp%3A%20A%20Novel%20Task%20for%20Contact%20Region%20Controllable%20Hand%20Grasp%0A%20%20Generation%0AAuthor%3A%20Yilin%20Wang%20and%20Chuan%20Guo%20and%20Li%20Cheng%20and%20Hai%20Jiang%0AAbstract%3A%20%20%20Can%20machine%20automatically%20generate%20multiple%20distinct%20and%20natural%20hand%20grasps%2C%0Agiven%20specific%20contact%20region%20of%20an%20object%20in%203D%3F%20This%20motivates%20us%20to%20consider%0Aa%20novel%20task%20of%20%5Ctextit%7BRegion%20Controllable%20Hand%20Grasp%20Generation%0A%28RegionGrasp%29%7D%2C%20as%20follows%3A%20given%20as%20input%20a%203D%20object%2C%20together%20with%20its%0Aspecific%20surface%20area%20selected%20as%20the%20intended%20contact%20region%2C%20to%20generate%20a%0Adiverse%20set%20of%20plausible%20hand%20grasps%20of%20the%20object%2C%20where%20the%20thumb%20finger%20tip%0Atouches%20the%20object%20surface%20on%20the%20contact%20region.%20To%20address%20this%20task%2C%0ARegionGrasp-CVAE%20is%20proposed%2C%20which%20consists%20of%20two%20main%20parts.%20First%2C%20to%0Aenable%20contact%20region-awareness%2C%20we%20propose%20ConditionNet%20as%20the%20condition%0Aencoder%20that%20includes%20in%20it%20a%20transformer-backboned%20object%20encoder%2C%20O-Enc%3B%20a%0Apretraining%20strategy%20is%20adopted%20by%20O-Enc%2C%20where%20the%20point%20patches%20of%20object%0Asurface%20are%20randomly%20masked%20off%20and%20subsequently%20restored%2C%20to%20further%20capture%0Asurface%20geometric%20information%20of%20the%20object.%20Second%2C%20to%20realize%20interaction%0Aawareness%2C%20HOINet%20is%20introduced%20to%20encode%20hand-object%20interaction%20features%20by%0Aentangling%20high-level%20hand%20features%20with%20embedded%20object%20features%20through%0Ageometric-aware%20multi-head%20cross%20attention.%20Empirical%20evaluations%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20qualitatively%20and%20quantitatively%20where%20it%20is%0Ashown%20to%20compare%20favorably%20with%20respect%20to%20the%20state%20of%20the%20art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegionGrasp%253A%2520A%2520Novel%2520Task%2520for%2520Contact%2520Region%2520Controllable%2520Hand%2520Grasp%250A%2520%2520Generation%26entry.906535625%3DYilin%2520Wang%2520and%2520Chuan%2520Guo%2520and%2520Li%2520Cheng%2520and%2520Hai%2520Jiang%26entry.1292438233%3D%2520%2520Can%2520machine%2520automatically%2520generate%2520multiple%2520distinct%2520and%2520natural%2520hand%2520grasps%252C%250Agiven%2520specific%2520contact%2520region%2520of%2520an%2520object%2520in%25203D%253F%2520This%2520motivates%2520us%2520to%2520consider%250Aa%2520novel%2520task%2520of%2520%255Ctextit%257BRegion%2520Controllable%2520Hand%2520Grasp%2520Generation%250A%2528RegionGrasp%2529%257D%252C%2520as%2520follows%253A%2520given%2520as%2520input%2520a%25203D%2520object%252C%2520together%2520with%2520its%250Aspecific%2520surface%2520area%2520selected%2520as%2520the%2520intended%2520contact%2520region%252C%2520to%2520generate%2520a%250Adiverse%2520set%2520of%2520plausible%2520hand%2520grasps%2520of%2520the%2520object%252C%2520where%2520the%2520thumb%2520finger%2520tip%250Atouches%2520the%2520object%2520surface%2520on%2520the%2520contact%2520region.%2520To%2520address%2520this%2520task%252C%250ARegionGrasp-CVAE%2520is%2520proposed%252C%2520which%2520consists%2520of%2520two%2520main%2520parts.%2520First%252C%2520to%250Aenable%2520contact%2520region-awareness%252C%2520we%2520propose%2520ConditionNet%2520as%2520the%2520condition%250Aencoder%2520that%2520includes%2520in%2520it%2520a%2520transformer-backboned%2520object%2520encoder%252C%2520O-Enc%253B%2520a%250Apretraining%2520strategy%2520is%2520adopted%2520by%2520O-Enc%252C%2520where%2520the%2520point%2520patches%2520of%2520object%250Asurface%2520are%2520randomly%2520masked%2520off%2520and%2520subsequently%2520restored%252C%2520to%2520further%2520capture%250Asurface%2520geometric%2520information%2520of%2520the%2520object.%2520Second%252C%2520to%2520realize%2520interaction%250Aawareness%252C%2520HOINet%2520is%2520introduced%2520to%2520encode%2520hand-object%2520interaction%2520features%2520by%250Aentangling%2520high-level%2520hand%2520features%2520with%2520embedded%2520object%2520features%2520through%250Ageometric-aware%2520multi-head%2520cross%2520attention.%2520Empirical%2520evaluations%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520approach%2520qualitatively%2520and%2520quantitatively%2520where%2520it%2520is%250Ashown%2520to%2520compare%2520favorably%2520with%2520respect%2520to%2520the%2520state%2520of%2520the%2520art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RegionGrasp%3A%20A%20Novel%20Task%20for%20Contact%20Region%20Controllable%20Hand%20Grasp%0A%20%20Generation&entry.906535625=Yilin%20Wang%20and%20Chuan%20Guo%20and%20Li%20Cheng%20and%20Hai%20Jiang&entry.1292438233=%20%20Can%20machine%20automatically%20generate%20multiple%20distinct%20and%20natural%20hand%20grasps%2C%0Agiven%20specific%20contact%20region%20of%20an%20object%20in%203D%3F%20This%20motivates%20us%20to%20consider%0Aa%20novel%20task%20of%20%5Ctextit%7BRegion%20Controllable%20Hand%20Grasp%20Generation%0A%28RegionGrasp%29%7D%2C%20as%20follows%3A%20given%20as%20input%20a%203D%20object%2C%20together%20with%20its%0Aspecific%20surface%20area%20selected%20as%20the%20intended%20contact%20region%2C%20to%20generate%20a%0Adiverse%20set%20of%20plausible%20hand%20grasps%20of%20the%20object%2C%20where%20the%20thumb%20finger%20tip%0Atouches%20the%20object%20surface%20on%20the%20contact%20region.%20To%20address%20this%20task%2C%0ARegionGrasp-CVAE%20is%20proposed%2C%20which%20consists%20of%20two%20main%20parts.%20First%2C%20to%0Aenable%20contact%20region-awareness%2C%20we%20propose%20ConditionNet%20as%20the%20condition%0Aencoder%20that%20includes%20in%20it%20a%20transformer-backboned%20object%20encoder%2C%20O-Enc%3B%20a%0Apretraining%20strategy%20is%20adopted%20by%20O-Enc%2C%20where%20the%20point%20patches%20of%20object%0Asurface%20are%20randomly%20masked%20off%20and%20subsequently%20restored%2C%20to%20further%20capture%0Asurface%20geometric%20information%20of%20the%20object.%20Second%2C%20to%20realize%20interaction%0Aawareness%2C%20HOINet%20is%20introduced%20to%20encode%20hand-object%20interaction%20features%20by%0Aentangling%20high-level%20hand%20features%20with%20embedded%20object%20features%20through%0Ageometric-aware%20multi-head%20cross%20attention.%20Empirical%20evaluations%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20qualitatively%20and%20quantitatively%20where%20it%20is%0Ashown%20to%20compare%20favorably%20with%20respect%20to%20the%20state%20of%20the%20art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07995v1&entry.124074799=Read"},
{"title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching", "author": "RuiKang OuYang and Bo Qiang and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust.\n", "link": "http://arxiv.org/abs/2409.09787v2", "date": "2024-10-10", "relevancy": 2.3755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&body=Title%3A%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching%0AAuthor%3A%20RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09787v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBNEM%253A%2520A%2520Boltzmann%2520Sampler%2520Based%2520on%2520Bootstrapped%2520Noised%2520Energy%2520Matching%26entry.906535625%3DRuiKang%2520OuYang%2520and%2520Bo%2520Qiang%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Developing%2520an%2520efficient%2520sampler%2520capable%2520of%2520generating%2520independent%2520and%250Aidentically%2520distributed%2520%2528IID%2529%2520samples%2520from%2520a%2520Boltzmann%2520distribution%2520is%2520a%250Acrucial%2520challenge%2520in%2520scientific%2520research%252C%2520e.g.%2520molecular%2520dynamics.%2520In%2520this%250Awork%252C%2520we%2520intend%2520to%2520learn%2520neural%2520samplers%2520given%2520energy%2520functions%2520instead%2520of%2520data%250Asampled%2520from%2520the%2520Boltzmann%2520distribution.%2520By%2520learning%2520the%2520energies%2520of%2520the%2520noised%250Adata%252C%2520we%2520propose%2520a%2520diffusion-based%2520sampler%252C%2520Noised%2520Energy%2520Matching%252C%2520which%250Atheoretically%2520has%2520lower%2520variance%2520and%2520more%2520complexity%2520compared%2520to%2520related%2520works.%250AFurthermore%252C%2520a%2520novel%2520bootstrapping%2520technique%2520is%2520applied%2520to%2520NEM%2520to%2520balance%250Abetween%2520bias%2520and%2520variance.%2520We%2520evaluate%2520NEM%2520and%2520BNEM%2520on%2520a%25202-dimensional%252040%250AGaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520and%2520a%25204-particle%2520double-well%2520potential%2520%2528DW-4%2529.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520BNEM%2520can%2520achieve%2520state-of-the-art%250Aperformance%2520while%2520being%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09787v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&entry.906535625=RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09787v2&entry.124074799=Read"},
{"title": "Packing Analysis: Packing Is More Appropriate for Large Models or\n  Datasets in Supervised Fine-tuning", "author": "Shuhe Wang and Guoyin Wang and Jiwei Li and Eduard Hovy and Chen Guo", "abstract": "  Packing, initially utilized in the pre-training phase, is an optimization\ntechnique designed to maximize hardware resource efficiency by combining\ndifferent training sequences to fit the model's maximum input length. Although\nit has demonstrated effectiveness during pre-training, there remains a lack of\ncomprehensive analysis for the supervised fine-tuning (SFT) stage on the\nfollowing points: (1) whether packing can effectively enhance training\nefficiency while maintaining performance, (2) the suitable size of the model\nand dataset for fine-tuning with the packing method, and (3) whether packing\nunrelated or related training samples might cause the model to either\nexcessively disregard or over-rely on the context.\n  In this paper, we perform extensive comparisons between SFT methods using\npadding and packing, covering SFT datasets ranging from 69K to 1.2M and models\nfrom 8B to 70B. This provides the first comprehensive analysis of the\nadvantages and limitations of packing versus padding, as well as practical\nconsiderations for implementing packing in various training scenarios. Our\nanalysis covers various benchmarks, including knowledge, reasoning, and coding,\nas well as GPT-based evaluations, time efficiency, and other fine-tuning\nparameters. We also open-source our code for fine-tuning and evaluation and\nprovide checkpoints fine-tuned on datasets of different sizes, aiming to\nadvance future research on packing methods. Code is available at:\nhttps://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.\n", "link": "http://arxiv.org/abs/2410.08081v1", "date": "2024-10-10", "relevancy": 2.3727, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4851}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4702}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Packing%20Analysis%3A%20Packing%20Is%20More%20Appropriate%20for%20Large%20Models%20or%0A%20%20Datasets%20in%20Supervised%20Fine-tuning&body=Title%3A%20Packing%20Analysis%3A%20Packing%20Is%20More%20Appropriate%20for%20Large%20Models%20or%0A%20%20Datasets%20in%20Supervised%20Fine-tuning%0AAuthor%3A%20Shuhe%20Wang%20and%20Guoyin%20Wang%20and%20Jiwei%20Li%20and%20Eduard%20Hovy%20and%20Chen%20Guo%0AAbstract%3A%20%20%20Packing%2C%20initially%20utilized%20in%20the%20pre-training%20phase%2C%20is%20an%20optimization%0Atechnique%20designed%20to%20maximize%20hardware%20resource%20efficiency%20by%20combining%0Adifferent%20training%20sequences%20to%20fit%20the%20model%27s%20maximum%20input%20length.%20Although%0Ait%20has%20demonstrated%20effectiveness%20during%20pre-training%2C%20there%20remains%20a%20lack%20of%0Acomprehensive%20analysis%20for%20the%20supervised%20fine-tuning%20%28SFT%29%20stage%20on%20the%0Afollowing%20points%3A%20%281%29%20whether%20packing%20can%20effectively%20enhance%20training%0Aefficiency%20while%20maintaining%20performance%2C%20%282%29%20the%20suitable%20size%20of%20the%20model%0Aand%20dataset%20for%20fine-tuning%20with%20the%20packing%20method%2C%20and%20%283%29%20whether%20packing%0Aunrelated%20or%20related%20training%20samples%20might%20cause%20the%20model%20to%20either%0Aexcessively%20disregard%20or%20over-rely%20on%20the%20context.%0A%20%20In%20this%20paper%2C%20we%20perform%20extensive%20comparisons%20between%20SFT%20methods%20using%0Apadding%20and%20packing%2C%20covering%20SFT%20datasets%20ranging%20from%2069K%20to%201.2M%20and%20models%0Afrom%208B%20to%2070B.%20This%20provides%20the%20first%20comprehensive%20analysis%20of%20the%0Aadvantages%20and%20limitations%20of%20packing%20versus%20padding%2C%20as%20well%20as%20practical%0Aconsiderations%20for%20implementing%20packing%20in%20various%20training%20scenarios.%20Our%0Aanalysis%20covers%20various%20benchmarks%2C%20including%20knowledge%2C%20reasoning%2C%20and%20coding%2C%0Aas%20well%20as%20GPT-based%20evaluations%2C%20time%20efficiency%2C%20and%20other%20fine-tuning%0Aparameters.%20We%20also%20open-source%20our%20code%20for%20fine-tuning%20and%20evaluation%20and%0Aprovide%20checkpoints%20fine-tuned%20on%20datasets%20of%20different%20sizes%2C%20aiming%20to%0Aadvance%20future%20research%20on%20packing%20methods.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ShuheWang1998/Packing-Analysis%3Ftab%3Dreadme-ov-file.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPacking%2520Analysis%253A%2520Packing%2520Is%2520More%2520Appropriate%2520for%2520Large%2520Models%2520or%250A%2520%2520Datasets%2520in%2520Supervised%2520Fine-tuning%26entry.906535625%3DShuhe%2520Wang%2520and%2520Guoyin%2520Wang%2520and%2520Jiwei%2520Li%2520and%2520Eduard%2520Hovy%2520and%2520Chen%2520Guo%26entry.1292438233%3D%2520%2520Packing%252C%2520initially%2520utilized%2520in%2520the%2520pre-training%2520phase%252C%2520is%2520an%2520optimization%250Atechnique%2520designed%2520to%2520maximize%2520hardware%2520resource%2520efficiency%2520by%2520combining%250Adifferent%2520training%2520sequences%2520to%2520fit%2520the%2520model%2527s%2520maximum%2520input%2520length.%2520Although%250Ait%2520has%2520demonstrated%2520effectiveness%2520during%2520pre-training%252C%2520there%2520remains%2520a%2520lack%2520of%250Acomprehensive%2520analysis%2520for%2520the%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520stage%2520on%2520the%250Afollowing%2520points%253A%2520%25281%2529%2520whether%2520packing%2520can%2520effectively%2520enhance%2520training%250Aefficiency%2520while%2520maintaining%2520performance%252C%2520%25282%2529%2520the%2520suitable%2520size%2520of%2520the%2520model%250Aand%2520dataset%2520for%2520fine-tuning%2520with%2520the%2520packing%2520method%252C%2520and%2520%25283%2529%2520whether%2520packing%250Aunrelated%2520or%2520related%2520training%2520samples%2520might%2520cause%2520the%2520model%2520to%2520either%250Aexcessively%2520disregard%2520or%2520over-rely%2520on%2520the%2520context.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520perform%2520extensive%2520comparisons%2520between%2520SFT%2520methods%2520using%250Apadding%2520and%2520packing%252C%2520covering%2520SFT%2520datasets%2520ranging%2520from%252069K%2520to%25201.2M%2520and%2520models%250Afrom%25208B%2520to%252070B.%2520This%2520provides%2520the%2520first%2520comprehensive%2520analysis%2520of%2520the%250Aadvantages%2520and%2520limitations%2520of%2520packing%2520versus%2520padding%252C%2520as%2520well%2520as%2520practical%250Aconsiderations%2520for%2520implementing%2520packing%2520in%2520various%2520training%2520scenarios.%2520Our%250Aanalysis%2520covers%2520various%2520benchmarks%252C%2520including%2520knowledge%252C%2520reasoning%252C%2520and%2520coding%252C%250Aas%2520well%2520as%2520GPT-based%2520evaluations%252C%2520time%2520efficiency%252C%2520and%2520other%2520fine-tuning%250Aparameters.%2520We%2520also%2520open-source%2520our%2520code%2520for%2520fine-tuning%2520and%2520evaluation%2520and%250Aprovide%2520checkpoints%2520fine-tuned%2520on%2520datasets%2520of%2520different%2520sizes%252C%2520aiming%2520to%250Aadvance%2520future%2520research%2520on%2520packing%2520methods.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ShuheWang1998/Packing-Analysis%253Ftab%253Dreadme-ov-file.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Packing%20Analysis%3A%20Packing%20Is%20More%20Appropriate%20for%20Large%20Models%20or%0A%20%20Datasets%20in%20Supervised%20Fine-tuning&entry.906535625=Shuhe%20Wang%20and%20Guoyin%20Wang%20and%20Jiwei%20Li%20and%20Eduard%20Hovy%20and%20Chen%20Guo&entry.1292438233=%20%20Packing%2C%20initially%20utilized%20in%20the%20pre-training%20phase%2C%20is%20an%20optimization%0Atechnique%20designed%20to%20maximize%20hardware%20resource%20efficiency%20by%20combining%0Adifferent%20training%20sequences%20to%20fit%20the%20model%27s%20maximum%20input%20length.%20Although%0Ait%20has%20demonstrated%20effectiveness%20during%20pre-training%2C%20there%20remains%20a%20lack%20of%0Acomprehensive%20analysis%20for%20the%20supervised%20fine-tuning%20%28SFT%29%20stage%20on%20the%0Afollowing%20points%3A%20%281%29%20whether%20packing%20can%20effectively%20enhance%20training%0Aefficiency%20while%20maintaining%20performance%2C%20%282%29%20the%20suitable%20size%20of%20the%20model%0Aand%20dataset%20for%20fine-tuning%20with%20the%20packing%20method%2C%20and%20%283%29%20whether%20packing%0Aunrelated%20or%20related%20training%20samples%20might%20cause%20the%20model%20to%20either%0Aexcessively%20disregard%20or%20over-rely%20on%20the%20context.%0A%20%20In%20this%20paper%2C%20we%20perform%20extensive%20comparisons%20between%20SFT%20methods%20using%0Apadding%20and%20packing%2C%20covering%20SFT%20datasets%20ranging%20from%2069K%20to%201.2M%20and%20models%0Afrom%208B%20to%2070B.%20This%20provides%20the%20first%20comprehensive%20analysis%20of%20the%0Aadvantages%20and%20limitations%20of%20packing%20versus%20padding%2C%20as%20well%20as%20practical%0Aconsiderations%20for%20implementing%20packing%20in%20various%20training%20scenarios.%20Our%0Aanalysis%20covers%20various%20benchmarks%2C%20including%20knowledge%2C%20reasoning%2C%20and%20coding%2C%0Aas%20well%20as%20GPT-based%20evaluations%2C%20time%20efficiency%2C%20and%20other%20fine-tuning%0Aparameters.%20We%20also%20open-source%20our%20code%20for%20fine-tuning%20and%20evaluation%20and%0Aprovide%20checkpoints%20fine-tuned%20on%20datasets%20of%20different%20sizes%2C%20aiming%20to%0Aadvance%20future%20research%20on%20packing%20methods.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ShuheWang1998/Packing-Analysis%3Ftab%3Dreadme-ov-file.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08081v1&entry.124074799=Read"},
{"title": "Gaussian Process Thompson Sampling via Rootfinding", "author": "Taiwo A. Adebiyi and Bach Do and Ruda Zhang", "abstract": "  Thompson sampling (TS) is a simple, effective stochastic policy in Bayesian\ndecision making. It samples the posterior belief about the reward profile and\noptimizes the sample to obtain a candidate decision. In continuous\noptimization, the posterior of the objective function is often a Gaussian\nprocess (GP), whose sample paths have numerous local optima, making their\nglobal optimization challenging. In this work, we introduce an efficient global\noptimization strategy for GP-TS that carefully selects starting points for\ngradient-based multi-start optimizers. It identifies all local optima of the\nprior sample via univariate global rootfinding, and optimizes the posterior\nsample using a differentiable, decoupled representation. We demonstrate\nremarkable improvement in the global optimization of GP posterior samples,\nespecially in high dimensions. This leads to dramatic improvements in the\noverall performance of Bayesian optimization using GP-TS acquisition functions,\nsurprisingly outperforming alternatives like GP-UCB and EI.\n", "link": "http://arxiv.org/abs/2410.08071v1", "date": "2024-10-10", "relevancy": 2.3577, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4973}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4599}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Thompson%20Sampling%20via%20Rootfinding&body=Title%3A%20Gaussian%20Process%20Thompson%20Sampling%20via%20Rootfinding%0AAuthor%3A%20Taiwo%20A.%20Adebiyi%20and%20Bach%20Do%20and%20Ruda%20Zhang%0AAbstract%3A%20%20%20Thompson%20sampling%20%28TS%29%20is%20a%20simple%2C%20effective%20stochastic%20policy%20in%20Bayesian%0Adecision%20making.%20It%20samples%20the%20posterior%20belief%20about%20the%20reward%20profile%20and%0Aoptimizes%20the%20sample%20to%20obtain%20a%20candidate%20decision.%20In%20continuous%0Aoptimization%2C%20the%20posterior%20of%20the%20objective%20function%20is%20often%20a%20Gaussian%0Aprocess%20%28GP%29%2C%20whose%20sample%20paths%20have%20numerous%20local%20optima%2C%20making%20their%0Aglobal%20optimization%20challenging.%20In%20this%20work%2C%20we%20introduce%20an%20efficient%20global%0Aoptimization%20strategy%20for%20GP-TS%20that%20carefully%20selects%20starting%20points%20for%0Agradient-based%20multi-start%20optimizers.%20It%20identifies%20all%20local%20optima%20of%20the%0Aprior%20sample%20via%20univariate%20global%20rootfinding%2C%20and%20optimizes%20the%20posterior%0Asample%20using%20a%20differentiable%2C%20decoupled%20representation.%20We%20demonstrate%0Aremarkable%20improvement%20in%20the%20global%20optimization%20of%20GP%20posterior%20samples%2C%0Aespecially%20in%20high%20dimensions.%20This%20leads%20to%20dramatic%20improvements%20in%20the%0Aoverall%20performance%20of%20Bayesian%20optimization%20using%20GP-TS%20acquisition%20functions%2C%0Asurprisingly%20outperforming%20alternatives%20like%20GP-UCB%20and%20EI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Thompson%2520Sampling%2520via%2520Rootfinding%26entry.906535625%3DTaiwo%2520A.%2520Adebiyi%2520and%2520Bach%2520Do%2520and%2520Ruda%2520Zhang%26entry.1292438233%3D%2520%2520Thompson%2520sampling%2520%2528TS%2529%2520is%2520a%2520simple%252C%2520effective%2520stochastic%2520policy%2520in%2520Bayesian%250Adecision%2520making.%2520It%2520samples%2520the%2520posterior%2520belief%2520about%2520the%2520reward%2520profile%2520and%250Aoptimizes%2520the%2520sample%2520to%2520obtain%2520a%2520candidate%2520decision.%2520In%2520continuous%250Aoptimization%252C%2520the%2520posterior%2520of%2520the%2520objective%2520function%2520is%2520often%2520a%2520Gaussian%250Aprocess%2520%2528GP%2529%252C%2520whose%2520sample%2520paths%2520have%2520numerous%2520local%2520optima%252C%2520making%2520their%250Aglobal%2520optimization%2520challenging.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520efficient%2520global%250Aoptimization%2520strategy%2520for%2520GP-TS%2520that%2520carefully%2520selects%2520starting%2520points%2520for%250Agradient-based%2520multi-start%2520optimizers.%2520It%2520identifies%2520all%2520local%2520optima%2520of%2520the%250Aprior%2520sample%2520via%2520univariate%2520global%2520rootfinding%252C%2520and%2520optimizes%2520the%2520posterior%250Asample%2520using%2520a%2520differentiable%252C%2520decoupled%2520representation.%2520We%2520demonstrate%250Aremarkable%2520improvement%2520in%2520the%2520global%2520optimization%2520of%2520GP%2520posterior%2520samples%252C%250Aespecially%2520in%2520high%2520dimensions.%2520This%2520leads%2520to%2520dramatic%2520improvements%2520in%2520the%250Aoverall%2520performance%2520of%2520Bayesian%2520optimization%2520using%2520GP-TS%2520acquisition%2520functions%252C%250Asurprisingly%2520outperforming%2520alternatives%2520like%2520GP-UCB%2520and%2520EI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Thompson%20Sampling%20via%20Rootfinding&entry.906535625=Taiwo%20A.%20Adebiyi%20and%20Bach%20Do%20and%20Ruda%20Zhang&entry.1292438233=%20%20Thompson%20sampling%20%28TS%29%20is%20a%20simple%2C%20effective%20stochastic%20policy%20in%20Bayesian%0Adecision%20making.%20It%20samples%20the%20posterior%20belief%20about%20the%20reward%20profile%20and%0Aoptimizes%20the%20sample%20to%20obtain%20a%20candidate%20decision.%20In%20continuous%0Aoptimization%2C%20the%20posterior%20of%20the%20objective%20function%20is%20often%20a%20Gaussian%0Aprocess%20%28GP%29%2C%20whose%20sample%20paths%20have%20numerous%20local%20optima%2C%20making%20their%0Aglobal%20optimization%20challenging.%20In%20this%20work%2C%20we%20introduce%20an%20efficient%20global%0Aoptimization%20strategy%20for%20GP-TS%20that%20carefully%20selects%20starting%20points%20for%0Agradient-based%20multi-start%20optimizers.%20It%20identifies%20all%20local%20optima%20of%20the%0Aprior%20sample%20via%20univariate%20global%20rootfinding%2C%20and%20optimizes%20the%20posterior%0Asample%20using%20a%20differentiable%2C%20decoupled%20representation.%20We%20demonstrate%0Aremarkable%20improvement%20in%20the%20global%20optimization%20of%20GP%20posterior%20samples%2C%0Aespecially%20in%20high%20dimensions.%20This%20leads%20to%20dramatic%20improvements%20in%20the%0Aoverall%20performance%20of%20Bayesian%20optimization%20using%20GP-TS%20acquisition%20functions%2C%0Asurprisingly%20outperforming%20alternatives%20like%20GP-UCB%20and%20EI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08071v1&entry.124074799=Read"},
{"title": "HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven\n  Generation", "author": "Shanyan Guan and Yanhao Ge and Ying Tai and Jian Yang and Wei Li and Mingyu You", "abstract": "  Recent advancements in text-to-image diffusion models have shown remarkable\ncreative capabilities with textual prompts, but generating personalized\ninstances based on specific subjects, known as subject-driven generation,\nremains challenging. To tackle this issue, we present a new hybrid framework\ncalled HybridBooth, which merges the benefits of optimization-based and\ndirect-regression methods. HybridBooth operates in two stages: the Word\nEmbedding Probe, which generates a robust initial word embedding using a\nfine-tuned encoder, and the Word Embedding Refinement, which further adapts the\nencoder to specific subject images by optimizing key parameters. This approach\nallows for effective and fast inversion of visual concepts into textual\nembedding, even from a single image, while maintaining the model's\ngeneralization capabilities.\n", "link": "http://arxiv.org/abs/2410.08192v1", "date": "2024-10-10", "relevancy": 2.3454, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6072}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.586}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridBooth%3A%20Hybrid%20Prompt%20Inversion%20for%20Efficient%20Subject-Driven%0A%20%20Generation&body=Title%3A%20HybridBooth%3A%20Hybrid%20Prompt%20Inversion%20for%20Efficient%20Subject-Driven%0A%20%20Generation%0AAuthor%3A%20Shanyan%20Guan%20and%20Yanhao%20Ge%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Wei%20Li%20and%20Mingyu%20You%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20diffusion%20models%20have%20shown%20remarkable%0Acreative%20capabilities%20with%20textual%20prompts%2C%20but%20generating%20personalized%0Ainstances%20based%20on%20specific%20subjects%2C%20known%20as%20subject-driven%20generation%2C%0Aremains%20challenging.%20To%20tackle%20this%20issue%2C%20we%20present%20a%20new%20hybrid%20framework%0Acalled%20HybridBooth%2C%20which%20merges%20the%20benefits%20of%20optimization-based%20and%0Adirect-regression%20methods.%20HybridBooth%20operates%20in%20two%20stages%3A%20the%20Word%0AEmbedding%20Probe%2C%20which%20generates%20a%20robust%20initial%20word%20embedding%20using%20a%0Afine-tuned%20encoder%2C%20and%20the%20Word%20Embedding%20Refinement%2C%20which%20further%20adapts%20the%0Aencoder%20to%20specific%20subject%20images%20by%20optimizing%20key%20parameters.%20This%20approach%0Aallows%20for%20effective%20and%20fast%20inversion%20of%20visual%20concepts%20into%20textual%0Aembedding%2C%20even%20from%20a%20single%20image%2C%20while%20maintaining%20the%20model%27s%0Ageneralization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridBooth%253A%2520Hybrid%2520Prompt%2520Inversion%2520for%2520Efficient%2520Subject-Driven%250A%2520%2520Generation%26entry.906535625%3DShanyan%2520Guan%2520and%2520Yanhao%2520Ge%2520and%2520Ying%2520Tai%2520and%2520Jian%2520Yang%2520and%2520Wei%2520Li%2520and%2520Mingyu%2520You%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520diffusion%2520models%2520have%2520shown%2520remarkable%250Acreative%2520capabilities%2520with%2520textual%2520prompts%252C%2520but%2520generating%2520personalized%250Ainstances%2520based%2520on%2520specific%2520subjects%252C%2520known%2520as%2520subject-driven%2520generation%252C%250Aremains%2520challenging.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520present%2520a%2520new%2520hybrid%2520framework%250Acalled%2520HybridBooth%252C%2520which%2520merges%2520the%2520benefits%2520of%2520optimization-based%2520and%250Adirect-regression%2520methods.%2520HybridBooth%2520operates%2520in%2520two%2520stages%253A%2520the%2520Word%250AEmbedding%2520Probe%252C%2520which%2520generates%2520a%2520robust%2520initial%2520word%2520embedding%2520using%2520a%250Afine-tuned%2520encoder%252C%2520and%2520the%2520Word%2520Embedding%2520Refinement%252C%2520which%2520further%2520adapts%2520the%250Aencoder%2520to%2520specific%2520subject%2520images%2520by%2520optimizing%2520key%2520parameters.%2520This%2520approach%250Aallows%2520for%2520effective%2520and%2520fast%2520inversion%2520of%2520visual%2520concepts%2520into%2520textual%250Aembedding%252C%2520even%2520from%2520a%2520single%2520image%252C%2520while%2520maintaining%2520the%2520model%2527s%250Ageneralization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridBooth%3A%20Hybrid%20Prompt%20Inversion%20for%20Efficient%20Subject-Driven%0A%20%20Generation&entry.906535625=Shanyan%20Guan%20and%20Yanhao%20Ge%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Wei%20Li%20and%20Mingyu%20You&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20diffusion%20models%20have%20shown%20remarkable%0Acreative%20capabilities%20with%20textual%20prompts%2C%20but%20generating%20personalized%0Ainstances%20based%20on%20specific%20subjects%2C%20known%20as%20subject-driven%20generation%2C%0Aremains%20challenging.%20To%20tackle%20this%20issue%2C%20we%20present%20a%20new%20hybrid%20framework%0Acalled%20HybridBooth%2C%20which%20merges%20the%20benefits%20of%20optimization-based%20and%0Adirect-regression%20methods.%20HybridBooth%20operates%20in%20two%20stages%3A%20the%20Word%0AEmbedding%20Probe%2C%20which%20generates%20a%20robust%20initial%20word%20embedding%20using%20a%0Afine-tuned%20encoder%2C%20and%20the%20Word%20Embedding%20Refinement%2C%20which%20further%20adapts%20the%0Aencoder%20to%20specific%20subject%20images%20by%20optimizing%20key%20parameters.%20This%20approach%0Aallows%20for%20effective%20and%20fast%20inversion%20of%20visual%20concepts%20into%20textual%0Aembedding%2C%20even%20from%20a%20single%20image%2C%20while%20maintaining%20the%20model%27s%0Ageneralization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08192v1&entry.124074799=Read"},
{"title": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages", "author": "Mitodru Niyogi and Arnab Bhattacharya", "abstract": "  We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters.\n", "link": "http://arxiv.org/abs/2401.18034v2", "date": "2024-10-10", "relevancy": 2.3337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paramanu%3A%20A%20Family%20of%20Novel%20Efficient%20Generative%20Foundation%20Language%0A%20%20Models%20for%20Indian%20Languages&body=Title%3A%20Paramanu%3A%20A%20Family%20of%20Novel%20Efficient%20Generative%20Foundation%20Language%0A%20%20Models%20for%20Indian%20Languages%0AAuthor%3A%20Mitodru%20Niyogi%20and%20Arnab%20Bhattacharya%0AAbstract%3A%20%20%20We%20present%20%22Paramanu%22%2C%20a%20family%20of%20novel%20language%20models%20%28LM%29%20for%20Indian%0Alanguages%2C%20consisting%20of%20auto-regressive%20monolingual%2C%20bilingual%2C%20and%0Amultilingual%20models%20pretrained%20from%20scratch.%20Currently%2C%20it%20covers%2010%20languages%0A%28Assamese%2C%20Bangla%2C%20Hindi%2C%20Konkani%2C%20Maithili%2C%20Marathi%2C%20Odia%2C%20Sanskrit%2C%20Tamil%2C%0ATelugu%29%20across%205%20scripts%20%28Bangla%2C%20Devanagari%2C%20Odia%2C%20Tamil%2C%20Telugu%29.%20The%20models%0Aare%20pretrained%20on%20a%20single%20GPU%20with%20context%20size%20of%201024%20and%20vary%20in%20size%20from%0A13.29%20million%20%28M%29%20to%20367.5%20M%20parameters.%20We%20proposed%20a%20RoPE%20embedding%20scaling%0Amethod%20that%20enables%20us%20to%20pretrain%20language%20models%20from%20scratch%20at%20larger%0Asequence%20length%20context%20size%20than%20typical%20GPU%20memory%20permits.%20We%20also%0Aintroduced%20a%20novel%20efficient%20Indic%20tokenizer%2C%20%22mBharat%22%2C%20using%20a%20combination%20of%0ABPE%20and%20Unigram%2C%20achieving%20the%20least%20fertility%20score%20and%20the%20ability%20to%0Atokenize%20unseen%20languages%20in%20both%20the%20same%20script%20%26%20Roman%20script.%20We%20also%0Aproposed%20and%20performed%20language-specific%20tokenization%20for%20multilingual%20models%20%26%0Adomain-specific%20tokenization%20for%20monolingual%20models.%20To%20address%20the%20%22curse%20of%0Amultilinguality%22%20in%20our%20mParamanu%20model%2C%20we%20pretrained%20on%20comparable%20corpora%0Abased%20on%20typological%20grouping%20within%20the%20same%20script.%20Our%20findings%20show%20a%0Alanguage%20transfer%20phenomenon%20from%20low-resource%20to%20high-resource%20languages%0Awithin%20languages%20of%20the%20same%20script%20%26%20typology.%20Human%20evaluations%20for%0Aopen-ended%20text%20generation%20demonstrated%20that%20Paramanu%20models%20outperformed%0Aseveral%20LLMs%2C%20despite%20being%2020%20to%2064%20times%20smaller.%20We%20created%0Ainstruction-tuning%20datasets%20%26%20instruction-tuned%20our%20models%20on%2023%2C000%0Ainstructions%20in%20respective%20languages.%20Comparisons%20with%20multilingual%20LLMs%20across%0Avarious%20benchmarks%20for%20natural%20language%20%28NL%29%20understanding%2C%20NL%20inference%2C%20%26%0Areading%20comprehension%20highlight%20the%20advantages%20of%20our%20models%3B%20leads%20to%20the%0Aconclusion%20that%20high%20quality%20generative%20LM%20are%20possible%20without%20high%20amount%20of%0Acompute%20power%20%26%20enormous%20number%20of%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.18034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParamanu%253A%2520A%2520Family%2520of%2520Novel%2520Efficient%2520Generative%2520Foundation%2520Language%250A%2520%2520Models%2520for%2520Indian%2520Languages%26entry.906535625%3DMitodru%2520Niyogi%2520and%2520Arnab%2520Bhattacharya%26entry.1292438233%3D%2520%2520We%2520present%2520%2522Paramanu%2522%252C%2520a%2520family%2520of%2520novel%2520language%2520models%2520%2528LM%2529%2520for%2520Indian%250Alanguages%252C%2520consisting%2520of%2520auto-regressive%2520monolingual%252C%2520bilingual%252C%2520and%250Amultilingual%2520models%2520pretrained%2520from%2520scratch.%2520Currently%252C%2520it%2520covers%252010%2520languages%250A%2528Assamese%252C%2520Bangla%252C%2520Hindi%252C%2520Konkani%252C%2520Maithili%252C%2520Marathi%252C%2520Odia%252C%2520Sanskrit%252C%2520Tamil%252C%250ATelugu%2529%2520across%25205%2520scripts%2520%2528Bangla%252C%2520Devanagari%252C%2520Odia%252C%2520Tamil%252C%2520Telugu%2529.%2520The%2520models%250Aare%2520pretrained%2520on%2520a%2520single%2520GPU%2520with%2520context%2520size%2520of%25201024%2520and%2520vary%2520in%2520size%2520from%250A13.29%2520million%2520%2528M%2529%2520to%2520367.5%2520M%2520parameters.%2520We%2520proposed%2520a%2520RoPE%2520embedding%2520scaling%250Amethod%2520that%2520enables%2520us%2520to%2520pretrain%2520language%2520models%2520from%2520scratch%2520at%2520larger%250Asequence%2520length%2520context%2520size%2520than%2520typical%2520GPU%2520memory%2520permits.%2520We%2520also%250Aintroduced%2520a%2520novel%2520efficient%2520Indic%2520tokenizer%252C%2520%2522mBharat%2522%252C%2520using%2520a%2520combination%2520of%250ABPE%2520and%2520Unigram%252C%2520achieving%2520the%2520least%2520fertility%2520score%2520and%2520the%2520ability%2520to%250Atokenize%2520unseen%2520languages%2520in%2520both%2520the%2520same%2520script%2520%2526%2520Roman%2520script.%2520We%2520also%250Aproposed%2520and%2520performed%2520language-specific%2520tokenization%2520for%2520multilingual%2520models%2520%2526%250Adomain-specific%2520tokenization%2520for%2520monolingual%2520models.%2520To%2520address%2520the%2520%2522curse%2520of%250Amultilinguality%2522%2520in%2520our%2520mParamanu%2520model%252C%2520we%2520pretrained%2520on%2520comparable%2520corpora%250Abased%2520on%2520typological%2520grouping%2520within%2520the%2520same%2520script.%2520Our%2520findings%2520show%2520a%250Alanguage%2520transfer%2520phenomenon%2520from%2520low-resource%2520to%2520high-resource%2520languages%250Awithin%2520languages%2520of%2520the%2520same%2520script%2520%2526%2520typology.%2520Human%2520evaluations%2520for%250Aopen-ended%2520text%2520generation%2520demonstrated%2520that%2520Paramanu%2520models%2520outperformed%250Aseveral%2520LLMs%252C%2520despite%2520being%252020%2520to%252064%2520times%2520smaller.%2520We%2520created%250Ainstruction-tuning%2520datasets%2520%2526%2520instruction-tuned%2520our%2520models%2520on%252023%252C000%250Ainstructions%2520in%2520respective%2520languages.%2520Comparisons%2520with%2520multilingual%2520LLMs%2520across%250Avarious%2520benchmarks%2520for%2520natural%2520language%2520%2528NL%2529%2520understanding%252C%2520NL%2520inference%252C%2520%2526%250Areading%2520comprehension%2520highlight%2520the%2520advantages%2520of%2520our%2520models%253B%2520leads%2520to%2520the%250Aconclusion%2520that%2520high%2520quality%2520generative%2520LM%2520are%2520possible%2520without%2520high%2520amount%2520of%250Acompute%2520power%2520%2526%2520enormous%2520number%2520of%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.18034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paramanu%3A%20A%20Family%20of%20Novel%20Efficient%20Generative%20Foundation%20Language%0A%20%20Models%20for%20Indian%20Languages&entry.906535625=Mitodru%20Niyogi%20and%20Arnab%20Bhattacharya&entry.1292438233=%20%20We%20present%20%22Paramanu%22%2C%20a%20family%20of%20novel%20language%20models%20%28LM%29%20for%20Indian%0Alanguages%2C%20consisting%20of%20auto-regressive%20monolingual%2C%20bilingual%2C%20and%0Amultilingual%20models%20pretrained%20from%20scratch.%20Currently%2C%20it%20covers%2010%20languages%0A%28Assamese%2C%20Bangla%2C%20Hindi%2C%20Konkani%2C%20Maithili%2C%20Marathi%2C%20Odia%2C%20Sanskrit%2C%20Tamil%2C%0ATelugu%29%20across%205%20scripts%20%28Bangla%2C%20Devanagari%2C%20Odia%2C%20Tamil%2C%20Telugu%29.%20The%20models%0Aare%20pretrained%20on%20a%20single%20GPU%20with%20context%20size%20of%201024%20and%20vary%20in%20size%20from%0A13.29%20million%20%28M%29%20to%20367.5%20M%20parameters.%20We%20proposed%20a%20RoPE%20embedding%20scaling%0Amethod%20that%20enables%20us%20to%20pretrain%20language%20models%20from%20scratch%20at%20larger%0Asequence%20length%20context%20size%20than%20typical%20GPU%20memory%20permits.%20We%20also%0Aintroduced%20a%20novel%20efficient%20Indic%20tokenizer%2C%20%22mBharat%22%2C%20using%20a%20combination%20of%0ABPE%20and%20Unigram%2C%20achieving%20the%20least%20fertility%20score%20and%20the%20ability%20to%0Atokenize%20unseen%20languages%20in%20both%20the%20same%20script%20%26%20Roman%20script.%20We%20also%0Aproposed%20and%20performed%20language-specific%20tokenization%20for%20multilingual%20models%20%26%0Adomain-specific%20tokenization%20for%20monolingual%20models.%20To%20address%20the%20%22curse%20of%0Amultilinguality%22%20in%20our%20mParamanu%20model%2C%20we%20pretrained%20on%20comparable%20corpora%0Abased%20on%20typological%20grouping%20within%20the%20same%20script.%20Our%20findings%20show%20a%0Alanguage%20transfer%20phenomenon%20from%20low-resource%20to%20high-resource%20languages%0Awithin%20languages%20of%20the%20same%20script%20%26%20typology.%20Human%20evaluations%20for%0Aopen-ended%20text%20generation%20demonstrated%20that%20Paramanu%20models%20outperformed%0Aseveral%20LLMs%2C%20despite%20being%2020%20to%2064%20times%20smaller.%20We%20created%0Ainstruction-tuning%20datasets%20%26%20instruction-tuned%20our%20models%20on%2023%2C000%0Ainstructions%20in%20respective%20languages.%20Comparisons%20with%20multilingual%20LLMs%20across%0Avarious%20benchmarks%20for%20natural%20language%20%28NL%29%20understanding%2C%20NL%20inference%2C%20%26%0Areading%20comprehension%20highlight%20the%20advantages%20of%20our%20models%3B%20leads%20to%20the%0Aconclusion%20that%20high%20quality%20generative%20LM%20are%20possible%20without%20high%20amount%20of%0Acompute%20power%20%26%20enormous%20number%20of%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.18034v2&entry.124074799=Read"},
{"title": "Non-transferable Pruning", "author": "Ruyi Ding and Lili Su and Aidong Adam Ding and Yunsi Fei", "abstract": "  Pretrained Deep Neural Networks (DNNs), developed from extensive datasets to\nintegrate multifaceted knowledge, are increasingly recognized as valuable\nintellectual property (IP). To safeguard these models against IP infringement,\nstrategies for ownership verification and usage authorization have emerged.\nUnlike most existing IP protection strategies that concentrate on restricting\ndirect access to the model, our study addresses an extended DNN IP issue:\napplicability authorization, aiming to prevent the misuse of learned knowledge,\nparticularly in unauthorized transfer learning scenarios. We propose\nNon-Transferable Pruning (NTP), a novel IP protection method that leverages\nmodel pruning to control a pretrained DNN's transferability to unauthorized\ndata domains. Selective pruning can deliberately diminish a model's suitability\non unauthorized domains, even with full fine-tuning. Specifically, our\nframework employs the alternating direction method of multipliers (ADMM) for\noptimizing both the model sparsity and an innovative non-transferable learning\nloss, augmented with Fisher space discriminative regularization, to constrain\nthe model's generalizability to the target dataset. We also propose a novel\neffective metric to measure the model non-transferability: Area Under the\nSample-wise Learning Curve (SLC-AUC). This metric facilitates consideration of\nfull fine-tuning across various sample sizes. Experimental results demonstrate\nthat NTP significantly surpasses the state-of-the-art non-transferable learning\nmethods, with an average SLC-AUC at $-0.54$ across diverse pairs of source and\ntarget domains, indicating that models trained with NTP do not suit for\ntransfer learning to unauthorized target domains. The efficacy of NTP is\nvalidated in both supervised and self-supervised learning contexts, confirming\nits applicability in real-world scenarios.\n", "link": "http://arxiv.org/abs/2410.08015v1", "date": "2024-10-10", "relevancy": 2.3023, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-transferable%20Pruning&body=Title%3A%20Non-transferable%20Pruning%0AAuthor%3A%20Ruyi%20Ding%20and%20Lili%20Su%20and%20Aidong%20Adam%20Ding%20and%20Yunsi%20Fei%0AAbstract%3A%20%20%20Pretrained%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20developed%20from%20extensive%20datasets%20to%0Aintegrate%20multifaceted%20knowledge%2C%20are%20increasingly%20recognized%20as%20valuable%0Aintellectual%20property%20%28IP%29.%20To%20safeguard%20these%20models%20against%20IP%20infringement%2C%0Astrategies%20for%20ownership%20verification%20and%20usage%20authorization%20have%20emerged.%0AUnlike%20most%20existing%20IP%20protection%20strategies%20that%20concentrate%20on%20restricting%0Adirect%20access%20to%20the%20model%2C%20our%20study%20addresses%20an%20extended%20DNN%20IP%20issue%3A%0Aapplicability%20authorization%2C%20aiming%20to%20prevent%20the%20misuse%20of%20learned%20knowledge%2C%0Aparticularly%20in%20unauthorized%20transfer%20learning%20scenarios.%20We%20propose%0ANon-Transferable%20Pruning%20%28NTP%29%2C%20a%20novel%20IP%20protection%20method%20that%20leverages%0Amodel%20pruning%20to%20control%20a%20pretrained%20DNN%27s%20transferability%20to%20unauthorized%0Adata%20domains.%20Selective%20pruning%20can%20deliberately%20diminish%20a%20model%27s%20suitability%0Aon%20unauthorized%20domains%2C%20even%20with%20full%20fine-tuning.%20Specifically%2C%20our%0Aframework%20employs%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%0Aoptimizing%20both%20the%20model%20sparsity%20and%20an%20innovative%20non-transferable%20learning%0Aloss%2C%20augmented%20with%20Fisher%20space%20discriminative%20regularization%2C%20to%20constrain%0Athe%20model%27s%20generalizability%20to%20the%20target%20dataset.%20We%20also%20propose%20a%20novel%0Aeffective%20metric%20to%20measure%20the%20model%20non-transferability%3A%20Area%20Under%20the%0ASample-wise%20Learning%20Curve%20%28SLC-AUC%29.%20This%20metric%20facilitates%20consideration%20of%0Afull%20fine-tuning%20across%20various%20sample%20sizes.%20Experimental%20results%20demonstrate%0Athat%20NTP%20significantly%20surpasses%20the%20state-of-the-art%20non-transferable%20learning%0Amethods%2C%20with%20an%20average%20SLC-AUC%20at%20%24-0.54%24%20across%20diverse%20pairs%20of%20source%20and%0Atarget%20domains%2C%20indicating%20that%20models%20trained%20with%20NTP%20do%20not%20suit%20for%0Atransfer%20learning%20to%20unauthorized%20target%20domains.%20The%20efficacy%20of%20NTP%20is%0Avalidated%20in%20both%20supervised%20and%20self-supervised%20learning%20contexts%2C%20confirming%0Aits%20applicability%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-transferable%2520Pruning%26entry.906535625%3DRuyi%2520Ding%2520and%2520Lili%2520Su%2520and%2520Aidong%2520Adam%2520Ding%2520and%2520Yunsi%2520Fei%26entry.1292438233%3D%2520%2520Pretrained%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520developed%2520from%2520extensive%2520datasets%2520to%250Aintegrate%2520multifaceted%2520knowledge%252C%2520are%2520increasingly%2520recognized%2520as%2520valuable%250Aintellectual%2520property%2520%2528IP%2529.%2520To%2520safeguard%2520these%2520models%2520against%2520IP%2520infringement%252C%250Astrategies%2520for%2520ownership%2520verification%2520and%2520usage%2520authorization%2520have%2520emerged.%250AUnlike%2520most%2520existing%2520IP%2520protection%2520strategies%2520that%2520concentrate%2520on%2520restricting%250Adirect%2520access%2520to%2520the%2520model%252C%2520our%2520study%2520addresses%2520an%2520extended%2520DNN%2520IP%2520issue%253A%250Aapplicability%2520authorization%252C%2520aiming%2520to%2520prevent%2520the%2520misuse%2520of%2520learned%2520knowledge%252C%250Aparticularly%2520in%2520unauthorized%2520transfer%2520learning%2520scenarios.%2520We%2520propose%250ANon-Transferable%2520Pruning%2520%2528NTP%2529%252C%2520a%2520novel%2520IP%2520protection%2520method%2520that%2520leverages%250Amodel%2520pruning%2520to%2520control%2520a%2520pretrained%2520DNN%2527s%2520transferability%2520to%2520unauthorized%250Adata%2520domains.%2520Selective%2520pruning%2520can%2520deliberately%2520diminish%2520a%2520model%2527s%2520suitability%250Aon%2520unauthorized%2520domains%252C%2520even%2520with%2520full%2520fine-tuning.%2520Specifically%252C%2520our%250Aframework%2520employs%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520for%250Aoptimizing%2520both%2520the%2520model%2520sparsity%2520and%2520an%2520innovative%2520non-transferable%2520learning%250Aloss%252C%2520augmented%2520with%2520Fisher%2520space%2520discriminative%2520regularization%252C%2520to%2520constrain%250Athe%2520model%2527s%2520generalizability%2520to%2520the%2520target%2520dataset.%2520We%2520also%2520propose%2520a%2520novel%250Aeffective%2520metric%2520to%2520measure%2520the%2520model%2520non-transferability%253A%2520Area%2520Under%2520the%250ASample-wise%2520Learning%2520Curve%2520%2528SLC-AUC%2529.%2520This%2520metric%2520facilitates%2520consideration%2520of%250Afull%2520fine-tuning%2520across%2520various%2520sample%2520sizes.%2520Experimental%2520results%2520demonstrate%250Athat%2520NTP%2520significantly%2520surpasses%2520the%2520state-of-the-art%2520non-transferable%2520learning%250Amethods%252C%2520with%2520an%2520average%2520SLC-AUC%2520at%2520%2524-0.54%2524%2520across%2520diverse%2520pairs%2520of%2520source%2520and%250Atarget%2520domains%252C%2520indicating%2520that%2520models%2520trained%2520with%2520NTP%2520do%2520not%2520suit%2520for%250Atransfer%2520learning%2520to%2520unauthorized%2520target%2520domains.%2520The%2520efficacy%2520of%2520NTP%2520is%250Avalidated%2520in%2520both%2520supervised%2520and%2520self-supervised%2520learning%2520contexts%252C%2520confirming%250Aits%2520applicability%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-transferable%20Pruning&entry.906535625=Ruyi%20Ding%20and%20Lili%20Su%20and%20Aidong%20Adam%20Ding%20and%20Yunsi%20Fei&entry.1292438233=%20%20Pretrained%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20developed%20from%20extensive%20datasets%20to%0Aintegrate%20multifaceted%20knowledge%2C%20are%20increasingly%20recognized%20as%20valuable%0Aintellectual%20property%20%28IP%29.%20To%20safeguard%20these%20models%20against%20IP%20infringement%2C%0Astrategies%20for%20ownership%20verification%20and%20usage%20authorization%20have%20emerged.%0AUnlike%20most%20existing%20IP%20protection%20strategies%20that%20concentrate%20on%20restricting%0Adirect%20access%20to%20the%20model%2C%20our%20study%20addresses%20an%20extended%20DNN%20IP%20issue%3A%0Aapplicability%20authorization%2C%20aiming%20to%20prevent%20the%20misuse%20of%20learned%20knowledge%2C%0Aparticularly%20in%20unauthorized%20transfer%20learning%20scenarios.%20We%20propose%0ANon-Transferable%20Pruning%20%28NTP%29%2C%20a%20novel%20IP%20protection%20method%20that%20leverages%0Amodel%20pruning%20to%20control%20a%20pretrained%20DNN%27s%20transferability%20to%20unauthorized%0Adata%20domains.%20Selective%20pruning%20can%20deliberately%20diminish%20a%20model%27s%20suitability%0Aon%20unauthorized%20domains%2C%20even%20with%20full%20fine-tuning.%20Specifically%2C%20our%0Aframework%20employs%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%0Aoptimizing%20both%20the%20model%20sparsity%20and%20an%20innovative%20non-transferable%20learning%0Aloss%2C%20augmented%20with%20Fisher%20space%20discriminative%20regularization%2C%20to%20constrain%0Athe%20model%27s%20generalizability%20to%20the%20target%20dataset.%20We%20also%20propose%20a%20novel%0Aeffective%20metric%20to%20measure%20the%20model%20non-transferability%3A%20Area%20Under%20the%0ASample-wise%20Learning%20Curve%20%28SLC-AUC%29.%20This%20metric%20facilitates%20consideration%20of%0Afull%20fine-tuning%20across%20various%20sample%20sizes.%20Experimental%20results%20demonstrate%0Athat%20NTP%20significantly%20surpasses%20the%20state-of-the-art%20non-transferable%20learning%0Amethods%2C%20with%20an%20average%20SLC-AUC%20at%20%24-0.54%24%20across%20diverse%20pairs%20of%20source%20and%0Atarget%20domains%2C%20indicating%20that%20models%20trained%20with%20NTP%20do%20not%20suit%20for%0Atransfer%20learning%20to%20unauthorized%20target%20domains.%20The%20efficacy%20of%20NTP%20is%0Avalidated%20in%20both%20supervised%20and%20self-supervised%20learning%20contexts%2C%20confirming%0Aits%20applicability%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08015v1&entry.124074799=Read"},
{"title": "Learning Equivariant Non-Local Electron Density Functionals", "author": "Nicholas Gao and Eike Eberhard and Stephan G\u00fcnnemann", "abstract": "  The accuracy of density functional theory hinges on the approximation of\nnon-local contributions to the exchange-correlation (XC) functional. To date,\nmachine-learned and human-designed approximations suffer from insufficient\naccuracy, limited scalability, or dependence on costly reference data. To\naddress these issues, we introduce Equivariant Graph Exchange Correlation\n(EG-XC), a novel non-local XC functional based on equivariant graph neural\nnetworks. EG-XC combines semi-local functionals with a non-local feature\ndensity parametrized by an equivariant nuclei-centered point cloud\nrepresentation of the electron density to capture long-range interactions. By\ndifferentiating through a self-consistent field solver, we train EG-XC\nrequiring only energy targets. In our empirical evaluation, we find EG-XC to\naccurately reconstruct `gold-standard' CCSD(T) energies on MD17. On\nout-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by\n35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size\nextrapolation on QM9, matching force fields trained on 5 times more and larger\nmolecules. On identical training sets, EG-XC yields on average 51% lower MAEs.\n", "link": "http://arxiv.org/abs/2410.07972v1", "date": "2024-10-10", "relevancy": 2.2989, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4793}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4565}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals&body=Title%3A%20Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals%0AAuthor%3A%20Nicholas%20Gao%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20The%20accuracy%20of%20density%20functional%20theory%20hinges%20on%20the%20approximation%20of%0Anon-local%20contributions%20to%20the%20exchange-correlation%20%28XC%29%20functional.%20To%20date%2C%0Amachine-learned%20and%20human-designed%20approximations%20suffer%20from%20insufficient%0Aaccuracy%2C%20limited%20scalability%2C%20or%20dependence%20on%20costly%20reference%20data.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Equivariant%20Graph%20Exchange%20Correlation%0A%28EG-XC%29%2C%20a%20novel%20non-local%20XC%20functional%20based%20on%20equivariant%20graph%20neural%0Anetworks.%20EG-XC%20combines%20semi-local%20functionals%20with%20a%20non-local%20feature%0Adensity%20parametrized%20by%20an%20equivariant%20nuclei-centered%20point%20cloud%0Arepresentation%20of%20the%20electron%20density%20to%20capture%20long-range%20interactions.%20By%0Adifferentiating%20through%20a%20self-consistent%20field%20solver%2C%20we%20train%20EG-XC%0Arequiring%20only%20energy%20targets.%20In%20our%20empirical%20evaluation%2C%20we%20find%20EG-XC%20to%0Aaccurately%20reconstruct%20%60gold-standard%27%20CCSD%28T%29%20energies%20on%20MD17.%20On%0Aout-of-distribution%20conformations%20of%203BPA%2C%20EG-XC%20reduces%20the%20relative%20MAE%20by%0A35%25%20to%2050%25.%20Remarkably%2C%20EG-XC%20excels%20in%20data%20efficiency%20and%20molecular%20size%0Aextrapolation%20on%20QM9%2C%20matching%20force%20fields%20trained%20on%205%20times%20more%20and%20larger%0Amolecules.%20On%20identical%20training%20sets%2C%20EG-XC%20yields%20on%20average%2051%25%20lower%20MAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Equivariant%2520Non-Local%2520Electron%2520Density%2520Functionals%26entry.906535625%3DNicholas%2520Gao%2520and%2520Eike%2520Eberhard%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520The%2520accuracy%2520of%2520density%2520functional%2520theory%2520hinges%2520on%2520the%2520approximation%2520of%250Anon-local%2520contributions%2520to%2520the%2520exchange-correlation%2520%2528XC%2529%2520functional.%2520To%2520date%252C%250Amachine-learned%2520and%2520human-designed%2520approximations%2520suffer%2520from%2520insufficient%250Aaccuracy%252C%2520limited%2520scalability%252C%2520or%2520dependence%2520on%2520costly%2520reference%2520data.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520Equivariant%2520Graph%2520Exchange%2520Correlation%250A%2528EG-XC%2529%252C%2520a%2520novel%2520non-local%2520XC%2520functional%2520based%2520on%2520equivariant%2520graph%2520neural%250Anetworks.%2520EG-XC%2520combines%2520semi-local%2520functionals%2520with%2520a%2520non-local%2520feature%250Adensity%2520parametrized%2520by%2520an%2520equivariant%2520nuclei-centered%2520point%2520cloud%250Arepresentation%2520of%2520the%2520electron%2520density%2520to%2520capture%2520long-range%2520interactions.%2520By%250Adifferentiating%2520through%2520a%2520self-consistent%2520field%2520solver%252C%2520we%2520train%2520EG-XC%250Arequiring%2520only%2520energy%2520targets.%2520In%2520our%2520empirical%2520evaluation%252C%2520we%2520find%2520EG-XC%2520to%250Aaccurately%2520reconstruct%2520%2560gold-standard%2527%2520CCSD%2528T%2529%2520energies%2520on%2520MD17.%2520On%250Aout-of-distribution%2520conformations%2520of%25203BPA%252C%2520EG-XC%2520reduces%2520the%2520relative%2520MAE%2520by%250A35%2525%2520to%252050%2525.%2520Remarkably%252C%2520EG-XC%2520excels%2520in%2520data%2520efficiency%2520and%2520molecular%2520size%250Aextrapolation%2520on%2520QM9%252C%2520matching%2520force%2520fields%2520trained%2520on%25205%2520times%2520more%2520and%2520larger%250Amolecules.%2520On%2520identical%2520training%2520sets%252C%2520EG-XC%2520yields%2520on%2520average%252051%2525%2520lower%2520MAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Equivariant%20Non-Local%20Electron%20Density%20Functionals&entry.906535625=Nicholas%20Gao%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20The%20accuracy%20of%20density%20functional%20theory%20hinges%20on%20the%20approximation%20of%0Anon-local%20contributions%20to%20the%20exchange-correlation%20%28XC%29%20functional.%20To%20date%2C%0Amachine-learned%20and%20human-designed%20approximations%20suffer%20from%20insufficient%0Aaccuracy%2C%20limited%20scalability%2C%20or%20dependence%20on%20costly%20reference%20data.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20Equivariant%20Graph%20Exchange%20Correlation%0A%28EG-XC%29%2C%20a%20novel%20non-local%20XC%20functional%20based%20on%20equivariant%20graph%20neural%0Anetworks.%20EG-XC%20combines%20semi-local%20functionals%20with%20a%20non-local%20feature%0Adensity%20parametrized%20by%20an%20equivariant%20nuclei-centered%20point%20cloud%0Arepresentation%20of%20the%20electron%20density%20to%20capture%20long-range%20interactions.%20By%0Adifferentiating%20through%20a%20self-consistent%20field%20solver%2C%20we%20train%20EG-XC%0Arequiring%20only%20energy%20targets.%20In%20our%20empirical%20evaluation%2C%20we%20find%20EG-XC%20to%0Aaccurately%20reconstruct%20%60gold-standard%27%20CCSD%28T%29%20energies%20on%20MD17.%20On%0Aout-of-distribution%20conformations%20of%203BPA%2C%20EG-XC%20reduces%20the%20relative%20MAE%20by%0A35%25%20to%2050%25.%20Remarkably%2C%20EG-XC%20excels%20in%20data%20efficiency%20and%20molecular%20size%0Aextrapolation%20on%20QM9%2C%20matching%20force%20fields%20trained%20on%205%20times%20more%20and%20larger%0Amolecules.%20On%20identical%20training%20sets%2C%20EG-XC%20yields%20on%20average%2051%25%20lower%20MAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07972v1&entry.124074799=Read"},
{"title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts", "author": "Anh-Quan Cao and Maximilian Jaritz and Matthieu Guillaumin and Raoul de Charette and Loris Bazzani", "abstract": "  Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.\n", "link": "http://arxiv.org/abs/2410.08211v1", "date": "2024-10-10", "relevancy": 2.2945, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.615}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5474}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatteCLIP%3A%20Unsupervised%20CLIP%20Fine-Tuning%20via%20LMM-Synthetic%20Texts&body=Title%3A%20LatteCLIP%3A%20Unsupervised%20CLIP%20Fine-Tuning%20via%20LMM-Synthetic%20Texts%0AAuthor%3A%20Anh-Quan%20Cao%20and%20Maximilian%20Jaritz%20and%20Matthieu%20Guillaumin%20and%20Raoul%20de%20Charette%20and%20Loris%20Bazzani%0AAbstract%3A%20%20%20Large-scale%20vision-language%20pre-trained%20%28VLP%29%20models%20%28e.g.%2C%20CLIP%29%20are%0Arenowned%20for%20their%20versatility%2C%20as%20they%20can%20be%20applied%20to%20diverse%20applications%0Ain%20a%20zero-shot%20setup.%20However%2C%20when%20these%20models%20are%20used%20in%20specific%20domains%2C%0Atheir%20performance%20often%20falls%20short%20due%20to%20domain%20gaps%20or%20the%0Aunder-representation%20of%20these%20domains%20in%20the%20training%20data.%20While%20fine-tuning%0AVLP%20models%20on%20custom%20datasets%20with%20human-annotated%20labels%20can%20address%20this%0Aissue%2C%20annotating%20even%20a%20small-scale%20dataset%20%28e.g.%2C%20100k%20samples%29%20can%20be%20an%0Aexpensive%20endeavor%2C%20often%20requiring%20expert%20annotators%20if%20the%20task%20is%20complex.%0ATo%20address%20these%20challenges%2C%20we%20propose%20LatteCLIP%2C%20an%20unsupervised%20method%20for%0Afine-tuning%20CLIP%20models%20on%20classification%20with%20known%20class%20names%20in%20custom%0Adomains%2C%20without%20relying%20on%20human%20annotations.%20Our%20method%20leverages%20Large%0AMultimodal%20Models%20%28LMMs%29%20to%20generate%20expressive%20textual%20descriptions%20for%20both%0Aindividual%20images%20and%20groups%20of%20images.%20These%20provide%20additional%20contextual%0Ainformation%20to%20guide%20the%20fine-tuning%20process%20in%20the%20custom%20domains.%20Since%0ALMM-generated%20descriptions%20are%20prone%20to%20hallucination%20or%20missing%20details%2C%20we%0Aintroduce%20a%20novel%20strategy%20to%20distill%20only%20the%20useful%20information%20and%20stabilize%0Athe%20training.%20Specifically%2C%20we%20learn%20rich%20per-class%20prototype%20representations%0Afrom%20noisy%20generated%20texts%20and%20dual%20pseudo-labels.%20Our%20experiments%20on%2010%0Adomain-specific%20datasets%20show%20that%20LatteCLIP%20outperforms%20pre-trained%20zero-shot%0Amethods%20by%20an%20average%20improvement%20of%20%2B4.74%20points%20in%20top-1%20accuracy%20and%20other%0Astate-of-the-art%20unsupervised%20methods%20by%20%2B3.45%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatteCLIP%253A%2520Unsupervised%2520CLIP%2520Fine-Tuning%2520via%2520LMM-Synthetic%2520Texts%26entry.906535625%3DAnh-Quan%2520Cao%2520and%2520Maximilian%2520Jaritz%2520and%2520Matthieu%2520Guillaumin%2520and%2520Raoul%2520de%2520Charette%2520and%2520Loris%2520Bazzani%26entry.1292438233%3D%2520%2520Large-scale%2520vision-language%2520pre-trained%2520%2528VLP%2529%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520are%250Arenowned%2520for%2520their%2520versatility%252C%2520as%2520they%2520can%2520be%2520applied%2520to%2520diverse%2520applications%250Ain%2520a%2520zero-shot%2520setup.%2520However%252C%2520when%2520these%2520models%2520are%2520used%2520in%2520specific%2520domains%252C%250Atheir%2520performance%2520often%2520falls%2520short%2520due%2520to%2520domain%2520gaps%2520or%2520the%250Aunder-representation%2520of%2520these%2520domains%2520in%2520the%2520training%2520data.%2520While%2520fine-tuning%250AVLP%2520models%2520on%2520custom%2520datasets%2520with%2520human-annotated%2520labels%2520can%2520address%2520this%250Aissue%252C%2520annotating%2520even%2520a%2520small-scale%2520dataset%2520%2528e.g.%252C%2520100k%2520samples%2529%2520can%2520be%2520an%250Aexpensive%2520endeavor%252C%2520often%2520requiring%2520expert%2520annotators%2520if%2520the%2520task%2520is%2520complex.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520LatteCLIP%252C%2520an%2520unsupervised%2520method%2520for%250Afine-tuning%2520CLIP%2520models%2520on%2520classification%2520with%2520known%2520class%2520names%2520in%2520custom%250Adomains%252C%2520without%2520relying%2520on%2520human%2520annotations.%2520Our%2520method%2520leverages%2520Large%250AMultimodal%2520Models%2520%2528LMMs%2529%2520to%2520generate%2520expressive%2520textual%2520descriptions%2520for%2520both%250Aindividual%2520images%2520and%2520groups%2520of%2520images.%2520These%2520provide%2520additional%2520contextual%250Ainformation%2520to%2520guide%2520the%2520fine-tuning%2520process%2520in%2520the%2520custom%2520domains.%2520Since%250ALMM-generated%2520descriptions%2520are%2520prone%2520to%2520hallucination%2520or%2520missing%2520details%252C%2520we%250Aintroduce%2520a%2520novel%2520strategy%2520to%2520distill%2520only%2520the%2520useful%2520information%2520and%2520stabilize%250Athe%2520training.%2520Specifically%252C%2520we%2520learn%2520rich%2520per-class%2520prototype%2520representations%250Afrom%2520noisy%2520generated%2520texts%2520and%2520dual%2520pseudo-labels.%2520Our%2520experiments%2520on%252010%250Adomain-specific%2520datasets%2520show%2520that%2520LatteCLIP%2520outperforms%2520pre-trained%2520zero-shot%250Amethods%2520by%2520an%2520average%2520improvement%2520of%2520%252B4.74%2520points%2520in%2520top-1%2520accuracy%2520and%2520other%250Astate-of-the-art%2520unsupervised%2520methods%2520by%2520%252B3.45%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatteCLIP%3A%20Unsupervised%20CLIP%20Fine-Tuning%20via%20LMM-Synthetic%20Texts&entry.906535625=Anh-Quan%20Cao%20and%20Maximilian%20Jaritz%20and%20Matthieu%20Guillaumin%20and%20Raoul%20de%20Charette%20and%20Loris%20Bazzani&entry.1292438233=%20%20Large-scale%20vision-language%20pre-trained%20%28VLP%29%20models%20%28e.g.%2C%20CLIP%29%20are%0Arenowned%20for%20their%20versatility%2C%20as%20they%20can%20be%20applied%20to%20diverse%20applications%0Ain%20a%20zero-shot%20setup.%20However%2C%20when%20these%20models%20are%20used%20in%20specific%20domains%2C%0Atheir%20performance%20often%20falls%20short%20due%20to%20domain%20gaps%20or%20the%0Aunder-representation%20of%20these%20domains%20in%20the%20training%20data.%20While%20fine-tuning%0AVLP%20models%20on%20custom%20datasets%20with%20human-annotated%20labels%20can%20address%20this%0Aissue%2C%20annotating%20even%20a%20small-scale%20dataset%20%28e.g.%2C%20100k%20samples%29%20can%20be%20an%0Aexpensive%20endeavor%2C%20often%20requiring%20expert%20annotators%20if%20the%20task%20is%20complex.%0ATo%20address%20these%20challenges%2C%20we%20propose%20LatteCLIP%2C%20an%20unsupervised%20method%20for%0Afine-tuning%20CLIP%20models%20on%20classification%20with%20known%20class%20names%20in%20custom%0Adomains%2C%20without%20relying%20on%20human%20annotations.%20Our%20method%20leverages%20Large%0AMultimodal%20Models%20%28LMMs%29%20to%20generate%20expressive%20textual%20descriptions%20for%20both%0Aindividual%20images%20and%20groups%20of%20images.%20These%20provide%20additional%20contextual%0Ainformation%20to%20guide%20the%20fine-tuning%20process%20in%20the%20custom%20domains.%20Since%0ALMM-generated%20descriptions%20are%20prone%20to%20hallucination%20or%20missing%20details%2C%20we%0Aintroduce%20a%20novel%20strategy%20to%20distill%20only%20the%20useful%20information%20and%20stabilize%0Athe%20training.%20Specifically%2C%20we%20learn%20rich%20per-class%20prototype%20representations%0Afrom%20noisy%20generated%20texts%20and%20dual%20pseudo-labels.%20Our%20experiments%20on%2010%0Adomain-specific%20datasets%20show%20that%20LatteCLIP%20outperforms%20pre-trained%20zero-shot%0Amethods%20by%20an%20average%20improvement%20of%20%2B4.74%20points%20in%20top-1%20accuracy%20and%20other%0Astate-of-the-art%20unsupervised%20methods%20by%20%2B3.45%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08211v1&entry.124074799=Read"},
{"title": "Visual Scratchpads: Enabling Global Reasoning in Vision", "author": "Aryo Lotfi and Enrico Fini and Samy Bengio and Moin Nabi and Emmanuel Abbe", "abstract": "  Modern vision models have achieved remarkable success in benchmarks where\nlocal features provide critical information about the target. There is now a\ngrowing interest in solving tasks that require more global reasoning, where\nlocal features offer no significant information. These tasks are reminiscent of\nthe connectivity tasks discussed by Minsky and Papert in 1969, which exposed\nthe limitations of the perceptron model and contributed to the first AI winter.\nIn this paper, we revisit such tasks by introducing four global visual\nbenchmarks involving path findings and mazes. We show that: (1) although\ntoday's large vision models largely surpass the expressivity limitations of the\nearly models, they still struggle with the learning efficiency; we put forward\nthe \"globality degree\" notion to understand this limitation; (2) we then\ndemonstrate that the picture changes and global reasoning becomes feasible with\nthe introduction of \"visual scratchpads\"; similarly to the text scratchpads and\nchain-of-thoughts used in language models, visual scratchpads help break down\nglobal tasks into simpler ones; (3) we finally show that some scratchpads are\nbetter than others, in particular, \"inductive scratchpads\" that take steps\nrelying on less information afford better out-of-distribution generalization\nand succeed for smaller model sizes.\n", "link": "http://arxiv.org/abs/2410.08165v1", "date": "2024-10-10", "relevancy": 2.2865, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Scratchpads%3A%20Enabling%20Global%20Reasoning%20in%20Vision&body=Title%3A%20Visual%20Scratchpads%3A%20Enabling%20Global%20Reasoning%20in%20Vision%0AAuthor%3A%20Aryo%20Lotfi%20and%20Enrico%20Fini%20and%20Samy%20Bengio%20and%20Moin%20Nabi%20and%20Emmanuel%20Abbe%0AAbstract%3A%20%20%20Modern%20vision%20models%20have%20achieved%20remarkable%20success%20in%20benchmarks%20where%0Alocal%20features%20provide%20critical%20information%20about%20the%20target.%20There%20is%20now%20a%0Agrowing%20interest%20in%20solving%20tasks%20that%20require%20more%20global%20reasoning%2C%20where%0Alocal%20features%20offer%20no%20significant%20information.%20These%20tasks%20are%20reminiscent%20of%0Athe%20connectivity%20tasks%20discussed%20by%20Minsky%20and%20Papert%20in%201969%2C%20which%20exposed%0Athe%20limitations%20of%20the%20perceptron%20model%20and%20contributed%20to%20the%20first%20AI%20winter.%0AIn%20this%20paper%2C%20we%20revisit%20such%20tasks%20by%20introducing%20four%20global%20visual%0Abenchmarks%20involving%20path%20findings%20and%20mazes.%20We%20show%20that%3A%20%281%29%20although%0Atoday%27s%20large%20vision%20models%20largely%20surpass%20the%20expressivity%20limitations%20of%20the%0Aearly%20models%2C%20they%20still%20struggle%20with%20the%20learning%20efficiency%3B%20we%20put%20forward%0Athe%20%22globality%20degree%22%20notion%20to%20understand%20this%20limitation%3B%20%282%29%20we%20then%0Ademonstrate%20that%20the%20picture%20changes%20and%20global%20reasoning%20becomes%20feasible%20with%0Athe%20introduction%20of%20%22visual%20scratchpads%22%3B%20similarly%20to%20the%20text%20scratchpads%20and%0Achain-of-thoughts%20used%20in%20language%20models%2C%20visual%20scratchpads%20help%20break%20down%0Aglobal%20tasks%20into%20simpler%20ones%3B%20%283%29%20we%20finally%20show%20that%20some%20scratchpads%20are%0Abetter%20than%20others%2C%20in%20particular%2C%20%22inductive%20scratchpads%22%20that%20take%20steps%0Arelying%20on%20less%20information%20afford%20better%20out-of-distribution%20generalization%0Aand%20succeed%20for%20smaller%20model%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Scratchpads%253A%2520Enabling%2520Global%2520Reasoning%2520in%2520Vision%26entry.906535625%3DAryo%2520Lotfi%2520and%2520Enrico%2520Fini%2520and%2520Samy%2520Bengio%2520and%2520Moin%2520Nabi%2520and%2520Emmanuel%2520Abbe%26entry.1292438233%3D%2520%2520Modern%2520vision%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520benchmarks%2520where%250Alocal%2520features%2520provide%2520critical%2520information%2520about%2520the%2520target.%2520There%2520is%2520now%2520a%250Agrowing%2520interest%2520in%2520solving%2520tasks%2520that%2520require%2520more%2520global%2520reasoning%252C%2520where%250Alocal%2520features%2520offer%2520no%2520significant%2520information.%2520These%2520tasks%2520are%2520reminiscent%2520of%250Athe%2520connectivity%2520tasks%2520discussed%2520by%2520Minsky%2520and%2520Papert%2520in%25201969%252C%2520which%2520exposed%250Athe%2520limitations%2520of%2520the%2520perceptron%2520model%2520and%2520contributed%2520to%2520the%2520first%2520AI%2520winter.%250AIn%2520this%2520paper%252C%2520we%2520revisit%2520such%2520tasks%2520by%2520introducing%2520four%2520global%2520visual%250Abenchmarks%2520involving%2520path%2520findings%2520and%2520mazes.%2520We%2520show%2520that%253A%2520%25281%2529%2520although%250Atoday%2527s%2520large%2520vision%2520models%2520largely%2520surpass%2520the%2520expressivity%2520limitations%2520of%2520the%250Aearly%2520models%252C%2520they%2520still%2520struggle%2520with%2520the%2520learning%2520efficiency%253B%2520we%2520put%2520forward%250Athe%2520%2522globality%2520degree%2522%2520notion%2520to%2520understand%2520this%2520limitation%253B%2520%25282%2529%2520we%2520then%250Ademonstrate%2520that%2520the%2520picture%2520changes%2520and%2520global%2520reasoning%2520becomes%2520feasible%2520with%250Athe%2520introduction%2520of%2520%2522visual%2520scratchpads%2522%253B%2520similarly%2520to%2520the%2520text%2520scratchpads%2520and%250Achain-of-thoughts%2520used%2520in%2520language%2520models%252C%2520visual%2520scratchpads%2520help%2520break%2520down%250Aglobal%2520tasks%2520into%2520simpler%2520ones%253B%2520%25283%2529%2520we%2520finally%2520show%2520that%2520some%2520scratchpads%2520are%250Abetter%2520than%2520others%252C%2520in%2520particular%252C%2520%2522inductive%2520scratchpads%2522%2520that%2520take%2520steps%250Arelying%2520on%2520less%2520information%2520afford%2520better%2520out-of-distribution%2520generalization%250Aand%2520succeed%2520for%2520smaller%2520model%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Scratchpads%3A%20Enabling%20Global%20Reasoning%20in%20Vision&entry.906535625=Aryo%20Lotfi%20and%20Enrico%20Fini%20and%20Samy%20Bengio%20and%20Moin%20Nabi%20and%20Emmanuel%20Abbe&entry.1292438233=%20%20Modern%20vision%20models%20have%20achieved%20remarkable%20success%20in%20benchmarks%20where%0Alocal%20features%20provide%20critical%20information%20about%20the%20target.%20There%20is%20now%20a%0Agrowing%20interest%20in%20solving%20tasks%20that%20require%20more%20global%20reasoning%2C%20where%0Alocal%20features%20offer%20no%20significant%20information.%20These%20tasks%20are%20reminiscent%20of%0Athe%20connectivity%20tasks%20discussed%20by%20Minsky%20and%20Papert%20in%201969%2C%20which%20exposed%0Athe%20limitations%20of%20the%20perceptron%20model%20and%20contributed%20to%20the%20first%20AI%20winter.%0AIn%20this%20paper%2C%20we%20revisit%20such%20tasks%20by%20introducing%20four%20global%20visual%0Abenchmarks%20involving%20path%20findings%20and%20mazes.%20We%20show%20that%3A%20%281%29%20although%0Atoday%27s%20large%20vision%20models%20largely%20surpass%20the%20expressivity%20limitations%20of%20the%0Aearly%20models%2C%20they%20still%20struggle%20with%20the%20learning%20efficiency%3B%20we%20put%20forward%0Athe%20%22globality%20degree%22%20notion%20to%20understand%20this%20limitation%3B%20%282%29%20we%20then%0Ademonstrate%20that%20the%20picture%20changes%20and%20global%20reasoning%20becomes%20feasible%20with%0Athe%20introduction%20of%20%22visual%20scratchpads%22%3B%20similarly%20to%20the%20text%20scratchpads%20and%0Achain-of-thoughts%20used%20in%20language%20models%2C%20visual%20scratchpads%20help%20break%20down%0Aglobal%20tasks%20into%20simpler%20ones%3B%20%283%29%20we%20finally%20show%20that%20some%20scratchpads%20are%0Abetter%20than%20others%2C%20in%20particular%2C%20%22inductive%20scratchpads%22%20that%20take%20steps%0Arelying%20on%20less%20information%20afford%20better%20out-of-distribution%20generalization%0Aand%20succeed%20for%20smaller%20model%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08165v1&entry.124074799=Read"},
{"title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding\n  Supervision", "author": "Shengcao Cao and Liang-Yan Gui and Yu-Xiong Wang", "abstract": "  Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://groundLMM.github.io.\n", "link": "http://arxiv.org/abs/2410.08209v1", "date": "2024-10-10", "relevancy": 2.2851, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emerging%20Pixel%20Grounding%20in%20Large%20Multimodal%20Models%20Without%20Grounding%0A%20%20Supervision&body=Title%3A%20Emerging%20Pixel%20Grounding%20in%20Large%20Multimodal%20Models%20Without%20Grounding%0A%20%20Supervision%0AAuthor%3A%20Shengcao%20Cao%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Current%20large%20multimodal%20models%20%28LMMs%29%20face%20challenges%20in%20grounding%2C%20which%0Arequires%20the%20model%20to%20relate%20language%20components%20to%20visual%20entities.%20Contrary%0Ato%20the%20common%20practice%20that%20fine-tunes%20LMMs%20with%20additional%20grounding%0Asupervision%2C%20we%20find%20that%20the%20grounding%20ability%20can%20in%20fact%20emerge%20in%20LMMs%0Atrained%20without%20explicit%20grounding%20supervision.%20To%20reveal%20this%20emerging%0Agrounding%2C%20we%20introduce%20an%20%22attend-and-segment%22%20method%20which%20leverages%0Aattention%20maps%20from%20standard%20LMMs%20to%20perform%20pixel-level%20segmentation.%0AFurthermore%2C%20to%20enhance%20the%20grounding%20ability%2C%20we%20propose%20DIFFLMM%2C%20an%20LMM%0Autilizing%20a%20diffusion-based%20visual%20encoder%2C%20as%20opposed%20to%20the%20standard%20CLIP%0Avisual%20encoder%2C%20and%20trained%20with%20the%20same%20weak%20supervision.%20Without%20being%0Aconstrained%20by%20the%20biases%20and%20limited%20scale%20of%20grounding-specific%20supervision%0Adata%2C%20our%20approach%20is%20more%20generalizable%20and%20scalable.%20We%20achieve%20competitive%0Aperformance%20on%20both%20grounding-specific%20and%20general%20visual%20question%20answering%0Abenchmarks%2C%20compared%20with%20grounding%20LMMs%20and%20generalist%20LMMs%2C%20respectively.%0ANotably%2C%20we%20achieve%20a%2044.2%20grounding%20mask%20recall%20on%20grounded%20conversation%0Ageneration%20without%20any%20grounding%20supervision%2C%20outperforming%20the%20extensively%0Asupervised%20model%20GLaMM.%20Project%20page%3A%20https%3A//groundLMM.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmerging%2520Pixel%2520Grounding%2520in%2520Large%2520Multimodal%2520Models%2520Without%2520Grounding%250A%2520%2520Supervision%26entry.906535625%3DShengcao%2520Cao%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Current%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520face%2520challenges%2520in%2520grounding%252C%2520which%250Arequires%2520the%2520model%2520to%2520relate%2520language%2520components%2520to%2520visual%2520entities.%2520Contrary%250Ato%2520the%2520common%2520practice%2520that%2520fine-tunes%2520LMMs%2520with%2520additional%2520grounding%250Asupervision%252C%2520we%2520find%2520that%2520the%2520grounding%2520ability%2520can%2520in%2520fact%2520emerge%2520in%2520LMMs%250Atrained%2520without%2520explicit%2520grounding%2520supervision.%2520To%2520reveal%2520this%2520emerging%250Agrounding%252C%2520we%2520introduce%2520an%2520%2522attend-and-segment%2522%2520method%2520which%2520leverages%250Aattention%2520maps%2520from%2520standard%2520LMMs%2520to%2520perform%2520pixel-level%2520segmentation.%250AFurthermore%252C%2520to%2520enhance%2520the%2520grounding%2520ability%252C%2520we%2520propose%2520DIFFLMM%252C%2520an%2520LMM%250Autilizing%2520a%2520diffusion-based%2520visual%2520encoder%252C%2520as%2520opposed%2520to%2520the%2520standard%2520CLIP%250Avisual%2520encoder%252C%2520and%2520trained%2520with%2520the%2520same%2520weak%2520supervision.%2520Without%2520being%250Aconstrained%2520by%2520the%2520biases%2520and%2520limited%2520scale%2520of%2520grounding-specific%2520supervision%250Adata%252C%2520our%2520approach%2520is%2520more%2520generalizable%2520and%2520scalable.%2520We%2520achieve%2520competitive%250Aperformance%2520on%2520both%2520grounding-specific%2520and%2520general%2520visual%2520question%2520answering%250Abenchmarks%252C%2520compared%2520with%2520grounding%2520LMMs%2520and%2520generalist%2520LMMs%252C%2520respectively.%250ANotably%252C%2520we%2520achieve%2520a%252044.2%2520grounding%2520mask%2520recall%2520on%2520grounded%2520conversation%250Ageneration%2520without%2520any%2520grounding%2520supervision%252C%2520outperforming%2520the%2520extensively%250Asupervised%2520model%2520GLaMM.%2520Project%2520page%253A%2520https%253A//groundLMM.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emerging%20Pixel%20Grounding%20in%20Large%20Multimodal%20Models%20Without%20Grounding%0A%20%20Supervision&entry.906535625=Shengcao%20Cao%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Current%20large%20multimodal%20models%20%28LMMs%29%20face%20challenges%20in%20grounding%2C%20which%0Arequires%20the%20model%20to%20relate%20language%20components%20to%20visual%20entities.%20Contrary%0Ato%20the%20common%20practice%20that%20fine-tunes%20LMMs%20with%20additional%20grounding%0Asupervision%2C%20we%20find%20that%20the%20grounding%20ability%20can%20in%20fact%20emerge%20in%20LMMs%0Atrained%20without%20explicit%20grounding%20supervision.%20To%20reveal%20this%20emerging%0Agrounding%2C%20we%20introduce%20an%20%22attend-and-segment%22%20method%20which%20leverages%0Aattention%20maps%20from%20standard%20LMMs%20to%20perform%20pixel-level%20segmentation.%0AFurthermore%2C%20to%20enhance%20the%20grounding%20ability%2C%20we%20propose%20DIFFLMM%2C%20an%20LMM%0Autilizing%20a%20diffusion-based%20visual%20encoder%2C%20as%20opposed%20to%20the%20standard%20CLIP%0Avisual%20encoder%2C%20and%20trained%20with%20the%20same%20weak%20supervision.%20Without%20being%0Aconstrained%20by%20the%20biases%20and%20limited%20scale%20of%20grounding-specific%20supervision%0Adata%2C%20our%20approach%20is%20more%20generalizable%20and%20scalable.%20We%20achieve%20competitive%0Aperformance%20on%20both%20grounding-specific%20and%20general%20visual%20question%20answering%0Abenchmarks%2C%20compared%20with%20grounding%20LMMs%20and%20generalist%20LMMs%2C%20respectively.%0ANotably%2C%20we%20achieve%20a%2044.2%20grounding%20mask%20recall%20on%20grounded%20conversation%0Ageneration%20without%20any%20grounding%20supervision%2C%20outperforming%20the%20extensively%0Asupervised%20model%20GLaMM.%20Project%20page%3A%20https%3A//groundLMM.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08209v1&entry.124074799=Read"},
{"title": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks", "author": "Mathis Pink and Vy A. Vo and Qinyuan Wu and Jianing Mu and Javier S. Turek and Uri Hasson and Kenneth A. Norman and Sebastian Michelmann and Alexander Huth and Mariya Toneva", "abstract": "  Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models.\n", "link": "http://arxiv.org/abs/2410.08133v1", "date": "2024-10-10", "relevancy": 2.275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Episodic%20Memory%20in%20LLMs%20with%20Sequence%20Order%20Recall%20Tasks&body=Title%3A%20Assessing%20Episodic%20Memory%20in%20LLMs%20with%20Sequence%20Order%20Recall%20Tasks%0AAuthor%3A%20Mathis%20Pink%20and%20Vy%20A.%20Vo%20and%20Qinyuan%20Wu%20and%20Jianing%20Mu%20and%20Javier%20S.%20Turek%20and%20Uri%20Hasson%20and%20Kenneth%20A.%20Norman%20and%20Sebastian%20Michelmann%20and%20Alexander%20Huth%20and%20Mariya%20Toneva%0AAbstract%3A%20%20%20Current%20LLM%20benchmarks%20focus%20on%20evaluating%20models%27%20memory%20of%20facts%20and%0Asemantic%20relations%2C%20primarily%20assessing%20semantic%20aspects%20of%20long-term%20memory.%0AHowever%2C%20in%20humans%2C%20long-term%20memory%20also%20includes%20episodic%20memory%2C%20which%20links%0Amemories%20to%20their%20contexts%2C%20such%20as%20the%20time%20and%20place%20they%20occurred.%20The%0Aability%20to%20contextualize%20memories%20is%20crucial%20for%20many%20cognitive%20tasks%20and%0Aeveryday%20functions.%20This%20form%20of%20memory%20has%20not%20been%20evaluated%20in%20LLMs%20with%0Aexisting%20benchmarks.%20To%20address%20the%20gap%20in%20evaluating%20memory%20in%20LLMs%2C%20we%0Aintroduce%20Sequence%20Order%20Recall%20Tasks%20%28SORT%29%2C%20which%20we%20adapt%20from%20tasks%20used%20to%0Astudy%20episodic%20memory%20in%20cognitive%20psychology.%20SORT%20requires%20LLMs%20to%20recall%20the%0Acorrect%20order%20of%20text%20segments%2C%20and%20provides%20a%20general%20framework%20that%20is%20both%0Aeasily%20extendable%20and%20does%20not%20require%20any%20additional%20annotations.%20We%20present%0Aan%20initial%20evaluation%20dataset%2C%20Book-SORT%2C%20comprising%2036k%20pairs%20of%20segments%0Aextracted%20from%209%20books%20recently%20added%20to%20the%20public%20domain.%20Based%20on%20a%20human%0Aexperiment%20with%20155%20participants%2C%20we%20show%20that%20humans%20can%20recall%20sequence%20order%0Abased%20on%20long-term%20memory%20of%20a%20book.%20We%20find%20that%20models%20can%20perform%20the%20task%0Awith%20high%20accuracy%20when%20relevant%20text%20is%20given%20in-context%20during%20the%20SORT%0Aevaluation.%20However%2C%20when%20presented%20with%20the%20book%20text%20only%20during%20training%2C%0ALLMs%27%20performance%20on%20SORT%20falls%20short.%20By%20allowing%20to%20evaluate%20more%20aspects%20of%0Amemory%2C%20we%20believe%20that%20SORT%20will%20aid%20in%20the%20emerging%20development%20of%0Amemory-augmented%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Episodic%2520Memory%2520in%2520LLMs%2520with%2520Sequence%2520Order%2520Recall%2520Tasks%26entry.906535625%3DMathis%2520Pink%2520and%2520Vy%2520A.%2520Vo%2520and%2520Qinyuan%2520Wu%2520and%2520Jianing%2520Mu%2520and%2520Javier%2520S.%2520Turek%2520and%2520Uri%2520Hasson%2520and%2520Kenneth%2520A.%2520Norman%2520and%2520Sebastian%2520Michelmann%2520and%2520Alexander%2520Huth%2520and%2520Mariya%2520Toneva%26entry.1292438233%3D%2520%2520Current%2520LLM%2520benchmarks%2520focus%2520on%2520evaluating%2520models%2527%2520memory%2520of%2520facts%2520and%250Asemantic%2520relations%252C%2520primarily%2520assessing%2520semantic%2520aspects%2520of%2520long-term%2520memory.%250AHowever%252C%2520in%2520humans%252C%2520long-term%2520memory%2520also%2520includes%2520episodic%2520memory%252C%2520which%2520links%250Amemories%2520to%2520their%2520contexts%252C%2520such%2520as%2520the%2520time%2520and%2520place%2520they%2520occurred.%2520The%250Aability%2520to%2520contextualize%2520memories%2520is%2520crucial%2520for%2520many%2520cognitive%2520tasks%2520and%250Aeveryday%2520functions.%2520This%2520form%2520of%2520memory%2520has%2520not%2520been%2520evaluated%2520in%2520LLMs%2520with%250Aexisting%2520benchmarks.%2520To%2520address%2520the%2520gap%2520in%2520evaluating%2520memory%2520in%2520LLMs%252C%2520we%250Aintroduce%2520Sequence%2520Order%2520Recall%2520Tasks%2520%2528SORT%2529%252C%2520which%2520we%2520adapt%2520from%2520tasks%2520used%2520to%250Astudy%2520episodic%2520memory%2520in%2520cognitive%2520psychology.%2520SORT%2520requires%2520LLMs%2520to%2520recall%2520the%250Acorrect%2520order%2520of%2520text%2520segments%252C%2520and%2520provides%2520a%2520general%2520framework%2520that%2520is%2520both%250Aeasily%2520extendable%2520and%2520does%2520not%2520require%2520any%2520additional%2520annotations.%2520We%2520present%250Aan%2520initial%2520evaluation%2520dataset%252C%2520Book-SORT%252C%2520comprising%252036k%2520pairs%2520of%2520segments%250Aextracted%2520from%25209%2520books%2520recently%2520added%2520to%2520the%2520public%2520domain.%2520Based%2520on%2520a%2520human%250Aexperiment%2520with%2520155%2520participants%252C%2520we%2520show%2520that%2520humans%2520can%2520recall%2520sequence%2520order%250Abased%2520on%2520long-term%2520memory%2520of%2520a%2520book.%2520We%2520find%2520that%2520models%2520can%2520perform%2520the%2520task%250Awith%2520high%2520accuracy%2520when%2520relevant%2520text%2520is%2520given%2520in-context%2520during%2520the%2520SORT%250Aevaluation.%2520However%252C%2520when%2520presented%2520with%2520the%2520book%2520text%2520only%2520during%2520training%252C%250ALLMs%2527%2520performance%2520on%2520SORT%2520falls%2520short.%2520By%2520allowing%2520to%2520evaluate%2520more%2520aspects%2520of%250Amemory%252C%2520we%2520believe%2520that%2520SORT%2520will%2520aid%2520in%2520the%2520emerging%2520development%2520of%250Amemory-augmented%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Episodic%20Memory%20in%20LLMs%20with%20Sequence%20Order%20Recall%20Tasks&entry.906535625=Mathis%20Pink%20and%20Vy%20A.%20Vo%20and%20Qinyuan%20Wu%20and%20Jianing%20Mu%20and%20Javier%20S.%20Turek%20and%20Uri%20Hasson%20and%20Kenneth%20A.%20Norman%20and%20Sebastian%20Michelmann%20and%20Alexander%20Huth%20and%20Mariya%20Toneva&entry.1292438233=%20%20Current%20LLM%20benchmarks%20focus%20on%20evaluating%20models%27%20memory%20of%20facts%20and%0Asemantic%20relations%2C%20primarily%20assessing%20semantic%20aspects%20of%20long-term%20memory.%0AHowever%2C%20in%20humans%2C%20long-term%20memory%20also%20includes%20episodic%20memory%2C%20which%20links%0Amemories%20to%20their%20contexts%2C%20such%20as%20the%20time%20and%20place%20they%20occurred.%20The%0Aability%20to%20contextualize%20memories%20is%20crucial%20for%20many%20cognitive%20tasks%20and%0Aeveryday%20functions.%20This%20form%20of%20memory%20has%20not%20been%20evaluated%20in%20LLMs%20with%0Aexisting%20benchmarks.%20To%20address%20the%20gap%20in%20evaluating%20memory%20in%20LLMs%2C%20we%0Aintroduce%20Sequence%20Order%20Recall%20Tasks%20%28SORT%29%2C%20which%20we%20adapt%20from%20tasks%20used%20to%0Astudy%20episodic%20memory%20in%20cognitive%20psychology.%20SORT%20requires%20LLMs%20to%20recall%20the%0Acorrect%20order%20of%20text%20segments%2C%20and%20provides%20a%20general%20framework%20that%20is%20both%0Aeasily%20extendable%20and%20does%20not%20require%20any%20additional%20annotations.%20We%20present%0Aan%20initial%20evaluation%20dataset%2C%20Book-SORT%2C%20comprising%2036k%20pairs%20of%20segments%0Aextracted%20from%209%20books%20recently%20added%20to%20the%20public%20domain.%20Based%20on%20a%20human%0Aexperiment%20with%20155%20participants%2C%20we%20show%20that%20humans%20can%20recall%20sequence%20order%0Abased%20on%20long-term%20memory%20of%20a%20book.%20We%20find%20that%20models%20can%20perform%20the%20task%0Awith%20high%20accuracy%20when%20relevant%20text%20is%20given%20in-context%20during%20the%20SORT%0Aevaluation.%20However%2C%20when%20presented%20with%20the%20book%20text%20only%20during%20training%2C%0ALLMs%27%20performance%20on%20SORT%20falls%20short.%20By%20allowing%20to%20evaluate%20more%20aspects%20of%0Amemory%2C%20we%20believe%20that%20SORT%20will%20aid%20in%20the%20emerging%20development%20of%0Amemory-augmented%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08133v1&entry.124074799=Read"},
{"title": "RISE-SDF: a Relightable Information-Shared Signed Distance Field for\n  Glossy Object Inverse Rendering", "author": "Deheng Zhang and Jingyu Wang and Shaofei Wang and Marko Mihajlovic and Sergey Prokudin and Hendrik P. A. Lensch and Siyu Tang", "abstract": "  In this paper, we propose a novel end-to-end relightable neural inverse\nrendering system that achieves high-quality reconstruction of geometry and\nmaterial properties, thus enabling high-quality relighting. The cornerstone of\nour method is a two-stage approach for learning a better factorization of scene\nparameters. In the first stage, we develop a reflection-aware radiance field\nusing a neural signed distance field (SDF) as the geometry representation and\ndeploy an MLP (multilayer perceptron) to estimate indirect illumination. In the\nsecond stage, we introduce a novel information-sharing network structure to\njointly learn the radiance field and the physically based factorization of the\nscene. For the physically based factorization, to reduce the noise caused by\nMonte Carlo sampling, we apply a split-sum approximation with a simplified\nDisney BRDF and cube mipmap as the environment light representation. In the\nrelighting phase, to enhance the quality of indirect illumination, we propose a\nsecond split-sum algorithm to trace secondary rays under the split-sum\nrendering framework. Furthermore, there is no dataset or protocol available to\nquantitatively evaluate the inverse rendering performance for glossy objects.\nTo assess the quality of material reconstruction and relighting, we have\ncreated a new dataset with ground truth BRDF parameters and relighting results.\nOur experiments demonstrate that our algorithm achieves state-of-the-art\nperformance in inverse rendering and relighting, with particularly strong\nresults in the reconstruction of highly reflective objects.\n", "link": "http://arxiv.org/abs/2409.20140v3", "date": "2024-10-10", "relevancy": 2.2696, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5807}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5604}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RISE-SDF%3A%20a%20Relightable%20Information-Shared%20Signed%20Distance%20Field%20for%0A%20%20Glossy%20Object%20Inverse%20Rendering&body=Title%3A%20RISE-SDF%3A%20a%20Relightable%20Information-Shared%20Signed%20Distance%20Field%20for%0A%20%20Glossy%20Object%20Inverse%20Rendering%0AAuthor%3A%20Deheng%20Zhang%20and%20Jingyu%20Wang%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Sergey%20Prokudin%20and%20Hendrik%20P.%20A.%20Lensch%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20end-to-end%20relightable%20neural%20inverse%0Arendering%20system%20that%20achieves%20high-quality%20reconstruction%20of%20geometry%20and%0Amaterial%20properties%2C%20thus%20enabling%20high-quality%20relighting.%20The%20cornerstone%20of%0Aour%20method%20is%20a%20two-stage%20approach%20for%20learning%20a%20better%20factorization%20of%20scene%0Aparameters.%20In%20the%20first%20stage%2C%20we%20develop%20a%20reflection-aware%20radiance%20field%0Ausing%20a%20neural%20signed%20distance%20field%20%28SDF%29%20as%20the%20geometry%20representation%20and%0Adeploy%20an%20MLP%20%28multilayer%20perceptron%29%20to%20estimate%20indirect%20illumination.%20In%20the%0Asecond%20stage%2C%20we%20introduce%20a%20novel%20information-sharing%20network%20structure%20to%0Ajointly%20learn%20the%20radiance%20field%20and%20the%20physically%20based%20factorization%20of%20the%0Ascene.%20For%20the%20physically%20based%20factorization%2C%20to%20reduce%20the%20noise%20caused%20by%0AMonte%20Carlo%20sampling%2C%20we%20apply%20a%20split-sum%20approximation%20with%20a%20simplified%0ADisney%20BRDF%20and%20cube%20mipmap%20as%20the%20environment%20light%20representation.%20In%20the%0Arelighting%20phase%2C%20to%20enhance%20the%20quality%20of%20indirect%20illumination%2C%20we%20propose%20a%0Asecond%20split-sum%20algorithm%20to%20trace%20secondary%20rays%20under%20the%20split-sum%0Arendering%20framework.%20Furthermore%2C%20there%20is%20no%20dataset%20or%20protocol%20available%20to%0Aquantitatively%20evaluate%20the%20inverse%20rendering%20performance%20for%20glossy%20objects.%0ATo%20assess%20the%20quality%20of%20material%20reconstruction%20and%20relighting%2C%20we%20have%0Acreated%20a%20new%20dataset%20with%20ground%20truth%20BRDF%20parameters%20and%20relighting%20results.%0AOur%20experiments%20demonstrate%20that%20our%20algorithm%20achieves%20state-of-the-art%0Aperformance%20in%20inverse%20rendering%20and%20relighting%2C%20with%20particularly%20strong%0Aresults%20in%20the%20reconstruction%20of%20highly%20reflective%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20140v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRISE-SDF%253A%2520a%2520Relightable%2520Information-Shared%2520Signed%2520Distance%2520Field%2520for%250A%2520%2520Glossy%2520Object%2520Inverse%2520Rendering%26entry.906535625%3DDeheng%2520Zhang%2520and%2520Jingyu%2520Wang%2520and%2520Shaofei%2520Wang%2520and%2520Marko%2520Mihajlovic%2520and%2520Sergey%2520Prokudin%2520and%2520Hendrik%2520P.%2520A.%2520Lensch%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520relightable%2520neural%2520inverse%250Arendering%2520system%2520that%2520achieves%2520high-quality%2520reconstruction%2520of%2520geometry%2520and%250Amaterial%2520properties%252C%2520thus%2520enabling%2520high-quality%2520relighting.%2520The%2520cornerstone%2520of%250Aour%2520method%2520is%2520a%2520two-stage%2520approach%2520for%2520learning%2520a%2520better%2520factorization%2520of%2520scene%250Aparameters.%2520In%2520the%2520first%2520stage%252C%2520we%2520develop%2520a%2520reflection-aware%2520radiance%2520field%250Ausing%2520a%2520neural%2520signed%2520distance%2520field%2520%2528SDF%2529%2520as%2520the%2520geometry%2520representation%2520and%250Adeploy%2520an%2520MLP%2520%2528multilayer%2520perceptron%2529%2520to%2520estimate%2520indirect%2520illumination.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520introduce%2520a%2520novel%2520information-sharing%2520network%2520structure%2520to%250Ajointly%2520learn%2520the%2520radiance%2520field%2520and%2520the%2520physically%2520based%2520factorization%2520of%2520the%250Ascene.%2520For%2520the%2520physically%2520based%2520factorization%252C%2520to%2520reduce%2520the%2520noise%2520caused%2520by%250AMonte%2520Carlo%2520sampling%252C%2520we%2520apply%2520a%2520split-sum%2520approximation%2520with%2520a%2520simplified%250ADisney%2520BRDF%2520and%2520cube%2520mipmap%2520as%2520the%2520environment%2520light%2520representation.%2520In%2520the%250Arelighting%2520phase%252C%2520to%2520enhance%2520the%2520quality%2520of%2520indirect%2520illumination%252C%2520we%2520propose%2520a%250Asecond%2520split-sum%2520algorithm%2520to%2520trace%2520secondary%2520rays%2520under%2520the%2520split-sum%250Arendering%2520framework.%2520Furthermore%252C%2520there%2520is%2520no%2520dataset%2520or%2520protocol%2520available%2520to%250Aquantitatively%2520evaluate%2520the%2520inverse%2520rendering%2520performance%2520for%2520glossy%2520objects.%250ATo%2520assess%2520the%2520quality%2520of%2520material%2520reconstruction%2520and%2520relighting%252C%2520we%2520have%250Acreated%2520a%2520new%2520dataset%2520with%2520ground%2520truth%2520BRDF%2520parameters%2520and%2520relighting%2520results.%250AOur%2520experiments%2520demonstrate%2520that%2520our%2520algorithm%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520inverse%2520rendering%2520and%2520relighting%252C%2520with%2520particularly%2520strong%250Aresults%2520in%2520the%2520reconstruction%2520of%2520highly%2520reflective%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20140v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISE-SDF%3A%20a%20Relightable%20Information-Shared%20Signed%20Distance%20Field%20for%0A%20%20Glossy%20Object%20Inverse%20Rendering&entry.906535625=Deheng%20Zhang%20and%20Jingyu%20Wang%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Sergey%20Prokudin%20and%20Hendrik%20P.%20A.%20Lensch%20and%20Siyu%20Tang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20end-to-end%20relightable%20neural%20inverse%0Arendering%20system%20that%20achieves%20high-quality%20reconstruction%20of%20geometry%20and%0Amaterial%20properties%2C%20thus%20enabling%20high-quality%20relighting.%20The%20cornerstone%20of%0Aour%20method%20is%20a%20two-stage%20approach%20for%20learning%20a%20better%20factorization%20of%20scene%0Aparameters.%20In%20the%20first%20stage%2C%20we%20develop%20a%20reflection-aware%20radiance%20field%0Ausing%20a%20neural%20signed%20distance%20field%20%28SDF%29%20as%20the%20geometry%20representation%20and%0Adeploy%20an%20MLP%20%28multilayer%20perceptron%29%20to%20estimate%20indirect%20illumination.%20In%20the%0Asecond%20stage%2C%20we%20introduce%20a%20novel%20information-sharing%20network%20structure%20to%0Ajointly%20learn%20the%20radiance%20field%20and%20the%20physically%20based%20factorization%20of%20the%0Ascene.%20For%20the%20physically%20based%20factorization%2C%20to%20reduce%20the%20noise%20caused%20by%0AMonte%20Carlo%20sampling%2C%20we%20apply%20a%20split-sum%20approximation%20with%20a%20simplified%0ADisney%20BRDF%20and%20cube%20mipmap%20as%20the%20environment%20light%20representation.%20In%20the%0Arelighting%20phase%2C%20to%20enhance%20the%20quality%20of%20indirect%20illumination%2C%20we%20propose%20a%0Asecond%20split-sum%20algorithm%20to%20trace%20secondary%20rays%20under%20the%20split-sum%0Arendering%20framework.%20Furthermore%2C%20there%20is%20no%20dataset%20or%20protocol%20available%20to%0Aquantitatively%20evaluate%20the%20inverse%20rendering%20performance%20for%20glossy%20objects.%0ATo%20assess%20the%20quality%20of%20material%20reconstruction%20and%20relighting%2C%20we%20have%0Acreated%20a%20new%20dataset%20with%20ground%20truth%20BRDF%20parameters%20and%20relighting%20results.%0AOur%20experiments%20demonstrate%20that%20our%20algorithm%20achieves%20state-of-the-art%0Aperformance%20in%20inverse%20rendering%20and%20relighting%2C%20with%20particularly%20strong%0Aresults%20in%20the%20reconstruction%20of%20highly%20reflective%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20140v3&entry.124074799=Read"},
{"title": "On the Evaluation of Generative Robotic Simulations", "author": "Feng Chen and Botian Xu and Pu Hua and Peiqi Duan and Yanchao Yang and Yi Ma and Huazhe Xu", "abstract": "  Due to the difficulty of acquiring extensive real-world data, robot\nsimulation has become crucial for parallel training and sim-to-real transfer,\nhighlighting the importance of scalable simulated robotic tasks. Foundation\nmodels have demonstrated impressive capacities in autonomously generating\nfeasible robotic tasks. However, this new paradigm underscores the challenge of\nadequately evaluating these autonomously generated tasks. To address this, we\npropose a comprehensive evaluation framework tailored to generative\nsimulations. Our framework segments evaluation into three core aspects:\nquality, diversity, and generalization. For single-task quality, we evaluate\nthe realism of the generated task and the completeness of the generated\ntrajectories using large language models and vision-language models. In terms\nof diversity, we measure both task and data diversity through text similarity\nof task descriptions and world model loss trained on collected task\ntrajectories. For task-level generalization, we assess the zero-shot\ngeneralization ability on unseen tasks of a policy trained with multiple\ngenerated tasks. Experiments conducted on three representative task generation\npipelines demonstrate that the results from our framework are highly consistent\nwith human evaluations, confirming the feasibility and validity of our\napproach. The findings reveal that while metrics of quality and diversity can\nbe achieved through certain methods, no single approach excels across all\nmetrics, suggesting a need for greater focus on balancing these different\nmetrics. Additionally, our analysis further highlights the common challenge of\nlow generalization capability faced by current works. Our anonymous website:\nhttps://sites.google.com/view/evaltasks.\n", "link": "http://arxiv.org/abs/2410.08172v1", "date": "2024-10-10", "relevancy": 2.2688, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.586}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5662}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Evaluation%20of%20Generative%20Robotic%20Simulations&body=Title%3A%20On%20the%20Evaluation%20of%20Generative%20Robotic%20Simulations%0AAuthor%3A%20Feng%20Chen%20and%20Botian%20Xu%20and%20Pu%20Hua%20and%20Peiqi%20Duan%20and%20Yanchao%20Yang%20and%20Yi%20Ma%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Due%20to%20the%20difficulty%20of%20acquiring%20extensive%20real-world%20data%2C%20robot%0Asimulation%20has%20become%20crucial%20for%20parallel%20training%20and%20sim-to-real%20transfer%2C%0Ahighlighting%20the%20importance%20of%20scalable%20simulated%20robotic%20tasks.%20Foundation%0Amodels%20have%20demonstrated%20impressive%20capacities%20in%20autonomously%20generating%0Afeasible%20robotic%20tasks.%20However%2C%20this%20new%20paradigm%20underscores%20the%20challenge%20of%0Aadequately%20evaluating%20these%20autonomously%20generated%20tasks.%20To%20address%20this%2C%20we%0Apropose%20a%20comprehensive%20evaluation%20framework%20tailored%20to%20generative%0Asimulations.%20Our%20framework%20segments%20evaluation%20into%20three%20core%20aspects%3A%0Aquality%2C%20diversity%2C%20and%20generalization.%20For%20single-task%20quality%2C%20we%20evaluate%0Athe%20realism%20of%20the%20generated%20task%20and%20the%20completeness%20of%20the%20generated%0Atrajectories%20using%20large%20language%20models%20and%20vision-language%20models.%20In%20terms%0Aof%20diversity%2C%20we%20measure%20both%20task%20and%20data%20diversity%20through%20text%20similarity%0Aof%20task%20descriptions%20and%20world%20model%20loss%20trained%20on%20collected%20task%0Atrajectories.%20For%20task-level%20generalization%2C%20we%20assess%20the%20zero-shot%0Ageneralization%20ability%20on%20unseen%20tasks%20of%20a%20policy%20trained%20with%20multiple%0Agenerated%20tasks.%20Experiments%20conducted%20on%20three%20representative%20task%20generation%0Apipelines%20demonstrate%20that%20the%20results%20from%20our%20framework%20are%20highly%20consistent%0Awith%20human%20evaluations%2C%20confirming%20the%20feasibility%20and%20validity%20of%20our%0Aapproach.%20The%20findings%20reveal%20that%20while%20metrics%20of%20quality%20and%20diversity%20can%0Abe%20achieved%20through%20certain%20methods%2C%20no%20single%20approach%20excels%20across%20all%0Ametrics%2C%20suggesting%20a%20need%20for%20greater%20focus%20on%20balancing%20these%20different%0Ametrics.%20Additionally%2C%20our%20analysis%20further%20highlights%20the%20common%20challenge%20of%0Alow%20generalization%20capability%20faced%20by%20current%20works.%20Our%20anonymous%20website%3A%0Ahttps%3A//sites.google.com/view/evaltasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Evaluation%2520of%2520Generative%2520Robotic%2520Simulations%26entry.906535625%3DFeng%2520Chen%2520and%2520Botian%2520Xu%2520and%2520Pu%2520Hua%2520and%2520Peiqi%2520Duan%2520and%2520Yanchao%2520Yang%2520and%2520Yi%2520Ma%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520difficulty%2520of%2520acquiring%2520extensive%2520real-world%2520data%252C%2520robot%250Asimulation%2520has%2520become%2520crucial%2520for%2520parallel%2520training%2520and%2520sim-to-real%2520transfer%252C%250Ahighlighting%2520the%2520importance%2520of%2520scalable%2520simulated%2520robotic%2520tasks.%2520Foundation%250Amodels%2520have%2520demonstrated%2520impressive%2520capacities%2520in%2520autonomously%2520generating%250Afeasible%2520robotic%2520tasks.%2520However%252C%2520this%2520new%2520paradigm%2520underscores%2520the%2520challenge%2520of%250Aadequately%2520evaluating%2520these%2520autonomously%2520generated%2520tasks.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520comprehensive%2520evaluation%2520framework%2520tailored%2520to%2520generative%250Asimulations.%2520Our%2520framework%2520segments%2520evaluation%2520into%2520three%2520core%2520aspects%253A%250Aquality%252C%2520diversity%252C%2520and%2520generalization.%2520For%2520single-task%2520quality%252C%2520we%2520evaluate%250Athe%2520realism%2520of%2520the%2520generated%2520task%2520and%2520the%2520completeness%2520of%2520the%2520generated%250Atrajectories%2520using%2520large%2520language%2520models%2520and%2520vision-language%2520models.%2520In%2520terms%250Aof%2520diversity%252C%2520we%2520measure%2520both%2520task%2520and%2520data%2520diversity%2520through%2520text%2520similarity%250Aof%2520task%2520descriptions%2520and%2520world%2520model%2520loss%2520trained%2520on%2520collected%2520task%250Atrajectories.%2520For%2520task-level%2520generalization%252C%2520we%2520assess%2520the%2520zero-shot%250Ageneralization%2520ability%2520on%2520unseen%2520tasks%2520of%2520a%2520policy%2520trained%2520with%2520multiple%250Agenerated%2520tasks.%2520Experiments%2520conducted%2520on%2520three%2520representative%2520task%2520generation%250Apipelines%2520demonstrate%2520that%2520the%2520results%2520from%2520our%2520framework%2520are%2520highly%2520consistent%250Awith%2520human%2520evaluations%252C%2520confirming%2520the%2520feasibility%2520and%2520validity%2520of%2520our%250Aapproach.%2520The%2520findings%2520reveal%2520that%2520while%2520metrics%2520of%2520quality%2520and%2520diversity%2520can%250Abe%2520achieved%2520through%2520certain%2520methods%252C%2520no%2520single%2520approach%2520excels%2520across%2520all%250Ametrics%252C%2520suggesting%2520a%2520need%2520for%2520greater%2520focus%2520on%2520balancing%2520these%2520different%250Ametrics.%2520Additionally%252C%2520our%2520analysis%2520further%2520highlights%2520the%2520common%2520challenge%2520of%250Alow%2520generalization%2520capability%2520faced%2520by%2520current%2520works.%2520Our%2520anonymous%2520website%253A%250Ahttps%253A//sites.google.com/view/evaltasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Evaluation%20of%20Generative%20Robotic%20Simulations&entry.906535625=Feng%20Chen%20and%20Botian%20Xu%20and%20Pu%20Hua%20and%20Peiqi%20Duan%20and%20Yanchao%20Yang%20and%20Yi%20Ma%20and%20Huazhe%20Xu&entry.1292438233=%20%20Due%20to%20the%20difficulty%20of%20acquiring%20extensive%20real-world%20data%2C%20robot%0Asimulation%20has%20become%20crucial%20for%20parallel%20training%20and%20sim-to-real%20transfer%2C%0Ahighlighting%20the%20importance%20of%20scalable%20simulated%20robotic%20tasks.%20Foundation%0Amodels%20have%20demonstrated%20impressive%20capacities%20in%20autonomously%20generating%0Afeasible%20robotic%20tasks.%20However%2C%20this%20new%20paradigm%20underscores%20the%20challenge%20of%0Aadequately%20evaluating%20these%20autonomously%20generated%20tasks.%20To%20address%20this%2C%20we%0Apropose%20a%20comprehensive%20evaluation%20framework%20tailored%20to%20generative%0Asimulations.%20Our%20framework%20segments%20evaluation%20into%20three%20core%20aspects%3A%0Aquality%2C%20diversity%2C%20and%20generalization.%20For%20single-task%20quality%2C%20we%20evaluate%0Athe%20realism%20of%20the%20generated%20task%20and%20the%20completeness%20of%20the%20generated%0Atrajectories%20using%20large%20language%20models%20and%20vision-language%20models.%20In%20terms%0Aof%20diversity%2C%20we%20measure%20both%20task%20and%20data%20diversity%20through%20text%20similarity%0Aof%20task%20descriptions%20and%20world%20model%20loss%20trained%20on%20collected%20task%0Atrajectories.%20For%20task-level%20generalization%2C%20we%20assess%20the%20zero-shot%0Ageneralization%20ability%20on%20unseen%20tasks%20of%20a%20policy%20trained%20with%20multiple%0Agenerated%20tasks.%20Experiments%20conducted%20on%20three%20representative%20task%20generation%0Apipelines%20demonstrate%20that%20the%20results%20from%20our%20framework%20are%20highly%20consistent%0Awith%20human%20evaluations%2C%20confirming%20the%20feasibility%20and%20validity%20of%20our%0Aapproach.%20The%20findings%20reveal%20that%20while%20metrics%20of%20quality%20and%20diversity%20can%0Abe%20achieved%20through%20certain%20methods%2C%20no%20single%20approach%20excels%20across%20all%0Ametrics%2C%20suggesting%20a%20need%20for%20greater%20focus%20on%20balancing%20these%20different%0Ametrics.%20Additionally%2C%20our%20analysis%20further%20highlights%20the%20common%20challenge%20of%0Alow%20generalization%20capability%20faced%20by%20current%20works.%20Our%20anonymous%20website%3A%0Ahttps%3A//sites.google.com/view/evaltasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08172v1&entry.124074799=Read"},
{"title": "GrabDAE: An Innovative Framework for Unsupervised Domain Adaptation\n  Utilizing Grab-Mask and Denoise Auto-Encoder", "author": "Junzhou Chen and Xuan Wen and Ronghui Zhang and Bingtao Ren and Di Wu and Zhigang Xu and Danwei Wang", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to adapt a model trained on a\nlabeled source domain to an unlabeled target domain by addressing the domain\nshift. Existing Unsupervised Domain Adaptation (UDA) methods often fall short\nin fully leveraging contextual information from the target domain, leading to\nsuboptimal decision boundary separation during source and target domain\nalignment. To address this, we introduce GrabDAE, an innovative UDA framework\ndesigned to tackle domain shift in visual classification tasks. GrabDAE\nincorporates two key innovations: the Grab-Mask module, which blurs background\ninformation in target domain images, enabling the model to focus on essential,\ndomain-relevant features through contrastive learning; and the Denoising\nAuto-Encoder (DAE), which enhances feature alignment by reconstructing features\nand filtering noise, ensuring a more robust adaptation to the target domain.\nThese components empower GrabDAE to effectively handle unlabeled target domain\ndata, significantly improving both classification accuracy and robustness.\nExtensive experiments on benchmark datasets, including VisDA-2017, Office-Home,\nand Office31, demonstrate that GrabDAE consistently surpasses state-of-the-art\nUDA methods, setting new performance benchmarks. By tackling UDA's critical\nchallenges with its novel feature masking and denoising approach, GrabDAE\noffers both significant theoretical and practical advancements in domain\nadaptation.\n", "link": "http://arxiv.org/abs/2410.08023v1", "date": "2024-10-10", "relevancy": 2.2608, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5858}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrabDAE%3A%20An%20Innovative%20Framework%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Utilizing%20Grab-Mask%20and%20Denoise%20Auto-Encoder&body=Title%3A%20GrabDAE%3A%20An%20Innovative%20Framework%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Utilizing%20Grab-Mask%20and%20Denoise%20Auto-Encoder%0AAuthor%3A%20Junzhou%20Chen%20and%20Xuan%20Wen%20and%20Ronghui%20Zhang%20and%20Bingtao%20Ren%20and%20Di%20Wu%20and%20Zhigang%20Xu%20and%20Danwei%20Wang%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20aims%20to%20adapt%20a%20model%20trained%20on%20a%0Alabeled%20source%20domain%20to%20an%20unlabeled%20target%20domain%20by%20addressing%20the%20domain%0Ashift.%20Existing%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20methods%20often%20fall%20short%0Ain%20fully%20leveraging%20contextual%20information%20from%20the%20target%20domain%2C%20leading%20to%0Asuboptimal%20decision%20boundary%20separation%20during%20source%20and%20target%20domain%0Aalignment.%20To%20address%20this%2C%20we%20introduce%20GrabDAE%2C%20an%20innovative%20UDA%20framework%0Adesigned%20to%20tackle%20domain%20shift%20in%20visual%20classification%20tasks.%20GrabDAE%0Aincorporates%20two%20key%20innovations%3A%20the%20Grab-Mask%20module%2C%20which%20blurs%20background%0Ainformation%20in%20target%20domain%20images%2C%20enabling%20the%20model%20to%20focus%20on%20essential%2C%0Adomain-relevant%20features%20through%20contrastive%20learning%3B%20and%20the%20Denoising%0AAuto-Encoder%20%28DAE%29%2C%20which%20enhances%20feature%20alignment%20by%20reconstructing%20features%0Aand%20filtering%20noise%2C%20ensuring%20a%20more%20robust%20adaptation%20to%20the%20target%20domain.%0AThese%20components%20empower%20GrabDAE%20to%20effectively%20handle%20unlabeled%20target%20domain%0Adata%2C%20significantly%20improving%20both%20classification%20accuracy%20and%20robustness.%0AExtensive%20experiments%20on%20benchmark%20datasets%2C%20including%20VisDA-2017%2C%20Office-Home%2C%0Aand%20Office31%2C%20demonstrate%20that%20GrabDAE%20consistently%20surpasses%20state-of-the-art%0AUDA%20methods%2C%20setting%20new%20performance%20benchmarks.%20By%20tackling%20UDA%27s%20critical%0Achallenges%20with%20its%20novel%20feature%20masking%20and%20denoising%20approach%2C%20GrabDAE%0Aoffers%20both%20significant%20theoretical%20and%20practical%20advancements%20in%20domain%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrabDAE%253A%2520An%2520Innovative%2520Framework%2520for%2520Unsupervised%2520Domain%2520Adaptation%250A%2520%2520Utilizing%2520Grab-Mask%2520and%2520Denoise%2520Auto-Encoder%26entry.906535625%3DJunzhou%2520Chen%2520and%2520Xuan%2520Wen%2520and%2520Ronghui%2520Zhang%2520and%2520Bingtao%2520Ren%2520and%2520Di%2520Wu%2520and%2520Zhigang%2520Xu%2520and%2520Danwei%2520Wang%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520aims%2520to%2520adapt%2520a%2520model%2520trained%2520on%2520a%250Alabeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain%2520by%2520addressing%2520the%2520domain%250Ashift.%2520Existing%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520methods%2520often%2520fall%2520short%250Ain%2520fully%2520leveraging%2520contextual%2520information%2520from%2520the%2520target%2520domain%252C%2520leading%2520to%250Asuboptimal%2520decision%2520boundary%2520separation%2520during%2520source%2520and%2520target%2520domain%250Aalignment.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GrabDAE%252C%2520an%2520innovative%2520UDA%2520framework%250Adesigned%2520to%2520tackle%2520domain%2520shift%2520in%2520visual%2520classification%2520tasks.%2520GrabDAE%250Aincorporates%2520two%2520key%2520innovations%253A%2520the%2520Grab-Mask%2520module%252C%2520which%2520blurs%2520background%250Ainformation%2520in%2520target%2520domain%2520images%252C%2520enabling%2520the%2520model%2520to%2520focus%2520on%2520essential%252C%250Adomain-relevant%2520features%2520through%2520contrastive%2520learning%253B%2520and%2520the%2520Denoising%250AAuto-Encoder%2520%2528DAE%2529%252C%2520which%2520enhances%2520feature%2520alignment%2520by%2520reconstructing%2520features%250Aand%2520filtering%2520noise%252C%2520ensuring%2520a%2520more%2520robust%2520adaptation%2520to%2520the%2520target%2520domain.%250AThese%2520components%2520empower%2520GrabDAE%2520to%2520effectively%2520handle%2520unlabeled%2520target%2520domain%250Adata%252C%2520significantly%2520improving%2520both%2520classification%2520accuracy%2520and%2520robustness.%250AExtensive%2520experiments%2520on%2520benchmark%2520datasets%252C%2520including%2520VisDA-2017%252C%2520Office-Home%252C%250Aand%2520Office31%252C%2520demonstrate%2520that%2520GrabDAE%2520consistently%2520surpasses%2520state-of-the-art%250AUDA%2520methods%252C%2520setting%2520new%2520performance%2520benchmarks.%2520By%2520tackling%2520UDA%2527s%2520critical%250Achallenges%2520with%2520its%2520novel%2520feature%2520masking%2520and%2520denoising%2520approach%252C%2520GrabDAE%250Aoffers%2520both%2520significant%2520theoretical%2520and%2520practical%2520advancements%2520in%2520domain%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrabDAE%3A%20An%20Innovative%20Framework%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Utilizing%20Grab-Mask%20and%20Denoise%20Auto-Encoder&entry.906535625=Junzhou%20Chen%20and%20Xuan%20Wen%20and%20Ronghui%20Zhang%20and%20Bingtao%20Ren%20and%20Di%20Wu%20and%20Zhigang%20Xu%20and%20Danwei%20Wang&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20aims%20to%20adapt%20a%20model%20trained%20on%20a%0Alabeled%20source%20domain%20to%20an%20unlabeled%20target%20domain%20by%20addressing%20the%20domain%0Ashift.%20Existing%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20methods%20often%20fall%20short%0Ain%20fully%20leveraging%20contextual%20information%20from%20the%20target%20domain%2C%20leading%20to%0Asuboptimal%20decision%20boundary%20separation%20during%20source%20and%20target%20domain%0Aalignment.%20To%20address%20this%2C%20we%20introduce%20GrabDAE%2C%20an%20innovative%20UDA%20framework%0Adesigned%20to%20tackle%20domain%20shift%20in%20visual%20classification%20tasks.%20GrabDAE%0Aincorporates%20two%20key%20innovations%3A%20the%20Grab-Mask%20module%2C%20which%20blurs%20background%0Ainformation%20in%20target%20domain%20images%2C%20enabling%20the%20model%20to%20focus%20on%20essential%2C%0Adomain-relevant%20features%20through%20contrastive%20learning%3B%20and%20the%20Denoising%0AAuto-Encoder%20%28DAE%29%2C%20which%20enhances%20feature%20alignment%20by%20reconstructing%20features%0Aand%20filtering%20noise%2C%20ensuring%20a%20more%20robust%20adaptation%20to%20the%20target%20domain.%0AThese%20components%20empower%20GrabDAE%20to%20effectively%20handle%20unlabeled%20target%20domain%0Adata%2C%20significantly%20improving%20both%20classification%20accuracy%20and%20robustness.%0AExtensive%20experiments%20on%20benchmark%20datasets%2C%20including%20VisDA-2017%2C%20Office-Home%2C%0Aand%20Office31%2C%20demonstrate%20that%20GrabDAE%20consistently%20surpasses%20state-of-the-art%0AUDA%20methods%2C%20setting%20new%20performance%20benchmarks.%20By%20tackling%20UDA%27s%20critical%0Achallenges%20with%20its%20novel%20feature%20masking%20and%20denoising%20approach%2C%20GrabDAE%0Aoffers%20both%20significant%20theoretical%20and%20practical%20advancements%20in%20domain%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08023v1&entry.124074799=Read"},
{"title": "Understanding Spatio-Temporal Relations in Human-Object Interaction\n  using Pyramid Graph Convolutional Network", "author": "Hao Xing and Darius Burschka", "abstract": "  Human activities recognition is an important task for an intelligent robot,\nespecially in the field of human-robot collaboration, it requires not only the\nlabel of sub-activities but also the temporal structure of the activity. In\norder to automatically recognize both the label and the temporal structure in\nsequence of human-object interaction, we propose a novel Pyramid Graph\nConvolutional Network (PGCN), which employs a pyramidal encoder-decoder\narchitecture consisting of an attention based graph convolution network and a\ntemporal pyramid pooling module for downsampling and upsampling interaction\nsequence on the temporal axis, respectively. The system represents the 2D or 3D\nspatial relation of human and objects from the detection results in video data\nas a graph. To learn the human-object relations, a new attention graph\nconvolutional network is trained to extract condensed information from the\ngraph representation. To segment action into sub-actions, a novel temporal\npyramid pooling module is proposed, which upsamples compressed features back to\nthe original time scale and classifies actions per frame.\n  We explore various attention layers, namely spatial attention, temporal\nattention and channel attention, and combine different upsampling decoders to\ntest the performance on action recognition and segmentation. We evaluate our\nmodel on two challenging datasets in the field of human-object interaction\nrecognition, i.e. Bimanual Actions and IKEA Assembly datasets. We demonstrate\nthat our classifier significantly improves both framewise action recognition\nand segmentation, e.g., F1 micro and F1@50 scores on Bimanual Actions dataset\nare improved by $4.3\\%$ and $8.5\\%$ respectively.\n", "link": "http://arxiv.org/abs/2410.07912v1", "date": "2024-10-10", "relevancy": 2.2565, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.575}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.572}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Spatio-Temporal%20Relations%20in%20Human-Object%20Interaction%0A%20%20using%20Pyramid%20Graph%20Convolutional%20Network&body=Title%3A%20Understanding%20Spatio-Temporal%20Relations%20in%20Human-Object%20Interaction%0A%20%20using%20Pyramid%20Graph%20Convolutional%20Network%0AAuthor%3A%20Hao%20Xing%20and%20Darius%20Burschka%0AAbstract%3A%20%20%20Human%20activities%20recognition%20is%20an%20important%20task%20for%20an%20intelligent%20robot%2C%0Aespecially%20in%20the%20field%20of%20human-robot%20collaboration%2C%20it%20requires%20not%20only%20the%0Alabel%20of%20sub-activities%20but%20also%20the%20temporal%20structure%20of%20the%20activity.%20In%0Aorder%20to%20automatically%20recognize%20both%20the%20label%20and%20the%20temporal%20structure%20in%0Asequence%20of%20human-object%20interaction%2C%20we%20propose%20a%20novel%20Pyramid%20Graph%0AConvolutional%20Network%20%28PGCN%29%2C%20which%20employs%20a%20pyramidal%20encoder-decoder%0Aarchitecture%20consisting%20of%20an%20attention%20based%20graph%20convolution%20network%20and%20a%0Atemporal%20pyramid%20pooling%20module%20for%20downsampling%20and%20upsampling%20interaction%0Asequence%20on%20the%20temporal%20axis%2C%20respectively.%20The%20system%20represents%20the%202D%20or%203D%0Aspatial%20relation%20of%20human%20and%20objects%20from%20the%20detection%20results%20in%20video%20data%0Aas%20a%20graph.%20To%20learn%20the%20human-object%20relations%2C%20a%20new%20attention%20graph%0Aconvolutional%20network%20is%20trained%20to%20extract%20condensed%20information%20from%20the%0Agraph%20representation.%20To%20segment%20action%20into%20sub-actions%2C%20a%20novel%20temporal%0Apyramid%20pooling%20module%20is%20proposed%2C%20which%20upsamples%20compressed%20features%20back%20to%0Athe%20original%20time%20scale%20and%20classifies%20actions%20per%20frame.%0A%20%20We%20explore%20various%20attention%20layers%2C%20namely%20spatial%20attention%2C%20temporal%0Aattention%20and%20channel%20attention%2C%20and%20combine%20different%20upsampling%20decoders%20to%0Atest%20the%20performance%20on%20action%20recognition%20and%20segmentation.%20We%20evaluate%20our%0Amodel%20on%20two%20challenging%20datasets%20in%20the%20field%20of%20human-object%20interaction%0Arecognition%2C%20i.e.%20Bimanual%20Actions%20and%20IKEA%20Assembly%20datasets.%20We%20demonstrate%0Athat%20our%20classifier%20significantly%20improves%20both%20framewise%20action%20recognition%0Aand%20segmentation%2C%20e.g.%2C%20F1%20micro%20and%20F1%4050%20scores%20on%20Bimanual%20Actions%20dataset%0Aare%20improved%20by%20%244.3%5C%25%24%20and%20%248.5%5C%25%24%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Spatio-Temporal%2520Relations%2520in%2520Human-Object%2520Interaction%250A%2520%2520using%2520Pyramid%2520Graph%2520Convolutional%2520Network%26entry.906535625%3DHao%2520Xing%2520and%2520Darius%2520Burschka%26entry.1292438233%3D%2520%2520Human%2520activities%2520recognition%2520is%2520an%2520important%2520task%2520for%2520an%2520intelligent%2520robot%252C%250Aespecially%2520in%2520the%2520field%2520of%2520human-robot%2520collaboration%252C%2520it%2520requires%2520not%2520only%2520the%250Alabel%2520of%2520sub-activities%2520but%2520also%2520the%2520temporal%2520structure%2520of%2520the%2520activity.%2520In%250Aorder%2520to%2520automatically%2520recognize%2520both%2520the%2520label%2520and%2520the%2520temporal%2520structure%2520in%250Asequence%2520of%2520human-object%2520interaction%252C%2520we%2520propose%2520a%2520novel%2520Pyramid%2520Graph%250AConvolutional%2520Network%2520%2528PGCN%2529%252C%2520which%2520employs%2520a%2520pyramidal%2520encoder-decoder%250Aarchitecture%2520consisting%2520of%2520an%2520attention%2520based%2520graph%2520convolution%2520network%2520and%2520a%250Atemporal%2520pyramid%2520pooling%2520module%2520for%2520downsampling%2520and%2520upsampling%2520interaction%250Asequence%2520on%2520the%2520temporal%2520axis%252C%2520respectively.%2520The%2520system%2520represents%2520the%25202D%2520or%25203D%250Aspatial%2520relation%2520of%2520human%2520and%2520objects%2520from%2520the%2520detection%2520results%2520in%2520video%2520data%250Aas%2520a%2520graph.%2520To%2520learn%2520the%2520human-object%2520relations%252C%2520a%2520new%2520attention%2520graph%250Aconvolutional%2520network%2520is%2520trained%2520to%2520extract%2520condensed%2520information%2520from%2520the%250Agraph%2520representation.%2520To%2520segment%2520action%2520into%2520sub-actions%252C%2520a%2520novel%2520temporal%250Apyramid%2520pooling%2520module%2520is%2520proposed%252C%2520which%2520upsamples%2520compressed%2520features%2520back%2520to%250Athe%2520original%2520time%2520scale%2520and%2520classifies%2520actions%2520per%2520frame.%250A%2520%2520We%2520explore%2520various%2520attention%2520layers%252C%2520namely%2520spatial%2520attention%252C%2520temporal%250Aattention%2520and%2520channel%2520attention%252C%2520and%2520combine%2520different%2520upsampling%2520decoders%2520to%250Atest%2520the%2520performance%2520on%2520action%2520recognition%2520and%2520segmentation.%2520We%2520evaluate%2520our%250Amodel%2520on%2520two%2520challenging%2520datasets%2520in%2520the%2520field%2520of%2520human-object%2520interaction%250Arecognition%252C%2520i.e.%2520Bimanual%2520Actions%2520and%2520IKEA%2520Assembly%2520datasets.%2520We%2520demonstrate%250Athat%2520our%2520classifier%2520significantly%2520improves%2520both%2520framewise%2520action%2520recognition%250Aand%2520segmentation%252C%2520e.g.%252C%2520F1%2520micro%2520and%2520F1%254050%2520scores%2520on%2520Bimanual%2520Actions%2520dataset%250Aare%2520improved%2520by%2520%25244.3%255C%2525%2524%2520and%2520%25248.5%255C%2525%2524%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Spatio-Temporal%20Relations%20in%20Human-Object%20Interaction%0A%20%20using%20Pyramid%20Graph%20Convolutional%20Network&entry.906535625=Hao%20Xing%20and%20Darius%20Burschka&entry.1292438233=%20%20Human%20activities%20recognition%20is%20an%20important%20task%20for%20an%20intelligent%20robot%2C%0Aespecially%20in%20the%20field%20of%20human-robot%20collaboration%2C%20it%20requires%20not%20only%20the%0Alabel%20of%20sub-activities%20but%20also%20the%20temporal%20structure%20of%20the%20activity.%20In%0Aorder%20to%20automatically%20recognize%20both%20the%20label%20and%20the%20temporal%20structure%20in%0Asequence%20of%20human-object%20interaction%2C%20we%20propose%20a%20novel%20Pyramid%20Graph%0AConvolutional%20Network%20%28PGCN%29%2C%20which%20employs%20a%20pyramidal%20encoder-decoder%0Aarchitecture%20consisting%20of%20an%20attention%20based%20graph%20convolution%20network%20and%20a%0Atemporal%20pyramid%20pooling%20module%20for%20downsampling%20and%20upsampling%20interaction%0Asequence%20on%20the%20temporal%20axis%2C%20respectively.%20The%20system%20represents%20the%202D%20or%203D%0Aspatial%20relation%20of%20human%20and%20objects%20from%20the%20detection%20results%20in%20video%20data%0Aas%20a%20graph.%20To%20learn%20the%20human-object%20relations%2C%20a%20new%20attention%20graph%0Aconvolutional%20network%20is%20trained%20to%20extract%20condensed%20information%20from%20the%0Agraph%20representation.%20To%20segment%20action%20into%20sub-actions%2C%20a%20novel%20temporal%0Apyramid%20pooling%20module%20is%20proposed%2C%20which%20upsamples%20compressed%20features%20back%20to%0Athe%20original%20time%20scale%20and%20classifies%20actions%20per%20frame.%0A%20%20We%20explore%20various%20attention%20layers%2C%20namely%20spatial%20attention%2C%20temporal%0Aattention%20and%20channel%20attention%2C%20and%20combine%20different%20upsampling%20decoders%20to%0Atest%20the%20performance%20on%20action%20recognition%20and%20segmentation.%20We%20evaluate%20our%0Amodel%20on%20two%20challenging%20datasets%20in%20the%20field%20of%20human-object%20interaction%0Arecognition%2C%20i.e.%20Bimanual%20Actions%20and%20IKEA%20Assembly%20datasets.%20We%20demonstrate%0Athat%20our%20classifier%20significantly%20improves%20both%20framewise%20action%20recognition%0Aand%20segmentation%2C%20e.g.%2C%20F1%20micro%20and%20F1%4050%20scores%20on%20Bimanual%20Actions%20dataset%0Aare%20improved%20by%20%244.3%5C%25%24%20and%20%248.5%5C%25%24%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07912v1&entry.124074799=Read"},
{"title": "QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm\n  Design", "author": "Rui Yang and Yuntian Gu and Ziruo Wang and Yitao Liang and Tongyang Li", "abstract": "  Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain.\n", "link": "http://arxiv.org/abs/2410.07961v1", "date": "2024-10-10", "relevancy": 2.2552, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QCircuitNet%3A%20A%20Large-Scale%20Hierarchical%20Dataset%20for%20Quantum%20Algorithm%0A%20%20Design&body=Title%3A%20QCircuitNet%3A%20A%20Large-Scale%20Hierarchical%20Dataset%20for%20Quantum%20Algorithm%0A%20%20Design%0AAuthor%3A%20Rui%20Yang%20and%20Yuntian%20Gu%20and%20Ziruo%20Wang%20and%20Yitao%20Liang%20and%20Tongyang%20Li%0AAbstract%3A%20%20%20Quantum%20computing%20is%20an%20emerging%20field%20recognized%20for%20the%20significant%20speedup%0Ait%20offers%20over%20classical%20computing%20through%20quantum%20algorithms.%20However%2C%0Adesigning%20and%20implementing%20quantum%20algorithms%20pose%20challenges%20due%20to%20the%0Acomplex%20nature%20of%20quantum%20mechanics%20and%20the%20necessity%20for%20precise%20control%20over%0Aquantum%20states.%20Despite%20the%20significant%20advancements%20in%20AI%2C%20there%20has%20been%20a%0Alack%20of%20datasets%20specifically%20tailored%20for%20this%20purpose.%20In%20this%20work%2C%20we%0Aintroduce%20QCircuitNet%2C%20the%20first%20benchmark%20and%20test%20dataset%20designed%20to%0Aevaluate%20AI%27s%20capability%20in%20designing%20and%20implementing%20quantum%20algorithms%20in%0Athe%20form%20of%20quantum%20circuit%20codes.%20Unlike%20using%20AI%20for%20writing%20traditional%0Acodes%2C%20this%20task%20is%20fundamentally%20different%20and%20significantly%20more%20complicated%0Adue%20to%20highly%20flexible%20design%20space%20and%20intricate%20manipulation%20of%20qubits.%20Our%0Akey%20contributions%20include%3A%201.%20A%20general%20framework%20which%20formulates%20the%20key%0Afeatures%20of%20quantum%20algorithm%20design%20task%20for%20Large%20Language%20Models.%202.%0AImplementation%20for%20a%20wide%20range%20of%20quantum%20algorithms%20from%20basic%20primitives%20to%0Aadvanced%20applications%2C%20with%20easy%20extension%20to%20more%20quantum%20algorithms.%203.%0AAutomatic%20validation%20and%20verification%20functions%2C%20allowing%20for%20iterative%0Aevaluation%20and%20interactive%20reasoning%20without%20human%20inspection.%204.%20Promising%0Apotential%20as%20a%20training%20dataset%20through%20primitive%20fine-tuning%20results.%20We%0Aobserved%20several%20interesting%20experimental%20phenomena%3A%20fine-tuning%20does%20not%0Aalways%20outperform%20few-shot%20learning%2C%20and%20LLMs%20tend%20to%20exhibit%20consistent%20error%0Apatterns.%20QCircuitNet%20provides%20a%20comprehensive%20benchmark%20for%20AI-driven%20quantum%0Aalgorithm%20design%2C%20offering%20advantages%20in%20model%20evaluation%20and%20improvement%2C%0Awhile%20also%20revealing%20some%20limitations%20of%20LLMs%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQCircuitNet%253A%2520A%2520Large-Scale%2520Hierarchical%2520Dataset%2520for%2520Quantum%2520Algorithm%250A%2520%2520Design%26entry.906535625%3DRui%2520Yang%2520and%2520Yuntian%2520Gu%2520and%2520Ziruo%2520Wang%2520and%2520Yitao%2520Liang%2520and%2520Tongyang%2520Li%26entry.1292438233%3D%2520%2520Quantum%2520computing%2520is%2520an%2520emerging%2520field%2520recognized%2520for%2520the%2520significant%2520speedup%250Ait%2520offers%2520over%2520classical%2520computing%2520through%2520quantum%2520algorithms.%2520However%252C%250Adesigning%2520and%2520implementing%2520quantum%2520algorithms%2520pose%2520challenges%2520due%2520to%2520the%250Acomplex%2520nature%2520of%2520quantum%2520mechanics%2520and%2520the%2520necessity%2520for%2520precise%2520control%2520over%250Aquantum%2520states.%2520Despite%2520the%2520significant%2520advancements%2520in%2520AI%252C%2520there%2520has%2520been%2520a%250Alack%2520of%2520datasets%2520specifically%2520tailored%2520for%2520this%2520purpose.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520QCircuitNet%252C%2520the%2520first%2520benchmark%2520and%2520test%2520dataset%2520designed%2520to%250Aevaluate%2520AI%2527s%2520capability%2520in%2520designing%2520and%2520implementing%2520quantum%2520algorithms%2520in%250Athe%2520form%2520of%2520quantum%2520circuit%2520codes.%2520Unlike%2520using%2520AI%2520for%2520writing%2520traditional%250Acodes%252C%2520this%2520task%2520is%2520fundamentally%2520different%2520and%2520significantly%2520more%2520complicated%250Adue%2520to%2520highly%2520flexible%2520design%2520space%2520and%2520intricate%2520manipulation%2520of%2520qubits.%2520Our%250Akey%2520contributions%2520include%253A%25201.%2520A%2520general%2520framework%2520which%2520formulates%2520the%2520key%250Afeatures%2520of%2520quantum%2520algorithm%2520design%2520task%2520for%2520Large%2520Language%2520Models.%25202.%250AImplementation%2520for%2520a%2520wide%2520range%2520of%2520quantum%2520algorithms%2520from%2520basic%2520primitives%2520to%250Aadvanced%2520applications%252C%2520with%2520easy%2520extension%2520to%2520more%2520quantum%2520algorithms.%25203.%250AAutomatic%2520validation%2520and%2520verification%2520functions%252C%2520allowing%2520for%2520iterative%250Aevaluation%2520and%2520interactive%2520reasoning%2520without%2520human%2520inspection.%25204.%2520Promising%250Apotential%2520as%2520a%2520training%2520dataset%2520through%2520primitive%2520fine-tuning%2520results.%2520We%250Aobserved%2520several%2520interesting%2520experimental%2520phenomena%253A%2520fine-tuning%2520does%2520not%250Aalways%2520outperform%2520few-shot%2520learning%252C%2520and%2520LLMs%2520tend%2520to%2520exhibit%2520consistent%2520error%250Apatterns.%2520QCircuitNet%2520provides%2520a%2520comprehensive%2520benchmark%2520for%2520AI-driven%2520quantum%250Aalgorithm%2520design%252C%2520offering%2520advantages%2520in%2520model%2520evaluation%2520and%2520improvement%252C%250Awhile%2520also%2520revealing%2520some%2520limitations%2520of%2520LLMs%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QCircuitNet%3A%20A%20Large-Scale%20Hierarchical%20Dataset%20for%20Quantum%20Algorithm%0A%20%20Design&entry.906535625=Rui%20Yang%20and%20Yuntian%20Gu%20and%20Ziruo%20Wang%20and%20Yitao%20Liang%20and%20Tongyang%20Li&entry.1292438233=%20%20Quantum%20computing%20is%20an%20emerging%20field%20recognized%20for%20the%20significant%20speedup%0Ait%20offers%20over%20classical%20computing%20through%20quantum%20algorithms.%20However%2C%0Adesigning%20and%20implementing%20quantum%20algorithms%20pose%20challenges%20due%20to%20the%0Acomplex%20nature%20of%20quantum%20mechanics%20and%20the%20necessity%20for%20precise%20control%20over%0Aquantum%20states.%20Despite%20the%20significant%20advancements%20in%20AI%2C%20there%20has%20been%20a%0Alack%20of%20datasets%20specifically%20tailored%20for%20this%20purpose.%20In%20this%20work%2C%20we%0Aintroduce%20QCircuitNet%2C%20the%20first%20benchmark%20and%20test%20dataset%20designed%20to%0Aevaluate%20AI%27s%20capability%20in%20designing%20and%20implementing%20quantum%20algorithms%20in%0Athe%20form%20of%20quantum%20circuit%20codes.%20Unlike%20using%20AI%20for%20writing%20traditional%0Acodes%2C%20this%20task%20is%20fundamentally%20different%20and%20significantly%20more%20complicated%0Adue%20to%20highly%20flexible%20design%20space%20and%20intricate%20manipulation%20of%20qubits.%20Our%0Akey%20contributions%20include%3A%201.%20A%20general%20framework%20which%20formulates%20the%20key%0Afeatures%20of%20quantum%20algorithm%20design%20task%20for%20Large%20Language%20Models.%202.%0AImplementation%20for%20a%20wide%20range%20of%20quantum%20algorithms%20from%20basic%20primitives%20to%0Aadvanced%20applications%2C%20with%20easy%20extension%20to%20more%20quantum%20algorithms.%203.%0AAutomatic%20validation%20and%20verification%20functions%2C%20allowing%20for%20iterative%0Aevaluation%20and%20interactive%20reasoning%20without%20human%20inspection.%204.%20Promising%0Apotential%20as%20a%20training%20dataset%20through%20primitive%20fine-tuning%20results.%20We%0Aobserved%20several%20interesting%20experimental%20phenomena%3A%20fine-tuning%20does%20not%0Aalways%20outperform%20few-shot%20learning%2C%20and%20LLMs%20tend%20to%20exhibit%20consistent%20error%0Apatterns.%20QCircuitNet%20provides%20a%20comprehensive%20benchmark%20for%20AI-driven%20quantum%0Aalgorithm%20design%2C%20offering%20advantages%20in%20model%20evaluation%20and%20improvement%2C%0Awhile%20also%20revealing%20some%20limitations%20of%20LLMs%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07961v1&entry.124074799=Read"},
{"title": "LADIMO: Face Morph Generation through Biometric Template Inversion with\n  Latent Diffusion", "author": "Marcel Grimmer and Christoph Busch", "abstract": "  Face morphing attacks pose a severe security threat to face recognition\nsystems, enabling the morphed face image to be verified against multiple\nidentities. To detect such manipulated images, the development of new face\nmorphing methods becomes essential to increase the diversity of training\ndatasets used for face morph detection. In this study, we present a\nrepresentation-level face morphing approach, namely LADIMO, that performs\nmorphing on two face recognition embeddings. Specifically, we train a Latent\nDiffusion Model to invert a biometric template - thus reconstructing the face\nimage from an FRS latent representation. Our subsequent vulnerability analysis\ndemonstrates the high morph attack potential in comparison to MIPGAN-II, an\nestablished GAN-based face morphing approach. Finally, we exploit the\nstochastic LADMIO model design in combination with our identity conditioning\nmechanism to create unlimited morphing attacks from a single face morph image\npair. We show that each face morph variant has an individual attack success\nrate, enabling us to maximize the morph attack potential by applying a simple\nre-sampling strategy. Code and pre-trained models available here:\nhttps://github.com/dasec/LADIMO\n", "link": "http://arxiv.org/abs/2410.07988v1", "date": "2024-10-10", "relevancy": 2.2354, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5667}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADIMO%3A%20Face%20Morph%20Generation%20through%20Biometric%20Template%20Inversion%20with%0A%20%20Latent%20Diffusion&body=Title%3A%20LADIMO%3A%20Face%20Morph%20Generation%20through%20Biometric%20Template%20Inversion%20with%0A%20%20Latent%20Diffusion%0AAuthor%3A%20Marcel%20Grimmer%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20Face%20morphing%20attacks%20pose%20a%20severe%20security%20threat%20to%20face%20recognition%0Asystems%2C%20enabling%20the%20morphed%20face%20image%20to%20be%20verified%20against%20multiple%0Aidentities.%20To%20detect%20such%20manipulated%20images%2C%20the%20development%20of%20new%20face%0Amorphing%20methods%20becomes%20essential%20to%20increase%20the%20diversity%20of%20training%0Adatasets%20used%20for%20face%20morph%20detection.%20In%20this%20study%2C%20we%20present%20a%0Arepresentation-level%20face%20morphing%20approach%2C%20namely%20LADIMO%2C%20that%20performs%0Amorphing%20on%20two%20face%20recognition%20embeddings.%20Specifically%2C%20we%20train%20a%20Latent%0ADiffusion%20Model%20to%20invert%20a%20biometric%20template%20-%20thus%20reconstructing%20the%20face%0Aimage%20from%20an%20FRS%20latent%20representation.%20Our%20subsequent%20vulnerability%20analysis%0Ademonstrates%20the%20high%20morph%20attack%20potential%20in%20comparison%20to%20MIPGAN-II%2C%20an%0Aestablished%20GAN-based%20face%20morphing%20approach.%20Finally%2C%20we%20exploit%20the%0Astochastic%20LADMIO%20model%20design%20in%20combination%20with%20our%20identity%20conditioning%0Amechanism%20to%20create%20unlimited%20morphing%20attacks%20from%20a%20single%20face%20morph%20image%0Apair.%20We%20show%20that%20each%20face%20morph%20variant%20has%20an%20individual%20attack%20success%0Arate%2C%20enabling%20us%20to%20maximize%20the%20morph%20attack%20potential%20by%20applying%20a%20simple%0Are-sampling%20strategy.%20Code%20and%20pre-trained%20models%20available%20here%3A%0Ahttps%3A//github.com/dasec/LADIMO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADIMO%253A%2520Face%2520Morph%2520Generation%2520through%2520Biometric%2520Template%2520Inversion%2520with%250A%2520%2520Latent%2520Diffusion%26entry.906535625%3DMarcel%2520Grimmer%2520and%2520Christoph%2520Busch%26entry.1292438233%3D%2520%2520Face%2520morphing%2520attacks%2520pose%2520a%2520severe%2520security%2520threat%2520to%2520face%2520recognition%250Asystems%252C%2520enabling%2520the%2520morphed%2520face%2520image%2520to%2520be%2520verified%2520against%2520multiple%250Aidentities.%2520To%2520detect%2520such%2520manipulated%2520images%252C%2520the%2520development%2520of%2520new%2520face%250Amorphing%2520methods%2520becomes%2520essential%2520to%2520increase%2520the%2520diversity%2520of%2520training%250Adatasets%2520used%2520for%2520face%2520morph%2520detection.%2520In%2520this%2520study%252C%2520we%2520present%2520a%250Arepresentation-level%2520face%2520morphing%2520approach%252C%2520namely%2520LADIMO%252C%2520that%2520performs%250Amorphing%2520on%2520two%2520face%2520recognition%2520embeddings.%2520Specifically%252C%2520we%2520train%2520a%2520Latent%250ADiffusion%2520Model%2520to%2520invert%2520a%2520biometric%2520template%2520-%2520thus%2520reconstructing%2520the%2520face%250Aimage%2520from%2520an%2520FRS%2520latent%2520representation.%2520Our%2520subsequent%2520vulnerability%2520analysis%250Ademonstrates%2520the%2520high%2520morph%2520attack%2520potential%2520in%2520comparison%2520to%2520MIPGAN-II%252C%2520an%250Aestablished%2520GAN-based%2520face%2520morphing%2520approach.%2520Finally%252C%2520we%2520exploit%2520the%250Astochastic%2520LADMIO%2520model%2520design%2520in%2520combination%2520with%2520our%2520identity%2520conditioning%250Amechanism%2520to%2520create%2520unlimited%2520morphing%2520attacks%2520from%2520a%2520single%2520face%2520morph%2520image%250Apair.%2520We%2520show%2520that%2520each%2520face%2520morph%2520variant%2520has%2520an%2520individual%2520attack%2520success%250Arate%252C%2520enabling%2520us%2520to%2520maximize%2520the%2520morph%2520attack%2520potential%2520by%2520applying%2520a%2520simple%250Are-sampling%2520strategy.%2520Code%2520and%2520pre-trained%2520models%2520available%2520here%253A%250Ahttps%253A//github.com/dasec/LADIMO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADIMO%3A%20Face%20Morph%20Generation%20through%20Biometric%20Template%20Inversion%20with%0A%20%20Latent%20Diffusion&entry.906535625=Marcel%20Grimmer%20and%20Christoph%20Busch&entry.1292438233=%20%20Face%20morphing%20attacks%20pose%20a%20severe%20security%20threat%20to%20face%20recognition%0Asystems%2C%20enabling%20the%20morphed%20face%20image%20to%20be%20verified%20against%20multiple%0Aidentities.%20To%20detect%20such%20manipulated%20images%2C%20the%20development%20of%20new%20face%0Amorphing%20methods%20becomes%20essential%20to%20increase%20the%20diversity%20of%20training%0Adatasets%20used%20for%20face%20morph%20detection.%20In%20this%20study%2C%20we%20present%20a%0Arepresentation-level%20face%20morphing%20approach%2C%20namely%20LADIMO%2C%20that%20performs%0Amorphing%20on%20two%20face%20recognition%20embeddings.%20Specifically%2C%20we%20train%20a%20Latent%0ADiffusion%20Model%20to%20invert%20a%20biometric%20template%20-%20thus%20reconstructing%20the%20face%0Aimage%20from%20an%20FRS%20latent%20representation.%20Our%20subsequent%20vulnerability%20analysis%0Ademonstrates%20the%20high%20morph%20attack%20potential%20in%20comparison%20to%20MIPGAN-II%2C%20an%0Aestablished%20GAN-based%20face%20morphing%20approach.%20Finally%2C%20we%20exploit%20the%0Astochastic%20LADMIO%20model%20design%20in%20combination%20with%20our%20identity%20conditioning%0Amechanism%20to%20create%20unlimited%20morphing%20attacks%20from%20a%20single%20face%20morph%20image%0Apair.%20We%20show%20that%20each%20face%20morph%20variant%20has%20an%20individual%20attack%20success%0Arate%2C%20enabling%20us%20to%20maximize%20the%20morph%20attack%20potential%20by%20applying%20a%20simple%0Are-sampling%20strategy.%20Code%20and%20pre-trained%20models%20available%20here%3A%0Ahttps%3A//github.com/dasec/LADIMO%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07988v1&entry.124074799=Read"},
{"title": "Dynamic Object Catching with Quadruped Robot Front Legs", "author": "Andr\u00e9 Schakkal and Guillaume Bellegarda and Auke Ijspeert", "abstract": "  This paper presents a framework for dynamic object catching using a quadruped\nrobot's front legs while it stands on its rear legs. The system integrates\ncomputer vision, trajectory prediction, and leg control to enable the quadruped\nto visually detect, track, and successfully catch a thrown object using an\nonboard camera. Leveraging a fine-tuned YOLOv8 model for object detection and a\nregression-based trajectory prediction module, the quadruped adapts its front\nleg positions iteratively to anticipate and intercept the object. The catching\nmaneuver involves identifying the optimal catching position, controlling the\nfront legs with Cartesian PD control, and closing the legs together at the\nright moment. We propose and validate three different methods for selecting the\noptimal catching position: 1) intersecting the predicted trajectory with a\nvertical plane, 2) selecting the point on the predicted trajectory with the\nminimal distance to the center of the robot's legs in their nominal position,\nand 3) selecting the point on the predicted trajectory with the highest\nlikelihood on a Gaussian Mixture Model (GMM) modelling the robot's reachable\nspace. Experimental results demonstrate robust catching capabilities across\nvarious scenarios, with the GMM method achieving the best performance, leading\nto an 80% catching success rate. A video demonstration of the system in action\ncan be found at https://youtu.be/sm7RdxRfIYg .\n", "link": "http://arxiv.org/abs/2410.08065v1", "date": "2024-10-10", "relevancy": 2.2014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5852}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Object%20Catching%20with%20Quadruped%20Robot%20Front%20Legs&body=Title%3A%20Dynamic%20Object%20Catching%20with%20Quadruped%20Robot%20Front%20Legs%0AAuthor%3A%20Andr%C3%A9%20Schakkal%20and%20Guillaume%20Bellegarda%20and%20Auke%20Ijspeert%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20for%20dynamic%20object%20catching%20using%20a%20quadruped%0Arobot%27s%20front%20legs%20while%20it%20stands%20on%20its%20rear%20legs.%20The%20system%20integrates%0Acomputer%20vision%2C%20trajectory%20prediction%2C%20and%20leg%20control%20to%20enable%20the%20quadruped%0Ato%20visually%20detect%2C%20track%2C%20and%20successfully%20catch%20a%20thrown%20object%20using%20an%0Aonboard%20camera.%20Leveraging%20a%20fine-tuned%20YOLOv8%20model%20for%20object%20detection%20and%20a%0Aregression-based%20trajectory%20prediction%20module%2C%20the%20quadruped%20adapts%20its%20front%0Aleg%20positions%20iteratively%20to%20anticipate%20and%20intercept%20the%20object.%20The%20catching%0Amaneuver%20involves%20identifying%20the%20optimal%20catching%20position%2C%20controlling%20the%0Afront%20legs%20with%20Cartesian%20PD%20control%2C%20and%20closing%20the%20legs%20together%20at%20the%0Aright%20moment.%20We%20propose%20and%20validate%20three%20different%20methods%20for%20selecting%20the%0Aoptimal%20catching%20position%3A%201%29%20intersecting%20the%20predicted%20trajectory%20with%20a%0Avertical%20plane%2C%202%29%20selecting%20the%20point%20on%20the%20predicted%20trajectory%20with%20the%0Aminimal%20distance%20to%20the%20center%20of%20the%20robot%27s%20legs%20in%20their%20nominal%20position%2C%0Aand%203%29%20selecting%20the%20point%20on%20the%20predicted%20trajectory%20with%20the%20highest%0Alikelihood%20on%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20modelling%20the%20robot%27s%20reachable%0Aspace.%20Experimental%20results%20demonstrate%20robust%20catching%20capabilities%20across%0Avarious%20scenarios%2C%20with%20the%20GMM%20method%20achieving%20the%20best%20performance%2C%20leading%0Ato%20an%2080%25%20catching%20success%20rate.%20A%20video%20demonstration%20of%20the%20system%20in%20action%0Acan%20be%20found%20at%20https%3A//youtu.be/sm7RdxRfIYg%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Object%2520Catching%2520with%2520Quadruped%2520Robot%2520Front%2520Legs%26entry.906535625%3DAndr%25C3%25A9%2520Schakkal%2520and%2520Guillaume%2520Bellegarda%2520and%2520Auke%2520Ijspeert%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520for%2520dynamic%2520object%2520catching%2520using%2520a%2520quadruped%250Arobot%2527s%2520front%2520legs%2520while%2520it%2520stands%2520on%2520its%2520rear%2520legs.%2520The%2520system%2520integrates%250Acomputer%2520vision%252C%2520trajectory%2520prediction%252C%2520and%2520leg%2520control%2520to%2520enable%2520the%2520quadruped%250Ato%2520visually%2520detect%252C%2520track%252C%2520and%2520successfully%2520catch%2520a%2520thrown%2520object%2520using%2520an%250Aonboard%2520camera.%2520Leveraging%2520a%2520fine-tuned%2520YOLOv8%2520model%2520for%2520object%2520detection%2520and%2520a%250Aregression-based%2520trajectory%2520prediction%2520module%252C%2520the%2520quadruped%2520adapts%2520its%2520front%250Aleg%2520positions%2520iteratively%2520to%2520anticipate%2520and%2520intercept%2520the%2520object.%2520The%2520catching%250Amaneuver%2520involves%2520identifying%2520the%2520optimal%2520catching%2520position%252C%2520controlling%2520the%250Afront%2520legs%2520with%2520Cartesian%2520PD%2520control%252C%2520and%2520closing%2520the%2520legs%2520together%2520at%2520the%250Aright%2520moment.%2520We%2520propose%2520and%2520validate%2520three%2520different%2520methods%2520for%2520selecting%2520the%250Aoptimal%2520catching%2520position%253A%25201%2529%2520intersecting%2520the%2520predicted%2520trajectory%2520with%2520a%250Avertical%2520plane%252C%25202%2529%2520selecting%2520the%2520point%2520on%2520the%2520predicted%2520trajectory%2520with%2520the%250Aminimal%2520distance%2520to%2520the%2520center%2520of%2520the%2520robot%2527s%2520legs%2520in%2520their%2520nominal%2520position%252C%250Aand%25203%2529%2520selecting%2520the%2520point%2520on%2520the%2520predicted%2520trajectory%2520with%2520the%2520highest%250Alikelihood%2520on%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520modelling%2520the%2520robot%2527s%2520reachable%250Aspace.%2520Experimental%2520results%2520demonstrate%2520robust%2520catching%2520capabilities%2520across%250Avarious%2520scenarios%252C%2520with%2520the%2520GMM%2520method%2520achieving%2520the%2520best%2520performance%252C%2520leading%250Ato%2520an%252080%2525%2520catching%2520success%2520rate.%2520A%2520video%2520demonstration%2520of%2520the%2520system%2520in%2520action%250Acan%2520be%2520found%2520at%2520https%253A//youtu.be/sm7RdxRfIYg%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Object%20Catching%20with%20Quadruped%20Robot%20Front%20Legs&entry.906535625=Andr%C3%A9%20Schakkal%20and%20Guillaume%20Bellegarda%20and%20Auke%20Ijspeert&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20for%20dynamic%20object%20catching%20using%20a%20quadruped%0Arobot%27s%20front%20legs%20while%20it%20stands%20on%20its%20rear%20legs.%20The%20system%20integrates%0Acomputer%20vision%2C%20trajectory%20prediction%2C%20and%20leg%20control%20to%20enable%20the%20quadruped%0Ato%20visually%20detect%2C%20track%2C%20and%20successfully%20catch%20a%20thrown%20object%20using%20an%0Aonboard%20camera.%20Leveraging%20a%20fine-tuned%20YOLOv8%20model%20for%20object%20detection%20and%20a%0Aregression-based%20trajectory%20prediction%20module%2C%20the%20quadruped%20adapts%20its%20front%0Aleg%20positions%20iteratively%20to%20anticipate%20and%20intercept%20the%20object.%20The%20catching%0Amaneuver%20involves%20identifying%20the%20optimal%20catching%20position%2C%20controlling%20the%0Afront%20legs%20with%20Cartesian%20PD%20control%2C%20and%20closing%20the%20legs%20together%20at%20the%0Aright%20moment.%20We%20propose%20and%20validate%20three%20different%20methods%20for%20selecting%20the%0Aoptimal%20catching%20position%3A%201%29%20intersecting%20the%20predicted%20trajectory%20with%20a%0Avertical%20plane%2C%202%29%20selecting%20the%20point%20on%20the%20predicted%20trajectory%20with%20the%0Aminimal%20distance%20to%20the%20center%20of%20the%20robot%27s%20legs%20in%20their%20nominal%20position%2C%0Aand%203%29%20selecting%20the%20point%20on%20the%20predicted%20trajectory%20with%20the%20highest%0Alikelihood%20on%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20modelling%20the%20robot%27s%20reachable%0Aspace.%20Experimental%20results%20demonstrate%20robust%20catching%20capabilities%20across%0Avarious%20scenarios%2C%20with%20the%20GMM%20method%20achieving%20the%20best%20performance%2C%20leading%0Ato%20an%2080%25%20catching%20success%20rate.%20A%20video%20demonstration%20of%20the%20system%20in%20action%0Acan%20be%20found%20at%20https%3A//youtu.be/sm7RdxRfIYg%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08065v1&entry.124074799=Read"},
{"title": "Q-VLM: Post-training Quantization for Large Vision-Language Models", "author": "Changyuan Wang and Ziwei Wang and Xiuwei Xu and Yansong Tang and Jie Zhou and Jiwen Lu", "abstract": "  In this paper, we propose a post-training quantization framework of large\nvision-language models (LVLMs) for efficient multi-modal inference.\nConventional quantization methods sequentially search the layer-wise rounding\nfunctions by minimizing activation discretization errors, which fails to\nacquire optimal quantization strategy without considering cross-layer\ndependency. On the contrary, we mine the cross-layer dependency that\nsignificantly influences discretization errors of the entire vision-language\nmodel, and embed this dependency into optimal quantization strategy searching\nwith low search cost. Specifically, we observe the strong correlation between\nthe activation entropy and the cross-layer dependency concerning output\ndiscretization errors. Therefore, we employ the entropy as the proxy to\npartition blocks optimally, which aims to achieve satisfying trade-offs between\ndiscretization errors and the search cost. Moreover, we optimize the visual\nencoder to disentangle the cross-layer dependency for fine-grained\ndecomposition of search space, so that the search cost is further reduced\nwithout harming the quantization accuracy. Experimental results demonstrate\nthat our method compresses the memory by 2.78x and increase generate speed by\n1.44x about 13B LLaVA model without performance degradation on diverse\nmulti-modal reasoning tasks. Code is available at\nhttps://github.com/ChangyuanWang17/QVLM.\n", "link": "http://arxiv.org/abs/2410.08119v1", "date": "2024-10-10", "relevancy": 2.197, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-VLM%3A%20Post-training%20Quantization%20for%20Large%20Vision-Language%20Models&body=Title%3A%20Q-VLM%3A%20Post-training%20Quantization%20for%20Large%20Vision-Language%20Models%0AAuthor%3A%20Changyuan%20Wang%20and%20Ziwei%20Wang%20and%20Xiuwei%20Xu%20and%20Yansong%20Tang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20post-training%20quantization%20framework%20of%20large%0Avision-language%20models%20%28LVLMs%29%20for%20efficient%20multi-modal%20inference.%0AConventional%20quantization%20methods%20sequentially%20search%20the%20layer-wise%20rounding%0Afunctions%20by%20minimizing%20activation%20discretization%20errors%2C%20which%20fails%20to%0Aacquire%20optimal%20quantization%20strategy%20without%20considering%20cross-layer%0Adependency.%20On%20the%20contrary%2C%20we%20mine%20the%20cross-layer%20dependency%20that%0Asignificantly%20influences%20discretization%20errors%20of%20the%20entire%20vision-language%0Amodel%2C%20and%20embed%20this%20dependency%20into%20optimal%20quantization%20strategy%20searching%0Awith%20low%20search%20cost.%20Specifically%2C%20we%20observe%20the%20strong%20correlation%20between%0Athe%20activation%20entropy%20and%20the%20cross-layer%20dependency%20concerning%20output%0Adiscretization%20errors.%20Therefore%2C%20we%20employ%20the%20entropy%20as%20the%20proxy%20to%0Apartition%20blocks%20optimally%2C%20which%20aims%20to%20achieve%20satisfying%20trade-offs%20between%0Adiscretization%20errors%20and%20the%20search%20cost.%20Moreover%2C%20we%20optimize%20the%20visual%0Aencoder%20to%20disentangle%20the%20cross-layer%20dependency%20for%20fine-grained%0Adecomposition%20of%20search%20space%2C%20so%20that%20the%20search%20cost%20is%20further%20reduced%0Awithout%20harming%20the%20quantization%20accuracy.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20compresses%20the%20memory%20by%202.78x%20and%20increase%20generate%20speed%20by%0A1.44x%20about%2013B%20LLaVA%20model%20without%20performance%20degradation%20on%20diverse%0Amulti-modal%20reasoning%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ChangyuanWang17/QVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-VLM%253A%2520Post-training%2520Quantization%2520for%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DChangyuan%2520Wang%2520and%2520Ziwei%2520Wang%2520and%2520Xiuwei%2520Xu%2520and%2520Yansong%2520Tang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520post-training%2520quantization%2520framework%2520of%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%2520for%2520efficient%2520multi-modal%2520inference.%250AConventional%2520quantization%2520methods%2520sequentially%2520search%2520the%2520layer-wise%2520rounding%250Afunctions%2520by%2520minimizing%2520activation%2520discretization%2520errors%252C%2520which%2520fails%2520to%250Aacquire%2520optimal%2520quantization%2520strategy%2520without%2520considering%2520cross-layer%250Adependency.%2520On%2520the%2520contrary%252C%2520we%2520mine%2520the%2520cross-layer%2520dependency%2520that%250Asignificantly%2520influences%2520discretization%2520errors%2520of%2520the%2520entire%2520vision-language%250Amodel%252C%2520and%2520embed%2520this%2520dependency%2520into%2520optimal%2520quantization%2520strategy%2520searching%250Awith%2520low%2520search%2520cost.%2520Specifically%252C%2520we%2520observe%2520the%2520strong%2520correlation%2520between%250Athe%2520activation%2520entropy%2520and%2520the%2520cross-layer%2520dependency%2520concerning%2520output%250Adiscretization%2520errors.%2520Therefore%252C%2520we%2520employ%2520the%2520entropy%2520as%2520the%2520proxy%2520to%250Apartition%2520blocks%2520optimally%252C%2520which%2520aims%2520to%2520achieve%2520satisfying%2520trade-offs%2520between%250Adiscretization%2520errors%2520and%2520the%2520search%2520cost.%2520Moreover%252C%2520we%2520optimize%2520the%2520visual%250Aencoder%2520to%2520disentangle%2520the%2520cross-layer%2520dependency%2520for%2520fine-grained%250Adecomposition%2520of%2520search%2520space%252C%2520so%2520that%2520the%2520search%2520cost%2520is%2520further%2520reduced%250Awithout%2520harming%2520the%2520quantization%2520accuracy.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520compresses%2520the%2520memory%2520by%25202.78x%2520and%2520increase%2520generate%2520speed%2520by%250A1.44x%2520about%252013B%2520LLaVA%2520model%2520without%2520performance%2520degradation%2520on%2520diverse%250Amulti-modal%2520reasoning%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ChangyuanWang17/QVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-VLM%3A%20Post-training%20Quantization%20for%20Large%20Vision-Language%20Models&entry.906535625=Changyuan%20Wang%20and%20Ziwei%20Wang%20and%20Xiuwei%20Xu%20and%20Yansong%20Tang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20post-training%20quantization%20framework%20of%20large%0Avision-language%20models%20%28LVLMs%29%20for%20efficient%20multi-modal%20inference.%0AConventional%20quantization%20methods%20sequentially%20search%20the%20layer-wise%20rounding%0Afunctions%20by%20minimizing%20activation%20discretization%20errors%2C%20which%20fails%20to%0Aacquire%20optimal%20quantization%20strategy%20without%20considering%20cross-layer%0Adependency.%20On%20the%20contrary%2C%20we%20mine%20the%20cross-layer%20dependency%20that%0Asignificantly%20influences%20discretization%20errors%20of%20the%20entire%20vision-language%0Amodel%2C%20and%20embed%20this%20dependency%20into%20optimal%20quantization%20strategy%20searching%0Awith%20low%20search%20cost.%20Specifically%2C%20we%20observe%20the%20strong%20correlation%20between%0Athe%20activation%20entropy%20and%20the%20cross-layer%20dependency%20concerning%20output%0Adiscretization%20errors.%20Therefore%2C%20we%20employ%20the%20entropy%20as%20the%20proxy%20to%0Apartition%20blocks%20optimally%2C%20which%20aims%20to%20achieve%20satisfying%20trade-offs%20between%0Adiscretization%20errors%20and%20the%20search%20cost.%20Moreover%2C%20we%20optimize%20the%20visual%0Aencoder%20to%20disentangle%20the%20cross-layer%20dependency%20for%20fine-grained%0Adecomposition%20of%20search%20space%2C%20so%20that%20the%20search%20cost%20is%20further%20reduced%0Awithout%20harming%20the%20quantization%20accuracy.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20compresses%20the%20memory%20by%202.78x%20and%20increase%20generate%20speed%20by%0A1.44x%20about%2013B%20LLaVA%20model%20without%20performance%20degradation%20on%20diverse%0Amulti-modal%20reasoning%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ChangyuanWang17/QVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08119v1&entry.124074799=Read"},
{"title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks", "author": "Michelle Halbheer and Dominik J. M\u00fchlematter and Alexander Becker and Dominik Narnhofer and Helge Aasen and Konrad Schindler and Mehmet Ozgur Turkoglu", "abstract": "  Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.\n", "link": "http://arxiv.org/abs/2405.14438v2", "date": "2024-10-10", "relevancy": 2.1945, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5968}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks&body=Title%3A%20LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks%0AAuthor%3A%20Michelle%20Halbheer%20and%20Dominik%20J.%20M%C3%BChlematter%20and%20Alexander%20Becker%20and%20Dominik%20Narnhofer%20and%20Helge%20Aasen%20and%20Konrad%20Schindler%20and%20Mehmet%20Ozgur%20Turkoglu%0AAbstract%3A%20%20%20Numerous%20crucial%20tasks%20in%20real-world%20decision-making%20rely%20on%20machine%20learning%0Aalgorithms%20with%20calibrated%20uncertainty%20estimates.%20However%2C%20modern%20methods%20often%0Ayield%20overconfident%20and%20uncalibrated%20predictions.%20Various%20approaches%20involve%0Atraining%20an%20ensemble%20of%20separate%20models%20to%20quantify%20the%20uncertainty%20related%20to%0Athe%20model%20itself%2C%20known%20as%20epistemic%20uncertainty.%20In%20an%20explicit%0Aimplementation%2C%20the%20ensemble%20approach%20has%20high%20computational%20cost%20and%20high%0Amemory%20requirements.%20This%20particular%20challenge%20is%20evident%20in%20state-of-the-art%0Aneural%20networks%20such%20as%20transformers%2C%20where%20even%20a%20single%20network%20is%20already%0Ademanding%20in%20terms%20of%20compute%20and%20memory.%20Consequently%2C%20efforts%20are%20made%20to%0Aemulate%20the%20ensemble%20model%20without%20actually%20instantiating%20separate%20ensemble%0Amembers%2C%20referred%20to%20as%20implicit%20ensembling.%20We%20introduce%20LoRA-Ensemble%2C%20a%0Aparameter-efficient%20deep%20ensemble%20method%20for%20self-attention%20networks%2C%20which%20is%0Abased%20on%20Low-Rank%20Adaptation%20%28LoRA%29.%20Initially%20developed%20for%20efficient%20LLM%0Afine-tuning%2C%20we%20extend%20LoRA%20to%20an%20implicit%20ensembling%20approach.%20By%20employing%20a%0Asingle%20pre-trained%20self-attention%20network%20with%20weights%20shared%20across%20all%0Amembers%2C%20we%20train%20member-specific%20low-rank%20matrices%20for%20the%20attention%0Aprojections.%20Our%20method%20exhibits%20superior%20calibration%20compared%20to%20explicit%0Aensembles%20and%20achieves%20similar%20or%20better%20accuracy%20across%20various%20prediction%0Atasks%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Ensemble%253A%2520Efficient%2520Uncertainty%2520Modelling%2520for%2520Self-attention%250A%2520%2520Networks%26entry.906535625%3DMichelle%2520Halbheer%2520and%2520Dominik%2520J.%2520M%25C3%25BChlematter%2520and%2520Alexander%2520Becker%2520and%2520Dominik%2520Narnhofer%2520and%2520Helge%2520Aasen%2520and%2520Konrad%2520Schindler%2520and%2520Mehmet%2520Ozgur%2520Turkoglu%26entry.1292438233%3D%2520%2520Numerous%2520crucial%2520tasks%2520in%2520real-world%2520decision-making%2520rely%2520on%2520machine%2520learning%250Aalgorithms%2520with%2520calibrated%2520uncertainty%2520estimates.%2520However%252C%2520modern%2520methods%2520often%250Ayield%2520overconfident%2520and%2520uncalibrated%2520predictions.%2520Various%2520approaches%2520involve%250Atraining%2520an%2520ensemble%2520of%2520separate%2520models%2520to%2520quantify%2520the%2520uncertainty%2520related%2520to%250Athe%2520model%2520itself%252C%2520known%2520as%2520epistemic%2520uncertainty.%2520In%2520an%2520explicit%250Aimplementation%252C%2520the%2520ensemble%2520approach%2520has%2520high%2520computational%2520cost%2520and%2520high%250Amemory%2520requirements.%2520This%2520particular%2520challenge%2520is%2520evident%2520in%2520state-of-the-art%250Aneural%2520networks%2520such%2520as%2520transformers%252C%2520where%2520even%2520a%2520single%2520network%2520is%2520already%250Ademanding%2520in%2520terms%2520of%2520compute%2520and%2520memory.%2520Consequently%252C%2520efforts%2520are%2520made%2520to%250Aemulate%2520the%2520ensemble%2520model%2520without%2520actually%2520instantiating%2520separate%2520ensemble%250Amembers%252C%2520referred%2520to%2520as%2520implicit%2520ensembling.%2520We%2520introduce%2520LoRA-Ensemble%252C%2520a%250Aparameter-efficient%2520deep%2520ensemble%2520method%2520for%2520self-attention%2520networks%252C%2520which%2520is%250Abased%2520on%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520Initially%2520developed%2520for%2520efficient%2520LLM%250Afine-tuning%252C%2520we%2520extend%2520LoRA%2520to%2520an%2520implicit%2520ensembling%2520approach.%2520By%2520employing%2520a%250Asingle%2520pre-trained%2520self-attention%2520network%2520with%2520weights%2520shared%2520across%2520all%250Amembers%252C%2520we%2520train%2520member-specific%2520low-rank%2520matrices%2520for%2520the%2520attention%250Aprojections.%2520Our%2520method%2520exhibits%2520superior%2520calibration%2520compared%2520to%2520explicit%250Aensembles%2520and%2520achieves%2520similar%2520or%2520better%2520accuracy%2520across%2520various%2520prediction%250Atasks%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks&entry.906535625=Michelle%20Halbheer%20and%20Dominik%20J.%20M%C3%BChlematter%20and%20Alexander%20Becker%20and%20Dominik%20Narnhofer%20and%20Helge%20Aasen%20and%20Konrad%20Schindler%20and%20Mehmet%20Ozgur%20Turkoglu&entry.1292438233=%20%20Numerous%20crucial%20tasks%20in%20real-world%20decision-making%20rely%20on%20machine%20learning%0Aalgorithms%20with%20calibrated%20uncertainty%20estimates.%20However%2C%20modern%20methods%20often%0Ayield%20overconfident%20and%20uncalibrated%20predictions.%20Various%20approaches%20involve%0Atraining%20an%20ensemble%20of%20separate%20models%20to%20quantify%20the%20uncertainty%20related%20to%0Athe%20model%20itself%2C%20known%20as%20epistemic%20uncertainty.%20In%20an%20explicit%0Aimplementation%2C%20the%20ensemble%20approach%20has%20high%20computational%20cost%20and%20high%0Amemory%20requirements.%20This%20particular%20challenge%20is%20evident%20in%20state-of-the-art%0Aneural%20networks%20such%20as%20transformers%2C%20where%20even%20a%20single%20network%20is%20already%0Ademanding%20in%20terms%20of%20compute%20and%20memory.%20Consequently%2C%20efforts%20are%20made%20to%0Aemulate%20the%20ensemble%20model%20without%20actually%20instantiating%20separate%20ensemble%0Amembers%2C%20referred%20to%20as%20implicit%20ensembling.%20We%20introduce%20LoRA-Ensemble%2C%20a%0Aparameter-efficient%20deep%20ensemble%20method%20for%20self-attention%20networks%2C%20which%20is%0Abased%20on%20Low-Rank%20Adaptation%20%28LoRA%29.%20Initially%20developed%20for%20efficient%20LLM%0Afine-tuning%2C%20we%20extend%20LoRA%20to%20an%20implicit%20ensembling%20approach.%20By%20employing%20a%0Asingle%20pre-trained%20self-attention%20network%20with%20weights%20shared%20across%20all%0Amembers%2C%20we%20train%20member-specific%20low-rank%20matrices%20for%20the%20attention%0Aprojections.%20Our%20method%20exhibits%20superior%20calibration%20compared%20to%20explicit%0Aensembles%20and%20achieves%20similar%20or%20better%20accuracy%20across%20various%20prediction%0Atasks%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14438v2&entry.124074799=Read"},
{"title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human", "author": "Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang", "abstract": "  We present Agent S, an open agentic framework that enables autonomous\ninteraction with computers through a Graphical User Interface (GUI), aimed at\ntransforming human-computer interaction by automating complex, multi-step\ntasks. Agent S aims to address three key challenges in automating computer\ntasks: acquiring domain-specific knowledge, planning over long task horizons,\nand handling dynamic, non-uniform interfaces. To this end, Agent S introduces\nexperience-augmented hierarchical planning, which learns from external\nknowledge search and internal experience retrieval at multiple levels,\nfacilitating efficient task planning and subtask execution. In addition, it\nemploys an Agent-Computer Interface (ACI) to better elicit the reasoning and\ncontrol capabilities of GUI agents based on Multimodal Large Language Models\n(MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the\nbaseline by 9.37% on success rate (an 83.6% relative improvement) and achieves\na new state-of-the-art. Comprehensive analysis highlights the effectiveness of\nindividual components and provides insights for future improvements.\nFurthermore, Agent S demonstrates broad generalizability to different operating\nsystems on a newly-released WindowsAgentArena benchmark. Code available at\nhttps://github.com/simular-ai/Agent-S.\n", "link": "http://arxiv.org/abs/2410.08164v1", "date": "2024-10-10", "relevancy": 2.1906, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6064}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5383}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20S%3A%20An%20Open%20Agentic%20Framework%20that%20Uses%20Computers%20Like%20a%20Human&body=Title%3A%20Agent%20S%3A%20An%20Open%20Agentic%20Framework%20that%20Uses%20Computers%20Like%20a%20Human%0AAuthor%3A%20Saaket%20Agashe%20and%20Jiuzhou%20Han%20and%20Shuyu%20Gan%20and%20Jiachen%20Yang%20and%20Ang%20Li%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20We%20present%20Agent%20S%2C%20an%20open%20agentic%20framework%20that%20enables%20autonomous%0Ainteraction%20with%20computers%20through%20a%20Graphical%20User%20Interface%20%28GUI%29%2C%20aimed%20at%0Atransforming%20human-computer%20interaction%20by%20automating%20complex%2C%20multi-step%0Atasks.%20Agent%20S%20aims%20to%20address%20three%20key%20challenges%20in%20automating%20computer%0Atasks%3A%20acquiring%20domain-specific%20knowledge%2C%20planning%20over%20long%20task%20horizons%2C%0Aand%20handling%20dynamic%2C%20non-uniform%20interfaces.%20To%20this%20end%2C%20Agent%20S%20introduces%0Aexperience-augmented%20hierarchical%20planning%2C%20which%20learns%20from%20external%0Aknowledge%20search%20and%20internal%20experience%20retrieval%20at%20multiple%20levels%2C%0Afacilitating%20efficient%20task%20planning%20and%20subtask%20execution.%20In%20addition%2C%20it%0Aemploys%20an%20Agent-Computer%20Interface%20%28ACI%29%20to%20better%20elicit%20the%20reasoning%20and%0Acontrol%20capabilities%20of%20GUI%20agents%20based%20on%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20Evaluation%20on%20the%20OSWorld%20benchmark%20shows%20that%20Agent%20S%20outperforms%20the%0Abaseline%20by%209.37%25%20on%20success%20rate%20%28an%2083.6%25%20relative%20improvement%29%20and%20achieves%0Aa%20new%20state-of-the-art.%20Comprehensive%20analysis%20highlights%20the%20effectiveness%20of%0Aindividual%20components%20and%20provides%20insights%20for%20future%20improvements.%0AFurthermore%2C%20Agent%20S%20demonstrates%20broad%20generalizability%20to%20different%20operating%0Asystems%20on%20a%20newly-released%20WindowsAgentArena%20benchmark.%20Code%20available%20at%0Ahttps%3A//github.com/simular-ai/Agent-S.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520S%253A%2520An%2520Open%2520Agentic%2520Framework%2520that%2520Uses%2520Computers%2520Like%2520a%2520Human%26entry.906535625%3DSaaket%2520Agashe%2520and%2520Jiuzhou%2520Han%2520and%2520Shuyu%2520Gan%2520and%2520Jiachen%2520Yang%2520and%2520Ang%2520Li%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520Agent%2520S%252C%2520an%2520open%2520agentic%2520framework%2520that%2520enables%2520autonomous%250Ainteraction%2520with%2520computers%2520through%2520a%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%252C%2520aimed%2520at%250Atransforming%2520human-computer%2520interaction%2520by%2520automating%2520complex%252C%2520multi-step%250Atasks.%2520Agent%2520S%2520aims%2520to%2520address%2520three%2520key%2520challenges%2520in%2520automating%2520computer%250Atasks%253A%2520acquiring%2520domain-specific%2520knowledge%252C%2520planning%2520over%2520long%2520task%2520horizons%252C%250Aand%2520handling%2520dynamic%252C%2520non-uniform%2520interfaces.%2520To%2520this%2520end%252C%2520Agent%2520S%2520introduces%250Aexperience-augmented%2520hierarchical%2520planning%252C%2520which%2520learns%2520from%2520external%250Aknowledge%2520search%2520and%2520internal%2520experience%2520retrieval%2520at%2520multiple%2520levels%252C%250Afacilitating%2520efficient%2520task%2520planning%2520and%2520subtask%2520execution.%2520In%2520addition%252C%2520it%250Aemploys%2520an%2520Agent-Computer%2520Interface%2520%2528ACI%2529%2520to%2520better%2520elicit%2520the%2520reasoning%2520and%250Acontrol%2520capabilities%2520of%2520GUI%2520agents%2520based%2520on%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529.%2520Evaluation%2520on%2520the%2520OSWorld%2520benchmark%2520shows%2520that%2520Agent%2520S%2520outperforms%2520the%250Abaseline%2520by%25209.37%2525%2520on%2520success%2520rate%2520%2528an%252083.6%2525%2520relative%2520improvement%2529%2520and%2520achieves%250Aa%2520new%2520state-of-the-art.%2520Comprehensive%2520analysis%2520highlights%2520the%2520effectiveness%2520of%250Aindividual%2520components%2520and%2520provides%2520insights%2520for%2520future%2520improvements.%250AFurthermore%252C%2520Agent%2520S%2520demonstrates%2520broad%2520generalizability%2520to%2520different%2520operating%250Asystems%2520on%2520a%2520newly-released%2520WindowsAgentArena%2520benchmark.%2520Code%2520available%2520at%250Ahttps%253A//github.com/simular-ai/Agent-S.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20S%3A%20An%20Open%20Agentic%20Framework%20that%20Uses%20Computers%20Like%20a%20Human&entry.906535625=Saaket%20Agashe%20and%20Jiuzhou%20Han%20and%20Shuyu%20Gan%20and%20Jiachen%20Yang%20and%20Ang%20Li%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20We%20present%20Agent%20S%2C%20an%20open%20agentic%20framework%20that%20enables%20autonomous%0Ainteraction%20with%20computers%20through%20a%20Graphical%20User%20Interface%20%28GUI%29%2C%20aimed%20at%0Atransforming%20human-computer%20interaction%20by%20automating%20complex%2C%20multi-step%0Atasks.%20Agent%20S%20aims%20to%20address%20three%20key%20challenges%20in%20automating%20computer%0Atasks%3A%20acquiring%20domain-specific%20knowledge%2C%20planning%20over%20long%20task%20horizons%2C%0Aand%20handling%20dynamic%2C%20non-uniform%20interfaces.%20To%20this%20end%2C%20Agent%20S%20introduces%0Aexperience-augmented%20hierarchical%20planning%2C%20which%20learns%20from%20external%0Aknowledge%20search%20and%20internal%20experience%20retrieval%20at%20multiple%20levels%2C%0Afacilitating%20efficient%20task%20planning%20and%20subtask%20execution.%20In%20addition%2C%20it%0Aemploys%20an%20Agent-Computer%20Interface%20%28ACI%29%20to%20better%20elicit%20the%20reasoning%20and%0Acontrol%20capabilities%20of%20GUI%20agents%20based%20on%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20Evaluation%20on%20the%20OSWorld%20benchmark%20shows%20that%20Agent%20S%20outperforms%20the%0Abaseline%20by%209.37%25%20on%20success%20rate%20%28an%2083.6%25%20relative%20improvement%29%20and%20achieves%0Aa%20new%20state-of-the-art.%20Comprehensive%20analysis%20highlights%20the%20effectiveness%20of%0Aindividual%20components%20and%20provides%20insights%20for%20future%20improvements.%0AFurthermore%2C%20Agent%20S%20demonstrates%20broad%20generalizability%20to%20different%20operating%0Asystems%20on%20a%20newly-released%20WindowsAgentArena%20benchmark.%20Code%20available%20at%0Ahttps%3A//github.com/simular-ai/Agent-S.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08164v1&entry.124074799=Read"},
{"title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning", "author": "Jinghuan Shang and Karl Schmeckpeper and Brandon B. May and Maria Vittoria Minniti and Tarik Kelestemur and David Watkins and Laura Herlant", "abstract": "  Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code,\nmodels, and demo are available at https://theia.theaiinstitute.com.\n", "link": "http://arxiv.org/abs/2407.20179v2", "date": "2024-10-10", "relevancy": 2.189, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning&body=Title%3A%20Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning%0AAuthor%3A%20Jinghuan%20Shang%20and%20Karl%20Schmeckpeper%20and%20Brandon%20B.%20May%20and%20Maria%20Vittoria%20Minniti%20and%20Tarik%20Kelestemur%20and%20David%20Watkins%20and%20Laura%20Herlant%0AAbstract%3A%20%20%20Vision-based%20robot%20policy%20learning%2C%20which%20maps%20visual%20inputs%20to%20actions%2C%0Anecessitates%20a%20holistic%20understanding%20of%20diverse%20visual%20tasks%20beyond%0Asingle-task%20needs%20like%20classification%20or%20segmentation.%20Inspired%20by%20this%2C%20we%0Aintroduce%20Theia%2C%20a%20vision%20foundation%20model%20for%20robot%20learning%20that%20distills%0Amultiple%20off-the-shelf%20vision%20foundation%20models%20trained%20on%20varied%20vision%20tasks.%0ATheia%27s%20rich%20visual%20representations%20encode%20diverse%20visual%20knowledge%2C%20enhancing%0Adownstream%20robot%20learning.%20Extensive%20experiments%20demonstrate%20that%20Theia%0Aoutperforms%20its%20teacher%20models%20and%20prior%20robot%20learning%20models%20using%20less%0Atraining%20data%20and%20smaller%20model%20sizes.%20Additionally%2C%20we%20quantify%20the%20quality%20of%0Apre-trained%20visual%20representations%20and%20hypothesize%20that%20higher%20entropy%20in%0Afeature%20norm%20distributions%20leads%20to%20improved%20robot%20learning%20performance.%20Code%2C%0Amodels%2C%20and%20demo%20are%20available%20at%20https%3A//theia.theaiinstitute.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheia%253A%2520Distilling%2520Diverse%2520Vision%2520Foundation%2520Models%2520for%2520Robot%2520Learning%26entry.906535625%3DJinghuan%2520Shang%2520and%2520Karl%2520Schmeckpeper%2520and%2520Brandon%2520B.%2520May%2520and%2520Maria%2520Vittoria%2520Minniti%2520and%2520Tarik%2520Kelestemur%2520and%2520David%2520Watkins%2520and%2520Laura%2520Herlant%26entry.1292438233%3D%2520%2520Vision-based%2520robot%2520policy%2520learning%252C%2520which%2520maps%2520visual%2520inputs%2520to%2520actions%252C%250Anecessitates%2520a%2520holistic%2520understanding%2520of%2520diverse%2520visual%2520tasks%2520beyond%250Asingle-task%2520needs%2520like%2520classification%2520or%2520segmentation.%2520Inspired%2520by%2520this%252C%2520we%250Aintroduce%2520Theia%252C%2520a%2520vision%2520foundation%2520model%2520for%2520robot%2520learning%2520that%2520distills%250Amultiple%2520off-the-shelf%2520vision%2520foundation%2520models%2520trained%2520on%2520varied%2520vision%2520tasks.%250ATheia%2527s%2520rich%2520visual%2520representations%2520encode%2520diverse%2520visual%2520knowledge%252C%2520enhancing%250Adownstream%2520robot%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Theia%250Aoutperforms%2520its%2520teacher%2520models%2520and%2520prior%2520robot%2520learning%2520models%2520using%2520less%250Atraining%2520data%2520and%2520smaller%2520model%2520sizes.%2520Additionally%252C%2520we%2520quantify%2520the%2520quality%2520of%250Apre-trained%2520visual%2520representations%2520and%2520hypothesize%2520that%2520higher%2520entropy%2520in%250Afeature%2520norm%2520distributions%2520leads%2520to%2520improved%2520robot%2520learning%2520performance.%2520Code%252C%250Amodels%252C%2520and%2520demo%2520are%2520available%2520at%2520https%253A//theia.theaiinstitute.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning&entry.906535625=Jinghuan%20Shang%20and%20Karl%20Schmeckpeper%20and%20Brandon%20B.%20May%20and%20Maria%20Vittoria%20Minniti%20and%20Tarik%20Kelestemur%20and%20David%20Watkins%20and%20Laura%20Herlant&entry.1292438233=%20%20Vision-based%20robot%20policy%20learning%2C%20which%20maps%20visual%20inputs%20to%20actions%2C%0Anecessitates%20a%20holistic%20understanding%20of%20diverse%20visual%20tasks%20beyond%0Asingle-task%20needs%20like%20classification%20or%20segmentation.%20Inspired%20by%20this%2C%20we%0Aintroduce%20Theia%2C%20a%20vision%20foundation%20model%20for%20robot%20learning%20that%20distills%0Amultiple%20off-the-shelf%20vision%20foundation%20models%20trained%20on%20varied%20vision%20tasks.%0ATheia%27s%20rich%20visual%20representations%20encode%20diverse%20visual%20knowledge%2C%20enhancing%0Adownstream%20robot%20learning.%20Extensive%20experiments%20demonstrate%20that%20Theia%0Aoutperforms%20its%20teacher%20models%20and%20prior%20robot%20learning%20models%20using%20less%0Atraining%20data%20and%20smaller%20model%20sizes.%20Additionally%2C%20we%20quantify%20the%20quality%20of%0Apre-trained%20visual%20representations%20and%20hypothesize%20that%20higher%20entropy%20in%0Afeature%20norm%20distributions%20leads%20to%20improved%20robot%20learning%20performance.%20Code%2C%0Amodels%2C%20and%20demo%20are%20available%20at%20https%3A//theia.theaiinstitute.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20179v2&entry.124074799=Read"},
{"title": "Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets", "author": "Tianjian Li and Haoran Xu and Weiting Tan and Kenton Murray and Daniel Khashabi", "abstract": "  Data availability across domains often follows a long-tail distribution: a\nfew domains have abundant data, while most face dat . a scarcity. This\nimbalance poses challenges in training language models uniformly across all\ndomains. In our study, we focus on multilingual settings, where data sizes vary\nsignificantly between high- and low-resource languages. Common strategies to\naddress this include upsampling low-resource languages (Temperature Sampling)\nor upweighting their loss (Scalarization). Although often considered\nequivalent, this assumption has not been proven, which motivates our study.\nThrough both theoretical and empirical analysis, we identify the conditions\nunder which these approaches are equivalent and when they diverge.\nSpecifically, we demonstrate that these two methods are equivalent under full\ngradient descent, but this equivalence breaks down with stochastic gradient\ndescent. Empirically, we observe that Temperature Sampling converges more\nquickly but is prone to overfitting. We argue that this faster convergence is\nlikely due to the lower variance in gradient estimations, as shown\ntheoretically. Based on these insights, we propose Cooldown, a strategy that\nreduces sampling temperature during training, accelerating convergence without\noverfitting to low-resource languages. Our method is competitive with existing\ndata re-weighting and offers computational efficiency.\n", "link": "http://arxiv.org/abs/2410.04579v2", "date": "2024-10-10", "relevancy": 2.1864, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upsample%20or%20Upweight%3F%20Balanced%20Training%20on%20Heavily%20Imbalanced%20Datasets&body=Title%3A%20Upsample%20or%20Upweight%3F%20Balanced%20Training%20on%20Heavily%20Imbalanced%20Datasets%0AAuthor%3A%20Tianjian%20Li%20and%20Haoran%20Xu%20and%20Weiting%20Tan%20and%20Kenton%20Murray%20and%20Daniel%20Khashabi%0AAbstract%3A%20%20%20Data%20availability%20across%20domains%20often%20follows%20a%20long-tail%20distribution%3A%20a%0Afew%20domains%20have%20abundant%20data%2C%20while%20most%20face%20dat%20.%20a%20scarcity.%20This%0Aimbalance%20poses%20challenges%20in%20training%20language%20models%20uniformly%20across%20all%0Adomains.%20In%20our%20study%2C%20we%20focus%20on%20multilingual%20settings%2C%20where%20data%20sizes%20vary%0Asignificantly%20between%20high-%20and%20low-resource%20languages.%20Common%20strategies%20to%0Aaddress%20this%20include%20upsampling%20low-resource%20languages%20%28Temperature%20Sampling%29%0Aor%20upweighting%20their%20loss%20%28Scalarization%29.%20Although%20often%20considered%0Aequivalent%2C%20this%20assumption%20has%20not%20been%20proven%2C%20which%20motivates%20our%20study.%0AThrough%20both%20theoretical%20and%20empirical%20analysis%2C%20we%20identify%20the%20conditions%0Aunder%20which%20these%20approaches%20are%20equivalent%20and%20when%20they%20diverge.%0ASpecifically%2C%20we%20demonstrate%20that%20these%20two%20methods%20are%20equivalent%20under%20full%0Agradient%20descent%2C%20but%20this%20equivalence%20breaks%20down%20with%20stochastic%20gradient%0Adescent.%20Empirically%2C%20we%20observe%20that%20Temperature%20Sampling%20converges%20more%0Aquickly%20but%20is%20prone%20to%20overfitting.%20We%20argue%20that%20this%20faster%20convergence%20is%0Alikely%20due%20to%20the%20lower%20variance%20in%20gradient%20estimations%2C%20as%20shown%0Atheoretically.%20Based%20on%20these%20insights%2C%20we%20propose%20Cooldown%2C%20a%20strategy%20that%0Areduces%20sampling%20temperature%20during%20training%2C%20accelerating%20convergence%20without%0Aoverfitting%20to%20low-resource%20languages.%20Our%20method%20is%20competitive%20with%20existing%0Adata%20re-weighting%20and%20offers%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpsample%2520or%2520Upweight%253F%2520Balanced%2520Training%2520on%2520Heavily%2520Imbalanced%2520Datasets%26entry.906535625%3DTianjian%2520Li%2520and%2520Haoran%2520Xu%2520and%2520Weiting%2520Tan%2520and%2520Kenton%2520Murray%2520and%2520Daniel%2520Khashabi%26entry.1292438233%3D%2520%2520Data%2520availability%2520across%2520domains%2520often%2520follows%2520a%2520long-tail%2520distribution%253A%2520a%250Afew%2520domains%2520have%2520abundant%2520data%252C%2520while%2520most%2520face%2520dat%2520.%2520a%2520scarcity.%2520This%250Aimbalance%2520poses%2520challenges%2520in%2520training%2520language%2520models%2520uniformly%2520across%2520all%250Adomains.%2520In%2520our%2520study%252C%2520we%2520focus%2520on%2520multilingual%2520settings%252C%2520where%2520data%2520sizes%2520vary%250Asignificantly%2520between%2520high-%2520and%2520low-resource%2520languages.%2520Common%2520strategies%2520to%250Aaddress%2520this%2520include%2520upsampling%2520low-resource%2520languages%2520%2528Temperature%2520Sampling%2529%250Aor%2520upweighting%2520their%2520loss%2520%2528Scalarization%2529.%2520Although%2520often%2520considered%250Aequivalent%252C%2520this%2520assumption%2520has%2520not%2520been%2520proven%252C%2520which%2520motivates%2520our%2520study.%250AThrough%2520both%2520theoretical%2520and%2520empirical%2520analysis%252C%2520we%2520identify%2520the%2520conditions%250Aunder%2520which%2520these%2520approaches%2520are%2520equivalent%2520and%2520when%2520they%2520diverge.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520these%2520two%2520methods%2520are%2520equivalent%2520under%2520full%250Agradient%2520descent%252C%2520but%2520this%2520equivalence%2520breaks%2520down%2520with%2520stochastic%2520gradient%250Adescent.%2520Empirically%252C%2520we%2520observe%2520that%2520Temperature%2520Sampling%2520converges%2520more%250Aquickly%2520but%2520is%2520prone%2520to%2520overfitting.%2520We%2520argue%2520that%2520this%2520faster%2520convergence%2520is%250Alikely%2520due%2520to%2520the%2520lower%2520variance%2520in%2520gradient%2520estimations%252C%2520as%2520shown%250Atheoretically.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520Cooldown%252C%2520a%2520strategy%2520that%250Areduces%2520sampling%2520temperature%2520during%2520training%252C%2520accelerating%2520convergence%2520without%250Aoverfitting%2520to%2520low-resource%2520languages.%2520Our%2520method%2520is%2520competitive%2520with%2520existing%250Adata%2520re-weighting%2520and%2520offers%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upsample%20or%20Upweight%3F%20Balanced%20Training%20on%20Heavily%20Imbalanced%20Datasets&entry.906535625=Tianjian%20Li%20and%20Haoran%20Xu%20and%20Weiting%20Tan%20and%20Kenton%20Murray%20and%20Daniel%20Khashabi&entry.1292438233=%20%20Data%20availability%20across%20domains%20often%20follows%20a%20long-tail%20distribution%3A%20a%0Afew%20domains%20have%20abundant%20data%2C%20while%20most%20face%20dat%20.%20a%20scarcity.%20This%0Aimbalance%20poses%20challenges%20in%20training%20language%20models%20uniformly%20across%20all%0Adomains.%20In%20our%20study%2C%20we%20focus%20on%20multilingual%20settings%2C%20where%20data%20sizes%20vary%0Asignificantly%20between%20high-%20and%20low-resource%20languages.%20Common%20strategies%20to%0Aaddress%20this%20include%20upsampling%20low-resource%20languages%20%28Temperature%20Sampling%29%0Aor%20upweighting%20their%20loss%20%28Scalarization%29.%20Although%20often%20considered%0Aequivalent%2C%20this%20assumption%20has%20not%20been%20proven%2C%20which%20motivates%20our%20study.%0AThrough%20both%20theoretical%20and%20empirical%20analysis%2C%20we%20identify%20the%20conditions%0Aunder%20which%20these%20approaches%20are%20equivalent%20and%20when%20they%20diverge.%0ASpecifically%2C%20we%20demonstrate%20that%20these%20two%20methods%20are%20equivalent%20under%20full%0Agradient%20descent%2C%20but%20this%20equivalence%20breaks%20down%20with%20stochastic%20gradient%0Adescent.%20Empirically%2C%20we%20observe%20that%20Temperature%20Sampling%20converges%20more%0Aquickly%20but%20is%20prone%20to%20overfitting.%20We%20argue%20that%20this%20faster%20convergence%20is%0Alikely%20due%20to%20the%20lower%20variance%20in%20gradient%20estimations%2C%20as%20shown%0Atheoretically.%20Based%20on%20these%20insights%2C%20we%20propose%20Cooldown%2C%20a%20strategy%20that%0Areduces%20sampling%20temperature%20during%20training%2C%20accelerating%20convergence%20without%0Aoverfitting%20to%20low-resource%20languages.%20Our%20method%20is%20competitive%20with%20existing%0Adata%20re-weighting%20and%20offers%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04579v2&entry.124074799=Read"},
{"title": "Understanding Human Activity with Uncertainty Measure for Novelty in\n  Graph Convolutional Networks", "author": "Hao Xing and Darius Burschka", "abstract": "  Understanding human activity is a crucial aspect of developing intelligent\nrobots, particularly in the domain of human-robot collaboration. Nevertheless,\nexisting systems encounter challenges such as over-segmentation, attributed to\nerrors in the up-sampling process of the decoder. In response, we introduce a\npromising solution: the Temporal Fusion Graph Convolutional Network. This\ninnovative approach aims to rectify the inadequate boundary estimation of\nindividual actions within an activity stream and mitigate the issue of\nover-segmentation in the temporal dimension.\n  Moreover, systems leveraging human activity recognition frameworks for\ndecision-making necessitate more than just the identification of actions. They\nrequire a confidence value indicative of the certainty regarding the\ncorrespondence between observations and training examples. This is crucial to\nprevent overly confident responses to unforeseen scenarios that were not part\nof the training data and may have resulted in mismatches due to weak similarity\nmeasures within the system. To address this, we propose the incorporation of a\nSpectral Normalized Residual connection aimed at enhancing efficient estimation\nof novelty in observations. This innovative approach ensures the preservation\nof input distance within the feature space by imposing constraints on the\nmaximum gradients of weight updates. By limiting these gradients, we promote a\nmore robust handling of novel situations, thereby mitigating the risks\nassociated with overconfidence. Our methodology involves the use of a Gaussian\nprocess to quantify the distance in feature space.\n", "link": "http://arxiv.org/abs/2410.07917v1", "date": "2024-10-10", "relevancy": 2.1782, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5411}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Human%20Activity%20with%20Uncertainty%20Measure%20for%20Novelty%20in%0A%20%20Graph%20Convolutional%20Networks&body=Title%3A%20Understanding%20Human%20Activity%20with%20Uncertainty%20Measure%20for%20Novelty%20in%0A%20%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Hao%20Xing%20and%20Darius%20Burschka%0AAbstract%3A%20%20%20Understanding%20human%20activity%20is%20a%20crucial%20aspect%20of%20developing%20intelligent%0Arobots%2C%20particularly%20in%20the%20domain%20of%20human-robot%20collaboration.%20Nevertheless%2C%0Aexisting%20systems%20encounter%20challenges%20such%20as%20over-segmentation%2C%20attributed%20to%0Aerrors%20in%20the%20up-sampling%20process%20of%20the%20decoder.%20In%20response%2C%20we%20introduce%20a%0Apromising%20solution%3A%20the%20Temporal%20Fusion%20Graph%20Convolutional%20Network.%20This%0Ainnovative%20approach%20aims%20to%20rectify%20the%20inadequate%20boundary%20estimation%20of%0Aindividual%20actions%20within%20an%20activity%20stream%20and%20mitigate%20the%20issue%20of%0Aover-segmentation%20in%20the%20temporal%20dimension.%0A%20%20Moreover%2C%20systems%20leveraging%20human%20activity%20recognition%20frameworks%20for%0Adecision-making%20necessitate%20more%20than%20just%20the%20identification%20of%20actions.%20They%0Arequire%20a%20confidence%20value%20indicative%20of%20the%20certainty%20regarding%20the%0Acorrespondence%20between%20observations%20and%20training%20examples.%20This%20is%20crucial%20to%0Aprevent%20overly%20confident%20responses%20to%20unforeseen%20scenarios%20that%20were%20not%20part%0Aof%20the%20training%20data%20and%20may%20have%20resulted%20in%20mismatches%20due%20to%20weak%20similarity%0Ameasures%20within%20the%20system.%20To%20address%20this%2C%20we%20propose%20the%20incorporation%20of%20a%0ASpectral%20Normalized%20Residual%20connection%20aimed%20at%20enhancing%20efficient%20estimation%0Aof%20novelty%20in%20observations.%20This%20innovative%20approach%20ensures%20the%20preservation%0Aof%20input%20distance%20within%20the%20feature%20space%20by%20imposing%20constraints%20on%20the%0Amaximum%20gradients%20of%20weight%20updates.%20By%20limiting%20these%20gradients%2C%20we%20promote%20a%0Amore%20robust%20handling%20of%20novel%20situations%2C%20thereby%20mitigating%20the%20risks%0Aassociated%20with%20overconfidence.%20Our%20methodology%20involves%20the%20use%20of%20a%20Gaussian%0Aprocess%20to%20quantify%20the%20distance%20in%20feature%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Human%2520Activity%2520with%2520Uncertainty%2520Measure%2520for%2520Novelty%2520in%250A%2520%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DHao%2520Xing%2520and%2520Darius%2520Burschka%26entry.1292438233%3D%2520%2520Understanding%2520human%2520activity%2520is%2520a%2520crucial%2520aspect%2520of%2520developing%2520intelligent%250Arobots%252C%2520particularly%2520in%2520the%2520domain%2520of%2520human-robot%2520collaboration.%2520Nevertheless%252C%250Aexisting%2520systems%2520encounter%2520challenges%2520such%2520as%2520over-segmentation%252C%2520attributed%2520to%250Aerrors%2520in%2520the%2520up-sampling%2520process%2520of%2520the%2520decoder.%2520In%2520response%252C%2520we%2520introduce%2520a%250Apromising%2520solution%253A%2520the%2520Temporal%2520Fusion%2520Graph%2520Convolutional%2520Network.%2520This%250Ainnovative%2520approach%2520aims%2520to%2520rectify%2520the%2520inadequate%2520boundary%2520estimation%2520of%250Aindividual%2520actions%2520within%2520an%2520activity%2520stream%2520and%2520mitigate%2520the%2520issue%2520of%250Aover-segmentation%2520in%2520the%2520temporal%2520dimension.%250A%2520%2520Moreover%252C%2520systems%2520leveraging%2520human%2520activity%2520recognition%2520frameworks%2520for%250Adecision-making%2520necessitate%2520more%2520than%2520just%2520the%2520identification%2520of%2520actions.%2520They%250Arequire%2520a%2520confidence%2520value%2520indicative%2520of%2520the%2520certainty%2520regarding%2520the%250Acorrespondence%2520between%2520observations%2520and%2520training%2520examples.%2520This%2520is%2520crucial%2520to%250Aprevent%2520overly%2520confident%2520responses%2520to%2520unforeseen%2520scenarios%2520that%2520were%2520not%2520part%250Aof%2520the%2520training%2520data%2520and%2520may%2520have%2520resulted%2520in%2520mismatches%2520due%2520to%2520weak%2520similarity%250Ameasures%2520within%2520the%2520system.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520incorporation%2520of%2520a%250ASpectral%2520Normalized%2520Residual%2520connection%2520aimed%2520at%2520enhancing%2520efficient%2520estimation%250Aof%2520novelty%2520in%2520observations.%2520This%2520innovative%2520approach%2520ensures%2520the%2520preservation%250Aof%2520input%2520distance%2520within%2520the%2520feature%2520space%2520by%2520imposing%2520constraints%2520on%2520the%250Amaximum%2520gradients%2520of%2520weight%2520updates.%2520By%2520limiting%2520these%2520gradients%252C%2520we%2520promote%2520a%250Amore%2520robust%2520handling%2520of%2520novel%2520situations%252C%2520thereby%2520mitigating%2520the%2520risks%250Aassociated%2520with%2520overconfidence.%2520Our%2520methodology%2520involves%2520the%2520use%2520of%2520a%2520Gaussian%250Aprocess%2520to%2520quantify%2520the%2520distance%2520in%2520feature%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Human%20Activity%20with%20Uncertainty%20Measure%20for%20Novelty%20in%0A%20%20Graph%20Convolutional%20Networks&entry.906535625=Hao%20Xing%20and%20Darius%20Burschka&entry.1292438233=%20%20Understanding%20human%20activity%20is%20a%20crucial%20aspect%20of%20developing%20intelligent%0Arobots%2C%20particularly%20in%20the%20domain%20of%20human-robot%20collaboration.%20Nevertheless%2C%0Aexisting%20systems%20encounter%20challenges%20such%20as%20over-segmentation%2C%20attributed%20to%0Aerrors%20in%20the%20up-sampling%20process%20of%20the%20decoder.%20In%20response%2C%20we%20introduce%20a%0Apromising%20solution%3A%20the%20Temporal%20Fusion%20Graph%20Convolutional%20Network.%20This%0Ainnovative%20approach%20aims%20to%20rectify%20the%20inadequate%20boundary%20estimation%20of%0Aindividual%20actions%20within%20an%20activity%20stream%20and%20mitigate%20the%20issue%20of%0Aover-segmentation%20in%20the%20temporal%20dimension.%0A%20%20Moreover%2C%20systems%20leveraging%20human%20activity%20recognition%20frameworks%20for%0Adecision-making%20necessitate%20more%20than%20just%20the%20identification%20of%20actions.%20They%0Arequire%20a%20confidence%20value%20indicative%20of%20the%20certainty%20regarding%20the%0Acorrespondence%20between%20observations%20and%20training%20examples.%20This%20is%20crucial%20to%0Aprevent%20overly%20confident%20responses%20to%20unforeseen%20scenarios%20that%20were%20not%20part%0Aof%20the%20training%20data%20and%20may%20have%20resulted%20in%20mismatches%20due%20to%20weak%20similarity%0Ameasures%20within%20the%20system.%20To%20address%20this%2C%20we%20propose%20the%20incorporation%20of%20a%0ASpectral%20Normalized%20Residual%20connection%20aimed%20at%20enhancing%20efficient%20estimation%0Aof%20novelty%20in%20observations.%20This%20innovative%20approach%20ensures%20the%20preservation%0Aof%20input%20distance%20within%20the%20feature%20space%20by%20imposing%20constraints%20on%20the%0Amaximum%20gradients%20of%20weight%20updates.%20By%20limiting%20these%20gradients%2C%20we%20promote%20a%0Amore%20robust%20handling%20of%20novel%20situations%2C%20thereby%20mitigating%20the%20risks%0Aassociated%20with%20overconfidence.%20Our%20methodology%20involves%20the%20use%20of%20a%20Gaussian%0Aprocess%20to%20quantify%20the%20distance%20in%20feature%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07917v1&entry.124074799=Read"},
{"title": "DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical\n  Awareness for Robust Fovea Localization", "author": "Sifan Song and Jinfeng Wang and Zilong Wang and Hongxing Wang and Jionglong Su and Xiaowei Ding and Kang Dang", "abstract": "  Accurate fovea localization is essential for analyzing retinal diseases to\nprevent irreversible vision loss. While current deep learning-based methods\noutperform traditional ones, they still face challenges such as the lack of\nlocal anatomical landmarks around the fovea, the inability to robustly handle\ndiseased retinal images, and the variations in image conditions. In this paper,\nwe propose a novel transformer-based architecture called DualStreamFoveaNet\n(DSFN) for multi-cue fusion. This architecture explicitly incorporates\nlong-range connections and global features using retina and vessel\ndistributions for robust fovea localization. We introduce a spatial attention\nmechanism in the dual-stream encoder to extract and fuse self-learned\nanatomical information, focusing more on features distributed along blood\nvessels and significantly reducing computational costs by decreasing token\nnumbers. Our extensive experiments show that the proposed architecture achieves\nstate-of-the-art performance on two public datasets and one large-scale private\ndataset. Furthermore, we demonstrate that the DSFN is more robust on both\nnormal and diseased retina images and has better generalization capacity in\ncross-dataset experiments.\n", "link": "http://arxiv.org/abs/2302.06961v5", "date": "2024-10-10", "relevancy": 2.173, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5372}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualStreamFoveaNet%3A%20A%20Dual%20Stream%20Fusion%20Architecture%20with%20Anatomical%0A%20%20Awareness%20for%20Robust%20Fovea%20Localization&body=Title%3A%20DualStreamFoveaNet%3A%20A%20Dual%20Stream%20Fusion%20Architecture%20with%20Anatomical%0A%20%20Awareness%20for%20Robust%20Fovea%20Localization%0AAuthor%3A%20Sifan%20Song%20and%20Jinfeng%20Wang%20and%20Zilong%20Wang%20and%20Hongxing%20Wang%20and%20Jionglong%20Su%20and%20Xiaowei%20Ding%20and%20Kang%20Dang%0AAbstract%3A%20%20%20Accurate%20fovea%20localization%20is%20essential%20for%20analyzing%20retinal%20diseases%20to%0Aprevent%20irreversible%20vision%20loss.%20While%20current%20deep%20learning-based%20methods%0Aoutperform%20traditional%20ones%2C%20they%20still%20face%20challenges%20such%20as%20the%20lack%20of%0Alocal%20anatomical%20landmarks%20around%20the%20fovea%2C%20the%20inability%20to%20robustly%20handle%0Adiseased%20retinal%20images%2C%20and%20the%20variations%20in%20image%20conditions.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20transformer-based%20architecture%20called%20DualStreamFoveaNet%0A%28DSFN%29%20for%20multi-cue%20fusion.%20This%20architecture%20explicitly%20incorporates%0Along-range%20connections%20and%20global%20features%20using%20retina%20and%20vessel%0Adistributions%20for%20robust%20fovea%20localization.%20We%20introduce%20a%20spatial%20attention%0Amechanism%20in%20the%20dual-stream%20encoder%20to%20extract%20and%20fuse%20self-learned%0Aanatomical%20information%2C%20focusing%20more%20on%20features%20distributed%20along%20blood%0Avessels%20and%20significantly%20reducing%20computational%20costs%20by%20decreasing%20token%0Anumbers.%20Our%20extensive%20experiments%20show%20that%20the%20proposed%20architecture%20achieves%0Astate-of-the-art%20performance%20on%20two%20public%20datasets%20and%20one%20large-scale%20private%0Adataset.%20Furthermore%2C%20we%20demonstrate%20that%20the%20DSFN%20is%20more%20robust%20on%20both%0Anormal%20and%20diseased%20retina%20images%20and%20has%20better%20generalization%20capacity%20in%0Across-dataset%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.06961v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualStreamFoveaNet%253A%2520A%2520Dual%2520Stream%2520Fusion%2520Architecture%2520with%2520Anatomical%250A%2520%2520Awareness%2520for%2520Robust%2520Fovea%2520Localization%26entry.906535625%3DSifan%2520Song%2520and%2520Jinfeng%2520Wang%2520and%2520Zilong%2520Wang%2520and%2520Hongxing%2520Wang%2520and%2520Jionglong%2520Su%2520and%2520Xiaowei%2520Ding%2520and%2520Kang%2520Dang%26entry.1292438233%3D%2520%2520Accurate%2520fovea%2520localization%2520is%2520essential%2520for%2520analyzing%2520retinal%2520diseases%2520to%250Aprevent%2520irreversible%2520vision%2520loss.%2520While%2520current%2520deep%2520learning-based%2520methods%250Aoutperform%2520traditional%2520ones%252C%2520they%2520still%2520face%2520challenges%2520such%2520as%2520the%2520lack%2520of%250Alocal%2520anatomical%2520landmarks%2520around%2520the%2520fovea%252C%2520the%2520inability%2520to%2520robustly%2520handle%250Adiseased%2520retinal%2520images%252C%2520and%2520the%2520variations%2520in%2520image%2520conditions.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520transformer-based%2520architecture%2520called%2520DualStreamFoveaNet%250A%2528DSFN%2529%2520for%2520multi-cue%2520fusion.%2520This%2520architecture%2520explicitly%2520incorporates%250Along-range%2520connections%2520and%2520global%2520features%2520using%2520retina%2520and%2520vessel%250Adistributions%2520for%2520robust%2520fovea%2520localization.%2520We%2520introduce%2520a%2520spatial%2520attention%250Amechanism%2520in%2520the%2520dual-stream%2520encoder%2520to%2520extract%2520and%2520fuse%2520self-learned%250Aanatomical%2520information%252C%2520focusing%2520more%2520on%2520features%2520distributed%2520along%2520blood%250Avessels%2520and%2520significantly%2520reducing%2520computational%2520costs%2520by%2520decreasing%2520token%250Anumbers.%2520Our%2520extensive%2520experiments%2520show%2520that%2520the%2520proposed%2520architecture%2520achieves%250Astate-of-the-art%2520performance%2520on%2520two%2520public%2520datasets%2520and%2520one%2520large-scale%2520private%250Adataset.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520the%2520DSFN%2520is%2520more%2520robust%2520on%2520both%250Anormal%2520and%2520diseased%2520retina%2520images%2520and%2520has%2520better%2520generalization%2520capacity%2520in%250Across-dataset%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.06961v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualStreamFoveaNet%3A%20A%20Dual%20Stream%20Fusion%20Architecture%20with%20Anatomical%0A%20%20Awareness%20for%20Robust%20Fovea%20Localization&entry.906535625=Sifan%20Song%20and%20Jinfeng%20Wang%20and%20Zilong%20Wang%20and%20Hongxing%20Wang%20and%20Jionglong%20Su%20and%20Xiaowei%20Ding%20and%20Kang%20Dang&entry.1292438233=%20%20Accurate%20fovea%20localization%20is%20essential%20for%20analyzing%20retinal%20diseases%20to%0Aprevent%20irreversible%20vision%20loss.%20While%20current%20deep%20learning-based%20methods%0Aoutperform%20traditional%20ones%2C%20they%20still%20face%20challenges%20such%20as%20the%20lack%20of%0Alocal%20anatomical%20landmarks%20around%20the%20fovea%2C%20the%20inability%20to%20robustly%20handle%0Adiseased%20retinal%20images%2C%20and%20the%20variations%20in%20image%20conditions.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20transformer-based%20architecture%20called%20DualStreamFoveaNet%0A%28DSFN%29%20for%20multi-cue%20fusion.%20This%20architecture%20explicitly%20incorporates%0Along-range%20connections%20and%20global%20features%20using%20retina%20and%20vessel%0Adistributions%20for%20robust%20fovea%20localization.%20We%20introduce%20a%20spatial%20attention%0Amechanism%20in%20the%20dual-stream%20encoder%20to%20extract%20and%20fuse%20self-learned%0Aanatomical%20information%2C%20focusing%20more%20on%20features%20distributed%20along%20blood%0Avessels%20and%20significantly%20reducing%20computational%20costs%20by%20decreasing%20token%0Anumbers.%20Our%20extensive%20experiments%20show%20that%20the%20proposed%20architecture%20achieves%0Astate-of-the-art%20performance%20on%20two%20public%20datasets%20and%20one%20large-scale%20private%0Adataset.%20Furthermore%2C%20we%20demonstrate%20that%20the%20DSFN%20is%20more%20robust%20on%20both%0Anormal%20and%20diseased%20retina%20images%20and%20has%20better%20generalization%20capacity%20in%0Across-dataset%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.06961v5&entry.124074799=Read"},
{"title": "Stress Detection Using PPG Signal and Combined Deep CNN-MLP Network", "author": "Yasin Hasanpoor and Koorosh Motaman and Bahram Tarvirdizadeh and Khalil Alipour and Mohammad Ghamari", "abstract": "  Stress has become a fact in people's lives. It has a significant effect on\nthe function of body systems and many key systems of the body including\nrespiratory, cardiovascular, and even reproductive systems are impacted by\nstress. It can be very helpful to detect stress episodes in early steps of its\nappearance to avoid damages it can cause to body systems. Using physiological\nsignals can be useful for stress detection as they reflect very important\ninformation about the human body. PPG signal due to its advantages is one of\nthe mostly used signal in this field. In this research work, we take advantage\nof PPG signals to detect stress events. The PPG signals used in this work are\ncollected from one of the newest publicly available datasets named as UBFC-Phys\nand a model is developed by using CNN-MLP deep learning algorithm. The results\nobtained from the proposed model indicate that stress can be detected with an\naccuracy of approximately 82 percent.\n", "link": "http://arxiv.org/abs/2410.07911v1", "date": "2024-10-10", "relevancy": 2.1719, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4355}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4354}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stress%20Detection%20Using%20PPG%20Signal%20and%20Combined%20Deep%20CNN-MLP%20Network&body=Title%3A%20Stress%20Detection%20Using%20PPG%20Signal%20and%20Combined%20Deep%20CNN-MLP%20Network%0AAuthor%3A%20Yasin%20Hasanpoor%20and%20Koorosh%20Motaman%20and%20Bahram%20Tarvirdizadeh%20and%20Khalil%20Alipour%20and%20Mohammad%20Ghamari%0AAbstract%3A%20%20%20Stress%20has%20become%20a%20fact%20in%20people%27s%20lives.%20It%20has%20a%20significant%20effect%20on%0Athe%20function%20of%20body%20systems%20and%20many%20key%20systems%20of%20the%20body%20including%0Arespiratory%2C%20cardiovascular%2C%20and%20even%20reproductive%20systems%20are%20impacted%20by%0Astress.%20It%20can%20be%20very%20helpful%20to%20detect%20stress%20episodes%20in%20early%20steps%20of%20its%0Aappearance%20to%20avoid%20damages%20it%20can%20cause%20to%20body%20systems.%20Using%20physiological%0Asignals%20can%20be%20useful%20for%20stress%20detection%20as%20they%20reflect%20very%20important%0Ainformation%20about%20the%20human%20body.%20PPG%20signal%20due%20to%20its%20advantages%20is%20one%20of%0Athe%20mostly%20used%20signal%20in%20this%20field.%20In%20this%20research%20work%2C%20we%20take%20advantage%0Aof%20PPG%20signals%20to%20detect%20stress%20events.%20The%20PPG%20signals%20used%20in%20this%20work%20are%0Acollected%20from%20one%20of%20the%20newest%20publicly%20available%20datasets%20named%20as%20UBFC-Phys%0Aand%20a%20model%20is%20developed%20by%20using%20CNN-MLP%20deep%20learning%20algorithm.%20The%20results%0Aobtained%20from%20the%20proposed%20model%20indicate%20that%20stress%20can%20be%20detected%20with%20an%0Aaccuracy%20of%20approximately%2082%20percent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStress%2520Detection%2520Using%2520PPG%2520Signal%2520and%2520Combined%2520Deep%2520CNN-MLP%2520Network%26entry.906535625%3DYasin%2520Hasanpoor%2520and%2520Koorosh%2520Motaman%2520and%2520Bahram%2520Tarvirdizadeh%2520and%2520Khalil%2520Alipour%2520and%2520Mohammad%2520Ghamari%26entry.1292438233%3D%2520%2520Stress%2520has%2520become%2520a%2520fact%2520in%2520people%2527s%2520lives.%2520It%2520has%2520a%2520significant%2520effect%2520on%250Athe%2520function%2520of%2520body%2520systems%2520and%2520many%2520key%2520systems%2520of%2520the%2520body%2520including%250Arespiratory%252C%2520cardiovascular%252C%2520and%2520even%2520reproductive%2520systems%2520are%2520impacted%2520by%250Astress.%2520It%2520can%2520be%2520very%2520helpful%2520to%2520detect%2520stress%2520episodes%2520in%2520early%2520steps%2520of%2520its%250Aappearance%2520to%2520avoid%2520damages%2520it%2520can%2520cause%2520to%2520body%2520systems.%2520Using%2520physiological%250Asignals%2520can%2520be%2520useful%2520for%2520stress%2520detection%2520as%2520they%2520reflect%2520very%2520important%250Ainformation%2520about%2520the%2520human%2520body.%2520PPG%2520signal%2520due%2520to%2520its%2520advantages%2520is%2520one%2520of%250Athe%2520mostly%2520used%2520signal%2520in%2520this%2520field.%2520In%2520this%2520research%2520work%252C%2520we%2520take%2520advantage%250Aof%2520PPG%2520signals%2520to%2520detect%2520stress%2520events.%2520The%2520PPG%2520signals%2520used%2520in%2520this%2520work%2520are%250Acollected%2520from%2520one%2520of%2520the%2520newest%2520publicly%2520available%2520datasets%2520named%2520as%2520UBFC-Phys%250Aand%2520a%2520model%2520is%2520developed%2520by%2520using%2520CNN-MLP%2520deep%2520learning%2520algorithm.%2520The%2520results%250Aobtained%2520from%2520the%2520proposed%2520model%2520indicate%2520that%2520stress%2520can%2520be%2520detected%2520with%2520an%250Aaccuracy%2520of%2520approximately%252082%2520percent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stress%20Detection%20Using%20PPG%20Signal%20and%20Combined%20Deep%20CNN-MLP%20Network&entry.906535625=Yasin%20Hasanpoor%20and%20Koorosh%20Motaman%20and%20Bahram%20Tarvirdizadeh%20and%20Khalil%20Alipour%20and%20Mohammad%20Ghamari&entry.1292438233=%20%20Stress%20has%20become%20a%20fact%20in%20people%27s%20lives.%20It%20has%20a%20significant%20effect%20on%0Athe%20function%20of%20body%20systems%20and%20many%20key%20systems%20of%20the%20body%20including%0Arespiratory%2C%20cardiovascular%2C%20and%20even%20reproductive%20systems%20are%20impacted%20by%0Astress.%20It%20can%20be%20very%20helpful%20to%20detect%20stress%20episodes%20in%20early%20steps%20of%20its%0Aappearance%20to%20avoid%20damages%20it%20can%20cause%20to%20body%20systems.%20Using%20physiological%0Asignals%20can%20be%20useful%20for%20stress%20detection%20as%20they%20reflect%20very%20important%0Ainformation%20about%20the%20human%20body.%20PPG%20signal%20due%20to%20its%20advantages%20is%20one%20of%0Athe%20mostly%20used%20signal%20in%20this%20field.%20In%20this%20research%20work%2C%20we%20take%20advantage%0Aof%20PPG%20signals%20to%20detect%20stress%20events.%20The%20PPG%20signals%20used%20in%20this%20work%20are%0Acollected%20from%20one%20of%20the%20newest%20publicly%20available%20datasets%20named%20as%20UBFC-Phys%0Aand%20a%20model%20is%20developed%20by%20using%20CNN-MLP%20deep%20learning%20algorithm.%20The%20results%0Aobtained%20from%20the%20proposed%20model%20indicate%20that%20stress%20can%20be%20detected%20with%20an%0Aaccuracy%20of%20approximately%2082%20percent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07911v1&entry.124074799=Read"},
{"title": "TANet: Triplet Attention Network for All-In-One Adverse Weather Image\n  Restoration", "author": "Hsing-Hua Wang and Fu-Jen Tsai and Yen-Yu Lin and Chia-Wen Lin", "abstract": "  Adverse weather image restoration aims to remove unwanted degraded artifacts,\nsuch as haze, rain, and snow, caused by adverse weather conditions. Existing\nmethods achieve remarkable results for addressing single-weather conditions.\nHowever, they face challenges when encountering unpredictable weather\nconditions, which often happen in real-world scenarios. Although different\nweather conditions exhibit different degradation patterns, they share common\ncharacteristics that are highly related and complementary, such as occlusions\ncaused by degradation patterns, color distortion, and contrast attenuation due\nto the scattering of atmospheric particles. Therefore, we focus on leveraging\ncommon knowledge across multiple weather conditions to restore images in a\nunified manner. In this paper, we propose a Triplet Attention Network (TANet)\nto efficiently and effectively address all-in-one adverse weather image\nrestoration. TANet consists of Triplet Attention Block (TAB) that incorporates\nthree types of attention mechanisms: Local Pixel-wise Attention (LPA) and\nGlobal Strip-wise Attention (GSA) to address occlusions caused by non-uniform\ndegradation patterns, and Global Distribution Attention (GDA) to address color\ndistortion and contrast attenuation caused by atmospheric phenomena. By\nleveraging common knowledge shared across different weather conditions, TANet\nsuccessfully addresses multiple weather conditions in a unified manner.\nExperimental results show that TANet efficiently and effectively achieves\nstate-of-the-art performance in all-in-one adverse weather image restoration.\nThe source code is available at https://github.com/xhuachris/TANet-ACCV-2024.\n", "link": "http://arxiv.org/abs/2410.08177v1", "date": "2024-10-10", "relevancy": 2.161, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TANet%3A%20Triplet%20Attention%20Network%20for%20All-In-One%20Adverse%20Weather%20Image%0A%20%20Restoration&body=Title%3A%20TANet%3A%20Triplet%20Attention%20Network%20for%20All-In-One%20Adverse%20Weather%20Image%0A%20%20Restoration%0AAuthor%3A%20Hsing-Hua%20Wang%20and%20Fu-Jen%20Tsai%20and%20Yen-Yu%20Lin%20and%20Chia-Wen%20Lin%0AAbstract%3A%20%20%20Adverse%20weather%20image%20restoration%20aims%20to%20remove%20unwanted%20degraded%20artifacts%2C%0Asuch%20as%20haze%2C%20rain%2C%20and%20snow%2C%20caused%20by%20adverse%20weather%20conditions.%20Existing%0Amethods%20achieve%20remarkable%20results%20for%20addressing%20single-weather%20conditions.%0AHowever%2C%20they%20face%20challenges%20when%20encountering%20unpredictable%20weather%0Aconditions%2C%20which%20often%20happen%20in%20real-world%20scenarios.%20Although%20different%0Aweather%20conditions%20exhibit%20different%20degradation%20patterns%2C%20they%20share%20common%0Acharacteristics%20that%20are%20highly%20related%20and%20complementary%2C%20such%20as%20occlusions%0Acaused%20by%20degradation%20patterns%2C%20color%20distortion%2C%20and%20contrast%20attenuation%20due%0Ato%20the%20scattering%20of%20atmospheric%20particles.%20Therefore%2C%20we%20focus%20on%20leveraging%0Acommon%20knowledge%20across%20multiple%20weather%20conditions%20to%20restore%20images%20in%20a%0Aunified%20manner.%20In%20this%20paper%2C%20we%20propose%20a%20Triplet%20Attention%20Network%20%28TANet%29%0Ato%20efficiently%20and%20effectively%20address%20all-in-one%20adverse%20weather%20image%0Arestoration.%20TANet%20consists%20of%20Triplet%20Attention%20Block%20%28TAB%29%20that%20incorporates%0Athree%20types%20of%20attention%20mechanisms%3A%20Local%20Pixel-wise%20Attention%20%28LPA%29%20and%0AGlobal%20Strip-wise%20Attention%20%28GSA%29%20to%20address%20occlusions%20caused%20by%20non-uniform%0Adegradation%20patterns%2C%20and%20Global%20Distribution%20Attention%20%28GDA%29%20to%20address%20color%0Adistortion%20and%20contrast%20attenuation%20caused%20by%20atmospheric%20phenomena.%20By%0Aleveraging%20common%20knowledge%20shared%20across%20different%20weather%20conditions%2C%20TANet%0Asuccessfully%20addresses%20multiple%20weather%20conditions%20in%20a%20unified%20manner.%0AExperimental%20results%20show%20that%20TANet%20efficiently%20and%20effectively%20achieves%0Astate-of-the-art%20performance%20in%20all-in-one%20adverse%20weather%20image%20restoration.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/xhuachris/TANet-ACCV-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTANet%253A%2520Triplet%2520Attention%2520Network%2520for%2520All-In-One%2520Adverse%2520Weather%2520Image%250A%2520%2520Restoration%26entry.906535625%3DHsing-Hua%2520Wang%2520and%2520Fu-Jen%2520Tsai%2520and%2520Yen-Yu%2520Lin%2520and%2520Chia-Wen%2520Lin%26entry.1292438233%3D%2520%2520Adverse%2520weather%2520image%2520restoration%2520aims%2520to%2520remove%2520unwanted%2520degraded%2520artifacts%252C%250Asuch%2520as%2520haze%252C%2520rain%252C%2520and%2520snow%252C%2520caused%2520by%2520adverse%2520weather%2520conditions.%2520Existing%250Amethods%2520achieve%2520remarkable%2520results%2520for%2520addressing%2520single-weather%2520conditions.%250AHowever%252C%2520they%2520face%2520challenges%2520when%2520encountering%2520unpredictable%2520weather%250Aconditions%252C%2520which%2520often%2520happen%2520in%2520real-world%2520scenarios.%2520Although%2520different%250Aweather%2520conditions%2520exhibit%2520different%2520degradation%2520patterns%252C%2520they%2520share%2520common%250Acharacteristics%2520that%2520are%2520highly%2520related%2520and%2520complementary%252C%2520such%2520as%2520occlusions%250Acaused%2520by%2520degradation%2520patterns%252C%2520color%2520distortion%252C%2520and%2520contrast%2520attenuation%2520due%250Ato%2520the%2520scattering%2520of%2520atmospheric%2520particles.%2520Therefore%252C%2520we%2520focus%2520on%2520leveraging%250Acommon%2520knowledge%2520across%2520multiple%2520weather%2520conditions%2520to%2520restore%2520images%2520in%2520a%250Aunified%2520manner.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Triplet%2520Attention%2520Network%2520%2528TANet%2529%250Ato%2520efficiently%2520and%2520effectively%2520address%2520all-in-one%2520adverse%2520weather%2520image%250Arestoration.%2520TANet%2520consists%2520of%2520Triplet%2520Attention%2520Block%2520%2528TAB%2529%2520that%2520incorporates%250Athree%2520types%2520of%2520attention%2520mechanisms%253A%2520Local%2520Pixel-wise%2520Attention%2520%2528LPA%2529%2520and%250AGlobal%2520Strip-wise%2520Attention%2520%2528GSA%2529%2520to%2520address%2520occlusions%2520caused%2520by%2520non-uniform%250Adegradation%2520patterns%252C%2520and%2520Global%2520Distribution%2520Attention%2520%2528GDA%2529%2520to%2520address%2520color%250Adistortion%2520and%2520contrast%2520attenuation%2520caused%2520by%2520atmospheric%2520phenomena.%2520By%250Aleveraging%2520common%2520knowledge%2520shared%2520across%2520different%2520weather%2520conditions%252C%2520TANet%250Asuccessfully%2520addresses%2520multiple%2520weather%2520conditions%2520in%2520a%2520unified%2520manner.%250AExperimental%2520results%2520show%2520that%2520TANet%2520efficiently%2520and%2520effectively%2520achieves%250Astate-of-the-art%2520performance%2520in%2520all-in-one%2520adverse%2520weather%2520image%2520restoration.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xhuachris/TANet-ACCV-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TANet%3A%20Triplet%20Attention%20Network%20for%20All-In-One%20Adverse%20Weather%20Image%0A%20%20Restoration&entry.906535625=Hsing-Hua%20Wang%20and%20Fu-Jen%20Tsai%20and%20Yen-Yu%20Lin%20and%20Chia-Wen%20Lin&entry.1292438233=%20%20Adverse%20weather%20image%20restoration%20aims%20to%20remove%20unwanted%20degraded%20artifacts%2C%0Asuch%20as%20haze%2C%20rain%2C%20and%20snow%2C%20caused%20by%20adverse%20weather%20conditions.%20Existing%0Amethods%20achieve%20remarkable%20results%20for%20addressing%20single-weather%20conditions.%0AHowever%2C%20they%20face%20challenges%20when%20encountering%20unpredictable%20weather%0Aconditions%2C%20which%20often%20happen%20in%20real-world%20scenarios.%20Although%20different%0Aweather%20conditions%20exhibit%20different%20degradation%20patterns%2C%20they%20share%20common%0Acharacteristics%20that%20are%20highly%20related%20and%20complementary%2C%20such%20as%20occlusions%0Acaused%20by%20degradation%20patterns%2C%20color%20distortion%2C%20and%20contrast%20attenuation%20due%0Ato%20the%20scattering%20of%20atmospheric%20particles.%20Therefore%2C%20we%20focus%20on%20leveraging%0Acommon%20knowledge%20across%20multiple%20weather%20conditions%20to%20restore%20images%20in%20a%0Aunified%20manner.%20In%20this%20paper%2C%20we%20propose%20a%20Triplet%20Attention%20Network%20%28TANet%29%0Ato%20efficiently%20and%20effectively%20address%20all-in-one%20adverse%20weather%20image%0Arestoration.%20TANet%20consists%20of%20Triplet%20Attention%20Block%20%28TAB%29%20that%20incorporates%0Athree%20types%20of%20attention%20mechanisms%3A%20Local%20Pixel-wise%20Attention%20%28LPA%29%20and%0AGlobal%20Strip-wise%20Attention%20%28GSA%29%20to%20address%20occlusions%20caused%20by%20non-uniform%0Adegradation%20patterns%2C%20and%20Global%20Distribution%20Attention%20%28GDA%29%20to%20address%20color%0Adistortion%20and%20contrast%20attenuation%20caused%20by%20atmospheric%20phenomena.%20By%0Aleveraging%20common%20knowledge%20shared%20across%20different%20weather%20conditions%2C%20TANet%0Asuccessfully%20addresses%20multiple%20weather%20conditions%20in%20a%20unified%20manner.%0AExperimental%20results%20show%20that%20TANet%20efficiently%20and%20effectively%20achieves%0Astate-of-the-art%20performance%20in%20all-in-one%20adverse%20weather%20image%20restoration.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/xhuachris/TANet-ACCV-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08177v1&entry.124074799=Read"},
{"title": "CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and\n  Subtyping in Whole Slide Images", "author": "Olga Fourkioti and Matt De Vries and Chen Jin and Daniel C. Alexander and Chris Bakal", "abstract": "  The visual examination of tissue biopsy sections is fundamental for cancer\ndiagnosis, with pathologists analyzing sections at multiple magnifications to\ndiscern tumor cells and their subtypes. However, existing attention-based\nmultiple instance learning (MIL) models used for analyzing Whole Slide Images\n(WSIs) in cancer diagnostics often overlook the contextual information of tumor\nand neighboring tiles, leading to misclassifications. To address this, we\npropose the Context-Aware Multiple Instance Learning (CAMIL) architecture.\nCAMIL incorporates neighbor-constrained attention to consider dependencies\namong tiles within a WSI and integrates contextual constraints as prior\nknowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell\nlung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17)\nmetastasis, achieving test AUCs of 97.5\\%, 95.9\\%, and 88.1\\%, respectively,\noutperforming other state-of-the-art methods. Additionally, CAMIL enhances\nmodel interpretability by identifying regions of high diagnostic value.\n", "link": "http://arxiv.org/abs/2305.05314v3", "date": "2024-10-10", "relevancy": 2.128, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5068}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMIL%3A%20Context-Aware%20Multiple%20Instance%20Learning%20for%20Cancer%20Detection%20and%0A%20%20Subtyping%20in%20Whole%20Slide%20Images&body=Title%3A%20CAMIL%3A%20Context-Aware%20Multiple%20Instance%20Learning%20for%20Cancer%20Detection%20and%0A%20%20Subtyping%20in%20Whole%20Slide%20Images%0AAuthor%3A%20Olga%20Fourkioti%20and%20Matt%20De%20Vries%20and%20Chen%20Jin%20and%20Daniel%20C.%20Alexander%20and%20Chris%20Bakal%0AAbstract%3A%20%20%20The%20visual%20examination%20of%20tissue%20biopsy%20sections%20is%20fundamental%20for%20cancer%0Adiagnosis%2C%20with%20pathologists%20analyzing%20sections%20at%20multiple%20magnifications%20to%0Adiscern%20tumor%20cells%20and%20their%20subtypes.%20However%2C%20existing%20attention-based%0Amultiple%20instance%20learning%20%28MIL%29%20models%20used%20for%20analyzing%20Whole%20Slide%20Images%0A%28WSIs%29%20in%20cancer%20diagnostics%20often%20overlook%20the%20contextual%20information%20of%20tumor%0Aand%20neighboring%20tiles%2C%20leading%20to%20misclassifications.%20To%20address%20this%2C%20we%0Apropose%20the%20Context-Aware%20Multiple%20Instance%20Learning%20%28CAMIL%29%20architecture.%0ACAMIL%20incorporates%20neighbor-constrained%20attention%20to%20consider%20dependencies%0Aamong%20tiles%20within%20a%20WSI%20and%20integrates%20contextual%20constraints%20as%20prior%0Aknowledge%20into%20the%20MIL%20model.%20We%20evaluated%20CAMIL%20on%20subtyping%20non-small%20cell%0Alung%20cancer%20%28TCGA-NSCLC%29%20and%20detecting%20lymph%20node%20%28CAMELYON16%20and%20CAMELYON17%29%0Ametastasis%2C%20achieving%20test%20AUCs%20of%2097.5%5C%25%2C%2095.9%5C%25%2C%20and%2088.1%5C%25%2C%20respectively%2C%0Aoutperforming%20other%20state-of-the-art%20methods.%20Additionally%2C%20CAMIL%20enhances%0Amodel%20interpretability%20by%20identifying%20regions%20of%20high%20diagnostic%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05314v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMIL%253A%2520Context-Aware%2520Multiple%2520Instance%2520Learning%2520for%2520Cancer%2520Detection%2520and%250A%2520%2520Subtyping%2520in%2520Whole%2520Slide%2520Images%26entry.906535625%3DOlga%2520Fourkioti%2520and%2520Matt%2520De%2520Vries%2520and%2520Chen%2520Jin%2520and%2520Daniel%2520C.%2520Alexander%2520and%2520Chris%2520Bakal%26entry.1292438233%3D%2520%2520The%2520visual%2520examination%2520of%2520tissue%2520biopsy%2520sections%2520is%2520fundamental%2520for%2520cancer%250Adiagnosis%252C%2520with%2520pathologists%2520analyzing%2520sections%2520at%2520multiple%2520magnifications%2520to%250Adiscern%2520tumor%2520cells%2520and%2520their%2520subtypes.%2520However%252C%2520existing%2520attention-based%250Amultiple%2520instance%2520learning%2520%2528MIL%2529%2520models%2520used%2520for%2520analyzing%2520Whole%2520Slide%2520Images%250A%2528WSIs%2529%2520in%2520cancer%2520diagnostics%2520often%2520overlook%2520the%2520contextual%2520information%2520of%2520tumor%250Aand%2520neighboring%2520tiles%252C%2520leading%2520to%2520misclassifications.%2520To%2520address%2520this%252C%2520we%250Apropose%2520the%2520Context-Aware%2520Multiple%2520Instance%2520Learning%2520%2528CAMIL%2529%2520architecture.%250ACAMIL%2520incorporates%2520neighbor-constrained%2520attention%2520to%2520consider%2520dependencies%250Aamong%2520tiles%2520within%2520a%2520WSI%2520and%2520integrates%2520contextual%2520constraints%2520as%2520prior%250Aknowledge%2520into%2520the%2520MIL%2520model.%2520We%2520evaluated%2520CAMIL%2520on%2520subtyping%2520non-small%2520cell%250Alung%2520cancer%2520%2528TCGA-NSCLC%2529%2520and%2520detecting%2520lymph%2520node%2520%2528CAMELYON16%2520and%2520CAMELYON17%2529%250Ametastasis%252C%2520achieving%2520test%2520AUCs%2520of%252097.5%255C%2525%252C%252095.9%255C%2525%252C%2520and%252088.1%255C%2525%252C%2520respectively%252C%250Aoutperforming%2520other%2520state-of-the-art%2520methods.%2520Additionally%252C%2520CAMIL%2520enhances%250Amodel%2520interpretability%2520by%2520identifying%2520regions%2520of%2520high%2520diagnostic%2520value.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05314v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMIL%3A%20Context-Aware%20Multiple%20Instance%20Learning%20for%20Cancer%20Detection%20and%0A%20%20Subtyping%20in%20Whole%20Slide%20Images&entry.906535625=Olga%20Fourkioti%20and%20Matt%20De%20Vries%20and%20Chen%20Jin%20and%20Daniel%20C.%20Alexander%20and%20Chris%20Bakal&entry.1292438233=%20%20The%20visual%20examination%20of%20tissue%20biopsy%20sections%20is%20fundamental%20for%20cancer%0Adiagnosis%2C%20with%20pathologists%20analyzing%20sections%20at%20multiple%20magnifications%20to%0Adiscern%20tumor%20cells%20and%20their%20subtypes.%20However%2C%20existing%20attention-based%0Amultiple%20instance%20learning%20%28MIL%29%20models%20used%20for%20analyzing%20Whole%20Slide%20Images%0A%28WSIs%29%20in%20cancer%20diagnostics%20often%20overlook%20the%20contextual%20information%20of%20tumor%0Aand%20neighboring%20tiles%2C%20leading%20to%20misclassifications.%20To%20address%20this%2C%20we%0Apropose%20the%20Context-Aware%20Multiple%20Instance%20Learning%20%28CAMIL%29%20architecture.%0ACAMIL%20incorporates%20neighbor-constrained%20attention%20to%20consider%20dependencies%0Aamong%20tiles%20within%20a%20WSI%20and%20integrates%20contextual%20constraints%20as%20prior%0Aknowledge%20into%20the%20MIL%20model.%20We%20evaluated%20CAMIL%20on%20subtyping%20non-small%20cell%0Alung%20cancer%20%28TCGA-NSCLC%29%20and%20detecting%20lymph%20node%20%28CAMELYON16%20and%20CAMELYON17%29%0Ametastasis%2C%20achieving%20test%20AUCs%20of%2097.5%5C%25%2C%2095.9%5C%25%2C%20and%2088.1%5C%25%2C%20respectively%2C%0Aoutperforming%20other%20state-of-the-art%20methods.%20Additionally%2C%20CAMIL%20enhances%0Amodel%20interpretability%20by%20identifying%20regions%20of%20high%20diagnostic%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05314v3&entry.124074799=Read"},
{"title": "Robust AI-Generated Text Detection by Restricted Embeddings", "author": "Kristian Kuznetsov and Eduard Tulchinskii and Laida Kushnareva and German Magai and Serguei Barannikov and Sergey Nikolenko and Irina Piontkovskaya", "abstract": "  Growing amount and quality of AI-generated texts makes detecting such content\nmore difficult. In most real-world scenarios, the domain (style and topic) of\ngenerated data and the generator model are not known in advance. In this work,\nwe focus on the robustness of classifier-based detectors of AI-generated text,\nnamely their ability to transfer to unseen generators or semantic domains. We\ninvestigate the geometry of the embedding space of Transformer-based text\nencoders and show that clearing out harmful linear subspaces helps to train a\nrobust classifier, ignoring domain-specific spurious features. We investigate\nseveral subspace decomposition and feature selection strategies and achieve\nsignificant improvements over state of the art methods in cross-domain and\ncross-generator transfer. Our best approaches for head-wise and\ncoordinate-based subspace removal increase the mean out-of-distribution (OOD)\nclassification score by up to 9% and 14% in particular setups for RoBERTa and\nBERT embeddings respectively. We release our code and data:\nhttps://github.com/SilverSolver/RobustATD\n", "link": "http://arxiv.org/abs/2410.08113v1", "date": "2024-10-10", "relevancy": 2.109, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5292}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.527}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20AI-Generated%20Text%20Detection%20by%20Restricted%20Embeddings&body=Title%3A%20Robust%20AI-Generated%20Text%20Detection%20by%20Restricted%20Embeddings%0AAuthor%3A%20Kristian%20Kuznetsov%20and%20Eduard%20Tulchinskii%20and%20Laida%20Kushnareva%20and%20German%20Magai%20and%20Serguei%20Barannikov%20and%20Sergey%20Nikolenko%20and%20Irina%20Piontkovskaya%0AAbstract%3A%20%20%20Growing%20amount%20and%20quality%20of%20AI-generated%20texts%20makes%20detecting%20such%20content%0Amore%20difficult.%20In%20most%20real-world%20scenarios%2C%20the%20domain%20%28style%20and%20topic%29%20of%0Agenerated%20data%20and%20the%20generator%20model%20are%20not%20known%20in%20advance.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20robustness%20of%20classifier-based%20detectors%20of%20AI-generated%20text%2C%0Anamely%20their%20ability%20to%20transfer%20to%20unseen%20generators%20or%20semantic%20domains.%20We%0Ainvestigate%20the%20geometry%20of%20the%20embedding%20space%20of%20Transformer-based%20text%0Aencoders%20and%20show%20that%20clearing%20out%20harmful%20linear%20subspaces%20helps%20to%20train%20a%0Arobust%20classifier%2C%20ignoring%20domain-specific%20spurious%20features.%20We%20investigate%0Aseveral%20subspace%20decomposition%20and%20feature%20selection%20strategies%20and%20achieve%0Asignificant%20improvements%20over%20state%20of%20the%20art%20methods%20in%20cross-domain%20and%0Across-generator%20transfer.%20Our%20best%20approaches%20for%20head-wise%20and%0Acoordinate-based%20subspace%20removal%20increase%20the%20mean%20out-of-distribution%20%28OOD%29%0Aclassification%20score%20by%20up%20to%209%25%20and%2014%25%20in%20particular%20setups%20for%20RoBERTa%20and%0ABERT%20embeddings%20respectively.%20We%20release%20our%20code%20and%20data%3A%0Ahttps%3A//github.com/SilverSolver/RobustATD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520AI-Generated%2520Text%2520Detection%2520by%2520Restricted%2520Embeddings%26entry.906535625%3DKristian%2520Kuznetsov%2520and%2520Eduard%2520Tulchinskii%2520and%2520Laida%2520Kushnareva%2520and%2520German%2520Magai%2520and%2520Serguei%2520Barannikov%2520and%2520Sergey%2520Nikolenko%2520and%2520Irina%2520Piontkovskaya%26entry.1292438233%3D%2520%2520Growing%2520amount%2520and%2520quality%2520of%2520AI-generated%2520texts%2520makes%2520detecting%2520such%2520content%250Amore%2520difficult.%2520In%2520most%2520real-world%2520scenarios%252C%2520the%2520domain%2520%2528style%2520and%2520topic%2529%2520of%250Agenerated%2520data%2520and%2520the%2520generator%2520model%2520are%2520not%2520known%2520in%2520advance.%2520In%2520this%2520work%252C%250Awe%2520focus%2520on%2520the%2520robustness%2520of%2520classifier-based%2520detectors%2520of%2520AI-generated%2520text%252C%250Anamely%2520their%2520ability%2520to%2520transfer%2520to%2520unseen%2520generators%2520or%2520semantic%2520domains.%2520We%250Ainvestigate%2520the%2520geometry%2520of%2520the%2520embedding%2520space%2520of%2520Transformer-based%2520text%250Aencoders%2520and%2520show%2520that%2520clearing%2520out%2520harmful%2520linear%2520subspaces%2520helps%2520to%2520train%2520a%250Arobust%2520classifier%252C%2520ignoring%2520domain-specific%2520spurious%2520features.%2520We%2520investigate%250Aseveral%2520subspace%2520decomposition%2520and%2520feature%2520selection%2520strategies%2520and%2520achieve%250Asignificant%2520improvements%2520over%2520state%2520of%2520the%2520art%2520methods%2520in%2520cross-domain%2520and%250Across-generator%2520transfer.%2520Our%2520best%2520approaches%2520for%2520head-wise%2520and%250Acoordinate-based%2520subspace%2520removal%2520increase%2520the%2520mean%2520out-of-distribution%2520%2528OOD%2529%250Aclassification%2520score%2520by%2520up%2520to%25209%2525%2520and%252014%2525%2520in%2520particular%2520setups%2520for%2520RoBERTa%2520and%250ABERT%2520embeddings%2520respectively.%2520We%2520release%2520our%2520code%2520and%2520data%253A%250Ahttps%253A//github.com/SilverSolver/RobustATD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20AI-Generated%20Text%20Detection%20by%20Restricted%20Embeddings&entry.906535625=Kristian%20Kuznetsov%20and%20Eduard%20Tulchinskii%20and%20Laida%20Kushnareva%20and%20German%20Magai%20and%20Serguei%20Barannikov%20and%20Sergey%20Nikolenko%20and%20Irina%20Piontkovskaya&entry.1292438233=%20%20Growing%20amount%20and%20quality%20of%20AI-generated%20texts%20makes%20detecting%20such%20content%0Amore%20difficult.%20In%20most%20real-world%20scenarios%2C%20the%20domain%20%28style%20and%20topic%29%20of%0Agenerated%20data%20and%20the%20generator%20model%20are%20not%20known%20in%20advance.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20robustness%20of%20classifier-based%20detectors%20of%20AI-generated%20text%2C%0Anamely%20their%20ability%20to%20transfer%20to%20unseen%20generators%20or%20semantic%20domains.%20We%0Ainvestigate%20the%20geometry%20of%20the%20embedding%20space%20of%20Transformer-based%20text%0Aencoders%20and%20show%20that%20clearing%20out%20harmful%20linear%20subspaces%20helps%20to%20train%20a%0Arobust%20classifier%2C%20ignoring%20domain-specific%20spurious%20features.%20We%20investigate%0Aseveral%20subspace%20decomposition%20and%20feature%20selection%20strategies%20and%20achieve%0Asignificant%20improvements%20over%20state%20of%20the%20art%20methods%20in%20cross-domain%20and%0Across-generator%20transfer.%20Our%20best%20approaches%20for%20head-wise%20and%0Acoordinate-based%20subspace%20removal%20increase%20the%20mean%20out-of-distribution%20%28OOD%29%0Aclassification%20score%20by%20up%20to%209%25%20and%2014%25%20in%20particular%20setups%20for%20RoBERTa%20and%0ABERT%20embeddings%20respectively.%20We%20release%20our%20code%20and%20data%3A%0Ahttps%3A//github.com/SilverSolver/RobustATD%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08113v1&entry.124074799=Read"},
{"title": "LiPO: LiDAR Inertial Odometry for ICP Comparison", "author": "Darwin Mick and Taylor Pool and Madankumar Sathenahally Nagaraju and Michael Kaess and Howie Choset and Matt Travers", "abstract": "  We introduce a LiDAR inertial odometry (LIO) framework, called LiPO, that\nenables direct comparisons of different iterative closest point (ICP) point\ncloud registration methods. The two common ICP methods we compare are\npoint-to-point (P2P) and point-to-feature (P2F). In our experience, within the\ncontext of LIO, P2F-ICP results in less drift and improved mapping accuracy\nwhen robots move aggressively through challenging environments when compared to\nP2P-ICP. However, P2F-ICP methods require more hand-tuned hyper-parameters that\nmake P2F-ICP less general across all environments and motions. In real-world\nfield robotics applications where robots are used across different\nenvironments, more general P2P-ICP methods may be preferred despite increased\ndrift. In this paper, we seek to better quantify the trade-off between P2P-ICP\nand P2F-ICP to help inform when each method should be used. To explore this\ntrade-off, we use LiPO to directly compare ICP methods and test on relevant\nbenchmark datasets as well as on our custom unpiloted ground vehicle (UGV). We\nfind that overall, P2F-ICP has reduced drift and improved mapping accuracy,\nbut, P2P-ICP is more consistent across all environments and motions with\nminimal drift increase.\n", "link": "http://arxiv.org/abs/2410.08097v1", "date": "2024-10-10", "relevancy": 2.1059, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiPO%3A%20LiDAR%20Inertial%20Odometry%20for%20ICP%20Comparison&body=Title%3A%20LiPO%3A%20LiDAR%20Inertial%20Odometry%20for%20ICP%20Comparison%0AAuthor%3A%20Darwin%20Mick%20and%20Taylor%20Pool%20and%20Madankumar%20Sathenahally%20Nagaraju%20and%20Michael%20Kaess%20and%20Howie%20Choset%20and%20Matt%20Travers%0AAbstract%3A%20%20%20We%20introduce%20a%20LiDAR%20inertial%20odometry%20%28LIO%29%20framework%2C%20called%20LiPO%2C%20that%0Aenables%20direct%20comparisons%20of%20different%20iterative%20closest%20point%20%28ICP%29%20point%0Acloud%20registration%20methods.%20The%20two%20common%20ICP%20methods%20we%20compare%20are%0Apoint-to-point%20%28P2P%29%20and%20point-to-feature%20%28P2F%29.%20In%20our%20experience%2C%20within%20the%0Acontext%20of%20LIO%2C%20P2F-ICP%20results%20in%20less%20drift%20and%20improved%20mapping%20accuracy%0Awhen%20robots%20move%20aggressively%20through%20challenging%20environments%20when%20compared%20to%0AP2P-ICP.%20However%2C%20P2F-ICP%20methods%20require%20more%20hand-tuned%20hyper-parameters%20that%0Amake%20P2F-ICP%20less%20general%20across%20all%20environments%20and%20motions.%20In%20real-world%0Afield%20robotics%20applications%20where%20robots%20are%20used%20across%20different%0Aenvironments%2C%20more%20general%20P2P-ICP%20methods%20may%20be%20preferred%20despite%20increased%0Adrift.%20In%20this%20paper%2C%20we%20seek%20to%20better%20quantify%20the%20trade-off%20between%20P2P-ICP%0Aand%20P2F-ICP%20to%20help%20inform%20when%20each%20method%20should%20be%20used.%20To%20explore%20this%0Atrade-off%2C%20we%20use%20LiPO%20to%20directly%20compare%20ICP%20methods%20and%20test%20on%20relevant%0Abenchmark%20datasets%20as%20well%20as%20on%20our%20custom%20unpiloted%20ground%20vehicle%20%28UGV%29.%20We%0Afind%20that%20overall%2C%20P2F-ICP%20has%20reduced%20drift%20and%20improved%20mapping%20accuracy%2C%0Abut%2C%20P2P-ICP%20is%20more%20consistent%20across%20all%20environments%20and%20motions%20with%0Aminimal%20drift%20increase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiPO%253A%2520LiDAR%2520Inertial%2520Odometry%2520for%2520ICP%2520Comparison%26entry.906535625%3DDarwin%2520Mick%2520and%2520Taylor%2520Pool%2520and%2520Madankumar%2520Sathenahally%2520Nagaraju%2520and%2520Michael%2520Kaess%2520and%2520Howie%2520Choset%2520and%2520Matt%2520Travers%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520LiDAR%2520inertial%2520odometry%2520%2528LIO%2529%2520framework%252C%2520called%2520LiPO%252C%2520that%250Aenables%2520direct%2520comparisons%2520of%2520different%2520iterative%2520closest%2520point%2520%2528ICP%2529%2520point%250Acloud%2520registration%2520methods.%2520The%2520two%2520common%2520ICP%2520methods%2520we%2520compare%2520are%250Apoint-to-point%2520%2528P2P%2529%2520and%2520point-to-feature%2520%2528P2F%2529.%2520In%2520our%2520experience%252C%2520within%2520the%250Acontext%2520of%2520LIO%252C%2520P2F-ICP%2520results%2520in%2520less%2520drift%2520and%2520improved%2520mapping%2520accuracy%250Awhen%2520robots%2520move%2520aggressively%2520through%2520challenging%2520environments%2520when%2520compared%2520to%250AP2P-ICP.%2520However%252C%2520P2F-ICP%2520methods%2520require%2520more%2520hand-tuned%2520hyper-parameters%2520that%250Amake%2520P2F-ICP%2520less%2520general%2520across%2520all%2520environments%2520and%2520motions.%2520In%2520real-world%250Afield%2520robotics%2520applications%2520where%2520robots%2520are%2520used%2520across%2520different%250Aenvironments%252C%2520more%2520general%2520P2P-ICP%2520methods%2520may%2520be%2520preferred%2520despite%2520increased%250Adrift.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520better%2520quantify%2520the%2520trade-off%2520between%2520P2P-ICP%250Aand%2520P2F-ICP%2520to%2520help%2520inform%2520when%2520each%2520method%2520should%2520be%2520used.%2520To%2520explore%2520this%250Atrade-off%252C%2520we%2520use%2520LiPO%2520to%2520directly%2520compare%2520ICP%2520methods%2520and%2520test%2520on%2520relevant%250Abenchmark%2520datasets%2520as%2520well%2520as%2520on%2520our%2520custom%2520unpiloted%2520ground%2520vehicle%2520%2528UGV%2529.%2520We%250Afind%2520that%2520overall%252C%2520P2F-ICP%2520has%2520reduced%2520drift%2520and%2520improved%2520mapping%2520accuracy%252C%250Abut%252C%2520P2P-ICP%2520is%2520more%2520consistent%2520across%2520all%2520environments%2520and%2520motions%2520with%250Aminimal%2520drift%2520increase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiPO%3A%20LiDAR%20Inertial%20Odometry%20for%20ICP%20Comparison&entry.906535625=Darwin%20Mick%20and%20Taylor%20Pool%20and%20Madankumar%20Sathenahally%20Nagaraju%20and%20Michael%20Kaess%20and%20Howie%20Choset%20and%20Matt%20Travers&entry.1292438233=%20%20We%20introduce%20a%20LiDAR%20inertial%20odometry%20%28LIO%29%20framework%2C%20called%20LiPO%2C%20that%0Aenables%20direct%20comparisons%20of%20different%20iterative%20closest%20point%20%28ICP%29%20point%0Acloud%20registration%20methods.%20The%20two%20common%20ICP%20methods%20we%20compare%20are%0Apoint-to-point%20%28P2P%29%20and%20point-to-feature%20%28P2F%29.%20In%20our%20experience%2C%20within%20the%0Acontext%20of%20LIO%2C%20P2F-ICP%20results%20in%20less%20drift%20and%20improved%20mapping%20accuracy%0Awhen%20robots%20move%20aggressively%20through%20challenging%20environments%20when%20compared%20to%0AP2P-ICP.%20However%2C%20P2F-ICP%20methods%20require%20more%20hand-tuned%20hyper-parameters%20that%0Amake%20P2F-ICP%20less%20general%20across%20all%20environments%20and%20motions.%20In%20real-world%0Afield%20robotics%20applications%20where%20robots%20are%20used%20across%20different%0Aenvironments%2C%20more%20general%20P2P-ICP%20methods%20may%20be%20preferred%20despite%20increased%0Adrift.%20In%20this%20paper%2C%20we%20seek%20to%20better%20quantify%20the%20trade-off%20between%20P2P-ICP%0Aand%20P2F-ICP%20to%20help%20inform%20when%20each%20method%20should%20be%20used.%20To%20explore%20this%0Atrade-off%2C%20we%20use%20LiPO%20to%20directly%20compare%20ICP%20methods%20and%20test%20on%20relevant%0Abenchmark%20datasets%20as%20well%20as%20on%20our%20custom%20unpiloted%20ground%20vehicle%20%28UGV%29.%20We%0Afind%20that%20overall%2C%20P2F-ICP%20has%20reduced%20drift%20and%20improved%20mapping%20accuracy%2C%0Abut%2C%20P2P-ICP%20is%20more%20consistent%20across%20all%20environments%20and%20motions%20with%0Aminimal%20drift%20increase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08097v1&entry.124074799=Read"},
{"title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System", "author": "Weize Chen and Jiarui Yuan and Chen Qian and Cheng Yang and Zhiyuan Liu and Maosong Sun", "abstract": "  Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).\n", "link": "http://arxiv.org/abs/2410.08115v1", "date": "2024-10-10", "relevancy": 2.0987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System&body=Title%3A%20Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System%0AAuthor%3A%20Weize%20Chen%20and%20Jiarui%20Yuan%20and%20Chen%20Qian%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20based%20multi-agent%20systems%20%28MAS%29%20show%20remarkable%0Apotential%20in%20collaborative%20problem-solving%2C%20yet%20they%20still%20face%20critical%0Achallenges%3A%20low%20communication%20efficiency%2C%20poor%20scalability%2C%20and%20a%20lack%20of%0Aeffective%20parameter-updating%20optimization%20methods.%20We%20present%20Optima%2C%20a%20novel%0Aframework%20that%20addresses%20these%20issues%20by%20significantly%20enhancing%20both%0Acommunication%20efficiency%20and%20task%20effectiveness%20in%20LLM-based%20MAS%20through%20LLM%0Atraining.%20Optima%20employs%20an%20iterative%20generate%2C%20rank%2C%20select%2C%20and%20train%0Aparadigm%20with%20a%20reward%20function%20balancing%20task%20performance%2C%20token%20efficiency%2C%0Aand%20communication%20readability.%20We%20explore%20various%20RL%20algorithms%2C%20including%0ASupervised%20Fine-Tuning%2C%20Direct%20Preference%20Optimization%2C%20and%20their%20hybrid%0Aapproaches%2C%20providing%20insights%20into%20their%20effectiveness-efficiency%20trade-offs.%0AWe%20integrate%20Monte%20Carlo%20Tree%20Search-inspired%20techniques%20for%20DPO%20data%0Ageneration%2C%20treating%20conversation%20turns%20as%20tree%20nodes%20to%20explore%20diverse%0Ainteraction%20paths.%20Evaluated%20on%20common%20multi-agent%20tasks%2C%20including%0Ainformation-asymmetric%20question%20answering%20and%20complex%20reasoning%2C%20Optima%20shows%0Aconsistent%20and%20substantial%20improvements%20over%20single-agent%20baselines%20and%20vanilla%0AMAS%20based%20on%20Llama%203%208B%2C%20achieving%20up%20to%202.8x%20performance%20gain%20with%20less%20than%0A10%5C%25%20tokens%20on%20tasks%20requiring%20heavy%20information%20exchange.%20Moreover%2C%20Optima%27s%0Aefficiency%20gains%20open%20new%20possibilities%20for%20leveraging%20inference-compute%20more%0Aeffectively%2C%20leading%20to%20improved%20inference-time%20scaling%20laws.%20By%20addressing%0Afundamental%20challenges%20in%20LLM-based%20MAS%2C%20Optima%20shows%20the%20potential%20towards%0Ascalable%2C%20efficient%2C%20and%20effective%20MAS%0A%28https%3A//chenweize1998.github.io/optima-project-page%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptima%253A%2520Optimizing%2520Effectiveness%2520and%2520Efficiency%2520for%2520LLM-Based%250A%2520%2520Multi-Agent%2520System%26entry.906535625%3DWeize%2520Chen%2520and%2520Jiarui%2520Yuan%2520and%2520Chen%2520Qian%2520and%2520Cheng%2520Yang%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520multi-agent%2520systems%2520%2528MAS%2529%2520show%2520remarkable%250Apotential%2520in%2520collaborative%2520problem-solving%252C%2520yet%2520they%2520still%2520face%2520critical%250Achallenges%253A%2520low%2520communication%2520efficiency%252C%2520poor%2520scalability%252C%2520and%2520a%2520lack%2520of%250Aeffective%2520parameter-updating%2520optimization%2520methods.%2520We%2520present%2520Optima%252C%2520a%2520novel%250Aframework%2520that%2520addresses%2520these%2520issues%2520by%2520significantly%2520enhancing%2520both%250Acommunication%2520efficiency%2520and%2520task%2520effectiveness%2520in%2520LLM-based%2520MAS%2520through%2520LLM%250Atraining.%2520Optima%2520employs%2520an%2520iterative%2520generate%252C%2520rank%252C%2520select%252C%2520and%2520train%250Aparadigm%2520with%2520a%2520reward%2520function%2520balancing%2520task%2520performance%252C%2520token%2520efficiency%252C%250Aand%2520communication%2520readability.%2520We%2520explore%2520various%2520RL%2520algorithms%252C%2520including%250ASupervised%2520Fine-Tuning%252C%2520Direct%2520Preference%2520Optimization%252C%2520and%2520their%2520hybrid%250Aapproaches%252C%2520providing%2520insights%2520into%2520their%2520effectiveness-efficiency%2520trade-offs.%250AWe%2520integrate%2520Monte%2520Carlo%2520Tree%2520Search-inspired%2520techniques%2520for%2520DPO%2520data%250Ageneration%252C%2520treating%2520conversation%2520turns%2520as%2520tree%2520nodes%2520to%2520explore%2520diverse%250Ainteraction%2520paths.%2520Evaluated%2520on%2520common%2520multi-agent%2520tasks%252C%2520including%250Ainformation-asymmetric%2520question%2520answering%2520and%2520complex%2520reasoning%252C%2520Optima%2520shows%250Aconsistent%2520and%2520substantial%2520improvements%2520over%2520single-agent%2520baselines%2520and%2520vanilla%250AMAS%2520based%2520on%2520Llama%25203%25208B%252C%2520achieving%2520up%2520to%25202.8x%2520performance%2520gain%2520with%2520less%2520than%250A10%255C%2525%2520tokens%2520on%2520tasks%2520requiring%2520heavy%2520information%2520exchange.%2520Moreover%252C%2520Optima%2527s%250Aefficiency%2520gains%2520open%2520new%2520possibilities%2520for%2520leveraging%2520inference-compute%2520more%250Aeffectively%252C%2520leading%2520to%2520improved%2520inference-time%2520scaling%2520laws.%2520By%2520addressing%250Afundamental%2520challenges%2520in%2520LLM-based%2520MAS%252C%2520Optima%2520shows%2520the%2520potential%2520towards%250Ascalable%252C%2520efficient%252C%2520and%2520effective%2520MAS%250A%2528https%253A//chenweize1998.github.io/optima-project-page%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System&entry.906535625=Weize%20Chen%20and%20Jiarui%20Yuan%20and%20Chen%20Qian%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20based%20multi-agent%20systems%20%28MAS%29%20show%20remarkable%0Apotential%20in%20collaborative%20problem-solving%2C%20yet%20they%20still%20face%20critical%0Achallenges%3A%20low%20communication%20efficiency%2C%20poor%20scalability%2C%20and%20a%20lack%20of%0Aeffective%20parameter-updating%20optimization%20methods.%20We%20present%20Optima%2C%20a%20novel%0Aframework%20that%20addresses%20these%20issues%20by%20significantly%20enhancing%20both%0Acommunication%20efficiency%20and%20task%20effectiveness%20in%20LLM-based%20MAS%20through%20LLM%0Atraining.%20Optima%20employs%20an%20iterative%20generate%2C%20rank%2C%20select%2C%20and%20train%0Aparadigm%20with%20a%20reward%20function%20balancing%20task%20performance%2C%20token%20efficiency%2C%0Aand%20communication%20readability.%20We%20explore%20various%20RL%20algorithms%2C%20including%0ASupervised%20Fine-Tuning%2C%20Direct%20Preference%20Optimization%2C%20and%20their%20hybrid%0Aapproaches%2C%20providing%20insights%20into%20their%20effectiveness-efficiency%20trade-offs.%0AWe%20integrate%20Monte%20Carlo%20Tree%20Search-inspired%20techniques%20for%20DPO%20data%0Ageneration%2C%20treating%20conversation%20turns%20as%20tree%20nodes%20to%20explore%20diverse%0Ainteraction%20paths.%20Evaluated%20on%20common%20multi-agent%20tasks%2C%20including%0Ainformation-asymmetric%20question%20answering%20and%20complex%20reasoning%2C%20Optima%20shows%0Aconsistent%20and%20substantial%20improvements%20over%20single-agent%20baselines%20and%20vanilla%0AMAS%20based%20on%20Llama%203%208B%2C%20achieving%20up%20to%202.8x%20performance%20gain%20with%20less%20than%0A10%5C%25%20tokens%20on%20tasks%20requiring%20heavy%20information%20exchange.%20Moreover%2C%20Optima%27s%0Aefficiency%20gains%20open%20new%20possibilities%20for%20leveraging%20inference-compute%20more%0Aeffectively%2C%20leading%20to%20improved%20inference-time%20scaling%20laws.%20By%20addressing%0Afundamental%20challenges%20in%20LLM-based%20MAS%2C%20Optima%20shows%20the%20potential%20towards%0Ascalable%2C%20efficient%2C%20and%20effective%20MAS%0A%28https%3A//chenweize1998.github.io/optima-project-page%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08115v1&entry.124074799=Read"},
{"title": "Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning", "author": "Jun Zhao and Jingqi Tong and Yurong Mou and Ming Zhang and Qi Zhang and Xuanjing Huang", "abstract": "  Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models.\n", "link": "http://arxiv.org/abs/2405.06680v4", "date": "2024-10-10", "relevancy": 2.0911, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Compositional%20Deficiency%20of%20Large%20Language%20Models%20in%0A%20%20Mathematical%20Reasoning&body=Title%3A%20Exploring%20the%20Compositional%20Deficiency%20of%20Large%20Language%20Models%20in%0A%20%20Mathematical%20Reasoning%0AAuthor%3A%20Jun%20Zhao%20and%20Jingqi%20Tong%20and%20Yurong%20Mou%20and%20Ming%20Zhang%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Human%20cognition%20exhibits%20systematic%20compositionality%2C%20the%20algebraic%20ability%0Ato%20generate%20infinite%20novel%20combinations%20from%20finite%20learned%20components%2C%20which%0Ais%20the%20key%20to%20understanding%20and%20reasoning%20about%20complex%20logic.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20compositionality%20of%20large%20language%20models%20%28LLMs%29%20in%0Amathematical%20reasoning.%20Specifically%2C%20we%20construct%20a%20new%20dataset%0A%5Ctextsc%7BMathTrap%7D%20by%20introducing%20carefully%20designed%20logical%20traps%20into%20the%0Aproblem%20descriptions%20of%20MATH%20and%20GSM8K.%20Since%20problems%20with%20logical%20flaws%20are%0Aquite%20rare%20in%20the%20real%20world%2C%20these%20represent%20%22unseen%22%20cases%20to%20LLMs.%20Solving%0Athese%20requires%20the%20models%20to%20systematically%20compose%20%281%29%20the%20mathematical%0Aknowledge%20involved%20in%20the%20original%20problems%20with%20%282%29%20knowledge%20related%20to%20the%0Aintroduced%20traps.%20Our%20experiments%20show%20that%20while%20LLMs%20possess%20both%20components%0Aof%20requisite%20knowledge%2C%20they%20do%20not%20%5Ctextbf%7Bspontaneously%7D%20combine%20them%20to%0Ahandle%20these%20novel%20cases.%20We%20explore%20several%20methods%20to%20mitigate%20this%0Adeficiency%2C%20such%20as%20natural%20language%20prompts%2C%20few-shot%20demonstrations%2C%20and%0Afine-tuning.%20Additionally%2C%20we%20test%20the%20recently%20released%20OpenAI%20o1%20model%20and%0Afind%20that%20human-like%20%60slow%20thinking%27%20helps%20improve%20the%20compositionality%20of%0ALLMs.%20Overall%2C%20systematic%20compositionality%20remains%20an%20open%20challenge%20for%20large%0Alanguage%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06680v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Compositional%2520Deficiency%2520of%2520Large%2520Language%2520Models%2520in%250A%2520%2520Mathematical%2520Reasoning%26entry.906535625%3DJun%2520Zhao%2520and%2520Jingqi%2520Tong%2520and%2520Yurong%2520Mou%2520and%2520Ming%2520Zhang%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520Human%2520cognition%2520exhibits%2520systematic%2520compositionality%252C%2520the%2520algebraic%2520ability%250Ato%2520generate%2520infinite%2520novel%2520combinations%2520from%2520finite%2520learned%2520components%252C%2520which%250Ais%2520the%2520key%2520to%2520understanding%2520and%2520reasoning%2520about%2520complex%2520logic.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520compositionality%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%250Amathematical%2520reasoning.%2520Specifically%252C%2520we%2520construct%2520a%2520new%2520dataset%250A%255Ctextsc%257BMathTrap%257D%2520by%2520introducing%2520carefully%2520designed%2520logical%2520traps%2520into%2520the%250Aproblem%2520descriptions%2520of%2520MATH%2520and%2520GSM8K.%2520Since%2520problems%2520with%2520logical%2520flaws%2520are%250Aquite%2520rare%2520in%2520the%2520real%2520world%252C%2520these%2520represent%2520%2522unseen%2522%2520cases%2520to%2520LLMs.%2520Solving%250Athese%2520requires%2520the%2520models%2520to%2520systematically%2520compose%2520%25281%2529%2520the%2520mathematical%250Aknowledge%2520involved%2520in%2520the%2520original%2520problems%2520with%2520%25282%2529%2520knowledge%2520related%2520to%2520the%250Aintroduced%2520traps.%2520Our%2520experiments%2520show%2520that%2520while%2520LLMs%2520possess%2520both%2520components%250Aof%2520requisite%2520knowledge%252C%2520they%2520do%2520not%2520%255Ctextbf%257Bspontaneously%257D%2520combine%2520them%2520to%250Ahandle%2520these%2520novel%2520cases.%2520We%2520explore%2520several%2520methods%2520to%2520mitigate%2520this%250Adeficiency%252C%2520such%2520as%2520natural%2520language%2520prompts%252C%2520few-shot%2520demonstrations%252C%2520and%250Afine-tuning.%2520Additionally%252C%2520we%2520test%2520the%2520recently%2520released%2520OpenAI%2520o1%2520model%2520and%250Afind%2520that%2520human-like%2520%2560slow%2520thinking%2527%2520helps%2520improve%2520the%2520compositionality%2520of%250ALLMs.%2520Overall%252C%2520systematic%2520compositionality%2520remains%2520an%2520open%2520challenge%2520for%2520large%250Alanguage%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06680v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Compositional%20Deficiency%20of%20Large%20Language%20Models%20in%0A%20%20Mathematical%20Reasoning&entry.906535625=Jun%20Zhao%20and%20Jingqi%20Tong%20and%20Yurong%20Mou%20and%20Ming%20Zhang%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Human%20cognition%20exhibits%20systematic%20compositionality%2C%20the%20algebraic%20ability%0Ato%20generate%20infinite%20novel%20combinations%20from%20finite%20learned%20components%2C%20which%0Ais%20the%20key%20to%20understanding%20and%20reasoning%20about%20complex%20logic.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20compositionality%20of%20large%20language%20models%20%28LLMs%29%20in%0Amathematical%20reasoning.%20Specifically%2C%20we%20construct%20a%20new%20dataset%0A%5Ctextsc%7BMathTrap%7D%20by%20introducing%20carefully%20designed%20logical%20traps%20into%20the%0Aproblem%20descriptions%20of%20MATH%20and%20GSM8K.%20Since%20problems%20with%20logical%20flaws%20are%0Aquite%20rare%20in%20the%20real%20world%2C%20these%20represent%20%22unseen%22%20cases%20to%20LLMs.%20Solving%0Athese%20requires%20the%20models%20to%20systematically%20compose%20%281%29%20the%20mathematical%0Aknowledge%20involved%20in%20the%20original%20problems%20with%20%282%29%20knowledge%20related%20to%20the%0Aintroduced%20traps.%20Our%20experiments%20show%20that%20while%20LLMs%20possess%20both%20components%0Aof%20requisite%20knowledge%2C%20they%20do%20not%20%5Ctextbf%7Bspontaneously%7D%20combine%20them%20to%0Ahandle%20these%20novel%20cases.%20We%20explore%20several%20methods%20to%20mitigate%20this%0Adeficiency%2C%20such%20as%20natural%20language%20prompts%2C%20few-shot%20demonstrations%2C%20and%0Afine-tuning.%20Additionally%2C%20we%20test%20the%20recently%20released%20OpenAI%20o1%20model%20and%0Afind%20that%20human-like%20%60slow%20thinking%27%20helps%20improve%20the%20compositionality%20of%0ALLMs.%20Overall%2C%20systematic%20compositionality%20remains%20an%20open%20challenge%20for%20large%0Alanguage%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06680v4&entry.124074799=Read"},
{"title": "Efficient Dictionary Learning with Switch Sparse Autoencoders", "author": "Anish Mudide and Joshua Engels and Eric J. Michaud and Max Tegmark and Christian Schroeder de Witt", "abstract": "  Sparse autoencoders (SAEs) are a recent technique for decomposing neural\nnetwork activations into human-interpretable features. However, in order for\nSAEs to identify all features represented in frontier models, it will be\nnecessary to scale them up to very high width, posing a computational\nchallenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE\narchitecture aimed at reducing the compute cost of training SAEs. Inspired by\nsparse mixture of experts models, Switch SAEs route activation vectors between\nsmaller \"expert\" SAEs, enabling SAEs to efficiently scale to many more\nfeatures. We present experiments comparing Switch SAEs with other SAE\narchitectures, and find that Switch SAEs deliver a substantial Pareto\nimprovement in the reconstruction vs. sparsity frontier for a given fixed\ntraining compute budget. We also study the geometry of features across experts,\nanalyze features duplicated across experts, and verify that Switch SAE features\nare as interpretable as features found by other SAE architectures.\n", "link": "http://arxiv.org/abs/2410.08201v1", "date": "2024-10-10", "relevancy": 2.0908, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5411}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Dictionary%20Learning%20with%20Switch%20Sparse%20Autoencoders&body=Title%3A%20Efficient%20Dictionary%20Learning%20with%20Switch%20Sparse%20Autoencoders%0AAuthor%3A%20Anish%20Mudide%20and%20Joshua%20Engels%20and%20Eric%20J.%20Michaud%20and%20Max%20Tegmark%20and%20Christian%20Schroeder%20de%20Witt%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20recent%20technique%20for%20decomposing%20neural%0Anetwork%20activations%20into%20human-interpretable%20features.%20However%2C%20in%20order%20for%0ASAEs%20to%20identify%20all%20features%20represented%20in%20frontier%20models%2C%20it%20will%20be%0Anecessary%20to%20scale%20them%20up%20to%20very%20high%20width%2C%20posing%20a%20computational%0Achallenge.%20In%20this%20work%2C%20we%20introduce%20Switch%20Sparse%20Autoencoders%2C%20a%20novel%20SAE%0Aarchitecture%20aimed%20at%20reducing%20the%20compute%20cost%20of%20training%20SAEs.%20Inspired%20by%0Asparse%20mixture%20of%20experts%20models%2C%20Switch%20SAEs%20route%20activation%20vectors%20between%0Asmaller%20%22expert%22%20SAEs%2C%20enabling%20SAEs%20to%20efficiently%20scale%20to%20many%20more%0Afeatures.%20We%20present%20experiments%20comparing%20Switch%20SAEs%20with%20other%20SAE%0Aarchitectures%2C%20and%20find%20that%20Switch%20SAEs%20deliver%20a%20substantial%20Pareto%0Aimprovement%20in%20the%20reconstruction%20vs.%20sparsity%20frontier%20for%20a%20given%20fixed%0Atraining%20compute%20budget.%20We%20also%20study%20the%20geometry%20of%20features%20across%20experts%2C%0Aanalyze%20features%20duplicated%20across%20experts%2C%20and%20verify%20that%20Switch%20SAE%20features%0Aare%20as%20interpretable%20as%20features%20found%20by%20other%20SAE%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Dictionary%2520Learning%2520with%2520Switch%2520Sparse%2520Autoencoders%26entry.906535625%3DAnish%2520Mudide%2520and%2520Joshua%2520Engels%2520and%2520Eric%2520J.%2520Michaud%2520and%2520Max%2520Tegmark%2520and%2520Christian%2520Schroeder%2520de%2520Witt%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520recent%2520technique%2520for%2520decomposing%2520neural%250Anetwork%2520activations%2520into%2520human-interpretable%2520features.%2520However%252C%2520in%2520order%2520for%250ASAEs%2520to%2520identify%2520all%2520features%2520represented%2520in%2520frontier%2520models%252C%2520it%2520will%2520be%250Anecessary%2520to%2520scale%2520them%2520up%2520to%2520very%2520high%2520width%252C%2520posing%2520a%2520computational%250Achallenge.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Switch%2520Sparse%2520Autoencoders%252C%2520a%2520novel%2520SAE%250Aarchitecture%2520aimed%2520at%2520reducing%2520the%2520compute%2520cost%2520of%2520training%2520SAEs.%2520Inspired%2520by%250Asparse%2520mixture%2520of%2520experts%2520models%252C%2520Switch%2520SAEs%2520route%2520activation%2520vectors%2520between%250Asmaller%2520%2522expert%2522%2520SAEs%252C%2520enabling%2520SAEs%2520to%2520efficiently%2520scale%2520to%2520many%2520more%250Afeatures.%2520We%2520present%2520experiments%2520comparing%2520Switch%2520SAEs%2520with%2520other%2520SAE%250Aarchitectures%252C%2520and%2520find%2520that%2520Switch%2520SAEs%2520deliver%2520a%2520substantial%2520Pareto%250Aimprovement%2520in%2520the%2520reconstruction%2520vs.%2520sparsity%2520frontier%2520for%2520a%2520given%2520fixed%250Atraining%2520compute%2520budget.%2520We%2520also%2520study%2520the%2520geometry%2520of%2520features%2520across%2520experts%252C%250Aanalyze%2520features%2520duplicated%2520across%2520experts%252C%2520and%2520verify%2520that%2520Switch%2520SAE%2520features%250Aare%2520as%2520interpretable%2520as%2520features%2520found%2520by%2520other%2520SAE%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Dictionary%20Learning%20with%20Switch%20Sparse%20Autoencoders&entry.906535625=Anish%20Mudide%20and%20Joshua%20Engels%20and%20Eric%20J.%20Michaud%20and%20Max%20Tegmark%20and%20Christian%20Schroeder%20de%20Witt&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20recent%20technique%20for%20decomposing%20neural%0Anetwork%20activations%20into%20human-interpretable%20features.%20However%2C%20in%20order%20for%0ASAEs%20to%20identify%20all%20features%20represented%20in%20frontier%20models%2C%20it%20will%20be%0Anecessary%20to%20scale%20them%20up%20to%20very%20high%20width%2C%20posing%20a%20computational%0Achallenge.%20In%20this%20work%2C%20we%20introduce%20Switch%20Sparse%20Autoencoders%2C%20a%20novel%20SAE%0Aarchitecture%20aimed%20at%20reducing%20the%20compute%20cost%20of%20training%20SAEs.%20Inspired%20by%0Asparse%20mixture%20of%20experts%20models%2C%20Switch%20SAEs%20route%20activation%20vectors%20between%0Asmaller%20%22expert%22%20SAEs%2C%20enabling%20SAEs%20to%20efficiently%20scale%20to%20many%20more%0Afeatures.%20We%20present%20experiments%20comparing%20Switch%20SAEs%20with%20other%20SAE%0Aarchitectures%2C%20and%20find%20that%20Switch%20SAEs%20deliver%20a%20substantial%20Pareto%0Aimprovement%20in%20the%20reconstruction%20vs.%20sparsity%20frontier%20for%20a%20given%20fixed%0Atraining%20compute%20budget.%20We%20also%20study%20the%20geometry%20of%20features%20across%20experts%2C%0Aanalyze%20features%20duplicated%20across%20experts%2C%20and%20verify%20that%20Switch%20SAE%20features%0Aare%20as%20interpretable%20as%20features%20found%20by%20other%20SAE%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08201v1&entry.124074799=Read"},
{"title": "Active Learning to Guide Labeling Efforts for Question Difficulty\n  Estimation", "author": "Arthur Thuy and Ekaterina Loginova and Dries F. Benoit", "abstract": "  In recent years, there has been a surge in research on Question Difficulty\nEstimation (QDE) using natural language processing techniques.\nTransformer-based neural networks achieve state-of-the-art performance,\nprimarily through supervised methods but with an isolated study in unsupervised\nlearning. While supervised methods focus on predictive performance, they\nrequire abundant labeled data. On the other hand, unsupervised methods do not\nrequire labeled data but rely on a different evaluation metric that is also\ncomputationally expensive in practice. This work bridges the research gap by\nexploring active learning for QDE, a supervised human-in-the-loop approach\nstriving to minimize the labeling efforts while matching the performance of\nstate-of-the-art models. The active learning process iteratively trains on a\nlabeled subset, acquiring labels from human experts only for the most\ninformative unlabeled data points. Furthermore, we propose a novel acquisition\nfunction PowerVariance to add the most informative samples to the labeled set,\na regression extension to the PowerBALD function popular in classification. We\nemploy DistilBERT for QDE and identify informative samples by applying Monte\nCarlo dropout to capture epistemic uncertainty in unlabeled samples. The\nexperiments demonstrate that active learning with PowerVariance acquisition\nachieves a performance close to fully supervised models after labeling only 10%\nof the training data. The proposed methodology promotes the responsible use of\neducational resources, makes QDE tools more accessible to course instructors,\nand is promising for other applications such as personalized support systems\nand question-answering tools.\n", "link": "http://arxiv.org/abs/2409.09258v2", "date": "2024-10-10", "relevancy": 2.0864, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.604}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20to%20Guide%20Labeling%20Efforts%20for%20Question%20Difficulty%0A%20%20Estimation&body=Title%3A%20Active%20Learning%20to%20Guide%20Labeling%20Efforts%20for%20Question%20Difficulty%0A%20%20Estimation%0AAuthor%3A%20Arthur%20Thuy%20and%20Ekaterina%20Loginova%20and%20Dries%20F.%20Benoit%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20a%20surge%20in%20research%20on%20Question%20Difficulty%0AEstimation%20%28QDE%29%20using%20natural%20language%20processing%20techniques.%0ATransformer-based%20neural%20networks%20achieve%20state-of-the-art%20performance%2C%0Aprimarily%20through%20supervised%20methods%20but%20with%20an%20isolated%20study%20in%20unsupervised%0Alearning.%20While%20supervised%20methods%20focus%20on%20predictive%20performance%2C%20they%0Arequire%20abundant%20labeled%20data.%20On%20the%20other%20hand%2C%20unsupervised%20methods%20do%20not%0Arequire%20labeled%20data%20but%20rely%20on%20a%20different%20evaluation%20metric%20that%20is%20also%0Acomputationally%20expensive%20in%20practice.%20This%20work%20bridges%20the%20research%20gap%20by%0Aexploring%20active%20learning%20for%20QDE%2C%20a%20supervised%20human-in-the-loop%20approach%0Astriving%20to%20minimize%20the%20labeling%20efforts%20while%20matching%20the%20performance%20of%0Astate-of-the-art%20models.%20The%20active%20learning%20process%20iteratively%20trains%20on%20a%0Alabeled%20subset%2C%20acquiring%20labels%20from%20human%20experts%20only%20for%20the%20most%0Ainformative%20unlabeled%20data%20points.%20Furthermore%2C%20we%20propose%20a%20novel%20acquisition%0Afunction%20PowerVariance%20to%20add%20the%20most%20informative%20samples%20to%20the%20labeled%20set%2C%0Aa%20regression%20extension%20to%20the%20PowerBALD%20function%20popular%20in%20classification.%20We%0Aemploy%20DistilBERT%20for%20QDE%20and%20identify%20informative%20samples%20by%20applying%20Monte%0ACarlo%20dropout%20to%20capture%20epistemic%20uncertainty%20in%20unlabeled%20samples.%20The%0Aexperiments%20demonstrate%20that%20active%20learning%20with%20PowerVariance%20acquisition%0Aachieves%20a%20performance%20close%20to%20fully%20supervised%20models%20after%20labeling%20only%2010%25%0Aof%20the%20training%20data.%20The%20proposed%20methodology%20promotes%20the%20responsible%20use%20of%0Aeducational%20resources%2C%20makes%20QDE%20tools%20more%20accessible%20to%20course%20instructors%2C%0Aand%20is%20promising%20for%20other%20applications%20such%20as%20personalized%20support%20systems%0Aand%20question-answering%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520to%2520Guide%2520Labeling%2520Efforts%2520for%2520Question%2520Difficulty%250A%2520%2520Estimation%26entry.906535625%3DArthur%2520Thuy%2520and%2520Ekaterina%2520Loginova%2520and%2520Dries%2520F.%2520Benoit%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520a%2520surge%2520in%2520research%2520on%2520Question%2520Difficulty%250AEstimation%2520%2528QDE%2529%2520using%2520natural%2520language%2520processing%2520techniques.%250ATransformer-based%2520neural%2520networks%2520achieve%2520state-of-the-art%2520performance%252C%250Aprimarily%2520through%2520supervised%2520methods%2520but%2520with%2520an%2520isolated%2520study%2520in%2520unsupervised%250Alearning.%2520While%2520supervised%2520methods%2520focus%2520on%2520predictive%2520performance%252C%2520they%250Arequire%2520abundant%2520labeled%2520data.%2520On%2520the%2520other%2520hand%252C%2520unsupervised%2520methods%2520do%2520not%250Arequire%2520labeled%2520data%2520but%2520rely%2520on%2520a%2520different%2520evaluation%2520metric%2520that%2520is%2520also%250Acomputationally%2520expensive%2520in%2520practice.%2520This%2520work%2520bridges%2520the%2520research%2520gap%2520by%250Aexploring%2520active%2520learning%2520for%2520QDE%252C%2520a%2520supervised%2520human-in-the-loop%2520approach%250Astriving%2520to%2520minimize%2520the%2520labeling%2520efforts%2520while%2520matching%2520the%2520performance%2520of%250Astate-of-the-art%2520models.%2520The%2520active%2520learning%2520process%2520iteratively%2520trains%2520on%2520a%250Alabeled%2520subset%252C%2520acquiring%2520labels%2520from%2520human%2520experts%2520only%2520for%2520the%2520most%250Ainformative%2520unlabeled%2520data%2520points.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520acquisition%250Afunction%2520PowerVariance%2520to%2520add%2520the%2520most%2520informative%2520samples%2520to%2520the%2520labeled%2520set%252C%250Aa%2520regression%2520extension%2520to%2520the%2520PowerBALD%2520function%2520popular%2520in%2520classification.%2520We%250Aemploy%2520DistilBERT%2520for%2520QDE%2520and%2520identify%2520informative%2520samples%2520by%2520applying%2520Monte%250ACarlo%2520dropout%2520to%2520capture%2520epistemic%2520uncertainty%2520in%2520unlabeled%2520samples.%2520The%250Aexperiments%2520demonstrate%2520that%2520active%2520learning%2520with%2520PowerVariance%2520acquisition%250Aachieves%2520a%2520performance%2520close%2520to%2520fully%2520supervised%2520models%2520after%2520labeling%2520only%252010%2525%250Aof%2520the%2520training%2520data.%2520The%2520proposed%2520methodology%2520promotes%2520the%2520responsible%2520use%2520of%250Aeducational%2520resources%252C%2520makes%2520QDE%2520tools%2520more%2520accessible%2520to%2520course%2520instructors%252C%250Aand%2520is%2520promising%2520for%2520other%2520applications%2520such%2520as%2520personalized%2520support%2520systems%250Aand%2520question-answering%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20to%20Guide%20Labeling%20Efforts%20for%20Question%20Difficulty%0A%20%20Estimation&entry.906535625=Arthur%20Thuy%20and%20Ekaterina%20Loginova%20and%20Dries%20F.%20Benoit&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20surge%20in%20research%20on%20Question%20Difficulty%0AEstimation%20%28QDE%29%20using%20natural%20language%20processing%20techniques.%0ATransformer-based%20neural%20networks%20achieve%20state-of-the-art%20performance%2C%0Aprimarily%20through%20supervised%20methods%20but%20with%20an%20isolated%20study%20in%20unsupervised%0Alearning.%20While%20supervised%20methods%20focus%20on%20predictive%20performance%2C%20they%0Arequire%20abundant%20labeled%20data.%20On%20the%20other%20hand%2C%20unsupervised%20methods%20do%20not%0Arequire%20labeled%20data%20but%20rely%20on%20a%20different%20evaluation%20metric%20that%20is%20also%0Acomputationally%20expensive%20in%20practice.%20This%20work%20bridges%20the%20research%20gap%20by%0Aexploring%20active%20learning%20for%20QDE%2C%20a%20supervised%20human-in-the-loop%20approach%0Astriving%20to%20minimize%20the%20labeling%20efforts%20while%20matching%20the%20performance%20of%0Astate-of-the-art%20models.%20The%20active%20learning%20process%20iteratively%20trains%20on%20a%0Alabeled%20subset%2C%20acquiring%20labels%20from%20human%20experts%20only%20for%20the%20most%0Ainformative%20unlabeled%20data%20points.%20Furthermore%2C%20we%20propose%20a%20novel%20acquisition%0Afunction%20PowerVariance%20to%20add%20the%20most%20informative%20samples%20to%20the%20labeled%20set%2C%0Aa%20regression%20extension%20to%20the%20PowerBALD%20function%20popular%20in%20classification.%20We%0Aemploy%20DistilBERT%20for%20QDE%20and%20identify%20informative%20samples%20by%20applying%20Monte%0ACarlo%20dropout%20to%20capture%20epistemic%20uncertainty%20in%20unlabeled%20samples.%20The%0Aexperiments%20demonstrate%20that%20active%20learning%20with%20PowerVariance%20acquisition%0Aachieves%20a%20performance%20close%20to%20fully%20supervised%20models%20after%20labeling%20only%2010%25%0Aof%20the%20training%20data.%20The%20proposed%20methodology%20promotes%20the%20responsible%20use%20of%0Aeducational%20resources%2C%20makes%20QDE%20tools%20more%20accessible%20to%20course%20instructors%2C%0Aand%20is%20promising%20for%20other%20applications%20such%20as%20personalized%20support%20systems%0Aand%20question-answering%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09258v2&entry.124074799=Read"},
{"title": "Unpacking Failure Modes of Generative Policies: Runtime Monitoring of\n  Consistency and Progress", "author": "Christopher Agia and Rohan Sinha and Jingyun Yang and Zi-ang Cao and Rika Antonova and Marco Pavone and Jeannette Bohg", "abstract": "  Robot behavior policies trained via imitation learning are prone to failure\nunder conditions that deviate from their training data. Thus, algorithms that\nmonitor learned policies at test time and provide early warnings of failure are\nnecessary to facilitate scalable deployment. We propose Sentinel, a runtime\nmonitoring framework that splits the detection of failures into two\ncomplementary categories: 1) Erratic failures, which we detect using\nstatistical measures of temporal action consistency, and 2) task progression\nfailures, where we use Vision Language Models (VLMs) to detect when the policy\nconfidently and consistently takes actions that do not solve the task. Our\napproach has two key strengths. First, because learned policies exhibit diverse\nfailure modes, combining complementary detectors leads to significantly higher\naccuracy at failure detection. Second, using a statistical temporal action\nconsistency measure ensures that we quickly detect when multimodal, generative\npolicies exhibit erratic behavior at negligible computational cost. In\ncontrast, we only use VLMs to detect failure modes that are less\ntime-sensitive. We demonstrate our approach in the context of diffusion\npolicies trained on robotic mobile manipulation domains in both simulation and\nthe real world. By unifying temporal consistency detection and VLM runtime\nmonitoring, Sentinel detects 18% more failures than using either of the two\ndetectors alone and significantly outperforms baselines, thus highlighting the\nimportance of assigning specialized detectors to complementary categories of\nfailure. Qualitative results are made available at\nhttps://sites.google.com/stanford.edu/sentinel.\n", "link": "http://arxiv.org/abs/2410.04640v2", "date": "2024-10-10", "relevancy": 2.0787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5711}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5107}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unpacking%20Failure%20Modes%20of%20Generative%20Policies%3A%20Runtime%20Monitoring%20of%0A%20%20Consistency%20and%20Progress&body=Title%3A%20Unpacking%20Failure%20Modes%20of%20Generative%20Policies%3A%20Runtime%20Monitoring%20of%0A%20%20Consistency%20and%20Progress%0AAuthor%3A%20Christopher%20Agia%20and%20Rohan%20Sinha%20and%20Jingyun%20Yang%20and%20Zi-ang%20Cao%20and%20Rika%20Antonova%20and%20Marco%20Pavone%20and%20Jeannette%20Bohg%0AAbstract%3A%20%20%20Robot%20behavior%20policies%20trained%20via%20imitation%20learning%20are%20prone%20to%20failure%0Aunder%20conditions%20that%20deviate%20from%20their%20training%20data.%20Thus%2C%20algorithms%20that%0Amonitor%20learned%20policies%20at%20test%20time%20and%20provide%20early%20warnings%20of%20failure%20are%0Anecessary%20to%20facilitate%20scalable%20deployment.%20We%20propose%20Sentinel%2C%20a%20runtime%0Amonitoring%20framework%20that%20splits%20the%20detection%20of%20failures%20into%20two%0Acomplementary%20categories%3A%201%29%20Erratic%20failures%2C%20which%20we%20detect%20using%0Astatistical%20measures%20of%20temporal%20action%20consistency%2C%20and%202%29%20task%20progression%0Afailures%2C%20where%20we%20use%20Vision%20Language%20Models%20%28VLMs%29%20to%20detect%20when%20the%20policy%0Aconfidently%20and%20consistently%20takes%20actions%20that%20do%20not%20solve%20the%20task.%20Our%0Aapproach%20has%20two%20key%20strengths.%20First%2C%20because%20learned%20policies%20exhibit%20diverse%0Afailure%20modes%2C%20combining%20complementary%20detectors%20leads%20to%20significantly%20higher%0Aaccuracy%20at%20failure%20detection.%20Second%2C%20using%20a%20statistical%20temporal%20action%0Aconsistency%20measure%20ensures%20that%20we%20quickly%20detect%20when%20multimodal%2C%20generative%0Apolicies%20exhibit%20erratic%20behavior%20at%20negligible%20computational%20cost.%20In%0Acontrast%2C%20we%20only%20use%20VLMs%20to%20detect%20failure%20modes%20that%20are%20less%0Atime-sensitive.%20We%20demonstrate%20our%20approach%20in%20the%20context%20of%20diffusion%0Apolicies%20trained%20on%20robotic%20mobile%20manipulation%20domains%20in%20both%20simulation%20and%0Athe%20real%20world.%20By%20unifying%20temporal%20consistency%20detection%20and%20VLM%20runtime%0Amonitoring%2C%20Sentinel%20detects%2018%25%20more%20failures%20than%20using%20either%20of%20the%20two%0Adetectors%20alone%20and%20significantly%20outperforms%20baselines%2C%20thus%20highlighting%20the%0Aimportance%20of%20assigning%20specialized%20detectors%20to%20complementary%20categories%20of%0Afailure.%20Qualitative%20results%20are%20made%20available%20at%0Ahttps%3A//sites.google.com/stanford.edu/sentinel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnpacking%2520Failure%2520Modes%2520of%2520Generative%2520Policies%253A%2520Runtime%2520Monitoring%2520of%250A%2520%2520Consistency%2520and%2520Progress%26entry.906535625%3DChristopher%2520Agia%2520and%2520Rohan%2520Sinha%2520and%2520Jingyun%2520Yang%2520and%2520Zi-ang%2520Cao%2520and%2520Rika%2520Antonova%2520and%2520Marco%2520Pavone%2520and%2520Jeannette%2520Bohg%26entry.1292438233%3D%2520%2520Robot%2520behavior%2520policies%2520trained%2520via%2520imitation%2520learning%2520are%2520prone%2520to%2520failure%250Aunder%2520conditions%2520that%2520deviate%2520from%2520their%2520training%2520data.%2520Thus%252C%2520algorithms%2520that%250Amonitor%2520learned%2520policies%2520at%2520test%2520time%2520and%2520provide%2520early%2520warnings%2520of%2520failure%2520are%250Anecessary%2520to%2520facilitate%2520scalable%2520deployment.%2520We%2520propose%2520Sentinel%252C%2520a%2520runtime%250Amonitoring%2520framework%2520that%2520splits%2520the%2520detection%2520of%2520failures%2520into%2520two%250Acomplementary%2520categories%253A%25201%2529%2520Erratic%2520failures%252C%2520which%2520we%2520detect%2520using%250Astatistical%2520measures%2520of%2520temporal%2520action%2520consistency%252C%2520and%25202%2529%2520task%2520progression%250Afailures%252C%2520where%2520we%2520use%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520to%2520detect%2520when%2520the%2520policy%250Aconfidently%2520and%2520consistently%2520takes%2520actions%2520that%2520do%2520not%2520solve%2520the%2520task.%2520Our%250Aapproach%2520has%2520two%2520key%2520strengths.%2520First%252C%2520because%2520learned%2520policies%2520exhibit%2520diverse%250Afailure%2520modes%252C%2520combining%2520complementary%2520detectors%2520leads%2520to%2520significantly%2520higher%250Aaccuracy%2520at%2520failure%2520detection.%2520Second%252C%2520using%2520a%2520statistical%2520temporal%2520action%250Aconsistency%2520measure%2520ensures%2520that%2520we%2520quickly%2520detect%2520when%2520multimodal%252C%2520generative%250Apolicies%2520exhibit%2520erratic%2520behavior%2520at%2520negligible%2520computational%2520cost.%2520In%250Acontrast%252C%2520we%2520only%2520use%2520VLMs%2520to%2520detect%2520failure%2520modes%2520that%2520are%2520less%250Atime-sensitive.%2520We%2520demonstrate%2520our%2520approach%2520in%2520the%2520context%2520of%2520diffusion%250Apolicies%2520trained%2520on%2520robotic%2520mobile%2520manipulation%2520domains%2520in%2520both%2520simulation%2520and%250Athe%2520real%2520world.%2520By%2520unifying%2520temporal%2520consistency%2520detection%2520and%2520VLM%2520runtime%250Amonitoring%252C%2520Sentinel%2520detects%252018%2525%2520more%2520failures%2520than%2520using%2520either%2520of%2520the%2520two%250Adetectors%2520alone%2520and%2520significantly%2520outperforms%2520baselines%252C%2520thus%2520highlighting%2520the%250Aimportance%2520of%2520assigning%2520specialized%2520detectors%2520to%2520complementary%2520categories%2520of%250Afailure.%2520Qualitative%2520results%2520are%2520made%2520available%2520at%250Ahttps%253A//sites.google.com/stanford.edu/sentinel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpacking%20Failure%20Modes%20of%20Generative%20Policies%3A%20Runtime%20Monitoring%20of%0A%20%20Consistency%20and%20Progress&entry.906535625=Christopher%20Agia%20and%20Rohan%20Sinha%20and%20Jingyun%20Yang%20and%20Zi-ang%20Cao%20and%20Rika%20Antonova%20and%20Marco%20Pavone%20and%20Jeannette%20Bohg&entry.1292438233=%20%20Robot%20behavior%20policies%20trained%20via%20imitation%20learning%20are%20prone%20to%20failure%0Aunder%20conditions%20that%20deviate%20from%20their%20training%20data.%20Thus%2C%20algorithms%20that%0Amonitor%20learned%20policies%20at%20test%20time%20and%20provide%20early%20warnings%20of%20failure%20are%0Anecessary%20to%20facilitate%20scalable%20deployment.%20We%20propose%20Sentinel%2C%20a%20runtime%0Amonitoring%20framework%20that%20splits%20the%20detection%20of%20failures%20into%20two%0Acomplementary%20categories%3A%201%29%20Erratic%20failures%2C%20which%20we%20detect%20using%0Astatistical%20measures%20of%20temporal%20action%20consistency%2C%20and%202%29%20task%20progression%0Afailures%2C%20where%20we%20use%20Vision%20Language%20Models%20%28VLMs%29%20to%20detect%20when%20the%20policy%0Aconfidently%20and%20consistently%20takes%20actions%20that%20do%20not%20solve%20the%20task.%20Our%0Aapproach%20has%20two%20key%20strengths.%20First%2C%20because%20learned%20policies%20exhibit%20diverse%0Afailure%20modes%2C%20combining%20complementary%20detectors%20leads%20to%20significantly%20higher%0Aaccuracy%20at%20failure%20detection.%20Second%2C%20using%20a%20statistical%20temporal%20action%0Aconsistency%20measure%20ensures%20that%20we%20quickly%20detect%20when%20multimodal%2C%20generative%0Apolicies%20exhibit%20erratic%20behavior%20at%20negligible%20computational%20cost.%20In%0Acontrast%2C%20we%20only%20use%20VLMs%20to%20detect%20failure%20modes%20that%20are%20less%0Atime-sensitive.%20We%20demonstrate%20our%20approach%20in%20the%20context%20of%20diffusion%0Apolicies%20trained%20on%20robotic%20mobile%20manipulation%20domains%20in%20both%20simulation%20and%0Athe%20real%20world.%20By%20unifying%20temporal%20consistency%20detection%20and%20VLM%20runtime%0Amonitoring%2C%20Sentinel%20detects%2018%25%20more%20failures%20than%20using%20either%20of%20the%20two%0Adetectors%20alone%20and%20significantly%20outperforms%20baselines%2C%20thus%20highlighting%20the%0Aimportance%20of%20assigning%20specialized%20detectors%20to%20complementary%20categories%20of%0Afailure.%20Qualitative%20results%20are%20made%20available%20at%0Ahttps%3A//sites.google.com/stanford.edu/sentinel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04640v2&entry.124074799=Read"},
{"title": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in\n  Human Activity Recognition", "author": "Mengxi Liu and Zimin Zhao and Daniel Gei\u00dfler and Bo Zhou and Sungho Suh and Paul Lukowicz", "abstract": "  Recent advancements in Artificial Neural Networks have significantly improved\nhuman activity recognition using multiple time-series sensors. While employing\nnumerous sensors with high-frequency sampling rates usually improves the\nresults, it often leads to data inefficiency and unnecessary expansion of the\nANN, posing a challenge for their practical deployment on edge devices.\nAddressing these issues, our work introduces a pragmatic framework for\ndata-efficient utilization in HAR tasks, considering the optimization of both\nsensor modalities and sampling rate simultaneously. Central to our approach are\nthe designed trainable parameters, termed 'Weight Scores,' which assess the\nsignificance of each sensor modality and sampling rate during the training\nphase. These scores guide the sensor modalities and sampling rate selection.\nThe pruning method allows users to make a trade-off between computational\nbudgets and performance by selecting the sensor modalities and sampling rates\naccording to the weight score ranking. We tested our framework's effectiveness\nin optimizing sensor modality and sampling rate selection using three public\nHAR benchmark datasets. The results show that the sensor and sampling rate\ncombination selected via CoSS achieves similar classification performance to\nconfigurations using the highest sampling rate with all sensors but at a\nreduced hardware cost.\n", "link": "http://arxiv.org/abs/2401.05426v2", "date": "2024-10-10", "relevancy": 2.0786, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5212}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoSS%3A%20Co-optimizing%20Sensor%20and%20Sampling%20Rate%20for%20Data-Efficient%20AI%20in%0A%20%20Human%20Activity%20Recognition&body=Title%3A%20CoSS%3A%20Co-optimizing%20Sensor%20and%20Sampling%20Rate%20for%20Data-Efficient%20AI%20in%0A%20%20Human%20Activity%20Recognition%0AAuthor%3A%20Mengxi%20Liu%20and%20Zimin%20Zhao%20and%20Daniel%20Gei%C3%9Fler%20and%20Bo%20Zhou%20and%20Sungho%20Suh%20and%20Paul%20Lukowicz%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Artificial%20Neural%20Networks%20have%20significantly%20improved%0Ahuman%20activity%20recognition%20using%20multiple%20time-series%20sensors.%20While%20employing%0Anumerous%20sensors%20with%20high-frequency%20sampling%20rates%20usually%20improves%20the%0Aresults%2C%20it%20often%20leads%20to%20data%20inefficiency%20and%20unnecessary%20expansion%20of%20the%0AANN%2C%20posing%20a%20challenge%20for%20their%20practical%20deployment%20on%20edge%20devices.%0AAddressing%20these%20issues%2C%20our%20work%20introduces%20a%20pragmatic%20framework%20for%0Adata-efficient%20utilization%20in%20HAR%20tasks%2C%20considering%20the%20optimization%20of%20both%0Asensor%20modalities%20and%20sampling%20rate%20simultaneously.%20Central%20to%20our%20approach%20are%0Athe%20designed%20trainable%20parameters%2C%20termed%20%27Weight%20Scores%2C%27%20which%20assess%20the%0Asignificance%20of%20each%20sensor%20modality%20and%20sampling%20rate%20during%20the%20training%0Aphase.%20These%20scores%20guide%20the%20sensor%20modalities%20and%20sampling%20rate%20selection.%0AThe%20pruning%20method%20allows%20users%20to%20make%20a%20trade-off%20between%20computational%0Abudgets%20and%20performance%20by%20selecting%20the%20sensor%20modalities%20and%20sampling%20rates%0Aaccording%20to%20the%20weight%20score%20ranking.%20We%20tested%20our%20framework%27s%20effectiveness%0Ain%20optimizing%20sensor%20modality%20and%20sampling%20rate%20selection%20using%20three%20public%0AHAR%20benchmark%20datasets.%20The%20results%20show%20that%20the%20sensor%20and%20sampling%20rate%0Acombination%20selected%20via%20CoSS%20achieves%20similar%20classification%20performance%20to%0Aconfigurations%20using%20the%20highest%20sampling%20rate%20with%20all%20sensors%20but%20at%20a%0Areduced%20hardware%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoSS%253A%2520Co-optimizing%2520Sensor%2520and%2520Sampling%2520Rate%2520for%2520Data-Efficient%2520AI%2520in%250A%2520%2520Human%2520Activity%2520Recognition%26entry.906535625%3DMengxi%2520Liu%2520and%2520Zimin%2520Zhao%2520and%2520Daniel%2520Gei%25C3%259Fler%2520and%2520Bo%2520Zhou%2520and%2520Sungho%2520Suh%2520and%2520Paul%2520Lukowicz%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Artificial%2520Neural%2520Networks%2520have%2520significantly%2520improved%250Ahuman%2520activity%2520recognition%2520using%2520multiple%2520time-series%2520sensors.%2520While%2520employing%250Anumerous%2520sensors%2520with%2520high-frequency%2520sampling%2520rates%2520usually%2520improves%2520the%250Aresults%252C%2520it%2520often%2520leads%2520to%2520data%2520inefficiency%2520and%2520unnecessary%2520expansion%2520of%2520the%250AANN%252C%2520posing%2520a%2520challenge%2520for%2520their%2520practical%2520deployment%2520on%2520edge%2520devices.%250AAddressing%2520these%2520issues%252C%2520our%2520work%2520introduces%2520a%2520pragmatic%2520framework%2520for%250Adata-efficient%2520utilization%2520in%2520HAR%2520tasks%252C%2520considering%2520the%2520optimization%2520of%2520both%250Asensor%2520modalities%2520and%2520sampling%2520rate%2520simultaneously.%2520Central%2520to%2520our%2520approach%2520are%250Athe%2520designed%2520trainable%2520parameters%252C%2520termed%2520%2527Weight%2520Scores%252C%2527%2520which%2520assess%2520the%250Asignificance%2520of%2520each%2520sensor%2520modality%2520and%2520sampling%2520rate%2520during%2520the%2520training%250Aphase.%2520These%2520scores%2520guide%2520the%2520sensor%2520modalities%2520and%2520sampling%2520rate%2520selection.%250AThe%2520pruning%2520method%2520allows%2520users%2520to%2520make%2520a%2520trade-off%2520between%2520computational%250Abudgets%2520and%2520performance%2520by%2520selecting%2520the%2520sensor%2520modalities%2520and%2520sampling%2520rates%250Aaccording%2520to%2520the%2520weight%2520score%2520ranking.%2520We%2520tested%2520our%2520framework%2527s%2520effectiveness%250Ain%2520optimizing%2520sensor%2520modality%2520and%2520sampling%2520rate%2520selection%2520using%2520three%2520public%250AHAR%2520benchmark%2520datasets.%2520The%2520results%2520show%2520that%2520the%2520sensor%2520and%2520sampling%2520rate%250Acombination%2520selected%2520via%2520CoSS%2520achieves%2520similar%2520classification%2520performance%2520to%250Aconfigurations%2520using%2520the%2520highest%2520sampling%2520rate%2520with%2520all%2520sensors%2520but%2520at%2520a%250Areduced%2520hardware%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoSS%3A%20Co-optimizing%20Sensor%20and%20Sampling%20Rate%20for%20Data-Efficient%20AI%20in%0A%20%20Human%20Activity%20Recognition&entry.906535625=Mengxi%20Liu%20and%20Zimin%20Zhao%20and%20Daniel%20Gei%C3%9Fler%20and%20Bo%20Zhou%20and%20Sungho%20Suh%20and%20Paul%20Lukowicz&entry.1292438233=%20%20Recent%20advancements%20in%20Artificial%20Neural%20Networks%20have%20significantly%20improved%0Ahuman%20activity%20recognition%20using%20multiple%20time-series%20sensors.%20While%20employing%0Anumerous%20sensors%20with%20high-frequency%20sampling%20rates%20usually%20improves%20the%0Aresults%2C%20it%20often%20leads%20to%20data%20inefficiency%20and%20unnecessary%20expansion%20of%20the%0AANN%2C%20posing%20a%20challenge%20for%20their%20practical%20deployment%20on%20edge%20devices.%0AAddressing%20these%20issues%2C%20our%20work%20introduces%20a%20pragmatic%20framework%20for%0Adata-efficient%20utilization%20in%20HAR%20tasks%2C%20considering%20the%20optimization%20of%20both%0Asensor%20modalities%20and%20sampling%20rate%20simultaneously.%20Central%20to%20our%20approach%20are%0Athe%20designed%20trainable%20parameters%2C%20termed%20%27Weight%20Scores%2C%27%20which%20assess%20the%0Asignificance%20of%20each%20sensor%20modality%20and%20sampling%20rate%20during%20the%20training%0Aphase.%20These%20scores%20guide%20the%20sensor%20modalities%20and%20sampling%20rate%20selection.%0AThe%20pruning%20method%20allows%20users%20to%20make%20a%20trade-off%20between%20computational%0Abudgets%20and%20performance%20by%20selecting%20the%20sensor%20modalities%20and%20sampling%20rates%0Aaccording%20to%20the%20weight%20score%20ranking.%20We%20tested%20our%20framework%27s%20effectiveness%0Ain%20optimizing%20sensor%20modality%20and%20sampling%20rate%20selection%20using%20three%20public%0AHAR%20benchmark%20datasets.%20The%20results%20show%20that%20the%20sensor%20and%20sampling%20rate%0Acombination%20selected%20via%20CoSS%20achieves%20similar%20classification%20performance%20to%0Aconfigurations%20using%20the%20highest%20sampling%20rate%20with%20all%20sensors%20but%20at%20a%0Areduced%20hardware%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05426v2&entry.124074799=Read"},
{"title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs", "author": "Shenao Zhang and Zhihan Liu and Boyi Liu and Yufeng Zhang and Yingxiang Yang and Yongfei Liu and Liyu Chen and Tao Sun and Zhaoran Wang", "abstract": "  Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.\n", "link": "http://arxiv.org/abs/2410.08067v1", "date": "2024-10-10", "relevancy": 2.0777, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs&body=Title%3A%20Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs%0AAuthor%3A%20Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Boyi%20Liu%20and%20Yufeng%20Zhang%20and%20Yingxiang%20Yang%20and%20Yongfei%20Liu%20and%20Liyu%20Chen%20and%20Tao%20Sun%20and%20Zhaoran%20Wang%0AAbstract%3A%20%20%20Preference%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20ability%20to%20adhere%20to%20human%20instructions%20and%20intentions.%20However%2C%0Aexisting%20direct%20alignment%20algorithms%20primarily%20focus%20on%20relative%20preferences%0Aand%20often%20overlook%20the%20qualitative%20aspects%20of%20responses.%20Striving%20to%20maximize%0Athe%20implicit%20reward%20gap%20between%20the%20chosen%20and%20the%20slightly%20inferior%20rejected%0Aresponses%20can%20cause%20overfitting%20and%20unnecessary%20unlearning%20of%20the%20high-quality%0Arejected%20responses.%20The%20unawareness%20of%20the%20reward%20scores%20also%20drives%20the%20LLM%20to%0Aindiscriminately%20favor%20the%20low-quality%20chosen%20responses%20and%20fail%20to%20generalize%0Ato%20responses%20with%20the%20highest%20rewards%2C%20which%20are%20sparse%20in%20data.%20To%20overcome%0Athese%20shortcomings%2C%20our%20study%20introduces%20reward-conditioned%20LLM%20policies%20that%0Adiscern%20and%20learn%20from%20the%20entire%20spectrum%20of%20response%20quality%20within%20the%0Adataset%2C%20helping%20extrapolate%20to%20more%20optimal%20regions.%20We%20propose%20an%20effective%0Ayet%20simple%20data%20relabeling%20method%20that%20conditions%20the%20preference%20pairs%20on%0Aquality%20scores%20to%20construct%20a%20reward-augmented%20dataset.%20This%20dataset%20is%20easily%0Aintegrated%20with%20existing%20direct%20alignment%20algorithms%20and%20is%20applicable%20to%20any%0Apreference%20dataset.%20The%20experimental%20results%20across%20instruction-following%0Abenchmarks%20including%20AlpacaEval%2C%20MT-Bench%2C%20and%20Arena-Hard-Auto%20demonstrate%20that%0Aour%20approach%20consistently%20boosts%20the%20performance%20of%20DPO%20by%20a%20considerable%0Amargin%20across%20diverse%20models.%20Additionally%2C%20our%20method%20improves%20the%20average%0Aaccuracy%20on%20various%20academic%20benchmarks.%20When%20applying%20our%20method%20to%20on-policy%0Adata%2C%20the%20resulting%20DPO%20model%20achieves%20SOTA%20results%20on%20AlpacaEval.%20Through%0Aablation%20studies%2C%20we%20demonstrate%20that%20our%20method%20not%20only%20maximizes%20the%20utility%0Aof%20preference%20data%20but%20also%20mitigates%20the%20issue%20of%20unlearning%2C%20demonstrating%0Aits%20broad%20effectiveness%20beyond%20mere%20dataset%20expansion.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shenao-zhang/reward-augmented-preference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Augmented%2520Data%2520Enhances%2520Direct%2520Preference%2520Alignment%2520of%2520LLMs%26entry.906535625%3DShenao%2520Zhang%2520and%2520Zhihan%2520Liu%2520and%2520Boyi%2520Liu%2520and%2520Yufeng%2520Zhang%2520and%2520Yingxiang%2520Yang%2520and%2520Yongfei%2520Liu%2520and%2520Liyu%2520Chen%2520and%2520Tao%2520Sun%2520and%2520Zhaoran%2520Wang%26entry.1292438233%3D%2520%2520Preference%2520alignment%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%250Aimproved%2520their%2520ability%2520to%2520adhere%2520to%2520human%2520instructions%2520and%2520intentions.%2520However%252C%250Aexisting%2520direct%2520alignment%2520algorithms%2520primarily%2520focus%2520on%2520relative%2520preferences%250Aand%2520often%2520overlook%2520the%2520qualitative%2520aspects%2520of%2520responses.%2520Striving%2520to%2520maximize%250Athe%2520implicit%2520reward%2520gap%2520between%2520the%2520chosen%2520and%2520the%2520slightly%2520inferior%2520rejected%250Aresponses%2520can%2520cause%2520overfitting%2520and%2520unnecessary%2520unlearning%2520of%2520the%2520high-quality%250Arejected%2520responses.%2520The%2520unawareness%2520of%2520the%2520reward%2520scores%2520also%2520drives%2520the%2520LLM%2520to%250Aindiscriminately%2520favor%2520the%2520low-quality%2520chosen%2520responses%2520and%2520fail%2520to%2520generalize%250Ato%2520responses%2520with%2520the%2520highest%2520rewards%252C%2520which%2520are%2520sparse%2520in%2520data.%2520To%2520overcome%250Athese%2520shortcomings%252C%2520our%2520study%2520introduces%2520reward-conditioned%2520LLM%2520policies%2520that%250Adiscern%2520and%2520learn%2520from%2520the%2520entire%2520spectrum%2520of%2520response%2520quality%2520within%2520the%250Adataset%252C%2520helping%2520extrapolate%2520to%2520more%2520optimal%2520regions.%2520We%2520propose%2520an%2520effective%250Ayet%2520simple%2520data%2520relabeling%2520method%2520that%2520conditions%2520the%2520preference%2520pairs%2520on%250Aquality%2520scores%2520to%2520construct%2520a%2520reward-augmented%2520dataset.%2520This%2520dataset%2520is%2520easily%250Aintegrated%2520with%2520existing%2520direct%2520alignment%2520algorithms%2520and%2520is%2520applicable%2520to%2520any%250Apreference%2520dataset.%2520The%2520experimental%2520results%2520across%2520instruction-following%250Abenchmarks%2520including%2520AlpacaEval%252C%2520MT-Bench%252C%2520and%2520Arena-Hard-Auto%2520demonstrate%2520that%250Aour%2520approach%2520consistently%2520boosts%2520the%2520performance%2520of%2520DPO%2520by%2520a%2520considerable%250Amargin%2520across%2520diverse%2520models.%2520Additionally%252C%2520our%2520method%2520improves%2520the%2520average%250Aaccuracy%2520on%2520various%2520academic%2520benchmarks.%2520When%2520applying%2520our%2520method%2520to%2520on-policy%250Adata%252C%2520the%2520resulting%2520DPO%2520model%2520achieves%2520SOTA%2520results%2520on%2520AlpacaEval.%2520Through%250Aablation%2520studies%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520not%2520only%2520maximizes%2520the%2520utility%250Aof%2520preference%2520data%2520but%2520also%2520mitigates%2520the%2520issue%2520of%2520unlearning%252C%2520demonstrating%250Aits%2520broad%2520effectiveness%2520beyond%2520mere%2520dataset%2520expansion.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shenao-zhang/reward-augmented-preference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs&entry.906535625=Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Boyi%20Liu%20and%20Yufeng%20Zhang%20and%20Yingxiang%20Yang%20and%20Yongfei%20Liu%20and%20Liyu%20Chen%20and%20Tao%20Sun%20and%20Zhaoran%20Wang&entry.1292438233=%20%20Preference%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20ability%20to%20adhere%20to%20human%20instructions%20and%20intentions.%20However%2C%0Aexisting%20direct%20alignment%20algorithms%20primarily%20focus%20on%20relative%20preferences%0Aand%20often%20overlook%20the%20qualitative%20aspects%20of%20responses.%20Striving%20to%20maximize%0Athe%20implicit%20reward%20gap%20between%20the%20chosen%20and%20the%20slightly%20inferior%20rejected%0Aresponses%20can%20cause%20overfitting%20and%20unnecessary%20unlearning%20of%20the%20high-quality%0Arejected%20responses.%20The%20unawareness%20of%20the%20reward%20scores%20also%20drives%20the%20LLM%20to%0Aindiscriminately%20favor%20the%20low-quality%20chosen%20responses%20and%20fail%20to%20generalize%0Ato%20responses%20with%20the%20highest%20rewards%2C%20which%20are%20sparse%20in%20data.%20To%20overcome%0Athese%20shortcomings%2C%20our%20study%20introduces%20reward-conditioned%20LLM%20policies%20that%0Adiscern%20and%20learn%20from%20the%20entire%20spectrum%20of%20response%20quality%20within%20the%0Adataset%2C%20helping%20extrapolate%20to%20more%20optimal%20regions.%20We%20propose%20an%20effective%0Ayet%20simple%20data%20relabeling%20method%20that%20conditions%20the%20preference%20pairs%20on%0Aquality%20scores%20to%20construct%20a%20reward-augmented%20dataset.%20This%20dataset%20is%20easily%0Aintegrated%20with%20existing%20direct%20alignment%20algorithms%20and%20is%20applicable%20to%20any%0Apreference%20dataset.%20The%20experimental%20results%20across%20instruction-following%0Abenchmarks%20including%20AlpacaEval%2C%20MT-Bench%2C%20and%20Arena-Hard-Auto%20demonstrate%20that%0Aour%20approach%20consistently%20boosts%20the%20performance%20of%20DPO%20by%20a%20considerable%0Amargin%20across%20diverse%20models.%20Additionally%2C%20our%20method%20improves%20the%20average%0Aaccuracy%20on%20various%20academic%20benchmarks.%20When%20applying%20our%20method%20to%20on-policy%0Adata%2C%20the%20resulting%20DPO%20model%20achieves%20SOTA%20results%20on%20AlpacaEval.%20Through%0Aablation%20studies%2C%20we%20demonstrate%20that%20our%20method%20not%20only%20maximizes%20the%20utility%0Aof%20preference%20data%20but%20also%20mitigates%20the%20issue%20of%20unlearning%2C%20demonstrating%0Aits%20broad%20effectiveness%20beyond%20mere%20dataset%20expansion.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shenao-zhang/reward-augmented-preference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08067v1&entry.124074799=Read"},
{"title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering", "author": "Yuan Sui and Yufei He and Nian Liu and Xiaoxin He and Kun Wang and Bryan Hooi", "abstract": "  Large language models are often challenged by generating erroneous or\n`hallucinated' responses, especially in complex reasoning tasks. To mitigate\nthis, we propose a retrieval augmented reasoning method, FiDeLiS, which\nenhances knowledge graph question answering by anchoring responses to\nstructured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced\nretrieval mechanism that fetches relevant entities and relations from a\nvector-based index of KGs to ensure high-recall retrieval. Once these entities\nand relations are retrieved, our method constructs candidate reasoning paths\nwhich are then refined using a stepwise beam search. This ensures that all the\npaths we create can be confidently linked back to KGs, ensuring they are\naccurate and reliable. A distinctive feature of our approach is its blend of\nnatural language planning with beam search to optimize the selection of\nreasoning paths. Moreover, we redesign the way reasoning paths are scored by\ntransforming this process into a deductive reasoning task, allowing the LLM to\nassess the validity of the paths through deductive reasoning rather than\ntraditional logit-based scoring. This helps avoid misleading reasoning chains\nand reduces unnecessary computational demand. Extensive experiments demonstrate\nthat our method, even as a training-free method which has lower computational\ncosts and superior generality, outperforms established strong baselines across\nthree datasets.\n", "link": "http://arxiv.org/abs/2405.13873v2", "date": "2024-10-10", "relevancy": 2.0578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FiDeLiS%3A%20Faithful%20Reasoning%20in%20Large%20Language%20Model%20for%20Knowledge%20Graph%0A%20%20Question%20Answering&body=Title%3A%20FiDeLiS%3A%20Faithful%20Reasoning%20in%20Large%20Language%20Model%20for%20Knowledge%20Graph%0A%20%20Question%20Answering%0AAuthor%3A%20Yuan%20Sui%20and%20Yufei%20He%20and%20Nian%20Liu%20and%20Xiaoxin%20He%20and%20Kun%20Wang%20and%20Bryan%20Hooi%0AAbstract%3A%20%20%20Large%20language%20models%20are%20often%20challenged%20by%20generating%20erroneous%20or%0A%60hallucinated%27%20responses%2C%20especially%20in%20complex%20reasoning%20tasks.%20To%20mitigate%0Athis%2C%20we%20propose%20a%20retrieval%20augmented%20reasoning%20method%2C%20FiDeLiS%2C%20which%0Aenhances%20knowledge%20graph%20question%20answering%20by%20anchoring%20responses%20to%0Astructured%2C%20verifiable%20reasoning%20paths.%20FiDeLiS%20uses%20a%20keyword-enhanced%0Aretrieval%20mechanism%20that%20fetches%20relevant%20entities%20and%20relations%20from%20a%0Avector-based%20index%20of%20KGs%20to%20ensure%20high-recall%20retrieval.%20Once%20these%20entities%0Aand%20relations%20are%20retrieved%2C%20our%20method%20constructs%20candidate%20reasoning%20paths%0Awhich%20are%20then%20refined%20using%20a%20stepwise%20beam%20search.%20This%20ensures%20that%20all%20the%0Apaths%20we%20create%20can%20be%20confidently%20linked%20back%20to%20KGs%2C%20ensuring%20they%20are%0Aaccurate%20and%20reliable.%20A%20distinctive%20feature%20of%20our%20approach%20is%20its%20blend%20of%0Anatural%20language%20planning%20with%20beam%20search%20to%20optimize%20the%20selection%20of%0Areasoning%20paths.%20Moreover%2C%20we%20redesign%20the%20way%20reasoning%20paths%20are%20scored%20by%0Atransforming%20this%20process%20into%20a%20deductive%20reasoning%20task%2C%20allowing%20the%20LLM%20to%0Aassess%20the%20validity%20of%20the%20paths%20through%20deductive%20reasoning%20rather%20than%0Atraditional%20logit-based%20scoring.%20This%20helps%20avoid%20misleading%20reasoning%20chains%0Aand%20reduces%20unnecessary%20computational%20demand.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%2C%20even%20as%20a%20training-free%20method%20which%20has%20lower%20computational%0Acosts%20and%20superior%20generality%2C%20outperforms%20established%20strong%20baselines%20across%0Athree%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiDeLiS%253A%2520Faithful%2520Reasoning%2520in%2520Large%2520Language%2520Model%2520for%2520Knowledge%2520Graph%250A%2520%2520Question%2520Answering%26entry.906535625%3DYuan%2520Sui%2520and%2520Yufei%2520He%2520and%2520Nian%2520Liu%2520and%2520Xiaoxin%2520He%2520and%2520Kun%2520Wang%2520and%2520Bryan%2520Hooi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520often%2520challenged%2520by%2520generating%2520erroneous%2520or%250A%2560hallucinated%2527%2520responses%252C%2520especially%2520in%2520complex%2520reasoning%2520tasks.%2520To%2520mitigate%250Athis%252C%2520we%2520propose%2520a%2520retrieval%2520augmented%2520reasoning%2520method%252C%2520FiDeLiS%252C%2520which%250Aenhances%2520knowledge%2520graph%2520question%2520answering%2520by%2520anchoring%2520responses%2520to%250Astructured%252C%2520verifiable%2520reasoning%2520paths.%2520FiDeLiS%2520uses%2520a%2520keyword-enhanced%250Aretrieval%2520mechanism%2520that%2520fetches%2520relevant%2520entities%2520and%2520relations%2520from%2520a%250Avector-based%2520index%2520of%2520KGs%2520to%2520ensure%2520high-recall%2520retrieval.%2520Once%2520these%2520entities%250Aand%2520relations%2520are%2520retrieved%252C%2520our%2520method%2520constructs%2520candidate%2520reasoning%2520paths%250Awhich%2520are%2520then%2520refined%2520using%2520a%2520stepwise%2520beam%2520search.%2520This%2520ensures%2520that%2520all%2520the%250Apaths%2520we%2520create%2520can%2520be%2520confidently%2520linked%2520back%2520to%2520KGs%252C%2520ensuring%2520they%2520are%250Aaccurate%2520and%2520reliable.%2520A%2520distinctive%2520feature%2520of%2520our%2520approach%2520is%2520its%2520blend%2520of%250Anatural%2520language%2520planning%2520with%2520beam%2520search%2520to%2520optimize%2520the%2520selection%2520of%250Areasoning%2520paths.%2520Moreover%252C%2520we%2520redesign%2520the%2520way%2520reasoning%2520paths%2520are%2520scored%2520by%250Atransforming%2520this%2520process%2520into%2520a%2520deductive%2520reasoning%2520task%252C%2520allowing%2520the%2520LLM%2520to%250Aassess%2520the%2520validity%2520of%2520the%2520paths%2520through%2520deductive%2520reasoning%2520rather%2520than%250Atraditional%2520logit-based%2520scoring.%2520This%2520helps%2520avoid%2520misleading%2520reasoning%2520chains%250Aand%2520reduces%2520unnecessary%2520computational%2520demand.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%252C%2520even%2520as%2520a%2520training-free%2520method%2520which%2520has%2520lower%2520computational%250Acosts%2520and%2520superior%2520generality%252C%2520outperforms%2520established%2520strong%2520baselines%2520across%250Athree%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FiDeLiS%3A%20Faithful%20Reasoning%20in%20Large%20Language%20Model%20for%20Knowledge%20Graph%0A%20%20Question%20Answering&entry.906535625=Yuan%20Sui%20and%20Yufei%20He%20and%20Nian%20Liu%20and%20Xiaoxin%20He%20and%20Kun%20Wang%20and%20Bryan%20Hooi&entry.1292438233=%20%20Large%20language%20models%20are%20often%20challenged%20by%20generating%20erroneous%20or%0A%60hallucinated%27%20responses%2C%20especially%20in%20complex%20reasoning%20tasks.%20To%20mitigate%0Athis%2C%20we%20propose%20a%20retrieval%20augmented%20reasoning%20method%2C%20FiDeLiS%2C%20which%0Aenhances%20knowledge%20graph%20question%20answering%20by%20anchoring%20responses%20to%0Astructured%2C%20verifiable%20reasoning%20paths.%20FiDeLiS%20uses%20a%20keyword-enhanced%0Aretrieval%20mechanism%20that%20fetches%20relevant%20entities%20and%20relations%20from%20a%0Avector-based%20index%20of%20KGs%20to%20ensure%20high-recall%20retrieval.%20Once%20these%20entities%0Aand%20relations%20are%20retrieved%2C%20our%20method%20constructs%20candidate%20reasoning%20paths%0Awhich%20are%20then%20refined%20using%20a%20stepwise%20beam%20search.%20This%20ensures%20that%20all%20the%0Apaths%20we%20create%20can%20be%20confidently%20linked%20back%20to%20KGs%2C%20ensuring%20they%20are%0Aaccurate%20and%20reliable.%20A%20distinctive%20feature%20of%20our%20approach%20is%20its%20blend%20of%0Anatural%20language%20planning%20with%20beam%20search%20to%20optimize%20the%20selection%20of%0Areasoning%20paths.%20Moreover%2C%20we%20redesign%20the%20way%20reasoning%20paths%20are%20scored%20by%0Atransforming%20this%20process%20into%20a%20deductive%20reasoning%20task%2C%20allowing%20the%20LLM%20to%0Aassess%20the%20validity%20of%20the%20paths%20through%20deductive%20reasoning%20rather%20than%0Atraditional%20logit-based%20scoring.%20This%20helps%20avoid%20misleading%20reasoning%20chains%0Aand%20reduces%20unnecessary%20computational%20demand.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%2C%20even%20as%20a%20training-free%20method%20which%20has%20lower%20computational%0Acosts%20and%20superior%20generality%2C%20outperforms%20established%20strong%20baselines%20across%0Athree%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13873v2&entry.124074799=Read"},
{"title": "Efficient Reinforcement Learning with Large Language Model Priors", "author": "Xue Yan and Yan Song and Xidong Feng and Mengyue Yang and Haifeng Zhang and Haitham Bou Ammar and Jun Wang", "abstract": "  In sequential decision-making (SDM) tasks, methods like reinforcement\nlearning (RL) and heuristic search have made notable advances in specific\ncases. However, they often require extensive exploration and face challenges in\ngeneralizing across diverse environments due to their limited grasp of the\nunderlying decision dynamics. In contrast, large language models (LLMs) have\nrecently emerged as powerful general-purpose tools, due to their capacity to\nmaintain vast amounts of domain-specific knowledge. To harness this rich prior\nknowledge for efficiently solving complex SDM tasks, we propose treating LLMs\nas prior action distributions and integrating them into RL frameworks through\nBayesian inference methods, making use of variational inference and direct\nposterior sampling. The proposed approaches facilitate the seamless\nincorporation of fixed LLM priors into both policy-based and value-based RL\nframeworks. Our experiments show that incorporating LLM-based action priors\nsignificantly reduces exploration and optimization complexity, substantially\nimproving sample efficiency compared to traditional RL techniques, e.g., using\nLLM priors decreases the number of required samples by over 90% in offline\nlearning scenarios.\n", "link": "http://arxiv.org/abs/2410.07927v1", "date": "2024-10-10", "relevancy": 2.0572, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Reinforcement%20Learning%20with%20Large%20Language%20Model%20Priors&body=Title%3A%20Efficient%20Reinforcement%20Learning%20with%20Large%20Language%20Model%20Priors%0AAuthor%3A%20Xue%20Yan%20and%20Yan%20Song%20and%20Xidong%20Feng%20and%20Mengyue%20Yang%20and%20Haifeng%20Zhang%20and%20Haitham%20Bou%20Ammar%20and%20Jun%20Wang%0AAbstract%3A%20%20%20In%20sequential%20decision-making%20%28SDM%29%20tasks%2C%20methods%20like%20reinforcement%0Alearning%20%28RL%29%20and%20heuristic%20search%20have%20made%20notable%20advances%20in%20specific%0Acases.%20However%2C%20they%20often%20require%20extensive%20exploration%20and%20face%20challenges%20in%0Ageneralizing%20across%20diverse%20environments%20due%20to%20their%20limited%20grasp%20of%20the%0Aunderlying%20decision%20dynamics.%20In%20contrast%2C%20large%20language%20models%20%28LLMs%29%20have%0Arecently%20emerged%20as%20powerful%20general-purpose%20tools%2C%20due%20to%20their%20capacity%20to%0Amaintain%20vast%20amounts%20of%20domain-specific%20knowledge.%20To%20harness%20this%20rich%20prior%0Aknowledge%20for%20efficiently%20solving%20complex%20SDM%20tasks%2C%20we%20propose%20treating%20LLMs%0Aas%20prior%20action%20distributions%20and%20integrating%20them%20into%20RL%20frameworks%20through%0ABayesian%20inference%20methods%2C%20making%20use%20of%20variational%20inference%20and%20direct%0Aposterior%20sampling.%20The%20proposed%20approaches%20facilitate%20the%20seamless%0Aincorporation%20of%20fixed%20LLM%20priors%20into%20both%20policy-based%20and%20value-based%20RL%0Aframeworks.%20Our%20experiments%20show%20that%20incorporating%20LLM-based%20action%20priors%0Asignificantly%20reduces%20exploration%20and%20optimization%20complexity%2C%20substantially%0Aimproving%20sample%20efficiency%20compared%20to%20traditional%20RL%20techniques%2C%20e.g.%2C%20using%0ALLM%20priors%20decreases%20the%20number%20of%20required%20samples%20by%20over%2090%25%20in%20offline%0Alearning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Reinforcement%2520Learning%2520with%2520Large%2520Language%2520Model%2520Priors%26entry.906535625%3DXue%2520Yan%2520and%2520Yan%2520Song%2520and%2520Xidong%2520Feng%2520and%2520Mengyue%2520Yang%2520and%2520Haifeng%2520Zhang%2520and%2520Haitham%2520Bou%2520Ammar%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520In%2520sequential%2520decision-making%2520%2528SDM%2529%2520tasks%252C%2520methods%2520like%2520reinforcement%250Alearning%2520%2528RL%2529%2520and%2520heuristic%2520search%2520have%2520made%2520notable%2520advances%2520in%2520specific%250Acases.%2520However%252C%2520they%2520often%2520require%2520extensive%2520exploration%2520and%2520face%2520challenges%2520in%250Ageneralizing%2520across%2520diverse%2520environments%2520due%2520to%2520their%2520limited%2520grasp%2520of%2520the%250Aunderlying%2520decision%2520dynamics.%2520In%2520contrast%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Arecently%2520emerged%2520as%2520powerful%2520general-purpose%2520tools%252C%2520due%2520to%2520their%2520capacity%2520to%250Amaintain%2520vast%2520amounts%2520of%2520domain-specific%2520knowledge.%2520To%2520harness%2520this%2520rich%2520prior%250Aknowledge%2520for%2520efficiently%2520solving%2520complex%2520SDM%2520tasks%252C%2520we%2520propose%2520treating%2520LLMs%250Aas%2520prior%2520action%2520distributions%2520and%2520integrating%2520them%2520into%2520RL%2520frameworks%2520through%250ABayesian%2520inference%2520methods%252C%2520making%2520use%2520of%2520variational%2520inference%2520and%2520direct%250Aposterior%2520sampling.%2520The%2520proposed%2520approaches%2520facilitate%2520the%2520seamless%250Aincorporation%2520of%2520fixed%2520LLM%2520priors%2520into%2520both%2520policy-based%2520and%2520value-based%2520RL%250Aframeworks.%2520Our%2520experiments%2520show%2520that%2520incorporating%2520LLM-based%2520action%2520priors%250Asignificantly%2520reduces%2520exploration%2520and%2520optimization%2520complexity%252C%2520substantially%250Aimproving%2520sample%2520efficiency%2520compared%2520to%2520traditional%2520RL%2520techniques%252C%2520e.g.%252C%2520using%250ALLM%2520priors%2520decreases%2520the%2520number%2520of%2520required%2520samples%2520by%2520over%252090%2525%2520in%2520offline%250Alearning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Reinforcement%20Learning%20with%20Large%20Language%20Model%20Priors&entry.906535625=Xue%20Yan%20and%20Yan%20Song%20and%20Xidong%20Feng%20and%20Mengyue%20Yang%20and%20Haifeng%20Zhang%20and%20Haitham%20Bou%20Ammar%20and%20Jun%20Wang&entry.1292438233=%20%20In%20sequential%20decision-making%20%28SDM%29%20tasks%2C%20methods%20like%20reinforcement%0Alearning%20%28RL%29%20and%20heuristic%20search%20have%20made%20notable%20advances%20in%20specific%0Acases.%20However%2C%20they%20often%20require%20extensive%20exploration%20and%20face%20challenges%20in%0Ageneralizing%20across%20diverse%20environments%20due%20to%20their%20limited%20grasp%20of%20the%0Aunderlying%20decision%20dynamics.%20In%20contrast%2C%20large%20language%20models%20%28LLMs%29%20have%0Arecently%20emerged%20as%20powerful%20general-purpose%20tools%2C%20due%20to%20their%20capacity%20to%0Amaintain%20vast%20amounts%20of%20domain-specific%20knowledge.%20To%20harness%20this%20rich%20prior%0Aknowledge%20for%20efficiently%20solving%20complex%20SDM%20tasks%2C%20we%20propose%20treating%20LLMs%0Aas%20prior%20action%20distributions%20and%20integrating%20them%20into%20RL%20frameworks%20through%0ABayesian%20inference%20methods%2C%20making%20use%20of%20variational%20inference%20and%20direct%0Aposterior%20sampling.%20The%20proposed%20approaches%20facilitate%20the%20seamless%0Aincorporation%20of%20fixed%20LLM%20priors%20into%20both%20policy-based%20and%20value-based%20RL%0Aframeworks.%20Our%20experiments%20show%20that%20incorporating%20LLM-based%20action%20priors%0Asignificantly%20reduces%20exploration%20and%20optimization%20complexity%2C%20substantially%0Aimproving%20sample%20efficiency%20compared%20to%20traditional%20RL%20techniques%2C%20e.g.%2C%20using%0ALLM%20priors%20decreases%20the%20number%20of%20required%20samples%20by%20over%2090%25%20in%20offline%0Alearning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07927v1&entry.124074799=Read"},
{"title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning", "author": "Amrith Setlur and Chirag Nagpal and Adam Fisch and Xinyang Geng and Jacob Eisenstein and Rishabh Agarwal and Alekh Agarwal and Jonathan Berant and Aviral Kumar", "abstract": "  A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs.\n", "link": "http://arxiv.org/abs/2410.08146v1", "date": "2024-10-10", "relevancy": 2.0556, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rewarding%20Progress%3A%20Scaling%20Automated%20Process%20Verifiers%20for%20LLM%0A%20%20Reasoning&body=Title%3A%20Rewarding%20Progress%3A%20Scaling%20Automated%20Process%20Verifiers%20for%20LLM%0A%20%20Reasoning%0AAuthor%3A%20Amrith%20Setlur%20and%20Chirag%20Nagpal%20and%20Adam%20Fisch%20and%20Xinyang%20Geng%20and%20Jacob%20Eisenstein%20and%20Rishabh%20Agarwal%20and%20Alekh%20Agarwal%20and%20Jonathan%20Berant%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20A%20promising%20approach%20for%20improving%20reasoning%20in%20large%20language%20models%20is%20to%0Ause%20process%20reward%20models%20%28PRMs%29.%20PRMs%20provide%20feedback%20at%20each%20step%20of%20a%0Amulti-step%20reasoning%20trace%2C%20potentially%20improving%20credit%20assignment%20over%0Aoutcome%20reward%20models%20%28ORMs%29%20that%20only%20provide%20feedback%20at%20the%20final%20step.%0AHowever%2C%20collecting%20dense%2C%20per-step%20human%20labels%20is%20not%20scalable%2C%20and%20training%0APRMs%20from%20automatically-labeled%20data%20has%20thus%20far%20led%20to%20limited%20gains.%20To%0Aimprove%20a%20base%20policy%20by%20running%20search%20against%20a%20PRM%20or%20using%20it%20as%20dense%0Arewards%20for%20reinforcement%20learning%20%28RL%29%2C%20we%20ask%3A%20%22How%20should%20we%20design%20process%0Arewards%3F%22.%20Our%20key%20insight%20is%20that%2C%20to%20be%20effective%2C%20the%20process%20reward%20for%20a%0Astep%20should%20measure%20progress%3A%20a%20change%20in%20the%20likelihood%20of%20producing%20a%20correct%0Aresponse%20in%20the%20future%2C%20before%20and%20after%20taking%20the%20step%2C%20corresponding%20to%20the%0Anotion%20of%20step-level%20advantages%20in%20RL.%20Crucially%2C%20this%20progress%20should%20be%0Ameasured%20under%20a%20prover%20policy%20distinct%20from%20the%20base%20policy.%20We%20theoretically%0Acharacterize%20the%20set%20of%20good%20provers%20and%20our%20results%20show%20that%20optimizing%0Aprocess%20rewards%20from%20such%20provers%20improves%20exploration%20during%20test-time%20search%0Aand%20online%20RL.%20In%20fact%2C%20our%20characterization%20shows%20that%20weak%20prover%20policies%0Acan%20substantially%20improve%20a%20stronger%20base%20policy%2C%20which%20we%20also%20observe%0Aempirically.%20We%20validate%20our%20claims%20by%20training%20process%20advantage%20verifiers%0A%28PAVs%29%20to%20predict%20progress%20under%20such%20provers%2C%20and%20show%20that%20compared%20to%20ORMs%2C%0Atest-time%20search%20against%20PAVs%20is%20%24%3E8%5C%25%24%20more%20accurate%2C%20and%20%241.5-5%5Ctimes%24%20more%0Acompute-efficient.%20Online%20RL%20with%20dense%20rewards%20from%20PAVs%20enables%20one%20of%20the%0Afirst%20results%20with%20%245-6%5Ctimes%24%20gain%20in%20sample%20efficiency%2C%20and%20%24%3E6%5C%25%24%20gain%20in%0Aaccuracy%2C%20over%20ORMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewarding%2520Progress%253A%2520Scaling%2520Automated%2520Process%2520Verifiers%2520for%2520LLM%250A%2520%2520Reasoning%26entry.906535625%3DAmrith%2520Setlur%2520and%2520Chirag%2520Nagpal%2520and%2520Adam%2520Fisch%2520and%2520Xinyang%2520Geng%2520and%2520Jacob%2520Eisenstein%2520and%2520Rishabh%2520Agarwal%2520and%2520Alekh%2520Agarwal%2520and%2520Jonathan%2520Berant%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520A%2520promising%2520approach%2520for%2520improving%2520reasoning%2520in%2520large%2520language%2520models%2520is%2520to%250Ause%2520process%2520reward%2520models%2520%2528PRMs%2529.%2520PRMs%2520provide%2520feedback%2520at%2520each%2520step%2520of%2520a%250Amulti-step%2520reasoning%2520trace%252C%2520potentially%2520improving%2520credit%2520assignment%2520over%250Aoutcome%2520reward%2520models%2520%2528ORMs%2529%2520that%2520only%2520provide%2520feedback%2520at%2520the%2520final%2520step.%250AHowever%252C%2520collecting%2520dense%252C%2520per-step%2520human%2520labels%2520is%2520not%2520scalable%252C%2520and%2520training%250APRMs%2520from%2520automatically-labeled%2520data%2520has%2520thus%2520far%2520led%2520to%2520limited%2520gains.%2520To%250Aimprove%2520a%2520base%2520policy%2520by%2520running%2520search%2520against%2520a%2520PRM%2520or%2520using%2520it%2520as%2520dense%250Arewards%2520for%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520we%2520ask%253A%2520%2522How%2520should%2520we%2520design%2520process%250Arewards%253F%2522.%2520Our%2520key%2520insight%2520is%2520that%252C%2520to%2520be%2520effective%252C%2520the%2520process%2520reward%2520for%2520a%250Astep%2520should%2520measure%2520progress%253A%2520a%2520change%2520in%2520the%2520likelihood%2520of%2520producing%2520a%2520correct%250Aresponse%2520in%2520the%2520future%252C%2520before%2520and%2520after%2520taking%2520the%2520step%252C%2520corresponding%2520to%2520the%250Anotion%2520of%2520step-level%2520advantages%2520in%2520RL.%2520Crucially%252C%2520this%2520progress%2520should%2520be%250Ameasured%2520under%2520a%2520prover%2520policy%2520distinct%2520from%2520the%2520base%2520policy.%2520We%2520theoretically%250Acharacterize%2520the%2520set%2520of%2520good%2520provers%2520and%2520our%2520results%2520show%2520that%2520optimizing%250Aprocess%2520rewards%2520from%2520such%2520provers%2520improves%2520exploration%2520during%2520test-time%2520search%250Aand%2520online%2520RL.%2520In%2520fact%252C%2520our%2520characterization%2520shows%2520that%2520weak%2520prover%2520policies%250Acan%2520substantially%2520improve%2520a%2520stronger%2520base%2520policy%252C%2520which%2520we%2520also%2520observe%250Aempirically.%2520We%2520validate%2520our%2520claims%2520by%2520training%2520process%2520advantage%2520verifiers%250A%2528PAVs%2529%2520to%2520predict%2520progress%2520under%2520such%2520provers%252C%2520and%2520show%2520that%2520compared%2520to%2520ORMs%252C%250Atest-time%2520search%2520against%2520PAVs%2520is%2520%2524%253E8%255C%2525%2524%2520more%2520accurate%252C%2520and%2520%25241.5-5%255Ctimes%2524%2520more%250Acompute-efficient.%2520Online%2520RL%2520with%2520dense%2520rewards%2520from%2520PAVs%2520enables%2520one%2520of%2520the%250Afirst%2520results%2520with%2520%25245-6%255Ctimes%2524%2520gain%2520in%2520sample%2520efficiency%252C%2520and%2520%2524%253E6%255C%2525%2524%2520gain%2520in%250Aaccuracy%252C%2520over%2520ORMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rewarding%20Progress%3A%20Scaling%20Automated%20Process%20Verifiers%20for%20LLM%0A%20%20Reasoning&entry.906535625=Amrith%20Setlur%20and%20Chirag%20Nagpal%20and%20Adam%20Fisch%20and%20Xinyang%20Geng%20and%20Jacob%20Eisenstein%20and%20Rishabh%20Agarwal%20and%20Alekh%20Agarwal%20and%20Jonathan%20Berant%20and%20Aviral%20Kumar&entry.1292438233=%20%20A%20promising%20approach%20for%20improving%20reasoning%20in%20large%20language%20models%20is%20to%0Ause%20process%20reward%20models%20%28PRMs%29.%20PRMs%20provide%20feedback%20at%20each%20step%20of%20a%0Amulti-step%20reasoning%20trace%2C%20potentially%20improving%20credit%20assignment%20over%0Aoutcome%20reward%20models%20%28ORMs%29%20that%20only%20provide%20feedback%20at%20the%20final%20step.%0AHowever%2C%20collecting%20dense%2C%20per-step%20human%20labels%20is%20not%20scalable%2C%20and%20training%0APRMs%20from%20automatically-labeled%20data%20has%20thus%20far%20led%20to%20limited%20gains.%20To%0Aimprove%20a%20base%20policy%20by%20running%20search%20against%20a%20PRM%20or%20using%20it%20as%20dense%0Arewards%20for%20reinforcement%20learning%20%28RL%29%2C%20we%20ask%3A%20%22How%20should%20we%20design%20process%0Arewards%3F%22.%20Our%20key%20insight%20is%20that%2C%20to%20be%20effective%2C%20the%20process%20reward%20for%20a%0Astep%20should%20measure%20progress%3A%20a%20change%20in%20the%20likelihood%20of%20producing%20a%20correct%0Aresponse%20in%20the%20future%2C%20before%20and%20after%20taking%20the%20step%2C%20corresponding%20to%20the%0Anotion%20of%20step-level%20advantages%20in%20RL.%20Crucially%2C%20this%20progress%20should%20be%0Ameasured%20under%20a%20prover%20policy%20distinct%20from%20the%20base%20policy.%20We%20theoretically%0Acharacterize%20the%20set%20of%20good%20provers%20and%20our%20results%20show%20that%20optimizing%0Aprocess%20rewards%20from%20such%20provers%20improves%20exploration%20during%20test-time%20search%0Aand%20online%20RL.%20In%20fact%2C%20our%20characterization%20shows%20that%20weak%20prover%20policies%0Acan%20substantially%20improve%20a%20stronger%20base%20policy%2C%20which%20we%20also%20observe%0Aempirically.%20We%20validate%20our%20claims%20by%20training%20process%20advantage%20verifiers%0A%28PAVs%29%20to%20predict%20progress%20under%20such%20provers%2C%20and%20show%20that%20compared%20to%20ORMs%2C%0Atest-time%20search%20against%20PAVs%20is%20%24%3E8%5C%25%24%20more%20accurate%2C%20and%20%241.5-5%5Ctimes%24%20more%0Acompute-efficient.%20Online%20RL%20with%20dense%20rewards%20from%20PAVs%20enables%20one%20of%20the%0Afirst%20results%20with%20%245-6%5Ctimes%24%20gain%20in%20sample%20efficiency%2C%20and%20%24%3E6%5C%25%24%20gain%20in%0Aaccuracy%2C%20over%20ORMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08146v1&entry.124074799=Read"},
{"title": "Continual Learning in the Frequency Domain", "author": "Ruiqi Liu and Boyu Diao and Libo Huang and Zijia An and Zhulin An and Yongjun Xu", "abstract": "  Continual learning (CL) is designed to learn new tasks while preserving\nexisting knowledge. Replaying samples from earlier tasks has proven to be an\neffective method to mitigate the forgetting of previously acquired knowledge.\nHowever, the current research on the training efficiency of rehearsal-based\nmethods is insufficient, which limits the practical application of CL systems\nin resource-limited scenarios. The human visual system (HVS) exhibits varying\nsensitivities to different frequency components, enabling the efficient\nelimination of visually redundant information. Inspired by HVS, we propose a\nnovel framework called Continual Learning in the Frequency Domain (CLFD). To\nour knowledge, this is the first study to utilize frequency domain features to\nenhance the performance and efficiency of CL training on edge devices. For the\ninput features of the feature extractor, CLFD employs wavelet transform to map\nthe original input image into the frequency domain, thereby effectively\nreducing the size of input feature maps. Regarding the output features of the\nfeature extractor, CLFD selectively utilizes output features for distinct\nclasses for classification, thereby balancing the reusability and interference\nof output features based on the frequency domain similarity of the classes\nacross various tasks. Optimizing only the input and output features of the\nfeature extractor allows for seamless integration of CLFD with various\nrehearsal-based methods. Extensive experiments conducted in both cloud and edge\nenvironments demonstrate that CLFD consistently improves the performance of\nstate-of-the-art (SOTA) methods in both precision and training efficiency.\nSpecifically, CLFD can increase the accuracy of the SOTA CL method by up to\n6.83% and reduce the training time by 2.6$\\times$.\n", "link": "http://arxiv.org/abs/2410.06645v2", "date": "2024-10-10", "relevancy": 2.0431, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20in%20the%20Frequency%20Domain&body=Title%3A%20Continual%20Learning%20in%20the%20Frequency%20Domain%0AAuthor%3A%20Ruiqi%20Liu%20and%20Boyu%20Diao%20and%20Libo%20Huang%20and%20Zijia%20An%20and%20Zhulin%20An%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20is%20designed%20to%20learn%20new%20tasks%20while%20preserving%0Aexisting%20knowledge.%20Replaying%20samples%20from%20earlier%20tasks%20has%20proven%20to%20be%20an%0Aeffective%20method%20to%20mitigate%20the%20forgetting%20of%20previously%20acquired%20knowledge.%0AHowever%2C%20the%20current%20research%20on%20the%20training%20efficiency%20of%20rehearsal-based%0Amethods%20is%20insufficient%2C%20which%20limits%20the%20practical%20application%20of%20CL%20systems%0Ain%20resource-limited%20scenarios.%20The%20human%20visual%20system%20%28HVS%29%20exhibits%20varying%0Asensitivities%20to%20different%20frequency%20components%2C%20enabling%20the%20efficient%0Aelimination%20of%20visually%20redundant%20information.%20Inspired%20by%20HVS%2C%20we%20propose%20a%0Anovel%20framework%20called%20Continual%20Learning%20in%20the%20Frequency%20Domain%20%28CLFD%29.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20study%20to%20utilize%20frequency%20domain%20features%20to%0Aenhance%20the%20performance%20and%20efficiency%20of%20CL%20training%20on%20edge%20devices.%20For%20the%0Ainput%20features%20of%20the%20feature%20extractor%2C%20CLFD%20employs%20wavelet%20transform%20to%20map%0Athe%20original%20input%20image%20into%20the%20frequency%20domain%2C%20thereby%20effectively%0Areducing%20the%20size%20of%20input%20feature%20maps.%20Regarding%20the%20output%20features%20of%20the%0Afeature%20extractor%2C%20CLFD%20selectively%20utilizes%20output%20features%20for%20distinct%0Aclasses%20for%20classification%2C%20thereby%20balancing%20the%20reusability%20and%20interference%0Aof%20output%20features%20based%20on%20the%20frequency%20domain%20similarity%20of%20the%20classes%0Aacross%20various%20tasks.%20Optimizing%20only%20the%20input%20and%20output%20features%20of%20the%0Afeature%20extractor%20allows%20for%20seamless%20integration%20of%20CLFD%20with%20various%0Arehearsal-based%20methods.%20Extensive%20experiments%20conducted%20in%20both%20cloud%20and%20edge%0Aenvironments%20demonstrate%20that%20CLFD%20consistently%20improves%20the%20performance%20of%0Astate-of-the-art%20%28SOTA%29%20methods%20in%20both%20precision%20and%20training%20efficiency.%0ASpecifically%2C%20CLFD%20can%20increase%20the%20accuracy%20of%20the%20SOTA%20CL%20method%20by%20up%20to%0A6.83%25%20and%20reduce%20the%20training%20time%20by%202.6%24%5Ctimes%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520in%2520the%2520Frequency%2520Domain%26entry.906535625%3DRuiqi%2520Liu%2520and%2520Boyu%2520Diao%2520and%2520Libo%2520Huang%2520and%2520Zijia%2520An%2520and%2520Zhulin%2520An%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520is%2520designed%2520to%2520learn%2520new%2520tasks%2520while%2520preserving%250Aexisting%2520knowledge.%2520Replaying%2520samples%2520from%2520earlier%2520tasks%2520has%2520proven%2520to%2520be%2520an%250Aeffective%2520method%2520to%2520mitigate%2520the%2520forgetting%2520of%2520previously%2520acquired%2520knowledge.%250AHowever%252C%2520the%2520current%2520research%2520on%2520the%2520training%2520efficiency%2520of%2520rehearsal-based%250Amethods%2520is%2520insufficient%252C%2520which%2520limits%2520the%2520practical%2520application%2520of%2520CL%2520systems%250Ain%2520resource-limited%2520scenarios.%2520The%2520human%2520visual%2520system%2520%2528HVS%2529%2520exhibits%2520varying%250Asensitivities%2520to%2520different%2520frequency%2520components%252C%2520enabling%2520the%2520efficient%250Aelimination%2520of%2520visually%2520redundant%2520information.%2520Inspired%2520by%2520HVS%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520called%2520Continual%2520Learning%2520in%2520the%2520Frequency%2520Domain%2520%2528CLFD%2529.%2520To%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520utilize%2520frequency%2520domain%2520features%2520to%250Aenhance%2520the%2520performance%2520and%2520efficiency%2520of%2520CL%2520training%2520on%2520edge%2520devices.%2520For%2520the%250Ainput%2520features%2520of%2520the%2520feature%2520extractor%252C%2520CLFD%2520employs%2520wavelet%2520transform%2520to%2520map%250Athe%2520original%2520input%2520image%2520into%2520the%2520frequency%2520domain%252C%2520thereby%2520effectively%250Areducing%2520the%2520size%2520of%2520input%2520feature%2520maps.%2520Regarding%2520the%2520output%2520features%2520of%2520the%250Afeature%2520extractor%252C%2520CLFD%2520selectively%2520utilizes%2520output%2520features%2520for%2520distinct%250Aclasses%2520for%2520classification%252C%2520thereby%2520balancing%2520the%2520reusability%2520and%2520interference%250Aof%2520output%2520features%2520based%2520on%2520the%2520frequency%2520domain%2520similarity%2520of%2520the%2520classes%250Aacross%2520various%2520tasks.%2520Optimizing%2520only%2520the%2520input%2520and%2520output%2520features%2520of%2520the%250Afeature%2520extractor%2520allows%2520for%2520seamless%2520integration%2520of%2520CLFD%2520with%2520various%250Arehearsal-based%2520methods.%2520Extensive%2520experiments%2520conducted%2520in%2520both%2520cloud%2520and%2520edge%250Aenvironments%2520demonstrate%2520that%2520CLFD%2520consistently%2520improves%2520the%2520performance%2520of%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520both%2520precision%2520and%2520training%2520efficiency.%250ASpecifically%252C%2520CLFD%2520can%2520increase%2520the%2520accuracy%2520of%2520the%2520SOTA%2520CL%2520method%2520by%2520up%2520to%250A6.83%2525%2520and%2520reduce%2520the%2520training%2520time%2520by%25202.6%2524%255Ctimes%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20in%20the%20Frequency%20Domain&entry.906535625=Ruiqi%20Liu%20and%20Boyu%20Diao%20and%20Libo%20Huang%20and%20Zijia%20An%20and%20Zhulin%20An%20and%20Yongjun%20Xu&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20is%20designed%20to%20learn%20new%20tasks%20while%20preserving%0Aexisting%20knowledge.%20Replaying%20samples%20from%20earlier%20tasks%20has%20proven%20to%20be%20an%0Aeffective%20method%20to%20mitigate%20the%20forgetting%20of%20previously%20acquired%20knowledge.%0AHowever%2C%20the%20current%20research%20on%20the%20training%20efficiency%20of%20rehearsal-based%0Amethods%20is%20insufficient%2C%20which%20limits%20the%20practical%20application%20of%20CL%20systems%0Ain%20resource-limited%20scenarios.%20The%20human%20visual%20system%20%28HVS%29%20exhibits%20varying%0Asensitivities%20to%20different%20frequency%20components%2C%20enabling%20the%20efficient%0Aelimination%20of%20visually%20redundant%20information.%20Inspired%20by%20HVS%2C%20we%20propose%20a%0Anovel%20framework%20called%20Continual%20Learning%20in%20the%20Frequency%20Domain%20%28CLFD%29.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20study%20to%20utilize%20frequency%20domain%20features%20to%0Aenhance%20the%20performance%20and%20efficiency%20of%20CL%20training%20on%20edge%20devices.%20For%20the%0Ainput%20features%20of%20the%20feature%20extractor%2C%20CLFD%20employs%20wavelet%20transform%20to%20map%0Athe%20original%20input%20image%20into%20the%20frequency%20domain%2C%20thereby%20effectively%0Areducing%20the%20size%20of%20input%20feature%20maps.%20Regarding%20the%20output%20features%20of%20the%0Afeature%20extractor%2C%20CLFD%20selectively%20utilizes%20output%20features%20for%20distinct%0Aclasses%20for%20classification%2C%20thereby%20balancing%20the%20reusability%20and%20interference%0Aof%20output%20features%20based%20on%20the%20frequency%20domain%20similarity%20of%20the%20classes%0Aacross%20various%20tasks.%20Optimizing%20only%20the%20input%20and%20output%20features%20of%20the%0Afeature%20extractor%20allows%20for%20seamless%20integration%20of%20CLFD%20with%20various%0Arehearsal-based%20methods.%20Extensive%20experiments%20conducted%20in%20both%20cloud%20and%20edge%0Aenvironments%20demonstrate%20that%20CLFD%20consistently%20improves%20the%20performance%20of%0Astate-of-the-art%20%28SOTA%29%20methods%20in%20both%20precision%20and%20training%20efficiency.%0ASpecifically%2C%20CLFD%20can%20increase%20the%20accuracy%20of%20the%20SOTA%20CL%20method%20by%20up%20to%0A6.83%25%20and%20reduce%20the%20training%20time%20by%202.6%24%5Ctimes%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06645v2&entry.124074799=Read"},
{"title": "Features are fate: a theory of transfer learning in high-dimensional\n  regression", "author": "Javan Tahir and Surya Ganguli and Grant M. Rotskoff", "abstract": "  With the emergence of large-scale pre-trained neural networks, methods to\nadapt such \"foundation\" models to data-limited downstream tasks have become a\nnecessity. Fine-tuning, preference optimization, and transfer learning have all\nbeen successfully employed for these purposes when the target task closely\nresembles the source task, but a precise theoretical understanding of \"task\nsimilarity\" is still lacking. While conventional wisdom suggests that simple\nmeasures of similarity between source and target distributions, such as\n$\\phi$-divergences or integral probability metrics, can directly predict the\nsuccess of transfer, we prove the surprising fact that, in general, this is not\nthe case. We adopt, instead, a feature-centric viewpoint on transfer learning\nand establish a number of theoretical results that demonstrate that when the\ntarget task is well represented by the feature space of the pre-trained model,\ntransfer learning outperforms training from scratch. We study deep linear\nnetworks as a minimal model of transfer learning in which we can analytically\ncharacterize the transferability phase diagram as a function of the target\ndataset size and the feature space overlap. For this model, we establish\nrigorously that when the feature space overlap between the source and target\ntasks is sufficiently strong, both linear transfer and fine-tuning improve\nperformance, especially in the low data limit. These results build on an\nemerging understanding of feature learning dynamics in deep linear networks,\nand we demonstrate numerically that the rigorous results we derive for the\nlinear case also apply to nonlinear networks.\n", "link": "http://arxiv.org/abs/2410.08194v1", "date": "2024-10-10", "relevancy": 2.038, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5191}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Features%20are%20fate%3A%20a%20theory%20of%20transfer%20learning%20in%20high-dimensional%0A%20%20regression&body=Title%3A%20Features%20are%20fate%3A%20a%20theory%20of%20transfer%20learning%20in%20high-dimensional%0A%20%20regression%0AAuthor%3A%20Javan%20Tahir%20and%20Surya%20Ganguli%20and%20Grant%20M.%20Rotskoff%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20large-scale%20pre-trained%20neural%20networks%2C%20methods%20to%0Aadapt%20such%20%22foundation%22%20models%20to%20data-limited%20downstream%20tasks%20have%20become%20a%0Anecessity.%20Fine-tuning%2C%20preference%20optimization%2C%20and%20transfer%20learning%20have%20all%0Abeen%20successfully%20employed%20for%20these%20purposes%20when%20the%20target%20task%20closely%0Aresembles%20the%20source%20task%2C%20but%20a%20precise%20theoretical%20understanding%20of%20%22task%0Asimilarity%22%20is%20still%20lacking.%20While%20conventional%20wisdom%20suggests%20that%20simple%0Ameasures%20of%20similarity%20between%20source%20and%20target%20distributions%2C%20such%20as%0A%24%5Cphi%24-divergences%20or%20integral%20probability%20metrics%2C%20can%20directly%20predict%20the%0Asuccess%20of%20transfer%2C%20we%20prove%20the%20surprising%20fact%20that%2C%20in%20general%2C%20this%20is%20not%0Athe%20case.%20We%20adopt%2C%20instead%2C%20a%20feature-centric%20viewpoint%20on%20transfer%20learning%0Aand%20establish%20a%20number%20of%20theoretical%20results%20that%20demonstrate%20that%20when%20the%0Atarget%20task%20is%20well%20represented%20by%20the%20feature%20space%20of%20the%20pre-trained%20model%2C%0Atransfer%20learning%20outperforms%20training%20from%20scratch.%20We%20study%20deep%20linear%0Anetworks%20as%20a%20minimal%20model%20of%20transfer%20learning%20in%20which%20we%20can%20analytically%0Acharacterize%20the%20transferability%20phase%20diagram%20as%20a%20function%20of%20the%20target%0Adataset%20size%20and%20the%20feature%20space%20overlap.%20For%20this%20model%2C%20we%20establish%0Arigorously%20that%20when%20the%20feature%20space%20overlap%20between%20the%20source%20and%20target%0Atasks%20is%20sufficiently%20strong%2C%20both%20linear%20transfer%20and%20fine-tuning%20improve%0Aperformance%2C%20especially%20in%20the%20low%20data%20limit.%20These%20results%20build%20on%20an%0Aemerging%20understanding%20of%20feature%20learning%20dynamics%20in%20deep%20linear%20networks%2C%0Aand%20we%20demonstrate%20numerically%20that%20the%20rigorous%20results%20we%20derive%20for%20the%0Alinear%20case%20also%20apply%20to%20nonlinear%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatures%2520are%2520fate%253A%2520a%2520theory%2520of%2520transfer%2520learning%2520in%2520high-dimensional%250A%2520%2520regression%26entry.906535625%3DJavan%2520Tahir%2520and%2520Surya%2520Ganguli%2520and%2520Grant%2520M.%2520Rotskoff%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520large-scale%2520pre-trained%2520neural%2520networks%252C%2520methods%2520to%250Aadapt%2520such%2520%2522foundation%2522%2520models%2520to%2520data-limited%2520downstream%2520tasks%2520have%2520become%2520a%250Anecessity.%2520Fine-tuning%252C%2520preference%2520optimization%252C%2520and%2520transfer%2520learning%2520have%2520all%250Abeen%2520successfully%2520employed%2520for%2520these%2520purposes%2520when%2520the%2520target%2520task%2520closely%250Aresembles%2520the%2520source%2520task%252C%2520but%2520a%2520precise%2520theoretical%2520understanding%2520of%2520%2522task%250Asimilarity%2522%2520is%2520still%2520lacking.%2520While%2520conventional%2520wisdom%2520suggests%2520that%2520simple%250Ameasures%2520of%2520similarity%2520between%2520source%2520and%2520target%2520distributions%252C%2520such%2520as%250A%2524%255Cphi%2524-divergences%2520or%2520integral%2520probability%2520metrics%252C%2520can%2520directly%2520predict%2520the%250Asuccess%2520of%2520transfer%252C%2520we%2520prove%2520the%2520surprising%2520fact%2520that%252C%2520in%2520general%252C%2520this%2520is%2520not%250Athe%2520case.%2520We%2520adopt%252C%2520instead%252C%2520a%2520feature-centric%2520viewpoint%2520on%2520transfer%2520learning%250Aand%2520establish%2520a%2520number%2520of%2520theoretical%2520results%2520that%2520demonstrate%2520that%2520when%2520the%250Atarget%2520task%2520is%2520well%2520represented%2520by%2520the%2520feature%2520space%2520of%2520the%2520pre-trained%2520model%252C%250Atransfer%2520learning%2520outperforms%2520training%2520from%2520scratch.%2520We%2520study%2520deep%2520linear%250Anetworks%2520as%2520a%2520minimal%2520model%2520of%2520transfer%2520learning%2520in%2520which%2520we%2520can%2520analytically%250Acharacterize%2520the%2520transferability%2520phase%2520diagram%2520as%2520a%2520function%2520of%2520the%2520target%250Adataset%2520size%2520and%2520the%2520feature%2520space%2520overlap.%2520For%2520this%2520model%252C%2520we%2520establish%250Arigorously%2520that%2520when%2520the%2520feature%2520space%2520overlap%2520between%2520the%2520source%2520and%2520target%250Atasks%2520is%2520sufficiently%2520strong%252C%2520both%2520linear%2520transfer%2520and%2520fine-tuning%2520improve%250Aperformance%252C%2520especially%2520in%2520the%2520low%2520data%2520limit.%2520These%2520results%2520build%2520on%2520an%250Aemerging%2520understanding%2520of%2520feature%2520learning%2520dynamics%2520in%2520deep%2520linear%2520networks%252C%250Aand%2520we%2520demonstrate%2520numerically%2520that%2520the%2520rigorous%2520results%2520we%2520derive%2520for%2520the%250Alinear%2520case%2520also%2520apply%2520to%2520nonlinear%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Features%20are%20fate%3A%20a%20theory%20of%20transfer%20learning%20in%20high-dimensional%0A%20%20regression&entry.906535625=Javan%20Tahir%20and%20Surya%20Ganguli%20and%20Grant%20M.%20Rotskoff&entry.1292438233=%20%20With%20the%20emergence%20of%20large-scale%20pre-trained%20neural%20networks%2C%20methods%20to%0Aadapt%20such%20%22foundation%22%20models%20to%20data-limited%20downstream%20tasks%20have%20become%20a%0Anecessity.%20Fine-tuning%2C%20preference%20optimization%2C%20and%20transfer%20learning%20have%20all%0Abeen%20successfully%20employed%20for%20these%20purposes%20when%20the%20target%20task%20closely%0Aresembles%20the%20source%20task%2C%20but%20a%20precise%20theoretical%20understanding%20of%20%22task%0Asimilarity%22%20is%20still%20lacking.%20While%20conventional%20wisdom%20suggests%20that%20simple%0Ameasures%20of%20similarity%20between%20source%20and%20target%20distributions%2C%20such%20as%0A%24%5Cphi%24-divergences%20or%20integral%20probability%20metrics%2C%20can%20directly%20predict%20the%0Asuccess%20of%20transfer%2C%20we%20prove%20the%20surprising%20fact%20that%2C%20in%20general%2C%20this%20is%20not%0Athe%20case.%20We%20adopt%2C%20instead%2C%20a%20feature-centric%20viewpoint%20on%20transfer%20learning%0Aand%20establish%20a%20number%20of%20theoretical%20results%20that%20demonstrate%20that%20when%20the%0Atarget%20task%20is%20well%20represented%20by%20the%20feature%20space%20of%20the%20pre-trained%20model%2C%0Atransfer%20learning%20outperforms%20training%20from%20scratch.%20We%20study%20deep%20linear%0Anetworks%20as%20a%20minimal%20model%20of%20transfer%20learning%20in%20which%20we%20can%20analytically%0Acharacterize%20the%20transferability%20phase%20diagram%20as%20a%20function%20of%20the%20target%0Adataset%20size%20and%20the%20feature%20space%20overlap.%20For%20this%20model%2C%20we%20establish%0Arigorously%20that%20when%20the%20feature%20space%20overlap%20between%20the%20source%20and%20target%0Atasks%20is%20sufficiently%20strong%2C%20both%20linear%20transfer%20and%20fine-tuning%20improve%0Aperformance%2C%20especially%20in%20the%20low%20data%20limit.%20These%20results%20build%20on%20an%0Aemerging%20understanding%20of%20feature%20learning%20dynamics%20in%20deep%20linear%20networks%2C%0Aand%20we%20demonstrate%20numerically%20that%20the%20rigorous%20results%20we%20derive%20for%20the%0Alinear%20case%20also%20apply%20to%20nonlinear%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08194v1&entry.124074799=Read"},
{"title": "PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point\n  Supervised Oriented Object Detection", "author": "Botao Ren and Xue Yang and Yi Yu and Junwei Luo and Zhidong Deng", "abstract": "  Single point supervised oriented object detection has gained attention and\nmade initial progress within the community. Diverse from those approaches\nrelying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB\nhas shown promise due to its prior-free feature. In this paper, we propose\nPointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotated\nboxes from points without relying on any other prior. Specifically, we first\ngenerate a Class Probability Map (CPM) by training the network with non-uniform\npositive and negative sampling. We show that the CPM is able to learn the\napproximate object regions and their contours. Then, Principal Component\nAnalysis (PCA) is applied to accurately estimate the orientation and the\nboundary of objects. By further incorporating a separation mechanism, we\nresolve the confusion caused by the overlapping on the CPM, enabling its\noperation in high-density scenarios. Extensive comparisons demonstrate that our\nmethod achieves a training speed 15.58x faster and an accuracy improvement of\n11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to the\nprevious state-of-the-art, PointOBB. This significantly advances the cutting\nedge of single point supervised oriented detection in the modular track.\n", "link": "http://arxiv.org/abs/2410.08210v1", "date": "2024-10-10", "relevancy": 2.0228, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.524}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointOBB-v2%3A%20Towards%20Simpler%2C%20Faster%2C%20and%20Stronger%20Single%20Point%0A%20%20Supervised%20Oriented%20Object%20Detection&body=Title%3A%20PointOBB-v2%3A%20Towards%20Simpler%2C%20Faster%2C%20and%20Stronger%20Single%20Point%0A%20%20Supervised%20Oriented%20Object%20Detection%0AAuthor%3A%20Botao%20Ren%20and%20Xue%20Yang%20and%20Yi%20Yu%20and%20Junwei%20Luo%20and%20Zhidong%20Deng%0AAbstract%3A%20%20%20Single%20point%20supervised%20oriented%20object%20detection%20has%20gained%20attention%20and%0Amade%20initial%20progress%20within%20the%20community.%20Diverse%20from%20those%20approaches%0Arelying%20on%20one-shot%20samples%20or%20powerful%20pretrained%20models%20%28e.g.%20SAM%29%2C%20PointOBB%0Ahas%20shown%20promise%20due%20to%20its%20prior-free%20feature.%20In%20this%20paper%2C%20we%20propose%0APointOBB-v2%2C%20a%20simpler%2C%20faster%2C%20and%20stronger%20method%20to%20generate%20pseudo%20rotated%0Aboxes%20from%20points%20without%20relying%20on%20any%20other%20prior.%20Specifically%2C%20we%20first%0Agenerate%20a%20Class%20Probability%20Map%20%28CPM%29%20by%20training%20the%20network%20with%20non-uniform%0Apositive%20and%20negative%20sampling.%20We%20show%20that%20the%20CPM%20is%20able%20to%20learn%20the%0Aapproximate%20object%20regions%20and%20their%20contours.%20Then%2C%20Principal%20Component%0AAnalysis%20%28PCA%29%20is%20applied%20to%20accurately%20estimate%20the%20orientation%20and%20the%0Aboundary%20of%20objects.%20By%20further%20incorporating%20a%20separation%20mechanism%2C%20we%0Aresolve%20the%20confusion%20caused%20by%20the%20overlapping%20on%20the%20CPM%2C%20enabling%20its%0Aoperation%20in%20high-density%20scenarios.%20Extensive%20comparisons%20demonstrate%20that%20our%0Amethod%20achieves%20a%20training%20speed%2015.58x%20faster%20and%20an%20accuracy%20improvement%20of%0A11.60%25/25.15%25/21.19%25%20on%20the%20DOTA-v1.0/v1.5/v2.0%20datasets%20compared%20to%20the%0Aprevious%20state-of-the-art%2C%20PointOBB.%20This%20significantly%20advances%20the%20cutting%0Aedge%20of%20single%20point%20supervised%20oriented%20detection%20in%20the%20modular%20track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointOBB-v2%253A%2520Towards%2520Simpler%252C%2520Faster%252C%2520and%2520Stronger%2520Single%2520Point%250A%2520%2520Supervised%2520Oriented%2520Object%2520Detection%26entry.906535625%3DBotao%2520Ren%2520and%2520Xue%2520Yang%2520and%2520Yi%2520Yu%2520and%2520Junwei%2520Luo%2520and%2520Zhidong%2520Deng%26entry.1292438233%3D%2520%2520Single%2520point%2520supervised%2520oriented%2520object%2520detection%2520has%2520gained%2520attention%2520and%250Amade%2520initial%2520progress%2520within%2520the%2520community.%2520Diverse%2520from%2520those%2520approaches%250Arelying%2520on%2520one-shot%2520samples%2520or%2520powerful%2520pretrained%2520models%2520%2528e.g.%2520SAM%2529%252C%2520PointOBB%250Ahas%2520shown%2520promise%2520due%2520to%2520its%2520prior-free%2520feature.%2520In%2520this%2520paper%252C%2520we%2520propose%250APointOBB-v2%252C%2520a%2520simpler%252C%2520faster%252C%2520and%2520stronger%2520method%2520to%2520generate%2520pseudo%2520rotated%250Aboxes%2520from%2520points%2520without%2520relying%2520on%2520any%2520other%2520prior.%2520Specifically%252C%2520we%2520first%250Agenerate%2520a%2520Class%2520Probability%2520Map%2520%2528CPM%2529%2520by%2520training%2520the%2520network%2520with%2520non-uniform%250Apositive%2520and%2520negative%2520sampling.%2520We%2520show%2520that%2520the%2520CPM%2520is%2520able%2520to%2520learn%2520the%250Aapproximate%2520object%2520regions%2520and%2520their%2520contours.%2520Then%252C%2520Principal%2520Component%250AAnalysis%2520%2528PCA%2529%2520is%2520applied%2520to%2520accurately%2520estimate%2520the%2520orientation%2520and%2520the%250Aboundary%2520of%2520objects.%2520By%2520further%2520incorporating%2520a%2520separation%2520mechanism%252C%2520we%250Aresolve%2520the%2520confusion%2520caused%2520by%2520the%2520overlapping%2520on%2520the%2520CPM%252C%2520enabling%2520its%250Aoperation%2520in%2520high-density%2520scenarios.%2520Extensive%2520comparisons%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520a%2520training%2520speed%252015.58x%2520faster%2520and%2520an%2520accuracy%2520improvement%2520of%250A11.60%2525/25.15%2525/21.19%2525%2520on%2520the%2520DOTA-v1.0/v1.5/v2.0%2520datasets%2520compared%2520to%2520the%250Aprevious%2520state-of-the-art%252C%2520PointOBB.%2520This%2520significantly%2520advances%2520the%2520cutting%250Aedge%2520of%2520single%2520point%2520supervised%2520oriented%2520detection%2520in%2520the%2520modular%2520track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointOBB-v2%3A%20Towards%20Simpler%2C%20Faster%2C%20and%20Stronger%20Single%20Point%0A%20%20Supervised%20Oriented%20Object%20Detection&entry.906535625=Botao%20Ren%20and%20Xue%20Yang%20and%20Yi%20Yu%20and%20Junwei%20Luo%20and%20Zhidong%20Deng&entry.1292438233=%20%20Single%20point%20supervised%20oriented%20object%20detection%20has%20gained%20attention%20and%0Amade%20initial%20progress%20within%20the%20community.%20Diverse%20from%20those%20approaches%0Arelying%20on%20one-shot%20samples%20or%20powerful%20pretrained%20models%20%28e.g.%20SAM%29%2C%20PointOBB%0Ahas%20shown%20promise%20due%20to%20its%20prior-free%20feature.%20In%20this%20paper%2C%20we%20propose%0APointOBB-v2%2C%20a%20simpler%2C%20faster%2C%20and%20stronger%20method%20to%20generate%20pseudo%20rotated%0Aboxes%20from%20points%20without%20relying%20on%20any%20other%20prior.%20Specifically%2C%20we%20first%0Agenerate%20a%20Class%20Probability%20Map%20%28CPM%29%20by%20training%20the%20network%20with%20non-uniform%0Apositive%20and%20negative%20sampling.%20We%20show%20that%20the%20CPM%20is%20able%20to%20learn%20the%0Aapproximate%20object%20regions%20and%20their%20contours.%20Then%2C%20Principal%20Component%0AAnalysis%20%28PCA%29%20is%20applied%20to%20accurately%20estimate%20the%20orientation%20and%20the%0Aboundary%20of%20objects.%20By%20further%20incorporating%20a%20separation%20mechanism%2C%20we%0Aresolve%20the%20confusion%20caused%20by%20the%20overlapping%20on%20the%20CPM%2C%20enabling%20its%0Aoperation%20in%20high-density%20scenarios.%20Extensive%20comparisons%20demonstrate%20that%20our%0Amethod%20achieves%20a%20training%20speed%2015.58x%20faster%20and%20an%20accuracy%20improvement%20of%0A11.60%25/25.15%25/21.19%25%20on%20the%20DOTA-v1.0/v1.5/v2.0%20datasets%20compared%20to%20the%0Aprevious%20state-of-the-art%2C%20PointOBB.%20This%20significantly%20advances%20the%20cutting%0Aedge%20of%20single%20point%20supervised%20oriented%20detection%20in%20the%20modular%20track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08210v1&entry.124074799=Read"},
{"title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image\n  Generation", "author": "Jiatao Gu and Yuyang Wang and Yizhe Zhang and Qihang Zhang and Dinghuai Zhang and Navdeep Jaitly and Josh Susskind and Shuangfei Zhai", "abstract": "  Diffusion models have become the dominant approach for visual generation.\nThey are trained by denoising a Markovian process that gradually adds noise to\nthe input. We argue that the Markovian property limits the models ability to\nfully utilize the generation trajectory, leading to inefficiencies during\ntraining and inference. In this paper, we propose DART, a transformer-based\nmodel that unifies autoregressive (AR) and diffusion within a non-Markovian\nframework. DART iteratively denoises image patches spatially and spectrally\nusing an AR model with the same architecture as standard language models. DART\ndoes not rely on image quantization, enabling more effective image modeling\nwhile maintaining flexibility. Furthermore, DART seamlessly trains with both\ntext and image data in a unified model. Our approach demonstrates competitive\nperformance on class-conditioned and text-to-image generation tasks, offering a\nscalable, efficient alternative to traditional diffusion models. Through this\nunified framework, DART sets a new benchmark for scalable, high-quality image\nsynthesis.\n", "link": "http://arxiv.org/abs/2410.08159v1", "date": "2024-10-10", "relevancy": 2.0187, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7155}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6717}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART%3A%20Denoising%20Autoregressive%20Transformer%20for%20Scalable%20Text-to-Image%0A%20%20Generation&body=Title%3A%20DART%3A%20Denoising%20Autoregressive%20Transformer%20for%20Scalable%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Jiatao%20Gu%20and%20Yuyang%20Wang%20and%20Yizhe%20Zhang%20and%20Qihang%20Zhang%20and%20Dinghuai%20Zhang%20and%20Navdeep%20Jaitly%20and%20Josh%20Susskind%20and%20Shuangfei%20Zhai%0AAbstract%3A%20%20%20Diffusion%20models%20have%20become%20the%20dominant%20approach%20for%20visual%20generation.%0AThey%20are%20trained%20by%20denoising%20a%20Markovian%20process%20that%20gradually%20adds%20noise%20to%0Athe%20input.%20We%20argue%20that%20the%20Markovian%20property%20limits%20the%20models%20ability%20to%0Afully%20utilize%20the%20generation%20trajectory%2C%20leading%20to%20inefficiencies%20during%0Atraining%20and%20inference.%20In%20this%20paper%2C%20we%20propose%20DART%2C%20a%20transformer-based%0Amodel%20that%20unifies%20autoregressive%20%28AR%29%20and%20diffusion%20within%20a%20non-Markovian%0Aframework.%20DART%20iteratively%20denoises%20image%20patches%20spatially%20and%20spectrally%0Ausing%20an%20AR%20model%20with%20the%20same%20architecture%20as%20standard%20language%20models.%20DART%0Adoes%20not%20rely%20on%20image%20quantization%2C%20enabling%20more%20effective%20image%20modeling%0Awhile%20maintaining%20flexibility.%20Furthermore%2C%20DART%20seamlessly%20trains%20with%20both%0Atext%20and%20image%20data%20in%20a%20unified%20model.%20Our%20approach%20demonstrates%20competitive%0Aperformance%20on%20class-conditioned%20and%20text-to-image%20generation%20tasks%2C%20offering%20a%0Ascalable%2C%20efficient%20alternative%20to%20traditional%20diffusion%20models.%20Through%20this%0Aunified%20framework%2C%20DART%20sets%20a%20new%20benchmark%20for%20scalable%2C%20high-quality%20image%0Asynthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART%253A%2520Denoising%2520Autoregressive%2520Transformer%2520for%2520Scalable%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DJiatao%2520Gu%2520and%2520Yuyang%2520Wang%2520and%2520Yizhe%2520Zhang%2520and%2520Qihang%2520Zhang%2520and%2520Dinghuai%2520Zhang%2520and%2520Navdeep%2520Jaitly%2520and%2520Josh%2520Susskind%2520and%2520Shuangfei%2520Zhai%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520become%2520the%2520dominant%2520approach%2520for%2520visual%2520generation.%250AThey%2520are%2520trained%2520by%2520denoising%2520a%2520Markovian%2520process%2520that%2520gradually%2520adds%2520noise%2520to%250Athe%2520input.%2520We%2520argue%2520that%2520the%2520Markovian%2520property%2520limits%2520the%2520models%2520ability%2520to%250Afully%2520utilize%2520the%2520generation%2520trajectory%252C%2520leading%2520to%2520inefficiencies%2520during%250Atraining%2520and%2520inference.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DART%252C%2520a%2520transformer-based%250Amodel%2520that%2520unifies%2520autoregressive%2520%2528AR%2529%2520and%2520diffusion%2520within%2520a%2520non-Markovian%250Aframework.%2520DART%2520iteratively%2520denoises%2520image%2520patches%2520spatially%2520and%2520spectrally%250Ausing%2520an%2520AR%2520model%2520with%2520the%2520same%2520architecture%2520as%2520standard%2520language%2520models.%2520DART%250Adoes%2520not%2520rely%2520on%2520image%2520quantization%252C%2520enabling%2520more%2520effective%2520image%2520modeling%250Awhile%2520maintaining%2520flexibility.%2520Furthermore%252C%2520DART%2520seamlessly%2520trains%2520with%2520both%250Atext%2520and%2520image%2520data%2520in%2520a%2520unified%2520model.%2520Our%2520approach%2520demonstrates%2520competitive%250Aperformance%2520on%2520class-conditioned%2520and%2520text-to-image%2520generation%2520tasks%252C%2520offering%2520a%250Ascalable%252C%2520efficient%2520alternative%2520to%2520traditional%2520diffusion%2520models.%2520Through%2520this%250Aunified%2520framework%252C%2520DART%2520sets%2520a%2520new%2520benchmark%2520for%2520scalable%252C%2520high-quality%2520image%250Asynthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20Denoising%20Autoregressive%20Transformer%20for%20Scalable%20Text-to-Image%0A%20%20Generation&entry.906535625=Jiatao%20Gu%20and%20Yuyang%20Wang%20and%20Yizhe%20Zhang%20and%20Qihang%20Zhang%20and%20Dinghuai%20Zhang%20and%20Navdeep%20Jaitly%20and%20Josh%20Susskind%20and%20Shuangfei%20Zhai&entry.1292438233=%20%20Diffusion%20models%20have%20become%20the%20dominant%20approach%20for%20visual%20generation.%0AThey%20are%20trained%20by%20denoising%20a%20Markovian%20process%20that%20gradually%20adds%20noise%20to%0Athe%20input.%20We%20argue%20that%20the%20Markovian%20property%20limits%20the%20models%20ability%20to%0Afully%20utilize%20the%20generation%20trajectory%2C%20leading%20to%20inefficiencies%20during%0Atraining%20and%20inference.%20In%20this%20paper%2C%20we%20propose%20DART%2C%20a%20transformer-based%0Amodel%20that%20unifies%20autoregressive%20%28AR%29%20and%20diffusion%20within%20a%20non-Markovian%0Aframework.%20DART%20iteratively%20denoises%20image%20patches%20spatially%20and%20spectrally%0Ausing%20an%20AR%20model%20with%20the%20same%20architecture%20as%20standard%20language%20models.%20DART%0Adoes%20not%20rely%20on%20image%20quantization%2C%20enabling%20more%20effective%20image%20modeling%0Awhile%20maintaining%20flexibility.%20Furthermore%2C%20DART%20seamlessly%20trains%20with%20both%0Atext%20and%20image%20data%20in%20a%20unified%20model.%20Our%20approach%20demonstrates%20competitive%0Aperformance%20on%20class-conditioned%20and%20text-to-image%20generation%20tasks%2C%20offering%20a%0Ascalable%2C%20efficient%20alternative%20to%20traditional%20diffusion%20models.%20Through%20this%0Aunified%20framework%2C%20DART%20sets%20a%20new%20benchmark%20for%20scalable%2C%20high-quality%20image%0Asynthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08159v1&entry.124074799=Read"},
{"title": "SelfFed: Self-supervised Federated Learning for Data Heterogeneity and\n  Label Scarcity in IoMT", "author": "Sunder Ali Khowaja and Kapal Dev and Syed Muhammad Anwar and Marius George Linguraru", "abstract": "  Self-supervised learning in federated learning paradigm has been gaining a\nlot of interest both in industry and research due to the collaborative learning\ncapability on unlabeled yet isolated data. However, self-supervised based\nfederated learning strategies suffer from performance degradation due to label\nscarcity and diverse data distributions, i.e., data heterogeneity. In this\npaper, we propose the SelfFed framework for Internet of Medical Things (IoMT).\nOur proposed SelfFed framework works in two phases. The first phase is the\npre-training paradigm that performs augmentive modeling using Swin Transformer\nbased encoder in a decentralized manner. The first phase of SelfFed framework\nhelps to overcome the data heterogeneity issue. The second phase is the\nfine-tuning paradigm that introduces contrastive network and a novel\naggregation strategy that is trained on limited labeled data for a target task\nin a decentralized manner. This fine-tuning stage overcomes the label scarcity\nproblem. We perform our experimental analysis on publicly available medical\nimaging datasets and show that our proposed SelfFed framework performs better\nwhen compared to existing baselines concerning non-independent and identically\ndistributed (IID) data and label scarcity. Our method achieves a maximum\nimprovement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID\ndataset. Further, our proposed method outperforms existing baselines even when\ntrained on a few (10%) labeled instances.\n", "link": "http://arxiv.org/abs/2307.01514v2", "date": "2024-10-10", "relevancy": 2.0163, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5087}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfFed%3A%20Self-supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20IoMT&body=Title%3A%20SelfFed%3A%20Self-supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20IoMT%0AAuthor%3A%20Sunder%20Ali%20Khowaja%20and%20Kapal%20Dev%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru%0AAbstract%3A%20%20%20Self-supervised%20learning%20in%20federated%20learning%20paradigm%20has%20been%20gaining%20a%0Alot%20of%20interest%20both%20in%20industry%20and%20research%20due%20to%20the%20collaborative%20learning%0Acapability%20on%20unlabeled%20yet%20isolated%20data.%20However%2C%20self-supervised%20based%0Afederated%20learning%20strategies%20suffer%20from%20performance%20degradation%20due%20to%20label%0Ascarcity%20and%20diverse%20data%20distributions%2C%20i.e.%2C%20data%20heterogeneity.%20In%20this%0Apaper%2C%20we%20propose%20the%20SelfFed%20framework%20for%20Internet%20of%20Medical%20Things%20%28IoMT%29.%0AOur%20proposed%20SelfFed%20framework%20works%20in%20two%20phases.%20The%20first%20phase%20is%20the%0Apre-training%20paradigm%20that%20performs%20augmentive%20modeling%20using%20Swin%20Transformer%0Abased%20encoder%20in%20a%20decentralized%20manner.%20The%20first%20phase%20of%20SelfFed%20framework%0Ahelps%20to%20overcome%20the%20data%20heterogeneity%20issue.%20The%20second%20phase%20is%20the%0Afine-tuning%20paradigm%20that%20introduces%20contrastive%20network%20and%20a%20novel%0Aaggregation%20strategy%20that%20is%20trained%20on%20limited%20labeled%20data%20for%20a%20target%20task%0Ain%20a%20decentralized%20manner.%20This%20fine-tuning%20stage%20overcomes%20the%20label%20scarcity%0Aproblem.%20We%20perform%20our%20experimental%20analysis%20on%20publicly%20available%20medical%0Aimaging%20datasets%20and%20show%20that%20our%20proposed%20SelfFed%20framework%20performs%20better%0Awhen%20compared%20to%20existing%20baselines%20concerning%20non-independent%20and%20identically%0Adistributed%20%28IID%29%20data%20and%20label%20scarcity.%20Our%20method%20achieves%20a%20maximum%0Aimprovement%20of%208.8%25%20and%204.1%25%20on%20Retina%20and%20COVID-FL%20datasets%20on%20non-IID%0Adataset.%20Further%2C%20our%20proposed%20method%20outperforms%20existing%20baselines%20even%20when%0Atrained%20on%20a%20few%20%2810%25%29%20labeled%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfFed%253A%2520Self-supervised%2520Federated%2520Learning%2520for%2520Data%2520Heterogeneity%2520and%250A%2520%2520Label%2520Scarcity%2520in%2520IoMT%26entry.906535625%3DSunder%2520Ali%2520Khowaja%2520and%2520Kapal%2520Dev%2520and%2520Syed%2520Muhammad%2520Anwar%2520and%2520Marius%2520George%2520Linguraru%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520in%2520federated%2520learning%2520paradigm%2520has%2520been%2520gaining%2520a%250Alot%2520of%2520interest%2520both%2520in%2520industry%2520and%2520research%2520due%2520to%2520the%2520collaborative%2520learning%250Acapability%2520on%2520unlabeled%2520yet%2520isolated%2520data.%2520However%252C%2520self-supervised%2520based%250Afederated%2520learning%2520strategies%2520suffer%2520from%2520performance%2520degradation%2520due%2520to%2520label%250Ascarcity%2520and%2520diverse%2520data%2520distributions%252C%2520i.e.%252C%2520data%2520heterogeneity.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520SelfFed%2520framework%2520for%2520Internet%2520of%2520Medical%2520Things%2520%2528IoMT%2529.%250AOur%2520proposed%2520SelfFed%2520framework%2520works%2520in%2520two%2520phases.%2520The%2520first%2520phase%2520is%2520the%250Apre-training%2520paradigm%2520that%2520performs%2520augmentive%2520modeling%2520using%2520Swin%2520Transformer%250Abased%2520encoder%2520in%2520a%2520decentralized%2520manner.%2520The%2520first%2520phase%2520of%2520SelfFed%2520framework%250Ahelps%2520to%2520overcome%2520the%2520data%2520heterogeneity%2520issue.%2520The%2520second%2520phase%2520is%2520the%250Afine-tuning%2520paradigm%2520that%2520introduces%2520contrastive%2520network%2520and%2520a%2520novel%250Aaggregation%2520strategy%2520that%2520is%2520trained%2520on%2520limited%2520labeled%2520data%2520for%2520a%2520target%2520task%250Ain%2520a%2520decentralized%2520manner.%2520This%2520fine-tuning%2520stage%2520overcomes%2520the%2520label%2520scarcity%250Aproblem.%2520We%2520perform%2520our%2520experimental%2520analysis%2520on%2520publicly%2520available%2520medical%250Aimaging%2520datasets%2520and%2520show%2520that%2520our%2520proposed%2520SelfFed%2520framework%2520performs%2520better%250Awhen%2520compared%2520to%2520existing%2520baselines%2520concerning%2520non-independent%2520and%2520identically%250Adistributed%2520%2528IID%2529%2520data%2520and%2520label%2520scarcity.%2520Our%2520method%2520achieves%2520a%2520maximum%250Aimprovement%2520of%25208.8%2525%2520and%25204.1%2525%2520on%2520Retina%2520and%2520COVID-FL%2520datasets%2520on%2520non-IID%250Adataset.%2520Further%252C%2520our%2520proposed%2520method%2520outperforms%2520existing%2520baselines%2520even%2520when%250Atrained%2520on%2520a%2520few%2520%252810%2525%2529%2520labeled%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfFed%3A%20Self-supervised%20Federated%20Learning%20for%20Data%20Heterogeneity%20and%0A%20%20Label%20Scarcity%20in%20IoMT&entry.906535625=Sunder%20Ali%20Khowaja%20and%20Kapal%20Dev%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru&entry.1292438233=%20%20Self-supervised%20learning%20in%20federated%20learning%20paradigm%20has%20been%20gaining%20a%0Alot%20of%20interest%20both%20in%20industry%20and%20research%20due%20to%20the%20collaborative%20learning%0Acapability%20on%20unlabeled%20yet%20isolated%20data.%20However%2C%20self-supervised%20based%0Afederated%20learning%20strategies%20suffer%20from%20performance%20degradation%20due%20to%20label%0Ascarcity%20and%20diverse%20data%20distributions%2C%20i.e.%2C%20data%20heterogeneity.%20In%20this%0Apaper%2C%20we%20propose%20the%20SelfFed%20framework%20for%20Internet%20of%20Medical%20Things%20%28IoMT%29.%0AOur%20proposed%20SelfFed%20framework%20works%20in%20two%20phases.%20The%20first%20phase%20is%20the%0Apre-training%20paradigm%20that%20performs%20augmentive%20modeling%20using%20Swin%20Transformer%0Abased%20encoder%20in%20a%20decentralized%20manner.%20The%20first%20phase%20of%20SelfFed%20framework%0Ahelps%20to%20overcome%20the%20data%20heterogeneity%20issue.%20The%20second%20phase%20is%20the%0Afine-tuning%20paradigm%20that%20introduces%20contrastive%20network%20and%20a%20novel%0Aaggregation%20strategy%20that%20is%20trained%20on%20limited%20labeled%20data%20for%20a%20target%20task%0Ain%20a%20decentralized%20manner.%20This%20fine-tuning%20stage%20overcomes%20the%20label%20scarcity%0Aproblem.%20We%20perform%20our%20experimental%20analysis%20on%20publicly%20available%20medical%0Aimaging%20datasets%20and%20show%20that%20our%20proposed%20SelfFed%20framework%20performs%20better%0Awhen%20compared%20to%20existing%20baselines%20concerning%20non-independent%20and%20identically%0Adistributed%20%28IID%29%20data%20and%20label%20scarcity.%20Our%20method%20achieves%20a%20maximum%0Aimprovement%20of%208.8%25%20and%204.1%25%20on%20Retina%20and%20COVID-FL%20datasets%20on%20non-IID%0Adataset.%20Further%2C%20our%20proposed%20method%20outperforms%20existing%20baselines%20even%20when%0Atrained%20on%20a%20few%20%2810%25%29%20labeled%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01514v2&entry.124074799=Read"},
{"title": "The Last Iterate Advantage: Empirical Auditing and Principled Heuristic\n  Analysis of Differentially Private SGD", "author": "Thomas Steinke and Milad Nasr and Arun Ganesh and Borja Balle and Christopher A. Choquette-Choo and Matthew Jagielski and Jamie Hayes and Abhradeep Guha Thakurta and Adam Smith and Andreas Terzis", "abstract": "  We propose a simple heuristic privacy analysis of noisy clipped stochastic\ngradient descent (DP-SGD) in the setting where only the last iterate is\nreleased and the intermediate iterates remain hidden. Namely, our heuristic\nassumes a linear structure for the model.\n  We show experimentally that our heuristic is predictive of the outcome of\nprivacy auditing applied to various training procedures. Thus it can be used\nprior to training as a rough estimate of the final privacy leakage. We also\nprobe the limitations of our heuristic by providing some artificial\ncounterexamples where it underestimates the privacy leakage.\n  The standard composition-based privacy analysis of DP-SGD effectively assumes\nthat the adversary has access to all intermediate iterates, which is often\nunrealistic. However, this analysis remains the state of the art in practice.\nWhile our heuristic does not replace a rigorous privacy analysis, it\nillustrates the large gap between the best theoretical upper bounds and the\nprivacy auditing lower bounds and sets a target for further work to improve the\ntheoretical privacy analyses. We also empirically support our heuristic and\nshow existing privacy auditing attacks are bounded by our heuristic analysis in\nboth vision and language tasks.\n", "link": "http://arxiv.org/abs/2410.06186v2", "date": "2024-10-10", "relevancy": 1.857, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4638}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Last%20Iterate%20Advantage%3A%20Empirical%20Auditing%20and%20Principled%20Heuristic%0A%20%20Analysis%20of%20Differentially%20Private%20SGD&body=Title%3A%20The%20Last%20Iterate%20Advantage%3A%20Empirical%20Auditing%20and%20Principled%20Heuristic%0A%20%20Analysis%20of%20Differentially%20Private%20SGD%0AAuthor%3A%20Thomas%20Steinke%20and%20Milad%20Nasr%20and%20Arun%20Ganesh%20and%20Borja%20Balle%20and%20Christopher%20A.%20Choquette-Choo%20and%20Matthew%20Jagielski%20and%20Jamie%20Hayes%20and%20Abhradeep%20Guha%20Thakurta%20and%20Adam%20Smith%20and%20Andreas%20Terzis%0AAbstract%3A%20%20%20We%20propose%20a%20simple%20heuristic%20privacy%20analysis%20of%20noisy%20clipped%20stochastic%0Agradient%20descent%20%28DP-SGD%29%20in%20the%20setting%20where%20only%20the%20last%20iterate%20is%0Areleased%20and%20the%20intermediate%20iterates%20remain%20hidden.%20Namely%2C%20our%20heuristic%0Aassumes%20a%20linear%20structure%20for%20the%20model.%0A%20%20We%20show%20experimentally%20that%20our%20heuristic%20is%20predictive%20of%20the%20outcome%20of%0Aprivacy%20auditing%20applied%20to%20various%20training%20procedures.%20Thus%20it%20can%20be%20used%0Aprior%20to%20training%20as%20a%20rough%20estimate%20of%20the%20final%20privacy%20leakage.%20We%20also%0Aprobe%20the%20limitations%20of%20our%20heuristic%20by%20providing%20some%20artificial%0Acounterexamples%20where%20it%20underestimates%20the%20privacy%20leakage.%0A%20%20The%20standard%20composition-based%20privacy%20analysis%20of%20DP-SGD%20effectively%20assumes%0Athat%20the%20adversary%20has%20access%20to%20all%20intermediate%20iterates%2C%20which%20is%20often%0Aunrealistic.%20However%2C%20this%20analysis%20remains%20the%20state%20of%20the%20art%20in%20practice.%0AWhile%20our%20heuristic%20does%20not%20replace%20a%20rigorous%20privacy%20analysis%2C%20it%0Aillustrates%20the%20large%20gap%20between%20the%20best%20theoretical%20upper%20bounds%20and%20the%0Aprivacy%20auditing%20lower%20bounds%20and%20sets%20a%20target%20for%20further%20work%20to%20improve%20the%0Atheoretical%20privacy%20analyses.%20We%20also%20empirically%20support%20our%20heuristic%20and%0Ashow%20existing%20privacy%20auditing%20attacks%20are%20bounded%20by%20our%20heuristic%20analysis%20in%0Aboth%20vision%20and%20language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06186v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Last%2520Iterate%2520Advantage%253A%2520Empirical%2520Auditing%2520and%2520Principled%2520Heuristic%250A%2520%2520Analysis%2520of%2520Differentially%2520Private%2520SGD%26entry.906535625%3DThomas%2520Steinke%2520and%2520Milad%2520Nasr%2520and%2520Arun%2520Ganesh%2520and%2520Borja%2520Balle%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Matthew%2520Jagielski%2520and%2520Jamie%2520Hayes%2520and%2520Abhradeep%2520Guha%2520Thakurta%2520and%2520Adam%2520Smith%2520and%2520Andreas%2520Terzis%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520simple%2520heuristic%2520privacy%2520analysis%2520of%2520noisy%2520clipped%2520stochastic%250Agradient%2520descent%2520%2528DP-SGD%2529%2520in%2520the%2520setting%2520where%2520only%2520the%2520last%2520iterate%2520is%250Areleased%2520and%2520the%2520intermediate%2520iterates%2520remain%2520hidden.%2520Namely%252C%2520our%2520heuristic%250Aassumes%2520a%2520linear%2520structure%2520for%2520the%2520model.%250A%2520%2520We%2520show%2520experimentally%2520that%2520our%2520heuristic%2520is%2520predictive%2520of%2520the%2520outcome%2520of%250Aprivacy%2520auditing%2520applied%2520to%2520various%2520training%2520procedures.%2520Thus%2520it%2520can%2520be%2520used%250Aprior%2520to%2520training%2520as%2520a%2520rough%2520estimate%2520of%2520the%2520final%2520privacy%2520leakage.%2520We%2520also%250Aprobe%2520the%2520limitations%2520of%2520our%2520heuristic%2520by%2520providing%2520some%2520artificial%250Acounterexamples%2520where%2520it%2520underestimates%2520the%2520privacy%2520leakage.%250A%2520%2520The%2520standard%2520composition-based%2520privacy%2520analysis%2520of%2520DP-SGD%2520effectively%2520assumes%250Athat%2520the%2520adversary%2520has%2520access%2520to%2520all%2520intermediate%2520iterates%252C%2520which%2520is%2520often%250Aunrealistic.%2520However%252C%2520this%2520analysis%2520remains%2520the%2520state%2520of%2520the%2520art%2520in%2520practice.%250AWhile%2520our%2520heuristic%2520does%2520not%2520replace%2520a%2520rigorous%2520privacy%2520analysis%252C%2520it%250Aillustrates%2520the%2520large%2520gap%2520between%2520the%2520best%2520theoretical%2520upper%2520bounds%2520and%2520the%250Aprivacy%2520auditing%2520lower%2520bounds%2520and%2520sets%2520a%2520target%2520for%2520further%2520work%2520to%2520improve%2520the%250Atheoretical%2520privacy%2520analyses.%2520We%2520also%2520empirically%2520support%2520our%2520heuristic%2520and%250Ashow%2520existing%2520privacy%2520auditing%2520attacks%2520are%2520bounded%2520by%2520our%2520heuristic%2520analysis%2520in%250Aboth%2520vision%2520and%2520language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06186v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Last%20Iterate%20Advantage%3A%20Empirical%20Auditing%20and%20Principled%20Heuristic%0A%20%20Analysis%20of%20Differentially%20Private%20SGD&entry.906535625=Thomas%20Steinke%20and%20Milad%20Nasr%20and%20Arun%20Ganesh%20and%20Borja%20Balle%20and%20Christopher%20A.%20Choquette-Choo%20and%20Matthew%20Jagielski%20and%20Jamie%20Hayes%20and%20Abhradeep%20Guha%20Thakurta%20and%20Adam%20Smith%20and%20Andreas%20Terzis&entry.1292438233=%20%20We%20propose%20a%20simple%20heuristic%20privacy%20analysis%20of%20noisy%20clipped%20stochastic%0Agradient%20descent%20%28DP-SGD%29%20in%20the%20setting%20where%20only%20the%20last%20iterate%20is%0Areleased%20and%20the%20intermediate%20iterates%20remain%20hidden.%20Namely%2C%20our%20heuristic%0Aassumes%20a%20linear%20structure%20for%20the%20model.%0A%20%20We%20show%20experimentally%20that%20our%20heuristic%20is%20predictive%20of%20the%20outcome%20of%0Aprivacy%20auditing%20applied%20to%20various%20training%20procedures.%20Thus%20it%20can%20be%20used%0Aprior%20to%20training%20as%20a%20rough%20estimate%20of%20the%20final%20privacy%20leakage.%20We%20also%0Aprobe%20the%20limitations%20of%20our%20heuristic%20by%20providing%20some%20artificial%0Acounterexamples%20where%20it%20underestimates%20the%20privacy%20leakage.%0A%20%20The%20standard%20composition-based%20privacy%20analysis%20of%20DP-SGD%20effectively%20assumes%0Athat%20the%20adversary%20has%20access%20to%20all%20intermediate%20iterates%2C%20which%20is%20often%0Aunrealistic.%20However%2C%20this%20analysis%20remains%20the%20state%20of%20the%20art%20in%20practice.%0AWhile%20our%20heuristic%20does%20not%20replace%20a%20rigorous%20privacy%20analysis%2C%20it%0Aillustrates%20the%20large%20gap%20between%20the%20best%20theoretical%20upper%20bounds%20and%20the%0Aprivacy%20auditing%20lower%20bounds%20and%20sets%20a%20target%20for%20further%20work%20to%20improve%20the%0Atheoretical%20privacy%20analyses.%20We%20also%20empirically%20support%20our%20heuristic%20and%0Ashow%20existing%20privacy%20auditing%20attacks%20are%20bounded%20by%20our%20heuristic%20analysis%20in%0Aboth%20vision%20and%20language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06186v2&entry.124074799=Read"},
{"title": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning", "author": "Kamesh R", "abstract": "  This paper presents Dynamic Prompting, a novel framework aimed at improving\nthe reasoning capabilities of Large Language Models (LLMs). In contrast to\nconventional static prompting methods, Dynamic Prompting enables the adaptive\nmodification of prompt sequences and step counts based on real-time task\ncomplexity and model performance. This dynamic adaptation facilitates more\nefficient problem-solving, particularly in smaller models, by reducing\nhallucinations and repetitive cycles. Our empirical evaluations demonstrate\nthat Dynamic Prompting allows smaller LLMs to perform competitively with much\nlarger models, thereby challenging the conventional emphasis on model size as\nthe primary determinant of reasoning efficacy.\n", "link": "http://arxiv.org/abs/2410.08130v1", "date": "2024-10-10", "relevancy": 1.9164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Beyond%20Size%3A%20Dynamic%20Prompting%20for%20More%20Effective%20Reasoning&body=Title%3A%20Think%20Beyond%20Size%3A%20Dynamic%20Prompting%20for%20More%20Effective%20Reasoning%0AAuthor%3A%20Kamesh%20R%0AAbstract%3A%20%20%20This%20paper%20presents%20Dynamic%20Prompting%2C%20a%20novel%20framework%20aimed%20at%20improving%0Athe%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%20contrast%20to%0Aconventional%20static%20prompting%20methods%2C%20Dynamic%20Prompting%20enables%20the%20adaptive%0Amodification%20of%20prompt%20sequences%20and%20step%20counts%20based%20on%20real-time%20task%0Acomplexity%20and%20model%20performance.%20This%20dynamic%20adaptation%20facilitates%20more%0Aefficient%20problem-solving%2C%20particularly%20in%20smaller%20models%2C%20by%20reducing%0Ahallucinations%20and%20repetitive%20cycles.%20Our%20empirical%20evaluations%20demonstrate%0Athat%20Dynamic%20Prompting%20allows%20smaller%20LLMs%20to%20perform%20competitively%20with%20much%0Alarger%20models%2C%20thereby%20challenging%20the%20conventional%20emphasis%20on%20model%20size%20as%0Athe%20primary%20determinant%20of%20reasoning%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Beyond%2520Size%253A%2520Dynamic%2520Prompting%2520for%2520More%2520Effective%2520Reasoning%26entry.906535625%3DKamesh%2520R%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Dynamic%2520Prompting%252C%2520a%2520novel%2520framework%2520aimed%2520at%2520improving%250Athe%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520In%2520contrast%2520to%250Aconventional%2520static%2520prompting%2520methods%252C%2520Dynamic%2520Prompting%2520enables%2520the%2520adaptive%250Amodification%2520of%2520prompt%2520sequences%2520and%2520step%2520counts%2520based%2520on%2520real-time%2520task%250Acomplexity%2520and%2520model%2520performance.%2520This%2520dynamic%2520adaptation%2520facilitates%2520more%250Aefficient%2520problem-solving%252C%2520particularly%2520in%2520smaller%2520models%252C%2520by%2520reducing%250Ahallucinations%2520and%2520repetitive%2520cycles.%2520Our%2520empirical%2520evaluations%2520demonstrate%250Athat%2520Dynamic%2520Prompting%2520allows%2520smaller%2520LLMs%2520to%2520perform%2520competitively%2520with%2520much%250Alarger%2520models%252C%2520thereby%2520challenging%2520the%2520conventional%2520emphasis%2520on%2520model%2520size%2520as%250Athe%2520primary%2520determinant%2520of%2520reasoning%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Beyond%20Size%3A%20Dynamic%20Prompting%20for%20More%20Effective%20Reasoning&entry.906535625=Kamesh%20R&entry.1292438233=%20%20This%20paper%20presents%20Dynamic%20Prompting%2C%20a%20novel%20framework%20aimed%20at%20improving%0Athe%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%20contrast%20to%0Aconventional%20static%20prompting%20methods%2C%20Dynamic%20Prompting%20enables%20the%20adaptive%0Amodification%20of%20prompt%20sequences%20and%20step%20counts%20based%20on%20real-time%20task%0Acomplexity%20and%20model%20performance.%20This%20dynamic%20adaptation%20facilitates%20more%0Aefficient%20problem-solving%2C%20particularly%20in%20smaller%20models%2C%20by%20reducing%0Ahallucinations%20and%20repetitive%20cycles.%20Our%20empirical%20evaluations%20demonstrate%0Athat%20Dynamic%20Prompting%20allows%20smaller%20LLMs%20to%20perform%20competitively%20with%20much%0Alarger%20models%2C%20thereby%20challenging%20the%20conventional%20emphasis%20on%20model%20size%20as%0Athe%20primary%20determinant%20of%20reasoning%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08130v1&entry.124074799=Read"},
{"title": "Advocating for the Silent: Enhancing Federated Generalization for\n  Non-Participating Clients", "author": "Zheshun Wu and Zenglin Xu and Dun Zeng and Qifan Wang and Jie Liu", "abstract": "  Federated Learning (FL) has surged in prominence due to its capability of\ncollaborative model training without direct data sharing. However, the vast\ndisparity in local data distributions among clients, often termed the\nNon-Independent Identically Distributed (Non-IID) challenge, poses a\nsignificant hurdle to FL's generalization efficacy. The scenario becomes even\nmore complex when not all clients participate in the training process, a common\noccurrence due to unstable network connections or limited computational\ncapacities. This can greatly complicate the assessment of the trained models'\ngeneralization abilities. While a plethora of recent studies has centered on\nthe generalization gap pertaining to unseen data from participating clients\nwith diverse distributions, the distinction between the training distributions\nof participating clients and the testing distributions of non-participating\nones has been largely overlooked. In response, our paper unveils an\ninformation-theoretic generalization framework for FL. Specifically, it\nquantifies generalization errors by evaluating the information entropy of local\ndistributions and discerning discrepancies across these distributions. Inspired\nby our deduced generalization bounds, we introduce a weighted aggregation\napproach and a duo of client selection strategies. These innovations are\ndesigned to strengthen FL's ability to generalize and thus ensure that trained\nmodels perform better on non-participating clients by incorporating a more\ndiverse range of client data distributions. Our extensive empirical evaluations\nreaffirm the potency of our proposed methods, aligning seamlessly with our\ntheoretical construct.\n", "link": "http://arxiv.org/abs/2310.07171v6", "date": "2024-10-10", "relevancy": 1.8978, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4789}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4762}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advocating%20for%20the%20Silent%3A%20Enhancing%20Federated%20Generalization%20for%0A%20%20Non-Participating%20Clients&body=Title%3A%20Advocating%20for%20the%20Silent%3A%20Enhancing%20Federated%20Generalization%20for%0A%20%20Non-Participating%20Clients%0AAuthor%3A%20Zheshun%20Wu%20and%20Zenglin%20Xu%20and%20Dun%20Zeng%20and%20Qifan%20Wang%20and%20Jie%20Liu%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20surged%20in%20prominence%20due%20to%20its%20capability%20of%0Acollaborative%20model%20training%20without%20direct%20data%20sharing.%20However%2C%20the%20vast%0Adisparity%20in%20local%20data%20distributions%20among%20clients%2C%20often%20termed%20the%0ANon-Independent%20Identically%20Distributed%20%28Non-IID%29%20challenge%2C%20poses%20a%0Asignificant%20hurdle%20to%20FL%27s%20generalization%20efficacy.%20The%20scenario%20becomes%20even%0Amore%20complex%20when%20not%20all%20clients%20participate%20in%20the%20training%20process%2C%20a%20common%0Aoccurrence%20due%20to%20unstable%20network%20connections%20or%20limited%20computational%0Acapacities.%20This%20can%20greatly%20complicate%20the%20assessment%20of%20the%20trained%20models%27%0Ageneralization%20abilities.%20While%20a%20plethora%20of%20recent%20studies%20has%20centered%20on%0Athe%20generalization%20gap%20pertaining%20to%20unseen%20data%20from%20participating%20clients%0Awith%20diverse%20distributions%2C%20the%20distinction%20between%20the%20training%20distributions%0Aof%20participating%20clients%20and%20the%20testing%20distributions%20of%20non-participating%0Aones%20has%20been%20largely%20overlooked.%20In%20response%2C%20our%20paper%20unveils%20an%0Ainformation-theoretic%20generalization%20framework%20for%20FL.%20Specifically%2C%20it%0Aquantifies%20generalization%20errors%20by%20evaluating%20the%20information%20entropy%20of%20local%0Adistributions%20and%20discerning%20discrepancies%20across%20these%20distributions.%20Inspired%0Aby%20our%20deduced%20generalization%20bounds%2C%20we%20introduce%20a%20weighted%20aggregation%0Aapproach%20and%20a%20duo%20of%20client%20selection%20strategies.%20These%20innovations%20are%0Adesigned%20to%20strengthen%20FL%27s%20ability%20to%20generalize%20and%20thus%20ensure%20that%20trained%0Amodels%20perform%20better%20on%20non-participating%20clients%20by%20incorporating%20a%20more%0Adiverse%20range%20of%20client%20data%20distributions.%20Our%20extensive%20empirical%20evaluations%0Areaffirm%20the%20potency%20of%20our%20proposed%20methods%2C%20aligning%20seamlessly%20with%20our%0Atheoretical%20construct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07171v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvocating%2520for%2520the%2520Silent%253A%2520Enhancing%2520Federated%2520Generalization%2520for%250A%2520%2520Non-Participating%2520Clients%26entry.906535625%3DZheshun%2520Wu%2520and%2520Zenglin%2520Xu%2520and%2520Dun%2520Zeng%2520and%2520Qifan%2520Wang%2520and%2520Jie%2520Liu%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520surged%2520in%2520prominence%2520due%2520to%2520its%2520capability%2520of%250Acollaborative%2520model%2520training%2520without%2520direct%2520data%2520sharing.%2520However%252C%2520the%2520vast%250Adisparity%2520in%2520local%2520data%2520distributions%2520among%2520clients%252C%2520often%2520termed%2520the%250ANon-Independent%2520Identically%2520Distributed%2520%2528Non-IID%2529%2520challenge%252C%2520poses%2520a%250Asignificant%2520hurdle%2520to%2520FL%2527s%2520generalization%2520efficacy.%2520The%2520scenario%2520becomes%2520even%250Amore%2520complex%2520when%2520not%2520all%2520clients%2520participate%2520in%2520the%2520training%2520process%252C%2520a%2520common%250Aoccurrence%2520due%2520to%2520unstable%2520network%2520connections%2520or%2520limited%2520computational%250Acapacities.%2520This%2520can%2520greatly%2520complicate%2520the%2520assessment%2520of%2520the%2520trained%2520models%2527%250Ageneralization%2520abilities.%2520While%2520a%2520plethora%2520of%2520recent%2520studies%2520has%2520centered%2520on%250Athe%2520generalization%2520gap%2520pertaining%2520to%2520unseen%2520data%2520from%2520participating%2520clients%250Awith%2520diverse%2520distributions%252C%2520the%2520distinction%2520between%2520the%2520training%2520distributions%250Aof%2520participating%2520clients%2520and%2520the%2520testing%2520distributions%2520of%2520non-participating%250Aones%2520has%2520been%2520largely%2520overlooked.%2520In%2520response%252C%2520our%2520paper%2520unveils%2520an%250Ainformation-theoretic%2520generalization%2520framework%2520for%2520FL.%2520Specifically%252C%2520it%250Aquantifies%2520generalization%2520errors%2520by%2520evaluating%2520the%2520information%2520entropy%2520of%2520local%250Adistributions%2520and%2520discerning%2520discrepancies%2520across%2520these%2520distributions.%2520Inspired%250Aby%2520our%2520deduced%2520generalization%2520bounds%252C%2520we%2520introduce%2520a%2520weighted%2520aggregation%250Aapproach%2520and%2520a%2520duo%2520of%2520client%2520selection%2520strategies.%2520These%2520innovations%2520are%250Adesigned%2520to%2520strengthen%2520FL%2527s%2520ability%2520to%2520generalize%2520and%2520thus%2520ensure%2520that%2520trained%250Amodels%2520perform%2520better%2520on%2520non-participating%2520clients%2520by%2520incorporating%2520a%2520more%250Adiverse%2520range%2520of%2520client%2520data%2520distributions.%2520Our%2520extensive%2520empirical%2520evaluations%250Areaffirm%2520the%2520potency%2520of%2520our%2520proposed%2520methods%252C%2520aligning%2520seamlessly%2520with%2520our%250Atheoretical%2520construct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07171v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advocating%20for%20the%20Silent%3A%20Enhancing%20Federated%20Generalization%20for%0A%20%20Non-Participating%20Clients&entry.906535625=Zheshun%20Wu%20and%20Zenglin%20Xu%20and%20Dun%20Zeng%20and%20Qifan%20Wang%20and%20Jie%20Liu&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20surged%20in%20prominence%20due%20to%20its%20capability%20of%0Acollaborative%20model%20training%20without%20direct%20data%20sharing.%20However%2C%20the%20vast%0Adisparity%20in%20local%20data%20distributions%20among%20clients%2C%20often%20termed%20the%0ANon-Independent%20Identically%20Distributed%20%28Non-IID%29%20challenge%2C%20poses%20a%0Asignificant%20hurdle%20to%20FL%27s%20generalization%20efficacy.%20The%20scenario%20becomes%20even%0Amore%20complex%20when%20not%20all%20clients%20participate%20in%20the%20training%20process%2C%20a%20common%0Aoccurrence%20due%20to%20unstable%20network%20connections%20or%20limited%20computational%0Acapacities.%20This%20can%20greatly%20complicate%20the%20assessment%20of%20the%20trained%20models%27%0Ageneralization%20abilities.%20While%20a%20plethora%20of%20recent%20studies%20has%20centered%20on%0Athe%20generalization%20gap%20pertaining%20to%20unseen%20data%20from%20participating%20clients%0Awith%20diverse%20distributions%2C%20the%20distinction%20between%20the%20training%20distributions%0Aof%20participating%20clients%20and%20the%20testing%20distributions%20of%20non-participating%0Aones%20has%20been%20largely%20overlooked.%20In%20response%2C%20our%20paper%20unveils%20an%0Ainformation-theoretic%20generalization%20framework%20for%20FL.%20Specifically%2C%20it%0Aquantifies%20generalization%20errors%20by%20evaluating%20the%20information%20entropy%20of%20local%0Adistributions%20and%20discerning%20discrepancies%20across%20these%20distributions.%20Inspired%0Aby%20our%20deduced%20generalization%20bounds%2C%20we%20introduce%20a%20weighted%20aggregation%0Aapproach%20and%20a%20duo%20of%20client%20selection%20strategies.%20These%20innovations%20are%0Adesigned%20to%20strengthen%20FL%27s%20ability%20to%20generalize%20and%20thus%20ensure%20that%20trained%0Amodels%20perform%20better%20on%20non-participating%20clients%20by%20incorporating%20a%20more%0Adiverse%20range%20of%20client%20data%20distributions.%20Our%20extensive%20empirical%20evaluations%0Areaffirm%20the%20potency%20of%20our%20proposed%20methods%2C%20aligning%20seamlessly%20with%20our%0Atheoretical%20construct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07171v6&entry.124074799=Read"},
{"title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models", "author": "Lang Cao and Jimeng Sun and Adam Cross", "abstract": "  Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.\n", "link": "http://arxiv.org/abs/2403.00953v2", "date": "2024-10-10", "relevancy": 1.4501, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4922}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4844}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoRD%3A%20An%20Automatic%20and%20End-to-End%20System%20for%20Rare%20Disease%20Knowledge%0A%20%20Graph%20Construction%20Based%20on%20Ontologies-enhanced%20Large%20Language%20Models&body=Title%3A%20AutoRD%3A%20An%20Automatic%20and%20End-to-End%20System%20for%20Rare%20Disease%20Knowledge%0A%20%20Graph%20Construction%20Based%20on%20Ontologies-enhanced%20Large%20Language%20Models%0AAuthor%3A%20Lang%20Cao%20and%20Jimeng%20Sun%20and%20Adam%20Cross%0AAbstract%3A%20%20%20Rare%20diseases%20affect%20millions%20worldwide%20but%20often%20face%20limited%20research%20focus%0Adue%20to%20their%20low%20prevalence.%20This%20results%20in%20prolonged%20diagnoses%20and%20a%20lack%20of%0Aapproved%20therapies.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Ashown%20promise%20in%20automating%20the%20extraction%20of%20medical%20information%2C%20offering%0Apotential%20to%20improve%20medical%20diagnosis%20and%20management.%20However%2C%20most%20LLMs%20lack%0Aprofessional%20medical%20knowledge%2C%20especially%20concerning%20rare%20diseases%2C%20and%0Astruggle%20to%20handle%20the%20latest%20rare%20disease%20information.%20They%20also%20cannot%0Aeffectively%20manage%20rare%20disease%20data%20and%20are%20not%20directly%20suitable%20for%0Adiagnosis%20and%20management%20tasks.%20Our%20objective%20is%20to%20create%20an%20end-to-end%20system%0Acalled%20AutoRD%2C%20which%20automates%20the%20extraction%20of%20information%20from%20medical%20texts%0Aabout%20rare%20diseases%2C%20focusing%20on%20entities%20and%20their%20relations.%20AutoRD%0Aintegrates%20up-to-date%20structured%20knowledge%20and%20demonstrates%20superior%0Aperformance%20in%20rare%20disease%20extraction%20tasks.%20We%20conduct%20various%20experiments%20to%0Aevaluate%20AutoRD%27s%20performance%2C%20aiming%20to%20surpass%20common%20LLMs%20and%20traditional%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoRD%253A%2520An%2520Automatic%2520and%2520End-to-End%2520System%2520for%2520Rare%2520Disease%2520Knowledge%250A%2520%2520Graph%2520Construction%2520Based%2520on%2520Ontologies-enhanced%2520Large%2520Language%2520Models%26entry.906535625%3DLang%2520Cao%2520and%2520Jimeng%2520Sun%2520and%2520Adam%2520Cross%26entry.1292438233%3D%2520%2520Rare%2520diseases%2520affect%2520millions%2520worldwide%2520but%2520often%2520face%2520limited%2520research%2520focus%250Adue%2520to%2520their%2520low%2520prevalence.%2520This%2520results%2520in%2520prolonged%2520diagnoses%2520and%2520a%2520lack%2520of%250Aapproved%2520therapies.%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Ashown%2520promise%2520in%2520automating%2520the%2520extraction%2520of%2520medical%2520information%252C%2520offering%250Apotential%2520to%2520improve%2520medical%2520diagnosis%2520and%2520management.%2520However%252C%2520most%2520LLMs%2520lack%250Aprofessional%2520medical%2520knowledge%252C%2520especially%2520concerning%2520rare%2520diseases%252C%2520and%250Astruggle%2520to%2520handle%2520the%2520latest%2520rare%2520disease%2520information.%2520They%2520also%2520cannot%250Aeffectively%2520manage%2520rare%2520disease%2520data%2520and%2520are%2520not%2520directly%2520suitable%2520for%250Adiagnosis%2520and%2520management%2520tasks.%2520Our%2520objective%2520is%2520to%2520create%2520an%2520end-to-end%2520system%250Acalled%2520AutoRD%252C%2520which%2520automates%2520the%2520extraction%2520of%2520information%2520from%2520medical%2520texts%250Aabout%2520rare%2520diseases%252C%2520focusing%2520on%2520entities%2520and%2520their%2520relations.%2520AutoRD%250Aintegrates%2520up-to-date%2520structured%2520knowledge%2520and%2520demonstrates%2520superior%250Aperformance%2520in%2520rare%2520disease%2520extraction%2520tasks.%2520We%2520conduct%2520various%2520experiments%2520to%250Aevaluate%2520AutoRD%2527s%2520performance%252C%2520aiming%2520to%2520surpass%2520common%2520LLMs%2520and%2520traditional%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoRD%3A%20An%20Automatic%20and%20End-to-End%20System%20for%20Rare%20Disease%20Knowledge%0A%20%20Graph%20Construction%20Based%20on%20Ontologies-enhanced%20Large%20Language%20Models&entry.906535625=Lang%20Cao%20and%20Jimeng%20Sun%20and%20Adam%20Cross&entry.1292438233=%20%20Rare%20diseases%20affect%20millions%20worldwide%20but%20often%20face%20limited%20research%20focus%0Adue%20to%20their%20low%20prevalence.%20This%20results%20in%20prolonged%20diagnoses%20and%20a%20lack%20of%0Aapproved%20therapies.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Ashown%20promise%20in%20automating%20the%20extraction%20of%20medical%20information%2C%20offering%0Apotential%20to%20improve%20medical%20diagnosis%20and%20management.%20However%2C%20most%20LLMs%20lack%0Aprofessional%20medical%20knowledge%2C%20especially%20concerning%20rare%20diseases%2C%20and%0Astruggle%20to%20handle%20the%20latest%20rare%20disease%20information.%20They%20also%20cannot%0Aeffectively%20manage%20rare%20disease%20data%20and%20are%20not%20directly%20suitable%20for%0Adiagnosis%20and%20management%20tasks.%20Our%20objective%20is%20to%20create%20an%20end-to-end%20system%0Acalled%20AutoRD%2C%20which%20automates%20the%20extraction%20of%20information%20from%20medical%20texts%0Aabout%20rare%20diseases%2C%20focusing%20on%20entities%20and%20their%20relations.%20AutoRD%0Aintegrates%20up-to-date%20structured%20knowledge%20and%20demonstrates%20superior%0Aperformance%20in%20rare%20disease%20extraction%20tasks.%20We%20conduct%20various%20experiments%20to%0Aevaluate%20AutoRD%27s%20performance%2C%20aiming%20to%20surpass%20common%20LLMs%20and%20traditional%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00953v2&entry.124074799=Read"},
{"title": "Machine Learning-based feasibility estimation of digital blocks in BCD\n  technology", "author": "Gabriele Faraone and Francesco Daghero and Eugenio Serianni and Dario Licastro and Nicola Di Carolo and Michelangelo Grosso and Giovanna Antonella Franchino and Daniele Jahier Pagliari", "abstract": "  Analog-on-Top Mixed Signal (AMS) Integrated Circuit (IC) design is a\ntime-consuming process predominantly carried out by hand. Within this flow,\nusually, some area is reserved by the top-level integrator for the placement of\ndigital blocks. Specific features of the area, such as size and shape, have a\nrelevant impact on the possibility of implementing the digital logic with the\nrequired functionality. We present a Machine Learning (ML)-based evaluation\nmethodology for predicting the feasibility of digital implementation using a\nset of high-level features. This approach aims to avoid time-consuming\nPlace-and-Route trials, enabling rapid feedback between Digital and Analog\nBack-End designers during top-level placement.\n", "link": "http://arxiv.org/abs/2410.07989v1", "date": "2024-10-10", "relevancy": 0.821, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.435}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4042}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning-based%20feasibility%20estimation%20of%20digital%20blocks%20in%20BCD%0A%20%20technology&body=Title%3A%20Machine%20Learning-based%20feasibility%20estimation%20of%20digital%20blocks%20in%20BCD%0A%20%20technology%0AAuthor%3A%20Gabriele%20Faraone%20and%20Francesco%20Daghero%20and%20Eugenio%20Serianni%20and%20Dario%20Licastro%20and%20Nicola%20Di%20Carolo%20and%20Michelangelo%20Grosso%20and%20Giovanna%20Antonella%20Franchino%20and%20Daniele%20Jahier%20Pagliari%0AAbstract%3A%20%20%20Analog-on-Top%20Mixed%20Signal%20%28AMS%29%20Integrated%20Circuit%20%28IC%29%20design%20is%20a%0Atime-consuming%20process%20predominantly%20carried%20out%20by%20hand.%20Within%20this%20flow%2C%0Ausually%2C%20some%20area%20is%20reserved%20by%20the%20top-level%20integrator%20for%20the%20placement%20of%0Adigital%20blocks.%20Specific%20features%20of%20the%20area%2C%20such%20as%20size%20and%20shape%2C%20have%20a%0Arelevant%20impact%20on%20the%20possibility%20of%20implementing%20the%20digital%20logic%20with%20the%0Arequired%20functionality.%20We%20present%20a%20Machine%20Learning%20%28ML%29-based%20evaluation%0Amethodology%20for%20predicting%20the%20feasibility%20of%20digital%20implementation%20using%20a%0Aset%20of%20high-level%20features.%20This%20approach%20aims%20to%20avoid%20time-consuming%0APlace-and-Route%20trials%2C%20enabling%20rapid%20feedback%20between%20Digital%20and%20Analog%0ABack-End%20designers%20during%20top-level%20placement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning-based%2520feasibility%2520estimation%2520of%2520digital%2520blocks%2520in%2520BCD%250A%2520%2520technology%26entry.906535625%3DGabriele%2520Faraone%2520and%2520Francesco%2520Daghero%2520and%2520Eugenio%2520Serianni%2520and%2520Dario%2520Licastro%2520and%2520Nicola%2520Di%2520Carolo%2520and%2520Michelangelo%2520Grosso%2520and%2520Giovanna%2520Antonella%2520Franchino%2520and%2520Daniele%2520Jahier%2520Pagliari%26entry.1292438233%3D%2520%2520Analog-on-Top%2520Mixed%2520Signal%2520%2528AMS%2529%2520Integrated%2520Circuit%2520%2528IC%2529%2520design%2520is%2520a%250Atime-consuming%2520process%2520predominantly%2520carried%2520out%2520by%2520hand.%2520Within%2520this%2520flow%252C%250Ausually%252C%2520some%2520area%2520is%2520reserved%2520by%2520the%2520top-level%2520integrator%2520for%2520the%2520placement%2520of%250Adigital%2520blocks.%2520Specific%2520features%2520of%2520the%2520area%252C%2520such%2520as%2520size%2520and%2520shape%252C%2520have%2520a%250Arelevant%2520impact%2520on%2520the%2520possibility%2520of%2520implementing%2520the%2520digital%2520logic%2520with%2520the%250Arequired%2520functionality.%2520We%2520present%2520a%2520Machine%2520Learning%2520%2528ML%2529-based%2520evaluation%250Amethodology%2520for%2520predicting%2520the%2520feasibility%2520of%2520digital%2520implementation%2520using%2520a%250Aset%2520of%2520high-level%2520features.%2520This%2520approach%2520aims%2520to%2520avoid%2520time-consuming%250APlace-and-Route%2520trials%252C%2520enabling%2520rapid%2520feedback%2520between%2520Digital%2520and%2520Analog%250ABack-End%2520designers%2520during%2520top-level%2520placement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning-based%20feasibility%20estimation%20of%20digital%20blocks%20in%20BCD%0A%20%20technology&entry.906535625=Gabriele%20Faraone%20and%20Francesco%20Daghero%20and%20Eugenio%20Serianni%20and%20Dario%20Licastro%20and%20Nicola%20Di%20Carolo%20and%20Michelangelo%20Grosso%20and%20Giovanna%20Antonella%20Franchino%20and%20Daniele%20Jahier%20Pagliari&entry.1292438233=%20%20Analog-on-Top%20Mixed%20Signal%20%28AMS%29%20Integrated%20Circuit%20%28IC%29%20design%20is%20a%0Atime-consuming%20process%20predominantly%20carried%20out%20by%20hand.%20Within%20this%20flow%2C%0Ausually%2C%20some%20area%20is%20reserved%20by%20the%20top-level%20integrator%20for%20the%20placement%20of%0Adigital%20blocks.%20Specific%20features%20of%20the%20area%2C%20such%20as%20size%20and%20shape%2C%20have%20a%0Arelevant%20impact%20on%20the%20possibility%20of%20implementing%20the%20digital%20logic%20with%20the%0Arequired%20functionality.%20We%20present%20a%20Machine%20Learning%20%28ML%29-based%20evaluation%0Amethodology%20for%20predicting%20the%20feasibility%20of%20digital%20implementation%20using%20a%0Aset%20of%20high-level%20features.%20This%20approach%20aims%20to%20avoid%20time-consuming%0APlace-and-Route%20trials%2C%20enabling%20rapid%20feedback%20between%20Digital%20and%20Analog%0ABack-End%20designers%20during%20top-level%20placement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07989v1&entry.124074799=Read"},
{"title": "Strategic Classification With Externalities", "author": "Yiling Chen and Safwan Hossain and Evi Micha and Ariel Procaccia", "abstract": "  We propose a new variant of the strategic classification problem: a principal\nreveals a classifier, and $n$ agents report their (possibly manipulated)\nfeatures to be classified. Motivated by real-world applications, our model\ncrucially allows the manipulation of one agent to affect another; that is, it\nexplicitly captures inter-agent externalities. The principal-agent interactions\nare formally modeled as a Stackelberg game, with the resulting agent\nmanipulation dynamics captured as a simultaneous game. We show that under\ncertain assumptions, the pure Nash Equilibrium of this agent manipulation game\nis unique and can be efficiently computed. Leveraging this result, PAC learning\nguarantees are established for the learner: informally, we show that it is\npossible to learn classifiers that minimize loss on the distribution, even when\na random number of agents are manipulating their way to a pure Nash\nEquilibrium. We also comment on the optimization of such classifiers through\ngradient-based approaches. This work sets the theoretical foundations for a\nmore realistic analysis of classifiers that are robust against multiple\nstrategic actors interacting in a common environment.\n", "link": "http://arxiv.org/abs/2410.08032v1", "date": "2024-10-10", "relevancy": 1.8253, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4631}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4584}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strategic%20Classification%20With%20Externalities&body=Title%3A%20Strategic%20Classification%20With%20Externalities%0AAuthor%3A%20Yiling%20Chen%20and%20Safwan%20Hossain%20and%20Evi%20Micha%20and%20Ariel%20Procaccia%0AAbstract%3A%20%20%20We%20propose%20a%20new%20variant%20of%20the%20strategic%20classification%20problem%3A%20a%20principal%0Areveals%20a%20classifier%2C%20and%20%24n%24%20agents%20report%20their%20%28possibly%20manipulated%29%0Afeatures%20to%20be%20classified.%20Motivated%20by%20real-world%20applications%2C%20our%20model%0Acrucially%20allows%20the%20manipulation%20of%20one%20agent%20to%20affect%20another%3B%20that%20is%2C%20it%0Aexplicitly%20captures%20inter-agent%20externalities.%20The%20principal-agent%20interactions%0Aare%20formally%20modeled%20as%20a%20Stackelberg%20game%2C%20with%20the%20resulting%20agent%0Amanipulation%20dynamics%20captured%20as%20a%20simultaneous%20game.%20We%20show%20that%20under%0Acertain%20assumptions%2C%20the%20pure%20Nash%20Equilibrium%20of%20this%20agent%20manipulation%20game%0Ais%20unique%20and%20can%20be%20efficiently%20computed.%20Leveraging%20this%20result%2C%20PAC%20learning%0Aguarantees%20are%20established%20for%20the%20learner%3A%20informally%2C%20we%20show%20that%20it%20is%0Apossible%20to%20learn%20classifiers%20that%20minimize%20loss%20on%20the%20distribution%2C%20even%20when%0Aa%20random%20number%20of%20agents%20are%20manipulating%20their%20way%20to%20a%20pure%20Nash%0AEquilibrium.%20We%20also%20comment%20on%20the%20optimization%20of%20such%20classifiers%20through%0Agradient-based%20approaches.%20This%20work%20sets%20the%20theoretical%20foundations%20for%20a%0Amore%20realistic%20analysis%20of%20classifiers%20that%20are%20robust%20against%20multiple%0Astrategic%20actors%20interacting%20in%20a%20common%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrategic%2520Classification%2520With%2520Externalities%26entry.906535625%3DYiling%2520Chen%2520and%2520Safwan%2520Hossain%2520and%2520Evi%2520Micha%2520and%2520Ariel%2520Procaccia%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520variant%2520of%2520the%2520strategic%2520classification%2520problem%253A%2520a%2520principal%250Areveals%2520a%2520classifier%252C%2520and%2520%2524n%2524%2520agents%2520report%2520their%2520%2528possibly%2520manipulated%2529%250Afeatures%2520to%2520be%2520classified.%2520Motivated%2520by%2520real-world%2520applications%252C%2520our%2520model%250Acrucially%2520allows%2520the%2520manipulation%2520of%2520one%2520agent%2520to%2520affect%2520another%253B%2520that%2520is%252C%2520it%250Aexplicitly%2520captures%2520inter-agent%2520externalities.%2520The%2520principal-agent%2520interactions%250Aare%2520formally%2520modeled%2520as%2520a%2520Stackelberg%2520game%252C%2520with%2520the%2520resulting%2520agent%250Amanipulation%2520dynamics%2520captured%2520as%2520a%2520simultaneous%2520game.%2520We%2520show%2520that%2520under%250Acertain%2520assumptions%252C%2520the%2520pure%2520Nash%2520Equilibrium%2520of%2520this%2520agent%2520manipulation%2520game%250Ais%2520unique%2520and%2520can%2520be%2520efficiently%2520computed.%2520Leveraging%2520this%2520result%252C%2520PAC%2520learning%250Aguarantees%2520are%2520established%2520for%2520the%2520learner%253A%2520informally%252C%2520we%2520show%2520that%2520it%2520is%250Apossible%2520to%2520learn%2520classifiers%2520that%2520minimize%2520loss%2520on%2520the%2520distribution%252C%2520even%2520when%250Aa%2520random%2520number%2520of%2520agents%2520are%2520manipulating%2520their%2520way%2520to%2520a%2520pure%2520Nash%250AEquilibrium.%2520We%2520also%2520comment%2520on%2520the%2520optimization%2520of%2520such%2520classifiers%2520through%250Agradient-based%2520approaches.%2520This%2520work%2520sets%2520the%2520theoretical%2520foundations%2520for%2520a%250Amore%2520realistic%2520analysis%2520of%2520classifiers%2520that%2520are%2520robust%2520against%2520multiple%250Astrategic%2520actors%2520interacting%2520in%2520a%2520common%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strategic%20Classification%20With%20Externalities&entry.906535625=Yiling%20Chen%20and%20Safwan%20Hossain%20and%20Evi%20Micha%20and%20Ariel%20Procaccia&entry.1292438233=%20%20We%20propose%20a%20new%20variant%20of%20the%20strategic%20classification%20problem%3A%20a%20principal%0Areveals%20a%20classifier%2C%20and%20%24n%24%20agents%20report%20their%20%28possibly%20manipulated%29%0Afeatures%20to%20be%20classified.%20Motivated%20by%20real-world%20applications%2C%20our%20model%0Acrucially%20allows%20the%20manipulation%20of%20one%20agent%20to%20affect%20another%3B%20that%20is%2C%20it%0Aexplicitly%20captures%20inter-agent%20externalities.%20The%20principal-agent%20interactions%0Aare%20formally%20modeled%20as%20a%20Stackelberg%20game%2C%20with%20the%20resulting%20agent%0Amanipulation%20dynamics%20captured%20as%20a%20simultaneous%20game.%20We%20show%20that%20under%0Acertain%20assumptions%2C%20the%20pure%20Nash%20Equilibrium%20of%20this%20agent%20manipulation%20game%0Ais%20unique%20and%20can%20be%20efficiently%20computed.%20Leveraging%20this%20result%2C%20PAC%20learning%0Aguarantees%20are%20established%20for%20the%20learner%3A%20informally%2C%20we%20show%20that%20it%20is%0Apossible%20to%20learn%20classifiers%20that%20minimize%20loss%20on%20the%20distribution%2C%20even%20when%0Aa%20random%20number%20of%20agents%20are%20manipulating%20their%20way%20to%20a%20pure%20Nash%0AEquilibrium.%20We%20also%20comment%20on%20the%20optimization%20of%20such%20classifiers%20through%0Agradient-based%20approaches.%20This%20work%20sets%20the%20theoretical%20foundations%20for%20a%0Amore%20realistic%20analysis%20of%20classifiers%20that%20are%20robust%20against%20multiple%0Astrategic%20actors%20interacting%20in%20a%20common%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08032v1&entry.124074799=Read"},
{"title": "Deriving Causal Order from Single-Variable Interventions: Guarantees &\n  Algorithm", "author": "Mathieu Chevalley and Patrick Schwab and Arash Mehrjou", "abstract": "  Targeted and uniform interventions to a system are crucial for unveiling\ncausal relationships. While several methods have been developed to leverage\ninterventional data for causal structure learning, their practical application\nin real-world scenarios often remains challenging. Recent benchmark studies\nhave highlighted these difficulties, even when large numbers of single-variable\nintervention samples are available. In this work, we demonstrate, both\ntheoretically and empirically, that such datasets contain a wealth of causal\ninformation that can be effectively extracted under realistic assumptions about\nthe data distribution. More specifically, we introduce the notion of\ninterventional faithfulness, which relies on comparisons between the marginal\ndistributions of each variable across observational and interventional\nsettings, and we introduce a score on causal orders. Under this assumption, we\nare able to prove strong theoretical guarantees on the optimum of our score\nthat also hold for large-scale settings. To empirically verify our theory, we\nintroduce Intersort, an algorithm designed to infer the causal order from\ndatasets containing large numbers of single-variable interventions by\napproximately optimizing our score. Intersort outperforms baselines (GIES,\nDCDI, PC and EASE) on almost all simulated data settings replicating common\nbenchmarks in the field. Our proposed novel approach to modeling interventional\ndatasets thus offers a promising avenue for advancing causal inference,\nhighlighting significant potential for further enhancements under realistic\nassumptions.\n", "link": "http://arxiv.org/abs/2405.18314v2", "date": "2024-10-10", "relevancy": 0.8316, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4294}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4095}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deriving%20Causal%20Order%20from%20Single-Variable%20Interventions%3A%20Guarantees%20%26%0A%20%20Algorithm&body=Title%3A%20Deriving%20Causal%20Order%20from%20Single-Variable%20Interventions%3A%20Guarantees%20%26%0A%20%20Algorithm%0AAuthor%3A%20Mathieu%20Chevalley%20and%20Patrick%20Schwab%20and%20Arash%20Mehrjou%0AAbstract%3A%20%20%20Targeted%20and%20uniform%20interventions%20to%20a%20system%20are%20crucial%20for%20unveiling%0Acausal%20relationships.%20While%20several%20methods%20have%20been%20developed%20to%20leverage%0Ainterventional%20data%20for%20causal%20structure%20learning%2C%20their%20practical%20application%0Ain%20real-world%20scenarios%20often%20remains%20challenging.%20Recent%20benchmark%20studies%0Ahave%20highlighted%20these%20difficulties%2C%20even%20when%20large%20numbers%20of%20single-variable%0Aintervention%20samples%20are%20available.%20In%20this%20work%2C%20we%20demonstrate%2C%20both%0Atheoretically%20and%20empirically%2C%20that%20such%20datasets%20contain%20a%20wealth%20of%20causal%0Ainformation%20that%20can%20be%20effectively%20extracted%20under%20realistic%20assumptions%20about%0Athe%20data%20distribution.%20More%20specifically%2C%20we%20introduce%20the%20notion%20of%0Ainterventional%20faithfulness%2C%20which%20relies%20on%20comparisons%20between%20the%20marginal%0Adistributions%20of%20each%20variable%20across%20observational%20and%20interventional%0Asettings%2C%20and%20we%20introduce%20a%20score%20on%20causal%20orders.%20Under%20this%20assumption%2C%20we%0Aare%20able%20to%20prove%20strong%20theoretical%20guarantees%20on%20the%20optimum%20of%20our%20score%0Athat%20also%20hold%20for%20large-scale%20settings.%20To%20empirically%20verify%20our%20theory%2C%20we%0Aintroduce%20Intersort%2C%20an%20algorithm%20designed%20to%20infer%20the%20causal%20order%20from%0Adatasets%20containing%20large%20numbers%20of%20single-variable%20interventions%20by%0Aapproximately%20optimizing%20our%20score.%20Intersort%20outperforms%20baselines%20%28GIES%2C%0ADCDI%2C%20PC%20and%20EASE%29%20on%20almost%20all%20simulated%20data%20settings%20replicating%20common%0Abenchmarks%20in%20the%20field.%20Our%20proposed%20novel%20approach%20to%20modeling%20interventional%0Adatasets%20thus%20offers%20a%20promising%20avenue%20for%20advancing%20causal%20inference%2C%0Ahighlighting%20significant%20potential%20for%20further%20enhancements%20under%20realistic%0Aassumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeriving%2520Causal%2520Order%2520from%2520Single-Variable%2520Interventions%253A%2520Guarantees%2520%2526%250A%2520%2520Algorithm%26entry.906535625%3DMathieu%2520Chevalley%2520and%2520Patrick%2520Schwab%2520and%2520Arash%2520Mehrjou%26entry.1292438233%3D%2520%2520Targeted%2520and%2520uniform%2520interventions%2520to%2520a%2520system%2520are%2520crucial%2520for%2520unveiling%250Acausal%2520relationships.%2520While%2520several%2520methods%2520have%2520been%2520developed%2520to%2520leverage%250Ainterventional%2520data%2520for%2520causal%2520structure%2520learning%252C%2520their%2520practical%2520application%250Ain%2520real-world%2520scenarios%2520often%2520remains%2520challenging.%2520Recent%2520benchmark%2520studies%250Ahave%2520highlighted%2520these%2520difficulties%252C%2520even%2520when%2520large%2520numbers%2520of%2520single-variable%250Aintervention%2520samples%2520are%2520available.%2520In%2520this%2520work%252C%2520we%2520demonstrate%252C%2520both%250Atheoretically%2520and%2520empirically%252C%2520that%2520such%2520datasets%2520contain%2520a%2520wealth%2520of%2520causal%250Ainformation%2520that%2520can%2520be%2520effectively%2520extracted%2520under%2520realistic%2520assumptions%2520about%250Athe%2520data%2520distribution.%2520More%2520specifically%252C%2520we%2520introduce%2520the%2520notion%2520of%250Ainterventional%2520faithfulness%252C%2520which%2520relies%2520on%2520comparisons%2520between%2520the%2520marginal%250Adistributions%2520of%2520each%2520variable%2520across%2520observational%2520and%2520interventional%250Asettings%252C%2520and%2520we%2520introduce%2520a%2520score%2520on%2520causal%2520orders.%2520Under%2520this%2520assumption%252C%2520we%250Aare%2520able%2520to%2520prove%2520strong%2520theoretical%2520guarantees%2520on%2520the%2520optimum%2520of%2520our%2520score%250Athat%2520also%2520hold%2520for%2520large-scale%2520settings.%2520To%2520empirically%2520verify%2520our%2520theory%252C%2520we%250Aintroduce%2520Intersort%252C%2520an%2520algorithm%2520designed%2520to%2520infer%2520the%2520causal%2520order%2520from%250Adatasets%2520containing%2520large%2520numbers%2520of%2520single-variable%2520interventions%2520by%250Aapproximately%2520optimizing%2520our%2520score.%2520Intersort%2520outperforms%2520baselines%2520%2528GIES%252C%250ADCDI%252C%2520PC%2520and%2520EASE%2529%2520on%2520almost%2520all%2520simulated%2520data%2520settings%2520replicating%2520common%250Abenchmarks%2520in%2520the%2520field.%2520Our%2520proposed%2520novel%2520approach%2520to%2520modeling%2520interventional%250Adatasets%2520thus%2520offers%2520a%2520promising%2520avenue%2520for%2520advancing%2520causal%2520inference%252C%250Ahighlighting%2520significant%2520potential%2520for%2520further%2520enhancements%2520under%2520realistic%250Aassumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deriving%20Causal%20Order%20from%20Single-Variable%20Interventions%3A%20Guarantees%20%26%0A%20%20Algorithm&entry.906535625=Mathieu%20Chevalley%20and%20Patrick%20Schwab%20and%20Arash%20Mehrjou&entry.1292438233=%20%20Targeted%20and%20uniform%20interventions%20to%20a%20system%20are%20crucial%20for%20unveiling%0Acausal%20relationships.%20While%20several%20methods%20have%20been%20developed%20to%20leverage%0Ainterventional%20data%20for%20causal%20structure%20learning%2C%20their%20practical%20application%0Ain%20real-world%20scenarios%20often%20remains%20challenging.%20Recent%20benchmark%20studies%0Ahave%20highlighted%20these%20difficulties%2C%20even%20when%20large%20numbers%20of%20single-variable%0Aintervention%20samples%20are%20available.%20In%20this%20work%2C%20we%20demonstrate%2C%20both%0Atheoretically%20and%20empirically%2C%20that%20such%20datasets%20contain%20a%20wealth%20of%20causal%0Ainformation%20that%20can%20be%20effectively%20extracted%20under%20realistic%20assumptions%20about%0Athe%20data%20distribution.%20More%20specifically%2C%20we%20introduce%20the%20notion%20of%0Ainterventional%20faithfulness%2C%20which%20relies%20on%20comparisons%20between%20the%20marginal%0Adistributions%20of%20each%20variable%20across%20observational%20and%20interventional%0Asettings%2C%20and%20we%20introduce%20a%20score%20on%20causal%20orders.%20Under%20this%20assumption%2C%20we%0Aare%20able%20to%20prove%20strong%20theoretical%20guarantees%20on%20the%20optimum%20of%20our%20score%0Athat%20also%20hold%20for%20large-scale%20settings.%20To%20empirically%20verify%20our%20theory%2C%20we%0Aintroduce%20Intersort%2C%20an%20algorithm%20designed%20to%20infer%20the%20causal%20order%20from%0Adatasets%20containing%20large%20numbers%20of%20single-variable%20interventions%20by%0Aapproximately%20optimizing%20our%20score.%20Intersort%20outperforms%20baselines%20%28GIES%2C%0ADCDI%2C%20PC%20and%20EASE%29%20on%20almost%20all%20simulated%20data%20settings%20replicating%20common%0Abenchmarks%20in%20the%20field.%20Our%20proposed%20novel%20approach%20to%20modeling%20interventional%0Adatasets%20thus%20offers%20a%20promising%20avenue%20for%20advancing%20causal%20inference%2C%0Ahighlighting%20significant%20potential%20for%20further%20enhancements%20under%20realistic%0Aassumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18314v2&entry.124074799=Read"},
{"title": "Differentiability in Unrolled Training of Neural Physics Simulators on\n  Transient Dynamics", "author": "Bjoern List and Li-Wei Chen and Kartik Bali and Nils Thuerey", "abstract": "  Unrolling training trajectories over time strongly influences the inference\naccuracy of neural network-augmented physics simulators. We analyze this in\nthree variants of training neural time-steppers. In addition to one-step setups\nand fully differentiable unrolling, we include a third, less widely used\nvariant: unrolling without temporal gradients. Comparing networks trained with\nthese three modalities disentangles the two dominant effects of unrolling,\ntraining distribution shift and long-term gradients. We present detailed study\nacross physical systems, network sizes and architectures, training setups, and\ntest scenarios. It also encompasses two simulation modes: In prediction setups,\nwe rely solely on neural networks to compute a trajectory. In contrast,\ncorrection setups include a numerical solver that is supported by a neural\nnetwork. Spanning these variations, our study provides the empirical basis for\nour main findings: Non-differentiable but unrolled training with a numerical\nsolver in a correction setup can yield substantial improvements over a fully\ndifferentiable prediction setup not utilizing this solver. The accuracy of\nmodels trained in a fully differentiable setup differs compared to their\nnon-differentiable counterparts. Differentiable ones perform best in a\ncomparison among correction networks as well as among prediction setups. For\nboth, the accuracy of non-differentiable unrolling comes close. Furthermore, we\nshow that these behaviors are invariant to the physical system, the network\narchitecture and size, and the numerical scheme. These results motivate\nintegrating non-differentiable numerical simulators into training setups even\nif full differentiability is unavailable. We show the convergence rate of\ncommon architectures to be low compared to numerical algorithms. This motivates\ncorrection setups combining neural and numerical parts which utilize benefits\nof both.\n", "link": "http://arxiv.org/abs/2402.12971v2", "date": "2024-10-10", "relevancy": 1.5126, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5094}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5037}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiability%20in%20Unrolled%20Training%20of%20Neural%20Physics%20Simulators%20on%0A%20%20Transient%20Dynamics&body=Title%3A%20Differentiability%20in%20Unrolled%20Training%20of%20Neural%20Physics%20Simulators%20on%0A%20%20Transient%20Dynamics%0AAuthor%3A%20Bjoern%20List%20and%20Li-Wei%20Chen%20and%20Kartik%20Bali%20and%20Nils%20Thuerey%0AAbstract%3A%20%20%20Unrolling%20training%20trajectories%20over%20time%20strongly%20influences%20the%20inference%0Aaccuracy%20of%20neural%20network-augmented%20physics%20simulators.%20We%20analyze%20this%20in%0Athree%20variants%20of%20training%20neural%20time-steppers.%20In%20addition%20to%20one-step%20setups%0Aand%20fully%20differentiable%20unrolling%2C%20we%20include%20a%20third%2C%20less%20widely%20used%0Avariant%3A%20unrolling%20without%20temporal%20gradients.%20Comparing%20networks%20trained%20with%0Athese%20three%20modalities%20disentangles%20the%20two%20dominant%20effects%20of%20unrolling%2C%0Atraining%20distribution%20shift%20and%20long-term%20gradients.%20We%20present%20detailed%20study%0Aacross%20physical%20systems%2C%20network%20sizes%20and%20architectures%2C%20training%20setups%2C%20and%0Atest%20scenarios.%20It%20also%20encompasses%20two%20simulation%20modes%3A%20In%20prediction%20setups%2C%0Awe%20rely%20solely%20on%20neural%20networks%20to%20compute%20a%20trajectory.%20In%20contrast%2C%0Acorrection%20setups%20include%20a%20numerical%20solver%20that%20is%20supported%20by%20a%20neural%0Anetwork.%20Spanning%20these%20variations%2C%20our%20study%20provides%20the%20empirical%20basis%20for%0Aour%20main%20findings%3A%20Non-differentiable%20but%20unrolled%20training%20with%20a%20numerical%0Asolver%20in%20a%20correction%20setup%20can%20yield%20substantial%20improvements%20over%20a%20fully%0Adifferentiable%20prediction%20setup%20not%20utilizing%20this%20solver.%20The%20accuracy%20of%0Amodels%20trained%20in%20a%20fully%20differentiable%20setup%20differs%20compared%20to%20their%0Anon-differentiable%20counterparts.%20Differentiable%20ones%20perform%20best%20in%20a%0Acomparison%20among%20correction%20networks%20as%20well%20as%20among%20prediction%20setups.%20For%0Aboth%2C%20the%20accuracy%20of%20non-differentiable%20unrolling%20comes%20close.%20Furthermore%2C%20we%0Ashow%20that%20these%20behaviors%20are%20invariant%20to%20the%20physical%20system%2C%20the%20network%0Aarchitecture%20and%20size%2C%20and%20the%20numerical%20scheme.%20These%20results%20motivate%0Aintegrating%20non-differentiable%20numerical%20simulators%20into%20training%20setups%20even%0Aif%20full%20differentiability%20is%20unavailable.%20We%20show%20the%20convergence%20rate%20of%0Acommon%20architectures%20to%20be%20low%20compared%20to%20numerical%20algorithms.%20This%20motivates%0Acorrection%20setups%20combining%20neural%20and%20numerical%20parts%20which%20utilize%20benefits%0Aof%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiability%2520in%2520Unrolled%2520Training%2520of%2520Neural%2520Physics%2520Simulators%2520on%250A%2520%2520Transient%2520Dynamics%26entry.906535625%3DBjoern%2520List%2520and%2520Li-Wei%2520Chen%2520and%2520Kartik%2520Bali%2520and%2520Nils%2520Thuerey%26entry.1292438233%3D%2520%2520Unrolling%2520training%2520trajectories%2520over%2520time%2520strongly%2520influences%2520the%2520inference%250Aaccuracy%2520of%2520neural%2520network-augmented%2520physics%2520simulators.%2520We%2520analyze%2520this%2520in%250Athree%2520variants%2520of%2520training%2520neural%2520time-steppers.%2520In%2520addition%2520to%2520one-step%2520setups%250Aand%2520fully%2520differentiable%2520unrolling%252C%2520we%2520include%2520a%2520third%252C%2520less%2520widely%2520used%250Avariant%253A%2520unrolling%2520without%2520temporal%2520gradients.%2520Comparing%2520networks%2520trained%2520with%250Athese%2520three%2520modalities%2520disentangles%2520the%2520two%2520dominant%2520effects%2520of%2520unrolling%252C%250Atraining%2520distribution%2520shift%2520and%2520long-term%2520gradients.%2520We%2520present%2520detailed%2520study%250Aacross%2520physical%2520systems%252C%2520network%2520sizes%2520and%2520architectures%252C%2520training%2520setups%252C%2520and%250Atest%2520scenarios.%2520It%2520also%2520encompasses%2520two%2520simulation%2520modes%253A%2520In%2520prediction%2520setups%252C%250Awe%2520rely%2520solely%2520on%2520neural%2520networks%2520to%2520compute%2520a%2520trajectory.%2520In%2520contrast%252C%250Acorrection%2520setups%2520include%2520a%2520numerical%2520solver%2520that%2520is%2520supported%2520by%2520a%2520neural%250Anetwork.%2520Spanning%2520these%2520variations%252C%2520our%2520study%2520provides%2520the%2520empirical%2520basis%2520for%250Aour%2520main%2520findings%253A%2520Non-differentiable%2520but%2520unrolled%2520training%2520with%2520a%2520numerical%250Asolver%2520in%2520a%2520correction%2520setup%2520can%2520yield%2520substantial%2520improvements%2520over%2520a%2520fully%250Adifferentiable%2520prediction%2520setup%2520not%2520utilizing%2520this%2520solver.%2520The%2520accuracy%2520of%250Amodels%2520trained%2520in%2520a%2520fully%2520differentiable%2520setup%2520differs%2520compared%2520to%2520their%250Anon-differentiable%2520counterparts.%2520Differentiable%2520ones%2520perform%2520best%2520in%2520a%250Acomparison%2520among%2520correction%2520networks%2520as%2520well%2520as%2520among%2520prediction%2520setups.%2520For%250Aboth%252C%2520the%2520accuracy%2520of%2520non-differentiable%2520unrolling%2520comes%2520close.%2520Furthermore%252C%2520we%250Ashow%2520that%2520these%2520behaviors%2520are%2520invariant%2520to%2520the%2520physical%2520system%252C%2520the%2520network%250Aarchitecture%2520and%2520size%252C%2520and%2520the%2520numerical%2520scheme.%2520These%2520results%2520motivate%250Aintegrating%2520non-differentiable%2520numerical%2520simulators%2520into%2520training%2520setups%2520even%250Aif%2520full%2520differentiability%2520is%2520unavailable.%2520We%2520show%2520the%2520convergence%2520rate%2520of%250Acommon%2520architectures%2520to%2520be%2520low%2520compared%2520to%2520numerical%2520algorithms.%2520This%2520motivates%250Acorrection%2520setups%2520combining%2520neural%2520and%2520numerical%2520parts%2520which%2520utilize%2520benefits%250Aof%2520both.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiability%20in%20Unrolled%20Training%20of%20Neural%20Physics%20Simulators%20on%0A%20%20Transient%20Dynamics&entry.906535625=Bjoern%20List%20and%20Li-Wei%20Chen%20and%20Kartik%20Bali%20and%20Nils%20Thuerey&entry.1292438233=%20%20Unrolling%20training%20trajectories%20over%20time%20strongly%20influences%20the%20inference%0Aaccuracy%20of%20neural%20network-augmented%20physics%20simulators.%20We%20analyze%20this%20in%0Athree%20variants%20of%20training%20neural%20time-steppers.%20In%20addition%20to%20one-step%20setups%0Aand%20fully%20differentiable%20unrolling%2C%20we%20include%20a%20third%2C%20less%20widely%20used%0Avariant%3A%20unrolling%20without%20temporal%20gradients.%20Comparing%20networks%20trained%20with%0Athese%20three%20modalities%20disentangles%20the%20two%20dominant%20effects%20of%20unrolling%2C%0Atraining%20distribution%20shift%20and%20long-term%20gradients.%20We%20present%20detailed%20study%0Aacross%20physical%20systems%2C%20network%20sizes%20and%20architectures%2C%20training%20setups%2C%20and%0Atest%20scenarios.%20It%20also%20encompasses%20two%20simulation%20modes%3A%20In%20prediction%20setups%2C%0Awe%20rely%20solely%20on%20neural%20networks%20to%20compute%20a%20trajectory.%20In%20contrast%2C%0Acorrection%20setups%20include%20a%20numerical%20solver%20that%20is%20supported%20by%20a%20neural%0Anetwork.%20Spanning%20these%20variations%2C%20our%20study%20provides%20the%20empirical%20basis%20for%0Aour%20main%20findings%3A%20Non-differentiable%20but%20unrolled%20training%20with%20a%20numerical%0Asolver%20in%20a%20correction%20setup%20can%20yield%20substantial%20improvements%20over%20a%20fully%0Adifferentiable%20prediction%20setup%20not%20utilizing%20this%20solver.%20The%20accuracy%20of%0Amodels%20trained%20in%20a%20fully%20differentiable%20setup%20differs%20compared%20to%20their%0Anon-differentiable%20counterparts.%20Differentiable%20ones%20perform%20best%20in%20a%0Acomparison%20among%20correction%20networks%20as%20well%20as%20among%20prediction%20setups.%20For%0Aboth%2C%20the%20accuracy%20of%20non-differentiable%20unrolling%20comes%20close.%20Furthermore%2C%20we%0Ashow%20that%20these%20behaviors%20are%20invariant%20to%20the%20physical%20system%2C%20the%20network%0Aarchitecture%20and%20size%2C%20and%20the%20numerical%20scheme.%20These%20results%20motivate%0Aintegrating%20non-differentiable%20numerical%20simulators%20into%20training%20setups%20even%0Aif%20full%20differentiability%20is%20unavailable.%20We%20show%20the%20convergence%20rate%20of%0Acommon%20architectures%20to%20be%20low%20compared%20to%20numerical%20algorithms.%20This%20motivates%0Acorrection%20setups%20combining%20neural%20and%20numerical%20parts%20which%20utilize%20benefits%0Aof%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12971v2&entry.124074799=Read"},
{"title": "Knowledge-Aware Parsimony Learning: A Perspective from Relational Graphs", "author": "Quanming Yao and Yongqi Zhang and Yaqing Wang and Nan Yin and James Kwok and Qiang Yang", "abstract": "  The scaling law, which involves the brute-force expansion of training\ndatasets and learnable parameters, has become a prevalent strategy for\ndeveloping more robust learning models. However, due to bottlenecks in data,\ncomputation, and trust, the sustainability of the scaling law is a serious\nconcern for the future of deep learning. In this paper, we address this issue\nby developing next-generation models in a parsimonious manner (i.e., achieving\ngreater potential with simpler models). The key is to drive models using\ndomain-specific knowledge, such as symbols, logic, and formulas, instead of\nrelying on the scaling law. This approach allows us to build a framework that\nuses this knowledge as \"building blocks\" to achieve parsimony in model design,\ntraining, and interpretation. Empirical results show that our methods surpass\nthose that typically follow the scaling law. We also demonstrate the\napplication of our framework in AI for science, specifically in the problem of\ndrug-drug interaction prediction. We hope our research can foster more diverse\ntechnical roadmaps in the era of foundation models.\n", "link": "http://arxiv.org/abs/2407.00478v2", "date": "2024-10-10", "relevancy": 1.9774, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Aware%20Parsimony%20Learning%3A%20A%20Perspective%20from%20Relational%20Graphs&body=Title%3A%20Knowledge-Aware%20Parsimony%20Learning%3A%20A%20Perspective%20from%20Relational%20Graphs%0AAuthor%3A%20Quanming%20Yao%20and%20Yongqi%20Zhang%20and%20Yaqing%20Wang%20and%20Nan%20Yin%20and%20James%20Kwok%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20The%20scaling%20law%2C%20which%20involves%20the%20brute-force%20expansion%20of%20training%0Adatasets%20and%20learnable%20parameters%2C%20has%20become%20a%20prevalent%20strategy%20for%0Adeveloping%20more%20robust%20learning%20models.%20However%2C%20due%20to%20bottlenecks%20in%20data%2C%0Acomputation%2C%20and%20trust%2C%20the%20sustainability%20of%20the%20scaling%20law%20is%20a%20serious%0Aconcern%20for%20the%20future%20of%20deep%20learning.%20In%20this%20paper%2C%20we%20address%20this%20issue%0Aby%20developing%20next-generation%20models%20in%20a%20parsimonious%20manner%20%28i.e.%2C%20achieving%0Agreater%20potential%20with%20simpler%20models%29.%20The%20key%20is%20to%20drive%20models%20using%0Adomain-specific%20knowledge%2C%20such%20as%20symbols%2C%20logic%2C%20and%20formulas%2C%20instead%20of%0Arelying%20on%20the%20scaling%20law.%20This%20approach%20allows%20us%20to%20build%20a%20framework%20that%0Auses%20this%20knowledge%20as%20%22building%20blocks%22%20to%20achieve%20parsimony%20in%20model%20design%2C%0Atraining%2C%20and%20interpretation.%20Empirical%20results%20show%20that%20our%20methods%20surpass%0Athose%20that%20typically%20follow%20the%20scaling%20law.%20We%20also%20demonstrate%20the%0Aapplication%20of%20our%20framework%20in%20AI%20for%20science%2C%20specifically%20in%20the%20problem%20of%0Adrug-drug%20interaction%20prediction.%20We%20hope%20our%20research%20can%20foster%20more%20diverse%0Atechnical%20roadmaps%20in%20the%20era%20of%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Aware%2520Parsimony%2520Learning%253A%2520A%2520Perspective%2520from%2520Relational%2520Graphs%26entry.906535625%3DQuanming%2520Yao%2520and%2520Yongqi%2520Zhang%2520and%2520Yaqing%2520Wang%2520and%2520Nan%2520Yin%2520and%2520James%2520Kwok%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520The%2520scaling%2520law%252C%2520which%2520involves%2520the%2520brute-force%2520expansion%2520of%2520training%250Adatasets%2520and%2520learnable%2520parameters%252C%2520has%2520become%2520a%2520prevalent%2520strategy%2520for%250Adeveloping%2520more%2520robust%2520learning%2520models.%2520However%252C%2520due%2520to%2520bottlenecks%2520in%2520data%252C%250Acomputation%252C%2520and%2520trust%252C%2520the%2520sustainability%2520of%2520the%2520scaling%2520law%2520is%2520a%2520serious%250Aconcern%2520for%2520the%2520future%2520of%2520deep%2520learning.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%250Aby%2520developing%2520next-generation%2520models%2520in%2520a%2520parsimonious%2520manner%2520%2528i.e.%252C%2520achieving%250Agreater%2520potential%2520with%2520simpler%2520models%2529.%2520The%2520key%2520is%2520to%2520drive%2520models%2520using%250Adomain-specific%2520knowledge%252C%2520such%2520as%2520symbols%252C%2520logic%252C%2520and%2520formulas%252C%2520instead%2520of%250Arelying%2520on%2520the%2520scaling%2520law.%2520This%2520approach%2520allows%2520us%2520to%2520build%2520a%2520framework%2520that%250Auses%2520this%2520knowledge%2520as%2520%2522building%2520blocks%2522%2520to%2520achieve%2520parsimony%2520in%2520model%2520design%252C%250Atraining%252C%2520and%2520interpretation.%2520Empirical%2520results%2520show%2520that%2520our%2520methods%2520surpass%250Athose%2520that%2520typically%2520follow%2520the%2520scaling%2520law.%2520We%2520also%2520demonstrate%2520the%250Aapplication%2520of%2520our%2520framework%2520in%2520AI%2520for%2520science%252C%2520specifically%2520in%2520the%2520problem%2520of%250Adrug-drug%2520interaction%2520prediction.%2520We%2520hope%2520our%2520research%2520can%2520foster%2520more%2520diverse%250Atechnical%2520roadmaps%2520in%2520the%2520era%2520of%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Aware%20Parsimony%20Learning%3A%20A%20Perspective%20from%20Relational%20Graphs&entry.906535625=Quanming%20Yao%20and%20Yongqi%20Zhang%20and%20Yaqing%20Wang%20and%20Nan%20Yin%20and%20James%20Kwok%20and%20Qiang%20Yang&entry.1292438233=%20%20The%20scaling%20law%2C%20which%20involves%20the%20brute-force%20expansion%20of%20training%0Adatasets%20and%20learnable%20parameters%2C%20has%20become%20a%20prevalent%20strategy%20for%0Adeveloping%20more%20robust%20learning%20models.%20However%2C%20due%20to%20bottlenecks%20in%20data%2C%0Acomputation%2C%20and%20trust%2C%20the%20sustainability%20of%20the%20scaling%20law%20is%20a%20serious%0Aconcern%20for%20the%20future%20of%20deep%20learning.%20In%20this%20paper%2C%20we%20address%20this%20issue%0Aby%20developing%20next-generation%20models%20in%20a%20parsimonious%20manner%20%28i.e.%2C%20achieving%0Agreater%20potential%20with%20simpler%20models%29.%20The%20key%20is%20to%20drive%20models%20using%0Adomain-specific%20knowledge%2C%20such%20as%20symbols%2C%20logic%2C%20and%20formulas%2C%20instead%20of%0Arelying%20on%20the%20scaling%20law.%20This%20approach%20allows%20us%20to%20build%20a%20framework%20that%0Auses%20this%20knowledge%20as%20%22building%20blocks%22%20to%20achieve%20parsimony%20in%20model%20design%2C%0Atraining%2C%20and%20interpretation.%20Empirical%20results%20show%20that%20our%20methods%20surpass%0Athose%20that%20typically%20follow%20the%20scaling%20law.%20We%20also%20demonstrate%20the%0Aapplication%20of%20our%20framework%20in%20AI%20for%20science%2C%20specifically%20in%20the%20problem%20of%0Adrug-drug%20interaction%20prediction.%20We%20hope%20our%20research%20can%20foster%20more%20diverse%0Atechnical%20roadmaps%20in%20the%20era%20of%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00478v2&entry.124074799=Read"},
{"title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models", "author": "Zhiyu Tan and Xiaomeng Yang and Luozheng Qin and Mengping Yang and Cheng Zhang and Hao Li", "abstract": "  The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment.\n", "link": "http://arxiv.org/abs/2406.16562v3", "date": "2024-10-10", "relevancy": 1.6909, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVALALIGN%3A%20Supervised%20Fine-Tuning%20Multimodal%20LLMs%20with%20Human-Aligned%0A%20%20Data%20for%20Evaluating%20Text-to-Image%20Models&body=Title%3A%20EVALALIGN%3A%20Supervised%20Fine-Tuning%20Multimodal%20LLMs%20with%20Human-Aligned%0A%20%20Data%20for%20Evaluating%20Text-to-Image%20Models%0AAuthor%3A%20Zhiyu%20Tan%20and%20Xiaomeng%20Yang%20and%20Luozheng%20Qin%20and%20Mengping%20Yang%20and%20Cheng%20Zhang%20and%20Hao%20Li%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20text-to-image%20generative%20models%20have%20been%0Aremarkable.%20Yet%2C%20the%20field%20suffers%20from%20a%20lack%20of%20evaluation%20metrics%20that%0Aaccurately%20reflect%20the%20performance%20of%20these%20models%2C%20particularly%20lacking%0Afine-grained%20metrics%20that%20can%20guide%20the%20optimization%20of%20the%20models.%20In%20this%0Apaper%2C%20we%20propose%20EvalAlign%2C%20a%20metric%20characterized%20by%20its%20accuracy%2C%20stability%2C%0Aand%20fine%20granularity.%20Our%20approach%20leverages%20the%20capabilities%20of%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20pre-trained%20on%20extensive%20data.%20We%20develop%0Aevaluation%20protocols%20that%20focus%20on%20two%20key%20dimensions%3A%20image%20faithfulness%20and%0Atext-image%20alignment.%20Each%20protocol%20comprises%20a%20set%20of%20detailed%2C%20fine-grained%0Ainstructions%20linked%20to%20specific%20scoring%20options%2C%20enabling%20precise%20manual%0Ascoring%20of%20the%20generated%20images.%20We%20supervised%20fine-tune%20%28SFT%29%20the%20MLLM%20to%0Aalign%20with%20human%20evaluative%20judgments%2C%20resulting%20in%20a%20robust%20evaluation%20model.%0AOur%20evaluation%20across%2024%20text-to-image%20generation%20models%20demonstrate%20that%0AEvalAlign%20not%20only%20provides%20superior%20metric%20stability%20but%20also%20aligns%20more%0Aclosely%20with%20human%20preferences%20than%20existing%20metrics%2C%20confirming%20its%0Aeffectiveness%20and%20utility%20in%20model%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16562v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVALALIGN%253A%2520Supervised%2520Fine-Tuning%2520Multimodal%2520LLMs%2520with%2520Human-Aligned%250A%2520%2520Data%2520for%2520Evaluating%2520Text-to-Image%2520Models%26entry.906535625%3DZhiyu%2520Tan%2520and%2520Xiaomeng%2520Yang%2520and%2520Luozheng%2520Qin%2520and%2520Mengping%2520Yang%2520and%2520Cheng%2520Zhang%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520text-to-image%2520generative%2520models%2520have%2520been%250Aremarkable.%2520Yet%252C%2520the%2520field%2520suffers%2520from%2520a%2520lack%2520of%2520evaluation%2520metrics%2520that%250Aaccurately%2520reflect%2520the%2520performance%2520of%2520these%2520models%252C%2520particularly%2520lacking%250Afine-grained%2520metrics%2520that%2520can%2520guide%2520the%2520optimization%2520of%2520the%2520models.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520EvalAlign%252C%2520a%2520metric%2520characterized%2520by%2520its%2520accuracy%252C%2520stability%252C%250Aand%2520fine%2520granularity.%2520Our%2520approach%2520leverages%2520the%2520capabilities%2520of%2520Multimodal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%2520pre-trained%2520on%2520extensive%2520data.%2520We%2520develop%250Aevaluation%2520protocols%2520that%2520focus%2520on%2520two%2520key%2520dimensions%253A%2520image%2520faithfulness%2520and%250Atext-image%2520alignment.%2520Each%2520protocol%2520comprises%2520a%2520set%2520of%2520detailed%252C%2520fine-grained%250Ainstructions%2520linked%2520to%2520specific%2520scoring%2520options%252C%2520enabling%2520precise%2520manual%250Ascoring%2520of%2520the%2520generated%2520images.%2520We%2520supervised%2520fine-tune%2520%2528SFT%2529%2520the%2520MLLM%2520to%250Aalign%2520with%2520human%2520evaluative%2520judgments%252C%2520resulting%2520in%2520a%2520robust%2520evaluation%2520model.%250AOur%2520evaluation%2520across%252024%2520text-to-image%2520generation%2520models%2520demonstrate%2520that%250AEvalAlign%2520not%2520only%2520provides%2520superior%2520metric%2520stability%2520but%2520also%2520aligns%2520more%250Aclosely%2520with%2520human%2520preferences%2520than%2520existing%2520metrics%252C%2520confirming%2520its%250Aeffectiveness%2520and%2520utility%2520in%2520model%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16562v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVALALIGN%3A%20Supervised%20Fine-Tuning%20Multimodal%20LLMs%20with%20Human-Aligned%0A%20%20Data%20for%20Evaluating%20Text-to-Image%20Models&entry.906535625=Zhiyu%20Tan%20and%20Xiaomeng%20Yang%20and%20Luozheng%20Qin%20and%20Mengping%20Yang%20and%20Cheng%20Zhang%20and%20Hao%20Li&entry.1292438233=%20%20The%20recent%20advancements%20in%20text-to-image%20generative%20models%20have%20been%0Aremarkable.%20Yet%2C%20the%20field%20suffers%20from%20a%20lack%20of%20evaluation%20metrics%20that%0Aaccurately%20reflect%20the%20performance%20of%20these%20models%2C%20particularly%20lacking%0Afine-grained%20metrics%20that%20can%20guide%20the%20optimization%20of%20the%20models.%20In%20this%0Apaper%2C%20we%20propose%20EvalAlign%2C%20a%20metric%20characterized%20by%20its%20accuracy%2C%20stability%2C%0Aand%20fine%20granularity.%20Our%20approach%20leverages%20the%20capabilities%20of%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20pre-trained%20on%20extensive%20data.%20We%20develop%0Aevaluation%20protocols%20that%20focus%20on%20two%20key%20dimensions%3A%20image%20faithfulness%20and%0Atext-image%20alignment.%20Each%20protocol%20comprises%20a%20set%20of%20detailed%2C%20fine-grained%0Ainstructions%20linked%20to%20specific%20scoring%20options%2C%20enabling%20precise%20manual%0Ascoring%20of%20the%20generated%20images.%20We%20supervised%20fine-tune%20%28SFT%29%20the%20MLLM%20to%0Aalign%20with%20human%20evaluative%20judgments%2C%20resulting%20in%20a%20robust%20evaluation%20model.%0AOur%20evaluation%20across%2024%20text-to-image%20generation%20models%20demonstrate%20that%0AEvalAlign%20not%20only%20provides%20superior%20metric%20stability%20but%20also%20aligns%20more%0Aclosely%20with%20human%20preferences%20than%20existing%20metrics%2C%20confirming%20its%0Aeffectiveness%20and%20utility%20in%20model%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16562v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


