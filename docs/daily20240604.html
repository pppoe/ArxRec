<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240603.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "EvGGS: A Collaborative Learning Framework for Event-based Generalizable\n  Gaussian Splatting", "author": "Jiaxu Wang and Junhao He and Ziyi Zhang and Mingyuan Sun and Jingkai Sun and Renjing Xu", "abstract": "  Event cameras offer promising advantages such as high dynamic range and low\nlatency, making them well-suited for challenging lighting conditions and\nfast-moving scenarios. However, reconstructing 3D scenes from raw event streams\nis difficult because event data is sparse and does not carry absolute color\ninformation. To release its potential in 3D reconstruction, we propose the\nfirst event-based generalizable 3D reconstruction framework, called EvGGS,\nwhich reconstructs scenes as 3D Gaussians from only event input in a\nfeedforward manner and can generalize to unseen cases without any retraining.\nThis framework includes a depth estimation module, an intensity reconstruction\nmodule, and a Gaussian regression module. These submodules connect in a\ncascading manner, and we collaboratively train them with a designed joint loss\nto make them mutually promote. To facilitate related studies, we build a novel\nevent-based 3D dataset with various material objects and calibrated labels of\ngrayscale images, depth maps, camera poses, and silhouettes. Experiments show\nmodels that have jointly trained significantly outperform those trained\nindividually. Our approach performs better than all baselines in reconstruction\nquality, and depth/intensity predictions with satisfactory rendering speed.\n", "link": "http://arxiv.org/abs/2405.14959v2", "date": "2024-06-03", "relevancy": 3.3128, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7261}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6752}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvGGS%3A%20A%20Collaborative%20Learning%20Framework%20for%20Event-based%20Generalizable%0A%20%20Gaussian%20Splatting&body=Title%3A%20EvGGS%3A%20A%20Collaborative%20Learning%20Framework%20for%20Event-based%20Generalizable%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Jiaxu%20Wang%20and%20Junhao%20He%20and%20Ziyi%20Zhang%20and%20Mingyuan%20Sun%20and%20Jingkai%20Sun%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Event%20cameras%20offer%20promising%20advantages%20such%20as%20high%20dynamic%20range%20and%20low%0Alatency%2C%20making%20them%20well-suited%20for%20challenging%20lighting%20conditions%20and%0Afast-moving%20scenarios.%20However%2C%20reconstructing%203D%20scenes%20from%20raw%20event%20streams%0Ais%20difficult%20because%20event%20data%20is%20sparse%20and%20does%20not%20carry%20absolute%20color%0Ainformation.%20To%20release%20its%20potential%20in%203D%20reconstruction%2C%20we%20propose%20the%0Afirst%20event-based%20generalizable%203D%20reconstruction%20framework%2C%20called%20EvGGS%2C%0Awhich%20reconstructs%20scenes%20as%203D%20Gaussians%20from%20only%20event%20input%20in%20a%0Afeedforward%20manner%20and%20can%20generalize%20to%20unseen%20cases%20without%20any%20retraining.%0AThis%20framework%20includes%20a%20depth%20estimation%20module%2C%20an%20intensity%20reconstruction%0Amodule%2C%20and%20a%20Gaussian%20regression%20module.%20These%20submodules%20connect%20in%20a%0Acascading%20manner%2C%20and%20we%20collaboratively%20train%20them%20with%20a%20designed%20joint%20loss%0Ato%20make%20them%20mutually%20promote.%20To%20facilitate%20related%20studies%2C%20we%20build%20a%20novel%0Aevent-based%203D%20dataset%20with%20various%20material%20objects%20and%20calibrated%20labels%20of%0Agrayscale%20images%2C%20depth%20maps%2C%20camera%20poses%2C%20and%20silhouettes.%20Experiments%20show%0Amodels%20that%20have%20jointly%20trained%20significantly%20outperform%20those%20trained%0Aindividually.%20Our%20approach%20performs%20better%20than%20all%20baselines%20in%20reconstruction%0Aquality%2C%20and%20depth/intensity%20predictions%20with%20satisfactory%20rendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvGGS%253A%2520A%2520Collaborative%2520Learning%2520Framework%2520for%2520Event-based%2520Generalizable%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DJiaxu%2520Wang%2520and%2520Junhao%2520He%2520and%2520Ziyi%2520Zhang%2520and%2520Mingyuan%2520Sun%2520and%2520Jingkai%2520Sun%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520promising%2520advantages%2520such%2520as%2520high%2520dynamic%2520range%2520and%2520low%250Alatency%252C%2520making%2520them%2520well-suited%2520for%2520challenging%2520lighting%2520conditions%2520and%250Afast-moving%2520scenarios.%2520However%252C%2520reconstructing%25203D%2520scenes%2520from%2520raw%2520event%2520streams%250Ais%2520difficult%2520because%2520event%2520data%2520is%2520sparse%2520and%2520does%2520not%2520carry%2520absolute%2520color%250Ainformation.%2520To%2520release%2520its%2520potential%2520in%25203D%2520reconstruction%252C%2520we%2520propose%2520the%250Afirst%2520event-based%2520generalizable%25203D%2520reconstruction%2520framework%252C%2520called%2520EvGGS%252C%250Awhich%2520reconstructs%2520scenes%2520as%25203D%2520Gaussians%2520from%2520only%2520event%2520input%2520in%2520a%250Afeedforward%2520manner%2520and%2520can%2520generalize%2520to%2520unseen%2520cases%2520without%2520any%2520retraining.%250AThis%2520framework%2520includes%2520a%2520depth%2520estimation%2520module%252C%2520an%2520intensity%2520reconstruction%250Amodule%252C%2520and%2520a%2520Gaussian%2520regression%2520module.%2520These%2520submodules%2520connect%2520in%2520a%250Acascading%2520manner%252C%2520and%2520we%2520collaboratively%2520train%2520them%2520with%2520a%2520designed%2520joint%2520loss%250Ato%2520make%2520them%2520mutually%2520promote.%2520To%2520facilitate%2520related%2520studies%252C%2520we%2520build%2520a%2520novel%250Aevent-based%25203D%2520dataset%2520with%2520various%2520material%2520objects%2520and%2520calibrated%2520labels%2520of%250Agrayscale%2520images%252C%2520depth%2520maps%252C%2520camera%2520poses%252C%2520and%2520silhouettes.%2520Experiments%2520show%250Amodels%2520that%2520have%2520jointly%2520trained%2520significantly%2520outperform%2520those%2520trained%250Aindividually.%2520Our%2520approach%2520performs%2520better%2520than%2520all%2520baselines%2520in%2520reconstruction%250Aquality%252C%2520and%2520depth/intensity%2520predictions%2520with%2520satisfactory%2520rendering%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvGGS%3A%20A%20Collaborative%20Learning%20Framework%20for%20Event-based%20Generalizable%0A%20%20Gaussian%20Splatting&entry.906535625=Jiaxu%20Wang%20and%20Junhao%20He%20and%20Ziyi%20Zhang%20and%20Mingyuan%20Sun%20and%20Jingkai%20Sun%20and%20Renjing%20Xu&entry.1292438233=%20%20Event%20cameras%20offer%20promising%20advantages%20such%20as%20high%20dynamic%20range%20and%20low%0Alatency%2C%20making%20them%20well-suited%20for%20challenging%20lighting%20conditions%20and%0Afast-moving%20scenarios.%20However%2C%20reconstructing%203D%20scenes%20from%20raw%20event%20streams%0Ais%20difficult%20because%20event%20data%20is%20sparse%20and%20does%20not%20carry%20absolute%20color%0Ainformation.%20To%20release%20its%20potential%20in%203D%20reconstruction%2C%20we%20propose%20the%0Afirst%20event-based%20generalizable%203D%20reconstruction%20framework%2C%20called%20EvGGS%2C%0Awhich%20reconstructs%20scenes%20as%203D%20Gaussians%20from%20only%20event%20input%20in%20a%0Afeedforward%20manner%20and%20can%20generalize%20to%20unseen%20cases%20without%20any%20retraining.%0AThis%20framework%20includes%20a%20depth%20estimation%20module%2C%20an%20intensity%20reconstruction%0Amodule%2C%20and%20a%20Gaussian%20regression%20module.%20These%20submodules%20connect%20in%20a%0Acascading%20manner%2C%20and%20we%20collaboratively%20train%20them%20with%20a%20designed%20joint%20loss%0Ato%20make%20them%20mutually%20promote.%20To%20facilitate%20related%20studies%2C%20we%20build%20a%20novel%0Aevent-based%203D%20dataset%20with%20various%20material%20objects%20and%20calibrated%20labels%20of%0Agrayscale%20images%2C%20depth%20maps%2C%20camera%20poses%2C%20and%20silhouettes.%20Experiments%20show%0Amodels%20that%20have%20jointly%20trained%20significantly%20outperform%20those%20trained%0Aindividually.%20Our%20approach%20performs%20better%20than%20all%20baselines%20in%20reconstruction%0Aquality%2C%20and%20depth/intensity%20predictions%20with%20satisfactory%20rendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14959v2&entry.124074799=Read"},
{"title": "A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D\n  Reconstruction", "author": "Jianghao Shen and Nan Xue and Tianfu Wu", "abstract": "  Learning 3D scene representation from a single-view image is a long-standing\nfundamental problem in computer vision, with the inherent ambiguity in\npredicting contents unseen from the input view. Built on the recently proposed\n3D Gaussian Splatting (3DGS), the Splatter Image method has made promising\nprogress on fast single-image novel view synthesis via learning a single 3D\nGaussian for each pixel based on the U-Net feature map of an input image.\nHowever, it has limited expressive power to represent occluded components that\nare not observable in the input view. To address this problem, this paper\npresents a Hierarchical Splatter Image method in which a pixel is worth more\nthan one 3D Gaussians. Specifically, each pixel is represented by a parent 3D\nGaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are\nlearned as done in the vanilla Splatter Image. Child 3D Gaussians are learned\nvia a lightweight Multi-Layer Perceptron (MLP) which takes as input the\nprojected image features of a parent 3D Gaussian and the embedding of a target\ncamera view. Both parent and child 3D Gaussians are learned end-to-end in a\nstage-wise way. The joint condition of input image features from eyes of the\nparent Gaussians and the target camera position facilitates learning to\nallocate child Gaussians to ``see the unseen'', recovering the occluded details\nthat are often missed by parent Gaussians.\n  In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D\ndatasets with state-of-the-art performance obtained, especially showing\npromising capabilities of reconstructing occluded contents in the input view.\n", "link": "http://arxiv.org/abs/2405.20310v3", "date": "2024-06-03", "relevancy": 3.1943, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.696}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&body=Title%3A%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction%0AAuthor%3A%20Jianghao%20Shen%20and%20Nan%20Xue%20and%20Tianfu%20Wu%0AAbstract%3A%20%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%20each%20pixel%20is%20represented%20by%20a%20parent%203D%0AGaussian%20and%20a%20small%20number%20of%20child%203D%20Gaussians.%20Parent%203D%20Gaussians%20are%0Alearned%20as%20done%20in%20the%20vanilla%20Splatter%20Image.%20Child%203D%20Gaussians%20are%20learned%0Avia%20a%20lightweight%20Multi-Layer%20Perceptron%20%28MLP%29%20which%20takes%20as%20input%20the%0Aprojected%20image%20features%20of%20a%20parent%203D%20Gaussian%20and%20the%20embedding%20of%20a%20target%0Acamera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%20are%20learned%20end-to-end%20in%20a%0Astage-wise%20way.%20The%20joint%20condition%20of%20input%20image%20features%20from%20eyes%20of%20the%0Aparent%20Gaussians%20and%20the%20target%20camera%20position%20facilitates%20learning%20to%0Aallocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%20recovering%20the%20occluded%20details%0Athat%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20310v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pixel%2520Is%2520Worth%2520More%2520Than%2520One%25203D%2520Gaussians%2520in%2520Single-View%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DJianghao%2520Shen%2520and%2520Nan%2520Xue%2520and%2520Tianfu%2520Wu%26entry.1292438233%3D%2520%2520Learning%25203D%2520scene%2520representation%2520from%2520a%2520single-view%2520image%2520is%2520a%2520long-standing%250Afundamental%2520problem%2520in%2520computer%2520vision%252C%2520with%2520the%2520inherent%2520ambiguity%2520in%250Apredicting%2520contents%2520unseen%2520from%2520the%2520input%2520view.%2520Built%2520on%2520the%2520recently%2520proposed%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520the%2520Splatter%2520Image%2520method%2520has%2520made%2520promising%250Aprogress%2520on%2520fast%2520single-image%2520novel%2520view%2520synthesis%2520via%2520learning%2520a%2520single%25203D%250AGaussian%2520for%2520each%2520pixel%2520based%2520on%2520the%2520U-Net%2520feature%2520map%2520of%2520an%2520input%2520image.%250AHowever%252C%2520it%2520has%2520limited%2520expressive%2520power%2520to%2520represent%2520occluded%2520components%2520that%250Aare%2520not%2520observable%2520in%2520the%2520input%2520view.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%250Apresents%2520a%2520Hierarchical%2520Splatter%2520Image%2520method%2520in%2520which%2520a%2520pixel%2520is%2520worth%2520more%250Athan%2520one%25203D%2520Gaussians.%2520Specifically%252C%2520each%2520pixel%2520is%2520represented%2520by%2520a%2520parent%25203D%250AGaussian%2520and%2520a%2520small%2520number%2520of%2520child%25203D%2520Gaussians.%2520Parent%25203D%2520Gaussians%2520are%250Alearned%2520as%2520done%2520in%2520the%2520vanilla%2520Splatter%2520Image.%2520Child%25203D%2520Gaussians%2520are%2520learned%250Avia%2520a%2520lightweight%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520which%2520takes%2520as%2520input%2520the%250Aprojected%2520image%2520features%2520of%2520a%2520parent%25203D%2520Gaussian%2520and%2520the%2520embedding%2520of%2520a%2520target%250Acamera%2520view.%2520Both%2520parent%2520and%2520child%25203D%2520Gaussians%2520are%2520learned%2520end-to-end%2520in%2520a%250Astage-wise%2520way.%2520The%2520joint%2520condition%2520of%2520input%2520image%2520features%2520from%2520eyes%2520of%2520the%250Aparent%2520Gaussians%2520and%2520the%2520target%2520camera%2520position%2520facilitates%2520learning%2520to%250Aallocate%2520child%2520Gaussians%2520to%2520%2560%2560see%2520the%2520unseen%2527%2527%252C%2520recovering%2520the%2520occluded%2520details%250Athat%2520are%2520often%2520missed%2520by%2520parent%2520Gaussians.%250A%2520%2520In%2520experiments%252C%2520the%2520proposed%2520method%2520is%2520tested%2520on%2520the%2520ShapeNet-SRN%2520and%2520CO3D%250Adatasets%2520with%2520state-of-the-art%2520performance%2520obtained%252C%2520especially%2520showing%250Apromising%2520capabilities%2520of%2520reconstructing%2520occluded%2520contents%2520in%2520the%2520input%2520view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20310v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&entry.906535625=Jianghao%20Shen%20and%20Nan%20Xue%20and%20Tianfu%20Wu&entry.1292438233=%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%20each%20pixel%20is%20represented%20by%20a%20parent%203D%0AGaussian%20and%20a%20small%20number%20of%20child%203D%20Gaussians.%20Parent%203D%20Gaussians%20are%0Alearned%20as%20done%20in%20the%20vanilla%20Splatter%20Image.%20Child%203D%20Gaussians%20are%20learned%0Avia%20a%20lightweight%20Multi-Layer%20Perceptron%20%28MLP%29%20which%20takes%20as%20input%20the%0Aprojected%20image%20features%20of%20a%20parent%203D%20Gaussian%20and%20the%20embedding%20of%20a%20target%0Acamera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%20are%20learned%20end-to-end%20in%20a%0Astage-wise%20way.%20The%20joint%20condition%20of%20input%20image%20features%20from%20eyes%20of%20the%0Aparent%20Gaussians%20and%20the%20target%20camera%20position%20facilitates%20learning%20to%0Aallocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%20recovering%20the%20occluded%20details%0Athat%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20310v3&entry.124074799=Read"},
{"title": "Patch-Wise Self-Supervised Visual Representation Learning: A\n  Fine-Grained Approach", "author": "Ali Javidani and Mohammad Amin Sadeghi and Babak Nadjar Araabi", "abstract": "  Self-supervised visual representation learning traditionally focuses on\nimage-level instance discrimination. Our study introduces an innovative,\nfine-grained dimension by integrating patch-level discrimination into these\nmethodologies. This integration allows for the simultaneous analysis of local\nand global visual features, thereby enriching the quality of the learned\nrepresentations. Initially, the original images undergo spatial augmentation.\nSubsequently, we employ a distinctive photometric patch-level augmentation,\nwhere each patch is individually augmented, independent from other patches\nwithin the same view. This approach generates a diverse training dataset with\ndistinct color variations in each segment. The augmented images are then\nprocessed through a self-distillation learning framework, utilizing the Vision\nTransformer (ViT) as its backbone. The proposed method minimizes the\nrepresentation distances across both image and patch levels to capture details\nfrom macro to micro perspectives. To this end, we present a simple yet\neffective patch-matching algorithm to find the corresponding patches across the\naugmented views. Thanks to the efficient structure of the patch-matching\nalgorithm, our method reduces computational complexity compared to similar\napproaches. Consequently, we achieve an advanced understanding of the model\nwithout adding significant computational requirements. We have extensively\npretrained our method on datasets of varied scales, such as Cifar10,\nImageNet-100, and ImageNet-1K. It demonstrates superior performance over\nstate-of-the-art self-supervised representation learning methods in image\nclassification and downstream tasks, such as copy detection and image\nretrieval. The implementation of our method is accessible on GitHub.\n", "link": "http://arxiv.org/abs/2310.18651v5", "date": "2024-06-03", "relevancy": 2.9014, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6631}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Wise%20Self-Supervised%20Visual%20Representation%20Learning%3A%20A%0A%20%20Fine-Grained%20Approach&body=Title%3A%20Patch-Wise%20Self-Supervised%20Visual%20Representation%20Learning%3A%20A%0A%20%20Fine-Grained%20Approach%0AAuthor%3A%20Ali%20Javidani%20and%20Mohammad%20Amin%20Sadeghi%20and%20Babak%20Nadjar%20Araabi%0AAbstract%3A%20%20%20Self-supervised%20visual%20representation%20learning%20traditionally%20focuses%20on%0Aimage-level%20instance%20discrimination.%20Our%20study%20introduces%20an%20innovative%2C%0Afine-grained%20dimension%20by%20integrating%20patch-level%20discrimination%20into%20these%0Amethodologies.%20This%20integration%20allows%20for%20the%20simultaneous%20analysis%20of%20local%0Aand%20global%20visual%20features%2C%20thereby%20enriching%20the%20quality%20of%20the%20learned%0Arepresentations.%20Initially%2C%20the%20original%20images%20undergo%20spatial%20augmentation.%0ASubsequently%2C%20we%20employ%20a%20distinctive%20photometric%20patch-level%20augmentation%2C%0Awhere%20each%20patch%20is%20individually%20augmented%2C%20independent%20from%20other%20patches%0Awithin%20the%20same%20view.%20This%20approach%20generates%20a%20diverse%20training%20dataset%20with%0Adistinct%20color%20variations%20in%20each%20segment.%20The%20augmented%20images%20are%20then%0Aprocessed%20through%20a%20self-distillation%20learning%20framework%2C%20utilizing%20the%20Vision%0ATransformer%20%28ViT%29%20as%20its%20backbone.%20The%20proposed%20method%20minimizes%20the%0Arepresentation%20distances%20across%20both%20image%20and%20patch%20levels%20to%20capture%20details%0Afrom%20macro%20to%20micro%20perspectives.%20To%20this%20end%2C%20we%20present%20a%20simple%20yet%0Aeffective%20patch-matching%20algorithm%20to%20find%20the%20corresponding%20patches%20across%20the%0Aaugmented%20views.%20Thanks%20to%20the%20efficient%20structure%20of%20the%20patch-matching%0Aalgorithm%2C%20our%20method%20reduces%20computational%20complexity%20compared%20to%20similar%0Aapproaches.%20Consequently%2C%20we%20achieve%20an%20advanced%20understanding%20of%20the%20model%0Awithout%20adding%20significant%20computational%20requirements.%20We%20have%20extensively%0Apretrained%20our%20method%20on%20datasets%20of%20varied%20scales%2C%20such%20as%20Cifar10%2C%0AImageNet-100%2C%20and%20ImageNet-1K.%20It%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20self-supervised%20representation%20learning%20methods%20in%20image%0Aclassification%20and%20downstream%20tasks%2C%20such%20as%20copy%20detection%20and%20image%0Aretrieval.%20The%20implementation%20of%20our%20method%20is%20accessible%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18651v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Wise%2520Self-Supervised%2520Visual%2520Representation%2520Learning%253A%2520A%250A%2520%2520Fine-Grained%2520Approach%26entry.906535625%3DAli%2520Javidani%2520and%2520Mohammad%2520Amin%2520Sadeghi%2520and%2520Babak%2520Nadjar%2520Araabi%26entry.1292438233%3D%2520%2520Self-supervised%2520visual%2520representation%2520learning%2520traditionally%2520focuses%2520on%250Aimage-level%2520instance%2520discrimination.%2520Our%2520study%2520introduces%2520an%2520innovative%252C%250Afine-grained%2520dimension%2520by%2520integrating%2520patch-level%2520discrimination%2520into%2520these%250Amethodologies.%2520This%2520integration%2520allows%2520for%2520the%2520simultaneous%2520analysis%2520of%2520local%250Aand%2520global%2520visual%2520features%252C%2520thereby%2520enriching%2520the%2520quality%2520of%2520the%2520learned%250Arepresentations.%2520Initially%252C%2520the%2520original%2520images%2520undergo%2520spatial%2520augmentation.%250ASubsequently%252C%2520we%2520employ%2520a%2520distinctive%2520photometric%2520patch-level%2520augmentation%252C%250Awhere%2520each%2520patch%2520is%2520individually%2520augmented%252C%2520independent%2520from%2520other%2520patches%250Awithin%2520the%2520same%2520view.%2520This%2520approach%2520generates%2520a%2520diverse%2520training%2520dataset%2520with%250Adistinct%2520color%2520variations%2520in%2520each%2520segment.%2520The%2520augmented%2520images%2520are%2520then%250Aprocessed%2520through%2520a%2520self-distillation%2520learning%2520framework%252C%2520utilizing%2520the%2520Vision%250ATransformer%2520%2528ViT%2529%2520as%2520its%2520backbone.%2520The%2520proposed%2520method%2520minimizes%2520the%250Arepresentation%2520distances%2520across%2520both%2520image%2520and%2520patch%2520levels%2520to%2520capture%2520details%250Afrom%2520macro%2520to%2520micro%2520perspectives.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520simple%2520yet%250Aeffective%2520patch-matching%2520algorithm%2520to%2520find%2520the%2520corresponding%2520patches%2520across%2520the%250Aaugmented%2520views.%2520Thanks%2520to%2520the%2520efficient%2520structure%2520of%2520the%2520patch-matching%250Aalgorithm%252C%2520our%2520method%2520reduces%2520computational%2520complexity%2520compared%2520to%2520similar%250Aapproaches.%2520Consequently%252C%2520we%2520achieve%2520an%2520advanced%2520understanding%2520of%2520the%2520model%250Awithout%2520adding%2520significant%2520computational%2520requirements.%2520We%2520have%2520extensively%250Apretrained%2520our%2520method%2520on%2520datasets%2520of%2520varied%2520scales%252C%2520such%2520as%2520Cifar10%252C%250AImageNet-100%252C%2520and%2520ImageNet-1K.%2520It%2520demonstrates%2520superior%2520performance%2520over%250Astate-of-the-art%2520self-supervised%2520representation%2520learning%2520methods%2520in%2520image%250Aclassification%2520and%2520downstream%2520tasks%252C%2520such%2520as%2520copy%2520detection%2520and%2520image%250Aretrieval.%2520The%2520implementation%2520of%2520our%2520method%2520is%2520accessible%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18651v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Wise%20Self-Supervised%20Visual%20Representation%20Learning%3A%20A%0A%20%20Fine-Grained%20Approach&entry.906535625=Ali%20Javidani%20and%20Mohammad%20Amin%20Sadeghi%20and%20Babak%20Nadjar%20Araabi&entry.1292438233=%20%20Self-supervised%20visual%20representation%20learning%20traditionally%20focuses%20on%0Aimage-level%20instance%20discrimination.%20Our%20study%20introduces%20an%20innovative%2C%0Afine-grained%20dimension%20by%20integrating%20patch-level%20discrimination%20into%20these%0Amethodologies.%20This%20integration%20allows%20for%20the%20simultaneous%20analysis%20of%20local%0Aand%20global%20visual%20features%2C%20thereby%20enriching%20the%20quality%20of%20the%20learned%0Arepresentations.%20Initially%2C%20the%20original%20images%20undergo%20spatial%20augmentation.%0ASubsequently%2C%20we%20employ%20a%20distinctive%20photometric%20patch-level%20augmentation%2C%0Awhere%20each%20patch%20is%20individually%20augmented%2C%20independent%20from%20other%20patches%0Awithin%20the%20same%20view.%20This%20approach%20generates%20a%20diverse%20training%20dataset%20with%0Adistinct%20color%20variations%20in%20each%20segment.%20The%20augmented%20images%20are%20then%0Aprocessed%20through%20a%20self-distillation%20learning%20framework%2C%20utilizing%20the%20Vision%0ATransformer%20%28ViT%29%20as%20its%20backbone.%20The%20proposed%20method%20minimizes%20the%0Arepresentation%20distances%20across%20both%20image%20and%20patch%20levels%20to%20capture%20details%0Afrom%20macro%20to%20micro%20perspectives.%20To%20this%20end%2C%20we%20present%20a%20simple%20yet%0Aeffective%20patch-matching%20algorithm%20to%20find%20the%20corresponding%20patches%20across%20the%0Aaugmented%20views.%20Thanks%20to%20the%20efficient%20structure%20of%20the%20patch-matching%0Aalgorithm%2C%20our%20method%20reduces%20computational%20complexity%20compared%20to%20similar%0Aapproaches.%20Consequently%2C%20we%20achieve%20an%20advanced%20understanding%20of%20the%20model%0Awithout%20adding%20significant%20computational%20requirements.%20We%20have%20extensively%0Apretrained%20our%20method%20on%20datasets%20of%20varied%20scales%2C%20such%20as%20Cifar10%2C%0AImageNet-100%2C%20and%20ImageNet-1K.%20It%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20self-supervised%20representation%20learning%20methods%20in%20image%0Aclassification%20and%20downstream%20tasks%2C%20such%20as%20copy%20detection%20and%20image%0Aretrieval.%20The%20implementation%20of%20our%20method%20is%20accessible%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18651v5&entry.124074799=Read"},
{"title": "Addressing Diverging Training Costs using Local Restoration for Precise\n  Bird's Eye View Map Construction", "author": "Minsu Kim and Giseop Kim and Sunwook Choi", "abstract": "  Recent advancements in Bird's Eye View (BEV) fusion for map construction have\ndemonstrated remarkable mapping of urban environments. However, their deep and\nbulky architecture incurs substantial amounts of backpropagation memory and\ncomputing latency. Consequently, the problem poses an unavoidable bottleneck in\nconstructing high-resolution (HR) BEV maps, as their large-sized features cause\nsignificant increases in costs including GPU memory consumption and computing\nlatency, named diverging training costs issue. Affected by the problem, most\nexisting methods adopt low-resolution (LR) BEV and struggle to estimate the\nprecise locations of urban scene components like road lanes, and sidewalks. As\nthe imprecision leads to risky self-driving, the diverging training costs issue\nhas to be resolved. In this paper, we address the issue with our novel Trumpet\nNeural Network (TNN) mechanism. The framework utilizes LR BEV space and outputs\nan up-sampled semantic BEV map to create a memory-efficient pipeline. To this\nend, we introduce Local Restoration of BEV representation. Specifically, the\nup-sampled BEV representation has severely aliased, blocky signals, and thick\nsemantic labels. Our proposed Local Restoration restores the signals and thins\n(or narrows down) the width of the labels. Our extensive experiments show that\nthe TNN mechanism provides a plug-and-play memory-efficient pipeline, thereby\nenabling the effective estimation of real-sized (or precise) semantic labels\nfor BEV map construction.\n", "link": "http://arxiv.org/abs/2405.01016v2", "date": "2024-06-03", "relevancy": 2.8566, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5889}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5634}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction&body=Title%3A%20Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction%0AAuthor%3A%20Minsu%20Kim%20and%20Giseop%20Kim%20and%20Sunwook%20Choi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Bird%27s%20Eye%20View%20%28BEV%29%20fusion%20for%20map%20construction%20have%0Ademonstrated%20remarkable%20mapping%20of%20urban%20environments.%20However%2C%20their%20deep%20and%0Abulky%20architecture%20incurs%20substantial%20amounts%20of%20backpropagation%20memory%20and%0Acomputing%20latency.%20Consequently%2C%20the%20problem%20poses%20an%20unavoidable%20bottleneck%20in%0Aconstructing%20high-resolution%20%28HR%29%20BEV%20maps%2C%20as%20their%20large-sized%20features%20cause%0Asignificant%20increases%20in%20costs%20including%20GPU%20memory%20consumption%20and%20computing%0Alatency%2C%20named%20diverging%20training%20costs%20issue.%20Affected%20by%20the%20problem%2C%20most%0Aexisting%20methods%20adopt%20low-resolution%20%28LR%29%20BEV%20and%20struggle%20to%20estimate%20the%0Aprecise%20locations%20of%20urban%20scene%20components%20like%20road%20lanes%2C%20and%20sidewalks.%20As%0Athe%20imprecision%20leads%20to%20risky%20self-driving%2C%20the%20diverging%20training%20costs%20issue%0Ahas%20to%20be%20resolved.%20In%20this%20paper%2C%20we%20address%20the%20issue%20with%20our%20novel%20Trumpet%0ANeural%20Network%20%28TNN%29%20mechanism.%20The%20framework%20utilizes%20LR%20BEV%20space%20and%20outputs%0Aan%20up-sampled%20semantic%20BEV%20map%20to%20create%20a%20memory-efficient%20pipeline.%20To%20this%0Aend%2C%20we%20introduce%20Local%20Restoration%20of%20BEV%20representation.%20Specifically%2C%20the%0Aup-sampled%20BEV%20representation%20has%20severely%20aliased%2C%20blocky%20signals%2C%20and%20thick%0Asemantic%20labels.%20Our%20proposed%20Local%20Restoration%20restores%20the%20signals%20and%20thins%0A%28or%20narrows%20down%29%20the%20width%20of%20the%20labels.%20Our%20extensive%20experiments%20show%20that%0Athe%20TNN%20mechanism%20provides%20a%20plug-and-play%20memory-efficient%20pipeline%2C%20thereby%0Aenabling%20the%20effective%20estimation%20of%20real-sized%20%28or%20precise%29%20semantic%20labels%0Afor%20BEV%20map%20construction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Diverging%2520Training%2520Costs%2520using%2520Local%2520Restoration%2520for%2520Precise%250A%2520%2520Bird%2527s%2520Eye%2520View%2520Map%2520Construction%26entry.906535625%3DMinsu%2520Kim%2520and%2520Giseop%2520Kim%2520and%2520Sunwook%2520Choi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520fusion%2520for%2520map%2520construction%2520have%250Ademonstrated%2520remarkable%2520mapping%2520of%2520urban%2520environments.%2520However%252C%2520their%2520deep%2520and%250Abulky%2520architecture%2520incurs%2520substantial%2520amounts%2520of%2520backpropagation%2520memory%2520and%250Acomputing%2520latency.%2520Consequently%252C%2520the%2520problem%2520poses%2520an%2520unavoidable%2520bottleneck%2520in%250Aconstructing%2520high-resolution%2520%2528HR%2529%2520BEV%2520maps%252C%2520as%2520their%2520large-sized%2520features%2520cause%250Asignificant%2520increases%2520in%2520costs%2520including%2520GPU%2520memory%2520consumption%2520and%2520computing%250Alatency%252C%2520named%2520diverging%2520training%2520costs%2520issue.%2520Affected%2520by%2520the%2520problem%252C%2520most%250Aexisting%2520methods%2520adopt%2520low-resolution%2520%2528LR%2529%2520BEV%2520and%2520struggle%2520to%2520estimate%2520the%250Aprecise%2520locations%2520of%2520urban%2520scene%2520components%2520like%2520road%2520lanes%252C%2520and%2520sidewalks.%2520As%250Athe%2520imprecision%2520leads%2520to%2520risky%2520self-driving%252C%2520the%2520diverging%2520training%2520costs%2520issue%250Ahas%2520to%2520be%2520resolved.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520issue%2520with%2520our%2520novel%2520Trumpet%250ANeural%2520Network%2520%2528TNN%2529%2520mechanism.%2520The%2520framework%2520utilizes%2520LR%2520BEV%2520space%2520and%2520outputs%250Aan%2520up-sampled%2520semantic%2520BEV%2520map%2520to%2520create%2520a%2520memory-efficient%2520pipeline.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520Local%2520Restoration%2520of%2520BEV%2520representation.%2520Specifically%252C%2520the%250Aup-sampled%2520BEV%2520representation%2520has%2520severely%2520aliased%252C%2520blocky%2520signals%252C%2520and%2520thick%250Asemantic%2520labels.%2520Our%2520proposed%2520Local%2520Restoration%2520restores%2520the%2520signals%2520and%2520thins%250A%2528or%2520narrows%2520down%2529%2520the%2520width%2520of%2520the%2520labels.%2520Our%2520extensive%2520experiments%2520show%2520that%250Athe%2520TNN%2520mechanism%2520provides%2520a%2520plug-and-play%2520memory-efficient%2520pipeline%252C%2520thereby%250Aenabling%2520the%2520effective%2520estimation%2520of%2520real-sized%2520%2528or%2520precise%2529%2520semantic%2520labels%250Afor%2520BEV%2520map%2520construction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction&entry.906535625=Minsu%20Kim%20and%20Giseop%20Kim%20and%20Sunwook%20Choi&entry.1292438233=%20%20Recent%20advancements%20in%20Bird%27s%20Eye%20View%20%28BEV%29%20fusion%20for%20map%20construction%20have%0Ademonstrated%20remarkable%20mapping%20of%20urban%20environments.%20However%2C%20their%20deep%20and%0Abulky%20architecture%20incurs%20substantial%20amounts%20of%20backpropagation%20memory%20and%0Acomputing%20latency.%20Consequently%2C%20the%20problem%20poses%20an%20unavoidable%20bottleneck%20in%0Aconstructing%20high-resolution%20%28HR%29%20BEV%20maps%2C%20as%20their%20large-sized%20features%20cause%0Asignificant%20increases%20in%20costs%20including%20GPU%20memory%20consumption%20and%20computing%0Alatency%2C%20named%20diverging%20training%20costs%20issue.%20Affected%20by%20the%20problem%2C%20most%0Aexisting%20methods%20adopt%20low-resolution%20%28LR%29%20BEV%20and%20struggle%20to%20estimate%20the%0Aprecise%20locations%20of%20urban%20scene%20components%20like%20road%20lanes%2C%20and%20sidewalks.%20As%0Athe%20imprecision%20leads%20to%20risky%20self-driving%2C%20the%20diverging%20training%20costs%20issue%0Ahas%20to%20be%20resolved.%20In%20this%20paper%2C%20we%20address%20the%20issue%20with%20our%20novel%20Trumpet%0ANeural%20Network%20%28TNN%29%20mechanism.%20The%20framework%20utilizes%20LR%20BEV%20space%20and%20outputs%0Aan%20up-sampled%20semantic%20BEV%20map%20to%20create%20a%20memory-efficient%20pipeline.%20To%20this%0Aend%2C%20we%20introduce%20Local%20Restoration%20of%20BEV%20representation.%20Specifically%2C%20the%0Aup-sampled%20BEV%20representation%20has%20severely%20aliased%2C%20blocky%20signals%2C%20and%20thick%0Asemantic%20labels.%20Our%20proposed%20Local%20Restoration%20restores%20the%20signals%20and%20thins%0A%28or%20narrows%20down%29%20the%20width%20of%20the%20labels.%20Our%20extensive%20experiments%20show%20that%0Athe%20TNN%20mechanism%20provides%20a%20plug-and-play%20memory-efficient%20pipeline%2C%20thereby%0Aenabling%20the%20effective%20estimation%20of%20real-sized%20%28or%20precise%29%20semantic%20labels%0Afor%20BEV%20map%20construction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01016v2&entry.124074799=Read"},
{"title": "ReShader: View-Dependent Highlights for Single Image View-Synthesis", "author": "Avinash Paliwal and Brandon Nguyen and Andrii Tsarov and Nima Khademi Kalantari", "abstract": "  In recent years, novel view synthesis from a single image has seen\nsignificant progress thanks to the rapid advancements in 3D scene\nrepresentation and image inpainting techniques. While the current approaches\nare able to synthesize geometrically consistent novel views, they often do not\nhandle the view-dependent effects properly. Specifically, the highlights in\ntheir synthesized images usually appear to be glued to the surfaces, making the\nnovel views unrealistic. To address this major problem, we make a key\nobservation that the process of synthesizing novel views requires changing the\nshading of the pixels based on the novel camera, and moving them to appropriate\nlocations. Therefore, we propose to split the view synthesis process into two\nindependent tasks of pixel reshading and relocation. During the reshading\nprocess, we take the single image as the input and adjust its shading based on\nthe novel camera. This reshaded image is then used as the input to an existing\nview synthesis method to relocate the pixels and produce the final novel view\nimage. We propose to use a neural network to perform reshading and generate a\nlarge set of synthetic input-reshaded pairs to train our network. We\ndemonstrate that our approach produces plausible novel view images with\nrealistic moving highlights on a variety of real world scenes.\n", "link": "http://arxiv.org/abs/2309.10689v3", "date": "2024-06-03", "relevancy": 2.8133, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5656}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReShader%3A%20View-Dependent%20Highlights%20for%20Single%20Image%20View-Synthesis&body=Title%3A%20ReShader%3A%20View-Dependent%20Highlights%20for%20Single%20Image%20View-Synthesis%0AAuthor%3A%20Avinash%20Paliwal%20and%20Brandon%20Nguyen%20and%20Andrii%20Tsarov%20and%20Nima%20Khademi%20Kalantari%0AAbstract%3A%20%20%20In%20recent%20years%2C%20novel%20view%20synthesis%20from%20a%20single%20image%20has%20seen%0Asignificant%20progress%20thanks%20to%20the%20rapid%20advancements%20in%203D%20scene%0Arepresentation%20and%20image%20inpainting%20techniques.%20While%20the%20current%20approaches%0Aare%20able%20to%20synthesize%20geometrically%20consistent%20novel%20views%2C%20they%20often%20do%20not%0Ahandle%20the%20view-dependent%20effects%20properly.%20Specifically%2C%20the%20highlights%20in%0Atheir%20synthesized%20images%20usually%20appear%20to%20be%20glued%20to%20the%20surfaces%2C%20making%20the%0Anovel%20views%20unrealistic.%20To%20address%20this%20major%20problem%2C%20we%20make%20a%20key%0Aobservation%20that%20the%20process%20of%20synthesizing%20novel%20views%20requires%20changing%20the%0Ashading%20of%20the%20pixels%20based%20on%20the%20novel%20camera%2C%20and%20moving%20them%20to%20appropriate%0Alocations.%20Therefore%2C%20we%20propose%20to%20split%20the%20view%20synthesis%20process%20into%20two%0Aindependent%20tasks%20of%20pixel%20reshading%20and%20relocation.%20During%20the%20reshading%0Aprocess%2C%20we%20take%20the%20single%20image%20as%20the%20input%20and%20adjust%20its%20shading%20based%20on%0Athe%20novel%20camera.%20This%20reshaded%20image%20is%20then%20used%20as%20the%20input%20to%20an%20existing%0Aview%20synthesis%20method%20to%20relocate%20the%20pixels%20and%20produce%20the%20final%20novel%20view%0Aimage.%20We%20propose%20to%20use%20a%20neural%20network%20to%20perform%20reshading%20and%20generate%20a%0Alarge%20set%20of%20synthetic%20input-reshaded%20pairs%20to%20train%20our%20network.%20We%0Ademonstrate%20that%20our%20approach%20produces%20plausible%20novel%20view%20images%20with%0Arealistic%20moving%20highlights%20on%20a%20variety%20of%20real%20world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10689v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReShader%253A%2520View-Dependent%2520Highlights%2520for%2520Single%2520Image%2520View-Synthesis%26entry.906535625%3DAvinash%2520Paliwal%2520and%2520Brandon%2520Nguyen%2520and%2520Andrii%2520Tsarov%2520and%2520Nima%2520Khademi%2520Kalantari%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520novel%2520view%2520synthesis%2520from%2520a%2520single%2520image%2520has%2520seen%250Asignificant%2520progress%2520thanks%2520to%2520the%2520rapid%2520advancements%2520in%25203D%2520scene%250Arepresentation%2520and%2520image%2520inpainting%2520techniques.%2520While%2520the%2520current%2520approaches%250Aare%2520able%2520to%2520synthesize%2520geometrically%2520consistent%2520novel%2520views%252C%2520they%2520often%2520do%2520not%250Ahandle%2520the%2520view-dependent%2520effects%2520properly.%2520Specifically%252C%2520the%2520highlights%2520in%250Atheir%2520synthesized%2520images%2520usually%2520appear%2520to%2520be%2520glued%2520to%2520the%2520surfaces%252C%2520making%2520the%250Anovel%2520views%2520unrealistic.%2520To%2520address%2520this%2520major%2520problem%252C%2520we%2520make%2520a%2520key%250Aobservation%2520that%2520the%2520process%2520of%2520synthesizing%2520novel%2520views%2520requires%2520changing%2520the%250Ashading%2520of%2520the%2520pixels%2520based%2520on%2520the%2520novel%2520camera%252C%2520and%2520moving%2520them%2520to%2520appropriate%250Alocations.%2520Therefore%252C%2520we%2520propose%2520to%2520split%2520the%2520view%2520synthesis%2520process%2520into%2520two%250Aindependent%2520tasks%2520of%2520pixel%2520reshading%2520and%2520relocation.%2520During%2520the%2520reshading%250Aprocess%252C%2520we%2520take%2520the%2520single%2520image%2520as%2520the%2520input%2520and%2520adjust%2520its%2520shading%2520based%2520on%250Athe%2520novel%2520camera.%2520This%2520reshaded%2520image%2520is%2520then%2520used%2520as%2520the%2520input%2520to%2520an%2520existing%250Aview%2520synthesis%2520method%2520to%2520relocate%2520the%2520pixels%2520and%2520produce%2520the%2520final%2520novel%2520view%250Aimage.%2520We%2520propose%2520to%2520use%2520a%2520neural%2520network%2520to%2520perform%2520reshading%2520and%2520generate%2520a%250Alarge%2520set%2520of%2520synthetic%2520input-reshaded%2520pairs%2520to%2520train%2520our%2520network.%2520We%250Ademonstrate%2520that%2520our%2520approach%2520produces%2520plausible%2520novel%2520view%2520images%2520with%250Arealistic%2520moving%2520highlights%2520on%2520a%2520variety%2520of%2520real%2520world%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10689v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReShader%3A%20View-Dependent%20Highlights%20for%20Single%20Image%20View-Synthesis&entry.906535625=Avinash%20Paliwal%20and%20Brandon%20Nguyen%20and%20Andrii%20Tsarov%20and%20Nima%20Khademi%20Kalantari&entry.1292438233=%20%20In%20recent%20years%2C%20novel%20view%20synthesis%20from%20a%20single%20image%20has%20seen%0Asignificant%20progress%20thanks%20to%20the%20rapid%20advancements%20in%203D%20scene%0Arepresentation%20and%20image%20inpainting%20techniques.%20While%20the%20current%20approaches%0Aare%20able%20to%20synthesize%20geometrically%20consistent%20novel%20views%2C%20they%20often%20do%20not%0Ahandle%20the%20view-dependent%20effects%20properly.%20Specifically%2C%20the%20highlights%20in%0Atheir%20synthesized%20images%20usually%20appear%20to%20be%20glued%20to%20the%20surfaces%2C%20making%20the%0Anovel%20views%20unrealistic.%20To%20address%20this%20major%20problem%2C%20we%20make%20a%20key%0Aobservation%20that%20the%20process%20of%20synthesizing%20novel%20views%20requires%20changing%20the%0Ashading%20of%20the%20pixels%20based%20on%20the%20novel%20camera%2C%20and%20moving%20them%20to%20appropriate%0Alocations.%20Therefore%2C%20we%20propose%20to%20split%20the%20view%20synthesis%20process%20into%20two%0Aindependent%20tasks%20of%20pixel%20reshading%20and%20relocation.%20During%20the%20reshading%0Aprocess%2C%20we%20take%20the%20single%20image%20as%20the%20input%20and%20adjust%20its%20shading%20based%20on%0Athe%20novel%20camera.%20This%20reshaded%20image%20is%20then%20used%20as%20the%20input%20to%20an%20existing%0Aview%20synthesis%20method%20to%20relocate%20the%20pixels%20and%20produce%20the%20final%20novel%20view%0Aimage.%20We%20propose%20to%20use%20a%20neural%20network%20to%20perform%20reshading%20and%20generate%20a%0Alarge%20set%20of%20synthetic%20input-reshaded%20pairs%20to%20train%20our%20network.%20We%0Ademonstrate%20that%20our%20approach%20produces%20plausible%20novel%20view%20images%20with%0Arealistic%20moving%20highlights%20on%20a%20variety%20of%20real%20world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10689v3&entry.124074799=Read"},
{"title": "ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and\n  Multilingual Natural Language Generation", "author": "Bang Yang and Fenglin Liu and Yuexian Zou and Xian Wu and Yaowei Wang and David A. Clifton", "abstract": "  Natural Language Generation (NLG) accepts input data in the form of images,\nvideos, or text and generates corresponding natural language text as output.\nExisting NLG methods mainly adopt a supervised approach and rely heavily on\ncoupled data-to-text pairs. However, for many targeted scenarios and for\nnon-English languages, sufficient quantities of labeled data are often not\navailable. To relax the dependency on labeled data of downstream tasks, we\npropose an intuitive and effective zero-shot learning framework, ZeroNLG, which\ncan deal with multiple NLG tasks, including image-to-text (image captioning),\nvideo-to-text (video captioning), and text-to-text (neural machine\ntranslation), across English, Chinese, German, and French within a unified\nframework. ZeroNLG does not require any labeled downstream pairs for training.\nDuring training, ZeroNLG (i) projects different domains (across modalities and\nlanguages) to corresponding coordinates in a shared common latent space; (ii)\nbridges different domains by aligning their corresponding coordinates in this\nspace; and (iii) builds an unsupervised multilingual auto-encoder to learn to\ngenerate text by reconstructing the input text given its coordinate in shared\nlatent space. Consequently, during inference, based on the data-to-text\npipeline, ZeroNLG can generate target sentences across different languages\ngiven the coordinate of input data in the common space. Within this unified\nframework, given visual (imaging or video) data as input, ZeroNLG can perform\nzero-shot visual captioning; given textual sentences as input, ZeroNLG can\nperform zero-shot machine translation. We present the results of extensive\nexperiments on twelve NLG tasks, showing that, without using any labeled\ndownstream pairs for training, ZeroNLG generates high-quality and believable\noutputs and significantly outperforms existing zero-shot methods.\n", "link": "http://arxiv.org/abs/2303.06458v3", "date": "2024-06-03", "relevancy": 2.7774, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5664}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroNLG%3A%20Aligning%20and%20Autoencoding%20Domains%20for%20Zero-Shot%20Multimodal%20and%0A%20%20Multilingual%20Natural%20Language%20Generation&body=Title%3A%20ZeroNLG%3A%20Aligning%20and%20Autoencoding%20Domains%20for%20Zero-Shot%20Multimodal%20and%0A%20%20Multilingual%20Natural%20Language%20Generation%0AAuthor%3A%20Bang%20Yang%20and%20Fenglin%20Liu%20and%20Yuexian%20Zou%20and%20Xian%20Wu%20and%20Yaowei%20Wang%20and%20David%20A.%20Clifton%0AAbstract%3A%20%20%20Natural%20Language%20Generation%20%28NLG%29%20accepts%20input%20data%20in%20the%20form%20of%20images%2C%0Avideos%2C%20or%20text%20and%20generates%20corresponding%20natural%20language%20text%20as%20output.%0AExisting%20NLG%20methods%20mainly%20adopt%20a%20supervised%20approach%20and%20rely%20heavily%20on%0Acoupled%20data-to-text%20pairs.%20However%2C%20for%20many%20targeted%20scenarios%20and%20for%0Anon-English%20languages%2C%20sufficient%20quantities%20of%20labeled%20data%20are%20often%20not%0Aavailable.%20To%20relax%20the%20dependency%20on%20labeled%20data%20of%20downstream%20tasks%2C%20we%0Apropose%20an%20intuitive%20and%20effective%20zero-shot%20learning%20framework%2C%20ZeroNLG%2C%20which%0Acan%20deal%20with%20multiple%20NLG%20tasks%2C%20including%20image-to-text%20%28image%20captioning%29%2C%0Avideo-to-text%20%28video%20captioning%29%2C%20and%20text-to-text%20%28neural%20machine%0Atranslation%29%2C%20across%20English%2C%20Chinese%2C%20German%2C%20and%20French%20within%20a%20unified%0Aframework.%20ZeroNLG%20does%20not%20require%20any%20labeled%20downstream%20pairs%20for%20training.%0ADuring%20training%2C%20ZeroNLG%20%28i%29%20projects%20different%20domains%20%28across%20modalities%20and%0Alanguages%29%20to%20corresponding%20coordinates%20in%20a%20shared%20common%20latent%20space%3B%20%28ii%29%0Abridges%20different%20domains%20by%20aligning%20their%20corresponding%20coordinates%20in%20this%0Aspace%3B%20and%20%28iii%29%20builds%20an%20unsupervised%20multilingual%20auto-encoder%20to%20learn%20to%0Agenerate%20text%20by%20reconstructing%20the%20input%20text%20given%20its%20coordinate%20in%20shared%0Alatent%20space.%20Consequently%2C%20during%20inference%2C%20based%20on%20the%20data-to-text%0Apipeline%2C%20ZeroNLG%20can%20generate%20target%20sentences%20across%20different%20languages%0Agiven%20the%20coordinate%20of%20input%20data%20in%20the%20common%20space.%20Within%20this%20unified%0Aframework%2C%20given%20visual%20%28imaging%20or%20video%29%20data%20as%20input%2C%20ZeroNLG%20can%20perform%0Azero-shot%20visual%20captioning%3B%20given%20textual%20sentences%20as%20input%2C%20ZeroNLG%20can%0Aperform%20zero-shot%20machine%20translation.%20We%20present%20the%20results%20of%20extensive%0Aexperiments%20on%20twelve%20NLG%20tasks%2C%20showing%20that%2C%20without%20using%20any%20labeled%0Adownstream%20pairs%20for%20training%2C%20ZeroNLG%20generates%20high-quality%20and%20believable%0Aoutputs%20and%20significantly%20outperforms%20existing%20zero-shot%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06458v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroNLG%253A%2520Aligning%2520and%2520Autoencoding%2520Domains%2520for%2520Zero-Shot%2520Multimodal%2520and%250A%2520%2520Multilingual%2520Natural%2520Language%2520Generation%26entry.906535625%3DBang%2520Yang%2520and%2520Fenglin%2520Liu%2520and%2520Yuexian%2520Zou%2520and%2520Xian%2520Wu%2520and%2520Yaowei%2520Wang%2520and%2520David%2520A.%2520Clifton%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Generation%2520%2528NLG%2529%2520accepts%2520input%2520data%2520in%2520the%2520form%2520of%2520images%252C%250Avideos%252C%2520or%2520text%2520and%2520generates%2520corresponding%2520natural%2520language%2520text%2520as%2520output.%250AExisting%2520NLG%2520methods%2520mainly%2520adopt%2520a%2520supervised%2520approach%2520and%2520rely%2520heavily%2520on%250Acoupled%2520data-to-text%2520pairs.%2520However%252C%2520for%2520many%2520targeted%2520scenarios%2520and%2520for%250Anon-English%2520languages%252C%2520sufficient%2520quantities%2520of%2520labeled%2520data%2520are%2520often%2520not%250Aavailable.%2520To%2520relax%2520the%2520dependency%2520on%2520labeled%2520data%2520of%2520downstream%2520tasks%252C%2520we%250Apropose%2520an%2520intuitive%2520and%2520effective%2520zero-shot%2520learning%2520framework%252C%2520ZeroNLG%252C%2520which%250Acan%2520deal%2520with%2520multiple%2520NLG%2520tasks%252C%2520including%2520image-to-text%2520%2528image%2520captioning%2529%252C%250Avideo-to-text%2520%2528video%2520captioning%2529%252C%2520and%2520text-to-text%2520%2528neural%2520machine%250Atranslation%2529%252C%2520across%2520English%252C%2520Chinese%252C%2520German%252C%2520and%2520French%2520within%2520a%2520unified%250Aframework.%2520ZeroNLG%2520does%2520not%2520require%2520any%2520labeled%2520downstream%2520pairs%2520for%2520training.%250ADuring%2520training%252C%2520ZeroNLG%2520%2528i%2529%2520projects%2520different%2520domains%2520%2528across%2520modalities%2520and%250Alanguages%2529%2520to%2520corresponding%2520coordinates%2520in%2520a%2520shared%2520common%2520latent%2520space%253B%2520%2528ii%2529%250Abridges%2520different%2520domains%2520by%2520aligning%2520their%2520corresponding%2520coordinates%2520in%2520this%250Aspace%253B%2520and%2520%2528iii%2529%2520builds%2520an%2520unsupervised%2520multilingual%2520auto-encoder%2520to%2520learn%2520to%250Agenerate%2520text%2520by%2520reconstructing%2520the%2520input%2520text%2520given%2520its%2520coordinate%2520in%2520shared%250Alatent%2520space.%2520Consequently%252C%2520during%2520inference%252C%2520based%2520on%2520the%2520data-to-text%250Apipeline%252C%2520ZeroNLG%2520can%2520generate%2520target%2520sentences%2520across%2520different%2520languages%250Agiven%2520the%2520coordinate%2520of%2520input%2520data%2520in%2520the%2520common%2520space.%2520Within%2520this%2520unified%250Aframework%252C%2520given%2520visual%2520%2528imaging%2520or%2520video%2529%2520data%2520as%2520input%252C%2520ZeroNLG%2520can%2520perform%250Azero-shot%2520visual%2520captioning%253B%2520given%2520textual%2520sentences%2520as%2520input%252C%2520ZeroNLG%2520can%250Aperform%2520zero-shot%2520machine%2520translation.%2520We%2520present%2520the%2520results%2520of%2520extensive%250Aexperiments%2520on%2520twelve%2520NLG%2520tasks%252C%2520showing%2520that%252C%2520without%2520using%2520any%2520labeled%250Adownstream%2520pairs%2520for%2520training%252C%2520ZeroNLG%2520generates%2520high-quality%2520and%2520believable%250Aoutputs%2520and%2520significantly%2520outperforms%2520existing%2520zero-shot%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06458v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroNLG%3A%20Aligning%20and%20Autoencoding%20Domains%20for%20Zero-Shot%20Multimodal%20and%0A%20%20Multilingual%20Natural%20Language%20Generation&entry.906535625=Bang%20Yang%20and%20Fenglin%20Liu%20and%20Yuexian%20Zou%20and%20Xian%20Wu%20and%20Yaowei%20Wang%20and%20David%20A.%20Clifton&entry.1292438233=%20%20Natural%20Language%20Generation%20%28NLG%29%20accepts%20input%20data%20in%20the%20form%20of%20images%2C%0Avideos%2C%20or%20text%20and%20generates%20corresponding%20natural%20language%20text%20as%20output.%0AExisting%20NLG%20methods%20mainly%20adopt%20a%20supervised%20approach%20and%20rely%20heavily%20on%0Acoupled%20data-to-text%20pairs.%20However%2C%20for%20many%20targeted%20scenarios%20and%20for%0Anon-English%20languages%2C%20sufficient%20quantities%20of%20labeled%20data%20are%20often%20not%0Aavailable.%20To%20relax%20the%20dependency%20on%20labeled%20data%20of%20downstream%20tasks%2C%20we%0Apropose%20an%20intuitive%20and%20effective%20zero-shot%20learning%20framework%2C%20ZeroNLG%2C%20which%0Acan%20deal%20with%20multiple%20NLG%20tasks%2C%20including%20image-to-text%20%28image%20captioning%29%2C%0Avideo-to-text%20%28video%20captioning%29%2C%20and%20text-to-text%20%28neural%20machine%0Atranslation%29%2C%20across%20English%2C%20Chinese%2C%20German%2C%20and%20French%20within%20a%20unified%0Aframework.%20ZeroNLG%20does%20not%20require%20any%20labeled%20downstream%20pairs%20for%20training.%0ADuring%20training%2C%20ZeroNLG%20%28i%29%20projects%20different%20domains%20%28across%20modalities%20and%0Alanguages%29%20to%20corresponding%20coordinates%20in%20a%20shared%20common%20latent%20space%3B%20%28ii%29%0Abridges%20different%20domains%20by%20aligning%20their%20corresponding%20coordinates%20in%20this%0Aspace%3B%20and%20%28iii%29%20builds%20an%20unsupervised%20multilingual%20auto-encoder%20to%20learn%20to%0Agenerate%20text%20by%20reconstructing%20the%20input%20text%20given%20its%20coordinate%20in%20shared%0Alatent%20space.%20Consequently%2C%20during%20inference%2C%20based%20on%20the%20data-to-text%0Apipeline%2C%20ZeroNLG%20can%20generate%20target%20sentences%20across%20different%20languages%0Agiven%20the%20coordinate%20of%20input%20data%20in%20the%20common%20space.%20Within%20this%20unified%0Aframework%2C%20given%20visual%20%28imaging%20or%20video%29%20data%20as%20input%2C%20ZeroNLG%20can%20perform%0Azero-shot%20visual%20captioning%3B%20given%20textual%20sentences%20as%20input%2C%20ZeroNLG%20can%0Aperform%20zero-shot%20machine%20translation.%20We%20present%20the%20results%20of%20extensive%0Aexperiments%20on%20twelve%20NLG%20tasks%2C%20showing%20that%2C%20without%20using%20any%20labeled%0Adownstream%20pairs%20for%20training%2C%20ZeroNLG%20generates%20high-quality%20and%20believable%0Aoutputs%20and%20significantly%20outperforms%20existing%20zero-shot%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06458v3&entry.124074799=Read"},
{"title": "Automatic Cranial Defect Reconstruction with Self-Supervised Deep\n  Deformable Masked Autoencoders", "author": "Marek Wodzinski and Daria Hemmerling and Mateusz Daniol", "abstract": "  Thousands of people suffer from cranial injuries every year. They require\npersonalized implants that need to be designed and manufactured before the\nreconstruction surgery. The manual design is expensive and time-consuming\nleading to searching for algorithms whose goal is to automatize the process.\nThe problem can be formulated as volumetric shape completion and solved by deep\nneural networks dedicated to supervised image segmentation. However, such an\napproach requires annotating the ground-truth defects which is costly and\ntime-consuming. Usually, the process is replaced with synthetic defect\ngeneration. However, even the synthetic ground-truth generation is\ntime-consuming and limits the data heterogeneity, thus the deep models'\ngeneralizability. In our work, we propose an alternative and simple approach to\nuse a self-supervised masked autoencoder to solve the problem. This approach by\ndesign increases the heterogeneity of the training set and can be seen as a\nform of data augmentation. We compare the proposed method with several\nstate-of-the-art deep neural networks and show both the quantitative and\nqualitative improvement on the SkullBreak and SkullFix datasets. The proposed\nmethod can be used to efficiently reconstruct the cranial defects in real time.\n", "link": "http://arxiv.org/abs/2404.13106v2", "date": "2024-06-03", "relevancy": 2.7467, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5469}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Cranial%20Defect%20Reconstruction%20with%20Self-Supervised%20Deep%0A%20%20Deformable%20Masked%20Autoencoders&body=Title%3A%20Automatic%20Cranial%20Defect%20Reconstruction%20with%20Self-Supervised%20Deep%0A%20%20Deformable%20Masked%20Autoencoders%0AAuthor%3A%20Marek%20Wodzinski%20and%20Daria%20Hemmerling%20and%20Mateusz%20Daniol%0AAbstract%3A%20%20%20Thousands%20of%20people%20suffer%20from%20cranial%20injuries%20every%20year.%20They%20require%0Apersonalized%20implants%20that%20need%20to%20be%20designed%20and%20manufactured%20before%20the%0Areconstruction%20surgery.%20The%20manual%20design%20is%20expensive%20and%20time-consuming%0Aleading%20to%20searching%20for%20algorithms%20whose%20goal%20is%20to%20automatize%20the%20process.%0AThe%20problem%20can%20be%20formulated%20as%20volumetric%20shape%20completion%20and%20solved%20by%20deep%0Aneural%20networks%20dedicated%20to%20supervised%20image%20segmentation.%20However%2C%20such%20an%0Aapproach%20requires%20annotating%20the%20ground-truth%20defects%20which%20is%20costly%20and%0Atime-consuming.%20Usually%2C%20the%20process%20is%20replaced%20with%20synthetic%20defect%0Ageneration.%20However%2C%20even%20the%20synthetic%20ground-truth%20generation%20is%0Atime-consuming%20and%20limits%20the%20data%20heterogeneity%2C%20thus%20the%20deep%20models%27%0Ageneralizability.%20In%20our%20work%2C%20we%20propose%20an%20alternative%20and%20simple%20approach%20to%0Ause%20a%20self-supervised%20masked%20autoencoder%20to%20solve%20the%20problem.%20This%20approach%20by%0Adesign%20increases%20the%20heterogeneity%20of%20the%20training%20set%20and%20can%20be%20seen%20as%20a%0Aform%20of%20data%20augmentation.%20We%20compare%20the%20proposed%20method%20with%20several%0Astate-of-the-art%20deep%20neural%20networks%20and%20show%20both%20the%20quantitative%20and%0Aqualitative%20improvement%20on%20the%20SkullBreak%20and%20SkullFix%20datasets.%20The%20proposed%0Amethod%20can%20be%20used%20to%20efficiently%20reconstruct%20the%20cranial%20defects%20in%20real%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13106v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Cranial%2520Defect%2520Reconstruction%2520with%2520Self-Supervised%2520Deep%250A%2520%2520Deformable%2520Masked%2520Autoencoders%26entry.906535625%3DMarek%2520Wodzinski%2520and%2520Daria%2520Hemmerling%2520and%2520Mateusz%2520Daniol%26entry.1292438233%3D%2520%2520Thousands%2520of%2520people%2520suffer%2520from%2520cranial%2520injuries%2520every%2520year.%2520They%2520require%250Apersonalized%2520implants%2520that%2520need%2520to%2520be%2520designed%2520and%2520manufactured%2520before%2520the%250Areconstruction%2520surgery.%2520The%2520manual%2520design%2520is%2520expensive%2520and%2520time-consuming%250Aleading%2520to%2520searching%2520for%2520algorithms%2520whose%2520goal%2520is%2520to%2520automatize%2520the%2520process.%250AThe%2520problem%2520can%2520be%2520formulated%2520as%2520volumetric%2520shape%2520completion%2520and%2520solved%2520by%2520deep%250Aneural%2520networks%2520dedicated%2520to%2520supervised%2520image%2520segmentation.%2520However%252C%2520such%2520an%250Aapproach%2520requires%2520annotating%2520the%2520ground-truth%2520defects%2520which%2520is%2520costly%2520and%250Atime-consuming.%2520Usually%252C%2520the%2520process%2520is%2520replaced%2520with%2520synthetic%2520defect%250Ageneration.%2520However%252C%2520even%2520the%2520synthetic%2520ground-truth%2520generation%2520is%250Atime-consuming%2520and%2520limits%2520the%2520data%2520heterogeneity%252C%2520thus%2520the%2520deep%2520models%2527%250Ageneralizability.%2520In%2520our%2520work%252C%2520we%2520propose%2520an%2520alternative%2520and%2520simple%2520approach%2520to%250Ause%2520a%2520self-supervised%2520masked%2520autoencoder%2520to%2520solve%2520the%2520problem.%2520This%2520approach%2520by%250Adesign%2520increases%2520the%2520heterogeneity%2520of%2520the%2520training%2520set%2520and%2520can%2520be%2520seen%2520as%2520a%250Aform%2520of%2520data%2520augmentation.%2520We%2520compare%2520the%2520proposed%2520method%2520with%2520several%250Astate-of-the-art%2520deep%2520neural%2520networks%2520and%2520show%2520both%2520the%2520quantitative%2520and%250Aqualitative%2520improvement%2520on%2520the%2520SkullBreak%2520and%2520SkullFix%2520datasets.%2520The%2520proposed%250Amethod%2520can%2520be%2520used%2520to%2520efficiently%2520reconstruct%2520the%2520cranial%2520defects%2520in%2520real%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13106v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Cranial%20Defect%20Reconstruction%20with%20Self-Supervised%20Deep%0A%20%20Deformable%20Masked%20Autoencoders&entry.906535625=Marek%20Wodzinski%20and%20Daria%20Hemmerling%20and%20Mateusz%20Daniol&entry.1292438233=%20%20Thousands%20of%20people%20suffer%20from%20cranial%20injuries%20every%20year.%20They%20require%0Apersonalized%20implants%20that%20need%20to%20be%20designed%20and%20manufactured%20before%20the%0Areconstruction%20surgery.%20The%20manual%20design%20is%20expensive%20and%20time-consuming%0Aleading%20to%20searching%20for%20algorithms%20whose%20goal%20is%20to%20automatize%20the%20process.%0AThe%20problem%20can%20be%20formulated%20as%20volumetric%20shape%20completion%20and%20solved%20by%20deep%0Aneural%20networks%20dedicated%20to%20supervised%20image%20segmentation.%20However%2C%20such%20an%0Aapproach%20requires%20annotating%20the%20ground-truth%20defects%20which%20is%20costly%20and%0Atime-consuming.%20Usually%2C%20the%20process%20is%20replaced%20with%20synthetic%20defect%0Ageneration.%20However%2C%20even%20the%20synthetic%20ground-truth%20generation%20is%0Atime-consuming%20and%20limits%20the%20data%20heterogeneity%2C%20thus%20the%20deep%20models%27%0Ageneralizability.%20In%20our%20work%2C%20we%20propose%20an%20alternative%20and%20simple%20approach%20to%0Ause%20a%20self-supervised%20masked%20autoencoder%20to%20solve%20the%20problem.%20This%20approach%20by%0Adesign%20increases%20the%20heterogeneity%20of%20the%20training%20set%20and%20can%20be%20seen%20as%20a%0Aform%20of%20data%20augmentation.%20We%20compare%20the%20proposed%20method%20with%20several%0Astate-of-the-art%20deep%20neural%20networks%20and%20show%20both%20the%20quantitative%20and%0Aqualitative%20improvement%20on%20the%20SkullBreak%20and%20SkullFix%20datasets.%20The%20proposed%0Amethod%20can%20be%20used%20to%20efficiently%20reconstruct%20the%20cranial%20defects%20in%20real%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13106v2&entry.124074799=Read"},
{"title": "Weak Augmentation Guided Relational Self-Supervised Learning", "author": "Mingkai Zheng and Shan You and Fei Wang and Chen Qian and Changshui Zhang and Xiaogang Wang and Chang Xu", "abstract": "  Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduce a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. To boost the performance, we\nargue that weak augmentations matter to represent a more reliable relation, and\nleverage momentum strategy for practical efficiency. The designed asymmetric\npredictor head and an InfoNCE warm-up strategy enhance the robustness to\nhyper-parameters and benefit the resulting performance. Experimental results\nshow that our proposed ReSSL substantially outperforms the state-of-the-art\nmethods across different network architectures, including various lightweight\nnetworks (\\eg, EfficientNet and MobileNet).\n", "link": "http://arxiv.org/abs/2203.08717v3", "date": "2024-06-03", "relevancy": 2.5868, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4929}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak%20Augmentation%20Guided%20Relational%20Self-Supervised%20Learning&body=Title%3A%20Weak%20Augmentation%20Guided%20Relational%20Self-Supervised%20Learning%0AAuthor%3A%20Mingkai%20Zheng%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Changshui%20Zhang%20and%20Xiaogang%20Wang%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Self-supervised%20Learning%20%28SSL%29%20including%20the%20mainstream%20contrastive%20learning%0Ahas%20achieved%20great%20success%20in%20learning%20visual%20representations%20without%20data%0Aannotations.%20However%2C%20most%20methods%20mainly%20focus%20on%20the%20instance%20level%0Ainformation%20%28%5Cie%2C%20the%20different%20augmented%20images%20of%20the%20same%20instance%20should%0Ahave%20the%20same%20feature%20or%20cluster%20into%20the%20same%20class%29%2C%20but%20there%20is%20a%20lack%20of%0Aattention%20on%20the%20relationships%20between%20different%20instances.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20SSL%20paradigm%2C%20which%20we%20term%20as%20relational%20self-supervised%0Alearning%20%28ReSSL%29%20framework%20that%20learns%20representations%20by%20modeling%20the%0Arelationship%20between%20different%20instances.%20Specifically%2C%20our%20proposed%20method%0Aemploys%20sharpened%20distribution%20of%20pairwise%20similarities%20among%20different%0Ainstances%20as%20%5Ctextit%7Brelation%7D%20metric%2C%20which%20is%20thus%20utilized%20to%20match%20the%0Afeature%20embeddings%20of%20different%20augmentations.%20To%20boost%20the%20performance%2C%20we%0Aargue%20that%20weak%20augmentations%20matter%20to%20represent%20a%20more%20reliable%20relation%2C%20and%0Aleverage%20momentum%20strategy%20for%20practical%20efficiency.%20The%20designed%20asymmetric%0Apredictor%20head%20and%20an%20InfoNCE%20warm-up%20strategy%20enhance%20the%20robustness%20to%0Ahyper-parameters%20and%20benefit%20the%20resulting%20performance.%20Experimental%20results%0Ashow%20that%20our%20proposed%20ReSSL%20substantially%20outperforms%20the%20state-of-the-art%0Amethods%20across%20different%20network%20architectures%2C%20including%20various%20lightweight%0Anetworks%20%28%5Ceg%2C%20EfficientNet%20and%20MobileNet%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.08717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak%2520Augmentation%2520Guided%2520Relational%2520Self-Supervised%2520Learning%26entry.906535625%3DMingkai%2520Zheng%2520and%2520Shan%2520You%2520and%2520Fei%2520Wang%2520and%2520Chen%2520Qian%2520and%2520Changshui%2520Zhang%2520and%2520Xiaogang%2520Wang%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Self-supervised%2520Learning%2520%2528SSL%2529%2520including%2520the%2520mainstream%2520contrastive%2520learning%250Ahas%2520achieved%2520great%2520success%2520in%2520learning%2520visual%2520representations%2520without%2520data%250Aannotations.%2520However%252C%2520most%2520methods%2520mainly%2520focus%2520on%2520the%2520instance%2520level%250Ainformation%2520%2528%255Cie%252C%2520the%2520different%2520augmented%2520images%2520of%2520the%2520same%2520instance%2520should%250Ahave%2520the%2520same%2520feature%2520or%2520cluster%2520into%2520the%2520same%2520class%2529%252C%2520but%2520there%2520is%2520a%2520lack%2520of%250Aattention%2520on%2520the%2520relationships%2520between%2520different%2520instances.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520SSL%2520paradigm%252C%2520which%2520we%2520term%2520as%2520relational%2520self-supervised%250Alearning%2520%2528ReSSL%2529%2520framework%2520that%2520learns%2520representations%2520by%2520modeling%2520the%250Arelationship%2520between%2520different%2520instances.%2520Specifically%252C%2520our%2520proposed%2520method%250Aemploys%2520sharpened%2520distribution%2520of%2520pairwise%2520similarities%2520among%2520different%250Ainstances%2520as%2520%255Ctextit%257Brelation%257D%2520metric%252C%2520which%2520is%2520thus%2520utilized%2520to%2520match%2520the%250Afeature%2520embeddings%2520of%2520different%2520augmentations.%2520To%2520boost%2520the%2520performance%252C%2520we%250Aargue%2520that%2520weak%2520augmentations%2520matter%2520to%2520represent%2520a%2520more%2520reliable%2520relation%252C%2520and%250Aleverage%2520momentum%2520strategy%2520for%2520practical%2520efficiency.%2520The%2520designed%2520asymmetric%250Apredictor%2520head%2520and%2520an%2520InfoNCE%2520warm-up%2520strategy%2520enhance%2520the%2520robustness%2520to%250Ahyper-parameters%2520and%2520benefit%2520the%2520resulting%2520performance.%2520Experimental%2520results%250Ashow%2520that%2520our%2520proposed%2520ReSSL%2520substantially%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520across%2520different%2520network%2520architectures%252C%2520including%2520various%2520lightweight%250Anetworks%2520%2528%255Ceg%252C%2520EfficientNet%2520and%2520MobileNet%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.08717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak%20Augmentation%20Guided%20Relational%20Self-Supervised%20Learning&entry.906535625=Mingkai%20Zheng%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Changshui%20Zhang%20and%20Xiaogang%20Wang%20and%20Chang%20Xu&entry.1292438233=%20%20Self-supervised%20Learning%20%28SSL%29%20including%20the%20mainstream%20contrastive%20learning%0Ahas%20achieved%20great%20success%20in%20learning%20visual%20representations%20without%20data%0Aannotations.%20However%2C%20most%20methods%20mainly%20focus%20on%20the%20instance%20level%0Ainformation%20%28%5Cie%2C%20the%20different%20augmented%20images%20of%20the%20same%20instance%20should%0Ahave%20the%20same%20feature%20or%20cluster%20into%20the%20same%20class%29%2C%20but%20there%20is%20a%20lack%20of%0Aattention%20on%20the%20relationships%20between%20different%20instances.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20SSL%20paradigm%2C%20which%20we%20term%20as%20relational%20self-supervised%0Alearning%20%28ReSSL%29%20framework%20that%20learns%20representations%20by%20modeling%20the%0Arelationship%20between%20different%20instances.%20Specifically%2C%20our%20proposed%20method%0Aemploys%20sharpened%20distribution%20of%20pairwise%20similarities%20among%20different%0Ainstances%20as%20%5Ctextit%7Brelation%7D%20metric%2C%20which%20is%20thus%20utilized%20to%20match%20the%0Afeature%20embeddings%20of%20different%20augmentations.%20To%20boost%20the%20performance%2C%20we%0Aargue%20that%20weak%20augmentations%20matter%20to%20represent%20a%20more%20reliable%20relation%2C%20and%0Aleverage%20momentum%20strategy%20for%20practical%20efficiency.%20The%20designed%20asymmetric%0Apredictor%20head%20and%20an%20InfoNCE%20warm-up%20strategy%20enhance%20the%20robustness%20to%0Ahyper-parameters%20and%20benefit%20the%20resulting%20performance.%20Experimental%20results%0Ashow%20that%20our%20proposed%20ReSSL%20substantially%20outperforms%20the%20state-of-the-art%0Amethods%20across%20different%20network%20architectures%2C%20including%20various%20lightweight%0Anetworks%20%28%5Ceg%2C%20EfficientNet%20and%20MobileNet%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.08717v3&entry.124074799=Read"},
{"title": "Federated Learning under Partially Class-Disjoint Data via Manifold\n  Reshaping", "author": "Ziqing Fan and Jiangchao Yao and Ruipeng Zhang and Lingjuan Lyu and Ya Zhang and Yanfeng Wang", "abstract": "  Statistical heterogeneity severely limits the performance of federated\nlearning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn,\nto alleviate this problem. Despite effectiveness, their considered scenario\ngenerally requires samples from almost all classes during the local training of\neach client, although some covariate shifts may exist among clients. In fact,\nthe natural case of partially class-disjoint data (PCDD), where each client\ncontributes a few classes (instead of all classes) of samples, is practical yet\nunderexplored. Specifically, the unique collapse and invasion characteristics\nof PCDD can induce the biased optimization direction in local training, which\nprevents the efficiency of federated learning. To address this dilemma, we\npropose a manifold reshaping approach called FedMR to calibrate the feature\nspace of local training. Our FedMR adds two interplaying losses to the vanilla\nfederated learning: one is intra-class loss to decorrelate feature dimensions\nfor anti-collapse; and the other one is inter-class loss to guarantee the\nproper margin among categories in the feature expansion. We conduct extensive\nexperiments on a range of datasets to demonstrate that our FedMR achieves much\nhigher accuracy and better communication efficiency. Source code is available\nat: https://github.com/MediaBrain-SJTU/FedMR.git.\n", "link": "http://arxiv.org/abs/2405.18983v2", "date": "2024-06-03", "relevancy": 2.5829, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping&body=Title%3A%20Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping%0AAuthor%3A%20Ziqing%20Fan%20and%20Jiangchao%20Yao%20and%20Ruipeng%20Zhang%20and%20Lingjuan%20Lyu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Statistical%20heterogeneity%20severely%20limits%20the%20performance%20of%20federated%0Alearning%20%28FL%29%2C%20motivating%20several%20explorations%20e.g.%2C%20FedProx%2C%20MOON%20and%20FedDyn%2C%0Ato%20alleviate%20this%20problem.%20Despite%20effectiveness%2C%20their%20considered%20scenario%0Agenerally%20requires%20samples%20from%20almost%20all%20classes%20during%20the%20local%20training%20of%0Aeach%20client%2C%20although%20some%20covariate%20shifts%20may%20exist%20among%20clients.%20In%20fact%2C%0Athe%20natural%20case%20of%20partially%20class-disjoint%20data%20%28PCDD%29%2C%20where%20each%20client%0Acontributes%20a%20few%20classes%20%28instead%20of%20all%20classes%29%20of%20samples%2C%20is%20practical%20yet%0Aunderexplored.%20Specifically%2C%20the%20unique%20collapse%20and%20invasion%20characteristics%0Aof%20PCDD%20can%20induce%20the%20biased%20optimization%20direction%20in%20local%20training%2C%20which%0Aprevents%20the%20efficiency%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%0Apropose%20a%20manifold%20reshaping%20approach%20called%20FedMR%20to%20calibrate%20the%20feature%0Aspace%20of%20local%20training.%20Our%20FedMR%20adds%20two%20interplaying%20losses%20to%20the%20vanilla%0Afederated%20learning%3A%20one%20is%20intra-class%20loss%20to%20decorrelate%20feature%20dimensions%0Afor%20anti-collapse%3B%20and%20the%20other%20one%20is%20inter-class%20loss%20to%20guarantee%20the%0Aproper%20margin%20among%20categories%20in%20the%20feature%20expansion.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20range%20of%20datasets%20to%20demonstrate%20that%20our%20FedMR%20achieves%20much%0Ahigher%20accuracy%20and%20better%20communication%20efficiency.%20Source%20code%20is%20available%0Aat%3A%20https%3A//github.com/MediaBrain-SJTU/FedMR.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520under%2520Partially%2520Class-Disjoint%2520Data%2520via%2520Manifold%250A%2520%2520Reshaping%26entry.906535625%3DZiqing%2520Fan%2520and%2520Jiangchao%2520Yao%2520and%2520Ruipeng%2520Zhang%2520and%2520Lingjuan%2520Lyu%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Statistical%2520heterogeneity%2520severely%2520limits%2520the%2520performance%2520of%2520federated%250Alearning%2520%2528FL%2529%252C%2520motivating%2520several%2520explorations%2520e.g.%252C%2520FedProx%252C%2520MOON%2520and%2520FedDyn%252C%250Ato%2520alleviate%2520this%2520problem.%2520Despite%2520effectiveness%252C%2520their%2520considered%2520scenario%250Agenerally%2520requires%2520samples%2520from%2520almost%2520all%2520classes%2520during%2520the%2520local%2520training%2520of%250Aeach%2520client%252C%2520although%2520some%2520covariate%2520shifts%2520may%2520exist%2520among%2520clients.%2520In%2520fact%252C%250Athe%2520natural%2520case%2520of%2520partially%2520class-disjoint%2520data%2520%2528PCDD%2529%252C%2520where%2520each%2520client%250Acontributes%2520a%2520few%2520classes%2520%2528instead%2520of%2520all%2520classes%2529%2520of%2520samples%252C%2520is%2520practical%2520yet%250Aunderexplored.%2520Specifically%252C%2520the%2520unique%2520collapse%2520and%2520invasion%2520characteristics%250Aof%2520PCDD%2520can%2520induce%2520the%2520biased%2520optimization%2520direction%2520in%2520local%2520training%252C%2520which%250Aprevents%2520the%2520efficiency%2520of%2520federated%2520learning.%2520To%2520address%2520this%2520dilemma%252C%2520we%250Apropose%2520a%2520manifold%2520reshaping%2520approach%2520called%2520FedMR%2520to%2520calibrate%2520the%2520feature%250Aspace%2520of%2520local%2520training.%2520Our%2520FedMR%2520adds%2520two%2520interplaying%2520losses%2520to%2520the%2520vanilla%250Afederated%2520learning%253A%2520one%2520is%2520intra-class%2520loss%2520to%2520decorrelate%2520feature%2520dimensions%250Afor%2520anti-collapse%253B%2520and%2520the%2520other%2520one%2520is%2520inter-class%2520loss%2520to%2520guarantee%2520the%250Aproper%2520margin%2520among%2520categories%2520in%2520the%2520feature%2520expansion.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520a%2520range%2520of%2520datasets%2520to%2520demonstrate%2520that%2520our%2520FedMR%2520achieves%2520much%250Ahigher%2520accuracy%2520and%2520better%2520communication%2520efficiency.%2520Source%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/MediaBrain-SJTU/FedMR.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20under%20Partially%20Class-Disjoint%20Data%20via%20Manifold%0A%20%20Reshaping&entry.906535625=Ziqing%20Fan%20and%20Jiangchao%20Yao%20and%20Ruipeng%20Zhang%20and%20Lingjuan%20Lyu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Statistical%20heterogeneity%20severely%20limits%20the%20performance%20of%20federated%0Alearning%20%28FL%29%2C%20motivating%20several%20explorations%20e.g.%2C%20FedProx%2C%20MOON%20and%20FedDyn%2C%0Ato%20alleviate%20this%20problem.%20Despite%20effectiveness%2C%20their%20considered%20scenario%0Agenerally%20requires%20samples%20from%20almost%20all%20classes%20during%20the%20local%20training%20of%0Aeach%20client%2C%20although%20some%20covariate%20shifts%20may%20exist%20among%20clients.%20In%20fact%2C%0Athe%20natural%20case%20of%20partially%20class-disjoint%20data%20%28PCDD%29%2C%20where%20each%20client%0Acontributes%20a%20few%20classes%20%28instead%20of%20all%20classes%29%20of%20samples%2C%20is%20practical%20yet%0Aunderexplored.%20Specifically%2C%20the%20unique%20collapse%20and%20invasion%20characteristics%0Aof%20PCDD%20can%20induce%20the%20biased%20optimization%20direction%20in%20local%20training%2C%20which%0Aprevents%20the%20efficiency%20of%20federated%20learning.%20To%20address%20this%20dilemma%2C%20we%0Apropose%20a%20manifold%20reshaping%20approach%20called%20FedMR%20to%20calibrate%20the%20feature%0Aspace%20of%20local%20training.%20Our%20FedMR%20adds%20two%20interplaying%20losses%20to%20the%20vanilla%0Afederated%20learning%3A%20one%20is%20intra-class%20loss%20to%20decorrelate%20feature%20dimensions%0Afor%20anti-collapse%3B%20and%20the%20other%20one%20is%20inter-class%20loss%20to%20guarantee%20the%0Aproper%20margin%20among%20categories%20in%20the%20feature%20expansion.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20range%20of%20datasets%20to%20demonstrate%20that%20our%20FedMR%20achieves%20much%0Ahigher%20accuracy%20and%20better%20communication%20efficiency.%20Source%20code%20is%20available%0Aat%3A%20https%3A//github.com/MediaBrain-SJTU/FedMR.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18983v2&entry.124074799=Read"},
{"title": "SPEAR:Exact Gradient Inversion of Batches in Federated Learning", "author": "Dimitar I. Dimitrov and Maximilian Baader and Mark Niklas M\u00fcller and Martin Vechev", "abstract": "  Federated learning is a framework for collaborative machine learning where\nclients only share gradient updates and not their private data with a server.\nHowever, it was recently shown that gradient inversion attacks can reconstruct\nthis data from the shared gradients. In the important honest-but-curious\nsetting, existing attacks enable exact reconstruction only for a batch size of\n$b=1$, with larger batches permitting only approximate reconstruction. In this\nwork, we propose SPEAR, the first algorithm reconstructing whole batches with\n$b >1$ exactly. SPEAR combines insights into the explicit low-rank structure of\ngradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced\ngradient sparsity to precisely filter out large numbers of incorrect samples,\nmaking a final reconstruction step tractable. We provide an efficient GPU\nimplementation for fully connected networks and show that it recovers\nhigh-dimensional ImageNet inputs in batches of up to $b \\lesssim 25$ exactly\nwhile scaling to large networks. Finally, we show theoretically that much\nlarger batches can be reconstructed with high probability given exponential\ntime.\n", "link": "http://arxiv.org/abs/2403.03945v2", "date": "2024-06-03", "relevancy": 2.5466, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5228}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5165}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPEAR%3AExact%20Gradient%20Inversion%20of%20Batches%20in%20Federated%20Learning&body=Title%3A%20SPEAR%3AExact%20Gradient%20Inversion%20of%20Batches%20in%20Federated%20Learning%0AAuthor%3A%20Dimitar%20I.%20Dimitrov%20and%20Maximilian%20Baader%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20framework%20for%20collaborative%20machine%20learning%20where%0Aclients%20only%20share%20gradient%20updates%20and%20not%20their%20private%20data%20with%20a%20server.%0AHowever%2C%20it%20was%20recently%20shown%20that%20gradient%20inversion%20attacks%20can%20reconstruct%0Athis%20data%20from%20the%20shared%20gradients.%20In%20the%20important%20honest-but-curious%0Asetting%2C%20existing%20attacks%20enable%20exact%20reconstruction%20only%20for%20a%20batch%20size%20of%0A%24b%3D1%24%2C%20with%20larger%20batches%20permitting%20only%20approximate%20reconstruction.%20In%20this%0Awork%2C%20we%20propose%20SPEAR%2C%20the%20first%20algorithm%20reconstructing%20whole%20batches%20with%0A%24b%20%3E1%24%20exactly.%20SPEAR%20combines%20insights%20into%20the%20explicit%20low-rank%20structure%20of%0Agradients%20with%20a%20sampling-based%20algorithm.%20Crucially%2C%20we%20leverage%20ReLU-induced%0Agradient%20sparsity%20to%20precisely%20filter%20out%20large%20numbers%20of%20incorrect%20samples%2C%0Amaking%20a%20final%20reconstruction%20step%20tractable.%20We%20provide%20an%20efficient%20GPU%0Aimplementation%20for%20fully%20connected%20networks%20and%20show%20that%20it%20recovers%0Ahigh-dimensional%20ImageNet%20inputs%20in%20batches%20of%20up%20to%20%24b%20%5Clesssim%2025%24%20exactly%0Awhile%20scaling%20to%20large%20networks.%20Finally%2C%20we%20show%20theoretically%20that%20much%0Alarger%20batches%20can%20be%20reconstructed%20with%20high%20probability%20given%20exponential%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPEAR%253AExact%2520Gradient%2520Inversion%2520of%2520Batches%2520in%2520Federated%2520Learning%26entry.906535625%3DDimitar%2520I.%2520Dimitrov%2520and%2520Maximilian%2520Baader%2520and%2520Mark%2520Niklas%2520M%25C3%25BCller%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520framework%2520for%2520collaborative%2520machine%2520learning%2520where%250Aclients%2520only%2520share%2520gradient%2520updates%2520and%2520not%2520their%2520private%2520data%2520with%2520a%2520server.%250AHowever%252C%2520it%2520was%2520recently%2520shown%2520that%2520gradient%2520inversion%2520attacks%2520can%2520reconstruct%250Athis%2520data%2520from%2520the%2520shared%2520gradients.%2520In%2520the%2520important%2520honest-but-curious%250Asetting%252C%2520existing%2520attacks%2520enable%2520exact%2520reconstruction%2520only%2520for%2520a%2520batch%2520size%2520of%250A%2524b%253D1%2524%252C%2520with%2520larger%2520batches%2520permitting%2520only%2520approximate%2520reconstruction.%2520In%2520this%250Awork%252C%2520we%2520propose%2520SPEAR%252C%2520the%2520first%2520algorithm%2520reconstructing%2520whole%2520batches%2520with%250A%2524b%2520%253E1%2524%2520exactly.%2520SPEAR%2520combines%2520insights%2520into%2520the%2520explicit%2520low-rank%2520structure%2520of%250Agradients%2520with%2520a%2520sampling-based%2520algorithm.%2520Crucially%252C%2520we%2520leverage%2520ReLU-induced%250Agradient%2520sparsity%2520to%2520precisely%2520filter%2520out%2520large%2520numbers%2520of%2520incorrect%2520samples%252C%250Amaking%2520a%2520final%2520reconstruction%2520step%2520tractable.%2520We%2520provide%2520an%2520efficient%2520GPU%250Aimplementation%2520for%2520fully%2520connected%2520networks%2520and%2520show%2520that%2520it%2520recovers%250Ahigh-dimensional%2520ImageNet%2520inputs%2520in%2520batches%2520of%2520up%2520to%2520%2524b%2520%255Clesssim%252025%2524%2520exactly%250Awhile%2520scaling%2520to%2520large%2520networks.%2520Finally%252C%2520we%2520show%2520theoretically%2520that%2520much%250Alarger%2520batches%2520can%2520be%2520reconstructed%2520with%2520high%2520probability%2520given%2520exponential%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEAR%3AExact%20Gradient%20Inversion%20of%20Batches%20in%20Federated%20Learning&entry.906535625=Dimitar%20I.%20Dimitrov%20and%20Maximilian%20Baader%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev&entry.1292438233=%20%20Federated%20learning%20is%20a%20framework%20for%20collaborative%20machine%20learning%20where%0Aclients%20only%20share%20gradient%20updates%20and%20not%20their%20private%20data%20with%20a%20server.%0AHowever%2C%20it%20was%20recently%20shown%20that%20gradient%20inversion%20attacks%20can%20reconstruct%0Athis%20data%20from%20the%20shared%20gradients.%20In%20the%20important%20honest-but-curious%0Asetting%2C%20existing%20attacks%20enable%20exact%20reconstruction%20only%20for%20a%20batch%20size%20of%0A%24b%3D1%24%2C%20with%20larger%20batches%20permitting%20only%20approximate%20reconstruction.%20In%20this%0Awork%2C%20we%20propose%20SPEAR%2C%20the%20first%20algorithm%20reconstructing%20whole%20batches%20with%0A%24b%20%3E1%24%20exactly.%20SPEAR%20combines%20insights%20into%20the%20explicit%20low-rank%20structure%20of%0Agradients%20with%20a%20sampling-based%20algorithm.%20Crucially%2C%20we%20leverage%20ReLU-induced%0Agradient%20sparsity%20to%20precisely%20filter%20out%20large%20numbers%20of%20incorrect%20samples%2C%0Amaking%20a%20final%20reconstruction%20step%20tractable.%20We%20provide%20an%20efficient%20GPU%0Aimplementation%20for%20fully%20connected%20networks%20and%20show%20that%20it%20recovers%0Ahigh-dimensional%20ImageNet%20inputs%20in%20batches%20of%20up%20to%20%24b%20%5Clesssim%2025%24%20exactly%0Awhile%20scaling%20to%20large%20networks.%20Finally%2C%20we%20show%20theoretically%20that%20much%0Alarger%20batches%20can%20be%20reconstructed%20with%20high%20probability%20given%20exponential%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03945v2&entry.124074799=Read"},
{"title": "Graph-based Forecasting with Missing Data through Spatiotemporal\n  Downsampling", "author": "Ivan Marisca and Cesare Alippi and Filippo Maria Bianchi", "abstract": "  Given a set of synchronous time series, each associated with a sensor-point\nin space and characterized by inter-series relationships, the problem of\nspatiotemporal forecasting consists of predicting future observations for each\npoint. Spatiotemporal graph neural networks achieve striking results by\nrepresenting the relationships across time series as a graph. Nonetheless, most\nexisting methods rely on the often unrealistic assumption that inputs are\nalways available and fail to capture hidden spatiotemporal dynamics when part\nof the data is missing. In this work, we tackle this problem through\nhierarchical spatiotemporal downsampling. The input time series are\nprogressively coarsened over time and space, obtaining a pool of\nrepresentations that capture heterogeneous temporal and spatial dynamics.\nConditioned on observations and missing data patterns, such representations are\ncombined by an interpretable attention mechanism to generate the forecasts. Our\napproach outperforms state-of-the-art methods on synthetic and real-world\nbenchmarks under different missing data distributions, particularly in the\npresence of contiguous blocks of missing values.\n", "link": "http://arxiv.org/abs/2402.10634v2", "date": "2024-06-03", "relevancy": 2.4905, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5306}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4835}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%20Forecasting%20with%20Missing%20Data%20through%20Spatiotemporal%0A%20%20Downsampling&body=Title%3A%20Graph-based%20Forecasting%20with%20Missing%20Data%20through%20Spatiotemporal%0A%20%20Downsampling%0AAuthor%3A%20Ivan%20Marisca%20and%20Cesare%20Alippi%20and%20Filippo%20Maria%20Bianchi%0AAbstract%3A%20%20%20Given%20a%20set%20of%20synchronous%20time%20series%2C%20each%20associated%20with%20a%20sensor-point%0Ain%20space%20and%20characterized%20by%20inter-series%20relationships%2C%20the%20problem%20of%0Aspatiotemporal%20forecasting%20consists%20of%20predicting%20future%20observations%20for%20each%0Apoint.%20Spatiotemporal%20graph%20neural%20networks%20achieve%20striking%20results%20by%0Arepresenting%20the%20relationships%20across%20time%20series%20as%20a%20graph.%20Nonetheless%2C%20most%0Aexisting%20methods%20rely%20on%20the%20often%20unrealistic%20assumption%20that%20inputs%20are%0Aalways%20available%20and%20fail%20to%20capture%20hidden%20spatiotemporal%20dynamics%20when%20part%0Aof%20the%20data%20is%20missing.%20In%20this%20work%2C%20we%20tackle%20this%20problem%20through%0Ahierarchical%20spatiotemporal%20downsampling.%20The%20input%20time%20series%20are%0Aprogressively%20coarsened%20over%20time%20and%20space%2C%20obtaining%20a%20pool%20of%0Arepresentations%20that%20capture%20heterogeneous%20temporal%20and%20spatial%20dynamics.%0AConditioned%20on%20observations%20and%20missing%20data%20patterns%2C%20such%20representations%20are%0Acombined%20by%20an%20interpretable%20attention%20mechanism%20to%20generate%20the%20forecasts.%20Our%0Aapproach%20outperforms%20state-of-the-art%20methods%20on%20synthetic%20and%20real-world%0Abenchmarks%20under%20different%20missing%20data%20distributions%2C%20particularly%20in%20the%0Apresence%20of%20contiguous%20blocks%20of%20missing%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%2520Forecasting%2520with%2520Missing%2520Data%2520through%2520Spatiotemporal%250A%2520%2520Downsampling%26entry.906535625%3DIvan%2520Marisca%2520and%2520Cesare%2520Alippi%2520and%2520Filippo%2520Maria%2520Bianchi%26entry.1292438233%3D%2520%2520Given%2520a%2520set%2520of%2520synchronous%2520time%2520series%252C%2520each%2520associated%2520with%2520a%2520sensor-point%250Ain%2520space%2520and%2520characterized%2520by%2520inter-series%2520relationships%252C%2520the%2520problem%2520of%250Aspatiotemporal%2520forecasting%2520consists%2520of%2520predicting%2520future%2520observations%2520for%2520each%250Apoint.%2520Spatiotemporal%2520graph%2520neural%2520networks%2520achieve%2520striking%2520results%2520by%250Arepresenting%2520the%2520relationships%2520across%2520time%2520series%2520as%2520a%2520graph.%2520Nonetheless%252C%2520most%250Aexisting%2520methods%2520rely%2520on%2520the%2520often%2520unrealistic%2520assumption%2520that%2520inputs%2520are%250Aalways%2520available%2520and%2520fail%2520to%2520capture%2520hidden%2520spatiotemporal%2520dynamics%2520when%2520part%250Aof%2520the%2520data%2520is%2520missing.%2520In%2520this%2520work%252C%2520we%2520tackle%2520this%2520problem%2520through%250Ahierarchical%2520spatiotemporal%2520downsampling.%2520The%2520input%2520time%2520series%2520are%250Aprogressively%2520coarsened%2520over%2520time%2520and%2520space%252C%2520obtaining%2520a%2520pool%2520of%250Arepresentations%2520that%2520capture%2520heterogeneous%2520temporal%2520and%2520spatial%2520dynamics.%250AConditioned%2520on%2520observations%2520and%2520missing%2520data%2520patterns%252C%2520such%2520representations%2520are%250Acombined%2520by%2520an%2520interpretable%2520attention%2520mechanism%2520to%2520generate%2520the%2520forecasts.%2520Our%250Aapproach%2520outperforms%2520state-of-the-art%2520methods%2520on%2520synthetic%2520and%2520real-world%250Abenchmarks%2520under%2520different%2520missing%2520data%2520distributions%252C%2520particularly%2520in%2520the%250Apresence%2520of%2520contiguous%2520blocks%2520of%2520missing%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%20Forecasting%20with%20Missing%20Data%20through%20Spatiotemporal%0A%20%20Downsampling&entry.906535625=Ivan%20Marisca%20and%20Cesare%20Alippi%20and%20Filippo%20Maria%20Bianchi&entry.1292438233=%20%20Given%20a%20set%20of%20synchronous%20time%20series%2C%20each%20associated%20with%20a%20sensor-point%0Ain%20space%20and%20characterized%20by%20inter-series%20relationships%2C%20the%20problem%20of%0Aspatiotemporal%20forecasting%20consists%20of%20predicting%20future%20observations%20for%20each%0Apoint.%20Spatiotemporal%20graph%20neural%20networks%20achieve%20striking%20results%20by%0Arepresenting%20the%20relationships%20across%20time%20series%20as%20a%20graph.%20Nonetheless%2C%20most%0Aexisting%20methods%20rely%20on%20the%20often%20unrealistic%20assumption%20that%20inputs%20are%0Aalways%20available%20and%20fail%20to%20capture%20hidden%20spatiotemporal%20dynamics%20when%20part%0Aof%20the%20data%20is%20missing.%20In%20this%20work%2C%20we%20tackle%20this%20problem%20through%0Ahierarchical%20spatiotemporal%20downsampling.%20The%20input%20time%20series%20are%0Aprogressively%20coarsened%20over%20time%20and%20space%2C%20obtaining%20a%20pool%20of%0Arepresentations%20that%20capture%20heterogeneous%20temporal%20and%20spatial%20dynamics.%0AConditioned%20on%20observations%20and%20missing%20data%20patterns%2C%20such%20representations%20are%0Acombined%20by%20an%20interpretable%20attention%20mechanism%20to%20generate%20the%20forecasts.%20Our%0Aapproach%20outperforms%20state-of-the-art%20methods%20on%20synthetic%20and%20real-world%0Abenchmarks%20under%20different%20missing%20data%20distributions%2C%20particularly%20in%20the%0Apresence%20of%20contiguous%20blocks%20of%20missing%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10634v2&entry.124074799=Read"},
{"title": "Learning Partially Aligned Item Representation for Cross-Domain\n  Sequential Recommendation", "author": "Mingjia Yin and Hao Wang and Wei Guo and Yong Liu and Zhi Li and Sirui Zhao and Defu Lian and Enhong Chen", "abstract": "  Cross-domain sequential recommendation (CDSR) aims to uncover and transfer\nusers' sequential preferences across multiple recommendation domains. While\nsignificant endeavors have been made, they primarily concentrated on developing\nadvanced transfer modules and aligning user representations using\nself-supervised learning techniques. However, the problem of aligning item\nrepresentations has received limited attention, and misaligned item\nrepresentations can potentially lead to sub-optimal sequential modeling and\nuser representation alignment. To this end, we propose a model-agnostic\nframework called \\textbf{C}ross-domain item representation \\textbf{A}lignment\nfor \\textbf{C}ross-\\textbf{D}omain \\textbf{S}equential \\textbf{R}ecommendation\n(\\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively\npartial alignment for item representations. Specifically, we first develop a\nsequence-aware feature augmentation strategy, which captures both collaborative\nand sequential item correlations, thus facilitating holistic item\nrepresentation generation. Next, we conduct an empirical study to investigate\nthe partial representation alignment problem from a spectrum perspective. It\nmotivates us to devise an adaptive spectrum filter, achieving partial alignment\nadaptively. Furthermore, the aligned item representations can be fed into\ndifferent sequential encoders to obtain user representations. The entire\nframework is optimized in a multi-task learning paradigm with an annealing\nstrategy. Extensive experiments have demonstrated that CA-CDSR can surpass\nstate-of-the-art baselines by a significant margin and can effectively align\nitems in representation spaces to enhance performance.\n", "link": "http://arxiv.org/abs/2405.12473v2", "date": "2024-06-03", "relevancy": 2.4661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Partially%20Aligned%20Item%20Representation%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation&body=Title%3A%20Learning%20Partially%20Aligned%20Item%20Representation%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Mingjia%20Yin%20and%20Hao%20Wang%20and%20Wei%20Guo%20and%20Yong%20Liu%20and%20Zhi%20Li%20and%20Sirui%20Zhao%20and%20Defu%20Lian%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Cross-domain%20sequential%20recommendation%20%28CDSR%29%20aims%20to%20uncover%20and%20transfer%0Ausers%27%20sequential%20preferences%20across%20multiple%20recommendation%20domains.%20While%0Asignificant%20endeavors%20have%20been%20made%2C%20they%20primarily%20concentrated%20on%20developing%0Aadvanced%20transfer%20modules%20and%20aligning%20user%20representations%20using%0Aself-supervised%20learning%20techniques.%20However%2C%20the%20problem%20of%20aligning%20item%0Arepresentations%20has%20received%20limited%20attention%2C%20and%20misaligned%20item%0Arepresentations%20can%20potentially%20lead%20to%20sub-optimal%20sequential%20modeling%20and%0Auser%20representation%20alignment.%20To%20this%20end%2C%20we%20propose%20a%20model-agnostic%0Aframework%20called%20%5Ctextbf%7BC%7Dross-domain%20item%20representation%20%5Ctextbf%7BA%7Dlignment%0Afor%20%5Ctextbf%7BC%7Dross-%5Ctextbf%7BD%7Domain%20%5Ctextbf%7BS%7Dequential%20%5Ctextbf%7BR%7Decommendation%0A%28%5Ctextbf%7BCA-CDSR%7D%29%2C%20which%20achieves%20sequence-aware%20generation%20and%20adaptively%0Apartial%20alignment%20for%20item%20representations.%20Specifically%2C%20we%20first%20develop%20a%0Asequence-aware%20feature%20augmentation%20strategy%2C%20which%20captures%20both%20collaborative%0Aand%20sequential%20item%20correlations%2C%20thus%20facilitating%20holistic%20item%0Arepresentation%20generation.%20Next%2C%20we%20conduct%20an%20empirical%20study%20to%20investigate%0Athe%20partial%20representation%20alignment%20problem%20from%20a%20spectrum%20perspective.%20It%0Amotivates%20us%20to%20devise%20an%20adaptive%20spectrum%20filter%2C%20achieving%20partial%20alignment%0Aadaptively.%20Furthermore%2C%20the%20aligned%20item%20representations%20can%20be%20fed%20into%0Adifferent%20sequential%20encoders%20to%20obtain%20user%20representations.%20The%20entire%0Aframework%20is%20optimized%20in%20a%20multi-task%20learning%20paradigm%20with%20an%20annealing%0Astrategy.%20Extensive%20experiments%20have%20demonstrated%20that%20CA-CDSR%20can%20surpass%0Astate-of-the-art%20baselines%20by%20a%20significant%20margin%20and%20can%20effectively%20align%0Aitems%20in%20representation%20spaces%20to%20enhance%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Partially%2520Aligned%2520Item%2520Representation%2520for%2520Cross-Domain%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DMingjia%2520Yin%2520and%2520Hao%2520Wang%2520and%2520Wei%2520Guo%2520and%2520Yong%2520Liu%2520and%2520Zhi%2520Li%2520and%2520Sirui%2520Zhao%2520and%2520Defu%2520Lian%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Cross-domain%2520sequential%2520recommendation%2520%2528CDSR%2529%2520aims%2520to%2520uncover%2520and%2520transfer%250Ausers%2527%2520sequential%2520preferences%2520across%2520multiple%2520recommendation%2520domains.%2520While%250Asignificant%2520endeavors%2520have%2520been%2520made%252C%2520they%2520primarily%2520concentrated%2520on%2520developing%250Aadvanced%2520transfer%2520modules%2520and%2520aligning%2520user%2520representations%2520using%250Aself-supervised%2520learning%2520techniques.%2520However%252C%2520the%2520problem%2520of%2520aligning%2520item%250Arepresentations%2520has%2520received%2520limited%2520attention%252C%2520and%2520misaligned%2520item%250Arepresentations%2520can%2520potentially%2520lead%2520to%2520sub-optimal%2520sequential%2520modeling%2520and%250Auser%2520representation%2520alignment.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520model-agnostic%250Aframework%2520called%2520%255Ctextbf%257BC%257Dross-domain%2520item%2520representation%2520%255Ctextbf%257BA%257Dlignment%250Afor%2520%255Ctextbf%257BC%257Dross-%255Ctextbf%257BD%257Domain%2520%255Ctextbf%257BS%257Dequential%2520%255Ctextbf%257BR%257Decommendation%250A%2528%255Ctextbf%257BCA-CDSR%257D%2529%252C%2520which%2520achieves%2520sequence-aware%2520generation%2520and%2520adaptively%250Apartial%2520alignment%2520for%2520item%2520representations.%2520Specifically%252C%2520we%2520first%2520develop%2520a%250Asequence-aware%2520feature%2520augmentation%2520strategy%252C%2520which%2520captures%2520both%2520collaborative%250Aand%2520sequential%2520item%2520correlations%252C%2520thus%2520facilitating%2520holistic%2520item%250Arepresentation%2520generation.%2520Next%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520to%2520investigate%250Athe%2520partial%2520representation%2520alignment%2520problem%2520from%2520a%2520spectrum%2520perspective.%2520It%250Amotivates%2520us%2520to%2520devise%2520an%2520adaptive%2520spectrum%2520filter%252C%2520achieving%2520partial%2520alignment%250Aadaptively.%2520Furthermore%252C%2520the%2520aligned%2520item%2520representations%2520can%2520be%2520fed%2520into%250Adifferent%2520sequential%2520encoders%2520to%2520obtain%2520user%2520representations.%2520The%2520entire%250Aframework%2520is%2520optimized%2520in%2520a%2520multi-task%2520learning%2520paradigm%2520with%2520an%2520annealing%250Astrategy.%2520Extensive%2520experiments%2520have%2520demonstrated%2520that%2520CA-CDSR%2520can%2520surpass%250Astate-of-the-art%2520baselines%2520by%2520a%2520significant%2520margin%2520and%2520can%2520effectively%2520align%250Aitems%2520in%2520representation%2520spaces%2520to%2520enhance%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Partially%20Aligned%20Item%20Representation%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation&entry.906535625=Mingjia%20Yin%20and%20Hao%20Wang%20and%20Wei%20Guo%20and%20Yong%20Liu%20and%20Zhi%20Li%20and%20Sirui%20Zhao%20and%20Defu%20Lian%20and%20Enhong%20Chen&entry.1292438233=%20%20Cross-domain%20sequential%20recommendation%20%28CDSR%29%20aims%20to%20uncover%20and%20transfer%0Ausers%27%20sequential%20preferences%20across%20multiple%20recommendation%20domains.%20While%0Asignificant%20endeavors%20have%20been%20made%2C%20they%20primarily%20concentrated%20on%20developing%0Aadvanced%20transfer%20modules%20and%20aligning%20user%20representations%20using%0Aself-supervised%20learning%20techniques.%20However%2C%20the%20problem%20of%20aligning%20item%0Arepresentations%20has%20received%20limited%20attention%2C%20and%20misaligned%20item%0Arepresentations%20can%20potentially%20lead%20to%20sub-optimal%20sequential%20modeling%20and%0Auser%20representation%20alignment.%20To%20this%20end%2C%20we%20propose%20a%20model-agnostic%0Aframework%20called%20%5Ctextbf%7BC%7Dross-domain%20item%20representation%20%5Ctextbf%7BA%7Dlignment%0Afor%20%5Ctextbf%7BC%7Dross-%5Ctextbf%7BD%7Domain%20%5Ctextbf%7BS%7Dequential%20%5Ctextbf%7BR%7Decommendation%0A%28%5Ctextbf%7BCA-CDSR%7D%29%2C%20which%20achieves%20sequence-aware%20generation%20and%20adaptively%0Apartial%20alignment%20for%20item%20representations.%20Specifically%2C%20we%20first%20develop%20a%0Asequence-aware%20feature%20augmentation%20strategy%2C%20which%20captures%20both%20collaborative%0Aand%20sequential%20item%20correlations%2C%20thus%20facilitating%20holistic%20item%0Arepresentation%20generation.%20Next%2C%20we%20conduct%20an%20empirical%20study%20to%20investigate%0Athe%20partial%20representation%20alignment%20problem%20from%20a%20spectrum%20perspective.%20It%0Amotivates%20us%20to%20devise%20an%20adaptive%20spectrum%20filter%2C%20achieving%20partial%20alignment%0Aadaptively.%20Furthermore%2C%20the%20aligned%20item%20representations%20can%20be%20fed%20into%0Adifferent%20sequential%20encoders%20to%20obtain%20user%20representations.%20The%20entire%0Aframework%20is%20optimized%20in%20a%20multi-task%20learning%20paradigm%20with%20an%20annealing%0Astrategy.%20Extensive%20experiments%20have%20demonstrated%20that%20CA-CDSR%20can%20surpass%0Astate-of-the-art%20baselines%20by%20a%20significant%20margin%20and%20can%20effectively%20align%0Aitems%20in%20representation%20spaces%20to%20enhance%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12473v2&entry.124074799=Read"},
{"title": "Graph Language Models", "author": "Moritz Plenz and Anette Frank", "abstract": "  While Language Models (LMs) are the workhorses of NLP, their interplay with\nstructured knowledge graphs (KGs) is still actively researched. Current methods\nfor encoding such graphs typically either (i) linearize them for embedding with\nLMs -- which underutilize structural information, or (ii) use Graph Neural\nNetworks (GNNs) to preserve the graph structure -- but GNNs cannot represent\ntext features as well as pretrained LMs. In our work we introduce a novel LM\ntype, the Graph Language Model (GLM), that integrates the strengths of both\napproaches and mitigates their weaknesses. The GLM parameters are initialized\nfrom a pretrained LM to enhance understanding of individual graph concepts and\ntriplets. Simultaneously, we design the GLM's architecture to incorporate graph\nbiases, thereby promoting effective knowledge distribution within the graph.\nThis enables GLMs to process graphs, texts, and interleaved inputs of both.\nEmpirical evaluations on relation classification tasks show that GLM embeddings\nsurpass both LM- and GNN-based baselines in supervised and zero-shot setting,\ndemonstrating their versatility.\n", "link": "http://arxiv.org/abs/2401.07105v3", "date": "2024-06-03", "relevancy": 2.4053, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Language%20Models&body=Title%3A%20Graph%20Language%20Models%0AAuthor%3A%20Moritz%20Plenz%20and%20Anette%20Frank%0AAbstract%3A%20%20%20While%20Language%20Models%20%28LMs%29%20are%20the%20workhorses%20of%20NLP%2C%20their%20interplay%20with%0Astructured%20knowledge%20graphs%20%28KGs%29%20is%20still%20actively%20researched.%20Current%20methods%0Afor%20encoding%20such%20graphs%20typically%20either%20%28i%29%20linearize%20them%20for%20embedding%20with%0ALMs%20--%20which%20underutilize%20structural%20information%2C%20or%20%28ii%29%20use%20Graph%20Neural%0ANetworks%20%28GNNs%29%20to%20preserve%20the%20graph%20structure%20--%20but%20GNNs%20cannot%20represent%0Atext%20features%20as%20well%20as%20pretrained%20LMs.%20In%20our%20work%20we%20introduce%20a%20novel%20LM%0Atype%2C%20the%20Graph%20Language%20Model%20%28GLM%29%2C%20that%20integrates%20the%20strengths%20of%20both%0Aapproaches%20and%20mitigates%20their%20weaknesses.%20The%20GLM%20parameters%20are%20initialized%0Afrom%20a%20pretrained%20LM%20to%20enhance%20understanding%20of%20individual%20graph%20concepts%20and%0Atriplets.%20Simultaneously%2C%20we%20design%20the%20GLM%27s%20architecture%20to%20incorporate%20graph%0Abiases%2C%20thereby%20promoting%20effective%20knowledge%20distribution%20within%20the%20graph.%0AThis%20enables%20GLMs%20to%20process%20graphs%2C%20texts%2C%20and%20interleaved%20inputs%20of%20both.%0AEmpirical%20evaluations%20on%20relation%20classification%20tasks%20show%20that%20GLM%20embeddings%0Asurpass%20both%20LM-%20and%20GNN-based%20baselines%20in%20supervised%20and%20zero-shot%20setting%2C%0Ademonstrating%20their%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07105v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Language%2520Models%26entry.906535625%3DMoritz%2520Plenz%2520and%2520Anette%2520Frank%26entry.1292438233%3D%2520%2520While%2520Language%2520Models%2520%2528LMs%2529%2520are%2520the%2520workhorses%2520of%2520NLP%252C%2520their%2520interplay%2520with%250Astructured%2520knowledge%2520graphs%2520%2528KGs%2529%2520is%2520still%2520actively%2520researched.%2520Current%2520methods%250Afor%2520encoding%2520such%2520graphs%2520typically%2520either%2520%2528i%2529%2520linearize%2520them%2520for%2520embedding%2520with%250ALMs%2520--%2520which%2520underutilize%2520structural%2520information%252C%2520or%2520%2528ii%2529%2520use%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520to%2520preserve%2520the%2520graph%2520structure%2520--%2520but%2520GNNs%2520cannot%2520represent%250Atext%2520features%2520as%2520well%2520as%2520pretrained%2520LMs.%2520In%2520our%2520work%2520we%2520introduce%2520a%2520novel%2520LM%250Atype%252C%2520the%2520Graph%2520Language%2520Model%2520%2528GLM%2529%252C%2520that%2520integrates%2520the%2520strengths%2520of%2520both%250Aapproaches%2520and%2520mitigates%2520their%2520weaknesses.%2520The%2520GLM%2520parameters%2520are%2520initialized%250Afrom%2520a%2520pretrained%2520LM%2520to%2520enhance%2520understanding%2520of%2520individual%2520graph%2520concepts%2520and%250Atriplets.%2520Simultaneously%252C%2520we%2520design%2520the%2520GLM%2527s%2520architecture%2520to%2520incorporate%2520graph%250Abiases%252C%2520thereby%2520promoting%2520effective%2520knowledge%2520distribution%2520within%2520the%2520graph.%250AThis%2520enables%2520GLMs%2520to%2520process%2520graphs%252C%2520texts%252C%2520and%2520interleaved%2520inputs%2520of%2520both.%250AEmpirical%2520evaluations%2520on%2520relation%2520classification%2520tasks%2520show%2520that%2520GLM%2520embeddings%250Asurpass%2520both%2520LM-%2520and%2520GNN-based%2520baselines%2520in%2520supervised%2520and%2520zero-shot%2520setting%252C%250Ademonstrating%2520their%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07105v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Language%20Models&entry.906535625=Moritz%20Plenz%20and%20Anette%20Frank&entry.1292438233=%20%20While%20Language%20Models%20%28LMs%29%20are%20the%20workhorses%20of%20NLP%2C%20their%20interplay%20with%0Astructured%20knowledge%20graphs%20%28KGs%29%20is%20still%20actively%20researched.%20Current%20methods%0Afor%20encoding%20such%20graphs%20typically%20either%20%28i%29%20linearize%20them%20for%20embedding%20with%0ALMs%20--%20which%20underutilize%20structural%20information%2C%20or%20%28ii%29%20use%20Graph%20Neural%0ANetworks%20%28GNNs%29%20to%20preserve%20the%20graph%20structure%20--%20but%20GNNs%20cannot%20represent%0Atext%20features%20as%20well%20as%20pretrained%20LMs.%20In%20our%20work%20we%20introduce%20a%20novel%20LM%0Atype%2C%20the%20Graph%20Language%20Model%20%28GLM%29%2C%20that%20integrates%20the%20strengths%20of%20both%0Aapproaches%20and%20mitigates%20their%20weaknesses.%20The%20GLM%20parameters%20are%20initialized%0Afrom%20a%20pretrained%20LM%20to%20enhance%20understanding%20of%20individual%20graph%20concepts%20and%0Atriplets.%20Simultaneously%2C%20we%20design%20the%20GLM%27s%20architecture%20to%20incorporate%20graph%0Abiases%2C%20thereby%20promoting%20effective%20knowledge%20distribution%20within%20the%20graph.%0AThis%20enables%20GLMs%20to%20process%20graphs%2C%20texts%2C%20and%20interleaved%20inputs%20of%20both.%0AEmpirical%20evaluations%20on%20relation%20classification%20tasks%20show%20that%20GLM%20embeddings%0Asurpass%20both%20LM-%20and%20GNN-based%20baselines%20in%20supervised%20and%20zero-shot%20setting%2C%0Ademonstrating%20their%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07105v3&entry.124074799=Read"},
{"title": "Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM\n  Game", "author": "Pengyu Cheng and Yifan Yang and Jian Li and Yong Dai and Tianhao Hu and Peixin Cao and Nan Du and Xiaolong Li", "abstract": "  Human preference alignment is essential to improve the interaction quality of\nlarge language models (LLMs). Existing alignment methods depend on manually\nannotated preference data to guide the LLM optimization directions. However,\ncontinuously updating LLMs for alignment raises a distribution gap between\nmodel-generated samples and human-annotated responses, hindering training\neffectiveness. To mitigate this issue, previous methods require additional\npreference annotation on newly generated samples to adapt to the shifted\ndistribution, which consumes a large amount of annotation resources. Targeting\nmore efficient human preference optimization, we propose an Adversarial\nPreference Optimization (APO) framework, in which the LLM and the reward model\nupdate alternatively via a min-max game. Through adversarial training, the\nreward model can adapt to the shifted generation distribution of the LLM\nwithout any additional annotation. With comprehensive experiments, we find the\nproposed adversarial training framework further enhances existing alignment\nbaselines in terms of LLM helpfulness and harmlessness. The code is at\nhttps://github.com/Linear95/APO.\n", "link": "http://arxiv.org/abs/2311.08045v4", "date": "2024-06-03", "relevancy": 2.3951, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Preference%20Optimization%3A%20Enhancing%20Your%20Alignment%20via%20RM-LLM%0A%20%20Game&body=Title%3A%20Adversarial%20Preference%20Optimization%3A%20Enhancing%20Your%20Alignment%20via%20RM-LLM%0A%20%20Game%0AAuthor%3A%20Pengyu%20Cheng%20and%20Yifan%20Yang%20and%20Jian%20Li%20and%20Yong%20Dai%20and%20Tianhao%20Hu%20and%20Peixin%20Cao%20and%20Nan%20Du%20and%20Xiaolong%20Li%0AAbstract%3A%20%20%20Human%20preference%20alignment%20is%20essential%20to%20improve%20the%20interaction%20quality%20of%0Alarge%20language%20models%20%28LLMs%29.%20Existing%20alignment%20methods%20depend%20on%20manually%0Aannotated%20preference%20data%20to%20guide%20the%20LLM%20optimization%20directions.%20However%2C%0Acontinuously%20updating%20LLMs%20for%20alignment%20raises%20a%20distribution%20gap%20between%0Amodel-generated%20samples%20and%20human-annotated%20responses%2C%20hindering%20training%0Aeffectiveness.%20To%20mitigate%20this%20issue%2C%20previous%20methods%20require%20additional%0Apreference%20annotation%20on%20newly%20generated%20samples%20to%20adapt%20to%20the%20shifted%0Adistribution%2C%20which%20consumes%20a%20large%20amount%20of%20annotation%20resources.%20Targeting%0Amore%20efficient%20human%20preference%20optimization%2C%20we%20propose%20an%20Adversarial%0APreference%20Optimization%20%28APO%29%20framework%2C%20in%20which%20the%20LLM%20and%20the%20reward%20model%0Aupdate%20alternatively%20via%20a%20min-max%20game.%20Through%20adversarial%20training%2C%20the%0Areward%20model%20can%20adapt%20to%20the%20shifted%20generation%20distribution%20of%20the%20LLM%0Awithout%20any%20additional%20annotation.%20With%20comprehensive%20experiments%2C%20we%20find%20the%0Aproposed%20adversarial%20training%20framework%20further%20enhances%20existing%20alignment%0Abaselines%20in%20terms%20of%20LLM%20helpfulness%20and%20harmlessness.%20The%20code%20is%20at%0Ahttps%3A//github.com/Linear95/APO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08045v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Preference%2520Optimization%253A%2520Enhancing%2520Your%2520Alignment%2520via%2520RM-LLM%250A%2520%2520Game%26entry.906535625%3DPengyu%2520Cheng%2520and%2520Yifan%2520Yang%2520and%2520Jian%2520Li%2520and%2520Yong%2520Dai%2520and%2520Tianhao%2520Hu%2520and%2520Peixin%2520Cao%2520and%2520Nan%2520Du%2520and%2520Xiaolong%2520Li%26entry.1292438233%3D%2520%2520Human%2520preference%2520alignment%2520is%2520essential%2520to%2520improve%2520the%2520interaction%2520quality%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Existing%2520alignment%2520methods%2520depend%2520on%2520manually%250Aannotated%2520preference%2520data%2520to%2520guide%2520the%2520LLM%2520optimization%2520directions.%2520However%252C%250Acontinuously%2520updating%2520LLMs%2520for%2520alignment%2520raises%2520a%2520distribution%2520gap%2520between%250Amodel-generated%2520samples%2520and%2520human-annotated%2520responses%252C%2520hindering%2520training%250Aeffectiveness.%2520To%2520mitigate%2520this%2520issue%252C%2520previous%2520methods%2520require%2520additional%250Apreference%2520annotation%2520on%2520newly%2520generated%2520samples%2520to%2520adapt%2520to%2520the%2520shifted%250Adistribution%252C%2520which%2520consumes%2520a%2520large%2520amount%2520of%2520annotation%2520resources.%2520Targeting%250Amore%2520efficient%2520human%2520preference%2520optimization%252C%2520we%2520propose%2520an%2520Adversarial%250APreference%2520Optimization%2520%2528APO%2529%2520framework%252C%2520in%2520which%2520the%2520LLM%2520and%2520the%2520reward%2520model%250Aupdate%2520alternatively%2520via%2520a%2520min-max%2520game.%2520Through%2520adversarial%2520training%252C%2520the%250Areward%2520model%2520can%2520adapt%2520to%2520the%2520shifted%2520generation%2520distribution%2520of%2520the%2520LLM%250Awithout%2520any%2520additional%2520annotation.%2520With%2520comprehensive%2520experiments%252C%2520we%2520find%2520the%250Aproposed%2520adversarial%2520training%2520framework%2520further%2520enhances%2520existing%2520alignment%250Abaselines%2520in%2520terms%2520of%2520LLM%2520helpfulness%2520and%2520harmlessness.%2520The%2520code%2520is%2520at%250Ahttps%253A//github.com/Linear95/APO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08045v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Preference%20Optimization%3A%20Enhancing%20Your%20Alignment%20via%20RM-LLM%0A%20%20Game&entry.906535625=Pengyu%20Cheng%20and%20Yifan%20Yang%20and%20Jian%20Li%20and%20Yong%20Dai%20and%20Tianhao%20Hu%20and%20Peixin%20Cao%20and%20Nan%20Du%20and%20Xiaolong%20Li&entry.1292438233=%20%20Human%20preference%20alignment%20is%20essential%20to%20improve%20the%20interaction%20quality%20of%0Alarge%20language%20models%20%28LLMs%29.%20Existing%20alignment%20methods%20depend%20on%20manually%0Aannotated%20preference%20data%20to%20guide%20the%20LLM%20optimization%20directions.%20However%2C%0Acontinuously%20updating%20LLMs%20for%20alignment%20raises%20a%20distribution%20gap%20between%0Amodel-generated%20samples%20and%20human-annotated%20responses%2C%20hindering%20training%0Aeffectiveness.%20To%20mitigate%20this%20issue%2C%20previous%20methods%20require%20additional%0Apreference%20annotation%20on%20newly%20generated%20samples%20to%20adapt%20to%20the%20shifted%0Adistribution%2C%20which%20consumes%20a%20large%20amount%20of%20annotation%20resources.%20Targeting%0Amore%20efficient%20human%20preference%20optimization%2C%20we%20propose%20an%20Adversarial%0APreference%20Optimization%20%28APO%29%20framework%2C%20in%20which%20the%20LLM%20and%20the%20reward%20model%0Aupdate%20alternatively%20via%20a%20min-max%20game.%20Through%20adversarial%20training%2C%20the%0Areward%20model%20can%20adapt%20to%20the%20shifted%20generation%20distribution%20of%20the%20LLM%0Awithout%20any%20additional%20annotation.%20With%20comprehensive%20experiments%2C%20we%20find%20the%0Aproposed%20adversarial%20training%20framework%20further%20enhances%20existing%20alignment%0Abaselines%20in%20terms%20of%20LLM%20helpfulness%20and%20harmlessness.%20The%20code%20is%20at%0Ahttps%3A//github.com/Linear95/APO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08045v4&entry.124074799=Read"},
{"title": "Domain Transfer Through Image-to-Image Translation for Uncertainty-Aware\n  Prostate Cancer Classification", "author": "Meng Zhou and Amoon Jamzad and Jason Izard and Alexandre Menard and Robert Siemens and Parvin Mousavi", "abstract": "  Prostate Cancer (PCa) is a prevalent disease among men, and multi-parametric\nMRIs offer a non-invasive method for its detection. While MRI-based deep\nlearning solutions have shown promise in supporting PCa diagnosis, acquiring\nsufficient training data, particularly in local clinics remains challenging.\nOne potential solution is to take advantage of publicly available datasets to\npre-train deep models and fine-tune them on the local data, but multi-source\nMRIs can pose challenges due to cross-domain distribution differences. These\nlimitations hinder the adoption of explainable and reliable deep-learning\nsolutions in local clinics for PCa diagnosis. In this work, we present a novel\napproach for unpaired image-to-image translation of prostate multi-parametric\nMRIs and an uncertainty-aware training approach for classifying clinically\nsignificant PCa, to be applied in data-constrained settings such as local and\nsmall clinics. Our approach involves a novel pipeline for translating unpaired\n3.0T multi-parametric prostate MRIs to 1.5T, thereby augmenting the available\ntraining data. Additionally, we introduce an evidential deep learning approach\nto estimate model uncertainty and employ dataset filtering techniques during\ntraining. Furthermore, we propose a simple, yet efficient Evidential Focal\nLoss, combining focal loss with evidential uncertainty, to train our model\neffectively. Our experiments demonstrate that the proposed method significantly\nimproves the Area Under ROC Curve (AUC) by over 20% compared to the previous\nwork. Our code is available at https://github.com/med-i-lab/DT_UE_PCa\n", "link": "http://arxiv.org/abs/2307.00479v2", "date": "2024-06-03", "relevancy": 2.3774, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6183}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Transfer%20Through%20Image-to-Image%20Translation%20for%20Uncertainty-Aware%0A%20%20Prostate%20Cancer%20Classification&body=Title%3A%20Domain%20Transfer%20Through%20Image-to-Image%20Translation%20for%20Uncertainty-Aware%0A%20%20Prostate%20Cancer%20Classification%0AAuthor%3A%20Meng%20Zhou%20and%20Amoon%20Jamzad%20and%20Jason%20Izard%20and%20Alexandre%20Menard%20and%20Robert%20Siemens%20and%20Parvin%20Mousavi%0AAbstract%3A%20%20%20Prostate%20Cancer%20%28PCa%29%20is%20a%20prevalent%20disease%20among%20men%2C%20and%20multi-parametric%0AMRIs%20offer%20a%20non-invasive%20method%20for%20its%20detection.%20While%20MRI-based%20deep%0Alearning%20solutions%20have%20shown%20promise%20in%20supporting%20PCa%20diagnosis%2C%20acquiring%0Asufficient%20training%20data%2C%20particularly%20in%20local%20clinics%20remains%20challenging.%0AOne%20potential%20solution%20is%20to%20take%20advantage%20of%20publicly%20available%20datasets%20to%0Apre-train%20deep%20models%20and%20fine-tune%20them%20on%20the%20local%20data%2C%20but%20multi-source%0AMRIs%20can%20pose%20challenges%20due%20to%20cross-domain%20distribution%20differences.%20These%0Alimitations%20hinder%20the%20adoption%20of%20explainable%20and%20reliable%20deep-learning%0Asolutions%20in%20local%20clinics%20for%20PCa%20diagnosis.%20In%20this%20work%2C%20we%20present%20a%20novel%0Aapproach%20for%20unpaired%20image-to-image%20translation%20of%20prostate%20multi-parametric%0AMRIs%20and%20an%20uncertainty-aware%20training%20approach%20for%20classifying%20clinically%0Asignificant%20PCa%2C%20to%20be%20applied%20in%20data-constrained%20settings%20such%20as%20local%20and%0Asmall%20clinics.%20Our%20approach%20involves%20a%20novel%20pipeline%20for%20translating%20unpaired%0A3.0T%20multi-parametric%20prostate%20MRIs%20to%201.5T%2C%20thereby%20augmenting%20the%20available%0Atraining%20data.%20Additionally%2C%20we%20introduce%20an%20evidential%20deep%20learning%20approach%0Ato%20estimate%20model%20uncertainty%20and%20employ%20dataset%20filtering%20techniques%20during%0Atraining.%20Furthermore%2C%20we%20propose%20a%20simple%2C%20yet%20efficient%20Evidential%20Focal%0ALoss%2C%20combining%20focal%20loss%20with%20evidential%20uncertainty%2C%20to%20train%20our%20model%0Aeffectively.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20significantly%0Aimproves%20the%20Area%20Under%20ROC%20Curve%20%28AUC%29%20by%20over%2020%25%20compared%20to%20the%20previous%0Awork.%20Our%20code%20is%20available%20at%20https%3A//github.com/med-i-lab/DT_UE_PCa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Transfer%2520Through%2520Image-to-Image%2520Translation%2520for%2520Uncertainty-Aware%250A%2520%2520Prostate%2520Cancer%2520Classification%26entry.906535625%3DMeng%2520Zhou%2520and%2520Amoon%2520Jamzad%2520and%2520Jason%2520Izard%2520and%2520Alexandre%2520Menard%2520and%2520Robert%2520Siemens%2520and%2520Parvin%2520Mousavi%26entry.1292438233%3D%2520%2520Prostate%2520Cancer%2520%2528PCa%2529%2520is%2520a%2520prevalent%2520disease%2520among%2520men%252C%2520and%2520multi-parametric%250AMRIs%2520offer%2520a%2520non-invasive%2520method%2520for%2520its%2520detection.%2520While%2520MRI-based%2520deep%250Alearning%2520solutions%2520have%2520shown%2520promise%2520in%2520supporting%2520PCa%2520diagnosis%252C%2520acquiring%250Asufficient%2520training%2520data%252C%2520particularly%2520in%2520local%2520clinics%2520remains%2520challenging.%250AOne%2520potential%2520solution%2520is%2520to%2520take%2520advantage%2520of%2520publicly%2520available%2520datasets%2520to%250Apre-train%2520deep%2520models%2520and%2520fine-tune%2520them%2520on%2520the%2520local%2520data%252C%2520but%2520multi-source%250AMRIs%2520can%2520pose%2520challenges%2520due%2520to%2520cross-domain%2520distribution%2520differences.%2520These%250Alimitations%2520hinder%2520the%2520adoption%2520of%2520explainable%2520and%2520reliable%2520deep-learning%250Asolutions%2520in%2520local%2520clinics%2520for%2520PCa%2520diagnosis.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%250Aapproach%2520for%2520unpaired%2520image-to-image%2520translation%2520of%2520prostate%2520multi-parametric%250AMRIs%2520and%2520an%2520uncertainty-aware%2520training%2520approach%2520for%2520classifying%2520clinically%250Asignificant%2520PCa%252C%2520to%2520be%2520applied%2520in%2520data-constrained%2520settings%2520such%2520as%2520local%2520and%250Asmall%2520clinics.%2520Our%2520approach%2520involves%2520a%2520novel%2520pipeline%2520for%2520translating%2520unpaired%250A3.0T%2520multi-parametric%2520prostate%2520MRIs%2520to%25201.5T%252C%2520thereby%2520augmenting%2520the%2520available%250Atraining%2520data.%2520Additionally%252C%2520we%2520introduce%2520an%2520evidential%2520deep%2520learning%2520approach%250Ato%2520estimate%2520model%2520uncertainty%2520and%2520employ%2520dataset%2520filtering%2520techniques%2520during%250Atraining.%2520Furthermore%252C%2520we%2520propose%2520a%2520simple%252C%2520yet%2520efficient%2520Evidential%2520Focal%250ALoss%252C%2520combining%2520focal%2520loss%2520with%2520evidential%2520uncertainty%252C%2520to%2520train%2520our%2520model%250Aeffectively.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520significantly%250Aimproves%2520the%2520Area%2520Under%2520ROC%2520Curve%2520%2528AUC%2529%2520by%2520over%252020%2525%2520compared%2520to%2520the%2520previous%250Awork.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/med-i-lab/DT_UE_PCa%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.00479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Transfer%20Through%20Image-to-Image%20Translation%20for%20Uncertainty-Aware%0A%20%20Prostate%20Cancer%20Classification&entry.906535625=Meng%20Zhou%20and%20Amoon%20Jamzad%20and%20Jason%20Izard%20and%20Alexandre%20Menard%20and%20Robert%20Siemens%20and%20Parvin%20Mousavi&entry.1292438233=%20%20Prostate%20Cancer%20%28PCa%29%20is%20a%20prevalent%20disease%20among%20men%2C%20and%20multi-parametric%0AMRIs%20offer%20a%20non-invasive%20method%20for%20its%20detection.%20While%20MRI-based%20deep%0Alearning%20solutions%20have%20shown%20promise%20in%20supporting%20PCa%20diagnosis%2C%20acquiring%0Asufficient%20training%20data%2C%20particularly%20in%20local%20clinics%20remains%20challenging.%0AOne%20potential%20solution%20is%20to%20take%20advantage%20of%20publicly%20available%20datasets%20to%0Apre-train%20deep%20models%20and%20fine-tune%20them%20on%20the%20local%20data%2C%20but%20multi-source%0AMRIs%20can%20pose%20challenges%20due%20to%20cross-domain%20distribution%20differences.%20These%0Alimitations%20hinder%20the%20adoption%20of%20explainable%20and%20reliable%20deep-learning%0Asolutions%20in%20local%20clinics%20for%20PCa%20diagnosis.%20In%20this%20work%2C%20we%20present%20a%20novel%0Aapproach%20for%20unpaired%20image-to-image%20translation%20of%20prostate%20multi-parametric%0AMRIs%20and%20an%20uncertainty-aware%20training%20approach%20for%20classifying%20clinically%0Asignificant%20PCa%2C%20to%20be%20applied%20in%20data-constrained%20settings%20such%20as%20local%20and%0Asmall%20clinics.%20Our%20approach%20involves%20a%20novel%20pipeline%20for%20translating%20unpaired%0A3.0T%20multi-parametric%20prostate%20MRIs%20to%201.5T%2C%20thereby%20augmenting%20the%20available%0Atraining%20data.%20Additionally%2C%20we%20introduce%20an%20evidential%20deep%20learning%20approach%0Ato%20estimate%20model%20uncertainty%20and%20employ%20dataset%20filtering%20techniques%20during%0Atraining.%20Furthermore%2C%20we%20propose%20a%20simple%2C%20yet%20efficient%20Evidential%20Focal%0ALoss%2C%20combining%20focal%20loss%20with%20evidential%20uncertainty%2C%20to%20train%20our%20model%0Aeffectively.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20significantly%0Aimproves%20the%20Area%20Under%20ROC%20Curve%20%28AUC%29%20by%20over%2020%25%20compared%20to%20the%20previous%0Awork.%20Our%20code%20is%20available%20at%20https%3A//github.com/med-i-lab/DT_UE_PCa%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00479v2&entry.124074799=Read"},
{"title": "ProtoGate: Prototype-based Neural Networks with Global-to-local Feature\n  Selection for Tabular Biomedical Data", "author": "Xiangjian Jiang and Andrei Margeloiu and Nikola Simidjievski and Mateja Jamnik", "abstract": "  Tabular biomedical data poses challenges in machine learning because it is\noften high-dimensional and typically low-sample-size (HDLSS). Previous research\nhas attempted to address these challenges via local feature selection, but\nexisting approaches often fail to achieve optimal performance due to their\nlimitation in identifying globally important features and their susceptibility\nto the co-adaptation problem. In this paper, we propose ProtoGate, a\nprototype-based neural model for feature selection on HDLSS data. ProtoGate\nfirst selects instance-wise features via adaptively balancing global and local\nfeature selection. Furthermore, ProtoGate employs a non-parametric\nprototype-based prediction mechanism to tackle the co-adaptation problem,\nensuring the feature selection results and predictions are consistent with\nunderlying data clusters. We conduct comprehensive experiments to evaluate the\nperformance and interpretability of ProtoGate on synthetic and real-world\ndatasets. The results show that ProtoGate generally outperforms\nstate-of-the-art methods in prediction accuracy by a clear margin while\nproviding high-fidelity feature selection and explainable predictions. Code is\navailable at https://github.com/SilenceX12138/ProtoGate.\n", "link": "http://arxiv.org/abs/2306.12330v2", "date": "2024-06-03", "relevancy": 2.3131, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4634}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4622}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoGate%3A%20Prototype-based%20Neural%20Networks%20with%20Global-to-local%20Feature%0A%20%20Selection%20for%20Tabular%20Biomedical%20Data&body=Title%3A%20ProtoGate%3A%20Prototype-based%20Neural%20Networks%20with%20Global-to-local%20Feature%0A%20%20Selection%20for%20Tabular%20Biomedical%20Data%0AAuthor%3A%20Xiangjian%20Jiang%20and%20Andrei%20Margeloiu%20and%20Nikola%20Simidjievski%20and%20Mateja%20Jamnik%0AAbstract%3A%20%20%20Tabular%20biomedical%20data%20poses%20challenges%20in%20machine%20learning%20because%20it%20is%0Aoften%20high-dimensional%20and%20typically%20low-sample-size%20%28HDLSS%29.%20Previous%20research%0Ahas%20attempted%20to%20address%20these%20challenges%20via%20local%20feature%20selection%2C%20but%0Aexisting%20approaches%20often%20fail%20to%20achieve%20optimal%20performance%20due%20to%20their%0Alimitation%20in%20identifying%20globally%20important%20features%20and%20their%20susceptibility%0Ato%20the%20co-adaptation%20problem.%20In%20this%20paper%2C%20we%20propose%20ProtoGate%2C%20a%0Aprototype-based%20neural%20model%20for%20feature%20selection%20on%20HDLSS%20data.%20ProtoGate%0Afirst%20selects%20instance-wise%20features%20via%20adaptively%20balancing%20global%20and%20local%0Afeature%20selection.%20Furthermore%2C%20ProtoGate%20employs%20a%20non-parametric%0Aprototype-based%20prediction%20mechanism%20to%20tackle%20the%20co-adaptation%20problem%2C%0Aensuring%20the%20feature%20selection%20results%20and%20predictions%20are%20consistent%20with%0Aunderlying%20data%20clusters.%20We%20conduct%20comprehensive%20experiments%20to%20evaluate%20the%0Aperformance%20and%20interpretability%20of%20ProtoGate%20on%20synthetic%20and%20real-world%0Adatasets.%20The%20results%20show%20that%20ProtoGate%20generally%20outperforms%0Astate-of-the-art%20methods%20in%20prediction%20accuracy%20by%20a%20clear%20margin%20while%0Aproviding%20high-fidelity%20feature%20selection%20and%20explainable%20predictions.%20Code%20is%0Aavailable%20at%20https%3A//github.com/SilenceX12138/ProtoGate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoGate%253A%2520Prototype-based%2520Neural%2520Networks%2520with%2520Global-to-local%2520Feature%250A%2520%2520Selection%2520for%2520Tabular%2520Biomedical%2520Data%26entry.906535625%3DXiangjian%2520Jiang%2520and%2520Andrei%2520Margeloiu%2520and%2520Nikola%2520Simidjievski%2520and%2520Mateja%2520Jamnik%26entry.1292438233%3D%2520%2520Tabular%2520biomedical%2520data%2520poses%2520challenges%2520in%2520machine%2520learning%2520because%2520it%2520is%250Aoften%2520high-dimensional%2520and%2520typically%2520low-sample-size%2520%2528HDLSS%2529.%2520Previous%2520research%250Ahas%2520attempted%2520to%2520address%2520these%2520challenges%2520via%2520local%2520feature%2520selection%252C%2520but%250Aexisting%2520approaches%2520often%2520fail%2520to%2520achieve%2520optimal%2520performance%2520due%2520to%2520their%250Alimitation%2520in%2520identifying%2520globally%2520important%2520features%2520and%2520their%2520susceptibility%250Ato%2520the%2520co-adaptation%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProtoGate%252C%2520a%250Aprototype-based%2520neural%2520model%2520for%2520feature%2520selection%2520on%2520HDLSS%2520data.%2520ProtoGate%250Afirst%2520selects%2520instance-wise%2520features%2520via%2520adaptively%2520balancing%2520global%2520and%2520local%250Afeature%2520selection.%2520Furthermore%252C%2520ProtoGate%2520employs%2520a%2520non-parametric%250Aprototype-based%2520prediction%2520mechanism%2520to%2520tackle%2520the%2520co-adaptation%2520problem%252C%250Aensuring%2520the%2520feature%2520selection%2520results%2520and%2520predictions%2520are%2520consistent%2520with%250Aunderlying%2520data%2520clusters.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%2520evaluate%2520the%250Aperformance%2520and%2520interpretability%2520of%2520ProtoGate%2520on%2520synthetic%2520and%2520real-world%250Adatasets.%2520The%2520results%2520show%2520that%2520ProtoGate%2520generally%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520prediction%2520accuracy%2520by%2520a%2520clear%2520margin%2520while%250Aproviding%2520high-fidelity%2520feature%2520selection%2520and%2520explainable%2520predictions.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/SilenceX12138/ProtoGate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.12330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoGate%3A%20Prototype-based%20Neural%20Networks%20with%20Global-to-local%20Feature%0A%20%20Selection%20for%20Tabular%20Biomedical%20Data&entry.906535625=Xiangjian%20Jiang%20and%20Andrei%20Margeloiu%20and%20Nikola%20Simidjievski%20and%20Mateja%20Jamnik&entry.1292438233=%20%20Tabular%20biomedical%20data%20poses%20challenges%20in%20machine%20learning%20because%20it%20is%0Aoften%20high-dimensional%20and%20typically%20low-sample-size%20%28HDLSS%29.%20Previous%20research%0Ahas%20attempted%20to%20address%20these%20challenges%20via%20local%20feature%20selection%2C%20but%0Aexisting%20approaches%20often%20fail%20to%20achieve%20optimal%20performance%20due%20to%20their%0Alimitation%20in%20identifying%20globally%20important%20features%20and%20their%20susceptibility%0Ato%20the%20co-adaptation%20problem.%20In%20this%20paper%2C%20we%20propose%20ProtoGate%2C%20a%0Aprototype-based%20neural%20model%20for%20feature%20selection%20on%20HDLSS%20data.%20ProtoGate%0Afirst%20selects%20instance-wise%20features%20via%20adaptively%20balancing%20global%20and%20local%0Afeature%20selection.%20Furthermore%2C%20ProtoGate%20employs%20a%20non-parametric%0Aprototype-based%20prediction%20mechanism%20to%20tackle%20the%20co-adaptation%20problem%2C%0Aensuring%20the%20feature%20selection%20results%20and%20predictions%20are%20consistent%20with%0Aunderlying%20data%20clusters.%20We%20conduct%20comprehensive%20experiments%20to%20evaluate%20the%0Aperformance%20and%20interpretability%20of%20ProtoGate%20on%20synthetic%20and%20real-world%0Adatasets.%20The%20results%20show%20that%20ProtoGate%20generally%20outperforms%0Astate-of-the-art%20methods%20in%20prediction%20accuracy%20by%20a%20clear%20margin%20while%0Aproviding%20high-fidelity%20feature%20selection%20and%20explainable%20predictions.%20Code%20is%0Aavailable%20at%20https%3A//github.com/SilenceX12138/ProtoGate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12330v2&entry.124074799=Read"},
{"title": "One-Shot Learning as Instruction Data Prospector for Large Language\n  Models", "author": "Yunshui Li and Binyuan Hui and Xiaobo Xia and Jiaxi Yang and Min Yang and Lei Zhang and Shuzheng Si and Ling-Hao Chen and Junhao Liu and Tongliang Liu and Fei Huang and Yongbin Li", "abstract": "  Contemporary practices in instruction tuning often hinge on enlarging data\nscaling without a clear strategy for ensuring data quality, inadvertently\nintroducing noise that may compromise model performance. To address this\nchallenge, we introduce \\textsc{Nuggets}, a novel and efficient methodology\nthat leverages one-shot learning to discern and select high-quality instruction\ndata from extensive datasets. \\textsc{Nuggets} assesses the potential of\nindividual instruction examples to act as effective one-shot learning\ninstances, thereby identifying those that can significantly improve performance\nacross diverse tasks. \\textsc{Nuggets} utilizes a scoring system based on the\nimpact of candidate examples on the perplexity of a diverse anchor set,\nfacilitating the selection of the most advantageous data for instruction\ntuning. Through comprehensive evaluations on two benchmarks, including MT-Bench\nand Alpaca-Eval, we show that instruction tuning with the top 1\\% of examples\ncurated by \\textsc{Nuggets} substantially outperforms conventional methods\nemploying the entire dataset.\n", "link": "http://arxiv.org/abs/2312.10302v4", "date": "2024-06-03", "relevancy": 2.3017, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4785}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4552}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Learning%20as%20Instruction%20Data%20Prospector%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20One-Shot%20Learning%20as%20Instruction%20Data%20Prospector%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yunshui%20Li%20and%20Binyuan%20Hui%20and%20Xiaobo%20Xia%20and%20Jiaxi%20Yang%20and%20Min%20Yang%20and%20Lei%20Zhang%20and%20Shuzheng%20Si%20and%20Ling-Hao%20Chen%20and%20Junhao%20Liu%20and%20Tongliang%20Liu%20and%20Fei%20Huang%20and%20Yongbin%20Li%0AAbstract%3A%20%20%20Contemporary%20practices%20in%20instruction%20tuning%20often%20hinge%20on%20enlarging%20data%0Ascaling%20without%20a%20clear%20strategy%20for%20ensuring%20data%20quality%2C%20inadvertently%0Aintroducing%20noise%20that%20may%20compromise%20model%20performance.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20%5Ctextsc%7BNuggets%7D%2C%20a%20novel%20and%20efficient%20methodology%0Athat%20leverages%20one-shot%20learning%20to%20discern%20and%20select%20high-quality%20instruction%0Adata%20from%20extensive%20datasets.%20%5Ctextsc%7BNuggets%7D%20assesses%20the%20potential%20of%0Aindividual%20instruction%20examples%20to%20act%20as%20effective%20one-shot%20learning%0Ainstances%2C%20thereby%20identifying%20those%20that%20can%20significantly%20improve%20performance%0Aacross%20diverse%20tasks.%20%5Ctextsc%7BNuggets%7D%20utilizes%20a%20scoring%20system%20based%20on%20the%0Aimpact%20of%20candidate%20examples%20on%20the%20perplexity%20of%20a%20diverse%20anchor%20set%2C%0Afacilitating%20the%20selection%20of%20the%20most%20advantageous%20data%20for%20instruction%0Atuning.%20Through%20comprehensive%20evaluations%20on%20two%20benchmarks%2C%20including%20MT-Bench%0Aand%20Alpaca-Eval%2C%20we%20show%20that%20instruction%20tuning%20with%20the%20top%201%5C%25%20of%20examples%0Acurated%20by%20%5Ctextsc%7BNuggets%7D%20substantially%20outperforms%20conventional%20methods%0Aemploying%20the%20entire%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10302v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Learning%2520as%2520Instruction%2520Data%2520Prospector%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYunshui%2520Li%2520and%2520Binyuan%2520Hui%2520and%2520Xiaobo%2520Xia%2520and%2520Jiaxi%2520Yang%2520and%2520Min%2520Yang%2520and%2520Lei%2520Zhang%2520and%2520Shuzheng%2520Si%2520and%2520Ling-Hao%2520Chen%2520and%2520Junhao%2520Liu%2520and%2520Tongliang%2520Liu%2520and%2520Fei%2520Huang%2520and%2520Yongbin%2520Li%26entry.1292438233%3D%2520%2520Contemporary%2520practices%2520in%2520instruction%2520tuning%2520often%2520hinge%2520on%2520enlarging%2520data%250Ascaling%2520without%2520a%2520clear%2520strategy%2520for%2520ensuring%2520data%2520quality%252C%2520inadvertently%250Aintroducing%2520noise%2520that%2520may%2520compromise%2520model%2520performance.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520%255Ctextsc%257BNuggets%257D%252C%2520a%2520novel%2520and%2520efficient%2520methodology%250Athat%2520leverages%2520one-shot%2520learning%2520to%2520discern%2520and%2520select%2520high-quality%2520instruction%250Adata%2520from%2520extensive%2520datasets.%2520%255Ctextsc%257BNuggets%257D%2520assesses%2520the%2520potential%2520of%250Aindividual%2520instruction%2520examples%2520to%2520act%2520as%2520effective%2520one-shot%2520learning%250Ainstances%252C%2520thereby%2520identifying%2520those%2520that%2520can%2520significantly%2520improve%2520performance%250Aacross%2520diverse%2520tasks.%2520%255Ctextsc%257BNuggets%257D%2520utilizes%2520a%2520scoring%2520system%2520based%2520on%2520the%250Aimpact%2520of%2520candidate%2520examples%2520on%2520the%2520perplexity%2520of%2520a%2520diverse%2520anchor%2520set%252C%250Afacilitating%2520the%2520selection%2520of%2520the%2520most%2520advantageous%2520data%2520for%2520instruction%250Atuning.%2520Through%2520comprehensive%2520evaluations%2520on%2520two%2520benchmarks%252C%2520including%2520MT-Bench%250Aand%2520Alpaca-Eval%252C%2520we%2520show%2520that%2520instruction%2520tuning%2520with%2520the%2520top%25201%255C%2525%2520of%2520examples%250Acurated%2520by%2520%255Ctextsc%257BNuggets%257D%2520substantially%2520outperforms%2520conventional%2520methods%250Aemploying%2520the%2520entire%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10302v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Learning%20as%20Instruction%20Data%20Prospector%20for%20Large%20Language%0A%20%20Models&entry.906535625=Yunshui%20Li%20and%20Binyuan%20Hui%20and%20Xiaobo%20Xia%20and%20Jiaxi%20Yang%20and%20Min%20Yang%20and%20Lei%20Zhang%20and%20Shuzheng%20Si%20and%20Ling-Hao%20Chen%20and%20Junhao%20Liu%20and%20Tongliang%20Liu%20and%20Fei%20Huang%20and%20Yongbin%20Li&entry.1292438233=%20%20Contemporary%20practices%20in%20instruction%20tuning%20often%20hinge%20on%20enlarging%20data%0Ascaling%20without%20a%20clear%20strategy%20for%20ensuring%20data%20quality%2C%20inadvertently%0Aintroducing%20noise%20that%20may%20compromise%20model%20performance.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20%5Ctextsc%7BNuggets%7D%2C%20a%20novel%20and%20efficient%20methodology%0Athat%20leverages%20one-shot%20learning%20to%20discern%20and%20select%20high-quality%20instruction%0Adata%20from%20extensive%20datasets.%20%5Ctextsc%7BNuggets%7D%20assesses%20the%20potential%20of%0Aindividual%20instruction%20examples%20to%20act%20as%20effective%20one-shot%20learning%0Ainstances%2C%20thereby%20identifying%20those%20that%20can%20significantly%20improve%20performance%0Aacross%20diverse%20tasks.%20%5Ctextsc%7BNuggets%7D%20utilizes%20a%20scoring%20system%20based%20on%20the%0Aimpact%20of%20candidate%20examples%20on%20the%20perplexity%20of%20a%20diverse%20anchor%20set%2C%0Afacilitating%20the%20selection%20of%20the%20most%20advantageous%20data%20for%20instruction%0Atuning.%20Through%20comprehensive%20evaluations%20on%20two%20benchmarks%2C%20including%20MT-Bench%0Aand%20Alpaca-Eval%2C%20we%20show%20that%20instruction%20tuning%20with%20the%20top%201%5C%25%20of%20examples%0Acurated%20by%20%5Ctextsc%7BNuggets%7D%20substantially%20outperforms%20conventional%20methods%0Aemploying%20the%20entire%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10302v4&entry.124074799=Read"},
{"title": "DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in\n  the Wild", "author": "Honghao Fu and Yufei Wang and Wenhan Yang and Bihan Wen", "abstract": "  Image quality assessment (IQA) plays a critical role in selecting\nhigh-quality images and guiding compression and enhancement methods in a series\nof applications. The blind IQA, which assesses the quality of in-the-wild\nimages containing complex authentic distortions without reference images, poses\ngreater challenges. Existing methods are limited to modeling a uniform\ndistribution with local patches and are bothered by the gap between low and\nhigh-level visions (caused by widely adopted pre-trained classification\nnetworks). In this paper, we propose a novel IQA method called diffusion\npriors-based IQA (DP-IQA), which leverages the prior knowledge from the\npre-trained diffusion model with its excellent powers to bridge semantic gaps\nin the perception of the visual quality of images. Specifically, we use\npre-trained stable diffusion as the backbone, extract multi-level features from\nthe denoising U-Net during the upsampling process at a specified timestep, and\ndecode them to estimate the image quality score. The text and image adapters\nare adopted to mitigate the domain gap for downstream tasks and correct the\ninformation loss caused by the variational autoencoder bottleneck. Finally, we\ndistill the knowledge in the above model into a CNN-based student model,\nsignificantly reducing the parameter to enhance applicability, with the student\nmodel performing similarly or even better than the teacher model surprisingly.\nExperimental results demonstrate that our DP-IQA achieves state-of-the-art\nresults on various in-the-wild datasets with better generalization capability,\nwhich shows the superiority of our method in global modeling and utilizing the\nhierarchical feature clues of diffusion for evaluating image quality.\n", "link": "http://arxiv.org/abs/2405.19996v3", "date": "2024-06-03", "relevancy": 2.2933, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&body=Title%3A%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild%0AAuthor%3A%20Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19996v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-IQA%253A%2520Utilizing%2520Diffusion%2520Prior%2520for%2520Blind%2520Image%2520Quality%2520Assessment%2520in%250A%2520%2520the%2520Wild%26entry.906535625%3DHonghao%2520Fu%2520and%2520Yufei%2520Wang%2520and%2520Wenhan%2520Yang%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Image%2520quality%2520assessment%2520%2528IQA%2529%2520plays%2520a%2520critical%2520role%2520in%2520selecting%250Ahigh-quality%2520images%2520and%2520guiding%2520compression%2520and%2520enhancement%2520methods%2520in%2520a%2520series%250Aof%2520applications.%2520The%2520blind%2520IQA%252C%2520which%2520assesses%2520the%2520quality%2520of%2520in-the-wild%250Aimages%2520containing%2520complex%2520authentic%2520distortions%2520without%2520reference%2520images%252C%2520poses%250Agreater%2520challenges.%2520Existing%2520methods%2520are%2520limited%2520to%2520modeling%2520a%2520uniform%250Adistribution%2520with%2520local%2520patches%2520and%2520are%2520bothered%2520by%2520the%2520gap%2520between%2520low%2520and%250Ahigh-level%2520visions%2520%2528caused%2520by%2520widely%2520adopted%2520pre-trained%2520classification%250Anetworks%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520IQA%2520method%2520called%2520diffusion%250Apriors-based%2520IQA%2520%2528DP-IQA%2529%252C%2520which%2520leverages%2520the%2520prior%2520knowledge%2520from%2520the%250Apre-trained%2520diffusion%2520model%2520with%2520its%2520excellent%2520powers%2520to%2520bridge%2520semantic%2520gaps%250Ain%2520the%2520perception%2520of%2520the%2520visual%2520quality%2520of%2520images.%2520Specifically%252C%2520we%2520use%250Apre-trained%2520stable%2520diffusion%2520as%2520the%2520backbone%252C%2520extract%2520multi-level%2520features%2520from%250Athe%2520denoising%2520U-Net%2520during%2520the%2520upsampling%2520process%2520at%2520a%2520specified%2520timestep%252C%2520and%250Adecode%2520them%2520to%2520estimate%2520the%2520image%2520quality%2520score.%2520The%2520text%2520and%2520image%2520adapters%250Aare%2520adopted%2520to%2520mitigate%2520the%2520domain%2520gap%2520for%2520downstream%2520tasks%2520and%2520correct%2520the%250Ainformation%2520loss%2520caused%2520by%2520the%2520variational%2520autoencoder%2520bottleneck.%2520Finally%252C%2520we%250Adistill%2520the%2520knowledge%2520in%2520the%2520above%2520model%2520into%2520a%2520CNN-based%2520student%2520model%252C%250Asignificantly%2520reducing%2520the%2520parameter%2520to%2520enhance%2520applicability%252C%2520with%2520the%2520student%250Amodel%2520performing%2520similarly%2520or%2520even%2520better%2520than%2520the%2520teacher%2520model%2520surprisingly.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520DP-IQA%2520achieves%2520state-of-the-art%250Aresults%2520on%2520various%2520in-the-wild%2520datasets%2520with%2520better%2520generalization%2520capability%252C%250Awhich%2520shows%2520the%2520superiority%2520of%2520our%2520method%2520in%2520global%2520modeling%2520and%2520utilizing%2520the%250Ahierarchical%2520feature%2520clues%2520of%2520diffusion%2520for%2520evaluating%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19996v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&entry.906535625=Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen&entry.1292438233=%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19996v3&entry.124074799=Read"},
{"title": "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge", "author": "Brendan Park and Madeline Janecek and Naser Ezzati-Jivan and Yifeng Li and Ali Emami", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable success in tasks\nlike the Winograd Schema Challenge (WSC), showcasing advanced textual\ncommon-sense reasoning. However, applying this reasoning to multimodal domains,\nwhere understanding text and images together is essential, remains a\nsubstantial challenge. To address this, we introduce WinoVis, a novel dataset\nspecifically designed to probe text-to-image models on pronoun disambiguation\nwithin multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion\nAttentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel\nevaluation framework that isolates the models' ability in pronoun\ndisambiguation from other visual processing challenges. Evaluation of\nsuccessive model versions reveals that, despite incremental advancements,\nStable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally\nsurpassing random guessing. Further error analysis identifies important areas\nfor future research aimed at advancing text-to-image models in their ability to\ninterpret and interact with the complex visual world.\n", "link": "http://arxiv.org/abs/2405.16277v3", "date": "2024-06-03", "relevancy": 2.2837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5676}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Picturing%20Ambiguity%3A%20A%20Visual%20Twist%20on%20the%20Winograd%20Schema%20Challenge&body=Title%3A%20Picturing%20Ambiguity%3A%20A%20Visual%20Twist%20on%20the%20Winograd%20Schema%20Challenge%0AAuthor%3A%20Brendan%20Park%20and%20Madeline%20Janecek%20and%20Naser%20Ezzati-Jivan%20and%20Yifeng%20Li%20and%20Ali%20Emami%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20in%20tasks%0Alike%20the%20Winograd%20Schema%20Challenge%20%28WSC%29%2C%20showcasing%20advanced%20textual%0Acommon-sense%20reasoning.%20However%2C%20applying%20this%20reasoning%20to%20multimodal%20domains%2C%0Awhere%20understanding%20text%20and%20images%20together%20is%20essential%2C%20remains%20a%0Asubstantial%20challenge.%20To%20address%20this%2C%20we%20introduce%20WinoVis%2C%20a%20novel%20dataset%0Aspecifically%20designed%20to%20probe%20text-to-image%20models%20on%20pronoun%20disambiguation%0Awithin%20multimodal%20contexts.%20Utilizing%20GPT-4%20for%20prompt%20generation%20and%20Diffusion%0AAttentive%20Attribution%20Maps%20%28DAAM%29%20for%20heatmap%20analysis%2C%20we%20propose%20a%20novel%0Aevaluation%20framework%20that%20isolates%20the%20models%27%20ability%20in%20pronoun%0Adisambiguation%20from%20other%20visual%20processing%20challenges.%20Evaluation%20of%0Asuccessive%20model%20versions%20reveals%20that%2C%20despite%20incremental%20advancements%2C%0AStable%20Diffusion%202.0%20achieves%20a%20precision%20of%2056.7%25%20on%20WinoVis%2C%20only%20marginally%0Asurpassing%20random%20guessing.%20Further%20error%20analysis%20identifies%20important%20areas%0Afor%20future%20research%20aimed%20at%20advancing%20text-to-image%20models%20in%20their%20ability%20to%0Ainterpret%20and%20interact%20with%20the%20complex%20visual%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16277v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPicturing%2520Ambiguity%253A%2520A%2520Visual%2520Twist%2520on%2520the%2520Winograd%2520Schema%2520Challenge%26entry.906535625%3DBrendan%2520Park%2520and%2520Madeline%2520Janecek%2520and%2520Naser%2520Ezzati-Jivan%2520and%2520Yifeng%2520Li%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520tasks%250Alike%2520the%2520Winograd%2520Schema%2520Challenge%2520%2528WSC%2529%252C%2520showcasing%2520advanced%2520textual%250Acommon-sense%2520reasoning.%2520However%252C%2520applying%2520this%2520reasoning%2520to%2520multimodal%2520domains%252C%250Awhere%2520understanding%2520text%2520and%2520images%2520together%2520is%2520essential%252C%2520remains%2520a%250Asubstantial%2520challenge.%2520To%2520address%2520this%252C%2520we%2520introduce%2520WinoVis%252C%2520a%2520novel%2520dataset%250Aspecifically%2520designed%2520to%2520probe%2520text-to-image%2520models%2520on%2520pronoun%2520disambiguation%250Awithin%2520multimodal%2520contexts.%2520Utilizing%2520GPT-4%2520for%2520prompt%2520generation%2520and%2520Diffusion%250AAttentive%2520Attribution%2520Maps%2520%2528DAAM%2529%2520for%2520heatmap%2520analysis%252C%2520we%2520propose%2520a%2520novel%250Aevaluation%2520framework%2520that%2520isolates%2520the%2520models%2527%2520ability%2520in%2520pronoun%250Adisambiguation%2520from%2520other%2520visual%2520processing%2520challenges.%2520Evaluation%2520of%250Asuccessive%2520model%2520versions%2520reveals%2520that%252C%2520despite%2520incremental%2520advancements%252C%250AStable%2520Diffusion%25202.0%2520achieves%2520a%2520precision%2520of%252056.7%2525%2520on%2520WinoVis%252C%2520only%2520marginally%250Asurpassing%2520random%2520guessing.%2520Further%2520error%2520analysis%2520identifies%2520important%2520areas%250Afor%2520future%2520research%2520aimed%2520at%2520advancing%2520text-to-image%2520models%2520in%2520their%2520ability%2520to%250Ainterpret%2520and%2520interact%2520with%2520the%2520complex%2520visual%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16277v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Picturing%20Ambiguity%3A%20A%20Visual%20Twist%20on%20the%20Winograd%20Schema%20Challenge&entry.906535625=Brendan%20Park%20and%20Madeline%20Janecek%20and%20Naser%20Ezzati-Jivan%20and%20Yifeng%20Li%20and%20Ali%20Emami&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20in%20tasks%0Alike%20the%20Winograd%20Schema%20Challenge%20%28WSC%29%2C%20showcasing%20advanced%20textual%0Acommon-sense%20reasoning.%20However%2C%20applying%20this%20reasoning%20to%20multimodal%20domains%2C%0Awhere%20understanding%20text%20and%20images%20together%20is%20essential%2C%20remains%20a%0Asubstantial%20challenge.%20To%20address%20this%2C%20we%20introduce%20WinoVis%2C%20a%20novel%20dataset%0Aspecifically%20designed%20to%20probe%20text-to-image%20models%20on%20pronoun%20disambiguation%0Awithin%20multimodal%20contexts.%20Utilizing%20GPT-4%20for%20prompt%20generation%20and%20Diffusion%0AAttentive%20Attribution%20Maps%20%28DAAM%29%20for%20heatmap%20analysis%2C%20we%20propose%20a%20novel%0Aevaluation%20framework%20that%20isolates%20the%20models%27%20ability%20in%20pronoun%0Adisambiguation%20from%20other%20visual%20processing%20challenges.%20Evaluation%20of%0Asuccessive%20model%20versions%20reveals%20that%2C%20despite%20incremental%20advancements%2C%0AStable%20Diffusion%202.0%20achieves%20a%20precision%20of%2056.7%25%20on%20WinoVis%2C%20only%20marginally%0Asurpassing%20random%20guessing.%20Further%20error%20analysis%20identifies%20important%20areas%0Afor%20future%20research%20aimed%20at%20advancing%20text-to-image%20models%20in%20their%20ability%20to%0Ainterpret%20and%20interact%20with%20the%20complex%20visual%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16277v3&entry.124074799=Read"},
{"title": "DeCoF: Generated Video Detection via Frame Consistency: The First\n  Benchmark Dataset", "author": "Long Ma and Jiajia Zhang and Hongping Deng and Ningyu Zhang and Qinglang Guo and Haiyang Yu and Yong Liao and Pengyuan Zhou", "abstract": "  The escalating quality of video generated by advanced video generation\nmethods results in new security challenges, while there have been few relevant\nresearch efforts: 1) There is no open-source dataset for generated video\ndetection, 2) No generated video detection method has been proposed so far. To\nthis end, we propose an open-source dataset and a detection method for\ngenerated video for the first time. First, we propose a scalable dataset\nconsisting of 964 prompts, covering various forgery targets, scenes, behaviors,\nand actions, as well as various generation models with different architectures\nand generation methods, including the most popular commercial models like\nOpenAI's Sora and Google's Veo. Second, we found via probing experiments that\nspatial artifact-based detectors lack generalizability. Hence, we propose a\nsimple yet effective \\textbf{de}tection model based on \\textbf{f}rame\n\\textbf{co}nsistency (\\textbf{DeCoF}), which focuses on temporal artifacts by\neliminating the impact of spatial artifacts during feature learning. Extensive\nexperiments demonstrate the efficacy of DeCoF in detecting videos generated by\nunseen video generation models and confirm its powerful generalizability across\nseveral commercially proprietary models. Our code and dataset will be released\nat \\url{https://anonymous.4open.science/r/DeCoF-8394}.\n", "link": "http://arxiv.org/abs/2402.02085v3", "date": "2024-06-03", "relevancy": 2.2745, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5832}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5716}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset&body=Title%3A%20DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset%0AAuthor%3A%20Long%20Ma%20and%20Jiajia%20Zhang%20and%20Hongping%20Deng%20and%20Ningyu%20Zhang%20and%20Qinglang%20Guo%20and%20Haiyang%20Yu%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou%0AAbstract%3A%20%20%20The%20escalating%20quality%20of%20video%20generated%20by%20advanced%20video%20generation%0Amethods%20results%20in%20new%20security%20challenges%2C%20while%20there%20have%20been%20few%20relevant%0Aresearch%20efforts%3A%201%29%20There%20is%20no%20open-source%20dataset%20for%20generated%20video%0Adetection%2C%202%29%20No%20generated%20video%20detection%20method%20has%20been%20proposed%20so%20far.%20To%0Athis%20end%2C%20we%20propose%20an%20open-source%20dataset%20and%20a%20detection%20method%20for%0Agenerated%20video%20for%20the%20first%20time.%20First%2C%20we%20propose%20a%20scalable%20dataset%0Aconsisting%20of%20964%20prompts%2C%20covering%20various%20forgery%20targets%2C%20scenes%2C%20behaviors%2C%0Aand%20actions%2C%20as%20well%20as%20various%20generation%20models%20with%20different%20architectures%0Aand%20generation%20methods%2C%20including%20the%20most%20popular%20commercial%20models%20like%0AOpenAI%27s%20Sora%20and%20Google%27s%20Veo.%20Second%2C%20we%20found%20via%20probing%20experiments%20that%0Aspatial%20artifact-based%20detectors%20lack%20generalizability.%20Hence%2C%20we%20propose%20a%0Asimple%20yet%20effective%20%5Ctextbf%7Bde%7Dtection%20model%20based%20on%20%5Ctextbf%7Bf%7Drame%0A%5Ctextbf%7Bco%7Dnsistency%20%28%5Ctextbf%7BDeCoF%7D%29%2C%20which%20focuses%20on%20temporal%20artifacts%20by%0Aeliminating%20the%20impact%20of%20spatial%20artifacts%20during%20feature%20learning.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20DeCoF%20in%20detecting%20videos%20generated%20by%0Aunseen%20video%20generation%20models%20and%20confirm%20its%20powerful%20generalizability%20across%0Aseveral%20commercially%20proprietary%20models.%20Our%20code%20and%20dataset%20will%20be%20released%0Aat%20%5Curl%7Bhttps%3A//anonymous.4open.science/r/DeCoF-8394%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCoF%253A%2520Generated%2520Video%2520Detection%2520via%2520Frame%2520Consistency%253A%2520The%2520First%250A%2520%2520Benchmark%2520Dataset%26entry.906535625%3DLong%2520Ma%2520and%2520Jiajia%2520Zhang%2520and%2520Hongping%2520Deng%2520and%2520Ningyu%2520Zhang%2520and%2520Qinglang%2520Guo%2520and%2520Haiyang%2520Yu%2520and%2520Yong%2520Liao%2520and%2520Pengyuan%2520Zhou%26entry.1292438233%3D%2520%2520The%2520escalating%2520quality%2520of%2520video%2520generated%2520by%2520advanced%2520video%2520generation%250Amethods%2520results%2520in%2520new%2520security%2520challenges%252C%2520while%2520there%2520have%2520been%2520few%2520relevant%250Aresearch%2520efforts%253A%25201%2529%2520There%2520is%2520no%2520open-source%2520dataset%2520for%2520generated%2520video%250Adetection%252C%25202%2529%2520No%2520generated%2520video%2520detection%2520method%2520has%2520been%2520proposed%2520so%2520far.%2520To%250Athis%2520end%252C%2520we%2520propose%2520an%2520open-source%2520dataset%2520and%2520a%2520detection%2520method%2520for%250Agenerated%2520video%2520for%2520the%2520first%2520time.%2520First%252C%2520we%2520propose%2520a%2520scalable%2520dataset%250Aconsisting%2520of%2520964%2520prompts%252C%2520covering%2520various%2520forgery%2520targets%252C%2520scenes%252C%2520behaviors%252C%250Aand%2520actions%252C%2520as%2520well%2520as%2520various%2520generation%2520models%2520with%2520different%2520architectures%250Aand%2520generation%2520methods%252C%2520including%2520the%2520most%2520popular%2520commercial%2520models%2520like%250AOpenAI%2527s%2520Sora%2520and%2520Google%2527s%2520Veo.%2520Second%252C%2520we%2520found%2520via%2520probing%2520experiments%2520that%250Aspatial%2520artifact-based%2520detectors%2520lack%2520generalizability.%2520Hence%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520%255Ctextbf%257Bde%257Dtection%2520model%2520based%2520on%2520%255Ctextbf%257Bf%257Drame%250A%255Ctextbf%257Bco%257Dnsistency%2520%2528%255Ctextbf%257BDeCoF%257D%2529%252C%2520which%2520focuses%2520on%2520temporal%2520artifacts%2520by%250Aeliminating%2520the%2520impact%2520of%2520spatial%2520artifacts%2520during%2520feature%2520learning.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520of%2520DeCoF%2520in%2520detecting%2520videos%2520generated%2520by%250Aunseen%2520video%2520generation%2520models%2520and%2520confirm%2520its%2520powerful%2520generalizability%2520across%250Aseveral%2520commercially%2520proprietary%2520models.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520released%250Aat%2520%255Curl%257Bhttps%253A//anonymous.4open.science/r/DeCoF-8394%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset&entry.906535625=Long%20Ma%20and%20Jiajia%20Zhang%20and%20Hongping%20Deng%20and%20Ningyu%20Zhang%20and%20Qinglang%20Guo%20and%20Haiyang%20Yu%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou&entry.1292438233=%20%20The%20escalating%20quality%20of%20video%20generated%20by%20advanced%20video%20generation%0Amethods%20results%20in%20new%20security%20challenges%2C%20while%20there%20have%20been%20few%20relevant%0Aresearch%20efforts%3A%201%29%20There%20is%20no%20open-source%20dataset%20for%20generated%20video%0Adetection%2C%202%29%20No%20generated%20video%20detection%20method%20has%20been%20proposed%20so%20far.%20To%0Athis%20end%2C%20we%20propose%20an%20open-source%20dataset%20and%20a%20detection%20method%20for%0Agenerated%20video%20for%20the%20first%20time.%20First%2C%20we%20propose%20a%20scalable%20dataset%0Aconsisting%20of%20964%20prompts%2C%20covering%20various%20forgery%20targets%2C%20scenes%2C%20behaviors%2C%0Aand%20actions%2C%20as%20well%20as%20various%20generation%20models%20with%20different%20architectures%0Aand%20generation%20methods%2C%20including%20the%20most%20popular%20commercial%20models%20like%0AOpenAI%27s%20Sora%20and%20Google%27s%20Veo.%20Second%2C%20we%20found%20via%20probing%20experiments%20that%0Aspatial%20artifact-based%20detectors%20lack%20generalizability.%20Hence%2C%20we%20propose%20a%0Asimple%20yet%20effective%20%5Ctextbf%7Bde%7Dtection%20model%20based%20on%20%5Ctextbf%7Bf%7Drame%0A%5Ctextbf%7Bco%7Dnsistency%20%28%5Ctextbf%7BDeCoF%7D%29%2C%20which%20focuses%20on%20temporal%20artifacts%20by%0Aeliminating%20the%20impact%20of%20spatial%20artifacts%20during%20feature%20learning.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20DeCoF%20in%20detecting%20videos%20generated%20by%0Aunseen%20video%20generation%20models%20and%20confirm%20its%20powerful%20generalizability%20across%0Aseveral%20commercially%20proprietary%20models.%20Our%20code%20and%20dataset%20will%20be%20released%0Aat%20%5Curl%7Bhttps%3A//anonymous.4open.science/r/DeCoF-8394%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02085v3&entry.124074799=Read"},
{"title": "Learning to Plan Maneuverable and Agile Flight Trajectory with\n  Optimization Embedded Networks", "author": "Zhichao Han and Long Xu and Fei Gao", "abstract": "  In recent times, an increasing number of researchers have been devoted to\nutilizing deep neural networks for end-to-end flight navigation. This approach\nhas gained traction due to its ability to bridge the gap between perception and\nplanning that exists in traditional methods, thereby eliminating delays between\nmodules. However, the practice of replacing original modules with neural\nnetworks in a black-box manner diminishes the overall system's robustness and\nstability. It lacks principled explanations and often fails to consistently\ngenerate high-quality motion trajectories. Furthermore, such methods often\nstruggle to rigorously account for the robot's kinematic constraints, resulting\nin the generation of trajectories that cannot be executed satisfactorily. In\nthis work, we combine the advantages of traditional methods and neural networks\nby proposing an optimization-embedded neural network. This network can learn\nhigh-quality trajectories directly from visual inputs without the need of\nmapping, while ensuring dynamic feasibility. Here, the deep neural network is\nemployed to directly extract environment safety regions from depth images.\nSubsequently, we employ a model-based approach to represent these regions as\nsafety constraints in trajectory optimization. Leveraging the availability of\nhighly efficient optimization algorithms, our method robustly converges to\nfeasible and optimal solutions that satisfy various user-defined constraints.\nMoreover, we differentiate the optimization process, allowing it to be trained\nas a layer within the neural network. This approach facilitates the direct\ninteraction between perception and planning, enabling the network to focus more\non the spatial regions where optimal solutions exist. As a result, it further\nenhances the quality and stability of the generated trajectories.\n", "link": "http://arxiv.org/abs/2405.07736v2", "date": "2024-06-03", "relevancy": 2.2435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks&body=Title%3A%20Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks%0AAuthor%3A%20Zhichao%20Han%20and%20Long%20Xu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20In%20recent%20times%2C%20an%20increasing%20number%20of%20researchers%20have%20been%20devoted%20to%0Autilizing%20deep%20neural%20networks%20for%20end-to-end%20flight%20navigation.%20This%20approach%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20bridge%20the%20gap%20between%20perception%20and%0Aplanning%20that%20exists%20in%20traditional%20methods%2C%20thereby%20eliminating%20delays%20between%0Amodules.%20However%2C%20the%20practice%20of%20replacing%20original%20modules%20with%20neural%0Anetworks%20in%20a%20black-box%20manner%20diminishes%20the%20overall%20system%27s%20robustness%20and%0Astability.%20It%20lacks%20principled%20explanations%20and%20often%20fails%20to%20consistently%0Agenerate%20high-quality%20motion%20trajectories.%20Furthermore%2C%20such%20methods%20often%0Astruggle%20to%20rigorously%20account%20for%20the%20robot%27s%20kinematic%20constraints%2C%20resulting%0Ain%20the%20generation%20of%20trajectories%20that%20cannot%20be%20executed%20satisfactorily.%20In%0Athis%20work%2C%20we%20combine%20the%20advantages%20of%20traditional%20methods%20and%20neural%20networks%0Aby%20proposing%20an%20optimization-embedded%20neural%20network.%20This%20network%20can%20learn%0Ahigh-quality%20trajectories%20directly%20from%20visual%20inputs%20without%20the%20need%20of%0Amapping%2C%20while%20ensuring%20dynamic%20feasibility.%20Here%2C%20the%20deep%20neural%20network%20is%0Aemployed%20to%20directly%20extract%20environment%20safety%20regions%20from%20depth%20images.%0ASubsequently%2C%20we%20employ%20a%20model-based%20approach%20to%20represent%20these%20regions%20as%0Asafety%20constraints%20in%20trajectory%20optimization.%20Leveraging%20the%20availability%20of%0Ahighly%20efficient%20optimization%20algorithms%2C%20our%20method%20robustly%20converges%20to%0Afeasible%20and%20optimal%20solutions%20that%20satisfy%20various%20user-defined%20constraints.%0AMoreover%2C%20we%20differentiate%20the%20optimization%20process%2C%20allowing%20it%20to%20be%20trained%0Aas%20a%20layer%20within%20the%20neural%20network.%20This%20approach%20facilitates%20the%20direct%0Ainteraction%20between%20perception%20and%20planning%2C%20enabling%20the%20network%20to%20focus%20more%0Aon%20the%20spatial%20regions%20where%20optimal%20solutions%20exist.%20As%20a%20result%2C%20it%20further%0Aenhances%20the%20quality%20and%20stability%20of%20the%20generated%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Plan%2520Maneuverable%2520and%2520Agile%2520Flight%2520Trajectory%2520with%250A%2520%2520Optimization%2520Embedded%2520Networks%26entry.906535625%3DZhichao%2520Han%2520and%2520Long%2520Xu%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520an%2520increasing%2520number%2520of%2520researchers%2520have%2520been%2520devoted%2520to%250Autilizing%2520deep%2520neural%2520networks%2520for%2520end-to-end%2520flight%2520navigation.%2520This%2520approach%250Ahas%2520gained%2520traction%2520due%2520to%2520its%2520ability%2520to%2520bridge%2520the%2520gap%2520between%2520perception%2520and%250Aplanning%2520that%2520exists%2520in%2520traditional%2520methods%252C%2520thereby%2520eliminating%2520delays%2520between%250Amodules.%2520However%252C%2520the%2520practice%2520of%2520replacing%2520original%2520modules%2520with%2520neural%250Anetworks%2520in%2520a%2520black-box%2520manner%2520diminishes%2520the%2520overall%2520system%2527s%2520robustness%2520and%250Astability.%2520It%2520lacks%2520principled%2520explanations%2520and%2520often%2520fails%2520to%2520consistently%250Agenerate%2520high-quality%2520motion%2520trajectories.%2520Furthermore%252C%2520such%2520methods%2520often%250Astruggle%2520to%2520rigorously%2520account%2520for%2520the%2520robot%2527s%2520kinematic%2520constraints%252C%2520resulting%250Ain%2520the%2520generation%2520of%2520trajectories%2520that%2520cannot%2520be%2520executed%2520satisfactorily.%2520In%250Athis%2520work%252C%2520we%2520combine%2520the%2520advantages%2520of%2520traditional%2520methods%2520and%2520neural%2520networks%250Aby%2520proposing%2520an%2520optimization-embedded%2520neural%2520network.%2520This%2520network%2520can%2520learn%250Ahigh-quality%2520trajectories%2520directly%2520from%2520visual%2520inputs%2520without%2520the%2520need%2520of%250Amapping%252C%2520while%2520ensuring%2520dynamic%2520feasibility.%2520Here%252C%2520the%2520deep%2520neural%2520network%2520is%250Aemployed%2520to%2520directly%2520extract%2520environment%2520safety%2520regions%2520from%2520depth%2520images.%250ASubsequently%252C%2520we%2520employ%2520a%2520model-based%2520approach%2520to%2520represent%2520these%2520regions%2520as%250Asafety%2520constraints%2520in%2520trajectory%2520optimization.%2520Leveraging%2520the%2520availability%2520of%250Ahighly%2520efficient%2520optimization%2520algorithms%252C%2520our%2520method%2520robustly%2520converges%2520to%250Afeasible%2520and%2520optimal%2520solutions%2520that%2520satisfy%2520various%2520user-defined%2520constraints.%250AMoreover%252C%2520we%2520differentiate%2520the%2520optimization%2520process%252C%2520allowing%2520it%2520to%2520be%2520trained%250Aas%2520a%2520layer%2520within%2520the%2520neural%2520network.%2520This%2520approach%2520facilitates%2520the%2520direct%250Ainteraction%2520between%2520perception%2520and%2520planning%252C%2520enabling%2520the%2520network%2520to%2520focus%2520more%250Aon%2520the%2520spatial%2520regions%2520where%2520optimal%2520solutions%2520exist.%2520As%2520a%2520result%252C%2520it%2520further%250Aenhances%2520the%2520quality%2520and%2520stability%2520of%2520the%2520generated%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks&entry.906535625=Zhichao%20Han%20and%20Long%20Xu%20and%20Fei%20Gao&entry.1292438233=%20%20In%20recent%20times%2C%20an%20increasing%20number%20of%20researchers%20have%20been%20devoted%20to%0Autilizing%20deep%20neural%20networks%20for%20end-to-end%20flight%20navigation.%20This%20approach%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20bridge%20the%20gap%20between%20perception%20and%0Aplanning%20that%20exists%20in%20traditional%20methods%2C%20thereby%20eliminating%20delays%20between%0Amodules.%20However%2C%20the%20practice%20of%20replacing%20original%20modules%20with%20neural%0Anetworks%20in%20a%20black-box%20manner%20diminishes%20the%20overall%20system%27s%20robustness%20and%0Astability.%20It%20lacks%20principled%20explanations%20and%20often%20fails%20to%20consistently%0Agenerate%20high-quality%20motion%20trajectories.%20Furthermore%2C%20such%20methods%20often%0Astruggle%20to%20rigorously%20account%20for%20the%20robot%27s%20kinematic%20constraints%2C%20resulting%0Ain%20the%20generation%20of%20trajectories%20that%20cannot%20be%20executed%20satisfactorily.%20In%0Athis%20work%2C%20we%20combine%20the%20advantages%20of%20traditional%20methods%20and%20neural%20networks%0Aby%20proposing%20an%20optimization-embedded%20neural%20network.%20This%20network%20can%20learn%0Ahigh-quality%20trajectories%20directly%20from%20visual%20inputs%20without%20the%20need%20of%0Amapping%2C%20while%20ensuring%20dynamic%20feasibility.%20Here%2C%20the%20deep%20neural%20network%20is%0Aemployed%20to%20directly%20extract%20environment%20safety%20regions%20from%20depth%20images.%0ASubsequently%2C%20we%20employ%20a%20model-based%20approach%20to%20represent%20these%20regions%20as%0Asafety%20constraints%20in%20trajectory%20optimization.%20Leveraging%20the%20availability%20of%0Ahighly%20efficient%20optimization%20algorithms%2C%20our%20method%20robustly%20converges%20to%0Afeasible%20and%20optimal%20solutions%20that%20satisfy%20various%20user-defined%20constraints.%0AMoreover%2C%20we%20differentiate%20the%20optimization%20process%2C%20allowing%20it%20to%20be%20trained%0Aas%20a%20layer%20within%20the%20neural%20network.%20This%20approach%20facilitates%20the%20direct%0Ainteraction%20between%20perception%20and%20planning%2C%20enabling%20the%20network%20to%20focus%20more%0Aon%20the%20spatial%20regions%20where%20optimal%20solutions%20exist.%20As%20a%20result%2C%20it%20further%0Aenhances%20the%20quality%20and%20stability%20of%20the%20generated%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07736v2&entry.124074799=Read"},
{"title": "Efficient Masked Autoencoders with Self-Consistency", "author": "Zhaowen Li and Yousong Zhu and Zhiyang Chen and Wei Li and Chaoyang Zhao and Rui Zhao and Ming Tang and Jinqiao Wang", "abstract": "  Inspired by the masked language modeling (MLM) in natural language processing\ntasks, the masked image modeling (MIM) has been recognized as a strong\nself-supervised pre-training method in computer vision. However, the high\nrandom mask ratio of MIM results in two serious problems: 1) the inadequate\ndata utilization of images within each iteration brings prolonged pre-training,\nand 2) the high inconsistency of predictions results in unreliable generations,\n$i.e.$, the prediction of the identical patch may be inconsistent in different\nmask rounds, leading to divergent semantics in the ultimately generated\noutcomes. To tackle these problems, we propose the efficient masked\nautoencoders with self-consistency (EMAE) to improve the pre-training\nefficiency and increase the consistency of MIM. In particular, we present a\nparallel mask strategy that divides the image into K non-overlapping parts,\neach of which is generated by a random mask with the same mask ratio. Then the\nMIM task is conducted parallelly on all parts in an iteration and the model\nminimizes the loss between the predictions and the masked patches. Besides, we\ndesign the self-consistency learning to further maintain the consistency of\npredictions of overlapping masked patches among parts. Overall, our method is\nable to exploit the data more efficiently and obtains reliable representations.\nExperiments on ImageNet show that EMAE achieves the best performance on\nViT-Large with only 13% of MAE pre-training time using NVIDIA A100 GPUs. After\npre-training on diverse datasets, EMAE consistently obtains state-of-the-art\ntransfer ability on a variety of downstream tasks, such as image\nclassification, object detection, and semantic segmentation.\n", "link": "http://arxiv.org/abs/2302.14431v2", "date": "2024-06-03", "relevancy": 2.2379, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5704}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Masked%20Autoencoders%20with%20Self-Consistency&body=Title%3A%20Efficient%20Masked%20Autoencoders%20with%20Self-Consistency%0AAuthor%3A%20Zhaowen%20Li%20and%20Yousong%20Zhu%20and%20Zhiyang%20Chen%20and%20Wei%20Li%20and%20Chaoyang%20Zhao%20and%20Rui%20Zhao%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Inspired%20by%20the%20masked%20language%20modeling%20%28MLM%29%20in%20natural%20language%20processing%0Atasks%2C%20the%20masked%20image%20modeling%20%28MIM%29%20has%20been%20recognized%20as%20a%20strong%0Aself-supervised%20pre-training%20method%20in%20computer%20vision.%20However%2C%20the%20high%0Arandom%20mask%20ratio%20of%20MIM%20results%20in%20two%20serious%20problems%3A%201%29%20the%20inadequate%0Adata%20utilization%20of%20images%20within%20each%20iteration%20brings%20prolonged%20pre-training%2C%0Aand%202%29%20the%20high%20inconsistency%20of%20predictions%20results%20in%20unreliable%20generations%2C%0A%24i.e.%24%2C%20the%20prediction%20of%20the%20identical%20patch%20may%20be%20inconsistent%20in%20different%0Amask%20rounds%2C%20leading%20to%20divergent%20semantics%20in%20the%20ultimately%20generated%0Aoutcomes.%20To%20tackle%20these%20problems%2C%20we%20propose%20the%20efficient%20masked%0Aautoencoders%20with%20self-consistency%20%28EMAE%29%20to%20improve%20the%20pre-training%0Aefficiency%20and%20increase%20the%20consistency%20of%20MIM.%20In%20particular%2C%20we%20present%20a%0Aparallel%20mask%20strategy%20that%20divides%20the%20image%20into%20K%20non-overlapping%20parts%2C%0Aeach%20of%20which%20is%20generated%20by%20a%20random%20mask%20with%20the%20same%20mask%20ratio.%20Then%20the%0AMIM%20task%20is%20conducted%20parallelly%20on%20all%20parts%20in%20an%20iteration%20and%20the%20model%0Aminimizes%20the%20loss%20between%20the%20predictions%20and%20the%20masked%20patches.%20Besides%2C%20we%0Adesign%20the%20self-consistency%20learning%20to%20further%20maintain%20the%20consistency%20of%0Apredictions%20of%20overlapping%20masked%20patches%20among%20parts.%20Overall%2C%20our%20method%20is%0Aable%20to%20exploit%20the%20data%20more%20efficiently%20and%20obtains%20reliable%20representations.%0AExperiments%20on%20ImageNet%20show%20that%20EMAE%20achieves%20the%20best%20performance%20on%0AViT-Large%20with%20only%2013%25%20of%20MAE%20pre-training%20time%20using%20NVIDIA%20A100%20GPUs.%20After%0Apre-training%20on%20diverse%20datasets%2C%20EMAE%20consistently%20obtains%20state-of-the-art%0Atransfer%20ability%20on%20a%20variety%20of%20downstream%20tasks%2C%20such%20as%20image%0Aclassification%2C%20object%20detection%2C%20and%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.14431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Masked%2520Autoencoders%2520with%2520Self-Consistency%26entry.906535625%3DZhaowen%2520Li%2520and%2520Yousong%2520Zhu%2520and%2520Zhiyang%2520Chen%2520and%2520Wei%2520Li%2520and%2520Chaoyang%2520Zhao%2520and%2520Rui%2520Zhao%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520masked%2520language%2520modeling%2520%2528MLM%2529%2520in%2520natural%2520language%2520processing%250Atasks%252C%2520the%2520masked%2520image%2520modeling%2520%2528MIM%2529%2520has%2520been%2520recognized%2520as%2520a%2520strong%250Aself-supervised%2520pre-training%2520method%2520in%2520computer%2520vision.%2520However%252C%2520the%2520high%250Arandom%2520mask%2520ratio%2520of%2520MIM%2520results%2520in%2520two%2520serious%2520problems%253A%25201%2529%2520the%2520inadequate%250Adata%2520utilization%2520of%2520images%2520within%2520each%2520iteration%2520brings%2520prolonged%2520pre-training%252C%250Aand%25202%2529%2520the%2520high%2520inconsistency%2520of%2520predictions%2520results%2520in%2520unreliable%2520generations%252C%250A%2524i.e.%2524%252C%2520the%2520prediction%2520of%2520the%2520identical%2520patch%2520may%2520be%2520inconsistent%2520in%2520different%250Amask%2520rounds%252C%2520leading%2520to%2520divergent%2520semantics%2520in%2520the%2520ultimately%2520generated%250Aoutcomes.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520the%2520efficient%2520masked%250Aautoencoders%2520with%2520self-consistency%2520%2528EMAE%2529%2520to%2520improve%2520the%2520pre-training%250Aefficiency%2520and%2520increase%2520the%2520consistency%2520of%2520MIM.%2520In%2520particular%252C%2520we%2520present%2520a%250Aparallel%2520mask%2520strategy%2520that%2520divides%2520the%2520image%2520into%2520K%2520non-overlapping%2520parts%252C%250Aeach%2520of%2520which%2520is%2520generated%2520by%2520a%2520random%2520mask%2520with%2520the%2520same%2520mask%2520ratio.%2520Then%2520the%250AMIM%2520task%2520is%2520conducted%2520parallelly%2520on%2520all%2520parts%2520in%2520an%2520iteration%2520and%2520the%2520model%250Aminimizes%2520the%2520loss%2520between%2520the%2520predictions%2520and%2520the%2520masked%2520patches.%2520Besides%252C%2520we%250Adesign%2520the%2520self-consistency%2520learning%2520to%2520further%2520maintain%2520the%2520consistency%2520of%250Apredictions%2520of%2520overlapping%2520masked%2520patches%2520among%2520parts.%2520Overall%252C%2520our%2520method%2520is%250Aable%2520to%2520exploit%2520the%2520data%2520more%2520efficiently%2520and%2520obtains%2520reliable%2520representations.%250AExperiments%2520on%2520ImageNet%2520show%2520that%2520EMAE%2520achieves%2520the%2520best%2520performance%2520on%250AViT-Large%2520with%2520only%252013%2525%2520of%2520MAE%2520pre-training%2520time%2520using%2520NVIDIA%2520A100%2520GPUs.%2520After%250Apre-training%2520on%2520diverse%2520datasets%252C%2520EMAE%2520consistently%2520obtains%2520state-of-the-art%250Atransfer%2520ability%2520on%2520a%2520variety%2520of%2520downstream%2520tasks%252C%2520such%2520as%2520image%250Aclassification%252C%2520object%2520detection%252C%2520and%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.14431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Masked%20Autoencoders%20with%20Self-Consistency&entry.906535625=Zhaowen%20Li%20and%20Yousong%20Zhu%20and%20Zhiyang%20Chen%20and%20Wei%20Li%20and%20Chaoyang%20Zhao%20and%20Rui%20Zhao%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Inspired%20by%20the%20masked%20language%20modeling%20%28MLM%29%20in%20natural%20language%20processing%0Atasks%2C%20the%20masked%20image%20modeling%20%28MIM%29%20has%20been%20recognized%20as%20a%20strong%0Aself-supervised%20pre-training%20method%20in%20computer%20vision.%20However%2C%20the%20high%0Arandom%20mask%20ratio%20of%20MIM%20results%20in%20two%20serious%20problems%3A%201%29%20the%20inadequate%0Adata%20utilization%20of%20images%20within%20each%20iteration%20brings%20prolonged%20pre-training%2C%0Aand%202%29%20the%20high%20inconsistency%20of%20predictions%20results%20in%20unreliable%20generations%2C%0A%24i.e.%24%2C%20the%20prediction%20of%20the%20identical%20patch%20may%20be%20inconsistent%20in%20different%0Amask%20rounds%2C%20leading%20to%20divergent%20semantics%20in%20the%20ultimately%20generated%0Aoutcomes.%20To%20tackle%20these%20problems%2C%20we%20propose%20the%20efficient%20masked%0Aautoencoders%20with%20self-consistency%20%28EMAE%29%20to%20improve%20the%20pre-training%0Aefficiency%20and%20increase%20the%20consistency%20of%20MIM.%20In%20particular%2C%20we%20present%20a%0Aparallel%20mask%20strategy%20that%20divides%20the%20image%20into%20K%20non-overlapping%20parts%2C%0Aeach%20of%20which%20is%20generated%20by%20a%20random%20mask%20with%20the%20same%20mask%20ratio.%20Then%20the%0AMIM%20task%20is%20conducted%20parallelly%20on%20all%20parts%20in%20an%20iteration%20and%20the%20model%0Aminimizes%20the%20loss%20between%20the%20predictions%20and%20the%20masked%20patches.%20Besides%2C%20we%0Adesign%20the%20self-consistency%20learning%20to%20further%20maintain%20the%20consistency%20of%0Apredictions%20of%20overlapping%20masked%20patches%20among%20parts.%20Overall%2C%20our%20method%20is%0Aable%20to%20exploit%20the%20data%20more%20efficiently%20and%20obtains%20reliable%20representations.%0AExperiments%20on%20ImageNet%20show%20that%20EMAE%20achieves%20the%20best%20performance%20on%0AViT-Large%20with%20only%2013%25%20of%20MAE%20pre-training%20time%20using%20NVIDIA%20A100%20GPUs.%20After%0Apre-training%20on%20diverse%20datasets%2C%20EMAE%20consistently%20obtains%20state-of-the-art%0Atransfer%20ability%20on%20a%20variety%20of%20downstream%20tasks%2C%20such%20as%20image%0Aclassification%2C%20object%20detection%2C%20and%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.14431v2&entry.124074799=Read"},
{"title": "NU-Class Net: A Novel Approach for Video Quality Enhancement", "author": "Parham Zilouchian Moghaddam and Mehdi Modarressi and Mohammad Amin Sadeghi", "abstract": "  Video content has experienced a surge in popularity, asserting its dominance\nover internet traffic and Internet of Things (IoT) networks. Video compression\nhas long been regarded as the primary means of efficiently managing the\nsubstantial multimedia traffic generated by video-capturing devices.\nNevertheless, video compression algorithms entail significant computational\ndemands in order to achieve substantial compression ratios. This complexity\npresents a formidable challenge when implementing efficient video coding\nstandards in resource-constrained embedded systems, such as IoT edge node\ncameras. To tackle this challenge, this paper introduces NU-Class Net, an\ninnovative deep-learning model designed to mitigate compression artifacts\nstemming from lossy compression codecs. This enhancement significantly elevates\nthe perceptible quality of low-bit-rate videos. By employing the NU-Class Net,\nthe video encoder within the video-capturing node can reduce output quality,\nthereby generating low-bit-rate videos and effectively curtailing both\ncomputation and bandwidth requirements at the edge. On the decoder side, which\nis typically less encumbered by resource limitations, NU-Class Net is applied\nafter the video decoder to compensate for artifacts and approximate the quality\nof the original video. Experimental results affirm the efficacy of the proposed\nmodel in enhancing the perceptible quality of videos, especially those streamed\nat low bit rates.\n", "link": "http://arxiv.org/abs/2401.01163v3", "date": "2024-06-03", "relevancy": 2.191, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5986}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5118}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NU-Class%20Net%3A%20A%20Novel%20Approach%20for%20Video%20Quality%20Enhancement&body=Title%3A%20NU-Class%20Net%3A%20A%20Novel%20Approach%20for%20Video%20Quality%20Enhancement%0AAuthor%3A%20Parham%20Zilouchian%20Moghaddam%20and%20Mehdi%20Modarressi%20and%20Mohammad%20Amin%20Sadeghi%0AAbstract%3A%20%20%20Video%20content%20has%20experienced%20a%20surge%20in%20popularity%2C%20asserting%20its%20dominance%0Aover%20internet%20traffic%20and%20Internet%20of%20Things%20%28IoT%29%20networks.%20Video%20compression%0Ahas%20long%20been%20regarded%20as%20the%20primary%20means%20of%20efficiently%20managing%20the%0Asubstantial%20multimedia%20traffic%20generated%20by%20video-capturing%20devices.%0ANevertheless%2C%20video%20compression%20algorithms%20entail%20significant%20computational%0Ademands%20in%20order%20to%20achieve%20substantial%20compression%20ratios.%20This%20complexity%0Apresents%20a%20formidable%20challenge%20when%20implementing%20efficient%20video%20coding%0Astandards%20in%20resource-constrained%20embedded%20systems%2C%20such%20as%20IoT%20edge%20node%0Acameras.%20To%20tackle%20this%20challenge%2C%20this%20paper%20introduces%20NU-Class%20Net%2C%20an%0Ainnovative%20deep-learning%20model%20designed%20to%20mitigate%20compression%20artifacts%0Astemming%20from%20lossy%20compression%20codecs.%20This%20enhancement%20significantly%20elevates%0Athe%20perceptible%20quality%20of%20low-bit-rate%20videos.%20By%20employing%20the%20NU-Class%20Net%2C%0Athe%20video%20encoder%20within%20the%20video-capturing%20node%20can%20reduce%20output%20quality%2C%0Athereby%20generating%20low-bit-rate%20videos%20and%20effectively%20curtailing%20both%0Acomputation%20and%20bandwidth%20requirements%20at%20the%20edge.%20On%20the%20decoder%20side%2C%20which%0Ais%20typically%20less%20encumbered%20by%20resource%20limitations%2C%20NU-Class%20Net%20is%20applied%0Aafter%20the%20video%20decoder%20to%20compensate%20for%20artifacts%20and%20approximate%20the%20quality%0Aof%20the%20original%20video.%20Experimental%20results%20affirm%20the%20efficacy%20of%20the%20proposed%0Amodel%20in%20enhancing%20the%20perceptible%20quality%20of%20videos%2C%20especially%20those%20streamed%0Aat%20low%20bit%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01163v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNU-Class%2520Net%253A%2520A%2520Novel%2520Approach%2520for%2520Video%2520Quality%2520Enhancement%26entry.906535625%3DParham%2520Zilouchian%2520Moghaddam%2520and%2520Mehdi%2520Modarressi%2520and%2520Mohammad%2520Amin%2520Sadeghi%26entry.1292438233%3D%2520%2520Video%2520content%2520has%2520experienced%2520a%2520surge%2520in%2520popularity%252C%2520asserting%2520its%2520dominance%250Aover%2520internet%2520traffic%2520and%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520networks.%2520Video%2520compression%250Ahas%2520long%2520been%2520regarded%2520as%2520the%2520primary%2520means%2520of%2520efficiently%2520managing%2520the%250Asubstantial%2520multimedia%2520traffic%2520generated%2520by%2520video-capturing%2520devices.%250ANevertheless%252C%2520video%2520compression%2520algorithms%2520entail%2520significant%2520computational%250Ademands%2520in%2520order%2520to%2520achieve%2520substantial%2520compression%2520ratios.%2520This%2520complexity%250Apresents%2520a%2520formidable%2520challenge%2520when%2520implementing%2520efficient%2520video%2520coding%250Astandards%2520in%2520resource-constrained%2520embedded%2520systems%252C%2520such%2520as%2520IoT%2520edge%2520node%250Acameras.%2520To%2520tackle%2520this%2520challenge%252C%2520this%2520paper%2520introduces%2520NU-Class%2520Net%252C%2520an%250Ainnovative%2520deep-learning%2520model%2520designed%2520to%2520mitigate%2520compression%2520artifacts%250Astemming%2520from%2520lossy%2520compression%2520codecs.%2520This%2520enhancement%2520significantly%2520elevates%250Athe%2520perceptible%2520quality%2520of%2520low-bit-rate%2520videos.%2520By%2520employing%2520the%2520NU-Class%2520Net%252C%250Athe%2520video%2520encoder%2520within%2520the%2520video-capturing%2520node%2520can%2520reduce%2520output%2520quality%252C%250Athereby%2520generating%2520low-bit-rate%2520videos%2520and%2520effectively%2520curtailing%2520both%250Acomputation%2520and%2520bandwidth%2520requirements%2520at%2520the%2520edge.%2520On%2520the%2520decoder%2520side%252C%2520which%250Ais%2520typically%2520less%2520encumbered%2520by%2520resource%2520limitations%252C%2520NU-Class%2520Net%2520is%2520applied%250Aafter%2520the%2520video%2520decoder%2520to%2520compensate%2520for%2520artifacts%2520and%2520approximate%2520the%2520quality%250Aof%2520the%2520original%2520video.%2520Experimental%2520results%2520affirm%2520the%2520efficacy%2520of%2520the%2520proposed%250Amodel%2520in%2520enhancing%2520the%2520perceptible%2520quality%2520of%2520videos%252C%2520especially%2520those%2520streamed%250Aat%2520low%2520bit%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01163v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NU-Class%20Net%3A%20A%20Novel%20Approach%20for%20Video%20Quality%20Enhancement&entry.906535625=Parham%20Zilouchian%20Moghaddam%20and%20Mehdi%20Modarressi%20and%20Mohammad%20Amin%20Sadeghi&entry.1292438233=%20%20Video%20content%20has%20experienced%20a%20surge%20in%20popularity%2C%20asserting%20its%20dominance%0Aover%20internet%20traffic%20and%20Internet%20of%20Things%20%28IoT%29%20networks.%20Video%20compression%0Ahas%20long%20been%20regarded%20as%20the%20primary%20means%20of%20efficiently%20managing%20the%0Asubstantial%20multimedia%20traffic%20generated%20by%20video-capturing%20devices.%0ANevertheless%2C%20video%20compression%20algorithms%20entail%20significant%20computational%0Ademands%20in%20order%20to%20achieve%20substantial%20compression%20ratios.%20This%20complexity%0Apresents%20a%20formidable%20challenge%20when%20implementing%20efficient%20video%20coding%0Astandards%20in%20resource-constrained%20embedded%20systems%2C%20such%20as%20IoT%20edge%20node%0Acameras.%20To%20tackle%20this%20challenge%2C%20this%20paper%20introduces%20NU-Class%20Net%2C%20an%0Ainnovative%20deep-learning%20model%20designed%20to%20mitigate%20compression%20artifacts%0Astemming%20from%20lossy%20compression%20codecs.%20This%20enhancement%20significantly%20elevates%0Athe%20perceptible%20quality%20of%20low-bit-rate%20videos.%20By%20employing%20the%20NU-Class%20Net%2C%0Athe%20video%20encoder%20within%20the%20video-capturing%20node%20can%20reduce%20output%20quality%2C%0Athereby%20generating%20low-bit-rate%20videos%20and%20effectively%20curtailing%20both%0Acomputation%20and%20bandwidth%20requirements%20at%20the%20edge.%20On%20the%20decoder%20side%2C%20which%0Ais%20typically%20less%20encumbered%20by%20resource%20limitations%2C%20NU-Class%20Net%20is%20applied%0Aafter%20the%20video%20decoder%20to%20compensate%20for%20artifacts%20and%20approximate%20the%20quality%0Aof%20the%20original%20video.%20Experimental%20results%20affirm%20the%20efficacy%20of%20the%20proposed%0Amodel%20in%20enhancing%20the%20perceptible%20quality%20of%20videos%2C%20especially%20those%20streamed%0Aat%20low%20bit%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01163v3&entry.124074799=Read"},
{"title": "Mitigating Motion Blur in Neural Radiance Fields with Events and Frames", "author": "Marco Cannici and Davide Scaramuzza", "abstract": "  Neural Radiance Fields (NeRFs) have shown great potential in novel view\nsynthesis. However, they struggle to render sharp images when the data used for\ntraining is affected by motion blur. On the other hand, event cameras excel in\ndynamic scenes as they measure brightness changes with microsecond resolution\nand are thus only marginally affected by blur. Recent methods attempt to\nenhance NeRF reconstructions under camera motion by fusing frames and events.\nHowever, they face challenges in recovering accurate color content or constrain\nthe NeRF to a set of predefined camera poses, harming reconstruction quality in\nchallenging conditions. This paper proposes a novel formulation addressing\nthese issues by leveraging both model- and learning-based modules. We\nexplicitly model the blur formation process, exploiting the event double\nintegral as an additional model-based prior. Additionally, we model the\nevent-pixel response using an end-to-end learnable response function, allowing\nour method to adapt to non-idealities in the real event-camera sensor. We show,\non synthetic and real data, that the proposed approach outperforms existing\ndeblur NeRFs that use only frames as well as those that combine frames and\nevents by +6.13dB and +2.48dB, respectively.\n", "link": "http://arxiv.org/abs/2403.19780v2", "date": "2024-06-03", "relevancy": 2.1779, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5663}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5422}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Motion%20Blur%20in%20Neural%20Radiance%20Fields%20with%20Events%20and%20Frames&body=Title%3A%20Mitigating%20Motion%20Blur%20in%20Neural%20Radiance%20Fields%20with%20Events%20and%20Frames%0AAuthor%3A%20Marco%20Cannici%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20great%20potential%20in%20novel%20view%0Asynthesis.%20However%2C%20they%20struggle%20to%20render%20sharp%20images%20when%20the%20data%20used%20for%0Atraining%20is%20affected%20by%20motion%20blur.%20On%20the%20other%20hand%2C%20event%20cameras%20excel%20in%0Adynamic%20scenes%20as%20they%20measure%20brightness%20changes%20with%20microsecond%20resolution%0Aand%20are%20thus%20only%20marginally%20affected%20by%20blur.%20Recent%20methods%20attempt%20to%0Aenhance%20NeRF%20reconstructions%20under%20camera%20motion%20by%20fusing%20frames%20and%20events.%0AHowever%2C%20they%20face%20challenges%20in%20recovering%20accurate%20color%20content%20or%20constrain%0Athe%20NeRF%20to%20a%20set%20of%20predefined%20camera%20poses%2C%20harming%20reconstruction%20quality%20in%0Achallenging%20conditions.%20This%20paper%20proposes%20a%20novel%20formulation%20addressing%0Athese%20issues%20by%20leveraging%20both%20model-%20and%20learning-based%20modules.%20We%0Aexplicitly%20model%20the%20blur%20formation%20process%2C%20exploiting%20the%20event%20double%0Aintegral%20as%20an%20additional%20model-based%20prior.%20Additionally%2C%20we%20model%20the%0Aevent-pixel%20response%20using%20an%20end-to-end%20learnable%20response%20function%2C%20allowing%0Aour%20method%20to%20adapt%20to%20non-idealities%20in%20the%20real%20event-camera%20sensor.%20We%20show%2C%0Aon%20synthetic%20and%20real%20data%2C%20that%20the%20proposed%20approach%20outperforms%20existing%0Adeblur%20NeRFs%20that%20use%20only%20frames%20as%20well%20as%20those%20that%20combine%20frames%20and%0Aevents%20by%20%2B6.13dB%20and%20%2B2.48dB%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Motion%2520Blur%2520in%2520Neural%2520Radiance%2520Fields%2520with%2520Events%2520and%2520Frames%26entry.906535625%3DMarco%2520Cannici%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520shown%2520great%2520potential%2520in%2520novel%2520view%250Asynthesis.%2520However%252C%2520they%2520struggle%2520to%2520render%2520sharp%2520images%2520when%2520the%2520data%2520used%2520for%250Atraining%2520is%2520affected%2520by%2520motion%2520blur.%2520On%2520the%2520other%2520hand%252C%2520event%2520cameras%2520excel%2520in%250Adynamic%2520scenes%2520as%2520they%2520measure%2520brightness%2520changes%2520with%2520microsecond%2520resolution%250Aand%2520are%2520thus%2520only%2520marginally%2520affected%2520by%2520blur.%2520Recent%2520methods%2520attempt%2520to%250Aenhance%2520NeRF%2520reconstructions%2520under%2520camera%2520motion%2520by%2520fusing%2520frames%2520and%2520events.%250AHowever%252C%2520they%2520face%2520challenges%2520in%2520recovering%2520accurate%2520color%2520content%2520or%2520constrain%250Athe%2520NeRF%2520to%2520a%2520set%2520of%2520predefined%2520camera%2520poses%252C%2520harming%2520reconstruction%2520quality%2520in%250Achallenging%2520conditions.%2520This%2520paper%2520proposes%2520a%2520novel%2520formulation%2520addressing%250Athese%2520issues%2520by%2520leveraging%2520both%2520model-%2520and%2520learning-based%2520modules.%2520We%250Aexplicitly%2520model%2520the%2520blur%2520formation%2520process%252C%2520exploiting%2520the%2520event%2520double%250Aintegral%2520as%2520an%2520additional%2520model-based%2520prior.%2520Additionally%252C%2520we%2520model%2520the%250Aevent-pixel%2520response%2520using%2520an%2520end-to-end%2520learnable%2520response%2520function%252C%2520allowing%250Aour%2520method%2520to%2520adapt%2520to%2520non-idealities%2520in%2520the%2520real%2520event-camera%2520sensor.%2520We%2520show%252C%250Aon%2520synthetic%2520and%2520real%2520data%252C%2520that%2520the%2520proposed%2520approach%2520outperforms%2520existing%250Adeblur%2520NeRFs%2520that%2520use%2520only%2520frames%2520as%2520well%2520as%2520those%2520that%2520combine%2520frames%2520and%250Aevents%2520by%2520%252B6.13dB%2520and%2520%252B2.48dB%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Motion%20Blur%20in%20Neural%20Radiance%20Fields%20with%20Events%20and%20Frames&entry.906535625=Marco%20Cannici%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20great%20potential%20in%20novel%20view%0Asynthesis.%20However%2C%20they%20struggle%20to%20render%20sharp%20images%20when%20the%20data%20used%20for%0Atraining%20is%20affected%20by%20motion%20blur.%20On%20the%20other%20hand%2C%20event%20cameras%20excel%20in%0Adynamic%20scenes%20as%20they%20measure%20brightness%20changes%20with%20microsecond%20resolution%0Aand%20are%20thus%20only%20marginally%20affected%20by%20blur.%20Recent%20methods%20attempt%20to%0Aenhance%20NeRF%20reconstructions%20under%20camera%20motion%20by%20fusing%20frames%20and%20events.%0AHowever%2C%20they%20face%20challenges%20in%20recovering%20accurate%20color%20content%20or%20constrain%0Athe%20NeRF%20to%20a%20set%20of%20predefined%20camera%20poses%2C%20harming%20reconstruction%20quality%20in%0Achallenging%20conditions.%20This%20paper%20proposes%20a%20novel%20formulation%20addressing%0Athese%20issues%20by%20leveraging%20both%20model-%20and%20learning-based%20modules.%20We%0Aexplicitly%20model%20the%20blur%20formation%20process%2C%20exploiting%20the%20event%20double%0Aintegral%20as%20an%20additional%20model-based%20prior.%20Additionally%2C%20we%20model%20the%0Aevent-pixel%20response%20using%20an%20end-to-end%20learnable%20response%20function%2C%20allowing%0Aour%20method%20to%20adapt%20to%20non-idealities%20in%20the%20real%20event-camera%20sensor.%20We%20show%2C%0Aon%20synthetic%20and%20real%20data%2C%20that%20the%20proposed%20approach%20outperforms%20existing%0Adeblur%20NeRFs%20that%20use%20only%20frames%20as%20well%20as%20those%20that%20combine%20frames%20and%0Aevents%20by%20%2B6.13dB%20and%20%2B2.48dB%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19780v2&entry.124074799=Read"},
{"title": "A Stochastic-Geometrical Framework for Object Pose Estimation based on\n  Mixture Models Avoiding the Correspondence Problem", "author": "Wolfgang Hoegele", "abstract": "  Background: Pose estimation of rigid objects is a practical challenge in\noptical metrology and computer vision. This paper presents a novel\nstochastic-geometrical modeling framework for object pose estimation based on\nobserving multiple feature points.\n  Methods: This framework utilizes mixture models for feature point densities\nin object space and for interpreting real measurements. Advantages are the\navoidance to resolve individual feature correspondences and to incorporate\ncorrect stochastic dependencies in multi-view applications. First, the general\nmodeling framework is presented, second, a general algorithm for pose\nestimation is derived, and third, two example models (camera and lateration\nsetup) are presented.\n  Results: Numerical experiments show the effectiveness of this modeling and\ngeneral algorithm by presenting four simulation scenarios for three observation\nsystems, including the dependence on measurement resolution, object\ndeformations and measurement noise. Probabilistic modeling utilizing mixture\nmodels shows the potential for accurate and robust pose estimations while\navoiding the correspondence problem.\n", "link": "http://arxiv.org/abs/2311.18107v5", "date": "2024-06-03", "relevancy": 2.1693, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.537}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Stochastic-Geometrical%20Framework%20for%20Object%20Pose%20Estimation%20based%20on%0A%20%20Mixture%20Models%20Avoiding%20the%20Correspondence%20Problem&body=Title%3A%20A%20Stochastic-Geometrical%20Framework%20for%20Object%20Pose%20Estimation%20based%20on%0A%20%20Mixture%20Models%20Avoiding%20the%20Correspondence%20Problem%0AAuthor%3A%20Wolfgang%20Hoegele%0AAbstract%3A%20%20%20Background%3A%20Pose%20estimation%20of%20rigid%20objects%20is%20a%20practical%20challenge%20in%0Aoptical%20metrology%20and%20computer%20vision.%20This%20paper%20presents%20a%20novel%0Astochastic-geometrical%20modeling%20framework%20for%20object%20pose%20estimation%20based%20on%0Aobserving%20multiple%20feature%20points.%0A%20%20Methods%3A%20This%20framework%20utilizes%20mixture%20models%20for%20feature%20point%20densities%0Ain%20object%20space%20and%20for%20interpreting%20real%20measurements.%20Advantages%20are%20the%0Aavoidance%20to%20resolve%20individual%20feature%20correspondences%20and%20to%20incorporate%0Acorrect%20stochastic%20dependencies%20in%20multi-view%20applications.%20First%2C%20the%20general%0Amodeling%20framework%20is%20presented%2C%20second%2C%20a%20general%20algorithm%20for%20pose%0Aestimation%20is%20derived%2C%20and%20third%2C%20two%20example%20models%20%28camera%20and%20lateration%0Asetup%29%20are%20presented.%0A%20%20Results%3A%20Numerical%20experiments%20show%20the%20effectiveness%20of%20this%20modeling%20and%0Ageneral%20algorithm%20by%20presenting%20four%20simulation%20scenarios%20for%20three%20observation%0Asystems%2C%20including%20the%20dependence%20on%20measurement%20resolution%2C%20object%0Adeformations%20and%20measurement%20noise.%20Probabilistic%20modeling%20utilizing%20mixture%0Amodels%20shows%20the%20potential%20for%20accurate%20and%20robust%20pose%20estimations%20while%0Aavoiding%20the%20correspondence%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18107v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Stochastic-Geometrical%2520Framework%2520for%2520Object%2520Pose%2520Estimation%2520based%2520on%250A%2520%2520Mixture%2520Models%2520Avoiding%2520the%2520Correspondence%2520Problem%26entry.906535625%3DWolfgang%2520Hoegele%26entry.1292438233%3D%2520%2520Background%253A%2520Pose%2520estimation%2520of%2520rigid%2520objects%2520is%2520a%2520practical%2520challenge%2520in%250Aoptical%2520metrology%2520and%2520computer%2520vision.%2520This%2520paper%2520presents%2520a%2520novel%250Astochastic-geometrical%2520modeling%2520framework%2520for%2520object%2520pose%2520estimation%2520based%2520on%250Aobserving%2520multiple%2520feature%2520points.%250A%2520%2520Methods%253A%2520This%2520framework%2520utilizes%2520mixture%2520models%2520for%2520feature%2520point%2520densities%250Ain%2520object%2520space%2520and%2520for%2520interpreting%2520real%2520measurements.%2520Advantages%2520are%2520the%250Aavoidance%2520to%2520resolve%2520individual%2520feature%2520correspondences%2520and%2520to%2520incorporate%250Acorrect%2520stochastic%2520dependencies%2520in%2520multi-view%2520applications.%2520First%252C%2520the%2520general%250Amodeling%2520framework%2520is%2520presented%252C%2520second%252C%2520a%2520general%2520algorithm%2520for%2520pose%250Aestimation%2520is%2520derived%252C%2520and%2520third%252C%2520two%2520example%2520models%2520%2528camera%2520and%2520lateration%250Asetup%2529%2520are%2520presented.%250A%2520%2520Results%253A%2520Numerical%2520experiments%2520show%2520the%2520effectiveness%2520of%2520this%2520modeling%2520and%250Ageneral%2520algorithm%2520by%2520presenting%2520four%2520simulation%2520scenarios%2520for%2520three%2520observation%250Asystems%252C%2520including%2520the%2520dependence%2520on%2520measurement%2520resolution%252C%2520object%250Adeformations%2520and%2520measurement%2520noise.%2520Probabilistic%2520modeling%2520utilizing%2520mixture%250Amodels%2520shows%2520the%2520potential%2520for%2520accurate%2520and%2520robust%2520pose%2520estimations%2520while%250Aavoiding%2520the%2520correspondence%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18107v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Stochastic-Geometrical%20Framework%20for%20Object%20Pose%20Estimation%20based%20on%0A%20%20Mixture%20Models%20Avoiding%20the%20Correspondence%20Problem&entry.906535625=Wolfgang%20Hoegele&entry.1292438233=%20%20Background%3A%20Pose%20estimation%20of%20rigid%20objects%20is%20a%20practical%20challenge%20in%0Aoptical%20metrology%20and%20computer%20vision.%20This%20paper%20presents%20a%20novel%0Astochastic-geometrical%20modeling%20framework%20for%20object%20pose%20estimation%20based%20on%0Aobserving%20multiple%20feature%20points.%0A%20%20Methods%3A%20This%20framework%20utilizes%20mixture%20models%20for%20feature%20point%20densities%0Ain%20object%20space%20and%20for%20interpreting%20real%20measurements.%20Advantages%20are%20the%0Aavoidance%20to%20resolve%20individual%20feature%20correspondences%20and%20to%20incorporate%0Acorrect%20stochastic%20dependencies%20in%20multi-view%20applications.%20First%2C%20the%20general%0Amodeling%20framework%20is%20presented%2C%20second%2C%20a%20general%20algorithm%20for%20pose%0Aestimation%20is%20derived%2C%20and%20third%2C%20two%20example%20models%20%28camera%20and%20lateration%0Asetup%29%20are%20presented.%0A%20%20Results%3A%20Numerical%20experiments%20show%20the%20effectiveness%20of%20this%20modeling%20and%0Ageneral%20algorithm%20by%20presenting%20four%20simulation%20scenarios%20for%20three%20observation%0Asystems%2C%20including%20the%20dependence%20on%20measurement%20resolution%2C%20object%0Adeformations%20and%20measurement%20noise.%20Probabilistic%20modeling%20utilizing%20mixture%0Amodels%20shows%20the%20potential%20for%20accurate%20and%20robust%20pose%20estimations%20while%0Aavoiding%20the%20correspondence%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18107v5&entry.124074799=Read"},
{"title": "Conservative Prediction via Data-Driven Confidence Minimization", "author": "Caroline Choi and Fahim Tajwar and Yoonho Lee and Huaxiu Yao and Ananya Kumar and Chelsea Finn", "abstract": "  In safety-critical applications of machine learning, it is often desirable\nfor a model to be conservative, abstaining from making predictions on unknown\ninputs which are not well-represented in the training data. However, detecting\nunknown examples is challenging, as it is impossible to anticipate all\npotential inputs at test time. To address this, prior work (Hendrycks et al.,\n2018) minimizes model confidence on an auxiliary outlier dataset carefully\ncurated to be disjoint from the training distribution. We theoretically analyze\nthe choice of auxiliary dataset for confidence minimization, revealing two\nactionable insights: (1) if the auxiliary set contains unknown examples similar\nto those seen at test time, confidence minimization leads to provable detection\nof unknown test examples, and (2) if the first condition is satisfied, it is\nunnecessary to filter out known examples for out-of-distribution (OOD)\ndetection. Motivated by these guidelines, we propose the Data-Driven Confidence\nMinimization (DCM) framework, which minimizes confidence on an uncertainty\ndataset. We apply DCM to two problem settings in which conservative prediction\nis paramount -- selective classification and OOD detection -- and provide a\nrealistic way to gather uncertainty data for each setting. In our experiments,\nDCM consistently outperforms existing selective classification approaches on 4\ndatasets when tested on unseen distributions and outperforms state-of-the-art\nOOD detection methods on 12 ID-OOD dataset pairs, reducing FPR (at TPR $95\\%$)\nby $6.3\\%$ and $58.1\\%$ on CIFAR-10 and CIFAR-100 compared to Outlier Exposure.\n", "link": "http://arxiv.org/abs/2306.04974v2", "date": "2024-06-03", "relevancy": 2.1397, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5502}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5457}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conservative%20Prediction%20via%20Data-Driven%20Confidence%20Minimization&body=Title%3A%20Conservative%20Prediction%20via%20Data-Driven%20Confidence%20Minimization%0AAuthor%3A%20Caroline%20Choi%20and%20Fahim%20Tajwar%20and%20Yoonho%20Lee%20and%20Huaxiu%20Yao%20and%20Ananya%20Kumar%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20In%20safety-critical%20applications%20of%20machine%20learning%2C%20it%20is%20often%20desirable%0Afor%20a%20model%20to%20be%20conservative%2C%20abstaining%20from%20making%20predictions%20on%20unknown%0Ainputs%20which%20are%20not%20well-represented%20in%20the%20training%20data.%20However%2C%20detecting%0Aunknown%20examples%20is%20challenging%2C%20as%20it%20is%20impossible%20to%20anticipate%20all%0Apotential%20inputs%20at%20test%20time.%20To%20address%20this%2C%20prior%20work%20%28Hendrycks%20et%20al.%2C%0A2018%29%20minimizes%20model%20confidence%20on%20an%20auxiliary%20outlier%20dataset%20carefully%0Acurated%20to%20be%20disjoint%20from%20the%20training%20distribution.%20We%20theoretically%20analyze%0Athe%20choice%20of%20auxiliary%20dataset%20for%20confidence%20minimization%2C%20revealing%20two%0Aactionable%20insights%3A%20%281%29%20if%20the%20auxiliary%20set%20contains%20unknown%20examples%20similar%0Ato%20those%20seen%20at%20test%20time%2C%20confidence%20minimization%20leads%20to%20provable%20detection%0Aof%20unknown%20test%20examples%2C%20and%20%282%29%20if%20the%20first%20condition%20is%20satisfied%2C%20it%20is%0Aunnecessary%20to%20filter%20out%20known%20examples%20for%20out-of-distribution%20%28OOD%29%0Adetection.%20Motivated%20by%20these%20guidelines%2C%20we%20propose%20the%20Data-Driven%20Confidence%0AMinimization%20%28DCM%29%20framework%2C%20which%20minimizes%20confidence%20on%20an%20uncertainty%0Adataset.%20We%20apply%20DCM%20to%20two%20problem%20settings%20in%20which%20conservative%20prediction%0Ais%20paramount%20--%20selective%20classification%20and%20OOD%20detection%20--%20and%20provide%20a%0Arealistic%20way%20to%20gather%20uncertainty%20data%20for%20each%20setting.%20In%20our%20experiments%2C%0ADCM%20consistently%20outperforms%20existing%20selective%20classification%20approaches%20on%204%0Adatasets%20when%20tested%20on%20unseen%20distributions%20and%20outperforms%20state-of-the-art%0AOOD%20detection%20methods%20on%2012%20ID-OOD%20dataset%20pairs%2C%20reducing%20FPR%20%28at%20TPR%20%2495%5C%25%24%29%0Aby%20%246.3%5C%25%24%20and%20%2458.1%5C%25%24%20on%20CIFAR-10%20and%20CIFAR-100%20compared%20to%20Outlier%20Exposure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConservative%2520Prediction%2520via%2520Data-Driven%2520Confidence%2520Minimization%26entry.906535625%3DCaroline%2520Choi%2520and%2520Fahim%2520Tajwar%2520and%2520Yoonho%2520Lee%2520and%2520Huaxiu%2520Yao%2520and%2520Ananya%2520Kumar%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520In%2520safety-critical%2520applications%2520of%2520machine%2520learning%252C%2520it%2520is%2520often%2520desirable%250Afor%2520a%2520model%2520to%2520be%2520conservative%252C%2520abstaining%2520from%2520making%2520predictions%2520on%2520unknown%250Ainputs%2520which%2520are%2520not%2520well-represented%2520in%2520the%2520training%2520data.%2520However%252C%2520detecting%250Aunknown%2520examples%2520is%2520challenging%252C%2520as%2520it%2520is%2520impossible%2520to%2520anticipate%2520all%250Apotential%2520inputs%2520at%2520test%2520time.%2520To%2520address%2520this%252C%2520prior%2520work%2520%2528Hendrycks%2520et%2520al.%252C%250A2018%2529%2520minimizes%2520model%2520confidence%2520on%2520an%2520auxiliary%2520outlier%2520dataset%2520carefully%250Acurated%2520to%2520be%2520disjoint%2520from%2520the%2520training%2520distribution.%2520We%2520theoretically%2520analyze%250Athe%2520choice%2520of%2520auxiliary%2520dataset%2520for%2520confidence%2520minimization%252C%2520revealing%2520two%250Aactionable%2520insights%253A%2520%25281%2529%2520if%2520the%2520auxiliary%2520set%2520contains%2520unknown%2520examples%2520similar%250Ato%2520those%2520seen%2520at%2520test%2520time%252C%2520confidence%2520minimization%2520leads%2520to%2520provable%2520detection%250Aof%2520unknown%2520test%2520examples%252C%2520and%2520%25282%2529%2520if%2520the%2520first%2520condition%2520is%2520satisfied%252C%2520it%2520is%250Aunnecessary%2520to%2520filter%2520out%2520known%2520examples%2520for%2520out-of-distribution%2520%2528OOD%2529%250Adetection.%2520Motivated%2520by%2520these%2520guidelines%252C%2520we%2520propose%2520the%2520Data-Driven%2520Confidence%250AMinimization%2520%2528DCM%2529%2520framework%252C%2520which%2520minimizes%2520confidence%2520on%2520an%2520uncertainty%250Adataset.%2520We%2520apply%2520DCM%2520to%2520two%2520problem%2520settings%2520in%2520which%2520conservative%2520prediction%250Ais%2520paramount%2520--%2520selective%2520classification%2520and%2520OOD%2520detection%2520--%2520and%2520provide%2520a%250Arealistic%2520way%2520to%2520gather%2520uncertainty%2520data%2520for%2520each%2520setting.%2520In%2520our%2520experiments%252C%250ADCM%2520consistently%2520outperforms%2520existing%2520selective%2520classification%2520approaches%2520on%25204%250Adatasets%2520when%2520tested%2520on%2520unseen%2520distributions%2520and%2520outperforms%2520state-of-the-art%250AOOD%2520detection%2520methods%2520on%252012%2520ID-OOD%2520dataset%2520pairs%252C%2520reducing%2520FPR%2520%2528at%2520TPR%2520%252495%255C%2525%2524%2529%250Aby%2520%25246.3%255C%2525%2524%2520and%2520%252458.1%255C%2525%2524%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520compared%2520to%2520Outlier%2520Exposure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conservative%20Prediction%20via%20Data-Driven%20Confidence%20Minimization&entry.906535625=Caroline%20Choi%20and%20Fahim%20Tajwar%20and%20Yoonho%20Lee%20and%20Huaxiu%20Yao%20and%20Ananya%20Kumar%20and%20Chelsea%20Finn&entry.1292438233=%20%20In%20safety-critical%20applications%20of%20machine%20learning%2C%20it%20is%20often%20desirable%0Afor%20a%20model%20to%20be%20conservative%2C%20abstaining%20from%20making%20predictions%20on%20unknown%0Ainputs%20which%20are%20not%20well-represented%20in%20the%20training%20data.%20However%2C%20detecting%0Aunknown%20examples%20is%20challenging%2C%20as%20it%20is%20impossible%20to%20anticipate%20all%0Apotential%20inputs%20at%20test%20time.%20To%20address%20this%2C%20prior%20work%20%28Hendrycks%20et%20al.%2C%0A2018%29%20minimizes%20model%20confidence%20on%20an%20auxiliary%20outlier%20dataset%20carefully%0Acurated%20to%20be%20disjoint%20from%20the%20training%20distribution.%20We%20theoretically%20analyze%0Athe%20choice%20of%20auxiliary%20dataset%20for%20confidence%20minimization%2C%20revealing%20two%0Aactionable%20insights%3A%20%281%29%20if%20the%20auxiliary%20set%20contains%20unknown%20examples%20similar%0Ato%20those%20seen%20at%20test%20time%2C%20confidence%20minimization%20leads%20to%20provable%20detection%0Aof%20unknown%20test%20examples%2C%20and%20%282%29%20if%20the%20first%20condition%20is%20satisfied%2C%20it%20is%0Aunnecessary%20to%20filter%20out%20known%20examples%20for%20out-of-distribution%20%28OOD%29%0Adetection.%20Motivated%20by%20these%20guidelines%2C%20we%20propose%20the%20Data-Driven%20Confidence%0AMinimization%20%28DCM%29%20framework%2C%20which%20minimizes%20confidence%20on%20an%20uncertainty%0Adataset.%20We%20apply%20DCM%20to%20two%20problem%20settings%20in%20which%20conservative%20prediction%0Ais%20paramount%20--%20selective%20classification%20and%20OOD%20detection%20--%20and%20provide%20a%0Arealistic%20way%20to%20gather%20uncertainty%20data%20for%20each%20setting.%20In%20our%20experiments%2C%0ADCM%20consistently%20outperforms%20existing%20selective%20classification%20approaches%20on%204%0Adatasets%20when%20tested%20on%20unseen%20distributions%20and%20outperforms%20state-of-the-art%0AOOD%20detection%20methods%20on%2012%20ID-OOD%20dataset%20pairs%2C%20reducing%20FPR%20%28at%20TPR%20%2495%5C%25%24%29%0Aby%20%246.3%5C%25%24%20and%20%2458.1%5C%25%24%20on%20CIFAR-10%20and%20CIFAR-100%20compared%20to%20Outlier%20Exposure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04974v2&entry.124074799=Read"},
{"title": "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis\n  Evaluation", "author": "Max Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen", "abstract": "  In the rapidly advancing field of conditional image generation research,\nchallenges such as limited explainability lie in effectively evaluating the\nperformance and capabilities of various models. This paper introduces VIEScore,\na Visual Instruction-guided Explainable metric for evaluating any conditional\nimage generation tasks. VIEScore leverages general knowledge from Multimodal\nLarge Language Models (MLLMs) as the backbone and does not require training or\nfine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image\ntasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of\n0.4 with human evaluations, while the human-to-human correlation is 0.45. (2)\nVIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v\nin evaluating synthetic images. (3) VIEScore achieves a correlation on par with\nhuman ratings in the generation tasks but struggles in editing tasks. With\nthese results, we believe VIEScore shows its great potential to replace human\njudges in evaluating image synthesis tasks.\n", "link": "http://arxiv.org/abs/2312.14867v2", "date": "2024-06-03", "relevancy": 2.1343, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5654}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5111}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIEScore%3A%20Towards%20Explainable%20Metrics%20for%20Conditional%20Image%20Synthesis%0A%20%20Evaluation&body=Title%3A%20VIEScore%3A%20Towards%20Explainable%20Metrics%20for%20Conditional%20Image%20Synthesis%0A%20%20Evaluation%0AAuthor%3A%20Max%20Ku%20and%20Dongfu%20Jiang%20and%20Cong%20Wei%20and%20Xiang%20Yue%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20In%20the%20rapidly%20advancing%20field%20of%20conditional%20image%20generation%20research%2C%0Achallenges%20such%20as%20limited%20explainability%20lie%20in%20effectively%20evaluating%20the%0Aperformance%20and%20capabilities%20of%20various%20models.%20This%20paper%20introduces%20VIEScore%2C%0Aa%20Visual%20Instruction-guided%20Explainable%20metric%20for%20evaluating%20any%20conditional%0Aimage%20generation%20tasks.%20VIEScore%20leverages%20general%20knowledge%20from%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20as%20the%20backbone%20and%20does%20not%20require%20training%20or%0Afine-tuning.%20We%20evaluate%20VIEScore%20on%20seven%20prominent%20tasks%20in%20conditional%20image%0Atasks%20and%20found%3A%20%281%29%20VIEScore%20%28GPT4-o%29%20achieves%20a%20high%20Spearman%20correlation%20of%0A0.4%20with%20human%20evaluations%2C%20while%20the%20human-to-human%20correlation%20is%200.45.%20%282%29%0AVIEScore%20%28with%20open-source%20MLLM%29%20is%20significantly%20weaker%20than%20GPT-4o%20and%20GPT-4v%0Ain%20evaluating%20synthetic%20images.%20%283%29%20VIEScore%20achieves%20a%20correlation%20on%20par%20with%0Ahuman%20ratings%20in%20the%20generation%20tasks%20but%20struggles%20in%20editing%20tasks.%20With%0Athese%20results%2C%20we%20believe%20VIEScore%20shows%20its%20great%20potential%20to%20replace%20human%0Ajudges%20in%20evaluating%20image%20synthesis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIEScore%253A%2520Towards%2520Explainable%2520Metrics%2520for%2520Conditional%2520Image%2520Synthesis%250A%2520%2520Evaluation%26entry.906535625%3DMax%2520Ku%2520and%2520Dongfu%2520Jiang%2520and%2520Cong%2520Wei%2520and%2520Xiang%2520Yue%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520advancing%2520field%2520of%2520conditional%2520image%2520generation%2520research%252C%250Achallenges%2520such%2520as%2520limited%2520explainability%2520lie%2520in%2520effectively%2520evaluating%2520the%250Aperformance%2520and%2520capabilities%2520of%2520various%2520models.%2520This%2520paper%2520introduces%2520VIEScore%252C%250Aa%2520Visual%2520Instruction-guided%2520Explainable%2520metric%2520for%2520evaluating%2520any%2520conditional%250Aimage%2520generation%2520tasks.%2520VIEScore%2520leverages%2520general%2520knowledge%2520from%2520Multimodal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%2520as%2520the%2520backbone%2520and%2520does%2520not%2520require%2520training%2520or%250Afine-tuning.%2520We%2520evaluate%2520VIEScore%2520on%2520seven%2520prominent%2520tasks%2520in%2520conditional%2520image%250Atasks%2520and%2520found%253A%2520%25281%2529%2520VIEScore%2520%2528GPT4-o%2529%2520achieves%2520a%2520high%2520Spearman%2520correlation%2520of%250A0.4%2520with%2520human%2520evaluations%252C%2520while%2520the%2520human-to-human%2520correlation%2520is%25200.45.%2520%25282%2529%250AVIEScore%2520%2528with%2520open-source%2520MLLM%2529%2520is%2520significantly%2520weaker%2520than%2520GPT-4o%2520and%2520GPT-4v%250Ain%2520evaluating%2520synthetic%2520images.%2520%25283%2529%2520VIEScore%2520achieves%2520a%2520correlation%2520on%2520par%2520with%250Ahuman%2520ratings%2520in%2520the%2520generation%2520tasks%2520but%2520struggles%2520in%2520editing%2520tasks.%2520With%250Athese%2520results%252C%2520we%2520believe%2520VIEScore%2520shows%2520its%2520great%2520potential%2520to%2520replace%2520human%250Ajudges%2520in%2520evaluating%2520image%2520synthesis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIEScore%3A%20Towards%20Explainable%20Metrics%20for%20Conditional%20Image%20Synthesis%0A%20%20Evaluation&entry.906535625=Max%20Ku%20and%20Dongfu%20Jiang%20and%20Cong%20Wei%20and%20Xiang%20Yue%20and%20Wenhu%20Chen&entry.1292438233=%20%20In%20the%20rapidly%20advancing%20field%20of%20conditional%20image%20generation%20research%2C%0Achallenges%20such%20as%20limited%20explainability%20lie%20in%20effectively%20evaluating%20the%0Aperformance%20and%20capabilities%20of%20various%20models.%20This%20paper%20introduces%20VIEScore%2C%0Aa%20Visual%20Instruction-guided%20Explainable%20metric%20for%20evaluating%20any%20conditional%0Aimage%20generation%20tasks.%20VIEScore%20leverages%20general%20knowledge%20from%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20as%20the%20backbone%20and%20does%20not%20require%20training%20or%0Afine-tuning.%20We%20evaluate%20VIEScore%20on%20seven%20prominent%20tasks%20in%20conditional%20image%0Atasks%20and%20found%3A%20%281%29%20VIEScore%20%28GPT4-o%29%20achieves%20a%20high%20Spearman%20correlation%20of%0A0.4%20with%20human%20evaluations%2C%20while%20the%20human-to-human%20correlation%20is%200.45.%20%282%29%0AVIEScore%20%28with%20open-source%20MLLM%29%20is%20significantly%20weaker%20than%20GPT-4o%20and%20GPT-4v%0Ain%20evaluating%20synthetic%20images.%20%283%29%20VIEScore%20achieves%20a%20correlation%20on%20par%20with%0Ahuman%20ratings%20in%20the%20generation%20tasks%20but%20struggles%20in%20editing%20tasks.%20With%0Athese%20results%2C%20we%20believe%20VIEScore%20shows%20its%20great%20potential%20to%20replace%20human%0Ajudges%20in%20evaluating%20image%20synthesis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14867v2&entry.124074799=Read"},
{"title": "Constrained Exploration via Reflected Replica Exchange Stochastic\n  Gradient Langevin Dynamics", "author": "Haoyang Zheng and Hengrong Du and Qi Feng and Wei Deng and Guang Lin", "abstract": "  Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an\neffective sampler for non-convex learning in large-scale datasets. However, the\nsimulation may encounter stagnation issues when the high-temperature chain\ndelves too deeply into the distribution tails. To tackle this issue, we propose\nreflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex\nexploration by utilizing reflection steps within a bounded domain.\nTheoretically, we observe that reducing the diameter of the domain enhances\nmixing rates, exhibiting a $\\textit{quadratic}$ behavior. Empirically, we test\nits performance through extensive experiments, including identifying dynamical\nsystems with physical constraints, simulations of constrained multi-modal\ndistributions, and image classification tasks. The theoretical and empirical\nfindings highlight the crucial role of constrained exploration in improving the\nsimulation efficiency.\n", "link": "http://arxiv.org/abs/2405.07839v2", "date": "2024-06-03", "relevancy": 2.1293, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5557}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5171}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics&body=Title%3A%20Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics%0AAuthor%3A%20Haoyang%20Zheng%20and%20Hengrong%20Du%20and%20Qi%20Feng%20and%20Wei%20Deng%20and%20Guang%20Lin%0AAbstract%3A%20%20%20Replica%20exchange%20stochastic%20gradient%20Langevin%20dynamics%20%28reSGLD%29%20is%20an%0Aeffective%20sampler%20for%20non-convex%20learning%20in%20large-scale%20datasets.%20However%2C%20the%0Asimulation%20may%20encounter%20stagnation%20issues%20when%20the%20high-temperature%20chain%0Adelves%20too%20deeply%20into%20the%20distribution%20tails.%20To%20tackle%20this%20issue%2C%20we%20propose%0Areflected%20reSGLD%20%28r2SGLD%29%3A%20an%20algorithm%20tailored%20for%20constrained%20non-convex%0Aexploration%20by%20utilizing%20reflection%20steps%20within%20a%20bounded%20domain.%0ATheoretically%2C%20we%20observe%20that%20reducing%20the%20diameter%20of%20the%20domain%20enhances%0Amixing%20rates%2C%20exhibiting%20a%20%24%5Ctextit%7Bquadratic%7D%24%20behavior.%20Empirically%2C%20we%20test%0Aits%20performance%20through%20extensive%20experiments%2C%20including%20identifying%20dynamical%0Asystems%20with%20physical%20constraints%2C%20simulations%20of%20constrained%20multi-modal%0Adistributions%2C%20and%20image%20classification%20tasks.%20The%20theoretical%20and%20empirical%0Afindings%20highlight%20the%20crucial%20role%20of%20constrained%20exploration%20in%20improving%20the%0Asimulation%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Exploration%2520via%2520Reflected%2520Replica%2520Exchange%2520Stochastic%250A%2520%2520Gradient%2520Langevin%2520Dynamics%26entry.906535625%3DHaoyang%2520Zheng%2520and%2520Hengrong%2520Du%2520and%2520Qi%2520Feng%2520and%2520Wei%2520Deng%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520Replica%2520exchange%2520stochastic%2520gradient%2520Langevin%2520dynamics%2520%2528reSGLD%2529%2520is%2520an%250Aeffective%2520sampler%2520for%2520non-convex%2520learning%2520in%2520large-scale%2520datasets.%2520However%252C%2520the%250Asimulation%2520may%2520encounter%2520stagnation%2520issues%2520when%2520the%2520high-temperature%2520chain%250Adelves%2520too%2520deeply%2520into%2520the%2520distribution%2520tails.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%250Areflected%2520reSGLD%2520%2528r2SGLD%2529%253A%2520an%2520algorithm%2520tailored%2520for%2520constrained%2520non-convex%250Aexploration%2520by%2520utilizing%2520reflection%2520steps%2520within%2520a%2520bounded%2520domain.%250ATheoretically%252C%2520we%2520observe%2520that%2520reducing%2520the%2520diameter%2520of%2520the%2520domain%2520enhances%250Amixing%2520rates%252C%2520exhibiting%2520a%2520%2524%255Ctextit%257Bquadratic%257D%2524%2520behavior.%2520Empirically%252C%2520we%2520test%250Aits%2520performance%2520through%2520extensive%2520experiments%252C%2520including%2520identifying%2520dynamical%250Asystems%2520with%2520physical%2520constraints%252C%2520simulations%2520of%2520constrained%2520multi-modal%250Adistributions%252C%2520and%2520image%2520classification%2520tasks.%2520The%2520theoretical%2520and%2520empirical%250Afindings%2520highlight%2520the%2520crucial%2520role%2520of%2520constrained%2520exploration%2520in%2520improving%2520the%250Asimulation%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics&entry.906535625=Haoyang%20Zheng%20and%20Hengrong%20Du%20and%20Qi%20Feng%20and%20Wei%20Deng%20and%20Guang%20Lin&entry.1292438233=%20%20Replica%20exchange%20stochastic%20gradient%20Langevin%20dynamics%20%28reSGLD%29%20is%20an%0Aeffective%20sampler%20for%20non-convex%20learning%20in%20large-scale%20datasets.%20However%2C%20the%0Asimulation%20may%20encounter%20stagnation%20issues%20when%20the%20high-temperature%20chain%0Adelves%20too%20deeply%20into%20the%20distribution%20tails.%20To%20tackle%20this%20issue%2C%20we%20propose%0Areflected%20reSGLD%20%28r2SGLD%29%3A%20an%20algorithm%20tailored%20for%20constrained%20non-convex%0Aexploration%20by%20utilizing%20reflection%20steps%20within%20a%20bounded%20domain.%0ATheoretically%2C%20we%20observe%20that%20reducing%20the%20diameter%20of%20the%20domain%20enhances%0Amixing%20rates%2C%20exhibiting%20a%20%24%5Ctextit%7Bquadratic%7D%24%20behavior.%20Empirically%2C%20we%20test%0Aits%20performance%20through%20extensive%20experiments%2C%20including%20identifying%20dynamical%0Asystems%20with%20physical%20constraints%2C%20simulations%20of%20constrained%20multi-modal%0Adistributions%2C%20and%20image%20classification%20tasks.%20The%20theoretical%20and%20empirical%0Afindings%20highlight%20the%20crucial%20role%20of%20constrained%20exploration%20in%20improving%20the%0Asimulation%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07839v2&entry.124074799=Read"},
{"title": "FissionFusion: Fast Geometric Generation and Hierarchical Souping for\n  Medical Image Analysis", "author": "Santosh Sanjeev and Nuren Zhaksylyk and Ibrahim Almakky and Anees Ur Rehman Hashmi and Mohammad Areeb Qazi and Mohammad Yaqub", "abstract": "  The scarcity of well-annotated medical datasets requires leveraging transfer\nlearning from broader datasets like ImageNet or pre-trained models like CLIP.\nModel soups averages multiple fine-tuned models aiming to improve performance\non In-Domain (ID) tasks and enhance robustness against Out-of-Distribution\n(OOD) datasets. However, applying these methods to the medical imaging domain\nfaces challenges and results in suboptimal performance. This is primarily due\nto differences in error surface characteristics that stem from data\ncomplexities such as heterogeneity, domain shift, class imbalance, and\ndistributional shifts between training and testing phases. To address this\nissue, we propose a hierarchical merging approach that involves local and\nglobal aggregation of models at various levels based on models' hyperparameter\nconfigurations. Furthermore, to alleviate the need for training a large number\nof models in the hyperparameter search, we introduce a computationally\nefficient method using a cyclical learning rate scheduler to produce multiple\nmodels for aggregation in the weight space. Our method demonstrates significant\nimprovements over the model souping approach across multiple datasets (around\n6% gain in HAM10000 and CheXpert datasets) while maintaining low computational\ncosts for model generation and selection. Moreover, we achieve better results\non OOD datasets than model soups. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/FissionFusion.\n", "link": "http://arxiv.org/abs/2403.13341v2", "date": "2024-06-03", "relevancy": 2.1, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FissionFusion%3A%20Fast%20Geometric%20Generation%20and%20Hierarchical%20Souping%20for%0A%20%20Medical%20Image%20Analysis&body=Title%3A%20FissionFusion%3A%20Fast%20Geometric%20Generation%20and%20Hierarchical%20Souping%20for%0A%20%20Medical%20Image%20Analysis%0AAuthor%3A%20Santosh%20Sanjeev%20and%20Nuren%20Zhaksylyk%20and%20Ibrahim%20Almakky%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Mohammad%20Areeb%20Qazi%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20The%20scarcity%20of%20well-annotated%20medical%20datasets%20requires%20leveraging%20transfer%0Alearning%20from%20broader%20datasets%20like%20ImageNet%20or%20pre-trained%20models%20like%20CLIP.%0AModel%20soups%20averages%20multiple%20fine-tuned%20models%20aiming%20to%20improve%20performance%0Aon%20In-Domain%20%28ID%29%20tasks%20and%20enhance%20robustness%20against%20Out-of-Distribution%0A%28OOD%29%20datasets.%20However%2C%20applying%20these%20methods%20to%20the%20medical%20imaging%20domain%0Afaces%20challenges%20and%20results%20in%20suboptimal%20performance.%20This%20is%20primarily%20due%0Ato%20differences%20in%20error%20surface%20characteristics%20that%20stem%20from%20data%0Acomplexities%20such%20as%20heterogeneity%2C%20domain%20shift%2C%20class%20imbalance%2C%20and%0Adistributional%20shifts%20between%20training%20and%20testing%20phases.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20hierarchical%20merging%20approach%20that%20involves%20local%20and%0Aglobal%20aggregation%20of%20models%20at%20various%20levels%20based%20on%20models%27%20hyperparameter%0Aconfigurations.%20Furthermore%2C%20to%20alleviate%20the%20need%20for%20training%20a%20large%20number%0Aof%20models%20in%20the%20hyperparameter%20search%2C%20we%20introduce%20a%20computationally%0Aefficient%20method%20using%20a%20cyclical%20learning%20rate%20scheduler%20to%20produce%20multiple%0Amodels%20for%20aggregation%20in%20the%20weight%20space.%20Our%20method%20demonstrates%20significant%0Aimprovements%20over%20the%20model%20souping%20approach%20across%20multiple%20datasets%20%28around%0A6%25%20gain%20in%20HAM10000%20and%20CheXpert%20datasets%29%20while%20maintaining%20low%20computational%0Acosts%20for%20model%20generation%20and%20selection.%20Moreover%2C%20we%20achieve%20better%20results%0Aon%20OOD%20datasets%20than%20model%20soups.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/FissionFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFissionFusion%253A%2520Fast%2520Geometric%2520Generation%2520and%2520Hierarchical%2520Souping%2520for%250A%2520%2520Medical%2520Image%2520Analysis%26entry.906535625%3DSantosh%2520Sanjeev%2520and%2520Nuren%2520Zhaksylyk%2520and%2520Ibrahim%2520Almakky%2520and%2520Anees%2520Ur%2520Rehman%2520Hashmi%2520and%2520Mohammad%2520Areeb%2520Qazi%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520well-annotated%2520medical%2520datasets%2520requires%2520leveraging%2520transfer%250Alearning%2520from%2520broader%2520datasets%2520like%2520ImageNet%2520or%2520pre-trained%2520models%2520like%2520CLIP.%250AModel%2520soups%2520averages%2520multiple%2520fine-tuned%2520models%2520aiming%2520to%2520improve%2520performance%250Aon%2520In-Domain%2520%2528ID%2529%2520tasks%2520and%2520enhance%2520robustness%2520against%2520Out-of-Distribution%250A%2528OOD%2529%2520datasets.%2520However%252C%2520applying%2520these%2520methods%2520to%2520the%2520medical%2520imaging%2520domain%250Afaces%2520challenges%2520and%2520results%2520in%2520suboptimal%2520performance.%2520This%2520is%2520primarily%2520due%250Ato%2520differences%2520in%2520error%2520surface%2520characteristics%2520that%2520stem%2520from%2520data%250Acomplexities%2520such%2520as%2520heterogeneity%252C%2520domain%2520shift%252C%2520class%2520imbalance%252C%2520and%250Adistributional%2520shifts%2520between%2520training%2520and%2520testing%2520phases.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520hierarchical%2520merging%2520approach%2520that%2520involves%2520local%2520and%250Aglobal%2520aggregation%2520of%2520models%2520at%2520various%2520levels%2520based%2520on%2520models%2527%2520hyperparameter%250Aconfigurations.%2520Furthermore%252C%2520to%2520alleviate%2520the%2520need%2520for%2520training%2520a%2520large%2520number%250Aof%2520models%2520in%2520the%2520hyperparameter%2520search%252C%2520we%2520introduce%2520a%2520computationally%250Aefficient%2520method%2520using%2520a%2520cyclical%2520learning%2520rate%2520scheduler%2520to%2520produce%2520multiple%250Amodels%2520for%2520aggregation%2520in%2520the%2520weight%2520space.%2520Our%2520method%2520demonstrates%2520significant%250Aimprovements%2520over%2520the%2520model%2520souping%2520approach%2520across%2520multiple%2520datasets%2520%2528around%250A6%2525%2520gain%2520in%2520HAM10000%2520and%2520CheXpert%2520datasets%2529%2520while%2520maintaining%2520low%2520computational%250Acosts%2520for%2520model%2520generation%2520and%2520selection.%2520Moreover%252C%2520we%2520achieve%2520better%2520results%250Aon%2520OOD%2520datasets%2520than%2520model%2520soups.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/BioMedIA-MBZUAI/FissionFusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FissionFusion%3A%20Fast%20Geometric%20Generation%20and%20Hierarchical%20Souping%20for%0A%20%20Medical%20Image%20Analysis&entry.906535625=Santosh%20Sanjeev%20and%20Nuren%20Zhaksylyk%20and%20Ibrahim%20Almakky%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Mohammad%20Areeb%20Qazi%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20The%20scarcity%20of%20well-annotated%20medical%20datasets%20requires%20leveraging%20transfer%0Alearning%20from%20broader%20datasets%20like%20ImageNet%20or%20pre-trained%20models%20like%20CLIP.%0AModel%20soups%20averages%20multiple%20fine-tuned%20models%20aiming%20to%20improve%20performance%0Aon%20In-Domain%20%28ID%29%20tasks%20and%20enhance%20robustness%20against%20Out-of-Distribution%0A%28OOD%29%20datasets.%20However%2C%20applying%20these%20methods%20to%20the%20medical%20imaging%20domain%0Afaces%20challenges%20and%20results%20in%20suboptimal%20performance.%20This%20is%20primarily%20due%0Ato%20differences%20in%20error%20surface%20characteristics%20that%20stem%20from%20data%0Acomplexities%20such%20as%20heterogeneity%2C%20domain%20shift%2C%20class%20imbalance%2C%20and%0Adistributional%20shifts%20between%20training%20and%20testing%20phases.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20hierarchical%20merging%20approach%20that%20involves%20local%20and%0Aglobal%20aggregation%20of%20models%20at%20various%20levels%20based%20on%20models%27%20hyperparameter%0Aconfigurations.%20Furthermore%2C%20to%20alleviate%20the%20need%20for%20training%20a%20large%20number%0Aof%20models%20in%20the%20hyperparameter%20search%2C%20we%20introduce%20a%20computationally%0Aefficient%20method%20using%20a%20cyclical%20learning%20rate%20scheduler%20to%20produce%20multiple%0Amodels%20for%20aggregation%20in%20the%20weight%20space.%20Our%20method%20demonstrates%20significant%0Aimprovements%20over%20the%20model%20souping%20approach%20across%20multiple%20datasets%20%28around%0A6%25%20gain%20in%20HAM10000%20and%20CheXpert%20datasets%29%20while%20maintaining%20low%20computational%0Acosts%20for%20model%20generation%20and%20selection.%20Moreover%2C%20we%20achieve%20better%20results%0Aon%20OOD%20datasets%20than%20model%20soups.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/FissionFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13341v2&entry.124074799=Read"},
{"title": "Flood and Echo Net: Algorithmically Aligned GNNs that Generalize", "author": "Jo\u00ebl Mathys and Florian Gr\u00f6tschla and Kalyan Varma Nadimpalli and Roger Wattenhofer", "abstract": "  Most Graph Neural Networks follow the standard message-passing framework\nwhere, in each step, all nodes simultaneously communicate with each other. We\nwant to challenge this paradigm by aligning the computation more closely to the\nexecution of distributed algorithms and propose the Flood and Echo Net. A\nsingle round of a Flood and Echo Net consists of an origin node and a flooding\nphase followed by an echo phase. First, during the flooding, messages are sent\nfrom the origin and propagated outwards throughout the entire graph. Then,\nduring the echo, the message flow reverses and messages are sent back towards\nthe origin. As nodes are only sparsely activated upon receiving a message, this\nleads to a wave-like activation pattern that traverses the graph. Through these\nsparse but parallel activations, the Net becomes more expressive than\ntraditional MPNNs which are limited by the 1-WL test and also is provably more\nefficient in terms of message complexity. Moreover, the mechanism's inherent\nability to generalize across graphs of varying sizes positions it as a\npractical architecture for the task of algorithmic learning. We test the Flood\nand Echo Net on a variety of synthetic tasks and the SALSA-CLRS benchmark and\nfind that the algorithmic alignment of the execution improves generalization to\nlarger graph sizes.\n", "link": "http://arxiv.org/abs/2310.06970v3", "date": "2024-06-03", "relevancy": 2.0353, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5511}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4798}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flood%20and%20Echo%20Net%3A%20Algorithmically%20Aligned%20GNNs%20that%20Generalize&body=Title%3A%20Flood%20and%20Echo%20Net%3A%20Algorithmically%20Aligned%20GNNs%20that%20Generalize%0AAuthor%3A%20Jo%C3%ABl%20Mathys%20and%20Florian%20Gr%C3%B6tschla%20and%20Kalyan%20Varma%20Nadimpalli%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Most%20Graph%20Neural%20Networks%20follow%20the%20standard%20message-passing%20framework%0Awhere%2C%20in%20each%20step%2C%20all%20nodes%20simultaneously%20communicate%20with%20each%20other.%20We%0Awant%20to%20challenge%20this%20paradigm%20by%20aligning%20the%20computation%20more%20closely%20to%20the%0Aexecution%20of%20distributed%20algorithms%20and%20propose%20the%20Flood%20and%20Echo%20Net.%20A%0Asingle%20round%20of%20a%20Flood%20and%20Echo%20Net%20consists%20of%20an%20origin%20node%20and%20a%20flooding%0Aphase%20followed%20by%20an%20echo%20phase.%20First%2C%20during%20the%20flooding%2C%20messages%20are%20sent%0Afrom%20the%20origin%20and%20propagated%20outwards%20throughout%20the%20entire%20graph.%20Then%2C%0Aduring%20the%20echo%2C%20the%20message%20flow%20reverses%20and%20messages%20are%20sent%20back%20towards%0Athe%20origin.%20As%20nodes%20are%20only%20sparsely%20activated%20upon%20receiving%20a%20message%2C%20this%0Aleads%20to%20a%20wave-like%20activation%20pattern%20that%20traverses%20the%20graph.%20Through%20these%0Asparse%20but%20parallel%20activations%2C%20the%20Net%20becomes%20more%20expressive%20than%0Atraditional%20MPNNs%20which%20are%20limited%20by%20the%201-WL%20test%20and%20also%20is%20provably%20more%0Aefficient%20in%20terms%20of%20message%20complexity.%20Moreover%2C%20the%20mechanism%27s%20inherent%0Aability%20to%20generalize%20across%20graphs%20of%20varying%20sizes%20positions%20it%20as%20a%0Apractical%20architecture%20for%20the%20task%20of%20algorithmic%20learning.%20We%20test%20the%20Flood%0Aand%20Echo%20Net%20on%20a%20variety%20of%20synthetic%20tasks%20and%20the%20SALSA-CLRS%20benchmark%20and%0Afind%20that%20the%20algorithmic%20alignment%20of%20the%20execution%20improves%20generalization%20to%0Alarger%20graph%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06970v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlood%2520and%2520Echo%2520Net%253A%2520Algorithmically%2520Aligned%2520GNNs%2520that%2520Generalize%26entry.906535625%3DJo%25C3%25ABl%2520Mathys%2520and%2520Florian%2520Gr%25C3%25B6tschla%2520and%2520Kalyan%2520Varma%2520Nadimpalli%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Most%2520Graph%2520Neural%2520Networks%2520follow%2520the%2520standard%2520message-passing%2520framework%250Awhere%252C%2520in%2520each%2520step%252C%2520all%2520nodes%2520simultaneously%2520communicate%2520with%2520each%2520other.%2520We%250Awant%2520to%2520challenge%2520this%2520paradigm%2520by%2520aligning%2520the%2520computation%2520more%2520closely%2520to%2520the%250Aexecution%2520of%2520distributed%2520algorithms%2520and%2520propose%2520the%2520Flood%2520and%2520Echo%2520Net.%2520A%250Asingle%2520round%2520of%2520a%2520Flood%2520and%2520Echo%2520Net%2520consists%2520of%2520an%2520origin%2520node%2520and%2520a%2520flooding%250Aphase%2520followed%2520by%2520an%2520echo%2520phase.%2520First%252C%2520during%2520the%2520flooding%252C%2520messages%2520are%2520sent%250Afrom%2520the%2520origin%2520and%2520propagated%2520outwards%2520throughout%2520the%2520entire%2520graph.%2520Then%252C%250Aduring%2520the%2520echo%252C%2520the%2520message%2520flow%2520reverses%2520and%2520messages%2520are%2520sent%2520back%2520towards%250Athe%2520origin.%2520As%2520nodes%2520are%2520only%2520sparsely%2520activated%2520upon%2520receiving%2520a%2520message%252C%2520this%250Aleads%2520to%2520a%2520wave-like%2520activation%2520pattern%2520that%2520traverses%2520the%2520graph.%2520Through%2520these%250Asparse%2520but%2520parallel%2520activations%252C%2520the%2520Net%2520becomes%2520more%2520expressive%2520than%250Atraditional%2520MPNNs%2520which%2520are%2520limited%2520by%2520the%25201-WL%2520test%2520and%2520also%2520is%2520provably%2520more%250Aefficient%2520in%2520terms%2520of%2520message%2520complexity.%2520Moreover%252C%2520the%2520mechanism%2527s%2520inherent%250Aability%2520to%2520generalize%2520across%2520graphs%2520of%2520varying%2520sizes%2520positions%2520it%2520as%2520a%250Apractical%2520architecture%2520for%2520the%2520task%2520of%2520algorithmic%2520learning.%2520We%2520test%2520the%2520Flood%250Aand%2520Echo%2520Net%2520on%2520a%2520variety%2520of%2520synthetic%2520tasks%2520and%2520the%2520SALSA-CLRS%2520benchmark%2520and%250Afind%2520that%2520the%2520algorithmic%2520alignment%2520of%2520the%2520execution%2520improves%2520generalization%2520to%250Alarger%2520graph%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06970v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flood%20and%20Echo%20Net%3A%20Algorithmically%20Aligned%20GNNs%20that%20Generalize&entry.906535625=Jo%C3%ABl%20Mathys%20and%20Florian%20Gr%C3%B6tschla%20and%20Kalyan%20Varma%20Nadimpalli%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Most%20Graph%20Neural%20Networks%20follow%20the%20standard%20message-passing%20framework%0Awhere%2C%20in%20each%20step%2C%20all%20nodes%20simultaneously%20communicate%20with%20each%20other.%20We%0Awant%20to%20challenge%20this%20paradigm%20by%20aligning%20the%20computation%20more%20closely%20to%20the%0Aexecution%20of%20distributed%20algorithms%20and%20propose%20the%20Flood%20and%20Echo%20Net.%20A%0Asingle%20round%20of%20a%20Flood%20and%20Echo%20Net%20consists%20of%20an%20origin%20node%20and%20a%20flooding%0Aphase%20followed%20by%20an%20echo%20phase.%20First%2C%20during%20the%20flooding%2C%20messages%20are%20sent%0Afrom%20the%20origin%20and%20propagated%20outwards%20throughout%20the%20entire%20graph.%20Then%2C%0Aduring%20the%20echo%2C%20the%20message%20flow%20reverses%20and%20messages%20are%20sent%20back%20towards%0Athe%20origin.%20As%20nodes%20are%20only%20sparsely%20activated%20upon%20receiving%20a%20message%2C%20this%0Aleads%20to%20a%20wave-like%20activation%20pattern%20that%20traverses%20the%20graph.%20Through%20these%0Asparse%20but%20parallel%20activations%2C%20the%20Net%20becomes%20more%20expressive%20than%0Atraditional%20MPNNs%20which%20are%20limited%20by%20the%201-WL%20test%20and%20also%20is%20provably%20more%0Aefficient%20in%20terms%20of%20message%20complexity.%20Moreover%2C%20the%20mechanism%27s%20inherent%0Aability%20to%20generalize%20across%20graphs%20of%20varying%20sizes%20positions%20it%20as%20a%0Apractical%20architecture%20for%20the%20task%20of%20algorithmic%20learning.%20We%20test%20the%20Flood%0Aand%20Echo%20Net%20on%20a%20variety%20of%20synthetic%20tasks%20and%20the%20SALSA-CLRS%20benchmark%20and%0Afind%20that%20the%20algorithmic%20alignment%20of%20the%20execution%20improves%20generalization%20to%0Alarger%20graph%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06970v3&entry.124074799=Read"},
{"title": "Robust and Conjugate Gaussian Process Regression", "author": "Matias Altamirano and Fran\u00e7ois-Xavier Briol and Jeremias Knoblauch", "abstract": "  To enable closed form conditioning, a common assumption in Gaussian process\n(GP) regression is independent and identically distributed Gaussian observation\nnoise. This strong and simplistic assumption is often violated in practice,\nwhich leads to unreliable inferences and uncertainty quantification.\nUnfortunately, existing methods for robustifying GPs break closed-form\nconditioning, which makes them less attractive to practitioners and\nsignificantly more computationally expensive. In this paper, we demonstrate how\nto perform provably robust and conjugate Gaussian process (RCGP) regression at\nvirtually no additional cost using generalised Bayesian inference. RCGP is\nparticularly versatile as it enables exact conjugate closed form updates in all\nsettings where standard GPs admit them. To demonstrate its strong empirical\nperformance, we deploy RCGP for problems ranging from Bayesian optimisation to\nsparse variational Gaussian processes.\n", "link": "http://arxiv.org/abs/2311.00463v2", "date": "2024-06-03", "relevancy": 2.0269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5111}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Conjugate%20Gaussian%20Process%20Regression&body=Title%3A%20Robust%20and%20Conjugate%20Gaussian%20Process%20Regression%0AAuthor%3A%20Matias%20Altamirano%20and%20Fran%C3%A7ois-Xavier%20Briol%20and%20Jeremias%20Knoblauch%0AAbstract%3A%20%20%20To%20enable%20closed%20form%20conditioning%2C%20a%20common%20assumption%20in%20Gaussian%20process%0A%28GP%29%20regression%20is%20independent%20and%20identically%20distributed%20Gaussian%20observation%0Anoise.%20This%20strong%20and%20simplistic%20assumption%20is%20often%20violated%20in%20practice%2C%0Awhich%20leads%20to%20unreliable%20inferences%20and%20uncertainty%20quantification.%0AUnfortunately%2C%20existing%20methods%20for%20robustifying%20GPs%20break%20closed-form%0Aconditioning%2C%20which%20makes%20them%20less%20attractive%20to%20practitioners%20and%0Asignificantly%20more%20computationally%20expensive.%20In%20this%20paper%2C%20we%20demonstrate%20how%0Ato%20perform%20provably%20robust%20and%20conjugate%20Gaussian%20process%20%28RCGP%29%20regression%20at%0Avirtually%20no%20additional%20cost%20using%20generalised%20Bayesian%20inference.%20RCGP%20is%0Aparticularly%20versatile%20as%20it%20enables%20exact%20conjugate%20closed%20form%20updates%20in%20all%0Asettings%20where%20standard%20GPs%20admit%20them.%20To%20demonstrate%20its%20strong%20empirical%0Aperformance%2C%20we%20deploy%20RCGP%20for%20problems%20ranging%20from%20Bayesian%20optimisation%20to%0Asparse%20variational%20Gaussian%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Conjugate%2520Gaussian%2520Process%2520Regression%26entry.906535625%3DMatias%2520Altamirano%2520and%2520Fran%25C3%25A7ois-Xavier%2520Briol%2520and%2520Jeremias%2520Knoblauch%26entry.1292438233%3D%2520%2520To%2520enable%2520closed%2520form%2520conditioning%252C%2520a%2520common%2520assumption%2520in%2520Gaussian%2520process%250A%2528GP%2529%2520regression%2520is%2520independent%2520and%2520identically%2520distributed%2520Gaussian%2520observation%250Anoise.%2520This%2520strong%2520and%2520simplistic%2520assumption%2520is%2520often%2520violated%2520in%2520practice%252C%250Awhich%2520leads%2520to%2520unreliable%2520inferences%2520and%2520uncertainty%2520quantification.%250AUnfortunately%252C%2520existing%2520methods%2520for%2520robustifying%2520GPs%2520break%2520closed-form%250Aconditioning%252C%2520which%2520makes%2520them%2520less%2520attractive%2520to%2520practitioners%2520and%250Asignificantly%2520more%2520computationally%2520expensive.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520how%250Ato%2520perform%2520provably%2520robust%2520and%2520conjugate%2520Gaussian%2520process%2520%2528RCGP%2529%2520regression%2520at%250Avirtually%2520no%2520additional%2520cost%2520using%2520generalised%2520Bayesian%2520inference.%2520RCGP%2520is%250Aparticularly%2520versatile%2520as%2520it%2520enables%2520exact%2520conjugate%2520closed%2520form%2520updates%2520in%2520all%250Asettings%2520where%2520standard%2520GPs%2520admit%2520them.%2520To%2520demonstrate%2520its%2520strong%2520empirical%250Aperformance%252C%2520we%2520deploy%2520RCGP%2520for%2520problems%2520ranging%2520from%2520Bayesian%2520optimisation%2520to%250Asparse%2520variational%2520Gaussian%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Conjugate%20Gaussian%20Process%20Regression&entry.906535625=Matias%20Altamirano%20and%20Fran%C3%A7ois-Xavier%20Briol%20and%20Jeremias%20Knoblauch&entry.1292438233=%20%20To%20enable%20closed%20form%20conditioning%2C%20a%20common%20assumption%20in%20Gaussian%20process%0A%28GP%29%20regression%20is%20independent%20and%20identically%20distributed%20Gaussian%20observation%0Anoise.%20This%20strong%20and%20simplistic%20assumption%20is%20often%20violated%20in%20practice%2C%0Awhich%20leads%20to%20unreliable%20inferences%20and%20uncertainty%20quantification.%0AUnfortunately%2C%20existing%20methods%20for%20robustifying%20GPs%20break%20closed-form%0Aconditioning%2C%20which%20makes%20them%20less%20attractive%20to%20practitioners%20and%0Asignificantly%20more%20computationally%20expensive.%20In%20this%20paper%2C%20we%20demonstrate%20how%0Ato%20perform%20provably%20robust%20and%20conjugate%20Gaussian%20process%20%28RCGP%29%20regression%20at%0Avirtually%20no%20additional%20cost%20using%20generalised%20Bayesian%20inference.%20RCGP%20is%0Aparticularly%20versatile%20as%20it%20enables%20exact%20conjugate%20closed%20form%20updates%20in%20all%0Asettings%20where%20standard%20GPs%20admit%20them.%20To%20demonstrate%20its%20strong%20empirical%0Aperformance%2C%20we%20deploy%20RCGP%20for%20problems%20ranging%20from%20Bayesian%20optimisation%20to%0Asparse%20variational%20Gaussian%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00463v2&entry.124074799=Read"},
{"title": "Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with\n  Spectral Imbalance", "author": "Chiraag Kaushik and Ran Liu and Chi-Heng Lin and Amrit Khera and Matthew Y Jin and Wenrui Ma and Vidya Muthukumar and Eva L Dyer", "abstract": "  Classification models are expected to perform equally well for different\nclasses, yet in practice, there are often large gaps in their performance. This\nissue of class bias is widely studied in cases of datasets with sample\nimbalance, but is relatively overlooked in balanced datasets. In this work, we\nintroduce the concept of spectral imbalance in features as a potential source\nfor class disparities and study the connections between spectral imbalance and\nclass bias in both theory and practice. To build the connection between\nspectral imbalance and class gap, we develop a theoretical framework for\nstudying class disparities and derive exact expressions for the per-class error\nin a high-dimensional mixture model setting. We then study this phenomenon in\n11 different state-of-the-art pretrained encoders and show how our proposed\nframework can be used to compare the quality of encoders, as well as evaluate\nand combine data augmentation strategies to mitigate the issue. Our work sheds\nlight on the class-dependent effects of learning, and provides new insights\ninto how state-of-the-art pretrained features may have unknown biases that can\nbe diagnosed through their spectra.\n", "link": "http://arxiv.org/abs/2402.11742v2", "date": "2024-06-03", "relevancy": 2.0118, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.576}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4581}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balanced%20Data%2C%20Imbalanced%20Spectra%3A%20Unveiling%20Class%20Disparities%20with%0A%20%20Spectral%20Imbalance&body=Title%3A%20Balanced%20Data%2C%20Imbalanced%20Spectra%3A%20Unveiling%20Class%20Disparities%20with%0A%20%20Spectral%20Imbalance%0AAuthor%3A%20Chiraag%20Kaushik%20and%20Ran%20Liu%20and%20Chi-Heng%20Lin%20and%20Amrit%20Khera%20and%20Matthew%20Y%20Jin%20and%20Wenrui%20Ma%20and%20Vidya%20Muthukumar%20and%20Eva%20L%20Dyer%0AAbstract%3A%20%20%20Classification%20models%20are%20expected%20to%20perform%20equally%20well%20for%20different%0Aclasses%2C%20yet%20in%20practice%2C%20there%20are%20often%20large%20gaps%20in%20their%20performance.%20This%0Aissue%20of%20class%20bias%20is%20widely%20studied%20in%20cases%20of%20datasets%20with%20sample%0Aimbalance%2C%20but%20is%20relatively%20overlooked%20in%20balanced%20datasets.%20In%20this%20work%2C%20we%0Aintroduce%20the%20concept%20of%20spectral%20imbalance%20in%20features%20as%20a%20potential%20source%0Afor%20class%20disparities%20and%20study%20the%20connections%20between%20spectral%20imbalance%20and%0Aclass%20bias%20in%20both%20theory%20and%20practice.%20To%20build%20the%20connection%20between%0Aspectral%20imbalance%20and%20class%20gap%2C%20we%20develop%20a%20theoretical%20framework%20for%0Astudying%20class%20disparities%20and%20derive%20exact%20expressions%20for%20the%20per-class%20error%0Ain%20a%20high-dimensional%20mixture%20model%20setting.%20We%20then%20study%20this%20phenomenon%20in%0A11%20different%20state-of-the-art%20pretrained%20encoders%20and%20show%20how%20our%20proposed%0Aframework%20can%20be%20used%20to%20compare%20the%20quality%20of%20encoders%2C%20as%20well%20as%20evaluate%0Aand%20combine%20data%20augmentation%20strategies%20to%20mitigate%20the%20issue.%20Our%20work%20sheds%0Alight%20on%20the%20class-dependent%20effects%20of%20learning%2C%20and%20provides%20new%20insights%0Ainto%20how%20state-of-the-art%20pretrained%20features%20may%20have%20unknown%20biases%20that%20can%0Abe%20diagnosed%20through%20their%20spectra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanced%2520Data%252C%2520Imbalanced%2520Spectra%253A%2520Unveiling%2520Class%2520Disparities%2520with%250A%2520%2520Spectral%2520Imbalance%26entry.906535625%3DChiraag%2520Kaushik%2520and%2520Ran%2520Liu%2520and%2520Chi-Heng%2520Lin%2520and%2520Amrit%2520Khera%2520and%2520Matthew%2520Y%2520Jin%2520and%2520Wenrui%2520Ma%2520and%2520Vidya%2520Muthukumar%2520and%2520Eva%2520L%2520Dyer%26entry.1292438233%3D%2520%2520Classification%2520models%2520are%2520expected%2520to%2520perform%2520equally%2520well%2520for%2520different%250Aclasses%252C%2520yet%2520in%2520practice%252C%2520there%2520are%2520often%2520large%2520gaps%2520in%2520their%2520performance.%2520This%250Aissue%2520of%2520class%2520bias%2520is%2520widely%2520studied%2520in%2520cases%2520of%2520datasets%2520with%2520sample%250Aimbalance%252C%2520but%2520is%2520relatively%2520overlooked%2520in%2520balanced%2520datasets.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520concept%2520of%2520spectral%2520imbalance%2520in%2520features%2520as%2520a%2520potential%2520source%250Afor%2520class%2520disparities%2520and%2520study%2520the%2520connections%2520between%2520spectral%2520imbalance%2520and%250Aclass%2520bias%2520in%2520both%2520theory%2520and%2520practice.%2520To%2520build%2520the%2520connection%2520between%250Aspectral%2520imbalance%2520and%2520class%2520gap%252C%2520we%2520develop%2520a%2520theoretical%2520framework%2520for%250Astudying%2520class%2520disparities%2520and%2520derive%2520exact%2520expressions%2520for%2520the%2520per-class%2520error%250Ain%2520a%2520high-dimensional%2520mixture%2520model%2520setting.%2520We%2520then%2520study%2520this%2520phenomenon%2520in%250A11%2520different%2520state-of-the-art%2520pretrained%2520encoders%2520and%2520show%2520how%2520our%2520proposed%250Aframework%2520can%2520be%2520used%2520to%2520compare%2520the%2520quality%2520of%2520encoders%252C%2520as%2520well%2520as%2520evaluate%250Aand%2520combine%2520data%2520augmentation%2520strategies%2520to%2520mitigate%2520the%2520issue.%2520Our%2520work%2520sheds%250Alight%2520on%2520the%2520class-dependent%2520effects%2520of%2520learning%252C%2520and%2520provides%2520new%2520insights%250Ainto%2520how%2520state-of-the-art%2520pretrained%2520features%2520may%2520have%2520unknown%2520biases%2520that%2520can%250Abe%2520diagnosed%2520through%2520their%2520spectra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%20Data%2C%20Imbalanced%20Spectra%3A%20Unveiling%20Class%20Disparities%20with%0A%20%20Spectral%20Imbalance&entry.906535625=Chiraag%20Kaushik%20and%20Ran%20Liu%20and%20Chi-Heng%20Lin%20and%20Amrit%20Khera%20and%20Matthew%20Y%20Jin%20and%20Wenrui%20Ma%20and%20Vidya%20Muthukumar%20and%20Eva%20L%20Dyer&entry.1292438233=%20%20Classification%20models%20are%20expected%20to%20perform%20equally%20well%20for%20different%0Aclasses%2C%20yet%20in%20practice%2C%20there%20are%20often%20large%20gaps%20in%20their%20performance.%20This%0Aissue%20of%20class%20bias%20is%20widely%20studied%20in%20cases%20of%20datasets%20with%20sample%0Aimbalance%2C%20but%20is%20relatively%20overlooked%20in%20balanced%20datasets.%20In%20this%20work%2C%20we%0Aintroduce%20the%20concept%20of%20spectral%20imbalance%20in%20features%20as%20a%20potential%20source%0Afor%20class%20disparities%20and%20study%20the%20connections%20between%20spectral%20imbalance%20and%0Aclass%20bias%20in%20both%20theory%20and%20practice.%20To%20build%20the%20connection%20between%0Aspectral%20imbalance%20and%20class%20gap%2C%20we%20develop%20a%20theoretical%20framework%20for%0Astudying%20class%20disparities%20and%20derive%20exact%20expressions%20for%20the%20per-class%20error%0Ain%20a%20high-dimensional%20mixture%20model%20setting.%20We%20then%20study%20this%20phenomenon%20in%0A11%20different%20state-of-the-art%20pretrained%20encoders%20and%20show%20how%20our%20proposed%0Aframework%20can%20be%20used%20to%20compare%20the%20quality%20of%20encoders%2C%20as%20well%20as%20evaluate%0Aand%20combine%20data%20augmentation%20strategies%20to%20mitigate%20the%20issue.%20Our%20work%20sheds%0Alight%20on%20the%20class-dependent%20effects%20of%20learning%2C%20and%20provides%20new%20insights%0Ainto%20how%20state-of-the-art%20pretrained%20features%20may%20have%20unknown%20biases%20that%20can%0Abe%20diagnosed%20through%20their%20spectra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11742v2&entry.124074799=Read"},
{"title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced\n  Zero/Few-Shot Forecasting of Multivariate Time Series", "author": "Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam", "abstract": "  Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40\\%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. Model weights for our\ninitial variant (TTM-Q) are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-v1. Model weights for\nmore sophisticated variants (TTM-B, TTM-E, and TTM-A) will be shared soon. The\nsource code for TTM can be accessed at\nhttps://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer.\n", "link": "http://arxiv.org/abs/2401.03955v6", "date": "2024-06-03", "relevancy": 2.011, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5117}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20Time%20Mixers%20%28TTMs%29%3A%20Fast%20Pre-trained%20Models%20for%20Enhanced%0A%20%20Zero/Few-Shot%20Forecasting%20of%20Multivariate%20Time%20Series&body=Title%3A%20Tiny%20Time%20Mixers%20%28TTMs%29%3A%20Fast%20Pre-trained%20Models%20for%20Enhanced%0A%20%20Zero/Few-Shot%20Forecasting%20of%20Multivariate%20Time%20Series%0AAuthor%3A%20Vijay%20Ekambaram%20and%20Arindam%20Jati%20and%20Pankaj%20Dayama%20and%20Sumanta%20Mukherjee%20and%20Nam%20H.%20Nguyen%20and%20Wesley%20M.%20Gifford%20and%20Chandra%20Reddy%20and%20Jayant%20Kalagnanam%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20excel%20in%20zero/few-shot%20learning%20for%20language%20and%0Avision%20tasks%20but%20face%20challenges%20in%20multivariate%20time%20series%20%28TS%29%20forecasting%0Adue%20to%20diverse%20data%20characteristics.%20Consequently%2C%20recent%20research%20efforts%20have%0Afocused%20on%20developing%20pre-trained%20TS%20forecasting%20models.%20These%20models%2C%20whether%0Abuilt%20from%20scratch%20or%20adapted%20from%20large%20language%20models%20%28LLMs%29%2C%20excel%20in%0Azero/few-shot%20forecasting%20tasks.%20However%2C%20they%20are%20limited%20by%20slow%20performance%2C%0Ahigh%20computational%20demands%2C%20and%20neglect%20of%20cross-channel%20and%20exogenous%0Acorrelations.%20To%20address%20this%2C%20we%20introduce%20Tiny%20Time%20Mixers%20%28TTM%29%2C%20a%20compact%0Amodel%20%28starting%20from%201M%20parameters%29%20with%20effective%20transfer%20learning%0Acapabilities%2C%20trained%20exclusively%20on%20public%20TS%20datasets.%20TTM%2C%20based%20on%20the%0Alight-weight%20TSMixer%20architecture%2C%20incorporates%20innovations%20like%20adaptive%0Apatching%2C%20diverse%20resolution%20sampling%2C%20and%20resolution%20prefix%20tuning%20to%20handle%0Apre-training%20on%20varied%20dataset%20resolutions%20with%20minimal%20model%20capacity.%0AAdditionally%2C%20it%20employs%20multi-level%20modeling%20to%20capture%20channel%20correlations%0Aand%20infuse%20exogenous%20signals%20during%20fine-tuning.%20TTM%20outperforms%20existing%0Apopular%20benchmarks%20in%20zero/few-shot%20forecasting%20by%20%284-40%5C%25%29%2C%20while%20reducing%0Acomputational%20requirements%20significantly.%20Moreover%2C%20TTMs%20are%20lightweight%20and%0Acan%20be%20executed%20even%20on%20CPU-only%20machines%2C%20enhancing%20usability%20and%20fostering%0Awider%20adoption%20in%20resource-constrained%20environments.%20Model%20weights%20for%20our%0Ainitial%20variant%20%28TTM-Q%29%20are%20available%20at%0Ahttps%3A//huggingface.co/ibm-granite/granite-timeseries-ttm-v1.%20Model%20weights%20for%0Amore%20sophisticated%20variants%20%28TTM-B%2C%20TTM-E%2C%20and%20TTM-A%29%20will%20be%20shared%20soon.%20The%0Asource%20code%20for%20TTM%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03955v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520Time%2520Mixers%2520%2528TTMs%2529%253A%2520Fast%2520Pre-trained%2520Models%2520for%2520Enhanced%250A%2520%2520Zero/Few-Shot%2520Forecasting%2520of%2520Multivariate%2520Time%2520Series%26entry.906535625%3DVijay%2520Ekambaram%2520and%2520Arindam%2520Jati%2520and%2520Pankaj%2520Dayama%2520and%2520Sumanta%2520Mukherjee%2520and%2520Nam%2520H.%2520Nguyen%2520and%2520Wesley%2520M.%2520Gifford%2520and%2520Chandra%2520Reddy%2520and%2520Jayant%2520Kalagnanam%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520excel%2520in%2520zero/few-shot%2520learning%2520for%2520language%2520and%250Avision%2520tasks%2520but%2520face%2520challenges%2520in%2520multivariate%2520time%2520series%2520%2528TS%2529%2520forecasting%250Adue%2520to%2520diverse%2520data%2520characteristics.%2520Consequently%252C%2520recent%2520research%2520efforts%2520have%250Afocused%2520on%2520developing%2520pre-trained%2520TS%2520forecasting%2520models.%2520These%2520models%252C%2520whether%250Abuilt%2520from%2520scratch%2520or%2520adapted%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520excel%2520in%250Azero/few-shot%2520forecasting%2520tasks.%2520However%252C%2520they%2520are%2520limited%2520by%2520slow%2520performance%252C%250Ahigh%2520computational%2520demands%252C%2520and%2520neglect%2520of%2520cross-channel%2520and%2520exogenous%250Acorrelations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Tiny%2520Time%2520Mixers%2520%2528TTM%2529%252C%2520a%2520compact%250Amodel%2520%2528starting%2520from%25201M%2520parameters%2529%2520with%2520effective%2520transfer%2520learning%250Acapabilities%252C%2520trained%2520exclusively%2520on%2520public%2520TS%2520datasets.%2520TTM%252C%2520based%2520on%2520the%250Alight-weight%2520TSMixer%2520architecture%252C%2520incorporates%2520innovations%2520like%2520adaptive%250Apatching%252C%2520diverse%2520resolution%2520sampling%252C%2520and%2520resolution%2520prefix%2520tuning%2520to%2520handle%250Apre-training%2520on%2520varied%2520dataset%2520resolutions%2520with%2520minimal%2520model%2520capacity.%250AAdditionally%252C%2520it%2520employs%2520multi-level%2520modeling%2520to%2520capture%2520channel%2520correlations%250Aand%2520infuse%2520exogenous%2520signals%2520during%2520fine-tuning.%2520TTM%2520outperforms%2520existing%250Apopular%2520benchmarks%2520in%2520zero/few-shot%2520forecasting%2520by%2520%25284-40%255C%2525%2529%252C%2520while%2520reducing%250Acomputational%2520requirements%2520significantly.%2520Moreover%252C%2520TTMs%2520are%2520lightweight%2520and%250Acan%2520be%2520executed%2520even%2520on%2520CPU-only%2520machines%252C%2520enhancing%2520usability%2520and%2520fostering%250Awider%2520adoption%2520in%2520resource-constrained%2520environments.%2520Model%2520weights%2520for%2520our%250Ainitial%2520variant%2520%2528TTM-Q%2529%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/ibm-granite/granite-timeseries-ttm-v1.%2520Model%2520weights%2520for%250Amore%2520sophisticated%2520variants%2520%2528TTM-B%252C%2520TTM-E%252C%2520and%2520TTM-A%2529%2520will%2520be%2520shared%2520soon.%2520The%250Asource%2520code%2520for%2520TTM%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03955v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20Time%20Mixers%20%28TTMs%29%3A%20Fast%20Pre-trained%20Models%20for%20Enhanced%0A%20%20Zero/Few-Shot%20Forecasting%20of%20Multivariate%20Time%20Series&entry.906535625=Vijay%20Ekambaram%20and%20Arindam%20Jati%20and%20Pankaj%20Dayama%20and%20Sumanta%20Mukherjee%20and%20Nam%20H.%20Nguyen%20and%20Wesley%20M.%20Gifford%20and%20Chandra%20Reddy%20and%20Jayant%20Kalagnanam&entry.1292438233=%20%20Large%20pre-trained%20models%20excel%20in%20zero/few-shot%20learning%20for%20language%20and%0Avision%20tasks%20but%20face%20challenges%20in%20multivariate%20time%20series%20%28TS%29%20forecasting%0Adue%20to%20diverse%20data%20characteristics.%20Consequently%2C%20recent%20research%20efforts%20have%0Afocused%20on%20developing%20pre-trained%20TS%20forecasting%20models.%20These%20models%2C%20whether%0Abuilt%20from%20scratch%20or%20adapted%20from%20large%20language%20models%20%28LLMs%29%2C%20excel%20in%0Azero/few-shot%20forecasting%20tasks.%20However%2C%20they%20are%20limited%20by%20slow%20performance%2C%0Ahigh%20computational%20demands%2C%20and%20neglect%20of%20cross-channel%20and%20exogenous%0Acorrelations.%20To%20address%20this%2C%20we%20introduce%20Tiny%20Time%20Mixers%20%28TTM%29%2C%20a%20compact%0Amodel%20%28starting%20from%201M%20parameters%29%20with%20effective%20transfer%20learning%0Acapabilities%2C%20trained%20exclusively%20on%20public%20TS%20datasets.%20TTM%2C%20based%20on%20the%0Alight-weight%20TSMixer%20architecture%2C%20incorporates%20innovations%20like%20adaptive%0Apatching%2C%20diverse%20resolution%20sampling%2C%20and%20resolution%20prefix%20tuning%20to%20handle%0Apre-training%20on%20varied%20dataset%20resolutions%20with%20minimal%20model%20capacity.%0AAdditionally%2C%20it%20employs%20multi-level%20modeling%20to%20capture%20channel%20correlations%0Aand%20infuse%20exogenous%20signals%20during%20fine-tuning.%20TTM%20outperforms%20existing%0Apopular%20benchmarks%20in%20zero/few-shot%20forecasting%20by%20%284-40%5C%25%29%2C%20while%20reducing%0Acomputational%20requirements%20significantly.%20Moreover%2C%20TTMs%20are%20lightweight%20and%0Acan%20be%20executed%20even%20on%20CPU-only%20machines%2C%20enhancing%20usability%20and%20fostering%0Awider%20adoption%20in%20resource-constrained%20environments.%20Model%20weights%20for%20our%0Ainitial%20variant%20%28TTM-Q%29%20are%20available%20at%0Ahttps%3A//huggingface.co/ibm-granite/granite-timeseries-ttm-v1.%20Model%20weights%20for%0Amore%20sophisticated%20variants%20%28TTM-B%2C%20TTM-E%2C%20and%20TTM-A%29%20will%20be%20shared%20soon.%20The%0Asource%20code%20for%20TTM%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03955v6&entry.124074799=Read"},
{"title": "HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional\n  Embedding", "author": "Zhao Li and Xin Wang and Jun Zhao and Wenbin Guo and Jianxin Li", "abstract": "  Knowledge hypergraph embedding models are usually computationally expensive\ndue to the inherent complex semantic information. However, existing works\nmainly focus on improving the effectiveness of knowledge hypergraph embedding,\nmaking the model architecture more complex and redundant. It is desirable and\nchallenging for knowledge hypergraph embedding to reach a trade-off between\nmodel effectiveness and efficiency. In this paper, we propose an end-to-end\nefficient n-ary knowledge hypergraph embedding model, HyCubE, which designs a\nnovel 3D circular convolutional neural network and the alternate mask stack\nstrategy to enhance the interaction and extraction of feature information\ncomprehensively. Furthermore, our proposed model achieves a better trade-off\nbetween effectiveness and efficiency by adaptively adjusting the 3D circular\nconvolutional layer structure to handle different arity knowledge hypergraphs\nwith fewer parameters. In addition, we use 1-N multilinear scoring based on the\nentity mask mechanism to further accelerate the model training efficiency.\nFinally, extensive experimental results on all datasets demonstrate that our\nproposed model consistently outperforms state-of-the-art baselines, with an\naverage improvement of 7.30%-9.53% and a maximum improvement of 33.82% across\nall metrics. Meanwhile, HyCubE is 4.12x faster, GPU memory usage is 52.19%\nlower, and the number of parameters is reduced by 85.21% compared with the\naverage metric of the latest state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2402.08961v2", "date": "2024-06-03", "relevancy": 2.0077, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5103}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyCubE%3A%20Efficient%20Knowledge%20Hypergraph%203D%20Circular%20Convolutional%0A%20%20Embedding&body=Title%3A%20HyCubE%3A%20Efficient%20Knowledge%20Hypergraph%203D%20Circular%20Convolutional%0A%20%20Embedding%0AAuthor%3A%20Zhao%20Li%20and%20Xin%20Wang%20and%20Jun%20Zhao%20and%20Wenbin%20Guo%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Knowledge%20hypergraph%20embedding%20models%20are%20usually%20computationally%20expensive%0Adue%20to%20the%20inherent%20complex%20semantic%20information.%20However%2C%20existing%20works%0Amainly%20focus%20on%20improving%20the%20effectiveness%20of%20knowledge%20hypergraph%20embedding%2C%0Amaking%20the%20model%20architecture%20more%20complex%20and%20redundant.%20It%20is%20desirable%20and%0Achallenging%20for%20knowledge%20hypergraph%20embedding%20to%20reach%20a%20trade-off%20between%0Amodel%20effectiveness%20and%20efficiency.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Aefficient%20n-ary%20knowledge%20hypergraph%20embedding%20model%2C%20HyCubE%2C%20which%20designs%20a%0Anovel%203D%20circular%20convolutional%20neural%20network%20and%20the%20alternate%20mask%20stack%0Astrategy%20to%20enhance%20the%20interaction%20and%20extraction%20of%20feature%20information%0Acomprehensively.%20Furthermore%2C%20our%20proposed%20model%20achieves%20a%20better%20trade-off%0Abetween%20effectiveness%20and%20efficiency%20by%20adaptively%20adjusting%20the%203D%20circular%0Aconvolutional%20layer%20structure%20to%20handle%20different%20arity%20knowledge%20hypergraphs%0Awith%20fewer%20parameters.%20In%20addition%2C%20we%20use%201-N%20multilinear%20scoring%20based%20on%20the%0Aentity%20mask%20mechanism%20to%20further%20accelerate%20the%20model%20training%20efficiency.%0AFinally%2C%20extensive%20experimental%20results%20on%20all%20datasets%20demonstrate%20that%20our%0Aproposed%20model%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20with%20an%0Aaverage%20improvement%20of%207.30%25-9.53%25%20and%20a%20maximum%20improvement%20of%2033.82%25%20across%0Aall%20metrics.%20Meanwhile%2C%20HyCubE%20is%204.12x%20faster%2C%20GPU%20memory%20usage%20is%2052.19%25%0Alower%2C%20and%20the%20number%20of%20parameters%20is%20reduced%20by%2085.21%25%20compared%20with%20the%0Aaverage%20metric%20of%20the%20latest%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyCubE%253A%2520Efficient%2520Knowledge%2520Hypergraph%25203D%2520Circular%2520Convolutional%250A%2520%2520Embedding%26entry.906535625%3DZhao%2520Li%2520and%2520Xin%2520Wang%2520and%2520Jun%2520Zhao%2520and%2520Wenbin%2520Guo%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Knowledge%2520hypergraph%2520embedding%2520models%2520are%2520usually%2520computationally%2520expensive%250Adue%2520to%2520the%2520inherent%2520complex%2520semantic%2520information.%2520However%252C%2520existing%2520works%250Amainly%2520focus%2520on%2520improving%2520the%2520effectiveness%2520of%2520knowledge%2520hypergraph%2520embedding%252C%250Amaking%2520the%2520model%2520architecture%2520more%2520complex%2520and%2520redundant.%2520It%2520is%2520desirable%2520and%250Achallenging%2520for%2520knowledge%2520hypergraph%2520embedding%2520to%2520reach%2520a%2520trade-off%2520between%250Amodel%2520effectiveness%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%250Aefficient%2520n-ary%2520knowledge%2520hypergraph%2520embedding%2520model%252C%2520HyCubE%252C%2520which%2520designs%2520a%250Anovel%25203D%2520circular%2520convolutional%2520neural%2520network%2520and%2520the%2520alternate%2520mask%2520stack%250Astrategy%2520to%2520enhance%2520the%2520interaction%2520and%2520extraction%2520of%2520feature%2520information%250Acomprehensively.%2520Furthermore%252C%2520our%2520proposed%2520model%2520achieves%2520a%2520better%2520trade-off%250Abetween%2520effectiveness%2520and%2520efficiency%2520by%2520adaptively%2520adjusting%2520the%25203D%2520circular%250Aconvolutional%2520layer%2520structure%2520to%2520handle%2520different%2520arity%2520knowledge%2520hypergraphs%250Awith%2520fewer%2520parameters.%2520In%2520addition%252C%2520we%2520use%25201-N%2520multilinear%2520scoring%2520based%2520on%2520the%250Aentity%2520mask%2520mechanism%2520to%2520further%2520accelerate%2520the%2520model%2520training%2520efficiency.%250AFinally%252C%2520extensive%2520experimental%2520results%2520on%2520all%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520model%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520with%2520an%250Aaverage%2520improvement%2520of%25207.30%2525-9.53%2525%2520and%2520a%2520maximum%2520improvement%2520of%252033.82%2525%2520across%250Aall%2520metrics.%2520Meanwhile%252C%2520HyCubE%2520is%25204.12x%2520faster%252C%2520GPU%2520memory%2520usage%2520is%252052.19%2525%250Alower%252C%2520and%2520the%2520number%2520of%2520parameters%2520is%2520reduced%2520by%252085.21%2525%2520compared%2520with%2520the%250Aaverage%2520metric%2520of%2520the%2520latest%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyCubE%3A%20Efficient%20Knowledge%20Hypergraph%203D%20Circular%20Convolutional%0A%20%20Embedding&entry.906535625=Zhao%20Li%20and%20Xin%20Wang%20and%20Jun%20Zhao%20and%20Wenbin%20Guo%20and%20Jianxin%20Li&entry.1292438233=%20%20Knowledge%20hypergraph%20embedding%20models%20are%20usually%20computationally%20expensive%0Adue%20to%20the%20inherent%20complex%20semantic%20information.%20However%2C%20existing%20works%0Amainly%20focus%20on%20improving%20the%20effectiveness%20of%20knowledge%20hypergraph%20embedding%2C%0Amaking%20the%20model%20architecture%20more%20complex%20and%20redundant.%20It%20is%20desirable%20and%0Achallenging%20for%20knowledge%20hypergraph%20embedding%20to%20reach%20a%20trade-off%20between%0Amodel%20effectiveness%20and%20efficiency.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Aefficient%20n-ary%20knowledge%20hypergraph%20embedding%20model%2C%20HyCubE%2C%20which%20designs%20a%0Anovel%203D%20circular%20convolutional%20neural%20network%20and%20the%20alternate%20mask%20stack%0Astrategy%20to%20enhance%20the%20interaction%20and%20extraction%20of%20feature%20information%0Acomprehensively.%20Furthermore%2C%20our%20proposed%20model%20achieves%20a%20better%20trade-off%0Abetween%20effectiveness%20and%20efficiency%20by%20adaptively%20adjusting%20the%203D%20circular%0Aconvolutional%20layer%20structure%20to%20handle%20different%20arity%20knowledge%20hypergraphs%0Awith%20fewer%20parameters.%20In%20addition%2C%20we%20use%201-N%20multilinear%20scoring%20based%20on%20the%0Aentity%20mask%20mechanism%20to%20further%20accelerate%20the%20model%20training%20efficiency.%0AFinally%2C%20extensive%20experimental%20results%20on%20all%20datasets%20demonstrate%20that%20our%0Aproposed%20model%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20with%20an%0Aaverage%20improvement%20of%207.30%25-9.53%25%20and%20a%20maximum%20improvement%20of%2033.82%25%20across%0Aall%20metrics.%20Meanwhile%2C%20HyCubE%20is%204.12x%20faster%2C%20GPU%20memory%20usage%20is%2052.19%25%0Alower%2C%20and%20the%20number%20of%20parameters%20is%20reduced%20by%2085.21%25%20compared%20with%20the%0Aaverage%20metric%20of%20the%20latest%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08961v2&entry.124074799=Read"},
{"title": "GIFT: Generative Interpretable Fine-Tuning", "author": "Chinmay Savadikar and Xi Song and Tianfu Wu", "abstract": "  We present Generative Interpretable Fine-Tuning (GIFT) for\nparameter-efficient fine-tuning of pretrained Transformer backbones, which can\nbe formulated as a simple factorized matrix multiplication in the parameter\nspace or equivalently in the activation space, and thus embraces built-in\ninterpretability. For a pretrained layer with weights $\\omega\\in\n\\mathbb{R}^{d_{out}\\times d_{in}}$, our proposed GIFT learns the fine-tuned\nweights $\\hat{\\omega}$ directly from $\\omega$ as $\\hat{\\omega}=\\omega \\cdot\n(\\mathbb{I}+\\phi_{d_{in}\\times r}\\cdot \\psi_{r\\times d_{in}})$ where\n$\\mathbb{I}$ is an identity matrix. $\\Theta=(\\phi, \\psi)$ are the learnable\nparameters of the two linear layers of GIFT with $r$ being a hyper-parameter.\n$\\Theta$ is shared by all the layers selected for fine-tuning, resulting in\nsignificantly fewer trainable parameters compared to Low-Rank Adaptation\n(LoRA). We perform comprehensive evaluations on natural language tasks\n(commonsense reasoning and sequence classification) and computer vision tasks\n(visual fine-grained classification). We obtain the best accuracy and parameter\nefficiency among baselines both on the Commonsense170k reasoning benchmark\nusing LLaMA-1 (7B) and Llama-2 (7B)/-3 (8B) and on the FGVC and VTAB visual\nrecognition benchmarks using ImageNet-21k pretrained Vision Transformer\n(ViT-B/16). Notably, we obtain 5.9% absolute increase in average accuracy with\n53.8 times reduction of parameters on Commonsense170k using Llama-3 (8B)\ncompared to LoRA. We obtain performance comparable to LoRA on the GLUE\nbenchmark but with significantly fewer parameters using RoBERTa-Base/Large. We\nshow the output of the first linear layer (i.e., $\\omega\\cdot \\phi$) is\nsurprisingly interpretable, which can play the role of a token-clustering head\nas a by-product to localize meaningful objects/parts in images for computer\nvision tasks. Our code is publicly available.\n", "link": "http://arxiv.org/abs/2312.00700v2", "date": "2024-06-03", "relevancy": 2.0034, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIFT%3A%20Generative%20Interpretable%20Fine-Tuning&body=Title%3A%20GIFT%3A%20Generative%20Interpretable%20Fine-Tuning%0AAuthor%3A%20Chinmay%20Savadikar%20and%20Xi%20Song%20and%20Tianfu%20Wu%0AAbstract%3A%20%20%20We%20present%20Generative%20Interpretable%20Fine-Tuning%20%28GIFT%29%20for%0Aparameter-efficient%20fine-tuning%20of%20pretrained%20Transformer%20backbones%2C%20which%20can%0Abe%20formulated%20as%20a%20simple%20factorized%20matrix%20multiplication%20in%20the%20parameter%0Aspace%20or%20equivalently%20in%20the%20activation%20space%2C%20and%20thus%20embraces%20built-in%0Ainterpretability.%20For%20a%20pretrained%20layer%20with%20weights%20%24%5Comega%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bd_%7Bout%7D%5Ctimes%20d_%7Bin%7D%7D%24%2C%20our%20proposed%20GIFT%20learns%20the%20fine-tuned%0Aweights%20%24%5Chat%7B%5Comega%7D%24%20directly%20from%20%24%5Comega%24%20as%20%24%5Chat%7B%5Comega%7D%3D%5Comega%20%5Ccdot%0A%28%5Cmathbb%7BI%7D%2B%5Cphi_%7Bd_%7Bin%7D%5Ctimes%20r%7D%5Ccdot%20%5Cpsi_%7Br%5Ctimes%20d_%7Bin%7D%7D%29%24%20where%0A%24%5Cmathbb%7BI%7D%24%20is%20an%20identity%20matrix.%20%24%5CTheta%3D%28%5Cphi%2C%20%5Cpsi%29%24%20are%20the%20learnable%0Aparameters%20of%20the%20two%20linear%20layers%20of%20GIFT%20with%20%24r%24%20being%20a%20hyper-parameter.%0A%24%5CTheta%24%20is%20shared%20by%20all%20the%20layers%20selected%20for%20fine-tuning%2C%20resulting%20in%0Asignificantly%20fewer%20trainable%20parameters%20compared%20to%20Low-Rank%20Adaptation%0A%28LoRA%29.%20We%20perform%20comprehensive%20evaluations%20on%20natural%20language%20tasks%0A%28commonsense%20reasoning%20and%20sequence%20classification%29%20and%20computer%20vision%20tasks%0A%28visual%20fine-grained%20classification%29.%20We%20obtain%20the%20best%20accuracy%20and%20parameter%0Aefficiency%20among%20baselines%20both%20on%20the%20Commonsense170k%20reasoning%20benchmark%0Ausing%20LLaMA-1%20%287B%29%20and%20Llama-2%20%287B%29/-3%20%288B%29%20and%20on%20the%20FGVC%20and%20VTAB%20visual%0Arecognition%20benchmarks%20using%20ImageNet-21k%20pretrained%20Vision%20Transformer%0A%28ViT-B/16%29.%20Notably%2C%20we%20obtain%205.9%25%20absolute%20increase%20in%20average%20accuracy%20with%0A53.8%20times%20reduction%20of%20parameters%20on%20Commonsense170k%20using%20Llama-3%20%288B%29%0Acompared%20to%20LoRA.%20We%20obtain%20performance%20comparable%20to%20LoRA%20on%20the%20GLUE%0Abenchmark%20but%20with%20significantly%20fewer%20parameters%20using%20RoBERTa-Base/Large.%20We%0Ashow%20the%20output%20of%20the%20first%20linear%20layer%20%28i.e.%2C%20%24%5Comega%5Ccdot%20%5Cphi%24%29%20is%0Asurprisingly%20interpretable%2C%20which%20can%20play%20the%20role%20of%20a%20token-clustering%20head%0Aas%20a%20by-product%20to%20localize%20meaningful%20objects/parts%20in%20images%20for%20computer%0Avision%20tasks.%20Our%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIFT%253A%2520Generative%2520Interpretable%2520Fine-Tuning%26entry.906535625%3DChinmay%2520Savadikar%2520and%2520Xi%2520Song%2520and%2520Tianfu%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520Generative%2520Interpretable%2520Fine-Tuning%2520%2528GIFT%2529%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520pretrained%2520Transformer%2520backbones%252C%2520which%2520can%250Abe%2520formulated%2520as%2520a%2520simple%2520factorized%2520matrix%2520multiplication%2520in%2520the%2520parameter%250Aspace%2520or%2520equivalently%2520in%2520the%2520activation%2520space%252C%2520and%2520thus%2520embraces%2520built-in%250Ainterpretability.%2520For%2520a%2520pretrained%2520layer%2520with%2520weights%2520%2524%255Comega%255Cin%250A%255Cmathbb%257BR%257D%255E%257Bd_%257Bout%257D%255Ctimes%2520d_%257Bin%257D%257D%2524%252C%2520our%2520proposed%2520GIFT%2520learns%2520the%2520fine-tuned%250Aweights%2520%2524%255Chat%257B%255Comega%257D%2524%2520directly%2520from%2520%2524%255Comega%2524%2520as%2520%2524%255Chat%257B%255Comega%257D%253D%255Comega%2520%255Ccdot%250A%2528%255Cmathbb%257BI%257D%252B%255Cphi_%257Bd_%257Bin%257D%255Ctimes%2520r%257D%255Ccdot%2520%255Cpsi_%257Br%255Ctimes%2520d_%257Bin%257D%257D%2529%2524%2520where%250A%2524%255Cmathbb%257BI%257D%2524%2520is%2520an%2520identity%2520matrix.%2520%2524%255CTheta%253D%2528%255Cphi%252C%2520%255Cpsi%2529%2524%2520are%2520the%2520learnable%250Aparameters%2520of%2520the%2520two%2520linear%2520layers%2520of%2520GIFT%2520with%2520%2524r%2524%2520being%2520a%2520hyper-parameter.%250A%2524%255CTheta%2524%2520is%2520shared%2520by%2520all%2520the%2520layers%2520selected%2520for%2520fine-tuning%252C%2520resulting%2520in%250Asignificantly%2520fewer%2520trainable%2520parameters%2520compared%2520to%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529.%2520We%2520perform%2520comprehensive%2520evaluations%2520on%2520natural%2520language%2520tasks%250A%2528commonsense%2520reasoning%2520and%2520sequence%2520classification%2529%2520and%2520computer%2520vision%2520tasks%250A%2528visual%2520fine-grained%2520classification%2529.%2520We%2520obtain%2520the%2520best%2520accuracy%2520and%2520parameter%250Aefficiency%2520among%2520baselines%2520both%2520on%2520the%2520Commonsense170k%2520reasoning%2520benchmark%250Ausing%2520LLaMA-1%2520%25287B%2529%2520and%2520Llama-2%2520%25287B%2529/-3%2520%25288B%2529%2520and%2520on%2520the%2520FGVC%2520and%2520VTAB%2520visual%250Arecognition%2520benchmarks%2520using%2520ImageNet-21k%2520pretrained%2520Vision%2520Transformer%250A%2528ViT-B/16%2529.%2520Notably%252C%2520we%2520obtain%25205.9%2525%2520absolute%2520increase%2520in%2520average%2520accuracy%2520with%250A53.8%2520times%2520reduction%2520of%2520parameters%2520on%2520Commonsense170k%2520using%2520Llama-3%2520%25288B%2529%250Acompared%2520to%2520LoRA.%2520We%2520obtain%2520performance%2520comparable%2520to%2520LoRA%2520on%2520the%2520GLUE%250Abenchmark%2520but%2520with%2520significantly%2520fewer%2520parameters%2520using%2520RoBERTa-Base/Large.%2520We%250Ashow%2520the%2520output%2520of%2520the%2520first%2520linear%2520layer%2520%2528i.e.%252C%2520%2524%255Comega%255Ccdot%2520%255Cphi%2524%2529%2520is%250Asurprisingly%2520interpretable%252C%2520which%2520can%2520play%2520the%2520role%2520of%2520a%2520token-clustering%2520head%250Aas%2520a%2520by-product%2520to%2520localize%2520meaningful%2520objects/parts%2520in%2520images%2520for%2520computer%250Avision%2520tasks.%2520Our%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIFT%3A%20Generative%20Interpretable%20Fine-Tuning&entry.906535625=Chinmay%20Savadikar%20and%20Xi%20Song%20and%20Tianfu%20Wu&entry.1292438233=%20%20We%20present%20Generative%20Interpretable%20Fine-Tuning%20%28GIFT%29%20for%0Aparameter-efficient%20fine-tuning%20of%20pretrained%20Transformer%20backbones%2C%20which%20can%0Abe%20formulated%20as%20a%20simple%20factorized%20matrix%20multiplication%20in%20the%20parameter%0Aspace%20or%20equivalently%20in%20the%20activation%20space%2C%20and%20thus%20embraces%20built-in%0Ainterpretability.%20For%20a%20pretrained%20layer%20with%20weights%20%24%5Comega%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bd_%7Bout%7D%5Ctimes%20d_%7Bin%7D%7D%24%2C%20our%20proposed%20GIFT%20learns%20the%20fine-tuned%0Aweights%20%24%5Chat%7B%5Comega%7D%24%20directly%20from%20%24%5Comega%24%20as%20%24%5Chat%7B%5Comega%7D%3D%5Comega%20%5Ccdot%0A%28%5Cmathbb%7BI%7D%2B%5Cphi_%7Bd_%7Bin%7D%5Ctimes%20r%7D%5Ccdot%20%5Cpsi_%7Br%5Ctimes%20d_%7Bin%7D%7D%29%24%20where%0A%24%5Cmathbb%7BI%7D%24%20is%20an%20identity%20matrix.%20%24%5CTheta%3D%28%5Cphi%2C%20%5Cpsi%29%24%20are%20the%20learnable%0Aparameters%20of%20the%20two%20linear%20layers%20of%20GIFT%20with%20%24r%24%20being%20a%20hyper-parameter.%0A%24%5CTheta%24%20is%20shared%20by%20all%20the%20layers%20selected%20for%20fine-tuning%2C%20resulting%20in%0Asignificantly%20fewer%20trainable%20parameters%20compared%20to%20Low-Rank%20Adaptation%0A%28LoRA%29.%20We%20perform%20comprehensive%20evaluations%20on%20natural%20language%20tasks%0A%28commonsense%20reasoning%20and%20sequence%20classification%29%20and%20computer%20vision%20tasks%0A%28visual%20fine-grained%20classification%29.%20We%20obtain%20the%20best%20accuracy%20and%20parameter%0Aefficiency%20among%20baselines%20both%20on%20the%20Commonsense170k%20reasoning%20benchmark%0Ausing%20LLaMA-1%20%287B%29%20and%20Llama-2%20%287B%29/-3%20%288B%29%20and%20on%20the%20FGVC%20and%20VTAB%20visual%0Arecognition%20benchmarks%20using%20ImageNet-21k%20pretrained%20Vision%20Transformer%0A%28ViT-B/16%29.%20Notably%2C%20we%20obtain%205.9%25%20absolute%20increase%20in%20average%20accuracy%20with%0A53.8%20times%20reduction%20of%20parameters%20on%20Commonsense170k%20using%20Llama-3%20%288B%29%0Acompared%20to%20LoRA.%20We%20obtain%20performance%20comparable%20to%20LoRA%20on%20the%20GLUE%0Abenchmark%20but%20with%20significantly%20fewer%20parameters%20using%20RoBERTa-Base/Large.%20We%0Ashow%20the%20output%20of%20the%20first%20linear%20layer%20%28i.e.%2C%20%24%5Comega%5Ccdot%20%5Cphi%24%29%20is%0Asurprisingly%20interpretable%2C%20which%20can%20play%20the%20role%20of%20a%20token-clustering%20head%0Aas%20a%20by-product%20to%20localize%20meaningful%20objects/parts%20in%20images%20for%20computer%0Avision%20tasks.%20Our%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00700v2&entry.124074799=Read"},
{"title": "CF-OPT: Counterfactual Explanations for Structured Prediction", "author": "Germain Vivier-Ardisson and Alexandre Forel and Axel Parmentier and Thibaut Vidal", "abstract": "  Optimization layers in deep neural networks have enjoyed a growing popularity\nin structured learning, improving the state of the art on a variety of\napplications. Yet, these pipelines lack interpretability since they are made of\ntwo opaque layers: a highly non-linear prediction model, such as a deep neural\nnetwork, and an optimization layer, which is typically a complex black-box\nsolver. Our goal is to improve the transparency of such methods by providing\ncounterfactual explanations. We build upon variational autoencoders a\nprincipled way of obtaining counterfactuals: working in the latent space leads\nto a natural notion of plausibility of explanations. We finally introduce a\nvariant of the classic loss for VAE training that improves their performance in\nour specific structured context. These provide the foundations of CF-OPT, a\nfirst-order optimization algorithm that can find counterfactual explanations\nfor a broad class of structured learning architectures. Our numerical results\nshow that both close and plausible explanations can be obtained for problems\nfrom the recent literature.\n", "link": "http://arxiv.org/abs/2405.18293v2", "date": "2024-06-03", "relevancy": 1.9964, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CF-OPT%3A%20Counterfactual%20Explanations%20for%20Structured%20Prediction&body=Title%3A%20CF-OPT%3A%20Counterfactual%20Explanations%20for%20Structured%20Prediction%0AAuthor%3A%20Germain%20Vivier-Ardisson%20and%20Alexandre%20Forel%20and%20Axel%20Parmentier%20and%20Thibaut%20Vidal%0AAbstract%3A%20%20%20Optimization%20layers%20in%20deep%20neural%20networks%20have%20enjoyed%20a%20growing%20popularity%0Ain%20structured%20learning%2C%20improving%20the%20state%20of%20the%20art%20on%20a%20variety%20of%0Aapplications.%20Yet%2C%20these%20pipelines%20lack%20interpretability%20since%20they%20are%20made%20of%0Atwo%20opaque%20layers%3A%20a%20highly%20non-linear%20prediction%20model%2C%20such%20as%20a%20deep%20neural%0Anetwork%2C%20and%20an%20optimization%20layer%2C%20which%20is%20typically%20a%20complex%20black-box%0Asolver.%20Our%20goal%20is%20to%20improve%20the%20transparency%20of%20such%20methods%20by%20providing%0Acounterfactual%20explanations.%20We%20build%20upon%20variational%20autoencoders%20a%0Aprincipled%20way%20of%20obtaining%20counterfactuals%3A%20working%20in%20the%20latent%20space%20leads%0Ato%20a%20natural%20notion%20of%20plausibility%20of%20explanations.%20We%20finally%20introduce%20a%0Avariant%20of%20the%20classic%20loss%20for%20VAE%20training%20that%20improves%20their%20performance%20in%0Aour%20specific%20structured%20context.%20These%20provide%20the%20foundations%20of%20CF-OPT%2C%20a%0Afirst-order%20optimization%20algorithm%20that%20can%20find%20counterfactual%20explanations%0Afor%20a%20broad%20class%20of%20structured%20learning%20architectures.%20Our%20numerical%20results%0Ashow%20that%20both%20close%20and%20plausible%20explanations%20can%20be%20obtained%20for%20problems%0Afrom%20the%20recent%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCF-OPT%253A%2520Counterfactual%2520Explanations%2520for%2520Structured%2520Prediction%26entry.906535625%3DGermain%2520Vivier-Ardisson%2520and%2520Alexandre%2520Forel%2520and%2520Axel%2520Parmentier%2520and%2520Thibaut%2520Vidal%26entry.1292438233%3D%2520%2520Optimization%2520layers%2520in%2520deep%2520neural%2520networks%2520have%2520enjoyed%2520a%2520growing%2520popularity%250Ain%2520structured%2520learning%252C%2520improving%2520the%2520state%2520of%2520the%2520art%2520on%2520a%2520variety%2520of%250Aapplications.%2520Yet%252C%2520these%2520pipelines%2520lack%2520interpretability%2520since%2520they%2520are%2520made%2520of%250Atwo%2520opaque%2520layers%253A%2520a%2520highly%2520non-linear%2520prediction%2520model%252C%2520such%2520as%2520a%2520deep%2520neural%250Anetwork%252C%2520and%2520an%2520optimization%2520layer%252C%2520which%2520is%2520typically%2520a%2520complex%2520black-box%250Asolver.%2520Our%2520goal%2520is%2520to%2520improve%2520the%2520transparency%2520of%2520such%2520methods%2520by%2520providing%250Acounterfactual%2520explanations.%2520We%2520build%2520upon%2520variational%2520autoencoders%2520a%250Aprincipled%2520way%2520of%2520obtaining%2520counterfactuals%253A%2520working%2520in%2520the%2520latent%2520space%2520leads%250Ato%2520a%2520natural%2520notion%2520of%2520plausibility%2520of%2520explanations.%2520We%2520finally%2520introduce%2520a%250Avariant%2520of%2520the%2520classic%2520loss%2520for%2520VAE%2520training%2520that%2520improves%2520their%2520performance%2520in%250Aour%2520specific%2520structured%2520context.%2520These%2520provide%2520the%2520foundations%2520of%2520CF-OPT%252C%2520a%250Afirst-order%2520optimization%2520algorithm%2520that%2520can%2520find%2520counterfactual%2520explanations%250Afor%2520a%2520broad%2520class%2520of%2520structured%2520learning%2520architectures.%2520Our%2520numerical%2520results%250Ashow%2520that%2520both%2520close%2520and%2520plausible%2520explanations%2520can%2520be%2520obtained%2520for%2520problems%250Afrom%2520the%2520recent%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CF-OPT%3A%20Counterfactual%20Explanations%20for%20Structured%20Prediction&entry.906535625=Germain%20Vivier-Ardisson%20and%20Alexandre%20Forel%20and%20Axel%20Parmentier%20and%20Thibaut%20Vidal&entry.1292438233=%20%20Optimization%20layers%20in%20deep%20neural%20networks%20have%20enjoyed%20a%20growing%20popularity%0Ain%20structured%20learning%2C%20improving%20the%20state%20of%20the%20art%20on%20a%20variety%20of%0Aapplications.%20Yet%2C%20these%20pipelines%20lack%20interpretability%20since%20they%20are%20made%20of%0Atwo%20opaque%20layers%3A%20a%20highly%20non-linear%20prediction%20model%2C%20such%20as%20a%20deep%20neural%0Anetwork%2C%20and%20an%20optimization%20layer%2C%20which%20is%20typically%20a%20complex%20black-box%0Asolver.%20Our%20goal%20is%20to%20improve%20the%20transparency%20of%20such%20methods%20by%20providing%0Acounterfactual%20explanations.%20We%20build%20upon%20variational%20autoencoders%20a%0Aprincipled%20way%20of%20obtaining%20counterfactuals%3A%20working%20in%20the%20latent%20space%20leads%0Ato%20a%20natural%20notion%20of%20plausibility%20of%20explanations.%20We%20finally%20introduce%20a%0Avariant%20of%20the%20classic%20loss%20for%20VAE%20training%20that%20improves%20their%20performance%20in%0Aour%20specific%20structured%20context.%20These%20provide%20the%20foundations%20of%20CF-OPT%2C%20a%0Afirst-order%20optimization%20algorithm%20that%20can%20find%20counterfactual%20explanations%0Afor%20a%20broad%20class%20of%20structured%20learning%20architectures.%20Our%20numerical%20results%0Ashow%20that%20both%20close%20and%20plausible%20explanations%20can%20be%20obtained%20for%20problems%0Afrom%20the%20recent%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18293v2&entry.124074799=Read"},
{"title": "The Topology and Geometry of Neural Representations", "author": "Baihan Lin and Nikolaus Kriegeskorte", "abstract": "  A central question for neuroscience is how to characterize brain\nrepresentations of perceptual and cognitive content. An ideal characterization\nshould distinguish different functional regions with robustness to noise and\nidiosyncrasies of individual brains that do not correspond to computational\ndifferences. Previous studies have characterized brain representations by their\nrepresentational geometry, which is defined by the representational\ndissimilarity matrix (RDM), a summary statistic that abstracts from the roles\nof individual neurons (or responses channels) and characterizes the\ndiscriminability of stimuli. Here we explore a further step of abstraction:\nfrom the geometry to the topology of brain representations. We propose\ntopological representational similarity analysis (tRSA), an extension of\nrepresentational similarity analysis (RSA) that uses a family of\ngeo-topological summary statistics that generalizes the RDM to characterize the\ntopology while de-emphasizing the geometry. We evaluate this new family of\nstatistics in terms of the sensitivity and specificity for model selection\nusing both simulations and fMRI data. In the simulations, the ground truth is a\ndata-generating layer representation in a neural network model and the models\nare the same and other layers in different model instances (trained from\ndifferent random seeds). In fMRI, the ground truth is a visual area and the\nmodels are the same and other areas measured in different subjects. Results\nshow that topology-sensitive characterizations of population codes are robust\nto noise and interindividual variability and maintain excellent sensitivity to\nthe unique representational signatures of different neural network layers and\nbrain regions. These methods enable researchers to calibrate comparisons among\nrepresentations in brains and models to be sensitive to the geometry, the\ntopology, or a combination of both.\n", "link": "http://arxiv.org/abs/2309.11028v3", "date": "2024-06-03", "relevancy": 1.985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5038}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4925}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Topology%20and%20Geometry%20of%20Neural%20Representations&body=Title%3A%20The%20Topology%20and%20Geometry%20of%20Neural%20Representations%0AAuthor%3A%20Baihan%20Lin%20and%20Nikolaus%20Kriegeskorte%0AAbstract%3A%20%20%20A%20central%20question%20for%20neuroscience%20is%20how%20to%20characterize%20brain%0Arepresentations%20of%20perceptual%20and%20cognitive%20content.%20An%20ideal%20characterization%0Ashould%20distinguish%20different%20functional%20regions%20with%20robustness%20to%20noise%20and%0Aidiosyncrasies%20of%20individual%20brains%20that%20do%20not%20correspond%20to%20computational%0Adifferences.%20Previous%20studies%20have%20characterized%20brain%20representations%20by%20their%0Arepresentational%20geometry%2C%20which%20is%20defined%20by%20the%20representational%0Adissimilarity%20matrix%20%28RDM%29%2C%20a%20summary%20statistic%20that%20abstracts%20from%20the%20roles%0Aof%20individual%20neurons%20%28or%20responses%20channels%29%20and%20characterizes%20the%0Adiscriminability%20of%20stimuli.%20Here%20we%20explore%20a%20further%20step%20of%20abstraction%3A%0Afrom%20the%20geometry%20to%20the%20topology%20of%20brain%20representations.%20We%20propose%0Atopological%20representational%20similarity%20analysis%20%28tRSA%29%2C%20an%20extension%20of%0Arepresentational%20similarity%20analysis%20%28RSA%29%20that%20uses%20a%20family%20of%0Ageo-topological%20summary%20statistics%20that%20generalizes%20the%20RDM%20to%20characterize%20the%0Atopology%20while%20de-emphasizing%20the%20geometry.%20We%20evaluate%20this%20new%20family%20of%0Astatistics%20in%20terms%20of%20the%20sensitivity%20and%20specificity%20for%20model%20selection%0Ausing%20both%20simulations%20and%20fMRI%20data.%20In%20the%20simulations%2C%20the%20ground%20truth%20is%20a%0Adata-generating%20layer%20representation%20in%20a%20neural%20network%20model%20and%20the%20models%0Aare%20the%20same%20and%20other%20layers%20in%20different%20model%20instances%20%28trained%20from%0Adifferent%20random%20seeds%29.%20In%20fMRI%2C%20the%20ground%20truth%20is%20a%20visual%20area%20and%20the%0Amodels%20are%20the%20same%20and%20other%20areas%20measured%20in%20different%20subjects.%20Results%0Ashow%20that%20topology-sensitive%20characterizations%20of%20population%20codes%20are%20robust%0Ato%20noise%20and%20interindividual%20variability%20and%20maintain%20excellent%20sensitivity%20to%0Athe%20unique%20representational%20signatures%20of%20different%20neural%20network%20layers%20and%0Abrain%20regions.%20These%20methods%20enable%20researchers%20to%20calibrate%20comparisons%20among%0Arepresentations%20in%20brains%20and%20models%20to%20be%20sensitive%20to%20the%20geometry%2C%20the%0Atopology%2C%20or%20a%20combination%20of%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11028v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Topology%2520and%2520Geometry%2520of%2520Neural%2520Representations%26entry.906535625%3DBaihan%2520Lin%2520and%2520Nikolaus%2520Kriegeskorte%26entry.1292438233%3D%2520%2520A%2520central%2520question%2520for%2520neuroscience%2520is%2520how%2520to%2520characterize%2520brain%250Arepresentations%2520of%2520perceptual%2520and%2520cognitive%2520content.%2520An%2520ideal%2520characterization%250Ashould%2520distinguish%2520different%2520functional%2520regions%2520with%2520robustness%2520to%2520noise%2520and%250Aidiosyncrasies%2520of%2520individual%2520brains%2520that%2520do%2520not%2520correspond%2520to%2520computational%250Adifferences.%2520Previous%2520studies%2520have%2520characterized%2520brain%2520representations%2520by%2520their%250Arepresentational%2520geometry%252C%2520which%2520is%2520defined%2520by%2520the%2520representational%250Adissimilarity%2520matrix%2520%2528RDM%2529%252C%2520a%2520summary%2520statistic%2520that%2520abstracts%2520from%2520the%2520roles%250Aof%2520individual%2520neurons%2520%2528or%2520responses%2520channels%2529%2520and%2520characterizes%2520the%250Adiscriminability%2520of%2520stimuli.%2520Here%2520we%2520explore%2520a%2520further%2520step%2520of%2520abstraction%253A%250Afrom%2520the%2520geometry%2520to%2520the%2520topology%2520of%2520brain%2520representations.%2520We%2520propose%250Atopological%2520representational%2520similarity%2520analysis%2520%2528tRSA%2529%252C%2520an%2520extension%2520of%250Arepresentational%2520similarity%2520analysis%2520%2528RSA%2529%2520that%2520uses%2520a%2520family%2520of%250Ageo-topological%2520summary%2520statistics%2520that%2520generalizes%2520the%2520RDM%2520to%2520characterize%2520the%250Atopology%2520while%2520de-emphasizing%2520the%2520geometry.%2520We%2520evaluate%2520this%2520new%2520family%2520of%250Astatistics%2520in%2520terms%2520of%2520the%2520sensitivity%2520and%2520specificity%2520for%2520model%2520selection%250Ausing%2520both%2520simulations%2520and%2520fMRI%2520data.%2520In%2520the%2520simulations%252C%2520the%2520ground%2520truth%2520is%2520a%250Adata-generating%2520layer%2520representation%2520in%2520a%2520neural%2520network%2520model%2520and%2520the%2520models%250Aare%2520the%2520same%2520and%2520other%2520layers%2520in%2520different%2520model%2520instances%2520%2528trained%2520from%250Adifferent%2520random%2520seeds%2529.%2520In%2520fMRI%252C%2520the%2520ground%2520truth%2520is%2520a%2520visual%2520area%2520and%2520the%250Amodels%2520are%2520the%2520same%2520and%2520other%2520areas%2520measured%2520in%2520different%2520subjects.%2520Results%250Ashow%2520that%2520topology-sensitive%2520characterizations%2520of%2520population%2520codes%2520are%2520robust%250Ato%2520noise%2520and%2520interindividual%2520variability%2520and%2520maintain%2520excellent%2520sensitivity%2520to%250Athe%2520unique%2520representational%2520signatures%2520of%2520different%2520neural%2520network%2520layers%2520and%250Abrain%2520regions.%2520These%2520methods%2520enable%2520researchers%2520to%2520calibrate%2520comparisons%2520among%250Arepresentations%2520in%2520brains%2520and%2520models%2520to%2520be%2520sensitive%2520to%2520the%2520geometry%252C%2520the%250Atopology%252C%2520or%2520a%2520combination%2520of%2520both.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11028v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Topology%20and%20Geometry%20of%20Neural%20Representations&entry.906535625=Baihan%20Lin%20and%20Nikolaus%20Kriegeskorte&entry.1292438233=%20%20A%20central%20question%20for%20neuroscience%20is%20how%20to%20characterize%20brain%0Arepresentations%20of%20perceptual%20and%20cognitive%20content.%20An%20ideal%20characterization%0Ashould%20distinguish%20different%20functional%20regions%20with%20robustness%20to%20noise%20and%0Aidiosyncrasies%20of%20individual%20brains%20that%20do%20not%20correspond%20to%20computational%0Adifferences.%20Previous%20studies%20have%20characterized%20brain%20representations%20by%20their%0Arepresentational%20geometry%2C%20which%20is%20defined%20by%20the%20representational%0Adissimilarity%20matrix%20%28RDM%29%2C%20a%20summary%20statistic%20that%20abstracts%20from%20the%20roles%0Aof%20individual%20neurons%20%28or%20responses%20channels%29%20and%20characterizes%20the%0Adiscriminability%20of%20stimuli.%20Here%20we%20explore%20a%20further%20step%20of%20abstraction%3A%0Afrom%20the%20geometry%20to%20the%20topology%20of%20brain%20representations.%20We%20propose%0Atopological%20representational%20similarity%20analysis%20%28tRSA%29%2C%20an%20extension%20of%0Arepresentational%20similarity%20analysis%20%28RSA%29%20that%20uses%20a%20family%20of%0Ageo-topological%20summary%20statistics%20that%20generalizes%20the%20RDM%20to%20characterize%20the%0Atopology%20while%20de-emphasizing%20the%20geometry.%20We%20evaluate%20this%20new%20family%20of%0Astatistics%20in%20terms%20of%20the%20sensitivity%20and%20specificity%20for%20model%20selection%0Ausing%20both%20simulations%20and%20fMRI%20data.%20In%20the%20simulations%2C%20the%20ground%20truth%20is%20a%0Adata-generating%20layer%20representation%20in%20a%20neural%20network%20model%20and%20the%20models%0Aare%20the%20same%20and%20other%20layers%20in%20different%20model%20instances%20%28trained%20from%0Adifferent%20random%20seeds%29.%20In%20fMRI%2C%20the%20ground%20truth%20is%20a%20visual%20area%20and%20the%0Amodels%20are%20the%20same%20and%20other%20areas%20measured%20in%20different%20subjects.%20Results%0Ashow%20that%20topology-sensitive%20characterizations%20of%20population%20codes%20are%20robust%0Ato%20noise%20and%20interindividual%20variability%20and%20maintain%20excellent%20sensitivity%20to%0Athe%20unique%20representational%20signatures%20of%20different%20neural%20network%20layers%20and%0Abrain%20regions.%20These%20methods%20enable%20researchers%20to%20calibrate%20comparisons%20among%0Arepresentations%20in%20brains%20and%20models%20to%20be%20sensitive%20to%20the%20geometry%2C%20the%0Atopology%2C%20or%20a%20combination%20of%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11028v3&entry.124074799=Read"},
{"title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework", "author": "Jian Hu and Xibin Wu and Weixun Wang and  Xianyu and Dehao Zhang and Yu Cao", "abstract": "  As large language models (LLMs) continue to grow by scaling laws,\nreinforcement learning from human feedback (RLHF) has gained significant\nattention due to its outstanding performance. However, unlike pretraining or\nfine-tuning a single model, scaling reinforcement learning from human feedback\n(RLHF) for training large language models poses coordination challenges across\nfour models. We present OpenRLHF, an open-source framework enabling efficient\nRLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the\nsame GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters\nusing Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and\ndiverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF\nprovides an out-of-the-box solution with optimized algorithms and launch\nscripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,\nrejection sampling, and other alignment techniques. Empowering state-of-the-art\nLLM development, OpenRLHF's code is available at\nhttps://github.com/OpenLLMAI/OpenRLHF.\n", "link": "http://arxiv.org/abs/2405.11143v2", "date": "2024-06-03", "relevancy": 1.9825, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenRLHF%3A%20An%20Easy-to-use%2C%20Scalable%20and%20High-performance%20RLHF%20Framework&body=Title%3A%20OpenRLHF%3A%20An%20Easy-to-use%2C%20Scalable%20and%20High-performance%20RLHF%20Framework%0AAuthor%3A%20Jian%20Hu%20and%20Xibin%20Wu%20and%20Weixun%20Wang%20and%20%20Xianyu%20and%20Dehao%20Zhang%20and%20Yu%20Cao%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20grow%20by%20scaling%20laws%2C%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20gained%20significant%0Aattention%20due%20to%20its%20outstanding%20performance.%20However%2C%20unlike%20pretraining%20or%0Afine-tuning%20a%20single%20model%2C%20scaling%20reinforcement%20learning%20from%20human%20feedback%0A%28RLHF%29%20for%20training%20large%20language%20models%20poses%20coordination%20challenges%20across%0Afour%20models.%20We%20present%20OpenRLHF%2C%20an%20open-source%20framework%20enabling%20efficient%0ARLHF%20scaling.%20Unlike%20existing%20RLHF%20frameworks%20that%20co-locate%20four%20models%20on%20the%0Asame%20GPUs%2C%20OpenRLHF%20re-designs%20scheduling%20for%20the%20models%20beyond%2070B%20parameters%0Ausing%20Ray%2C%20vLLM%2C%20and%20DeepSpeed%2C%20leveraging%20improved%20resource%20utilization%20and%0Adiverse%20training%20approaches.%20Integrating%20seamlessly%20with%20Hugging%20Face%2C%20OpenRLHF%0Aprovides%20an%20out-of-the-box%20solution%20with%20optimized%20algorithms%20and%20launch%0Ascripts%2C%20which%20ensures%20user-friendliness.%20OpenRLHF%20implements%20RLHF%2C%20DPO%2C%0Arejection%20sampling%2C%20and%20other%20alignment%20techniques.%20Empowering%20state-of-the-art%0ALLM%20development%2C%20OpenRLHF%27s%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenLLMAI/OpenRLHF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenRLHF%253A%2520An%2520Easy-to-use%252C%2520Scalable%2520and%2520High-performance%2520RLHF%2520Framework%26entry.906535625%3DJian%2520Hu%2520and%2520Xibin%2520Wu%2520and%2520Weixun%2520Wang%2520and%2520%2520Xianyu%2520and%2520Dehao%2520Zhang%2520and%2520Yu%2520Cao%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520grow%2520by%2520scaling%2520laws%252C%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520has%2520gained%2520significant%250Aattention%2520due%2520to%2520its%2520outstanding%2520performance.%2520However%252C%2520unlike%2520pretraining%2520or%250Afine-tuning%2520a%2520single%2520model%252C%2520scaling%2520reinforcement%2520learning%2520from%2520human%2520feedback%250A%2528RLHF%2529%2520for%2520training%2520large%2520language%2520models%2520poses%2520coordination%2520challenges%2520across%250Afour%2520models.%2520We%2520present%2520OpenRLHF%252C%2520an%2520open-source%2520framework%2520enabling%2520efficient%250ARLHF%2520scaling.%2520Unlike%2520existing%2520RLHF%2520frameworks%2520that%2520co-locate%2520four%2520models%2520on%2520the%250Asame%2520GPUs%252C%2520OpenRLHF%2520re-designs%2520scheduling%2520for%2520the%2520models%2520beyond%252070B%2520parameters%250Ausing%2520Ray%252C%2520vLLM%252C%2520and%2520DeepSpeed%252C%2520leveraging%2520improved%2520resource%2520utilization%2520and%250Adiverse%2520training%2520approaches.%2520Integrating%2520seamlessly%2520with%2520Hugging%2520Face%252C%2520OpenRLHF%250Aprovides%2520an%2520out-of-the-box%2520solution%2520with%2520optimized%2520algorithms%2520and%2520launch%250Ascripts%252C%2520which%2520ensures%2520user-friendliness.%2520OpenRLHF%2520implements%2520RLHF%252C%2520DPO%252C%250Arejection%2520sampling%252C%2520and%2520other%2520alignment%2520techniques.%2520Empowering%2520state-of-the-art%250ALLM%2520development%252C%2520OpenRLHF%2527s%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/OpenLLMAI/OpenRLHF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenRLHF%3A%20An%20Easy-to-use%2C%20Scalable%20and%20High-performance%20RLHF%20Framework&entry.906535625=Jian%20Hu%20and%20Xibin%20Wu%20and%20Weixun%20Wang%20and%20%20Xianyu%20and%20Dehao%20Zhang%20and%20Yu%20Cao&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20grow%20by%20scaling%20laws%2C%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20gained%20significant%0Aattention%20due%20to%20its%20outstanding%20performance.%20However%2C%20unlike%20pretraining%20or%0Afine-tuning%20a%20single%20model%2C%20scaling%20reinforcement%20learning%20from%20human%20feedback%0A%28RLHF%29%20for%20training%20large%20language%20models%20poses%20coordination%20challenges%20across%0Afour%20models.%20We%20present%20OpenRLHF%2C%20an%20open-source%20framework%20enabling%20efficient%0ARLHF%20scaling.%20Unlike%20existing%20RLHF%20frameworks%20that%20co-locate%20four%20models%20on%20the%0Asame%20GPUs%2C%20OpenRLHF%20re-designs%20scheduling%20for%20the%20models%20beyond%2070B%20parameters%0Ausing%20Ray%2C%20vLLM%2C%20and%20DeepSpeed%2C%20leveraging%20improved%20resource%20utilization%20and%0Adiverse%20training%20approaches.%20Integrating%20seamlessly%20with%20Hugging%20Face%2C%20OpenRLHF%0Aprovides%20an%20out-of-the-box%20solution%20with%20optimized%20algorithms%20and%20launch%0Ascripts%2C%20which%20ensures%20user-friendliness.%20OpenRLHF%20implements%20RLHF%2C%20DPO%2C%0Arejection%20sampling%2C%20and%20other%20alignment%20techniques.%20Empowering%20state-of-the-art%0ALLM%20development%2C%20OpenRLHF%27s%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenLLMAI/OpenRLHF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11143v2&entry.124074799=Read"},
{"title": "Parallel and Proximal Constrained Linear-Quadratic Methods for Real-Time\n  Nonlinear MPC", "author": "Wilson Jallet and Ewen Dantec and Etienne Arlaud and Justin Carpentier and Nicolas Mansard", "abstract": "  Recent strides in nonlinear model predictive control (NMPC) underscore a\ndependence on numerical advancements to efficiently and accurately solve\nlarge-scale problems. Given the substantial number of variables characterizing\ntypical whole-body optimal control (OC) problems - often numbering in the\nthousands - exploiting the sparse structure of the numerical problem becomes\ncrucial to meet computational demands, typically in the range of a few\nmilliseconds. Addressing the linear-quadratic regulator (LQR) problem is a\nfundamental building block for computing Newton or Sequential Quadratic\nProgramming (SQP) steps in direct optimal control methods. This paper\nconcentrates on equality-constrained problems featuring implicit system\ndynamics and dual regularization, a characteristic of advanced interiorpoint or\naugmented Lagrangian solvers. Here, we introduce a parallel algorithm for\nsolving an LQR problem with dual regularization. Leveraging a rewriting of the\nLQR recursion through block elimination, we first enhanced the efficiency of\nthe serial algorithm and then subsequently generalized it to handle parametric\nproblems. This extension enables us to split decision variables and solve\nmultiple subproblems concurrently. Our algorithm is implemented in our\nnonlinear numerical optimal control library ALIGATOR. It showcases improved\nperformance over previous serial formulations and we validate its efficacy by\ndeploying it in the model predictive control of a real quadruped robot.\n", "link": "http://arxiv.org/abs/2405.09197v2", "date": "2024-06-03", "relevancy": 1.9774, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4927}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20and%20Proximal%20Constrained%20Linear-Quadratic%20Methods%20for%20Real-Time%0A%20%20Nonlinear%20MPC&body=Title%3A%20Parallel%20and%20Proximal%20Constrained%20Linear-Quadratic%20Methods%20for%20Real-Time%0A%20%20Nonlinear%20MPC%0AAuthor%3A%20Wilson%20Jallet%20and%20Ewen%20Dantec%20and%20Etienne%20Arlaud%20and%20Justin%20Carpentier%20and%20Nicolas%20Mansard%0AAbstract%3A%20%20%20Recent%20strides%20in%20nonlinear%20model%20predictive%20control%20%28NMPC%29%20underscore%20a%0Adependence%20on%20numerical%20advancements%20to%20efficiently%20and%20accurately%20solve%0Alarge-scale%20problems.%20Given%20the%20substantial%20number%20of%20variables%20characterizing%0Atypical%20whole-body%20optimal%20control%20%28OC%29%20problems%20-%20often%20numbering%20in%20the%0Athousands%20-%20exploiting%20the%20sparse%20structure%20of%20the%20numerical%20problem%20becomes%0Acrucial%20to%20meet%20computational%20demands%2C%20typically%20in%20the%20range%20of%20a%20few%0Amilliseconds.%20Addressing%20the%20linear-quadratic%20regulator%20%28LQR%29%20problem%20is%20a%0Afundamental%20building%20block%20for%20computing%20Newton%20or%20Sequential%20Quadratic%0AProgramming%20%28SQP%29%20steps%20in%20direct%20optimal%20control%20methods.%20This%20paper%0Aconcentrates%20on%20equality-constrained%20problems%20featuring%20implicit%20system%0Adynamics%20and%20dual%20regularization%2C%20a%20characteristic%20of%20advanced%20interiorpoint%20or%0Aaugmented%20Lagrangian%20solvers.%20Here%2C%20we%20introduce%20a%20parallel%20algorithm%20for%0Asolving%20an%20LQR%20problem%20with%20dual%20regularization.%20Leveraging%20a%20rewriting%20of%20the%0ALQR%20recursion%20through%20block%20elimination%2C%20we%20first%20enhanced%20the%20efficiency%20of%0Athe%20serial%20algorithm%20and%20then%20subsequently%20generalized%20it%20to%20handle%20parametric%0Aproblems.%20This%20extension%20enables%20us%20to%20split%20decision%20variables%20and%20solve%0Amultiple%20subproblems%20concurrently.%20Our%20algorithm%20is%20implemented%20in%20our%0Anonlinear%20numerical%20optimal%20control%20library%20ALIGATOR.%20It%20showcases%20improved%0Aperformance%20over%20previous%20serial%20formulations%20and%20we%20validate%20its%20efficacy%20by%0Adeploying%20it%20in%20the%20model%20predictive%20control%20of%20a%20real%20quadruped%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520and%2520Proximal%2520Constrained%2520Linear-Quadratic%2520Methods%2520for%2520Real-Time%250A%2520%2520Nonlinear%2520MPC%26entry.906535625%3DWilson%2520Jallet%2520and%2520Ewen%2520Dantec%2520and%2520Etienne%2520Arlaud%2520and%2520Justin%2520Carpentier%2520and%2520Nicolas%2520Mansard%26entry.1292438233%3D%2520%2520Recent%2520strides%2520in%2520nonlinear%2520model%2520predictive%2520control%2520%2528NMPC%2529%2520underscore%2520a%250Adependence%2520on%2520numerical%2520advancements%2520to%2520efficiently%2520and%2520accurately%2520solve%250Alarge-scale%2520problems.%2520Given%2520the%2520substantial%2520number%2520of%2520variables%2520characterizing%250Atypical%2520whole-body%2520optimal%2520control%2520%2528OC%2529%2520problems%2520-%2520often%2520numbering%2520in%2520the%250Athousands%2520-%2520exploiting%2520the%2520sparse%2520structure%2520of%2520the%2520numerical%2520problem%2520becomes%250Acrucial%2520to%2520meet%2520computational%2520demands%252C%2520typically%2520in%2520the%2520range%2520of%2520a%2520few%250Amilliseconds.%2520Addressing%2520the%2520linear-quadratic%2520regulator%2520%2528LQR%2529%2520problem%2520is%2520a%250Afundamental%2520building%2520block%2520for%2520computing%2520Newton%2520or%2520Sequential%2520Quadratic%250AProgramming%2520%2528SQP%2529%2520steps%2520in%2520direct%2520optimal%2520control%2520methods.%2520This%2520paper%250Aconcentrates%2520on%2520equality-constrained%2520problems%2520featuring%2520implicit%2520system%250Adynamics%2520and%2520dual%2520regularization%252C%2520a%2520characteristic%2520of%2520advanced%2520interiorpoint%2520or%250Aaugmented%2520Lagrangian%2520solvers.%2520Here%252C%2520we%2520introduce%2520a%2520parallel%2520algorithm%2520for%250Asolving%2520an%2520LQR%2520problem%2520with%2520dual%2520regularization.%2520Leveraging%2520a%2520rewriting%2520of%2520the%250ALQR%2520recursion%2520through%2520block%2520elimination%252C%2520we%2520first%2520enhanced%2520the%2520efficiency%2520of%250Athe%2520serial%2520algorithm%2520and%2520then%2520subsequently%2520generalized%2520it%2520to%2520handle%2520parametric%250Aproblems.%2520This%2520extension%2520enables%2520us%2520to%2520split%2520decision%2520variables%2520and%2520solve%250Amultiple%2520subproblems%2520concurrently.%2520Our%2520algorithm%2520is%2520implemented%2520in%2520our%250Anonlinear%2520numerical%2520optimal%2520control%2520library%2520ALIGATOR.%2520It%2520showcases%2520improved%250Aperformance%2520over%2520previous%2520serial%2520formulations%2520and%2520we%2520validate%2520its%2520efficacy%2520by%250Adeploying%2520it%2520in%2520the%2520model%2520predictive%2520control%2520of%2520a%2520real%2520quadruped%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20and%20Proximal%20Constrained%20Linear-Quadratic%20Methods%20for%20Real-Time%0A%20%20Nonlinear%20MPC&entry.906535625=Wilson%20Jallet%20and%20Ewen%20Dantec%20and%20Etienne%20Arlaud%20and%20Justin%20Carpentier%20and%20Nicolas%20Mansard&entry.1292438233=%20%20Recent%20strides%20in%20nonlinear%20model%20predictive%20control%20%28NMPC%29%20underscore%20a%0Adependence%20on%20numerical%20advancements%20to%20efficiently%20and%20accurately%20solve%0Alarge-scale%20problems.%20Given%20the%20substantial%20number%20of%20variables%20characterizing%0Atypical%20whole-body%20optimal%20control%20%28OC%29%20problems%20-%20often%20numbering%20in%20the%0Athousands%20-%20exploiting%20the%20sparse%20structure%20of%20the%20numerical%20problem%20becomes%0Acrucial%20to%20meet%20computational%20demands%2C%20typically%20in%20the%20range%20of%20a%20few%0Amilliseconds.%20Addressing%20the%20linear-quadratic%20regulator%20%28LQR%29%20problem%20is%20a%0Afundamental%20building%20block%20for%20computing%20Newton%20or%20Sequential%20Quadratic%0AProgramming%20%28SQP%29%20steps%20in%20direct%20optimal%20control%20methods.%20This%20paper%0Aconcentrates%20on%20equality-constrained%20problems%20featuring%20implicit%20system%0Adynamics%20and%20dual%20regularization%2C%20a%20characteristic%20of%20advanced%20interiorpoint%20or%0Aaugmented%20Lagrangian%20solvers.%20Here%2C%20we%20introduce%20a%20parallel%20algorithm%20for%0Asolving%20an%20LQR%20problem%20with%20dual%20regularization.%20Leveraging%20a%20rewriting%20of%20the%0ALQR%20recursion%20through%20block%20elimination%2C%20we%20first%20enhanced%20the%20efficiency%20of%0Athe%20serial%20algorithm%20and%20then%20subsequently%20generalized%20it%20to%20handle%20parametric%0Aproblems.%20This%20extension%20enables%20us%20to%20split%20decision%20variables%20and%20solve%0Amultiple%20subproblems%20concurrently.%20Our%20algorithm%20is%20implemented%20in%20our%0Anonlinear%20numerical%20optimal%20control%20library%20ALIGATOR.%20It%20showcases%20improved%0Aperformance%20over%20previous%20serial%20formulations%20and%20we%20validate%20its%20efficacy%20by%0Adeploying%20it%20in%20the%20model%20predictive%20control%20of%20a%20real%20quadruped%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09197v2&entry.124074799=Read"},
{"title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM\n  Agents Exponentially Fast", "author": "Xiangming Gu and Xiaosen Zheng and Tianyu Pang and Chao Du and Qian Liu and Ye Wang and Jing Jiang and Min Lin", "abstract": "  A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.\n", "link": "http://arxiv.org/abs/2402.08567v2", "date": "2024-06-03", "relevancy": 1.9761, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.495}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4943}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Smith%3A%20A%20Single%20Image%20Can%20Jailbreak%20One%20Million%20Multimodal%20LLM%0A%20%20Agents%20Exponentially%20Fast&body=Title%3A%20Agent%20Smith%3A%20A%20Single%20Image%20Can%20Jailbreak%20One%20Million%20Multimodal%20LLM%0A%20%20Agents%20Exponentially%20Fast%0AAuthor%3A%20Xiangming%20Gu%20and%20Xiaosen%20Zheng%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Qian%20Liu%20and%20Ye%20Wang%20and%20Jing%20Jiang%20and%20Min%20Lin%0AAbstract%3A%20%20%20A%20multimodal%20large%20language%20model%20%28MLLM%29%20agent%20can%20receive%20instructions%2C%0Acapture%20images%2C%20retrieve%20histories%20from%20memory%2C%20and%20decide%20which%20tools%20to%20use.%0ANonetheless%2C%20red-teaming%20efforts%20have%20revealed%20that%20adversarial%20images/prompts%0Acan%20jailbreak%20an%20MLLM%20and%20cause%20unaligned%20behaviors.%20In%20this%20work%2C%20we%20report%20an%0Aeven%20more%20severe%20safety%20issue%20in%20multi-agent%20environments%2C%20referred%20to%20as%0Ainfectious%20jailbreak.%20It%20entails%20the%20adversary%20simply%20jailbreaking%20a%20single%0Aagent%2C%20and%20without%20any%20further%20intervention%20from%20the%20adversary%2C%20%28almost%29%20all%0Aagents%20will%20become%20infected%20exponentially%20fast%20and%20exhibit%20harmful%20behaviors.%0ATo%20validate%20the%20feasibility%20of%20infectious%20jailbreak%2C%20we%20simulate%20multi-agent%0Aenvironments%20containing%20up%20to%20one%20million%20LLaVA-1.5%20agents%2C%20and%20employ%0Arandomized%20pair-wise%20chat%20as%20a%20proof-of-concept%20instantiation%20for%20multi-agent%0Ainteraction.%20Our%20results%20show%20that%20feeding%20an%20%28infectious%29%20adversarial%20image%0Ainto%20the%20memory%20of%20any%20randomly%20chosen%20agent%20is%20sufficient%20to%20achieve%0Ainfectious%20jailbreak.%20Finally%2C%20we%20derive%20a%20simple%20principle%20for%20determining%0Awhether%20a%20defense%20mechanism%20can%20provably%20restrain%20the%20spread%20of%20infectious%0Ajailbreak%2C%20but%20how%20to%20design%20a%20practical%20defense%20that%20meets%20this%20principle%0Aremains%20an%20open%20question%20to%20investigate.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//sail-sg.github.io/Agent-Smith/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Smith%253A%2520A%2520Single%2520Image%2520Can%2520Jailbreak%2520One%2520Million%2520Multimodal%2520LLM%250A%2520%2520Agents%2520Exponentially%2520Fast%26entry.906535625%3DXiangming%2520Gu%2520and%2520Xiaosen%2520Zheng%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Qian%2520Liu%2520and%2520Ye%2520Wang%2520and%2520Jing%2520Jiang%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520A%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520agent%2520can%2520receive%2520instructions%252C%250Acapture%2520images%252C%2520retrieve%2520histories%2520from%2520memory%252C%2520and%2520decide%2520which%2520tools%2520to%2520use.%250ANonetheless%252C%2520red-teaming%2520efforts%2520have%2520revealed%2520that%2520adversarial%2520images/prompts%250Acan%2520jailbreak%2520an%2520MLLM%2520and%2520cause%2520unaligned%2520behaviors.%2520In%2520this%2520work%252C%2520we%2520report%2520an%250Aeven%2520more%2520severe%2520safety%2520issue%2520in%2520multi-agent%2520environments%252C%2520referred%2520to%2520as%250Ainfectious%2520jailbreak.%2520It%2520entails%2520the%2520adversary%2520simply%2520jailbreaking%2520a%2520single%250Aagent%252C%2520and%2520without%2520any%2520further%2520intervention%2520from%2520the%2520adversary%252C%2520%2528almost%2529%2520all%250Aagents%2520will%2520become%2520infected%2520exponentially%2520fast%2520and%2520exhibit%2520harmful%2520behaviors.%250ATo%2520validate%2520the%2520feasibility%2520of%2520infectious%2520jailbreak%252C%2520we%2520simulate%2520multi-agent%250Aenvironments%2520containing%2520up%2520to%2520one%2520million%2520LLaVA-1.5%2520agents%252C%2520and%2520employ%250Arandomized%2520pair-wise%2520chat%2520as%2520a%2520proof-of-concept%2520instantiation%2520for%2520multi-agent%250Ainteraction.%2520Our%2520results%2520show%2520that%2520feeding%2520an%2520%2528infectious%2529%2520adversarial%2520image%250Ainto%2520the%2520memory%2520of%2520any%2520randomly%2520chosen%2520agent%2520is%2520sufficient%2520to%2520achieve%250Ainfectious%2520jailbreak.%2520Finally%252C%2520we%2520derive%2520a%2520simple%2520principle%2520for%2520determining%250Awhether%2520a%2520defense%2520mechanism%2520can%2520provably%2520restrain%2520the%2520spread%2520of%2520infectious%250Ajailbreak%252C%2520but%2520how%2520to%2520design%2520a%2520practical%2520defense%2520that%2520meets%2520this%2520principle%250Aremains%2520an%2520open%2520question%2520to%2520investigate.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//sail-sg.github.io/Agent-Smith/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Smith%3A%20A%20Single%20Image%20Can%20Jailbreak%20One%20Million%20Multimodal%20LLM%0A%20%20Agents%20Exponentially%20Fast&entry.906535625=Xiangming%20Gu%20and%20Xiaosen%20Zheng%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Qian%20Liu%20and%20Ye%20Wang%20and%20Jing%20Jiang%20and%20Min%20Lin&entry.1292438233=%20%20A%20multimodal%20large%20language%20model%20%28MLLM%29%20agent%20can%20receive%20instructions%2C%0Acapture%20images%2C%20retrieve%20histories%20from%20memory%2C%20and%20decide%20which%20tools%20to%20use.%0ANonetheless%2C%20red-teaming%20efforts%20have%20revealed%20that%20adversarial%20images/prompts%0Acan%20jailbreak%20an%20MLLM%20and%20cause%20unaligned%20behaviors.%20In%20this%20work%2C%20we%20report%20an%0Aeven%20more%20severe%20safety%20issue%20in%20multi-agent%20environments%2C%20referred%20to%20as%0Ainfectious%20jailbreak.%20It%20entails%20the%20adversary%20simply%20jailbreaking%20a%20single%0Aagent%2C%20and%20without%20any%20further%20intervention%20from%20the%20adversary%2C%20%28almost%29%20all%0Aagents%20will%20become%20infected%20exponentially%20fast%20and%20exhibit%20harmful%20behaviors.%0ATo%20validate%20the%20feasibility%20of%20infectious%20jailbreak%2C%20we%20simulate%20multi-agent%0Aenvironments%20containing%20up%20to%20one%20million%20LLaVA-1.5%20agents%2C%20and%20employ%0Arandomized%20pair-wise%20chat%20as%20a%20proof-of-concept%20instantiation%20for%20multi-agent%0Ainteraction.%20Our%20results%20show%20that%20feeding%20an%20%28infectious%29%20adversarial%20image%0Ainto%20the%20memory%20of%20any%20randomly%20chosen%20agent%20is%20sufficient%20to%20achieve%0Ainfectious%20jailbreak.%20Finally%2C%20we%20derive%20a%20simple%20principle%20for%20determining%0Awhether%20a%20defense%20mechanism%20can%20provably%20restrain%20the%20spread%20of%20infectious%0Ajailbreak%2C%20but%20how%20to%20design%20a%20practical%20defense%20that%20meets%20this%20principle%0Aremains%20an%20open%20question%20to%20investigate.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//sail-sg.github.io/Agent-Smith/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08567v2&entry.124074799=Read"},
{"title": "The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency, and Usability in Artificial Intelligence", "author": "Matt White and Ibrahim Haddad and Cailean Osborne and Xiao-Yang Liu Yanglet and Ahmed Abdelmonsef and Sachin Varghese", "abstract": "  Generative AI (GAI) offers unprecedented opportunities for research and\ninnovation, but its commercialization has raised concerns about transparency,\nreproducibility, and safety. Many open GAI models lack the necessary components\nfor full understanding and reproducibility, and some use restrictive licenses\nwhilst claiming to be ``open-source''. To address these concerns, we propose\nthe Model Openness Framework (MOF), a ranked classification system that rates\nmachine learning models based on their completeness and openness, following\nprinciples of open science, open source, open data, and open access. The MOF\nrequires specific components of the model development lifecycle to be included\nand released under appropriate open licenses. This framework aims to prevent\nmisrepresentation of models claiming to be open, guide researchers and\ndevelopers in providing all model components under permissive licenses, and\nhelp individuals and organizations identify models that can be safely adopted\nwithout restrictions. By promoting transparency and reproducibility, the MOF\ncombats ``openwashing'' practices and establishes completeness and openness as\nprimary criteria alongside the core tenets of responsible AI. Wide adoption of\nthe MOF will foster a more open AI ecosystem, benefiting research, innovation,\nand adoption of state-of-the-art models.\n", "link": "http://arxiv.org/abs/2403.13784v3", "date": "2024-06-03", "relevancy": 1.9754, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Model%20Openness%20Framework%3A%20Promoting%20Completeness%20and%20Openness%20for%0A%20%20Reproducibility%2C%20Transparency%2C%20and%20Usability%20in%20Artificial%20Intelligence&body=Title%3A%20The%20Model%20Openness%20Framework%3A%20Promoting%20Completeness%20and%20Openness%20for%0A%20%20Reproducibility%2C%20Transparency%2C%20and%20Usability%20in%20Artificial%20Intelligence%0AAuthor%3A%20Matt%20White%20and%20Ibrahim%20Haddad%20and%20Cailean%20Osborne%20and%20Xiao-Yang%20Liu%20Yanglet%20and%20Ahmed%20Abdelmonsef%20and%20Sachin%20Varghese%0AAbstract%3A%20%20%20Generative%20AI%20%28GAI%29%20offers%20unprecedented%20opportunities%20for%20research%20and%0Ainnovation%2C%20but%20its%20commercialization%20has%20raised%20concerns%20about%20transparency%2C%0Areproducibility%2C%20and%20safety.%20Many%20open%20GAI%20models%20lack%20the%20necessary%20components%0Afor%20full%20understanding%20and%20reproducibility%2C%20and%20some%20use%20restrictive%20licenses%0Awhilst%20claiming%20to%20be%20%60%60open-source%27%27.%20To%20address%20these%20concerns%2C%20we%20propose%0Athe%20Model%20Openness%20Framework%20%28MOF%29%2C%20a%20ranked%20classification%20system%20that%20rates%0Amachine%20learning%20models%20based%20on%20their%20completeness%20and%20openness%2C%20following%0Aprinciples%20of%20open%20science%2C%20open%20source%2C%20open%20data%2C%20and%20open%20access.%20The%20MOF%0Arequires%20specific%20components%20of%20the%20model%20development%20lifecycle%20to%20be%20included%0Aand%20released%20under%20appropriate%20open%20licenses.%20This%20framework%20aims%20to%20prevent%0Amisrepresentation%20of%20models%20claiming%20to%20be%20open%2C%20guide%20researchers%20and%0Adevelopers%20in%20providing%20all%20model%20components%20under%20permissive%20licenses%2C%20and%0Ahelp%20individuals%20and%20organizations%20identify%20models%20that%20can%20be%20safely%20adopted%0Awithout%20restrictions.%20By%20promoting%20transparency%20and%20reproducibility%2C%20the%20MOF%0Acombats%20%60%60openwashing%27%27%20practices%20and%20establishes%20completeness%20and%20openness%20as%0Aprimary%20criteria%20alongside%20the%20core%20tenets%20of%20responsible%20AI.%20Wide%20adoption%20of%0Athe%20MOF%20will%20foster%20a%20more%20open%20AI%20ecosystem%2C%20benefiting%20research%2C%20innovation%2C%0Aand%20adoption%20of%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13784v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Model%2520Openness%2520Framework%253A%2520Promoting%2520Completeness%2520and%2520Openness%2520for%250A%2520%2520Reproducibility%252C%2520Transparency%252C%2520and%2520Usability%2520in%2520Artificial%2520Intelligence%26entry.906535625%3DMatt%2520White%2520and%2520Ibrahim%2520Haddad%2520and%2520Cailean%2520Osborne%2520and%2520Xiao-Yang%2520Liu%2520Yanglet%2520and%2520Ahmed%2520Abdelmonsef%2520and%2520Sachin%2520Varghese%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GAI%2529%2520offers%2520unprecedented%2520opportunities%2520for%2520research%2520and%250Ainnovation%252C%2520but%2520its%2520commercialization%2520has%2520raised%2520concerns%2520about%2520transparency%252C%250Areproducibility%252C%2520and%2520safety.%2520Many%2520open%2520GAI%2520models%2520lack%2520the%2520necessary%2520components%250Afor%2520full%2520understanding%2520and%2520reproducibility%252C%2520and%2520some%2520use%2520restrictive%2520licenses%250Awhilst%2520claiming%2520to%2520be%2520%2560%2560open-source%2527%2527.%2520To%2520address%2520these%2520concerns%252C%2520we%2520propose%250Athe%2520Model%2520Openness%2520Framework%2520%2528MOF%2529%252C%2520a%2520ranked%2520classification%2520system%2520that%2520rates%250Amachine%2520learning%2520models%2520based%2520on%2520their%2520completeness%2520and%2520openness%252C%2520following%250Aprinciples%2520of%2520open%2520science%252C%2520open%2520source%252C%2520open%2520data%252C%2520and%2520open%2520access.%2520The%2520MOF%250Arequires%2520specific%2520components%2520of%2520the%2520model%2520development%2520lifecycle%2520to%2520be%2520included%250Aand%2520released%2520under%2520appropriate%2520open%2520licenses.%2520This%2520framework%2520aims%2520to%2520prevent%250Amisrepresentation%2520of%2520models%2520claiming%2520to%2520be%2520open%252C%2520guide%2520researchers%2520and%250Adevelopers%2520in%2520providing%2520all%2520model%2520components%2520under%2520permissive%2520licenses%252C%2520and%250Ahelp%2520individuals%2520and%2520organizations%2520identify%2520models%2520that%2520can%2520be%2520safely%2520adopted%250Awithout%2520restrictions.%2520By%2520promoting%2520transparency%2520and%2520reproducibility%252C%2520the%2520MOF%250Acombats%2520%2560%2560openwashing%2527%2527%2520practices%2520and%2520establishes%2520completeness%2520and%2520openness%2520as%250Aprimary%2520criteria%2520alongside%2520the%2520core%2520tenets%2520of%2520responsible%2520AI.%2520Wide%2520adoption%2520of%250Athe%2520MOF%2520will%2520foster%2520a%2520more%2520open%2520AI%2520ecosystem%252C%2520benefiting%2520research%252C%2520innovation%252C%250Aand%2520adoption%2520of%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13784v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Model%20Openness%20Framework%3A%20Promoting%20Completeness%20and%20Openness%20for%0A%20%20Reproducibility%2C%20Transparency%2C%20and%20Usability%20in%20Artificial%20Intelligence&entry.906535625=Matt%20White%20and%20Ibrahim%20Haddad%20and%20Cailean%20Osborne%20and%20Xiao-Yang%20Liu%20Yanglet%20and%20Ahmed%20Abdelmonsef%20and%20Sachin%20Varghese&entry.1292438233=%20%20Generative%20AI%20%28GAI%29%20offers%20unprecedented%20opportunities%20for%20research%20and%0Ainnovation%2C%20but%20its%20commercialization%20has%20raised%20concerns%20about%20transparency%2C%0Areproducibility%2C%20and%20safety.%20Many%20open%20GAI%20models%20lack%20the%20necessary%20components%0Afor%20full%20understanding%20and%20reproducibility%2C%20and%20some%20use%20restrictive%20licenses%0Awhilst%20claiming%20to%20be%20%60%60open-source%27%27.%20To%20address%20these%20concerns%2C%20we%20propose%0Athe%20Model%20Openness%20Framework%20%28MOF%29%2C%20a%20ranked%20classification%20system%20that%20rates%0Amachine%20learning%20models%20based%20on%20their%20completeness%20and%20openness%2C%20following%0Aprinciples%20of%20open%20science%2C%20open%20source%2C%20open%20data%2C%20and%20open%20access.%20The%20MOF%0Arequires%20specific%20components%20of%20the%20model%20development%20lifecycle%20to%20be%20included%0Aand%20released%20under%20appropriate%20open%20licenses.%20This%20framework%20aims%20to%20prevent%0Amisrepresentation%20of%20models%20claiming%20to%20be%20open%2C%20guide%20researchers%20and%0Adevelopers%20in%20providing%20all%20model%20components%20under%20permissive%20licenses%2C%20and%0Ahelp%20individuals%20and%20organizations%20identify%20models%20that%20can%20be%20safely%20adopted%0Awithout%20restrictions.%20By%20promoting%20transparency%20and%20reproducibility%2C%20the%20MOF%0Acombats%20%60%60openwashing%27%27%20practices%20and%20establishes%20completeness%20and%20openness%20as%0Aprimary%20criteria%20alongside%20the%20core%20tenets%20of%20responsible%20AI.%20Wide%20adoption%20of%0Athe%20MOF%20will%20foster%20a%20more%20open%20AI%20ecosystem%2C%20benefiting%20research%2C%20innovation%2C%0Aand%20adoption%20of%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13784v3&entry.124074799=Read"},
{"title": "Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative\n  Diseases", "author": "Mateusz Daniol and Daria Hemmerling and Jakub Sikora and Pawel Jemiolo and Marek Wodzinski and Magdalena Wojcik-Pedziwiatr", "abstract": "  Parkinson's disease ranks as the second most prevalent neurodegenerative\ndisorder globally. This research aims to develop a system leveraging Mixed\nReality capabilities for tracking and assessing eye movements. In this paper,\nwe present a medical scenario and outline the development of an application\ndesigned to capture eye-tracking signals through Mixed Reality technology for\nthe evaluation of neurodegenerative diseases. Additionally, we introduce a\npipeline for extracting clinically relevant features from eye-gaze analysis,\ndescribing the capabilities of the proposed system from a medical perspective.\nThe study involved a cohort of healthy control individuals and patients\nsuffering from Parkinson's disease, showcasing the feasibility and potential of\nthe proposed technology for non-intrusive monitoring of eye movement patterns\nfor the diagnosis of neurodegenerative diseases.\n  Clinical relevance - Developing a non-invasive biomarker for Parkinson's\ndisease is urgently needed to accurately detect the disease's onset. This would\nallow for the timely introduction of neuroprotective treatment at the earliest\nstage and enable the continuous monitoring of intervention outcomes. The\nability to detect subtle changes in eye movements allows for early diagnosis,\noffering a critical window for intervention before more pronounced symptoms\nemerge. Eye tracking provides objective and quantifiable biomarkers, ensuring\nreliable assessments of disease progression and cognitive function. The eye\ngaze analysis using Mixed Reality glasses is wireless, facilitating convenient\nassessments in both home and hospital settings. The approach offers the\nadvantage of utilizing hardware that requires no additional specialized\nattachments, enabling examinations through personal eyewear.\n", "link": "http://arxiv.org/abs/2404.12984v2", "date": "2024-06-03", "relevancy": 1.9652, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases&body=Title%3A%20Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases%0AAuthor%3A%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Jakub%20Sikora%20and%20Pawel%20Jemiolo%20and%20Marek%20Wodzinski%20and%20Magdalena%20Wojcik-Pedziwiatr%0AAbstract%3A%20%20%20Parkinson%27s%20disease%20ranks%20as%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20globally.%20This%20research%20aims%20to%20develop%20a%20system%20leveraging%20Mixed%0AReality%20capabilities%20for%20tracking%20and%20assessing%20eye%20movements.%20In%20this%20paper%2C%0Awe%20present%20a%20medical%20scenario%20and%20outline%20the%20development%20of%20an%20application%0Adesigned%20to%20capture%20eye-tracking%20signals%20through%20Mixed%20Reality%20technology%20for%0Athe%20evaluation%20of%20neurodegenerative%20diseases.%20Additionally%2C%20we%20introduce%20a%0Apipeline%20for%20extracting%20clinically%20relevant%20features%20from%20eye-gaze%20analysis%2C%0Adescribing%20the%20capabilities%20of%20the%20proposed%20system%20from%20a%20medical%20perspective.%0AThe%20study%20involved%20a%20cohort%20of%20healthy%20control%20individuals%20and%20patients%0Asuffering%20from%20Parkinson%27s%20disease%2C%20showcasing%20the%20feasibility%20and%20potential%20of%0Athe%20proposed%20technology%20for%20non-intrusive%20monitoring%20of%20eye%20movement%20patterns%0Afor%20the%20diagnosis%20of%20neurodegenerative%20diseases.%0A%20%20Clinical%20relevance%20-%20Developing%20a%20non-invasive%20biomarker%20for%20Parkinson%27s%0Adisease%20is%20urgently%20needed%20to%20accurately%20detect%20the%20disease%27s%20onset.%20This%20would%0Aallow%20for%20the%20timely%20introduction%20of%20neuroprotective%20treatment%20at%20the%20earliest%0Astage%20and%20enable%20the%20continuous%20monitoring%20of%20intervention%20outcomes.%20The%0Aability%20to%20detect%20subtle%20changes%20in%20eye%20movements%20allows%20for%20early%20diagnosis%2C%0Aoffering%20a%20critical%20window%20for%20intervention%20before%20more%20pronounced%20symptoms%0Aemerge.%20Eye%20tracking%20provides%20objective%20and%20quantifiable%20biomarkers%2C%20ensuring%0Areliable%20assessments%20of%20disease%20progression%20and%20cognitive%20function.%20The%20eye%0Agaze%20analysis%20using%20Mixed%20Reality%20glasses%20is%20wireless%2C%20facilitating%20convenient%0Aassessments%20in%20both%20home%20and%20hospital%20settings.%20The%20approach%20offers%20the%0Aadvantage%20of%20utilizing%20hardware%20that%20requires%20no%20additional%20specialized%0Aattachments%2C%20enabling%20examinations%20through%20personal%20eyewear.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEye-tracking%2520in%2520Mixed%2520Reality%2520for%2520Diagnosis%2520of%2520Neurodegenerative%250A%2520%2520Diseases%26entry.906535625%3DMateusz%2520Daniol%2520and%2520Daria%2520Hemmerling%2520and%2520Jakub%2520Sikora%2520and%2520Pawel%2520Jemiolo%2520and%2520Marek%2520Wodzinski%2520and%2520Magdalena%2520Wojcik-Pedziwiatr%26entry.1292438233%3D%2520%2520Parkinson%2527s%2520disease%2520ranks%2520as%2520the%2520second%2520most%2520prevalent%2520neurodegenerative%250Adisorder%2520globally.%2520This%2520research%2520aims%2520to%2520develop%2520a%2520system%2520leveraging%2520Mixed%250AReality%2520capabilities%2520for%2520tracking%2520and%2520assessing%2520eye%2520movements.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520medical%2520scenario%2520and%2520outline%2520the%2520development%2520of%2520an%2520application%250Adesigned%2520to%2520capture%2520eye-tracking%2520signals%2520through%2520Mixed%2520Reality%2520technology%2520for%250Athe%2520evaluation%2520of%2520neurodegenerative%2520diseases.%2520Additionally%252C%2520we%2520introduce%2520a%250Apipeline%2520for%2520extracting%2520clinically%2520relevant%2520features%2520from%2520eye-gaze%2520analysis%252C%250Adescribing%2520the%2520capabilities%2520of%2520the%2520proposed%2520system%2520from%2520a%2520medical%2520perspective.%250AThe%2520study%2520involved%2520a%2520cohort%2520of%2520healthy%2520control%2520individuals%2520and%2520patients%250Asuffering%2520from%2520Parkinson%2527s%2520disease%252C%2520showcasing%2520the%2520feasibility%2520and%2520potential%2520of%250Athe%2520proposed%2520technology%2520for%2520non-intrusive%2520monitoring%2520of%2520eye%2520movement%2520patterns%250Afor%2520the%2520diagnosis%2520of%2520neurodegenerative%2520diseases.%250A%2520%2520Clinical%2520relevance%2520-%2520Developing%2520a%2520non-invasive%2520biomarker%2520for%2520Parkinson%2527s%250Adisease%2520is%2520urgently%2520needed%2520to%2520accurately%2520detect%2520the%2520disease%2527s%2520onset.%2520This%2520would%250Aallow%2520for%2520the%2520timely%2520introduction%2520of%2520neuroprotective%2520treatment%2520at%2520the%2520earliest%250Astage%2520and%2520enable%2520the%2520continuous%2520monitoring%2520of%2520intervention%2520outcomes.%2520The%250Aability%2520to%2520detect%2520subtle%2520changes%2520in%2520eye%2520movements%2520allows%2520for%2520early%2520diagnosis%252C%250Aoffering%2520a%2520critical%2520window%2520for%2520intervention%2520before%2520more%2520pronounced%2520symptoms%250Aemerge.%2520Eye%2520tracking%2520provides%2520objective%2520and%2520quantifiable%2520biomarkers%252C%2520ensuring%250Areliable%2520assessments%2520of%2520disease%2520progression%2520and%2520cognitive%2520function.%2520The%2520eye%250Agaze%2520analysis%2520using%2520Mixed%2520Reality%2520glasses%2520is%2520wireless%252C%2520facilitating%2520convenient%250Aassessments%2520in%2520both%2520home%2520and%2520hospital%2520settings.%2520The%2520approach%2520offers%2520the%250Aadvantage%2520of%2520utilizing%2520hardware%2520that%2520requires%2520no%2520additional%2520specialized%250Aattachments%252C%2520enabling%2520examinations%2520through%2520personal%2520eyewear.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases&entry.906535625=Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Jakub%20Sikora%20and%20Pawel%20Jemiolo%20and%20Marek%20Wodzinski%20and%20Magdalena%20Wojcik-Pedziwiatr&entry.1292438233=%20%20Parkinson%27s%20disease%20ranks%20as%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20globally.%20This%20research%20aims%20to%20develop%20a%20system%20leveraging%20Mixed%0AReality%20capabilities%20for%20tracking%20and%20assessing%20eye%20movements.%20In%20this%20paper%2C%0Awe%20present%20a%20medical%20scenario%20and%20outline%20the%20development%20of%20an%20application%0Adesigned%20to%20capture%20eye-tracking%20signals%20through%20Mixed%20Reality%20technology%20for%0Athe%20evaluation%20of%20neurodegenerative%20diseases.%20Additionally%2C%20we%20introduce%20a%0Apipeline%20for%20extracting%20clinically%20relevant%20features%20from%20eye-gaze%20analysis%2C%0Adescribing%20the%20capabilities%20of%20the%20proposed%20system%20from%20a%20medical%20perspective.%0AThe%20study%20involved%20a%20cohort%20of%20healthy%20control%20individuals%20and%20patients%0Asuffering%20from%20Parkinson%27s%20disease%2C%20showcasing%20the%20feasibility%20and%20potential%20of%0Athe%20proposed%20technology%20for%20non-intrusive%20monitoring%20of%20eye%20movement%20patterns%0Afor%20the%20diagnosis%20of%20neurodegenerative%20diseases.%0A%20%20Clinical%20relevance%20-%20Developing%20a%20non-invasive%20biomarker%20for%20Parkinson%27s%0Adisease%20is%20urgently%20needed%20to%20accurately%20detect%20the%20disease%27s%20onset.%20This%20would%0Aallow%20for%20the%20timely%20introduction%20of%20neuroprotective%20treatment%20at%20the%20earliest%0Astage%20and%20enable%20the%20continuous%20monitoring%20of%20intervention%20outcomes.%20The%0Aability%20to%20detect%20subtle%20changes%20in%20eye%20movements%20allows%20for%20early%20diagnosis%2C%0Aoffering%20a%20critical%20window%20for%20intervention%20before%20more%20pronounced%20symptoms%0Aemerge.%20Eye%20tracking%20provides%20objective%20and%20quantifiable%20biomarkers%2C%20ensuring%0Areliable%20assessments%20of%20disease%20progression%20and%20cognitive%20function.%20The%20eye%0Agaze%20analysis%20using%20Mixed%20Reality%20glasses%20is%20wireless%2C%20facilitating%20convenient%0Aassessments%20in%20both%20home%20and%20hospital%20settings.%20The%20approach%20offers%20the%0Aadvantage%20of%20utilizing%20hardware%20that%20requires%20no%20additional%20specialized%0Aattachments%2C%20enabling%20examinations%20through%20personal%20eyewear.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12984v2&entry.124074799=Read"},
{"title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers", "author": "Yuchuan Tian and Zhijun Tu and Hanting Chen and Jie Hu and Chao Xu and Yunhe Wang", "abstract": "  Diffusion Transformers (DiTs) introduce the transformer architecture to\ndiffusion tasks for latent-space image generation. With an isotropic\narchitecture that chains a series of transformer blocks, DiTs demonstrate\ncompetitive performance and good scalability; but meanwhile, the abandonment of\nU-Net by DiTs and their following improvements is worth rethinking. To this\nend, we conduct a simple toy experiment by comparing a U-Net architectured DiT\nwith an isotropic one. It turns out that the U-Net architecture only gain a\nslight advantage amid the U-Net inductive bias, indicating potential\nredundancies within the U-Net-style DiT. Inspired by the discovery that U-Net\nbackbone features are low-frequency-dominated, we perform token downsampling on\nthe query-key-value tuple for self-attention that bring further improvements\ndespite a considerable amount of reduction in computation. Based on\nself-attention with downsampled tokens, we propose a series of U-shaped DiTs\n(U-DiTs) in the paper and conduct extensive experiments to demonstrate the\nextraordinary performance of U-DiT models. The proposed U-DiT could outperform\nDiT-XL/2 with only 1/6 of its computation cost. Codes are available at\nhttps://github.com/YuchuanTian/U-DiT.\n", "link": "http://arxiv.org/abs/2405.02730v2", "date": "2024-06-03", "relevancy": 1.9639, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6918}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.617}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers&body=Title%3A%20U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers%0AAuthor%3A%20Yuchuan%20Tian%20and%20Zhijun%20Tu%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Chao%20Xu%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20introduce%20the%20transformer%20architecture%20to%0Adiffusion%20tasks%20for%20latent-space%20image%20generation.%20With%20an%20isotropic%0Aarchitecture%20that%20chains%20a%20series%20of%20transformer%20blocks%2C%20DiTs%20demonstrate%0Acompetitive%20performance%20and%20good%20scalability%3B%20but%20meanwhile%2C%20the%20abandonment%20of%0AU-Net%20by%20DiTs%20and%20their%20following%20improvements%20is%20worth%20rethinking.%20To%20this%0Aend%2C%20we%20conduct%20a%20simple%20toy%20experiment%20by%20comparing%20a%20U-Net%20architectured%20DiT%0Awith%20an%20isotropic%20one.%20It%20turns%20out%20that%20the%20U-Net%20architecture%20only%20gain%20a%0Aslight%20advantage%20amid%20the%20U-Net%20inductive%20bias%2C%20indicating%20potential%0Aredundancies%20within%20the%20U-Net-style%20DiT.%20Inspired%20by%20the%20discovery%20that%20U-Net%0Abackbone%20features%20are%20low-frequency-dominated%2C%20we%20perform%20token%20downsampling%20on%0Athe%20query-key-value%20tuple%20for%20self-attention%20that%20bring%20further%20improvements%0Adespite%20a%20considerable%20amount%20of%20reduction%20in%20computation.%20Based%20on%0Aself-attention%20with%20downsampled%20tokens%2C%20we%20propose%20a%20series%20of%20U-shaped%20DiTs%0A%28U-DiTs%29%20in%20the%20paper%20and%20conduct%20extensive%20experiments%20to%20demonstrate%20the%0Aextraordinary%20performance%20of%20U-DiT%20models.%20The%20proposed%20U-DiT%20could%20outperform%0ADiT-XL/2%20with%20only%201/6%20of%20its%20computation%20cost.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/YuchuanTian/U-DiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-DiTs%253A%2520Downsample%2520Tokens%2520in%2520U-Shaped%2520Diffusion%2520Transformers%26entry.906535625%3DYuchuan%2520Tian%2520and%2520Zhijun%2520Tu%2520and%2520Hanting%2520Chen%2520and%2520Jie%2520Hu%2520and%2520Chao%2520Xu%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520introduce%2520the%2520transformer%2520architecture%2520to%250Adiffusion%2520tasks%2520for%2520latent-space%2520image%2520generation.%2520With%2520an%2520isotropic%250Aarchitecture%2520that%2520chains%2520a%2520series%2520of%2520transformer%2520blocks%252C%2520DiTs%2520demonstrate%250Acompetitive%2520performance%2520and%2520good%2520scalability%253B%2520but%2520meanwhile%252C%2520the%2520abandonment%2520of%250AU-Net%2520by%2520DiTs%2520and%2520their%2520following%2520improvements%2520is%2520worth%2520rethinking.%2520To%2520this%250Aend%252C%2520we%2520conduct%2520a%2520simple%2520toy%2520experiment%2520by%2520comparing%2520a%2520U-Net%2520architectured%2520DiT%250Awith%2520an%2520isotropic%2520one.%2520It%2520turns%2520out%2520that%2520the%2520U-Net%2520architecture%2520only%2520gain%2520a%250Aslight%2520advantage%2520amid%2520the%2520U-Net%2520inductive%2520bias%252C%2520indicating%2520potential%250Aredundancies%2520within%2520the%2520U-Net-style%2520DiT.%2520Inspired%2520by%2520the%2520discovery%2520that%2520U-Net%250Abackbone%2520features%2520are%2520low-frequency-dominated%252C%2520we%2520perform%2520token%2520downsampling%2520on%250Athe%2520query-key-value%2520tuple%2520for%2520self-attention%2520that%2520bring%2520further%2520improvements%250Adespite%2520a%2520considerable%2520amount%2520of%2520reduction%2520in%2520computation.%2520Based%2520on%250Aself-attention%2520with%2520downsampled%2520tokens%252C%2520we%2520propose%2520a%2520series%2520of%2520U-shaped%2520DiTs%250A%2528U-DiTs%2529%2520in%2520the%2520paper%2520and%2520conduct%2520extensive%2520experiments%2520to%2520demonstrate%2520the%250Aextraordinary%2520performance%2520of%2520U-DiT%2520models.%2520The%2520proposed%2520U-DiT%2520could%2520outperform%250ADiT-XL/2%2520with%2520only%25201/6%2520of%2520its%2520computation%2520cost.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/YuchuanTian/U-DiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers&entry.906535625=Yuchuan%20Tian%20and%20Zhijun%20Tu%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Chao%20Xu%20and%20Yunhe%20Wang&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20introduce%20the%20transformer%20architecture%20to%0Adiffusion%20tasks%20for%20latent-space%20image%20generation.%20With%20an%20isotropic%0Aarchitecture%20that%20chains%20a%20series%20of%20transformer%20blocks%2C%20DiTs%20demonstrate%0Acompetitive%20performance%20and%20good%20scalability%3B%20but%20meanwhile%2C%20the%20abandonment%20of%0AU-Net%20by%20DiTs%20and%20their%20following%20improvements%20is%20worth%20rethinking.%20To%20this%0Aend%2C%20we%20conduct%20a%20simple%20toy%20experiment%20by%20comparing%20a%20U-Net%20architectured%20DiT%0Awith%20an%20isotropic%20one.%20It%20turns%20out%20that%20the%20U-Net%20architecture%20only%20gain%20a%0Aslight%20advantage%20amid%20the%20U-Net%20inductive%20bias%2C%20indicating%20potential%0Aredundancies%20within%20the%20U-Net-style%20DiT.%20Inspired%20by%20the%20discovery%20that%20U-Net%0Abackbone%20features%20are%20low-frequency-dominated%2C%20we%20perform%20token%20downsampling%20on%0Athe%20query-key-value%20tuple%20for%20self-attention%20that%20bring%20further%20improvements%0Adespite%20a%20considerable%20amount%20of%20reduction%20in%20computation.%20Based%20on%0Aself-attention%20with%20downsampled%20tokens%2C%20we%20propose%20a%20series%20of%20U-shaped%20DiTs%0A%28U-DiTs%29%20in%20the%20paper%20and%20conduct%20extensive%20experiments%20to%20demonstrate%20the%0Aextraordinary%20performance%20of%20U-DiT%20models.%20The%20proposed%20U-DiT%20could%20outperform%0ADiT-XL/2%20with%20only%201/6%20of%20its%20computation%20cost.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/YuchuanTian/U-DiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02730v2&entry.124074799=Read"},
{"title": "Uplift Modeling Under Limited Supervision", "author": "George Panagopoulos and Daniele Malitesta and Fragkiskos D. Malliaros and Jun Pang", "abstract": "  Estimating causal effects in e-commerce tends to involve costly treatment\nassignments which can be impractical in large-scale settings. Leveraging\nmachine learning to predict such treatment effects without actual intervention\nis a standard practice to diminish the risk. However, existing methods for\ntreatment effect prediction tend to rely on training sets of substantial size,\nwhich are built from real experiments and are thus inherently risky to create.\nIn this work we propose a graph neural network to diminish the required\ntraining set size, relying on graphs that are common in e-commerce data.\nSpecifically, we view the problem as node regression with a restricted number\nof labeled instances, develop a two-model neural architecture akin to previous\ncausal effect estimators, and test varying message-passing layers for encoding.\nFurthermore, as an extra step, we combine the model with an acquisition\nfunction to guide the creation of the training set in settings with extremely\nlow experimental budget. The framework is flexible since each step can be used\nseparately with other models or treatment policies. The experiments on real\nlarge-scale networks indicate a clear advantage of our methodology over the\nstate of the art, which in many cases performs close to random, underlining the\nneed for models that can generalize with limited supervision to reduce\nexperimental risks.\n", "link": "http://arxiv.org/abs/2403.19289v2", "date": "2024-06-03", "relevancy": 1.9639, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4817}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uplift%20Modeling%20Under%20Limited%20Supervision&body=Title%3A%20Uplift%20Modeling%20Under%20Limited%20Supervision%0AAuthor%3A%20George%20Panagopoulos%20and%20Daniele%20Malitesta%20and%20Fragkiskos%20D.%20Malliaros%20and%20Jun%20Pang%0AAbstract%3A%20%20%20Estimating%20causal%20effects%20in%20e-commerce%20tends%20to%20involve%20costly%20treatment%0Aassignments%20which%20can%20be%20impractical%20in%20large-scale%20settings.%20Leveraging%0Amachine%20learning%20to%20predict%20such%20treatment%20effects%20without%20actual%20intervention%0Ais%20a%20standard%20practice%20to%20diminish%20the%20risk.%20However%2C%20existing%20methods%20for%0Atreatment%20effect%20prediction%20tend%20to%20rely%20on%20training%20sets%20of%20substantial%20size%2C%0Awhich%20are%20built%20from%20real%20experiments%20and%20are%20thus%20inherently%20risky%20to%20create.%0AIn%20this%20work%20we%20propose%20a%20graph%20neural%20network%20to%20diminish%20the%20required%0Atraining%20set%20size%2C%20relying%20on%20graphs%20that%20are%20common%20in%20e-commerce%20data.%0ASpecifically%2C%20we%20view%20the%20problem%20as%20node%20regression%20with%20a%20restricted%20number%0Aof%20labeled%20instances%2C%20develop%20a%20two-model%20neural%20architecture%20akin%20to%20previous%0Acausal%20effect%20estimators%2C%20and%20test%20varying%20message-passing%20layers%20for%20encoding.%0AFurthermore%2C%20as%20an%20extra%20step%2C%20we%20combine%20the%20model%20with%20an%20acquisition%0Afunction%20to%20guide%20the%20creation%20of%20the%20training%20set%20in%20settings%20with%20extremely%0Alow%20experimental%20budget.%20The%20framework%20is%20flexible%20since%20each%20step%20can%20be%20used%0Aseparately%20with%20other%20models%20or%20treatment%20policies.%20The%20experiments%20on%20real%0Alarge-scale%20networks%20indicate%20a%20clear%20advantage%20of%20our%20methodology%20over%20the%0Astate%20of%20the%20art%2C%20which%20in%20many%20cases%20performs%20close%20to%20random%2C%20underlining%20the%0Aneed%20for%20models%20that%20can%20generalize%20with%20limited%20supervision%20to%20reduce%0Aexperimental%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUplift%2520Modeling%2520Under%2520Limited%2520Supervision%26entry.906535625%3DGeorge%2520Panagopoulos%2520and%2520Daniele%2520Malitesta%2520and%2520Fragkiskos%2520D.%2520Malliaros%2520and%2520Jun%2520Pang%26entry.1292438233%3D%2520%2520Estimating%2520causal%2520effects%2520in%2520e-commerce%2520tends%2520to%2520involve%2520costly%2520treatment%250Aassignments%2520which%2520can%2520be%2520impractical%2520in%2520large-scale%2520settings.%2520Leveraging%250Amachine%2520learning%2520to%2520predict%2520such%2520treatment%2520effects%2520without%2520actual%2520intervention%250Ais%2520a%2520standard%2520practice%2520to%2520diminish%2520the%2520risk.%2520However%252C%2520existing%2520methods%2520for%250Atreatment%2520effect%2520prediction%2520tend%2520to%2520rely%2520on%2520training%2520sets%2520of%2520substantial%2520size%252C%250Awhich%2520are%2520built%2520from%2520real%2520experiments%2520and%2520are%2520thus%2520inherently%2520risky%2520to%2520create.%250AIn%2520this%2520work%2520we%2520propose%2520a%2520graph%2520neural%2520network%2520to%2520diminish%2520the%2520required%250Atraining%2520set%2520size%252C%2520relying%2520on%2520graphs%2520that%2520are%2520common%2520in%2520e-commerce%2520data.%250ASpecifically%252C%2520we%2520view%2520the%2520problem%2520as%2520node%2520regression%2520with%2520a%2520restricted%2520number%250Aof%2520labeled%2520instances%252C%2520develop%2520a%2520two-model%2520neural%2520architecture%2520akin%2520to%2520previous%250Acausal%2520effect%2520estimators%252C%2520and%2520test%2520varying%2520message-passing%2520layers%2520for%2520encoding.%250AFurthermore%252C%2520as%2520an%2520extra%2520step%252C%2520we%2520combine%2520the%2520model%2520with%2520an%2520acquisition%250Afunction%2520to%2520guide%2520the%2520creation%2520of%2520the%2520training%2520set%2520in%2520settings%2520with%2520extremely%250Alow%2520experimental%2520budget.%2520The%2520framework%2520is%2520flexible%2520since%2520each%2520step%2520can%2520be%2520used%250Aseparately%2520with%2520other%2520models%2520or%2520treatment%2520policies.%2520The%2520experiments%2520on%2520real%250Alarge-scale%2520networks%2520indicate%2520a%2520clear%2520advantage%2520of%2520our%2520methodology%2520over%2520the%250Astate%2520of%2520the%2520art%252C%2520which%2520in%2520many%2520cases%2520performs%2520close%2520to%2520random%252C%2520underlining%2520the%250Aneed%2520for%2520models%2520that%2520can%2520generalize%2520with%2520limited%2520supervision%2520to%2520reduce%250Aexperimental%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uplift%20Modeling%20Under%20Limited%20Supervision&entry.906535625=George%20Panagopoulos%20and%20Daniele%20Malitesta%20and%20Fragkiskos%20D.%20Malliaros%20and%20Jun%20Pang&entry.1292438233=%20%20Estimating%20causal%20effects%20in%20e-commerce%20tends%20to%20involve%20costly%20treatment%0Aassignments%20which%20can%20be%20impractical%20in%20large-scale%20settings.%20Leveraging%0Amachine%20learning%20to%20predict%20such%20treatment%20effects%20without%20actual%20intervention%0Ais%20a%20standard%20practice%20to%20diminish%20the%20risk.%20However%2C%20existing%20methods%20for%0Atreatment%20effect%20prediction%20tend%20to%20rely%20on%20training%20sets%20of%20substantial%20size%2C%0Awhich%20are%20built%20from%20real%20experiments%20and%20are%20thus%20inherently%20risky%20to%20create.%0AIn%20this%20work%20we%20propose%20a%20graph%20neural%20network%20to%20diminish%20the%20required%0Atraining%20set%20size%2C%20relying%20on%20graphs%20that%20are%20common%20in%20e-commerce%20data.%0ASpecifically%2C%20we%20view%20the%20problem%20as%20node%20regression%20with%20a%20restricted%20number%0Aof%20labeled%20instances%2C%20develop%20a%20two-model%20neural%20architecture%20akin%20to%20previous%0Acausal%20effect%20estimators%2C%20and%20test%20varying%20message-passing%20layers%20for%20encoding.%0AFurthermore%2C%20as%20an%20extra%20step%2C%20we%20combine%20the%20model%20with%20an%20acquisition%0Afunction%20to%20guide%20the%20creation%20of%20the%20training%20set%20in%20settings%20with%20extremely%0Alow%20experimental%20budget.%20The%20framework%20is%20flexible%20since%20each%20step%20can%20be%20used%0Aseparately%20with%20other%20models%20or%20treatment%20policies.%20The%20experiments%20on%20real%0Alarge-scale%20networks%20indicate%20a%20clear%20advantage%20of%20our%20methodology%20over%20the%0Astate%20of%20the%20art%2C%20which%20in%20many%20cases%20performs%20close%20to%20random%2C%20underlining%20the%0Aneed%20for%20models%20that%20can%20generalize%20with%20limited%20supervision%20to%20reduce%0Aexperimental%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19289v2&entry.124074799=Read"},
{"title": "PAC-Bayesian Generalization Bounds for Knowledge Graph Representation\n  Learning", "author": "Jaejun Lee and Minsung Hwang and Joyce Jiyoung Whang", "abstract": "  While a number of knowledge graph representation learning (KGRL) methods have\nbeen proposed over the past decade, very few theoretical analyses have been\nconducted on them. In this paper, we present the first PAC-Bayesian\ngeneralization bounds for KGRL methods. To analyze a broad class of KGRL\nmodels, we propose a generic framework named ReED (Relation-aware\nEncoder-Decoder), which consists of a relation-aware message passing encoder\nand a triplet classification decoder. Our ReED framework can express at least\n15 different existing KGRL models, including not only graph neural\nnetwork-based models such as R-GCN and CompGCN but also shallow-architecture\nmodels such as RotatE and ANALOGY. Our generalization bounds for the ReED\nframework provide theoretical grounds for the commonly used tricks in KGRL,\ne.g., parameter-sharing and weight normalization schemes, and guide desirable\ndesign choices for practical KGRL methods. We empirically show that the\ncritical factors in our generalization bounds can explain actual generalization\nerrors on three real-world knowledge graphs.\n", "link": "http://arxiv.org/abs/2405.06418v2", "date": "2024-06-03", "relevancy": 1.9604, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4932}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning&body=Title%3A%20PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning%0AAuthor%3A%20Jaejun%20Lee%20and%20Minsung%20Hwang%20and%20Joyce%20Jiyoung%20Whang%0AAbstract%3A%20%20%20While%20a%20number%20of%20knowledge%20graph%20representation%20learning%20%28KGRL%29%20methods%20have%0Abeen%20proposed%20over%20the%20past%20decade%2C%20very%20few%20theoretical%20analyses%20have%20been%0Aconducted%20on%20them.%20In%20this%20paper%2C%20we%20present%20the%20first%20PAC-Bayesian%0Ageneralization%20bounds%20for%20KGRL%20methods.%20To%20analyze%20a%20broad%20class%20of%20KGRL%0Amodels%2C%20we%20propose%20a%20generic%20framework%20named%20ReED%20%28Relation-aware%0AEncoder-Decoder%29%2C%20which%20consists%20of%20a%20relation-aware%20message%20passing%20encoder%0Aand%20a%20triplet%20classification%20decoder.%20Our%20ReED%20framework%20can%20express%20at%20least%0A15%20different%20existing%20KGRL%20models%2C%20including%20not%20only%20graph%20neural%0Anetwork-based%20models%20such%20as%20R-GCN%20and%20CompGCN%20but%20also%20shallow-architecture%0Amodels%20such%20as%20RotatE%20and%20ANALOGY.%20Our%20generalization%20bounds%20for%20the%20ReED%0Aframework%20provide%20theoretical%20grounds%20for%20the%20commonly%20used%20tricks%20in%20KGRL%2C%0Ae.g.%2C%20parameter-sharing%20and%20weight%20normalization%20schemes%2C%20and%20guide%20desirable%0Adesign%20choices%20for%20practical%20KGRL%20methods.%20We%20empirically%20show%20that%20the%0Acritical%20factors%20in%20our%20generalization%20bounds%20can%20explain%20actual%20generalization%0Aerrors%20on%20three%20real-world%20knowledge%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC-Bayesian%2520Generalization%2520Bounds%2520for%2520Knowledge%2520Graph%2520Representation%250A%2520%2520Learning%26entry.906535625%3DJaejun%2520Lee%2520and%2520Minsung%2520Hwang%2520and%2520Joyce%2520Jiyoung%2520Whang%26entry.1292438233%3D%2520%2520While%2520a%2520number%2520of%2520knowledge%2520graph%2520representation%2520learning%2520%2528KGRL%2529%2520methods%2520have%250Abeen%2520proposed%2520over%2520the%2520past%2520decade%252C%2520very%2520few%2520theoretical%2520analyses%2520have%2520been%250Aconducted%2520on%2520them.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520PAC-Bayesian%250Ageneralization%2520bounds%2520for%2520KGRL%2520methods.%2520To%2520analyze%2520a%2520broad%2520class%2520of%2520KGRL%250Amodels%252C%2520we%2520propose%2520a%2520generic%2520framework%2520named%2520ReED%2520%2528Relation-aware%250AEncoder-Decoder%2529%252C%2520which%2520consists%2520of%2520a%2520relation-aware%2520message%2520passing%2520encoder%250Aand%2520a%2520triplet%2520classification%2520decoder.%2520Our%2520ReED%2520framework%2520can%2520express%2520at%2520least%250A15%2520different%2520existing%2520KGRL%2520models%252C%2520including%2520not%2520only%2520graph%2520neural%250Anetwork-based%2520models%2520such%2520as%2520R-GCN%2520and%2520CompGCN%2520but%2520also%2520shallow-architecture%250Amodels%2520such%2520as%2520RotatE%2520and%2520ANALOGY.%2520Our%2520generalization%2520bounds%2520for%2520the%2520ReED%250Aframework%2520provide%2520theoretical%2520grounds%2520for%2520the%2520commonly%2520used%2520tricks%2520in%2520KGRL%252C%250Ae.g.%252C%2520parameter-sharing%2520and%2520weight%2520normalization%2520schemes%252C%2520and%2520guide%2520desirable%250Adesign%2520choices%2520for%2520practical%2520KGRL%2520methods.%2520We%2520empirically%2520show%2520that%2520the%250Acritical%2520factors%2520in%2520our%2520generalization%2520bounds%2520can%2520explain%2520actual%2520generalization%250Aerrors%2520on%2520three%2520real-world%2520knowledge%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning&entry.906535625=Jaejun%20Lee%20and%20Minsung%20Hwang%20and%20Joyce%20Jiyoung%20Whang&entry.1292438233=%20%20While%20a%20number%20of%20knowledge%20graph%20representation%20learning%20%28KGRL%29%20methods%20have%0Abeen%20proposed%20over%20the%20past%20decade%2C%20very%20few%20theoretical%20analyses%20have%20been%0Aconducted%20on%20them.%20In%20this%20paper%2C%20we%20present%20the%20first%20PAC-Bayesian%0Ageneralization%20bounds%20for%20KGRL%20methods.%20To%20analyze%20a%20broad%20class%20of%20KGRL%0Amodels%2C%20we%20propose%20a%20generic%20framework%20named%20ReED%20%28Relation-aware%0AEncoder-Decoder%29%2C%20which%20consists%20of%20a%20relation-aware%20message%20passing%20encoder%0Aand%20a%20triplet%20classification%20decoder.%20Our%20ReED%20framework%20can%20express%20at%20least%0A15%20different%20existing%20KGRL%20models%2C%20including%20not%20only%20graph%20neural%0Anetwork-based%20models%20such%20as%20R-GCN%20and%20CompGCN%20but%20also%20shallow-architecture%0Amodels%20such%20as%20RotatE%20and%20ANALOGY.%20Our%20generalization%20bounds%20for%20the%20ReED%0Aframework%20provide%20theoretical%20grounds%20for%20the%20commonly%20used%20tricks%20in%20KGRL%2C%0Ae.g.%2C%20parameter-sharing%20and%20weight%20normalization%20schemes%2C%20and%20guide%20desirable%0Adesign%20choices%20for%20practical%20KGRL%20methods.%20We%20empirically%20show%20that%20the%0Acritical%20factors%20in%20our%20generalization%20bounds%20can%20explain%20actual%20generalization%0Aerrors%20on%20three%20real-world%20knowledge%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06418v2&entry.124074799=Read"},
{"title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion", "author": "Jiayi Yao and Hanchen Li and Yuhan Liu and Siddhant Ray and Yihua Cheng and Qizheng Zhang and Kuntai Du and Shan Lu and Junchen Jiang", "abstract": "  Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, and when they are not,\ntheir precomputed KV caches cannot be directly used since they ignore the\ntext's cross-attention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n  This paper tackles just one question: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? We present CacheBlend, a scheme that reuses the\npre-computed KV caches, regardless prefix or not, and selectively recomputes\nthe KV values of a small subset of tokens to partially update each reused KV\ncache. In the meantime,the small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,allowing\nCacheBlend to store KV caches in slower devices with more storage capacity\nwhile retrieving them without increasing the inference delay. By comparing\nCacheBlend with the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full\nKV recompute, without compromising generation quality or incurring more storage\ncost.\n", "link": "http://arxiv.org/abs/2405.16444v2", "date": "2024-06-03", "relevancy": 1.948, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5115}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CacheBlend%3A%20Fast%20Large%20Language%20Model%20Serving%20for%20RAG%20with%20Cached%0A%20%20Knowledge%20Fusion&body=Title%3A%20CacheBlend%3A%20Fast%20Large%20Language%20Model%20Serving%20for%20RAG%20with%20Cached%0A%20%20Knowledge%20Fusion%0AAuthor%3A%20Jiayi%20Yao%20and%20Hanchen%20Li%20and%20Yuhan%20Liu%20and%20Siddhant%20Ray%20and%20Yihua%20Cheng%20and%20Qizheng%20Zhang%20and%20Kuntai%20Du%20and%20Shan%20Lu%20and%20Junchen%20Jiang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20incorporate%20multiple%20text%20chunks%20in%20their%0Ainputs%20to%20provide%20the%20necessary%20contexts.%20To%20speed%20up%20the%20prefill%20of%20the%20long%0ALLM%20inputs%2C%20one%20can%20pre-compute%20the%20KV%20cache%20of%20a%20text%20and%20re-use%20the%20KV%20cache%0Awhen%20the%20context%20is%20reused%20as%20the%20prefix%20of%20another%20LLM%20input.%20However%2C%20the%0Areused%20text%20chunks%20are%20not%20always%20the%20input%20prefix%2C%20and%20when%20they%20are%20not%2C%0Atheir%20precomputed%20KV%20caches%20cannot%20be%20directly%20used%20since%20they%20ignore%20the%0Atext%27s%20cross-attention%20with%20the%20preceding%20text%20in%20the%20LLM%20input.%20Thus%2C%20the%0Abenefits%20of%20reusing%20KV%20caches%20remain%20largely%20unrealized.%0A%20%20This%20paper%20tackles%20just%20one%20question%3A%20when%20an%20LLM%20input%20contains%20multiple%0Atext%20chunks%2C%20how%20to%20quickly%20combine%20their%20precomputed%20KV%20caches%20in%20order%20to%0Aachieve%20the%20same%20generation%20quality%20as%20the%20expensive%20full%20prefill%20%28i.e.%2C%0Awithout%20reusing%20KV%20cache%29%3F%20We%20present%20CacheBlend%2C%20a%20scheme%20that%20reuses%20the%0Apre-computed%20KV%20caches%2C%20regardless%20prefix%20or%20not%2C%20and%20selectively%20recomputes%0Athe%20KV%20values%20of%20a%20small%20subset%20of%20tokens%20to%20partially%20update%20each%20reused%20KV%0Acache.%20In%20the%20meantime%2Cthe%20small%20extra%20delay%20for%20recomputing%20some%20tokens%20can%20be%0Apipelined%20with%20the%20retrieval%20of%20KV%20caches%20within%20the%20same%20job%2Callowing%0ACacheBlend%20to%20store%20KV%20caches%20in%20slower%20devices%20with%20more%20storage%20capacity%0Awhile%20retrieving%20them%20without%20increasing%20the%20inference%20delay.%20By%20comparing%0ACacheBlend%20with%20the%20state-of-the-art%20KV%20cache%20reusing%20schemes%20on%20three%0Aopen-source%20LLMs%20of%20various%20sizes%20and%20four%20popular%20benchmark%20datasets%20of%0Adifferent%20tasks%2C%20we%20show%20that%20CacheBlend%20reduces%20time-to-first-token%20%28TTFT%29%20by%0A2.2-3.3X%20and%20increases%20the%20inference%20throughput%20by%202.8-5X%2C%20compared%20with%20full%0AKV%20recompute%2C%20without%20compromising%20generation%20quality%20or%20incurring%20more%20storage%0Acost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCacheBlend%253A%2520Fast%2520Large%2520Language%2520Model%2520Serving%2520for%2520RAG%2520with%2520Cached%250A%2520%2520Knowledge%2520Fusion%26entry.906535625%3DJiayi%2520Yao%2520and%2520Hanchen%2520Li%2520and%2520Yuhan%2520Liu%2520and%2520Siddhant%2520Ray%2520and%2520Yihua%2520Cheng%2520and%2520Qizheng%2520Zhang%2520and%2520Kuntai%2520Du%2520and%2520Shan%2520Lu%2520and%2520Junchen%2520Jiang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520incorporate%2520multiple%2520text%2520chunks%2520in%2520their%250Ainputs%2520to%2520provide%2520the%2520necessary%2520contexts.%2520To%2520speed%2520up%2520the%2520prefill%2520of%2520the%2520long%250ALLM%2520inputs%252C%2520one%2520can%2520pre-compute%2520the%2520KV%2520cache%2520of%2520a%2520text%2520and%2520re-use%2520the%2520KV%2520cache%250Awhen%2520the%2520context%2520is%2520reused%2520as%2520the%2520prefix%2520of%2520another%2520LLM%2520input.%2520However%252C%2520the%250Areused%2520text%2520chunks%2520are%2520not%2520always%2520the%2520input%2520prefix%252C%2520and%2520when%2520they%2520are%2520not%252C%250Atheir%2520precomputed%2520KV%2520caches%2520cannot%2520be%2520directly%2520used%2520since%2520they%2520ignore%2520the%250Atext%2527s%2520cross-attention%2520with%2520the%2520preceding%2520text%2520in%2520the%2520LLM%2520input.%2520Thus%252C%2520the%250Abenefits%2520of%2520reusing%2520KV%2520caches%2520remain%2520largely%2520unrealized.%250A%2520%2520This%2520paper%2520tackles%2520just%2520one%2520question%253A%2520when%2520an%2520LLM%2520input%2520contains%2520multiple%250Atext%2520chunks%252C%2520how%2520to%2520quickly%2520combine%2520their%2520precomputed%2520KV%2520caches%2520in%2520order%2520to%250Aachieve%2520the%2520same%2520generation%2520quality%2520as%2520the%2520expensive%2520full%2520prefill%2520%2528i.e.%252C%250Awithout%2520reusing%2520KV%2520cache%2529%253F%2520We%2520present%2520CacheBlend%252C%2520a%2520scheme%2520that%2520reuses%2520the%250Apre-computed%2520KV%2520caches%252C%2520regardless%2520prefix%2520or%2520not%252C%2520and%2520selectively%2520recomputes%250Athe%2520KV%2520values%2520of%2520a%2520small%2520subset%2520of%2520tokens%2520to%2520partially%2520update%2520each%2520reused%2520KV%250Acache.%2520In%2520the%2520meantime%252Cthe%2520small%2520extra%2520delay%2520for%2520recomputing%2520some%2520tokens%2520can%2520be%250Apipelined%2520with%2520the%2520retrieval%2520of%2520KV%2520caches%2520within%2520the%2520same%2520job%252Callowing%250ACacheBlend%2520to%2520store%2520KV%2520caches%2520in%2520slower%2520devices%2520with%2520more%2520storage%2520capacity%250Awhile%2520retrieving%2520them%2520without%2520increasing%2520the%2520inference%2520delay.%2520By%2520comparing%250ACacheBlend%2520with%2520the%2520state-of-the-art%2520KV%2520cache%2520reusing%2520schemes%2520on%2520three%250Aopen-source%2520LLMs%2520of%2520various%2520sizes%2520and%2520four%2520popular%2520benchmark%2520datasets%2520of%250Adifferent%2520tasks%252C%2520we%2520show%2520that%2520CacheBlend%2520reduces%2520time-to-first-token%2520%2528TTFT%2529%2520by%250A2.2-3.3X%2520and%2520increases%2520the%2520inference%2520throughput%2520by%25202.8-5X%252C%2520compared%2520with%2520full%250AKV%2520recompute%252C%2520without%2520compromising%2520generation%2520quality%2520or%2520incurring%2520more%2520storage%250Acost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CacheBlend%3A%20Fast%20Large%20Language%20Model%20Serving%20for%20RAG%20with%20Cached%0A%20%20Knowledge%20Fusion&entry.906535625=Jiayi%20Yao%20and%20Hanchen%20Li%20and%20Yuhan%20Liu%20and%20Siddhant%20Ray%20and%20Yihua%20Cheng%20and%20Qizheng%20Zhang%20and%20Kuntai%20Du%20and%20Shan%20Lu%20and%20Junchen%20Jiang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20incorporate%20multiple%20text%20chunks%20in%20their%0Ainputs%20to%20provide%20the%20necessary%20contexts.%20To%20speed%20up%20the%20prefill%20of%20the%20long%0ALLM%20inputs%2C%20one%20can%20pre-compute%20the%20KV%20cache%20of%20a%20text%20and%20re-use%20the%20KV%20cache%0Awhen%20the%20context%20is%20reused%20as%20the%20prefix%20of%20another%20LLM%20input.%20However%2C%20the%0Areused%20text%20chunks%20are%20not%20always%20the%20input%20prefix%2C%20and%20when%20they%20are%20not%2C%0Atheir%20precomputed%20KV%20caches%20cannot%20be%20directly%20used%20since%20they%20ignore%20the%0Atext%27s%20cross-attention%20with%20the%20preceding%20text%20in%20the%20LLM%20input.%20Thus%2C%20the%0Abenefits%20of%20reusing%20KV%20caches%20remain%20largely%20unrealized.%0A%20%20This%20paper%20tackles%20just%20one%20question%3A%20when%20an%20LLM%20input%20contains%20multiple%0Atext%20chunks%2C%20how%20to%20quickly%20combine%20their%20precomputed%20KV%20caches%20in%20order%20to%0Aachieve%20the%20same%20generation%20quality%20as%20the%20expensive%20full%20prefill%20%28i.e.%2C%0Awithout%20reusing%20KV%20cache%29%3F%20We%20present%20CacheBlend%2C%20a%20scheme%20that%20reuses%20the%0Apre-computed%20KV%20caches%2C%20regardless%20prefix%20or%20not%2C%20and%20selectively%20recomputes%0Athe%20KV%20values%20of%20a%20small%20subset%20of%20tokens%20to%20partially%20update%20each%20reused%20KV%0Acache.%20In%20the%20meantime%2Cthe%20small%20extra%20delay%20for%20recomputing%20some%20tokens%20can%20be%0Apipelined%20with%20the%20retrieval%20of%20KV%20caches%20within%20the%20same%20job%2Callowing%0ACacheBlend%20to%20store%20KV%20caches%20in%20slower%20devices%20with%20more%20storage%20capacity%0Awhile%20retrieving%20them%20without%20increasing%20the%20inference%20delay.%20By%20comparing%0ACacheBlend%20with%20the%20state-of-the-art%20KV%20cache%20reusing%20schemes%20on%20three%0Aopen-source%20LLMs%20of%20various%20sizes%20and%20four%20popular%20benchmark%20datasets%20of%0Adifferent%20tasks%2C%20we%20show%20that%20CacheBlend%20reduces%20time-to-first-token%20%28TTFT%29%20by%0A2.2-3.3X%20and%20increases%20the%20inference%20throughput%20by%202.8-5X%2C%20compared%20with%20full%0AKV%20recompute%2C%20without%20compromising%20generation%20quality%20or%20incurring%20more%20storage%0Acost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16444v2&entry.124074799=Read"},
{"title": "How Flawed Is ECE? An Analysis via Logit Smoothing", "author": "Muthu Chidambaram and Holden Lee and Colin McSwiggen and Semon Rezchikov", "abstract": "  Informally, a model is calibrated if its predictions are correct with a\nprobability that matches the confidence of the prediction. By far the most\ncommon method in the literature for measuring calibration is the expected\ncalibration error (ECE). Recent work, however, has pointed out drawbacks of\nECE, such as the fact that it is discontinuous in the space of predictors. In\nthis work, we ask: how fundamental are these issues, and what are their impacts\non existing results? Towards this end, we completely characterize the\ndiscontinuities of ECE with respect to general probability measures on Polish\nspaces. We then use the nature of these discontinuities to motivate a novel\ncontinuous, easily estimated miscalibration metric, which we term\nLogit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained\nimage classification models, we show in initial experiments that binned ECE\nclosely tracks LS-ECE, indicating that the theoretical pathologies of ECE may\nbe avoidable in practice.\n", "link": "http://arxiv.org/abs/2402.10046v2", "date": "2024-06-03", "relevancy": 1.943, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.502}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4877}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Flawed%20Is%20ECE%3F%20An%20Analysis%20via%20Logit%20Smoothing&body=Title%3A%20How%20Flawed%20Is%20ECE%3F%20An%20Analysis%20via%20Logit%20Smoothing%0AAuthor%3A%20Muthu%20Chidambaram%20and%20Holden%20Lee%20and%20Colin%20McSwiggen%20and%20Semon%20Rezchikov%0AAbstract%3A%20%20%20Informally%2C%20a%20model%20is%20calibrated%20if%20its%20predictions%20are%20correct%20with%20a%0Aprobability%20that%20matches%20the%20confidence%20of%20the%20prediction.%20By%20far%20the%20most%0Acommon%20method%20in%20the%20literature%20for%20measuring%20calibration%20is%20the%20expected%0Acalibration%20error%20%28ECE%29.%20Recent%20work%2C%20however%2C%20has%20pointed%20out%20drawbacks%20of%0AECE%2C%20such%20as%20the%20fact%20that%20it%20is%20discontinuous%20in%20the%20space%20of%20predictors.%20In%0Athis%20work%2C%20we%20ask%3A%20how%20fundamental%20are%20these%20issues%2C%20and%20what%20are%20their%20impacts%0Aon%20existing%20results%3F%20Towards%20this%20end%2C%20we%20completely%20characterize%20the%0Adiscontinuities%20of%20ECE%20with%20respect%20to%20general%20probability%20measures%20on%20Polish%0Aspaces.%20We%20then%20use%20the%20nature%20of%20these%20discontinuities%20to%20motivate%20a%20novel%0Acontinuous%2C%20easily%20estimated%20miscalibration%20metric%2C%20which%20we%20term%0ALogit-Smoothed%20ECE%20%28LS-ECE%29.%20By%20comparing%20the%20ECE%20and%20LS-ECE%20of%20pre-trained%0Aimage%20classification%20models%2C%20we%20show%20in%20initial%20experiments%20that%20binned%20ECE%0Aclosely%20tracks%20LS-ECE%2C%20indicating%20that%20the%20theoretical%20pathologies%20of%20ECE%20may%0Abe%20avoidable%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Flawed%2520Is%2520ECE%253F%2520An%2520Analysis%2520via%2520Logit%2520Smoothing%26entry.906535625%3DMuthu%2520Chidambaram%2520and%2520Holden%2520Lee%2520and%2520Colin%2520McSwiggen%2520and%2520Semon%2520Rezchikov%26entry.1292438233%3D%2520%2520Informally%252C%2520a%2520model%2520is%2520calibrated%2520if%2520its%2520predictions%2520are%2520correct%2520with%2520a%250Aprobability%2520that%2520matches%2520the%2520confidence%2520of%2520the%2520prediction.%2520By%2520far%2520the%2520most%250Acommon%2520method%2520in%2520the%2520literature%2520for%2520measuring%2520calibration%2520is%2520the%2520expected%250Acalibration%2520error%2520%2528ECE%2529.%2520Recent%2520work%252C%2520however%252C%2520has%2520pointed%2520out%2520drawbacks%2520of%250AECE%252C%2520such%2520as%2520the%2520fact%2520that%2520it%2520is%2520discontinuous%2520in%2520the%2520space%2520of%2520predictors.%2520In%250Athis%2520work%252C%2520we%2520ask%253A%2520how%2520fundamental%2520are%2520these%2520issues%252C%2520and%2520what%2520are%2520their%2520impacts%250Aon%2520existing%2520results%253F%2520Towards%2520this%2520end%252C%2520we%2520completely%2520characterize%2520the%250Adiscontinuities%2520of%2520ECE%2520with%2520respect%2520to%2520general%2520probability%2520measures%2520on%2520Polish%250Aspaces.%2520We%2520then%2520use%2520the%2520nature%2520of%2520these%2520discontinuities%2520to%2520motivate%2520a%2520novel%250Acontinuous%252C%2520easily%2520estimated%2520miscalibration%2520metric%252C%2520which%2520we%2520term%250ALogit-Smoothed%2520ECE%2520%2528LS-ECE%2529.%2520By%2520comparing%2520the%2520ECE%2520and%2520LS-ECE%2520of%2520pre-trained%250Aimage%2520classification%2520models%252C%2520we%2520show%2520in%2520initial%2520experiments%2520that%2520binned%2520ECE%250Aclosely%2520tracks%2520LS-ECE%252C%2520indicating%2520that%2520the%2520theoretical%2520pathologies%2520of%2520ECE%2520may%250Abe%2520avoidable%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Flawed%20Is%20ECE%3F%20An%20Analysis%20via%20Logit%20Smoothing&entry.906535625=Muthu%20Chidambaram%20and%20Holden%20Lee%20and%20Colin%20McSwiggen%20and%20Semon%20Rezchikov&entry.1292438233=%20%20Informally%2C%20a%20model%20is%20calibrated%20if%20its%20predictions%20are%20correct%20with%20a%0Aprobability%20that%20matches%20the%20confidence%20of%20the%20prediction.%20By%20far%20the%20most%0Acommon%20method%20in%20the%20literature%20for%20measuring%20calibration%20is%20the%20expected%0Acalibration%20error%20%28ECE%29.%20Recent%20work%2C%20however%2C%20has%20pointed%20out%20drawbacks%20of%0AECE%2C%20such%20as%20the%20fact%20that%20it%20is%20discontinuous%20in%20the%20space%20of%20predictors.%20In%0Athis%20work%2C%20we%20ask%3A%20how%20fundamental%20are%20these%20issues%2C%20and%20what%20are%20their%20impacts%0Aon%20existing%20results%3F%20Towards%20this%20end%2C%20we%20completely%20characterize%20the%0Adiscontinuities%20of%20ECE%20with%20respect%20to%20general%20probability%20measures%20on%20Polish%0Aspaces.%20We%20then%20use%20the%20nature%20of%20these%20discontinuities%20to%20motivate%20a%20novel%0Acontinuous%2C%20easily%20estimated%20miscalibration%20metric%2C%20which%20we%20term%0ALogit-Smoothed%20ECE%20%28LS-ECE%29.%20By%20comparing%20the%20ECE%20and%20LS-ECE%20of%20pre-trained%0Aimage%20classification%20models%2C%20we%20show%20in%20initial%20experiments%20that%20binned%20ECE%0Aclosely%20tracks%20LS-ECE%2C%20indicating%20that%20the%20theoretical%20pathologies%20of%20ECE%20may%0Abe%20avoidable%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10046v2&entry.124074799=Read"},
{"title": "Towards Faithful and Robust LLM Specialists for Evidence-Based\n  Question-Answering", "author": "Tobias Schimanski and Jingwei Ni and Mathias Kraus and Elliott Ash and Markus Leippold", "abstract": "  Advances towards more faithful and traceable answers of Large Language Models\n(LLMs) are crucial for various research and practical endeavors. One avenue in\nreaching this goal is basing the answers on reliable sources. However, this\nEvidence-Based QA has proven to work insufficiently with LLMs in terms of\nciting the correct sources (source quality) and truthfully representing the\ninformation within sources (answer attributability). In this work, we\nsystematically investigate how to robustly fine-tune LLMs for better source\nquality and answer attributability. Specifically, we introduce a data\ngeneration pipeline with automated data quality filters, which can synthesize\ndiversified high-quality training and testing data at scale. We further\nintroduce four test sets to benchmark the robustness of fine-tuned specialist\nmodels. Extensive evaluation shows that fine-tuning on synthetic data improves\nperformance on both in- and out-of-distribution. Furthermore, we show that data\nquality, which can be drastically improved by proposed quality filters, matters\nmore than quantity in improving Evidence-Based QA.\n", "link": "http://arxiv.org/abs/2402.08277v5", "date": "2024-06-03", "relevancy": 1.9365, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5656}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering&body=Title%3A%20Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering%0AAuthor%3A%20Tobias%20Schimanski%20and%20Jingwei%20Ni%20and%20Mathias%20Kraus%20and%20Elliott%20Ash%20and%20Markus%20Leippold%0AAbstract%3A%20%20%20Advances%20towards%20more%20faithful%20and%20traceable%20answers%20of%20Large%20Language%20Models%0A%28LLMs%29%20are%20crucial%20for%20various%20research%20and%20practical%20endeavors.%20One%20avenue%20in%0Areaching%20this%20goal%20is%20basing%20the%20answers%20on%20reliable%20sources.%20However%2C%20this%0AEvidence-Based%20QA%20has%20proven%20to%20work%20insufficiently%20with%20LLMs%20in%20terms%20of%0Aciting%20the%20correct%20sources%20%28source%20quality%29%20and%20truthfully%20representing%20the%0Ainformation%20within%20sources%20%28answer%20attributability%29.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20how%20to%20robustly%20fine-tune%20LLMs%20for%20better%20source%0Aquality%20and%20answer%20attributability.%20Specifically%2C%20we%20introduce%20a%20data%0Ageneration%20pipeline%20with%20automated%20data%20quality%20filters%2C%20which%20can%20synthesize%0Adiversified%20high-quality%20training%20and%20testing%20data%20at%20scale.%20We%20further%0Aintroduce%20four%20test%20sets%20to%20benchmark%20the%20robustness%20of%20fine-tuned%20specialist%0Amodels.%20Extensive%20evaluation%20shows%20that%20fine-tuning%20on%20synthetic%20data%20improves%0Aperformance%20on%20both%20in-%20and%20out-of-distribution.%20Furthermore%2C%20we%20show%20that%20data%0Aquality%2C%20which%20can%20be%20drastically%20improved%20by%20proposed%20quality%20filters%2C%20matters%0Amore%20than%20quantity%20in%20improving%20Evidence-Based%20QA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08277v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520and%2520Robust%2520LLM%2520Specialists%2520for%2520Evidence-Based%250A%2520%2520Question-Answering%26entry.906535625%3DTobias%2520Schimanski%2520and%2520Jingwei%2520Ni%2520and%2520Mathias%2520Kraus%2520and%2520Elliott%2520Ash%2520and%2520Markus%2520Leippold%26entry.1292438233%3D%2520%2520Advances%2520towards%2520more%2520faithful%2520and%2520traceable%2520answers%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520are%2520crucial%2520for%2520various%2520research%2520and%2520practical%2520endeavors.%2520One%2520avenue%2520in%250Areaching%2520this%2520goal%2520is%2520basing%2520the%2520answers%2520on%2520reliable%2520sources.%2520However%252C%2520this%250AEvidence-Based%2520QA%2520has%2520proven%2520to%2520work%2520insufficiently%2520with%2520LLMs%2520in%2520terms%2520of%250Aciting%2520the%2520correct%2520sources%2520%2528source%2520quality%2529%2520and%2520truthfully%2520representing%2520the%250Ainformation%2520within%2520sources%2520%2528answer%2520attributability%2529.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520how%2520to%2520robustly%2520fine-tune%2520LLMs%2520for%2520better%2520source%250Aquality%2520and%2520answer%2520attributability.%2520Specifically%252C%2520we%2520introduce%2520a%2520data%250Ageneration%2520pipeline%2520with%2520automated%2520data%2520quality%2520filters%252C%2520which%2520can%2520synthesize%250Adiversified%2520high-quality%2520training%2520and%2520testing%2520data%2520at%2520scale.%2520We%2520further%250Aintroduce%2520four%2520test%2520sets%2520to%2520benchmark%2520the%2520robustness%2520of%2520fine-tuned%2520specialist%250Amodels.%2520Extensive%2520evaluation%2520shows%2520that%2520fine-tuning%2520on%2520synthetic%2520data%2520improves%250Aperformance%2520on%2520both%2520in-%2520and%2520out-of-distribution.%2520Furthermore%252C%2520we%2520show%2520that%2520data%250Aquality%252C%2520which%2520can%2520be%2520drastically%2520improved%2520by%2520proposed%2520quality%2520filters%252C%2520matters%250Amore%2520than%2520quantity%2520in%2520improving%2520Evidence-Based%2520QA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08277v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering&entry.906535625=Tobias%20Schimanski%20and%20Jingwei%20Ni%20and%20Mathias%20Kraus%20and%20Elliott%20Ash%20and%20Markus%20Leippold&entry.1292438233=%20%20Advances%20towards%20more%20faithful%20and%20traceable%20answers%20of%20Large%20Language%20Models%0A%28LLMs%29%20are%20crucial%20for%20various%20research%20and%20practical%20endeavors.%20One%20avenue%20in%0Areaching%20this%20goal%20is%20basing%20the%20answers%20on%20reliable%20sources.%20However%2C%20this%0AEvidence-Based%20QA%20has%20proven%20to%20work%20insufficiently%20with%20LLMs%20in%20terms%20of%0Aciting%20the%20correct%20sources%20%28source%20quality%29%20and%20truthfully%20representing%20the%0Ainformation%20within%20sources%20%28answer%20attributability%29.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20how%20to%20robustly%20fine-tune%20LLMs%20for%20better%20source%0Aquality%20and%20answer%20attributability.%20Specifically%2C%20we%20introduce%20a%20data%0Ageneration%20pipeline%20with%20automated%20data%20quality%20filters%2C%20which%20can%20synthesize%0Adiversified%20high-quality%20training%20and%20testing%20data%20at%20scale.%20We%20further%0Aintroduce%20four%20test%20sets%20to%20benchmark%20the%20robustness%20of%20fine-tuned%20specialist%0Amodels.%20Extensive%20evaluation%20shows%20that%20fine-tuning%20on%20synthetic%20data%20improves%0Aperformance%20on%20both%20in-%20and%20out-of-distribution.%20Furthermore%2C%20we%20show%20that%20data%0Aquality%2C%20which%20can%20be%20drastically%20improved%20by%20proposed%20quality%20filters%2C%20matters%0Amore%20than%20quantity%20in%20improving%20Evidence-Based%20QA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08277v5&entry.124074799=Read"},
{"title": "Aligner: Efficient Alignment by Learning to Correct", "author": "Jiaming Ji and Boyuan Chen and Hantao Lou and Donghai Hong and Borong Zhang and Xuehai Pan and Juntao Dai and Tianyi Qiu and Yaodong Yang", "abstract": "  With the rapid development of large language models (LLMs) and ever-evolving\npractical requirements, finding an efficient and effective alignment method has\nnever been more critical. However, the tension between the complexity of\ncurrent alignment methods and the need for rapid iteration in deployment\nscenarios necessitates the development of a model-agnostic alignment approach\nthat can operate under these constraints. In this paper, we introduce Aligner,\na novel and simple alignment paradigm that learns the correctional residuals\nbetween preferred and dispreferred answers using a small model. Designed as a\nmodel-agnostic, plug-and-play module, Aligner can be directly applied to\nvarious open-source and API-based models with only one-off training, making it\nsuitable for rapid iteration. Notably, Aligner can be applied to any powerful,\nlarge-scale upstream models. Moreover, it can even iteratively bootstrap the\nupstream models using corrected responses as synthetic human preference data,\nbreaking through the model's performance ceiling. Our experiments demonstrate\nperformance improvements by deploying the same Aligner model across 11\ndifferent LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and\nhonesty). Specifically, Aligner-7B has achieved an average improvement of\n68.9\\% in helpfulness and 23.8\\% in harmlessness across the tested LLMs while\nalso effectively reducing hallucination. In the Alpaca-Eval leaderboard,\nstacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0\\% to\n58.3\\%, surpassing GPT-4 Omni's 57.5\\% Win Rate (community report).\n", "link": "http://arxiv.org/abs/2402.02416v3", "date": "2024-06-03", "relevancy": 1.9351, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligner%3A%20Efficient%20Alignment%20by%20Learning%20to%20Correct&body=Title%3A%20Aligner%3A%20Efficient%20Alignment%20by%20Learning%20to%20Correct%0AAuthor%3A%20Jiaming%20Ji%20and%20Boyuan%20Chen%20and%20Hantao%20Lou%20and%20Donghai%20Hong%20and%20Borong%20Zhang%20and%20Xuehai%20Pan%20and%20Juntao%20Dai%20and%20Tianyi%20Qiu%20and%20Yaodong%20Yang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20and%20ever-evolving%0Apractical%20requirements%2C%20finding%20an%20efficient%20and%20effective%20alignment%20method%20has%0Anever%20been%20more%20critical.%20However%2C%20the%20tension%20between%20the%20complexity%20of%0Acurrent%20alignment%20methods%20and%20the%20need%20for%20rapid%20iteration%20in%20deployment%0Ascenarios%20necessitates%20the%20development%20of%20a%20model-agnostic%20alignment%20approach%0Athat%20can%20operate%20under%20these%20constraints.%20In%20this%20paper%2C%20we%20introduce%20Aligner%2C%0Aa%20novel%20and%20simple%20alignment%20paradigm%20that%20learns%20the%20correctional%20residuals%0Abetween%20preferred%20and%20dispreferred%20answers%20using%20a%20small%20model.%20Designed%20as%20a%0Amodel-agnostic%2C%20plug-and-play%20module%2C%20Aligner%20can%20be%20directly%20applied%20to%0Avarious%20open-source%20and%20API-based%20models%20with%20only%20one-off%20training%2C%20making%20it%0Asuitable%20for%20rapid%20iteration.%20Notably%2C%20Aligner%20can%20be%20applied%20to%20any%20powerful%2C%0Alarge-scale%20upstream%20models.%20Moreover%2C%20it%20can%20even%20iteratively%20bootstrap%20the%0Aupstream%20models%20using%20corrected%20responses%20as%20synthetic%20human%20preference%20data%2C%0Abreaking%20through%20the%20model%27s%20performance%20ceiling.%20Our%20experiments%20demonstrate%0Aperformance%20improvements%20by%20deploying%20the%20same%20Aligner%20model%20across%2011%0Adifferent%20LLMs%2C%20evaluated%20on%20the%203H%20dimensions%20%28helpfulness%2C%20harmlessness%2C%20and%0Ahonesty%29.%20Specifically%2C%20Aligner-7B%20has%20achieved%20an%20average%20improvement%20of%0A68.9%5C%25%20in%20helpfulness%20and%2023.8%5C%25%20in%20harmlessness%20across%20the%20tested%20LLMs%20while%0Aalso%20effectively%20reducing%20hallucination.%20In%20the%20Alpaca-Eval%20leaderboard%2C%0Astacking%20Aligner-2B%20on%20GPT-4%20Turbo%20improved%20its%20LC%20Win%20Rate%20from%2055.0%5C%25%20to%0A58.3%5C%25%2C%20surpassing%20GPT-4%20Omni%27s%2057.5%5C%25%20Win%20Rate%20%28community%20report%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02416v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligner%253A%2520Efficient%2520Alignment%2520by%2520Learning%2520to%2520Correct%26entry.906535625%3DJiaming%2520Ji%2520and%2520Boyuan%2520Chen%2520and%2520Hantao%2520Lou%2520and%2520Donghai%2520Hong%2520and%2520Borong%2520Zhang%2520and%2520Xuehai%2520Pan%2520and%2520Juntao%2520Dai%2520and%2520Tianyi%2520Qiu%2520and%2520Yaodong%2520Yang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520ever-evolving%250Apractical%2520requirements%252C%2520finding%2520an%2520efficient%2520and%2520effective%2520alignment%2520method%2520has%250Anever%2520been%2520more%2520critical.%2520However%252C%2520the%2520tension%2520between%2520the%2520complexity%2520of%250Acurrent%2520alignment%2520methods%2520and%2520the%2520need%2520for%2520rapid%2520iteration%2520in%2520deployment%250Ascenarios%2520necessitates%2520the%2520development%2520of%2520a%2520model-agnostic%2520alignment%2520approach%250Athat%2520can%2520operate%2520under%2520these%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Aligner%252C%250Aa%2520novel%2520and%2520simple%2520alignment%2520paradigm%2520that%2520learns%2520the%2520correctional%2520residuals%250Abetween%2520preferred%2520and%2520dispreferred%2520answers%2520using%2520a%2520small%2520model.%2520Designed%2520as%2520a%250Amodel-agnostic%252C%2520plug-and-play%2520module%252C%2520Aligner%2520can%2520be%2520directly%2520applied%2520to%250Avarious%2520open-source%2520and%2520API-based%2520models%2520with%2520only%2520one-off%2520training%252C%2520making%2520it%250Asuitable%2520for%2520rapid%2520iteration.%2520Notably%252C%2520Aligner%2520can%2520be%2520applied%2520to%2520any%2520powerful%252C%250Alarge-scale%2520upstream%2520models.%2520Moreover%252C%2520it%2520can%2520even%2520iteratively%2520bootstrap%2520the%250Aupstream%2520models%2520using%2520corrected%2520responses%2520as%2520synthetic%2520human%2520preference%2520data%252C%250Abreaking%2520through%2520the%2520model%2527s%2520performance%2520ceiling.%2520Our%2520experiments%2520demonstrate%250Aperformance%2520improvements%2520by%2520deploying%2520the%2520same%2520Aligner%2520model%2520across%252011%250Adifferent%2520LLMs%252C%2520evaluated%2520on%2520the%25203H%2520dimensions%2520%2528helpfulness%252C%2520harmlessness%252C%2520and%250Ahonesty%2529.%2520Specifically%252C%2520Aligner-7B%2520has%2520achieved%2520an%2520average%2520improvement%2520of%250A68.9%255C%2525%2520in%2520helpfulness%2520and%252023.8%255C%2525%2520in%2520harmlessness%2520across%2520the%2520tested%2520LLMs%2520while%250Aalso%2520effectively%2520reducing%2520hallucination.%2520In%2520the%2520Alpaca-Eval%2520leaderboard%252C%250Astacking%2520Aligner-2B%2520on%2520GPT-4%2520Turbo%2520improved%2520its%2520LC%2520Win%2520Rate%2520from%252055.0%255C%2525%2520to%250A58.3%255C%2525%252C%2520surpassing%2520GPT-4%2520Omni%2527s%252057.5%255C%2525%2520Win%2520Rate%2520%2528community%2520report%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02416v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligner%3A%20Efficient%20Alignment%20by%20Learning%20to%20Correct&entry.906535625=Jiaming%20Ji%20and%20Boyuan%20Chen%20and%20Hantao%20Lou%20and%20Donghai%20Hong%20and%20Borong%20Zhang%20and%20Xuehai%20Pan%20and%20Juntao%20Dai%20and%20Tianyi%20Qiu%20and%20Yaodong%20Yang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20and%20ever-evolving%0Apractical%20requirements%2C%20finding%20an%20efficient%20and%20effective%20alignment%20method%20has%0Anever%20been%20more%20critical.%20However%2C%20the%20tension%20between%20the%20complexity%20of%0Acurrent%20alignment%20methods%20and%20the%20need%20for%20rapid%20iteration%20in%20deployment%0Ascenarios%20necessitates%20the%20development%20of%20a%20model-agnostic%20alignment%20approach%0Athat%20can%20operate%20under%20these%20constraints.%20In%20this%20paper%2C%20we%20introduce%20Aligner%2C%0Aa%20novel%20and%20simple%20alignment%20paradigm%20that%20learns%20the%20correctional%20residuals%0Abetween%20preferred%20and%20dispreferred%20answers%20using%20a%20small%20model.%20Designed%20as%20a%0Amodel-agnostic%2C%20plug-and-play%20module%2C%20Aligner%20can%20be%20directly%20applied%20to%0Avarious%20open-source%20and%20API-based%20models%20with%20only%20one-off%20training%2C%20making%20it%0Asuitable%20for%20rapid%20iteration.%20Notably%2C%20Aligner%20can%20be%20applied%20to%20any%20powerful%2C%0Alarge-scale%20upstream%20models.%20Moreover%2C%20it%20can%20even%20iteratively%20bootstrap%20the%0Aupstream%20models%20using%20corrected%20responses%20as%20synthetic%20human%20preference%20data%2C%0Abreaking%20through%20the%20model%27s%20performance%20ceiling.%20Our%20experiments%20demonstrate%0Aperformance%20improvements%20by%20deploying%20the%20same%20Aligner%20model%20across%2011%0Adifferent%20LLMs%2C%20evaluated%20on%20the%203H%20dimensions%20%28helpfulness%2C%20harmlessness%2C%20and%0Ahonesty%29.%20Specifically%2C%20Aligner-7B%20has%20achieved%20an%20average%20improvement%20of%0A68.9%5C%25%20in%20helpfulness%20and%2023.8%5C%25%20in%20harmlessness%20across%20the%20tested%20LLMs%20while%0Aalso%20effectively%20reducing%20hallucination.%20In%20the%20Alpaca-Eval%20leaderboard%2C%0Astacking%20Aligner-2B%20on%20GPT-4%20Turbo%20improved%20its%20LC%20Win%20Rate%20from%2055.0%5C%25%20to%0A58.3%5C%25%2C%20surpassing%20GPT-4%20Omni%27s%2057.5%5C%25%20Win%20Rate%20%28community%20report%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02416v3&entry.124074799=Read"},
{"title": "Efficient and Generalizable Certified Unlearning: A Hessian-free\n  Recollection Approach", "author": "Xinbao Qiao and Meng Zhang and Ming Tang and Ermin Wei", "abstract": "  Machine unlearning strives to uphold the data owners' right to be forgotten\nby enabling models to selectively forget specific data. Recent advances suggest\nprecomputing and storing statistics extracted from second-order information and\nimplementing unlearning through Newton-style updates. However, the theoretical\nanalysis of these works often depends on restrictive assumptions of convexity\nand smoothness, and those mentioned operations on Hessian matrix are extremely\ncostly. As a result, applying these works to high-dimensional models becomes\nchallenging. In this paper, we propose an efficient Hessian-free certified\nunlearning. We propose to maintain a statistical vector for each data, computed\nthrough affine stochastic recursion approximation of the difference between\nretrained and learned models. Our analysis does not involve inverting Hessian\nand thus can be extended to non-convex non-smooth objectives. Under same\nassumptions, we demonstrate advancements of proposed method beyond the\nstate-of-the-art theoretical studies, in terms of generalization, unlearning\nguarantee, deletion capacity, and computation/storage complexity, and we show\nthat the unlearned model of our proposed approach is close to or same as the\nretrained model. Based on the strategy of recollecting statistics for\nforgetting data, we develop an algorithm that achieves near-instantaneous\nunlearning as it only requires a vector addition operation. Experiments\ndemonstrate that the proposed scheme surpasses existing results by orders of\nmagnitude in terms of time/storage costs, while also enhancing accuracy.\n", "link": "http://arxiv.org/abs/2404.01712v3", "date": "2024-06-03", "relevancy": 1.9253, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4872}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Generalizable%20Certified%20Unlearning%3A%20A%20Hessian-free%0A%20%20Recollection%20Approach&body=Title%3A%20Efficient%20and%20Generalizable%20Certified%20Unlearning%3A%20A%20Hessian-free%0A%20%20Recollection%20Approach%0AAuthor%3A%20Xinbao%20Qiao%20and%20Meng%20Zhang%20and%20Ming%20Tang%20and%20Ermin%20Wei%0AAbstract%3A%20%20%20Machine%20unlearning%20strives%20to%20uphold%20the%20data%20owners%27%20right%20to%20be%20forgotten%0Aby%20enabling%20models%20to%20selectively%20forget%20specific%20data.%20Recent%20advances%20suggest%0Aprecomputing%20and%20storing%20statistics%20extracted%20from%20second-order%20information%20and%0Aimplementing%20unlearning%20through%20Newton-style%20updates.%20However%2C%20the%20theoretical%0Aanalysis%20of%20these%20works%20often%20depends%20on%20restrictive%20assumptions%20of%20convexity%0Aand%20smoothness%2C%20and%20those%20mentioned%20operations%20on%20Hessian%20matrix%20are%20extremely%0Acostly.%20As%20a%20result%2C%20applying%20these%20works%20to%20high-dimensional%20models%20becomes%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20Hessian-free%20certified%0Aunlearning.%20We%20propose%20to%20maintain%20a%20statistical%20vector%20for%20each%20data%2C%20computed%0Athrough%20affine%20stochastic%20recursion%20approximation%20of%20the%20difference%20between%0Aretrained%20and%20learned%20models.%20Our%20analysis%20does%20not%20involve%20inverting%20Hessian%0Aand%20thus%20can%20be%20extended%20to%20non-convex%20non-smooth%20objectives.%20Under%20same%0Aassumptions%2C%20we%20demonstrate%20advancements%20of%20proposed%20method%20beyond%20the%0Astate-of-the-art%20theoretical%20studies%2C%20in%20terms%20of%20generalization%2C%20unlearning%0Aguarantee%2C%20deletion%20capacity%2C%20and%20computation/storage%20complexity%2C%20and%20we%20show%0Athat%20the%20unlearned%20model%20of%20our%20proposed%20approach%20is%20close%20to%20or%20same%20as%20the%0Aretrained%20model.%20Based%20on%20the%20strategy%20of%20recollecting%20statistics%20for%0Aforgetting%20data%2C%20we%20develop%20an%20algorithm%20that%20achieves%20near-instantaneous%0Aunlearning%20as%20it%20only%20requires%20a%20vector%20addition%20operation.%20Experiments%0Ademonstrate%20that%20the%20proposed%20scheme%20surpasses%20existing%20results%20by%20orders%20of%0Amagnitude%20in%20terms%20of%20time/storage%20costs%2C%20while%20also%20enhancing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Generalizable%2520Certified%2520Unlearning%253A%2520A%2520Hessian-free%250A%2520%2520Recollection%2520Approach%26entry.906535625%3DXinbao%2520Qiao%2520and%2520Meng%2520Zhang%2520and%2520Ming%2520Tang%2520and%2520Ermin%2520Wei%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520strives%2520to%2520uphold%2520the%2520data%2520owners%2527%2520right%2520to%2520be%2520forgotten%250Aby%2520enabling%2520models%2520to%2520selectively%2520forget%2520specific%2520data.%2520Recent%2520advances%2520suggest%250Aprecomputing%2520and%2520storing%2520statistics%2520extracted%2520from%2520second-order%2520information%2520and%250Aimplementing%2520unlearning%2520through%2520Newton-style%2520updates.%2520However%252C%2520the%2520theoretical%250Aanalysis%2520of%2520these%2520works%2520often%2520depends%2520on%2520restrictive%2520assumptions%2520of%2520convexity%250Aand%2520smoothness%252C%2520and%2520those%2520mentioned%2520operations%2520on%2520Hessian%2520matrix%2520are%2520extremely%250Acostly.%2520As%2520a%2520result%252C%2520applying%2520these%2520works%2520to%2520high-dimensional%2520models%2520becomes%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520Hessian-free%2520certified%250Aunlearning.%2520We%2520propose%2520to%2520maintain%2520a%2520statistical%2520vector%2520for%2520each%2520data%252C%2520computed%250Athrough%2520affine%2520stochastic%2520recursion%2520approximation%2520of%2520the%2520difference%2520between%250Aretrained%2520and%2520learned%2520models.%2520Our%2520analysis%2520does%2520not%2520involve%2520inverting%2520Hessian%250Aand%2520thus%2520can%2520be%2520extended%2520to%2520non-convex%2520non-smooth%2520objectives.%2520Under%2520same%250Aassumptions%252C%2520we%2520demonstrate%2520advancements%2520of%2520proposed%2520method%2520beyond%2520the%250Astate-of-the-art%2520theoretical%2520studies%252C%2520in%2520terms%2520of%2520generalization%252C%2520unlearning%250Aguarantee%252C%2520deletion%2520capacity%252C%2520and%2520computation/storage%2520complexity%252C%2520and%2520we%2520show%250Athat%2520the%2520unlearned%2520model%2520of%2520our%2520proposed%2520approach%2520is%2520close%2520to%2520or%2520same%2520as%2520the%250Aretrained%2520model.%2520Based%2520on%2520the%2520strategy%2520of%2520recollecting%2520statistics%2520for%250Aforgetting%2520data%252C%2520we%2520develop%2520an%2520algorithm%2520that%2520achieves%2520near-instantaneous%250Aunlearning%2520as%2520it%2520only%2520requires%2520a%2520vector%2520addition%2520operation.%2520Experiments%250Ademonstrate%2520that%2520the%2520proposed%2520scheme%2520surpasses%2520existing%2520results%2520by%2520orders%2520of%250Amagnitude%2520in%2520terms%2520of%2520time/storage%2520costs%252C%2520while%2520also%2520enhancing%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Generalizable%20Certified%20Unlearning%3A%20A%20Hessian-free%0A%20%20Recollection%20Approach&entry.906535625=Xinbao%20Qiao%20and%20Meng%20Zhang%20and%20Ming%20Tang%20and%20Ermin%20Wei&entry.1292438233=%20%20Machine%20unlearning%20strives%20to%20uphold%20the%20data%20owners%27%20right%20to%20be%20forgotten%0Aby%20enabling%20models%20to%20selectively%20forget%20specific%20data.%20Recent%20advances%20suggest%0Aprecomputing%20and%20storing%20statistics%20extracted%20from%20second-order%20information%20and%0Aimplementing%20unlearning%20through%20Newton-style%20updates.%20However%2C%20the%20theoretical%0Aanalysis%20of%20these%20works%20often%20depends%20on%20restrictive%20assumptions%20of%20convexity%0Aand%20smoothness%2C%20and%20those%20mentioned%20operations%20on%20Hessian%20matrix%20are%20extremely%0Acostly.%20As%20a%20result%2C%20applying%20these%20works%20to%20high-dimensional%20models%20becomes%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20Hessian-free%20certified%0Aunlearning.%20We%20propose%20to%20maintain%20a%20statistical%20vector%20for%20each%20data%2C%20computed%0Athrough%20affine%20stochastic%20recursion%20approximation%20of%20the%20difference%20between%0Aretrained%20and%20learned%20models.%20Our%20analysis%20does%20not%20involve%20inverting%20Hessian%0Aand%20thus%20can%20be%20extended%20to%20non-convex%20non-smooth%20objectives.%20Under%20same%0Aassumptions%2C%20we%20demonstrate%20advancements%20of%20proposed%20method%20beyond%20the%0Astate-of-the-art%20theoretical%20studies%2C%20in%20terms%20of%20generalization%2C%20unlearning%0Aguarantee%2C%20deletion%20capacity%2C%20and%20computation/storage%20complexity%2C%20and%20we%20show%0Athat%20the%20unlearned%20model%20of%20our%20proposed%20approach%20is%20close%20to%20or%20same%20as%20the%0Aretrained%20model.%20Based%20on%20the%20strategy%20of%20recollecting%20statistics%20for%0Aforgetting%20data%2C%20we%20develop%20an%20algorithm%20that%20achieves%20near-instantaneous%0Aunlearning%20as%20it%20only%20requires%20a%20vector%20addition%20operation.%20Experiments%0Ademonstrate%20that%20the%20proposed%20scheme%20surpasses%20existing%20results%20by%20orders%20of%0Amagnitude%20in%20terms%20of%20time/storage%20costs%2C%20while%20also%20enhancing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01712v3&entry.124074799=Read"},
{"title": "SyntaxShap: Syntax-aware Explainability Method for Text Generation", "author": "Kenza Amara and Rita Sevastjanova and Mennatallah El-Assady", "abstract": "  To harness the power of large language models in safety-critical domains, we\nneed to ensure the explainability of their predictions. However, despite the\nsignificant attention to model interpretability, there remains an unexplored\ndomain in explaining sequence-to-sequence tasks using methods tailored for\ntextual data. This paper introduces SyntaxShap, a local, model-agnostic\nexplainability method for text generation that takes into consideration the\nsyntax in the text data. The presented work extends Shapley values to account\nfor parsing-based syntactic dependencies. Taking a game theoric approach,\nSyntaxShap only considers coalitions constraint by the dependency tree. We\nadopt a model-based evaluation to compare SyntaxShap and its weighted form to\nstate-of-the-art explainability methods adapted to text generation tasks, using\ndiverse metrics including faithfulness, coherency, and semantic alignment of\nthe explanations to the model. We show that our syntax-aware method produces\nexplanations that help build more faithful and coherent explanations for\npredictions by autoregressive models. Confronted with the misalignment of human\nand AI model reasoning, this paper also highlights the need for cautious\nevaluation strategies in explainable AI.\n", "link": "http://arxiv.org/abs/2402.09259v2", "date": "2024-06-03", "relevancy": 1.9148, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4821}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyntaxShap%3A%20Syntax-aware%20Explainability%20Method%20for%20Text%20Generation&body=Title%3A%20SyntaxShap%3A%20Syntax-aware%20Explainability%20Method%20for%20Text%20Generation%0AAuthor%3A%20Kenza%20Amara%20and%20Rita%20Sevastjanova%20and%20Mennatallah%20El-Assady%0AAbstract%3A%20%20%20To%20harness%20the%20power%20of%20large%20language%20models%20in%20safety-critical%20domains%2C%20we%0Aneed%20to%20ensure%20the%20explainability%20of%20their%20predictions.%20However%2C%20despite%20the%0Asignificant%20attention%20to%20model%20interpretability%2C%20there%20remains%20an%20unexplored%0Adomain%20in%20explaining%20sequence-to-sequence%20tasks%20using%20methods%20tailored%20for%0Atextual%20data.%20This%20paper%20introduces%20SyntaxShap%2C%20a%20local%2C%20model-agnostic%0Aexplainability%20method%20for%20text%20generation%20that%20takes%20into%20consideration%20the%0Asyntax%20in%20the%20text%20data.%20The%20presented%20work%20extends%20Shapley%20values%20to%20account%0Afor%20parsing-based%20syntactic%20dependencies.%20Taking%20a%20game%20theoric%20approach%2C%0ASyntaxShap%20only%20considers%20coalitions%20constraint%20by%20the%20dependency%20tree.%20We%0Aadopt%20a%20model-based%20evaluation%20to%20compare%20SyntaxShap%20and%20its%20weighted%20form%20to%0Astate-of-the-art%20explainability%20methods%20adapted%20to%20text%20generation%20tasks%2C%20using%0Adiverse%20metrics%20including%20faithfulness%2C%20coherency%2C%20and%20semantic%20alignment%20of%0Athe%20explanations%20to%20the%20model.%20We%20show%20that%20our%20syntax-aware%20method%20produces%0Aexplanations%20that%20help%20build%20more%20faithful%20and%20coherent%20explanations%20for%0Apredictions%20by%20autoregressive%20models.%20Confronted%20with%20the%20misalignment%20of%20human%0Aand%20AI%20model%20reasoning%2C%20this%20paper%20also%20highlights%20the%20need%20for%20cautious%0Aevaluation%20strategies%20in%20explainable%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyntaxShap%253A%2520Syntax-aware%2520Explainability%2520Method%2520for%2520Text%2520Generation%26entry.906535625%3DKenza%2520Amara%2520and%2520Rita%2520Sevastjanova%2520and%2520Mennatallah%2520El-Assady%26entry.1292438233%3D%2520%2520To%2520harness%2520the%2520power%2520of%2520large%2520language%2520models%2520in%2520safety-critical%2520domains%252C%2520we%250Aneed%2520to%2520ensure%2520the%2520explainability%2520of%2520their%2520predictions.%2520However%252C%2520despite%2520the%250Asignificant%2520attention%2520to%2520model%2520interpretability%252C%2520there%2520remains%2520an%2520unexplored%250Adomain%2520in%2520explaining%2520sequence-to-sequence%2520tasks%2520using%2520methods%2520tailored%2520for%250Atextual%2520data.%2520This%2520paper%2520introduces%2520SyntaxShap%252C%2520a%2520local%252C%2520model-agnostic%250Aexplainability%2520method%2520for%2520text%2520generation%2520that%2520takes%2520into%2520consideration%2520the%250Asyntax%2520in%2520the%2520text%2520data.%2520The%2520presented%2520work%2520extends%2520Shapley%2520values%2520to%2520account%250Afor%2520parsing-based%2520syntactic%2520dependencies.%2520Taking%2520a%2520game%2520theoric%2520approach%252C%250ASyntaxShap%2520only%2520considers%2520coalitions%2520constraint%2520by%2520the%2520dependency%2520tree.%2520We%250Aadopt%2520a%2520model-based%2520evaluation%2520to%2520compare%2520SyntaxShap%2520and%2520its%2520weighted%2520form%2520to%250Astate-of-the-art%2520explainability%2520methods%2520adapted%2520to%2520text%2520generation%2520tasks%252C%2520using%250Adiverse%2520metrics%2520including%2520faithfulness%252C%2520coherency%252C%2520and%2520semantic%2520alignment%2520of%250Athe%2520explanations%2520to%2520the%2520model.%2520We%2520show%2520that%2520our%2520syntax-aware%2520method%2520produces%250Aexplanations%2520that%2520help%2520build%2520more%2520faithful%2520and%2520coherent%2520explanations%2520for%250Apredictions%2520by%2520autoregressive%2520models.%2520Confronted%2520with%2520the%2520misalignment%2520of%2520human%250Aand%2520AI%2520model%2520reasoning%252C%2520this%2520paper%2520also%2520highlights%2520the%2520need%2520for%2520cautious%250Aevaluation%2520strategies%2520in%2520explainable%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyntaxShap%3A%20Syntax-aware%20Explainability%20Method%20for%20Text%20Generation&entry.906535625=Kenza%20Amara%20and%20Rita%20Sevastjanova%20and%20Mennatallah%20El-Assady&entry.1292438233=%20%20To%20harness%20the%20power%20of%20large%20language%20models%20in%20safety-critical%20domains%2C%20we%0Aneed%20to%20ensure%20the%20explainability%20of%20their%20predictions.%20However%2C%20despite%20the%0Asignificant%20attention%20to%20model%20interpretability%2C%20there%20remains%20an%20unexplored%0Adomain%20in%20explaining%20sequence-to-sequence%20tasks%20using%20methods%20tailored%20for%0Atextual%20data.%20This%20paper%20introduces%20SyntaxShap%2C%20a%20local%2C%20model-agnostic%0Aexplainability%20method%20for%20text%20generation%20that%20takes%20into%20consideration%20the%0Asyntax%20in%20the%20text%20data.%20The%20presented%20work%20extends%20Shapley%20values%20to%20account%0Afor%20parsing-based%20syntactic%20dependencies.%20Taking%20a%20game%20theoric%20approach%2C%0ASyntaxShap%20only%20considers%20coalitions%20constraint%20by%20the%20dependency%20tree.%20We%0Aadopt%20a%20model-based%20evaluation%20to%20compare%20SyntaxShap%20and%20its%20weighted%20form%20to%0Astate-of-the-art%20explainability%20methods%20adapted%20to%20text%20generation%20tasks%2C%20using%0Adiverse%20metrics%20including%20faithfulness%2C%20coherency%2C%20and%20semantic%20alignment%20of%0Athe%20explanations%20to%20the%20model.%20We%20show%20that%20our%20syntax-aware%20method%20produces%0Aexplanations%20that%20help%20build%20more%20faithful%20and%20coherent%20explanations%20for%0Apredictions%20by%20autoregressive%20models.%20Confronted%20with%20the%20misalignment%20of%20human%0Aand%20AI%20model%20reasoning%2C%20this%20paper%20also%20highlights%20the%20need%20for%20cautious%0Aevaluation%20strategies%20in%20explainable%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09259v2&entry.124074799=Read"},
{"title": "Graph External Attention Enhanced Transformer", "author": "Jianqing Liang and Min Chen and Jiye Liang", "abstract": "  The Transformer architecture has recently gained considerable attention in\nthe field of graph representation learning, as it naturally overcomes several\nlimitations of Graph Neural Networks (GNNs) with customized attention\nmechanisms or positional and structural encodings. Despite making some\nprogress, existing works tend to overlook external information of graphs,\nspecifically the correlation between graphs. Intuitively, graphs with similar\nstructures should have similar representations. Therefore, we propose Graph\nExternal Attention (GEA) -- a novel attention mechanism that leverages multiple\nexternal node/edge key-value units to capture inter-graph correlations\nimplicitly. On this basis, we design an effective architecture called Graph\nExternal Attention Enhanced Transformer (GEAET), which integrates local\nstructure and global interaction information for more comprehensive graph\nrepresentations. Extensive experiments on benchmark datasets demonstrate that\nGEAET achieves state-of-the-art empirical performance. The source code is\navailable for reproducibility at: https://github.com/icm1018/GEAET.\n", "link": "http://arxiv.org/abs/2405.21061v2", "date": "2024-06-03", "relevancy": 1.9129, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5126}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20External%20Attention%20Enhanced%20Transformer&body=Title%3A%20Graph%20External%20Attention%20Enhanced%20Transformer%0AAuthor%3A%20Jianqing%20Liang%20and%20Min%20Chen%20and%20Jiye%20Liang%0AAbstract%3A%20%20%20The%20Transformer%20architecture%20has%20recently%20gained%20considerable%20attention%20in%0Athe%20field%20of%20graph%20representation%20learning%2C%20as%20it%20naturally%20overcomes%20several%0Alimitations%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20customized%20attention%0Amechanisms%20or%20positional%20and%20structural%20encodings.%20Despite%20making%20some%0Aprogress%2C%20existing%20works%20tend%20to%20overlook%20external%20information%20of%20graphs%2C%0Aspecifically%20the%20correlation%20between%20graphs.%20Intuitively%2C%20graphs%20with%20similar%0Astructures%20should%20have%20similar%20representations.%20Therefore%2C%20we%20propose%20Graph%0AExternal%20Attention%20%28GEA%29%20--%20a%20novel%20attention%20mechanism%20that%20leverages%20multiple%0Aexternal%20node/edge%20key-value%20units%20to%20capture%20inter-graph%20correlations%0Aimplicitly.%20On%20this%20basis%2C%20we%20design%20an%20effective%20architecture%20called%20Graph%0AExternal%20Attention%20Enhanced%20Transformer%20%28GEAET%29%2C%20which%20integrates%20local%0Astructure%20and%20global%20interaction%20information%20for%20more%20comprehensive%20graph%0Arepresentations.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0AGEAET%20achieves%20state-of-the-art%20empirical%20performance.%20The%20source%20code%20is%0Aavailable%20for%20reproducibility%20at%3A%20https%3A//github.com/icm1018/GEAET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520External%2520Attention%2520Enhanced%2520Transformer%26entry.906535625%3DJianqing%2520Liang%2520and%2520Min%2520Chen%2520and%2520Jiye%2520Liang%26entry.1292438233%3D%2520%2520The%2520Transformer%2520architecture%2520has%2520recently%2520gained%2520considerable%2520attention%2520in%250Athe%2520field%2520of%2520graph%2520representation%2520learning%252C%2520as%2520it%2520naturally%2520overcomes%2520several%250Alimitations%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520with%2520customized%2520attention%250Amechanisms%2520or%2520positional%2520and%2520structural%2520encodings.%2520Despite%2520making%2520some%250Aprogress%252C%2520existing%2520works%2520tend%2520to%2520overlook%2520external%2520information%2520of%2520graphs%252C%250Aspecifically%2520the%2520correlation%2520between%2520graphs.%2520Intuitively%252C%2520graphs%2520with%2520similar%250Astructures%2520should%2520have%2520similar%2520representations.%2520Therefore%252C%2520we%2520propose%2520Graph%250AExternal%2520Attention%2520%2528GEA%2529%2520--%2520a%2520novel%2520attention%2520mechanism%2520that%2520leverages%2520multiple%250Aexternal%2520node/edge%2520key-value%2520units%2520to%2520capture%2520inter-graph%2520correlations%250Aimplicitly.%2520On%2520this%2520basis%252C%2520we%2520design%2520an%2520effective%2520architecture%2520called%2520Graph%250AExternal%2520Attention%2520Enhanced%2520Transformer%2520%2528GEAET%2529%252C%2520which%2520integrates%2520local%250Astructure%2520and%2520global%2520interaction%2520information%2520for%2520more%2520comprehensive%2520graph%250Arepresentations.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%250AGEAET%2520achieves%2520state-of-the-art%2520empirical%2520performance.%2520The%2520source%2520code%2520is%250Aavailable%2520for%2520reproducibility%2520at%253A%2520https%253A//github.com/icm1018/GEAET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20External%20Attention%20Enhanced%20Transformer&entry.906535625=Jianqing%20Liang%20and%20Min%20Chen%20and%20Jiye%20Liang&entry.1292438233=%20%20The%20Transformer%20architecture%20has%20recently%20gained%20considerable%20attention%20in%0Athe%20field%20of%20graph%20representation%20learning%2C%20as%20it%20naturally%20overcomes%20several%0Alimitations%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20customized%20attention%0Amechanisms%20or%20positional%20and%20structural%20encodings.%20Despite%20making%20some%0Aprogress%2C%20existing%20works%20tend%20to%20overlook%20external%20information%20of%20graphs%2C%0Aspecifically%20the%20correlation%20between%20graphs.%20Intuitively%2C%20graphs%20with%20similar%0Astructures%20should%20have%20similar%20representations.%20Therefore%2C%20we%20propose%20Graph%0AExternal%20Attention%20%28GEA%29%20--%20a%20novel%20attention%20mechanism%20that%20leverages%20multiple%0Aexternal%20node/edge%20key-value%20units%20to%20capture%20inter-graph%20correlations%0Aimplicitly.%20On%20this%20basis%2C%20we%20design%20an%20effective%20architecture%20called%20Graph%0AExternal%20Attention%20Enhanced%20Transformer%20%28GEAET%29%2C%20which%20integrates%20local%0Astructure%20and%20global%20interaction%20information%20for%20more%20comprehensive%20graph%0Arepresentations.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0AGEAET%20achieves%20state-of-the-art%20empirical%20performance.%20The%20source%20code%20is%0Aavailable%20for%20reproducibility%20at%3A%20https%3A//github.com/icm1018/GEAET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21061v2&entry.124074799=Read"},
{"title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient\n  LLMs Under Compression", "author": "Junyuan Hong and Jinhao Duan and Chenhui Zhang and Zhangheng Li and Chulin Xie and Kelsey Lieberman and James Diffenderfer and Brian Bartoldson and Ajay Jaiswal and Kaidi Xu and Bhavya Kailkhura and Dan Hendrycks and Dawn Song and Zhangyang Wang and Bo Li", "abstract": "  Compressing high-capability Large Language Models (LLMs) has emerged as a\nfavored strategy for resource-efficient inferences. While state-of-the-art\n(SoTA) compression methods boast impressive advancements in preserving benign\ntask performance, the potential risks of compression in terms of safety and\ntrustworthiness have been largely neglected. This study conducts the first,\nthorough evaluation of three (3) leading LLMs using five (5) SoTA compression\ntechniques across eight (8) trustworthiness dimensions. Our experiments\nhighlight the intricate interplay between compression and trustworthiness,\nrevealing some interesting patterns. We find that quantization is currently a\nmore effective approach than pruning in achieving efficiency and\ntrustworthiness simultaneously. For instance, a 4-bit quantized model retains\nthe trustworthiness of its original counterpart, but model pruning\nsignificantly degrades trustworthiness, even at 50% sparsity. Moreover,\nemploying quantization within a moderate bit range could unexpectedly improve\ncertain trustworthiness dimensions such as ethics and fairness. Conversely,\nextreme quantization to very low bit levels (3 bits) tends to reduce\ntrustworthiness significantly. This increased risk cannot be uncovered by\nlooking at benign performance alone, in turn, mandating comprehensive\ntrustworthiness evaluation in practice. These findings culminate in practical\nrecommendations for simultaneously achieving high utility, efficiency, and\ntrustworthiness in LLMs. Code and models are available at\nhttps://decoding-comp-trust.github.io.\n", "link": "http://arxiv.org/abs/2403.15447v2", "date": "2024-06-03", "relevancy": 1.8882, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4767}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Compressed%20Trust%3A%20Scrutinizing%20the%20Trustworthiness%20of%20Efficient%0A%20%20LLMs%20Under%20Compression&body=Title%3A%20Decoding%20Compressed%20Trust%3A%20Scrutinizing%20the%20Trustworthiness%20of%20Efficient%0A%20%20LLMs%20Under%20Compression%0AAuthor%3A%20Junyuan%20Hong%20and%20Jinhao%20Duan%20and%20Chenhui%20Zhang%20and%20Zhangheng%20Li%20and%20Chulin%20Xie%20and%20Kelsey%20Lieberman%20and%20James%20Diffenderfer%20and%20Brian%20Bartoldson%20and%20Ajay%20Jaiswal%20and%20Kaidi%20Xu%20and%20Bhavya%20Kailkhura%20and%20Dan%20Hendrycks%20and%20Dawn%20Song%20and%20Zhangyang%20Wang%20and%20Bo%20Li%0AAbstract%3A%20%20%20Compressing%20high-capability%20Large%20Language%20Models%20%28LLMs%29%20has%20emerged%20as%20a%0Afavored%20strategy%20for%20resource-efficient%20inferences.%20While%20state-of-the-art%0A%28SoTA%29%20compression%20methods%20boast%20impressive%20advancements%20in%20preserving%20benign%0Atask%20performance%2C%20the%20potential%20risks%20of%20compression%20in%20terms%20of%20safety%20and%0Atrustworthiness%20have%20been%20largely%20neglected.%20This%20study%20conducts%20the%20first%2C%0Athorough%20evaluation%20of%20three%20%283%29%20leading%20LLMs%20using%20five%20%285%29%20SoTA%20compression%0Atechniques%20across%20eight%20%288%29%20trustworthiness%20dimensions.%20Our%20experiments%0Ahighlight%20the%20intricate%20interplay%20between%20compression%20and%20trustworthiness%2C%0Arevealing%20some%20interesting%20patterns.%20We%20find%20that%20quantization%20is%20currently%20a%0Amore%20effective%20approach%20than%20pruning%20in%20achieving%20efficiency%20and%0Atrustworthiness%20simultaneously.%20For%20instance%2C%20a%204-bit%20quantized%20model%20retains%0Athe%20trustworthiness%20of%20its%20original%20counterpart%2C%20but%20model%20pruning%0Asignificantly%20degrades%20trustworthiness%2C%20even%20at%2050%25%20sparsity.%20Moreover%2C%0Aemploying%20quantization%20within%20a%20moderate%20bit%20range%20could%20unexpectedly%20improve%0Acertain%20trustworthiness%20dimensions%20such%20as%20ethics%20and%20fairness.%20Conversely%2C%0Aextreme%20quantization%20to%20very%20low%20bit%20levels%20%283%20bits%29%20tends%20to%20reduce%0Atrustworthiness%20significantly.%20This%20increased%20risk%20cannot%20be%20uncovered%20by%0Alooking%20at%20benign%20performance%20alone%2C%20in%20turn%2C%20mandating%20comprehensive%0Atrustworthiness%20evaluation%20in%20practice.%20These%20findings%20culminate%20in%20practical%0Arecommendations%20for%20simultaneously%20achieving%20high%20utility%2C%20efficiency%2C%20and%0Atrustworthiness%20in%20LLMs.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//decoding-comp-trust.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Compressed%2520Trust%253A%2520Scrutinizing%2520the%2520Trustworthiness%2520of%2520Efficient%250A%2520%2520LLMs%2520Under%2520Compression%26entry.906535625%3DJunyuan%2520Hong%2520and%2520Jinhao%2520Duan%2520and%2520Chenhui%2520Zhang%2520and%2520Zhangheng%2520Li%2520and%2520Chulin%2520Xie%2520and%2520Kelsey%2520Lieberman%2520and%2520James%2520Diffenderfer%2520and%2520Brian%2520Bartoldson%2520and%2520Ajay%2520Jaiswal%2520and%2520Kaidi%2520Xu%2520and%2520Bhavya%2520Kailkhura%2520and%2520Dan%2520Hendrycks%2520and%2520Dawn%2520Song%2520and%2520Zhangyang%2520Wang%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Compressing%2520high-capability%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520emerged%2520as%2520a%250Afavored%2520strategy%2520for%2520resource-efficient%2520inferences.%2520While%2520state-of-the-art%250A%2528SoTA%2529%2520compression%2520methods%2520boast%2520impressive%2520advancements%2520in%2520preserving%2520benign%250Atask%2520performance%252C%2520the%2520potential%2520risks%2520of%2520compression%2520in%2520terms%2520of%2520safety%2520and%250Atrustworthiness%2520have%2520been%2520largely%2520neglected.%2520This%2520study%2520conducts%2520the%2520first%252C%250Athorough%2520evaluation%2520of%2520three%2520%25283%2529%2520leading%2520LLMs%2520using%2520five%2520%25285%2529%2520SoTA%2520compression%250Atechniques%2520across%2520eight%2520%25288%2529%2520trustworthiness%2520dimensions.%2520Our%2520experiments%250Ahighlight%2520the%2520intricate%2520interplay%2520between%2520compression%2520and%2520trustworthiness%252C%250Arevealing%2520some%2520interesting%2520patterns.%2520We%2520find%2520that%2520quantization%2520is%2520currently%2520a%250Amore%2520effective%2520approach%2520than%2520pruning%2520in%2520achieving%2520efficiency%2520and%250Atrustworthiness%2520simultaneously.%2520For%2520instance%252C%2520a%25204-bit%2520quantized%2520model%2520retains%250Athe%2520trustworthiness%2520of%2520its%2520original%2520counterpart%252C%2520but%2520model%2520pruning%250Asignificantly%2520degrades%2520trustworthiness%252C%2520even%2520at%252050%2525%2520sparsity.%2520Moreover%252C%250Aemploying%2520quantization%2520within%2520a%2520moderate%2520bit%2520range%2520could%2520unexpectedly%2520improve%250Acertain%2520trustworthiness%2520dimensions%2520such%2520as%2520ethics%2520and%2520fairness.%2520Conversely%252C%250Aextreme%2520quantization%2520to%2520very%2520low%2520bit%2520levels%2520%25283%2520bits%2529%2520tends%2520to%2520reduce%250Atrustworthiness%2520significantly.%2520This%2520increased%2520risk%2520cannot%2520be%2520uncovered%2520by%250Alooking%2520at%2520benign%2520performance%2520alone%252C%2520in%2520turn%252C%2520mandating%2520comprehensive%250Atrustworthiness%2520evaluation%2520in%2520practice.%2520These%2520findings%2520culminate%2520in%2520practical%250Arecommendations%2520for%2520simultaneously%2520achieving%2520high%2520utility%252C%2520efficiency%252C%2520and%250Atrustworthiness%2520in%2520LLMs.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//decoding-comp-trust.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Compressed%20Trust%3A%20Scrutinizing%20the%20Trustworthiness%20of%20Efficient%0A%20%20LLMs%20Under%20Compression&entry.906535625=Junyuan%20Hong%20and%20Jinhao%20Duan%20and%20Chenhui%20Zhang%20and%20Zhangheng%20Li%20and%20Chulin%20Xie%20and%20Kelsey%20Lieberman%20and%20James%20Diffenderfer%20and%20Brian%20Bartoldson%20and%20Ajay%20Jaiswal%20and%20Kaidi%20Xu%20and%20Bhavya%20Kailkhura%20and%20Dan%20Hendrycks%20and%20Dawn%20Song%20and%20Zhangyang%20Wang%20and%20Bo%20Li&entry.1292438233=%20%20Compressing%20high-capability%20Large%20Language%20Models%20%28LLMs%29%20has%20emerged%20as%20a%0Afavored%20strategy%20for%20resource-efficient%20inferences.%20While%20state-of-the-art%0A%28SoTA%29%20compression%20methods%20boast%20impressive%20advancements%20in%20preserving%20benign%0Atask%20performance%2C%20the%20potential%20risks%20of%20compression%20in%20terms%20of%20safety%20and%0Atrustworthiness%20have%20been%20largely%20neglected.%20This%20study%20conducts%20the%20first%2C%0Athorough%20evaluation%20of%20three%20%283%29%20leading%20LLMs%20using%20five%20%285%29%20SoTA%20compression%0Atechniques%20across%20eight%20%288%29%20trustworthiness%20dimensions.%20Our%20experiments%0Ahighlight%20the%20intricate%20interplay%20between%20compression%20and%20trustworthiness%2C%0Arevealing%20some%20interesting%20patterns.%20We%20find%20that%20quantization%20is%20currently%20a%0Amore%20effective%20approach%20than%20pruning%20in%20achieving%20efficiency%20and%0Atrustworthiness%20simultaneously.%20For%20instance%2C%20a%204-bit%20quantized%20model%20retains%0Athe%20trustworthiness%20of%20its%20original%20counterpart%2C%20but%20model%20pruning%0Asignificantly%20degrades%20trustworthiness%2C%20even%20at%2050%25%20sparsity.%20Moreover%2C%0Aemploying%20quantization%20within%20a%20moderate%20bit%20range%20could%20unexpectedly%20improve%0Acertain%20trustworthiness%20dimensions%20such%20as%20ethics%20and%20fairness.%20Conversely%2C%0Aextreme%20quantization%20to%20very%20low%20bit%20levels%20%283%20bits%29%20tends%20to%20reduce%0Atrustworthiness%20significantly.%20This%20increased%20risk%20cannot%20be%20uncovered%20by%0Alooking%20at%20benign%20performance%20alone%2C%20in%20turn%2C%20mandating%20comprehensive%0Atrustworthiness%20evaluation%20in%20practice.%20These%20findings%20culminate%20in%20practical%0Arecommendations%20for%20simultaneously%20achieving%20high%20utility%2C%20efficiency%2C%20and%0Atrustworthiness%20in%20LLMs.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//decoding-comp-trust.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15447v2&entry.124074799=Read"},
{"title": "A Survey on Self-Evolution of Large Language Models", "author": "Zhengwei Tao and Ting-En Lin and Xiancai Chen and Hangyu Li and Yuchuan Wu and Yongbin Li and Zhi Jin and Fei Huang and Dacheng Tao and Jingren Zhou", "abstract": "  Large language models (LLMs) have significantly advanced in various fields\nand intelligent agent applications. However, current LLMs that learn from human\nor external model supervision are costly and may face performance ceilings as\ntask complexity and diversity increase. To address this issue, self-evolution\napproaches that enable LLM to autonomously acquire, refine, and learn from\nexperiences generated by the model itself are rapidly growing. This new\ntraining paradigm inspired by the human experiential learning process offers\nthe potential to scale LLMs towards superintelligence. In this work, we present\na comprehensive survey of self-evolution approaches in LLMs. We first propose a\nconceptual framework for self-evolution and outline the evolving process as\niterative cycles composed of four phases: experience acquisition, experience\nrefinement, updating, and evaluation. Second, we categorize the evolution\nobjectives of LLMs and LLM-based agents; then, we summarize the literature and\nprovide taxonomy and insights for each module. Lastly, we pinpoint existing\nchallenges and propose future directions to improve self-evolution frameworks,\nequipping researchers with critical insights to fast-track the development of\nself-evolving LLMs. Our corresponding GitHub repository is available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM\n", "link": "http://arxiv.org/abs/2404.14387v2", "date": "2024-06-03", "relevancy": 1.8864, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4781}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4674}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models%0AAuthor%3A%20Zhengwei%20Tao%20and%20Ting-En%20Lin%20and%20Xiancai%20Chen%20and%20Hangyu%20Li%20and%20Yuchuan%20Wu%20and%20Yongbin%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Dacheng%20Tao%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20in%20various%20fields%0Aand%20intelligent%20agent%20applications.%20However%2C%20current%20LLMs%20that%20learn%20from%20human%0Aor%20external%20model%20supervision%20are%20costly%20and%20may%20face%20performance%20ceilings%20as%0Atask%20complexity%20and%20diversity%20increase.%20To%20address%20this%20issue%2C%20self-evolution%0Aapproaches%20that%20enable%20LLM%20to%20autonomously%20acquire%2C%20refine%2C%20and%20learn%20from%0Aexperiences%20generated%20by%20the%20model%20itself%20are%20rapidly%20growing.%20This%20new%0Atraining%20paradigm%20inspired%20by%20the%20human%20experiential%20learning%20process%20offers%0Athe%20potential%20to%20scale%20LLMs%20towards%20superintelligence.%20In%20this%20work%2C%20we%20present%0Aa%20comprehensive%20survey%20of%20self-evolution%20approaches%20in%20LLMs.%20We%20first%20propose%20a%0Aconceptual%20framework%20for%20self-evolution%20and%20outline%20the%20evolving%20process%20as%0Aiterative%20cycles%20composed%20of%20four%20phases%3A%20experience%20acquisition%2C%20experience%0Arefinement%2C%20updating%2C%20and%20evaluation.%20Second%2C%20we%20categorize%20the%20evolution%0Aobjectives%20of%20LLMs%20and%20LLM-based%20agents%3B%20then%2C%20we%20summarize%20the%20literature%20and%0Aprovide%20taxonomy%20and%20insights%20for%20each%20module.%20Lastly%2C%20we%20pinpoint%20existing%0Achallenges%20and%20propose%20future%20directions%20to%20improve%20self-evolution%20frameworks%2C%0Aequipping%20researchers%20with%20critical%20insights%20to%20fast-track%20the%20development%20of%0Aself-evolving%20LLMs.%20Our%20corresponding%20GitHub%20repository%20is%20available%20at%0Ahttps%3A//github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Self-Evolution%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DZhengwei%2520Tao%2520and%2520Ting-En%2520Lin%2520and%2520Xiancai%2520Chen%2520and%2520Hangyu%2520Li%2520and%2520Yuchuan%2520Wu%2520and%2520Yongbin%2520Li%2520and%2520Zhi%2520Jin%2520and%2520Fei%2520Huang%2520and%2520Dacheng%2520Tao%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520in%2520various%2520fields%250Aand%2520intelligent%2520agent%2520applications.%2520However%252C%2520current%2520LLMs%2520that%2520learn%2520from%2520human%250Aor%2520external%2520model%2520supervision%2520are%2520costly%2520and%2520may%2520face%2520performance%2520ceilings%2520as%250Atask%2520complexity%2520and%2520diversity%2520increase.%2520To%2520address%2520this%2520issue%252C%2520self-evolution%250Aapproaches%2520that%2520enable%2520LLM%2520to%2520autonomously%2520acquire%252C%2520refine%252C%2520and%2520learn%2520from%250Aexperiences%2520generated%2520by%2520the%2520model%2520itself%2520are%2520rapidly%2520growing.%2520This%2520new%250Atraining%2520paradigm%2520inspired%2520by%2520the%2520human%2520experiential%2520learning%2520process%2520offers%250Athe%2520potential%2520to%2520scale%2520LLMs%2520towards%2520superintelligence.%2520In%2520this%2520work%252C%2520we%2520present%250Aa%2520comprehensive%2520survey%2520of%2520self-evolution%2520approaches%2520in%2520LLMs.%2520We%2520first%2520propose%2520a%250Aconceptual%2520framework%2520for%2520self-evolution%2520and%2520outline%2520the%2520evolving%2520process%2520as%250Aiterative%2520cycles%2520composed%2520of%2520four%2520phases%253A%2520experience%2520acquisition%252C%2520experience%250Arefinement%252C%2520updating%252C%2520and%2520evaluation.%2520Second%252C%2520we%2520categorize%2520the%2520evolution%250Aobjectives%2520of%2520LLMs%2520and%2520LLM-based%2520agents%253B%2520then%252C%2520we%2520summarize%2520the%2520literature%2520and%250Aprovide%2520taxonomy%2520and%2520insights%2520for%2520each%2520module.%2520Lastly%252C%2520we%2520pinpoint%2520existing%250Achallenges%2520and%2520propose%2520future%2520directions%2520to%2520improve%2520self-evolution%2520frameworks%252C%250Aequipping%2520researchers%2520with%2520critical%2520insights%2520to%2520fast-track%2520the%2520development%2520of%250Aself-evolving%2520LLMs.%2520Our%2520corresponding%2520GitHub%2520repository%2520is%2520available%2520at%250Ahttps%253A//github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models&entry.906535625=Zhengwei%20Tao%20and%20Ting-En%20Lin%20and%20Xiancai%20Chen%20and%20Hangyu%20Li%20and%20Yuchuan%20Wu%20and%20Yongbin%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Dacheng%20Tao%20and%20Jingren%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20in%20various%20fields%0Aand%20intelligent%20agent%20applications.%20However%2C%20current%20LLMs%20that%20learn%20from%20human%0Aor%20external%20model%20supervision%20are%20costly%20and%20may%20face%20performance%20ceilings%20as%0Atask%20complexity%20and%20diversity%20increase.%20To%20address%20this%20issue%2C%20self-evolution%0Aapproaches%20that%20enable%20LLM%20to%20autonomously%20acquire%2C%20refine%2C%20and%20learn%20from%0Aexperiences%20generated%20by%20the%20model%20itself%20are%20rapidly%20growing.%20This%20new%0Atraining%20paradigm%20inspired%20by%20the%20human%20experiential%20learning%20process%20offers%0Athe%20potential%20to%20scale%20LLMs%20towards%20superintelligence.%20In%20this%20work%2C%20we%20present%0Aa%20comprehensive%20survey%20of%20self-evolution%20approaches%20in%20LLMs.%20We%20first%20propose%20a%0Aconceptual%20framework%20for%20self-evolution%20and%20outline%20the%20evolving%20process%20as%0Aiterative%20cycles%20composed%20of%20four%20phases%3A%20experience%20acquisition%2C%20experience%0Arefinement%2C%20updating%2C%20and%20evaluation.%20Second%2C%20we%20categorize%20the%20evolution%0Aobjectives%20of%20LLMs%20and%20LLM-based%20agents%3B%20then%2C%20we%20summarize%20the%20literature%20and%0Aprovide%20taxonomy%20and%20insights%20for%20each%20module.%20Lastly%2C%20we%20pinpoint%20existing%0Achallenges%20and%20propose%20future%20directions%20to%20improve%20self-evolution%20frameworks%2C%0Aequipping%20researchers%20with%20critical%20insights%20to%20fast-track%20the%20development%20of%0Aself-evolving%20LLMs.%20Our%20corresponding%20GitHub%20repository%20is%20available%20at%0Ahttps%3A//github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14387v2&entry.124074799=Read"},
{"title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating\n  Representative and Affinity Bias in Large Language Models", "author": "Abhishek Kumar and Sarfaroz Yunusov and Ali Emami", "abstract": "  Research on Large Language Models (LLMs) has often neglected subtle biases\nthat, although less apparent, can significantly influence the models' outputs\ntoward particular social narratives. This study addresses two such biases\nwithin LLMs: representative bias, which denotes a tendency of LLMs to generate\noutputs that mirror the experiences of certain identity groups, and affinity\nbias, reflecting the models' evaluative preferences for specific narratives or\nviewpoints. We introduce two novel metrics to measure these biases: the\nRepresentative Bias Score (RBS) and the Affinity Bias Score (ABS), and present\nthe Creativity-Oriented Generation Suite (CoGS), a collection of open-ended\ntasks such as short story writing and poetry composition, designed with\ncustomized rubrics to detect these subtle biases. Our analysis uncovers marked\nrepresentative biases in prominent LLMs, with a preference for identities\nassociated with being white, straight, and men. Furthermore, our investigation\nof affinity bias reveals distinctive evaluative patterns within each model,\nakin to `bias fingerprints'. This trend is also seen in human evaluators,\nhighlighting a complex interplay between human and machine bias perceptions.\n", "link": "http://arxiv.org/abs/2405.14555v4", "date": "2024-06-03", "relevancy": 1.8786, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4712}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subtle%20Biases%20Need%20Subtler%20Measures%3A%20Dual%20Metrics%20for%20Evaluating%0A%20%20Representative%20and%20Affinity%20Bias%20in%20Large%20Language%20Models&body=Title%3A%20Subtle%20Biases%20Need%20Subtler%20Measures%3A%20Dual%20Metrics%20for%20Evaluating%0A%20%20Representative%20and%20Affinity%20Bias%20in%20Large%20Language%20Models%0AAuthor%3A%20Abhishek%20Kumar%20and%20Sarfaroz%20Yunusov%20and%20Ali%20Emami%0AAbstract%3A%20%20%20Research%20on%20Large%20Language%20Models%20%28LLMs%29%20has%20often%20neglected%20subtle%20biases%0Athat%2C%20although%20less%20apparent%2C%20can%20significantly%20influence%20the%20models%27%20outputs%0Atoward%20particular%20social%20narratives.%20This%20study%20addresses%20two%20such%20biases%0Awithin%20LLMs%3A%20representative%20bias%2C%20which%20denotes%20a%20tendency%20of%20LLMs%20to%20generate%0Aoutputs%20that%20mirror%20the%20experiences%20of%20certain%20identity%20groups%2C%20and%20affinity%0Abias%2C%20reflecting%20the%20models%27%20evaluative%20preferences%20for%20specific%20narratives%20or%0Aviewpoints.%20We%20introduce%20two%20novel%20metrics%20to%20measure%20these%20biases%3A%20the%0ARepresentative%20Bias%20Score%20%28RBS%29%20and%20the%20Affinity%20Bias%20Score%20%28ABS%29%2C%20and%20present%0Athe%20Creativity-Oriented%20Generation%20Suite%20%28CoGS%29%2C%20a%20collection%20of%20open-ended%0Atasks%20such%20as%20short%20story%20writing%20and%20poetry%20composition%2C%20designed%20with%0Acustomized%20rubrics%20to%20detect%20these%20subtle%20biases.%20Our%20analysis%20uncovers%20marked%0Arepresentative%20biases%20in%20prominent%20LLMs%2C%20with%20a%20preference%20for%20identities%0Aassociated%20with%20being%20white%2C%20straight%2C%20and%20men.%20Furthermore%2C%20our%20investigation%0Aof%20affinity%20bias%20reveals%20distinctive%20evaluative%20patterns%20within%20each%20model%2C%0Aakin%20to%20%60bias%20fingerprints%27.%20This%20trend%20is%20also%20seen%20in%20human%20evaluators%2C%0Ahighlighting%20a%20complex%20interplay%20between%20human%20and%20machine%20bias%20perceptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14555v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubtle%2520Biases%2520Need%2520Subtler%2520Measures%253A%2520Dual%2520Metrics%2520for%2520Evaluating%250A%2520%2520Representative%2520and%2520Affinity%2520Bias%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAbhishek%2520Kumar%2520and%2520Sarfaroz%2520Yunusov%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520Research%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520often%2520neglected%2520subtle%2520biases%250Athat%252C%2520although%2520less%2520apparent%252C%2520can%2520significantly%2520influence%2520the%2520models%2527%2520outputs%250Atoward%2520particular%2520social%2520narratives.%2520This%2520study%2520addresses%2520two%2520such%2520biases%250Awithin%2520LLMs%253A%2520representative%2520bias%252C%2520which%2520denotes%2520a%2520tendency%2520of%2520LLMs%2520to%2520generate%250Aoutputs%2520that%2520mirror%2520the%2520experiences%2520of%2520certain%2520identity%2520groups%252C%2520and%2520affinity%250Abias%252C%2520reflecting%2520the%2520models%2527%2520evaluative%2520preferences%2520for%2520specific%2520narratives%2520or%250Aviewpoints.%2520We%2520introduce%2520two%2520novel%2520metrics%2520to%2520measure%2520these%2520biases%253A%2520the%250ARepresentative%2520Bias%2520Score%2520%2528RBS%2529%2520and%2520the%2520Affinity%2520Bias%2520Score%2520%2528ABS%2529%252C%2520and%2520present%250Athe%2520Creativity-Oriented%2520Generation%2520Suite%2520%2528CoGS%2529%252C%2520a%2520collection%2520of%2520open-ended%250Atasks%2520such%2520as%2520short%2520story%2520writing%2520and%2520poetry%2520composition%252C%2520designed%2520with%250Acustomized%2520rubrics%2520to%2520detect%2520these%2520subtle%2520biases.%2520Our%2520analysis%2520uncovers%2520marked%250Arepresentative%2520biases%2520in%2520prominent%2520LLMs%252C%2520with%2520a%2520preference%2520for%2520identities%250Aassociated%2520with%2520being%2520white%252C%2520straight%252C%2520and%2520men.%2520Furthermore%252C%2520our%2520investigation%250Aof%2520affinity%2520bias%2520reveals%2520distinctive%2520evaluative%2520patterns%2520within%2520each%2520model%252C%250Aakin%2520to%2520%2560bias%2520fingerprints%2527.%2520This%2520trend%2520is%2520also%2520seen%2520in%2520human%2520evaluators%252C%250Ahighlighting%2520a%2520complex%2520interplay%2520between%2520human%2520and%2520machine%2520bias%2520perceptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14555v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subtle%20Biases%20Need%20Subtler%20Measures%3A%20Dual%20Metrics%20for%20Evaluating%0A%20%20Representative%20and%20Affinity%20Bias%20in%20Large%20Language%20Models&entry.906535625=Abhishek%20Kumar%20and%20Sarfaroz%20Yunusov%20and%20Ali%20Emami&entry.1292438233=%20%20Research%20on%20Large%20Language%20Models%20%28LLMs%29%20has%20often%20neglected%20subtle%20biases%0Athat%2C%20although%20less%20apparent%2C%20can%20significantly%20influence%20the%20models%27%20outputs%0Atoward%20particular%20social%20narratives.%20This%20study%20addresses%20two%20such%20biases%0Awithin%20LLMs%3A%20representative%20bias%2C%20which%20denotes%20a%20tendency%20of%20LLMs%20to%20generate%0Aoutputs%20that%20mirror%20the%20experiences%20of%20certain%20identity%20groups%2C%20and%20affinity%0Abias%2C%20reflecting%20the%20models%27%20evaluative%20preferences%20for%20specific%20narratives%20or%0Aviewpoints.%20We%20introduce%20two%20novel%20metrics%20to%20measure%20these%20biases%3A%20the%0ARepresentative%20Bias%20Score%20%28RBS%29%20and%20the%20Affinity%20Bias%20Score%20%28ABS%29%2C%20and%20present%0Athe%20Creativity-Oriented%20Generation%20Suite%20%28CoGS%29%2C%20a%20collection%20of%20open-ended%0Atasks%20such%20as%20short%20story%20writing%20and%20poetry%20composition%2C%20designed%20with%0Acustomized%20rubrics%20to%20detect%20these%20subtle%20biases.%20Our%20analysis%20uncovers%20marked%0Arepresentative%20biases%20in%20prominent%20LLMs%2C%20with%20a%20preference%20for%20identities%0Aassociated%20with%20being%20white%2C%20straight%2C%20and%20men.%20Furthermore%2C%20our%20investigation%0Aof%20affinity%20bias%20reveals%20distinctive%20evaluative%20patterns%20within%20each%20model%2C%0Aakin%20to%20%60bias%20fingerprints%27.%20This%20trend%20is%20also%20seen%20in%20human%20evaluators%2C%0Ahighlighting%20a%20complex%20interplay%20between%20human%20and%20machine%20bias%20perceptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14555v4&entry.124074799=Read"},
{"title": "Overcoming Saturation in Density Ratio Estimation by Iterated\n  Regularization", "author": "Lukas Gruber and Markus Holzleitner and Johannes Lehner and Sepp Hochreiter and Werner Zellinger", "abstract": "  Estimating the ratio of two probability densities from finitely many samples,\nis a central task in machine learning and statistics. In this work, we show\nthat a large class of kernel methods for density ratio estimation suffers from\nerror saturation, which prevents algorithms from achieving fast error\nconvergence rates on highly regular learning problems. To resolve saturation,\nwe introduce iterated regularization in density ratio estimation to achieve\nfast error rates. Our methods outperform its non-iteratively regularized\nversions on benchmarks for density ratio estimation as well as on large-scale\nevaluations for importance-weighted ensembling of deep unsupervised domain\nadaptation models.\n", "link": "http://arxiv.org/abs/2402.13891v2", "date": "2024-06-03", "relevancy": 1.8758, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4892}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4608}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Saturation%20in%20Density%20Ratio%20Estimation%20by%20Iterated%0A%20%20Regularization&body=Title%3A%20Overcoming%20Saturation%20in%20Density%20Ratio%20Estimation%20by%20Iterated%0A%20%20Regularization%0AAuthor%3A%20Lukas%20Gruber%20and%20Markus%20Holzleitner%20and%20Johannes%20Lehner%20and%20Sepp%20Hochreiter%20and%20Werner%20Zellinger%0AAbstract%3A%20%20%20Estimating%20the%20ratio%20of%20two%20probability%20densities%20from%20finitely%20many%20samples%2C%0Ais%20a%20central%20task%20in%20machine%20learning%20and%20statistics.%20In%20this%20work%2C%20we%20show%0Athat%20a%20large%20class%20of%20kernel%20methods%20for%20density%20ratio%20estimation%20suffers%20from%0Aerror%20saturation%2C%20which%20prevents%20algorithms%20from%20achieving%20fast%20error%0Aconvergence%20rates%20on%20highly%20regular%20learning%20problems.%20To%20resolve%20saturation%2C%0Awe%20introduce%20iterated%20regularization%20in%20density%20ratio%20estimation%20to%20achieve%0Afast%20error%20rates.%20Our%20methods%20outperform%20its%20non-iteratively%20regularized%0Aversions%20on%20benchmarks%20for%20density%20ratio%20estimation%20as%20well%20as%20on%20large-scale%0Aevaluations%20for%20importance-weighted%20ensembling%20of%20deep%20unsupervised%20domain%0Aadaptation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Saturation%2520in%2520Density%2520Ratio%2520Estimation%2520by%2520Iterated%250A%2520%2520Regularization%26entry.906535625%3DLukas%2520Gruber%2520and%2520Markus%2520Holzleitner%2520and%2520Johannes%2520Lehner%2520and%2520Sepp%2520Hochreiter%2520and%2520Werner%2520Zellinger%26entry.1292438233%3D%2520%2520Estimating%2520the%2520ratio%2520of%2520two%2520probability%2520densities%2520from%2520finitely%2520many%2520samples%252C%250Ais%2520a%2520central%2520task%2520in%2520machine%2520learning%2520and%2520statistics.%2520In%2520this%2520work%252C%2520we%2520show%250Athat%2520a%2520large%2520class%2520of%2520kernel%2520methods%2520for%2520density%2520ratio%2520estimation%2520suffers%2520from%250Aerror%2520saturation%252C%2520which%2520prevents%2520algorithms%2520from%2520achieving%2520fast%2520error%250Aconvergence%2520rates%2520on%2520highly%2520regular%2520learning%2520problems.%2520To%2520resolve%2520saturation%252C%250Awe%2520introduce%2520iterated%2520regularization%2520in%2520density%2520ratio%2520estimation%2520to%2520achieve%250Afast%2520error%2520rates.%2520Our%2520methods%2520outperform%2520its%2520non-iteratively%2520regularized%250Aversions%2520on%2520benchmarks%2520for%2520density%2520ratio%2520estimation%2520as%2520well%2520as%2520on%2520large-scale%250Aevaluations%2520for%2520importance-weighted%2520ensembling%2520of%2520deep%2520unsupervised%2520domain%250Aadaptation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Saturation%20in%20Density%20Ratio%20Estimation%20by%20Iterated%0A%20%20Regularization&entry.906535625=Lukas%20Gruber%20and%20Markus%20Holzleitner%20and%20Johannes%20Lehner%20and%20Sepp%20Hochreiter%20and%20Werner%20Zellinger&entry.1292438233=%20%20Estimating%20the%20ratio%20of%20two%20probability%20densities%20from%20finitely%20many%20samples%2C%0Ais%20a%20central%20task%20in%20machine%20learning%20and%20statistics.%20In%20this%20work%2C%20we%20show%0Athat%20a%20large%20class%20of%20kernel%20methods%20for%20density%20ratio%20estimation%20suffers%20from%0Aerror%20saturation%2C%20which%20prevents%20algorithms%20from%20achieving%20fast%20error%0Aconvergence%20rates%20on%20highly%20regular%20learning%20problems.%20To%20resolve%20saturation%2C%0Awe%20introduce%20iterated%20regularization%20in%20density%20ratio%20estimation%20to%20achieve%0Afast%20error%20rates.%20Our%20methods%20outperform%20its%20non-iteratively%20regularized%0Aversions%20on%20benchmarks%20for%20density%20ratio%20estimation%20as%20well%20as%20on%20large-scale%0Aevaluations%20for%20importance-weighted%20ensembling%20of%20deep%20unsupervised%20domain%0Aadaptation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13891v2&entry.124074799=Read"},
{"title": "Feature Attribution with Necessity and Sufficiency via Dual-stage\n  Perturbation Test for Causal Explanation", "author": "Xuexin Chen and Ruichu Cai and Zhengting Huang and Yuxuan Zhu and Julien Horwood and Zhifeng Hao and Zijian Li and Jose Miguel Hernandez-Lobato", "abstract": "  We investigate the problem of explainability for machine learning models,\nfocusing on Feature Attribution Methods (FAMs) that evaluate feature importance\nthrough perturbation tests. Despite their utility, FAMs struggle to distinguish\nthe contributions of different features, when their prediction changes are\nsimilar after perturbation. To enhance FAMs' discriminative power, we introduce\nFeature Attribution with Necessity and Sufficiency (FANS), which find a\nneighborhood of the input such that perturbing samples within this neighborhood\nhave a high Probability of being Necessity and Sufficiency (PNS) cause for the\nchange in predictions, and use this PNS as the importance of the feature.\nSpecifically, FANS compute this PNS via a heuristic strategy for estimating the\nneighborhood and a perturbation test involving two stages (factual and\ninterventional) for counterfactual reasoning. To generate counterfactual\nsamples, we use a resampling-based approach on the observed samples to\napproximate the required conditional distribution. We demonstrate that FANS\noutperforms existing attribution methods on six benchmarks. Please refer to the\nsource code via \\url{https://github.com/DMIRLAB-Group/FANS}.\n", "link": "http://arxiv.org/abs/2402.08845v3", "date": "2024-06-03", "relevancy": 1.872, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4857}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation&body=Title%3A%20Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation%0AAuthor%3A%20Xuexin%20Chen%20and%20Ruichu%20Cai%20and%20Zhengting%20Huang%20and%20Yuxuan%20Zhu%20and%20Julien%20Horwood%20and%20Zhifeng%20Hao%20and%20Zijian%20Li%20and%20Jose%20Miguel%20Hernandez-Lobato%0AAbstract%3A%20%20%20We%20investigate%20the%20problem%20of%20explainability%20for%20machine%20learning%20models%2C%0Afocusing%20on%20Feature%20Attribution%20Methods%20%28FAMs%29%20that%20evaluate%20feature%20importance%0Athrough%20perturbation%20tests.%20Despite%20their%20utility%2C%20FAMs%20struggle%20to%20distinguish%0Athe%20contributions%20of%20different%20features%2C%20when%20their%20prediction%20changes%20are%0Asimilar%20after%20perturbation.%20To%20enhance%20FAMs%27%20discriminative%20power%2C%20we%20introduce%0AFeature%20Attribution%20with%20Necessity%20and%20Sufficiency%20%28FANS%29%2C%20which%20find%20a%0Aneighborhood%20of%20the%20input%20such%20that%20perturbing%20samples%20within%20this%20neighborhood%0Ahave%20a%20high%20Probability%20of%20being%20Necessity%20and%20Sufficiency%20%28PNS%29%20cause%20for%20the%0Achange%20in%20predictions%2C%20and%20use%20this%20PNS%20as%20the%20importance%20of%20the%20feature.%0ASpecifically%2C%20FANS%20compute%20this%20PNS%20via%20a%20heuristic%20strategy%20for%20estimating%20the%0Aneighborhood%20and%20a%20perturbation%20test%20involving%20two%20stages%20%28factual%20and%0Ainterventional%29%20for%20counterfactual%20reasoning.%20To%20generate%20counterfactual%0Asamples%2C%20we%20use%20a%20resampling-based%20approach%20on%20the%20observed%20samples%20to%0Aapproximate%20the%20required%20conditional%20distribution.%20We%20demonstrate%20that%20FANS%0Aoutperforms%20existing%20attribution%20methods%20on%20six%20benchmarks.%20Please%20refer%20to%20the%0Asource%20code%20via%20%5Curl%7Bhttps%3A//github.com/DMIRLAB-Group/FANS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08845v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Attribution%2520with%2520Necessity%2520and%2520Sufficiency%2520via%2520Dual-stage%250A%2520%2520Perturbation%2520Test%2520for%2520Causal%2520Explanation%26entry.906535625%3DXuexin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Zhengting%2520Huang%2520and%2520Yuxuan%2520Zhu%2520and%2520Julien%2520Horwood%2520and%2520Zhifeng%2520Hao%2520and%2520Zijian%2520Li%2520and%2520Jose%2520Miguel%2520Hernandez-Lobato%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520problem%2520of%2520explainability%2520for%2520machine%2520learning%2520models%252C%250Afocusing%2520on%2520Feature%2520Attribution%2520Methods%2520%2528FAMs%2529%2520that%2520evaluate%2520feature%2520importance%250Athrough%2520perturbation%2520tests.%2520Despite%2520their%2520utility%252C%2520FAMs%2520struggle%2520to%2520distinguish%250Athe%2520contributions%2520of%2520different%2520features%252C%2520when%2520their%2520prediction%2520changes%2520are%250Asimilar%2520after%2520perturbation.%2520To%2520enhance%2520FAMs%2527%2520discriminative%2520power%252C%2520we%2520introduce%250AFeature%2520Attribution%2520with%2520Necessity%2520and%2520Sufficiency%2520%2528FANS%2529%252C%2520which%2520find%2520a%250Aneighborhood%2520of%2520the%2520input%2520such%2520that%2520perturbing%2520samples%2520within%2520this%2520neighborhood%250Ahave%2520a%2520high%2520Probability%2520of%2520being%2520Necessity%2520and%2520Sufficiency%2520%2528PNS%2529%2520cause%2520for%2520the%250Achange%2520in%2520predictions%252C%2520and%2520use%2520this%2520PNS%2520as%2520the%2520importance%2520of%2520the%2520feature.%250ASpecifically%252C%2520FANS%2520compute%2520this%2520PNS%2520via%2520a%2520heuristic%2520strategy%2520for%2520estimating%2520the%250Aneighborhood%2520and%2520a%2520perturbation%2520test%2520involving%2520two%2520stages%2520%2528factual%2520and%250Ainterventional%2529%2520for%2520counterfactual%2520reasoning.%2520To%2520generate%2520counterfactual%250Asamples%252C%2520we%2520use%2520a%2520resampling-based%2520approach%2520on%2520the%2520observed%2520samples%2520to%250Aapproximate%2520the%2520required%2520conditional%2520distribution.%2520We%2520demonstrate%2520that%2520FANS%250Aoutperforms%2520existing%2520attribution%2520methods%2520on%2520six%2520benchmarks.%2520Please%2520refer%2520to%2520the%250Asource%2520code%2520via%2520%255Curl%257Bhttps%253A//github.com/DMIRLAB-Group/FANS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08845v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation&entry.906535625=Xuexin%20Chen%20and%20Ruichu%20Cai%20and%20Zhengting%20Huang%20and%20Yuxuan%20Zhu%20and%20Julien%20Horwood%20and%20Zhifeng%20Hao%20and%20Zijian%20Li%20and%20Jose%20Miguel%20Hernandez-Lobato&entry.1292438233=%20%20We%20investigate%20the%20problem%20of%20explainability%20for%20machine%20learning%20models%2C%0Afocusing%20on%20Feature%20Attribution%20Methods%20%28FAMs%29%20that%20evaluate%20feature%20importance%0Athrough%20perturbation%20tests.%20Despite%20their%20utility%2C%20FAMs%20struggle%20to%20distinguish%0Athe%20contributions%20of%20different%20features%2C%20when%20their%20prediction%20changes%20are%0Asimilar%20after%20perturbation.%20To%20enhance%20FAMs%27%20discriminative%20power%2C%20we%20introduce%0AFeature%20Attribution%20with%20Necessity%20and%20Sufficiency%20%28FANS%29%2C%20which%20find%20a%0Aneighborhood%20of%20the%20input%20such%20that%20perturbing%20samples%20within%20this%20neighborhood%0Ahave%20a%20high%20Probability%20of%20being%20Necessity%20and%20Sufficiency%20%28PNS%29%20cause%20for%20the%0Achange%20in%20predictions%2C%20and%20use%20this%20PNS%20as%20the%20importance%20of%20the%20feature.%0ASpecifically%2C%20FANS%20compute%20this%20PNS%20via%20a%20heuristic%20strategy%20for%20estimating%20the%0Aneighborhood%20and%20a%20perturbation%20test%20involving%20two%20stages%20%28factual%20and%0Ainterventional%29%20for%20counterfactual%20reasoning.%20To%20generate%20counterfactual%0Asamples%2C%20we%20use%20a%20resampling-based%20approach%20on%20the%20observed%20samples%20to%0Aapproximate%20the%20required%20conditional%20distribution.%20We%20demonstrate%20that%20FANS%0Aoutperforms%20existing%20attribution%20methods%20on%20six%20benchmarks.%20Please%20refer%20to%20the%0Asource%20code%20via%20%5Curl%7Bhttps%3A//github.com/DMIRLAB-Group/FANS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08845v3&entry.124074799=Read"},
{"title": "Loss Symmetry and Noise Equilibrium of Stochastic Gradient Descent", "author": "Liu Ziyin and Mingze Wang and Hongchao Li and Lei Wu", "abstract": "  Symmetries exist abundantly in the loss function of neural networks. We\ncharacterize the learning dynamics of stochastic gradient descent (SGD) when\nexponential symmetries, a broad subclass of continuous symmetries, exist in the\nloss function. We establish that when gradient noises do not balance, SGD has\nthe tendency to move the model parameters toward a point where noises from\ndifferent directions are balanced. Here, a special type of fixed point in the\nconstant directions of the loss function emerges as a candidate for solutions\nfor SGD. As the main theoretical result, we prove that every parameter $\\theta$\nconnects without loss function barrier to a unique noise-balanced fixed point\n$\\theta^*$. The theory implies that the balancing of gradient noise can serve\nas a novel alternative mechanism for relevant phenomena such as progressive\nsharpening and flattening and can be applied to understand common practical\nproblems such as representation normalization, matrix factorization, warmup,\nand formation of latent representations.\n", "link": "http://arxiv.org/abs/2402.07193v2", "date": "2024-06-03", "relevancy": 1.8488, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4758}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4605}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loss%20Symmetry%20and%20Noise%20Equilibrium%20of%20Stochastic%20Gradient%20Descent&body=Title%3A%20Loss%20Symmetry%20and%20Noise%20Equilibrium%20of%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Liu%20Ziyin%20and%20Mingze%20Wang%20and%20Hongchao%20Li%20and%20Lei%20Wu%0AAbstract%3A%20%20%20Symmetries%20exist%20abundantly%20in%20the%20loss%20function%20of%20neural%20networks.%20We%0Acharacterize%20the%20learning%20dynamics%20of%20stochastic%20gradient%20descent%20%28SGD%29%20when%0Aexponential%20symmetries%2C%20a%20broad%20subclass%20of%20continuous%20symmetries%2C%20exist%20in%20the%0Aloss%20function.%20We%20establish%20that%20when%20gradient%20noises%20do%20not%20balance%2C%20SGD%20has%0Athe%20tendency%20to%20move%20the%20model%20parameters%20toward%20a%20point%20where%20noises%20from%0Adifferent%20directions%20are%20balanced.%20Here%2C%20a%20special%20type%20of%20fixed%20point%20in%20the%0Aconstant%20directions%20of%20the%20loss%20function%20emerges%20as%20a%20candidate%20for%20solutions%0Afor%20SGD.%20As%20the%20main%20theoretical%20result%2C%20we%20prove%20that%20every%20parameter%20%24%5Ctheta%24%0Aconnects%20without%20loss%20function%20barrier%20to%20a%20unique%20noise-balanced%20fixed%20point%0A%24%5Ctheta%5E%2A%24.%20The%20theory%20implies%20that%20the%20balancing%20of%20gradient%20noise%20can%20serve%0Aas%20a%20novel%20alternative%20mechanism%20for%20relevant%20phenomena%20such%20as%20progressive%0Asharpening%20and%20flattening%20and%20can%20be%20applied%20to%20understand%20common%20practical%0Aproblems%20such%20as%20representation%20normalization%2C%20matrix%20factorization%2C%20warmup%2C%0Aand%20formation%20of%20latent%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoss%2520Symmetry%2520and%2520Noise%2520Equilibrium%2520of%2520Stochastic%2520Gradient%2520Descent%26entry.906535625%3DLiu%2520Ziyin%2520and%2520Mingze%2520Wang%2520and%2520Hongchao%2520Li%2520and%2520Lei%2520Wu%26entry.1292438233%3D%2520%2520Symmetries%2520exist%2520abundantly%2520in%2520the%2520loss%2520function%2520of%2520neural%2520networks.%2520We%250Acharacterize%2520the%2520learning%2520dynamics%2520of%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520when%250Aexponential%2520symmetries%252C%2520a%2520broad%2520subclass%2520of%2520continuous%2520symmetries%252C%2520exist%2520in%2520the%250Aloss%2520function.%2520We%2520establish%2520that%2520when%2520gradient%2520noises%2520do%2520not%2520balance%252C%2520SGD%2520has%250Athe%2520tendency%2520to%2520move%2520the%2520model%2520parameters%2520toward%2520a%2520point%2520where%2520noises%2520from%250Adifferent%2520directions%2520are%2520balanced.%2520Here%252C%2520a%2520special%2520type%2520of%2520fixed%2520point%2520in%2520the%250Aconstant%2520directions%2520of%2520the%2520loss%2520function%2520emerges%2520as%2520a%2520candidate%2520for%2520solutions%250Afor%2520SGD.%2520As%2520the%2520main%2520theoretical%2520result%252C%2520we%2520prove%2520that%2520every%2520parameter%2520%2524%255Ctheta%2524%250Aconnects%2520without%2520loss%2520function%2520barrier%2520to%2520a%2520unique%2520noise-balanced%2520fixed%2520point%250A%2524%255Ctheta%255E%252A%2524.%2520The%2520theory%2520implies%2520that%2520the%2520balancing%2520of%2520gradient%2520noise%2520can%2520serve%250Aas%2520a%2520novel%2520alternative%2520mechanism%2520for%2520relevant%2520phenomena%2520such%2520as%2520progressive%250Asharpening%2520and%2520flattening%2520and%2520can%2520be%2520applied%2520to%2520understand%2520common%2520practical%250Aproblems%2520such%2520as%2520representation%2520normalization%252C%2520matrix%2520factorization%252C%2520warmup%252C%250Aand%2520formation%2520of%2520latent%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loss%20Symmetry%20and%20Noise%20Equilibrium%20of%20Stochastic%20Gradient%20Descent&entry.906535625=Liu%20Ziyin%20and%20Mingze%20Wang%20and%20Hongchao%20Li%20and%20Lei%20Wu&entry.1292438233=%20%20Symmetries%20exist%20abundantly%20in%20the%20loss%20function%20of%20neural%20networks.%20We%0Acharacterize%20the%20learning%20dynamics%20of%20stochastic%20gradient%20descent%20%28SGD%29%20when%0Aexponential%20symmetries%2C%20a%20broad%20subclass%20of%20continuous%20symmetries%2C%20exist%20in%20the%0Aloss%20function.%20We%20establish%20that%20when%20gradient%20noises%20do%20not%20balance%2C%20SGD%20has%0Athe%20tendency%20to%20move%20the%20model%20parameters%20toward%20a%20point%20where%20noises%20from%0Adifferent%20directions%20are%20balanced.%20Here%2C%20a%20special%20type%20of%20fixed%20point%20in%20the%0Aconstant%20directions%20of%20the%20loss%20function%20emerges%20as%20a%20candidate%20for%20solutions%0Afor%20SGD.%20As%20the%20main%20theoretical%20result%2C%20we%20prove%20that%20every%20parameter%20%24%5Ctheta%24%0Aconnects%20without%20loss%20function%20barrier%20to%20a%20unique%20noise-balanced%20fixed%20point%0A%24%5Ctheta%5E%2A%24.%20The%20theory%20implies%20that%20the%20balancing%20of%20gradient%20noise%20can%20serve%0Aas%20a%20novel%20alternative%20mechanism%20for%20relevant%20phenomena%20such%20as%20progressive%0Asharpening%20and%20flattening%20and%20can%20be%20applied%20to%20understand%20common%20practical%0Aproblems%20such%20as%20representation%20normalization%2C%20matrix%20factorization%2C%20warmup%2C%0Aand%20formation%20of%20latent%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07193v2&entry.124074799=Read"},
{"title": "Multiscale Causal Learning", "author": "Michael Timothy Bennett", "abstract": "  Biological intelligence is more sample-efficient than artificial intelligence\n(AI), learning from fewer examples. Here we answer why. Given data, there can\nbe many policies which seem \"correct\" because they perfectly fit the data.\nHowever, only one correct policy could have actually caused the data.\nSample-efficiency requires a means of discerning which. Previous work showed\nsample efficiency is maximised by weak-policy-optimisation (WPO); preferring\npolicies that more weakly constrain what is considered to be correct, given\nfinite resources. Biology's sample-efficiency demonstrates it is better at WPO.\nTo understand how, we formalise the \"multiscale-competency-architecture\" (MCA)\nobserved in biological systems, as a sequence of nested\n\"agentic-abstraction-layers\". We show that WPO at low levels enables synthesis\nof weaker policies at high. We call this \"multiscale-causal-learning\", and\nargue this is how we might construct more scale-able, sample-efficient and\nreliable AI. Furthermore, a sufficiently weak policy at low levels is a\nprecondition of collective policy at higher levels. The higher level \"identity\"\nof the collective is lost if lower levels use an insufficiently weak policy\n(e.g. cells may become isolated from the collective informational structure and\nrevert to primitive behaviour). This has implications for biology, machine\nlearning, AI-safety, and philosophy.\n", "link": "http://arxiv.org/abs/2405.02325v2", "date": "2024-06-03", "relevancy": 1.8449, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4537}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiscale%20Causal%20Learning&body=Title%3A%20Multiscale%20Causal%20Learning%0AAuthor%3A%20Michael%20Timothy%20Bennett%0AAbstract%3A%20%20%20Biological%20intelligence%20is%20more%20sample-efficient%20than%20artificial%20intelligence%0A%28AI%29%2C%20learning%20from%20fewer%20examples.%20Here%20we%20answer%20why.%20Given%20data%2C%20there%20can%0Abe%20many%20policies%20which%20seem%20%22correct%22%20because%20they%20perfectly%20fit%20the%20data.%0AHowever%2C%20only%20one%20correct%20policy%20could%20have%20actually%20caused%20the%20data.%0ASample-efficiency%20requires%20a%20means%20of%20discerning%20which.%20Previous%20work%20showed%0Asample%20efficiency%20is%20maximised%20by%20weak-policy-optimisation%20%28WPO%29%3B%20preferring%0Apolicies%20that%20more%20weakly%20constrain%20what%20is%20considered%20to%20be%20correct%2C%20given%0Afinite%20resources.%20Biology%27s%20sample-efficiency%20demonstrates%20it%20is%20better%20at%20WPO.%0ATo%20understand%20how%2C%20we%20formalise%20the%20%22multiscale-competency-architecture%22%20%28MCA%29%0Aobserved%20in%20biological%20systems%2C%20as%20a%20sequence%20of%20nested%0A%22agentic-abstraction-layers%22.%20We%20show%20that%20WPO%20at%20low%20levels%20enables%20synthesis%0Aof%20weaker%20policies%20at%20high.%20We%20call%20this%20%22multiscale-causal-learning%22%2C%20and%0Aargue%20this%20is%20how%20we%20might%20construct%20more%20scale-able%2C%20sample-efficient%20and%0Areliable%20AI.%20Furthermore%2C%20a%20sufficiently%20weak%20policy%20at%20low%20levels%20is%20a%0Aprecondition%20of%20collective%20policy%20at%20higher%20levels.%20The%20higher%20level%20%22identity%22%0Aof%20the%20collective%20is%20lost%20if%20lower%20levels%20use%20an%20insufficiently%20weak%20policy%0A%28e.g.%20cells%20may%20become%20isolated%20from%20the%20collective%20informational%20structure%20and%0Arevert%20to%20primitive%20behaviour%29.%20This%20has%20implications%20for%20biology%2C%20machine%0Alearning%2C%20AI-safety%2C%20and%20philosophy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiscale%2520Causal%2520Learning%26entry.906535625%3DMichael%2520Timothy%2520Bennett%26entry.1292438233%3D%2520%2520Biological%2520intelligence%2520is%2520more%2520sample-efficient%2520than%2520artificial%2520intelligence%250A%2528AI%2529%252C%2520learning%2520from%2520fewer%2520examples.%2520Here%2520we%2520answer%2520why.%2520Given%2520data%252C%2520there%2520can%250Abe%2520many%2520policies%2520which%2520seem%2520%2522correct%2522%2520because%2520they%2520perfectly%2520fit%2520the%2520data.%250AHowever%252C%2520only%2520one%2520correct%2520policy%2520could%2520have%2520actually%2520caused%2520the%2520data.%250ASample-efficiency%2520requires%2520a%2520means%2520of%2520discerning%2520which.%2520Previous%2520work%2520showed%250Asample%2520efficiency%2520is%2520maximised%2520by%2520weak-policy-optimisation%2520%2528WPO%2529%253B%2520preferring%250Apolicies%2520that%2520more%2520weakly%2520constrain%2520what%2520is%2520considered%2520to%2520be%2520correct%252C%2520given%250Afinite%2520resources.%2520Biology%2527s%2520sample-efficiency%2520demonstrates%2520it%2520is%2520better%2520at%2520WPO.%250ATo%2520understand%2520how%252C%2520we%2520formalise%2520the%2520%2522multiscale-competency-architecture%2522%2520%2528MCA%2529%250Aobserved%2520in%2520biological%2520systems%252C%2520as%2520a%2520sequence%2520of%2520nested%250A%2522agentic-abstraction-layers%2522.%2520We%2520show%2520that%2520WPO%2520at%2520low%2520levels%2520enables%2520synthesis%250Aof%2520weaker%2520policies%2520at%2520high.%2520We%2520call%2520this%2520%2522multiscale-causal-learning%2522%252C%2520and%250Aargue%2520this%2520is%2520how%2520we%2520might%2520construct%2520more%2520scale-able%252C%2520sample-efficient%2520and%250Areliable%2520AI.%2520Furthermore%252C%2520a%2520sufficiently%2520weak%2520policy%2520at%2520low%2520levels%2520is%2520a%250Aprecondition%2520of%2520collective%2520policy%2520at%2520higher%2520levels.%2520The%2520higher%2520level%2520%2522identity%2522%250Aof%2520the%2520collective%2520is%2520lost%2520if%2520lower%2520levels%2520use%2520an%2520insufficiently%2520weak%2520policy%250A%2528e.g.%2520cells%2520may%2520become%2520isolated%2520from%2520the%2520collective%2520informational%2520structure%2520and%250Arevert%2520to%2520primitive%2520behaviour%2529.%2520This%2520has%2520implications%2520for%2520biology%252C%2520machine%250Alearning%252C%2520AI-safety%252C%2520and%2520philosophy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiscale%20Causal%20Learning&entry.906535625=Michael%20Timothy%20Bennett&entry.1292438233=%20%20Biological%20intelligence%20is%20more%20sample-efficient%20than%20artificial%20intelligence%0A%28AI%29%2C%20learning%20from%20fewer%20examples.%20Here%20we%20answer%20why.%20Given%20data%2C%20there%20can%0Abe%20many%20policies%20which%20seem%20%22correct%22%20because%20they%20perfectly%20fit%20the%20data.%0AHowever%2C%20only%20one%20correct%20policy%20could%20have%20actually%20caused%20the%20data.%0ASample-efficiency%20requires%20a%20means%20of%20discerning%20which.%20Previous%20work%20showed%0Asample%20efficiency%20is%20maximised%20by%20weak-policy-optimisation%20%28WPO%29%3B%20preferring%0Apolicies%20that%20more%20weakly%20constrain%20what%20is%20considered%20to%20be%20correct%2C%20given%0Afinite%20resources.%20Biology%27s%20sample-efficiency%20demonstrates%20it%20is%20better%20at%20WPO.%0ATo%20understand%20how%2C%20we%20formalise%20the%20%22multiscale-competency-architecture%22%20%28MCA%29%0Aobserved%20in%20biological%20systems%2C%20as%20a%20sequence%20of%20nested%0A%22agentic-abstraction-layers%22.%20We%20show%20that%20WPO%20at%20low%20levels%20enables%20synthesis%0Aof%20weaker%20policies%20at%20high.%20We%20call%20this%20%22multiscale-causal-learning%22%2C%20and%0Aargue%20this%20is%20how%20we%20might%20construct%20more%20scale-able%2C%20sample-efficient%20and%0Areliable%20AI.%20Furthermore%2C%20a%20sufficiently%20weak%20policy%20at%20low%20levels%20is%20a%0Aprecondition%20of%20collective%20policy%20at%20higher%20levels.%20The%20higher%20level%20%22identity%22%0Aof%20the%20collective%20is%20lost%20if%20lower%20levels%20use%20an%20insufficiently%20weak%20policy%0A%28e.g.%20cells%20may%20become%20isolated%20from%20the%20collective%20informational%20structure%20and%0Arevert%20to%20primitive%20behaviour%29.%20This%20has%20implications%20for%20biology%2C%20machine%0Alearning%2C%20AI-safety%2C%20and%20philosophy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02325v2&entry.124074799=Read"},
{"title": "Do we need rebalancing strategies? A theoretical and empirical study\n  around SMOTE and its variants", "author": "Abdoulaye Sakho and Emmanuel Malherbe and Erwan Scornet", "abstract": "  Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing\nstrategy for handling imbalanced tabular data sets. However, few works analyze\nSMOTE theoretically. In this paper, we prove that SMOTE (with default\nparameter) simply copies the original minority samples asymptotically. We also\nprove that SMOTE exhibits boundary artifacts, thus justifying existing SMOTE\nvariants. Then we introduce two new SMOTE-related strategies, and compare them\nwith state-of-the-art rebalancing procedures. Surprisingly, for most data sets,\nwe observe that applying no rebalancing strategy is competitive in terms of\npredictive performances, with tuned random forests. For highly imbalanced data\nsets, our new method, named Multivariate Gaussian SMOTE, is competitive.\nBesides, our analysis sheds some lights on the behavior of common rebalancing\nstrategies, when used in conjunction with random forests.\n", "link": "http://arxiv.org/abs/2402.03819v2", "date": "2024-06-03", "relevancy": 1.8278, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4451}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20we%20need%20rebalancing%20strategies%3F%20A%20theoretical%20and%20empirical%20study%0A%20%20around%20SMOTE%20and%20its%20variants&body=Title%3A%20Do%20we%20need%20rebalancing%20strategies%3F%20A%20theoretical%20and%20empirical%20study%0A%20%20around%20SMOTE%20and%20its%20variants%0AAuthor%3A%20Abdoulaye%20Sakho%20and%20Emmanuel%20Malherbe%20and%20Erwan%20Scornet%0AAbstract%3A%20%20%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20is%20a%20common%20rebalancing%0Astrategy%20for%20handling%20imbalanced%20tabular%20data%20sets.%20However%2C%20few%20works%20analyze%0ASMOTE%20theoretically.%20In%20this%20paper%2C%20we%20prove%20that%20SMOTE%20%28with%20default%0Aparameter%29%20simply%20copies%20the%20original%20minority%20samples%20asymptotically.%20We%20also%0Aprove%20that%20SMOTE%20exhibits%20boundary%20artifacts%2C%20thus%20justifying%20existing%20SMOTE%0Avariants.%20Then%20we%20introduce%20two%20new%20SMOTE-related%20strategies%2C%20and%20compare%20them%0Awith%20state-of-the-art%20rebalancing%20procedures.%20Surprisingly%2C%20for%20most%20data%20sets%2C%0Awe%20observe%20that%20applying%20no%20rebalancing%20strategy%20is%20competitive%20in%20terms%20of%0Apredictive%20performances%2C%20with%20tuned%20random%20forests.%20For%20highly%20imbalanced%20data%0Asets%2C%20our%20new%20method%2C%20named%20Multivariate%20Gaussian%20SMOTE%2C%20is%20competitive.%0ABesides%2C%20our%20analysis%20sheds%20some%20lights%20on%20the%20behavior%20of%20common%20rebalancing%0Astrategies%2C%20when%20used%20in%20conjunction%20with%20random%20forests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520we%2520need%2520rebalancing%2520strategies%253F%2520A%2520theoretical%2520and%2520empirical%2520study%250A%2520%2520around%2520SMOTE%2520and%2520its%2520variants%26entry.906535625%3DAbdoulaye%2520Sakho%2520and%2520Emmanuel%2520Malherbe%2520and%2520Erwan%2520Scornet%26entry.1292438233%3D%2520%2520Synthetic%2520Minority%2520Oversampling%2520Technique%2520%2528SMOTE%2529%2520is%2520a%2520common%2520rebalancing%250Astrategy%2520for%2520handling%2520imbalanced%2520tabular%2520data%2520sets.%2520However%252C%2520few%2520works%2520analyze%250ASMOTE%2520theoretically.%2520In%2520this%2520paper%252C%2520we%2520prove%2520that%2520SMOTE%2520%2528with%2520default%250Aparameter%2529%2520simply%2520copies%2520the%2520original%2520minority%2520samples%2520asymptotically.%2520We%2520also%250Aprove%2520that%2520SMOTE%2520exhibits%2520boundary%2520artifacts%252C%2520thus%2520justifying%2520existing%2520SMOTE%250Avariants.%2520Then%2520we%2520introduce%2520two%2520new%2520SMOTE-related%2520strategies%252C%2520and%2520compare%2520them%250Awith%2520state-of-the-art%2520rebalancing%2520procedures.%2520Surprisingly%252C%2520for%2520most%2520data%2520sets%252C%250Awe%2520observe%2520that%2520applying%2520no%2520rebalancing%2520strategy%2520is%2520competitive%2520in%2520terms%2520of%250Apredictive%2520performances%252C%2520with%2520tuned%2520random%2520forests.%2520For%2520highly%2520imbalanced%2520data%250Asets%252C%2520our%2520new%2520method%252C%2520named%2520Multivariate%2520Gaussian%2520SMOTE%252C%2520is%2520competitive.%250ABesides%252C%2520our%2520analysis%2520sheds%2520some%2520lights%2520on%2520the%2520behavior%2520of%2520common%2520rebalancing%250Astrategies%252C%2520when%2520used%2520in%2520conjunction%2520with%2520random%2520forests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20we%20need%20rebalancing%20strategies%3F%20A%20theoretical%20and%20empirical%20study%0A%20%20around%20SMOTE%20and%20its%20variants&entry.906535625=Abdoulaye%20Sakho%20and%20Emmanuel%20Malherbe%20and%20Erwan%20Scornet&entry.1292438233=%20%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20is%20a%20common%20rebalancing%0Astrategy%20for%20handling%20imbalanced%20tabular%20data%20sets.%20However%2C%20few%20works%20analyze%0ASMOTE%20theoretically.%20In%20this%20paper%2C%20we%20prove%20that%20SMOTE%20%28with%20default%0Aparameter%29%20simply%20copies%20the%20original%20minority%20samples%20asymptotically.%20We%20also%0Aprove%20that%20SMOTE%20exhibits%20boundary%20artifacts%2C%20thus%20justifying%20existing%20SMOTE%0Avariants.%20Then%20we%20introduce%20two%20new%20SMOTE-related%20strategies%2C%20and%20compare%20them%0Awith%20state-of-the-art%20rebalancing%20procedures.%20Surprisingly%2C%20for%20most%20data%20sets%2C%0Awe%20observe%20that%20applying%20no%20rebalancing%20strategy%20is%20competitive%20in%20terms%20of%0Apredictive%20performances%2C%20with%20tuned%20random%20forests.%20For%20highly%20imbalanced%20data%0Asets%2C%20our%20new%20method%2C%20named%20Multivariate%20Gaussian%20SMOTE%2C%20is%20competitive.%0ABesides%2C%20our%20analysis%20sheds%20some%20lights%20on%20the%20behavior%20of%20common%20rebalancing%0Astrategies%2C%20when%20used%20in%20conjunction%20with%20random%20forests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03819v2&entry.124074799=Read"},
{"title": "iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition", "author": "Mengxi Liu and Vitor Fortes Rey and Yu Zhang and Lala Shakti Swarup Ray and Bo Zhou and Paul Lukowicz", "abstract": "  Automatic and precise fitness activity recognition can be beneficial in\naspects from promoting a healthy lifestyle to personalized preventative\nhealthcare. While IMUs are currently the prominent fitness tracking modality,\nthrough iMove, we show bio-impedence can help improve IMU-based fitness\ntracking through sensor fusion and contrastive learning.To evaluate our\nmethods, we conducted an experiment including six upper body fitness activities\nperformed by ten subjects over five days to collect synchronized data from\nbio-impedance across two wrists and IMU on the left wrist.The contrastive\nlearning framework uses the two modalities to train a better IMU-only\nclassification model, where bio-impedance is only required at the training\nphase, by which the average Macro F1 score with the input of a single IMU was\nimproved by 3.22 \\% reaching 84.71 \\% compared to the 81.49 \\% of the IMU\nbaseline model. We have also shown how bio-impedance can improve human activity\nrecognition (HAR) directly through sensor fusion, reaching an average Macro F1\nscore of 89.57 \\% (two modalities required for both training and inference)\neven if Bio-impedance alone has an average macro F1 score of 75.36 \\%, which is\noutperformed by IMU alone. In addition, similar results were obtained in an\nextended study on lower body fitness activity classification, demonstrating the\ngeneralisability of our approach.Our findings underscore the potential of\nsensor fusion and contrastive learning as valuable tools for advancing fitness\nactivity recognition, with bio-impedance playing a pivotal role in augmenting\nthe capabilities of IMU-based systems.\n", "link": "http://arxiv.org/abs/2402.09445v2", "date": "2024-06-03", "relevancy": 1.825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5317}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4417}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMove%3A%20Exploring%20Bio-impedance%20Sensing%20for%20Fitness%20Activity%20Recognition&body=Title%3A%20iMove%3A%20Exploring%20Bio-impedance%20Sensing%20for%20Fitness%20Activity%20Recognition%0AAuthor%3A%20Mengxi%20Liu%20and%20Vitor%20Fortes%20Rey%20and%20Yu%20Zhang%20and%20Lala%20Shakti%20Swarup%20Ray%20and%20Bo%20Zhou%20and%20Paul%20Lukowicz%0AAbstract%3A%20%20%20Automatic%20and%20precise%20fitness%20activity%20recognition%20can%20be%20beneficial%20in%0Aaspects%20from%20promoting%20a%20healthy%20lifestyle%20to%20personalized%20preventative%0Ahealthcare.%20While%20IMUs%20are%20currently%20the%20prominent%20fitness%20tracking%20modality%2C%0Athrough%20iMove%2C%20we%20show%20bio-impedence%20can%20help%20improve%20IMU-based%20fitness%0Atracking%20through%20sensor%20fusion%20and%20contrastive%20learning.To%20evaluate%20our%0Amethods%2C%20we%20conducted%20an%20experiment%20including%20six%20upper%20body%20fitness%20activities%0Aperformed%20by%20ten%20subjects%20over%20five%20days%20to%20collect%20synchronized%20data%20from%0Abio-impedance%20across%20two%20wrists%20and%20IMU%20on%20the%20left%20wrist.The%20contrastive%0Alearning%20framework%20uses%20the%20two%20modalities%20to%20train%20a%20better%20IMU-only%0Aclassification%20model%2C%20where%20bio-impedance%20is%20only%20required%20at%20the%20training%0Aphase%2C%20by%20which%20the%20average%20Macro%20F1%20score%20with%20the%20input%20of%20a%20single%20IMU%20was%0Aimproved%20by%203.22%20%5C%25%20reaching%2084.71%20%5C%25%20compared%20to%20the%2081.49%20%5C%25%20of%20the%20IMU%0Abaseline%20model.%20We%20have%20also%20shown%20how%20bio-impedance%20can%20improve%20human%20activity%0Arecognition%20%28HAR%29%20directly%20through%20sensor%20fusion%2C%20reaching%20an%20average%20Macro%20F1%0Ascore%20of%2089.57%20%5C%25%20%28two%20modalities%20required%20for%20both%20training%20and%20inference%29%0Aeven%20if%20Bio-impedance%20alone%20has%20an%20average%20macro%20F1%20score%20of%2075.36%20%5C%25%2C%20which%20is%0Aoutperformed%20by%20IMU%20alone.%20In%20addition%2C%20similar%20results%20were%20obtained%20in%20an%0Aextended%20study%20on%20lower%20body%20fitness%20activity%20classification%2C%20demonstrating%20the%0Ageneralisability%20of%20our%20approach.Our%20findings%20underscore%20the%20potential%20of%0Asensor%20fusion%20and%20contrastive%20learning%20as%20valuable%20tools%20for%20advancing%20fitness%0Aactivity%20recognition%2C%20with%20bio-impedance%20playing%20a%20pivotal%20role%20in%20augmenting%0Athe%20capabilities%20of%20IMU-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMove%253A%2520Exploring%2520Bio-impedance%2520Sensing%2520for%2520Fitness%2520Activity%2520Recognition%26entry.906535625%3DMengxi%2520Liu%2520and%2520Vitor%2520Fortes%2520Rey%2520and%2520Yu%2520Zhang%2520and%2520Lala%2520Shakti%2520Swarup%2520Ray%2520and%2520Bo%2520Zhou%2520and%2520Paul%2520Lukowicz%26entry.1292438233%3D%2520%2520Automatic%2520and%2520precise%2520fitness%2520activity%2520recognition%2520can%2520be%2520beneficial%2520in%250Aaspects%2520from%2520promoting%2520a%2520healthy%2520lifestyle%2520to%2520personalized%2520preventative%250Ahealthcare.%2520While%2520IMUs%2520are%2520currently%2520the%2520prominent%2520fitness%2520tracking%2520modality%252C%250Athrough%2520iMove%252C%2520we%2520show%2520bio-impedence%2520can%2520help%2520improve%2520IMU-based%2520fitness%250Atracking%2520through%2520sensor%2520fusion%2520and%2520contrastive%2520learning.To%2520evaluate%2520our%250Amethods%252C%2520we%2520conducted%2520an%2520experiment%2520including%2520six%2520upper%2520body%2520fitness%2520activities%250Aperformed%2520by%2520ten%2520subjects%2520over%2520five%2520days%2520to%2520collect%2520synchronized%2520data%2520from%250Abio-impedance%2520across%2520two%2520wrists%2520and%2520IMU%2520on%2520the%2520left%2520wrist.The%2520contrastive%250Alearning%2520framework%2520uses%2520the%2520two%2520modalities%2520to%2520train%2520a%2520better%2520IMU-only%250Aclassification%2520model%252C%2520where%2520bio-impedance%2520is%2520only%2520required%2520at%2520the%2520training%250Aphase%252C%2520by%2520which%2520the%2520average%2520Macro%2520F1%2520score%2520with%2520the%2520input%2520of%2520a%2520single%2520IMU%2520was%250Aimproved%2520by%25203.22%2520%255C%2525%2520reaching%252084.71%2520%255C%2525%2520compared%2520to%2520the%252081.49%2520%255C%2525%2520of%2520the%2520IMU%250Abaseline%2520model.%2520We%2520have%2520also%2520shown%2520how%2520bio-impedance%2520can%2520improve%2520human%2520activity%250Arecognition%2520%2528HAR%2529%2520directly%2520through%2520sensor%2520fusion%252C%2520reaching%2520an%2520average%2520Macro%2520F1%250Ascore%2520of%252089.57%2520%255C%2525%2520%2528two%2520modalities%2520required%2520for%2520both%2520training%2520and%2520inference%2529%250Aeven%2520if%2520Bio-impedance%2520alone%2520has%2520an%2520average%2520macro%2520F1%2520score%2520of%252075.36%2520%255C%2525%252C%2520which%2520is%250Aoutperformed%2520by%2520IMU%2520alone.%2520In%2520addition%252C%2520similar%2520results%2520were%2520obtained%2520in%2520an%250Aextended%2520study%2520on%2520lower%2520body%2520fitness%2520activity%2520classification%252C%2520demonstrating%2520the%250Ageneralisability%2520of%2520our%2520approach.Our%2520findings%2520underscore%2520the%2520potential%2520of%250Asensor%2520fusion%2520and%2520contrastive%2520learning%2520as%2520valuable%2520tools%2520for%2520advancing%2520fitness%250Aactivity%2520recognition%252C%2520with%2520bio-impedance%2520playing%2520a%2520pivotal%2520role%2520in%2520augmenting%250Athe%2520capabilities%2520of%2520IMU-based%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMove%3A%20Exploring%20Bio-impedance%20Sensing%20for%20Fitness%20Activity%20Recognition&entry.906535625=Mengxi%20Liu%20and%20Vitor%20Fortes%20Rey%20and%20Yu%20Zhang%20and%20Lala%20Shakti%20Swarup%20Ray%20and%20Bo%20Zhou%20and%20Paul%20Lukowicz&entry.1292438233=%20%20Automatic%20and%20precise%20fitness%20activity%20recognition%20can%20be%20beneficial%20in%0Aaspects%20from%20promoting%20a%20healthy%20lifestyle%20to%20personalized%20preventative%0Ahealthcare.%20While%20IMUs%20are%20currently%20the%20prominent%20fitness%20tracking%20modality%2C%0Athrough%20iMove%2C%20we%20show%20bio-impedence%20can%20help%20improve%20IMU-based%20fitness%0Atracking%20through%20sensor%20fusion%20and%20contrastive%20learning.To%20evaluate%20our%0Amethods%2C%20we%20conducted%20an%20experiment%20including%20six%20upper%20body%20fitness%20activities%0Aperformed%20by%20ten%20subjects%20over%20five%20days%20to%20collect%20synchronized%20data%20from%0Abio-impedance%20across%20two%20wrists%20and%20IMU%20on%20the%20left%20wrist.The%20contrastive%0Alearning%20framework%20uses%20the%20two%20modalities%20to%20train%20a%20better%20IMU-only%0Aclassification%20model%2C%20where%20bio-impedance%20is%20only%20required%20at%20the%20training%0Aphase%2C%20by%20which%20the%20average%20Macro%20F1%20score%20with%20the%20input%20of%20a%20single%20IMU%20was%0Aimproved%20by%203.22%20%5C%25%20reaching%2084.71%20%5C%25%20compared%20to%20the%2081.49%20%5C%25%20of%20the%20IMU%0Abaseline%20model.%20We%20have%20also%20shown%20how%20bio-impedance%20can%20improve%20human%20activity%0Arecognition%20%28HAR%29%20directly%20through%20sensor%20fusion%2C%20reaching%20an%20average%20Macro%20F1%0Ascore%20of%2089.57%20%5C%25%20%28two%20modalities%20required%20for%20both%20training%20and%20inference%29%0Aeven%20if%20Bio-impedance%20alone%20has%20an%20average%20macro%20F1%20score%20of%2075.36%20%5C%25%2C%20which%20is%0Aoutperformed%20by%20IMU%20alone.%20In%20addition%2C%20similar%20results%20were%20obtained%20in%20an%0Aextended%20study%20on%20lower%20body%20fitness%20activity%20classification%2C%20demonstrating%20the%0Ageneralisability%20of%20our%20approach.Our%20findings%20underscore%20the%20potential%20of%0Asensor%20fusion%20and%20contrastive%20learning%20as%20valuable%20tools%20for%20advancing%20fitness%0Aactivity%20recognition%2C%20with%20bio-impedance%20playing%20a%20pivotal%20role%20in%20augmenting%0Athe%20capabilities%20of%20IMU-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09445v2&entry.124074799=Read"},
{"title": "Functional Bilevel Optimization for Machine Learning", "author": "Ieva Petrulionyte and Julien Mairal and Michael Arbel", "abstract": "  In this paper, we introduce a new functional point of view on bilevel\noptimization problems for machine learning, where the inner objective is\nminimized over a function space. These types of problems are most often solved\nby using methods developed in the parametric setting, where the inner objective\nis strongly convex with respect to the parameters of the prediction function.\nThe functional point of view does not rely on this assumption and notably\nallows using over-parameterized neural networks as the inner prediction\nfunction. We propose scalable and efficient algorithms for the functional\nbilevel optimization problem and illustrate the benefits of our approach on\ninstrumental regression and reinforcement learning tasks.\n", "link": "http://arxiv.org/abs/2403.20233v2", "date": "2024-06-03", "relevancy": 1.8237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4753}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functional%20Bilevel%20Optimization%20for%20Machine%20Learning&body=Title%3A%20Functional%20Bilevel%20Optimization%20for%20Machine%20Learning%0AAuthor%3A%20Ieva%20Petrulionyte%20and%20Julien%20Mairal%20and%20Michael%20Arbel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20functional%20point%20of%20view%20on%20bilevel%0Aoptimization%20problems%20for%20machine%20learning%2C%20where%20the%20inner%20objective%20is%0Aminimized%20over%20a%20function%20space.%20These%20types%20of%20problems%20are%20most%20often%20solved%0Aby%20using%20methods%20developed%20in%20the%20parametric%20setting%2C%20where%20the%20inner%20objective%0Ais%20strongly%20convex%20with%20respect%20to%20the%20parameters%20of%20the%20prediction%20function.%0AThe%20functional%20point%20of%20view%20does%20not%20rely%20on%20this%20assumption%20and%20notably%0Aallows%20using%20over-parameterized%20neural%20networks%20as%20the%20inner%20prediction%0Afunction.%20We%20propose%20scalable%20and%20efficient%20algorithms%20for%20the%20functional%0Abilevel%20optimization%20problem%20and%20illustrate%20the%20benefits%20of%20our%20approach%20on%0Ainstrumental%20regression%20and%20reinforcement%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctional%2520Bilevel%2520Optimization%2520for%2520Machine%2520Learning%26entry.906535625%3DIeva%2520Petrulionyte%2520and%2520Julien%2520Mairal%2520and%2520Michael%2520Arbel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520functional%2520point%2520of%2520view%2520on%2520bilevel%250Aoptimization%2520problems%2520for%2520machine%2520learning%252C%2520where%2520the%2520inner%2520objective%2520is%250Aminimized%2520over%2520a%2520function%2520space.%2520These%2520types%2520of%2520problems%2520are%2520most%2520often%2520solved%250Aby%2520using%2520methods%2520developed%2520in%2520the%2520parametric%2520setting%252C%2520where%2520the%2520inner%2520objective%250Ais%2520strongly%2520convex%2520with%2520respect%2520to%2520the%2520parameters%2520of%2520the%2520prediction%2520function.%250AThe%2520functional%2520point%2520of%2520view%2520does%2520not%2520rely%2520on%2520this%2520assumption%2520and%2520notably%250Aallows%2520using%2520over-parameterized%2520neural%2520networks%2520as%2520the%2520inner%2520prediction%250Afunction.%2520We%2520propose%2520scalable%2520and%2520efficient%2520algorithms%2520for%2520the%2520functional%250Abilevel%2520optimization%2520problem%2520and%2520illustrate%2520the%2520benefits%2520of%2520our%2520approach%2520on%250Ainstrumental%2520regression%2520and%2520reinforcement%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Bilevel%20Optimization%20for%20Machine%20Learning&entry.906535625=Ieva%20Petrulionyte%20and%20Julien%20Mairal%20and%20Michael%20Arbel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20functional%20point%20of%20view%20on%20bilevel%0Aoptimization%20problems%20for%20machine%20learning%2C%20where%20the%20inner%20objective%20is%0Aminimized%20over%20a%20function%20space.%20These%20types%20of%20problems%20are%20most%20often%20solved%0Aby%20using%20methods%20developed%20in%20the%20parametric%20setting%2C%20where%20the%20inner%20objective%0Ais%20strongly%20convex%20with%20respect%20to%20the%20parameters%20of%20the%20prediction%20function.%0AThe%20functional%20point%20of%20view%20does%20not%20rely%20on%20this%20assumption%20and%20notably%0Aallows%20using%20over-parameterized%20neural%20networks%20as%20the%20inner%20prediction%0Afunction.%20We%20propose%20scalable%20and%20efficient%20algorithms%20for%20the%20functional%0Abilevel%20optimization%20problem%20and%20illustrate%20the%20benefits%20of%20our%20approach%20on%0Ainstrumental%20regression%20and%20reinforcement%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20233v2&entry.124074799=Read"},
{"title": "Machine Learning with Confidential Computing: A Systematization of\n  Knowledge", "author": "Fan Mo and Zahra Tarkhani and Hamed Haddadi", "abstract": "  Privacy and security challenges in Machine Learning (ML) have become\nincreasingly severe, along with ML's pervasive development and the recent\ndemonstration of large attack surfaces. As a mature system-oriented approach,\nConfidential Computing has been utilized in both academia and industry to\nmitigate privacy and security issues in various ML scenarios. In this paper,\nthe conjunction between ML and Confidential Computing is investigated. We\nsystematize the prior work on Confidential Computing-assisted ML techniques\nthat provide i) confidentiality guarantees and ii) integrity assurances, and\ndiscuss their advanced features and drawbacks. Key challenges are further\nidentified, and we provide dedicated analyses of the limitations in existing\nTrusted Execution Environment (TEE) systems for ML use cases. Finally,\nprospective works are discussed, including grounded privacy definitions for\nclosed-loop protection, partitioned executions of efficient ML, dedicated\nTEE-assisted designs for ML, TEE-aware ML, and ML full pipeline guarantees. By\nproviding these potential solutions in our systematization of knowledge, we aim\nto build the bridge to help achieve a much stronger TEE-enabled ML for privacy\nguarantees without introducing computation and system costs.\n", "link": "http://arxiv.org/abs/2208.10134v3", "date": "2024-06-03", "relevancy": 1.8187, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20with%20Confidential%20Computing%3A%20A%20Systematization%20of%0A%20%20Knowledge&body=Title%3A%20Machine%20Learning%20with%20Confidential%20Computing%3A%20A%20Systematization%20of%0A%20%20Knowledge%0AAuthor%3A%20Fan%20Mo%20and%20Zahra%20Tarkhani%20and%20Hamed%20Haddadi%0AAbstract%3A%20%20%20Privacy%20and%20security%20challenges%20in%20Machine%20Learning%20%28ML%29%20have%20become%0Aincreasingly%20severe%2C%20along%20with%20ML%27s%20pervasive%20development%20and%20the%20recent%0Ademonstration%20of%20large%20attack%20surfaces.%20As%20a%20mature%20system-oriented%20approach%2C%0AConfidential%20Computing%20has%20been%20utilized%20in%20both%20academia%20and%20industry%20to%0Amitigate%20privacy%20and%20security%20issues%20in%20various%20ML%20scenarios.%20In%20this%20paper%2C%0Athe%20conjunction%20between%20ML%20and%20Confidential%20Computing%20is%20investigated.%20We%0Asystematize%20the%20prior%20work%20on%20Confidential%20Computing-assisted%20ML%20techniques%0Athat%20provide%20i%29%20confidentiality%20guarantees%20and%20ii%29%20integrity%20assurances%2C%20and%0Adiscuss%20their%20advanced%20features%20and%20drawbacks.%20Key%20challenges%20are%20further%0Aidentified%2C%20and%20we%20provide%20dedicated%20analyses%20of%20the%20limitations%20in%20existing%0ATrusted%20Execution%20Environment%20%28TEE%29%20systems%20for%20ML%20use%20cases.%20Finally%2C%0Aprospective%20works%20are%20discussed%2C%20including%20grounded%20privacy%20definitions%20for%0Aclosed-loop%20protection%2C%20partitioned%20executions%20of%20efficient%20ML%2C%20dedicated%0ATEE-assisted%20designs%20for%20ML%2C%20TEE-aware%20ML%2C%20and%20ML%20full%20pipeline%20guarantees.%20By%0Aproviding%20these%20potential%20solutions%20in%20our%20systematization%20of%20knowledge%2C%20we%20aim%0Ato%20build%20the%20bridge%20to%20help%20achieve%20a%20much%20stronger%20TEE-enabled%20ML%20for%20privacy%0Aguarantees%20without%20introducing%20computation%20and%20system%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.10134v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520with%2520Confidential%2520Computing%253A%2520A%2520Systematization%2520of%250A%2520%2520Knowledge%26entry.906535625%3DFan%2520Mo%2520and%2520Zahra%2520Tarkhani%2520and%2520Hamed%2520Haddadi%26entry.1292438233%3D%2520%2520Privacy%2520and%2520security%2520challenges%2520in%2520Machine%2520Learning%2520%2528ML%2529%2520have%2520become%250Aincreasingly%2520severe%252C%2520along%2520with%2520ML%2527s%2520pervasive%2520development%2520and%2520the%2520recent%250Ademonstration%2520of%2520large%2520attack%2520surfaces.%2520As%2520a%2520mature%2520system-oriented%2520approach%252C%250AConfidential%2520Computing%2520has%2520been%2520utilized%2520in%2520both%2520academia%2520and%2520industry%2520to%250Amitigate%2520privacy%2520and%2520security%2520issues%2520in%2520various%2520ML%2520scenarios.%2520In%2520this%2520paper%252C%250Athe%2520conjunction%2520between%2520ML%2520and%2520Confidential%2520Computing%2520is%2520investigated.%2520We%250Asystematize%2520the%2520prior%2520work%2520on%2520Confidential%2520Computing-assisted%2520ML%2520techniques%250Athat%2520provide%2520i%2529%2520confidentiality%2520guarantees%2520and%2520ii%2529%2520integrity%2520assurances%252C%2520and%250Adiscuss%2520their%2520advanced%2520features%2520and%2520drawbacks.%2520Key%2520challenges%2520are%2520further%250Aidentified%252C%2520and%2520we%2520provide%2520dedicated%2520analyses%2520of%2520the%2520limitations%2520in%2520existing%250ATrusted%2520Execution%2520Environment%2520%2528TEE%2529%2520systems%2520for%2520ML%2520use%2520cases.%2520Finally%252C%250Aprospective%2520works%2520are%2520discussed%252C%2520including%2520grounded%2520privacy%2520definitions%2520for%250Aclosed-loop%2520protection%252C%2520partitioned%2520executions%2520of%2520efficient%2520ML%252C%2520dedicated%250ATEE-assisted%2520designs%2520for%2520ML%252C%2520TEE-aware%2520ML%252C%2520and%2520ML%2520full%2520pipeline%2520guarantees.%2520By%250Aproviding%2520these%2520potential%2520solutions%2520in%2520our%2520systematization%2520of%2520knowledge%252C%2520we%2520aim%250Ato%2520build%2520the%2520bridge%2520to%2520help%2520achieve%2520a%2520much%2520stronger%2520TEE-enabled%2520ML%2520for%2520privacy%250Aguarantees%2520without%2520introducing%2520computation%2520and%2520system%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.10134v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20with%20Confidential%20Computing%3A%20A%20Systematization%20of%0A%20%20Knowledge&entry.906535625=Fan%20Mo%20and%20Zahra%20Tarkhani%20and%20Hamed%20Haddadi&entry.1292438233=%20%20Privacy%20and%20security%20challenges%20in%20Machine%20Learning%20%28ML%29%20have%20become%0Aincreasingly%20severe%2C%20along%20with%20ML%27s%20pervasive%20development%20and%20the%20recent%0Ademonstration%20of%20large%20attack%20surfaces.%20As%20a%20mature%20system-oriented%20approach%2C%0AConfidential%20Computing%20has%20been%20utilized%20in%20both%20academia%20and%20industry%20to%0Amitigate%20privacy%20and%20security%20issues%20in%20various%20ML%20scenarios.%20In%20this%20paper%2C%0Athe%20conjunction%20between%20ML%20and%20Confidential%20Computing%20is%20investigated.%20We%0Asystematize%20the%20prior%20work%20on%20Confidential%20Computing-assisted%20ML%20techniques%0Athat%20provide%20i%29%20confidentiality%20guarantees%20and%20ii%29%20integrity%20assurances%2C%20and%0Adiscuss%20their%20advanced%20features%20and%20drawbacks.%20Key%20challenges%20are%20further%0Aidentified%2C%20and%20we%20provide%20dedicated%20analyses%20of%20the%20limitations%20in%20existing%0ATrusted%20Execution%20Environment%20%28TEE%29%20systems%20for%20ML%20use%20cases.%20Finally%2C%0Aprospective%20works%20are%20discussed%2C%20including%20grounded%20privacy%20definitions%20for%0Aclosed-loop%20protection%2C%20partitioned%20executions%20of%20efficient%20ML%2C%20dedicated%0ATEE-assisted%20designs%20for%20ML%2C%20TEE-aware%20ML%2C%20and%20ML%20full%20pipeline%20guarantees.%20By%0Aproviding%20these%20potential%20solutions%20in%20our%20systematization%20of%20knowledge%2C%20we%20aim%0Ato%20build%20the%20bridge%20to%20help%20achieve%20a%20much%20stronger%20TEE-enabled%20ML%20for%20privacy%0Aguarantees%20without%20introducing%20computation%20and%20system%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.10134v3&entry.124074799=Read"},
{"title": "Clover: Closed-Loop Verifiable Code Generation", "author": "Chuyue Sun and Ying Sheng and Oded Padon and Clark Barrett", "abstract": "  The use of large language models for code generation is a rapidly growing\ntrend in software development. However, without effective methods for ensuring\nthe correctness of generated code, this trend could lead to any number of\nundesirable outcomes. In this paper, we lay out a vision for addressing this\nchallenge: the Clover paradigm, short for Closed-Loop Verifiable Code\nGeneration, which reduces correctness checking to the more accessible problem\nof consistency checking. At the core of Clover lies a checker that performs\nconsistency checks among code, docstrings, and formal annotations. The checker\nis implemented using a novel integration of formal verification tools and large\nlanguage models. We provide a theoretical analysis to support our thesis that\nClover should be effective at consistency checking. We also empirically\ninvestigate its feasibility on a hand-designed dataset (CloverBench) featuring\nannotated Dafny programs at a textbook level of difficulty. Experimental\nresults show that for this dataset, (i) LLMs are reasonably successful at\nautomatically generating formal specifications; and (ii) our consistency\nchecker achieves a promising acceptance rate (up to 87%) for correct instances\nwhile maintaining zero tolerance for incorrect ones (no false positives).\n", "link": "http://arxiv.org/abs/2310.17807v3", "date": "2024-06-03", "relevancy": 1.8181, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4759}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4398}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clover%3A%20Closed-Loop%20Verifiable%20Code%20Generation&body=Title%3A%20Clover%3A%20Closed-Loop%20Verifiable%20Code%20Generation%0AAuthor%3A%20Chuyue%20Sun%20and%20Ying%20Sheng%20and%20Oded%20Padon%20and%20Clark%20Barrett%0AAbstract%3A%20%20%20The%20use%20of%20large%20language%20models%20for%20code%20generation%20is%20a%20rapidly%20growing%0Atrend%20in%20software%20development.%20However%2C%20without%20effective%20methods%20for%20ensuring%0Athe%20correctness%20of%20generated%20code%2C%20this%20trend%20could%20lead%20to%20any%20number%20of%0Aundesirable%20outcomes.%20In%20this%20paper%2C%20we%20lay%20out%20a%20vision%20for%20addressing%20this%0Achallenge%3A%20the%20Clover%20paradigm%2C%20short%20for%20Closed-Loop%20Verifiable%20Code%0AGeneration%2C%20which%20reduces%20correctness%20checking%20to%20the%20more%20accessible%20problem%0Aof%20consistency%20checking.%20At%20the%20core%20of%20Clover%20lies%20a%20checker%20that%20performs%0Aconsistency%20checks%20among%20code%2C%20docstrings%2C%20and%20formal%20annotations.%20The%20checker%0Ais%20implemented%20using%20a%20novel%20integration%20of%20formal%20verification%20tools%20and%20large%0Alanguage%20models.%20We%20provide%20a%20theoretical%20analysis%20to%20support%20our%20thesis%20that%0AClover%20should%20be%20effective%20at%20consistency%20checking.%20We%20also%20empirically%0Ainvestigate%20its%20feasibility%20on%20a%20hand-designed%20dataset%20%28CloverBench%29%20featuring%0Aannotated%20Dafny%20programs%20at%20a%20textbook%20level%20of%20difficulty.%20Experimental%0Aresults%20show%20that%20for%20this%20dataset%2C%20%28i%29%20LLMs%20are%20reasonably%20successful%20at%0Aautomatically%20generating%20formal%20specifications%3B%20and%20%28ii%29%20our%20consistency%0Achecker%20achieves%20a%20promising%20acceptance%20rate%20%28up%20to%2087%25%29%20for%20correct%20instances%0Awhile%20maintaining%20zero%20tolerance%20for%20incorrect%20ones%20%28no%20false%20positives%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17807v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClover%253A%2520Closed-Loop%2520Verifiable%2520Code%2520Generation%26entry.906535625%3DChuyue%2520Sun%2520and%2520Ying%2520Sheng%2520and%2520Oded%2520Padon%2520and%2520Clark%2520Barrett%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520large%2520language%2520models%2520for%2520code%2520generation%2520is%2520a%2520rapidly%2520growing%250Atrend%2520in%2520software%2520development.%2520However%252C%2520without%2520effective%2520methods%2520for%2520ensuring%250Athe%2520correctness%2520of%2520generated%2520code%252C%2520this%2520trend%2520could%2520lead%2520to%2520any%2520number%2520of%250Aundesirable%2520outcomes.%2520In%2520this%2520paper%252C%2520we%2520lay%2520out%2520a%2520vision%2520for%2520addressing%2520this%250Achallenge%253A%2520the%2520Clover%2520paradigm%252C%2520short%2520for%2520Closed-Loop%2520Verifiable%2520Code%250AGeneration%252C%2520which%2520reduces%2520correctness%2520checking%2520to%2520the%2520more%2520accessible%2520problem%250Aof%2520consistency%2520checking.%2520At%2520the%2520core%2520of%2520Clover%2520lies%2520a%2520checker%2520that%2520performs%250Aconsistency%2520checks%2520among%2520code%252C%2520docstrings%252C%2520and%2520formal%2520annotations.%2520The%2520checker%250Ais%2520implemented%2520using%2520a%2520novel%2520integration%2520of%2520formal%2520verification%2520tools%2520and%2520large%250Alanguage%2520models.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520to%2520support%2520our%2520thesis%2520that%250AClover%2520should%2520be%2520effective%2520at%2520consistency%2520checking.%2520We%2520also%2520empirically%250Ainvestigate%2520its%2520feasibility%2520on%2520a%2520hand-designed%2520dataset%2520%2528CloverBench%2529%2520featuring%250Aannotated%2520Dafny%2520programs%2520at%2520a%2520textbook%2520level%2520of%2520difficulty.%2520Experimental%250Aresults%2520show%2520that%2520for%2520this%2520dataset%252C%2520%2528i%2529%2520LLMs%2520are%2520reasonably%2520successful%2520at%250Aautomatically%2520generating%2520formal%2520specifications%253B%2520and%2520%2528ii%2529%2520our%2520consistency%250Achecker%2520achieves%2520a%2520promising%2520acceptance%2520rate%2520%2528up%2520to%252087%2525%2529%2520for%2520correct%2520instances%250Awhile%2520maintaining%2520zero%2520tolerance%2520for%2520incorrect%2520ones%2520%2528no%2520false%2520positives%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17807v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clover%3A%20Closed-Loop%20Verifiable%20Code%20Generation&entry.906535625=Chuyue%20Sun%20and%20Ying%20Sheng%20and%20Oded%20Padon%20and%20Clark%20Barrett&entry.1292438233=%20%20The%20use%20of%20large%20language%20models%20for%20code%20generation%20is%20a%20rapidly%20growing%0Atrend%20in%20software%20development.%20However%2C%20without%20effective%20methods%20for%20ensuring%0Athe%20correctness%20of%20generated%20code%2C%20this%20trend%20could%20lead%20to%20any%20number%20of%0Aundesirable%20outcomes.%20In%20this%20paper%2C%20we%20lay%20out%20a%20vision%20for%20addressing%20this%0Achallenge%3A%20the%20Clover%20paradigm%2C%20short%20for%20Closed-Loop%20Verifiable%20Code%0AGeneration%2C%20which%20reduces%20correctness%20checking%20to%20the%20more%20accessible%20problem%0Aof%20consistency%20checking.%20At%20the%20core%20of%20Clover%20lies%20a%20checker%20that%20performs%0Aconsistency%20checks%20among%20code%2C%20docstrings%2C%20and%20formal%20annotations.%20The%20checker%0Ais%20implemented%20using%20a%20novel%20integration%20of%20formal%20verification%20tools%20and%20large%0Alanguage%20models.%20We%20provide%20a%20theoretical%20analysis%20to%20support%20our%20thesis%20that%0AClover%20should%20be%20effective%20at%20consistency%20checking.%20We%20also%20empirically%0Ainvestigate%20its%20feasibility%20on%20a%20hand-designed%20dataset%20%28CloverBench%29%20featuring%0Aannotated%20Dafny%20programs%20at%20a%20textbook%20level%20of%20difficulty.%20Experimental%0Aresults%20show%20that%20for%20this%20dataset%2C%20%28i%29%20LLMs%20are%20reasonably%20successful%20at%0Aautomatically%20generating%20formal%20specifications%3B%20and%20%28ii%29%20our%20consistency%0Achecker%20achieves%20a%20promising%20acceptance%20rate%20%28up%20to%2087%25%29%20for%20correct%20instances%0Awhile%20maintaining%20zero%20tolerance%20for%20incorrect%20ones%20%28no%20false%20positives%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17807v3&entry.124074799=Read"},
{"title": "PowerGraph: A power grid benchmark dataset for graph neural networks", "author": "Anna Varbella and Kenza Amara and Blazhe Gjorgiev and Mennatallah El-Assady and Giovanni Sansavini", "abstract": "  Power grids are critical infrastructures of paramount importance to modern\nsociety and, therefore, engineered to operate under diverse conditions and\nfailures. The ongoing energy transition poses new challenges for the\ndecision-makers and system operators. Therefore, we must develop grid analysis\nalgorithms to ensure reliable operations. These key tools include power flow\nanalysis and system security analysis, both needed for effective operational\nand strategic planning. The literature review shows a growing trend of machine\nlearning (ML) models that perform these analyses effectively. In particular,\nGraph Neural Networks (GNNs) stand out in such applications because of the\ngraph-based structure of power grids. However, there is a lack of publicly\navailable graph datasets for training and benchmarking ML models in electrical\npower grid applications. First, we present PowerGraph, which comprises\nGNN-tailored datasets for i) power flows, ii) optimal power flows, and iii)\ncascading failure analyses of power grids. Second, we provide ground-truth\nexplanations for the cascading failure analysis. Finally, we perform a complete\nbenchmarking of GNN methods for node-level and graph-level tasks and\nexplainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse\ntasks that includes power flow and fault scenarios with real-world\nexplanations, providing a valuable resource for developing improved GNN models\nfor node-level, graph-level tasks and explainability methods in power system\nmodeling. The dataset is available at\nhttps://figshare.com/articles/dataset/PowerGraph/22820534 and the code at\nhttps://github.com/PowerGraph-Datasets.\n", "link": "http://arxiv.org/abs/2402.02827v2", "date": "2024-06-03", "relevancy": 1.8144, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4808}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PowerGraph%3A%20A%20power%20grid%20benchmark%20dataset%20for%20graph%20neural%20networks&body=Title%3A%20PowerGraph%3A%20A%20power%20grid%20benchmark%20dataset%20for%20graph%20neural%20networks%0AAuthor%3A%20Anna%20Varbella%20and%20Kenza%20Amara%20and%20Blazhe%20Gjorgiev%20and%20Mennatallah%20El-Assady%20and%20Giovanni%20Sansavini%0AAbstract%3A%20%20%20Power%20grids%20are%20critical%20infrastructures%20of%20paramount%20importance%20to%20modern%0Asociety%20and%2C%20therefore%2C%20engineered%20to%20operate%20under%20diverse%20conditions%20and%0Afailures.%20The%20ongoing%20energy%20transition%20poses%20new%20challenges%20for%20the%0Adecision-makers%20and%20system%20operators.%20Therefore%2C%20we%20must%20develop%20grid%20analysis%0Aalgorithms%20to%20ensure%20reliable%20operations.%20These%20key%20tools%20include%20power%20flow%0Aanalysis%20and%20system%20security%20analysis%2C%20both%20needed%20for%20effective%20operational%0Aand%20strategic%20planning.%20The%20literature%20review%20shows%20a%20growing%20trend%20of%20machine%0Alearning%20%28ML%29%20models%20that%20perform%20these%20analyses%20effectively.%20In%20particular%2C%0AGraph%20Neural%20Networks%20%28GNNs%29%20stand%20out%20in%20such%20applications%20because%20of%20the%0Agraph-based%20structure%20of%20power%20grids.%20However%2C%20there%20is%20a%20lack%20of%20publicly%0Aavailable%20graph%20datasets%20for%20training%20and%20benchmarking%20ML%20models%20in%20electrical%0Apower%20grid%20applications.%20First%2C%20we%20present%20PowerGraph%2C%20which%20comprises%0AGNN-tailored%20datasets%20for%20i%29%20power%20flows%2C%20ii%29%20optimal%20power%20flows%2C%20and%20iii%29%0Acascading%20failure%20analyses%20of%20power%20grids.%20Second%2C%20we%20provide%20ground-truth%0Aexplanations%20for%20the%20cascading%20failure%20analysis.%20Finally%2C%20we%20perform%20a%20complete%0Abenchmarking%20of%20GNN%20methods%20for%20node-level%20and%20graph-level%20tasks%20and%0Aexplainability.%20Overall%2C%20PowerGraph%20is%20a%20multifaceted%20GNN%20dataset%20for%20diverse%0Atasks%20that%20includes%20power%20flow%20and%20fault%20scenarios%20with%20real-world%0Aexplanations%2C%20providing%20a%20valuable%20resource%20for%20developing%20improved%20GNN%20models%0Afor%20node-level%2C%20graph-level%20tasks%20and%20explainability%20methods%20in%20power%20system%0Amodeling.%20The%20dataset%20is%20available%20at%0Ahttps%3A//figshare.com/articles/dataset/PowerGraph/22820534%20and%20the%20code%20at%0Ahttps%3A//github.com/PowerGraph-Datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPowerGraph%253A%2520A%2520power%2520grid%2520benchmark%2520dataset%2520for%2520graph%2520neural%2520networks%26entry.906535625%3DAnna%2520Varbella%2520and%2520Kenza%2520Amara%2520and%2520Blazhe%2520Gjorgiev%2520and%2520Mennatallah%2520El-Assady%2520and%2520Giovanni%2520Sansavini%26entry.1292438233%3D%2520%2520Power%2520grids%2520are%2520critical%2520infrastructures%2520of%2520paramount%2520importance%2520to%2520modern%250Asociety%2520and%252C%2520therefore%252C%2520engineered%2520to%2520operate%2520under%2520diverse%2520conditions%2520and%250Afailures.%2520The%2520ongoing%2520energy%2520transition%2520poses%2520new%2520challenges%2520for%2520the%250Adecision-makers%2520and%2520system%2520operators.%2520Therefore%252C%2520we%2520must%2520develop%2520grid%2520analysis%250Aalgorithms%2520to%2520ensure%2520reliable%2520operations.%2520These%2520key%2520tools%2520include%2520power%2520flow%250Aanalysis%2520and%2520system%2520security%2520analysis%252C%2520both%2520needed%2520for%2520effective%2520operational%250Aand%2520strategic%2520planning.%2520The%2520literature%2520review%2520shows%2520a%2520growing%2520trend%2520of%2520machine%250Alearning%2520%2528ML%2529%2520models%2520that%2520perform%2520these%2520analyses%2520effectively.%2520In%2520particular%252C%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520stand%2520out%2520in%2520such%2520applications%2520because%2520of%2520the%250Agraph-based%2520structure%2520of%2520power%2520grids.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520publicly%250Aavailable%2520graph%2520datasets%2520for%2520training%2520and%2520benchmarking%2520ML%2520models%2520in%2520electrical%250Apower%2520grid%2520applications.%2520First%252C%2520we%2520present%2520PowerGraph%252C%2520which%2520comprises%250AGNN-tailored%2520datasets%2520for%2520i%2529%2520power%2520flows%252C%2520ii%2529%2520optimal%2520power%2520flows%252C%2520and%2520iii%2529%250Acascading%2520failure%2520analyses%2520of%2520power%2520grids.%2520Second%252C%2520we%2520provide%2520ground-truth%250Aexplanations%2520for%2520the%2520cascading%2520failure%2520analysis.%2520Finally%252C%2520we%2520perform%2520a%2520complete%250Abenchmarking%2520of%2520GNN%2520methods%2520for%2520node-level%2520and%2520graph-level%2520tasks%2520and%250Aexplainability.%2520Overall%252C%2520PowerGraph%2520is%2520a%2520multifaceted%2520GNN%2520dataset%2520for%2520diverse%250Atasks%2520that%2520includes%2520power%2520flow%2520and%2520fault%2520scenarios%2520with%2520real-world%250Aexplanations%252C%2520providing%2520a%2520valuable%2520resource%2520for%2520developing%2520improved%2520GNN%2520models%250Afor%2520node-level%252C%2520graph-level%2520tasks%2520and%2520explainability%2520methods%2520in%2520power%2520system%250Amodeling.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//figshare.com/articles/dataset/PowerGraph/22820534%2520and%2520the%2520code%2520at%250Ahttps%253A//github.com/PowerGraph-Datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PowerGraph%3A%20A%20power%20grid%20benchmark%20dataset%20for%20graph%20neural%20networks&entry.906535625=Anna%20Varbella%20and%20Kenza%20Amara%20and%20Blazhe%20Gjorgiev%20and%20Mennatallah%20El-Assady%20and%20Giovanni%20Sansavini&entry.1292438233=%20%20Power%20grids%20are%20critical%20infrastructures%20of%20paramount%20importance%20to%20modern%0Asociety%20and%2C%20therefore%2C%20engineered%20to%20operate%20under%20diverse%20conditions%20and%0Afailures.%20The%20ongoing%20energy%20transition%20poses%20new%20challenges%20for%20the%0Adecision-makers%20and%20system%20operators.%20Therefore%2C%20we%20must%20develop%20grid%20analysis%0Aalgorithms%20to%20ensure%20reliable%20operations.%20These%20key%20tools%20include%20power%20flow%0Aanalysis%20and%20system%20security%20analysis%2C%20both%20needed%20for%20effective%20operational%0Aand%20strategic%20planning.%20The%20literature%20review%20shows%20a%20growing%20trend%20of%20machine%0Alearning%20%28ML%29%20models%20that%20perform%20these%20analyses%20effectively.%20In%20particular%2C%0AGraph%20Neural%20Networks%20%28GNNs%29%20stand%20out%20in%20such%20applications%20because%20of%20the%0Agraph-based%20structure%20of%20power%20grids.%20However%2C%20there%20is%20a%20lack%20of%20publicly%0Aavailable%20graph%20datasets%20for%20training%20and%20benchmarking%20ML%20models%20in%20electrical%0Apower%20grid%20applications.%20First%2C%20we%20present%20PowerGraph%2C%20which%20comprises%0AGNN-tailored%20datasets%20for%20i%29%20power%20flows%2C%20ii%29%20optimal%20power%20flows%2C%20and%20iii%29%0Acascading%20failure%20analyses%20of%20power%20grids.%20Second%2C%20we%20provide%20ground-truth%0Aexplanations%20for%20the%20cascading%20failure%20analysis.%20Finally%2C%20we%20perform%20a%20complete%0Abenchmarking%20of%20GNN%20methods%20for%20node-level%20and%20graph-level%20tasks%20and%0Aexplainability.%20Overall%2C%20PowerGraph%20is%20a%20multifaceted%20GNN%20dataset%20for%20diverse%0Atasks%20that%20includes%20power%20flow%20and%20fault%20scenarios%20with%20real-world%0Aexplanations%2C%20providing%20a%20valuable%20resource%20for%20developing%20improved%20GNN%20models%0Afor%20node-level%2C%20graph-level%20tasks%20and%20explainability%20methods%20in%20power%20system%0Amodeling.%20The%20dataset%20is%20available%20at%0Ahttps%3A//figshare.com/articles/dataset/PowerGraph/22820534%20and%20the%20code%20at%0Ahttps%3A//github.com/PowerGraph-Datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02827v2&entry.124074799=Read"},
{"title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation", "author": "Abdelrhman Werby and Chenguang Huang and Martin B\u00fcchner and Abhinav Valada and Wolfram Burgard", "abstract": "  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n", "link": "http://arxiv.org/abs/2403.17846v2", "date": "2024-06-03", "relevancy": 1.8004, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6025}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation&body=Title%3A%20Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation%0AAuthor%3A%20Abdelrhman%20Werby%20and%20Chenguang%20Huang%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Recent%20open-vocabulary%20robot%20mapping%20methods%20enrich%20dense%20geometric%20maps%20with%0Apre-trained%20visual-language%20features.%20While%20these%20maps%20allow%20for%20the%20prediction%0Aof%20point-wise%20saliency%20maps%20when%20queried%20for%20a%20certain%20language%20concept%2C%0Alarge-scale%20environments%20and%20abstract%20queries%20beyond%20the%20object%20level%20still%0Apose%20a%20considerable%20hurdle%2C%20ultimately%20limiting%20language-grounded%20robotic%0Anavigation.%20In%20this%20work%2C%20we%20present%20HOV-SG%2C%20a%20hierarchical%20open-vocabulary%203D%0Ascene%20graph%20mapping%20approach%20for%20language-grounded%20robot%20navigation.%20Leveraging%0Aopen-vocabulary%20vision%20foundation%20models%2C%20we%20first%20obtain%20state-of-the-art%0Aopen-vocabulary%20segment-level%20maps%20in%203D%20and%20subsequently%20construct%20a%203D%20scene%0Agraph%20hierarchy%20consisting%20of%20floor%2C%20room%2C%20and%20object%20concepts%2C%20each%20enriched%0Awith%20open-vocabulary%20features.%20Our%20approach%20is%20able%20to%20represent%20multi-story%0Abuildings%20and%20allows%20robotic%20traversal%20of%20those%20using%20a%20cross-floor%20Voronoi%0Agraph.%20HOV-SG%20is%20evaluated%20on%20three%20distinct%20datasets%20and%20surpasses%20previous%0Abaselines%20in%20open-vocabulary%20semantic%20accuracy%20on%20the%20object%2C%20room%2C%20and%20floor%0Alevel%20while%20producing%20a%2075%25%20reduction%20in%20representation%20size%20compared%20to%20dense%0Aopen-vocabulary%20maps.%20In%20order%20to%20prove%20the%20efficacy%20and%20generalization%0Acapabilities%20of%20HOV-SG%2C%20we%20showcase%20successful%20long-horizon%0Alanguage-conditioned%20robot%20navigation%20within%20real-world%20multi-storage%0Aenvironments.%20We%20provide%20code%20and%20trial%20video%20data%20at%20http%3A//hovsg.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Open-Vocabulary%25203D%2520Scene%2520Graphs%2520for%2520Language-Grounded%2520Robot%250A%2520%2520Navigation%26entry.906535625%3DAbdelrhman%2520Werby%2520and%2520Chenguang%2520Huang%2520and%2520Martin%2520B%25C3%25BCchner%2520and%2520Abhinav%2520Valada%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Recent%2520open-vocabulary%2520robot%2520mapping%2520methods%2520enrich%2520dense%2520geometric%2520maps%2520with%250Apre-trained%2520visual-language%2520features.%2520While%2520these%2520maps%2520allow%2520for%2520the%2520prediction%250Aof%2520point-wise%2520saliency%2520maps%2520when%2520queried%2520for%2520a%2520certain%2520language%2520concept%252C%250Alarge-scale%2520environments%2520and%2520abstract%2520queries%2520beyond%2520the%2520object%2520level%2520still%250Apose%2520a%2520considerable%2520hurdle%252C%2520ultimately%2520limiting%2520language-grounded%2520robotic%250Anavigation.%2520In%2520this%2520work%252C%2520we%2520present%2520HOV-SG%252C%2520a%2520hierarchical%2520open-vocabulary%25203D%250Ascene%2520graph%2520mapping%2520approach%2520for%2520language-grounded%2520robot%2520navigation.%2520Leveraging%250Aopen-vocabulary%2520vision%2520foundation%2520models%252C%2520we%2520first%2520obtain%2520state-of-the-art%250Aopen-vocabulary%2520segment-level%2520maps%2520in%25203D%2520and%2520subsequently%2520construct%2520a%25203D%2520scene%250Agraph%2520hierarchy%2520consisting%2520of%2520floor%252C%2520room%252C%2520and%2520object%2520concepts%252C%2520each%2520enriched%250Awith%2520open-vocabulary%2520features.%2520Our%2520approach%2520is%2520able%2520to%2520represent%2520multi-story%250Abuildings%2520and%2520allows%2520robotic%2520traversal%2520of%2520those%2520using%2520a%2520cross-floor%2520Voronoi%250Agraph.%2520HOV-SG%2520is%2520evaluated%2520on%2520three%2520distinct%2520datasets%2520and%2520surpasses%2520previous%250Abaselines%2520in%2520open-vocabulary%2520semantic%2520accuracy%2520on%2520the%2520object%252C%2520room%252C%2520and%2520floor%250Alevel%2520while%2520producing%2520a%252075%2525%2520reduction%2520in%2520representation%2520size%2520compared%2520to%2520dense%250Aopen-vocabulary%2520maps.%2520In%2520order%2520to%2520prove%2520the%2520efficacy%2520and%2520generalization%250Acapabilities%2520of%2520HOV-SG%252C%2520we%2520showcase%2520successful%2520long-horizon%250Alanguage-conditioned%2520robot%2520navigation%2520within%2520real-world%2520multi-storage%250Aenvironments.%2520We%2520provide%2520code%2520and%2520trial%2520video%2520data%2520at%2520http%253A//hovsg.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation&entry.906535625=Abdelrhman%20Werby%20and%20Chenguang%20Huang%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Recent%20open-vocabulary%20robot%20mapping%20methods%20enrich%20dense%20geometric%20maps%20with%0Apre-trained%20visual-language%20features.%20While%20these%20maps%20allow%20for%20the%20prediction%0Aof%20point-wise%20saliency%20maps%20when%20queried%20for%20a%20certain%20language%20concept%2C%0Alarge-scale%20environments%20and%20abstract%20queries%20beyond%20the%20object%20level%20still%0Apose%20a%20considerable%20hurdle%2C%20ultimately%20limiting%20language-grounded%20robotic%0Anavigation.%20In%20this%20work%2C%20we%20present%20HOV-SG%2C%20a%20hierarchical%20open-vocabulary%203D%0Ascene%20graph%20mapping%20approach%20for%20language-grounded%20robot%20navigation.%20Leveraging%0Aopen-vocabulary%20vision%20foundation%20models%2C%20we%20first%20obtain%20state-of-the-art%0Aopen-vocabulary%20segment-level%20maps%20in%203D%20and%20subsequently%20construct%20a%203D%20scene%0Agraph%20hierarchy%20consisting%20of%20floor%2C%20room%2C%20and%20object%20concepts%2C%20each%20enriched%0Awith%20open-vocabulary%20features.%20Our%20approach%20is%20able%20to%20represent%20multi-story%0Abuildings%20and%20allows%20robotic%20traversal%20of%20those%20using%20a%20cross-floor%20Voronoi%0Agraph.%20HOV-SG%20is%20evaluated%20on%20three%20distinct%20datasets%20and%20surpasses%20previous%0Abaselines%20in%20open-vocabulary%20semantic%20accuracy%20on%20the%20object%2C%20room%2C%20and%20floor%0Alevel%20while%20producing%20a%2075%25%20reduction%20in%20representation%20size%20compared%20to%20dense%0Aopen-vocabulary%20maps.%20In%20order%20to%20prove%20the%20efficacy%20and%20generalization%0Acapabilities%20of%20HOV-SG%2C%20we%20showcase%20successful%20long-horizon%0Alanguage-conditioned%20robot%20navigation%20within%20real-world%20multi-storage%0Aenvironments.%20We%20provide%20code%20and%20trial%20video%20data%20at%20http%3A//hovsg.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17846v2&entry.124074799=Read"},
{"title": "Interpreting and Improving Diffusion Models from an Optimization\n  Perspective", "author": "Frank Permenter and Chenyang Yuan", "abstract": "  Denoising is intuitively related to projection. Indeed, under the manifold\nhypothesis, adding random noise is approximately equivalent to orthogonal\nperturbation. Hence, learning to denoise is approximately learning to project.\nIn this paper, we use this observation to interpret denoising diffusion models\nas approximate gradient descent applied to the Euclidean distance function. We\nthen provide straight-forward convergence analysis of the DDIM sampler under\nsimple assumptions on the projection error of the denoiser. Finally, we propose\na new gradient-estimation sampler, generalizing DDIM using insights from our\ntheoretical results. In as few as 5-10 function evaluations, our sampler\nachieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models\nand can generate high quality samples on latent diffusion models.\n", "link": "http://arxiv.org/abs/2306.04848v4", "date": "2024-06-03", "relevancy": 1.7778, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6213}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5856}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20and%20Improving%20Diffusion%20Models%20from%20an%20Optimization%0A%20%20Perspective&body=Title%3A%20Interpreting%20and%20Improving%20Diffusion%20Models%20from%20an%20Optimization%0A%20%20Perspective%0AAuthor%3A%20Frank%20Permenter%20and%20Chenyang%20Yuan%0AAbstract%3A%20%20%20Denoising%20is%20intuitively%20related%20to%20projection.%20Indeed%2C%20under%20the%20manifold%0Ahypothesis%2C%20adding%20random%20noise%20is%20approximately%20equivalent%20to%20orthogonal%0Aperturbation.%20Hence%2C%20learning%20to%20denoise%20is%20approximately%20learning%20to%20project.%0AIn%20this%20paper%2C%20we%20use%20this%20observation%20to%20interpret%20denoising%20diffusion%20models%0Aas%20approximate%20gradient%20descent%20applied%20to%20the%20Euclidean%20distance%20function.%20We%0Athen%20provide%20straight-forward%20convergence%20analysis%20of%20the%20DDIM%20sampler%20under%0Asimple%20assumptions%20on%20the%20projection%20error%20of%20the%20denoiser.%20Finally%2C%20we%20propose%0Aa%20new%20gradient-estimation%20sampler%2C%20generalizing%20DDIM%20using%20insights%20from%20our%0Atheoretical%20results.%20In%20as%20few%20as%205-10%20function%20evaluations%2C%20our%20sampler%0Aachieves%20state-of-the-art%20FID%20scores%20on%20pretrained%20CIFAR-10%20and%20CelebA%20models%0Aand%20can%20generate%20high%20quality%20samples%20on%20latent%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04848v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520and%2520Improving%2520Diffusion%2520Models%2520from%2520an%2520Optimization%250A%2520%2520Perspective%26entry.906535625%3DFrank%2520Permenter%2520and%2520Chenyang%2520Yuan%26entry.1292438233%3D%2520%2520Denoising%2520is%2520intuitively%2520related%2520to%2520projection.%2520Indeed%252C%2520under%2520the%2520manifold%250Ahypothesis%252C%2520adding%2520random%2520noise%2520is%2520approximately%2520equivalent%2520to%2520orthogonal%250Aperturbation.%2520Hence%252C%2520learning%2520to%2520denoise%2520is%2520approximately%2520learning%2520to%2520project.%250AIn%2520this%2520paper%252C%2520we%2520use%2520this%2520observation%2520to%2520interpret%2520denoising%2520diffusion%2520models%250Aas%2520approximate%2520gradient%2520descent%2520applied%2520to%2520the%2520Euclidean%2520distance%2520function.%2520We%250Athen%2520provide%2520straight-forward%2520convergence%2520analysis%2520of%2520the%2520DDIM%2520sampler%2520under%250Asimple%2520assumptions%2520on%2520the%2520projection%2520error%2520of%2520the%2520denoiser.%2520Finally%252C%2520we%2520propose%250Aa%2520new%2520gradient-estimation%2520sampler%252C%2520generalizing%2520DDIM%2520using%2520insights%2520from%2520our%250Atheoretical%2520results.%2520In%2520as%2520few%2520as%25205-10%2520function%2520evaluations%252C%2520our%2520sampler%250Aachieves%2520state-of-the-art%2520FID%2520scores%2520on%2520pretrained%2520CIFAR-10%2520and%2520CelebA%2520models%250Aand%2520can%2520generate%2520high%2520quality%2520samples%2520on%2520latent%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04848v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20and%20Improving%20Diffusion%20Models%20from%20an%20Optimization%0A%20%20Perspective&entry.906535625=Frank%20Permenter%20and%20Chenyang%20Yuan&entry.1292438233=%20%20Denoising%20is%20intuitively%20related%20to%20projection.%20Indeed%2C%20under%20the%20manifold%0Ahypothesis%2C%20adding%20random%20noise%20is%20approximately%20equivalent%20to%20orthogonal%0Aperturbation.%20Hence%2C%20learning%20to%20denoise%20is%20approximately%20learning%20to%20project.%0AIn%20this%20paper%2C%20we%20use%20this%20observation%20to%20interpret%20denoising%20diffusion%20models%0Aas%20approximate%20gradient%20descent%20applied%20to%20the%20Euclidean%20distance%20function.%20We%0Athen%20provide%20straight-forward%20convergence%20analysis%20of%20the%20DDIM%20sampler%20under%0Asimple%20assumptions%20on%20the%20projection%20error%20of%20the%20denoiser.%20Finally%2C%20we%20propose%0Aa%20new%20gradient-estimation%20sampler%2C%20generalizing%20DDIM%20using%20insights%20from%20our%0Atheoretical%20results.%20In%20as%20few%20as%205-10%20function%20evaluations%2C%20our%20sampler%0Aachieves%20state-of-the-art%20FID%20scores%20on%20pretrained%20CIFAR-10%20and%20CelebA%20models%0Aand%20can%20generate%20high%20quality%20samples%20on%20latent%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04848v4&entry.124074799=Read"},
{"title": "Vision-based Situational Graphs Exploiting Fiducial Markers for the\n  Integration of Semantic Entities", "author": "Ali Tourani and Hriday Bavle and Jose Luis Sanchez-Lopez and Deniz Isinsu Avsar and Rafael Munoz Salinas and Holger Voos", "abstract": "  Situational Graphs (S-Graphs) merge geometric models of the environment\ngenerated by Simultaneous Localization and Mapping (SLAM) approaches with 3D\nscene graphs into a multi-layered jointly optimizable factor graph. As an\nadvantage, S-Graphs not only offer a more comprehensive robotic situational\nawareness by combining geometric maps with diverse hierarchically organized\nsemantic entities and their topological relationships within one graph, but\nthey also lead to improved performance of localization and mapping on the SLAM\nlevel by exploiting semantic information. In this paper, we introduce a\nvision-based version of S-Graphs where a conventional \\ac{VSLAM} system is used\nfor low-level feature tracking and mapping. In addition, the framework exploits\nthe potential of fiducial markers (both visible as well as our recently\nintroduced transparent or fully invisible markers) to encode comprehensive\ninformation about environments and the objects within them. The markers aid in\nidentifying and mapping structural-level semantic entities, including walls and\ndoors in the environment, with reliable poses in the global reference,\nsubsequently establishing meaningful associations with higher-level entities,\nincluding corridors and rooms. However, in addition to including semantic\nentities, the semantic and geometric constraints imposed by the fiducial\nmarkers are also utilized to improve the reconstructed map's quality and reduce\nlocalization errors. Experimental results on a real-world dataset collected\nusing legged robots show that our framework excels in crafting a richer,\nmulti-layered hierarchical map and enhances robot pose accuracy at the same\ntime.\n", "link": "http://arxiv.org/abs/2309.10461v2", "date": "2024-06-03", "relevancy": 1.7506, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5923}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-based%20Situational%20Graphs%20Exploiting%20Fiducial%20Markers%20for%20the%0A%20%20Integration%20of%20Semantic%20Entities&body=Title%3A%20Vision-based%20Situational%20Graphs%20Exploiting%20Fiducial%20Markers%20for%20the%0A%20%20Integration%20of%20Semantic%20Entities%0AAuthor%3A%20Ali%20Tourani%20and%20Hriday%20Bavle%20and%20Jose%20Luis%20Sanchez-Lopez%20and%20Deniz%20Isinsu%20Avsar%20and%20Rafael%20Munoz%20Salinas%20and%20Holger%20Voos%0AAbstract%3A%20%20%20Situational%20Graphs%20%28S-Graphs%29%20merge%20geometric%20models%20of%20the%20environment%0Agenerated%20by%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20approaches%20with%203D%0Ascene%20graphs%20into%20a%20multi-layered%20jointly%20optimizable%20factor%20graph.%20As%20an%0Aadvantage%2C%20S-Graphs%20not%20only%20offer%20a%20more%20comprehensive%20robotic%20situational%0Aawareness%20by%20combining%20geometric%20maps%20with%20diverse%20hierarchically%20organized%0Asemantic%20entities%20and%20their%20topological%20relationships%20within%20one%20graph%2C%20but%0Athey%20also%20lead%20to%20improved%20performance%20of%20localization%20and%20mapping%20on%20the%20SLAM%0Alevel%20by%20exploiting%20semantic%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Avision-based%20version%20of%20S-Graphs%20where%20a%20conventional%20%5Cac%7BVSLAM%7D%20system%20is%20used%0Afor%20low-level%20feature%20tracking%20and%20mapping.%20In%20addition%2C%20the%20framework%20exploits%0Athe%20potential%20of%20fiducial%20markers%20%28both%20visible%20as%20well%20as%20our%20recently%0Aintroduced%20transparent%20or%20fully%20invisible%20markers%29%20to%20encode%20comprehensive%0Ainformation%20about%20environments%20and%20the%20objects%20within%20them.%20The%20markers%20aid%20in%0Aidentifying%20and%20mapping%20structural-level%20semantic%20entities%2C%20including%20walls%20and%0Adoors%20in%20the%20environment%2C%20with%20reliable%20poses%20in%20the%20global%20reference%2C%0Asubsequently%20establishing%20meaningful%20associations%20with%20higher-level%20entities%2C%0Aincluding%20corridors%20and%20rooms.%20However%2C%20in%20addition%20to%20including%20semantic%0Aentities%2C%20the%20semantic%20and%20geometric%20constraints%20imposed%20by%20the%20fiducial%0Amarkers%20are%20also%20utilized%20to%20improve%20the%20reconstructed%20map%27s%20quality%20and%20reduce%0Alocalization%20errors.%20Experimental%20results%20on%20a%20real-world%20dataset%20collected%0Ausing%20legged%20robots%20show%20that%20our%20framework%20excels%20in%20crafting%20a%20richer%2C%0Amulti-layered%20hierarchical%20map%20and%20enhances%20robot%20pose%20accuracy%20at%20the%20same%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-based%2520Situational%2520Graphs%2520Exploiting%2520Fiducial%2520Markers%2520for%2520the%250A%2520%2520Integration%2520of%2520Semantic%2520Entities%26entry.906535625%3DAli%2520Tourani%2520and%2520Hriday%2520Bavle%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%2520and%2520Deniz%2520Isinsu%2520Avsar%2520and%2520Rafael%2520Munoz%2520Salinas%2520and%2520Holger%2520Voos%26entry.1292438233%3D%2520%2520Situational%2520Graphs%2520%2528S-Graphs%2529%2520merge%2520geometric%2520models%2520of%2520the%2520environment%250Agenerated%2520by%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520approaches%2520with%25203D%250Ascene%2520graphs%2520into%2520a%2520multi-layered%2520jointly%2520optimizable%2520factor%2520graph.%2520As%2520an%250Aadvantage%252C%2520S-Graphs%2520not%2520only%2520offer%2520a%2520more%2520comprehensive%2520robotic%2520situational%250Aawareness%2520by%2520combining%2520geometric%2520maps%2520with%2520diverse%2520hierarchically%2520organized%250Asemantic%2520entities%2520and%2520their%2520topological%2520relationships%2520within%2520one%2520graph%252C%2520but%250Athey%2520also%2520lead%2520to%2520improved%2520performance%2520of%2520localization%2520and%2520mapping%2520on%2520the%2520SLAM%250Alevel%2520by%2520exploiting%2520semantic%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Avision-based%2520version%2520of%2520S-Graphs%2520where%2520a%2520conventional%2520%255Cac%257BVSLAM%257D%2520system%2520is%2520used%250Afor%2520low-level%2520feature%2520tracking%2520and%2520mapping.%2520In%2520addition%252C%2520the%2520framework%2520exploits%250Athe%2520potential%2520of%2520fiducial%2520markers%2520%2528both%2520visible%2520as%2520well%2520as%2520our%2520recently%250Aintroduced%2520transparent%2520or%2520fully%2520invisible%2520markers%2529%2520to%2520encode%2520comprehensive%250Ainformation%2520about%2520environments%2520and%2520the%2520objects%2520within%2520them.%2520The%2520markers%2520aid%2520in%250Aidentifying%2520and%2520mapping%2520structural-level%2520semantic%2520entities%252C%2520including%2520walls%2520and%250Adoors%2520in%2520the%2520environment%252C%2520with%2520reliable%2520poses%2520in%2520the%2520global%2520reference%252C%250Asubsequently%2520establishing%2520meaningful%2520associations%2520with%2520higher-level%2520entities%252C%250Aincluding%2520corridors%2520and%2520rooms.%2520However%252C%2520in%2520addition%2520to%2520including%2520semantic%250Aentities%252C%2520the%2520semantic%2520and%2520geometric%2520constraints%2520imposed%2520by%2520the%2520fiducial%250Amarkers%2520are%2520also%2520utilized%2520to%2520improve%2520the%2520reconstructed%2520map%2527s%2520quality%2520and%2520reduce%250Alocalization%2520errors.%2520Experimental%2520results%2520on%2520a%2520real-world%2520dataset%2520collected%250Ausing%2520legged%2520robots%2520show%2520that%2520our%2520framework%2520excels%2520in%2520crafting%2520a%2520richer%252C%250Amulti-layered%2520hierarchical%2520map%2520and%2520enhances%2520robot%2520pose%2520accuracy%2520at%2520the%2520same%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20Situational%20Graphs%20Exploiting%20Fiducial%20Markers%20for%20the%0A%20%20Integration%20of%20Semantic%20Entities&entry.906535625=Ali%20Tourani%20and%20Hriday%20Bavle%20and%20Jose%20Luis%20Sanchez-Lopez%20and%20Deniz%20Isinsu%20Avsar%20and%20Rafael%20Munoz%20Salinas%20and%20Holger%20Voos&entry.1292438233=%20%20Situational%20Graphs%20%28S-Graphs%29%20merge%20geometric%20models%20of%20the%20environment%0Agenerated%20by%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20approaches%20with%203D%0Ascene%20graphs%20into%20a%20multi-layered%20jointly%20optimizable%20factor%20graph.%20As%20an%0Aadvantage%2C%20S-Graphs%20not%20only%20offer%20a%20more%20comprehensive%20robotic%20situational%0Aawareness%20by%20combining%20geometric%20maps%20with%20diverse%20hierarchically%20organized%0Asemantic%20entities%20and%20their%20topological%20relationships%20within%20one%20graph%2C%20but%0Athey%20also%20lead%20to%20improved%20performance%20of%20localization%20and%20mapping%20on%20the%20SLAM%0Alevel%20by%20exploiting%20semantic%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Avision-based%20version%20of%20S-Graphs%20where%20a%20conventional%20%5Cac%7BVSLAM%7D%20system%20is%20used%0Afor%20low-level%20feature%20tracking%20and%20mapping.%20In%20addition%2C%20the%20framework%20exploits%0Athe%20potential%20of%20fiducial%20markers%20%28both%20visible%20as%20well%20as%20our%20recently%0Aintroduced%20transparent%20or%20fully%20invisible%20markers%29%20to%20encode%20comprehensive%0Ainformation%20about%20environments%20and%20the%20objects%20within%20them.%20The%20markers%20aid%20in%0Aidentifying%20and%20mapping%20structural-level%20semantic%20entities%2C%20including%20walls%20and%0Adoors%20in%20the%20environment%2C%20with%20reliable%20poses%20in%20the%20global%20reference%2C%0Asubsequently%20establishing%20meaningful%20associations%20with%20higher-level%20entities%2C%0Aincluding%20corridors%20and%20rooms.%20However%2C%20in%20addition%20to%20including%20semantic%0Aentities%2C%20the%20semantic%20and%20geometric%20constraints%20imposed%20by%20the%20fiducial%0Amarkers%20are%20also%20utilized%20to%20improve%20the%20reconstructed%20map%27s%20quality%20and%20reduce%0Alocalization%20errors.%20Experimental%20results%20on%20a%20real-world%20dataset%20collected%0Ausing%20legged%20robots%20show%20that%20our%20framework%20excels%20in%20crafting%20a%20richer%2C%0Amulti-layered%20hierarchical%20map%20and%20enhances%20robot%20pose%20accuracy%20at%20the%20same%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10461v2&entry.124074799=Read"},
{"title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement\n  Learning", "author": "Michael Matthews and Michael Beukman and Benjamin Ellis and Mikayel Samvelyan and Matthew Jackson and Samuel Coward and Jakob Foerster", "abstract": "  Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms. We identify that existing benchmarks\nused for research into open-ended learning fall into one of two categories.\nEither they are too slow for meaningful research to be performed without\nenormous computational resources, like Crafter, NetHack and Minecraft, or they\nare not complex enough to pose a significant challenge, like Minigrid and\nProcgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite\nof Crafter in JAX that runs up to 250x faster than the Python-native original.\nA run of PPO using 1 billion environment interactions finishes in under an hour\nusing only a single GPU and averages 90% of the optimal reward. To provide a\nmore compelling challenge we present the main Craftax benchmark, a significant\nextension of the Crafter mechanics with elements inspired from NetHack. Solving\nCraftax requires deep exploration, long term planning and memory, as well as\ncontinual adaptation to novel situations as more of the world is discovered. We\nshow that existing methods including global and episodic exploration, as well\nas unsupervised environment design fail to make material progress on the\nbenchmark. We believe that Craftax can for the first time allow researchers to\nexperiment in a complex, open-ended environment with limited computational\nresources.\n", "link": "http://arxiv.org/abs/2402.16801v2", "date": "2024-06-03", "relevancy": 1.7438, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4395}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Craftax%3A%20A%20Lightning-Fast%20Benchmark%20for%20Open-Ended%20Reinforcement%0A%20%20Learning&body=Title%3A%20Craftax%3A%20A%20Lightning-Fast%20Benchmark%20for%20Open-Ended%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Michael%20Matthews%20and%20Michael%20Beukman%20and%20Benjamin%20Ellis%20and%20Mikayel%20Samvelyan%20and%20Matthew%20Jackson%20and%20Samuel%20Coward%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Benchmarks%20play%20a%20crucial%20role%20in%20the%20development%20and%20analysis%20of%0Areinforcement%20learning%20%28RL%29%20algorithms.%20We%20identify%20that%20existing%20benchmarks%0Aused%20for%20research%20into%20open-ended%20learning%20fall%20into%20one%20of%20two%20categories.%0AEither%20they%20are%20too%20slow%20for%20meaningful%20research%20to%20be%20performed%20without%0Aenormous%20computational%20resources%2C%20like%20Crafter%2C%20NetHack%20and%20Minecraft%2C%20or%20they%0Aare%20not%20complex%20enough%20to%20pose%20a%20significant%20challenge%2C%20like%20Minigrid%20and%0AProcgen.%20To%20remedy%20this%2C%20we%20first%20present%20Craftax-Classic%3A%20a%20ground-up%20rewrite%0Aof%20Crafter%20in%20JAX%20that%20runs%20up%20to%20250x%20faster%20than%20the%20Python-native%20original.%0AA%20run%20of%20PPO%20using%201%20billion%20environment%20interactions%20finishes%20in%20under%20an%20hour%0Ausing%20only%20a%20single%20GPU%20and%20averages%2090%25%20of%20the%20optimal%20reward.%20To%20provide%20a%0Amore%20compelling%20challenge%20we%20present%20the%20main%20Craftax%20benchmark%2C%20a%20significant%0Aextension%20of%20the%20Crafter%20mechanics%20with%20elements%20inspired%20from%20NetHack.%20Solving%0ACraftax%20requires%20deep%20exploration%2C%20long%20term%20planning%20and%20memory%2C%20as%20well%20as%0Acontinual%20adaptation%20to%20novel%20situations%20as%20more%20of%20the%20world%20is%20discovered.%20We%0Ashow%20that%20existing%20methods%20including%20global%20and%20episodic%20exploration%2C%20as%20well%0Aas%20unsupervised%20environment%20design%20fail%20to%20make%20material%20progress%20on%20the%0Abenchmark.%20We%20believe%20that%20Craftax%20can%20for%20the%20first%20time%20allow%20researchers%20to%0Aexperiment%20in%20a%20complex%2C%20open-ended%20environment%20with%20limited%20computational%0Aresources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCraftax%253A%2520A%2520Lightning-Fast%2520Benchmark%2520for%2520Open-Ended%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMichael%2520Matthews%2520and%2520Michael%2520Beukman%2520and%2520Benjamin%2520Ellis%2520and%2520Mikayel%2520Samvelyan%2520and%2520Matthew%2520Jackson%2520and%2520Samuel%2520Coward%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Benchmarks%2520play%2520a%2520crucial%2520role%2520in%2520the%2520development%2520and%2520analysis%2520of%250Areinforcement%2520learning%2520%2528RL%2529%2520algorithms.%2520We%2520identify%2520that%2520existing%2520benchmarks%250Aused%2520for%2520research%2520into%2520open-ended%2520learning%2520fall%2520into%2520one%2520of%2520two%2520categories.%250AEither%2520they%2520are%2520too%2520slow%2520for%2520meaningful%2520research%2520to%2520be%2520performed%2520without%250Aenormous%2520computational%2520resources%252C%2520like%2520Crafter%252C%2520NetHack%2520and%2520Minecraft%252C%2520or%2520they%250Aare%2520not%2520complex%2520enough%2520to%2520pose%2520a%2520significant%2520challenge%252C%2520like%2520Minigrid%2520and%250AProcgen.%2520To%2520remedy%2520this%252C%2520we%2520first%2520present%2520Craftax-Classic%253A%2520a%2520ground-up%2520rewrite%250Aof%2520Crafter%2520in%2520JAX%2520that%2520runs%2520up%2520to%2520250x%2520faster%2520than%2520the%2520Python-native%2520original.%250AA%2520run%2520of%2520PPO%2520using%25201%2520billion%2520environment%2520interactions%2520finishes%2520in%2520under%2520an%2520hour%250Ausing%2520only%2520a%2520single%2520GPU%2520and%2520averages%252090%2525%2520of%2520the%2520optimal%2520reward.%2520To%2520provide%2520a%250Amore%2520compelling%2520challenge%2520we%2520present%2520the%2520main%2520Craftax%2520benchmark%252C%2520a%2520significant%250Aextension%2520of%2520the%2520Crafter%2520mechanics%2520with%2520elements%2520inspired%2520from%2520NetHack.%2520Solving%250ACraftax%2520requires%2520deep%2520exploration%252C%2520long%2520term%2520planning%2520and%2520memory%252C%2520as%2520well%2520as%250Acontinual%2520adaptation%2520to%2520novel%2520situations%2520as%2520more%2520of%2520the%2520world%2520is%2520discovered.%2520We%250Ashow%2520that%2520existing%2520methods%2520including%2520global%2520and%2520episodic%2520exploration%252C%2520as%2520well%250Aas%2520unsupervised%2520environment%2520design%2520fail%2520to%2520make%2520material%2520progress%2520on%2520the%250Abenchmark.%2520We%2520believe%2520that%2520Craftax%2520can%2520for%2520the%2520first%2520time%2520allow%2520researchers%2520to%250Aexperiment%2520in%2520a%2520complex%252C%2520open-ended%2520environment%2520with%2520limited%2520computational%250Aresources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Craftax%3A%20A%20Lightning-Fast%20Benchmark%20for%20Open-Ended%20Reinforcement%0A%20%20Learning&entry.906535625=Michael%20Matthews%20and%20Michael%20Beukman%20and%20Benjamin%20Ellis%20and%20Mikayel%20Samvelyan%20and%20Matthew%20Jackson%20and%20Samuel%20Coward%20and%20Jakob%20Foerster&entry.1292438233=%20%20Benchmarks%20play%20a%20crucial%20role%20in%20the%20development%20and%20analysis%20of%0Areinforcement%20learning%20%28RL%29%20algorithms.%20We%20identify%20that%20existing%20benchmarks%0Aused%20for%20research%20into%20open-ended%20learning%20fall%20into%20one%20of%20two%20categories.%0AEither%20they%20are%20too%20slow%20for%20meaningful%20research%20to%20be%20performed%20without%0Aenormous%20computational%20resources%2C%20like%20Crafter%2C%20NetHack%20and%20Minecraft%2C%20or%20they%0Aare%20not%20complex%20enough%20to%20pose%20a%20significant%20challenge%2C%20like%20Minigrid%20and%0AProcgen.%20To%20remedy%20this%2C%20we%20first%20present%20Craftax-Classic%3A%20a%20ground-up%20rewrite%0Aof%20Crafter%20in%20JAX%20that%20runs%20up%20to%20250x%20faster%20than%20the%20Python-native%20original.%0AA%20run%20of%20PPO%20using%201%20billion%20environment%20interactions%20finishes%20in%20under%20an%20hour%0Ausing%20only%20a%20single%20GPU%20and%20averages%2090%25%20of%20the%20optimal%20reward.%20To%20provide%20a%0Amore%20compelling%20challenge%20we%20present%20the%20main%20Craftax%20benchmark%2C%20a%20significant%0Aextension%20of%20the%20Crafter%20mechanics%20with%20elements%20inspired%20from%20NetHack.%20Solving%0ACraftax%20requires%20deep%20exploration%2C%20long%20term%20planning%20and%20memory%2C%20as%20well%20as%0Acontinual%20adaptation%20to%20novel%20situations%20as%20more%20of%20the%20world%20is%20discovered.%20We%0Ashow%20that%20existing%20methods%20including%20global%20and%20episodic%20exploration%2C%20as%20well%0Aas%20unsupervised%20environment%20design%20fail%20to%20make%20material%20progress%20on%20the%0Abenchmark.%20We%20believe%20that%20Craftax%20can%20for%20the%20first%20time%20allow%20researchers%20to%0Aexperiment%20in%20a%20complex%2C%20open-ended%20environment%20with%20limited%20computational%0Aresources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16801v2&entry.124074799=Read"},
{"title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation", "author": "Zachary Novack and Julian McAuley and Taylor Berg-Kirkpatrick and Nicholas J. Bryan", "abstract": "  We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose\nframe-work for controlling pre-trained text-to-music diffusion models at\ninference-time via optimizing initial noise latents. Our method can be used to\noptimize through any differentiable feature matching loss to achieve a target\n(stylized) output and leverages gradient checkpointing for memory efficiency.\nWe demonstrate a surprisingly wide-range of applications for music generation\nincluding inpainting, outpainting, and looping as well as intensity, melody,\nand musical structure control - all without ever fine-tuning the underlying\nmodel. When we compare our approach against related training, guidance, and\noptimization-based methods, we find DITTO achieves state-of-the-art performance\non nearly all tasks, including outperforming comparable approaches on\ncontrollability, audio quality, and computational efficiency, thus opening the\ndoor for high-quality, flexible, training-free control of diffusion models.\nSound examples can be found at https://DITTO-Music.github.io/web/.\n", "link": "http://arxiv.org/abs/2401.12179v2", "date": "2024-06-03", "relevancy": 1.7346, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5656}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DITTO%3A%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%20Generation&body=Title%3A%20DITTO%3A%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%20Generation%0AAuthor%3A%20Zachary%20Novack%20and%20Julian%20McAuley%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nicholas%20J.%20Bryan%0AAbstract%3A%20%20%20We%20propose%20Diffusion%20Inference-Time%20T-Optimization%20%28DITTO%29%2C%20a%20general-purpose%0Aframe-work%20for%20controlling%20pre-trained%20text-to-music%20diffusion%20models%20at%0Ainference-time%20via%20optimizing%20initial%20noise%20latents.%20Our%20method%20can%20be%20used%20to%0Aoptimize%20through%20any%20differentiable%20feature%20matching%20loss%20to%20achieve%20a%20target%0A%28stylized%29%20output%20and%20leverages%20gradient%20checkpointing%20for%20memory%20efficiency.%0AWe%20demonstrate%20a%20surprisingly%20wide-range%20of%20applications%20for%20music%20generation%0Aincluding%20inpainting%2C%20outpainting%2C%20and%20looping%20as%20well%20as%20intensity%2C%20melody%2C%0Aand%20musical%20structure%20control%20-%20all%20without%20ever%20fine-tuning%20the%20underlying%0Amodel.%20When%20we%20compare%20our%20approach%20against%20related%20training%2C%20guidance%2C%20and%0Aoptimization-based%20methods%2C%20we%20find%20DITTO%20achieves%20state-of-the-art%20performance%0Aon%20nearly%20all%20tasks%2C%20including%20outperforming%20comparable%20approaches%20on%0Acontrollability%2C%20audio%20quality%2C%20and%20computational%20efficiency%2C%20thus%20opening%20the%0Adoor%20for%20high-quality%2C%20flexible%2C%20training-free%20control%20of%20diffusion%20models.%0ASound%20examples%20can%20be%20found%20at%20https%3A//DITTO-Music.github.io/web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDITTO%253A%2520Diffusion%2520Inference-Time%2520T-Optimization%2520for%2520Music%2520Generation%26entry.906535625%3DZachary%2520Novack%2520and%2520Julian%2520McAuley%2520and%2520Taylor%2520Berg-Kirkpatrick%2520and%2520Nicholas%2520J.%2520Bryan%26entry.1292438233%3D%2520%2520We%2520propose%2520Diffusion%2520Inference-Time%2520T-Optimization%2520%2528DITTO%2529%252C%2520a%2520general-purpose%250Aframe-work%2520for%2520controlling%2520pre-trained%2520text-to-music%2520diffusion%2520models%2520at%250Ainference-time%2520via%2520optimizing%2520initial%2520noise%2520latents.%2520Our%2520method%2520can%2520be%2520used%2520to%250Aoptimize%2520through%2520any%2520differentiable%2520feature%2520matching%2520loss%2520to%2520achieve%2520a%2520target%250A%2528stylized%2529%2520output%2520and%2520leverages%2520gradient%2520checkpointing%2520for%2520memory%2520efficiency.%250AWe%2520demonstrate%2520a%2520surprisingly%2520wide-range%2520of%2520applications%2520for%2520music%2520generation%250Aincluding%2520inpainting%252C%2520outpainting%252C%2520and%2520looping%2520as%2520well%2520as%2520intensity%252C%2520melody%252C%250Aand%2520musical%2520structure%2520control%2520-%2520all%2520without%2520ever%2520fine-tuning%2520the%2520underlying%250Amodel.%2520When%2520we%2520compare%2520our%2520approach%2520against%2520related%2520training%252C%2520guidance%252C%2520and%250Aoptimization-based%2520methods%252C%2520we%2520find%2520DITTO%2520achieves%2520state-of-the-art%2520performance%250Aon%2520nearly%2520all%2520tasks%252C%2520including%2520outperforming%2520comparable%2520approaches%2520on%250Acontrollability%252C%2520audio%2520quality%252C%2520and%2520computational%2520efficiency%252C%2520thus%2520opening%2520the%250Adoor%2520for%2520high-quality%252C%2520flexible%252C%2520training-free%2520control%2520of%2520diffusion%2520models.%250ASound%2520examples%2520can%2520be%2520found%2520at%2520https%253A//DITTO-Music.github.io/web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DITTO%3A%20Diffusion%20Inference-Time%20T-Optimization%20for%20Music%20Generation&entry.906535625=Zachary%20Novack%20and%20Julian%20McAuley%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nicholas%20J.%20Bryan&entry.1292438233=%20%20We%20propose%20Diffusion%20Inference-Time%20T-Optimization%20%28DITTO%29%2C%20a%20general-purpose%0Aframe-work%20for%20controlling%20pre-trained%20text-to-music%20diffusion%20models%20at%0Ainference-time%20via%20optimizing%20initial%20noise%20latents.%20Our%20method%20can%20be%20used%20to%0Aoptimize%20through%20any%20differentiable%20feature%20matching%20loss%20to%20achieve%20a%20target%0A%28stylized%29%20output%20and%20leverages%20gradient%20checkpointing%20for%20memory%20efficiency.%0AWe%20demonstrate%20a%20surprisingly%20wide-range%20of%20applications%20for%20music%20generation%0Aincluding%20inpainting%2C%20outpainting%2C%20and%20looping%20as%20well%20as%20intensity%2C%20melody%2C%0Aand%20musical%20structure%20control%20-%20all%20without%20ever%20fine-tuning%20the%20underlying%0Amodel.%20When%20we%20compare%20our%20approach%20against%20related%20training%2C%20guidance%2C%20and%0Aoptimization-based%20methods%2C%20we%20find%20DITTO%20achieves%20state-of-the-art%20performance%0Aon%20nearly%20all%20tasks%2C%20including%20outperforming%20comparable%20approaches%20on%0Acontrollability%2C%20audio%20quality%2C%20and%20computational%20efficiency%2C%20thus%20opening%20the%0Adoor%20for%20high-quality%2C%20flexible%2C%20training-free%20control%20of%20diffusion%20models.%0ASound%20examples%20can%20be%20found%20at%20https%3A//DITTO-Music.github.io/web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12179v2&entry.124074799=Read"},
{"title": "Feature Importance Disparities for Data Bias Investigations", "author": "Peter W. Chang and Leor Fishman and Seth Neel", "abstract": "  It is widely held that one cause of downstream bias in classifiers is bias\npresent in the training data. Rectifying such biases may involve\ncontext-dependent interventions such as training separate models on subgroups,\nremoving features with bias in the collection process, or even conducting\nreal-world experiments to ascertain sources of bias. Despite the need for such\ndata bias investigations, few automated methods exist to assist practitioners\nin these efforts. In this paper, we present one such method that given a\ndataset $X$ consisting of protected and unprotected features, outcomes $y$, and\na regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with\nthe following property: $g$ corresponds to a subset of the training dataset\n$(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller)\ninfluence in the subgroup $g$, than on the dataset overall, which we call\nfeature importance disparity (FID). We show across $4$ datasets and $4$ common\nfeature importance methods of broad interest to the machine learning community\nthat we can efficiently find subgroups with large FID values even over\nexponentially large subgroup classes and in practice these groups correspond to\nsubgroups with potentially serious bias issues as measured by standard fairness\nmetrics.\n", "link": "http://arxiv.org/abs/2303.01704v4", "date": "2024-06-03", "relevancy": 1.729, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4512}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4202}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Importance%20Disparities%20for%20Data%20Bias%20Investigations&body=Title%3A%20Feature%20Importance%20Disparities%20for%20Data%20Bias%20Investigations%0AAuthor%3A%20Peter%20W.%20Chang%20and%20Leor%20Fishman%20and%20Seth%20Neel%0AAbstract%3A%20%20%20It%20is%20widely%20held%20that%20one%20cause%20of%20downstream%20bias%20in%20classifiers%20is%20bias%0Apresent%20in%20the%20training%20data.%20Rectifying%20such%20biases%20may%20involve%0Acontext-dependent%20interventions%20such%20as%20training%20separate%20models%20on%20subgroups%2C%0Aremoving%20features%20with%20bias%20in%20the%20collection%20process%2C%20or%20even%20conducting%0Areal-world%20experiments%20to%20ascertain%20sources%20of%20bias.%20Despite%20the%20need%20for%20such%0Adata%20bias%20investigations%2C%20few%20automated%20methods%20exist%20to%20assist%20practitioners%0Ain%20these%20efforts.%20In%20this%20paper%2C%20we%20present%20one%20such%20method%20that%20given%20a%0Adataset%20%24X%24%20consisting%20of%20protected%20and%20unprotected%20features%2C%20outcomes%20%24y%24%2C%20and%0Aa%20regressor%20%24h%24%20that%20predicts%20%24y%24%20given%20%24X%24%2C%20outputs%20a%20tuple%20%24%28f_j%2C%20g%29%24%2C%20with%0Athe%20following%20property%3A%20%24g%24%20corresponds%20to%20a%20subset%20of%20the%20training%20dataset%0A%24%28X%2C%20y%29%24%2C%20such%20that%20the%20%24j%5E%7Bth%7D%24%20feature%20%24f_j%24%20has%20much%20larger%20%28or%20smaller%29%0Ainfluence%20in%20the%20subgroup%20%24g%24%2C%20than%20on%20the%20dataset%20overall%2C%20which%20we%20call%0Afeature%20importance%20disparity%20%28FID%29.%20We%20show%20across%20%244%24%20datasets%20and%20%244%24%20common%0Afeature%20importance%20methods%20of%20broad%20interest%20to%20the%20machine%20learning%20community%0Athat%20we%20can%20efficiently%20find%20subgroups%20with%20large%20FID%20values%20even%20over%0Aexponentially%20large%20subgroup%20classes%20and%20in%20practice%20these%20groups%20correspond%20to%0Asubgroups%20with%20potentially%20serious%20bias%20issues%20as%20measured%20by%20standard%20fairness%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01704v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Importance%2520Disparities%2520for%2520Data%2520Bias%2520Investigations%26entry.906535625%3DPeter%2520W.%2520Chang%2520and%2520Leor%2520Fishman%2520and%2520Seth%2520Neel%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520held%2520that%2520one%2520cause%2520of%2520downstream%2520bias%2520in%2520classifiers%2520is%2520bias%250Apresent%2520in%2520the%2520training%2520data.%2520Rectifying%2520such%2520biases%2520may%2520involve%250Acontext-dependent%2520interventions%2520such%2520as%2520training%2520separate%2520models%2520on%2520subgroups%252C%250Aremoving%2520features%2520with%2520bias%2520in%2520the%2520collection%2520process%252C%2520or%2520even%2520conducting%250Areal-world%2520experiments%2520to%2520ascertain%2520sources%2520of%2520bias.%2520Despite%2520the%2520need%2520for%2520such%250Adata%2520bias%2520investigations%252C%2520few%2520automated%2520methods%2520exist%2520to%2520assist%2520practitioners%250Ain%2520these%2520efforts.%2520In%2520this%2520paper%252C%2520we%2520present%2520one%2520such%2520method%2520that%2520given%2520a%250Adataset%2520%2524X%2524%2520consisting%2520of%2520protected%2520and%2520unprotected%2520features%252C%2520outcomes%2520%2524y%2524%252C%2520and%250Aa%2520regressor%2520%2524h%2524%2520that%2520predicts%2520%2524y%2524%2520given%2520%2524X%2524%252C%2520outputs%2520a%2520tuple%2520%2524%2528f_j%252C%2520g%2529%2524%252C%2520with%250Athe%2520following%2520property%253A%2520%2524g%2524%2520corresponds%2520to%2520a%2520subset%2520of%2520the%2520training%2520dataset%250A%2524%2528X%252C%2520y%2529%2524%252C%2520such%2520that%2520the%2520%2524j%255E%257Bth%257D%2524%2520feature%2520%2524f_j%2524%2520has%2520much%2520larger%2520%2528or%2520smaller%2529%250Ainfluence%2520in%2520the%2520subgroup%2520%2524g%2524%252C%2520than%2520on%2520the%2520dataset%2520overall%252C%2520which%2520we%2520call%250Afeature%2520importance%2520disparity%2520%2528FID%2529.%2520We%2520show%2520across%2520%25244%2524%2520datasets%2520and%2520%25244%2524%2520common%250Afeature%2520importance%2520methods%2520of%2520broad%2520interest%2520to%2520the%2520machine%2520learning%2520community%250Athat%2520we%2520can%2520efficiently%2520find%2520subgroups%2520with%2520large%2520FID%2520values%2520even%2520over%250Aexponentially%2520large%2520subgroup%2520classes%2520and%2520in%2520practice%2520these%2520groups%2520correspond%2520to%250Asubgroups%2520with%2520potentially%2520serious%2520bias%2520issues%2520as%2520measured%2520by%2520standard%2520fairness%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.01704v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Importance%20Disparities%20for%20Data%20Bias%20Investigations&entry.906535625=Peter%20W.%20Chang%20and%20Leor%20Fishman%20and%20Seth%20Neel&entry.1292438233=%20%20It%20is%20widely%20held%20that%20one%20cause%20of%20downstream%20bias%20in%20classifiers%20is%20bias%0Apresent%20in%20the%20training%20data.%20Rectifying%20such%20biases%20may%20involve%0Acontext-dependent%20interventions%20such%20as%20training%20separate%20models%20on%20subgroups%2C%0Aremoving%20features%20with%20bias%20in%20the%20collection%20process%2C%20or%20even%20conducting%0Areal-world%20experiments%20to%20ascertain%20sources%20of%20bias.%20Despite%20the%20need%20for%20such%0Adata%20bias%20investigations%2C%20few%20automated%20methods%20exist%20to%20assist%20practitioners%0Ain%20these%20efforts.%20In%20this%20paper%2C%20we%20present%20one%20such%20method%20that%20given%20a%0Adataset%20%24X%24%20consisting%20of%20protected%20and%20unprotected%20features%2C%20outcomes%20%24y%24%2C%20and%0Aa%20regressor%20%24h%24%20that%20predicts%20%24y%24%20given%20%24X%24%2C%20outputs%20a%20tuple%20%24%28f_j%2C%20g%29%24%2C%20with%0Athe%20following%20property%3A%20%24g%24%20corresponds%20to%20a%20subset%20of%20the%20training%20dataset%0A%24%28X%2C%20y%29%24%2C%20such%20that%20the%20%24j%5E%7Bth%7D%24%20feature%20%24f_j%24%20has%20much%20larger%20%28or%20smaller%29%0Ainfluence%20in%20the%20subgroup%20%24g%24%2C%20than%20on%20the%20dataset%20overall%2C%20which%20we%20call%0Afeature%20importance%20disparity%20%28FID%29.%20We%20show%20across%20%244%24%20datasets%20and%20%244%24%20common%0Afeature%20importance%20methods%20of%20broad%20interest%20to%20the%20machine%20learning%20community%0Athat%20we%20can%20efficiently%20find%20subgroups%20with%20large%20FID%20values%20even%20over%0Aexponentially%20large%20subgroup%20classes%20and%20in%20practice%20these%20groups%20correspond%20to%0Asubgroups%20with%20potentially%20serious%20bias%20issues%20as%20measured%20by%20standard%20fairness%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01704v4&entry.124074799=Read"},
{"title": "Accelerating Graph Neural Networks via Edge Pruning for Power Allocation\n  in Wireless Networks", "author": "Lili Chen and Jingge Zhu and Jamie Evans", "abstract": "  Graph Neural Networks (GNNs) have recently emerged as a promising approach to\ntackling power allocation problems in wireless networks. Since unpaired\ntransmitters and receivers are often spatially distant, the distance-based\nthreshold is proposed to reduce the computation time by excluding or including\nthe channel state information in GNNs. In this paper, we are the first to\nintroduce a neighbour-based threshold approach to GNNs to reduce the time\ncomplexity. Furthermore, we conduct a comprehensive analysis of both\ndistance-based and neighbour-based thresholds and provide recommendations for\nselecting the appropriate value in different communication channel scenarios.\nWe design the corresponding neighbour-based Graph Neural Networks (N-GNN) with\nthe aim of allocating transmit powers to maximise the network throughput. Our\nresults show that our proposed N-GNN offer significant advantages in terms of\nreducing time complexity while preserving strong performance and generalisation\ncapacity. Besides, we show that by choosing a suitable threshold, the time\ncomplexity is reduced from O(|V|^2) to O(|V|), where |V| is the total number of\ntransceiver pairs.\n", "link": "http://arxiv.org/abs/2305.12639v2", "date": "2024-06-03", "relevancy": 1.7254, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4524}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4256}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Graph%20Neural%20Networks%20via%20Edge%20Pruning%20for%20Power%20Allocation%0A%20%20in%20Wireless%20Networks&body=Title%3A%20Accelerating%20Graph%20Neural%20Networks%20via%20Edge%20Pruning%20for%20Power%20Allocation%0A%20%20in%20Wireless%20Networks%0AAuthor%3A%20Lili%20Chen%20and%20Jingge%20Zhu%20and%20Jamie%20Evans%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20emerged%20as%20a%20promising%20approach%20to%0Atackling%20power%20allocation%20problems%20in%20wireless%20networks.%20Since%20unpaired%0Atransmitters%20and%20receivers%20are%20often%20spatially%20distant%2C%20the%20distance-based%0Athreshold%20is%20proposed%20to%20reduce%20the%20computation%20time%20by%20excluding%20or%20including%0Athe%20channel%20state%20information%20in%20GNNs.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%0Aintroduce%20a%20neighbour-based%20threshold%20approach%20to%20GNNs%20to%20reduce%20the%20time%0Acomplexity.%20Furthermore%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20both%0Adistance-based%20and%20neighbour-based%20thresholds%20and%20provide%20recommendations%20for%0Aselecting%20the%20appropriate%20value%20in%20different%20communication%20channel%20scenarios.%0AWe%20design%20the%20corresponding%20neighbour-based%20Graph%20Neural%20Networks%20%28N-GNN%29%20with%0Athe%20aim%20of%20allocating%20transmit%20powers%20to%20maximise%20the%20network%20throughput.%20Our%0Aresults%20show%20that%20our%20proposed%20N-GNN%20offer%20significant%20advantages%20in%20terms%20of%0Areducing%20time%20complexity%20while%20preserving%20strong%20performance%20and%20generalisation%0Acapacity.%20Besides%2C%20we%20show%20that%20by%20choosing%20a%20suitable%20threshold%2C%20the%20time%0Acomplexity%20is%20reduced%20from%20O%28%7CV%7C%5E2%29%20to%20O%28%7CV%7C%29%2C%20where%20%7CV%7C%20is%20the%20total%20number%20of%0Atransceiver%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Graph%2520Neural%2520Networks%2520via%2520Edge%2520Pruning%2520for%2520Power%2520Allocation%250A%2520%2520in%2520Wireless%2520Networks%26entry.906535625%3DLili%2520Chen%2520and%2520Jingge%2520Zhu%2520and%2520Jamie%2520Evans%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520approach%2520to%250Atackling%2520power%2520allocation%2520problems%2520in%2520wireless%2520networks.%2520Since%2520unpaired%250Atransmitters%2520and%2520receivers%2520are%2520often%2520spatially%2520distant%252C%2520the%2520distance-based%250Athreshold%2520is%2520proposed%2520to%2520reduce%2520the%2520computation%2520time%2520by%2520excluding%2520or%2520including%250Athe%2520channel%2520state%2520information%2520in%2520GNNs.%2520In%2520this%2520paper%252C%2520we%2520are%2520the%2520first%2520to%250Aintroduce%2520a%2520neighbour-based%2520threshold%2520approach%2520to%2520GNNs%2520to%2520reduce%2520the%2520time%250Acomplexity.%2520Furthermore%252C%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520both%250Adistance-based%2520and%2520neighbour-based%2520thresholds%2520and%2520provide%2520recommendations%2520for%250Aselecting%2520the%2520appropriate%2520value%2520in%2520different%2520communication%2520channel%2520scenarios.%250AWe%2520design%2520the%2520corresponding%2520neighbour-based%2520Graph%2520Neural%2520Networks%2520%2528N-GNN%2529%2520with%250Athe%2520aim%2520of%2520allocating%2520transmit%2520powers%2520to%2520maximise%2520the%2520network%2520throughput.%2520Our%250Aresults%2520show%2520that%2520our%2520proposed%2520N-GNN%2520offer%2520significant%2520advantages%2520in%2520terms%2520of%250Areducing%2520time%2520complexity%2520while%2520preserving%2520strong%2520performance%2520and%2520generalisation%250Acapacity.%2520Besides%252C%2520we%2520show%2520that%2520by%2520choosing%2520a%2520suitable%2520threshold%252C%2520the%2520time%250Acomplexity%2520is%2520reduced%2520from%2520O%2528%257CV%257C%255E2%2529%2520to%2520O%2528%257CV%257C%2529%252C%2520where%2520%257CV%257C%2520is%2520the%2520total%2520number%2520of%250Atransceiver%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Graph%20Neural%20Networks%20via%20Edge%20Pruning%20for%20Power%20Allocation%0A%20%20in%20Wireless%20Networks&entry.906535625=Lili%20Chen%20and%20Jingge%20Zhu%20and%20Jamie%20Evans&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20emerged%20as%20a%20promising%20approach%20to%0Atackling%20power%20allocation%20problems%20in%20wireless%20networks.%20Since%20unpaired%0Atransmitters%20and%20receivers%20are%20often%20spatially%20distant%2C%20the%20distance-based%0Athreshold%20is%20proposed%20to%20reduce%20the%20computation%20time%20by%20excluding%20or%20including%0Athe%20channel%20state%20information%20in%20GNNs.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%0Aintroduce%20a%20neighbour-based%20threshold%20approach%20to%20GNNs%20to%20reduce%20the%20time%0Acomplexity.%20Furthermore%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20both%0Adistance-based%20and%20neighbour-based%20thresholds%20and%20provide%20recommendations%20for%0Aselecting%20the%20appropriate%20value%20in%20different%20communication%20channel%20scenarios.%0AWe%20design%20the%20corresponding%20neighbour-based%20Graph%20Neural%20Networks%20%28N-GNN%29%20with%0Athe%20aim%20of%20allocating%20transmit%20powers%20to%20maximise%20the%20network%20throughput.%20Our%0Aresults%20show%20that%20our%20proposed%20N-GNN%20offer%20significant%20advantages%20in%20terms%20of%0Areducing%20time%20complexity%20while%20preserving%20strong%20performance%20and%20generalisation%0Acapacity.%20Besides%2C%20we%20show%20that%20by%20choosing%20a%20suitable%20threshold%2C%20the%20time%0Acomplexity%20is%20reduced%20from%20O%28%7CV%7C%5E2%29%20to%20O%28%7CV%7C%29%2C%20where%20%7CV%7C%20is%20the%20total%20number%20of%0Atransceiver%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12639v2&entry.124074799=Read"},
{"title": "Embedding Privacy in Computational Social Science and Artificial\n  Intelligence Research", "author": "Keenan Jones and Fatima Zahrah and Jason R. C. Nurse", "abstract": "  Privacy is a human right. It ensures that individuals are free to engage in\ndiscussions, participate in groups, and form relationships online or offline\nwithout fear of their data being inappropriately harvested, analyzed, or\notherwise used to harm them. Preserving privacy has emerged as a critical\nfactor in research, particularly in the computational social science (CSS),\nartificial intelligence (AI) and data science domains, given their reliance on\nindividuals' data for novel insights. The increasing use of advanced\ncomputational models stands to exacerbate privacy concerns because, if\ninappropriately used, they can quickly infringe privacy rights and lead to\nadverse effects for individuals -- especially vulnerable groups -- and society.\nWe have already witnessed a host of privacy issues emerge with the advent of\nlarge language models (LLMs), such as ChatGPT, which further demonstrate the\nimportance of embedding privacy from the start. This article contributes to the\nfield by discussing the role of privacy and the issues that researchers working\nin CSS, AI, data science and related domains are likely to face. It then\npresents several key considerations for researchers to ensure participant\nprivacy is best preserved in their research design, data collection and use,\nanalysis, and dissemination of research results.\n", "link": "http://arxiv.org/abs/2404.11515v2", "date": "2024-06-03", "relevancy": 1.7181, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.439}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4285}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research&body=Title%3A%20Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research%0AAuthor%3A%20Keenan%20Jones%20and%20Fatima%20Zahrah%20and%20Jason%20R.%20C.%20Nurse%0AAbstract%3A%20%20%20Privacy%20is%20a%20human%20right.%20It%20ensures%20that%20individuals%20are%20free%20to%20engage%20in%0Adiscussions%2C%20participate%20in%20groups%2C%20and%20form%20relationships%20online%20or%20offline%0Awithout%20fear%20of%20their%20data%20being%20inappropriately%20harvested%2C%20analyzed%2C%20or%0Aotherwise%20used%20to%20harm%20them.%20Preserving%20privacy%20has%20emerged%20as%20a%20critical%0Afactor%20in%20research%2C%20particularly%20in%20the%20computational%20social%20science%20%28CSS%29%2C%0Aartificial%20intelligence%20%28AI%29%20and%20data%20science%20domains%2C%20given%20their%20reliance%20on%0Aindividuals%27%20data%20for%20novel%20insights.%20The%20increasing%20use%20of%20advanced%0Acomputational%20models%20stands%20to%20exacerbate%20privacy%20concerns%20because%2C%20if%0Ainappropriately%20used%2C%20they%20can%20quickly%20infringe%20privacy%20rights%20and%20lead%20to%0Aadverse%20effects%20for%20individuals%20--%20especially%20vulnerable%20groups%20--%20and%20society.%0AWe%20have%20already%20witnessed%20a%20host%20of%20privacy%20issues%20emerge%20with%20the%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20which%20further%20demonstrate%20the%0Aimportance%20of%20embedding%20privacy%20from%20the%20start.%20This%20article%20contributes%20to%20the%0Afield%20by%20discussing%20the%20role%20of%20privacy%20and%20the%20issues%20that%20researchers%20working%0Ain%20CSS%2C%20AI%2C%20data%20science%20and%20related%20domains%20are%20likely%20to%20face.%20It%20then%0Apresents%20several%20key%20considerations%20for%20researchers%20to%20ensure%20participant%0Aprivacy%20is%20best%20preserved%20in%20their%20research%20design%2C%20data%20collection%20and%20use%2C%0Aanalysis%2C%20and%20dissemination%20of%20research%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Privacy%2520in%2520Computational%2520Social%2520Science%2520and%2520Artificial%250A%2520%2520Intelligence%2520Research%26entry.906535625%3DKeenan%2520Jones%2520and%2520Fatima%2520Zahrah%2520and%2520Jason%2520R.%2520C.%2520Nurse%26entry.1292438233%3D%2520%2520Privacy%2520is%2520a%2520human%2520right.%2520It%2520ensures%2520that%2520individuals%2520are%2520free%2520to%2520engage%2520in%250Adiscussions%252C%2520participate%2520in%2520groups%252C%2520and%2520form%2520relationships%2520online%2520or%2520offline%250Awithout%2520fear%2520of%2520their%2520data%2520being%2520inappropriately%2520harvested%252C%2520analyzed%252C%2520or%250Aotherwise%2520used%2520to%2520harm%2520them.%2520Preserving%2520privacy%2520has%2520emerged%2520as%2520a%2520critical%250Afactor%2520in%2520research%252C%2520particularly%2520in%2520the%2520computational%2520social%2520science%2520%2528CSS%2529%252C%250Aartificial%2520intelligence%2520%2528AI%2529%2520and%2520data%2520science%2520domains%252C%2520given%2520their%2520reliance%2520on%250Aindividuals%2527%2520data%2520for%2520novel%2520insights.%2520The%2520increasing%2520use%2520of%2520advanced%250Acomputational%2520models%2520stands%2520to%2520exacerbate%2520privacy%2520concerns%2520because%252C%2520if%250Ainappropriately%2520used%252C%2520they%2520can%2520quickly%2520infringe%2520privacy%2520rights%2520and%2520lead%2520to%250Aadverse%2520effects%2520for%2520individuals%2520--%2520especially%2520vulnerable%2520groups%2520--%2520and%2520society.%250AWe%2520have%2520already%2520witnessed%2520a%2520host%2520of%2520privacy%2520issues%2520emerge%2520with%2520the%2520advent%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%2520ChatGPT%252C%2520which%2520further%2520demonstrate%2520the%250Aimportance%2520of%2520embedding%2520privacy%2520from%2520the%2520start.%2520This%2520article%2520contributes%2520to%2520the%250Afield%2520by%2520discussing%2520the%2520role%2520of%2520privacy%2520and%2520the%2520issues%2520that%2520researchers%2520working%250Ain%2520CSS%252C%2520AI%252C%2520data%2520science%2520and%2520related%2520domains%2520are%2520likely%2520to%2520face.%2520It%2520then%250Apresents%2520several%2520key%2520considerations%2520for%2520researchers%2520to%2520ensure%2520participant%250Aprivacy%2520is%2520best%2520preserved%2520in%2520their%2520research%2520design%252C%2520data%2520collection%2520and%2520use%252C%250Aanalysis%252C%2520and%2520dissemination%2520of%2520research%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research&entry.906535625=Keenan%20Jones%20and%20Fatima%20Zahrah%20and%20Jason%20R.%20C.%20Nurse&entry.1292438233=%20%20Privacy%20is%20a%20human%20right.%20It%20ensures%20that%20individuals%20are%20free%20to%20engage%20in%0Adiscussions%2C%20participate%20in%20groups%2C%20and%20form%20relationships%20online%20or%20offline%0Awithout%20fear%20of%20their%20data%20being%20inappropriately%20harvested%2C%20analyzed%2C%20or%0Aotherwise%20used%20to%20harm%20them.%20Preserving%20privacy%20has%20emerged%20as%20a%20critical%0Afactor%20in%20research%2C%20particularly%20in%20the%20computational%20social%20science%20%28CSS%29%2C%0Aartificial%20intelligence%20%28AI%29%20and%20data%20science%20domains%2C%20given%20their%20reliance%20on%0Aindividuals%27%20data%20for%20novel%20insights.%20The%20increasing%20use%20of%20advanced%0Acomputational%20models%20stands%20to%20exacerbate%20privacy%20concerns%20because%2C%20if%0Ainappropriately%20used%2C%20they%20can%20quickly%20infringe%20privacy%20rights%20and%20lead%20to%0Aadverse%20effects%20for%20individuals%20--%20especially%20vulnerable%20groups%20--%20and%20society.%0AWe%20have%20already%20witnessed%20a%20host%20of%20privacy%20issues%20emerge%20with%20the%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20which%20further%20demonstrate%20the%0Aimportance%20of%20embedding%20privacy%20from%20the%20start.%20This%20article%20contributes%20to%20the%0Afield%20by%20discussing%20the%20role%20of%20privacy%20and%20the%20issues%20that%20researchers%20working%0Ain%20CSS%2C%20AI%2C%20data%20science%20and%20related%20domains%20are%20likely%20to%20face.%20It%20then%0Apresents%20several%20key%20considerations%20for%20researchers%20to%20ensure%20participant%0Aprivacy%20is%20best%20preserved%20in%20their%20research%20design%2C%20data%20collection%20and%20use%2C%0Aanalysis%2C%20and%20dissemination%20of%20research%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11515v2&entry.124074799=Read"},
{"title": "Cheap Talking Algorithms", "author": "Daniele Condorelli and Massimiliano Furlan", "abstract": "  We simulate behaviour of two independent reinforcement learning algorithms\nplaying the Crawford and Sobel (1982) game of strategic information\ntransmission. We adopt memoryless algorithms to capture learning in a static\ngame where a large population interacts anonymously. We show that sender and\nreceiver converge to Nash equilibrium play. The level of informativeness of the\nsender's cheap talk decreases as the bias increases and, at intermediate level\nof the bias, it matches the level predicted by the Pareto optimal equilibrium\nor by the second best one. Conclusions are robust to alternative specifications\nof the learning hyperparameters and of the game.\n", "link": "http://arxiv.org/abs/2310.07867v5", "date": "2024-06-03", "relevancy": 1.7067, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4119}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cheap%20Talking%20Algorithms&body=Title%3A%20Cheap%20Talking%20Algorithms%0AAuthor%3A%20Daniele%20Condorelli%20and%20Massimiliano%20Furlan%0AAbstract%3A%20%20%20We%20simulate%20behaviour%20of%20two%20independent%20reinforcement%20learning%20algorithms%0Aplaying%20the%20Crawford%20and%20Sobel%20%281982%29%20game%20of%20strategic%20information%0Atransmission.%20We%20adopt%20memoryless%20algorithms%20to%20capture%20learning%20in%20a%20static%0Agame%20where%20a%20large%20population%20interacts%20anonymously.%20We%20show%20that%20sender%20and%0Areceiver%20converge%20to%20Nash%20equilibrium%20play.%20The%20level%20of%20informativeness%20of%20the%0Asender%27s%20cheap%20talk%20decreases%20as%20the%20bias%20increases%20and%2C%20at%20intermediate%20level%0Aof%20the%20bias%2C%20it%20matches%20the%20level%20predicted%20by%20the%20Pareto%20optimal%20equilibrium%0Aor%20by%20the%20second%20best%20one.%20Conclusions%20are%20robust%20to%20alternative%20specifications%0Aof%20the%20learning%20hyperparameters%20and%20of%20the%20game.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07867v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheap%2520Talking%2520Algorithms%26entry.906535625%3DDaniele%2520Condorelli%2520and%2520Massimiliano%2520Furlan%26entry.1292438233%3D%2520%2520We%2520simulate%2520behaviour%2520of%2520two%2520independent%2520reinforcement%2520learning%2520algorithms%250Aplaying%2520the%2520Crawford%2520and%2520Sobel%2520%25281982%2529%2520game%2520of%2520strategic%2520information%250Atransmission.%2520We%2520adopt%2520memoryless%2520algorithms%2520to%2520capture%2520learning%2520in%2520a%2520static%250Agame%2520where%2520a%2520large%2520population%2520interacts%2520anonymously.%2520We%2520show%2520that%2520sender%2520and%250Areceiver%2520converge%2520to%2520Nash%2520equilibrium%2520play.%2520The%2520level%2520of%2520informativeness%2520of%2520the%250Asender%2527s%2520cheap%2520talk%2520decreases%2520as%2520the%2520bias%2520increases%2520and%252C%2520at%2520intermediate%2520level%250Aof%2520the%2520bias%252C%2520it%2520matches%2520the%2520level%2520predicted%2520by%2520the%2520Pareto%2520optimal%2520equilibrium%250Aor%2520by%2520the%2520second%2520best%2520one.%2520Conclusions%2520are%2520robust%2520to%2520alternative%2520specifications%250Aof%2520the%2520learning%2520hyperparameters%2520and%2520of%2520the%2520game.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07867v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cheap%20Talking%20Algorithms&entry.906535625=Daniele%20Condorelli%20and%20Massimiliano%20Furlan&entry.1292438233=%20%20We%20simulate%20behaviour%20of%20two%20independent%20reinforcement%20learning%20algorithms%0Aplaying%20the%20Crawford%20and%20Sobel%20%281982%29%20game%20of%20strategic%20information%0Atransmission.%20We%20adopt%20memoryless%20algorithms%20to%20capture%20learning%20in%20a%20static%0Agame%20where%20a%20large%20population%20interacts%20anonymously.%20We%20show%20that%20sender%20and%0Areceiver%20converge%20to%20Nash%20equilibrium%20play.%20The%20level%20of%20informativeness%20of%20the%0Asender%27s%20cheap%20talk%20decreases%20as%20the%20bias%20increases%20and%2C%20at%20intermediate%20level%0Aof%20the%20bias%2C%20it%20matches%20the%20level%20predicted%20by%20the%20Pareto%20optimal%20equilibrium%0Aor%20by%20the%20second%20best%20one.%20Conclusions%20are%20robust%20to%20alternative%20specifications%0Aof%20the%20learning%20hyperparameters%20and%20of%20the%20game.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07867v5&entry.124074799=Read"},
{"title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the\n  Wild", "author": "Huayang Li and Siheng Li and Deng Cai and Longyue Wang and Lemao Liu and Taro Watanabe and Yujiu Yang and Shuming Shi", "abstract": "  Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.\n", "link": "http://arxiv.org/abs/2309.08637v5", "date": "2024-06-03", "relevancy": 1.6897, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5383}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextBind%3A%20Multi-turn%20Interleaved%20Multimodal%20Instruction-following%20in%20the%0A%20%20Wild&body=Title%3A%20TextBind%3A%20Multi-turn%20Interleaved%20Multimodal%20Instruction-following%20in%20the%0A%20%20Wild%0AAuthor%3A%20Huayang%20Li%20and%20Siheng%20Li%20and%20Deng%20Cai%20and%20Longyue%20Wang%20and%20Lemao%20Liu%20and%20Taro%20Watanabe%20and%20Yujiu%20Yang%20and%20Shuming%20Shi%0AAbstract%3A%20%20%20Large%20language%20models%20with%20instruction-following%20abilities%20have%0Arevolutionized%20the%20field%20of%20artificial%20intelligence.%20These%20models%20show%0Aexceptional%20generalizability%20to%20tackle%20various%20real-world%20tasks%20through%20their%0Anatural%20language%20interfaces.%20However%2C%20their%20performance%20heavily%20relies%20on%0Ahigh-quality%20exemplar%20data%2C%20which%20is%20often%20difficult%20to%20obtain.%20This%20challenge%0Ais%20further%20exacerbated%20when%20it%20comes%20to%20multimodal%20instruction%20following.%20We%0Aintroduce%20TextBind%2C%20an%20almost%20annotation-free%20framework%20for%20empowering%20larger%0Alanguage%20models%20with%20the%20multi-turn%20interleaved%20multimodal%0Ainstruction-following%20capabilities.%20Our%20approach%20requires%20only%20image-caption%0Apairs%20and%20generates%20multi-turn%20multimodal%20instruction-response%20conversations%0Afrom%20a%20language%20model.%20To%20accommodate%20interleaved%20image-text%20inputs%20and%0Aoutputs%2C%20we%20devise%20MIM%2C%20a%20language%20model-centric%20architecture%20that%20seamlessly%0Aintegrates%20image%20encoder%20and%20decoder%20models.%20We%20release%20our%20dataset%2C%20model%2C%20and%0Ademo%20to%20foster%20future%20research%20in%20the%20area%20of%20multimodal%20instruction%20following.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08637v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextBind%253A%2520Multi-turn%2520Interleaved%2520Multimodal%2520Instruction-following%2520in%2520the%250A%2520%2520Wild%26entry.906535625%3DHuayang%2520Li%2520and%2520Siheng%2520Li%2520and%2520Deng%2520Cai%2520and%2520Longyue%2520Wang%2520and%2520Lemao%2520Liu%2520and%2520Taro%2520Watanabe%2520and%2520Yujiu%2520Yang%2520and%2520Shuming%2520Shi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520with%2520instruction-following%2520abilities%2520have%250Arevolutionized%2520the%2520field%2520of%2520artificial%2520intelligence.%2520These%2520models%2520show%250Aexceptional%2520generalizability%2520to%2520tackle%2520various%2520real-world%2520tasks%2520through%2520their%250Anatural%2520language%2520interfaces.%2520However%252C%2520their%2520performance%2520heavily%2520relies%2520on%250Ahigh-quality%2520exemplar%2520data%252C%2520which%2520is%2520often%2520difficult%2520to%2520obtain.%2520This%2520challenge%250Ais%2520further%2520exacerbated%2520when%2520it%2520comes%2520to%2520multimodal%2520instruction%2520following.%2520We%250Aintroduce%2520TextBind%252C%2520an%2520almost%2520annotation-free%2520framework%2520for%2520empowering%2520larger%250Alanguage%2520models%2520with%2520the%2520multi-turn%2520interleaved%2520multimodal%250Ainstruction-following%2520capabilities.%2520Our%2520approach%2520requires%2520only%2520image-caption%250Apairs%2520and%2520generates%2520multi-turn%2520multimodal%2520instruction-response%2520conversations%250Afrom%2520a%2520language%2520model.%2520To%2520accommodate%2520interleaved%2520image-text%2520inputs%2520and%250Aoutputs%252C%2520we%2520devise%2520MIM%252C%2520a%2520language%2520model-centric%2520architecture%2520that%2520seamlessly%250Aintegrates%2520image%2520encoder%2520and%2520decoder%2520models.%2520We%2520release%2520our%2520dataset%252C%2520model%252C%2520and%250Ademo%2520to%2520foster%2520future%2520research%2520in%2520the%2520area%2520of%2520multimodal%2520instruction%2520following.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08637v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextBind%3A%20Multi-turn%20Interleaved%20Multimodal%20Instruction-following%20in%20the%0A%20%20Wild&entry.906535625=Huayang%20Li%20and%20Siheng%20Li%20and%20Deng%20Cai%20and%20Longyue%20Wang%20and%20Lemao%20Liu%20and%20Taro%20Watanabe%20and%20Yujiu%20Yang%20and%20Shuming%20Shi&entry.1292438233=%20%20Large%20language%20models%20with%20instruction-following%20abilities%20have%0Arevolutionized%20the%20field%20of%20artificial%20intelligence.%20These%20models%20show%0Aexceptional%20generalizability%20to%20tackle%20various%20real-world%20tasks%20through%20their%0Anatural%20language%20interfaces.%20However%2C%20their%20performance%20heavily%20relies%20on%0Ahigh-quality%20exemplar%20data%2C%20which%20is%20often%20difficult%20to%20obtain.%20This%20challenge%0Ais%20further%20exacerbated%20when%20it%20comes%20to%20multimodal%20instruction%20following.%20We%0Aintroduce%20TextBind%2C%20an%20almost%20annotation-free%20framework%20for%20empowering%20larger%0Alanguage%20models%20with%20the%20multi-turn%20interleaved%20multimodal%0Ainstruction-following%20capabilities.%20Our%20approach%20requires%20only%20image-caption%0Apairs%20and%20generates%20multi-turn%20multimodal%20instruction-response%20conversations%0Afrom%20a%20language%20model.%20To%20accommodate%20interleaved%20image-text%20inputs%20and%0Aoutputs%2C%20we%20devise%20MIM%2C%20a%20language%20model-centric%20architecture%20that%20seamlessly%0Aintegrates%20image%20encoder%20and%20decoder%20models.%20We%20release%20our%20dataset%2C%20model%2C%20and%0Ademo%20to%20foster%20future%20research%20in%20the%20area%20of%20multimodal%20instruction%20following.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08637v5&entry.124074799=Read"},
{"title": "Unlock the Power of Algorithm Features: A Generalization Analysis for\n  Algorithm Selection", "author": "Xingyu Wu and Yan Zhong and Jibin Wu and Yuxiao Huang and Sheng-hao Wu and Kay Chen Tan", "abstract": "  In the algorithm selection research, the discussion surrounding algorithm\nfeatures has been significantly overshadowed by the emphasis on problem\nfeatures. Although a few empirical studies have yielded evidence regarding the\neffectiveness of algorithm features, the potential benefits of incorporating\nalgorithm features into algorithm selection models and their suitability for\ndifferent scenarios remain unclear. In this paper, we address this gap by\nproposing the first provable guarantee for algorithm selection based on\nalgorithm features, taking a generalization perspective. We analyze the\nbenefits and costs associated with algorithm features and investigate how the\ngeneralization error is affected by different factors. Specifically, we examine\nadaptive and predefined algorithm features under transductive and inductive\nlearning paradigms, respectively, and derive upper bounds for the\ngeneralization error based on their model's Rademacher complexity. Our\ntheoretical findings not only provide tight upper bounds, but also offer\nanalytical insights into the impact of various factors, such as the training\nscale of problem instances and candidate algorithms, model parameters, feature\nvalues, and distributional differences between the training and test data.\nNotably, we demonstrate how models will benefit from algorithm features in\ncomplex scenarios involving many algorithms, and proves the positive\ncorrelation between generalization error bound and $\\chi^2$-divergence of\ndistributions.\n", "link": "http://arxiv.org/abs/2405.11349v2", "date": "2024-06-03", "relevancy": 1.6887, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4199}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlock%20the%20Power%20of%20Algorithm%20Features%3A%20A%20Generalization%20Analysis%20for%0A%20%20Algorithm%20Selection&body=Title%3A%20Unlock%20the%20Power%20of%20Algorithm%20Features%3A%20A%20Generalization%20Analysis%20for%0A%20%20Algorithm%20Selection%0AAuthor%3A%20Xingyu%20Wu%20and%20Yan%20Zhong%20and%20Jibin%20Wu%20and%20Yuxiao%20Huang%20and%20Sheng-hao%20Wu%20and%20Kay%20Chen%20Tan%0AAbstract%3A%20%20%20In%20the%20algorithm%20selection%20research%2C%20the%20discussion%20surrounding%20algorithm%0Afeatures%20has%20been%20significantly%20overshadowed%20by%20the%20emphasis%20on%20problem%0Afeatures.%20Although%20a%20few%20empirical%20studies%20have%20yielded%20evidence%20regarding%20the%0Aeffectiveness%20of%20algorithm%20features%2C%20the%20potential%20benefits%20of%20incorporating%0Aalgorithm%20features%20into%20algorithm%20selection%20models%20and%20their%20suitability%20for%0Adifferent%20scenarios%20remain%20unclear.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%0Aproposing%20the%20first%20provable%20guarantee%20for%20algorithm%20selection%20based%20on%0Aalgorithm%20features%2C%20taking%20a%20generalization%20perspective.%20We%20analyze%20the%0Abenefits%20and%20costs%20associated%20with%20algorithm%20features%20and%20investigate%20how%20the%0Ageneralization%20error%20is%20affected%20by%20different%20factors.%20Specifically%2C%20we%20examine%0Aadaptive%20and%20predefined%20algorithm%20features%20under%20transductive%20and%20inductive%0Alearning%20paradigms%2C%20respectively%2C%20and%20derive%20upper%20bounds%20for%20the%0Ageneralization%20error%20based%20on%20their%20model%27s%20Rademacher%20complexity.%20Our%0Atheoretical%20findings%20not%20only%20provide%20tight%20upper%20bounds%2C%20but%20also%20offer%0Aanalytical%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%20the%20training%0Ascale%20of%20problem%20instances%20and%20candidate%20algorithms%2C%20model%20parameters%2C%20feature%0Avalues%2C%20and%20distributional%20differences%20between%20the%20training%20and%20test%20data.%0ANotably%2C%20we%20demonstrate%20how%20models%20will%20benefit%20from%20algorithm%20features%20in%0Acomplex%20scenarios%20involving%20many%20algorithms%2C%20and%20proves%20the%20positive%0Acorrelation%20between%20generalization%20error%20bound%20and%20%24%5Cchi%5E2%24-divergence%20of%0Adistributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlock%2520the%2520Power%2520of%2520Algorithm%2520Features%253A%2520A%2520Generalization%2520Analysis%2520for%250A%2520%2520Algorithm%2520Selection%26entry.906535625%3DXingyu%2520Wu%2520and%2520Yan%2520Zhong%2520and%2520Jibin%2520Wu%2520and%2520Yuxiao%2520Huang%2520and%2520Sheng-hao%2520Wu%2520and%2520Kay%2520Chen%2520Tan%26entry.1292438233%3D%2520%2520In%2520the%2520algorithm%2520selection%2520research%252C%2520the%2520discussion%2520surrounding%2520algorithm%250Afeatures%2520has%2520been%2520significantly%2520overshadowed%2520by%2520the%2520emphasis%2520on%2520problem%250Afeatures.%2520Although%2520a%2520few%2520empirical%2520studies%2520have%2520yielded%2520evidence%2520regarding%2520the%250Aeffectiveness%2520of%2520algorithm%2520features%252C%2520the%2520potential%2520benefits%2520of%2520incorporating%250Aalgorithm%2520features%2520into%2520algorithm%2520selection%2520models%2520and%2520their%2520suitability%2520for%250Adifferent%2520scenarios%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%250Aproposing%2520the%2520first%2520provable%2520guarantee%2520for%2520algorithm%2520selection%2520based%2520on%250Aalgorithm%2520features%252C%2520taking%2520a%2520generalization%2520perspective.%2520We%2520analyze%2520the%250Abenefits%2520and%2520costs%2520associated%2520with%2520algorithm%2520features%2520and%2520investigate%2520how%2520the%250Ageneralization%2520error%2520is%2520affected%2520by%2520different%2520factors.%2520Specifically%252C%2520we%2520examine%250Aadaptive%2520and%2520predefined%2520algorithm%2520features%2520under%2520transductive%2520and%2520inductive%250Alearning%2520paradigms%252C%2520respectively%252C%2520and%2520derive%2520upper%2520bounds%2520for%2520the%250Ageneralization%2520error%2520based%2520on%2520their%2520model%2527s%2520Rademacher%2520complexity.%2520Our%250Atheoretical%2520findings%2520not%2520only%2520provide%2520tight%2520upper%2520bounds%252C%2520but%2520also%2520offer%250Aanalytical%2520insights%2520into%2520the%2520impact%2520of%2520various%2520factors%252C%2520such%2520as%2520the%2520training%250Ascale%2520of%2520problem%2520instances%2520and%2520candidate%2520algorithms%252C%2520model%2520parameters%252C%2520feature%250Avalues%252C%2520and%2520distributional%2520differences%2520between%2520the%2520training%2520and%2520test%2520data.%250ANotably%252C%2520we%2520demonstrate%2520how%2520models%2520will%2520benefit%2520from%2520algorithm%2520features%2520in%250Acomplex%2520scenarios%2520involving%2520many%2520algorithms%252C%2520and%2520proves%2520the%2520positive%250Acorrelation%2520between%2520generalization%2520error%2520bound%2520and%2520%2524%255Cchi%255E2%2524-divergence%2520of%250Adistributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlock%20the%20Power%20of%20Algorithm%20Features%3A%20A%20Generalization%20Analysis%20for%0A%20%20Algorithm%20Selection&entry.906535625=Xingyu%20Wu%20and%20Yan%20Zhong%20and%20Jibin%20Wu%20and%20Yuxiao%20Huang%20and%20Sheng-hao%20Wu%20and%20Kay%20Chen%20Tan&entry.1292438233=%20%20In%20the%20algorithm%20selection%20research%2C%20the%20discussion%20surrounding%20algorithm%0Afeatures%20has%20been%20significantly%20overshadowed%20by%20the%20emphasis%20on%20problem%0Afeatures.%20Although%20a%20few%20empirical%20studies%20have%20yielded%20evidence%20regarding%20the%0Aeffectiveness%20of%20algorithm%20features%2C%20the%20potential%20benefits%20of%20incorporating%0Aalgorithm%20features%20into%20algorithm%20selection%20models%20and%20their%20suitability%20for%0Adifferent%20scenarios%20remain%20unclear.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%0Aproposing%20the%20first%20provable%20guarantee%20for%20algorithm%20selection%20based%20on%0Aalgorithm%20features%2C%20taking%20a%20generalization%20perspective.%20We%20analyze%20the%0Abenefits%20and%20costs%20associated%20with%20algorithm%20features%20and%20investigate%20how%20the%0Ageneralization%20error%20is%20affected%20by%20different%20factors.%20Specifically%2C%20we%20examine%0Aadaptive%20and%20predefined%20algorithm%20features%20under%20transductive%20and%20inductive%0Alearning%20paradigms%2C%20respectively%2C%20and%20derive%20upper%20bounds%20for%20the%0Ageneralization%20error%20based%20on%20their%20model%27s%20Rademacher%20complexity.%20Our%0Atheoretical%20findings%20not%20only%20provide%20tight%20upper%20bounds%2C%20but%20also%20offer%0Aanalytical%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%20the%20training%0Ascale%20of%20problem%20instances%20and%20candidate%20algorithms%2C%20model%20parameters%2C%20feature%0Avalues%2C%20and%20distributional%20differences%20between%20the%20training%20and%20test%20data.%0ANotably%2C%20we%20demonstrate%20how%20models%20will%20benefit%20from%20algorithm%20features%20in%0Acomplex%20scenarios%20involving%20many%20algorithms%2C%20and%20proves%20the%20positive%0Acorrelation%20between%20generalization%20error%20bound%20and%20%24%5Cchi%5E2%24-divergence%20of%0Adistributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11349v2&entry.124074799=Read"},
{"title": "Quantum Generative Diffusion Model: A Fully Quantum-Mechanical Model for\n  Generating Quantum State Ensemble", "author": "Chuangtao Chen and Qinglin Zhao and MengChu Zhou and Zhimin He and Zhili Sun and Haozhen Situ", "abstract": "  Classical diffusion models have shown superior generative results and have\nbeen applied to many problems. Exploring these models in the quantum domain can\nadvance the field of quantum generative learning. In this paper, we introduce\nthe Quantum Generative Diffusion Model (QGDM), a simple and elegant quantum\ncounterpart of classical diffusion models.\n  The core idea of QGDM is that any target quantum state can be transformed\ninto a completely mixed state, which has the highest entropy and maximum\nuncertainty about the system, through a non-unitary forward process.\nSubsequently, a trainable backward process can be used to recover the target\nstate from the completely mixed state. The design requirements for QGDM's\nbackward process include ensuring non-unitarity while maintaining a low number\nof parameters. To achieve this, we introduce partial trace operations in the\nbackward process to enforce non-unitary. Additionally, we control the number of\ntrainable parameters by using a parameter-sharing strategy and incorporating\ntemporal information as an input in the backward process. Furthermore, we\nintroduce a resource-efficient version of QGDM, which reduces the number of\nauxiliary qubits while preserving impressive generative capabilities.\n  Our proposed models exhibit better convergence performance than Quantum\nGenerative Adversarial Networks (QGANs) because our models optimize a convex\ndistance function using gradient descent. Comparative results with QGANs\ndemonstrate the effectiveness of our models in generating both pure and mixed\nquantum states. Notably, our models achieve 53.03% higher fidelity in\nmixed-state generation tasks compared to QGANs. These results highlight the\npotential of the proposed models to tackle challenging quantum generation\ntasks.\n", "link": "http://arxiv.org/abs/2401.07039v2", "date": "2024-06-03", "relevancy": 1.6767, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5673}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5504}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Generative%20Diffusion%20Model%3A%20A%20Fully%20Quantum-Mechanical%20Model%20for%0A%20%20Generating%20Quantum%20State%20Ensemble&body=Title%3A%20Quantum%20Generative%20Diffusion%20Model%3A%20A%20Fully%20Quantum-Mechanical%20Model%20for%0A%20%20Generating%20Quantum%20State%20Ensemble%0AAuthor%3A%20Chuangtao%20Chen%20and%20Qinglin%20Zhao%20and%20MengChu%20Zhou%20and%20Zhimin%20He%20and%20Zhili%20Sun%20and%20Haozhen%20Situ%0AAbstract%3A%20%20%20Classical%20diffusion%20models%20have%20shown%20superior%20generative%20results%20and%20have%0Abeen%20applied%20to%20many%20problems.%20Exploring%20these%20models%20in%20the%20quantum%20domain%20can%0Aadvance%20the%20field%20of%20quantum%20generative%20learning.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Quantum%20Generative%20Diffusion%20Model%20%28QGDM%29%2C%20a%20simple%20and%20elegant%20quantum%0Acounterpart%20of%20classical%20diffusion%20models.%0A%20%20The%20core%20idea%20of%20QGDM%20is%20that%20any%20target%20quantum%20state%20can%20be%20transformed%0Ainto%20a%20completely%20mixed%20state%2C%20which%20has%20the%20highest%20entropy%20and%20maximum%0Auncertainty%20about%20the%20system%2C%20through%20a%20non-unitary%20forward%20process.%0ASubsequently%2C%20a%20trainable%20backward%20process%20can%20be%20used%20to%20recover%20the%20target%0Astate%20from%20the%20completely%20mixed%20state.%20The%20design%20requirements%20for%20QGDM%27s%0Abackward%20process%20include%20ensuring%20non-unitarity%20while%20maintaining%20a%20low%20number%0Aof%20parameters.%20To%20achieve%20this%2C%20we%20introduce%20partial%20trace%20operations%20in%20the%0Abackward%20process%20to%20enforce%20non-unitary.%20Additionally%2C%20we%20control%20the%20number%20of%0Atrainable%20parameters%20by%20using%20a%20parameter-sharing%20strategy%20and%20incorporating%0Atemporal%20information%20as%20an%20input%20in%20the%20backward%20process.%20Furthermore%2C%20we%0Aintroduce%20a%20resource-efficient%20version%20of%20QGDM%2C%20which%20reduces%20the%20number%20of%0Aauxiliary%20qubits%20while%20preserving%20impressive%20generative%20capabilities.%0A%20%20Our%20proposed%20models%20exhibit%20better%20convergence%20performance%20than%20Quantum%0AGenerative%20Adversarial%20Networks%20%28QGANs%29%20because%20our%20models%20optimize%20a%20convex%0Adistance%20function%20using%20gradient%20descent.%20Comparative%20results%20with%20QGANs%0Ademonstrate%20the%20effectiveness%20of%20our%20models%20in%20generating%20both%20pure%20and%20mixed%0Aquantum%20states.%20Notably%2C%20our%20models%20achieve%2053.03%25%20higher%20fidelity%20in%0Amixed-state%20generation%20tasks%20compared%20to%20QGANs.%20These%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20models%20to%20tackle%20challenging%20quantum%20generation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Generative%2520Diffusion%2520Model%253A%2520A%2520Fully%2520Quantum-Mechanical%2520Model%2520for%250A%2520%2520Generating%2520Quantum%2520State%2520Ensemble%26entry.906535625%3DChuangtao%2520Chen%2520and%2520Qinglin%2520Zhao%2520and%2520MengChu%2520Zhou%2520and%2520Zhimin%2520He%2520and%2520Zhili%2520Sun%2520and%2520Haozhen%2520Situ%26entry.1292438233%3D%2520%2520Classical%2520diffusion%2520models%2520have%2520shown%2520superior%2520generative%2520results%2520and%2520have%250Abeen%2520applied%2520to%2520many%2520problems.%2520Exploring%2520these%2520models%2520in%2520the%2520quantum%2520domain%2520can%250Aadvance%2520the%2520field%2520of%2520quantum%2520generative%2520learning.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Athe%2520Quantum%2520Generative%2520Diffusion%2520Model%2520%2528QGDM%2529%252C%2520a%2520simple%2520and%2520elegant%2520quantum%250Acounterpart%2520of%2520classical%2520diffusion%2520models.%250A%2520%2520The%2520core%2520idea%2520of%2520QGDM%2520is%2520that%2520any%2520target%2520quantum%2520state%2520can%2520be%2520transformed%250Ainto%2520a%2520completely%2520mixed%2520state%252C%2520which%2520has%2520the%2520highest%2520entropy%2520and%2520maximum%250Auncertainty%2520about%2520the%2520system%252C%2520through%2520a%2520non-unitary%2520forward%2520process.%250ASubsequently%252C%2520a%2520trainable%2520backward%2520process%2520can%2520be%2520used%2520to%2520recover%2520the%2520target%250Astate%2520from%2520the%2520completely%2520mixed%2520state.%2520The%2520design%2520requirements%2520for%2520QGDM%2527s%250Abackward%2520process%2520include%2520ensuring%2520non-unitarity%2520while%2520maintaining%2520a%2520low%2520number%250Aof%2520parameters.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520partial%2520trace%2520operations%2520in%2520the%250Abackward%2520process%2520to%2520enforce%2520non-unitary.%2520Additionally%252C%2520we%2520control%2520the%2520number%2520of%250Atrainable%2520parameters%2520by%2520using%2520a%2520parameter-sharing%2520strategy%2520and%2520incorporating%250Atemporal%2520information%2520as%2520an%2520input%2520in%2520the%2520backward%2520process.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520resource-efficient%2520version%2520of%2520QGDM%252C%2520which%2520reduces%2520the%2520number%2520of%250Aauxiliary%2520qubits%2520while%2520preserving%2520impressive%2520generative%2520capabilities.%250A%2520%2520Our%2520proposed%2520models%2520exhibit%2520better%2520convergence%2520performance%2520than%2520Quantum%250AGenerative%2520Adversarial%2520Networks%2520%2528QGANs%2529%2520because%2520our%2520models%2520optimize%2520a%2520convex%250Adistance%2520function%2520using%2520gradient%2520descent.%2520Comparative%2520results%2520with%2520QGANs%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520models%2520in%2520generating%2520both%2520pure%2520and%2520mixed%250Aquantum%2520states.%2520Notably%252C%2520our%2520models%2520achieve%252053.03%2525%2520higher%2520fidelity%2520in%250Amixed-state%2520generation%2520tasks%2520compared%2520to%2520QGANs.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520the%2520proposed%2520models%2520to%2520tackle%2520challenging%2520quantum%2520generation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Generative%20Diffusion%20Model%3A%20A%20Fully%20Quantum-Mechanical%20Model%20for%0A%20%20Generating%20Quantum%20State%20Ensemble&entry.906535625=Chuangtao%20Chen%20and%20Qinglin%20Zhao%20and%20MengChu%20Zhou%20and%20Zhimin%20He%20and%20Zhili%20Sun%20and%20Haozhen%20Situ&entry.1292438233=%20%20Classical%20diffusion%20models%20have%20shown%20superior%20generative%20results%20and%20have%0Abeen%20applied%20to%20many%20problems.%20Exploring%20these%20models%20in%20the%20quantum%20domain%20can%0Aadvance%20the%20field%20of%20quantum%20generative%20learning.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Quantum%20Generative%20Diffusion%20Model%20%28QGDM%29%2C%20a%20simple%20and%20elegant%20quantum%0Acounterpart%20of%20classical%20diffusion%20models.%0A%20%20The%20core%20idea%20of%20QGDM%20is%20that%20any%20target%20quantum%20state%20can%20be%20transformed%0Ainto%20a%20completely%20mixed%20state%2C%20which%20has%20the%20highest%20entropy%20and%20maximum%0Auncertainty%20about%20the%20system%2C%20through%20a%20non-unitary%20forward%20process.%0ASubsequently%2C%20a%20trainable%20backward%20process%20can%20be%20used%20to%20recover%20the%20target%0Astate%20from%20the%20completely%20mixed%20state.%20The%20design%20requirements%20for%20QGDM%27s%0Abackward%20process%20include%20ensuring%20non-unitarity%20while%20maintaining%20a%20low%20number%0Aof%20parameters.%20To%20achieve%20this%2C%20we%20introduce%20partial%20trace%20operations%20in%20the%0Abackward%20process%20to%20enforce%20non-unitary.%20Additionally%2C%20we%20control%20the%20number%20of%0Atrainable%20parameters%20by%20using%20a%20parameter-sharing%20strategy%20and%20incorporating%0Atemporal%20information%20as%20an%20input%20in%20the%20backward%20process.%20Furthermore%2C%20we%0Aintroduce%20a%20resource-efficient%20version%20of%20QGDM%2C%20which%20reduces%20the%20number%20of%0Aauxiliary%20qubits%20while%20preserving%20impressive%20generative%20capabilities.%0A%20%20Our%20proposed%20models%20exhibit%20better%20convergence%20performance%20than%20Quantum%0AGenerative%20Adversarial%20Networks%20%28QGANs%29%20because%20our%20models%20optimize%20a%20convex%0Adistance%20function%20using%20gradient%20descent.%20Comparative%20results%20with%20QGANs%0Ademonstrate%20the%20effectiveness%20of%20our%20models%20in%20generating%20both%20pure%20and%20mixed%0Aquantum%20states.%20Notably%2C%20our%20models%20achieve%2053.03%25%20higher%20fidelity%20in%0Amixed-state%20generation%20tasks%20compared%20to%20QGANs.%20These%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20models%20to%20tackle%20challenging%20quantum%20generation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07039v2&entry.124074799=Read"},
{"title": "Robotic Imitation of Human Actions", "author": "Josua Spisak and Matthias Kerzel and Stefan Wermter", "abstract": "  Imitation can allow us to quickly gain an understanding of a new task.\nThrough a demonstration, we can gain direct knowledge about which actions need\nto be performed and which goals they have. In this paper, we introduce a new\napproach to imitation learning that tackles the challenges of a robot imitating\na human, such as the change in perspective and body schema. Our approach can\nuse a single human demonstration to abstract information about the demonstrated\ntask, and use that information to generalise and replicate it. We facilitate\nthis ability by a new integration of two state-of-the-art methods: a diffusion\naction segmentation model to abstract temporal information from the\ndemonstration and an open vocabulary object detector for spatial information.\nFurthermore, we refine the abstracted information and use symbolic reasoning to\ncreate an action plan utilising inverse kinematics, to allow the robot to\nimitate the demonstrated action.\n", "link": "http://arxiv.org/abs/2401.08381v2", "date": "2024-06-03", "relevancy": 1.6702, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6279}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.58}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Imitation%20of%20Human%20Actions&body=Title%3A%20Robotic%20Imitation%20of%20Human%20Actions%0AAuthor%3A%20Josua%20Spisak%20and%20Matthias%20Kerzel%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Imitation%20can%20allow%20us%20to%20quickly%20gain%20an%20understanding%20of%20a%20new%20task.%0AThrough%20a%20demonstration%2C%20we%20can%20gain%20direct%20knowledge%20about%20which%20actions%20need%0Ato%20be%20performed%20and%20which%20goals%20they%20have.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0Aapproach%20to%20imitation%20learning%20that%20tackles%20the%20challenges%20of%20a%20robot%20imitating%0Aa%20human%2C%20such%20as%20the%20change%20in%20perspective%20and%20body%20schema.%20Our%20approach%20can%0Ause%20a%20single%20human%20demonstration%20to%20abstract%20information%20about%20the%20demonstrated%0Atask%2C%20and%20use%20that%20information%20to%20generalise%20and%20replicate%20it.%20We%20facilitate%0Athis%20ability%20by%20a%20new%20integration%20of%20two%20state-of-the-art%20methods%3A%20a%20diffusion%0Aaction%20segmentation%20model%20to%20abstract%20temporal%20information%20from%20the%0Ademonstration%20and%20an%20open%20vocabulary%20object%20detector%20for%20spatial%20information.%0AFurthermore%2C%20we%20refine%20the%20abstracted%20information%20and%20use%20symbolic%20reasoning%20to%0Acreate%20an%20action%20plan%20utilising%20inverse%20kinematics%2C%20to%20allow%20the%20robot%20to%0Aimitate%20the%20demonstrated%20action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Imitation%2520of%2520Human%2520Actions%26entry.906535625%3DJosua%2520Spisak%2520and%2520Matthias%2520Kerzel%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520Imitation%2520can%2520allow%2520us%2520to%2520quickly%2520gain%2520an%2520understanding%2520of%2520a%2520new%2520task.%250AThrough%2520a%2520demonstration%252C%2520we%2520can%2520gain%2520direct%2520knowledge%2520about%2520which%2520actions%2520need%250Ato%2520be%2520performed%2520and%2520which%2520goals%2520they%2520have.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%250Aapproach%2520to%2520imitation%2520learning%2520that%2520tackles%2520the%2520challenges%2520of%2520a%2520robot%2520imitating%250Aa%2520human%252C%2520such%2520as%2520the%2520change%2520in%2520perspective%2520and%2520body%2520schema.%2520Our%2520approach%2520can%250Ause%2520a%2520single%2520human%2520demonstration%2520to%2520abstract%2520information%2520about%2520the%2520demonstrated%250Atask%252C%2520and%2520use%2520that%2520information%2520to%2520generalise%2520and%2520replicate%2520it.%2520We%2520facilitate%250Athis%2520ability%2520by%2520a%2520new%2520integration%2520of%2520two%2520state-of-the-art%2520methods%253A%2520a%2520diffusion%250Aaction%2520segmentation%2520model%2520to%2520abstract%2520temporal%2520information%2520from%2520the%250Ademonstration%2520and%2520an%2520open%2520vocabulary%2520object%2520detector%2520for%2520spatial%2520information.%250AFurthermore%252C%2520we%2520refine%2520the%2520abstracted%2520information%2520and%2520use%2520symbolic%2520reasoning%2520to%250Acreate%2520an%2520action%2520plan%2520utilising%2520inverse%2520kinematics%252C%2520to%2520allow%2520the%2520robot%2520to%250Aimitate%2520the%2520demonstrated%2520action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Imitation%20of%20Human%20Actions&entry.906535625=Josua%20Spisak%20and%20Matthias%20Kerzel%20and%20Stefan%20Wermter&entry.1292438233=%20%20Imitation%20can%20allow%20us%20to%20quickly%20gain%20an%20understanding%20of%20a%20new%20task.%0AThrough%20a%20demonstration%2C%20we%20can%20gain%20direct%20knowledge%20about%20which%20actions%20need%0Ato%20be%20performed%20and%20which%20goals%20they%20have.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0Aapproach%20to%20imitation%20learning%20that%20tackles%20the%20challenges%20of%20a%20robot%20imitating%0Aa%20human%2C%20such%20as%20the%20change%20in%20perspective%20and%20body%20schema.%20Our%20approach%20can%0Ause%20a%20single%20human%20demonstration%20to%20abstract%20information%20about%20the%20demonstrated%0Atask%2C%20and%20use%20that%20information%20to%20generalise%20and%20replicate%20it.%20We%20facilitate%0Athis%20ability%20by%20a%20new%20integration%20of%20two%20state-of-the-art%20methods%3A%20a%20diffusion%0Aaction%20segmentation%20model%20to%20abstract%20temporal%20information%20from%20the%0Ademonstration%20and%20an%20open%20vocabulary%20object%20detector%20for%20spatial%20information.%0AFurthermore%2C%20we%20refine%20the%20abstracted%20information%20and%20use%20symbolic%20reasoning%20to%0Acreate%20an%20action%20plan%20utilising%20inverse%20kinematics%2C%20to%20allow%20the%20robot%20to%0Aimitate%20the%20demonstrated%20action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08381v2&entry.124074799=Read"},
{"title": "Connecting the Dots: Collaborative Fine-tuning for Black-Box\n  Vision-Language Models", "author": "Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan", "abstract": "  With the emergence of pretrained vision-language models (VLMs), considerable\nefforts have been devoted to fine-tuning them for downstream tasks. Despite the\nprogress made in designing efficient fine-tuning methods, such methods require\naccess to the model's parameters, which can be challenging as model owners\noften opt to provide their models as a black box to safeguard model ownership.\nThis paper proposes a \\textbf{C}ollabo\\textbf{ra}tive\n\\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning\nblack-box VLMs to downstream tasks, where one only has access to the input\nprompts and the output predictions of the model. CraFT comprises two modules, a\nprompt generation module for learning text prompts and a prediction refinement\nmodule for enhancing output predictions in residual style. Additionally, we\nintroduce an auxiliary prediction-consistent loss to promote consistent\noptimization across these modules. These modules are optimized by a novel\ncollaborative training algorithm. Extensive experiments on few-shot\nclassification over 15 datasets demonstrate the superiority of CraFT. The\nresults show that CraFT achieves a decent gain of about 12\\% with 16-shot\ndatasets and only 8,000 queries. Moreover, CraFT trains faster and uses only\nabout 1/80 of the memory footprint for deployment, while sacrificing only\n1.62\\% compared to the white-box method. Our code is publicly available at\nhttps://github.com/mrflogs/CraFT .\n", "link": "http://arxiv.org/abs/2402.04050v2", "date": "2024-06-03", "relevancy": 1.6618, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.548}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20the%20Dots%3A%20Collaborative%20Fine-tuning%20for%20Black-Box%0A%20%20Vision-Language%20Models&body=Title%3A%20Connecting%20the%20Dots%3A%20Collaborative%20Fine-tuning%20for%20Black-Box%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20pretrained%20vision-language%20models%20%28VLMs%29%2C%20considerable%0Aefforts%20have%20been%20devoted%20to%20fine-tuning%20them%20for%20downstream%20tasks.%20Despite%20the%0Aprogress%20made%20in%20designing%20efficient%20fine-tuning%20methods%2C%20such%20methods%20require%0Aaccess%20to%20the%20model%27s%20parameters%2C%20which%20can%20be%20challenging%20as%20model%20owners%0Aoften%20opt%20to%20provide%20their%20models%20as%20a%20black%20box%20to%20safeguard%20model%20ownership.%0AThis%20paper%20proposes%20a%20%5Ctextbf%7BC%7Dollabo%5Ctextbf%7Bra%7Dtive%0A%5Ctextbf%7BF%7Dine-%5Ctextbf%7BT%7Duning%20%28%5Ctextbf%7BCraFT%7D%29%20approach%20for%20fine-tuning%0Ablack-box%20VLMs%20to%20downstream%20tasks%2C%20where%20one%20only%20has%20access%20to%20the%20input%0Aprompts%20and%20the%20output%20predictions%20of%20the%20model.%20CraFT%20comprises%20two%20modules%2C%20a%0Aprompt%20generation%20module%20for%20learning%20text%20prompts%20and%20a%20prediction%20refinement%0Amodule%20for%20enhancing%20output%20predictions%20in%20residual%20style.%20Additionally%2C%20we%0Aintroduce%20an%20auxiliary%20prediction-consistent%20loss%20to%20promote%20consistent%0Aoptimization%20across%20these%20modules.%20These%20modules%20are%20optimized%20by%20a%20novel%0Acollaborative%20training%20algorithm.%20Extensive%20experiments%20on%20few-shot%0Aclassification%20over%2015%20datasets%20demonstrate%20the%20superiority%20of%20CraFT.%20The%0Aresults%20show%20that%20CraFT%20achieves%20a%20decent%20gain%20of%20about%2012%5C%25%20with%2016-shot%0Adatasets%20and%20only%208%2C000%20queries.%20Moreover%2C%20CraFT%20trains%20faster%20and%20uses%20only%0Aabout%201/80%20of%20the%20memory%20footprint%20for%20deployment%2C%20while%20sacrificing%20only%0A1.62%5C%25%20compared%20to%20the%20white-box%20method.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mrflogs/CraFT%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520the%2520Dots%253A%2520Collaborative%2520Fine-tuning%2520for%2520Black-Box%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DZhengbo%2520Wang%2520and%2520Jian%2520Liang%2520and%2520Ran%2520He%2520and%2520Zilei%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520pretrained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520considerable%250Aefforts%2520have%2520been%2520devoted%2520to%2520fine-tuning%2520them%2520for%2520downstream%2520tasks.%2520Despite%2520the%250Aprogress%2520made%2520in%2520designing%2520efficient%2520fine-tuning%2520methods%252C%2520such%2520methods%2520require%250Aaccess%2520to%2520the%2520model%2527s%2520parameters%252C%2520which%2520can%2520be%2520challenging%2520as%2520model%2520owners%250Aoften%2520opt%2520to%2520provide%2520their%2520models%2520as%2520a%2520black%2520box%2520to%2520safeguard%2520model%2520ownership.%250AThis%2520paper%2520proposes%2520a%2520%255Ctextbf%257BC%257Dollabo%255Ctextbf%257Bra%257Dtive%250A%255Ctextbf%257BF%257Dine-%255Ctextbf%257BT%257Duning%2520%2528%255Ctextbf%257BCraFT%257D%2529%2520approach%2520for%2520fine-tuning%250Ablack-box%2520VLMs%2520to%2520downstream%2520tasks%252C%2520where%2520one%2520only%2520has%2520access%2520to%2520the%2520input%250Aprompts%2520and%2520the%2520output%2520predictions%2520of%2520the%2520model.%2520CraFT%2520comprises%2520two%2520modules%252C%2520a%250Aprompt%2520generation%2520module%2520for%2520learning%2520text%2520prompts%2520and%2520a%2520prediction%2520refinement%250Amodule%2520for%2520enhancing%2520output%2520predictions%2520in%2520residual%2520style.%2520Additionally%252C%2520we%250Aintroduce%2520an%2520auxiliary%2520prediction-consistent%2520loss%2520to%2520promote%2520consistent%250Aoptimization%2520across%2520these%2520modules.%2520These%2520modules%2520are%2520optimized%2520by%2520a%2520novel%250Acollaborative%2520training%2520algorithm.%2520Extensive%2520experiments%2520on%2520few-shot%250Aclassification%2520over%252015%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520CraFT.%2520The%250Aresults%2520show%2520that%2520CraFT%2520achieves%2520a%2520decent%2520gain%2520of%2520about%252012%255C%2525%2520with%252016-shot%250Adatasets%2520and%2520only%25208%252C000%2520queries.%2520Moreover%252C%2520CraFT%2520trains%2520faster%2520and%2520uses%2520only%250Aabout%25201/80%2520of%2520the%2520memory%2520footprint%2520for%2520deployment%252C%2520while%2520sacrificing%2520only%250A1.62%255C%2525%2520compared%2520to%2520the%2520white-box%2520method.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mrflogs/CraFT%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20the%20Dots%3A%20Collaborative%20Fine-tuning%20for%20Black-Box%0A%20%20Vision-Language%20Models&entry.906535625=Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20With%20the%20emergence%20of%20pretrained%20vision-language%20models%20%28VLMs%29%2C%20considerable%0Aefforts%20have%20been%20devoted%20to%20fine-tuning%20them%20for%20downstream%20tasks.%20Despite%20the%0Aprogress%20made%20in%20designing%20efficient%20fine-tuning%20methods%2C%20such%20methods%20require%0Aaccess%20to%20the%20model%27s%20parameters%2C%20which%20can%20be%20challenging%20as%20model%20owners%0Aoften%20opt%20to%20provide%20their%20models%20as%20a%20black%20box%20to%20safeguard%20model%20ownership.%0AThis%20paper%20proposes%20a%20%5Ctextbf%7BC%7Dollabo%5Ctextbf%7Bra%7Dtive%0A%5Ctextbf%7BF%7Dine-%5Ctextbf%7BT%7Duning%20%28%5Ctextbf%7BCraFT%7D%29%20approach%20for%20fine-tuning%0Ablack-box%20VLMs%20to%20downstream%20tasks%2C%20where%20one%20only%20has%20access%20to%20the%20input%0Aprompts%20and%20the%20output%20predictions%20of%20the%20model.%20CraFT%20comprises%20two%20modules%2C%20a%0Aprompt%20generation%20module%20for%20learning%20text%20prompts%20and%20a%20prediction%20refinement%0Amodule%20for%20enhancing%20output%20predictions%20in%20residual%20style.%20Additionally%2C%20we%0Aintroduce%20an%20auxiliary%20prediction-consistent%20loss%20to%20promote%20consistent%0Aoptimization%20across%20these%20modules.%20These%20modules%20are%20optimized%20by%20a%20novel%0Acollaborative%20training%20algorithm.%20Extensive%20experiments%20on%20few-shot%0Aclassification%20over%2015%20datasets%20demonstrate%20the%20superiority%20of%20CraFT.%20The%0Aresults%20show%20that%20CraFT%20achieves%20a%20decent%20gain%20of%20about%2012%5C%25%20with%2016-shot%0Adatasets%20and%20only%208%2C000%20queries.%20Moreover%2C%20CraFT%20trains%20faster%20and%20uses%20only%0Aabout%201/80%20of%20the%20memory%20footprint%20for%20deployment%2C%20while%20sacrificing%20only%0A1.62%5C%25%20compared%20to%20the%20white-box%20method.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mrflogs/CraFT%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04050v2&entry.124074799=Read"},
{"title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained\n  Representations", "author": "Benedikt Alkin and Lukas Miklautz and Sepp Hochreiter and Johannes Brandstetter", "abstract": "  We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning\nboost for pre-trained MIM models. MIM-Refiner is motivated by the insight that\nstrong representations within MIM models generally reside in intermediate\nlayers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are\nconnected to different intermediate layers. In each head, a modified nearest\nneighbor objective constructs semantic clusters that capture semantic\ninformation which improves performance on downstream tasks, including\noff-the-shelf and fine-tuning settings.\n  The refinement process is short and simple - yet highly effective. Within a\nfew epochs, we refine the features of MIM models from subpar to\nstate-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with\ndata2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing\n(84.7%) and low-shot classification among models that are pre-trained on\nImageNet-1K. At ImageNet-1K 1-shot classification, MIM-Refiner advances the\nstate-of-the-art to 64.2%, outperforming larger models that were trained on up\nto 2000 times more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B.\n", "link": "http://arxiv.org/abs/2402.10093v2", "date": "2024-06-03", "relevancy": 1.6309, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5791}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5026}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIM-Refiner%3A%20A%20Contrastive%20Learning%20Boost%20from%20Intermediate%20Pre-Trained%0A%20%20Representations&body=Title%3A%20MIM-Refiner%3A%20A%20Contrastive%20Learning%20Boost%20from%20Intermediate%20Pre-Trained%0A%20%20Representations%0AAuthor%3A%20Benedikt%20Alkin%20and%20Lukas%20Miklautz%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20We%20introduce%20MIM%20%28Masked%20Image%20Modeling%29-Refiner%2C%20a%20contrastive%20learning%0Aboost%20for%20pre-trained%20MIM%20models.%20MIM-Refiner%20is%20motivated%20by%20the%20insight%20that%0Astrong%20representations%20within%20MIM%20models%20generally%20reside%20in%20intermediate%0Alayers.%20Accordingly%2C%20MIM-Refiner%20leverages%20multiple%20contrastive%20heads%20that%20are%0Aconnected%20to%20different%20intermediate%20layers.%20In%20each%20head%2C%20a%20modified%20nearest%0Aneighbor%20objective%20constructs%20semantic%20clusters%20that%20capture%20semantic%0Ainformation%20which%20improves%20performance%20on%20downstream%20tasks%2C%20including%0Aoff-the-shelf%20and%20fine-tuning%20settings.%0A%20%20The%20refinement%20process%20is%20short%20and%20simple%20-%20yet%20highly%20effective.%20Within%20a%0Afew%20epochs%2C%20we%20refine%20the%20features%20of%20MIM%20models%20from%20subpar%20to%0Astate-of-the-art%2C%20off-the-shelf%20features.%20Refining%20a%20ViT-H%2C%20pre-trained%20with%0Adata2vec%202.0%20on%20ImageNet-1K%2C%20sets%20a%20new%20state-of-the-art%20in%20linear%20probing%0A%2884.7%25%29%20and%20low-shot%20classification%20among%20models%20that%20are%20pre-trained%20on%0AImageNet-1K.%20At%20ImageNet-1K%201-shot%20classification%2C%20MIM-Refiner%20advances%20the%0Astate-of-the-art%20to%2064.2%25%2C%20outperforming%20larger%20models%20that%20were%20trained%20on%20up%0Ato%202000%20times%20more%20data%20such%20as%20DINOv2-g%2C%20OpenCLIP-G%20and%20MAWS-6.5B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIM-Refiner%253A%2520A%2520Contrastive%2520Learning%2520Boost%2520from%2520Intermediate%2520Pre-Trained%250A%2520%2520Representations%26entry.906535625%3DBenedikt%2520Alkin%2520and%2520Lukas%2520Miklautz%2520and%2520Sepp%2520Hochreiter%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520We%2520introduce%2520MIM%2520%2528Masked%2520Image%2520Modeling%2529-Refiner%252C%2520a%2520contrastive%2520learning%250Aboost%2520for%2520pre-trained%2520MIM%2520models.%2520MIM-Refiner%2520is%2520motivated%2520by%2520the%2520insight%2520that%250Astrong%2520representations%2520within%2520MIM%2520models%2520generally%2520reside%2520in%2520intermediate%250Alayers.%2520Accordingly%252C%2520MIM-Refiner%2520leverages%2520multiple%2520contrastive%2520heads%2520that%2520are%250Aconnected%2520to%2520different%2520intermediate%2520layers.%2520In%2520each%2520head%252C%2520a%2520modified%2520nearest%250Aneighbor%2520objective%2520constructs%2520semantic%2520clusters%2520that%2520capture%2520semantic%250Ainformation%2520which%2520improves%2520performance%2520on%2520downstream%2520tasks%252C%2520including%250Aoff-the-shelf%2520and%2520fine-tuning%2520settings.%250A%2520%2520The%2520refinement%2520process%2520is%2520short%2520and%2520simple%2520-%2520yet%2520highly%2520effective.%2520Within%2520a%250Afew%2520epochs%252C%2520we%2520refine%2520the%2520features%2520of%2520MIM%2520models%2520from%2520subpar%2520to%250Astate-of-the-art%252C%2520off-the-shelf%2520features.%2520Refining%2520a%2520ViT-H%252C%2520pre-trained%2520with%250Adata2vec%25202.0%2520on%2520ImageNet-1K%252C%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520linear%2520probing%250A%252884.7%2525%2529%2520and%2520low-shot%2520classification%2520among%2520models%2520that%2520are%2520pre-trained%2520on%250AImageNet-1K.%2520At%2520ImageNet-1K%25201-shot%2520classification%252C%2520MIM-Refiner%2520advances%2520the%250Astate-of-the-art%2520to%252064.2%2525%252C%2520outperforming%2520larger%2520models%2520that%2520were%2520trained%2520on%2520up%250Ato%25202000%2520times%2520more%2520data%2520such%2520as%2520DINOv2-g%252C%2520OpenCLIP-G%2520and%2520MAWS-6.5B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIM-Refiner%3A%20A%20Contrastive%20Learning%20Boost%20from%20Intermediate%20Pre-Trained%0A%20%20Representations&entry.906535625=Benedikt%20Alkin%20and%20Lukas%20Miklautz%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20We%20introduce%20MIM%20%28Masked%20Image%20Modeling%29-Refiner%2C%20a%20contrastive%20learning%0Aboost%20for%20pre-trained%20MIM%20models.%20MIM-Refiner%20is%20motivated%20by%20the%20insight%20that%0Astrong%20representations%20within%20MIM%20models%20generally%20reside%20in%20intermediate%0Alayers.%20Accordingly%2C%20MIM-Refiner%20leverages%20multiple%20contrastive%20heads%20that%20are%0Aconnected%20to%20different%20intermediate%20layers.%20In%20each%20head%2C%20a%20modified%20nearest%0Aneighbor%20objective%20constructs%20semantic%20clusters%20that%20capture%20semantic%0Ainformation%20which%20improves%20performance%20on%20downstream%20tasks%2C%20including%0Aoff-the-shelf%20and%20fine-tuning%20settings.%0A%20%20The%20refinement%20process%20is%20short%20and%20simple%20-%20yet%20highly%20effective.%20Within%20a%0Afew%20epochs%2C%20we%20refine%20the%20features%20of%20MIM%20models%20from%20subpar%20to%0Astate-of-the-art%2C%20off-the-shelf%20features.%20Refining%20a%20ViT-H%2C%20pre-trained%20with%0Adata2vec%202.0%20on%20ImageNet-1K%2C%20sets%20a%20new%20state-of-the-art%20in%20linear%20probing%0A%2884.7%25%29%20and%20low-shot%20classification%20among%20models%20that%20are%20pre-trained%20on%0AImageNet-1K.%20At%20ImageNet-1K%201-shot%20classification%2C%20MIM-Refiner%20advances%20the%0Astate-of-the-art%20to%2064.2%25%2C%20outperforming%20larger%20models%20that%20were%20trained%20on%20up%0Ato%202000%20times%20more%20data%20such%20as%20DINOv2-g%2C%20OpenCLIP-G%20and%20MAWS-6.5B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10093v2&entry.124074799=Read"},
{"title": "Iterative Motion Editing with Natural Language", "author": "Purvi Goel and Kuan-Chieh Wang and C. Karen Liu and Kayvon Fatahalian", "abstract": "  Text-to-motion diffusion models can generate realistic animations from text\nprompts, but do not support fine-grained motion editing controls. In this\npaper, we present a method for using natural language to iteratively specify\nlocal edits to existing character animations, a task that is common in most\ncomputer animation workflows. Our key idea is to represent a space of motion\nedits using a set of kinematic motion editing operators (MEOs) whose effects on\nthe source motion is well-aligned with user expectations. We provide an\nalgorithm that leverages pre-existing language models to translate textual\ndescriptions of motion edits into source code for programs that define and\nexecute sequences of MEOs on a source animation. We execute MEOs by first\ntranslating them into keyframe constraints, and then use diffusion-based motion\nmodels to generate output motions that respect these constraints. Through a\nuser study and quantitative evaluation, we demonstrate that our system can\nperform motion edits that respect the animator's editing intent, remain\nfaithful to the original animation (it edits the original animation, but does\nnot dramatically change it), and yield realistic character animation results.\n", "link": "http://arxiv.org/abs/2312.11538v2", "date": "2024-06-03", "relevancy": 1.6183, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6183}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5249}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Motion%20Editing%20with%20Natural%20Language&body=Title%3A%20Iterative%20Motion%20Editing%20with%20Natural%20Language%0AAuthor%3A%20Purvi%20Goel%20and%20Kuan-Chieh%20Wang%20and%20C.%20Karen%20Liu%20and%20Kayvon%20Fatahalian%0AAbstract%3A%20%20%20Text-to-motion%20diffusion%20models%20can%20generate%20realistic%20animations%20from%20text%0Aprompts%2C%20but%20do%20not%20support%20fine-grained%20motion%20editing%20controls.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20using%20natural%20language%20to%20iteratively%20specify%0Alocal%20edits%20to%20existing%20character%20animations%2C%20a%20task%20that%20is%20common%20in%20most%0Acomputer%20animation%20workflows.%20Our%20key%20idea%20is%20to%20represent%20a%20space%20of%20motion%0Aedits%20using%20a%20set%20of%20kinematic%20motion%20editing%20operators%20%28MEOs%29%20whose%20effects%20on%0Athe%20source%20motion%20is%20well-aligned%20with%20user%20expectations.%20We%20provide%20an%0Aalgorithm%20that%20leverages%20pre-existing%20language%20models%20to%20translate%20textual%0Adescriptions%20of%20motion%20edits%20into%20source%20code%20for%20programs%20that%20define%20and%0Aexecute%20sequences%20of%20MEOs%20on%20a%20source%20animation.%20We%20execute%20MEOs%20by%20first%0Atranslating%20them%20into%20keyframe%20constraints%2C%20and%20then%20use%20diffusion-based%20motion%0Amodels%20to%20generate%20output%20motions%20that%20respect%20these%20constraints.%20Through%20a%0Auser%20study%20and%20quantitative%20evaluation%2C%20we%20demonstrate%20that%20our%20system%20can%0Aperform%20motion%20edits%20that%20respect%20the%20animator%27s%20editing%20intent%2C%20remain%0Afaithful%20to%20the%20original%20animation%20%28it%20edits%20the%20original%20animation%2C%20but%20does%0Anot%20dramatically%20change%20it%29%2C%20and%20yield%20realistic%20character%20animation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Motion%2520Editing%2520with%2520Natural%2520Language%26entry.906535625%3DPurvi%2520Goel%2520and%2520Kuan-Chieh%2520Wang%2520and%2520C.%2520Karen%2520Liu%2520and%2520Kayvon%2520Fatahalian%26entry.1292438233%3D%2520%2520Text-to-motion%2520diffusion%2520models%2520can%2520generate%2520realistic%2520animations%2520from%2520text%250Aprompts%252C%2520but%2520do%2520not%2520support%2520fine-grained%2520motion%2520editing%2520controls.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520method%2520for%2520using%2520natural%2520language%2520to%2520iteratively%2520specify%250Alocal%2520edits%2520to%2520existing%2520character%2520animations%252C%2520a%2520task%2520that%2520is%2520common%2520in%2520most%250Acomputer%2520animation%2520workflows.%2520Our%2520key%2520idea%2520is%2520to%2520represent%2520a%2520space%2520of%2520motion%250Aedits%2520using%2520a%2520set%2520of%2520kinematic%2520motion%2520editing%2520operators%2520%2528MEOs%2529%2520whose%2520effects%2520on%250Athe%2520source%2520motion%2520is%2520well-aligned%2520with%2520user%2520expectations.%2520We%2520provide%2520an%250Aalgorithm%2520that%2520leverages%2520pre-existing%2520language%2520models%2520to%2520translate%2520textual%250Adescriptions%2520of%2520motion%2520edits%2520into%2520source%2520code%2520for%2520programs%2520that%2520define%2520and%250Aexecute%2520sequences%2520of%2520MEOs%2520on%2520a%2520source%2520animation.%2520We%2520execute%2520MEOs%2520by%2520first%250Atranslating%2520them%2520into%2520keyframe%2520constraints%252C%2520and%2520then%2520use%2520diffusion-based%2520motion%250Amodels%2520to%2520generate%2520output%2520motions%2520that%2520respect%2520these%2520constraints.%2520Through%2520a%250Auser%2520study%2520and%2520quantitative%2520evaluation%252C%2520we%2520demonstrate%2520that%2520our%2520system%2520can%250Aperform%2520motion%2520edits%2520that%2520respect%2520the%2520animator%2527s%2520editing%2520intent%252C%2520remain%250Afaithful%2520to%2520the%2520original%2520animation%2520%2528it%2520edits%2520the%2520original%2520animation%252C%2520but%2520does%250Anot%2520dramatically%2520change%2520it%2529%252C%2520and%2520yield%2520realistic%2520character%2520animation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Motion%20Editing%20with%20Natural%20Language&entry.906535625=Purvi%20Goel%20and%20Kuan-Chieh%20Wang%20and%20C.%20Karen%20Liu%20and%20Kayvon%20Fatahalian&entry.1292438233=%20%20Text-to-motion%20diffusion%20models%20can%20generate%20realistic%20animations%20from%20text%0Aprompts%2C%20but%20do%20not%20support%20fine-grained%20motion%20editing%20controls.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20using%20natural%20language%20to%20iteratively%20specify%0Alocal%20edits%20to%20existing%20character%20animations%2C%20a%20task%20that%20is%20common%20in%20most%0Acomputer%20animation%20workflows.%20Our%20key%20idea%20is%20to%20represent%20a%20space%20of%20motion%0Aedits%20using%20a%20set%20of%20kinematic%20motion%20editing%20operators%20%28MEOs%29%20whose%20effects%20on%0Athe%20source%20motion%20is%20well-aligned%20with%20user%20expectations.%20We%20provide%20an%0Aalgorithm%20that%20leverages%20pre-existing%20language%20models%20to%20translate%20textual%0Adescriptions%20of%20motion%20edits%20into%20source%20code%20for%20programs%20that%20define%20and%0Aexecute%20sequences%20of%20MEOs%20on%20a%20source%20animation.%20We%20execute%20MEOs%20by%20first%0Atranslating%20them%20into%20keyframe%20constraints%2C%20and%20then%20use%20diffusion-based%20motion%0Amodels%20to%20generate%20output%20motions%20that%20respect%20these%20constraints.%20Through%20a%0Auser%20study%20and%20quantitative%20evaluation%2C%20we%20demonstrate%20that%20our%20system%20can%0Aperform%20motion%20edits%20that%20respect%20the%20animator%27s%20editing%20intent%2C%20remain%0Afaithful%20to%20the%20original%20animation%20%28it%20edits%20the%20original%20animation%2C%20but%20does%0Anot%20dramatically%20change%20it%29%2C%20and%20yield%20realistic%20character%20animation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11538v2&entry.124074799=Read"},
{"title": "SpeechAct: Towards Generating Whole-body Motion from Speech", "author": "Jinsong Zhang and Minjie Zhu and Yuxiang Zhang and Yebin Liu and Kun Li", "abstract": "  This paper addresses the problem of generating whole-body motion from speech.\nDespite great successes, prior methods still struggle to produce reasonable and\ndiverse whole-body motions from speech. This is due to their reliance on\nsuboptimal representations and a lack of strategies for generating diverse\nresults. To address these challenges, we present a novel hybrid point\nrepresentation to achieve accurate and continuous motion generation, e.g.,\navoiding foot skating, and this representation can be transformed into an\neasy-to-use representation, i.e., SMPL-X body mesh, for many applications. To\ngenerate whole-body motion from speech, for facial motion, closely tied to the\naudio signal, we introduce an encoder-decoder architecture to achieve\ndeterministic outcomes. However, for the body and hands, which have weaker\nconnections to the audio signal, we aim to generate diverse yet reasonable\nmotions. To boost diversity in motion generation, we propose a contrastive\nmotion learning method to encourage the model to produce more distinctive\nrepresentations. Specifically, we design a robust VQ-VAE to learn a quantized\nmotion codebook using our hybrid representation. Then, we regress the motion\nrepresentation from the audio signal by a translation model employing our\ncontrastive motion learning method. Experimental results validate the superior\nperformance and the correctness of our model. The project page is available for\nresearch purposes at http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.\n", "link": "http://arxiv.org/abs/2311.17425v3", "date": "2024-06-03", "relevancy": 1.6149, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpeechAct%3A%20Towards%20Generating%20Whole-body%20Motion%20from%20Speech&body=Title%3A%20SpeechAct%3A%20Towards%20Generating%20Whole-body%20Motion%20from%20Speech%0AAuthor%3A%20Jinsong%20Zhang%20and%20Minjie%20Zhu%20and%20Yuxiang%20Zhang%20and%20Yebin%20Liu%20and%20Kun%20Li%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20generating%20whole-body%20motion%20from%20speech.%0ADespite%20great%20successes%2C%20prior%20methods%20still%20struggle%20to%20produce%20reasonable%20and%0Adiverse%20whole-body%20motions%20from%20speech.%20This%20is%20due%20to%20their%20reliance%20on%0Asuboptimal%20representations%20and%20a%20lack%20of%20strategies%20for%20generating%20diverse%0Aresults.%20To%20address%20these%20challenges%2C%20we%20present%20a%20novel%20hybrid%20point%0Arepresentation%20to%20achieve%20accurate%20and%20continuous%20motion%20generation%2C%20e.g.%2C%0Aavoiding%20foot%20skating%2C%20and%20this%20representation%20can%20be%20transformed%20into%20an%0Aeasy-to-use%20representation%2C%20i.e.%2C%20SMPL-X%20body%20mesh%2C%20for%20many%20applications.%20To%0Agenerate%20whole-body%20motion%20from%20speech%2C%20for%20facial%20motion%2C%20closely%20tied%20to%20the%0Aaudio%20signal%2C%20we%20introduce%20an%20encoder-decoder%20architecture%20to%20achieve%0Adeterministic%20outcomes.%20However%2C%20for%20the%20body%20and%20hands%2C%20which%20have%20weaker%0Aconnections%20to%20the%20audio%20signal%2C%20we%20aim%20to%20generate%20diverse%20yet%20reasonable%0Amotions.%20To%20boost%20diversity%20in%20motion%20generation%2C%20we%20propose%20a%20contrastive%0Amotion%20learning%20method%20to%20encourage%20the%20model%20to%20produce%20more%20distinctive%0Arepresentations.%20Specifically%2C%20we%20design%20a%20robust%20VQ-VAE%20to%20learn%20a%20quantized%0Amotion%20codebook%20using%20our%20hybrid%20representation.%20Then%2C%20we%20regress%20the%20motion%0Arepresentation%20from%20the%20audio%20signal%20by%20a%20translation%20model%20employing%20our%0Acontrastive%20motion%20learning%20method.%20Experimental%20results%20validate%20the%20superior%0Aperformance%20and%20the%20correctness%20of%20our%20model.%20The%20project%20page%20is%20available%20for%0Aresearch%20purposes%20at%20http%3A//cic.tju.edu.cn/faculty/likun/projects/SpeechAct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17425v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeechAct%253A%2520Towards%2520Generating%2520Whole-body%2520Motion%2520from%2520Speech%26entry.906535625%3DJinsong%2520Zhang%2520and%2520Minjie%2520Zhu%2520and%2520Yuxiang%2520Zhang%2520and%2520Yebin%2520Liu%2520and%2520Kun%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520generating%2520whole-body%2520motion%2520from%2520speech.%250ADespite%2520great%2520successes%252C%2520prior%2520methods%2520still%2520struggle%2520to%2520produce%2520reasonable%2520and%250Adiverse%2520whole-body%2520motions%2520from%2520speech.%2520This%2520is%2520due%2520to%2520their%2520reliance%2520on%250Asuboptimal%2520representations%2520and%2520a%2520lack%2520of%2520strategies%2520for%2520generating%2520diverse%250Aresults.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520a%2520novel%2520hybrid%2520point%250Arepresentation%2520to%2520achieve%2520accurate%2520and%2520continuous%2520motion%2520generation%252C%2520e.g.%252C%250Aavoiding%2520foot%2520skating%252C%2520and%2520this%2520representation%2520can%2520be%2520transformed%2520into%2520an%250Aeasy-to-use%2520representation%252C%2520i.e.%252C%2520SMPL-X%2520body%2520mesh%252C%2520for%2520many%2520applications.%2520To%250Agenerate%2520whole-body%2520motion%2520from%2520speech%252C%2520for%2520facial%2520motion%252C%2520closely%2520tied%2520to%2520the%250Aaudio%2520signal%252C%2520we%2520introduce%2520an%2520encoder-decoder%2520architecture%2520to%2520achieve%250Adeterministic%2520outcomes.%2520However%252C%2520for%2520the%2520body%2520and%2520hands%252C%2520which%2520have%2520weaker%250Aconnections%2520to%2520the%2520audio%2520signal%252C%2520we%2520aim%2520to%2520generate%2520diverse%2520yet%2520reasonable%250Amotions.%2520To%2520boost%2520diversity%2520in%2520motion%2520generation%252C%2520we%2520propose%2520a%2520contrastive%250Amotion%2520learning%2520method%2520to%2520encourage%2520the%2520model%2520to%2520produce%2520more%2520distinctive%250Arepresentations.%2520Specifically%252C%2520we%2520design%2520a%2520robust%2520VQ-VAE%2520to%2520learn%2520a%2520quantized%250Amotion%2520codebook%2520using%2520our%2520hybrid%2520representation.%2520Then%252C%2520we%2520regress%2520the%2520motion%250Arepresentation%2520from%2520the%2520audio%2520signal%2520by%2520a%2520translation%2520model%2520employing%2520our%250Acontrastive%2520motion%2520learning%2520method.%2520Experimental%2520results%2520validate%2520the%2520superior%250Aperformance%2520and%2520the%2520correctness%2520of%2520our%2520model.%2520The%2520project%2520page%2520is%2520available%2520for%250Aresearch%2520purposes%2520at%2520http%253A//cic.tju.edu.cn/faculty/likun/projects/SpeechAct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17425v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpeechAct%3A%20Towards%20Generating%20Whole-body%20Motion%20from%20Speech&entry.906535625=Jinsong%20Zhang%20and%20Minjie%20Zhu%20and%20Yuxiang%20Zhang%20and%20Yebin%20Liu%20and%20Kun%20Li&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20generating%20whole-body%20motion%20from%20speech.%0ADespite%20great%20successes%2C%20prior%20methods%20still%20struggle%20to%20produce%20reasonable%20and%0Adiverse%20whole-body%20motions%20from%20speech.%20This%20is%20due%20to%20their%20reliance%20on%0Asuboptimal%20representations%20and%20a%20lack%20of%20strategies%20for%20generating%20diverse%0Aresults.%20To%20address%20these%20challenges%2C%20we%20present%20a%20novel%20hybrid%20point%0Arepresentation%20to%20achieve%20accurate%20and%20continuous%20motion%20generation%2C%20e.g.%2C%0Aavoiding%20foot%20skating%2C%20and%20this%20representation%20can%20be%20transformed%20into%20an%0Aeasy-to-use%20representation%2C%20i.e.%2C%20SMPL-X%20body%20mesh%2C%20for%20many%20applications.%20To%0Agenerate%20whole-body%20motion%20from%20speech%2C%20for%20facial%20motion%2C%20closely%20tied%20to%20the%0Aaudio%20signal%2C%20we%20introduce%20an%20encoder-decoder%20architecture%20to%20achieve%0Adeterministic%20outcomes.%20However%2C%20for%20the%20body%20and%20hands%2C%20which%20have%20weaker%0Aconnections%20to%20the%20audio%20signal%2C%20we%20aim%20to%20generate%20diverse%20yet%20reasonable%0Amotions.%20To%20boost%20diversity%20in%20motion%20generation%2C%20we%20propose%20a%20contrastive%0Amotion%20learning%20method%20to%20encourage%20the%20model%20to%20produce%20more%20distinctive%0Arepresentations.%20Specifically%2C%20we%20design%20a%20robust%20VQ-VAE%20to%20learn%20a%20quantized%0Amotion%20codebook%20using%20our%20hybrid%20representation.%20Then%2C%20we%20regress%20the%20motion%0Arepresentation%20from%20the%20audio%20signal%20by%20a%20translation%20model%20employing%20our%0Acontrastive%20motion%20learning%20method.%20Experimental%20results%20validate%20the%20superior%0Aperformance%20and%20the%20correctness%20of%20our%20model.%20The%20project%20page%20is%20available%20for%0Aresearch%20purposes%20at%20http%3A//cic.tju.edu.cn/faculty/likun/projects/SpeechAct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17425v3&entry.124074799=Read"},
{"title": "Diffusion Model-Augmented Behavioral Cloning", "author": "Shang-Fu Chen and Hsiang-Chun Wang and Ming-Hao Hsu and Chun-Mao Lai and Shao-Hua Sun", "abstract": "  Imitation learning addresses the challenge of learning by observing an\nexpert's demonstrations without access to reward signals from environments.\nMost existing imitation learning methods that do not require interacting with\nenvironments either model the expert distribution as the conditional\nprobability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s,\na). Despite the simplicity of modeling the conditional probability with BC, it\nusually struggles with generalization. While modeling the joint probability can\nimprove generalization performance, the inference procedure is often\ntime-consuming, and the model can suffer from manifold overfitting. This work\nproposes an imitation learning framework that benefits from modeling both the\nconditional and joint probability of the expert distribution. Our proposed\nDiffusion Model-Augmented Behavioral Cloning (DBC) employs a diffusion model\ntrained to model expert behaviors and learns a policy to optimize both the BC\nloss (conditional) and our proposed diffusion model loss (joint). DBC\noutperforms baselines in various continuous control tasks in navigation, robot\narm manipulation, dexterous manipulation, and locomotion. We design additional\nexperiments to verify the limitations of modeling either the conditional\nprobability or the joint probability of the expert distribution, as well as\ncompare different generative models. Ablation studies justify the effectiveness\nof our design choices.\n", "link": "http://arxiv.org/abs/2302.13335v4", "date": "2024-06-03", "relevancy": 1.5968, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Model-Augmented%20Behavioral%20Cloning&body=Title%3A%20Diffusion%20Model-Augmented%20Behavioral%20Cloning%0AAuthor%3A%20Shang-Fu%20Chen%20and%20Hsiang-Chun%20Wang%20and%20Ming-Hao%20Hsu%20and%20Chun-Mao%20Lai%20and%20Shao-Hua%20Sun%0AAbstract%3A%20%20%20Imitation%20learning%20addresses%20the%20challenge%20of%20learning%20by%20observing%20an%0Aexpert%27s%20demonstrations%20without%20access%20to%20reward%20signals%20from%20environments.%0AMost%20existing%20imitation%20learning%20methods%20that%20do%20not%20require%20interacting%20with%0Aenvironments%20either%20model%20the%20expert%20distribution%20as%20the%20conditional%0Aprobability%20p%28a%7Cs%29%20%28e.g.%2C%20behavioral%20cloning%2C%20BC%29%20or%20the%20joint%20probability%20p%28s%2C%0Aa%29.%20Despite%20the%20simplicity%20of%20modeling%20the%20conditional%20probability%20with%20BC%2C%20it%0Ausually%20struggles%20with%20generalization.%20While%20modeling%20the%20joint%20probability%20can%0Aimprove%20generalization%20performance%2C%20the%20inference%20procedure%20is%20often%0Atime-consuming%2C%20and%20the%20model%20can%20suffer%20from%20manifold%20overfitting.%20This%20work%0Aproposes%20an%20imitation%20learning%20framework%20that%20benefits%20from%20modeling%20both%20the%0Aconditional%20and%20joint%20probability%20of%20the%20expert%20distribution.%20Our%20proposed%0ADiffusion%20Model-Augmented%20Behavioral%20Cloning%20%28DBC%29%20employs%20a%20diffusion%20model%0Atrained%20to%20model%20expert%20behaviors%20and%20learns%20a%20policy%20to%20optimize%20both%20the%20BC%0Aloss%20%28conditional%29%20and%20our%20proposed%20diffusion%20model%20loss%20%28joint%29.%20DBC%0Aoutperforms%20baselines%20in%20various%20continuous%20control%20tasks%20in%20navigation%2C%20robot%0Aarm%20manipulation%2C%20dexterous%20manipulation%2C%20and%20locomotion.%20We%20design%20additional%0Aexperiments%20to%20verify%20the%20limitations%20of%20modeling%20either%20the%20conditional%0Aprobability%20or%20the%20joint%20probability%20of%20the%20expert%20distribution%2C%20as%20well%20as%0Acompare%20different%20generative%20models.%20Ablation%20studies%20justify%20the%20effectiveness%0Aof%20our%20design%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13335v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Model-Augmented%2520Behavioral%2520Cloning%26entry.906535625%3DShang-Fu%2520Chen%2520and%2520Hsiang-Chun%2520Wang%2520and%2520Ming-Hao%2520Hsu%2520and%2520Chun-Mao%2520Lai%2520and%2520Shao-Hua%2520Sun%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520addresses%2520the%2520challenge%2520of%2520learning%2520by%2520observing%2520an%250Aexpert%2527s%2520demonstrations%2520without%2520access%2520to%2520reward%2520signals%2520from%2520environments.%250AMost%2520existing%2520imitation%2520learning%2520methods%2520that%2520do%2520not%2520require%2520interacting%2520with%250Aenvironments%2520either%2520model%2520the%2520expert%2520distribution%2520as%2520the%2520conditional%250Aprobability%2520p%2528a%257Cs%2529%2520%2528e.g.%252C%2520behavioral%2520cloning%252C%2520BC%2529%2520or%2520the%2520joint%2520probability%2520p%2528s%252C%250Aa%2529.%2520Despite%2520the%2520simplicity%2520of%2520modeling%2520the%2520conditional%2520probability%2520with%2520BC%252C%2520it%250Ausually%2520struggles%2520with%2520generalization.%2520While%2520modeling%2520the%2520joint%2520probability%2520can%250Aimprove%2520generalization%2520performance%252C%2520the%2520inference%2520procedure%2520is%2520often%250Atime-consuming%252C%2520and%2520the%2520model%2520can%2520suffer%2520from%2520manifold%2520overfitting.%2520This%2520work%250Aproposes%2520an%2520imitation%2520learning%2520framework%2520that%2520benefits%2520from%2520modeling%2520both%2520the%250Aconditional%2520and%2520joint%2520probability%2520of%2520the%2520expert%2520distribution.%2520Our%2520proposed%250ADiffusion%2520Model-Augmented%2520Behavioral%2520Cloning%2520%2528DBC%2529%2520employs%2520a%2520diffusion%2520model%250Atrained%2520to%2520model%2520expert%2520behaviors%2520and%2520learns%2520a%2520policy%2520to%2520optimize%2520both%2520the%2520BC%250Aloss%2520%2528conditional%2529%2520and%2520our%2520proposed%2520diffusion%2520model%2520loss%2520%2528joint%2529.%2520DBC%250Aoutperforms%2520baselines%2520in%2520various%2520continuous%2520control%2520tasks%2520in%2520navigation%252C%2520robot%250Aarm%2520manipulation%252C%2520dexterous%2520manipulation%252C%2520and%2520locomotion.%2520We%2520design%2520additional%250Aexperiments%2520to%2520verify%2520the%2520limitations%2520of%2520modeling%2520either%2520the%2520conditional%250Aprobability%2520or%2520the%2520joint%2520probability%2520of%2520the%2520expert%2520distribution%252C%2520as%2520well%2520as%250Acompare%2520different%2520generative%2520models.%2520Ablation%2520studies%2520justify%2520the%2520effectiveness%250Aof%2520our%2520design%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13335v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Model-Augmented%20Behavioral%20Cloning&entry.906535625=Shang-Fu%20Chen%20and%20Hsiang-Chun%20Wang%20and%20Ming-Hao%20Hsu%20and%20Chun-Mao%20Lai%20and%20Shao-Hua%20Sun&entry.1292438233=%20%20Imitation%20learning%20addresses%20the%20challenge%20of%20learning%20by%20observing%20an%0Aexpert%27s%20demonstrations%20without%20access%20to%20reward%20signals%20from%20environments.%0AMost%20existing%20imitation%20learning%20methods%20that%20do%20not%20require%20interacting%20with%0Aenvironments%20either%20model%20the%20expert%20distribution%20as%20the%20conditional%0Aprobability%20p%28a%7Cs%29%20%28e.g.%2C%20behavioral%20cloning%2C%20BC%29%20or%20the%20joint%20probability%20p%28s%2C%0Aa%29.%20Despite%20the%20simplicity%20of%20modeling%20the%20conditional%20probability%20with%20BC%2C%20it%0Ausually%20struggles%20with%20generalization.%20While%20modeling%20the%20joint%20probability%20can%0Aimprove%20generalization%20performance%2C%20the%20inference%20procedure%20is%20often%0Atime-consuming%2C%20and%20the%20model%20can%20suffer%20from%20manifold%20overfitting.%20This%20work%0Aproposes%20an%20imitation%20learning%20framework%20that%20benefits%20from%20modeling%20both%20the%0Aconditional%20and%20joint%20probability%20of%20the%20expert%20distribution.%20Our%20proposed%0ADiffusion%20Model-Augmented%20Behavioral%20Cloning%20%28DBC%29%20employs%20a%20diffusion%20model%0Atrained%20to%20model%20expert%20behaviors%20and%20learns%20a%20policy%20to%20optimize%20both%20the%20BC%0Aloss%20%28conditional%29%20and%20our%20proposed%20diffusion%20model%20loss%20%28joint%29.%20DBC%0Aoutperforms%20baselines%20in%20various%20continuous%20control%20tasks%20in%20navigation%2C%20robot%0Aarm%20manipulation%2C%20dexterous%20manipulation%2C%20and%20locomotion.%20We%20design%20additional%0Aexperiments%20to%20verify%20the%20limitations%20of%20modeling%20either%20the%20conditional%0Aprobability%20or%20the%20joint%20probability%20of%20the%20expert%20distribution%2C%20as%20well%20as%0Acompare%20different%20generative%20models.%20Ablation%20studies%20justify%20the%20effectiveness%0Aof%20our%20design%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13335v4&entry.124074799=Read"},
{"title": "Significance of Chain of Thought in Gender Bias Mitigation for\n  English-Dravidian Machine Translation", "author": "Lavanya Prahallad and Radhika Mamidi", "abstract": "  Gender bias in machine translation (MT) sys- tems poses a significant\nchallenge to achieving accurate and inclusive translations. This paper examines\ngender bias in machine translation systems for languages such as Telugu and\nKan- nada from the Dravidian family, analyzing how gender inflections affect\ntranslation accuracy and neutrality using Google Translate and Chat- GPT. It\nfinds that while plural forms can reduce bias, individual-centric sentences\noften main- tain the bias due to historical stereotypes. The study evaluates\nthe Chain of Thought process- ing, noting significant bias mitigation from 80%\nto 4% in Telugu and from 40% to 0% in Kan- nada. It also compares Telugu and\nKannada translations, emphasizing the need for language specific strategies to\naddress these challenges and suggesting directions for future research to\nenhance fairness in both data preparation and prompts during inference.\n", "link": "http://arxiv.org/abs/2405.19701v2", "date": "2024-06-03", "relevancy": 1.5967, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Significance%20of%20Chain%20of%20Thought%20in%20Gender%20Bias%20Mitigation%20for%0A%20%20English-Dravidian%20Machine%20Translation&body=Title%3A%20Significance%20of%20Chain%20of%20Thought%20in%20Gender%20Bias%20Mitigation%20for%0A%20%20English-Dravidian%20Machine%20Translation%0AAuthor%3A%20Lavanya%20Prahallad%20and%20Radhika%20Mamidi%0AAbstract%3A%20%20%20Gender%20bias%20in%20machine%20translation%20%28MT%29%20sys-%20tems%20poses%20a%20significant%0Achallenge%20to%20achieving%20accurate%20and%20inclusive%20translations.%20This%20paper%20examines%0Agender%20bias%20in%20machine%20translation%20systems%20for%20languages%20such%20as%20Telugu%20and%0AKan-%20nada%20from%20the%20Dravidian%20family%2C%20analyzing%20how%20gender%20inflections%20affect%0Atranslation%20accuracy%20and%20neutrality%20using%20Google%20Translate%20and%20Chat-%20GPT.%20It%0Afinds%20that%20while%20plural%20forms%20can%20reduce%20bias%2C%20individual-centric%20sentences%0Aoften%20main-%20tain%20the%20bias%20due%20to%20historical%20stereotypes.%20The%20study%20evaluates%0Athe%20Chain%20of%20Thought%20process-%20ing%2C%20noting%20significant%20bias%20mitigation%20from%2080%25%0Ato%204%25%20in%20Telugu%20and%20from%2040%25%20to%200%25%20in%20Kan-%20nada.%20It%20also%20compares%20Telugu%20and%0AKannada%20translations%2C%20emphasizing%20the%20need%20for%20language%20specific%20strategies%20to%0Aaddress%20these%20challenges%20and%20suggesting%20directions%20for%20future%20research%20to%0Aenhance%20fairness%20in%20both%20data%20preparation%20and%20prompts%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignificance%2520of%2520Chain%2520of%2520Thought%2520in%2520Gender%2520Bias%2520Mitigation%2520for%250A%2520%2520English-Dravidian%2520Machine%2520Translation%26entry.906535625%3DLavanya%2520Prahallad%2520and%2520Radhika%2520Mamidi%26entry.1292438233%3D%2520%2520Gender%2520bias%2520in%2520machine%2520translation%2520%2528MT%2529%2520sys-%2520tems%2520poses%2520a%2520significant%250Achallenge%2520to%2520achieving%2520accurate%2520and%2520inclusive%2520translations.%2520This%2520paper%2520examines%250Agender%2520bias%2520in%2520machine%2520translation%2520systems%2520for%2520languages%2520such%2520as%2520Telugu%2520and%250AKan-%2520nada%2520from%2520the%2520Dravidian%2520family%252C%2520analyzing%2520how%2520gender%2520inflections%2520affect%250Atranslation%2520accuracy%2520and%2520neutrality%2520using%2520Google%2520Translate%2520and%2520Chat-%2520GPT.%2520It%250Afinds%2520that%2520while%2520plural%2520forms%2520can%2520reduce%2520bias%252C%2520individual-centric%2520sentences%250Aoften%2520main-%2520tain%2520the%2520bias%2520due%2520to%2520historical%2520stereotypes.%2520The%2520study%2520evaluates%250Athe%2520Chain%2520of%2520Thought%2520process-%2520ing%252C%2520noting%2520significant%2520bias%2520mitigation%2520from%252080%2525%250Ato%25204%2525%2520in%2520Telugu%2520and%2520from%252040%2525%2520to%25200%2525%2520in%2520Kan-%2520nada.%2520It%2520also%2520compares%2520Telugu%2520and%250AKannada%2520translations%252C%2520emphasizing%2520the%2520need%2520for%2520language%2520specific%2520strategies%2520to%250Aaddress%2520these%2520challenges%2520and%2520suggesting%2520directions%2520for%2520future%2520research%2520to%250Aenhance%2520fairness%2520in%2520both%2520data%2520preparation%2520and%2520prompts%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Significance%20of%20Chain%20of%20Thought%20in%20Gender%20Bias%20Mitigation%20for%0A%20%20English-Dravidian%20Machine%20Translation&entry.906535625=Lavanya%20Prahallad%20and%20Radhika%20Mamidi&entry.1292438233=%20%20Gender%20bias%20in%20machine%20translation%20%28MT%29%20sys-%20tems%20poses%20a%20significant%0Achallenge%20to%20achieving%20accurate%20and%20inclusive%20translations.%20This%20paper%20examines%0Agender%20bias%20in%20machine%20translation%20systems%20for%20languages%20such%20as%20Telugu%20and%0AKan-%20nada%20from%20the%20Dravidian%20family%2C%20analyzing%20how%20gender%20inflections%20affect%0Atranslation%20accuracy%20and%20neutrality%20using%20Google%20Translate%20and%20Chat-%20GPT.%20It%0Afinds%20that%20while%20plural%20forms%20can%20reduce%20bias%2C%20individual-centric%20sentences%0Aoften%20main-%20tain%20the%20bias%20due%20to%20historical%20stereotypes.%20The%20study%20evaluates%0Athe%20Chain%20of%20Thought%20process-%20ing%2C%20noting%20significant%20bias%20mitigation%20from%2080%25%0Ato%204%25%20in%20Telugu%20and%20from%2040%25%20to%200%25%20in%20Kan-%20nada.%20It%20also%20compares%20Telugu%20and%0AKannada%20translations%2C%20emphasizing%20the%20need%20for%20language%20specific%20strategies%20to%0Aaddress%20these%20challenges%20and%20suggesting%20directions%20for%20future%20research%20to%0Aenhance%20fairness%20in%20both%20data%20preparation%20and%20prompts%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19701v2&entry.124074799=Read"},
{"title": "Confidence Under the Hood: An Investigation into the\n  Confidence-Probability Alignment in Large Language Models", "author": "Abhishek Kumar and Robert Morabito and Sanzhar Umbet and Jad Kabbara and Ali Emami", "abstract": "  As the use of Large Language Models (LLMs) becomes more widespread,\nunderstanding their self-evaluation of confidence in generated responses\nbecomes increasingly important as it is integral to the reliability of the\noutput of these models. We introduce the concept of Confidence-Probability\nAlignment, that connects an LLM's internal confidence, quantified by token\nprobabilities, to the confidence conveyed in the model's response when\nexplicitly asked about its certainty. Using various datasets and prompting\ntechniques that encourage model introspection, we probe the alignment between\nmodels' internal and expressed confidence. These techniques encompass using\nstructured evaluation scales to rate confidence, including answer options when\nprompting, and eliciting the model's confidence level for outputs it does not\nrecognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed\nthe strongest confidence-probability alignment, with an average Spearman's\n$\\hat{\\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the\nongoing efforts to facilitate risk assessment in the application of LLMs and to\nfurther our understanding of model trustworthiness.\n", "link": "http://arxiv.org/abs/2405.16282v3", "date": "2024-06-03", "relevancy": 1.585, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5401}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Under%20the%20Hood%3A%20An%20Investigation%20into%20the%0A%20%20Confidence-Probability%20Alignment%20in%20Large%20Language%20Models&body=Title%3A%20Confidence%20Under%20the%20Hood%3A%20An%20Investigation%20into%20the%0A%20%20Confidence-Probability%20Alignment%20in%20Large%20Language%20Models%0AAuthor%3A%20Abhishek%20Kumar%20and%20Robert%20Morabito%20and%20Sanzhar%20Umbet%20and%20Jad%20Kabbara%20and%20Ali%20Emami%0AAbstract%3A%20%20%20As%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20becomes%20more%20widespread%2C%0Aunderstanding%20their%20self-evaluation%20of%20confidence%20in%20generated%20responses%0Abecomes%20increasingly%20important%20as%20it%20is%20integral%20to%20the%20reliability%20of%20the%0Aoutput%20of%20these%20models.%20We%20introduce%20the%20concept%20of%20Confidence-Probability%0AAlignment%2C%20that%20connects%20an%20LLM%27s%20internal%20confidence%2C%20quantified%20by%20token%0Aprobabilities%2C%20to%20the%20confidence%20conveyed%20in%20the%20model%27s%20response%20when%0Aexplicitly%20asked%20about%20its%20certainty.%20Using%20various%20datasets%20and%20prompting%0Atechniques%20that%20encourage%20model%20introspection%2C%20we%20probe%20the%20alignment%20between%0Amodels%27%20internal%20and%20expressed%20confidence.%20These%20techniques%20encompass%20using%0Astructured%20evaluation%20scales%20to%20rate%20confidence%2C%20including%20answer%20options%20when%0Aprompting%2C%20and%20eliciting%20the%20model%27s%20confidence%20level%20for%20outputs%20it%20does%20not%0Arecognize%20as%20its%20own.%20Notably%2C%20among%20the%20models%20analyzed%2C%20OpenAI%27s%20GPT-4%20showed%0Athe%20strongest%20confidence-probability%20alignment%2C%20with%20an%20average%20Spearman%27s%0A%24%5Chat%7B%5Crho%7D%24%20of%200.42%2C%20across%20a%20wide%20range%20of%20tasks.%20Our%20work%20contributes%20to%20the%0Aongoing%20efforts%20to%20facilitate%20risk%20assessment%20in%20the%20application%20of%20LLMs%20and%20to%0Afurther%20our%20understanding%20of%20model%20trustworthiness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16282v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Under%2520the%2520Hood%253A%2520An%2520Investigation%2520into%2520the%250A%2520%2520Confidence-Probability%2520Alignment%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAbhishek%2520Kumar%2520and%2520Robert%2520Morabito%2520and%2520Sanzhar%2520Umbet%2520and%2520Jad%2520Kabbara%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520As%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520becomes%2520more%2520widespread%252C%250Aunderstanding%2520their%2520self-evaluation%2520of%2520confidence%2520in%2520generated%2520responses%250Abecomes%2520increasingly%2520important%2520as%2520it%2520is%2520integral%2520to%2520the%2520reliability%2520of%2520the%250Aoutput%2520of%2520these%2520models.%2520We%2520introduce%2520the%2520concept%2520of%2520Confidence-Probability%250AAlignment%252C%2520that%2520connects%2520an%2520LLM%2527s%2520internal%2520confidence%252C%2520quantified%2520by%2520token%250Aprobabilities%252C%2520to%2520the%2520confidence%2520conveyed%2520in%2520the%2520model%2527s%2520response%2520when%250Aexplicitly%2520asked%2520about%2520its%2520certainty.%2520Using%2520various%2520datasets%2520and%2520prompting%250Atechniques%2520that%2520encourage%2520model%2520introspection%252C%2520we%2520probe%2520the%2520alignment%2520between%250Amodels%2527%2520internal%2520and%2520expressed%2520confidence.%2520These%2520techniques%2520encompass%2520using%250Astructured%2520evaluation%2520scales%2520to%2520rate%2520confidence%252C%2520including%2520answer%2520options%2520when%250Aprompting%252C%2520and%2520eliciting%2520the%2520model%2527s%2520confidence%2520level%2520for%2520outputs%2520it%2520does%2520not%250Arecognize%2520as%2520its%2520own.%2520Notably%252C%2520among%2520the%2520models%2520analyzed%252C%2520OpenAI%2527s%2520GPT-4%2520showed%250Athe%2520strongest%2520confidence-probability%2520alignment%252C%2520with%2520an%2520average%2520Spearman%2527s%250A%2524%255Chat%257B%255Crho%257D%2524%2520of%25200.42%252C%2520across%2520a%2520wide%2520range%2520of%2520tasks.%2520Our%2520work%2520contributes%2520to%2520the%250Aongoing%2520efforts%2520to%2520facilitate%2520risk%2520assessment%2520in%2520the%2520application%2520of%2520LLMs%2520and%2520to%250Afurther%2520our%2520understanding%2520of%2520model%2520trustworthiness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16282v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Under%20the%20Hood%3A%20An%20Investigation%20into%20the%0A%20%20Confidence-Probability%20Alignment%20in%20Large%20Language%20Models&entry.906535625=Abhishek%20Kumar%20and%20Robert%20Morabito%20and%20Sanzhar%20Umbet%20and%20Jad%20Kabbara%20and%20Ali%20Emami&entry.1292438233=%20%20As%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20becomes%20more%20widespread%2C%0Aunderstanding%20their%20self-evaluation%20of%20confidence%20in%20generated%20responses%0Abecomes%20increasingly%20important%20as%20it%20is%20integral%20to%20the%20reliability%20of%20the%0Aoutput%20of%20these%20models.%20We%20introduce%20the%20concept%20of%20Confidence-Probability%0AAlignment%2C%20that%20connects%20an%20LLM%27s%20internal%20confidence%2C%20quantified%20by%20token%0Aprobabilities%2C%20to%20the%20confidence%20conveyed%20in%20the%20model%27s%20response%20when%0Aexplicitly%20asked%20about%20its%20certainty.%20Using%20various%20datasets%20and%20prompting%0Atechniques%20that%20encourage%20model%20introspection%2C%20we%20probe%20the%20alignment%20between%0Amodels%27%20internal%20and%20expressed%20confidence.%20These%20techniques%20encompass%20using%0Astructured%20evaluation%20scales%20to%20rate%20confidence%2C%20including%20answer%20options%20when%0Aprompting%2C%20and%20eliciting%20the%20model%27s%20confidence%20level%20for%20outputs%20it%20does%20not%0Arecognize%20as%20its%20own.%20Notably%2C%20among%20the%20models%20analyzed%2C%20OpenAI%27s%20GPT-4%20showed%0Athe%20strongest%20confidence-probability%20alignment%2C%20with%20an%20average%20Spearman%27s%0A%24%5Chat%7B%5Crho%7D%24%20of%200.42%2C%20across%20a%20wide%20range%20of%20tasks.%20Our%20work%20contributes%20to%20the%0Aongoing%20efforts%20to%20facilitate%20risk%20assessment%20in%20the%20application%20of%20LLMs%20and%20to%0Afurther%20our%20understanding%20of%20model%20trustworthiness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16282v3&entry.124074799=Read"},
{"title": "Multistep Consistency Models", "author": "Jonathan Heek and Emiel Hoogeboom and Tim Salimans", "abstract": "  Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas a $\\infty$-step consistency\nmodel is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation, using simple\nlosses without adversarial training. We also show that our method scales to a\ntext-to-image diffusion model, generating samples that are close to the quality\nof the original model.\n", "link": "http://arxiv.org/abs/2403.06807v2", "date": "2024-06-03", "relevancy": 1.5745, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5579}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5225}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multistep%20Consistency%20Models&body=Title%3A%20Multistep%20Consistency%20Models%0AAuthor%3A%20Jonathan%20Heek%20and%20Emiel%20Hoogeboom%20and%20Tim%20Salimans%0AAbstract%3A%20%20%20Diffusion%20models%20are%20relatively%20easy%20to%20train%20but%20require%20many%20steps%20to%0Agenerate%20samples.%20Consistency%20models%20are%20far%20more%20difficult%20to%20train%2C%20but%0Agenerate%20samples%20in%20a%20single%20step.%0A%20%20In%20this%20paper%20we%20propose%20Multistep%20Consistency%20Models%3A%20A%20unification%20between%0AConsistency%20Models%20%28Song%20et%20al.%2C%202023%29%20and%20TRACT%20%28Berthelot%20et%20al.%2C%202023%29%20that%0Acan%20interpolate%20between%20a%20consistency%20model%20and%20a%20diffusion%20model%3A%20a%20trade-off%0Abetween%20sampling%20speed%20and%20sampling%20quality.%20Specifically%2C%20a%201-step%20consistency%0Amodel%20is%20a%20conventional%20consistency%20model%20whereas%20a%20%24%5Cinfty%24-step%20consistency%0Amodel%20is%20a%20diffusion%20model.%0A%20%20Multistep%20Consistency%20Models%20work%20really%20well%20in%20practice.%20By%20increasing%20the%0Asample%20budget%20from%20a%20single%20step%20to%202-8%20steps%2C%20we%20can%20train%20models%20more%20easily%0Athat%20generate%20higher%20quality%20samples%2C%20while%20retaining%20much%20of%20the%20sampling%0Aspeed%20benefits.%20Notable%20results%20are%201.4%20FID%20on%20Imagenet%2064%20in%208%20step%20and%202.1%0AFID%20on%20Imagenet128%20in%208%20steps%20with%20consistency%20distillation%2C%20using%20simple%0Alosses%20without%20adversarial%20training.%20We%20also%20show%20that%20our%20method%20scales%20to%20a%0Atext-to-image%20diffusion%20model%2C%20generating%20samples%20that%20are%20close%20to%20the%20quality%0Aof%20the%20original%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultistep%2520Consistency%2520Models%26entry.906535625%3DJonathan%2520Heek%2520and%2520Emiel%2520Hoogeboom%2520and%2520Tim%2520Salimans%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520are%2520relatively%2520easy%2520to%2520train%2520but%2520require%2520many%2520steps%2520to%250Agenerate%2520samples.%2520Consistency%2520models%2520are%2520far%2520more%2520difficult%2520to%2520train%252C%2520but%250Agenerate%2520samples%2520in%2520a%2520single%2520step.%250A%2520%2520In%2520this%2520paper%2520we%2520propose%2520Multistep%2520Consistency%2520Models%253A%2520A%2520unification%2520between%250AConsistency%2520Models%2520%2528Song%2520et%2520al.%252C%25202023%2529%2520and%2520TRACT%2520%2528Berthelot%2520et%2520al.%252C%25202023%2529%2520that%250Acan%2520interpolate%2520between%2520a%2520consistency%2520model%2520and%2520a%2520diffusion%2520model%253A%2520a%2520trade-off%250Abetween%2520sampling%2520speed%2520and%2520sampling%2520quality.%2520Specifically%252C%2520a%25201-step%2520consistency%250Amodel%2520is%2520a%2520conventional%2520consistency%2520model%2520whereas%2520a%2520%2524%255Cinfty%2524-step%2520consistency%250Amodel%2520is%2520a%2520diffusion%2520model.%250A%2520%2520Multistep%2520Consistency%2520Models%2520work%2520really%2520well%2520in%2520practice.%2520By%2520increasing%2520the%250Asample%2520budget%2520from%2520a%2520single%2520step%2520to%25202-8%2520steps%252C%2520we%2520can%2520train%2520models%2520more%2520easily%250Athat%2520generate%2520higher%2520quality%2520samples%252C%2520while%2520retaining%2520much%2520of%2520the%2520sampling%250Aspeed%2520benefits.%2520Notable%2520results%2520are%25201.4%2520FID%2520on%2520Imagenet%252064%2520in%25208%2520step%2520and%25202.1%250AFID%2520on%2520Imagenet128%2520in%25208%2520steps%2520with%2520consistency%2520distillation%252C%2520using%2520simple%250Alosses%2520without%2520adversarial%2520training.%2520We%2520also%2520show%2520that%2520our%2520method%2520scales%2520to%2520a%250Atext-to-image%2520diffusion%2520model%252C%2520generating%2520samples%2520that%2520are%2520close%2520to%2520the%2520quality%250Aof%2520the%2520original%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multistep%20Consistency%20Models&entry.906535625=Jonathan%20Heek%20and%20Emiel%20Hoogeboom%20and%20Tim%20Salimans&entry.1292438233=%20%20Diffusion%20models%20are%20relatively%20easy%20to%20train%20but%20require%20many%20steps%20to%0Agenerate%20samples.%20Consistency%20models%20are%20far%20more%20difficult%20to%20train%2C%20but%0Agenerate%20samples%20in%20a%20single%20step.%0A%20%20In%20this%20paper%20we%20propose%20Multistep%20Consistency%20Models%3A%20A%20unification%20between%0AConsistency%20Models%20%28Song%20et%20al.%2C%202023%29%20and%20TRACT%20%28Berthelot%20et%20al.%2C%202023%29%20that%0Acan%20interpolate%20between%20a%20consistency%20model%20and%20a%20diffusion%20model%3A%20a%20trade-off%0Abetween%20sampling%20speed%20and%20sampling%20quality.%20Specifically%2C%20a%201-step%20consistency%0Amodel%20is%20a%20conventional%20consistency%20model%20whereas%20a%20%24%5Cinfty%24-step%20consistency%0Amodel%20is%20a%20diffusion%20model.%0A%20%20Multistep%20Consistency%20Models%20work%20really%20well%20in%20practice.%20By%20increasing%20the%0Asample%20budget%20from%20a%20single%20step%20to%202-8%20steps%2C%20we%20can%20train%20models%20more%20easily%0Athat%20generate%20higher%20quality%20samples%2C%20while%20retaining%20much%20of%20the%20sampling%0Aspeed%20benefits.%20Notable%20results%20are%201.4%20FID%20on%20Imagenet%2064%20in%208%20step%20and%202.1%0AFID%20on%20Imagenet128%20in%208%20steps%20with%20consistency%20distillation%2C%20using%20simple%0Alosses%20without%20adversarial%20training.%20We%20also%20show%20that%20our%20method%20scales%20to%20a%0Atext-to-image%20diffusion%20model%2C%20generating%20samples%20that%20are%20close%20to%20the%20quality%0Aof%20the%20original%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06807v2&entry.124074799=Read"},
{"title": "Generate and Pray: Using SALLMS to Evaluate the Security of LLM\n  Generated Code", "author": "Mohammed Latif Siddiq and Joanna C. S. Santos and Sajith Devareddy and Anna Muller", "abstract": "  With the growing popularity of Large Language Models (LLMs) in software\nengineers' daily practices, it is important to ensure that the code generated\nby these tools is not only functionally correct but also free of\nvulnerabilities. Although LLMs can help developers to be more productive, prior\nempirical studies have shown that LLMs can generate insecure code. There are\ntwo contributing factors to the insecure code generation. First, existing\ndatasets used to evaluate LLMs do not adequately represent genuine software\nengineering tasks sensitive to security. Instead, they are often based on\ncompetitive programming challenges or classroom-type coding tasks. In\nreal-world applications, the code produced is integrated into larger codebases,\nintroducing potential security risks. Second, existing evaluation metrics\nprimarily focus on the functional correctness of the generated code while\nignoring security considerations. Therefore, in this paper, we described SALLM,\na framework to benchmark LLMs' abilities to generate secure code\nsystematically. This framework has three major components: a novel dataset of\nsecurity-centric Python prompts, configurable assessment techniques to evaluate\nthe generated code, and novel metrics to evaluate the models' performance from\nthe perspective of secure code generation.\n", "link": "http://arxiv.org/abs/2311.00889v2", "date": "2024-06-03", "relevancy": 1.5587, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4062}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%20and%20Pray%3A%20Using%20SALLMS%20to%20Evaluate%20the%20Security%20of%20LLM%0A%20%20Generated%20Code&body=Title%3A%20Generate%20and%20Pray%3A%20Using%20SALLMS%20to%20Evaluate%20the%20Security%20of%20LLM%0A%20%20Generated%20Code%0AAuthor%3A%20Mohammed%20Latif%20Siddiq%20and%20Joanna%20C.%20S.%20Santos%20and%20Sajith%20Devareddy%20and%20Anna%20Muller%0AAbstract%3A%20%20%20With%20the%20growing%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20software%0Aengineers%27%20daily%20practices%2C%20it%20is%20important%20to%20ensure%20that%20the%20code%20generated%0Aby%20these%20tools%20is%20not%20only%20functionally%20correct%20but%20also%20free%20of%0Avulnerabilities.%20Although%20LLMs%20can%20help%20developers%20to%20be%20more%20productive%2C%20prior%0Aempirical%20studies%20have%20shown%20that%20LLMs%20can%20generate%20insecure%20code.%20There%20are%0Atwo%20contributing%20factors%20to%20the%20insecure%20code%20generation.%20First%2C%20existing%0Adatasets%20used%20to%20evaluate%20LLMs%20do%20not%20adequately%20represent%20genuine%20software%0Aengineering%20tasks%20sensitive%20to%20security.%20Instead%2C%20they%20are%20often%20based%20on%0Acompetitive%20programming%20challenges%20or%20classroom-type%20coding%20tasks.%20In%0Areal-world%20applications%2C%20the%20code%20produced%20is%20integrated%20into%20larger%20codebases%2C%0Aintroducing%20potential%20security%20risks.%20Second%2C%20existing%20evaluation%20metrics%0Aprimarily%20focus%20on%20the%20functional%20correctness%20of%20the%20generated%20code%20while%0Aignoring%20security%20considerations.%20Therefore%2C%20in%20this%20paper%2C%20we%20described%20SALLM%2C%0Aa%20framework%20to%20benchmark%20LLMs%27%20abilities%20to%20generate%20secure%20code%0Asystematically.%20This%20framework%20has%20three%20major%20components%3A%20a%20novel%20dataset%20of%0Asecurity-centric%20Python%20prompts%2C%20configurable%20assessment%20techniques%20to%20evaluate%0Athe%20generated%20code%2C%20and%20novel%20metrics%20to%20evaluate%20the%20models%27%20performance%20from%0Athe%20perspective%20of%20secure%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%2520and%2520Pray%253A%2520Using%2520SALLMS%2520to%2520Evaluate%2520the%2520Security%2520of%2520LLM%250A%2520%2520Generated%2520Code%26entry.906535625%3DMohammed%2520Latif%2520Siddiq%2520and%2520Joanna%2520C.%2520S.%2520Santos%2520and%2520Sajith%2520Devareddy%2520and%2520Anna%2520Muller%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520popularity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520software%250Aengineers%2527%2520daily%2520practices%252C%2520it%2520is%2520important%2520to%2520ensure%2520that%2520the%2520code%2520generated%250Aby%2520these%2520tools%2520is%2520not%2520only%2520functionally%2520correct%2520but%2520also%2520free%2520of%250Avulnerabilities.%2520Although%2520LLMs%2520can%2520help%2520developers%2520to%2520be%2520more%2520productive%252C%2520prior%250Aempirical%2520studies%2520have%2520shown%2520that%2520LLMs%2520can%2520generate%2520insecure%2520code.%2520There%2520are%250Atwo%2520contributing%2520factors%2520to%2520the%2520insecure%2520code%2520generation.%2520First%252C%2520existing%250Adatasets%2520used%2520to%2520evaluate%2520LLMs%2520do%2520not%2520adequately%2520represent%2520genuine%2520software%250Aengineering%2520tasks%2520sensitive%2520to%2520security.%2520Instead%252C%2520they%2520are%2520often%2520based%2520on%250Acompetitive%2520programming%2520challenges%2520or%2520classroom-type%2520coding%2520tasks.%2520In%250Areal-world%2520applications%252C%2520the%2520code%2520produced%2520is%2520integrated%2520into%2520larger%2520codebases%252C%250Aintroducing%2520potential%2520security%2520risks.%2520Second%252C%2520existing%2520evaluation%2520metrics%250Aprimarily%2520focus%2520on%2520the%2520functional%2520correctness%2520of%2520the%2520generated%2520code%2520while%250Aignoring%2520security%2520considerations.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520described%2520SALLM%252C%250Aa%2520framework%2520to%2520benchmark%2520LLMs%2527%2520abilities%2520to%2520generate%2520secure%2520code%250Asystematically.%2520This%2520framework%2520has%2520three%2520major%2520components%253A%2520a%2520novel%2520dataset%2520of%250Asecurity-centric%2520Python%2520prompts%252C%2520configurable%2520assessment%2520techniques%2520to%2520evaluate%250Athe%2520generated%2520code%252C%2520and%2520novel%2520metrics%2520to%2520evaluate%2520the%2520models%2527%2520performance%2520from%250Athe%2520perspective%2520of%2520secure%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%20and%20Pray%3A%20Using%20SALLMS%20to%20Evaluate%20the%20Security%20of%20LLM%0A%20%20Generated%20Code&entry.906535625=Mohammed%20Latif%20Siddiq%20and%20Joanna%20C.%20S.%20Santos%20and%20Sajith%20Devareddy%20and%20Anna%20Muller&entry.1292438233=%20%20With%20the%20growing%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20software%0Aengineers%27%20daily%20practices%2C%20it%20is%20important%20to%20ensure%20that%20the%20code%20generated%0Aby%20these%20tools%20is%20not%20only%20functionally%20correct%20but%20also%20free%20of%0Avulnerabilities.%20Although%20LLMs%20can%20help%20developers%20to%20be%20more%20productive%2C%20prior%0Aempirical%20studies%20have%20shown%20that%20LLMs%20can%20generate%20insecure%20code.%20There%20are%0Atwo%20contributing%20factors%20to%20the%20insecure%20code%20generation.%20First%2C%20existing%0Adatasets%20used%20to%20evaluate%20LLMs%20do%20not%20adequately%20represent%20genuine%20software%0Aengineering%20tasks%20sensitive%20to%20security.%20Instead%2C%20they%20are%20often%20based%20on%0Acompetitive%20programming%20challenges%20or%20classroom-type%20coding%20tasks.%20In%0Areal-world%20applications%2C%20the%20code%20produced%20is%20integrated%20into%20larger%20codebases%2C%0Aintroducing%20potential%20security%20risks.%20Second%2C%20existing%20evaluation%20metrics%0Aprimarily%20focus%20on%20the%20functional%20correctness%20of%20the%20generated%20code%20while%0Aignoring%20security%20considerations.%20Therefore%2C%20in%20this%20paper%2C%20we%20described%20SALLM%2C%0Aa%20framework%20to%20benchmark%20LLMs%27%20abilities%20to%20generate%20secure%20code%0Asystematically.%20This%20framework%20has%20three%20major%20components%3A%20a%20novel%20dataset%20of%0Asecurity-centric%20Python%20prompts%2C%20configurable%20assessment%20techniques%20to%20evaluate%0Athe%20generated%20code%2C%20and%20novel%20metrics%20to%20evaluate%20the%20models%27%20performance%20from%0Athe%20perspective%20of%20secure%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00889v2&entry.124074799=Read"},
{"title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare", "author": "Fatemeh Nazary and Yashar Deldjoo and Tommaso Di Noia and Eugenio di Sciascio", "abstract": "  The integration of Large Language Models (LLMs) into healthcare diagnostics\noffers a promising avenue for clinical decision-making. This study outlines the\ndevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)\nby integrating medical domain knowledge using a multi-layered structured\nprompt. We also explore the efficacy of two communication styles between the\nuser and LLMs: the Numerical Conversational (NC) style, which processes data\nincrementally, and the Natural Language Single-Turn (NL-ST) style, which\nemploys long narrative prompts.\n  Our study systematically evaluates the diagnostic accuracy and risk factors,\nincluding gender bias and false negative rates, using a dataset of 920 patient\nrecords in various few-shot scenarios. Results indicate that traditional\nclinical machine learning (ML) models generally outperform LLMs in zero-shot\nand few-shot settings. However, the performance gap narrows significantly when\nemploying few-shot examples alongside effective explainable AI (XAI) methods as\nsources of domain knowledge. Moreover, with sufficient time and an increased\nnumber of examples, the conversational style (NC) nearly matches the\nperformance of ML models. Most notably, LLMs demonstrate comparable or superior\ncost-sensitive accuracy relative to ML models.\n  This research confirms that, with appropriate domain knowledge and tailored\ncommunication strategies, LLMs can significantly enhance diagnostic processes.\nThe findings highlight the importance of optimizing the number of training\nexamples and communication styles to improve accuracy and reduce biases in LLM\napplications.\n", "link": "http://arxiv.org/abs/2405.06270v3", "date": "2024-06-03", "relevancy": 1.5508, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XAI4LLM.%20Let%20Machine%20Learning%20Models%20and%20LLMs%20Collaborate%20for%20Enhanced%0A%20%20In-Context%20Learning%20in%20Healthcare&body=Title%3A%20XAI4LLM.%20Let%20Machine%20Learning%20Models%20and%20LLMs%20Collaborate%20for%20Enhanced%0A%20%20In-Context%20Learning%20in%20Healthcare%0AAuthor%3A%20Fatemeh%20Nazary%20and%20Yashar%20Deldjoo%20and%20Tommaso%20Di%20Noia%20and%20Eugenio%20di%20Sciascio%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20healthcare%20diagnostics%0Aoffers%20a%20promising%20avenue%20for%20clinical%20decision-making.%20This%20study%20outlines%20the%0Adevelopment%20of%20a%20novel%20method%20for%20zero-shot/few-shot%20in-context%20learning%20%28ICL%29%0Aby%20integrating%20medical%20domain%20knowledge%20using%20a%20multi-layered%20structured%0Aprompt.%20We%20also%20explore%20the%20efficacy%20of%20two%20communication%20styles%20between%20the%0Auser%20and%20LLMs%3A%20the%20Numerical%20Conversational%20%28NC%29%20style%2C%20which%20processes%20data%0Aincrementally%2C%20and%20the%20Natural%20Language%20Single-Turn%20%28NL-ST%29%20style%2C%20which%0Aemploys%20long%20narrative%20prompts.%0A%20%20Our%20study%20systematically%20evaluates%20the%20diagnostic%20accuracy%20and%20risk%20factors%2C%0Aincluding%20gender%20bias%20and%20false%20negative%20rates%2C%20using%20a%20dataset%20of%20920%20patient%0Arecords%20in%20various%20few-shot%20scenarios.%20Results%20indicate%20that%20traditional%0Aclinical%20machine%20learning%20%28ML%29%20models%20generally%20outperform%20LLMs%20in%20zero-shot%0Aand%20few-shot%20settings.%20However%2C%20the%20performance%20gap%20narrows%20significantly%20when%0Aemploying%20few-shot%20examples%20alongside%20effective%20explainable%20AI%20%28XAI%29%20methods%20as%0Asources%20of%20domain%20knowledge.%20Moreover%2C%20with%20sufficient%20time%20and%20an%20increased%0Anumber%20of%20examples%2C%20the%20conversational%20style%20%28NC%29%20nearly%20matches%20the%0Aperformance%20of%20ML%20models.%20Most%20notably%2C%20LLMs%20demonstrate%20comparable%20or%20superior%0Acost-sensitive%20accuracy%20relative%20to%20ML%20models.%0A%20%20This%20research%20confirms%20that%2C%20with%20appropriate%20domain%20knowledge%20and%20tailored%0Acommunication%20strategies%2C%20LLMs%20can%20significantly%20enhance%20diagnostic%20processes.%0AThe%20findings%20highlight%20the%20importance%20of%20optimizing%20the%20number%20of%20training%0Aexamples%20and%20communication%20styles%20to%20improve%20accuracy%20and%20reduce%20biases%20in%20LLM%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06270v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXAI4LLM.%2520Let%2520Machine%2520Learning%2520Models%2520and%2520LLMs%2520Collaborate%2520for%2520Enhanced%250A%2520%2520In-Context%2520Learning%2520in%2520Healthcare%26entry.906535625%3DFatemeh%2520Nazary%2520and%2520Yashar%2520Deldjoo%2520and%2520Tommaso%2520Di%2520Noia%2520and%2520Eugenio%2520di%2520Sciascio%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520healthcare%2520diagnostics%250Aoffers%2520a%2520promising%2520avenue%2520for%2520clinical%2520decision-making.%2520This%2520study%2520outlines%2520the%250Adevelopment%2520of%2520a%2520novel%2520method%2520for%2520zero-shot/few-shot%2520in-context%2520learning%2520%2528ICL%2529%250Aby%2520integrating%2520medical%2520domain%2520knowledge%2520using%2520a%2520multi-layered%2520structured%250Aprompt.%2520We%2520also%2520explore%2520the%2520efficacy%2520of%2520two%2520communication%2520styles%2520between%2520the%250Auser%2520and%2520LLMs%253A%2520the%2520Numerical%2520Conversational%2520%2528NC%2529%2520style%252C%2520which%2520processes%2520data%250Aincrementally%252C%2520and%2520the%2520Natural%2520Language%2520Single-Turn%2520%2528NL-ST%2529%2520style%252C%2520which%250Aemploys%2520long%2520narrative%2520prompts.%250A%2520%2520Our%2520study%2520systematically%2520evaluates%2520the%2520diagnostic%2520accuracy%2520and%2520risk%2520factors%252C%250Aincluding%2520gender%2520bias%2520and%2520false%2520negative%2520rates%252C%2520using%2520a%2520dataset%2520of%2520920%2520patient%250Arecords%2520in%2520various%2520few-shot%2520scenarios.%2520Results%2520indicate%2520that%2520traditional%250Aclinical%2520machine%2520learning%2520%2528ML%2529%2520models%2520generally%2520outperform%2520LLMs%2520in%2520zero-shot%250Aand%2520few-shot%2520settings.%2520However%252C%2520the%2520performance%2520gap%2520narrows%2520significantly%2520when%250Aemploying%2520few-shot%2520examples%2520alongside%2520effective%2520explainable%2520AI%2520%2528XAI%2529%2520methods%2520as%250Asources%2520of%2520domain%2520knowledge.%2520Moreover%252C%2520with%2520sufficient%2520time%2520and%2520an%2520increased%250Anumber%2520of%2520examples%252C%2520the%2520conversational%2520style%2520%2528NC%2529%2520nearly%2520matches%2520the%250Aperformance%2520of%2520ML%2520models.%2520Most%2520notably%252C%2520LLMs%2520demonstrate%2520comparable%2520or%2520superior%250Acost-sensitive%2520accuracy%2520relative%2520to%2520ML%2520models.%250A%2520%2520This%2520research%2520confirms%2520that%252C%2520with%2520appropriate%2520domain%2520knowledge%2520and%2520tailored%250Acommunication%2520strategies%252C%2520LLMs%2520can%2520significantly%2520enhance%2520diagnostic%2520processes.%250AThe%2520findings%2520highlight%2520the%2520importance%2520of%2520optimizing%2520the%2520number%2520of%2520training%250Aexamples%2520and%2520communication%2520styles%2520to%2520improve%2520accuracy%2520and%2520reduce%2520biases%2520in%2520LLM%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06270v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XAI4LLM.%20Let%20Machine%20Learning%20Models%20and%20LLMs%20Collaborate%20for%20Enhanced%0A%20%20In-Context%20Learning%20in%20Healthcare&entry.906535625=Fatemeh%20Nazary%20and%20Yashar%20Deldjoo%20and%20Tommaso%20Di%20Noia%20and%20Eugenio%20di%20Sciascio&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20healthcare%20diagnostics%0Aoffers%20a%20promising%20avenue%20for%20clinical%20decision-making.%20This%20study%20outlines%20the%0Adevelopment%20of%20a%20novel%20method%20for%20zero-shot/few-shot%20in-context%20learning%20%28ICL%29%0Aby%20integrating%20medical%20domain%20knowledge%20using%20a%20multi-layered%20structured%0Aprompt.%20We%20also%20explore%20the%20efficacy%20of%20two%20communication%20styles%20between%20the%0Auser%20and%20LLMs%3A%20the%20Numerical%20Conversational%20%28NC%29%20style%2C%20which%20processes%20data%0Aincrementally%2C%20and%20the%20Natural%20Language%20Single-Turn%20%28NL-ST%29%20style%2C%20which%0Aemploys%20long%20narrative%20prompts.%0A%20%20Our%20study%20systematically%20evaluates%20the%20diagnostic%20accuracy%20and%20risk%20factors%2C%0Aincluding%20gender%20bias%20and%20false%20negative%20rates%2C%20using%20a%20dataset%20of%20920%20patient%0Arecords%20in%20various%20few-shot%20scenarios.%20Results%20indicate%20that%20traditional%0Aclinical%20machine%20learning%20%28ML%29%20models%20generally%20outperform%20LLMs%20in%20zero-shot%0Aand%20few-shot%20settings.%20However%2C%20the%20performance%20gap%20narrows%20significantly%20when%0Aemploying%20few-shot%20examples%20alongside%20effective%20explainable%20AI%20%28XAI%29%20methods%20as%0Asources%20of%20domain%20knowledge.%20Moreover%2C%20with%20sufficient%20time%20and%20an%20increased%0Anumber%20of%20examples%2C%20the%20conversational%20style%20%28NC%29%20nearly%20matches%20the%0Aperformance%20of%20ML%20models.%20Most%20notably%2C%20LLMs%20demonstrate%20comparable%20or%20superior%0Acost-sensitive%20accuracy%20relative%20to%20ML%20models.%0A%20%20This%20research%20confirms%20that%2C%20with%20appropriate%20domain%20knowledge%20and%20tailored%0Acommunication%20strategies%2C%20LLMs%20can%20significantly%20enhance%20diagnostic%20processes.%0AThe%20findings%20highlight%20the%20importance%20of%20optimizing%20the%20number%20of%20training%0Aexamples%20and%20communication%20styles%20to%20improve%20accuracy%20and%20reduce%20biases%20in%20LLM%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06270v3&entry.124074799=Read"},
{"title": "Large Language Models are Zero-Shot Next Location Predictors", "author": "Ciro Beneduce and Bruno Lepri and Massimiliano Luca", "abstract": "  Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution among many others. The models designed to tackle next-location\nprediction, however, require a significant amount of individual-level\ninformation to be trained effectively. Such data may be scarce or even\nunavailable in some geographic regions or peculiar scenarios (e.g., cold-start\nin recommendation systems). Moreover, the design of a next-location predictor\nable to generalize or geographically transfer knowledge is still an open\nresearch challenge. Recent advances in natural language processing have led to\na rapid diffusion of Large Language Models (LLMs) which have shown good\ngeneralization and reasoning capabilities. These insights, coupled with the\nrecent findings that LLMs are rich in geographical knowledge, allowed us to\nbelieve that these models can act as zero-shot next-location predictors. This\npaper evaluates the capabilities of many popular LLMs in this role,\nspecifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we\ntested the models on three real-world mobility datasets. The results show that\nLLMs can obtain accuracies up to 32.4%, a significant relative improvement of\nover 600% when compared to sophisticated DL models specifically designed for\nhuman mobility. Moreover, we show that other LLMs are unable to perform the\ntask properly. To prevent positively biased results, we also propose a\nframework inspired by other studies to test data contamination. Finally, we\nexplored the possibility of using LLMs as text-based explainers for\nnext-location prediction showing that can effectively provide an explanation\nfor their decision. Notably, 7B models provide more generic, but still\nreliable, explanations compared to larger counterparts. Code:\ngithub.com/ssai-trento/LLM-zero-shot-NL\n", "link": "http://arxiv.org/abs/2405.20962v2", "date": "2024-06-03", "relevancy": 1.5495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5288}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20are%20Zero-Shot%20Next%20Location%20Predictors&body=Title%3A%20Large%20Language%20Models%20are%20Zero-Shot%20Next%20Location%20Predictors%0AAuthor%3A%20Ciro%20Beneduce%20and%20Bruno%20Lepri%20and%20Massimiliano%20Luca%0AAbstract%3A%20%20%20Predicting%20the%20locations%20an%20individual%20will%20visit%20in%20the%20future%20is%20crucial%0Afor%20solving%20many%20societal%20issues%20like%20disease%20diffusion%20and%20reduction%20of%0Apollution%20among%20many%20others.%20The%20models%20designed%20to%20tackle%20next-location%0Aprediction%2C%20however%2C%20require%20a%20significant%20amount%20of%20individual-level%0Ainformation%20to%20be%20trained%20effectively.%20Such%20data%20may%20be%20scarce%20or%20even%0Aunavailable%20in%20some%20geographic%20regions%20or%20peculiar%20scenarios%20%28e.g.%2C%20cold-start%0Ain%20recommendation%20systems%29.%20Moreover%2C%20the%20design%20of%20a%20next-location%20predictor%0Aable%20to%20generalize%20or%20geographically%20transfer%20knowledge%20is%20still%20an%20open%0Aresearch%20challenge.%20Recent%20advances%20in%20natural%20language%20processing%20have%20led%20to%0Aa%20rapid%20diffusion%20of%20Large%20Language%20Models%20%28LLMs%29%20which%20have%20shown%20good%0Ageneralization%20and%20reasoning%20capabilities.%20These%20insights%2C%20coupled%20with%20the%0Arecent%20findings%20that%20LLMs%20are%20rich%20in%20geographical%20knowledge%2C%20allowed%20us%20to%0Abelieve%20that%20these%20models%20can%20act%20as%20zero-shot%20next-location%20predictors.%20This%0Apaper%20evaluates%20the%20capabilities%20of%20many%20popular%20LLMs%20in%20this%20role%2C%0Aspecifically%20Llama%2C%20GPT-3.5%20and%20Mistral%207B.%20After%20designing%20a%20proper%20prompt%2C%20we%0Atested%20the%20models%20on%20three%20real-world%20mobility%20datasets.%20The%20results%20show%20that%0ALLMs%20can%20obtain%20accuracies%20up%20to%2032.4%25%2C%20a%20significant%20relative%20improvement%20of%0Aover%20600%25%20when%20compared%20to%20sophisticated%20DL%20models%20specifically%20designed%20for%0Ahuman%20mobility.%20Moreover%2C%20we%20show%20that%20other%20LLMs%20are%20unable%20to%20perform%20the%0Atask%20properly.%20To%20prevent%20positively%20biased%20results%2C%20we%20also%20propose%20a%0Aframework%20inspired%20by%20other%20studies%20to%20test%20data%20contamination.%20Finally%2C%20we%0Aexplored%20the%20possibility%20of%20using%20LLMs%20as%20text-based%20explainers%20for%0Anext-location%20prediction%20showing%20that%20can%20effectively%20provide%20an%20explanation%0Afor%20their%20decision.%20Notably%2C%207B%20models%20provide%20more%20generic%2C%20but%20still%0Areliable%2C%20explanations%20compared%20to%20larger%20counterparts.%20Code%3A%0Agithub.com/ssai-trento/LLM-zero-shot-NL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520are%2520Zero-Shot%2520Next%2520Location%2520Predictors%26entry.906535625%3DCiro%2520Beneduce%2520and%2520Bruno%2520Lepri%2520and%2520Massimiliano%2520Luca%26entry.1292438233%3D%2520%2520Predicting%2520the%2520locations%2520an%2520individual%2520will%2520visit%2520in%2520the%2520future%2520is%2520crucial%250Afor%2520solving%2520many%2520societal%2520issues%2520like%2520disease%2520diffusion%2520and%2520reduction%2520of%250Apollution%2520among%2520many%2520others.%2520The%2520models%2520designed%2520to%2520tackle%2520next-location%250Aprediction%252C%2520however%252C%2520require%2520a%2520significant%2520amount%2520of%2520individual-level%250Ainformation%2520to%2520be%2520trained%2520effectively.%2520Such%2520data%2520may%2520be%2520scarce%2520or%2520even%250Aunavailable%2520in%2520some%2520geographic%2520regions%2520or%2520peculiar%2520scenarios%2520%2528e.g.%252C%2520cold-start%250Ain%2520recommendation%2520systems%2529.%2520Moreover%252C%2520the%2520design%2520of%2520a%2520next-location%2520predictor%250Aable%2520to%2520generalize%2520or%2520geographically%2520transfer%2520knowledge%2520is%2520still%2520an%2520open%250Aresearch%2520challenge.%2520Recent%2520advances%2520in%2520natural%2520language%2520processing%2520have%2520led%2520to%250Aa%2520rapid%2520diffusion%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520which%2520have%2520shown%2520good%250Ageneralization%2520and%2520reasoning%2520capabilities.%2520These%2520insights%252C%2520coupled%2520with%2520the%250Arecent%2520findings%2520that%2520LLMs%2520are%2520rich%2520in%2520geographical%2520knowledge%252C%2520allowed%2520us%2520to%250Abelieve%2520that%2520these%2520models%2520can%2520act%2520as%2520zero-shot%2520next-location%2520predictors.%2520This%250Apaper%2520evaluates%2520the%2520capabilities%2520of%2520many%2520popular%2520LLMs%2520in%2520this%2520role%252C%250Aspecifically%2520Llama%252C%2520GPT-3.5%2520and%2520Mistral%25207B.%2520After%2520designing%2520a%2520proper%2520prompt%252C%2520we%250Atested%2520the%2520models%2520on%2520three%2520real-world%2520mobility%2520datasets.%2520The%2520results%2520show%2520that%250ALLMs%2520can%2520obtain%2520accuracies%2520up%2520to%252032.4%2525%252C%2520a%2520significant%2520relative%2520improvement%2520of%250Aover%2520600%2525%2520when%2520compared%2520to%2520sophisticated%2520DL%2520models%2520specifically%2520designed%2520for%250Ahuman%2520mobility.%2520Moreover%252C%2520we%2520show%2520that%2520other%2520LLMs%2520are%2520unable%2520to%2520perform%2520the%250Atask%2520properly.%2520To%2520prevent%2520positively%2520biased%2520results%252C%2520we%2520also%2520propose%2520a%250Aframework%2520inspired%2520by%2520other%2520studies%2520to%2520test%2520data%2520contamination.%2520Finally%252C%2520we%250Aexplored%2520the%2520possibility%2520of%2520using%2520LLMs%2520as%2520text-based%2520explainers%2520for%250Anext-location%2520prediction%2520showing%2520that%2520can%2520effectively%2520provide%2520an%2520explanation%250Afor%2520their%2520decision.%2520Notably%252C%25207B%2520models%2520provide%2520more%2520generic%252C%2520but%2520still%250Areliable%252C%2520explanations%2520compared%2520to%2520larger%2520counterparts.%2520Code%253A%250Agithub.com/ssai-trento/LLM-zero-shot-NL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20are%20Zero-Shot%20Next%20Location%20Predictors&entry.906535625=Ciro%20Beneduce%20and%20Bruno%20Lepri%20and%20Massimiliano%20Luca&entry.1292438233=%20%20Predicting%20the%20locations%20an%20individual%20will%20visit%20in%20the%20future%20is%20crucial%0Afor%20solving%20many%20societal%20issues%20like%20disease%20diffusion%20and%20reduction%20of%0Apollution%20among%20many%20others.%20The%20models%20designed%20to%20tackle%20next-location%0Aprediction%2C%20however%2C%20require%20a%20significant%20amount%20of%20individual-level%0Ainformation%20to%20be%20trained%20effectively.%20Such%20data%20may%20be%20scarce%20or%20even%0Aunavailable%20in%20some%20geographic%20regions%20or%20peculiar%20scenarios%20%28e.g.%2C%20cold-start%0Ain%20recommendation%20systems%29.%20Moreover%2C%20the%20design%20of%20a%20next-location%20predictor%0Aable%20to%20generalize%20or%20geographically%20transfer%20knowledge%20is%20still%20an%20open%0Aresearch%20challenge.%20Recent%20advances%20in%20natural%20language%20processing%20have%20led%20to%0Aa%20rapid%20diffusion%20of%20Large%20Language%20Models%20%28LLMs%29%20which%20have%20shown%20good%0Ageneralization%20and%20reasoning%20capabilities.%20These%20insights%2C%20coupled%20with%20the%0Arecent%20findings%20that%20LLMs%20are%20rich%20in%20geographical%20knowledge%2C%20allowed%20us%20to%0Abelieve%20that%20these%20models%20can%20act%20as%20zero-shot%20next-location%20predictors.%20This%0Apaper%20evaluates%20the%20capabilities%20of%20many%20popular%20LLMs%20in%20this%20role%2C%0Aspecifically%20Llama%2C%20GPT-3.5%20and%20Mistral%207B.%20After%20designing%20a%20proper%20prompt%2C%20we%0Atested%20the%20models%20on%20three%20real-world%20mobility%20datasets.%20The%20results%20show%20that%0ALLMs%20can%20obtain%20accuracies%20up%20to%2032.4%25%2C%20a%20significant%20relative%20improvement%20of%0Aover%20600%25%20when%20compared%20to%20sophisticated%20DL%20models%20specifically%20designed%20for%0Ahuman%20mobility.%20Moreover%2C%20we%20show%20that%20other%20LLMs%20are%20unable%20to%20perform%20the%0Atask%20properly.%20To%20prevent%20positively%20biased%20results%2C%20we%20also%20propose%20a%0Aframework%20inspired%20by%20other%20studies%20to%20test%20data%20contamination.%20Finally%2C%20we%0Aexplored%20the%20possibility%20of%20using%20LLMs%20as%20text-based%20explainers%20for%0Anext-location%20prediction%20showing%20that%20can%20effectively%20provide%20an%20explanation%0Afor%20their%20decision.%20Notably%2C%207B%20models%20provide%20more%20generic%2C%20but%20still%0Areliable%2C%20explanations%20compared%20to%20larger%20counterparts.%20Code%3A%0Agithub.com/ssai-trento/LLM-zero-shot-NL%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20962v2&entry.124074799=Read"},
{"title": "Knockout: A simple way to handle missing inputs", "author": "Minh Nguyen and Batuhan K. Karaman and Heejong Kim and Alan Q. Wang and Fengbei Liu and Mert R. Sabuncu", "abstract": "  Deep learning models can extract predictive and actionable information from\ncomplex inputs. The richer the inputs, the better these models usually perform.\nHowever, models that leverage rich inputs (e.g., multi-modality) can be\ndifficult to deploy widely, because some inputs may be missing at inference.\nCurrent popular solutions to this problem include marginalization, imputation,\nand training multiple models. Marginalization can obtain calibrated predictions\nbut it is computationally costly and therefore only feasible for low\ndimensional inputs. Imputation may result in inaccurate predictions because it\nemploys point estimates for missing variables and does not work well for high\ndimensional inputs (e.g., images). Training multiple models whereby each model\ntakes different subsets of inputs can work well but requires knowing missing\ninput patterns in advance. Furthermore, training and retaining multiple models\ncan be costly. We propose an efficient way to learn both the conditional\ndistribution using full inputs and the marginal distributions. Our method,\nKnockout, randomly replaces input features with appropriate placeholder values\nduring training. We provide a theoretical justification of Knockout and show\nthat it can be viewed as an implicit marginalization strategy. We evaluate\nKnockout in a wide range of simulations and real-world datasets and show that\nit can offer strong empirical performance.\n", "link": "http://arxiv.org/abs/2405.20448v2", "date": "2024-06-03", "relevancy": 1.5423, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.522}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5095}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knockout%3A%20A%20simple%20way%20to%20handle%20missing%20inputs&body=Title%3A%20Knockout%3A%20A%20simple%20way%20to%20handle%20missing%20inputs%0AAuthor%3A%20Minh%20Nguyen%20and%20Batuhan%20K.%20Karaman%20and%20Heejong%20Kim%20and%20Alan%20Q.%20Wang%20and%20Fengbei%20Liu%20and%20Mert%20R.%20Sabuncu%0AAbstract%3A%20%20%20Deep%20learning%20models%20can%20extract%20predictive%20and%20actionable%20information%20from%0Acomplex%20inputs.%20The%20richer%20the%20inputs%2C%20the%20better%20these%20models%20usually%20perform.%0AHowever%2C%20models%20that%20leverage%20rich%20inputs%20%28e.g.%2C%20multi-modality%29%20can%20be%0Adifficult%20to%20deploy%20widely%2C%20because%20some%20inputs%20may%20be%20missing%20at%20inference.%0ACurrent%20popular%20solutions%20to%20this%20problem%20include%20marginalization%2C%20imputation%2C%0Aand%20training%20multiple%20models.%20Marginalization%20can%20obtain%20calibrated%20predictions%0Abut%20it%20is%20computationally%20costly%20and%20therefore%20only%20feasible%20for%20low%0Adimensional%20inputs.%20Imputation%20may%20result%20in%20inaccurate%20predictions%20because%20it%0Aemploys%20point%20estimates%20for%20missing%20variables%20and%20does%20not%20work%20well%20for%20high%0Adimensional%20inputs%20%28e.g.%2C%20images%29.%20Training%20multiple%20models%20whereby%20each%20model%0Atakes%20different%20subsets%20of%20inputs%20can%20work%20well%20but%20requires%20knowing%20missing%0Ainput%20patterns%20in%20advance.%20Furthermore%2C%20training%20and%20retaining%20multiple%20models%0Acan%20be%20costly.%20We%20propose%20an%20efficient%20way%20to%20learn%20both%20the%20conditional%0Adistribution%20using%20full%20inputs%20and%20the%20marginal%20distributions.%20Our%20method%2C%0AKnockout%2C%20randomly%20replaces%20input%20features%20with%20appropriate%20placeholder%20values%0Aduring%20training.%20We%20provide%20a%20theoretical%20justification%20of%20Knockout%20and%20show%0Athat%20it%20can%20be%20viewed%20as%20an%20implicit%20marginalization%20strategy.%20We%20evaluate%0AKnockout%20in%20a%20wide%20range%20of%20simulations%20and%20real-world%20datasets%20and%20show%20that%0Ait%20can%20offer%20strong%20empirical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnockout%253A%2520A%2520simple%2520way%2520to%2520handle%2520missing%2520inputs%26entry.906535625%3DMinh%2520Nguyen%2520and%2520Batuhan%2520K.%2520Karaman%2520and%2520Heejong%2520Kim%2520and%2520Alan%2520Q.%2520Wang%2520and%2520Fengbei%2520Liu%2520and%2520Mert%2520R.%2520Sabuncu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520can%2520extract%2520predictive%2520and%2520actionable%2520information%2520from%250Acomplex%2520inputs.%2520The%2520richer%2520the%2520inputs%252C%2520the%2520better%2520these%2520models%2520usually%2520perform.%250AHowever%252C%2520models%2520that%2520leverage%2520rich%2520inputs%2520%2528e.g.%252C%2520multi-modality%2529%2520can%2520be%250Adifficult%2520to%2520deploy%2520widely%252C%2520because%2520some%2520inputs%2520may%2520be%2520missing%2520at%2520inference.%250ACurrent%2520popular%2520solutions%2520to%2520this%2520problem%2520include%2520marginalization%252C%2520imputation%252C%250Aand%2520training%2520multiple%2520models.%2520Marginalization%2520can%2520obtain%2520calibrated%2520predictions%250Abut%2520it%2520is%2520computationally%2520costly%2520and%2520therefore%2520only%2520feasible%2520for%2520low%250Adimensional%2520inputs.%2520Imputation%2520may%2520result%2520in%2520inaccurate%2520predictions%2520because%2520it%250Aemploys%2520point%2520estimates%2520for%2520missing%2520variables%2520and%2520does%2520not%2520work%2520well%2520for%2520high%250Adimensional%2520inputs%2520%2528e.g.%252C%2520images%2529.%2520Training%2520multiple%2520models%2520whereby%2520each%2520model%250Atakes%2520different%2520subsets%2520of%2520inputs%2520can%2520work%2520well%2520but%2520requires%2520knowing%2520missing%250Ainput%2520patterns%2520in%2520advance.%2520Furthermore%252C%2520training%2520and%2520retaining%2520multiple%2520models%250Acan%2520be%2520costly.%2520We%2520propose%2520an%2520efficient%2520way%2520to%2520learn%2520both%2520the%2520conditional%250Adistribution%2520using%2520full%2520inputs%2520and%2520the%2520marginal%2520distributions.%2520Our%2520method%252C%250AKnockout%252C%2520randomly%2520replaces%2520input%2520features%2520with%2520appropriate%2520placeholder%2520values%250Aduring%2520training.%2520We%2520provide%2520a%2520theoretical%2520justification%2520of%2520Knockout%2520and%2520show%250Athat%2520it%2520can%2520be%2520viewed%2520as%2520an%2520implicit%2520marginalization%2520strategy.%2520We%2520evaluate%250AKnockout%2520in%2520a%2520wide%2520range%2520of%2520simulations%2520and%2520real-world%2520datasets%2520and%2520show%2520that%250Ait%2520can%2520offer%2520strong%2520empirical%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knockout%3A%20A%20simple%20way%20to%20handle%20missing%20inputs&entry.906535625=Minh%20Nguyen%20and%20Batuhan%20K.%20Karaman%20and%20Heejong%20Kim%20and%20Alan%20Q.%20Wang%20and%20Fengbei%20Liu%20and%20Mert%20R.%20Sabuncu&entry.1292438233=%20%20Deep%20learning%20models%20can%20extract%20predictive%20and%20actionable%20information%20from%0Acomplex%20inputs.%20The%20richer%20the%20inputs%2C%20the%20better%20these%20models%20usually%20perform.%0AHowever%2C%20models%20that%20leverage%20rich%20inputs%20%28e.g.%2C%20multi-modality%29%20can%20be%0Adifficult%20to%20deploy%20widely%2C%20because%20some%20inputs%20may%20be%20missing%20at%20inference.%0ACurrent%20popular%20solutions%20to%20this%20problem%20include%20marginalization%2C%20imputation%2C%0Aand%20training%20multiple%20models.%20Marginalization%20can%20obtain%20calibrated%20predictions%0Abut%20it%20is%20computationally%20costly%20and%20therefore%20only%20feasible%20for%20low%0Adimensional%20inputs.%20Imputation%20may%20result%20in%20inaccurate%20predictions%20because%20it%0Aemploys%20point%20estimates%20for%20missing%20variables%20and%20does%20not%20work%20well%20for%20high%0Adimensional%20inputs%20%28e.g.%2C%20images%29.%20Training%20multiple%20models%20whereby%20each%20model%0Atakes%20different%20subsets%20of%20inputs%20can%20work%20well%20but%20requires%20knowing%20missing%0Ainput%20patterns%20in%20advance.%20Furthermore%2C%20training%20and%20retaining%20multiple%20models%0Acan%20be%20costly.%20We%20propose%20an%20efficient%20way%20to%20learn%20both%20the%20conditional%0Adistribution%20using%20full%20inputs%20and%20the%20marginal%20distributions.%20Our%20method%2C%0AKnockout%2C%20randomly%20replaces%20input%20features%20with%20appropriate%20placeholder%20values%0Aduring%20training.%20We%20provide%20a%20theoretical%20justification%20of%20Knockout%20and%20show%0Athat%20it%20can%20be%20viewed%20as%20an%20implicit%20marginalization%20strategy.%20We%20evaluate%0AKnockout%20in%20a%20wide%20range%20of%20simulations%20and%20real-world%20datasets%20and%20show%20that%0Ait%20can%20offer%20strong%20empirical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20448v2&entry.124074799=Read"},
{"title": "Planning with a Learned Policy Basis to Optimally Solve Complex Tasks", "author": "Guillermo Infante and David Kuric and Anders Jonsson and Vicen\u00e7 G\u00f3mez and Herke van Hoof", "abstract": "  Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.\n", "link": "http://arxiv.org/abs/2403.15301v2", "date": "2024-06-03", "relevancy": 1.4404, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4784}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning%20with%20a%20Learned%20Policy%20Basis%20to%20Optimally%20Solve%20Complex%20Tasks&body=Title%3A%20Planning%20with%20a%20Learned%20Policy%20Basis%20to%20Optimally%20Solve%20Complex%20Tasks%0AAuthor%3A%20Guillermo%20Infante%20and%20David%20Kuric%20and%20Anders%20Jonsson%20and%20Vicen%C3%A7%20G%C3%B3mez%20and%20Herke%20van%20Hoof%0AAbstract%3A%20%20%20Conventional%20reinforcement%20learning%20%28RL%29%20methods%20can%20successfully%20solve%20a%0Awide%20range%20of%20sequential%20decision%20problems.%20However%2C%20learning%20policies%20that%20can%0Ageneralize%20predictably%20across%20multiple%20tasks%20in%20a%20setting%20with%20non-Markovian%0Areward%20specifications%20is%20a%20challenging%20problem.%20We%20propose%20to%20use%20successor%0Afeatures%20to%20learn%20a%20policy%20basis%20so%20that%20each%20%28sub%29policy%20in%20it%20solves%20a%0Awell-defined%20subproblem.%20In%20a%20task%20described%20by%20a%20finite%20state%20automaton%20%28FSA%29%0Athat%20involves%20the%20same%20set%20of%20subproblems%2C%20the%20combination%20of%20these%0A%28sub%29policies%20can%20then%20be%20used%20to%20generate%20an%20optimal%20solution%20without%0Aadditional%20learning.%20In%20contrast%20to%20other%20methods%20that%20combine%20%28sub%29policies%0Avia%20planning%2C%20our%20method%20asymptotically%20attains%20global%20optimality%2C%20even%20in%0Astochastic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning%2520with%2520a%2520Learned%2520Policy%2520Basis%2520to%2520Optimally%2520Solve%2520Complex%2520Tasks%26entry.906535625%3DGuillermo%2520Infante%2520and%2520David%2520Kuric%2520and%2520Anders%2520Jonsson%2520and%2520Vicen%25C3%25A7%2520G%25C3%25B3mez%2520and%2520Herke%2520van%2520Hoof%26entry.1292438233%3D%2520%2520Conventional%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520can%2520successfully%2520solve%2520a%250Awide%2520range%2520of%2520sequential%2520decision%2520problems.%2520However%252C%2520learning%2520policies%2520that%2520can%250Ageneralize%2520predictably%2520across%2520multiple%2520tasks%2520in%2520a%2520setting%2520with%2520non-Markovian%250Areward%2520specifications%2520is%2520a%2520challenging%2520problem.%2520We%2520propose%2520to%2520use%2520successor%250Afeatures%2520to%2520learn%2520a%2520policy%2520basis%2520so%2520that%2520each%2520%2528sub%2529policy%2520in%2520it%2520solves%2520a%250Awell-defined%2520subproblem.%2520In%2520a%2520task%2520described%2520by%2520a%2520finite%2520state%2520automaton%2520%2528FSA%2529%250Athat%2520involves%2520the%2520same%2520set%2520of%2520subproblems%252C%2520the%2520combination%2520of%2520these%250A%2528sub%2529policies%2520can%2520then%2520be%2520used%2520to%2520generate%2520an%2520optimal%2520solution%2520without%250Aadditional%2520learning.%2520In%2520contrast%2520to%2520other%2520methods%2520that%2520combine%2520%2528sub%2529policies%250Avia%2520planning%252C%2520our%2520method%2520asymptotically%2520attains%2520global%2520optimality%252C%2520even%2520in%250Astochastic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20with%20a%20Learned%20Policy%20Basis%20to%20Optimally%20Solve%20Complex%20Tasks&entry.906535625=Guillermo%20Infante%20and%20David%20Kuric%20and%20Anders%20Jonsson%20and%20Vicen%C3%A7%20G%C3%B3mez%20and%20Herke%20van%20Hoof&entry.1292438233=%20%20Conventional%20reinforcement%20learning%20%28RL%29%20methods%20can%20successfully%20solve%20a%0Awide%20range%20of%20sequential%20decision%20problems.%20However%2C%20learning%20policies%20that%20can%0Ageneralize%20predictably%20across%20multiple%20tasks%20in%20a%20setting%20with%20non-Markovian%0Areward%20specifications%20is%20a%20challenging%20problem.%20We%20propose%20to%20use%20successor%0Afeatures%20to%20learn%20a%20policy%20basis%20so%20that%20each%20%28sub%29policy%20in%20it%20solves%20a%0Awell-defined%20subproblem.%20In%20a%20task%20described%20by%20a%20finite%20state%20automaton%20%28FSA%29%0Athat%20involves%20the%20same%20set%20of%20subproblems%2C%20the%20combination%20of%20these%0A%28sub%29policies%20can%20then%20be%20used%20to%20generate%20an%20optimal%20solution%20without%0Aadditional%20learning.%20In%20contrast%20to%20other%20methods%20that%20combine%20%28sub%29policies%0Avia%20planning%2C%20our%20method%20asymptotically%20attains%20global%20optimality%2C%20even%20in%0Astochastic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15301v2&entry.124074799=Read"},
{"title": "Efficient Inverse Design Optimization through Multi-fidelity\n  Simulations, Machine Learning, and Search Space Reduction Strategies", "author": "Luka Grbcic and Juliane M\u00fcller and Wibe Albert de Jong", "abstract": "  This paper introduces a methodology designed to augment the inverse design\noptimization process in scenarios constrained by limited compute, through the\nstrategic synergy of multi-fidelity evaluations, machine learning models, and\noptimization algorithms. The proposed methodology is analyzed on two distinct\nengineering inverse design problems: airfoil inverse design and the scalar\nfield reconstruction problem. It leverages a machine learning model trained\nwith low-fidelity simulation data, in each optimization cycle, thereby\nproficiently predicting a target variable and discerning whether a\nhigh-fidelity simulation is necessitated, which notably conserves computational\nresources. Additionally, the machine learning model is strategically deployed\nprior to optimization to compress the design space boundaries, thereby further\naccelerating convergence toward the optimal solution. The methodology has been\nemployed to enhance two optimization algorithms, namely Differential Evolution\nand Particle Swarm Optimization. Comparative analyses illustrate performance\nimprovements across both algorithms. Notably, this method is adaptable across\nany inverse design application, facilitating a synergy between a representative\nlow-fidelity ML model, and high-fidelity simulation, and can be seamlessly\napplied across any variety of population-based optimization algorithms.}\n", "link": "http://arxiv.org/abs/2312.03654v2", "date": "2024-06-03", "relevancy": 1.4641, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4875}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Inverse%20Design%20Optimization%20through%20Multi-fidelity%0A%20%20Simulations%2C%20Machine%20Learning%2C%20and%20Search%20Space%20Reduction%20Strategies&body=Title%3A%20Efficient%20Inverse%20Design%20Optimization%20through%20Multi-fidelity%0A%20%20Simulations%2C%20Machine%20Learning%2C%20and%20Search%20Space%20Reduction%20Strategies%0AAuthor%3A%20Luka%20Grbcic%20and%20Juliane%20M%C3%BCller%20and%20Wibe%20Albert%20de%20Jong%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20methodology%20designed%20to%20augment%20the%20inverse%20design%0Aoptimization%20process%20in%20scenarios%20constrained%20by%20limited%20compute%2C%20through%20the%0Astrategic%20synergy%20of%20multi-fidelity%20evaluations%2C%20machine%20learning%20models%2C%20and%0Aoptimization%20algorithms.%20The%20proposed%20methodology%20is%20analyzed%20on%20two%20distinct%0Aengineering%20inverse%20design%20problems%3A%20airfoil%20inverse%20design%20and%20the%20scalar%0Afield%20reconstruction%20problem.%20It%20leverages%20a%20machine%20learning%20model%20trained%0Awith%20low-fidelity%20simulation%20data%2C%20in%20each%20optimization%20cycle%2C%20thereby%0Aproficiently%20predicting%20a%20target%20variable%20and%20discerning%20whether%20a%0Ahigh-fidelity%20simulation%20is%20necessitated%2C%20which%20notably%20conserves%20computational%0Aresources.%20Additionally%2C%20the%20machine%20learning%20model%20is%20strategically%20deployed%0Aprior%20to%20optimization%20to%20compress%20the%20design%20space%20boundaries%2C%20thereby%20further%0Aaccelerating%20convergence%20toward%20the%20optimal%20solution.%20The%20methodology%20has%20been%0Aemployed%20to%20enhance%20two%20optimization%20algorithms%2C%20namely%20Differential%20Evolution%0Aand%20Particle%20Swarm%20Optimization.%20Comparative%20analyses%20illustrate%20performance%0Aimprovements%20across%20both%20algorithms.%20Notably%2C%20this%20method%20is%20adaptable%20across%0Aany%20inverse%20design%20application%2C%20facilitating%20a%20synergy%20between%20a%20representative%0Alow-fidelity%20ML%20model%2C%20and%20high-fidelity%20simulation%2C%20and%20can%20be%20seamlessly%0Aapplied%20across%20any%20variety%20of%20population-based%20optimization%20algorithms.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Inverse%2520Design%2520Optimization%2520through%2520Multi-fidelity%250A%2520%2520Simulations%252C%2520Machine%2520Learning%252C%2520and%2520Search%2520Space%2520Reduction%2520Strategies%26entry.906535625%3DLuka%2520Grbcic%2520and%2520Juliane%2520M%25C3%25BCller%2520and%2520Wibe%2520Albert%2520de%2520Jong%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520methodology%2520designed%2520to%2520augment%2520the%2520inverse%2520design%250Aoptimization%2520process%2520in%2520scenarios%2520constrained%2520by%2520limited%2520compute%252C%2520through%2520the%250Astrategic%2520synergy%2520of%2520multi-fidelity%2520evaluations%252C%2520machine%2520learning%2520models%252C%2520and%250Aoptimization%2520algorithms.%2520The%2520proposed%2520methodology%2520is%2520analyzed%2520on%2520two%2520distinct%250Aengineering%2520inverse%2520design%2520problems%253A%2520airfoil%2520inverse%2520design%2520and%2520the%2520scalar%250Afield%2520reconstruction%2520problem.%2520It%2520leverages%2520a%2520machine%2520learning%2520model%2520trained%250Awith%2520low-fidelity%2520simulation%2520data%252C%2520in%2520each%2520optimization%2520cycle%252C%2520thereby%250Aproficiently%2520predicting%2520a%2520target%2520variable%2520and%2520discerning%2520whether%2520a%250Ahigh-fidelity%2520simulation%2520is%2520necessitated%252C%2520which%2520notably%2520conserves%2520computational%250Aresources.%2520Additionally%252C%2520the%2520machine%2520learning%2520model%2520is%2520strategically%2520deployed%250Aprior%2520to%2520optimization%2520to%2520compress%2520the%2520design%2520space%2520boundaries%252C%2520thereby%2520further%250Aaccelerating%2520convergence%2520toward%2520the%2520optimal%2520solution.%2520The%2520methodology%2520has%2520been%250Aemployed%2520to%2520enhance%2520two%2520optimization%2520algorithms%252C%2520namely%2520Differential%2520Evolution%250Aand%2520Particle%2520Swarm%2520Optimization.%2520Comparative%2520analyses%2520illustrate%2520performance%250Aimprovements%2520across%2520both%2520algorithms.%2520Notably%252C%2520this%2520method%2520is%2520adaptable%2520across%250Aany%2520inverse%2520design%2520application%252C%2520facilitating%2520a%2520synergy%2520between%2520a%2520representative%250Alow-fidelity%2520ML%2520model%252C%2520and%2520high-fidelity%2520simulation%252C%2520and%2520can%2520be%2520seamlessly%250Aapplied%2520across%2520any%2520variety%2520of%2520population-based%2520optimization%2520algorithms.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Inverse%20Design%20Optimization%20through%20Multi-fidelity%0A%20%20Simulations%2C%20Machine%20Learning%2C%20and%20Search%20Space%20Reduction%20Strategies&entry.906535625=Luka%20Grbcic%20and%20Juliane%20M%C3%BCller%20and%20Wibe%20Albert%20de%20Jong&entry.1292438233=%20%20This%20paper%20introduces%20a%20methodology%20designed%20to%20augment%20the%20inverse%20design%0Aoptimization%20process%20in%20scenarios%20constrained%20by%20limited%20compute%2C%20through%20the%0Astrategic%20synergy%20of%20multi-fidelity%20evaluations%2C%20machine%20learning%20models%2C%20and%0Aoptimization%20algorithms.%20The%20proposed%20methodology%20is%20analyzed%20on%20two%20distinct%0Aengineering%20inverse%20design%20problems%3A%20airfoil%20inverse%20design%20and%20the%20scalar%0Afield%20reconstruction%20problem.%20It%20leverages%20a%20machine%20learning%20model%20trained%0Awith%20low-fidelity%20simulation%20data%2C%20in%20each%20optimization%20cycle%2C%20thereby%0Aproficiently%20predicting%20a%20target%20variable%20and%20discerning%20whether%20a%0Ahigh-fidelity%20simulation%20is%20necessitated%2C%20which%20notably%20conserves%20computational%0Aresources.%20Additionally%2C%20the%20machine%20learning%20model%20is%20strategically%20deployed%0Aprior%20to%20optimization%20to%20compress%20the%20design%20space%20boundaries%2C%20thereby%20further%0Aaccelerating%20convergence%20toward%20the%20optimal%20solution.%20The%20methodology%20has%20been%0Aemployed%20to%20enhance%20two%20optimization%20algorithms%2C%20namely%20Differential%20Evolution%0Aand%20Particle%20Swarm%20Optimization.%20Comparative%20analyses%20illustrate%20performance%0Aimprovements%20across%20both%20algorithms.%20Notably%2C%20this%20method%20is%20adaptable%20across%0Aany%20inverse%20design%20application%2C%20facilitating%20a%20synergy%20between%20a%20representative%0Alow-fidelity%20ML%20model%2C%20and%20high-fidelity%20simulation%2C%20and%20can%20be%20seamlessly%0Aapplied%20across%20any%20variety%20of%20population-based%20optimization%20algorithms.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03654v2&entry.124074799=Read"},
{"title": "Particle identification with machine learning from incomplete data in\n  the ALICE experiment", "author": "Maja Karwowska and \u0141ukasz Graczykowski and Kamil Deja and Mi\u0142osz Kasak and Ma\u0142gorzata Janik", "abstract": "  The ALICE experiment at the LHC measures properties of the strongly\ninteracting matter formed in ultrarelativistic heavy-ion collisions. Such\nstudies require accurate particle identification (PID). ALICE provides PID\ninformation via several detectors for particles with momentum from about 100\nMeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular\ncuts. A much better performance can be achieved with machine learning (ML)\nmethods. Our solution uses multiple neural networks (NN) serving as binary\nclassifiers. Moreover, we extended our particle classifier with Feature Set\nEmbedding and attention in order to train on data with incomplete samples. We\nalso present the integration of the ML project with the ALICE analysis\nsoftware, and we discuss domain adaptation, the ML technique needed to transfer\nthe knowledge between simulated and real experimental data.\n", "link": "http://arxiv.org/abs/2403.17436v2", "date": "2024-06-03", "relevancy": 1.3692, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment&body=Title%3A%20Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment%0AAuthor%3A%20Maja%20Karwowska%20and%20%C5%81ukasz%20Graczykowski%20and%20Kamil%20Deja%20and%20Mi%C5%82osz%20Kasak%20and%20Ma%C5%82gorzata%20Janik%0AAbstract%3A%20%20%20The%20ALICE%20experiment%20at%20the%20LHC%20measures%20properties%20of%20the%20strongly%0Ainteracting%20matter%20formed%20in%20ultrarelativistic%20heavy-ion%20collisions.%20Such%0Astudies%20require%20accurate%20particle%20identification%20%28PID%29.%20ALICE%20provides%20PID%0Ainformation%20via%20several%20detectors%20for%20particles%20with%20momentum%20from%20about%20100%0AMeV/c%20up%20to%2020%20GeV/c.%20Traditionally%2C%20particles%20are%20selected%20with%20rectangular%0Acuts.%20A%20much%20better%20performance%20can%20be%20achieved%20with%20machine%20learning%20%28ML%29%0Amethods.%20Our%20solution%20uses%20multiple%20neural%20networks%20%28NN%29%20serving%20as%20binary%0Aclassifiers.%20Moreover%2C%20we%20extended%20our%20particle%20classifier%20with%20Feature%20Set%0AEmbedding%20and%20attention%20in%20order%20to%20train%20on%20data%20with%20incomplete%20samples.%20We%0Aalso%20present%20the%20integration%20of%20the%20ML%20project%20with%20the%20ALICE%20analysis%0Asoftware%2C%20and%20we%20discuss%20domain%20adaptation%2C%20the%20ML%20technique%20needed%20to%20transfer%0Athe%20knowledge%20between%20simulated%20and%20real%20experimental%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticle%2520identification%2520with%2520machine%2520learning%2520from%2520incomplete%2520data%2520in%250A%2520%2520the%2520ALICE%2520experiment%26entry.906535625%3DMaja%2520Karwowska%2520and%2520%25C5%2581ukasz%2520Graczykowski%2520and%2520Kamil%2520Deja%2520and%2520Mi%25C5%2582osz%2520Kasak%2520and%2520Ma%25C5%2582gorzata%2520Janik%26entry.1292438233%3D%2520%2520The%2520ALICE%2520experiment%2520at%2520the%2520LHC%2520measures%2520properties%2520of%2520the%2520strongly%250Ainteracting%2520matter%2520formed%2520in%2520ultrarelativistic%2520heavy-ion%2520collisions.%2520Such%250Astudies%2520require%2520accurate%2520particle%2520identification%2520%2528PID%2529.%2520ALICE%2520provides%2520PID%250Ainformation%2520via%2520several%2520detectors%2520for%2520particles%2520with%2520momentum%2520from%2520about%2520100%250AMeV/c%2520up%2520to%252020%2520GeV/c.%2520Traditionally%252C%2520particles%2520are%2520selected%2520with%2520rectangular%250Acuts.%2520A%2520much%2520better%2520performance%2520can%2520be%2520achieved%2520with%2520machine%2520learning%2520%2528ML%2529%250Amethods.%2520Our%2520solution%2520uses%2520multiple%2520neural%2520networks%2520%2528NN%2529%2520serving%2520as%2520binary%250Aclassifiers.%2520Moreover%252C%2520we%2520extended%2520our%2520particle%2520classifier%2520with%2520Feature%2520Set%250AEmbedding%2520and%2520attention%2520in%2520order%2520to%2520train%2520on%2520data%2520with%2520incomplete%2520samples.%2520We%250Aalso%2520present%2520the%2520integration%2520of%2520the%2520ML%2520project%2520with%2520the%2520ALICE%2520analysis%250Asoftware%252C%2520and%2520we%2520discuss%2520domain%2520adaptation%252C%2520the%2520ML%2520technique%2520needed%2520to%2520transfer%250Athe%2520knowledge%2520between%2520simulated%2520and%2520real%2520experimental%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment&entry.906535625=Maja%20Karwowska%20and%20%C5%81ukasz%20Graczykowski%20and%20Kamil%20Deja%20and%20Mi%C5%82osz%20Kasak%20and%20Ma%C5%82gorzata%20Janik&entry.1292438233=%20%20The%20ALICE%20experiment%20at%20the%20LHC%20measures%20properties%20of%20the%20strongly%0Ainteracting%20matter%20formed%20in%20ultrarelativistic%20heavy-ion%20collisions.%20Such%0Astudies%20require%20accurate%20particle%20identification%20%28PID%29.%20ALICE%20provides%20PID%0Ainformation%20via%20several%20detectors%20for%20particles%20with%20momentum%20from%20about%20100%0AMeV/c%20up%20to%2020%20GeV/c.%20Traditionally%2C%20particles%20are%20selected%20with%20rectangular%0Acuts.%20A%20much%20better%20performance%20can%20be%20achieved%20with%20machine%20learning%20%28ML%29%0Amethods.%20Our%20solution%20uses%20multiple%20neural%20networks%20%28NN%29%20serving%20as%20binary%0Aclassifiers.%20Moreover%2C%20we%20extended%20our%20particle%20classifier%20with%20Feature%20Set%0AEmbedding%20and%20attention%20in%20order%20to%20train%20on%20data%20with%20incomplete%20samples.%20We%0Aalso%20present%20the%20integration%20of%20the%20ML%20project%20with%20the%20ALICE%20analysis%0Asoftware%2C%20and%20we%20discuss%20domain%20adaptation%2C%20the%20ML%20technique%20needed%20to%20transfer%0Athe%20knowledge%20between%20simulated%20and%20real%20experimental%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17436v2&entry.124074799=Read"},
{"title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind\n  Reasoning Capabilities of Large Language Models", "author": "Hainiu Xu and Runcong Zhao and Lixing Zhu and Jinhua Du and Yulan He", "abstract": "  Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track\nof the mental states of others, is pivotal in developing socially intelligent\nagents. However, prevalent N-ToM benchmarks have several shortcomings,\nincluding the presence of ambiguous and artificial narratives, absence of\npersonality traits and preferences, a lack of questions addressing characters'\npsychological mental states, and limited diversity in the questions posed. In\nresponse to these issues, we construct OpenToM, a new benchmark for assessing\nN-ToM with (1) longer and clearer narrative stories, (2) characters with\nexplicit personality traits, (3) actions that are triggered by character\nintentions, and (4) questions designed to challenge LLMs' capabilities of\nmodeling characters' mental states of both the physical and psychological\nworld. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling\ncertain aspects of mental states in the physical world but fall short when\ntracking characters' mental states in the psychological world.\n", "link": "http://arxiv.org/abs/2402.06044v3", "date": "2024-06-03", "relevancy": 1.5153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenToM%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Theory-of-Mind%0A%20%20Reasoning%20Capabilities%20of%20Large%20Language%20Models&body=Title%3A%20OpenToM%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Theory-of-Mind%0A%20%20Reasoning%20Capabilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Hainiu%20Xu%20and%20Runcong%20Zhao%20and%20Lixing%20Zhu%20and%20Jinhua%20Du%20and%20Yulan%20He%0AAbstract%3A%20%20%20Neural%20Theory-of-Mind%20%28N-ToM%29%2C%20machine%27s%20ability%20to%20understand%20and%20keep%20track%0Aof%20the%20mental%20states%20of%20others%2C%20is%20pivotal%20in%20developing%20socially%20intelligent%0Aagents.%20However%2C%20prevalent%20N-ToM%20benchmarks%20have%20several%20shortcomings%2C%0Aincluding%20the%20presence%20of%20ambiguous%20and%20artificial%20narratives%2C%20absence%20of%0Apersonality%20traits%20and%20preferences%2C%20a%20lack%20of%20questions%20addressing%20characters%27%0Apsychological%20mental%20states%2C%20and%20limited%20diversity%20in%20the%20questions%20posed.%20In%0Aresponse%20to%20these%20issues%2C%20we%20construct%20OpenToM%2C%20a%20new%20benchmark%20for%20assessing%0AN-ToM%20with%20%281%29%20longer%20and%20clearer%20narrative%20stories%2C%20%282%29%20characters%20with%0Aexplicit%20personality%20traits%2C%20%283%29%20actions%20that%20are%20triggered%20by%20character%0Aintentions%2C%20and%20%284%29%20questions%20designed%20to%20challenge%20LLMs%27%20capabilities%20of%0Amodeling%20characters%27%20mental%20states%20of%20both%20the%20physical%20and%20psychological%0Aworld.%20Using%20OpenToM%2C%20we%20reveal%20that%20state-of-the-art%20LLMs%20thrive%20at%20modeling%0Acertain%20aspects%20of%20mental%20states%20in%20the%20physical%20world%20but%20fall%20short%20when%0Atracking%20characters%27%20mental%20states%20in%20the%20psychological%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06044v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenToM%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520Theory-of-Mind%250A%2520%2520Reasoning%2520Capabilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DHainiu%2520Xu%2520and%2520Runcong%2520Zhao%2520and%2520Lixing%2520Zhu%2520and%2520Jinhua%2520Du%2520and%2520Yulan%2520He%26entry.1292438233%3D%2520%2520Neural%2520Theory-of-Mind%2520%2528N-ToM%2529%252C%2520machine%2527s%2520ability%2520to%2520understand%2520and%2520keep%2520track%250Aof%2520the%2520mental%2520states%2520of%2520others%252C%2520is%2520pivotal%2520in%2520developing%2520socially%2520intelligent%250Aagents.%2520However%252C%2520prevalent%2520N-ToM%2520benchmarks%2520have%2520several%2520shortcomings%252C%250Aincluding%2520the%2520presence%2520of%2520ambiguous%2520and%2520artificial%2520narratives%252C%2520absence%2520of%250Apersonality%2520traits%2520and%2520preferences%252C%2520a%2520lack%2520of%2520questions%2520addressing%2520characters%2527%250Apsychological%2520mental%2520states%252C%2520and%2520limited%2520diversity%2520in%2520the%2520questions%2520posed.%2520In%250Aresponse%2520to%2520these%2520issues%252C%2520we%2520construct%2520OpenToM%252C%2520a%2520new%2520benchmark%2520for%2520assessing%250AN-ToM%2520with%2520%25281%2529%2520longer%2520and%2520clearer%2520narrative%2520stories%252C%2520%25282%2529%2520characters%2520with%250Aexplicit%2520personality%2520traits%252C%2520%25283%2529%2520actions%2520that%2520are%2520triggered%2520by%2520character%250Aintentions%252C%2520and%2520%25284%2529%2520questions%2520designed%2520to%2520challenge%2520LLMs%2527%2520capabilities%2520of%250Amodeling%2520characters%2527%2520mental%2520states%2520of%2520both%2520the%2520physical%2520and%2520psychological%250Aworld.%2520Using%2520OpenToM%252C%2520we%2520reveal%2520that%2520state-of-the-art%2520LLMs%2520thrive%2520at%2520modeling%250Acertain%2520aspects%2520of%2520mental%2520states%2520in%2520the%2520physical%2520world%2520but%2520fall%2520short%2520when%250Atracking%2520characters%2527%2520mental%2520states%2520in%2520the%2520psychological%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06044v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenToM%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Theory-of-Mind%0A%20%20Reasoning%20Capabilities%20of%20Large%20Language%20Models&entry.906535625=Hainiu%20Xu%20and%20Runcong%20Zhao%20and%20Lixing%20Zhu%20and%20Jinhua%20Du%20and%20Yulan%20He&entry.1292438233=%20%20Neural%20Theory-of-Mind%20%28N-ToM%29%2C%20machine%27s%20ability%20to%20understand%20and%20keep%20track%0Aof%20the%20mental%20states%20of%20others%2C%20is%20pivotal%20in%20developing%20socially%20intelligent%0Aagents.%20However%2C%20prevalent%20N-ToM%20benchmarks%20have%20several%20shortcomings%2C%0Aincluding%20the%20presence%20of%20ambiguous%20and%20artificial%20narratives%2C%20absence%20of%0Apersonality%20traits%20and%20preferences%2C%20a%20lack%20of%20questions%20addressing%20characters%27%0Apsychological%20mental%20states%2C%20and%20limited%20diversity%20in%20the%20questions%20posed.%20In%0Aresponse%20to%20these%20issues%2C%20we%20construct%20OpenToM%2C%20a%20new%20benchmark%20for%20assessing%0AN-ToM%20with%20%281%29%20longer%20and%20clearer%20narrative%20stories%2C%20%282%29%20characters%20with%0Aexplicit%20personality%20traits%2C%20%283%29%20actions%20that%20are%20triggered%20by%20character%0Aintentions%2C%20and%20%284%29%20questions%20designed%20to%20challenge%20LLMs%27%20capabilities%20of%0Amodeling%20characters%27%20mental%20states%20of%20both%20the%20physical%20and%20psychological%0Aworld.%20Using%20OpenToM%2C%20we%20reveal%20that%20state-of-the-art%20LLMs%20thrive%20at%20modeling%0Acertain%20aspects%20of%20mental%20states%20in%20the%20physical%20world%20but%20fall%20short%20when%0Atracking%20characters%27%20mental%20states%20in%20the%20psychological%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06044v3&entry.124074799=Read"},
{"title": "Generalization Bounds for Heavy-Tailed SDEs through the Fractional\n  Fokker-Planck Equation", "author": "Benjamin Dupuis and Umut \u015eim\u015fekli", "abstract": "  Understanding the generalization properties of heavy-tailed stochastic\noptimization algorithms has attracted increasing attention over the past years.\nWhile illuminating interesting aspects of stochastic optimizers by using\nheavy-tailed stochastic differential equations as proxies, prior works either\nprovided expected generalization bounds, or introduced non-computable\ninformation theoretic terms. Addressing these drawbacks, in this work, we prove\nhigh-probability generalization bounds for heavy-tailed SDEs which do not\ncontain any nontrivial information theoretic terms. To achieve this goal, we\ndevelop new proof techniques based on estimating the entropy flows associated\nwith the so-called fractional Fokker-Planck equation (a partial differential\nequation that governs the evolution of the distribution of the corresponding\nheavy-tailed SDE). In addition to obtaining high-probability bounds, we show\nthat our bounds have a better dependence on the dimension of parameters as\ncompared to prior art. Our results further identify a phase transition\nphenomenon, which suggests that heavy tails can be either beneficial or harmful\ndepending on the problem structure. We support our theory with experiments\nconducted in a variety of settings.\n", "link": "http://arxiv.org/abs/2402.07723v2", "date": "2024-06-03", "relevancy": 1.3214, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4571}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4417}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Bounds%20for%20Heavy-Tailed%20SDEs%20through%20the%20Fractional%0A%20%20Fokker-Planck%20Equation&body=Title%3A%20Generalization%20Bounds%20for%20Heavy-Tailed%20SDEs%20through%20the%20Fractional%0A%20%20Fokker-Planck%20Equation%0AAuthor%3A%20Benjamin%20Dupuis%20and%20Umut%20%C5%9Eim%C5%9Fekli%0AAbstract%3A%20%20%20Understanding%20the%20generalization%20properties%20of%20heavy-tailed%20stochastic%0Aoptimization%20algorithms%20has%20attracted%20increasing%20attention%20over%20the%20past%20years.%0AWhile%20illuminating%20interesting%20aspects%20of%20stochastic%20optimizers%20by%20using%0Aheavy-tailed%20stochastic%20differential%20equations%20as%20proxies%2C%20prior%20works%20either%0Aprovided%20expected%20generalization%20bounds%2C%20or%20introduced%20non-computable%0Ainformation%20theoretic%20terms.%20Addressing%20these%20drawbacks%2C%20in%20this%20work%2C%20we%20prove%0Ahigh-probability%20generalization%20bounds%20for%20heavy-tailed%20SDEs%20which%20do%20not%0Acontain%20any%20nontrivial%20information%20theoretic%20terms.%20To%20achieve%20this%20goal%2C%20we%0Adevelop%20new%20proof%20techniques%20based%20on%20estimating%20the%20entropy%20flows%20associated%0Awith%20the%20so-called%20fractional%20Fokker-Planck%20equation%20%28a%20partial%20differential%0Aequation%20that%20governs%20the%20evolution%20of%20the%20distribution%20of%20the%20corresponding%0Aheavy-tailed%20SDE%29.%20In%20addition%20to%20obtaining%20high-probability%20bounds%2C%20we%20show%0Athat%20our%20bounds%20have%20a%20better%20dependence%20on%20the%20dimension%20of%20parameters%20as%0Acompared%20to%20prior%20art.%20Our%20results%20further%20identify%20a%20phase%20transition%0Aphenomenon%2C%20which%20suggests%20that%20heavy%20tails%20can%20be%20either%20beneficial%20or%20harmful%0Adepending%20on%20the%20problem%20structure.%20We%20support%20our%20theory%20with%20experiments%0Aconducted%20in%20a%20variety%20of%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Bounds%2520for%2520Heavy-Tailed%2520SDEs%2520through%2520the%2520Fractional%250A%2520%2520Fokker-Planck%2520Equation%26entry.906535625%3DBenjamin%2520Dupuis%2520and%2520Umut%2520%25C5%259Eim%25C5%259Fekli%26entry.1292438233%3D%2520%2520Understanding%2520the%2520generalization%2520properties%2520of%2520heavy-tailed%2520stochastic%250Aoptimization%2520algorithms%2520has%2520attracted%2520increasing%2520attention%2520over%2520the%2520past%2520years.%250AWhile%2520illuminating%2520interesting%2520aspects%2520of%2520stochastic%2520optimizers%2520by%2520using%250Aheavy-tailed%2520stochastic%2520differential%2520equations%2520as%2520proxies%252C%2520prior%2520works%2520either%250Aprovided%2520expected%2520generalization%2520bounds%252C%2520or%2520introduced%2520non-computable%250Ainformation%2520theoretic%2520terms.%2520Addressing%2520these%2520drawbacks%252C%2520in%2520this%2520work%252C%2520we%2520prove%250Ahigh-probability%2520generalization%2520bounds%2520for%2520heavy-tailed%2520SDEs%2520which%2520do%2520not%250Acontain%2520any%2520nontrivial%2520information%2520theoretic%2520terms.%2520To%2520achieve%2520this%2520goal%252C%2520we%250Adevelop%2520new%2520proof%2520techniques%2520based%2520on%2520estimating%2520the%2520entropy%2520flows%2520associated%250Awith%2520the%2520so-called%2520fractional%2520Fokker-Planck%2520equation%2520%2528a%2520partial%2520differential%250Aequation%2520that%2520governs%2520the%2520evolution%2520of%2520the%2520distribution%2520of%2520the%2520corresponding%250Aheavy-tailed%2520SDE%2529.%2520In%2520addition%2520to%2520obtaining%2520high-probability%2520bounds%252C%2520we%2520show%250Athat%2520our%2520bounds%2520have%2520a%2520better%2520dependence%2520on%2520the%2520dimension%2520of%2520parameters%2520as%250Acompared%2520to%2520prior%2520art.%2520Our%2520results%2520further%2520identify%2520a%2520phase%2520transition%250Aphenomenon%252C%2520which%2520suggests%2520that%2520heavy%2520tails%2520can%2520be%2520either%2520beneficial%2520or%2520harmful%250Adepending%2520on%2520the%2520problem%2520structure.%2520We%2520support%2520our%2520theory%2520with%2520experiments%250Aconducted%2520in%2520a%2520variety%2520of%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Bounds%20for%20Heavy-Tailed%20SDEs%20through%20the%20Fractional%0A%20%20Fokker-Planck%20Equation&entry.906535625=Benjamin%20Dupuis%20and%20Umut%20%C5%9Eim%C5%9Fekli&entry.1292438233=%20%20Understanding%20the%20generalization%20properties%20of%20heavy-tailed%20stochastic%0Aoptimization%20algorithms%20has%20attracted%20increasing%20attention%20over%20the%20past%20years.%0AWhile%20illuminating%20interesting%20aspects%20of%20stochastic%20optimizers%20by%20using%0Aheavy-tailed%20stochastic%20differential%20equations%20as%20proxies%2C%20prior%20works%20either%0Aprovided%20expected%20generalization%20bounds%2C%20or%20introduced%20non-computable%0Ainformation%20theoretic%20terms.%20Addressing%20these%20drawbacks%2C%20in%20this%20work%2C%20we%20prove%0Ahigh-probability%20generalization%20bounds%20for%20heavy-tailed%20SDEs%20which%20do%20not%0Acontain%20any%20nontrivial%20information%20theoretic%20terms.%20To%20achieve%20this%20goal%2C%20we%0Adevelop%20new%20proof%20techniques%20based%20on%20estimating%20the%20entropy%20flows%20associated%0Awith%20the%20so-called%20fractional%20Fokker-Planck%20equation%20%28a%20partial%20differential%0Aequation%20that%20governs%20the%20evolution%20of%20the%20distribution%20of%20the%20corresponding%0Aheavy-tailed%20SDE%29.%20In%20addition%20to%20obtaining%20high-probability%20bounds%2C%20we%20show%0Athat%20our%20bounds%20have%20a%20better%20dependence%20on%20the%20dimension%20of%20parameters%20as%0Acompared%20to%20prior%20art.%20Our%20results%20further%20identify%20a%20phase%20transition%0Aphenomenon%2C%20which%20suggests%20that%20heavy%20tails%20can%20be%20either%20beneficial%20or%20harmful%0Adepending%20on%20the%20problem%20structure.%20We%20support%20our%20theory%20with%20experiments%0Aconducted%20in%20a%20variety%20of%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07723v2&entry.124074799=Read"},
{"title": "1-Lipschitz Neural Networks are more expressive with N-Activations", "author": "Bernd Prach and Christoph H. Lampert", "abstract": "  A crucial property for achieving secure, trustworthy and interpretable deep\nlearning systems is their robustness: small changes to a system's inputs should\nnot result in large changes to its outputs. Mathematically, this means one\nstrives for networks with a small Lipschitz constant. Several recent works have\nfocused on how to construct such Lipschitz networks, typically by imposing\nconstraints on the weight matrices. In this work, we study an orthogonal\naspect, namely the role of the activation function. We show that commonly used\nactivation functions, such as MaxMin, as well as all piece-wise linear ones\nwith two segments unnecessarily restrict the class of representable functions,\neven in the simplest one-dimensional setting. We furthermore introduce the new\nN-activation function that is provably more expressive than currently popular\nactivation functions. We provide code at\nhttps://github.com/berndprach/NActivation.\n", "link": "http://arxiv.org/abs/2311.06103v2", "date": "2024-06-03", "relevancy": 1.356, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4454}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%201-Lipschitz%20Neural%20Networks%20are%20more%20expressive%20with%20N-Activations&body=Title%3A%201-Lipschitz%20Neural%20Networks%20are%20more%20expressive%20with%20N-Activations%0AAuthor%3A%20Bernd%20Prach%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20A%20crucial%20property%20for%20achieving%20secure%2C%20trustworthy%20and%20interpretable%20deep%0Alearning%20systems%20is%20their%20robustness%3A%20small%20changes%20to%20a%20system%27s%20inputs%20should%0Anot%20result%20in%20large%20changes%20to%20its%20outputs.%20Mathematically%2C%20this%20means%20one%0Astrives%20for%20networks%20with%20a%20small%20Lipschitz%20constant.%20Several%20recent%20works%20have%0Afocused%20on%20how%20to%20construct%20such%20Lipschitz%20networks%2C%20typically%20by%20imposing%0Aconstraints%20on%20the%20weight%20matrices.%20In%20this%20work%2C%20we%20study%20an%20orthogonal%0Aaspect%2C%20namely%20the%20role%20of%20the%20activation%20function.%20We%20show%20that%20commonly%20used%0Aactivation%20functions%2C%20such%20as%20MaxMin%2C%20as%20well%20as%20all%20piece-wise%20linear%20ones%0Awith%20two%20segments%20unnecessarily%20restrict%20the%20class%20of%20representable%20functions%2C%0Aeven%20in%20the%20simplest%20one-dimensional%20setting.%20We%20furthermore%20introduce%20the%20new%0AN-activation%20function%20that%20is%20provably%20more%20expressive%20than%20currently%20popular%0Aactivation%20functions.%20We%20provide%20code%20at%0Ahttps%3A//github.com/berndprach/NActivation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D1-Lipschitz%2520Neural%2520Networks%2520are%2520more%2520expressive%2520with%2520N-Activations%26entry.906535625%3DBernd%2520Prach%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520A%2520crucial%2520property%2520for%2520achieving%2520secure%252C%2520trustworthy%2520and%2520interpretable%2520deep%250Alearning%2520systems%2520is%2520their%2520robustness%253A%2520small%2520changes%2520to%2520a%2520system%2527s%2520inputs%2520should%250Anot%2520result%2520in%2520large%2520changes%2520to%2520its%2520outputs.%2520Mathematically%252C%2520this%2520means%2520one%250Astrives%2520for%2520networks%2520with%2520a%2520small%2520Lipschitz%2520constant.%2520Several%2520recent%2520works%2520have%250Afocused%2520on%2520how%2520to%2520construct%2520such%2520Lipschitz%2520networks%252C%2520typically%2520by%2520imposing%250Aconstraints%2520on%2520the%2520weight%2520matrices.%2520In%2520this%2520work%252C%2520we%2520study%2520an%2520orthogonal%250Aaspect%252C%2520namely%2520the%2520role%2520of%2520the%2520activation%2520function.%2520We%2520show%2520that%2520commonly%2520used%250Aactivation%2520functions%252C%2520such%2520as%2520MaxMin%252C%2520as%2520well%2520as%2520all%2520piece-wise%2520linear%2520ones%250Awith%2520two%2520segments%2520unnecessarily%2520restrict%2520the%2520class%2520of%2520representable%2520functions%252C%250Aeven%2520in%2520the%2520simplest%2520one-dimensional%2520setting.%2520We%2520furthermore%2520introduce%2520the%2520new%250AN-activation%2520function%2520that%2520is%2520provably%2520more%2520expressive%2520than%2520currently%2520popular%250Aactivation%2520functions.%2520We%2520provide%2520code%2520at%250Ahttps%253A//github.com/berndprach/NActivation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1-Lipschitz%20Neural%20Networks%20are%20more%20expressive%20with%20N-Activations&entry.906535625=Bernd%20Prach%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20A%20crucial%20property%20for%20achieving%20secure%2C%20trustworthy%20and%20interpretable%20deep%0Alearning%20systems%20is%20their%20robustness%3A%20small%20changes%20to%20a%20system%27s%20inputs%20should%0Anot%20result%20in%20large%20changes%20to%20its%20outputs.%20Mathematically%2C%20this%20means%20one%0Astrives%20for%20networks%20with%20a%20small%20Lipschitz%20constant.%20Several%20recent%20works%20have%0Afocused%20on%20how%20to%20construct%20such%20Lipschitz%20networks%2C%20typically%20by%20imposing%0Aconstraints%20on%20the%20weight%20matrices.%20In%20this%20work%2C%20we%20study%20an%20orthogonal%0Aaspect%2C%20namely%20the%20role%20of%20the%20activation%20function.%20We%20show%20that%20commonly%20used%0Aactivation%20functions%2C%20such%20as%20MaxMin%2C%20as%20well%20as%20all%20piece-wise%20linear%20ones%0Awith%20two%20segments%20unnecessarily%20restrict%20the%20class%20of%20representable%20functions%2C%0Aeven%20in%20the%20simplest%20one-dimensional%20setting.%20We%20furthermore%20introduce%20the%20new%0AN-activation%20function%20that%20is%20provably%20more%20expressive%20than%20currently%20popular%0Aactivation%20functions.%20We%20provide%20code%20at%0Ahttps%3A//github.com/berndprach/NActivation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06103v2&entry.124074799=Read"},
{"title": "Human vs. Machine: Behavioral Differences Between Expert Humans and\n  Language Models in Wargame Simulations", "author": "Max Lamparth and Anthony Corso and Jacob Ganz and Oriana Skylar Mastro and Jacquelyn Schneider and Harold Trinkunas", "abstract": "  To some, the advent of artificial intelligence (AI) promises better\ndecision-making and increased military effectiveness while reducing the\ninfluence of human error and emotions. However, there is still debate about how\nAI systems, especially large language models (LLMs), behave compared to humans\nin high-stakes military decision-making scenarios with the potential for\nincreased risks towards escalation and unnecessary conflicts. To test this\npotential and scrutinize the use of LLMs for such purposes, we use a new\nwargame experiment with 107 national security experts designed to look at\ncrisis escalation in a fictional US-China scenario and compare human players to\nLLM-simulated responses in separate simulations. Wargames have a long history\nin the development of military strategy and the response of nations to threats\nor attacks. Here, we show a considerable high-level agreement in the LLM and\nhuman responses and significant quantitative and qualitative differences in\nindividual actions and strategic tendencies. These differences depend on\nintrinsic biases in LLMs regarding the appropriate level of violence following\nstrategic instructions, the choice of LLM, and whether the LLMs are tasked to\ndecide for a team of players directly or first to simulate dialog between\nplayers. When simulating the dialog, the discussions lack quality and maintain\na farcical harmony. The LLM simulations cannot account for human player\ncharacteristics, showing no significant difference even for extreme traits,\nsuch as \"pacifist\" or \"aggressive sociopath\". Our results motivate policymakers\nto be cautious before granting autonomy or following AI-based strategy\nrecommendations.\n", "link": "http://arxiv.org/abs/2403.03407v2", "date": "2024-06-03", "relevancy": 1.2941, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4473}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4269}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20vs.%20Machine%3A%20Behavioral%20Differences%20Between%20Expert%20Humans%20and%0A%20%20Language%20Models%20in%20Wargame%20Simulations&body=Title%3A%20Human%20vs.%20Machine%3A%20Behavioral%20Differences%20Between%20Expert%20Humans%20and%0A%20%20Language%20Models%20in%20Wargame%20Simulations%0AAuthor%3A%20Max%20Lamparth%20and%20Anthony%20Corso%20and%20Jacob%20Ganz%20and%20Oriana%20Skylar%20Mastro%20and%20Jacquelyn%20Schneider%20and%20Harold%20Trinkunas%0AAbstract%3A%20%20%20To%20some%2C%20the%20advent%20of%20artificial%20intelligence%20%28AI%29%20promises%20better%0Adecision-making%20and%20increased%20military%20effectiveness%20while%20reducing%20the%0Ainfluence%20of%20human%20error%20and%20emotions.%20However%2C%20there%20is%20still%20debate%20about%20how%0AAI%20systems%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20behave%20compared%20to%20humans%0Ain%20high-stakes%20military%20decision-making%20scenarios%20with%20the%20potential%20for%0Aincreased%20risks%20towards%20escalation%20and%20unnecessary%20conflicts.%20To%20test%20this%0Apotential%20and%20scrutinize%20the%20use%20of%20LLMs%20for%20such%20purposes%2C%20we%20use%20a%20new%0Awargame%20experiment%20with%20107%20national%20security%20experts%20designed%20to%20look%20at%0Acrisis%20escalation%20in%20a%20fictional%20US-China%20scenario%20and%20compare%20human%20players%20to%0ALLM-simulated%20responses%20in%20separate%20simulations.%20Wargames%20have%20a%20long%20history%0Ain%20the%20development%20of%20military%20strategy%20and%20the%20response%20of%20nations%20to%20threats%0Aor%20attacks.%20Here%2C%20we%20show%20a%20considerable%20high-level%20agreement%20in%20the%20LLM%20and%0Ahuman%20responses%20and%20significant%20quantitative%20and%20qualitative%20differences%20in%0Aindividual%20actions%20and%20strategic%20tendencies.%20These%20differences%20depend%20on%0Aintrinsic%20biases%20in%20LLMs%20regarding%20the%20appropriate%20level%20of%20violence%20following%0Astrategic%20instructions%2C%20the%20choice%20of%20LLM%2C%20and%20whether%20the%20LLMs%20are%20tasked%20to%0Adecide%20for%20a%20team%20of%20players%20directly%20or%20first%20to%20simulate%20dialog%20between%0Aplayers.%20When%20simulating%20the%20dialog%2C%20the%20discussions%20lack%20quality%20and%20maintain%0Aa%20farcical%20harmony.%20The%20LLM%20simulations%20cannot%20account%20for%20human%20player%0Acharacteristics%2C%20showing%20no%20significant%20difference%20even%20for%20extreme%20traits%2C%0Asuch%20as%20%22pacifist%22%20or%20%22aggressive%20sociopath%22.%20Our%20results%20motivate%20policymakers%0Ato%20be%20cautious%20before%20granting%20autonomy%20or%20following%20AI-based%20strategy%0Arecommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520vs.%2520Machine%253A%2520Behavioral%2520Differences%2520Between%2520Expert%2520Humans%2520and%250A%2520%2520Language%2520Models%2520in%2520Wargame%2520Simulations%26entry.906535625%3DMax%2520Lamparth%2520and%2520Anthony%2520Corso%2520and%2520Jacob%2520Ganz%2520and%2520Oriana%2520Skylar%2520Mastro%2520and%2520Jacquelyn%2520Schneider%2520and%2520Harold%2520Trinkunas%26entry.1292438233%3D%2520%2520To%2520some%252C%2520the%2520advent%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520promises%2520better%250Adecision-making%2520and%2520increased%2520military%2520effectiveness%2520while%2520reducing%2520the%250Ainfluence%2520of%2520human%2520error%2520and%2520emotions.%2520However%252C%2520there%2520is%2520still%2520debate%2520about%2520how%250AAI%2520systems%252C%2520especially%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520behave%2520compared%2520to%2520humans%250Ain%2520high-stakes%2520military%2520decision-making%2520scenarios%2520with%2520the%2520potential%2520for%250Aincreased%2520risks%2520towards%2520escalation%2520and%2520unnecessary%2520conflicts.%2520To%2520test%2520this%250Apotential%2520and%2520scrutinize%2520the%2520use%2520of%2520LLMs%2520for%2520such%2520purposes%252C%2520we%2520use%2520a%2520new%250Awargame%2520experiment%2520with%2520107%2520national%2520security%2520experts%2520designed%2520to%2520look%2520at%250Acrisis%2520escalation%2520in%2520a%2520fictional%2520US-China%2520scenario%2520and%2520compare%2520human%2520players%2520to%250ALLM-simulated%2520responses%2520in%2520separate%2520simulations.%2520Wargames%2520have%2520a%2520long%2520history%250Ain%2520the%2520development%2520of%2520military%2520strategy%2520and%2520the%2520response%2520of%2520nations%2520to%2520threats%250Aor%2520attacks.%2520Here%252C%2520we%2520show%2520a%2520considerable%2520high-level%2520agreement%2520in%2520the%2520LLM%2520and%250Ahuman%2520responses%2520and%2520significant%2520quantitative%2520and%2520qualitative%2520differences%2520in%250Aindividual%2520actions%2520and%2520strategic%2520tendencies.%2520These%2520differences%2520depend%2520on%250Aintrinsic%2520biases%2520in%2520LLMs%2520regarding%2520the%2520appropriate%2520level%2520of%2520violence%2520following%250Astrategic%2520instructions%252C%2520the%2520choice%2520of%2520LLM%252C%2520and%2520whether%2520the%2520LLMs%2520are%2520tasked%2520to%250Adecide%2520for%2520a%2520team%2520of%2520players%2520directly%2520or%2520first%2520to%2520simulate%2520dialog%2520between%250Aplayers.%2520When%2520simulating%2520the%2520dialog%252C%2520the%2520discussions%2520lack%2520quality%2520and%2520maintain%250Aa%2520farcical%2520harmony.%2520The%2520LLM%2520simulations%2520cannot%2520account%2520for%2520human%2520player%250Acharacteristics%252C%2520showing%2520no%2520significant%2520difference%2520even%2520for%2520extreme%2520traits%252C%250Asuch%2520as%2520%2522pacifist%2522%2520or%2520%2522aggressive%2520sociopath%2522.%2520Our%2520results%2520motivate%2520policymakers%250Ato%2520be%2520cautious%2520before%2520granting%2520autonomy%2520or%2520following%2520AI-based%2520strategy%250Arecommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20vs.%20Machine%3A%20Behavioral%20Differences%20Between%20Expert%20Humans%20and%0A%20%20Language%20Models%20in%20Wargame%20Simulations&entry.906535625=Max%20Lamparth%20and%20Anthony%20Corso%20and%20Jacob%20Ganz%20and%20Oriana%20Skylar%20Mastro%20and%20Jacquelyn%20Schneider%20and%20Harold%20Trinkunas&entry.1292438233=%20%20To%20some%2C%20the%20advent%20of%20artificial%20intelligence%20%28AI%29%20promises%20better%0Adecision-making%20and%20increased%20military%20effectiveness%20while%20reducing%20the%0Ainfluence%20of%20human%20error%20and%20emotions.%20However%2C%20there%20is%20still%20debate%20about%20how%0AAI%20systems%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20behave%20compared%20to%20humans%0Ain%20high-stakes%20military%20decision-making%20scenarios%20with%20the%20potential%20for%0Aincreased%20risks%20towards%20escalation%20and%20unnecessary%20conflicts.%20To%20test%20this%0Apotential%20and%20scrutinize%20the%20use%20of%20LLMs%20for%20such%20purposes%2C%20we%20use%20a%20new%0Awargame%20experiment%20with%20107%20national%20security%20experts%20designed%20to%20look%20at%0Acrisis%20escalation%20in%20a%20fictional%20US-China%20scenario%20and%20compare%20human%20players%20to%0ALLM-simulated%20responses%20in%20separate%20simulations.%20Wargames%20have%20a%20long%20history%0Ain%20the%20development%20of%20military%20strategy%20and%20the%20response%20of%20nations%20to%20threats%0Aor%20attacks.%20Here%2C%20we%20show%20a%20considerable%20high-level%20agreement%20in%20the%20LLM%20and%0Ahuman%20responses%20and%20significant%20quantitative%20and%20qualitative%20differences%20in%0Aindividual%20actions%20and%20strategic%20tendencies.%20These%20differences%20depend%20on%0Aintrinsic%20biases%20in%20LLMs%20regarding%20the%20appropriate%20level%20of%20violence%20following%0Astrategic%20instructions%2C%20the%20choice%20of%20LLM%2C%20and%20whether%20the%20LLMs%20are%20tasked%20to%0Adecide%20for%20a%20team%20of%20players%20directly%20or%20first%20to%20simulate%20dialog%20between%0Aplayers.%20When%20simulating%20the%20dialog%2C%20the%20discussions%20lack%20quality%20and%20maintain%0Aa%20farcical%20harmony.%20The%20LLM%20simulations%20cannot%20account%20for%20human%20player%0Acharacteristics%2C%20showing%20no%20significant%20difference%20even%20for%20extreme%20traits%2C%0Asuch%20as%20%22pacifist%22%20or%20%22aggressive%20sociopath%22.%20Our%20results%20motivate%20policymakers%0Ato%20be%20cautious%20before%20granting%20autonomy%20or%20following%20AI-based%20strategy%0Arecommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03407v2&entry.124074799=Read"},
{"title": "FAdam: Adam is a natural gradient optimizer using diagonal empirical\n  Fisher information", "author": "Dongseong Hwang", "abstract": "  This paper establishes a mathematical foundation for the Adam optimizer,\nelucidating its connection to natural gradient descent through Riemannian and\ninformation geometry. We rigorously analyze the diagonal empirical Fisher\ninformation matrix (FIM) in Adam, clarifying all detailed approximations and\nadvocating for the use of log probability functions as loss, which should be\nbased on discrete distributions, due to the limitations of empirical FIM. Our\nanalysis uncovers flaws in the original Adam algorithm, leading to proposed\ncorrections such as enhanced momentum calculations, adjusted bias corrections,\nadaptive epsilon, and gradient clipping. We refine the weight decay term based\non our theoretical framework. Our modified algorithm, Fisher Adam (FAdam),\ndemonstrates superior performance across diverse domains including LLM, ASR,\nand VQ-VAE, achieving state-of-the-art results in ASR.\n", "link": "http://arxiv.org/abs/2405.12807v5", "date": "2024-06-03", "relevancy": 1.39, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4674}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4642}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information&body=Title%3A%20FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information%0AAuthor%3A%20Dongseong%20Hwang%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20mathematical%20foundation%20for%20the%20Adam%20optimizer%2C%0Aelucidating%20its%20connection%20to%20natural%20gradient%20descent%20through%20Riemannian%20and%0Ainformation%20geometry.%20We%20rigorously%20analyze%20the%20diagonal%20empirical%20Fisher%0Ainformation%20matrix%20%28FIM%29%20in%20Adam%2C%20clarifying%20all%20detailed%20approximations%20and%0Aadvocating%20for%20the%20use%20of%20log%20probability%20functions%20as%20loss%2C%20which%20should%20be%0Abased%20on%20discrete%20distributions%2C%20due%20to%20the%20limitations%20of%20empirical%20FIM.%20Our%0Aanalysis%20uncovers%20flaws%20in%20the%20original%20Adam%20algorithm%2C%20leading%20to%20proposed%0Acorrections%20such%20as%20enhanced%20momentum%20calculations%2C%20adjusted%20bias%20corrections%2C%0Aadaptive%20epsilon%2C%20and%20gradient%20clipping.%20We%20refine%20the%20weight%20decay%20term%20based%0Aon%20our%20theoretical%20framework.%20Our%20modified%20algorithm%2C%20Fisher%20Adam%20%28FAdam%29%2C%0Ademonstrates%20superior%20performance%20across%20diverse%20domains%20including%20LLM%2C%20ASR%2C%0Aand%20VQ-VAE%2C%20achieving%20state-of-the-art%20results%20in%20ASR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12807v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAdam%253A%2520Adam%2520is%2520a%2520natural%2520gradient%2520optimizer%2520using%2520diagonal%2520empirical%250A%2520%2520Fisher%2520information%26entry.906535625%3DDongseong%2520Hwang%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520mathematical%2520foundation%2520for%2520the%2520Adam%2520optimizer%252C%250Aelucidating%2520its%2520connection%2520to%2520natural%2520gradient%2520descent%2520through%2520Riemannian%2520and%250Ainformation%2520geometry.%2520We%2520rigorously%2520analyze%2520the%2520diagonal%2520empirical%2520Fisher%250Ainformation%2520matrix%2520%2528FIM%2529%2520in%2520Adam%252C%2520clarifying%2520all%2520detailed%2520approximations%2520and%250Aadvocating%2520for%2520the%2520use%2520of%2520log%2520probability%2520functions%2520as%2520loss%252C%2520which%2520should%2520be%250Abased%2520on%2520discrete%2520distributions%252C%2520due%2520to%2520the%2520limitations%2520of%2520empirical%2520FIM.%2520Our%250Aanalysis%2520uncovers%2520flaws%2520in%2520the%2520original%2520Adam%2520algorithm%252C%2520leading%2520to%2520proposed%250Acorrections%2520such%2520as%2520enhanced%2520momentum%2520calculations%252C%2520adjusted%2520bias%2520corrections%252C%250Aadaptive%2520epsilon%252C%2520and%2520gradient%2520clipping.%2520We%2520refine%2520the%2520weight%2520decay%2520term%2520based%250Aon%2520our%2520theoretical%2520framework.%2520Our%2520modified%2520algorithm%252C%2520Fisher%2520Adam%2520%2528FAdam%2529%252C%250Ademonstrates%2520superior%2520performance%2520across%2520diverse%2520domains%2520including%2520LLM%252C%2520ASR%252C%250Aand%2520VQ-VAE%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520ASR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12807v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information&entry.906535625=Dongseong%20Hwang&entry.1292438233=%20%20This%20paper%20establishes%20a%20mathematical%20foundation%20for%20the%20Adam%20optimizer%2C%0Aelucidating%20its%20connection%20to%20natural%20gradient%20descent%20through%20Riemannian%20and%0Ainformation%20geometry.%20We%20rigorously%20analyze%20the%20diagonal%20empirical%20Fisher%0Ainformation%20matrix%20%28FIM%29%20in%20Adam%2C%20clarifying%20all%20detailed%20approximations%20and%0Aadvocating%20for%20the%20use%20of%20log%20probability%20functions%20as%20loss%2C%20which%20should%20be%0Abased%20on%20discrete%20distributions%2C%20due%20to%20the%20limitations%20of%20empirical%20FIM.%20Our%0Aanalysis%20uncovers%20flaws%20in%20the%20original%20Adam%20algorithm%2C%20leading%20to%20proposed%0Acorrections%20such%20as%20enhanced%20momentum%20calculations%2C%20adjusted%20bias%20corrections%2C%0Aadaptive%20epsilon%2C%20and%20gradient%20clipping.%20We%20refine%20the%20weight%20decay%20term%20based%0Aon%20our%20theoretical%20framework.%20Our%20modified%20algorithm%2C%20Fisher%20Adam%20%28FAdam%29%2C%0Ademonstrates%20superior%20performance%20across%20diverse%20domains%20including%20LLM%2C%20ASR%2C%0Aand%20VQ-VAE%2C%20achieving%20state-of-the-art%20results%20in%20ASR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12807v5&entry.124074799=Read"},
{"title": "Predictive Coding beyond Correlations", "author": "Tommaso Salvatori and Luca Pinchetti and Amine M'Charrak and Beren Millidge and Thomas Lukasiewicz", "abstract": "  Recently, there has been extensive research on the capabilities of\nbiologically plausible algorithms. In this work, we show how one of such\nalgorithms, called predictive coding, is able to perform causal inference\ntasks. First, we show how a simple change in the inference process of\npredictive coding enables to compute interventions without the need to mutilate\nor redefine a causal graph. Then, we explore applications in cases where the\ngraph is unknown, and has to be inferred from observational data. Empirically,\nwe show how such findings can be used to improve the performance of predictive\ncoding in image classification tasks, and conclude that such models are able to\nperform simple end-to-end causal inference tasks.\n", "link": "http://arxiv.org/abs/2306.15479v2", "date": "2024-06-03", "relevancy": 1.3877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4445}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Coding%20beyond%20Correlations&body=Title%3A%20Predictive%20Coding%20beyond%20Correlations%0AAuthor%3A%20Tommaso%20Salvatori%20and%20Luca%20Pinchetti%20and%20Amine%20M%27Charrak%20and%20Beren%20Millidge%20and%20Thomas%20Lukasiewicz%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20extensive%20research%20on%20the%20capabilities%20of%0Abiologically%20plausible%20algorithms.%20In%20this%20work%2C%20we%20show%20how%20one%20of%20such%0Aalgorithms%2C%20called%20predictive%20coding%2C%20is%20able%20to%20perform%20causal%20inference%0Atasks.%20First%2C%20we%20show%20how%20a%20simple%20change%20in%20the%20inference%20process%20of%0Apredictive%20coding%20enables%20to%20compute%20interventions%20without%20the%20need%20to%20mutilate%0Aor%20redefine%20a%20causal%20graph.%20Then%2C%20we%20explore%20applications%20in%20cases%20where%20the%0Agraph%20is%20unknown%2C%20and%20has%20to%20be%20inferred%20from%20observational%20data.%20Empirically%2C%0Awe%20show%20how%20such%20findings%20can%20be%20used%20to%20improve%20the%20performance%20of%20predictive%0Acoding%20in%20image%20classification%20tasks%2C%20and%20conclude%20that%20such%20models%20are%20able%20to%0Aperform%20simple%20end-to-end%20causal%20inference%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Coding%2520beyond%2520Correlations%26entry.906535625%3DTommaso%2520Salvatori%2520and%2520Luca%2520Pinchetti%2520and%2520Amine%2520M%2527Charrak%2520and%2520Beren%2520Millidge%2520and%2520Thomas%2520Lukasiewicz%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520extensive%2520research%2520on%2520the%2520capabilities%2520of%250Abiologically%2520plausible%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520show%2520how%2520one%2520of%2520such%250Aalgorithms%252C%2520called%2520predictive%2520coding%252C%2520is%2520able%2520to%2520perform%2520causal%2520inference%250Atasks.%2520First%252C%2520we%2520show%2520how%2520a%2520simple%2520change%2520in%2520the%2520inference%2520process%2520of%250Apredictive%2520coding%2520enables%2520to%2520compute%2520interventions%2520without%2520the%2520need%2520to%2520mutilate%250Aor%2520redefine%2520a%2520causal%2520graph.%2520Then%252C%2520we%2520explore%2520applications%2520in%2520cases%2520where%2520the%250Agraph%2520is%2520unknown%252C%2520and%2520has%2520to%2520be%2520inferred%2520from%2520observational%2520data.%2520Empirically%252C%250Awe%2520show%2520how%2520such%2520findings%2520can%2520be%2520used%2520to%2520improve%2520the%2520performance%2520of%2520predictive%250Acoding%2520in%2520image%2520classification%2520tasks%252C%2520and%2520conclude%2520that%2520such%2520models%2520are%2520able%2520to%250Aperform%2520simple%2520end-to-end%2520causal%2520inference%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Coding%20beyond%20Correlations&entry.906535625=Tommaso%20Salvatori%20and%20Luca%20Pinchetti%20and%20Amine%20M%27Charrak%20and%20Beren%20Millidge%20and%20Thomas%20Lukasiewicz&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20extensive%20research%20on%20the%20capabilities%20of%0Abiologically%20plausible%20algorithms.%20In%20this%20work%2C%20we%20show%20how%20one%20of%20such%0Aalgorithms%2C%20called%20predictive%20coding%2C%20is%20able%20to%20perform%20causal%20inference%0Atasks.%20First%2C%20we%20show%20how%20a%20simple%20change%20in%20the%20inference%20process%20of%0Apredictive%20coding%20enables%20to%20compute%20interventions%20without%20the%20need%20to%20mutilate%0Aor%20redefine%20a%20causal%20graph.%20Then%2C%20we%20explore%20applications%20in%20cases%20where%20the%0Agraph%20is%20unknown%2C%20and%20has%20to%20be%20inferred%20from%20observational%20data.%20Empirically%2C%0Awe%20show%20how%20such%20findings%20can%20be%20used%20to%20improve%20the%20performance%20of%20predictive%0Acoding%20in%20image%20classification%20tasks%2C%20and%20conclude%20that%20such%20models%20are%20able%20to%0Aperform%20simple%20end-to-end%20causal%20inference%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15479v2&entry.124074799=Read"},
{"title": "Estimating the normal-inverse-Wishart distribution", "author": "Jonathan So", "abstract": "  The normal-inverse-Wishart (NIW) distribution is commonly used as a prior\ndistribution for the mean and covariance parameters of a multivariate normal\ndistribution. The family of NIW distributions is also a minimal exponential\nfamily. In this short note we describe a convergent procedure for converting\nfrom mean parameters to natural parameters in the NIW family, or --\nequivalently -- for performing maximum likelihood estimation of the natural\nparameters given observed sufficient statistics. This is needed, for example,\nwhen using a NIW base family in expectation propagation.\n", "link": "http://arxiv.org/abs/2405.16088v2", "date": "2024-06-03", "relevancy": 0.9978, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3375}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3335}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20normal-inverse-Wishart%20distribution&body=Title%3A%20Estimating%20the%20normal-inverse-Wishart%20distribution%0AAuthor%3A%20Jonathan%20So%0AAbstract%3A%20%20%20The%20normal-inverse-Wishart%20%28NIW%29%20distribution%20is%20commonly%20used%20as%20a%20prior%0Adistribution%20for%20the%20mean%20and%20covariance%20parameters%20of%20a%20multivariate%20normal%0Adistribution.%20The%20family%20of%20NIW%20distributions%20is%20also%20a%20minimal%20exponential%0Afamily.%20In%20this%20short%20note%20we%20describe%20a%20convergent%20procedure%20for%20converting%0Afrom%20mean%20parameters%20to%20natural%20parameters%20in%20the%20NIW%20family%2C%20or%20--%0Aequivalently%20--%20for%20performing%20maximum%20likelihood%20estimation%20of%20the%20natural%0Aparameters%20given%20observed%20sufficient%20statistics.%20This%20is%20needed%2C%20for%20example%2C%0Awhen%20using%20a%20NIW%20base%20family%20in%20expectation%20propagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520normal-inverse-Wishart%2520distribution%26entry.906535625%3DJonathan%2520So%26entry.1292438233%3D%2520%2520The%2520normal-inverse-Wishart%2520%2528NIW%2529%2520distribution%2520is%2520commonly%2520used%2520as%2520a%2520prior%250Adistribution%2520for%2520the%2520mean%2520and%2520covariance%2520parameters%2520of%2520a%2520multivariate%2520normal%250Adistribution.%2520The%2520family%2520of%2520NIW%2520distributions%2520is%2520also%2520a%2520minimal%2520exponential%250Afamily.%2520In%2520this%2520short%2520note%2520we%2520describe%2520a%2520convergent%2520procedure%2520for%2520converting%250Afrom%2520mean%2520parameters%2520to%2520natural%2520parameters%2520in%2520the%2520NIW%2520family%252C%2520or%2520--%250Aequivalently%2520--%2520for%2520performing%2520maximum%2520likelihood%2520estimation%2520of%2520the%2520natural%250Aparameters%2520given%2520observed%2520sufficient%2520statistics.%2520This%2520is%2520needed%252C%2520for%2520example%252C%250Awhen%2520using%2520a%2520NIW%2520base%2520family%2520in%2520expectation%2520propagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20normal-inverse-Wishart%20distribution&entry.906535625=Jonathan%20So&entry.1292438233=%20%20The%20normal-inverse-Wishart%20%28NIW%29%20distribution%20is%20commonly%20used%20as%20a%20prior%0Adistribution%20for%20the%20mean%20and%20covariance%20parameters%20of%20a%20multivariate%20normal%0Adistribution.%20The%20family%20of%20NIW%20distributions%20is%20also%20a%20minimal%20exponential%0Afamily.%20In%20this%20short%20note%20we%20describe%20a%20convergent%20procedure%20for%20converting%0Afrom%20mean%20parameters%20to%20natural%20parameters%20in%20the%20NIW%20family%2C%20or%20--%0Aequivalently%20--%20for%20performing%20maximum%20likelihood%20estimation%20of%20the%20natural%0Aparameters%20given%20observed%20sufficient%20statistics.%20This%20is%20needed%2C%20for%20example%2C%0Awhen%20using%20a%20NIW%20base%20family%20in%20expectation%20propagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16088v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


