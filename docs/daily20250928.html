<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250925.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "REArtGS: Reconstructing and Generating Articulated Objects via 3D\n  Gaussian Splatting with Geometric and Motion Constraints", "author": "Di Wu and Liu Liu and Zhou Linli and Anran Huang and Liangtu Song and Qiaojun Yu and Qi Wu and Cewu Lu", "abstract": "  Articulated objects, as prevalent entities in human life, their 3D\nrepresentations play crucial roles across various applications. However,\nachieving both high-fidelity textured surface reconstruction and dynamic\ngeneration for articulated objects remains challenging for existing methods. In\nthis paper, we present REArtGS, a novel framework that introduces additional\ngeometric and motion constraints to 3D Gaussian primitives, enabling realistic\nsurface reconstruction and generation for articulated objects. Specifically,\ngiven multi-view RGB images of arbitrary two states of articulated objects, we\nfirst introduce an unbiased Signed Distance Field (SDF) guidance to regularize\nGaussian opacity fields, enhancing geometry constraints and improving surface\nreconstruction quality. Then we establish deformable fields for 3D Gaussians\nconstrained by the kinematic structures of articulated objects, achieving\nunsupervised generation of surface meshes in unseen states. Extensive\nexperiments on both synthetic and real datasets demonstrate our approach\nachieves high-quality textured surface reconstruction for given states, and\nenables high-fidelity surface generation for unseen states. Project site:\nhttps://sites.google.com/view/reartgs/home.\n", "link": "http://arxiv.org/abs/2503.06677v4", "date": "2025-09-25", "relevancy": 3.5489, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7168}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7075}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%0A%20%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints&body=Title%3A%20REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%0A%20%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints%0AAuthor%3A%20Di%20Wu%20and%20Liu%20Liu%20and%20Zhou%20Linli%20and%20Anran%20Huang%20and%20Liangtu%20Song%20and%20Qiaojun%20Yu%20and%20Qi%20Wu%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Articulated%20objects%2C%20as%20prevalent%20entities%20in%20human%20life%2C%20their%203D%0Arepresentations%20play%20crucial%20roles%20across%20various%20applications.%20However%2C%0Aachieving%20both%20high-fidelity%20textured%20surface%20reconstruction%20and%20dynamic%0Ageneration%20for%20articulated%20objects%20remains%20challenging%20for%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20REArtGS%2C%20a%20novel%20framework%20that%20introduces%20additional%0Ageometric%20and%20motion%20constraints%20to%203D%20Gaussian%20primitives%2C%20enabling%20realistic%0Asurface%20reconstruction%20and%20generation%20for%20articulated%20objects.%20Specifically%2C%0Agiven%20multi-view%20RGB%20images%20of%20arbitrary%20two%20states%20of%20articulated%20objects%2C%20we%0Afirst%20introduce%20an%20unbiased%20Signed%20Distance%20Field%20%28SDF%29%20guidance%20to%20regularize%0AGaussian%20opacity%20fields%2C%20enhancing%20geometry%20constraints%20and%20improving%20surface%0Areconstruction%20quality.%20Then%20we%20establish%20deformable%20fields%20for%203D%20Gaussians%0Aconstrained%20by%20the%20kinematic%20structures%20of%20articulated%20objects%2C%20achieving%0Aunsupervised%20generation%20of%20surface%20meshes%20in%20unseen%20states.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20our%20approach%0Aachieves%20high-quality%20textured%20surface%20reconstruction%20for%20given%20states%2C%20and%0Aenables%20high-fidelity%20surface%20generation%20for%20unseen%20states.%20Project%20site%3A%0Ahttps%3A//sites.google.com/view/reartgs/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06677v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREArtGS%253A%2520Reconstructing%2520and%2520Generating%2520Articulated%2520Objects%2520via%25203D%250A%2520%2520Gaussian%2520Splatting%2520with%2520Geometric%2520and%2520Motion%2520Constraints%26entry.906535625%3DDi%2520Wu%2520and%2520Liu%2520Liu%2520and%2520Zhou%2520Linli%2520and%2520Anran%2520Huang%2520and%2520Liangtu%2520Song%2520and%2520Qiaojun%2520Yu%2520and%2520Qi%2520Wu%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Articulated%2520objects%252C%2520as%2520prevalent%2520entities%2520in%2520human%2520life%252C%2520their%25203D%250Arepresentations%2520play%2520crucial%2520roles%2520across%2520various%2520applications.%2520However%252C%250Aachieving%2520both%2520high-fidelity%2520textured%2520surface%2520reconstruction%2520and%2520dynamic%250Ageneration%2520for%2520articulated%2520objects%2520remains%2520challenging%2520for%2520existing%2520methods.%2520In%250Athis%2520paper%252C%2520we%2520present%2520REArtGS%252C%2520a%2520novel%2520framework%2520that%2520introduces%2520additional%250Ageometric%2520and%2520motion%2520constraints%2520to%25203D%2520Gaussian%2520primitives%252C%2520enabling%2520realistic%250Asurface%2520reconstruction%2520and%2520generation%2520for%2520articulated%2520objects.%2520Specifically%252C%250Agiven%2520multi-view%2520RGB%2520images%2520of%2520arbitrary%2520two%2520states%2520of%2520articulated%2520objects%252C%2520we%250Afirst%2520introduce%2520an%2520unbiased%2520Signed%2520Distance%2520Field%2520%2528SDF%2529%2520guidance%2520to%2520regularize%250AGaussian%2520opacity%2520fields%252C%2520enhancing%2520geometry%2520constraints%2520and%2520improving%2520surface%250Areconstruction%2520quality.%2520Then%2520we%2520establish%2520deformable%2520fields%2520for%25203D%2520Gaussians%250Aconstrained%2520by%2520the%2520kinematic%2520structures%2520of%2520articulated%2520objects%252C%2520achieving%250Aunsupervised%2520generation%2520of%2520surface%2520meshes%2520in%2520unseen%2520states.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520our%2520approach%250Aachieves%2520high-quality%2520textured%2520surface%2520reconstruction%2520for%2520given%2520states%252C%2520and%250Aenables%2520high-fidelity%2520surface%2520generation%2520for%2520unseen%2520states.%2520Project%2520site%253A%250Ahttps%253A//sites.google.com/view/reartgs/home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06677v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%0A%20%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints&entry.906535625=Di%20Wu%20and%20Liu%20Liu%20and%20Zhou%20Linli%20and%20Anran%20Huang%20and%20Liangtu%20Song%20and%20Qiaojun%20Yu%20and%20Qi%20Wu%20and%20Cewu%20Lu&entry.1292438233=%20%20Articulated%20objects%2C%20as%20prevalent%20entities%20in%20human%20life%2C%20their%203D%0Arepresentations%20play%20crucial%20roles%20across%20various%20applications.%20However%2C%0Aachieving%20both%20high-fidelity%20textured%20surface%20reconstruction%20and%20dynamic%0Ageneration%20for%20articulated%20objects%20remains%20challenging%20for%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20REArtGS%2C%20a%20novel%20framework%20that%20introduces%20additional%0Ageometric%20and%20motion%20constraints%20to%203D%20Gaussian%20primitives%2C%20enabling%20realistic%0Asurface%20reconstruction%20and%20generation%20for%20articulated%20objects.%20Specifically%2C%0Agiven%20multi-view%20RGB%20images%20of%20arbitrary%20two%20states%20of%20articulated%20objects%2C%20we%0Afirst%20introduce%20an%20unbiased%20Signed%20Distance%20Field%20%28SDF%29%20guidance%20to%20regularize%0AGaussian%20opacity%20fields%2C%20enhancing%20geometry%20constraints%20and%20improving%20surface%0Areconstruction%20quality.%20Then%20we%20establish%20deformable%20fields%20for%203D%20Gaussians%0Aconstrained%20by%20the%20kinematic%20structures%20of%20articulated%20objects%2C%20achieving%0Aunsupervised%20generation%20of%20surface%20meshes%20in%20unseen%20states.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20our%20approach%0Aachieves%20high-quality%20textured%20surface%20reconstruction%20for%20given%20states%2C%20and%0Aenables%20high-fidelity%20surface%20generation%20for%20unseen%20states.%20Project%20site%3A%0Ahttps%3A//sites.google.com/view/reartgs/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06677v4&entry.124074799=Read"},
{"title": "NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation\n  via Neural Newtonian Dynamics", "author": "Yu Yuan and Xijun Wang and Tharindu Wickremasinghe and Zeeshan Nadir and Bole Ma and Stanley H. Chan", "abstract": "  A primary bottleneck in large-scale text-to-video generation today is\nphysical consistency and controllability. Despite recent advances,\nstate-of-the-art models often produce unrealistic motions, such as objects\nfalling upward, or abrupt changes in velocity and direction. Moreover, these\nmodels lack precise parameter control, struggling to generate physically\nconsistent dynamics under different initial conditions. We argue that this\nfundamental limitation stems from current models learning motion distributions\nsolely from appearance, while lacking an understanding of the underlying\ndynamics. In this work, we propose NewtonGen, a framework that integrates\ndata-driven synthesis with learnable physical principles. At its core lies\ntrainable Neural Newtonian Dynamics (NND), which can model and predict a\nvariety of Newtonian motions, thereby injecting latent dynamical constraints\ninto the video generation process. By jointly leveraging data priors and\ndynamical guidance, NewtonGen enables physically consistent video synthesis\nwith precise parameter control.\n", "link": "http://arxiv.org/abs/2509.21309v1", "date": "2025-09-25", "relevancy": 3.259, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7235}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6307}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NewtonGen%3A%20Physics-Consistent%20and%20Controllable%20Text-to-Video%20Generation%0A%20%20via%20Neural%20Newtonian%20Dynamics&body=Title%3A%20NewtonGen%3A%20Physics-Consistent%20and%20Controllable%20Text-to-Video%20Generation%0A%20%20via%20Neural%20Newtonian%20Dynamics%0AAuthor%3A%20Yu%20Yuan%20and%20Xijun%20Wang%20and%20Tharindu%20Wickremasinghe%20and%20Zeeshan%20Nadir%20and%20Bole%20Ma%20and%20Stanley%20H.%20Chan%0AAbstract%3A%20%20%20A%20primary%20bottleneck%20in%20large-scale%20text-to-video%20generation%20today%20is%0Aphysical%20consistency%20and%20controllability.%20Despite%20recent%20advances%2C%0Astate-of-the-art%20models%20often%20produce%20unrealistic%20motions%2C%20such%20as%20objects%0Afalling%20upward%2C%20or%20abrupt%20changes%20in%20velocity%20and%20direction.%20Moreover%2C%20these%0Amodels%20lack%20precise%20parameter%20control%2C%20struggling%20to%20generate%20physically%0Aconsistent%20dynamics%20under%20different%20initial%20conditions.%20We%20argue%20that%20this%0Afundamental%20limitation%20stems%20from%20current%20models%20learning%20motion%20distributions%0Asolely%20from%20appearance%2C%20while%20lacking%20an%20understanding%20of%20the%20underlying%0Adynamics.%20In%20this%20work%2C%20we%20propose%20NewtonGen%2C%20a%20framework%20that%20integrates%0Adata-driven%20synthesis%20with%20learnable%20physical%20principles.%20At%20its%20core%20lies%0Atrainable%20Neural%20Newtonian%20Dynamics%20%28NND%29%2C%20which%20can%20model%20and%20predict%20a%0Avariety%20of%20Newtonian%20motions%2C%20thereby%20injecting%20latent%20dynamical%20constraints%0Ainto%20the%20video%20generation%20process.%20By%20jointly%20leveraging%20data%20priors%20and%0Adynamical%20guidance%2C%20NewtonGen%20enables%20physically%20consistent%20video%20synthesis%0Awith%20precise%20parameter%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNewtonGen%253A%2520Physics-Consistent%2520and%2520Controllable%2520Text-to-Video%2520Generation%250A%2520%2520via%2520Neural%2520Newtonian%2520Dynamics%26entry.906535625%3DYu%2520Yuan%2520and%2520Xijun%2520Wang%2520and%2520Tharindu%2520Wickremasinghe%2520and%2520Zeeshan%2520Nadir%2520and%2520Bole%2520Ma%2520and%2520Stanley%2520H.%2520Chan%26entry.1292438233%3D%2520%2520A%2520primary%2520bottleneck%2520in%2520large-scale%2520text-to-video%2520generation%2520today%2520is%250Aphysical%2520consistency%2520and%2520controllability.%2520Despite%2520recent%2520advances%252C%250Astate-of-the-art%2520models%2520often%2520produce%2520unrealistic%2520motions%252C%2520such%2520as%2520objects%250Afalling%2520upward%252C%2520or%2520abrupt%2520changes%2520in%2520velocity%2520and%2520direction.%2520Moreover%252C%2520these%250Amodels%2520lack%2520precise%2520parameter%2520control%252C%2520struggling%2520to%2520generate%2520physically%250Aconsistent%2520dynamics%2520under%2520different%2520initial%2520conditions.%2520We%2520argue%2520that%2520this%250Afundamental%2520limitation%2520stems%2520from%2520current%2520models%2520learning%2520motion%2520distributions%250Asolely%2520from%2520appearance%252C%2520while%2520lacking%2520an%2520understanding%2520of%2520the%2520underlying%250Adynamics.%2520In%2520this%2520work%252C%2520we%2520propose%2520NewtonGen%252C%2520a%2520framework%2520that%2520integrates%250Adata-driven%2520synthesis%2520with%2520learnable%2520physical%2520principles.%2520At%2520its%2520core%2520lies%250Atrainable%2520Neural%2520Newtonian%2520Dynamics%2520%2528NND%2529%252C%2520which%2520can%2520model%2520and%2520predict%2520a%250Avariety%2520of%2520Newtonian%2520motions%252C%2520thereby%2520injecting%2520latent%2520dynamical%2520constraints%250Ainto%2520the%2520video%2520generation%2520process.%2520By%2520jointly%2520leveraging%2520data%2520priors%2520and%250Adynamical%2520guidance%252C%2520NewtonGen%2520enables%2520physically%2520consistent%2520video%2520synthesis%250Awith%2520precise%2520parameter%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NewtonGen%3A%20Physics-Consistent%20and%20Controllable%20Text-to-Video%20Generation%0A%20%20via%20Neural%20Newtonian%20Dynamics&entry.906535625=Yu%20Yuan%20and%20Xijun%20Wang%20and%20Tharindu%20Wickremasinghe%20and%20Zeeshan%20Nadir%20and%20Bole%20Ma%20and%20Stanley%20H.%20Chan&entry.1292438233=%20%20A%20primary%20bottleneck%20in%20large-scale%20text-to-video%20generation%20today%20is%0Aphysical%20consistency%20and%20controllability.%20Despite%20recent%20advances%2C%0Astate-of-the-art%20models%20often%20produce%20unrealistic%20motions%2C%20such%20as%20objects%0Afalling%20upward%2C%20or%20abrupt%20changes%20in%20velocity%20and%20direction.%20Moreover%2C%20these%0Amodels%20lack%20precise%20parameter%20control%2C%20struggling%20to%20generate%20physically%0Aconsistent%20dynamics%20under%20different%20initial%20conditions.%20We%20argue%20that%20this%0Afundamental%20limitation%20stems%20from%20current%20models%20learning%20motion%20distributions%0Asolely%20from%20appearance%2C%20while%20lacking%20an%20understanding%20of%20the%20underlying%0Adynamics.%20In%20this%20work%2C%20we%20propose%20NewtonGen%2C%20a%20framework%20that%20integrates%0Adata-driven%20synthesis%20with%20learnable%20physical%20principles.%20At%20its%20core%20lies%0Atrainable%20Neural%20Newtonian%20Dynamics%20%28NND%29%2C%20which%20can%20model%20and%20predict%20a%0Avariety%20of%20Newtonian%20motions%2C%20thereby%20injecting%20latent%20dynamical%20constraints%0Ainto%20the%20video%20generation%20process.%20By%20jointly%20leveraging%20data%20priors%20and%0Adynamical%20guidance%2C%20NewtonGen%20enables%20physically%20consistent%20video%20synthesis%0Awith%20precise%20parameter%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21309v1&entry.124074799=Read"},
{"title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question\n  Answering", "author": "Rongtao Xu and Han Gao and Mingming Yu and Dong An and Shunpeng Chen and Changwei Wang and Li Guo and Xiaodan Liang and Shibiao Xu", "abstract": "  With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.\n", "link": "http://arxiv.org/abs/2507.12026v2", "date": "2025-09-25", "relevancy": 3.2479, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-MoRe%3A%20Unified%20Modal-Contextual%20Reasoning%20for%20Embodied%20Question%0A%20%20Answering&body=Title%3A%203D-MoRe%3A%20Unified%20Modal-Contextual%20Reasoning%20for%20Embodied%20Question%0A%20%20Answering%0AAuthor%3A%20Rongtao%20Xu%20and%20Han%20Gao%20and%20Mingming%20Yu%20and%20Dong%20An%20and%20Shunpeng%20Chen%20and%20Changwei%20Wang%20and%20Li%20Guo%20and%20Xiaodan%20Liang%20and%20Shibiao%20Xu%0AAbstract%3A%20%20%20With%20the%20growing%20need%20for%20diverse%20and%20scalable%20data%20in%20indoor%20scene%20tasks%2C%0Asuch%20as%20question%20answering%20and%20dense%20captioning%2C%20we%20propose%203D-MoRe%2C%20a%20novel%0Aparadigm%20designed%20to%20generate%20large-scale%203D-language%20datasets%20by%20leveraging%0Athe%20strengths%20of%20foundational%20models.%20The%20framework%20integrates%20key%20components%2C%0Aincluding%20multi-modal%20embedding%2C%20cross-modal%20interaction%2C%20and%20a%20language%20model%0Adecoder%2C%20to%20process%20natural%20language%20instructions%20and%203D%20scene%20data.%20This%0Aapproach%20facilitates%20enhanced%20reasoning%20and%20response%20generation%20in%20complex%203D%0Aenvironments.%20Using%20the%20ScanNet%203D%20scene%20dataset%2C%20along%20with%20text%20annotations%0Afrom%20ScanQA%20and%20ScanRefer%2C%203D-MoRe%20generates%2062%2C000%20question-answer%20%28QA%29%20pairs%0Aand%2073%2C000%20object%20descriptions%20across%201%2C513%20scenes.%20We%20also%20employ%20various%20data%0Aaugmentation%20techniques%20and%20implement%20semantic%20filtering%20to%20ensure%20high-quality%0Adata.%20Experiments%20on%20ScanQA%20demonstrate%20that%203D-MoRe%20significantly%20outperforms%0Astate-of-the-art%20baselines%2C%20with%20the%20CIDEr%20score%20improving%20by%202.15%5C%25.%0ASimilarly%2C%20on%20ScanRefer%2C%20our%20approach%20achieves%20a%20notable%20increase%20in%20CIDEr%400.5%0Aby%201.84%5C%25%2C%20highlighting%20its%20effectiveness%20in%20both%20tasks.%20Our%20code%20and%20generated%0Adatasets%20will%20be%20publicly%20released%20to%20benefit%20the%20community%2C%20and%20both%20can%20be%0Aaccessed%20on%20the%20https%3A//3D-MoRe.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-MoRe%253A%2520Unified%2520Modal-Contextual%2520Reasoning%2520for%2520Embodied%2520Question%250A%2520%2520Answering%26entry.906535625%3DRongtao%2520Xu%2520and%2520Han%2520Gao%2520and%2520Mingming%2520Yu%2520and%2520Dong%2520An%2520and%2520Shunpeng%2520Chen%2520and%2520Changwei%2520Wang%2520and%2520Li%2520Guo%2520and%2520Xiaodan%2520Liang%2520and%2520Shibiao%2520Xu%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520need%2520for%2520diverse%2520and%2520scalable%2520data%2520in%2520indoor%2520scene%2520tasks%252C%250Asuch%2520as%2520question%2520answering%2520and%2520dense%2520captioning%252C%2520we%2520propose%25203D-MoRe%252C%2520a%2520novel%250Aparadigm%2520designed%2520to%2520generate%2520large-scale%25203D-language%2520datasets%2520by%2520leveraging%250Athe%2520strengths%2520of%2520foundational%2520models.%2520The%2520framework%2520integrates%2520key%2520components%252C%250Aincluding%2520multi-modal%2520embedding%252C%2520cross-modal%2520interaction%252C%2520and%2520a%2520language%2520model%250Adecoder%252C%2520to%2520process%2520natural%2520language%2520instructions%2520and%25203D%2520scene%2520data.%2520This%250Aapproach%2520facilitates%2520enhanced%2520reasoning%2520and%2520response%2520generation%2520in%2520complex%25203D%250Aenvironments.%2520Using%2520the%2520ScanNet%25203D%2520scene%2520dataset%252C%2520along%2520with%2520text%2520annotations%250Afrom%2520ScanQA%2520and%2520ScanRefer%252C%25203D-MoRe%2520generates%252062%252C000%2520question-answer%2520%2528QA%2529%2520pairs%250Aand%252073%252C000%2520object%2520descriptions%2520across%25201%252C513%2520scenes.%2520We%2520also%2520employ%2520various%2520data%250Aaugmentation%2520techniques%2520and%2520implement%2520semantic%2520filtering%2520to%2520ensure%2520high-quality%250Adata.%2520Experiments%2520on%2520ScanQA%2520demonstrate%2520that%25203D-MoRe%2520significantly%2520outperforms%250Astate-of-the-art%2520baselines%252C%2520with%2520the%2520CIDEr%2520score%2520improving%2520by%25202.15%255C%2525.%250ASimilarly%252C%2520on%2520ScanRefer%252C%2520our%2520approach%2520achieves%2520a%2520notable%2520increase%2520in%2520CIDEr%25400.5%250Aby%25201.84%255C%2525%252C%2520highlighting%2520its%2520effectiveness%2520in%2520both%2520tasks.%2520Our%2520code%2520and%2520generated%250Adatasets%2520will%2520be%2520publicly%2520released%2520to%2520benefit%2520the%2520community%252C%2520and%2520both%2520can%2520be%250Aaccessed%2520on%2520the%2520https%253A//3D-MoRe.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-MoRe%3A%20Unified%20Modal-Contextual%20Reasoning%20for%20Embodied%20Question%0A%20%20Answering&entry.906535625=Rongtao%20Xu%20and%20Han%20Gao%20and%20Mingming%20Yu%20and%20Dong%20An%20and%20Shunpeng%20Chen%20and%20Changwei%20Wang%20and%20Li%20Guo%20and%20Xiaodan%20Liang%20and%20Shibiao%20Xu&entry.1292438233=%20%20With%20the%20growing%20need%20for%20diverse%20and%20scalable%20data%20in%20indoor%20scene%20tasks%2C%0Asuch%20as%20question%20answering%20and%20dense%20captioning%2C%20we%20propose%203D-MoRe%2C%20a%20novel%0Aparadigm%20designed%20to%20generate%20large-scale%203D-language%20datasets%20by%20leveraging%0Athe%20strengths%20of%20foundational%20models.%20The%20framework%20integrates%20key%20components%2C%0Aincluding%20multi-modal%20embedding%2C%20cross-modal%20interaction%2C%20and%20a%20language%20model%0Adecoder%2C%20to%20process%20natural%20language%20instructions%20and%203D%20scene%20data.%20This%0Aapproach%20facilitates%20enhanced%20reasoning%20and%20response%20generation%20in%20complex%203D%0Aenvironments.%20Using%20the%20ScanNet%203D%20scene%20dataset%2C%20along%20with%20text%20annotations%0Afrom%20ScanQA%20and%20ScanRefer%2C%203D-MoRe%20generates%2062%2C000%20question-answer%20%28QA%29%20pairs%0Aand%2073%2C000%20object%20descriptions%20across%201%2C513%20scenes.%20We%20also%20employ%20various%20data%0Aaugmentation%20techniques%20and%20implement%20semantic%20filtering%20to%20ensure%20high-quality%0Adata.%20Experiments%20on%20ScanQA%20demonstrate%20that%203D-MoRe%20significantly%20outperforms%0Astate-of-the-art%20baselines%2C%20with%20the%20CIDEr%20score%20improving%20by%202.15%5C%25.%0ASimilarly%2C%20on%20ScanRefer%2C%20our%20approach%20achieves%20a%20notable%20increase%20in%20CIDEr%400.5%0Aby%201.84%5C%25%2C%20highlighting%20its%20effectiveness%20in%20both%20tasks.%20Our%20code%20and%20generated%0Adatasets%20will%20be%20publicly%20released%20to%20benefit%20the%20community%2C%20and%20both%20can%20be%0Aaccessed%20on%20the%20https%3A//3D-MoRe.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12026v2&entry.124074799=Read"},
{"title": "Decipher-MR: A Vision-Language Foundation Model for 3D MRI\n  Representations", "author": "Zhijian Yang and Noel DSouza and Istvan Megyeri and Xiaojian Xu and Amin Honarmandi Shandiz and Farzin Haddadpour and Krisztian Koos and Laszlo Rusko and Emanuele Valeriano and Bharadwaj Swaninathan and Lei Wu and Parminder Bhatia and Taha Kass-Hout and Erhan Bas", "abstract": "  Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in\nclinical diagnosis and research, yet its complexity and heterogeneity pose\nchallenges for automated analysis, particularly in scalable and generalizable\nmachine learning applications. While foundation models have revolutionized\nnatural language and vision tasks, their application to MRI remains limited due\nto data scarcity and narrow anatomical focus. In this work, we present\nDecipher-MR, a 3D MRI-specific vision-language foundation model trained on a\nlarge-scale dataset comprising 200,000 MRI series from over 22,000 studies\nspanning diverse anatomical regions, sequences, and pathologies. Decipher-MR\nintegrates self-supervised vision learning with report-guided text supervision\nto build robust, generalizable representations, enabling effective adaptation\nacross broad applications. To enable robust and diverse clinical tasks with\nminimal computational overhead, Decipher-MR supports a modular design that\nenables tuning of lightweight, task-specific decoders attached to a frozen\npretrained encoder. Following this setting, we evaluate Decipher-MR across\ndiverse benchmarks including disease classification, demographic prediction,\nanatomical localization, and cross-modal retrieval, demonstrating consistent\nperformance gains over existing foundation models and task-specific approaches.\nOur results establish Decipher-MR as a scalable and versatile foundation for\nMRI-based AI, facilitating efficient development across clinical and research\ndomains.\n", "link": "http://arxiv.org/abs/2509.21249v1", "date": "2025-09-25", "relevancy": 3.1172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decipher-MR%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20MRI%0A%20%20Representations&body=Title%3A%20Decipher-MR%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20MRI%0A%20%20Representations%0AAuthor%3A%20Zhijian%20Yang%20and%20Noel%20DSouza%20and%20Istvan%20Megyeri%20and%20Xiaojian%20Xu%20and%20Amin%20Honarmandi%20Shandiz%20and%20Farzin%20Haddadpour%20and%20Krisztian%20Koos%20and%20Laszlo%20Rusko%20and%20Emanuele%20Valeriano%20and%20Bharadwaj%20Swaninathan%20and%20Lei%20Wu%20and%20Parminder%20Bhatia%20and%20Taha%20Kass-Hout%20and%20Erhan%20Bas%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20critical%20medical%20imaging%20modality%20in%0Aclinical%20diagnosis%20and%20research%2C%20yet%20its%20complexity%20and%20heterogeneity%20pose%0Achallenges%20for%20automated%20analysis%2C%20particularly%20in%20scalable%20and%20generalizable%0Amachine%20learning%20applications.%20While%20foundation%20models%20have%20revolutionized%0Anatural%20language%20and%20vision%20tasks%2C%20their%20application%20to%20MRI%20remains%20limited%20due%0Ato%20data%20scarcity%20and%20narrow%20anatomical%20focus.%20In%20this%20work%2C%20we%20present%0ADecipher-MR%2C%20a%203D%20MRI-specific%20vision-language%20foundation%20model%20trained%20on%20a%0Alarge-scale%20dataset%20comprising%20200%2C000%20MRI%20series%20from%20over%2022%2C000%20studies%0Aspanning%20diverse%20anatomical%20regions%2C%20sequences%2C%20and%20pathologies.%20Decipher-MR%0Aintegrates%20self-supervised%20vision%20learning%20with%20report-guided%20text%20supervision%0Ato%20build%20robust%2C%20generalizable%20representations%2C%20enabling%20effective%20adaptation%0Aacross%20broad%20applications.%20To%20enable%20robust%20and%20diverse%20clinical%20tasks%20with%0Aminimal%20computational%20overhead%2C%20Decipher-MR%20supports%20a%20modular%20design%20that%0Aenables%20tuning%20of%20lightweight%2C%20task-specific%20decoders%20attached%20to%20a%20frozen%0Apretrained%20encoder.%20Following%20this%20setting%2C%20we%20evaluate%20Decipher-MR%20across%0Adiverse%20benchmarks%20including%20disease%20classification%2C%20demographic%20prediction%2C%0Aanatomical%20localization%2C%20and%20cross-modal%20retrieval%2C%20demonstrating%20consistent%0Aperformance%20gains%20over%20existing%20foundation%20models%20and%20task-specific%20approaches.%0AOur%20results%20establish%20Decipher-MR%20as%20a%20scalable%20and%20versatile%20foundation%20for%0AMRI-based%20AI%2C%20facilitating%20efficient%20development%20across%20clinical%20and%20research%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecipher-MR%253A%2520A%2520Vision-Language%2520Foundation%2520Model%2520for%25203D%2520MRI%250A%2520%2520Representations%26entry.906535625%3DZhijian%2520Yang%2520and%2520Noel%2520DSouza%2520and%2520Istvan%2520Megyeri%2520and%2520Xiaojian%2520Xu%2520and%2520Amin%2520Honarmandi%2520Shandiz%2520and%2520Farzin%2520Haddadpour%2520and%2520Krisztian%2520Koos%2520and%2520Laszlo%2520Rusko%2520and%2520Emanuele%2520Valeriano%2520and%2520Bharadwaj%2520Swaninathan%2520and%2520Lei%2520Wu%2520and%2520Parminder%2520Bhatia%2520and%2520Taha%2520Kass-Hout%2520and%2520Erhan%2520Bas%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520a%2520critical%2520medical%2520imaging%2520modality%2520in%250Aclinical%2520diagnosis%2520and%2520research%252C%2520yet%2520its%2520complexity%2520and%2520heterogeneity%2520pose%250Achallenges%2520for%2520automated%2520analysis%252C%2520particularly%2520in%2520scalable%2520and%2520generalizable%250Amachine%2520learning%2520applications.%2520While%2520foundation%2520models%2520have%2520revolutionized%250Anatural%2520language%2520and%2520vision%2520tasks%252C%2520their%2520application%2520to%2520MRI%2520remains%2520limited%2520due%250Ato%2520data%2520scarcity%2520and%2520narrow%2520anatomical%2520focus.%2520In%2520this%2520work%252C%2520we%2520present%250ADecipher-MR%252C%2520a%25203D%2520MRI-specific%2520vision-language%2520foundation%2520model%2520trained%2520on%2520a%250Alarge-scale%2520dataset%2520comprising%2520200%252C000%2520MRI%2520series%2520from%2520over%252022%252C000%2520studies%250Aspanning%2520diverse%2520anatomical%2520regions%252C%2520sequences%252C%2520and%2520pathologies.%2520Decipher-MR%250Aintegrates%2520self-supervised%2520vision%2520learning%2520with%2520report-guided%2520text%2520supervision%250Ato%2520build%2520robust%252C%2520generalizable%2520representations%252C%2520enabling%2520effective%2520adaptation%250Aacross%2520broad%2520applications.%2520To%2520enable%2520robust%2520and%2520diverse%2520clinical%2520tasks%2520with%250Aminimal%2520computational%2520overhead%252C%2520Decipher-MR%2520supports%2520a%2520modular%2520design%2520that%250Aenables%2520tuning%2520of%2520lightweight%252C%2520task-specific%2520decoders%2520attached%2520to%2520a%2520frozen%250Apretrained%2520encoder.%2520Following%2520this%2520setting%252C%2520we%2520evaluate%2520Decipher-MR%2520across%250Adiverse%2520benchmarks%2520including%2520disease%2520classification%252C%2520demographic%2520prediction%252C%250Aanatomical%2520localization%252C%2520and%2520cross-modal%2520retrieval%252C%2520demonstrating%2520consistent%250Aperformance%2520gains%2520over%2520existing%2520foundation%2520models%2520and%2520task-specific%2520approaches.%250AOur%2520results%2520establish%2520Decipher-MR%2520as%2520a%2520scalable%2520and%2520versatile%2520foundation%2520for%250AMRI-based%2520AI%252C%2520facilitating%2520efficient%2520development%2520across%2520clinical%2520and%2520research%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decipher-MR%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20MRI%0A%20%20Representations&entry.906535625=Zhijian%20Yang%20and%20Noel%20DSouza%20and%20Istvan%20Megyeri%20and%20Xiaojian%20Xu%20and%20Amin%20Honarmandi%20Shandiz%20and%20Farzin%20Haddadpour%20and%20Krisztian%20Koos%20and%20Laszlo%20Rusko%20and%20Emanuele%20Valeriano%20and%20Bharadwaj%20Swaninathan%20and%20Lei%20Wu%20and%20Parminder%20Bhatia%20and%20Taha%20Kass-Hout%20and%20Erhan%20Bas&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20critical%20medical%20imaging%20modality%20in%0Aclinical%20diagnosis%20and%20research%2C%20yet%20its%20complexity%20and%20heterogeneity%20pose%0Achallenges%20for%20automated%20analysis%2C%20particularly%20in%20scalable%20and%20generalizable%0Amachine%20learning%20applications.%20While%20foundation%20models%20have%20revolutionized%0Anatural%20language%20and%20vision%20tasks%2C%20their%20application%20to%20MRI%20remains%20limited%20due%0Ato%20data%20scarcity%20and%20narrow%20anatomical%20focus.%20In%20this%20work%2C%20we%20present%0ADecipher-MR%2C%20a%203D%20MRI-specific%20vision-language%20foundation%20model%20trained%20on%20a%0Alarge-scale%20dataset%20comprising%20200%2C000%20MRI%20series%20from%20over%2022%2C000%20studies%0Aspanning%20diverse%20anatomical%20regions%2C%20sequences%2C%20and%20pathologies.%20Decipher-MR%0Aintegrates%20self-supervised%20vision%20learning%20with%20report-guided%20text%20supervision%0Ato%20build%20robust%2C%20generalizable%20representations%2C%20enabling%20effective%20adaptation%0Aacross%20broad%20applications.%20To%20enable%20robust%20and%20diverse%20clinical%20tasks%20with%0Aminimal%20computational%20overhead%2C%20Decipher-MR%20supports%20a%20modular%20design%20that%0Aenables%20tuning%20of%20lightweight%2C%20task-specific%20decoders%20attached%20to%20a%20frozen%0Apretrained%20encoder.%20Following%20this%20setting%2C%20we%20evaluate%20Decipher-MR%20across%0Adiverse%20benchmarks%20including%20disease%20classification%2C%20demographic%20prediction%2C%0Aanatomical%20localization%2C%20and%20cross-modal%20retrieval%2C%20demonstrating%20consistent%0Aperformance%20gains%20over%20existing%20foundation%20models%20and%20task-specific%20approaches.%0AOur%20results%20establish%20Decipher-MR%20as%20a%20scalable%20and%20versatile%20foundation%20for%0AMRI-based%20AI%2C%20facilitating%20efficient%20development%20across%20clinical%20and%20research%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21249v1&entry.124074799=Read"},
{"title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds", "author": "Luis Augenstein and No\u00e9mie Jaquier and Tamim Asfour and Leonel Rozo", "abstract": "  Human-like motion generation for robots often draws inspiration from\nbiomechanical studies, which often categorize complex human motions into\nhierarchical taxonomies. While these taxonomies provide rich structural\ninformation about how movements relate to one another, this information is\nfrequently overlooked in motion generation models, leading to a disconnect\nbetween the generated motions and their underlying hierarchical structure. This\npaper introduces the \\ac{gphdm}, a novel approach that learns latent\nrepresentations preserving both the hierarchical structure of motions and their\ntemporal dynamics to ensure physical consistency. Our model achieves this by\nextending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to\nthe hyperbolic manifold and integrating it with taxonomy-aware inductive\nbiases. Building on this geometry- and taxonomy-aware frameworks, we propose\nthree novel mechanisms for generating motions that are both\ntaxonomically-structured and physically-consistent: two probabilistic recursive\napproaches and a method based on pullback-metric geodesics. Experiments on\ngenerating realistic motion sequences on the hand grasping taxonomy show that\nthe proposed GPHDM faithfully encodes the underlying taxonomy and temporal\ndynamics, and generates novel physically-consistent trajectories.\n", "link": "http://arxiv.org/abs/2509.21281v1", "date": "2025-09-25", "relevancy": 2.9864, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5821}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taxonomy-aware%20Dynamic%20Motion%20Generation%20on%20Hyperbolic%20Manifolds&body=Title%3A%20Taxonomy-aware%20Dynamic%20Motion%20Generation%20on%20Hyperbolic%20Manifolds%0AAuthor%3A%20Luis%20Augenstein%20and%20No%C3%A9mie%20Jaquier%20and%20Tamim%20Asfour%20and%20Leonel%20Rozo%0AAbstract%3A%20%20%20Human-like%20motion%20generation%20for%20robots%20often%20draws%20inspiration%20from%0Abiomechanical%20studies%2C%20which%20often%20categorize%20complex%20human%20motions%20into%0Ahierarchical%20taxonomies.%20While%20these%20taxonomies%20provide%20rich%20structural%0Ainformation%20about%20how%20movements%20relate%20to%20one%20another%2C%20this%20information%20is%0Afrequently%20overlooked%20in%20motion%20generation%20models%2C%20leading%20to%20a%20disconnect%0Abetween%20the%20generated%20motions%20and%20their%20underlying%20hierarchical%20structure.%20This%0Apaper%20introduces%20the%20%5Cac%7Bgphdm%7D%2C%20a%20novel%20approach%20that%20learns%20latent%0Arepresentations%20preserving%20both%20the%20hierarchical%20structure%20of%20motions%20and%20their%0Atemporal%20dynamics%20to%20ensure%20physical%20consistency.%20Our%20model%20achieves%20this%20by%0Aextending%20the%20dynamics%20prior%20of%20the%20Gaussian%20Process%20Dynamical%20Model%20%28GPDM%29%20to%0Athe%20hyperbolic%20manifold%20and%20integrating%20it%20with%20taxonomy-aware%20inductive%0Abiases.%20Building%20on%20this%20geometry-%20and%20taxonomy-aware%20frameworks%2C%20we%20propose%0Athree%20novel%20mechanisms%20for%20generating%20motions%20that%20are%20both%0Ataxonomically-structured%20and%20physically-consistent%3A%20two%20probabilistic%20recursive%0Aapproaches%20and%20a%20method%20based%20on%20pullback-metric%20geodesics.%20Experiments%20on%0Agenerating%20realistic%20motion%20sequences%20on%20the%20hand%20grasping%20taxonomy%20show%20that%0Athe%20proposed%20GPHDM%20faithfully%20encodes%20the%20underlying%20taxonomy%20and%20temporal%0Adynamics%2C%20and%20generates%20novel%20physically-consistent%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaxonomy-aware%2520Dynamic%2520Motion%2520Generation%2520on%2520Hyperbolic%2520Manifolds%26entry.906535625%3DLuis%2520Augenstein%2520and%2520No%25C3%25A9mie%2520Jaquier%2520and%2520Tamim%2520Asfour%2520and%2520Leonel%2520Rozo%26entry.1292438233%3D%2520%2520Human-like%2520motion%2520generation%2520for%2520robots%2520often%2520draws%2520inspiration%2520from%250Abiomechanical%2520studies%252C%2520which%2520often%2520categorize%2520complex%2520human%2520motions%2520into%250Ahierarchical%2520taxonomies.%2520While%2520these%2520taxonomies%2520provide%2520rich%2520structural%250Ainformation%2520about%2520how%2520movements%2520relate%2520to%2520one%2520another%252C%2520this%2520information%2520is%250Afrequently%2520overlooked%2520in%2520motion%2520generation%2520models%252C%2520leading%2520to%2520a%2520disconnect%250Abetween%2520the%2520generated%2520motions%2520and%2520their%2520underlying%2520hierarchical%2520structure.%2520This%250Apaper%2520introduces%2520the%2520%255Cac%257Bgphdm%257D%252C%2520a%2520novel%2520approach%2520that%2520learns%2520latent%250Arepresentations%2520preserving%2520both%2520the%2520hierarchical%2520structure%2520of%2520motions%2520and%2520their%250Atemporal%2520dynamics%2520to%2520ensure%2520physical%2520consistency.%2520Our%2520model%2520achieves%2520this%2520by%250Aextending%2520the%2520dynamics%2520prior%2520of%2520the%2520Gaussian%2520Process%2520Dynamical%2520Model%2520%2528GPDM%2529%2520to%250Athe%2520hyperbolic%2520manifold%2520and%2520integrating%2520it%2520with%2520taxonomy-aware%2520inductive%250Abiases.%2520Building%2520on%2520this%2520geometry-%2520and%2520taxonomy-aware%2520frameworks%252C%2520we%2520propose%250Athree%2520novel%2520mechanisms%2520for%2520generating%2520motions%2520that%2520are%2520both%250Ataxonomically-structured%2520and%2520physically-consistent%253A%2520two%2520probabilistic%2520recursive%250Aapproaches%2520and%2520a%2520method%2520based%2520on%2520pullback-metric%2520geodesics.%2520Experiments%2520on%250Agenerating%2520realistic%2520motion%2520sequences%2520on%2520the%2520hand%2520grasping%2520taxonomy%2520show%2520that%250Athe%2520proposed%2520GPHDM%2520faithfully%2520encodes%2520the%2520underlying%2520taxonomy%2520and%2520temporal%250Adynamics%252C%2520and%2520generates%2520novel%2520physically-consistent%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taxonomy-aware%20Dynamic%20Motion%20Generation%20on%20Hyperbolic%20Manifolds&entry.906535625=Luis%20Augenstein%20and%20No%C3%A9mie%20Jaquier%20and%20Tamim%20Asfour%20and%20Leonel%20Rozo&entry.1292438233=%20%20Human-like%20motion%20generation%20for%20robots%20often%20draws%20inspiration%20from%0Abiomechanical%20studies%2C%20which%20often%20categorize%20complex%20human%20motions%20into%0Ahierarchical%20taxonomies.%20While%20these%20taxonomies%20provide%20rich%20structural%0Ainformation%20about%20how%20movements%20relate%20to%20one%20another%2C%20this%20information%20is%0Afrequently%20overlooked%20in%20motion%20generation%20models%2C%20leading%20to%20a%20disconnect%0Abetween%20the%20generated%20motions%20and%20their%20underlying%20hierarchical%20structure.%20This%0Apaper%20introduces%20the%20%5Cac%7Bgphdm%7D%2C%20a%20novel%20approach%20that%20learns%20latent%0Arepresentations%20preserving%20both%20the%20hierarchical%20structure%20of%20motions%20and%20their%0Atemporal%20dynamics%20to%20ensure%20physical%20consistency.%20Our%20model%20achieves%20this%20by%0Aextending%20the%20dynamics%20prior%20of%20the%20Gaussian%20Process%20Dynamical%20Model%20%28GPDM%29%20to%0Athe%20hyperbolic%20manifold%20and%20integrating%20it%20with%20taxonomy-aware%20inductive%0Abiases.%20Building%20on%20this%20geometry-%20and%20taxonomy-aware%20frameworks%2C%20we%20propose%0Athree%20novel%20mechanisms%20for%20generating%20motions%20that%20are%20both%0Ataxonomically-structured%20and%20physically-consistent%3A%20two%20probabilistic%20recursive%0Aapproaches%20and%20a%20method%20based%20on%20pullback-metric%20geodesics.%20Experiments%20on%0Agenerating%20realistic%20motion%20sequences%20on%20the%20hand%20grasping%20taxonomy%20show%20that%0Athe%20proposed%20GPHDM%20faithfully%20encodes%20the%20underlying%20taxonomy%20and%20temporal%0Adynamics%2C%20and%20generates%20novel%20physically-consistent%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21281v1&entry.124074799=Read"},
{"title": "Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in\n  Vision-Language Models", "author": "Suaiba Amina Salahuddin and Teresa Dorszewski and Marit Almenning Martiniussen and Tone Hovda and Antonio Portaluri and Solveig Thrun and Michael Kampffmeyer and Elisabeth Wetzer and Kristoffer Wickstr\u00f8m and Robert Jenssen", "abstract": "  Understanding what deep learning (DL) models learn is essential for the safe\ndeployment of artificial intelligence (AI) in clinical settings. While previous\nwork has focused on pixel-based explainability methods, less attention has been\npaid to the textual concepts learned by these models, which may better reflect\nthe reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\nconcept-based explainability framework for systematically dissecting DL vision\nmodels trained for mammography. Leveraging a mammography-specific\nvision-language model (Mammo-CLIP) as a \"dissector,\" our approach labels\nneurons at specified layers with human-interpretable textual concepts and\nquantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we\ninvestigate three key questions: (1) how concept learning differs between DL\nvision models trained on general image datasets versus mammography-specific\ndatasets; (2) how fine-tuning for downstream mammography tasks affects concept\nspecialisation; and (3) which mammography-relevant concepts remain\nunderrepresented. We show that models trained on mammography data capture more\nclinically relevant concepts and align more closely with radiologists'\nworkflows than models not trained on mammography data. Fine-tuning for\ntask-specific classification enhances the capture of certain concept categories\n(e.g., benign calcifications) but can reduce coverage of others (e.g.,\ndensity-related features), indicating a trade-off between specialisation and\ngeneralisation. Our findings show that Mammo-CLIP Dissect provides insights\ninto how convolutional neural networks (CNNs) capture mammography-specific\nknowledge. By comparing models across training data and fine-tuning regimes, we\nreveal how domain-specific training and task-specific adaptation shape concept\nlearning. Code and concept set are available:\nhttps://github.com/Suaiba/Mammo-CLIP-Dissect.\n", "link": "http://arxiv.org/abs/2509.21102v1", "date": "2025-09-25", "relevancy": 2.9864, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mammo-CLIP%20Dissect%3A%20A%20Framework%20for%20Analysing%20Mammography%20Concepts%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Mammo-CLIP%20Dissect%3A%20A%20Framework%20for%20Analysing%20Mammography%20Concepts%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Suaiba%20Amina%20Salahuddin%20and%20Teresa%20Dorszewski%20and%20Marit%20Almenning%20Martiniussen%20and%20Tone%20Hovda%20and%20Antonio%20Portaluri%20and%20Solveig%20Thrun%20and%20Michael%20Kampffmeyer%20and%20Elisabeth%20Wetzer%20and%20Kristoffer%20Wickstr%C3%B8m%20and%20Robert%20Jenssen%0AAbstract%3A%20%20%20Understanding%20what%20deep%20learning%20%28DL%29%20models%20learn%20is%20essential%20for%20the%20safe%0Adeployment%20of%20artificial%20intelligence%20%28AI%29%20in%20clinical%20settings.%20While%20previous%0Awork%20has%20focused%20on%20pixel-based%20explainability%20methods%2C%20less%20attention%20has%20been%0Apaid%20to%20the%20textual%20concepts%20learned%20by%20these%20models%2C%20which%20may%20better%20reflect%0Athe%20reasoning%20used%20by%20clinicians.%20We%20introduce%20Mammo-CLIP%20Dissect%2C%20the%20first%0Aconcept-based%20explainability%20framework%20for%20systematically%20dissecting%20DL%20vision%0Amodels%20trained%20for%20mammography.%20Leveraging%20a%20mammography-specific%0Avision-language%20model%20%28Mammo-CLIP%29%20as%20a%20%22dissector%2C%22%20our%20approach%20labels%0Aneurons%20at%20specified%20layers%20with%20human-interpretable%20textual%20concepts%20and%0Aquantifies%20their%20alignment%20to%20domain%20knowledge.%20Using%20Mammo-CLIP%20Dissect%2C%20we%0Ainvestigate%20three%20key%20questions%3A%20%281%29%20how%20concept%20learning%20differs%20between%20DL%0Avision%20models%20trained%20on%20general%20image%20datasets%20versus%20mammography-specific%0Adatasets%3B%20%282%29%20how%20fine-tuning%20for%20downstream%20mammography%20tasks%20affects%20concept%0Aspecialisation%3B%20and%20%283%29%20which%20mammography-relevant%20concepts%20remain%0Aunderrepresented.%20We%20show%20that%20models%20trained%20on%20mammography%20data%20capture%20more%0Aclinically%20relevant%20concepts%20and%20align%20more%20closely%20with%20radiologists%27%0Aworkflows%20than%20models%20not%20trained%20on%20mammography%20data.%20Fine-tuning%20for%0Atask-specific%20classification%20enhances%20the%20capture%20of%20certain%20concept%20categories%0A%28e.g.%2C%20benign%20calcifications%29%20but%20can%20reduce%20coverage%20of%20others%20%28e.g.%2C%0Adensity-related%20features%29%2C%20indicating%20a%20trade-off%20between%20specialisation%20and%0Ageneralisation.%20Our%20findings%20show%20that%20Mammo-CLIP%20Dissect%20provides%20insights%0Ainto%20how%20convolutional%20neural%20networks%20%28CNNs%29%20capture%20mammography-specific%0Aknowledge.%20By%20comparing%20models%20across%20training%20data%20and%20fine-tuning%20regimes%2C%20we%0Areveal%20how%20domain-specific%20training%20and%20task-specific%20adaptation%20shape%20concept%0Alearning.%20Code%20and%20concept%20set%20are%20available%3A%0Ahttps%3A//github.com/Suaiba/Mammo-CLIP-Dissect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMammo-CLIP%2520Dissect%253A%2520A%2520Framework%2520for%2520Analysing%2520Mammography%2520Concepts%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DSuaiba%2520Amina%2520Salahuddin%2520and%2520Teresa%2520Dorszewski%2520and%2520Marit%2520Almenning%2520Martiniussen%2520and%2520Tone%2520Hovda%2520and%2520Antonio%2520Portaluri%2520and%2520Solveig%2520Thrun%2520and%2520Michael%2520Kampffmeyer%2520and%2520Elisabeth%2520Wetzer%2520and%2520Kristoffer%2520Wickstr%25C3%25B8m%2520and%2520Robert%2520Jenssen%26entry.1292438233%3D%2520%2520Understanding%2520what%2520deep%2520learning%2520%2528DL%2529%2520models%2520learn%2520is%2520essential%2520for%2520the%2520safe%250Adeployment%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520clinical%2520settings.%2520While%2520previous%250Awork%2520has%2520focused%2520on%2520pixel-based%2520explainability%2520methods%252C%2520less%2520attention%2520has%2520been%250Apaid%2520to%2520the%2520textual%2520concepts%2520learned%2520by%2520these%2520models%252C%2520which%2520may%2520better%2520reflect%250Athe%2520reasoning%2520used%2520by%2520clinicians.%2520We%2520introduce%2520Mammo-CLIP%2520Dissect%252C%2520the%2520first%250Aconcept-based%2520explainability%2520framework%2520for%2520systematically%2520dissecting%2520DL%2520vision%250Amodels%2520trained%2520for%2520mammography.%2520Leveraging%2520a%2520mammography-specific%250Avision-language%2520model%2520%2528Mammo-CLIP%2529%2520as%2520a%2520%2522dissector%252C%2522%2520our%2520approach%2520labels%250Aneurons%2520at%2520specified%2520layers%2520with%2520human-interpretable%2520textual%2520concepts%2520and%250Aquantifies%2520their%2520alignment%2520to%2520domain%2520knowledge.%2520Using%2520Mammo-CLIP%2520Dissect%252C%2520we%250Ainvestigate%2520three%2520key%2520questions%253A%2520%25281%2529%2520how%2520concept%2520learning%2520differs%2520between%2520DL%250Avision%2520models%2520trained%2520on%2520general%2520image%2520datasets%2520versus%2520mammography-specific%250Adatasets%253B%2520%25282%2529%2520how%2520fine-tuning%2520for%2520downstream%2520mammography%2520tasks%2520affects%2520concept%250Aspecialisation%253B%2520and%2520%25283%2529%2520which%2520mammography-relevant%2520concepts%2520remain%250Aunderrepresented.%2520We%2520show%2520that%2520models%2520trained%2520on%2520mammography%2520data%2520capture%2520more%250Aclinically%2520relevant%2520concepts%2520and%2520align%2520more%2520closely%2520with%2520radiologists%2527%250Aworkflows%2520than%2520models%2520not%2520trained%2520on%2520mammography%2520data.%2520Fine-tuning%2520for%250Atask-specific%2520classification%2520enhances%2520the%2520capture%2520of%2520certain%2520concept%2520categories%250A%2528e.g.%252C%2520benign%2520calcifications%2529%2520but%2520can%2520reduce%2520coverage%2520of%2520others%2520%2528e.g.%252C%250Adensity-related%2520features%2529%252C%2520indicating%2520a%2520trade-off%2520between%2520specialisation%2520and%250Ageneralisation.%2520Our%2520findings%2520show%2520that%2520Mammo-CLIP%2520Dissect%2520provides%2520insights%250Ainto%2520how%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520capture%2520mammography-specific%250Aknowledge.%2520By%2520comparing%2520models%2520across%2520training%2520data%2520and%2520fine-tuning%2520regimes%252C%2520we%250Areveal%2520how%2520domain-specific%2520training%2520and%2520task-specific%2520adaptation%2520shape%2520concept%250Alearning.%2520Code%2520and%2520concept%2520set%2520are%2520available%253A%250Ahttps%253A//github.com/Suaiba/Mammo-CLIP-Dissect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mammo-CLIP%20Dissect%3A%20A%20Framework%20for%20Analysing%20Mammography%20Concepts%20in%0A%20%20Vision-Language%20Models&entry.906535625=Suaiba%20Amina%20Salahuddin%20and%20Teresa%20Dorszewski%20and%20Marit%20Almenning%20Martiniussen%20and%20Tone%20Hovda%20and%20Antonio%20Portaluri%20and%20Solveig%20Thrun%20and%20Michael%20Kampffmeyer%20and%20Elisabeth%20Wetzer%20and%20Kristoffer%20Wickstr%C3%B8m%20and%20Robert%20Jenssen&entry.1292438233=%20%20Understanding%20what%20deep%20learning%20%28DL%29%20models%20learn%20is%20essential%20for%20the%20safe%0Adeployment%20of%20artificial%20intelligence%20%28AI%29%20in%20clinical%20settings.%20While%20previous%0Awork%20has%20focused%20on%20pixel-based%20explainability%20methods%2C%20less%20attention%20has%20been%0Apaid%20to%20the%20textual%20concepts%20learned%20by%20these%20models%2C%20which%20may%20better%20reflect%0Athe%20reasoning%20used%20by%20clinicians.%20We%20introduce%20Mammo-CLIP%20Dissect%2C%20the%20first%0Aconcept-based%20explainability%20framework%20for%20systematically%20dissecting%20DL%20vision%0Amodels%20trained%20for%20mammography.%20Leveraging%20a%20mammography-specific%0Avision-language%20model%20%28Mammo-CLIP%29%20as%20a%20%22dissector%2C%22%20our%20approach%20labels%0Aneurons%20at%20specified%20layers%20with%20human-interpretable%20textual%20concepts%20and%0Aquantifies%20their%20alignment%20to%20domain%20knowledge.%20Using%20Mammo-CLIP%20Dissect%2C%20we%0Ainvestigate%20three%20key%20questions%3A%20%281%29%20how%20concept%20learning%20differs%20between%20DL%0Avision%20models%20trained%20on%20general%20image%20datasets%20versus%20mammography-specific%0Adatasets%3B%20%282%29%20how%20fine-tuning%20for%20downstream%20mammography%20tasks%20affects%20concept%0Aspecialisation%3B%20and%20%283%29%20which%20mammography-relevant%20concepts%20remain%0Aunderrepresented.%20We%20show%20that%20models%20trained%20on%20mammography%20data%20capture%20more%0Aclinically%20relevant%20concepts%20and%20align%20more%20closely%20with%20radiologists%27%0Aworkflows%20than%20models%20not%20trained%20on%20mammography%20data.%20Fine-tuning%20for%0Atask-specific%20classification%20enhances%20the%20capture%20of%20certain%20concept%20categories%0A%28e.g.%2C%20benign%20calcifications%29%20but%20can%20reduce%20coverage%20of%20others%20%28e.g.%2C%0Adensity-related%20features%29%2C%20indicating%20a%20trade-off%20between%20specialisation%20and%0Ageneralisation.%20Our%20findings%20show%20that%20Mammo-CLIP%20Dissect%20provides%20insights%0Ainto%20how%20convolutional%20neural%20networks%20%28CNNs%29%20capture%20mammography-specific%0Aknowledge.%20By%20comparing%20models%20across%20training%20data%20and%20fine-tuning%20regimes%2C%20we%0Areveal%20how%20domain-specific%20training%20and%20task-specific%20adaptation%20shape%20concept%0Alearning.%20Code%20and%20concept%20set%20are%20available%3A%0Ahttps%3A//github.com/Suaiba/Mammo-CLIP-Dissect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21102v1&entry.124074799=Read"},
{"title": "CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction", "author": "Suyi Chen and Haibin Ling", "abstract": "  As a critical modality for structural biology, cryogenic electron microscopy\n(cryo-EM) facilitates the determination of macromolecular structures at\nnear-atomic resolution. The core computational task in single-particle cryo-EM\nis to reconstruct the 3D electrostatic potential of a molecule from noisy 2D\nprojections acquired at unknown orientations. Gaussian mixture models (GMMs)\nprovide a continuous, compact, and physically interpretable representation for\nmolecular density and have recently gained interest in cryo-EM reconstruction.\nHowever, existing methods rely on external consensus maps or atomic models for\ninitialization, limiting their use in self-contained pipelines. In parallel,\ndifferentiable rendering techniques such as Gaussian splatting have\ndemonstrated remarkable scalability and efficiency for volumetric\nrepresentations, suggesting a natural fit for GMM-based cryo-EM reconstruction.\nHowever, off-the-shelf Gaussian splatting methods are designed for\nphotorealistic view synthesis and remain incompatible with cryo-EM due to\nmismatches in the image formation physics, reconstruction objectives, and\ncoordinate systems. Addressing these issues, we propose cryoSplat, a GMM-based\nmethod that integrates Gaussian splatting with the physics of cryo-EM image\nformation. In particular, we develop an orthogonal projection-aware Gaussian\nsplatting, with adaptations such as a view-dependent normalization term and\nFFT-aligned coordinate system tailored for cryo-EM imaging. These innovations\nenable stable and efficient homogeneous reconstruction directly from raw\ncryo-EM particle images using random initialization. Experimental results on\nreal datasets validate the effectiveness and robustness of cryoSplat over\nrepresentative baselines. The code will be released upon publication.\n", "link": "http://arxiv.org/abs/2508.04929v3", "date": "2025-09-25", "relevancy": 2.9693, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.625}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5877}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction&body=Title%3A%20CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction%0AAuthor%3A%20Suyi%20Chen%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20As%20a%20critical%20modality%20for%20structural%20biology%2C%20cryogenic%20electron%20microscopy%0A%28cryo-EM%29%20facilitates%20the%20determination%20of%20macromolecular%20structures%20at%0Anear-atomic%20resolution.%20The%20core%20computational%20task%20in%20single-particle%20cryo-EM%0Ais%20to%20reconstruct%20the%203D%20electrostatic%20potential%20of%20a%20molecule%20from%20noisy%202D%0Aprojections%20acquired%20at%20unknown%20orientations.%20Gaussian%20mixture%20models%20%28GMMs%29%0Aprovide%20a%20continuous%2C%20compact%2C%20and%20physically%20interpretable%20representation%20for%0Amolecular%20density%20and%20have%20recently%20gained%20interest%20in%20cryo-EM%20reconstruction.%0AHowever%2C%20existing%20methods%20rely%20on%20external%20consensus%20maps%20or%20atomic%20models%20for%0Ainitialization%2C%20limiting%20their%20use%20in%20self-contained%20pipelines.%20In%20parallel%2C%0Adifferentiable%20rendering%20techniques%20such%20as%20Gaussian%20splatting%20have%0Ademonstrated%20remarkable%20scalability%20and%20efficiency%20for%20volumetric%0Arepresentations%2C%20suggesting%20a%20natural%20fit%20for%20GMM-based%20cryo-EM%20reconstruction.%0AHowever%2C%20off-the-shelf%20Gaussian%20splatting%20methods%20are%20designed%20for%0Aphotorealistic%20view%20synthesis%20and%20remain%20incompatible%20with%20cryo-EM%20due%20to%0Amismatches%20in%20the%20image%20formation%20physics%2C%20reconstruction%20objectives%2C%20and%0Acoordinate%20systems.%20Addressing%20these%20issues%2C%20we%20propose%20cryoSplat%2C%20a%20GMM-based%0Amethod%20that%20integrates%20Gaussian%20splatting%20with%20the%20physics%20of%20cryo-EM%20image%0Aformation.%20In%20particular%2C%20we%20develop%20an%20orthogonal%20projection-aware%20Gaussian%0Asplatting%2C%20with%20adaptations%20such%20as%20a%20view-dependent%20normalization%20term%20and%0AFFT-aligned%20coordinate%20system%20tailored%20for%20cryo-EM%20imaging.%20These%20innovations%0Aenable%20stable%20and%20efficient%20homogeneous%20reconstruction%20directly%20from%20raw%0Acryo-EM%20particle%20images%20using%20random%20initialization.%20Experimental%20results%20on%0Areal%20datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20cryoSplat%20over%0Arepresentative%20baselines.%20The%20code%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04929v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryoSplat%253A%2520Gaussian%2520Splatting%2520for%2520Cryo-EM%2520Homogeneous%2520Reconstruction%26entry.906535625%3DSuyi%2520Chen%2520and%2520Haibin%2520Ling%26entry.1292438233%3D%2520%2520As%2520a%2520critical%2520modality%2520for%2520structural%2520biology%252C%2520cryogenic%2520electron%2520microscopy%250A%2528cryo-EM%2529%2520facilitates%2520the%2520determination%2520of%2520macromolecular%2520structures%2520at%250Anear-atomic%2520resolution.%2520The%2520core%2520computational%2520task%2520in%2520single-particle%2520cryo-EM%250Ais%2520to%2520reconstruct%2520the%25203D%2520electrostatic%2520potential%2520of%2520a%2520molecule%2520from%2520noisy%25202D%250Aprojections%2520acquired%2520at%2520unknown%2520orientations.%2520Gaussian%2520mixture%2520models%2520%2528GMMs%2529%250Aprovide%2520a%2520continuous%252C%2520compact%252C%2520and%2520physically%2520interpretable%2520representation%2520for%250Amolecular%2520density%2520and%2520have%2520recently%2520gained%2520interest%2520in%2520cryo-EM%2520reconstruction.%250AHowever%252C%2520existing%2520methods%2520rely%2520on%2520external%2520consensus%2520maps%2520or%2520atomic%2520models%2520for%250Ainitialization%252C%2520limiting%2520their%2520use%2520in%2520self-contained%2520pipelines.%2520In%2520parallel%252C%250Adifferentiable%2520rendering%2520techniques%2520such%2520as%2520Gaussian%2520splatting%2520have%250Ademonstrated%2520remarkable%2520scalability%2520and%2520efficiency%2520for%2520volumetric%250Arepresentations%252C%2520suggesting%2520a%2520natural%2520fit%2520for%2520GMM-based%2520cryo-EM%2520reconstruction.%250AHowever%252C%2520off-the-shelf%2520Gaussian%2520splatting%2520methods%2520are%2520designed%2520for%250Aphotorealistic%2520view%2520synthesis%2520and%2520remain%2520incompatible%2520with%2520cryo-EM%2520due%2520to%250Amismatches%2520in%2520the%2520image%2520formation%2520physics%252C%2520reconstruction%2520objectives%252C%2520and%250Acoordinate%2520systems.%2520Addressing%2520these%2520issues%252C%2520we%2520propose%2520cryoSplat%252C%2520a%2520GMM-based%250Amethod%2520that%2520integrates%2520Gaussian%2520splatting%2520with%2520the%2520physics%2520of%2520cryo-EM%2520image%250Aformation.%2520In%2520particular%252C%2520we%2520develop%2520an%2520orthogonal%2520projection-aware%2520Gaussian%250Asplatting%252C%2520with%2520adaptations%2520such%2520as%2520a%2520view-dependent%2520normalization%2520term%2520and%250AFFT-aligned%2520coordinate%2520system%2520tailored%2520for%2520cryo-EM%2520imaging.%2520These%2520innovations%250Aenable%2520stable%2520and%2520efficient%2520homogeneous%2520reconstruction%2520directly%2520from%2520raw%250Acryo-EM%2520particle%2520images%2520using%2520random%2520initialization.%2520Experimental%2520results%2520on%250Areal%2520datasets%2520validate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520cryoSplat%2520over%250Arepresentative%2520baselines.%2520The%2520code%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04929v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction&entry.906535625=Suyi%20Chen%20and%20Haibin%20Ling&entry.1292438233=%20%20As%20a%20critical%20modality%20for%20structural%20biology%2C%20cryogenic%20electron%20microscopy%0A%28cryo-EM%29%20facilitates%20the%20determination%20of%20macromolecular%20structures%20at%0Anear-atomic%20resolution.%20The%20core%20computational%20task%20in%20single-particle%20cryo-EM%0Ais%20to%20reconstruct%20the%203D%20electrostatic%20potential%20of%20a%20molecule%20from%20noisy%202D%0Aprojections%20acquired%20at%20unknown%20orientations.%20Gaussian%20mixture%20models%20%28GMMs%29%0Aprovide%20a%20continuous%2C%20compact%2C%20and%20physically%20interpretable%20representation%20for%0Amolecular%20density%20and%20have%20recently%20gained%20interest%20in%20cryo-EM%20reconstruction.%0AHowever%2C%20existing%20methods%20rely%20on%20external%20consensus%20maps%20or%20atomic%20models%20for%0Ainitialization%2C%20limiting%20their%20use%20in%20self-contained%20pipelines.%20In%20parallel%2C%0Adifferentiable%20rendering%20techniques%20such%20as%20Gaussian%20splatting%20have%0Ademonstrated%20remarkable%20scalability%20and%20efficiency%20for%20volumetric%0Arepresentations%2C%20suggesting%20a%20natural%20fit%20for%20GMM-based%20cryo-EM%20reconstruction.%0AHowever%2C%20off-the-shelf%20Gaussian%20splatting%20methods%20are%20designed%20for%0Aphotorealistic%20view%20synthesis%20and%20remain%20incompatible%20with%20cryo-EM%20due%20to%0Amismatches%20in%20the%20image%20formation%20physics%2C%20reconstruction%20objectives%2C%20and%0Acoordinate%20systems.%20Addressing%20these%20issues%2C%20we%20propose%20cryoSplat%2C%20a%20GMM-based%0Amethod%20that%20integrates%20Gaussian%20splatting%20with%20the%20physics%20of%20cryo-EM%20image%0Aformation.%20In%20particular%2C%20we%20develop%20an%20orthogonal%20projection-aware%20Gaussian%0Asplatting%2C%20with%20adaptations%20such%20as%20a%20view-dependent%20normalization%20term%20and%0AFFT-aligned%20coordinate%20system%20tailored%20for%20cryo-EM%20imaging.%20These%20innovations%0Aenable%20stable%20and%20efficient%20homogeneous%20reconstruction%20directly%20from%20raw%0Acryo-EM%20particle%20images%20using%20random%20initialization.%20Experimental%20results%20on%0Areal%20datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20cryoSplat%20over%0Arepresentative%20baselines.%20The%20code%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04929v3&entry.124074799=Read"},
{"title": "Learning to Look: Cognitive Attention Alignment with Vision-Language\n  Models", "author": "Ryan L. Yang and Dipkamal Bhusal and Nidhi Rastogi", "abstract": "  Convolutional Neural Networks (CNNs) frequently \"cheat\" by exploiting\nsuperficial correlations, raising concerns about whether they make predictions\nfor the right reasons. Inspired by cognitive science, which highlights the role\nof attention in robust human perception, recent methods have sought to guide\nmodel attention using concept-based supervision and explanation regularization.\nHowever, these techniques depend on labor-intensive, expert-provided\nannotations, limiting their scalability. We propose a scalable framework that\nleverages vision-language models to automatically generate semantic attention\nmaps using natural language prompts. By introducing an auxiliary loss that\naligns CNN attention with these language-guided maps, our approach promotes\nmore reliable and cognitively plausible decision-making without manual\nannotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,\nshow that our method achieves state-of-the-art performance on ColorMNIST and\nremains competitive with annotation-heavy baselines on DecoyMNIST,\ndemonstrating improved generalization, reduced shortcut reliance, and model\nattention that better reflects human intuition.\n", "link": "http://arxiv.org/abs/2509.21247v1", "date": "2025-09-25", "relevancy": 2.9685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Look%3A%20Cognitive%20Attention%20Alignment%20with%20Vision-Language%0A%20%20Models&body=Title%3A%20Learning%20to%20Look%3A%20Cognitive%20Attention%20Alignment%20with%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Ryan%20L.%20Yang%20and%20Dipkamal%20Bhusal%20and%20Nidhi%20Rastogi%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20frequently%20%22cheat%22%20by%20exploiting%0Asuperficial%20correlations%2C%20raising%20concerns%20about%20whether%20they%20make%20predictions%0Afor%20the%20right%20reasons.%20Inspired%20by%20cognitive%20science%2C%20which%20highlights%20the%20role%0Aof%20attention%20in%20robust%20human%20perception%2C%20recent%20methods%20have%20sought%20to%20guide%0Amodel%20attention%20using%20concept-based%20supervision%20and%20explanation%20regularization.%0AHowever%2C%20these%20techniques%20depend%20on%20labor-intensive%2C%20expert-provided%0Aannotations%2C%20limiting%20their%20scalability.%20We%20propose%20a%20scalable%20framework%20that%0Aleverages%20vision-language%20models%20to%20automatically%20generate%20semantic%20attention%0Amaps%20using%20natural%20language%20prompts.%20By%20introducing%20an%20auxiliary%20loss%20that%0Aaligns%20CNN%20attention%20with%20these%20language-guided%20maps%2C%20our%20approach%20promotes%0Amore%20reliable%20and%20cognitively%20plausible%20decision-making%20without%20manual%0Aannotation.%20Experiments%20on%20challenging%20datasets%2C%20ColoredMNIST%20and%20DecoyMNIST%2C%0Ashow%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20ColorMNIST%20and%0Aremains%20competitive%20with%20annotation-heavy%20baselines%20on%20DecoyMNIST%2C%0Ademonstrating%20improved%20generalization%2C%20reduced%20shortcut%20reliance%2C%20and%20model%0Aattention%20that%20better%20reflects%20human%20intuition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Look%253A%2520Cognitive%2520Attention%2520Alignment%2520with%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DRyan%2520L.%2520Yang%2520and%2520Dipkamal%2520Bhusal%2520and%2520Nidhi%2520Rastogi%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520frequently%2520%2522cheat%2522%2520by%2520exploiting%250Asuperficial%2520correlations%252C%2520raising%2520concerns%2520about%2520whether%2520they%2520make%2520predictions%250Afor%2520the%2520right%2520reasons.%2520Inspired%2520by%2520cognitive%2520science%252C%2520which%2520highlights%2520the%2520role%250Aof%2520attention%2520in%2520robust%2520human%2520perception%252C%2520recent%2520methods%2520have%2520sought%2520to%2520guide%250Amodel%2520attention%2520using%2520concept-based%2520supervision%2520and%2520explanation%2520regularization.%250AHowever%252C%2520these%2520techniques%2520depend%2520on%2520labor-intensive%252C%2520expert-provided%250Aannotations%252C%2520limiting%2520their%2520scalability.%2520We%2520propose%2520a%2520scalable%2520framework%2520that%250Aleverages%2520vision-language%2520models%2520to%2520automatically%2520generate%2520semantic%2520attention%250Amaps%2520using%2520natural%2520language%2520prompts.%2520By%2520introducing%2520an%2520auxiliary%2520loss%2520that%250Aaligns%2520CNN%2520attention%2520with%2520these%2520language-guided%2520maps%252C%2520our%2520approach%2520promotes%250Amore%2520reliable%2520and%2520cognitively%2520plausible%2520decision-making%2520without%2520manual%250Aannotation.%2520Experiments%2520on%2520challenging%2520datasets%252C%2520ColoredMNIST%2520and%2520DecoyMNIST%252C%250Ashow%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520ColorMNIST%2520and%250Aremains%2520competitive%2520with%2520annotation-heavy%2520baselines%2520on%2520DecoyMNIST%252C%250Ademonstrating%2520improved%2520generalization%252C%2520reduced%2520shortcut%2520reliance%252C%2520and%2520model%250Aattention%2520that%2520better%2520reflects%2520human%2520intuition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Look%3A%20Cognitive%20Attention%20Alignment%20with%20Vision-Language%0A%20%20Models&entry.906535625=Ryan%20L.%20Yang%20and%20Dipkamal%20Bhusal%20and%20Nidhi%20Rastogi&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20frequently%20%22cheat%22%20by%20exploiting%0Asuperficial%20correlations%2C%20raising%20concerns%20about%20whether%20they%20make%20predictions%0Afor%20the%20right%20reasons.%20Inspired%20by%20cognitive%20science%2C%20which%20highlights%20the%20role%0Aof%20attention%20in%20robust%20human%20perception%2C%20recent%20methods%20have%20sought%20to%20guide%0Amodel%20attention%20using%20concept-based%20supervision%20and%20explanation%20regularization.%0AHowever%2C%20these%20techniques%20depend%20on%20labor-intensive%2C%20expert-provided%0Aannotations%2C%20limiting%20their%20scalability.%20We%20propose%20a%20scalable%20framework%20that%0Aleverages%20vision-language%20models%20to%20automatically%20generate%20semantic%20attention%0Amaps%20using%20natural%20language%20prompts.%20By%20introducing%20an%20auxiliary%20loss%20that%0Aaligns%20CNN%20attention%20with%20these%20language-guided%20maps%2C%20our%20approach%20promotes%0Amore%20reliable%20and%20cognitively%20plausible%20decision-making%20without%20manual%0Aannotation.%20Experiments%20on%20challenging%20datasets%2C%20ColoredMNIST%20and%20DecoyMNIST%2C%0Ashow%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20ColorMNIST%20and%0Aremains%20competitive%20with%20annotation-heavy%20baselines%20on%20DecoyMNIST%2C%0Ademonstrating%20improved%20generalization%2C%20reduced%20shortcut%20reliance%2C%20and%20model%0Aattention%20that%20better%20reflects%20human%20intuition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21247v1&entry.124074799=Read"},
{"title": "DisCoCLIP: A Distributional Compositional Tensor Network Encoder for\n  Vision-Language Understanding", "author": "Kin Ian Lo and Hala Hawashin and Mina Abbaszadeh and Tilen Limback-Stokin and Hadi Wazni and Mehrnoosh Sadrzadeh", "abstract": "  Recent vision-language models excel at large-scale image-text alignment but\noften neglect the compositional structure of language, leading to failures on\ntasks that hinge on word order and predicate-argument structure. We introduce\nDisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer\nwith a novel tensor network text encoder that explicitly encodes syntactic\nstructure. Sentences are parsed with a Combinatory Categorial Grammar parser to\nyield distributional word tensors whose contractions mirror the sentence's\ngrammatical derivation. To keep the model efficient, high-order tensors are\nfactorized with tensor decompositions, reducing parameter count from tens of\nmillions to under one million. Trained end-to-end with a self-supervised\ncontrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and\nword order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,\nboosts ARO attribution and relation scores by over 9% and 4%, and achieves\n93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that\nembedding explicit linguistic structure via tensor networks yields\ninterpretable, parameter-efficient representations that substantially improve\ncompositional reasoning in vision-language tasks.\n", "link": "http://arxiv.org/abs/2509.21287v1", "date": "2025-09-25", "relevancy": 2.9375, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCoCLIP%3A%20A%20Distributional%20Compositional%20Tensor%20Network%20Encoder%20for%0A%20%20Vision-Language%20Understanding&body=Title%3A%20DisCoCLIP%3A%20A%20Distributional%20Compositional%20Tensor%20Network%20Encoder%20for%0A%20%20Vision-Language%20Understanding%0AAuthor%3A%20Kin%20Ian%20Lo%20and%20Hala%20Hawashin%20and%20Mina%20Abbaszadeh%20and%20Tilen%20Limback-Stokin%20and%20Hadi%20Wazni%20and%20Mehrnoosh%20Sadrzadeh%0AAbstract%3A%20%20%20Recent%20vision-language%20models%20excel%20at%20large-scale%20image-text%20alignment%20but%0Aoften%20neglect%20the%20compositional%20structure%20of%20language%2C%20leading%20to%20failures%20on%0Atasks%20that%20hinge%20on%20word%20order%20and%20predicate-argument%20structure.%20We%20introduce%0ADisCoCLIP%2C%20a%20multimodal%20encoder%20that%20combines%20a%20frozen%20CLIP%20vision%20transformer%0Awith%20a%20novel%20tensor%20network%20text%20encoder%20that%20explicitly%20encodes%20syntactic%0Astructure.%20Sentences%20are%20parsed%20with%20a%20Combinatory%20Categorial%20Grammar%20parser%20to%0Ayield%20distributional%20word%20tensors%20whose%20contractions%20mirror%20the%20sentence%27s%0Agrammatical%20derivation.%20To%20keep%20the%20model%20efficient%2C%20high-order%20tensors%20are%0Afactorized%20with%20tensor%20decompositions%2C%20reducing%20parameter%20count%20from%20tens%20of%0Amillions%20to%20under%20one%20million.%20Trained%20end-to-end%20with%20a%20self-supervised%0Acontrastive%20loss%2C%20DisCoCLIP%20markedly%20improves%20sensitivity%20to%20verb%20semantics%20and%0Aword%20order%3A%20it%20raises%20CLIP%27s%20SVO-Probes%20verb%20accuracy%20from%2077.6%25%20to%2082.4%25%2C%0Aboosts%20ARO%20attribution%20and%20relation%20scores%20by%20over%209%25%20and%204%25%2C%20and%20achieves%0A93.7%25%20on%20a%20newly%20introduced%20SVO-Swap%20benchmark.%20These%20results%20demonstrate%20that%0Aembedding%20explicit%20linguistic%20structure%20via%20tensor%20networks%20yields%0Ainterpretable%2C%20parameter-efficient%20representations%20that%20substantially%20improve%0Acompositional%20reasoning%20in%20vision-language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCoCLIP%253A%2520A%2520Distributional%2520Compositional%2520Tensor%2520Network%2520Encoder%2520for%250A%2520%2520Vision-Language%2520Understanding%26entry.906535625%3DKin%2520Ian%2520Lo%2520and%2520Hala%2520Hawashin%2520and%2520Mina%2520Abbaszadeh%2520and%2520Tilen%2520Limback-Stokin%2520and%2520Hadi%2520Wazni%2520and%2520Mehrnoosh%2520Sadrzadeh%26entry.1292438233%3D%2520%2520Recent%2520vision-language%2520models%2520excel%2520at%2520large-scale%2520image-text%2520alignment%2520but%250Aoften%2520neglect%2520the%2520compositional%2520structure%2520of%2520language%252C%2520leading%2520to%2520failures%2520on%250Atasks%2520that%2520hinge%2520on%2520word%2520order%2520and%2520predicate-argument%2520structure.%2520We%2520introduce%250ADisCoCLIP%252C%2520a%2520multimodal%2520encoder%2520that%2520combines%2520a%2520frozen%2520CLIP%2520vision%2520transformer%250Awith%2520a%2520novel%2520tensor%2520network%2520text%2520encoder%2520that%2520explicitly%2520encodes%2520syntactic%250Astructure.%2520Sentences%2520are%2520parsed%2520with%2520a%2520Combinatory%2520Categorial%2520Grammar%2520parser%2520to%250Ayield%2520distributional%2520word%2520tensors%2520whose%2520contractions%2520mirror%2520the%2520sentence%2527s%250Agrammatical%2520derivation.%2520To%2520keep%2520the%2520model%2520efficient%252C%2520high-order%2520tensors%2520are%250Afactorized%2520with%2520tensor%2520decompositions%252C%2520reducing%2520parameter%2520count%2520from%2520tens%2520of%250Amillions%2520to%2520under%2520one%2520million.%2520Trained%2520end-to-end%2520with%2520a%2520self-supervised%250Acontrastive%2520loss%252C%2520DisCoCLIP%2520markedly%2520improves%2520sensitivity%2520to%2520verb%2520semantics%2520and%250Aword%2520order%253A%2520it%2520raises%2520CLIP%2527s%2520SVO-Probes%2520verb%2520accuracy%2520from%252077.6%2525%2520to%252082.4%2525%252C%250Aboosts%2520ARO%2520attribution%2520and%2520relation%2520scores%2520by%2520over%25209%2525%2520and%25204%2525%252C%2520and%2520achieves%250A93.7%2525%2520on%2520a%2520newly%2520introduced%2520SVO-Swap%2520benchmark.%2520These%2520results%2520demonstrate%2520that%250Aembedding%2520explicit%2520linguistic%2520structure%2520via%2520tensor%2520networks%2520yields%250Ainterpretable%252C%2520parameter-efficient%2520representations%2520that%2520substantially%2520improve%250Acompositional%2520reasoning%2520in%2520vision-language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCoCLIP%3A%20A%20Distributional%20Compositional%20Tensor%20Network%20Encoder%20for%0A%20%20Vision-Language%20Understanding&entry.906535625=Kin%20Ian%20Lo%20and%20Hala%20Hawashin%20and%20Mina%20Abbaszadeh%20and%20Tilen%20Limback-Stokin%20and%20Hadi%20Wazni%20and%20Mehrnoosh%20Sadrzadeh&entry.1292438233=%20%20Recent%20vision-language%20models%20excel%20at%20large-scale%20image-text%20alignment%20but%0Aoften%20neglect%20the%20compositional%20structure%20of%20language%2C%20leading%20to%20failures%20on%0Atasks%20that%20hinge%20on%20word%20order%20and%20predicate-argument%20structure.%20We%20introduce%0ADisCoCLIP%2C%20a%20multimodal%20encoder%20that%20combines%20a%20frozen%20CLIP%20vision%20transformer%0Awith%20a%20novel%20tensor%20network%20text%20encoder%20that%20explicitly%20encodes%20syntactic%0Astructure.%20Sentences%20are%20parsed%20with%20a%20Combinatory%20Categorial%20Grammar%20parser%20to%0Ayield%20distributional%20word%20tensors%20whose%20contractions%20mirror%20the%20sentence%27s%0Agrammatical%20derivation.%20To%20keep%20the%20model%20efficient%2C%20high-order%20tensors%20are%0Afactorized%20with%20tensor%20decompositions%2C%20reducing%20parameter%20count%20from%20tens%20of%0Amillions%20to%20under%20one%20million.%20Trained%20end-to-end%20with%20a%20self-supervised%0Acontrastive%20loss%2C%20DisCoCLIP%20markedly%20improves%20sensitivity%20to%20verb%20semantics%20and%0Aword%20order%3A%20it%20raises%20CLIP%27s%20SVO-Probes%20verb%20accuracy%20from%2077.6%25%20to%2082.4%25%2C%0Aboosts%20ARO%20attribution%20and%20relation%20scores%20by%20over%209%25%20and%204%25%2C%20and%20achieves%0A93.7%25%20on%20a%20newly%20introduced%20SVO-Swap%20benchmark.%20These%20results%20demonstrate%20that%0Aembedding%20explicit%20linguistic%20structure%20via%20tensor%20networks%20yields%0Ainterpretable%2C%20parameter-efficient%20representations%20that%20substantially%20improve%0Acompositional%20reasoning%20in%20vision-language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21287v1&entry.124074799=Read"},
{"title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence", "author": "Sihan Yang and Runsen Xu and Yiman Xie and Sizhe Yang and Mo Li and Jingli Lin and Chenming Zhu and Xiaochen Chen and Haodong Duan and Xiangyu Yue and Dahua Lin and Tai Wang and Jiangmiao Pang", "abstract": "  Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .\n", "link": "http://arxiv.org/abs/2505.23764v2", "date": "2025-09-25", "relevancy": 2.9026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence&body=Title%3A%20MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence%0AAuthor%3A%20Sihan%20Yang%20and%20Runsen%20Xu%20and%20Yiman%20Xie%20and%20Sizhe%20Yang%20and%20Mo%20Li%20and%20Jingli%20Lin%20and%20Chenming%20Zhu%20and%20Xiaochen%20Chen%20and%20Haodong%20Duan%20and%20Xiangyu%20Yue%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Spatial%20intelligence%20is%20essential%20for%20multimodal%20large%20language%20models%0A%28MLLMs%29%20operating%20in%20the%20complex%20physical%20world.%20Existing%20benchmarks%2C%20however%2C%0Aprobe%20only%20single-image%20relations%20and%20thus%20fail%20to%20assess%20the%20multi-image%0Aspatial%20reasoning%20that%20real-world%20deployments%20demand.%20We%20introduce%20MMSI-Bench%2C%0Aa%20VQA%20benchmark%20dedicated%20to%20multi-image%20spatial%20intelligence.%20Six%203D-vision%0Aresearchers%20spent%20more%20than%20300%20hours%20meticulously%20crafting%201%2C000%20challenging%2C%0Aunambiguous%20multiple-choice%20questions%20from%20over%20120%2C000%20images%2C%20each%20paired%0Awith%20carefully%20designed%20distractors%20and%20a%20step-by-step%20reasoning%20process.%20We%0Aconduct%20extensive%20experiments%20and%20thoroughly%20evaluate%2034%20open-source%20and%0Aproprietary%20MLLMs%2C%20observing%20a%20wide%20gap%3A%20the%20strongest%20open-source%20model%0Aattains%20roughly%2030%25%20accuracy%20and%20OpenAI%27s%20o3%20reasoning%20model%20reaches%2040%25%2C%20while%0Ahumans%20score%2097%25.%20These%20results%20underscore%20the%20challenging%20nature%20of%20MMSI-Bench%0Aand%20the%20substantial%20headroom%20for%20future%20research.%20Leveraging%20the%20annotated%0Areasoning%20processes%2C%20we%20also%20provide%20an%20automated%20error%20analysis%20pipeline%20that%0Adiagnoses%20four%20dominant%20failure%20modes%2C%20including%20%281%29%20grounding%20errors%2C%20%282%29%0Aoverlap-matching%20and%20scene-reconstruction%20errors%2C%20%283%29%20situation-transformation%0Areasoning%20errors%2C%20and%20%284%29%20spatial-logic%20errors%2C%20offering%20valuable%20insights%20for%0Aadvancing%20multi-image%20spatial%20intelligence.%20Project%20page%3A%0Ahttps%3A//runsenxu.com/projects/MMSI_Bench%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMSI-Bench%253A%2520A%2520Benchmark%2520for%2520Multi-Image%2520Spatial%2520Intelligence%26entry.906535625%3DSihan%2520Yang%2520and%2520Runsen%2520Xu%2520and%2520Yiman%2520Xie%2520and%2520Sizhe%2520Yang%2520and%2520Mo%2520Li%2520and%2520Jingli%2520Lin%2520and%2520Chenming%2520Zhu%2520and%2520Xiaochen%2520Chen%2520and%2520Haodong%2520Duan%2520and%2520Xiangyu%2520Yue%2520and%2520Dahua%2520Lin%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%2520is%2520essential%2520for%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520operating%2520in%2520the%2520complex%2520physical%2520world.%2520Existing%2520benchmarks%252C%2520however%252C%250Aprobe%2520only%2520single-image%2520relations%2520and%2520thus%2520fail%2520to%2520assess%2520the%2520multi-image%250Aspatial%2520reasoning%2520that%2520real-world%2520deployments%2520demand.%2520We%2520introduce%2520MMSI-Bench%252C%250Aa%2520VQA%2520benchmark%2520dedicated%2520to%2520multi-image%2520spatial%2520intelligence.%2520Six%25203D-vision%250Aresearchers%2520spent%2520more%2520than%2520300%2520hours%2520meticulously%2520crafting%25201%252C000%2520challenging%252C%250Aunambiguous%2520multiple-choice%2520questions%2520from%2520over%2520120%252C000%2520images%252C%2520each%2520paired%250Awith%2520carefully%2520designed%2520distractors%2520and%2520a%2520step-by-step%2520reasoning%2520process.%2520We%250Aconduct%2520extensive%2520experiments%2520and%2520thoroughly%2520evaluate%252034%2520open-source%2520and%250Aproprietary%2520MLLMs%252C%2520observing%2520a%2520wide%2520gap%253A%2520the%2520strongest%2520open-source%2520model%250Aattains%2520roughly%252030%2525%2520accuracy%2520and%2520OpenAI%2527s%2520o3%2520reasoning%2520model%2520reaches%252040%2525%252C%2520while%250Ahumans%2520score%252097%2525.%2520These%2520results%2520underscore%2520the%2520challenging%2520nature%2520of%2520MMSI-Bench%250Aand%2520the%2520substantial%2520headroom%2520for%2520future%2520research.%2520Leveraging%2520the%2520annotated%250Areasoning%2520processes%252C%2520we%2520also%2520provide%2520an%2520automated%2520error%2520analysis%2520pipeline%2520that%250Adiagnoses%2520four%2520dominant%2520failure%2520modes%252C%2520including%2520%25281%2529%2520grounding%2520errors%252C%2520%25282%2529%250Aoverlap-matching%2520and%2520scene-reconstruction%2520errors%252C%2520%25283%2529%2520situation-transformation%250Areasoning%2520errors%252C%2520and%2520%25284%2529%2520spatial-logic%2520errors%252C%2520offering%2520valuable%2520insights%2520for%250Aadvancing%2520multi-image%2520spatial%2520intelligence.%2520Project%2520page%253A%250Ahttps%253A//runsenxu.com/projects/MMSI_Bench%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence&entry.906535625=Sihan%20Yang%20and%20Runsen%20Xu%20and%20Yiman%20Xie%20and%20Sizhe%20Yang%20and%20Mo%20Li%20and%20Jingli%20Lin%20and%20Chenming%20Zhu%20and%20Xiaochen%20Chen%20and%20Haodong%20Duan%20and%20Xiangyu%20Yue%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Spatial%20intelligence%20is%20essential%20for%20multimodal%20large%20language%20models%0A%28MLLMs%29%20operating%20in%20the%20complex%20physical%20world.%20Existing%20benchmarks%2C%20however%2C%0Aprobe%20only%20single-image%20relations%20and%20thus%20fail%20to%20assess%20the%20multi-image%0Aspatial%20reasoning%20that%20real-world%20deployments%20demand.%20We%20introduce%20MMSI-Bench%2C%0Aa%20VQA%20benchmark%20dedicated%20to%20multi-image%20spatial%20intelligence.%20Six%203D-vision%0Aresearchers%20spent%20more%20than%20300%20hours%20meticulously%20crafting%201%2C000%20challenging%2C%0Aunambiguous%20multiple-choice%20questions%20from%20over%20120%2C000%20images%2C%20each%20paired%0Awith%20carefully%20designed%20distractors%20and%20a%20step-by-step%20reasoning%20process.%20We%0Aconduct%20extensive%20experiments%20and%20thoroughly%20evaluate%2034%20open-source%20and%0Aproprietary%20MLLMs%2C%20observing%20a%20wide%20gap%3A%20the%20strongest%20open-source%20model%0Aattains%20roughly%2030%25%20accuracy%20and%20OpenAI%27s%20o3%20reasoning%20model%20reaches%2040%25%2C%20while%0Ahumans%20score%2097%25.%20These%20results%20underscore%20the%20challenging%20nature%20of%20MMSI-Bench%0Aand%20the%20substantial%20headroom%20for%20future%20research.%20Leveraging%20the%20annotated%0Areasoning%20processes%2C%20we%20also%20provide%20an%20automated%20error%20analysis%20pipeline%20that%0Adiagnoses%20four%20dominant%20failure%20modes%2C%20including%20%281%29%20grounding%20errors%2C%20%282%29%0Aoverlap-matching%20and%20scene-reconstruction%20errors%2C%20%283%29%20situation-transformation%0Areasoning%20errors%2C%20and%20%284%29%20spatial-logic%20errors%2C%20offering%20valuable%20insights%20for%0Aadvancing%20multi-image%20spatial%20intelligence.%20Project%20page%3A%0Ahttps%3A//runsenxu.com/projects/MMSI_Bench%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23764v2&entry.124074799=Read"},
{"title": "P3-SAM: Native 3D Part Segmentation", "author": "Changfeng Ma and Yang Li and Xinhao Yan and Jiachen Xu and Yunhan Yang and Chunshi Wang and Zibo Zhao and Yanwen Guo and Zhuo Chen and Chunchao Guo", "abstract": "  Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P$^3$-SAM, designed to fully automate the\nsegmentation of any 3D objects into components. Inspired by SAM, P$^3$-SAM\nconsists of a feature extractor, multiple segmentation heads, and an IoU\npredictor, enabling interactive segmentation for users. We also propose an\nalgorithm to automatically select and merge masks predicted by our model for\npart instance segmentation. Our model is trained on a newly built dataset\ncontaining nearly 3.7 million models with reasonable segmentation labels.\nComparisons show that our method achieves precise segmentation results and\nstrong robustness on any complex objects, attaining state-of-the-art\nperformance. Our project page is available at\nhttps://murcherful.github.io/P3-SAM/.\n", "link": "http://arxiv.org/abs/2509.06784v4", "date": "2025-09-25", "relevancy": 2.8716, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5764}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5764}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3-SAM%3A%20Native%203D%20Part%20Segmentation&body=Title%3A%20P3-SAM%3A%20Native%203D%20Part%20Segmentation%0AAuthor%3A%20Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P%24%5E3%24-SAM%2C%20designed%20to%20fully%20automate%20the%0Asegmentation%20of%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P%24%5E3%24-SAM%0Aconsists%20of%20a%20feature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%0Apredictor%2C%20enabling%20interactive%20segmentation%20for%20users.%20We%20also%20propose%20an%0Aalgorithm%20to%20automatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%0Apart%20instance%20segmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%0Acontaining%20nearly%203.7%20million%20models%20with%20reasonable%20segmentation%20labels.%0AComparisons%20show%20that%20our%20method%20achieves%20precise%20segmentation%20results%20and%0Astrong%20robustness%20on%20any%20complex%20objects%2C%20attaining%20state-of-the-art%0Aperformance.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//murcherful.github.io/P3-SAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06784v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3-SAM%253A%2520Native%25203D%2520Part%2520Segmentation%26entry.906535625%3DChangfeng%2520Ma%2520and%2520Yang%2520Li%2520and%2520Xinhao%2520Yan%2520and%2520Jiachen%2520Xu%2520and%2520Yunhan%2520Yang%2520and%2520Chunshi%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Yanwen%2520Guo%2520and%2520Zhuo%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Segmenting%25203D%2520assets%2520into%2520their%2520constituent%2520parts%2520is%2520crucial%2520for%2520enhancing%25203D%250Aunderstanding%252C%2520facilitating%2520model%2520reuse%252C%2520and%2520supporting%2520various%2520applications%250Asuch%2520as%2520part%2520generation.%2520However%252C%2520current%2520methods%2520face%2520limitations%2520such%2520as%2520poor%250Arobustness%2520when%2520dealing%2520with%2520complex%2520objects%2520and%2520cannot%2520fully%2520automate%2520the%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520native%25203D%2520point-promptable%2520part%250Asegmentation%2520model%2520termed%2520P%2524%255E3%2524-SAM%252C%2520designed%2520to%2520fully%2520automate%2520the%250Asegmentation%2520of%2520any%25203D%2520objects%2520into%2520components.%2520Inspired%2520by%2520SAM%252C%2520P%2524%255E3%2524-SAM%250Aconsists%2520of%2520a%2520feature%2520extractor%252C%2520multiple%2520segmentation%2520heads%252C%2520and%2520an%2520IoU%250Apredictor%252C%2520enabling%2520interactive%2520segmentation%2520for%2520users.%2520We%2520also%2520propose%2520an%250Aalgorithm%2520to%2520automatically%2520select%2520and%2520merge%2520masks%2520predicted%2520by%2520our%2520model%2520for%250Apart%2520instance%2520segmentation.%2520Our%2520model%2520is%2520trained%2520on%2520a%2520newly%2520built%2520dataset%250Acontaining%2520nearly%25203.7%2520million%2520models%2520with%2520reasonable%2520segmentation%2520labels.%250AComparisons%2520show%2520that%2520our%2520method%2520achieves%2520precise%2520segmentation%2520results%2520and%250Astrong%2520robustness%2520on%2520any%2520complex%2520objects%252C%2520attaining%2520state-of-the-art%250Aperformance.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//murcherful.github.io/P3-SAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06784v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3-SAM%3A%20Native%203D%20Part%20Segmentation&entry.906535625=Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P%24%5E3%24-SAM%2C%20designed%20to%20fully%20automate%20the%0Asegmentation%20of%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P%24%5E3%24-SAM%0Aconsists%20of%20a%20feature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%0Apredictor%2C%20enabling%20interactive%20segmentation%20for%20users.%20We%20also%20propose%20an%0Aalgorithm%20to%20automatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%0Apart%20instance%20segmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%0Acontaining%20nearly%203.7%20million%20models%20with%20reasonable%20segmentation%20labels.%0AComparisons%20show%20that%20our%20method%20achieves%20precise%20segmentation%20results%20and%0Astrong%20robustness%20on%20any%20complex%20objects%2C%20attaining%20state-of-the-art%0Aperformance.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//murcherful.github.io/P3-SAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06784v4&entry.124074799=Read"},
{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal\n  Reasoning by Iterative Perception", "author": "Ziang Yan and Xinhao Li and Yinan He and Zhengrong Yue and Xiangyu Zeng and Yali Wang and Yu Qiao and Limin Wang and Yi Wang", "abstract": "  Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception.\n", "link": "http://arxiv.org/abs/2509.21100v1", "date": "2025-09-25", "relevancy": 2.8677, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoChat-R1.5%3A%20Visual%20Test-Time%20Scaling%20to%20Reinforce%20Multimodal%0A%20%20Reasoning%20by%20Iterative%20Perception&body=Title%3A%20VideoChat-R1.5%3A%20Visual%20Test-Time%20Scaling%20to%20Reinforce%20Multimodal%0A%20%20Reasoning%20by%20Iterative%20Perception%0AAuthor%3A%20Ziang%20Yan%20and%20Xinhao%20Li%20and%20Yinan%20He%20and%20Zhengrong%20Yue%20and%20Xiangyu%20Zeng%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Yi%20Wang%0AAbstract%3A%20%20%20Inducing%20reasoning%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20is%20critical%0Afor%20achieving%20human-level%20perception%20and%20understanding.%20Existing%20methods%20mainly%0Aleverage%20LLM%20reasoning%20to%20analyze%20parsed%20visuals%2C%20often%20limited%20by%20static%0Aperception%20stages.%20This%20paper%20introduces%20Visual%20Test-Time%20Scaling%20%28VTTS%29%2C%20a%0Anovel%20approach%20to%20enhance%20MLLMs%27%20reasoning%20via%20iterative%20perception%20during%0Ainference.%20VTTS%20mimics%20humans%27%20hierarchical%20attention%20by%20progressively%20refining%0Afocus%20on%20high-confidence%20spatio-temporal%20regions%2C%20guided%20by%20updated%20textual%0Apredictions.%20Specifically%2C%20VTTS%20employs%20an%20Iterative%20Perception%20%28ITP%29%0Amechanism%2C%20incorporating%20reinforcement%20learning%20with%20spatio-temporal%0Asupervision%20to%20optimize%20reasoning.%20To%20support%20this%20paradigm%2C%20we%20also%20present%0AVTTS-80K%2C%20a%20dataset%20tailored%20for%20iterative%20perception.%20These%20designs%20allows%20a%0AMLLM%20to%20enhance%20its%20performance%20by%20increasing%20its%20perceptual%20compute.%20Extensive%0Aexperiments%20validate%20VTTS%27s%20effectiveness%20and%20generalization%20across%20diverse%0Atasks%20and%20benchmarks.%20Our%20newly%20introduced%20Videochat-R1.5%20model%20has%20achieved%0Aremarkable%20improvements%2C%20with%20an%20average%20increase%20of%20over%205%5C%25%2C%20compared%20to%0Arobust%20baselines%20such%20as%20Qwen2.5VL-3B%20and%20-7B%2C%20across%20more%20than%2015%20benchmarks%0Athat%20encompass%20video%20conversation%2C%20video%20reasoning%2C%20and%20spatio-temporal%0Aperception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoChat-R1.5%253A%2520Visual%2520Test-Time%2520Scaling%2520to%2520Reinforce%2520Multimodal%250A%2520%2520Reasoning%2520by%2520Iterative%2520Perception%26entry.906535625%3DZiang%2520Yan%2520and%2520Xinhao%2520Li%2520and%2520Yinan%2520He%2520and%2520Zhengrong%2520Yue%2520and%2520Xiangyu%2520Zeng%2520and%2520Yali%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520Inducing%2520reasoning%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520is%2520critical%250Afor%2520achieving%2520human-level%2520perception%2520and%2520understanding.%2520Existing%2520methods%2520mainly%250Aleverage%2520LLM%2520reasoning%2520to%2520analyze%2520parsed%2520visuals%252C%2520often%2520limited%2520by%2520static%250Aperception%2520stages.%2520This%2520paper%2520introduces%2520Visual%2520Test-Time%2520Scaling%2520%2528VTTS%2529%252C%2520a%250Anovel%2520approach%2520to%2520enhance%2520MLLMs%2527%2520reasoning%2520via%2520iterative%2520perception%2520during%250Ainference.%2520VTTS%2520mimics%2520humans%2527%2520hierarchical%2520attention%2520by%2520progressively%2520refining%250Afocus%2520on%2520high-confidence%2520spatio-temporal%2520regions%252C%2520guided%2520by%2520updated%2520textual%250Apredictions.%2520Specifically%252C%2520VTTS%2520employs%2520an%2520Iterative%2520Perception%2520%2528ITP%2529%250Amechanism%252C%2520incorporating%2520reinforcement%2520learning%2520with%2520spatio-temporal%250Asupervision%2520to%2520optimize%2520reasoning.%2520To%2520support%2520this%2520paradigm%252C%2520we%2520also%2520present%250AVTTS-80K%252C%2520a%2520dataset%2520tailored%2520for%2520iterative%2520perception.%2520These%2520designs%2520allows%2520a%250AMLLM%2520to%2520enhance%2520its%2520performance%2520by%2520increasing%2520its%2520perceptual%2520compute.%2520Extensive%250Aexperiments%2520validate%2520VTTS%2527s%2520effectiveness%2520and%2520generalization%2520across%2520diverse%250Atasks%2520and%2520benchmarks.%2520Our%2520newly%2520introduced%2520Videochat-R1.5%2520model%2520has%2520achieved%250Aremarkable%2520improvements%252C%2520with%2520an%2520average%2520increase%2520of%2520over%25205%255C%2525%252C%2520compared%2520to%250Arobust%2520baselines%2520such%2520as%2520Qwen2.5VL-3B%2520and%2520-7B%252C%2520across%2520more%2520than%252015%2520benchmarks%250Athat%2520encompass%2520video%2520conversation%252C%2520video%2520reasoning%252C%2520and%2520spatio-temporal%250Aperception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoChat-R1.5%3A%20Visual%20Test-Time%20Scaling%20to%20Reinforce%20Multimodal%0A%20%20Reasoning%20by%20Iterative%20Perception&entry.906535625=Ziang%20Yan%20and%20Xinhao%20Li%20and%20Yinan%20He%20and%20Zhengrong%20Yue%20and%20Xiangyu%20Zeng%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Yi%20Wang&entry.1292438233=%20%20Inducing%20reasoning%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20is%20critical%0Afor%20achieving%20human-level%20perception%20and%20understanding.%20Existing%20methods%20mainly%0Aleverage%20LLM%20reasoning%20to%20analyze%20parsed%20visuals%2C%20often%20limited%20by%20static%0Aperception%20stages.%20This%20paper%20introduces%20Visual%20Test-Time%20Scaling%20%28VTTS%29%2C%20a%0Anovel%20approach%20to%20enhance%20MLLMs%27%20reasoning%20via%20iterative%20perception%20during%0Ainference.%20VTTS%20mimics%20humans%27%20hierarchical%20attention%20by%20progressively%20refining%0Afocus%20on%20high-confidence%20spatio-temporal%20regions%2C%20guided%20by%20updated%20textual%0Apredictions.%20Specifically%2C%20VTTS%20employs%20an%20Iterative%20Perception%20%28ITP%29%0Amechanism%2C%20incorporating%20reinforcement%20learning%20with%20spatio-temporal%0Asupervision%20to%20optimize%20reasoning.%20To%20support%20this%20paradigm%2C%20we%20also%20present%0AVTTS-80K%2C%20a%20dataset%20tailored%20for%20iterative%20perception.%20These%20designs%20allows%20a%0AMLLM%20to%20enhance%20its%20performance%20by%20increasing%20its%20perceptual%20compute.%20Extensive%0Aexperiments%20validate%20VTTS%27s%20effectiveness%20and%20generalization%20across%20diverse%0Atasks%20and%20benchmarks.%20Our%20newly%20introduced%20Videochat-R1.5%20model%20has%20achieved%0Aremarkable%20improvements%2C%20with%20an%20average%20increase%20of%20over%205%5C%25%2C%20compared%20to%0Arobust%20baselines%20such%20as%20Qwen2.5VL-3B%20and%20-7B%2C%20across%20more%20than%2015%20benchmarks%0Athat%20encompass%20video%20conversation%2C%20video%20reasoning%2C%20and%20spatio-temporal%0Aperception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21100v1&entry.124074799=Read"},
{"title": "CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling", "author": "Yuze He and Yanning Zhou and Wang Zhao and Jingwen Ye and Yushi Bai and Kaiwen Xiao and Yong-Jin Liu and Zhongqian Sun and Wei Yang", "abstract": "  We present CHARM, a novel parametric representation and generative framework\nfor anime hairstyle modeling. While traditional hair modeling methods focus on\nrealistic hair using strand-based or volumetric representations, anime\nhairstyle exhibits highly stylized, piecewise-structured geometry that\nchallenges existing techniques. Existing works often rely on dense mesh\nmodeling or hand-crafted spline curves, making them inefficient for editing and\nunsuitable for scalable learning. CHARM introduces a compact, invertible\ncontrol-point-based parameterization, where a sequence of control points\nrepresents each hair card, and each point is encoded with only five geometric\nparameters. This efficient and accurate representation supports both\nartist-friendly design and learning-based generation. Built upon this\nrepresentation, CHARM introduces an autoregressive generative framework that\neffectively generates anime hairstyles from input images or point clouds. By\ninterpreting anime hairstyles as a sequential \"hair language\", our\nautoregressive transformer captures both local geometry and global hairstyle\ntopology, resulting in high-fidelity anime hairstyle creation. To facilitate\nboth training and evaluation of anime hairstyle generation, we construct\nAnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with\nseparated hair cards and processed mesh data. Extensive experiments demonstrate\nstate-of-the-art performance of CHARM in both reconstruction accuracy and\ngeneration quality, offering an expressive and scalable solution for anime\nhairstyle modeling. Project page: https://hyzcluster.github.io/charm/\n", "link": "http://arxiv.org/abs/2509.21114v1", "date": "2025-09-25", "relevancy": 2.8348, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5825}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5664}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHARM%3A%20Control-point-based%203D%20Anime%20Hairstyle%20Auto-Regressive%20Modeling&body=Title%3A%20CHARM%3A%20Control-point-based%203D%20Anime%20Hairstyle%20Auto-Regressive%20Modeling%0AAuthor%3A%20Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Jingwen%20Ye%20and%20Yushi%20Bai%20and%20Kaiwen%20Xiao%20and%20Yong-Jin%20Liu%20and%20Zhongqian%20Sun%20and%20Wei%20Yang%0AAbstract%3A%20%20%20We%20present%20CHARM%2C%20a%20novel%20parametric%20representation%20and%20generative%20framework%0Afor%20anime%20hairstyle%20modeling.%20While%20traditional%20hair%20modeling%20methods%20focus%20on%0Arealistic%20hair%20using%20strand-based%20or%20volumetric%20representations%2C%20anime%0Ahairstyle%20exhibits%20highly%20stylized%2C%20piecewise-structured%20geometry%20that%0Achallenges%20existing%20techniques.%20Existing%20works%20often%20rely%20on%20dense%20mesh%0Amodeling%20or%20hand-crafted%20spline%20curves%2C%20making%20them%20inefficient%20for%20editing%20and%0Aunsuitable%20for%20scalable%20learning.%20CHARM%20introduces%20a%20compact%2C%20invertible%0Acontrol-point-based%20parameterization%2C%20where%20a%20sequence%20of%20control%20points%0Arepresents%20each%20hair%20card%2C%20and%20each%20point%20is%20encoded%20with%20only%20five%20geometric%0Aparameters.%20This%20efficient%20and%20accurate%20representation%20supports%20both%0Aartist-friendly%20design%20and%20learning-based%20generation.%20Built%20upon%20this%0Arepresentation%2C%20CHARM%20introduces%20an%20autoregressive%20generative%20framework%20that%0Aeffectively%20generates%20anime%20hairstyles%20from%20input%20images%20or%20point%20clouds.%20By%0Ainterpreting%20anime%20hairstyles%20as%20a%20sequential%20%22hair%20language%22%2C%20our%0Aautoregressive%20transformer%20captures%20both%20local%20geometry%20and%20global%20hairstyle%0Atopology%2C%20resulting%20in%20high-fidelity%20anime%20hairstyle%20creation.%20To%20facilitate%0Aboth%20training%20and%20evaluation%20of%20anime%20hairstyle%20generation%2C%20we%20construct%0AAnimeHair%2C%20a%20large-scale%20dataset%20of%2037K%20high-quality%20anime%20hairstyles%20with%0Aseparated%20hair%20cards%20and%20processed%20mesh%20data.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20performance%20of%20CHARM%20in%20both%20reconstruction%20accuracy%20and%0Ageneration%20quality%2C%20offering%20an%20expressive%20and%20scalable%20solution%20for%20anime%0Ahairstyle%20modeling.%20Project%20page%3A%20https%3A//hyzcluster.github.io/charm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHARM%253A%2520Control-point-based%25203D%2520Anime%2520Hairstyle%2520Auto-Regressive%2520Modeling%26entry.906535625%3DYuze%2520He%2520and%2520Yanning%2520Zhou%2520and%2520Wang%2520Zhao%2520and%2520Jingwen%2520Ye%2520and%2520Yushi%2520Bai%2520and%2520Kaiwen%2520Xiao%2520and%2520Yong-Jin%2520Liu%2520and%2520Zhongqian%2520Sun%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520CHARM%252C%2520a%2520novel%2520parametric%2520representation%2520and%2520generative%2520framework%250Afor%2520anime%2520hairstyle%2520modeling.%2520While%2520traditional%2520hair%2520modeling%2520methods%2520focus%2520on%250Arealistic%2520hair%2520using%2520strand-based%2520or%2520volumetric%2520representations%252C%2520anime%250Ahairstyle%2520exhibits%2520highly%2520stylized%252C%2520piecewise-structured%2520geometry%2520that%250Achallenges%2520existing%2520techniques.%2520Existing%2520works%2520often%2520rely%2520on%2520dense%2520mesh%250Amodeling%2520or%2520hand-crafted%2520spline%2520curves%252C%2520making%2520them%2520inefficient%2520for%2520editing%2520and%250Aunsuitable%2520for%2520scalable%2520learning.%2520CHARM%2520introduces%2520a%2520compact%252C%2520invertible%250Acontrol-point-based%2520parameterization%252C%2520where%2520a%2520sequence%2520of%2520control%2520points%250Arepresents%2520each%2520hair%2520card%252C%2520and%2520each%2520point%2520is%2520encoded%2520with%2520only%2520five%2520geometric%250Aparameters.%2520This%2520efficient%2520and%2520accurate%2520representation%2520supports%2520both%250Aartist-friendly%2520design%2520and%2520learning-based%2520generation.%2520Built%2520upon%2520this%250Arepresentation%252C%2520CHARM%2520introduces%2520an%2520autoregressive%2520generative%2520framework%2520that%250Aeffectively%2520generates%2520anime%2520hairstyles%2520from%2520input%2520images%2520or%2520point%2520clouds.%2520By%250Ainterpreting%2520anime%2520hairstyles%2520as%2520a%2520sequential%2520%2522hair%2520language%2522%252C%2520our%250Aautoregressive%2520transformer%2520captures%2520both%2520local%2520geometry%2520and%2520global%2520hairstyle%250Atopology%252C%2520resulting%2520in%2520high-fidelity%2520anime%2520hairstyle%2520creation.%2520To%2520facilitate%250Aboth%2520training%2520and%2520evaluation%2520of%2520anime%2520hairstyle%2520generation%252C%2520we%2520construct%250AAnimeHair%252C%2520a%2520large-scale%2520dataset%2520of%252037K%2520high-quality%2520anime%2520hairstyles%2520with%250Aseparated%2520hair%2520cards%2520and%2520processed%2520mesh%2520data.%2520Extensive%2520experiments%2520demonstrate%250Astate-of-the-art%2520performance%2520of%2520CHARM%2520in%2520both%2520reconstruction%2520accuracy%2520and%250Ageneration%2520quality%252C%2520offering%2520an%2520expressive%2520and%2520scalable%2520solution%2520for%2520anime%250Ahairstyle%2520modeling.%2520Project%2520page%253A%2520https%253A//hyzcluster.github.io/charm/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHARM%3A%20Control-point-based%203D%20Anime%20Hairstyle%20Auto-Regressive%20Modeling&entry.906535625=Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Jingwen%20Ye%20and%20Yushi%20Bai%20and%20Kaiwen%20Xiao%20and%20Yong-Jin%20Liu%20and%20Zhongqian%20Sun%20and%20Wei%20Yang&entry.1292438233=%20%20We%20present%20CHARM%2C%20a%20novel%20parametric%20representation%20and%20generative%20framework%0Afor%20anime%20hairstyle%20modeling.%20While%20traditional%20hair%20modeling%20methods%20focus%20on%0Arealistic%20hair%20using%20strand-based%20or%20volumetric%20representations%2C%20anime%0Ahairstyle%20exhibits%20highly%20stylized%2C%20piecewise-structured%20geometry%20that%0Achallenges%20existing%20techniques.%20Existing%20works%20often%20rely%20on%20dense%20mesh%0Amodeling%20or%20hand-crafted%20spline%20curves%2C%20making%20them%20inefficient%20for%20editing%20and%0Aunsuitable%20for%20scalable%20learning.%20CHARM%20introduces%20a%20compact%2C%20invertible%0Acontrol-point-based%20parameterization%2C%20where%20a%20sequence%20of%20control%20points%0Arepresents%20each%20hair%20card%2C%20and%20each%20point%20is%20encoded%20with%20only%20five%20geometric%0Aparameters.%20This%20efficient%20and%20accurate%20representation%20supports%20both%0Aartist-friendly%20design%20and%20learning-based%20generation.%20Built%20upon%20this%0Arepresentation%2C%20CHARM%20introduces%20an%20autoregressive%20generative%20framework%20that%0Aeffectively%20generates%20anime%20hairstyles%20from%20input%20images%20or%20point%20clouds.%20By%0Ainterpreting%20anime%20hairstyles%20as%20a%20sequential%20%22hair%20language%22%2C%20our%0Aautoregressive%20transformer%20captures%20both%20local%20geometry%20and%20global%20hairstyle%0Atopology%2C%20resulting%20in%20high-fidelity%20anime%20hairstyle%20creation.%20To%20facilitate%0Aboth%20training%20and%20evaluation%20of%20anime%20hairstyle%20generation%2C%20we%20construct%0AAnimeHair%2C%20a%20large-scale%20dataset%20of%2037K%20high-quality%20anime%20hairstyles%20with%0Aseparated%20hair%20cards%20and%20processed%20mesh%20data.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20performance%20of%20CHARM%20in%20both%20reconstruction%20accuracy%20and%0Ageneration%20quality%2C%20offering%20an%20expressive%20and%20scalable%20solution%20for%20anime%0Ahairstyle%20modeling.%20Project%20page%3A%20https%3A//hyzcluster.github.io/charm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21114v1&entry.124074799=Read"},
{"title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning", "author": "You-Won Jang and Yu-Jung Heo and Jaeseok Kim and Minsu Lee and Du-Seong Chang and Byoung-Tak Zhang", "abstract": "  The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works.\n", "link": "http://arxiv.org/abs/2509.21251v1", "date": "2025-09-25", "relevancy": 2.8297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-tuned%20Self-Questioning%20Framework%20for%20Multimodal%20Reasoning&body=Title%3A%20Instruction-tuned%20Self-Questioning%20Framework%20for%20Multimodal%20Reasoning%0AAuthor%3A%20You-Won%20Jang%20and%20Yu-Jung%20Heo%20and%20Jaeseok%20Kim%20and%20Minsu%20Lee%20and%20Du-Seong%20Chang%20and%20Byoung-Tak%20Zhang%0AAbstract%3A%20%20%20The%20field%20of%20vision-language%20understanding%20has%20been%20actively%20researched%20in%0Arecent%20years%2C%20thanks%20to%20the%20development%20of%20Large%20Language%20Models~%28LLMs%29.%0AHowever%2C%20it%20still%20needs%20help%20with%20problems%20requiring%20multi-step%20reasoning%2C%20even%0Afor%20very%20simple%20questions.%20Recent%20studies%20adopt%20LLMs%20to%20tackle%20this%20problem%20by%0Aiteratively%20generating%20sub-questions%20and%20answers.%20However%2C%20there%20are%0Adisadvantages%20such%20as%201%29%20the%20fine-grained%20visual%20contents%20of%20images%20are%20not%0Aavailable%20using%20LLMs%20that%20cannot%20read%20visual%20information%2C%202%29%20internal%0Amechanisms%20are%20inaccessible%20and%20difficult%20to%20reproduce%20by%20using%20black-box%20LLMs.%0ATo%20solve%20these%20problems%2C%20we%20propose%20the%20SQ%20%28Self-Questioning%29-InstructBLIP%2C%0Awhich%20improves%20inference%20performance%20by%20generating%20image-aware%20informative%0Asub-questions%20and%20sub-answers%20iteratively.%20The%20SQ-InstructBLIP%2C%20which%20consists%0Aof%20a%20Questioner%2C%20Answerer%2C%20and%20Reasoner%20that%20share%20the%20same%20architecture.%0AQuestioner%20and%20Answerer%20generate%20sub-questions%20and%20sub-answers%20to%20help%20infer%0Athe%20main-question%2C%20and%20Reasoner%20performs%20reasoning%20on%20the%20main-question%0Aconsidering%20the%20generated%20sub-question%20information.%20Our%20experiments%20show%20that%0Athe%20proposed%20method%20SQ-InstructBLIP%2C%20which%20uses%20the%20generated%20sub-questions%20as%0Aadditional%20information%20when%20solving%20the%20VQA%20task%2C%20performs%20more%20accurate%0Areasoning%20than%20the%20previous%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-tuned%2520Self-Questioning%2520Framework%2520for%2520Multimodal%2520Reasoning%26entry.906535625%3DYou-Won%2520Jang%2520and%2520Yu-Jung%2520Heo%2520and%2520Jaeseok%2520Kim%2520and%2520Minsu%2520Lee%2520and%2520Du-Seong%2520Chang%2520and%2520Byoung-Tak%2520Zhang%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520vision-language%2520understanding%2520has%2520been%2520actively%2520researched%2520in%250Arecent%2520years%252C%2520thanks%2520to%2520the%2520development%2520of%2520Large%2520Language%2520Models~%2528LLMs%2529.%250AHowever%252C%2520it%2520still%2520needs%2520help%2520with%2520problems%2520requiring%2520multi-step%2520reasoning%252C%2520even%250Afor%2520very%2520simple%2520questions.%2520Recent%2520studies%2520adopt%2520LLMs%2520to%2520tackle%2520this%2520problem%2520by%250Aiteratively%2520generating%2520sub-questions%2520and%2520answers.%2520However%252C%2520there%2520are%250Adisadvantages%2520such%2520as%25201%2529%2520the%2520fine-grained%2520visual%2520contents%2520of%2520images%2520are%2520not%250Aavailable%2520using%2520LLMs%2520that%2520cannot%2520read%2520visual%2520information%252C%25202%2529%2520internal%250Amechanisms%2520are%2520inaccessible%2520and%2520difficult%2520to%2520reproduce%2520by%2520using%2520black-box%2520LLMs.%250ATo%2520solve%2520these%2520problems%252C%2520we%2520propose%2520the%2520SQ%2520%2528Self-Questioning%2529-InstructBLIP%252C%250Awhich%2520improves%2520inference%2520performance%2520by%2520generating%2520image-aware%2520informative%250Asub-questions%2520and%2520sub-answers%2520iteratively.%2520The%2520SQ-InstructBLIP%252C%2520which%2520consists%250Aof%2520a%2520Questioner%252C%2520Answerer%252C%2520and%2520Reasoner%2520that%2520share%2520the%2520same%2520architecture.%250AQuestioner%2520and%2520Answerer%2520generate%2520sub-questions%2520and%2520sub-answers%2520to%2520help%2520infer%250Athe%2520main-question%252C%2520and%2520Reasoner%2520performs%2520reasoning%2520on%2520the%2520main-question%250Aconsidering%2520the%2520generated%2520sub-question%2520information.%2520Our%2520experiments%2520show%2520that%250Athe%2520proposed%2520method%2520SQ-InstructBLIP%252C%2520which%2520uses%2520the%2520generated%2520sub-questions%2520as%250Aadditional%2520information%2520when%2520solving%2520the%2520VQA%2520task%252C%2520performs%2520more%2520accurate%250Areasoning%2520than%2520the%2520previous%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-tuned%20Self-Questioning%20Framework%20for%20Multimodal%20Reasoning&entry.906535625=You-Won%20Jang%20and%20Yu-Jung%20Heo%20and%20Jaeseok%20Kim%20and%20Minsu%20Lee%20and%20Du-Seong%20Chang%20and%20Byoung-Tak%20Zhang&entry.1292438233=%20%20The%20field%20of%20vision-language%20understanding%20has%20been%20actively%20researched%20in%0Arecent%20years%2C%20thanks%20to%20the%20development%20of%20Large%20Language%20Models~%28LLMs%29.%0AHowever%2C%20it%20still%20needs%20help%20with%20problems%20requiring%20multi-step%20reasoning%2C%20even%0Afor%20very%20simple%20questions.%20Recent%20studies%20adopt%20LLMs%20to%20tackle%20this%20problem%20by%0Aiteratively%20generating%20sub-questions%20and%20answers.%20However%2C%20there%20are%0Adisadvantages%20such%20as%201%29%20the%20fine-grained%20visual%20contents%20of%20images%20are%20not%0Aavailable%20using%20LLMs%20that%20cannot%20read%20visual%20information%2C%202%29%20internal%0Amechanisms%20are%20inaccessible%20and%20difficult%20to%20reproduce%20by%20using%20black-box%20LLMs.%0ATo%20solve%20these%20problems%2C%20we%20propose%20the%20SQ%20%28Self-Questioning%29-InstructBLIP%2C%0Awhich%20improves%20inference%20performance%20by%20generating%20image-aware%20informative%0Asub-questions%20and%20sub-answers%20iteratively.%20The%20SQ-InstructBLIP%2C%20which%20consists%0Aof%20a%20Questioner%2C%20Answerer%2C%20and%20Reasoner%20that%20share%20the%20same%20architecture.%0AQuestioner%20and%20Answerer%20generate%20sub-questions%20and%20sub-answers%20to%20help%20infer%0Athe%20main-question%2C%20and%20Reasoner%20performs%20reasoning%20on%20the%20main-question%0Aconsidering%20the%20generated%20sub-question%20information.%20Our%20experiments%20show%20that%0Athe%20proposed%20method%20SQ-InstructBLIP%2C%20which%20uses%20the%20generated%20sub-questions%20as%0Aadditional%20information%20when%20solving%20the%20VQA%20task%2C%20performs%20more%20accurate%0Areasoning%20than%20the%20previous%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21251v1&entry.124074799=Read"},
{"title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic\n  Alignment", "author": "Shengzhu Yang and Jiawei Du and Shuai Lu and Weihang Zhang and Ningli Wang and Huiqi Li", "abstract": "  Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.\n", "link": "http://arxiv.org/abs/2508.06434v2", "date": "2025-09-25", "relevancy": 2.824, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment&body=Title%3A%20CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment%0AAuthor%3A%20Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Ningli%20Wang%20and%20Huiqi%20Li%0AAbstract%3A%20%20%20Large-scale%20natural%20image-text%20datasets%2C%20especially%20those%20automatically%0Acollected%20from%20the%20web%2C%20often%20suffer%20from%20loose%20semantic%20alignment%20due%20to%20weak%0Asupervision%2C%20while%20medical%20datasets%20tend%20to%20have%20high%20cross-modal%20correlation%0Abut%20low%20content%20diversity.%20These%20properties%20pose%20a%20common%20challenge%20for%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%3A%20they%20hinder%20the%20model%27s%20ability%0Ato%20learn%20robust%20and%20generalizable%20representations.%20In%20this%20work%2C%20we%20propose%0ACLIPin%2C%20a%20unified%20non-contrastive%20plug-in%20that%20can%20be%20seamlessly%20integrated%0Ainto%20CLIP-style%20architectures%20to%20improve%20multimodal%20semantic%20alignment%2C%0Aproviding%20stronger%20supervision%20and%20enhancing%20alignment%20robustness.%20Furthermore%2C%0Atwo%20shared%20pre-projectors%20are%20designed%20for%20image%20and%20text%20modalities%0Arespectively%20to%20facilitate%20the%20integration%20of%20contrastive%20and%20non-contrastive%0Alearning%20in%20a%20parameter-compromise%20manner.%20Extensive%20experiments%20on%20diverse%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20and%20generality%20of%20CLIPin%20as%20a%0Aplug-and-play%20component%20compatible%20with%20various%20contrastive%20frameworks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/T6Yang/CLIPin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPin%253A%2520A%2520Non-contrastive%2520Plug-in%2520to%2520CLIP%2520for%2520Multimodal%2520Semantic%250A%2520%2520Alignment%26entry.906535625%3DShengzhu%2520Yang%2520and%2520Jiawei%2520Du%2520and%2520Shuai%2520Lu%2520and%2520Weihang%2520Zhang%2520and%2520Ningli%2520Wang%2520and%2520Huiqi%2520Li%26entry.1292438233%3D%2520%2520Large-scale%2520natural%2520image-text%2520datasets%252C%2520especially%2520those%2520automatically%250Acollected%2520from%2520the%2520web%252C%2520often%2520suffer%2520from%2520loose%2520semantic%2520alignment%2520due%2520to%2520weak%250Asupervision%252C%2520while%2520medical%2520datasets%2520tend%2520to%2520have%2520high%2520cross-modal%2520correlation%250Abut%2520low%2520content%2520diversity.%2520These%2520properties%2520pose%2520a%2520common%2520challenge%2520for%250Acontrastive%2520language-image%2520pretraining%2520%2528CLIP%2529%253A%2520they%2520hinder%2520the%2520model%2527s%2520ability%250Ato%2520learn%2520robust%2520and%2520generalizable%2520representations.%2520In%2520this%2520work%252C%2520we%2520propose%250ACLIPin%252C%2520a%2520unified%2520non-contrastive%2520plug-in%2520that%2520can%2520be%2520seamlessly%2520integrated%250Ainto%2520CLIP-style%2520architectures%2520to%2520improve%2520multimodal%2520semantic%2520alignment%252C%250Aproviding%2520stronger%2520supervision%2520and%2520enhancing%2520alignment%2520robustness.%2520Furthermore%252C%250Atwo%2520shared%2520pre-projectors%2520are%2520designed%2520for%2520image%2520and%2520text%2520modalities%250Arespectively%2520to%2520facilitate%2520the%2520integration%2520of%2520contrastive%2520and%2520non-contrastive%250Alearning%2520in%2520a%2520parameter-compromise%2520manner.%2520Extensive%2520experiments%2520on%2520diverse%250Adownstream%2520tasks%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520CLIPin%2520as%2520a%250Aplug-and-play%2520component%2520compatible%2520with%2520various%2520contrastive%2520frameworks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/T6Yang/CLIPin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPin%3A%20A%20Non-contrastive%20Plug-in%20to%20CLIP%20for%20Multimodal%20Semantic%0A%20%20Alignment&entry.906535625=Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Ningli%20Wang%20and%20Huiqi%20Li&entry.1292438233=%20%20Large-scale%20natural%20image-text%20datasets%2C%20especially%20those%20automatically%0Acollected%20from%20the%20web%2C%20often%20suffer%20from%20loose%20semantic%20alignment%20due%20to%20weak%0Asupervision%2C%20while%20medical%20datasets%20tend%20to%20have%20high%20cross-modal%20correlation%0Abut%20low%20content%20diversity.%20These%20properties%20pose%20a%20common%20challenge%20for%0Acontrastive%20language-image%20pretraining%20%28CLIP%29%3A%20they%20hinder%20the%20model%27s%20ability%0Ato%20learn%20robust%20and%20generalizable%20representations.%20In%20this%20work%2C%20we%20propose%0ACLIPin%2C%20a%20unified%20non-contrastive%20plug-in%20that%20can%20be%20seamlessly%20integrated%0Ainto%20CLIP-style%20architectures%20to%20improve%20multimodal%20semantic%20alignment%2C%0Aproviding%20stronger%20supervision%20and%20enhancing%20alignment%20robustness.%20Furthermore%2C%0Atwo%20shared%20pre-projectors%20are%20designed%20for%20image%20and%20text%20modalities%0Arespectively%20to%20facilitate%20the%20integration%20of%20contrastive%20and%20non-contrastive%0Alearning%20in%20a%20parameter-compromise%20manner.%20Extensive%20experiments%20on%20diverse%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20and%20generality%20of%20CLIPin%20as%20a%0Aplug-and-play%20component%20compatible%20with%20various%20contrastive%20frameworks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/T6Yang/CLIPin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06434v2&entry.124074799=Read"},
{"title": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle\n  Zero-shot Radiology Recognition?", "author": "Bangyan Li and Wenxuan Huang and Zhenkun Gao and Yeqiang Wang and Yunhang Shen and Jingzhong Lin and Ling You and Yuxiang Shen and Shaohui Lin and Wanli Ouyang and Yuling Sun", "abstract": "  Recently, Multimodal Large Language Models (MLLMs) have demonstrated\nexceptional capabilities in visual understanding and reasoning across various\nvision-language tasks. However, we found that MLLMs cannot process effectively\nfrom fine-grained medical image data in the traditional Visual Question\nAnswering (VQA) pipeline, as they do not exploit the captured features and\navailable medical knowledge fully, results in MLLMs usually performing poorly\nin zero-shot medical disease recognition. Fortunately, this limitation does not\nindicate that MLLMs are fundamentally incapable of addressing fine-grained\nrecognition tasks. From a feature representation perspective, MLLMs demonstrate\nconsiderable potential for tackling such challenging problems. Thus, to address\nthis challenge, we propose LLaVA-RadZ, a simple yet effective framework for\nzero-shot medical disease recognition via utilizing the existing MLLM features.\nSpecifically, we design an end-to-end training strategy, termed Decoding-Side\nFeature Alignment Training (DFAT) to take advantage of the characteristics of\nthe MLLM decoder architecture and incorporate modality-specific tokens tailored\nfor different modalities. Additionally, we introduce a Domain Knowledge\nAnchoring Module (DKAM) to exploit the intrinsic medical knowledge of large\nmodels, which mitigates the category semantic gap in image-text alignment.\nExtensive experiments demonstrate that our LLaVA-RadZ significantly outperforms\ntraditional MLLMs in zero-shot disease recognition, achieving the comparable\nperformance to the well-established and highly-optimized CLIP-based approaches.\n", "link": "http://arxiv.org/abs/2503.07487v2", "date": "2025-09-25", "relevancy": 2.8106, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-RadZ%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Tackle%0A%20%20Zero-shot%20Radiology%20Recognition%3F&body=Title%3A%20LLaVA-RadZ%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Tackle%0A%20%20Zero-shot%20Radiology%20Recognition%3F%0AAuthor%3A%20Bangyan%20Li%20and%20Wenxuan%20Huang%20and%20Zhenkun%20Gao%20and%20Yeqiang%20Wang%20and%20Yunhang%20Shen%20and%20Jingzhong%20Lin%20and%20Ling%20You%20and%20Yuxiang%20Shen%20and%20Shaohui%20Lin%20and%20Wanli%20Ouyang%20and%20Yuling%20Sun%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aexceptional%20capabilities%20in%20visual%20understanding%20and%20reasoning%20across%20various%0Avision-language%20tasks.%20However%2C%20we%20found%20that%20MLLMs%20cannot%20process%20effectively%0Afrom%20fine-grained%20medical%20image%20data%20in%20the%20traditional%20Visual%20Question%0AAnswering%20%28VQA%29%20pipeline%2C%20as%20they%20do%20not%20exploit%20the%20captured%20features%20and%0Aavailable%20medical%20knowledge%20fully%2C%20results%20in%20MLLMs%20usually%20performing%20poorly%0Ain%20zero-shot%20medical%20disease%20recognition.%20Fortunately%2C%20this%20limitation%20does%20not%0Aindicate%20that%20MLLMs%20are%20fundamentally%20incapable%20of%20addressing%20fine-grained%0Arecognition%20tasks.%20From%20a%20feature%20representation%20perspective%2C%20MLLMs%20demonstrate%0Aconsiderable%20potential%20for%20tackling%20such%20challenging%20problems.%20Thus%2C%20to%20address%0Athis%20challenge%2C%20we%20propose%20LLaVA-RadZ%2C%20a%20simple%20yet%20effective%20framework%20for%0Azero-shot%20medical%20disease%20recognition%20via%20utilizing%20the%20existing%20MLLM%20features.%0ASpecifically%2C%20we%20design%20an%20end-to-end%20training%20strategy%2C%20termed%20Decoding-Side%0AFeature%20Alignment%20Training%20%28DFAT%29%20to%20take%20advantage%20of%20the%20characteristics%20of%0Athe%20MLLM%20decoder%20architecture%20and%20incorporate%20modality-specific%20tokens%20tailored%0Afor%20different%20modalities.%20Additionally%2C%20we%20introduce%20a%20Domain%20Knowledge%0AAnchoring%20Module%20%28DKAM%29%20to%20exploit%20the%20intrinsic%20medical%20knowledge%20of%20large%0Amodels%2C%20which%20mitigates%20the%20category%20semantic%20gap%20in%20image-text%20alignment.%0AExtensive%20experiments%20demonstrate%20that%20our%20LLaVA-RadZ%20significantly%20outperforms%0Atraditional%20MLLMs%20in%20zero-shot%20disease%20recognition%2C%20achieving%20the%20comparable%0Aperformance%20to%20the%20well-established%20and%20highly-optimized%20CLIP-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-RadZ%253A%2520Can%2520Multimodal%2520Large%2520Language%2520Models%2520Effectively%2520Tackle%250A%2520%2520Zero-shot%2520Radiology%2520Recognition%253F%26entry.906535625%3DBangyan%2520Li%2520and%2520Wenxuan%2520Huang%2520and%2520Zhenkun%2520Gao%2520and%2520Yeqiang%2520Wang%2520and%2520Yunhang%2520Shen%2520and%2520Jingzhong%2520Lin%2520and%2520Ling%2520You%2520and%2520Yuxiang%2520Shen%2520and%2520Shaohui%2520Lin%2520and%2520Wanli%2520Ouyang%2520and%2520Yuling%2520Sun%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%250Aexceptional%2520capabilities%2520in%2520visual%2520understanding%2520and%2520reasoning%2520across%2520various%250Avision-language%2520tasks.%2520However%252C%2520we%2520found%2520that%2520MLLMs%2520cannot%2520process%2520effectively%250Afrom%2520fine-grained%2520medical%2520image%2520data%2520in%2520the%2520traditional%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520pipeline%252C%2520as%2520they%2520do%2520not%2520exploit%2520the%2520captured%2520features%2520and%250Aavailable%2520medical%2520knowledge%2520fully%252C%2520results%2520in%2520MLLMs%2520usually%2520performing%2520poorly%250Ain%2520zero-shot%2520medical%2520disease%2520recognition.%2520Fortunately%252C%2520this%2520limitation%2520does%2520not%250Aindicate%2520that%2520MLLMs%2520are%2520fundamentally%2520incapable%2520of%2520addressing%2520fine-grained%250Arecognition%2520tasks.%2520From%2520a%2520feature%2520representation%2520perspective%252C%2520MLLMs%2520demonstrate%250Aconsiderable%2520potential%2520for%2520tackling%2520such%2520challenging%2520problems.%2520Thus%252C%2520to%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520LLaVA-RadZ%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520for%250Azero-shot%2520medical%2520disease%2520recognition%2520via%2520utilizing%2520the%2520existing%2520MLLM%2520features.%250ASpecifically%252C%2520we%2520design%2520an%2520end-to-end%2520training%2520strategy%252C%2520termed%2520Decoding-Side%250AFeature%2520Alignment%2520Training%2520%2528DFAT%2529%2520to%2520take%2520advantage%2520of%2520the%2520characteristics%2520of%250Athe%2520MLLM%2520decoder%2520architecture%2520and%2520incorporate%2520modality-specific%2520tokens%2520tailored%250Afor%2520different%2520modalities.%2520Additionally%252C%2520we%2520introduce%2520a%2520Domain%2520Knowledge%250AAnchoring%2520Module%2520%2528DKAM%2529%2520to%2520exploit%2520the%2520intrinsic%2520medical%2520knowledge%2520of%2520large%250Amodels%252C%2520which%2520mitigates%2520the%2520category%2520semantic%2520gap%2520in%2520image-text%2520alignment.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520LLaVA-RadZ%2520significantly%2520outperforms%250Atraditional%2520MLLMs%2520in%2520zero-shot%2520disease%2520recognition%252C%2520achieving%2520the%2520comparable%250Aperformance%2520to%2520the%2520well-established%2520and%2520highly-optimized%2520CLIP-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-RadZ%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Tackle%0A%20%20Zero-shot%20Radiology%20Recognition%3F&entry.906535625=Bangyan%20Li%20and%20Wenxuan%20Huang%20and%20Zhenkun%20Gao%20and%20Yeqiang%20Wang%20and%20Yunhang%20Shen%20and%20Jingzhong%20Lin%20and%20Ling%20You%20and%20Yuxiang%20Shen%20and%20Shaohui%20Lin%20and%20Wanli%20Ouyang%20and%20Yuling%20Sun&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aexceptional%20capabilities%20in%20visual%20understanding%20and%20reasoning%20across%20various%0Avision-language%20tasks.%20However%2C%20we%20found%20that%20MLLMs%20cannot%20process%20effectively%0Afrom%20fine-grained%20medical%20image%20data%20in%20the%20traditional%20Visual%20Question%0AAnswering%20%28VQA%29%20pipeline%2C%20as%20they%20do%20not%20exploit%20the%20captured%20features%20and%0Aavailable%20medical%20knowledge%20fully%2C%20results%20in%20MLLMs%20usually%20performing%20poorly%0Ain%20zero-shot%20medical%20disease%20recognition.%20Fortunately%2C%20this%20limitation%20does%20not%0Aindicate%20that%20MLLMs%20are%20fundamentally%20incapable%20of%20addressing%20fine-grained%0Arecognition%20tasks.%20From%20a%20feature%20representation%20perspective%2C%20MLLMs%20demonstrate%0Aconsiderable%20potential%20for%20tackling%20such%20challenging%20problems.%20Thus%2C%20to%20address%0Athis%20challenge%2C%20we%20propose%20LLaVA-RadZ%2C%20a%20simple%20yet%20effective%20framework%20for%0Azero-shot%20medical%20disease%20recognition%20via%20utilizing%20the%20existing%20MLLM%20features.%0ASpecifically%2C%20we%20design%20an%20end-to-end%20training%20strategy%2C%20termed%20Decoding-Side%0AFeature%20Alignment%20Training%20%28DFAT%29%20to%20take%20advantage%20of%20the%20characteristics%20of%0Athe%20MLLM%20decoder%20architecture%20and%20incorporate%20modality-specific%20tokens%20tailored%0Afor%20different%20modalities.%20Additionally%2C%20we%20introduce%20a%20Domain%20Knowledge%0AAnchoring%20Module%20%28DKAM%29%20to%20exploit%20the%20intrinsic%20medical%20knowledge%20of%20large%0Amodels%2C%20which%20mitigates%20the%20category%20semantic%20gap%20in%20image-text%20alignment.%0AExtensive%20experiments%20demonstrate%20that%20our%20LLaVA-RadZ%20significantly%20outperforms%0Atraditional%20MLLMs%20in%20zero-shot%20disease%20recognition%2C%20achieving%20the%20comparable%0Aperformance%20to%20the%20well-established%20and%20highly-optimized%20CLIP-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07487v2&entry.124074799=Read"},
{"title": "Sigma: Semantically Informative Pre-training for Skeleton-based Sign\n  Language Understanding", "author": "Muxin Pu and Mei Kuan Lim and Chun Yong Chong and Chen Change Loy", "abstract": "  Pre-training has proven effective for learning transferable features in sign\nlanguage understanding (SLU) tasks. Recently, skeleton-based methods have\ngained increasing attention because they can robustly handle variations in\nsubjects and backgrounds without being affected by appearance or environmental\nfactors. Current SLU methods continue to face three key limitations: 1) weak\nsemantic grounding, as models often capture low-level motion patterns from\nskeletal data but struggle to relate them to linguistic meaning; 2) imbalance\nbetween local details and global context, with models either focusing too\nnarrowly on fine-grained cues or overlooking them for broader context; and 3)\ninefficient cross-modal learning, as constructing semantically aligned\nrepresentations across modalities remains difficult. To address these, we\npropose Sigma, a unified skeleton-based SLU framework featuring: 1) a\nsign-aware early fusion mechanism that facilitates deep interaction between\nvisual and textual modalities, enriching visual features with linguistic\ncontext; 2) a hierarchical alignment learning strategy that jointly maximises\nagreements across different levels of paired features from different\nmodalities, effectively capturing both fine-grained details and high-level\nsemantic relationships; and 3) a unified pre-training framework that combines\ncontrastive learning, text matching and language modelling to promote semantic\nconsistency and generalisation. Sigma achieves new state-of-the-art results on\nisolated sign language recognition, continuous sign language recognition, and\ngloss-free sign language translation on multiple benchmarks spanning different\nsign and spoken languages, demonstrating the impact of semantically informative\npre-training and the effectiveness of skeletal data as a stand-alone solution\nfor SLU.\n", "link": "http://arxiv.org/abs/2509.21223v1", "date": "2025-09-25", "relevancy": 2.7866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%0A%20%20Language%20Understanding&body=Title%3A%20Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%0A%20%20Language%20Understanding%0AAuthor%3A%20Muxin%20Pu%20and%20Mei%20Kuan%20Lim%20and%20Chun%20Yong%20Chong%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Pre-training%20has%20proven%20effective%20for%20learning%20transferable%20features%20in%20sign%0Alanguage%20understanding%20%28SLU%29%20tasks.%20Recently%2C%20skeleton-based%20methods%20have%0Agained%20increasing%20attention%20because%20they%20can%20robustly%20handle%20variations%20in%0Asubjects%20and%20backgrounds%20without%20being%20affected%20by%20appearance%20or%20environmental%0Afactors.%20Current%20SLU%20methods%20continue%20to%20face%20three%20key%20limitations%3A%201%29%20weak%0Asemantic%20grounding%2C%20as%20models%20often%20capture%20low-level%20motion%20patterns%20from%0Askeletal%20data%20but%20struggle%20to%20relate%20them%20to%20linguistic%20meaning%3B%202%29%20imbalance%0Abetween%20local%20details%20and%20global%20context%2C%20with%20models%20either%20focusing%20too%0Anarrowly%20on%20fine-grained%20cues%20or%20overlooking%20them%20for%20broader%20context%3B%20and%203%29%0Ainefficient%20cross-modal%20learning%2C%20as%20constructing%20semantically%20aligned%0Arepresentations%20across%20modalities%20remains%20difficult.%20To%20address%20these%2C%20we%0Apropose%20Sigma%2C%20a%20unified%20skeleton-based%20SLU%20framework%20featuring%3A%201%29%20a%0Asign-aware%20early%20fusion%20mechanism%20that%20facilitates%20deep%20interaction%20between%0Avisual%20and%20textual%20modalities%2C%20enriching%20visual%20features%20with%20linguistic%0Acontext%3B%202%29%20a%20hierarchical%20alignment%20learning%20strategy%20that%20jointly%20maximises%0Aagreements%20across%20different%20levels%20of%20paired%20features%20from%20different%0Amodalities%2C%20effectively%20capturing%20both%20fine-grained%20details%20and%20high-level%0Asemantic%20relationships%3B%20and%203%29%20a%20unified%20pre-training%20framework%20that%20combines%0Acontrastive%20learning%2C%20text%20matching%20and%20language%20modelling%20to%20promote%20semantic%0Aconsistency%20and%20generalisation.%20Sigma%20achieves%20new%20state-of-the-art%20results%20on%0Aisolated%20sign%20language%20recognition%2C%20continuous%20sign%20language%20recognition%2C%20and%0Agloss-free%20sign%20language%20translation%20on%20multiple%20benchmarks%20spanning%20different%0Asign%20and%20spoken%20languages%2C%20demonstrating%20the%20impact%20of%20semantically%20informative%0Apre-training%20and%20the%20effectiveness%20of%20skeletal%20data%20as%20a%20stand-alone%20solution%0Afor%20SLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigma%253A%2520Semantically%2520Informative%2520Pre-training%2520for%2520Skeleton-based%2520Sign%250A%2520%2520Language%2520Understanding%26entry.906535625%3DMuxin%2520Pu%2520and%2520Mei%2520Kuan%2520Lim%2520and%2520Chun%2520Yong%2520Chong%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Pre-training%2520has%2520proven%2520effective%2520for%2520learning%2520transferable%2520features%2520in%2520sign%250Alanguage%2520understanding%2520%2528SLU%2529%2520tasks.%2520Recently%252C%2520skeleton-based%2520methods%2520have%250Agained%2520increasing%2520attention%2520because%2520they%2520can%2520robustly%2520handle%2520variations%2520in%250Asubjects%2520and%2520backgrounds%2520without%2520being%2520affected%2520by%2520appearance%2520or%2520environmental%250Afactors.%2520Current%2520SLU%2520methods%2520continue%2520to%2520face%2520three%2520key%2520limitations%253A%25201%2529%2520weak%250Asemantic%2520grounding%252C%2520as%2520models%2520often%2520capture%2520low-level%2520motion%2520patterns%2520from%250Askeletal%2520data%2520but%2520struggle%2520to%2520relate%2520them%2520to%2520linguistic%2520meaning%253B%25202%2529%2520imbalance%250Abetween%2520local%2520details%2520and%2520global%2520context%252C%2520with%2520models%2520either%2520focusing%2520too%250Anarrowly%2520on%2520fine-grained%2520cues%2520or%2520overlooking%2520them%2520for%2520broader%2520context%253B%2520and%25203%2529%250Ainefficient%2520cross-modal%2520learning%252C%2520as%2520constructing%2520semantically%2520aligned%250Arepresentations%2520across%2520modalities%2520remains%2520difficult.%2520To%2520address%2520these%252C%2520we%250Apropose%2520Sigma%252C%2520a%2520unified%2520skeleton-based%2520SLU%2520framework%2520featuring%253A%25201%2529%2520a%250Asign-aware%2520early%2520fusion%2520mechanism%2520that%2520facilitates%2520deep%2520interaction%2520between%250Avisual%2520and%2520textual%2520modalities%252C%2520enriching%2520visual%2520features%2520with%2520linguistic%250Acontext%253B%25202%2529%2520a%2520hierarchical%2520alignment%2520learning%2520strategy%2520that%2520jointly%2520maximises%250Aagreements%2520across%2520different%2520levels%2520of%2520paired%2520features%2520from%2520different%250Amodalities%252C%2520effectively%2520capturing%2520both%2520fine-grained%2520details%2520and%2520high-level%250Asemantic%2520relationships%253B%2520and%25203%2529%2520a%2520unified%2520pre-training%2520framework%2520that%2520combines%250Acontrastive%2520learning%252C%2520text%2520matching%2520and%2520language%2520modelling%2520to%2520promote%2520semantic%250Aconsistency%2520and%2520generalisation.%2520Sigma%2520achieves%2520new%2520state-of-the-art%2520results%2520on%250Aisolated%2520sign%2520language%2520recognition%252C%2520continuous%2520sign%2520language%2520recognition%252C%2520and%250Agloss-free%2520sign%2520language%2520translation%2520on%2520multiple%2520benchmarks%2520spanning%2520different%250Asign%2520and%2520spoken%2520languages%252C%2520demonstrating%2520the%2520impact%2520of%2520semantically%2520informative%250Apre-training%2520and%2520the%2520effectiveness%2520of%2520skeletal%2520data%2520as%2520a%2520stand-alone%2520solution%250Afor%2520SLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%0A%20%20Language%20Understanding&entry.906535625=Muxin%20Pu%20and%20Mei%20Kuan%20Lim%20and%20Chun%20Yong%20Chong%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Pre-training%20has%20proven%20effective%20for%20learning%20transferable%20features%20in%20sign%0Alanguage%20understanding%20%28SLU%29%20tasks.%20Recently%2C%20skeleton-based%20methods%20have%0Agained%20increasing%20attention%20because%20they%20can%20robustly%20handle%20variations%20in%0Asubjects%20and%20backgrounds%20without%20being%20affected%20by%20appearance%20or%20environmental%0Afactors.%20Current%20SLU%20methods%20continue%20to%20face%20three%20key%20limitations%3A%201%29%20weak%0Asemantic%20grounding%2C%20as%20models%20often%20capture%20low-level%20motion%20patterns%20from%0Askeletal%20data%20but%20struggle%20to%20relate%20them%20to%20linguistic%20meaning%3B%202%29%20imbalance%0Abetween%20local%20details%20and%20global%20context%2C%20with%20models%20either%20focusing%20too%0Anarrowly%20on%20fine-grained%20cues%20or%20overlooking%20them%20for%20broader%20context%3B%20and%203%29%0Ainefficient%20cross-modal%20learning%2C%20as%20constructing%20semantically%20aligned%0Arepresentations%20across%20modalities%20remains%20difficult.%20To%20address%20these%2C%20we%0Apropose%20Sigma%2C%20a%20unified%20skeleton-based%20SLU%20framework%20featuring%3A%201%29%20a%0Asign-aware%20early%20fusion%20mechanism%20that%20facilitates%20deep%20interaction%20between%0Avisual%20and%20textual%20modalities%2C%20enriching%20visual%20features%20with%20linguistic%0Acontext%3B%202%29%20a%20hierarchical%20alignment%20learning%20strategy%20that%20jointly%20maximises%0Aagreements%20across%20different%20levels%20of%20paired%20features%20from%20different%0Amodalities%2C%20effectively%20capturing%20both%20fine-grained%20details%20and%20high-level%0Asemantic%20relationships%3B%20and%203%29%20a%20unified%20pre-training%20framework%20that%20combines%0Acontrastive%20learning%2C%20text%20matching%20and%20language%20modelling%20to%20promote%20semantic%0Aconsistency%20and%20generalisation.%20Sigma%20achieves%20new%20state-of-the-art%20results%20on%0Aisolated%20sign%20language%20recognition%2C%20continuous%20sign%20language%20recognition%2C%20and%0Agloss-free%20sign%20language%20translation%20on%20multiple%20benchmarks%20spanning%20different%0Asign%20and%20spoken%20languages%2C%20demonstrating%20the%20impact%20of%20semantically%20informative%0Apre-training%20and%20the%20effectiveness%20of%20skeletal%20data%20as%20a%20stand-alone%20solution%0Afor%20SLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21223v1&entry.124074799=Read"},
{"title": "Learning Conformal Explainers for Image Classifiers", "author": "Amr Alkhatib and Stephanie Lowry", "abstract": "  Feature attribution methods are widely used for explaining image-based\npredictions, as they provide feature-level insights that can be intuitively\nvisualized. However, such explanations often vary in their robustness and may\nfail to faithfully reflect the reasoning of the underlying black-box model. To\naddress these limitations, we propose a novel conformal prediction-based\napproach that enables users to directly control the fidelity of the generated\nexplanations. The method identifies a subset of salient features that is\nsufficient to preserve the model's prediction, regardless of the information\ncarried by the excluded features, and without demanding access to ground-truth\nexplanations for calibration. Four conformity functions are proposed to\nquantify the extent to which explanations conform to the model's predictions.\nThe approach is empirically evaluated using five explainers across six image\ndatasets. The empirical results demonstrate that FastSHAP consistently\noutperforms the competing methods in terms of both fidelity and informational\nefficiency, the latter measured by the size of the explanation regions.\nFurthermore, the results reveal that conformity measures based on super-pixels\nare more effective than their pixel-wise counterparts.\n", "link": "http://arxiv.org/abs/2509.21209v1", "date": "2025-09-25", "relevancy": 2.7385, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Conformal%20Explainers%20for%20Image%20Classifiers&body=Title%3A%20Learning%20Conformal%20Explainers%20for%20Image%20Classifiers%0AAuthor%3A%20Amr%20Alkhatib%20and%20Stephanie%20Lowry%0AAbstract%3A%20%20%20Feature%20attribution%20methods%20are%20widely%20used%20for%20explaining%20image-based%0Apredictions%2C%20as%20they%20provide%20feature-level%20insights%20that%20can%20be%20intuitively%0Avisualized.%20However%2C%20such%20explanations%20often%20vary%20in%20their%20robustness%20and%20may%0Afail%20to%20faithfully%20reflect%20the%20reasoning%20of%20the%20underlying%20black-box%20model.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20conformal%20prediction-based%0Aapproach%20that%20enables%20users%20to%20directly%20control%20the%20fidelity%20of%20the%20generated%0Aexplanations.%20The%20method%20identifies%20a%20subset%20of%20salient%20features%20that%20is%0Asufficient%20to%20preserve%20the%20model%27s%20prediction%2C%20regardless%20of%20the%20information%0Acarried%20by%20the%20excluded%20features%2C%20and%20without%20demanding%20access%20to%20ground-truth%0Aexplanations%20for%20calibration.%20Four%20conformity%20functions%20are%20proposed%20to%0Aquantify%20the%20extent%20to%20which%20explanations%20conform%20to%20the%20model%27s%20predictions.%0AThe%20approach%20is%20empirically%20evaluated%20using%20five%20explainers%20across%20six%20image%0Adatasets.%20The%20empirical%20results%20demonstrate%20that%20FastSHAP%20consistently%0Aoutperforms%20the%20competing%20methods%20in%20terms%20of%20both%20fidelity%20and%20informational%0Aefficiency%2C%20the%20latter%20measured%20by%20the%20size%20of%20the%20explanation%20regions.%0AFurthermore%2C%20the%20results%20reveal%20that%20conformity%20measures%20based%20on%20super-pixels%0Aare%20more%20effective%20than%20their%20pixel-wise%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Conformal%2520Explainers%2520for%2520Image%2520Classifiers%26entry.906535625%3DAmr%2520Alkhatib%2520and%2520Stephanie%2520Lowry%26entry.1292438233%3D%2520%2520Feature%2520attribution%2520methods%2520are%2520widely%2520used%2520for%2520explaining%2520image-based%250Apredictions%252C%2520as%2520they%2520provide%2520feature-level%2520insights%2520that%2520can%2520be%2520intuitively%250Avisualized.%2520However%252C%2520such%2520explanations%2520often%2520vary%2520in%2520their%2520robustness%2520and%2520may%250Afail%2520to%2520faithfully%2520reflect%2520the%2520reasoning%2520of%2520the%2520underlying%2520black-box%2520model.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520conformal%2520prediction-based%250Aapproach%2520that%2520enables%2520users%2520to%2520directly%2520control%2520the%2520fidelity%2520of%2520the%2520generated%250Aexplanations.%2520The%2520method%2520identifies%2520a%2520subset%2520of%2520salient%2520features%2520that%2520is%250Asufficient%2520to%2520preserve%2520the%2520model%2527s%2520prediction%252C%2520regardless%2520of%2520the%2520information%250Acarried%2520by%2520the%2520excluded%2520features%252C%2520and%2520without%2520demanding%2520access%2520to%2520ground-truth%250Aexplanations%2520for%2520calibration.%2520Four%2520conformity%2520functions%2520are%2520proposed%2520to%250Aquantify%2520the%2520extent%2520to%2520which%2520explanations%2520conform%2520to%2520the%2520model%2527s%2520predictions.%250AThe%2520approach%2520is%2520empirically%2520evaluated%2520using%2520five%2520explainers%2520across%2520six%2520image%250Adatasets.%2520The%2520empirical%2520results%2520demonstrate%2520that%2520FastSHAP%2520consistently%250Aoutperforms%2520the%2520competing%2520methods%2520in%2520terms%2520of%2520both%2520fidelity%2520and%2520informational%250Aefficiency%252C%2520the%2520latter%2520measured%2520by%2520the%2520size%2520of%2520the%2520explanation%2520regions.%250AFurthermore%252C%2520the%2520results%2520reveal%2520that%2520conformity%2520measures%2520based%2520on%2520super-pixels%250Aare%2520more%2520effective%2520than%2520their%2520pixel-wise%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Conformal%20Explainers%20for%20Image%20Classifiers&entry.906535625=Amr%20Alkhatib%20and%20Stephanie%20Lowry&entry.1292438233=%20%20Feature%20attribution%20methods%20are%20widely%20used%20for%20explaining%20image-based%0Apredictions%2C%20as%20they%20provide%20feature-level%20insights%20that%20can%20be%20intuitively%0Avisualized.%20However%2C%20such%20explanations%20often%20vary%20in%20their%20robustness%20and%20may%0Afail%20to%20faithfully%20reflect%20the%20reasoning%20of%20the%20underlying%20black-box%20model.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20conformal%20prediction-based%0Aapproach%20that%20enables%20users%20to%20directly%20control%20the%20fidelity%20of%20the%20generated%0Aexplanations.%20The%20method%20identifies%20a%20subset%20of%20salient%20features%20that%20is%0Asufficient%20to%20preserve%20the%20model%27s%20prediction%2C%20regardless%20of%20the%20information%0Acarried%20by%20the%20excluded%20features%2C%20and%20without%20demanding%20access%20to%20ground-truth%0Aexplanations%20for%20calibration.%20Four%20conformity%20functions%20are%20proposed%20to%0Aquantify%20the%20extent%20to%20which%20explanations%20conform%20to%20the%20model%27s%20predictions.%0AThe%20approach%20is%20empirically%20evaluated%20using%20five%20explainers%20across%20six%20image%0Adatasets.%20The%20empirical%20results%20demonstrate%20that%20FastSHAP%20consistently%0Aoutperforms%20the%20competing%20methods%20in%20terms%20of%20both%20fidelity%20and%20informational%0Aefficiency%2C%20the%20latter%20measured%20by%20the%20size%20of%20the%20explanation%20regions.%0AFurthermore%2C%20the%20results%20reveal%20that%20conformity%20measures%20based%20on%20super-pixels%0Aare%20more%20effective%20than%20their%20pixel-wise%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21209v1&entry.124074799=Read"},
{"title": "Dense Semantic Matching with VGGT Prior", "author": "Songlin Yang and Tianyi Wei and Yushi Lan and Zeqi Xiao and Anyi Rao and Xingang Pan", "abstract": "  Semantic matching aims to establish pixel-level correspondences between\ninstances of the same category and represents a fundamental task in computer\nvision. Existing approaches suffer from two limitations: (i) Geometric\nAmbiguity: Their reliance on 2D foundation model features (e.g., Stable\nDiffusion, DINO) often fails to disambiguate symmetric structures, requiring\nextra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their\npixel-wise matching ignores cross-image invisibility and neglects manifold\npreservation. These challenges call for geometry-aware pixel descriptors and\nholistic dense correspondence mechanisms. Inspired by recent advances in 3D\ngeometric foundation models, we turn to VGGT, which provides geometry-grounded\nfeatures and holistic dense matching capabilities well aligned with these\nneeds. However, directly transferring VGGT is challenging, as it was originally\ndesigned for geometry matching within cross views of a single instance,\nmisaligned with cross-instance semantic matching, and further hindered by the\nscarcity of dense semantic annotations. To address this, we propose an approach\nthat (i) retains VGGT's intrinsic strengths by reusing early feature stages,\nfine-tuning later ones, and adding a semantic head for bidirectional\ncorrespondences; and (ii) adapts VGGT to the semantic matching scenario under\ndata scarcity through cycle-consistent training strategy, synthetic data\naugmentation, and progressive training recipe with aliasing artifact\nmitigation. Extensive experiments demonstrate that our approach achieves\nsuperior geometry awareness, matching reliability, and manifold preservation,\noutperforming previous baselines.\n", "link": "http://arxiv.org/abs/2509.21263v1", "date": "2025-09-25", "relevancy": 2.7266, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Semantic%20Matching%20with%20VGGT%20Prior&body=Title%3A%20Dense%20Semantic%20Matching%20with%20VGGT%20Prior%0AAuthor%3A%20Songlin%20Yang%20and%20Tianyi%20Wei%20and%20Yushi%20Lan%20and%20Zeqi%20Xiao%20and%20Anyi%20Rao%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20Semantic%20matching%20aims%20to%20establish%20pixel-level%20correspondences%20between%0Ainstances%20of%20the%20same%20category%20and%20represents%20a%20fundamental%20task%20in%20computer%0Avision.%20Existing%20approaches%20suffer%20from%20two%20limitations%3A%20%28i%29%20Geometric%0AAmbiguity%3A%20Their%20reliance%20on%202D%20foundation%20model%20features%20%28e.g.%2C%20Stable%0ADiffusion%2C%20DINO%29%20often%20fails%20to%20disambiguate%20symmetric%20structures%2C%20requiring%0Aextra%20fine-tuning%20yet%20lacking%20generalization%3B%20%28ii%29%20Nearest-Neighbor%20Rule%3A%20Their%0Apixel-wise%20matching%20ignores%20cross-image%20invisibility%20and%20neglects%20manifold%0Apreservation.%20These%20challenges%20call%20for%20geometry-aware%20pixel%20descriptors%20and%0Aholistic%20dense%20correspondence%20mechanisms.%20Inspired%20by%20recent%20advances%20in%203D%0Ageometric%20foundation%20models%2C%20we%20turn%20to%20VGGT%2C%20which%20provides%20geometry-grounded%0Afeatures%20and%20holistic%20dense%20matching%20capabilities%20well%20aligned%20with%20these%0Aneeds.%20However%2C%20directly%20transferring%20VGGT%20is%20challenging%2C%20as%20it%20was%20originally%0Adesigned%20for%20geometry%20matching%20within%20cross%20views%20of%20a%20single%20instance%2C%0Amisaligned%20with%20cross-instance%20semantic%20matching%2C%20and%20further%20hindered%20by%20the%0Ascarcity%20of%20dense%20semantic%20annotations.%20To%20address%20this%2C%20we%20propose%20an%20approach%0Athat%20%28i%29%20retains%20VGGT%27s%20intrinsic%20strengths%20by%20reusing%20early%20feature%20stages%2C%0Afine-tuning%20later%20ones%2C%20and%20adding%20a%20semantic%20head%20for%20bidirectional%0Acorrespondences%3B%20and%20%28ii%29%20adapts%20VGGT%20to%20the%20semantic%20matching%20scenario%20under%0Adata%20scarcity%20through%20cycle-consistent%20training%20strategy%2C%20synthetic%20data%0Aaugmentation%2C%20and%20progressive%20training%20recipe%20with%20aliasing%20artifact%0Amitigation.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20geometry%20awareness%2C%20matching%20reliability%2C%20and%20manifold%20preservation%2C%0Aoutperforming%20previous%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Semantic%2520Matching%2520with%2520VGGT%2520Prior%26entry.906535625%3DSonglin%2520Yang%2520and%2520Tianyi%2520Wei%2520and%2520Yushi%2520Lan%2520and%2520Zeqi%2520Xiao%2520and%2520Anyi%2520Rao%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520Semantic%2520matching%2520aims%2520to%2520establish%2520pixel-level%2520correspondences%2520between%250Ainstances%2520of%2520the%2520same%2520category%2520and%2520represents%2520a%2520fundamental%2520task%2520in%2520computer%250Avision.%2520Existing%2520approaches%2520suffer%2520from%2520two%2520limitations%253A%2520%2528i%2529%2520Geometric%250AAmbiguity%253A%2520Their%2520reliance%2520on%25202D%2520foundation%2520model%2520features%2520%2528e.g.%252C%2520Stable%250ADiffusion%252C%2520DINO%2529%2520often%2520fails%2520to%2520disambiguate%2520symmetric%2520structures%252C%2520requiring%250Aextra%2520fine-tuning%2520yet%2520lacking%2520generalization%253B%2520%2528ii%2529%2520Nearest-Neighbor%2520Rule%253A%2520Their%250Apixel-wise%2520matching%2520ignores%2520cross-image%2520invisibility%2520and%2520neglects%2520manifold%250Apreservation.%2520These%2520challenges%2520call%2520for%2520geometry-aware%2520pixel%2520descriptors%2520and%250Aholistic%2520dense%2520correspondence%2520mechanisms.%2520Inspired%2520by%2520recent%2520advances%2520in%25203D%250Ageometric%2520foundation%2520models%252C%2520we%2520turn%2520to%2520VGGT%252C%2520which%2520provides%2520geometry-grounded%250Afeatures%2520and%2520holistic%2520dense%2520matching%2520capabilities%2520well%2520aligned%2520with%2520these%250Aneeds.%2520However%252C%2520directly%2520transferring%2520VGGT%2520is%2520challenging%252C%2520as%2520it%2520was%2520originally%250Adesigned%2520for%2520geometry%2520matching%2520within%2520cross%2520views%2520of%2520a%2520single%2520instance%252C%250Amisaligned%2520with%2520cross-instance%2520semantic%2520matching%252C%2520and%2520further%2520hindered%2520by%2520the%250Ascarcity%2520of%2520dense%2520semantic%2520annotations.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520approach%250Athat%2520%2528i%2529%2520retains%2520VGGT%2527s%2520intrinsic%2520strengths%2520by%2520reusing%2520early%2520feature%2520stages%252C%250Afine-tuning%2520later%2520ones%252C%2520and%2520adding%2520a%2520semantic%2520head%2520for%2520bidirectional%250Acorrespondences%253B%2520and%2520%2528ii%2529%2520adapts%2520VGGT%2520to%2520the%2520semantic%2520matching%2520scenario%2520under%250Adata%2520scarcity%2520through%2520cycle-consistent%2520training%2520strategy%252C%2520synthetic%2520data%250Aaugmentation%252C%2520and%2520progressive%2520training%2520recipe%2520with%2520aliasing%2520artifact%250Amitigation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Asuperior%2520geometry%2520awareness%252C%2520matching%2520reliability%252C%2520and%2520manifold%2520preservation%252C%250Aoutperforming%2520previous%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Semantic%20Matching%20with%20VGGT%20Prior&entry.906535625=Songlin%20Yang%20and%20Tianyi%20Wei%20and%20Yushi%20Lan%20and%20Zeqi%20Xiao%20and%20Anyi%20Rao%20and%20Xingang%20Pan&entry.1292438233=%20%20Semantic%20matching%20aims%20to%20establish%20pixel-level%20correspondences%20between%0Ainstances%20of%20the%20same%20category%20and%20represents%20a%20fundamental%20task%20in%20computer%0Avision.%20Existing%20approaches%20suffer%20from%20two%20limitations%3A%20%28i%29%20Geometric%0AAmbiguity%3A%20Their%20reliance%20on%202D%20foundation%20model%20features%20%28e.g.%2C%20Stable%0ADiffusion%2C%20DINO%29%20often%20fails%20to%20disambiguate%20symmetric%20structures%2C%20requiring%0Aextra%20fine-tuning%20yet%20lacking%20generalization%3B%20%28ii%29%20Nearest-Neighbor%20Rule%3A%20Their%0Apixel-wise%20matching%20ignores%20cross-image%20invisibility%20and%20neglects%20manifold%0Apreservation.%20These%20challenges%20call%20for%20geometry-aware%20pixel%20descriptors%20and%0Aholistic%20dense%20correspondence%20mechanisms.%20Inspired%20by%20recent%20advances%20in%203D%0Ageometric%20foundation%20models%2C%20we%20turn%20to%20VGGT%2C%20which%20provides%20geometry-grounded%0Afeatures%20and%20holistic%20dense%20matching%20capabilities%20well%20aligned%20with%20these%0Aneeds.%20However%2C%20directly%20transferring%20VGGT%20is%20challenging%2C%20as%20it%20was%20originally%0Adesigned%20for%20geometry%20matching%20within%20cross%20views%20of%20a%20single%20instance%2C%0Amisaligned%20with%20cross-instance%20semantic%20matching%2C%20and%20further%20hindered%20by%20the%0Ascarcity%20of%20dense%20semantic%20annotations.%20To%20address%20this%2C%20we%20propose%20an%20approach%0Athat%20%28i%29%20retains%20VGGT%27s%20intrinsic%20strengths%20by%20reusing%20early%20feature%20stages%2C%0Afine-tuning%20later%20ones%2C%20and%20adding%20a%20semantic%20head%20for%20bidirectional%0Acorrespondences%3B%20and%20%28ii%29%20adapts%20VGGT%20to%20the%20semantic%20matching%20scenario%20under%0Adata%20scarcity%20through%20cycle-consistent%20training%20strategy%2C%20synthetic%20data%0Aaugmentation%2C%20and%20progressive%20training%20recipe%20with%20aliasing%20artifact%0Amitigation.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20geometry%20awareness%2C%20matching%20reliability%2C%20and%20manifold%20preservation%2C%0Aoutperforming%20previous%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21263v1&entry.124074799=Read"},
{"title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization", "author": "Ruiyu Wang and Shizhao Sun and Weijian Ma and Jiang Bian", "abstract": "  Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines.\n", "link": "http://arxiv.org/abs/2509.21150v1", "date": "2025-09-25", "relevancy": 2.6629, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Tokenizer%3A%20Towards%20Text-based%20CAD%20Prototyping%20via%20Modality-Specific%0A%20%20Tokenization&body=Title%3A%20CAD-Tokenizer%3A%20Towards%20Text-based%20CAD%20Prototyping%20via%20Modality-Specific%0A%20%20Tokenization%0AAuthor%3A%20Ruiyu%20Wang%20and%20Shizhao%20Sun%20and%20Weijian%20Ma%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Computer-Aided%20Design%20%28CAD%29%20is%20a%20foundational%20component%20of%20industrial%0Aprototyping%2C%20where%20models%20are%20defined%20not%20by%20raw%20coordinates%20but%20by%0Aconstruction%20sequences%20such%20as%20sketches%20and%20extrusions.%20This%20sequential%0Astructure%20enables%20both%20efficient%20prototype%20initialization%20and%20subsequent%0Aediting.%20Text-guided%20CAD%20prototyping%2C%20which%20unifies%20Text-to-CAD%20generation%20and%0ACAD%20editing%2C%20has%20the%20potential%20to%20streamline%20the%20entire%20design%20pipeline.%0AHowever%2C%20prior%20work%20has%20not%20explored%20this%20setting%2C%20largely%20because%20standard%0Alarge%20language%20model%20%28LLM%29%20tokenizers%20decompose%20CAD%20sequences%20into%0Anatural-language%20word%20pieces%2C%20failing%20to%20capture%20primitive-level%20CAD%20semantics%0Aand%20hindering%20attention%20modules%20from%20modeling%20geometric%20structure.%20We%0Aconjecture%20that%20a%20multimodal%20tokenization%20strategy%2C%20aligned%20with%20CAD%27s%0Aprimitive%20and%20structural%20nature%2C%20can%20provide%20more%20effective%20representations.%20To%0Athis%20end%2C%20we%20propose%20CAD-Tokenizer%2C%20a%20framework%20that%20represents%20CAD%20data%20with%0Amodality-specific%20tokens%20using%20a%20sequence-based%20VQ-VAE%20with%20primitive-level%0Apooling%20and%20constrained%20decoding.%20This%20design%20produces%20compact%2C%20primitive-aware%0Arepresentations%20that%20align%20with%20CAD%27s%20structural%20nature.%20Applied%20to%20unified%0Atext-guided%20CAD%20prototyping%2C%20CAD-Tokenizer%20significantly%20improves%20instruction%0Afollowing%20and%20generation%20quality%2C%20achieving%20better%20quantitative%20and%20qualitative%0Aperformance%20over%20both%20general-purpose%20LLMs%20and%20task-specific%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Tokenizer%253A%2520Towards%2520Text-based%2520CAD%2520Prototyping%2520via%2520Modality-Specific%250A%2520%2520Tokenization%26entry.906535625%3DRuiyu%2520Wang%2520and%2520Shizhao%2520Sun%2520and%2520Weijian%2520Ma%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520is%2520a%2520foundational%2520component%2520of%2520industrial%250Aprototyping%252C%2520where%2520models%2520are%2520defined%2520not%2520by%2520raw%2520coordinates%2520but%2520by%250Aconstruction%2520sequences%2520such%2520as%2520sketches%2520and%2520extrusions.%2520This%2520sequential%250Astructure%2520enables%2520both%2520efficient%2520prototype%2520initialization%2520and%2520subsequent%250Aediting.%2520Text-guided%2520CAD%2520prototyping%252C%2520which%2520unifies%2520Text-to-CAD%2520generation%2520and%250ACAD%2520editing%252C%2520has%2520the%2520potential%2520to%2520streamline%2520the%2520entire%2520design%2520pipeline.%250AHowever%252C%2520prior%2520work%2520has%2520not%2520explored%2520this%2520setting%252C%2520largely%2520because%2520standard%250Alarge%2520language%2520model%2520%2528LLM%2529%2520tokenizers%2520decompose%2520CAD%2520sequences%2520into%250Anatural-language%2520word%2520pieces%252C%2520failing%2520to%2520capture%2520primitive-level%2520CAD%2520semantics%250Aand%2520hindering%2520attention%2520modules%2520from%2520modeling%2520geometric%2520structure.%2520We%250Aconjecture%2520that%2520a%2520multimodal%2520tokenization%2520strategy%252C%2520aligned%2520with%2520CAD%2527s%250Aprimitive%2520and%2520structural%2520nature%252C%2520can%2520provide%2520more%2520effective%2520representations.%2520To%250Athis%2520end%252C%2520we%2520propose%2520CAD-Tokenizer%252C%2520a%2520framework%2520that%2520represents%2520CAD%2520data%2520with%250Amodality-specific%2520tokens%2520using%2520a%2520sequence-based%2520VQ-VAE%2520with%2520primitive-level%250Apooling%2520and%2520constrained%2520decoding.%2520This%2520design%2520produces%2520compact%252C%2520primitive-aware%250Arepresentations%2520that%2520align%2520with%2520CAD%2527s%2520structural%2520nature.%2520Applied%2520to%2520unified%250Atext-guided%2520CAD%2520prototyping%252C%2520CAD-Tokenizer%2520significantly%2520improves%2520instruction%250Afollowing%2520and%2520generation%2520quality%252C%2520achieving%2520better%2520quantitative%2520and%2520qualitative%250Aperformance%2520over%2520both%2520general-purpose%2520LLMs%2520and%2520task-specific%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Tokenizer%3A%20Towards%20Text-based%20CAD%20Prototyping%20via%20Modality-Specific%0A%20%20Tokenization&entry.906535625=Ruiyu%20Wang%20and%20Shizhao%20Sun%20and%20Weijian%20Ma%20and%20Jiang%20Bian&entry.1292438233=%20%20Computer-Aided%20Design%20%28CAD%29%20is%20a%20foundational%20component%20of%20industrial%0Aprototyping%2C%20where%20models%20are%20defined%20not%20by%20raw%20coordinates%20but%20by%0Aconstruction%20sequences%20such%20as%20sketches%20and%20extrusions.%20This%20sequential%0Astructure%20enables%20both%20efficient%20prototype%20initialization%20and%20subsequent%0Aediting.%20Text-guided%20CAD%20prototyping%2C%20which%20unifies%20Text-to-CAD%20generation%20and%0ACAD%20editing%2C%20has%20the%20potential%20to%20streamline%20the%20entire%20design%20pipeline.%0AHowever%2C%20prior%20work%20has%20not%20explored%20this%20setting%2C%20largely%20because%20standard%0Alarge%20language%20model%20%28LLM%29%20tokenizers%20decompose%20CAD%20sequences%20into%0Anatural-language%20word%20pieces%2C%20failing%20to%20capture%20primitive-level%20CAD%20semantics%0Aand%20hindering%20attention%20modules%20from%20modeling%20geometric%20structure.%20We%0Aconjecture%20that%20a%20multimodal%20tokenization%20strategy%2C%20aligned%20with%20CAD%27s%0Aprimitive%20and%20structural%20nature%2C%20can%20provide%20more%20effective%20representations.%20To%0Athis%20end%2C%20we%20propose%20CAD-Tokenizer%2C%20a%20framework%20that%20represents%20CAD%20data%20with%0Amodality-specific%20tokens%20using%20a%20sequence-based%20VQ-VAE%20with%20primitive-level%0Apooling%20and%20constrained%20decoding.%20This%20design%20produces%20compact%2C%20primitive-aware%0Arepresentations%20that%20align%20with%20CAD%27s%20structural%20nature.%20Applied%20to%20unified%0Atext-guided%20CAD%20prototyping%2C%20CAD-Tokenizer%20significantly%20improves%20instruction%0Afollowing%20and%20generation%20quality%2C%20achieving%20better%20quantitative%20and%20qualitative%0Aperformance%20over%20both%20general-purpose%20LLMs%20and%20task-specific%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21150v1&entry.124074799=Read"},
{"title": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy", "author": "Aymen Bouguerra and Daniel Montoya and Alexandra Gomez-Villa and Fabio Arnez and Chokri Mraidha", "abstract": "  The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.\n", "link": "http://arxiv.org/abs/2509.21173v1", "date": "2025-09-25", "relevancy": 2.6521, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy&body=Title%3A%20Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy%0AAuthor%3A%20Aymen%20Bouguerra%20and%20Daniel%20Montoya%20and%20Alexandra%20Gomez-Villa%20and%20Fabio%20Arnez%20and%20Chokri%20Mraidha%0AAbstract%3A%20%20%20The%20powerful%20zero-shot%20generalization%20capabilities%20of%20vision-language%20models%0A%28VLMs%29%20like%20CLIP%20have%20enabled%20new%20paradigms%20for%20safety-related%20tasks%20such%20as%0Aout-of-distribution%20%28OOD%29%20detection.%20However%2C%20additional%20aspects%20crucial%20for%0Athe%20computationally%20efficient%20and%20reliable%20deployment%20of%20CLIP%20are%20still%0Aoverlooked.%20In%20particular%2C%20the%20impact%20of%20quantization%20on%20CLIP%27s%20performance%0Abeyond%20accuracy%20remains%20underexplored.%20This%20work%20presents%20a%20large-scale%0Aevaluation%20of%20quantization%20on%20CLIP%20models%2C%20assessing%20not%20only%20in-distribution%0Aaccuracy%20but%20a%20comprehensive%20suite%20of%20reliability%20metrics%20and%20revealing%0Acounterintuitive%20results%20driven%20by%20pre-training%20source.%20We%20demonstrate%20that%0Aquantization%20consistently%20improves%20calibration%20for%20typically%20underconfident%0Apre-trained%20models%2C%20while%20often%20degrading%20it%20for%20overconfident%20variants.%0AIntriguingly%2C%20this%20degradation%20in%20calibration%20does%20not%20preclude%20gains%20in%20other%0Areliability%20metrics%3B%20we%20find%20that%20OOD%20detection%20can%20still%20improve%20for%20these%0Asame%20poorly%20calibrated%20models.%20Furthermore%2C%20we%20identify%20specific%0Aquantization-aware%20training%20%28QAT%29%20methods%20that%20yield%20simultaneous%20gains%20in%0Azero-shot%20accuracy%2C%20calibration%2C%20and%20OOD%20robustness%2C%20challenging%20the%20view%20of%20a%0Astrict%20efficiency-performance%20trade-off.%20These%20findings%20offer%20critical%20insights%0Afor%20navigating%20the%20multi-objective%20problem%20of%20deploying%20efficient%2C%20reliable%2C%0Aand%20robust%20VLMs%20by%20utilizing%20quantization%20beyond%20its%20conventional%20role.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Less%2520Precise%2520Be%2520More%2520Reliable%253F%2520A%2520Systematic%2520Evaluation%2520of%250A%2520%2520Quantization%2527s%2520Impact%2520on%2520CLIP%2520Beyond%2520Accuracy%26entry.906535625%3DAymen%2520Bouguerra%2520and%2520Daniel%2520Montoya%2520and%2520Alexandra%2520Gomez-Villa%2520and%2520Fabio%2520Arnez%2520and%2520Chokri%2520Mraidha%26entry.1292438233%3D%2520%2520The%2520powerful%2520zero-shot%2520generalization%2520capabilities%2520of%2520vision-language%2520models%250A%2528VLMs%2529%2520like%2520CLIP%2520have%2520enabled%2520new%2520paradigms%2520for%2520safety-related%2520tasks%2520such%2520as%250Aout-of-distribution%2520%2528OOD%2529%2520detection.%2520However%252C%2520additional%2520aspects%2520crucial%2520for%250Athe%2520computationally%2520efficient%2520and%2520reliable%2520deployment%2520of%2520CLIP%2520are%2520still%250Aoverlooked.%2520In%2520particular%252C%2520the%2520impact%2520of%2520quantization%2520on%2520CLIP%2527s%2520performance%250Abeyond%2520accuracy%2520remains%2520underexplored.%2520This%2520work%2520presents%2520a%2520large-scale%250Aevaluation%2520of%2520quantization%2520on%2520CLIP%2520models%252C%2520assessing%2520not%2520only%2520in-distribution%250Aaccuracy%2520but%2520a%2520comprehensive%2520suite%2520of%2520reliability%2520metrics%2520and%2520revealing%250Acounterintuitive%2520results%2520driven%2520by%2520pre-training%2520source.%2520We%2520demonstrate%2520that%250Aquantization%2520consistently%2520improves%2520calibration%2520for%2520typically%2520underconfident%250Apre-trained%2520models%252C%2520while%2520often%2520degrading%2520it%2520for%2520overconfident%2520variants.%250AIntriguingly%252C%2520this%2520degradation%2520in%2520calibration%2520does%2520not%2520preclude%2520gains%2520in%2520other%250Areliability%2520metrics%253B%2520we%2520find%2520that%2520OOD%2520detection%2520can%2520still%2520improve%2520for%2520these%250Asame%2520poorly%2520calibrated%2520models.%2520Furthermore%252C%2520we%2520identify%2520specific%250Aquantization-aware%2520training%2520%2528QAT%2529%2520methods%2520that%2520yield%2520simultaneous%2520gains%2520in%250Azero-shot%2520accuracy%252C%2520calibration%252C%2520and%2520OOD%2520robustness%252C%2520challenging%2520the%2520view%2520of%2520a%250Astrict%2520efficiency-performance%2520trade-off.%2520These%2520findings%2520offer%2520critical%2520insights%250Afor%2520navigating%2520the%2520multi-objective%2520problem%2520of%2520deploying%2520efficient%252C%2520reliable%252C%250Aand%2520robust%2520VLMs%2520by%2520utilizing%2520quantization%2520beyond%2520its%2520conventional%2520role.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy&entry.906535625=Aymen%20Bouguerra%20and%20Daniel%20Montoya%20and%20Alexandra%20Gomez-Villa%20and%20Fabio%20Arnez%20and%20Chokri%20Mraidha&entry.1292438233=%20%20The%20powerful%20zero-shot%20generalization%20capabilities%20of%20vision-language%20models%0A%28VLMs%29%20like%20CLIP%20have%20enabled%20new%20paradigms%20for%20safety-related%20tasks%20such%20as%0Aout-of-distribution%20%28OOD%29%20detection.%20However%2C%20additional%20aspects%20crucial%20for%0Athe%20computationally%20efficient%20and%20reliable%20deployment%20of%20CLIP%20are%20still%0Aoverlooked.%20In%20particular%2C%20the%20impact%20of%20quantization%20on%20CLIP%27s%20performance%0Abeyond%20accuracy%20remains%20underexplored.%20This%20work%20presents%20a%20large-scale%0Aevaluation%20of%20quantization%20on%20CLIP%20models%2C%20assessing%20not%20only%20in-distribution%0Aaccuracy%20but%20a%20comprehensive%20suite%20of%20reliability%20metrics%20and%20revealing%0Acounterintuitive%20results%20driven%20by%20pre-training%20source.%20We%20demonstrate%20that%0Aquantization%20consistently%20improves%20calibration%20for%20typically%20underconfident%0Apre-trained%20models%2C%20while%20often%20degrading%20it%20for%20overconfident%20variants.%0AIntriguingly%2C%20this%20degradation%20in%20calibration%20does%20not%20preclude%20gains%20in%20other%0Areliability%20metrics%3B%20we%20find%20that%20OOD%20detection%20can%20still%20improve%20for%20these%0Asame%20poorly%20calibrated%20models.%20Furthermore%2C%20we%20identify%20specific%0Aquantization-aware%20training%20%28QAT%29%20methods%20that%20yield%20simultaneous%20gains%20in%0Azero-shot%20accuracy%2C%20calibration%2C%20and%20OOD%20robustness%2C%20challenging%20the%20view%20of%20a%0Astrict%20efficiency-performance%20trade-off.%20These%20findings%20offer%20critical%20insights%0Afor%20navigating%20the%20multi-objective%20problem%20of%20deploying%20efficient%2C%20reliable%2C%0Aand%20robust%20VLMs%20by%20utilizing%20quantization%20beyond%20its%20conventional%20role.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21173v1&entry.124074799=Read"},
{"title": "EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft\n  Grained for Classification Task", "author": "Riccardo La Grassa and Ignazio Gallo and Nicola Landro", "abstract": "  Fine-grained classification models are designed to focus on the relevant\ndetails necessary to distinguish highly similar classes, particularly when\nintra-class variance is high and inter-class variance is low. Most existing\nmodels rely on part annotations such as bounding boxes, part locations, or\ntextual attributes to enhance classification performance, while others employ\nsophisticated techniques to automatically extract attention maps. We posit that\npart-based approaches, including automatic cropping methods, suffer from an\nincomplete representation of local features, which are fundamental for\ndistinguishing similar objects. While fine-grained classification aims to\nrecognize the leaves of a hierarchical structure, humans recognize objects by\nalso forming semantic associations. In this paper, we leverage semantic\nassociations structured as a hierarchy (taxonomy) as supervised signals within\nan end-to-end deep neural network model, termed EnGraf-Net. Extensive\nexperiments on three well-known datasets CIFAR-100, CUB-200-2011, and\nFGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing\nfine-grained models, showing competitive performance with the most recent\nstate-of-the-art approaches, without requiring cropping techniques or manual\nannotations.\n", "link": "http://arxiv.org/abs/2509.21061v1", "date": "2025-09-25", "relevancy": 2.6443, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5402}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnGraf-Net%3A%20Multiple%20Granularity%20Branch%20Network%20with%20Fine-Coarse%20Graft%0A%20%20Grained%20for%20Classification%20Task&body=Title%3A%20EnGraf-Net%3A%20Multiple%20Granularity%20Branch%20Network%20with%20Fine-Coarse%20Graft%0A%20%20Grained%20for%20Classification%20Task%0AAuthor%3A%20Riccardo%20La%20Grassa%20and%20Ignazio%20Gallo%20and%20Nicola%20Landro%0AAbstract%3A%20%20%20Fine-grained%20classification%20models%20are%20designed%20to%20focus%20on%20the%20relevant%0Adetails%20necessary%20to%20distinguish%20highly%20similar%20classes%2C%20particularly%20when%0Aintra-class%20variance%20is%20high%20and%20inter-class%20variance%20is%20low.%20Most%20existing%0Amodels%20rely%20on%20part%20annotations%20such%20as%20bounding%20boxes%2C%20part%20locations%2C%20or%0Atextual%20attributes%20to%20enhance%20classification%20performance%2C%20while%20others%20employ%0Asophisticated%20techniques%20to%20automatically%20extract%20attention%20maps.%20We%20posit%20that%0Apart-based%20approaches%2C%20including%20automatic%20cropping%20methods%2C%20suffer%20from%20an%0Aincomplete%20representation%20of%20local%20features%2C%20which%20are%20fundamental%20for%0Adistinguishing%20similar%20objects.%20While%20fine-grained%20classification%20aims%20to%0Arecognize%20the%20leaves%20of%20a%20hierarchical%20structure%2C%20humans%20recognize%20objects%20by%0Aalso%20forming%20semantic%20associations.%20In%20this%20paper%2C%20we%20leverage%20semantic%0Aassociations%20structured%20as%20a%20hierarchy%20%28taxonomy%29%20as%20supervised%20signals%20within%0Aan%20end-to-end%20deep%20neural%20network%20model%2C%20termed%20EnGraf-Net.%20Extensive%0Aexperiments%20on%20three%20well-known%20datasets%20CIFAR-100%2C%20CUB-200-2011%2C%20and%0AFGVC-Aircraft%20demonstrate%20the%20superiority%20of%20EnGraf-Net%20over%20many%20existing%0Afine-grained%20models%2C%20showing%20competitive%20performance%20with%20the%20most%20recent%0Astate-of-the-art%20approaches%2C%20without%20requiring%20cropping%20techniques%20or%20manual%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnGraf-Net%253A%2520Multiple%2520Granularity%2520Branch%2520Network%2520with%2520Fine-Coarse%2520Graft%250A%2520%2520Grained%2520for%2520Classification%2520Task%26entry.906535625%3DRiccardo%2520La%2520Grassa%2520and%2520Ignazio%2520Gallo%2520and%2520Nicola%2520Landro%26entry.1292438233%3D%2520%2520Fine-grained%2520classification%2520models%2520are%2520designed%2520to%2520focus%2520on%2520the%2520relevant%250Adetails%2520necessary%2520to%2520distinguish%2520highly%2520similar%2520classes%252C%2520particularly%2520when%250Aintra-class%2520variance%2520is%2520high%2520and%2520inter-class%2520variance%2520is%2520low.%2520Most%2520existing%250Amodels%2520rely%2520on%2520part%2520annotations%2520such%2520as%2520bounding%2520boxes%252C%2520part%2520locations%252C%2520or%250Atextual%2520attributes%2520to%2520enhance%2520classification%2520performance%252C%2520while%2520others%2520employ%250Asophisticated%2520techniques%2520to%2520automatically%2520extract%2520attention%2520maps.%2520We%2520posit%2520that%250Apart-based%2520approaches%252C%2520including%2520automatic%2520cropping%2520methods%252C%2520suffer%2520from%2520an%250Aincomplete%2520representation%2520of%2520local%2520features%252C%2520which%2520are%2520fundamental%2520for%250Adistinguishing%2520similar%2520objects.%2520While%2520fine-grained%2520classification%2520aims%2520to%250Arecognize%2520the%2520leaves%2520of%2520a%2520hierarchical%2520structure%252C%2520humans%2520recognize%2520objects%2520by%250Aalso%2520forming%2520semantic%2520associations.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520semantic%250Aassociations%2520structured%2520as%2520a%2520hierarchy%2520%2528taxonomy%2529%2520as%2520supervised%2520signals%2520within%250Aan%2520end-to-end%2520deep%2520neural%2520network%2520model%252C%2520termed%2520EnGraf-Net.%2520Extensive%250Aexperiments%2520on%2520three%2520well-known%2520datasets%2520CIFAR-100%252C%2520CUB-200-2011%252C%2520and%250AFGVC-Aircraft%2520demonstrate%2520the%2520superiority%2520of%2520EnGraf-Net%2520over%2520many%2520existing%250Afine-grained%2520models%252C%2520showing%2520competitive%2520performance%2520with%2520the%2520most%2520recent%250Astate-of-the-art%2520approaches%252C%2520without%2520requiring%2520cropping%2520techniques%2520or%2520manual%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnGraf-Net%3A%20Multiple%20Granularity%20Branch%20Network%20with%20Fine-Coarse%20Graft%0A%20%20Grained%20for%20Classification%20Task&entry.906535625=Riccardo%20La%20Grassa%20and%20Ignazio%20Gallo%20and%20Nicola%20Landro&entry.1292438233=%20%20Fine-grained%20classification%20models%20are%20designed%20to%20focus%20on%20the%20relevant%0Adetails%20necessary%20to%20distinguish%20highly%20similar%20classes%2C%20particularly%20when%0Aintra-class%20variance%20is%20high%20and%20inter-class%20variance%20is%20low.%20Most%20existing%0Amodels%20rely%20on%20part%20annotations%20such%20as%20bounding%20boxes%2C%20part%20locations%2C%20or%0Atextual%20attributes%20to%20enhance%20classification%20performance%2C%20while%20others%20employ%0Asophisticated%20techniques%20to%20automatically%20extract%20attention%20maps.%20We%20posit%20that%0Apart-based%20approaches%2C%20including%20automatic%20cropping%20methods%2C%20suffer%20from%20an%0Aincomplete%20representation%20of%20local%20features%2C%20which%20are%20fundamental%20for%0Adistinguishing%20similar%20objects.%20While%20fine-grained%20classification%20aims%20to%0Arecognize%20the%20leaves%20of%20a%20hierarchical%20structure%2C%20humans%20recognize%20objects%20by%0Aalso%20forming%20semantic%20associations.%20In%20this%20paper%2C%20we%20leverage%20semantic%0Aassociations%20structured%20as%20a%20hierarchy%20%28taxonomy%29%20as%20supervised%20signals%20within%0Aan%20end-to-end%20deep%20neural%20network%20model%2C%20termed%20EnGraf-Net.%20Extensive%0Aexperiments%20on%20three%20well-known%20datasets%20CIFAR-100%2C%20CUB-200-2011%2C%20and%0AFGVC-Aircraft%20demonstrate%20the%20superiority%20of%20EnGraf-Net%20over%20many%20existing%0Afine-grained%20models%2C%20showing%20competitive%20performance%20with%20the%20most%20recent%0Astate-of-the-art%20approaches%2C%20without%20requiring%20cropping%20techniques%20or%20manual%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21061v1&entry.124074799=Read"},
{"title": "A Sentinel-3 foundation model for ocean colour", "author": "Geoffrey Dawson and Remy Vandaele and Andrew Taylor and David Moffat and Helen Tamura-Wicks and Sarah Jackson and Rosie Lickorish and Paolo Fraccaro and Hywel Williams and Chunbo Luo and Anne Jones", "abstract": "  Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive\nunlabelled datasets, have the potential to drastically change AI applications\nin ocean science, where labelled data are often sparse and expensive to\ncollect. In this work, we describe a new foundation model using the Prithvi-EO\nVision Transformer architecture which has been pre-trained to reconstruct data\nfrom the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the\nmodel by fine-tuning on two downstream marine earth observation tasks. We first\nassess model performance compared to current baseline models used to quantify\nchlorophyll concentration. We then evaluate the FMs ability to refine remote\nsensing-based estimates of ocean primary production. Our results demonstrate\nthe utility of self-trained FMs for marine monitoring, in particular for making\nuse of small amounts of high quality labelled data and in capturing detailed\nspatial patterns of ocean colour whilst matching point observations. We\nconclude that this new generation of geospatial AI models has the potential to\nprovide more robust, data-driven insights into ocean ecosystems and their role\nin global climate processes.\n", "link": "http://arxiv.org/abs/2509.21273v1", "date": "2025-09-25", "relevancy": 2.6306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sentinel-3%20foundation%20model%20for%20ocean%20colour&body=Title%3A%20A%20Sentinel-3%20foundation%20model%20for%20ocean%20colour%0AAuthor%3A%20Geoffrey%20Dawson%20and%20Remy%20Vandaele%20and%20Andrew%20Taylor%20and%20David%20Moffat%20and%20Helen%20Tamura-Wicks%20and%20Sarah%20Jackson%20and%20Rosie%20Lickorish%20and%20Paolo%20Fraccaro%20and%20Hywel%20Williams%20and%20Chunbo%20Luo%20and%20Anne%20Jones%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20Foundation%20models%20%28FMs%29%2C%20pre-trained%20on%20massive%0Aunlabelled%20datasets%2C%20have%20the%20potential%20to%20drastically%20change%20AI%20applications%0Ain%20ocean%20science%2C%20where%20labelled%20data%20are%20often%20sparse%20and%20expensive%20to%0Acollect.%20In%20this%20work%2C%20we%20describe%20a%20new%20foundation%20model%20using%20the%20Prithvi-EO%0AVision%20Transformer%20architecture%20which%20has%20been%20pre-trained%20to%20reconstruct%20data%0Afrom%20the%20Sentinel-3%20Ocean%20and%20Land%20Colour%20Instrument%20%28OLCI%29.%20We%20evaluate%20the%0Amodel%20by%20fine-tuning%20on%20two%20downstream%20marine%20earth%20observation%20tasks.%20We%20first%0Aassess%20model%20performance%20compared%20to%20current%20baseline%20models%20used%20to%20quantify%0Achlorophyll%20concentration.%20We%20then%20evaluate%20the%20FMs%20ability%20to%20refine%20remote%0Asensing-based%20estimates%20of%20ocean%20primary%20production.%20Our%20results%20demonstrate%0Athe%20utility%20of%20self-trained%20FMs%20for%20marine%20monitoring%2C%20in%20particular%20for%20making%0Ause%20of%20small%20amounts%20of%20high%20quality%20labelled%20data%20and%20in%20capturing%20detailed%0Aspatial%20patterns%20of%20ocean%20colour%20whilst%20matching%20point%20observations.%20We%0Aconclude%20that%20this%20new%20generation%20of%20geospatial%20AI%20models%20has%20the%20potential%20to%0Aprovide%20more%20robust%2C%20data-driven%20insights%20into%20ocean%20ecosystems%20and%20their%20role%0Ain%20global%20climate%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sentinel-3%2520foundation%2520model%2520for%2520ocean%2520colour%26entry.906535625%3DGeoffrey%2520Dawson%2520and%2520Remy%2520Vandaele%2520and%2520Andrew%2520Taylor%2520and%2520David%2520Moffat%2520and%2520Helen%2520Tamura-Wicks%2520and%2520Sarah%2520Jackson%2520and%2520Rosie%2520Lickorish%2520and%2520Paolo%2520Fraccaro%2520and%2520Hywel%2520Williams%2520and%2520Chunbo%2520Luo%2520and%2520Anne%2520Jones%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520Foundation%2520models%2520%2528FMs%2529%252C%2520pre-trained%2520on%2520massive%250Aunlabelled%2520datasets%252C%2520have%2520the%2520potential%2520to%2520drastically%2520change%2520AI%2520applications%250Ain%2520ocean%2520science%252C%2520where%2520labelled%2520data%2520are%2520often%2520sparse%2520and%2520expensive%2520to%250Acollect.%2520In%2520this%2520work%252C%2520we%2520describe%2520a%2520new%2520foundation%2520model%2520using%2520the%2520Prithvi-EO%250AVision%2520Transformer%2520architecture%2520which%2520has%2520been%2520pre-trained%2520to%2520reconstruct%2520data%250Afrom%2520the%2520Sentinel-3%2520Ocean%2520and%2520Land%2520Colour%2520Instrument%2520%2528OLCI%2529.%2520We%2520evaluate%2520the%250Amodel%2520by%2520fine-tuning%2520on%2520two%2520downstream%2520marine%2520earth%2520observation%2520tasks.%2520We%2520first%250Aassess%2520model%2520performance%2520compared%2520to%2520current%2520baseline%2520models%2520used%2520to%2520quantify%250Achlorophyll%2520concentration.%2520We%2520then%2520evaluate%2520the%2520FMs%2520ability%2520to%2520refine%2520remote%250Asensing-based%2520estimates%2520of%2520ocean%2520primary%2520production.%2520Our%2520results%2520demonstrate%250Athe%2520utility%2520of%2520self-trained%2520FMs%2520for%2520marine%2520monitoring%252C%2520in%2520particular%2520for%2520making%250Ause%2520of%2520small%2520amounts%2520of%2520high%2520quality%2520labelled%2520data%2520and%2520in%2520capturing%2520detailed%250Aspatial%2520patterns%2520of%2520ocean%2520colour%2520whilst%2520matching%2520point%2520observations.%2520We%250Aconclude%2520that%2520this%2520new%2520generation%2520of%2520geospatial%2520AI%2520models%2520has%2520the%2520potential%2520to%250Aprovide%2520more%2520robust%252C%2520data-driven%2520insights%2520into%2520ocean%2520ecosystems%2520and%2520their%2520role%250Ain%2520global%2520climate%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sentinel-3%20foundation%20model%20for%20ocean%20colour&entry.906535625=Geoffrey%20Dawson%20and%20Remy%20Vandaele%20and%20Andrew%20Taylor%20and%20David%20Moffat%20and%20Helen%20Tamura-Wicks%20and%20Sarah%20Jackson%20and%20Rosie%20Lickorish%20and%20Paolo%20Fraccaro%20and%20Hywel%20Williams%20and%20Chunbo%20Luo%20and%20Anne%20Jones&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20Foundation%20models%20%28FMs%29%2C%20pre-trained%20on%20massive%0Aunlabelled%20datasets%2C%20have%20the%20potential%20to%20drastically%20change%20AI%20applications%0Ain%20ocean%20science%2C%20where%20labelled%20data%20are%20often%20sparse%20and%20expensive%20to%0Acollect.%20In%20this%20work%2C%20we%20describe%20a%20new%20foundation%20model%20using%20the%20Prithvi-EO%0AVision%20Transformer%20architecture%20which%20has%20been%20pre-trained%20to%20reconstruct%20data%0Afrom%20the%20Sentinel-3%20Ocean%20and%20Land%20Colour%20Instrument%20%28OLCI%29.%20We%20evaluate%20the%0Amodel%20by%20fine-tuning%20on%20two%20downstream%20marine%20earth%20observation%20tasks.%20We%20first%0Aassess%20model%20performance%20compared%20to%20current%20baseline%20models%20used%20to%20quantify%0Achlorophyll%20concentration.%20We%20then%20evaluate%20the%20FMs%20ability%20to%20refine%20remote%0Asensing-based%20estimates%20of%20ocean%20primary%20production.%20Our%20results%20demonstrate%0Athe%20utility%20of%20self-trained%20FMs%20for%20marine%20monitoring%2C%20in%20particular%20for%20making%0Ause%20of%20small%20amounts%20of%20high%20quality%20labelled%20data%20and%20in%20capturing%20detailed%0Aspatial%20patterns%20of%20ocean%20colour%20whilst%20matching%20point%20observations.%20We%0Aconclude%20that%20this%20new%20generation%20of%20geospatial%20AI%20models%20has%20the%20potential%20to%0Aprovide%20more%20robust%2C%20data-driven%20insights%20into%20ocean%20ecosystems%20and%20their%20role%0Ain%20global%20climate%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21273v1&entry.124074799=Read"},
{"title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach", "author": "Yongda Yu and Guohao Shi and Xianwei Wu and Haochuan He and XueMing Gu and Qianqian Zhao and Kui Liu and Qiushi Wang and Zhao Tian and Haifeng Shen and Guoping Rong", "abstract": "  Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.\n", "link": "http://arxiv.org/abs/2509.21170v1", "date": "2025-09-25", "relevancy": 2.6244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20LLMs%20to%20Analyze%20Multiple%20Dimensions%20of%20Code%20Review%3A%20A%0A%20%20Maximum%20Entropy%20Regulated%20Long%20Chain-of-Thought%20Approach&body=Title%3A%20Fine-Tuning%20LLMs%20to%20Analyze%20Multiple%20Dimensions%20of%20Code%20Review%3A%20A%0A%20%20Maximum%20Entropy%20Regulated%20Long%20Chain-of-Thought%20Approach%0AAuthor%3A%20Yongda%20Yu%20and%20Guohao%20Shi%20and%20Xianwei%20Wu%20and%20Haochuan%20He%20and%20XueMing%20Gu%20and%20Qianqian%20Zhao%20and%20Kui%20Liu%20and%20Qiushi%20Wang%20and%20Zhao%20Tian%20and%20Haifeng%20Shen%20and%20Guoping%20Rong%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20great%20potential%20in%20supporting%0Aautomated%20code%20review%20due%20to%20their%20impressive%20capabilities%20in%20context%0Aunderstanding%20and%20reasoning.%20However%2C%20these%20capabilities%20are%20still%20limited%0Acompared%20to%20human-level%20cognition%20because%20they%20are%20heavily%20influenced%20by%20the%0Atraining%20data.%20Recent%20research%20has%20demonstrated%20significantly%20improved%0Aperformance%20through%20fine-tuning%20LLMs%20with%20code%20review%20data.%20However%2C%20compared%0Ato%20human%20reviewers%20who%20often%20simultaneously%20analyze%20multiple%20dimensions%20of%20code%0Areview%20to%20better%20identify%20issues%2C%20the%20full%20potential%20of%20these%20methods%20is%0Ahampered%20by%20the%20limited%20or%20vague%20information%20used%20to%20fine-tune%20the%20models.%20This%0Apaper%20contributes%20MelcotCR%2C%20a%20chain-of-thought%20%28COT%29%20fine-tuning%20approach%20that%0Atrains%20LLMs%20with%20an%20impressive%20reasoning%20ability%20to%20analyze%20multiple%20dimensions%0Aof%20code%20review%20by%20harnessing%20long%20COT%20techniques%20to%20provide%20rich%20structured%0Ainformation.%20To%20address%20context%20loss%20and%20reasoning%20logic%20loss%20issues%20that%0Afrequently%20occur%20when%20LLMs%20process%20long%20COT%20prompts%2C%20we%20propose%20a%20solution%20that%0Acombines%20the%20Maximum%20Entropy%20%28ME%29%20modeling%20principle%20with%20pre-defined%20reasoning%0Apathways%20in%20MelcotCR%20to%20enable%20more%20effective%20utilization%20of%20in-context%0Aknowledge%20within%20long%20COT%20prompts%20while%20strengthening%20the%20logical%20tightness%20of%0Athe%20reasoning%20process.%20Empirical%20evaluations%20on%20our%20curated%20MelcotCR%20dataset%0Aand%20the%20public%20CodeReviewer%20dataset%20reveal%20that%20a%20low-parameter%20base%20model%2C%0Asuch%20as%2014B%20Qwen2.5%2C%20fine-tuned%20with%20MelcotCR%20can%20surpass%20state-of-the-art%0Amethods%20in%20terms%20of%20the%20accuracy%20of%20detecting%20and%20describing%20code%20issues%2C%20with%0Aits%20performance%20remarkably%20on%20par%20with%20that%20of%20the%20671B%20DeepSeek-R1%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520LLMs%2520to%2520Analyze%2520Multiple%2520Dimensions%2520of%2520Code%2520Review%253A%2520A%250A%2520%2520Maximum%2520Entropy%2520Regulated%2520Long%2520Chain-of-Thought%2520Approach%26entry.906535625%3DYongda%2520Yu%2520and%2520Guohao%2520Shi%2520and%2520Xianwei%2520Wu%2520and%2520Haochuan%2520He%2520and%2520XueMing%2520Gu%2520and%2520Qianqian%2520Zhao%2520and%2520Kui%2520Liu%2520and%2520Qiushi%2520Wang%2520and%2520Zhao%2520Tian%2520and%2520Haifeng%2520Shen%2520and%2520Guoping%2520Rong%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520great%2520potential%2520in%2520supporting%250Aautomated%2520code%2520review%2520due%2520to%2520their%2520impressive%2520capabilities%2520in%2520context%250Aunderstanding%2520and%2520reasoning.%2520However%252C%2520these%2520capabilities%2520are%2520still%2520limited%250Acompared%2520to%2520human-level%2520cognition%2520because%2520they%2520are%2520heavily%2520influenced%2520by%2520the%250Atraining%2520data.%2520Recent%2520research%2520has%2520demonstrated%2520significantly%2520improved%250Aperformance%2520through%2520fine-tuning%2520LLMs%2520with%2520code%2520review%2520data.%2520However%252C%2520compared%250Ato%2520human%2520reviewers%2520who%2520often%2520simultaneously%2520analyze%2520multiple%2520dimensions%2520of%2520code%250Areview%2520to%2520better%2520identify%2520issues%252C%2520the%2520full%2520potential%2520of%2520these%2520methods%2520is%250Ahampered%2520by%2520the%2520limited%2520or%2520vague%2520information%2520used%2520to%2520fine-tune%2520the%2520models.%2520This%250Apaper%2520contributes%2520MelcotCR%252C%2520a%2520chain-of-thought%2520%2528COT%2529%2520fine-tuning%2520approach%2520that%250Atrains%2520LLMs%2520with%2520an%2520impressive%2520reasoning%2520ability%2520to%2520analyze%2520multiple%2520dimensions%250Aof%2520code%2520review%2520by%2520harnessing%2520long%2520COT%2520techniques%2520to%2520provide%2520rich%2520structured%250Ainformation.%2520To%2520address%2520context%2520loss%2520and%2520reasoning%2520logic%2520loss%2520issues%2520that%250Afrequently%2520occur%2520when%2520LLMs%2520process%2520long%2520COT%2520prompts%252C%2520we%2520propose%2520a%2520solution%2520that%250Acombines%2520the%2520Maximum%2520Entropy%2520%2528ME%2529%2520modeling%2520principle%2520with%2520pre-defined%2520reasoning%250Apathways%2520in%2520MelcotCR%2520to%2520enable%2520more%2520effective%2520utilization%2520of%2520in-context%250Aknowledge%2520within%2520long%2520COT%2520prompts%2520while%2520strengthening%2520the%2520logical%2520tightness%2520of%250Athe%2520reasoning%2520process.%2520Empirical%2520evaluations%2520on%2520our%2520curated%2520MelcotCR%2520dataset%250Aand%2520the%2520public%2520CodeReviewer%2520dataset%2520reveal%2520that%2520a%2520low-parameter%2520base%2520model%252C%250Asuch%2520as%252014B%2520Qwen2.5%252C%2520fine-tuned%2520with%2520MelcotCR%2520can%2520surpass%2520state-of-the-art%250Amethods%2520in%2520terms%2520of%2520the%2520accuracy%2520of%2520detecting%2520and%2520describing%2520code%2520issues%252C%2520with%250Aits%2520performance%2520remarkably%2520on%2520par%2520with%2520that%2520of%2520the%2520671B%2520DeepSeek-R1%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20LLMs%20to%20Analyze%20Multiple%20Dimensions%20of%20Code%20Review%3A%20A%0A%20%20Maximum%20Entropy%20Regulated%20Long%20Chain-of-Thought%20Approach&entry.906535625=Yongda%20Yu%20and%20Guohao%20Shi%20and%20Xianwei%20Wu%20and%20Haochuan%20He%20and%20XueMing%20Gu%20and%20Qianqian%20Zhao%20and%20Kui%20Liu%20and%20Qiushi%20Wang%20and%20Zhao%20Tian%20and%20Haifeng%20Shen%20and%20Guoping%20Rong&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20great%20potential%20in%20supporting%0Aautomated%20code%20review%20due%20to%20their%20impressive%20capabilities%20in%20context%0Aunderstanding%20and%20reasoning.%20However%2C%20these%20capabilities%20are%20still%20limited%0Acompared%20to%20human-level%20cognition%20because%20they%20are%20heavily%20influenced%20by%20the%0Atraining%20data.%20Recent%20research%20has%20demonstrated%20significantly%20improved%0Aperformance%20through%20fine-tuning%20LLMs%20with%20code%20review%20data.%20However%2C%20compared%0Ato%20human%20reviewers%20who%20often%20simultaneously%20analyze%20multiple%20dimensions%20of%20code%0Areview%20to%20better%20identify%20issues%2C%20the%20full%20potential%20of%20these%20methods%20is%0Ahampered%20by%20the%20limited%20or%20vague%20information%20used%20to%20fine-tune%20the%20models.%20This%0Apaper%20contributes%20MelcotCR%2C%20a%20chain-of-thought%20%28COT%29%20fine-tuning%20approach%20that%0Atrains%20LLMs%20with%20an%20impressive%20reasoning%20ability%20to%20analyze%20multiple%20dimensions%0Aof%20code%20review%20by%20harnessing%20long%20COT%20techniques%20to%20provide%20rich%20structured%0Ainformation.%20To%20address%20context%20loss%20and%20reasoning%20logic%20loss%20issues%20that%0Afrequently%20occur%20when%20LLMs%20process%20long%20COT%20prompts%2C%20we%20propose%20a%20solution%20that%0Acombines%20the%20Maximum%20Entropy%20%28ME%29%20modeling%20principle%20with%20pre-defined%20reasoning%0Apathways%20in%20MelcotCR%20to%20enable%20more%20effective%20utilization%20of%20in-context%0Aknowledge%20within%20long%20COT%20prompts%20while%20strengthening%20the%20logical%20tightness%20of%0Athe%20reasoning%20process.%20Empirical%20evaluations%20on%20our%20curated%20MelcotCR%20dataset%0Aand%20the%20public%20CodeReviewer%20dataset%20reveal%20that%20a%20low-parameter%20base%20model%2C%0Asuch%20as%2014B%20Qwen2.5%2C%20fine-tuned%20with%20MelcotCR%20can%20surpass%20state-of-the-art%0Amethods%20in%20terms%20of%20the%20accuracy%20of%20detecting%20and%20describing%20code%20issues%2C%20with%0Aits%20performance%20remarkably%20on%20par%20with%20that%20of%20the%20671B%20DeepSeek-R1%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21170v1&entry.124074799=Read"},
{"title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding", "author": "I\u00f1igo Alonso and Imanol Miranda and Eneko Agirre and Mirella Lapata", "abstract": "  While table understanding increasingly relies on pixel-only settings where\ntables are processed as visual representations, current benchmarks\npredominantly use synthetic renderings that lack the complexity and visual\ndiversity of real-world tables. Additionally, existing visual table\nunderstanding (VTU) datasets offer fixed examples with single visualizations\nand pre-defined instructions, providing no access to underlying serialized data\nfor reformulation. We introduce TABLET, a large-scale VTU dataset with 4\nmillion examples across 20 tasks, grounded in 2 million unique tables where 88%\npreserve original visualizations. Each example includes paired image-HTML\nrepresentations, comprehensive metadata, and provenance information linking\nback to the source datasets. Fine-tuning vision-language models like\nQwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while\nincreasing robustness on real-world table visualizations. By preserving\noriginal visualizations and maintaining example traceability in a unified\nlarge-scale collection, TABLET establishes a foundation for robust training and\nextensible evaluation of future VTU models.\n", "link": "http://arxiv.org/abs/2509.21205v1", "date": "2025-09-25", "relevancy": 2.615, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&body=Title%3A%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding%0AAuthor%3A%20I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata%0AAbstract%3A%20%20%20While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%20where%0Atables%20are%20processed%20as%20visual%20representations%2C%20current%20benchmarks%0Apredominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%0Adiversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%0Aunderstanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%0Aand%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%0Afor%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%0Amillion%20examples%20across%2020%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%0Apreserve%20original%20visualizations.%20Each%20example%20includes%20paired%20image-HTML%0Arepresentations%2C%20comprehensive%20metadata%2C%20and%20provenance%20information%20linking%0Aback%20to%20the%20source%20datasets.%20Fine-tuning%20vision-language%20models%20like%0AQwen2.5-VL-7B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%0Aincreasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%0Aoriginal%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%0Alarge-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%0Aextensible%20evaluation%20of%20future%20VTU%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTABLET%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Robust%2520Visual%2520Table%2520Understanding%26entry.906535625%3DI%25C3%25B1igo%2520Alonso%2520and%2520Imanol%2520Miranda%2520and%2520Eneko%2520Agirre%2520and%2520Mirella%2520Lapata%26entry.1292438233%3D%2520%2520While%2520table%2520understanding%2520increasingly%2520relies%2520on%2520pixel-only%2520settings%2520where%250Atables%2520are%2520processed%2520as%2520visual%2520representations%252C%2520current%2520benchmarks%250Apredominantly%2520use%2520synthetic%2520renderings%2520that%2520lack%2520the%2520complexity%2520and%2520visual%250Adiversity%2520of%2520real-world%2520tables.%2520Additionally%252C%2520existing%2520visual%2520table%250Aunderstanding%2520%2528VTU%2529%2520datasets%2520offer%2520fixed%2520examples%2520with%2520single%2520visualizations%250Aand%2520pre-defined%2520instructions%252C%2520providing%2520no%2520access%2520to%2520underlying%2520serialized%2520data%250Afor%2520reformulation.%2520We%2520introduce%2520TABLET%252C%2520a%2520large-scale%2520VTU%2520dataset%2520with%25204%250Amillion%2520examples%2520across%252020%2520tasks%252C%2520grounded%2520in%25202%2520million%2520unique%2520tables%2520where%252088%2525%250Apreserve%2520original%2520visualizations.%2520Each%2520example%2520includes%2520paired%2520image-HTML%250Arepresentations%252C%2520comprehensive%2520metadata%252C%2520and%2520provenance%2520information%2520linking%250Aback%2520to%2520the%2520source%2520datasets.%2520Fine-tuning%2520vision-language%2520models%2520like%250AQwen2.5-VL-7B%2520on%2520TABLET%2520improves%2520performance%2520on%2520seen%2520and%2520unseen%2520VTU%2520tasks%2520while%250Aincreasing%2520robustness%2520on%2520real-world%2520table%2520visualizations.%2520By%2520preserving%250Aoriginal%2520visualizations%2520and%2520maintaining%2520example%2520traceability%2520in%2520a%2520unified%250Alarge-scale%2520collection%252C%2520TABLET%2520establishes%2520a%2520foundation%2520for%2520robust%2520training%2520and%250Aextensible%2520evaluation%2520of%2520future%2520VTU%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&entry.906535625=I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata&entry.1292438233=%20%20While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%20where%0Atables%20are%20processed%20as%20visual%20representations%2C%20current%20benchmarks%0Apredominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%0Adiversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%0Aunderstanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%0Aand%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%0Afor%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%0Amillion%20examples%20across%2020%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%0Apreserve%20original%20visualizations.%20Each%20example%20includes%20paired%20image-HTML%0Arepresentations%2C%20comprehensive%20metadata%2C%20and%20provenance%20information%20linking%0Aback%20to%20the%20source%20datasets.%20Fine-tuning%20vision-language%20models%20like%0AQwen2.5-VL-7B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%0Aincreasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%0Aoriginal%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%0Alarge-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%0Aextensible%20evaluation%20of%20future%20VTU%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21205v1&entry.124074799=Read"},
{"title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models\n  Using Themselves", "author": "Ruofan Wang and Juncheng Li and Yixu Wang and Bo Wang and Xiaosen Wang and Yan Teng and Yingchun Wang and Xingjun Ma and Yu-Gang Jiang", "abstract": "  As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses. VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench.\n", "link": "http://arxiv.org/abs/2411.00827v6", "date": "2025-09-25", "relevancy": 2.5866, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDEATOR%3A%20Jailbreaking%20and%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20Using%20Themselves&body=Title%3A%20IDEATOR%3A%20Jailbreaking%20and%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20Using%20Themselves%0AAuthor%3A%20Ruofan%20Wang%20and%20Juncheng%20Li%20and%20Yixu%20Wang%20and%20Bo%20Wang%20and%20Xiaosen%20Wang%20and%20Yan%20Teng%20and%20Yingchun%20Wang%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20As%20large%20Vision-Language%20Models%20%28VLMs%29%20gain%20prominence%2C%20ensuring%20their%20safe%0Adeployment%20has%20become%20critical.%20Recent%20studies%20have%20explored%20VLM%20robustness%0Aagainst%20jailbreak%20attacks-techniques%20that%20exploit%20model%20vulnerabilities%20to%0Aelicit%20harmful%20outputs.%20However%2C%20the%20limited%20availability%20of%20diverse%20multimodal%0Adata%20has%20constrained%20current%20approaches%20to%20rely%20heavily%20on%20adversarial%20or%0Amanually%20crafted%20images%20derived%20from%20harmful%20text%20datasets%2C%20which%20often%20lack%0Aeffectiveness%20and%20diversity%20across%20different%20contexts.%20In%20this%20paper%2C%20we%0Apropose%20IDEATOR%2C%20a%20novel%20jailbreak%20method%20that%20autonomously%20generates%20malicious%0Aimage-text%20pairs%20for%20black-box%20jailbreak%20attacks.%20IDEATOR%20is%20grounded%20in%20the%0Ainsight%20that%20VLMs%20themselves%20could%20serve%20as%20powerful%20red%20team%20models%20for%0Agenerating%20multimodal%20jailbreak%20prompts.%20Specifically%2C%20IDEATOR%20leverages%20a%20VLM%0Ato%20create%20targeted%20jailbreak%20texts%20and%20pairs%20them%20with%20jailbreak%20images%0Agenerated%20by%20a%20state-of-the-art%20diffusion%20model.%20Extensive%20experiments%0Ademonstrate%20IDEATOR%27s%20high%20effectiveness%20and%20transferability%2C%20achieving%20a%2094%25%0Aattack%20success%20rate%20%28ASR%29%20in%20jailbreaking%20MiniGPT-4%20with%20an%20average%20of%20only%0A5.34%20queries%2C%20and%20high%20ASRs%20of%2082%25%2C%2088%25%2C%20and%2075%25%20when%20transferred%20to%20LLaVA%2C%0AInstructBLIP%2C%20and%20Chameleon%2C%20respectively.%20Building%20on%20IDEATOR%27s%20strong%0Atransferability%20and%20automated%20process%2C%20we%20introduce%20the%20VLJailbreakBench%2C%20a%0Asafety%20benchmark%20comprising%203%2C654%20multimodal%20jailbreak%20samples.%20Our%20benchmark%0Aresults%20on%2011%20recently%20released%20VLMs%20reveal%20significant%20gaps%20in%20safety%0Aalignment.%20For%20instance%2C%20our%20challenge%20set%20achieves%20ASRs%20of%2046.31%25%20on%20GPT-4o%0Aand%2019.65%25%20on%20Claude-3.5-Sonnet%2C%20underscoring%20the%20urgent%20need%20for%20stronger%0Adefenses.%20VLJailbreakBench%20is%20publicly%20available%20at%0Ahttps%3A//roywang021.github.io/VLJailbreakBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00827v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDEATOR%253A%2520Jailbreaking%2520and%2520Benchmarking%2520Large%2520Vision-Language%2520Models%250A%2520%2520Using%2520Themselves%26entry.906535625%3DRuofan%2520Wang%2520and%2520Juncheng%2520Li%2520and%2520Yixu%2520Wang%2520and%2520Bo%2520Wang%2520and%2520Xiaosen%2520Wang%2520and%2520Yan%2520Teng%2520and%2520Yingchun%2520Wang%2520and%2520Xingjun%2520Ma%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520As%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520gain%2520prominence%252C%2520ensuring%2520their%2520safe%250Adeployment%2520has%2520become%2520critical.%2520Recent%2520studies%2520have%2520explored%2520VLM%2520robustness%250Aagainst%2520jailbreak%2520attacks-techniques%2520that%2520exploit%2520model%2520vulnerabilities%2520to%250Aelicit%2520harmful%2520outputs.%2520However%252C%2520the%2520limited%2520availability%2520of%2520diverse%2520multimodal%250Adata%2520has%2520constrained%2520current%2520approaches%2520to%2520rely%2520heavily%2520on%2520adversarial%2520or%250Amanually%2520crafted%2520images%2520derived%2520from%2520harmful%2520text%2520datasets%252C%2520which%2520often%2520lack%250Aeffectiveness%2520and%2520diversity%2520across%2520different%2520contexts.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520IDEATOR%252C%2520a%2520novel%2520jailbreak%2520method%2520that%2520autonomously%2520generates%2520malicious%250Aimage-text%2520pairs%2520for%2520black-box%2520jailbreak%2520attacks.%2520IDEATOR%2520is%2520grounded%2520in%2520the%250Ainsight%2520that%2520VLMs%2520themselves%2520could%2520serve%2520as%2520powerful%2520red%2520team%2520models%2520for%250Agenerating%2520multimodal%2520jailbreak%2520prompts.%2520Specifically%252C%2520IDEATOR%2520leverages%2520a%2520VLM%250Ato%2520create%2520targeted%2520jailbreak%2520texts%2520and%2520pairs%2520them%2520with%2520jailbreak%2520images%250Agenerated%2520by%2520a%2520state-of-the-art%2520diffusion%2520model.%2520Extensive%2520experiments%250Ademonstrate%2520IDEATOR%2527s%2520high%2520effectiveness%2520and%2520transferability%252C%2520achieving%2520a%252094%2525%250Aattack%2520success%2520rate%2520%2528ASR%2529%2520in%2520jailbreaking%2520MiniGPT-4%2520with%2520an%2520average%2520of%2520only%250A5.34%2520queries%252C%2520and%2520high%2520ASRs%2520of%252082%2525%252C%252088%2525%252C%2520and%252075%2525%2520when%2520transferred%2520to%2520LLaVA%252C%250AInstructBLIP%252C%2520and%2520Chameleon%252C%2520respectively.%2520Building%2520on%2520IDEATOR%2527s%2520strong%250Atransferability%2520and%2520automated%2520process%252C%2520we%2520introduce%2520the%2520VLJailbreakBench%252C%2520a%250Asafety%2520benchmark%2520comprising%25203%252C654%2520multimodal%2520jailbreak%2520samples.%2520Our%2520benchmark%250Aresults%2520on%252011%2520recently%2520released%2520VLMs%2520reveal%2520significant%2520gaps%2520in%2520safety%250Aalignment.%2520For%2520instance%252C%2520our%2520challenge%2520set%2520achieves%2520ASRs%2520of%252046.31%2525%2520on%2520GPT-4o%250Aand%252019.65%2525%2520on%2520Claude-3.5-Sonnet%252C%2520underscoring%2520the%2520urgent%2520need%2520for%2520stronger%250Adefenses.%2520VLJailbreakBench%2520is%2520publicly%2520available%2520at%250Ahttps%253A//roywang021.github.io/VLJailbreakBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00827v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDEATOR%3A%20Jailbreaking%20and%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20Using%20Themselves&entry.906535625=Ruofan%20Wang%20and%20Juncheng%20Li%20and%20Yixu%20Wang%20and%20Bo%20Wang%20and%20Xiaosen%20Wang%20and%20Yan%20Teng%20and%20Yingchun%20Wang%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20As%20large%20Vision-Language%20Models%20%28VLMs%29%20gain%20prominence%2C%20ensuring%20their%20safe%0Adeployment%20has%20become%20critical.%20Recent%20studies%20have%20explored%20VLM%20robustness%0Aagainst%20jailbreak%20attacks-techniques%20that%20exploit%20model%20vulnerabilities%20to%0Aelicit%20harmful%20outputs.%20However%2C%20the%20limited%20availability%20of%20diverse%20multimodal%0Adata%20has%20constrained%20current%20approaches%20to%20rely%20heavily%20on%20adversarial%20or%0Amanually%20crafted%20images%20derived%20from%20harmful%20text%20datasets%2C%20which%20often%20lack%0Aeffectiveness%20and%20diversity%20across%20different%20contexts.%20In%20this%20paper%2C%20we%0Apropose%20IDEATOR%2C%20a%20novel%20jailbreak%20method%20that%20autonomously%20generates%20malicious%0Aimage-text%20pairs%20for%20black-box%20jailbreak%20attacks.%20IDEATOR%20is%20grounded%20in%20the%0Ainsight%20that%20VLMs%20themselves%20could%20serve%20as%20powerful%20red%20team%20models%20for%0Agenerating%20multimodal%20jailbreak%20prompts.%20Specifically%2C%20IDEATOR%20leverages%20a%20VLM%0Ato%20create%20targeted%20jailbreak%20texts%20and%20pairs%20them%20with%20jailbreak%20images%0Agenerated%20by%20a%20state-of-the-art%20diffusion%20model.%20Extensive%20experiments%0Ademonstrate%20IDEATOR%27s%20high%20effectiveness%20and%20transferability%2C%20achieving%20a%2094%25%0Aattack%20success%20rate%20%28ASR%29%20in%20jailbreaking%20MiniGPT-4%20with%20an%20average%20of%20only%0A5.34%20queries%2C%20and%20high%20ASRs%20of%2082%25%2C%2088%25%2C%20and%2075%25%20when%20transferred%20to%20LLaVA%2C%0AInstructBLIP%2C%20and%20Chameleon%2C%20respectively.%20Building%20on%20IDEATOR%27s%20strong%0Atransferability%20and%20automated%20process%2C%20we%20introduce%20the%20VLJailbreakBench%2C%20a%0Asafety%20benchmark%20comprising%203%2C654%20multimodal%20jailbreak%20samples.%20Our%20benchmark%0Aresults%20on%2011%20recently%20released%20VLMs%20reveal%20significant%20gaps%20in%20safety%0Aalignment.%20For%20instance%2C%20our%20challenge%20set%20achieves%20ASRs%20of%2046.31%25%20on%20GPT-4o%0Aand%2019.65%25%20on%20Claude-3.5-Sonnet%2C%20underscoring%20the%20urgent%20need%20for%20stronger%0Adefenses.%20VLJailbreakBench%20is%20publicly%20available%20at%0Ahttps%3A//roywang021.github.io/VLJailbreakBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00827v6&entry.124074799=Read"},
{"title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks", "author": "Taishi Nakamura and Satoki Ishikawa and Masaki Kawamura and Takumi Okamoto and Daisuke Nohara and Jun Suzuki and Rio Yokota", "abstract": "  Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization skills and reasoning skills. By\ntraining MoE families that vary total parameters, active parameters, and\ntop-$k$ routing under fixed compute budgets, we disentangle pre-training loss\nfrom downstream accuracy. Our results reveal two principles. First, Active\nFLOPs: models with identical training loss but greater active compute achieve\nhigher reasoning accuracy. Second, Total tokens per parameter (TPP):\nmemorization tasks improve with more parameters, while reasoning tasks benefit\nfrom optimal TPP, indicating that reasoning is data-hungry. Neither\nreinforcement learning post-training (GRPO) nor increased test-time compute\nalters these trends. We therefore argue that optimal MoE sparsity must be\ndetermined jointly by active FLOPs and TPP, revising the classical picture of\ncompute-optimal scaling. Our model checkpoints, code and logs are open-source\nat https://github.com/rioyokotalab/optimal-sparsity.\n", "link": "http://arxiv.org/abs/2508.18672v2", "date": "2025-09-25", "relevancy": 2.5854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Sparsity%20of%20Mixture-of-Experts%20Language%20Models%20for%20Reasoning%0A%20%20Tasks&body=Title%3A%20Optimal%20Sparsity%20of%20Mixture-of-Experts%20Language%20Models%20for%20Reasoning%0A%20%20Tasks%0AAuthor%3A%20Taishi%20Nakamura%20and%20Satoki%20Ishikawa%20and%20Masaki%20Kawamura%20and%20Takumi%20Okamoto%20and%20Daisuke%20Nohara%20and%20Jun%20Suzuki%20and%20Rio%20Yokota%0AAbstract%3A%20%20%20Empirical%20scaling%20laws%20have%20driven%20the%20evolution%20of%20large%20language%20models%0A%28LLMs%29%2C%20yet%20their%20coefficients%20shift%20whenever%20the%20model%20architecture%20or%20data%0Apipeline%20changes.%20Mixture-of-Experts%20%28MoE%29%20models%2C%20now%20standard%20in%0Astate-of-the-art%20systems%2C%20introduce%20a%20new%20sparsity%20dimension%20that%20current%0Adense-model%20frontiers%20overlook.%20We%20investigate%20how%20MoE%20sparsity%20influences%20two%0Adistinct%20capability%20regimes%3A%20memorization%20skills%20and%20reasoning%20skills.%20By%0Atraining%20MoE%20families%20that%20vary%20total%20parameters%2C%20active%20parameters%2C%20and%0Atop-%24k%24%20routing%20under%20fixed%20compute%20budgets%2C%20we%20disentangle%20pre-training%20loss%0Afrom%20downstream%20accuracy.%20Our%20results%20reveal%20two%20principles.%20First%2C%20Active%0AFLOPs%3A%20models%20with%20identical%20training%20loss%20but%20greater%20active%20compute%20achieve%0Ahigher%20reasoning%20accuracy.%20Second%2C%20Total%20tokens%20per%20parameter%20%28TPP%29%3A%0Amemorization%20tasks%20improve%20with%20more%20parameters%2C%20while%20reasoning%20tasks%20benefit%0Afrom%20optimal%20TPP%2C%20indicating%20that%20reasoning%20is%20data-hungry.%20Neither%0Areinforcement%20learning%20post-training%20%28GRPO%29%20nor%20increased%20test-time%20compute%0Aalters%20these%20trends.%20We%20therefore%20argue%20that%20optimal%20MoE%20sparsity%20must%20be%0Adetermined%20jointly%20by%20active%20FLOPs%20and%20TPP%2C%20revising%20the%20classical%20picture%20of%0Acompute-optimal%20scaling.%20Our%20model%20checkpoints%2C%20code%20and%20logs%20are%20open-source%0Aat%20https%3A//github.com/rioyokotalab/optimal-sparsity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Sparsity%2520of%2520Mixture-of-Experts%2520Language%2520Models%2520for%2520Reasoning%250A%2520%2520Tasks%26entry.906535625%3DTaishi%2520Nakamura%2520and%2520Satoki%2520Ishikawa%2520and%2520Masaki%2520Kawamura%2520and%2520Takumi%2520Okamoto%2520and%2520Daisuke%2520Nohara%2520and%2520Jun%2520Suzuki%2520and%2520Rio%2520Yokota%26entry.1292438233%3D%2520%2520Empirical%2520scaling%2520laws%2520have%2520driven%2520the%2520evolution%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520yet%2520their%2520coefficients%2520shift%2520whenever%2520the%2520model%2520architecture%2520or%2520data%250Apipeline%2520changes.%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%252C%2520now%2520standard%2520in%250Astate-of-the-art%2520systems%252C%2520introduce%2520a%2520new%2520sparsity%2520dimension%2520that%2520current%250Adense-model%2520frontiers%2520overlook.%2520We%2520investigate%2520how%2520MoE%2520sparsity%2520influences%2520two%250Adistinct%2520capability%2520regimes%253A%2520memorization%2520skills%2520and%2520reasoning%2520skills.%2520By%250Atraining%2520MoE%2520families%2520that%2520vary%2520total%2520parameters%252C%2520active%2520parameters%252C%2520and%250Atop-%2524k%2524%2520routing%2520under%2520fixed%2520compute%2520budgets%252C%2520we%2520disentangle%2520pre-training%2520loss%250Afrom%2520downstream%2520accuracy.%2520Our%2520results%2520reveal%2520two%2520principles.%2520First%252C%2520Active%250AFLOPs%253A%2520models%2520with%2520identical%2520training%2520loss%2520but%2520greater%2520active%2520compute%2520achieve%250Ahigher%2520reasoning%2520accuracy.%2520Second%252C%2520Total%2520tokens%2520per%2520parameter%2520%2528TPP%2529%253A%250Amemorization%2520tasks%2520improve%2520with%2520more%2520parameters%252C%2520while%2520reasoning%2520tasks%2520benefit%250Afrom%2520optimal%2520TPP%252C%2520indicating%2520that%2520reasoning%2520is%2520data-hungry.%2520Neither%250Areinforcement%2520learning%2520post-training%2520%2528GRPO%2529%2520nor%2520increased%2520test-time%2520compute%250Aalters%2520these%2520trends.%2520We%2520therefore%2520argue%2520that%2520optimal%2520MoE%2520sparsity%2520must%2520be%250Adetermined%2520jointly%2520by%2520active%2520FLOPs%2520and%2520TPP%252C%2520revising%2520the%2520classical%2520picture%2520of%250Acompute-optimal%2520scaling.%2520Our%2520model%2520checkpoints%252C%2520code%2520and%2520logs%2520are%2520open-source%250Aat%2520https%253A//github.com/rioyokotalab/optimal-sparsity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Sparsity%20of%20Mixture-of-Experts%20Language%20Models%20for%20Reasoning%0A%20%20Tasks&entry.906535625=Taishi%20Nakamura%20and%20Satoki%20Ishikawa%20and%20Masaki%20Kawamura%20and%20Takumi%20Okamoto%20and%20Daisuke%20Nohara%20and%20Jun%20Suzuki%20and%20Rio%20Yokota&entry.1292438233=%20%20Empirical%20scaling%20laws%20have%20driven%20the%20evolution%20of%20large%20language%20models%0A%28LLMs%29%2C%20yet%20their%20coefficients%20shift%20whenever%20the%20model%20architecture%20or%20data%0Apipeline%20changes.%20Mixture-of-Experts%20%28MoE%29%20models%2C%20now%20standard%20in%0Astate-of-the-art%20systems%2C%20introduce%20a%20new%20sparsity%20dimension%20that%20current%0Adense-model%20frontiers%20overlook.%20We%20investigate%20how%20MoE%20sparsity%20influences%20two%0Adistinct%20capability%20regimes%3A%20memorization%20skills%20and%20reasoning%20skills.%20By%0Atraining%20MoE%20families%20that%20vary%20total%20parameters%2C%20active%20parameters%2C%20and%0Atop-%24k%24%20routing%20under%20fixed%20compute%20budgets%2C%20we%20disentangle%20pre-training%20loss%0Afrom%20downstream%20accuracy.%20Our%20results%20reveal%20two%20principles.%20First%2C%20Active%0AFLOPs%3A%20models%20with%20identical%20training%20loss%20but%20greater%20active%20compute%20achieve%0Ahigher%20reasoning%20accuracy.%20Second%2C%20Total%20tokens%20per%20parameter%20%28TPP%29%3A%0Amemorization%20tasks%20improve%20with%20more%20parameters%2C%20while%20reasoning%20tasks%20benefit%0Afrom%20optimal%20TPP%2C%20indicating%20that%20reasoning%20is%20data-hungry.%20Neither%0Areinforcement%20learning%20post-training%20%28GRPO%29%20nor%20increased%20test-time%20compute%0Aalters%20these%20trends.%20We%20therefore%20argue%20that%20optimal%20MoE%20sparsity%20must%20be%0Adetermined%20jointly%20by%20active%20FLOPs%20and%20TPP%2C%20revising%20the%20classical%20picture%20of%0Acompute-optimal%20scaling.%20Our%20model%20checkpoints%2C%20code%20and%20logs%20are%20open-source%0Aat%20https%3A//github.com/rioyokotalab/optimal-sparsity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18672v2&entry.124074799=Read"},
{"title": "Supervised Graph Contrastive Learning for Gene Regulatory Networks", "author": "Sho Oshima and Yuji Okamoto and Taisei Tosaki and Ryosuke Kojima and Yasushi Okuno", "abstract": "  Graph Contrastive Learning (GCL) is a powerful self-supervised learning\nframework that performs data augmentation through graph perturbations, with\ngrowing applications in the analysis of biological networks such as Gene\nRegulatory Networks (GRNs). The artificial perturbations commonly used in GCL,\nsuch as node dropping, induce structural changes that can diverge from\nbiological reality. This concern has contributed to a broader trend in graph\nrepresentation learning toward augmentation-free methods, which view such\nstructural changes as problematic and to be avoided. However, this trend\noverlooks the fundamental insight that structural changes from biologically\nmeaningful perturbations are not a problem to be avoided but a rich source of\ninformation, thereby ignoring the valuable opportunity to leverage data from\nreal biological experiments. Motivated by this insight, we propose SupGCL\n(Supervised Graph Contrastive Learning), a new GCL method for GRNs that\ndirectly incorporates biological perturbations from gene knockdown experiments\nas supervision. SupGCL is a probabilistic formulation that continuously\ngeneralizes conventional GCL, linking artificial augmentations with real\nperturbations measured in knockdown experiments and using the latter as\nexplicit supervisory signals. To assess effectiveness, we train GRN\nrepresentations with SupGCL and evaluate their performance on downstream tasks.\nThe evaluation includes both node-level tasks, such as gene function\nclassification, and graph-level tasks on patient-specific GRNs, such as patient\nsurvival hazard prediction. Across 13 tasks built from GRN datasets derived\nfrom patients with three cancer types, SupGCL consistently outperforms\nstate-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2505.17786v4", "date": "2025-09-25", "relevancy": 2.5193, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5125}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Graph%20Contrastive%20Learning%20for%20Gene%20Regulatory%20Networks&body=Title%3A%20Supervised%20Graph%20Contrastive%20Learning%20for%20Gene%20Regulatory%20Networks%0AAuthor%3A%20Sho%20Oshima%20and%20Yuji%20Okamoto%20and%20Taisei%20Tosaki%20and%20Ryosuke%20Kojima%20and%20Yasushi%20Okuno%0AAbstract%3A%20%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20is%20a%20powerful%20self-supervised%20learning%0Aframework%20that%20performs%20data%20augmentation%20through%20graph%20perturbations%2C%20with%0Agrowing%20applications%20in%20the%20analysis%20of%20biological%20networks%20such%20as%20Gene%0ARegulatory%20Networks%20%28GRNs%29.%20The%20artificial%20perturbations%20commonly%20used%20in%20GCL%2C%0Asuch%20as%20node%20dropping%2C%20induce%20structural%20changes%20that%20can%20diverge%20from%0Abiological%20reality.%20This%20concern%20has%20contributed%20to%20a%20broader%20trend%20in%20graph%0Arepresentation%20learning%20toward%20augmentation-free%20methods%2C%20which%20view%20such%0Astructural%20changes%20as%20problematic%20and%20to%20be%20avoided.%20However%2C%20this%20trend%0Aoverlooks%20the%20fundamental%20insight%20that%20structural%20changes%20from%20biologically%0Ameaningful%20perturbations%20are%20not%20a%20problem%20to%20be%20avoided%20but%20a%20rich%20source%20of%0Ainformation%2C%20thereby%20ignoring%20the%20valuable%20opportunity%20to%20leverage%20data%20from%0Areal%20biological%20experiments.%20Motivated%20by%20this%20insight%2C%20we%20propose%20SupGCL%0A%28Supervised%20Graph%20Contrastive%20Learning%29%2C%20a%20new%20GCL%20method%20for%20GRNs%20that%0Adirectly%20incorporates%20biological%20perturbations%20from%20gene%20knockdown%20experiments%0Aas%20supervision.%20SupGCL%20is%20a%20probabilistic%20formulation%20that%20continuously%0Ageneralizes%20conventional%20GCL%2C%20linking%20artificial%20augmentations%20with%20real%0Aperturbations%20measured%20in%20knockdown%20experiments%20and%20using%20the%20latter%20as%0Aexplicit%20supervisory%20signals.%20To%20assess%20effectiveness%2C%20we%20train%20GRN%0Arepresentations%20with%20SupGCL%20and%20evaluate%20their%20performance%20on%20downstream%20tasks.%0AThe%20evaluation%20includes%20both%20node-level%20tasks%2C%20such%20as%20gene%20function%0Aclassification%2C%20and%20graph-level%20tasks%20on%20patient-specific%20GRNs%2C%20such%20as%20patient%0Asurvival%20hazard%20prediction.%20Across%2013%20tasks%20built%20from%20GRN%20datasets%20derived%0Afrom%20patients%20with%20three%20cancer%20types%2C%20SupGCL%20consistently%20outperforms%0Astate-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17786v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Graph%2520Contrastive%2520Learning%2520for%2520Gene%2520Regulatory%2520Networks%26entry.906535625%3DSho%2520Oshima%2520and%2520Yuji%2520Okamoto%2520and%2520Taisei%2520Tosaki%2520and%2520Ryosuke%2520Kojima%2520and%2520Yasushi%2520Okuno%26entry.1292438233%3D%2520%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520is%2520a%2520powerful%2520self-supervised%2520learning%250Aframework%2520that%2520performs%2520data%2520augmentation%2520through%2520graph%2520perturbations%252C%2520with%250Agrowing%2520applications%2520in%2520the%2520analysis%2520of%2520biological%2520networks%2520such%2520as%2520Gene%250ARegulatory%2520Networks%2520%2528GRNs%2529.%2520The%2520artificial%2520perturbations%2520commonly%2520used%2520in%2520GCL%252C%250Asuch%2520as%2520node%2520dropping%252C%2520induce%2520structural%2520changes%2520that%2520can%2520diverge%2520from%250Abiological%2520reality.%2520This%2520concern%2520has%2520contributed%2520to%2520a%2520broader%2520trend%2520in%2520graph%250Arepresentation%2520learning%2520toward%2520augmentation-free%2520methods%252C%2520which%2520view%2520such%250Astructural%2520changes%2520as%2520problematic%2520and%2520to%2520be%2520avoided.%2520However%252C%2520this%2520trend%250Aoverlooks%2520the%2520fundamental%2520insight%2520that%2520structural%2520changes%2520from%2520biologically%250Ameaningful%2520perturbations%2520are%2520not%2520a%2520problem%2520to%2520be%2520avoided%2520but%2520a%2520rich%2520source%2520of%250Ainformation%252C%2520thereby%2520ignoring%2520the%2520valuable%2520opportunity%2520to%2520leverage%2520data%2520from%250Areal%2520biological%2520experiments.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520SupGCL%250A%2528Supervised%2520Graph%2520Contrastive%2520Learning%2529%252C%2520a%2520new%2520GCL%2520method%2520for%2520GRNs%2520that%250Adirectly%2520incorporates%2520biological%2520perturbations%2520from%2520gene%2520knockdown%2520experiments%250Aas%2520supervision.%2520SupGCL%2520is%2520a%2520probabilistic%2520formulation%2520that%2520continuously%250Ageneralizes%2520conventional%2520GCL%252C%2520linking%2520artificial%2520augmentations%2520with%2520real%250Aperturbations%2520measured%2520in%2520knockdown%2520experiments%2520and%2520using%2520the%2520latter%2520as%250Aexplicit%2520supervisory%2520signals.%2520To%2520assess%2520effectiveness%252C%2520we%2520train%2520GRN%250Arepresentations%2520with%2520SupGCL%2520and%2520evaluate%2520their%2520performance%2520on%2520downstream%2520tasks.%250AThe%2520evaluation%2520includes%2520both%2520node-level%2520tasks%252C%2520such%2520as%2520gene%2520function%250Aclassification%252C%2520and%2520graph-level%2520tasks%2520on%2520patient-specific%2520GRNs%252C%2520such%2520as%2520patient%250Asurvival%2520hazard%2520prediction.%2520Across%252013%2520tasks%2520built%2520from%2520GRN%2520datasets%2520derived%250Afrom%2520patients%2520with%2520three%2520cancer%2520types%252C%2520SupGCL%2520consistently%2520outperforms%250Astate-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17786v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Graph%20Contrastive%20Learning%20for%20Gene%20Regulatory%20Networks&entry.906535625=Sho%20Oshima%20and%20Yuji%20Okamoto%20and%20Taisei%20Tosaki%20and%20Ryosuke%20Kojima%20and%20Yasushi%20Okuno&entry.1292438233=%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20is%20a%20powerful%20self-supervised%20learning%0Aframework%20that%20performs%20data%20augmentation%20through%20graph%20perturbations%2C%20with%0Agrowing%20applications%20in%20the%20analysis%20of%20biological%20networks%20such%20as%20Gene%0ARegulatory%20Networks%20%28GRNs%29.%20The%20artificial%20perturbations%20commonly%20used%20in%20GCL%2C%0Asuch%20as%20node%20dropping%2C%20induce%20structural%20changes%20that%20can%20diverge%20from%0Abiological%20reality.%20This%20concern%20has%20contributed%20to%20a%20broader%20trend%20in%20graph%0Arepresentation%20learning%20toward%20augmentation-free%20methods%2C%20which%20view%20such%0Astructural%20changes%20as%20problematic%20and%20to%20be%20avoided.%20However%2C%20this%20trend%0Aoverlooks%20the%20fundamental%20insight%20that%20structural%20changes%20from%20biologically%0Ameaningful%20perturbations%20are%20not%20a%20problem%20to%20be%20avoided%20but%20a%20rich%20source%20of%0Ainformation%2C%20thereby%20ignoring%20the%20valuable%20opportunity%20to%20leverage%20data%20from%0Areal%20biological%20experiments.%20Motivated%20by%20this%20insight%2C%20we%20propose%20SupGCL%0A%28Supervised%20Graph%20Contrastive%20Learning%29%2C%20a%20new%20GCL%20method%20for%20GRNs%20that%0Adirectly%20incorporates%20biological%20perturbations%20from%20gene%20knockdown%20experiments%0Aas%20supervision.%20SupGCL%20is%20a%20probabilistic%20formulation%20that%20continuously%0Ageneralizes%20conventional%20GCL%2C%20linking%20artificial%20augmentations%20with%20real%0Aperturbations%20measured%20in%20knockdown%20experiments%20and%20using%20the%20latter%20as%0Aexplicit%20supervisory%20signals.%20To%20assess%20effectiveness%2C%20we%20train%20GRN%0Arepresentations%20with%20SupGCL%20and%20evaluate%20their%20performance%20on%20downstream%20tasks.%0AThe%20evaluation%20includes%20both%20node-level%20tasks%2C%20such%20as%20gene%20function%0Aclassification%2C%20and%20graph-level%20tasks%20on%20patient-specific%20GRNs%2C%20such%20as%20patient%0Asurvival%20hazard%20prediction.%20Across%2013%20tasks%20built%20from%20GRN%20datasets%20derived%0Afrom%20patients%20with%20three%20cancer%20types%2C%20SupGCL%20consistently%20outperforms%0Astate-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17786v4&entry.124074799=Read"},
{"title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix", "author": "Ahmet Caner Y\u00fcz\u00fcg\u00fcler and Ahmet \u00c7elik and Jiawei Zhuang and Lukas Cavigelli", "abstract": "  Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size.\n", "link": "http://arxiv.org/abs/2509.21081v1", "date": "2025-09-25", "relevancy": 2.4846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4852}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix&body=Title%3A%20TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix%0AAuthor%3A%20Ahmet%20Caner%20Y%C3%BCz%C3%BCg%C3%BCler%20and%20Ahmet%20%C3%87elik%20and%20Jiawei%20Zhuang%20and%20Lukas%20Cavigelli%0AAbstract%3A%20%20%20Multi-Head%20Latent%20Attention%20%28MLA%29%20is%20a%20recent%20attention%20mechanism%20adopted%20in%0Astate-of-the-art%20LLMs%20such%20as%20DeepSeek-v3%20and%20Kimi%20K2.%20Thanks%20to%20its%20novel%0Aformulation%2C%20MLA%20allows%20two%20functionally%20equivalent%20but%20computationally%0Adistinct%20kernel%20implementations%3A%20naive%20and%20absorb.%20While%20the%20naive%20kernels%0A%28e.g.%2C%20FlashAttention%29%20are%20typically%20preferred%20in%20training%20and%20prefill%20for%0Atheir%20computational%20efficiency%2C%20existing%20decoding%20kernels%20%28e.g.%2C%20FlashMLA%29%20rely%0Aon%20the%20absorb%20method%20to%20minimize%20HBM%20bandwidth%20usage.%20However%2C%20the%0Acompute-bound%20nature%20of%20the%20absorb%20implementations%20prohibits%20performance%0Abenefits%20from%20data%20reuse%20opportunities%20in%20attention%20calculations%2C%20such%20as%0Ashared%20prefixes.%20In%20this%20work%2C%20we%20introduce%20TyphoonMLA%2C%20a%20hybrid%20approach%20that%0Acombines%20naive%20and%20absorb%20formulations%20to%20harness%20the%20strengths%20of%20both.%0ATyphoonMLA%20effectively%20leverages%20the%20shared%20prefix%20by%20applying%20the%20naive%0Aformulation%20to%20the%20compute-bound%20parts%20of%20attention%20calculations%2C%20while%0Areducing%20the%20bandwidth%20requirements%20for%20non-shared%20parts%20by%20using%20the%20absorb%0Aformulation.%20As%20a%20result%2C%20TyphoonMLA%20improves%20the%20throughput%20of%20attention%0Acalculations%20in%20MLA%20architectures%20by%20up%20to%203x%20and%203.24x%20on%20NPU%20and%20GPUs%2C%20with%0Aonly%20a%203%25%20overhead%20in%20HBM%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTyphoonMLA%253A%2520A%2520Mixed%2520Naive-Absorb%2520MLA%2520Kernel%2520For%2520Shared%2520Prefix%26entry.906535625%3DAhmet%2520Caner%2520Y%25C3%25BCz%25C3%25BCg%25C3%25BCler%2520and%2520Ahmet%2520%25C3%2587elik%2520and%2520Jiawei%2520Zhuang%2520and%2520Lukas%2520Cavigelli%26entry.1292438233%3D%2520%2520Multi-Head%2520Latent%2520Attention%2520%2528MLA%2529%2520is%2520a%2520recent%2520attention%2520mechanism%2520adopted%2520in%250Astate-of-the-art%2520LLMs%2520such%2520as%2520DeepSeek-v3%2520and%2520Kimi%2520K2.%2520Thanks%2520to%2520its%2520novel%250Aformulation%252C%2520MLA%2520allows%2520two%2520functionally%2520equivalent%2520but%2520computationally%250Adistinct%2520kernel%2520implementations%253A%2520naive%2520and%2520absorb.%2520While%2520the%2520naive%2520kernels%250A%2528e.g.%252C%2520FlashAttention%2529%2520are%2520typically%2520preferred%2520in%2520training%2520and%2520prefill%2520for%250Atheir%2520computational%2520efficiency%252C%2520existing%2520decoding%2520kernels%2520%2528e.g.%252C%2520FlashMLA%2529%2520rely%250Aon%2520the%2520absorb%2520method%2520to%2520minimize%2520HBM%2520bandwidth%2520usage.%2520However%252C%2520the%250Acompute-bound%2520nature%2520of%2520the%2520absorb%2520implementations%2520prohibits%2520performance%250Abenefits%2520from%2520data%2520reuse%2520opportunities%2520in%2520attention%2520calculations%252C%2520such%2520as%250Ashared%2520prefixes.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TyphoonMLA%252C%2520a%2520hybrid%2520approach%2520that%250Acombines%2520naive%2520and%2520absorb%2520formulations%2520to%2520harness%2520the%2520strengths%2520of%2520both.%250ATyphoonMLA%2520effectively%2520leverages%2520the%2520shared%2520prefix%2520by%2520applying%2520the%2520naive%250Aformulation%2520to%2520the%2520compute-bound%2520parts%2520of%2520attention%2520calculations%252C%2520while%250Areducing%2520the%2520bandwidth%2520requirements%2520for%2520non-shared%2520parts%2520by%2520using%2520the%2520absorb%250Aformulation.%2520As%2520a%2520result%252C%2520TyphoonMLA%2520improves%2520the%2520throughput%2520of%2520attention%250Acalculations%2520in%2520MLA%2520architectures%2520by%2520up%2520to%25203x%2520and%25203.24x%2520on%2520NPU%2520and%2520GPUs%252C%2520with%250Aonly%2520a%25203%2525%2520overhead%2520in%2520HBM%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix&entry.906535625=Ahmet%20Caner%20Y%C3%BCz%C3%BCg%C3%BCler%20and%20Ahmet%20%C3%87elik%20and%20Jiawei%20Zhuang%20and%20Lukas%20Cavigelli&entry.1292438233=%20%20Multi-Head%20Latent%20Attention%20%28MLA%29%20is%20a%20recent%20attention%20mechanism%20adopted%20in%0Astate-of-the-art%20LLMs%20such%20as%20DeepSeek-v3%20and%20Kimi%20K2.%20Thanks%20to%20its%20novel%0Aformulation%2C%20MLA%20allows%20two%20functionally%20equivalent%20but%20computationally%0Adistinct%20kernel%20implementations%3A%20naive%20and%20absorb.%20While%20the%20naive%20kernels%0A%28e.g.%2C%20FlashAttention%29%20are%20typically%20preferred%20in%20training%20and%20prefill%20for%0Atheir%20computational%20efficiency%2C%20existing%20decoding%20kernels%20%28e.g.%2C%20FlashMLA%29%20rely%0Aon%20the%20absorb%20method%20to%20minimize%20HBM%20bandwidth%20usage.%20However%2C%20the%0Acompute-bound%20nature%20of%20the%20absorb%20implementations%20prohibits%20performance%0Abenefits%20from%20data%20reuse%20opportunities%20in%20attention%20calculations%2C%20such%20as%0Ashared%20prefixes.%20In%20this%20work%2C%20we%20introduce%20TyphoonMLA%2C%20a%20hybrid%20approach%20that%0Acombines%20naive%20and%20absorb%20formulations%20to%20harness%20the%20strengths%20of%20both.%0ATyphoonMLA%20effectively%20leverages%20the%20shared%20prefix%20by%20applying%20the%20naive%0Aformulation%20to%20the%20compute-bound%20parts%20of%20attention%20calculations%2C%20while%0Areducing%20the%20bandwidth%20requirements%20for%20non-shared%20parts%20by%20using%20the%20absorb%0Aformulation.%20As%20a%20result%2C%20TyphoonMLA%20improves%20the%20throughput%20of%20attention%0Acalculations%20in%20MLA%20architectures%20by%20up%20to%203x%20and%203.24x%20on%20NPU%20and%20GPUs%2C%20with%0Aonly%20a%203%25%20overhead%20in%20HBM%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21081v1&entry.124074799=Read"},
{"title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under\n  Cognitive Load Theory", "author": "HaoYang Shang and Xuan Liu and Zi Liang and Jie Zhang and Haibo Hu and Song Guo", "abstract": "  Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings.\n", "link": "http://arxiv.org/abs/2506.06843v2", "date": "2025-09-25", "relevancy": 2.4738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20United%20Minds%20or%20Isolated%20Agents%3F%20Exploring%20Coordination%20of%20LLMs%20under%0A%20%20Cognitive%20Load%20Theory&body=Title%3A%20United%20Minds%20or%20Isolated%20Agents%3F%20Exploring%20Coordination%20of%20LLMs%20under%0A%20%20Cognitive%20Load%20Theory%0AAuthor%3A%20HaoYang%20Shang%20and%20Xuan%20Liu%20and%20Zi%20Liang%20and%20Jie%20Zhang%20and%20Haibo%20Hu%20and%20Song%20Guo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20a%20notable%20performance%20ceiling%20on%0Acomplex%2C%20multi-faceted%20tasks%2C%20as%20they%20often%20fail%20to%20integrate%20diverse%0Ainformation%20or%20adhere%20to%20multiple%20constraints.%20We%20posit%20that%20such%20limitation%0Aarises%20when%20the%20demands%20of%20a%20task%20exceed%20the%20LLM%27s%20effective%20cognitive%20load%0Acapacity.%20This%20interpretation%20draws%20a%20strong%20analogy%20to%20Cognitive%20Load%20Theory%0A%28CLT%29%20in%20cognitive%20science%2C%20which%20explains%20similar%20performance%20boundaries%20in%0Athe%20human%20mind%2C%20and%20is%20further%20supported%20by%20emerging%20evidence%20that%20reveals%20LLMs%0Ahave%20bounded%20working%20memory%20characteristics.%20Building%20upon%20this%20CLT-grounded%0Aunderstanding%2C%20we%20introduce%20CoThinker%2C%20a%20novel%20LLM-based%20multi-agent%20framework%0Adesigned%20to%20mitigate%20cognitive%20overload%20and%20enhance%20collaborative%0Aproblem-solving%20abilities.%20CoThinker%20operationalizes%20CLT%20principles%20by%0Adistributing%20intrinsic%20cognitive%20load%20through%20agent%20specialization%20and%20managing%0Atransactional%20load%20via%20structured%20communication%20and%20a%20collective%20working%0Amemory.%20We%20empirically%20validate%20CoThinker%20on%20complex%20problem-solving%20tasks%20and%0Afabricated%20high%20cognitive%20load%20scenarios%2C%20demonstrating%20improvements%20over%0Aexisting%20multi-agent%20baselines%20in%20solution%20quality%20and%20efficiency.%20Our%20analysis%0Areveals%20characteristic%20interaction%20patterns%2C%20providing%20insights%20into%20the%0Aemergence%20of%20collective%20cognition%20and%20effective%20load%20management%2C%20thus%20offering%0Aa%20principled%20approach%20to%20overcoming%20LLM%20performance%20ceilings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnited%2520Minds%2520or%2520Isolated%2520Agents%253F%2520Exploring%2520Coordination%2520of%2520LLMs%2520under%250A%2520%2520Cognitive%2520Load%2520Theory%26entry.906535625%3DHaoYang%2520Shang%2520and%2520Xuan%2520Liu%2520and%2520Zi%2520Liang%2520and%2520Jie%2520Zhang%2520and%2520Haibo%2520Hu%2520and%2520Song%2520Guo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520a%2520notable%2520performance%2520ceiling%2520on%250Acomplex%252C%2520multi-faceted%2520tasks%252C%2520as%2520they%2520often%2520fail%2520to%2520integrate%2520diverse%250Ainformation%2520or%2520adhere%2520to%2520multiple%2520constraints.%2520We%2520posit%2520that%2520such%2520limitation%250Aarises%2520when%2520the%2520demands%2520of%2520a%2520task%2520exceed%2520the%2520LLM%2527s%2520effective%2520cognitive%2520load%250Acapacity.%2520This%2520interpretation%2520draws%2520a%2520strong%2520analogy%2520to%2520Cognitive%2520Load%2520Theory%250A%2528CLT%2529%2520in%2520cognitive%2520science%252C%2520which%2520explains%2520similar%2520performance%2520boundaries%2520in%250Athe%2520human%2520mind%252C%2520and%2520is%2520further%2520supported%2520by%2520emerging%2520evidence%2520that%2520reveals%2520LLMs%250Ahave%2520bounded%2520working%2520memory%2520characteristics.%2520Building%2520upon%2520this%2520CLT-grounded%250Aunderstanding%252C%2520we%2520introduce%2520CoThinker%252C%2520a%2520novel%2520LLM-based%2520multi-agent%2520framework%250Adesigned%2520to%2520mitigate%2520cognitive%2520overload%2520and%2520enhance%2520collaborative%250Aproblem-solving%2520abilities.%2520CoThinker%2520operationalizes%2520CLT%2520principles%2520by%250Adistributing%2520intrinsic%2520cognitive%2520load%2520through%2520agent%2520specialization%2520and%2520managing%250Atransactional%2520load%2520via%2520structured%2520communication%2520and%2520a%2520collective%2520working%250Amemory.%2520We%2520empirically%2520validate%2520CoThinker%2520on%2520complex%2520problem-solving%2520tasks%2520and%250Afabricated%2520high%2520cognitive%2520load%2520scenarios%252C%2520demonstrating%2520improvements%2520over%250Aexisting%2520multi-agent%2520baselines%2520in%2520solution%2520quality%2520and%2520efficiency.%2520Our%2520analysis%250Areveals%2520characteristic%2520interaction%2520patterns%252C%2520providing%2520insights%2520into%2520the%250Aemergence%2520of%2520collective%2520cognition%2520and%2520effective%2520load%2520management%252C%2520thus%2520offering%250Aa%2520principled%2520approach%2520to%2520overcoming%2520LLM%2520performance%2520ceilings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=United%20Minds%20or%20Isolated%20Agents%3F%20Exploring%20Coordination%20of%20LLMs%20under%0A%20%20Cognitive%20Load%20Theory&entry.906535625=HaoYang%20Shang%20and%20Xuan%20Liu%20and%20Zi%20Liang%20and%20Jie%20Zhang%20and%20Haibo%20Hu%20and%20Song%20Guo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20a%20notable%20performance%20ceiling%20on%0Acomplex%2C%20multi-faceted%20tasks%2C%20as%20they%20often%20fail%20to%20integrate%20diverse%0Ainformation%20or%20adhere%20to%20multiple%20constraints.%20We%20posit%20that%20such%20limitation%0Aarises%20when%20the%20demands%20of%20a%20task%20exceed%20the%20LLM%27s%20effective%20cognitive%20load%0Acapacity.%20This%20interpretation%20draws%20a%20strong%20analogy%20to%20Cognitive%20Load%20Theory%0A%28CLT%29%20in%20cognitive%20science%2C%20which%20explains%20similar%20performance%20boundaries%20in%0Athe%20human%20mind%2C%20and%20is%20further%20supported%20by%20emerging%20evidence%20that%20reveals%20LLMs%0Ahave%20bounded%20working%20memory%20characteristics.%20Building%20upon%20this%20CLT-grounded%0Aunderstanding%2C%20we%20introduce%20CoThinker%2C%20a%20novel%20LLM-based%20multi-agent%20framework%0Adesigned%20to%20mitigate%20cognitive%20overload%20and%20enhance%20collaborative%0Aproblem-solving%20abilities.%20CoThinker%20operationalizes%20CLT%20principles%20by%0Adistributing%20intrinsic%20cognitive%20load%20through%20agent%20specialization%20and%20managing%0Atransactional%20load%20via%20structured%20communication%20and%20a%20collective%20working%0Amemory.%20We%20empirically%20validate%20CoThinker%20on%20complex%20problem-solving%20tasks%20and%0Afabricated%20high%20cognitive%20load%20scenarios%2C%20demonstrating%20improvements%20over%0Aexisting%20multi-agent%20baselines%20in%20solution%20quality%20and%20efficiency.%20Our%20analysis%0Areveals%20characteristic%20interaction%20patterns%2C%20providing%20insights%20into%20the%0Aemergence%20of%20collective%20cognition%20and%20effective%20load%20management%2C%20thus%20offering%0Aa%20principled%20approach%20to%20overcoming%20LLM%20performance%20ceilings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06843v2&entry.124074799=Read"},
{"title": "Structure-Attribute Transformations with Markov Chain Boost Graph Domain\n  Adaptation", "author": "Zhen Liu and Yongtao Zhang and Shaobo Ren and Yuxin You", "abstract": "  Graph domain adaptation has gained significant attention in label-scarce\nscenarios across different graph domains. Traditional approaches to graph\ndomain adaptation primarily focus on transforming node attributes over raw\ngraph structures and aligning the distributions of the transformed node\nfeatures across networks. However, these methods often struggle with the\nunderlying structural heterogeneity between distinct graph domains, which leads\nto suboptimal distribution alignment. To address this limitation, we propose\nStructure-Attribute Transformation with Markov Chain (SATMC), a novel framework\nthat sequentially aligns distributions across networks via both graph structure\nand attribute transformations. To mitigate the negative influence of\ndomain-private information and further enhance the model's generalization,\nSATMC introduces a private domain information reduction mechanism and an\nempirical Wasserstein distance. Theoretical proofs suggest that SATMC can\nachieve a tighter error bound for cross-network node classification compared to\nexisting graph domain adaptation methods. Extensive experiments on nine pairs\nof publicly available cross-domain datasets show that SATMC outperforms\nstate-of-the-art methods in the cross-network node classification task. The\ncode is available at https://github.com/GiantZhangYT/SATMC.\n", "link": "http://arxiv.org/abs/2509.21059v1", "date": "2025-09-25", "relevancy": 2.4646, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5296}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.479}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Attribute%20Transformations%20with%20Markov%20Chain%20Boost%20Graph%20Domain%0A%20%20Adaptation&body=Title%3A%20Structure-Attribute%20Transformations%20with%20Markov%20Chain%20Boost%20Graph%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Zhen%20Liu%20and%20Yongtao%20Zhang%20and%20Shaobo%20Ren%20and%20Yuxin%20You%0AAbstract%3A%20%20%20Graph%20domain%20adaptation%20has%20gained%20significant%20attention%20in%20label-scarce%0Ascenarios%20across%20different%20graph%20domains.%20Traditional%20approaches%20to%20graph%0Adomain%20adaptation%20primarily%20focus%20on%20transforming%20node%20attributes%20over%20raw%0Agraph%20structures%20and%20aligning%20the%20distributions%20of%20the%20transformed%20node%0Afeatures%20across%20networks.%20However%2C%20these%20methods%20often%20struggle%20with%20the%0Aunderlying%20structural%20heterogeneity%20between%20distinct%20graph%20domains%2C%20which%20leads%0Ato%20suboptimal%20distribution%20alignment.%20To%20address%20this%20limitation%2C%20we%20propose%0AStructure-Attribute%20Transformation%20with%20Markov%20Chain%20%28SATMC%29%2C%20a%20novel%20framework%0Athat%20sequentially%20aligns%20distributions%20across%20networks%20via%20both%20graph%20structure%0Aand%20attribute%20transformations.%20To%20mitigate%20the%20negative%20influence%20of%0Adomain-private%20information%20and%20further%20enhance%20the%20model%27s%20generalization%2C%0ASATMC%20introduces%20a%20private%20domain%20information%20reduction%20mechanism%20and%20an%0Aempirical%20Wasserstein%20distance.%20Theoretical%20proofs%20suggest%20that%20SATMC%20can%0Aachieve%20a%20tighter%20error%20bound%20for%20cross-network%20node%20classification%20compared%20to%0Aexisting%20graph%20domain%20adaptation%20methods.%20Extensive%20experiments%20on%20nine%20pairs%0Aof%20publicly%20available%20cross-domain%20datasets%20show%20that%20SATMC%20outperforms%0Astate-of-the-art%20methods%20in%20the%20cross-network%20node%20classification%20task.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GiantZhangYT/SATMC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Attribute%2520Transformations%2520with%2520Markov%2520Chain%2520Boost%2520Graph%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DZhen%2520Liu%2520and%2520Yongtao%2520Zhang%2520and%2520Shaobo%2520Ren%2520and%2520Yuxin%2520You%26entry.1292438233%3D%2520%2520Graph%2520domain%2520adaptation%2520has%2520gained%2520significant%2520attention%2520in%2520label-scarce%250Ascenarios%2520across%2520different%2520graph%2520domains.%2520Traditional%2520approaches%2520to%2520graph%250Adomain%2520adaptation%2520primarily%2520focus%2520on%2520transforming%2520node%2520attributes%2520over%2520raw%250Agraph%2520structures%2520and%2520aligning%2520the%2520distributions%2520of%2520the%2520transformed%2520node%250Afeatures%2520across%2520networks.%2520However%252C%2520these%2520methods%2520often%2520struggle%2520with%2520the%250Aunderlying%2520structural%2520heterogeneity%2520between%2520distinct%2520graph%2520domains%252C%2520which%2520leads%250Ato%2520suboptimal%2520distribution%2520alignment.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AStructure-Attribute%2520Transformation%2520with%2520Markov%2520Chain%2520%2528SATMC%2529%252C%2520a%2520novel%2520framework%250Athat%2520sequentially%2520aligns%2520distributions%2520across%2520networks%2520via%2520both%2520graph%2520structure%250Aand%2520attribute%2520transformations.%2520To%2520mitigate%2520the%2520negative%2520influence%2520of%250Adomain-private%2520information%2520and%2520further%2520enhance%2520the%2520model%2527s%2520generalization%252C%250ASATMC%2520introduces%2520a%2520private%2520domain%2520information%2520reduction%2520mechanism%2520and%2520an%250Aempirical%2520Wasserstein%2520distance.%2520Theoretical%2520proofs%2520suggest%2520that%2520SATMC%2520can%250Aachieve%2520a%2520tighter%2520error%2520bound%2520for%2520cross-network%2520node%2520classification%2520compared%2520to%250Aexisting%2520graph%2520domain%2520adaptation%2520methods.%2520Extensive%2520experiments%2520on%2520nine%2520pairs%250Aof%2520publicly%2520available%2520cross-domain%2520datasets%2520show%2520that%2520SATMC%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520the%2520cross-network%2520node%2520classification%2520task.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/GiantZhangYT/SATMC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Attribute%20Transformations%20with%20Markov%20Chain%20Boost%20Graph%20Domain%0A%20%20Adaptation&entry.906535625=Zhen%20Liu%20and%20Yongtao%20Zhang%20and%20Shaobo%20Ren%20and%20Yuxin%20You&entry.1292438233=%20%20Graph%20domain%20adaptation%20has%20gained%20significant%20attention%20in%20label-scarce%0Ascenarios%20across%20different%20graph%20domains.%20Traditional%20approaches%20to%20graph%0Adomain%20adaptation%20primarily%20focus%20on%20transforming%20node%20attributes%20over%20raw%0Agraph%20structures%20and%20aligning%20the%20distributions%20of%20the%20transformed%20node%0Afeatures%20across%20networks.%20However%2C%20these%20methods%20often%20struggle%20with%20the%0Aunderlying%20structural%20heterogeneity%20between%20distinct%20graph%20domains%2C%20which%20leads%0Ato%20suboptimal%20distribution%20alignment.%20To%20address%20this%20limitation%2C%20we%20propose%0AStructure-Attribute%20Transformation%20with%20Markov%20Chain%20%28SATMC%29%2C%20a%20novel%20framework%0Athat%20sequentially%20aligns%20distributions%20across%20networks%20via%20both%20graph%20structure%0Aand%20attribute%20transformations.%20To%20mitigate%20the%20negative%20influence%20of%0Adomain-private%20information%20and%20further%20enhance%20the%20model%27s%20generalization%2C%0ASATMC%20introduces%20a%20private%20domain%20information%20reduction%20mechanism%20and%20an%0Aempirical%20Wasserstein%20distance.%20Theoretical%20proofs%20suggest%20that%20SATMC%20can%0Aachieve%20a%20tighter%20error%20bound%20for%20cross-network%20node%20classification%20compared%20to%0Aexisting%20graph%20domain%20adaptation%20methods.%20Extensive%20experiments%20on%20nine%20pairs%0Aof%20publicly%20available%20cross-domain%20datasets%20show%20that%20SATMC%20outperforms%0Astate-of-the-art%20methods%20in%20the%20cross-network%20node%20classification%20task.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GiantZhangYT/SATMC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21059v1&entry.124074799=Read"},
{"title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D\n  Assets", "author": "Team Hunyuan3D and  : and Bowen Zhang and Chunchao Guo and Haolin Liu and Hongyu Yan and Huiwen Shi and Jingwei Huang and Junlin Yu and Kunhong Li and  Linus and Penghao Wang and Qingxiang Lin and Sicong Liu and Xianghui Yang and Yixuan Tang and Yunfei Zhao and Zeqiang Lai and Zhihao Liang and Zibo Zhao", "abstract": "  Recent advances in 3D-native generative models have accelerated asset\ncreation for games, film, and design. However, most methods still rely\nprimarily on image or text conditioning and lack fine-grained, cross-modal\ncontrols, which limits controllability and practical adoption. To address this\ngap, we present Hunyuan3D-Omni, a unified framework for fine-grained,\ncontrollable 3D asset generation built on Hunyuan3D 2.1. In addition to images,\nHunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose\npriors as conditioning signals, enabling precise control over geometry,\ntopology, and pose. Instead of separate heads for each modality, our model\nunifies all signals in a single cross-modal architecture. We train with a\nprogressive, difficulty-aware sampling strategy that selects one control\nmodality per example and biases sampling toward harder signals (e.g., skeletal\npose) while downweighting easier ones (e.g., point clouds), encouraging robust\nmulti-modal fusion and graceful handling of missing inputs. Experiments show\nthat these additional controls improve generation accuracy, enable\ngeometry-aware transformations, and increase robustness for production\nworkflows.\n", "link": "http://arxiv.org/abs/2509.21245v1", "date": "2025-09-25", "relevancy": 2.4502, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6232}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6104}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan3D-Omni%3A%20A%20Unified%20Framework%20for%20Controllable%20Generation%20of%203D%0A%20%20Assets&body=Title%3A%20Hunyuan3D-Omni%3A%20A%20Unified%20Framework%20for%20Controllable%20Generation%20of%203D%0A%20%20Assets%0AAuthor%3A%20Team%20Hunyuan3D%20and%20%20%3A%20and%20Bowen%20Zhang%20and%20Chunchao%20Guo%20and%20Haolin%20Liu%20and%20Hongyu%20Yan%20and%20Huiwen%20Shi%20and%20Jingwei%20Huang%20and%20Junlin%20Yu%20and%20Kunhong%20Li%20and%20%20Linus%20and%20Penghao%20Wang%20and%20Qingxiang%20Lin%20and%20Sicong%20Liu%20and%20Xianghui%20Yang%20and%20Yixuan%20Tang%20and%20Yunfei%20Zhao%20and%20Zeqiang%20Lai%20and%20Zhihao%20Liang%20and%20Zibo%20Zhao%0AAbstract%3A%20%20%20Recent%20advances%20in%203D-native%20generative%20models%20have%20accelerated%20asset%0Acreation%20for%20games%2C%20film%2C%20and%20design.%20However%2C%20most%20methods%20still%20rely%0Aprimarily%20on%20image%20or%20text%20conditioning%20and%20lack%20fine-grained%2C%20cross-modal%0Acontrols%2C%20which%20limits%20controllability%20and%20practical%20adoption.%20To%20address%20this%0Agap%2C%20we%20present%20Hunyuan3D-Omni%2C%20a%20unified%20framework%20for%20fine-grained%2C%0Acontrollable%203D%20asset%20generation%20built%20on%20Hunyuan3D%202.1.%20In%20addition%20to%20images%2C%0AHunyuan3D-Omni%20accepts%20point%20clouds%2C%20voxels%2C%20bounding%20boxes%2C%20and%20skeletal%20pose%0Apriors%20as%20conditioning%20signals%2C%20enabling%20precise%20control%20over%20geometry%2C%0Atopology%2C%20and%20pose.%20Instead%20of%20separate%20heads%20for%20each%20modality%2C%20our%20model%0Aunifies%20all%20signals%20in%20a%20single%20cross-modal%20architecture.%20We%20train%20with%20a%0Aprogressive%2C%20difficulty-aware%20sampling%20strategy%20that%20selects%20one%20control%0Amodality%20per%20example%20and%20biases%20sampling%20toward%20harder%20signals%20%28e.g.%2C%20skeletal%0Apose%29%20while%20downweighting%20easier%20ones%20%28e.g.%2C%20point%20clouds%29%2C%20encouraging%20robust%0Amulti-modal%20fusion%20and%20graceful%20handling%20of%20missing%20inputs.%20Experiments%20show%0Athat%20these%20additional%20controls%20improve%20generation%20accuracy%2C%20enable%0Ageometry-aware%20transformations%2C%20and%20increase%20robustness%20for%20production%0Aworkflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan3D-Omni%253A%2520A%2520Unified%2520Framework%2520for%2520Controllable%2520Generation%2520of%25203D%250A%2520%2520Assets%26entry.906535625%3DTeam%2520Hunyuan3D%2520and%2520%2520%253A%2520and%2520Bowen%2520Zhang%2520and%2520Chunchao%2520Guo%2520and%2520Haolin%2520Liu%2520and%2520Hongyu%2520Yan%2520and%2520Huiwen%2520Shi%2520and%2520Jingwei%2520Huang%2520and%2520Junlin%2520Yu%2520and%2520Kunhong%2520Li%2520and%2520%2520Linus%2520and%2520Penghao%2520Wang%2520and%2520Qingxiang%2520Lin%2520and%2520Sicong%2520Liu%2520and%2520Xianghui%2520Yang%2520and%2520Yixuan%2520Tang%2520and%2520Yunfei%2520Zhao%2520and%2520Zeqiang%2520Lai%2520and%2520Zhihao%2520Liang%2520and%2520Zibo%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D-native%2520generative%2520models%2520have%2520accelerated%2520asset%250Acreation%2520for%2520games%252C%2520film%252C%2520and%2520design.%2520However%252C%2520most%2520methods%2520still%2520rely%250Aprimarily%2520on%2520image%2520or%2520text%2520conditioning%2520and%2520lack%2520fine-grained%252C%2520cross-modal%250Acontrols%252C%2520which%2520limits%2520controllability%2520and%2520practical%2520adoption.%2520To%2520address%2520this%250Agap%252C%2520we%2520present%2520Hunyuan3D-Omni%252C%2520a%2520unified%2520framework%2520for%2520fine-grained%252C%250Acontrollable%25203D%2520asset%2520generation%2520built%2520on%2520Hunyuan3D%25202.1.%2520In%2520addition%2520to%2520images%252C%250AHunyuan3D-Omni%2520accepts%2520point%2520clouds%252C%2520voxels%252C%2520bounding%2520boxes%252C%2520and%2520skeletal%2520pose%250Apriors%2520as%2520conditioning%2520signals%252C%2520enabling%2520precise%2520control%2520over%2520geometry%252C%250Atopology%252C%2520and%2520pose.%2520Instead%2520of%2520separate%2520heads%2520for%2520each%2520modality%252C%2520our%2520model%250Aunifies%2520all%2520signals%2520in%2520a%2520single%2520cross-modal%2520architecture.%2520We%2520train%2520with%2520a%250Aprogressive%252C%2520difficulty-aware%2520sampling%2520strategy%2520that%2520selects%2520one%2520control%250Amodality%2520per%2520example%2520and%2520biases%2520sampling%2520toward%2520harder%2520signals%2520%2528e.g.%252C%2520skeletal%250Apose%2529%2520while%2520downweighting%2520easier%2520ones%2520%2528e.g.%252C%2520point%2520clouds%2529%252C%2520encouraging%2520robust%250Amulti-modal%2520fusion%2520and%2520graceful%2520handling%2520of%2520missing%2520inputs.%2520Experiments%2520show%250Athat%2520these%2520additional%2520controls%2520improve%2520generation%2520accuracy%252C%2520enable%250Ageometry-aware%2520transformations%252C%2520and%2520increase%2520robustness%2520for%2520production%250Aworkflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan3D-Omni%3A%20A%20Unified%20Framework%20for%20Controllable%20Generation%20of%203D%0A%20%20Assets&entry.906535625=Team%20Hunyuan3D%20and%20%20%3A%20and%20Bowen%20Zhang%20and%20Chunchao%20Guo%20and%20Haolin%20Liu%20and%20Hongyu%20Yan%20and%20Huiwen%20Shi%20and%20Jingwei%20Huang%20and%20Junlin%20Yu%20and%20Kunhong%20Li%20and%20%20Linus%20and%20Penghao%20Wang%20and%20Qingxiang%20Lin%20and%20Sicong%20Liu%20and%20Xianghui%20Yang%20and%20Yixuan%20Tang%20and%20Yunfei%20Zhao%20and%20Zeqiang%20Lai%20and%20Zhihao%20Liang%20and%20Zibo%20Zhao&entry.1292438233=%20%20Recent%20advances%20in%203D-native%20generative%20models%20have%20accelerated%20asset%0Acreation%20for%20games%2C%20film%2C%20and%20design.%20However%2C%20most%20methods%20still%20rely%0Aprimarily%20on%20image%20or%20text%20conditioning%20and%20lack%20fine-grained%2C%20cross-modal%0Acontrols%2C%20which%20limits%20controllability%20and%20practical%20adoption.%20To%20address%20this%0Agap%2C%20we%20present%20Hunyuan3D-Omni%2C%20a%20unified%20framework%20for%20fine-grained%2C%0Acontrollable%203D%20asset%20generation%20built%20on%20Hunyuan3D%202.1.%20In%20addition%20to%20images%2C%0AHunyuan3D-Omni%20accepts%20point%20clouds%2C%20voxels%2C%20bounding%20boxes%2C%20and%20skeletal%20pose%0Apriors%20as%20conditioning%20signals%2C%20enabling%20precise%20control%20over%20geometry%2C%0Atopology%2C%20and%20pose.%20Instead%20of%20separate%20heads%20for%20each%20modality%2C%20our%20model%0Aunifies%20all%20signals%20in%20a%20single%20cross-modal%20architecture.%20We%20train%20with%20a%0Aprogressive%2C%20difficulty-aware%20sampling%20strategy%20that%20selects%20one%20control%0Amodality%20per%20example%20and%20biases%20sampling%20toward%20harder%20signals%20%28e.g.%2C%20skeletal%0Apose%29%20while%20downweighting%20easier%20ones%20%28e.g.%2C%20point%20clouds%29%2C%20encouraging%20robust%0Amulti-modal%20fusion%20and%20graceful%20handling%20of%20missing%20inputs.%20Experiments%20show%0Athat%20these%20additional%20controls%20improve%20generation%20accuracy%2C%20enable%0Ageometry-aware%20transformations%2C%20and%20increase%20robustness%20for%20production%0Aworkflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21245v1&entry.124074799=Read"},
{"title": "Quantized Visual Geometry Grounded Transformer", "author": "Weilun Feng and Haotong Qin and Mingqiang Wu and Chuanguang Yang and Yuqi Li and Xiangqi Li and Zhulin An and Libo Huang and Yulun Zhang and Michele Magno and Yongjun Xu", "abstract": "  Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT.\n", "link": "http://arxiv.org/abs/2509.21302v1", "date": "2025-09-25", "relevancy": 2.4237, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6246}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6103}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantized%20Visual%20Geometry%20Grounded%20Transformer&body=Title%3A%20Quantized%20Visual%20Geometry%20Grounded%20Transformer%0AAuthor%3A%20Weilun%20Feng%20and%20Haotong%20Qin%20and%20Mingqiang%20Wu%20and%20Chuanguang%20Yang%20and%20Yuqi%20Li%20and%20Xiangqi%20Li%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Yulun%20Zhang%20and%20Michele%20Magno%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Learning-based%203D%20reconstruction%20models%2C%20represented%20by%20Visual%20Geometry%0AGrounded%20Transformers%20%28VGGTs%29%2C%20have%20made%20remarkable%20progress%20with%20the%20use%20of%0Alarge-scale%20transformers.%20Their%20prohibitive%20computational%20and%20memory%20costs%0Aseverely%20hinder%20real-world%20deployment.%20Post-Training%20Quantization%20%28PTQ%29%20has%0Abecome%20a%20common%20practice%20for%20compressing%20and%20accelerating%20models.%20However%2C%20we%0Aempirically%20observe%20that%20PTQ%20faces%20unique%20obstacles%20when%20compressing%0Abillion-scale%20VGGTs%3A%20the%20data-independent%20special%20tokens%20induce%20heavy-tailed%0Aactivation%20distributions%2C%20while%20the%20multi-view%20nature%20of%203D%20data%20makes%0Acalibration%20sample%20selection%20highly%20unstable.%20This%20paper%20proposes%20the%20first%0AQuantization%20framework%20for%20VGGTs%2C%20namely%20QuantVGGT.%20This%20mainly%20relies%20on%20two%0Atechnical%20contributions%3A%20First%2C%20we%20introduce%20Dual-Smoothed%20Fine-Grained%0AQuantization%2C%20which%20integrates%20pre-global%20Hadamard%20rotation%20and%20post-local%0Achannel%20smoothing%20to%20mitigate%20heavy-tailed%20distributions%20and%20inter-channel%0Avariance%20robustly.%20Second%2C%20we%20design%20Noise-Filtered%20Diverse%20Sampling%2C%20which%0Afilters%20outliers%20via%20deep-layer%20statistics%20and%20constructs%20frame-aware%20diverse%0Acalibration%20clusters%20to%20ensure%20stable%20quantization%20ranges.%20Comprehensive%0Aexperiments%20demonstrate%20that%20QuantVGGT%20achieves%20the%20state-of-the-art%20results%0Aacross%20different%20benchmarks%20and%20bit-width%2C%20surpassing%20the%20previous%0Astate-of-the-art%20generic%20quantization%20method%20with%20a%20great%20margin.%20We%20highlight%0Athat%20our%204-bit%20QuantVGGT%20can%20deliver%20a%203.7%24%5Ctimes%24%20memory%20reduction%20and%0A2.5%24%5Ctimes%24%20acceleration%20in%20real-hardware%20inference%2C%20while%20maintaining%0Areconstruction%20accuracy%20above%2098%5C%25%20of%20its%20full-precision%20counterpart.%20This%0Ademonstrates%20the%20vast%20advantages%20and%20practicality%20of%20QuantVGGT%20in%0Aresource-constrained%20scenarios.%20Our%20code%20is%20released%20in%0Ahttps%3A//github.com/wlfeng0509/QuantVGGT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantized%2520Visual%2520Geometry%2520Grounded%2520Transformer%26entry.906535625%3DWeilun%2520Feng%2520and%2520Haotong%2520Qin%2520and%2520Mingqiang%2520Wu%2520and%2520Chuanguang%2520Yang%2520and%2520Yuqi%2520Li%2520and%2520Xiangqi%2520Li%2520and%2520Zhulin%2520An%2520and%2520Libo%2520Huang%2520and%2520Yulun%2520Zhang%2520and%2520Michele%2520Magno%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Learning-based%25203D%2520reconstruction%2520models%252C%2520represented%2520by%2520Visual%2520Geometry%250AGrounded%2520Transformers%2520%2528VGGTs%2529%252C%2520have%2520made%2520remarkable%2520progress%2520with%2520the%2520use%2520of%250Alarge-scale%2520transformers.%2520Their%2520prohibitive%2520computational%2520and%2520memory%2520costs%250Aseverely%2520hinder%2520real-world%2520deployment.%2520Post-Training%2520Quantization%2520%2528PTQ%2529%2520has%250Abecome%2520a%2520common%2520practice%2520for%2520compressing%2520and%2520accelerating%2520models.%2520However%252C%2520we%250Aempirically%2520observe%2520that%2520PTQ%2520faces%2520unique%2520obstacles%2520when%2520compressing%250Abillion-scale%2520VGGTs%253A%2520the%2520data-independent%2520special%2520tokens%2520induce%2520heavy-tailed%250Aactivation%2520distributions%252C%2520while%2520the%2520multi-view%2520nature%2520of%25203D%2520data%2520makes%250Acalibration%2520sample%2520selection%2520highly%2520unstable.%2520This%2520paper%2520proposes%2520the%2520first%250AQuantization%2520framework%2520for%2520VGGTs%252C%2520namely%2520QuantVGGT.%2520This%2520mainly%2520relies%2520on%2520two%250Atechnical%2520contributions%253A%2520First%252C%2520we%2520introduce%2520Dual-Smoothed%2520Fine-Grained%250AQuantization%252C%2520which%2520integrates%2520pre-global%2520Hadamard%2520rotation%2520and%2520post-local%250Achannel%2520smoothing%2520to%2520mitigate%2520heavy-tailed%2520distributions%2520and%2520inter-channel%250Avariance%2520robustly.%2520Second%252C%2520we%2520design%2520Noise-Filtered%2520Diverse%2520Sampling%252C%2520which%250Afilters%2520outliers%2520via%2520deep-layer%2520statistics%2520and%2520constructs%2520frame-aware%2520diverse%250Acalibration%2520clusters%2520to%2520ensure%2520stable%2520quantization%2520ranges.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520QuantVGGT%2520achieves%2520the%2520state-of-the-art%2520results%250Aacross%2520different%2520benchmarks%2520and%2520bit-width%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520generic%2520quantization%2520method%2520with%2520a%2520great%2520margin.%2520We%2520highlight%250Athat%2520our%25204-bit%2520QuantVGGT%2520can%2520deliver%2520a%25203.7%2524%255Ctimes%2524%2520memory%2520reduction%2520and%250A2.5%2524%255Ctimes%2524%2520acceleration%2520in%2520real-hardware%2520inference%252C%2520while%2520maintaining%250Areconstruction%2520accuracy%2520above%252098%255C%2525%2520of%2520its%2520full-precision%2520counterpart.%2520This%250Ademonstrates%2520the%2520vast%2520advantages%2520and%2520practicality%2520of%2520QuantVGGT%2520in%250Aresource-constrained%2520scenarios.%2520Our%2520code%2520is%2520released%2520in%250Ahttps%253A//github.com/wlfeng0509/QuantVGGT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantized%20Visual%20Geometry%20Grounded%20Transformer&entry.906535625=Weilun%20Feng%20and%20Haotong%20Qin%20and%20Mingqiang%20Wu%20and%20Chuanguang%20Yang%20and%20Yuqi%20Li%20and%20Xiangqi%20Li%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Yulun%20Zhang%20and%20Michele%20Magno%20and%20Yongjun%20Xu&entry.1292438233=%20%20Learning-based%203D%20reconstruction%20models%2C%20represented%20by%20Visual%20Geometry%0AGrounded%20Transformers%20%28VGGTs%29%2C%20have%20made%20remarkable%20progress%20with%20the%20use%20of%0Alarge-scale%20transformers.%20Their%20prohibitive%20computational%20and%20memory%20costs%0Aseverely%20hinder%20real-world%20deployment.%20Post-Training%20Quantization%20%28PTQ%29%20has%0Abecome%20a%20common%20practice%20for%20compressing%20and%20accelerating%20models.%20However%2C%20we%0Aempirically%20observe%20that%20PTQ%20faces%20unique%20obstacles%20when%20compressing%0Abillion-scale%20VGGTs%3A%20the%20data-independent%20special%20tokens%20induce%20heavy-tailed%0Aactivation%20distributions%2C%20while%20the%20multi-view%20nature%20of%203D%20data%20makes%0Acalibration%20sample%20selection%20highly%20unstable.%20This%20paper%20proposes%20the%20first%0AQuantization%20framework%20for%20VGGTs%2C%20namely%20QuantVGGT.%20This%20mainly%20relies%20on%20two%0Atechnical%20contributions%3A%20First%2C%20we%20introduce%20Dual-Smoothed%20Fine-Grained%0AQuantization%2C%20which%20integrates%20pre-global%20Hadamard%20rotation%20and%20post-local%0Achannel%20smoothing%20to%20mitigate%20heavy-tailed%20distributions%20and%20inter-channel%0Avariance%20robustly.%20Second%2C%20we%20design%20Noise-Filtered%20Diverse%20Sampling%2C%20which%0Afilters%20outliers%20via%20deep-layer%20statistics%20and%20constructs%20frame-aware%20diverse%0Acalibration%20clusters%20to%20ensure%20stable%20quantization%20ranges.%20Comprehensive%0Aexperiments%20demonstrate%20that%20QuantVGGT%20achieves%20the%20state-of-the-art%20results%0Aacross%20different%20benchmarks%20and%20bit-width%2C%20surpassing%20the%20previous%0Astate-of-the-art%20generic%20quantization%20method%20with%20a%20great%20margin.%20We%20highlight%0Athat%20our%204-bit%20QuantVGGT%20can%20deliver%20a%203.7%24%5Ctimes%24%20memory%20reduction%20and%0A2.5%24%5Ctimes%24%20acceleration%20in%20real-hardware%20inference%2C%20while%20maintaining%0Areconstruction%20accuracy%20above%2098%5C%25%20of%20its%20full-precision%20counterpart.%20This%0Ademonstrates%20the%20vast%20advantages%20and%20practicality%20of%20QuantVGGT%20in%0Aresource-constrained%20scenarios.%20Our%20code%20is%20released%20in%0Ahttps%3A//github.com/wlfeng0509/QuantVGGT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21302v1&entry.124074799=Read"},
{"title": "Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image\n  Matching and 3D Registration", "author": "Qianliang Wu and Haobo Jiang and Yaqing Ding and Lei Luo and Jun Li and Jin Xie and Xiaojun Wu and Jian Yang", "abstract": "  Establishing reliable correspondences is crucial for all registration tasks,\nincluding 2D image registration, 3D point cloud registration, and 2D-3D\nimage-to-point cloud registration. However, these tasks are often complicated\nby challenges such as scale inconsistencies, symmetry, and large deformations,\nwhich can lead to ambiguous matches. Previous feature-based and\ncorrespondence-based methods typically rely on geometric or semantic features\nto generate or polish initial potential correspondences. Some methods typically\nleverage specific geometric priors, such as topological preservation, to devise\ndiverse and innovative strategies tailored to a given enhancement goal, which\ncannot be exhaustively enumerated. Additionally, many previous approaches rely\non a single-step prediction head, which can struggle with local minima in\ncomplex matching scenarios. To address these challenges, we introduce an\ninnovative paradigm that leverages a diffusion model in matrix space for robust\nmatching matrix estimation. Our model treats correspondence estimation as a\ndenoising diffusion process in the matching matrix space, gradually refining\nthe intermediate matching matrix to the optimal one. Specifically, we apply the\ndiffusion model in the doubly stochastic matrix space for 3D-3D and 2D-3D\nregistration tasks. In the 2D image registration task, we deploy the diffusion\nmodel in a matrix subspace where dual-softmax projection regularization is\napplied. For all three registration tasks, we provide adaptive matching matrix\nembedding implementations tailored to the specific characteristics of each task\nwhile maintaining a consistent \"match-to-warp\" encoding pattern. Furthermore,\nwe adopt a lightweight design for the denoising module. In inference, once\npoints or image features are extracted and fixed, this module performs\nmulti-step denoising predictions through reverse sampling.\n", "link": "http://arxiv.org/abs/2503.04127v3", "date": "2025-09-25", "relevancy": 2.3902, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6281}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5978}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Reg%20v2%3A%20Diffusion-Based%20Matching%20Matrix%20Estimation%20for%20Image%0A%20%20Matching%20and%203D%20Registration&body=Title%3A%20Diff-Reg%20v2%3A%20Diffusion-Based%20Matching%20Matrix%20Estimation%20for%20Image%0A%20%20Matching%20and%203D%20Registration%0AAuthor%3A%20Qianliang%20Wu%20and%20Haobo%20Jiang%20and%20Yaqing%20Ding%20and%20Lei%20Luo%20and%20Jun%20Li%20and%20Jin%20Xie%20and%20Xiaojun%20Wu%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Establishing%20reliable%20correspondences%20is%20crucial%20for%20all%20registration%20tasks%2C%0Aincluding%202D%20image%20registration%2C%203D%20point%20cloud%20registration%2C%20and%202D-3D%0Aimage-to-point%20cloud%20registration.%20However%2C%20these%20tasks%20are%20often%20complicated%0Aby%20challenges%20such%20as%20scale%20inconsistencies%2C%20symmetry%2C%20and%20large%20deformations%2C%0Awhich%20can%20lead%20to%20ambiguous%20matches.%20Previous%20feature-based%20and%0Acorrespondence-based%20methods%20typically%20rely%20on%20geometric%20or%20semantic%20features%0Ato%20generate%20or%20polish%20initial%20potential%20correspondences.%20Some%20methods%20typically%0Aleverage%20specific%20geometric%20priors%2C%20such%20as%20topological%20preservation%2C%20to%20devise%0Adiverse%20and%20innovative%20strategies%20tailored%20to%20a%20given%20enhancement%20goal%2C%20which%0Acannot%20be%20exhaustively%20enumerated.%20Additionally%2C%20many%20previous%20approaches%20rely%0Aon%20a%20single-step%20prediction%20head%2C%20which%20can%20struggle%20with%20local%20minima%20in%0Acomplex%20matching%20scenarios.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%0Ainnovative%20paradigm%20that%20leverages%20a%20diffusion%20model%20in%20matrix%20space%20for%20robust%0Amatching%20matrix%20estimation.%20Our%20model%20treats%20correspondence%20estimation%20as%20a%0Adenoising%20diffusion%20process%20in%20the%20matching%20matrix%20space%2C%20gradually%20refining%0Athe%20intermediate%20matching%20matrix%20to%20the%20optimal%20one.%20Specifically%2C%20we%20apply%20the%0Adiffusion%20model%20in%20the%20doubly%20stochastic%20matrix%20space%20for%203D-3D%20and%202D-3D%0Aregistration%20tasks.%20In%20the%202D%20image%20registration%20task%2C%20we%20deploy%20the%20diffusion%0Amodel%20in%20a%20matrix%20subspace%20where%20dual-softmax%20projection%20regularization%20is%0Aapplied.%20For%20all%20three%20registration%20tasks%2C%20we%20provide%20adaptive%20matching%20matrix%0Aembedding%20implementations%20tailored%20to%20the%20specific%20characteristics%20of%20each%20task%0Awhile%20maintaining%20a%20consistent%20%22match-to-warp%22%20encoding%20pattern.%20Furthermore%2C%0Awe%20adopt%20a%20lightweight%20design%20for%20the%20denoising%20module.%20In%20inference%2C%20once%0Apoints%20or%20image%20features%20are%20extracted%20and%20fixed%2C%20this%20module%20performs%0Amulti-step%20denoising%20predictions%20through%20reverse%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04127v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Reg%2520v2%253A%2520Diffusion-Based%2520Matching%2520Matrix%2520Estimation%2520for%2520Image%250A%2520%2520Matching%2520and%25203D%2520Registration%26entry.906535625%3DQianliang%2520Wu%2520and%2520Haobo%2520Jiang%2520and%2520Yaqing%2520Ding%2520and%2520Lei%2520Luo%2520and%2520Jun%2520Li%2520and%2520Jin%2520Xie%2520and%2520Xiaojun%2520Wu%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Establishing%2520reliable%2520correspondences%2520is%2520crucial%2520for%2520all%2520registration%2520tasks%252C%250Aincluding%25202D%2520image%2520registration%252C%25203D%2520point%2520cloud%2520registration%252C%2520and%25202D-3D%250Aimage-to-point%2520cloud%2520registration.%2520However%252C%2520these%2520tasks%2520are%2520often%2520complicated%250Aby%2520challenges%2520such%2520as%2520scale%2520inconsistencies%252C%2520symmetry%252C%2520and%2520large%2520deformations%252C%250Awhich%2520can%2520lead%2520to%2520ambiguous%2520matches.%2520Previous%2520feature-based%2520and%250Acorrespondence-based%2520methods%2520typically%2520rely%2520on%2520geometric%2520or%2520semantic%2520features%250Ato%2520generate%2520or%2520polish%2520initial%2520potential%2520correspondences.%2520Some%2520methods%2520typically%250Aleverage%2520specific%2520geometric%2520priors%252C%2520such%2520as%2520topological%2520preservation%252C%2520to%2520devise%250Adiverse%2520and%2520innovative%2520strategies%2520tailored%2520to%2520a%2520given%2520enhancement%2520goal%252C%2520which%250Acannot%2520be%2520exhaustively%2520enumerated.%2520Additionally%252C%2520many%2520previous%2520approaches%2520rely%250Aon%2520a%2520single-step%2520prediction%2520head%252C%2520which%2520can%2520struggle%2520with%2520local%2520minima%2520in%250Acomplex%2520matching%2520scenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520an%250Ainnovative%2520paradigm%2520that%2520leverages%2520a%2520diffusion%2520model%2520in%2520matrix%2520space%2520for%2520robust%250Amatching%2520matrix%2520estimation.%2520Our%2520model%2520treats%2520correspondence%2520estimation%2520as%2520a%250Adenoising%2520diffusion%2520process%2520in%2520the%2520matching%2520matrix%2520space%252C%2520gradually%2520refining%250Athe%2520intermediate%2520matching%2520matrix%2520to%2520the%2520optimal%2520one.%2520Specifically%252C%2520we%2520apply%2520the%250Adiffusion%2520model%2520in%2520the%2520doubly%2520stochastic%2520matrix%2520space%2520for%25203D-3D%2520and%25202D-3D%250Aregistration%2520tasks.%2520In%2520the%25202D%2520image%2520registration%2520task%252C%2520we%2520deploy%2520the%2520diffusion%250Amodel%2520in%2520a%2520matrix%2520subspace%2520where%2520dual-softmax%2520projection%2520regularization%2520is%250Aapplied.%2520For%2520all%2520three%2520registration%2520tasks%252C%2520we%2520provide%2520adaptive%2520matching%2520matrix%250Aembedding%2520implementations%2520tailored%2520to%2520the%2520specific%2520characteristics%2520of%2520each%2520task%250Awhile%2520maintaining%2520a%2520consistent%2520%2522match-to-warp%2522%2520encoding%2520pattern.%2520Furthermore%252C%250Awe%2520adopt%2520a%2520lightweight%2520design%2520for%2520the%2520denoising%2520module.%2520In%2520inference%252C%2520once%250Apoints%2520or%2520image%2520features%2520are%2520extracted%2520and%2520fixed%252C%2520this%2520module%2520performs%250Amulti-step%2520denoising%2520predictions%2520through%2520reverse%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04127v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Reg%20v2%3A%20Diffusion-Based%20Matching%20Matrix%20Estimation%20for%20Image%0A%20%20Matching%20and%203D%20Registration&entry.906535625=Qianliang%20Wu%20and%20Haobo%20Jiang%20and%20Yaqing%20Ding%20and%20Lei%20Luo%20and%20Jun%20Li%20and%20Jin%20Xie%20and%20Xiaojun%20Wu%20and%20Jian%20Yang&entry.1292438233=%20%20Establishing%20reliable%20correspondences%20is%20crucial%20for%20all%20registration%20tasks%2C%0Aincluding%202D%20image%20registration%2C%203D%20point%20cloud%20registration%2C%20and%202D-3D%0Aimage-to-point%20cloud%20registration.%20However%2C%20these%20tasks%20are%20often%20complicated%0Aby%20challenges%20such%20as%20scale%20inconsistencies%2C%20symmetry%2C%20and%20large%20deformations%2C%0Awhich%20can%20lead%20to%20ambiguous%20matches.%20Previous%20feature-based%20and%0Acorrespondence-based%20methods%20typically%20rely%20on%20geometric%20or%20semantic%20features%0Ato%20generate%20or%20polish%20initial%20potential%20correspondences.%20Some%20methods%20typically%0Aleverage%20specific%20geometric%20priors%2C%20such%20as%20topological%20preservation%2C%20to%20devise%0Adiverse%20and%20innovative%20strategies%20tailored%20to%20a%20given%20enhancement%20goal%2C%20which%0Acannot%20be%20exhaustively%20enumerated.%20Additionally%2C%20many%20previous%20approaches%20rely%0Aon%20a%20single-step%20prediction%20head%2C%20which%20can%20struggle%20with%20local%20minima%20in%0Acomplex%20matching%20scenarios.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%0Ainnovative%20paradigm%20that%20leverages%20a%20diffusion%20model%20in%20matrix%20space%20for%20robust%0Amatching%20matrix%20estimation.%20Our%20model%20treats%20correspondence%20estimation%20as%20a%0Adenoising%20diffusion%20process%20in%20the%20matching%20matrix%20space%2C%20gradually%20refining%0Athe%20intermediate%20matching%20matrix%20to%20the%20optimal%20one.%20Specifically%2C%20we%20apply%20the%0Adiffusion%20model%20in%20the%20doubly%20stochastic%20matrix%20space%20for%203D-3D%20and%202D-3D%0Aregistration%20tasks.%20In%20the%202D%20image%20registration%20task%2C%20we%20deploy%20the%20diffusion%0Amodel%20in%20a%20matrix%20subspace%20where%20dual-softmax%20projection%20regularization%20is%0Aapplied.%20For%20all%20three%20registration%20tasks%2C%20we%20provide%20adaptive%20matching%20matrix%0Aembedding%20implementations%20tailored%20to%20the%20specific%20characteristics%20of%20each%20task%0Awhile%20maintaining%20a%20consistent%20%22match-to-warp%22%20encoding%20pattern.%20Furthermore%2C%0Awe%20adopt%20a%20lightweight%20design%20for%20the%20denoising%20module.%20In%20inference%2C%20once%0Apoints%20or%20image%20features%20are%20extracted%20and%20fixed%2C%20this%20module%20performs%0Amulti-step%20denoising%20predictions%20through%20reverse%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04127v3&entry.124074799=Read"},
{"title": "DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free\n  Grasps", "author": "Md Faizal Karim and Vignesh Vembar and Keshab Patra and Gaurav Singh and K Madhava Krishna", "abstract": "  Reliable dual-arm grasping is essential for manipulating large and complex\nobjects but remains a challenging problem due to stability, collision, and\ngeneralization requirements. Prior methods typically decompose the task into\ntwo independent grasp proposals, relying on region priors or heuristics that\nlimit generalization and provide no principled guarantee of stability. We\npropose DAGDiff, an end-to-end framework that directly denoises to grasp pairs\nin the SE(3) x SE(3) space. Our key insight is that stability and collision can\nbe enforced more effectively by guiding the diffusion process with classifier\nsignals, rather than relying on explicit region detection or object priors. To\nthis end, DAGDiff integrates geometry-, stability-, and collision-aware\nguidance terms that steer the generative process toward grasps that are\nphysically valid and force-closure compliant. We comprehensively evaluate\nDAGDiff through analytical force-closure checks, collision analysis, and\nlarge-scale physics-based simulations, showing consistent improvements over\nprevious work on these metrics. Finally, we demonstrate that our framework\ngenerates dual-arm grasps directly on real-world point clouds of previously\nunseen objects, which are executed on a heterogeneous dual-arm setup where two\nmanipulators reliably grasp and lift them.\n", "link": "http://arxiv.org/abs/2509.21145v1", "date": "2025-09-25", "relevancy": 2.363, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAGDiff%3A%20Guiding%20Dual-Arm%20Grasp%20Diffusion%20to%20Stable%20and%20Collision-Free%0A%20%20Grasps&body=Title%3A%20DAGDiff%3A%20Guiding%20Dual-Arm%20Grasp%20Diffusion%20to%20Stable%20and%20Collision-Free%0A%20%20Grasps%0AAuthor%3A%20Md%20Faizal%20Karim%20and%20Vignesh%20Vembar%20and%20Keshab%20Patra%20and%20Gaurav%20Singh%20and%20K%20Madhava%20Krishna%0AAbstract%3A%20%20%20Reliable%20dual-arm%20grasping%20is%20essential%20for%20manipulating%20large%20and%20complex%0Aobjects%20but%20remains%20a%20challenging%20problem%20due%20to%20stability%2C%20collision%2C%20and%0Ageneralization%20requirements.%20Prior%20methods%20typically%20decompose%20the%20task%20into%0Atwo%20independent%20grasp%20proposals%2C%20relying%20on%20region%20priors%20or%20heuristics%20that%0Alimit%20generalization%20and%20provide%20no%20principled%20guarantee%20of%20stability.%20We%0Apropose%20DAGDiff%2C%20an%20end-to-end%20framework%20that%20directly%20denoises%20to%20grasp%20pairs%0Ain%20the%20SE%283%29%20x%20SE%283%29%20space.%20Our%20key%20insight%20is%20that%20stability%20and%20collision%20can%0Abe%20enforced%20more%20effectively%20by%20guiding%20the%20diffusion%20process%20with%20classifier%0Asignals%2C%20rather%20than%20relying%20on%20explicit%20region%20detection%20or%20object%20priors.%20To%0Athis%20end%2C%20DAGDiff%20integrates%20geometry-%2C%20stability-%2C%20and%20collision-aware%0Aguidance%20terms%20that%20steer%20the%20generative%20process%20toward%20grasps%20that%20are%0Aphysically%20valid%20and%20force-closure%20compliant.%20We%20comprehensively%20evaluate%0ADAGDiff%20through%20analytical%20force-closure%20checks%2C%20collision%20analysis%2C%20and%0Alarge-scale%20physics-based%20simulations%2C%20showing%20consistent%20improvements%20over%0Aprevious%20work%20on%20these%20metrics.%20Finally%2C%20we%20demonstrate%20that%20our%20framework%0Agenerates%20dual-arm%20grasps%20directly%20on%20real-world%20point%20clouds%20of%20previously%0Aunseen%20objects%2C%20which%20are%20executed%20on%20a%20heterogeneous%20dual-arm%20setup%20where%20two%0Amanipulators%20reliably%20grasp%20and%20lift%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAGDiff%253A%2520Guiding%2520Dual-Arm%2520Grasp%2520Diffusion%2520to%2520Stable%2520and%2520Collision-Free%250A%2520%2520Grasps%26entry.906535625%3DMd%2520Faizal%2520Karim%2520and%2520Vignesh%2520Vembar%2520and%2520Keshab%2520Patra%2520and%2520Gaurav%2520Singh%2520and%2520K%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Reliable%2520dual-arm%2520grasping%2520is%2520essential%2520for%2520manipulating%2520large%2520and%2520complex%250Aobjects%2520but%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520stability%252C%2520collision%252C%2520and%250Ageneralization%2520requirements.%2520Prior%2520methods%2520typically%2520decompose%2520the%2520task%2520into%250Atwo%2520independent%2520grasp%2520proposals%252C%2520relying%2520on%2520region%2520priors%2520or%2520heuristics%2520that%250Alimit%2520generalization%2520and%2520provide%2520no%2520principled%2520guarantee%2520of%2520stability.%2520We%250Apropose%2520DAGDiff%252C%2520an%2520end-to-end%2520framework%2520that%2520directly%2520denoises%2520to%2520grasp%2520pairs%250Ain%2520the%2520SE%25283%2529%2520x%2520SE%25283%2529%2520space.%2520Our%2520key%2520insight%2520is%2520that%2520stability%2520and%2520collision%2520can%250Abe%2520enforced%2520more%2520effectively%2520by%2520guiding%2520the%2520diffusion%2520process%2520with%2520classifier%250Asignals%252C%2520rather%2520than%2520relying%2520on%2520explicit%2520region%2520detection%2520or%2520object%2520priors.%2520To%250Athis%2520end%252C%2520DAGDiff%2520integrates%2520geometry-%252C%2520stability-%252C%2520and%2520collision-aware%250Aguidance%2520terms%2520that%2520steer%2520the%2520generative%2520process%2520toward%2520grasps%2520that%2520are%250Aphysically%2520valid%2520and%2520force-closure%2520compliant.%2520We%2520comprehensively%2520evaluate%250ADAGDiff%2520through%2520analytical%2520force-closure%2520checks%252C%2520collision%2520analysis%252C%2520and%250Alarge-scale%2520physics-based%2520simulations%252C%2520showing%2520consistent%2520improvements%2520over%250Aprevious%2520work%2520on%2520these%2520metrics.%2520Finally%252C%2520we%2520demonstrate%2520that%2520our%2520framework%250Agenerates%2520dual-arm%2520grasps%2520directly%2520on%2520real-world%2520point%2520clouds%2520of%2520previously%250Aunseen%2520objects%252C%2520which%2520are%2520executed%2520on%2520a%2520heterogeneous%2520dual-arm%2520setup%2520where%2520two%250Amanipulators%2520reliably%2520grasp%2520and%2520lift%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAGDiff%3A%20Guiding%20Dual-Arm%20Grasp%20Diffusion%20to%20Stable%20and%20Collision-Free%0A%20%20Grasps&entry.906535625=Md%20Faizal%20Karim%20and%20Vignesh%20Vembar%20and%20Keshab%20Patra%20and%20Gaurav%20Singh%20and%20K%20Madhava%20Krishna&entry.1292438233=%20%20Reliable%20dual-arm%20grasping%20is%20essential%20for%20manipulating%20large%20and%20complex%0Aobjects%20but%20remains%20a%20challenging%20problem%20due%20to%20stability%2C%20collision%2C%20and%0Ageneralization%20requirements.%20Prior%20methods%20typically%20decompose%20the%20task%20into%0Atwo%20independent%20grasp%20proposals%2C%20relying%20on%20region%20priors%20or%20heuristics%20that%0Alimit%20generalization%20and%20provide%20no%20principled%20guarantee%20of%20stability.%20We%0Apropose%20DAGDiff%2C%20an%20end-to-end%20framework%20that%20directly%20denoises%20to%20grasp%20pairs%0Ain%20the%20SE%283%29%20x%20SE%283%29%20space.%20Our%20key%20insight%20is%20that%20stability%20and%20collision%20can%0Abe%20enforced%20more%20effectively%20by%20guiding%20the%20diffusion%20process%20with%20classifier%0Asignals%2C%20rather%20than%20relying%20on%20explicit%20region%20detection%20or%20object%20priors.%20To%0Athis%20end%2C%20DAGDiff%20integrates%20geometry-%2C%20stability-%2C%20and%20collision-aware%0Aguidance%20terms%20that%20steer%20the%20generative%20process%20toward%20grasps%20that%20are%0Aphysically%20valid%20and%20force-closure%20compliant.%20We%20comprehensively%20evaluate%0ADAGDiff%20through%20analytical%20force-closure%20checks%2C%20collision%20analysis%2C%20and%0Alarge-scale%20physics-based%20simulations%2C%20showing%20consistent%20improvements%20over%0Aprevious%20work%20on%20these%20metrics.%20Finally%2C%20we%20demonstrate%20that%20our%20framework%0Agenerates%20dual-arm%20grasps%20directly%20on%20real-world%20point%20clouds%20of%20previously%0Aunseen%20objects%2C%20which%20are%20executed%20on%20a%20heterogeneous%20dual-arm%20setup%20where%20two%0Amanipulators%20reliably%20grasp%20and%20lift%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21145v1&entry.124074799=Read"},
{"title": "Constrained Decoding for Robotics Foundation Models", "author": "Parv Kapoor and Akila Ganlath and Changliu Liu and Sebastian Scherer and Eunsuk Kang", "abstract": "  Recent advances in the development of robotic foundation models have led to\npromising end-to-end and general-purpose capabilities in robotic systems. These\nmodels are pretrained on vast datasets of robot trajectories to process\nmulti-modal inputs and directly output a sequence of action that the system\nthen executes in the real world. Although this approach is attractive from the\nperspective of improved generalization across diverse tasks, these models are\nstill data-driven and, therefore, lack explicit notions of behavioral\ncorrectness and safety constraints. We address these limitations by introducing\na constrained decoding framework for robotics foundation models that enforces\nlogical constraints on action trajectories in dynamical systems. Our method\nensures that generated actions provably satisfy signal temporal logic (STL)\nspecifications at runtime without retraining, while remaining agnostic of the\nunderlying foundation model. We perform comprehensive evaluation of our\napproach across state-of-the-art navigation foundation models and we show that\nour decoding-time interventions are useful not only for filtering unsafe\nactions but also for conditional action-generation. Videos available on our\nwebsite: https://constrained-robot-fms.github.io\n", "link": "http://arxiv.org/abs/2509.01728v2", "date": "2025-09-25", "relevancy": 2.3488, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Decoding%20for%20Robotics%20Foundation%20Models&body=Title%3A%20Constrained%20Decoding%20for%20Robotics%20Foundation%20Models%0AAuthor%3A%20Parv%20Kapoor%20and%20Akila%20Ganlath%20and%20Changliu%20Liu%20and%20Sebastian%20Scherer%20and%20Eunsuk%20Kang%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20development%20of%20robotic%20foundation%20models%20have%20led%20to%0Apromising%20end-to-end%20and%20general-purpose%20capabilities%20in%20robotic%20systems.%20These%0Amodels%20are%20pretrained%20on%20vast%20datasets%20of%20robot%20trajectories%20to%20process%0Amulti-modal%20inputs%20and%20directly%20output%20a%20sequence%20of%20action%20that%20the%20system%0Athen%20executes%20in%20the%20real%20world.%20Although%20this%20approach%20is%20attractive%20from%20the%0Aperspective%20of%20improved%20generalization%20across%20diverse%20tasks%2C%20these%20models%20are%0Astill%20data-driven%20and%2C%20therefore%2C%20lack%20explicit%20notions%20of%20behavioral%0Acorrectness%20and%20safety%20constraints.%20We%20address%20these%20limitations%20by%20introducing%0Aa%20constrained%20decoding%20framework%20for%20robotics%20foundation%20models%20that%20enforces%0Alogical%20constraints%20on%20action%20trajectories%20in%20dynamical%20systems.%20Our%20method%0Aensures%20that%20generated%20actions%20provably%20satisfy%20signal%20temporal%20logic%20%28STL%29%0Aspecifications%20at%20runtime%20without%20retraining%2C%20while%20remaining%20agnostic%20of%20the%0Aunderlying%20foundation%20model.%20We%20perform%20comprehensive%20evaluation%20of%20our%0Aapproach%20across%20state-of-the-art%20navigation%20foundation%20models%20and%20we%20show%20that%0Aour%20decoding-time%20interventions%20are%20useful%20not%20only%20for%20filtering%20unsafe%0Aactions%20but%20also%20for%20conditional%20action-generation.%20Videos%20available%20on%20our%0Awebsite%3A%20https%3A//constrained-robot-fms.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Decoding%2520for%2520Robotics%2520Foundation%2520Models%26entry.906535625%3DParv%2520Kapoor%2520and%2520Akila%2520Ganlath%2520and%2520Changliu%2520Liu%2520and%2520Sebastian%2520Scherer%2520and%2520Eunsuk%2520Kang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520the%2520development%2520of%2520robotic%2520foundation%2520models%2520have%2520led%2520to%250Apromising%2520end-to-end%2520and%2520general-purpose%2520capabilities%2520in%2520robotic%2520systems.%2520These%250Amodels%2520are%2520pretrained%2520on%2520vast%2520datasets%2520of%2520robot%2520trajectories%2520to%2520process%250Amulti-modal%2520inputs%2520and%2520directly%2520output%2520a%2520sequence%2520of%2520action%2520that%2520the%2520system%250Athen%2520executes%2520in%2520the%2520real%2520world.%2520Although%2520this%2520approach%2520is%2520attractive%2520from%2520the%250Aperspective%2520of%2520improved%2520generalization%2520across%2520diverse%2520tasks%252C%2520these%2520models%2520are%250Astill%2520data-driven%2520and%252C%2520therefore%252C%2520lack%2520explicit%2520notions%2520of%2520behavioral%250Acorrectness%2520and%2520safety%2520constraints.%2520We%2520address%2520these%2520limitations%2520by%2520introducing%250Aa%2520constrained%2520decoding%2520framework%2520for%2520robotics%2520foundation%2520models%2520that%2520enforces%250Alogical%2520constraints%2520on%2520action%2520trajectories%2520in%2520dynamical%2520systems.%2520Our%2520method%250Aensures%2520that%2520generated%2520actions%2520provably%2520satisfy%2520signal%2520temporal%2520logic%2520%2528STL%2529%250Aspecifications%2520at%2520runtime%2520without%2520retraining%252C%2520while%2520remaining%2520agnostic%2520of%2520the%250Aunderlying%2520foundation%2520model.%2520We%2520perform%2520comprehensive%2520evaluation%2520of%2520our%250Aapproach%2520across%2520state-of-the-art%2520navigation%2520foundation%2520models%2520and%2520we%2520show%2520that%250Aour%2520decoding-time%2520interventions%2520are%2520useful%2520not%2520only%2520for%2520filtering%2520unsafe%250Aactions%2520but%2520also%2520for%2520conditional%2520action-generation.%2520Videos%2520available%2520on%2520our%250Awebsite%253A%2520https%253A//constrained-robot-fms.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Decoding%20for%20Robotics%20Foundation%20Models&entry.906535625=Parv%20Kapoor%20and%20Akila%20Ganlath%20and%20Changliu%20Liu%20and%20Sebastian%20Scherer%20and%20Eunsuk%20Kang&entry.1292438233=%20%20Recent%20advances%20in%20the%20development%20of%20robotic%20foundation%20models%20have%20led%20to%0Apromising%20end-to-end%20and%20general-purpose%20capabilities%20in%20robotic%20systems.%20These%0Amodels%20are%20pretrained%20on%20vast%20datasets%20of%20robot%20trajectories%20to%20process%0Amulti-modal%20inputs%20and%20directly%20output%20a%20sequence%20of%20action%20that%20the%20system%0Athen%20executes%20in%20the%20real%20world.%20Although%20this%20approach%20is%20attractive%20from%20the%0Aperspective%20of%20improved%20generalization%20across%20diverse%20tasks%2C%20these%20models%20are%0Astill%20data-driven%20and%2C%20therefore%2C%20lack%20explicit%20notions%20of%20behavioral%0Acorrectness%20and%20safety%20constraints.%20We%20address%20these%20limitations%20by%20introducing%0Aa%20constrained%20decoding%20framework%20for%20robotics%20foundation%20models%20that%20enforces%0Alogical%20constraints%20on%20action%20trajectories%20in%20dynamical%20systems.%20Our%20method%0Aensures%20that%20generated%20actions%20provably%20satisfy%20signal%20temporal%20logic%20%28STL%29%0Aspecifications%20at%20runtime%20without%20retraining%2C%20while%20remaining%20agnostic%20of%20the%0Aunderlying%20foundation%20model.%20We%20perform%20comprehensive%20evaluation%20of%20our%0Aapproach%20across%20state-of-the-art%20navigation%20foundation%20models%20and%20we%20show%20that%0Aour%20decoding-time%20interventions%20are%20useful%20not%20only%20for%20filtering%20unsafe%0Aactions%20but%20also%20for%20conditional%20action-generation.%20Videos%20available%20on%20our%0Awebsite%3A%20https%3A//constrained-robot-fms.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01728v2&entry.124074799=Read"},
{"title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion\n  Vectors", "author": "Binhua Huang and Ni Wang and Arjun Pakrashi and Soumyabrata Dev", "abstract": "  Video action recognition is a fundamental task in computer vision, but\nstate-of-the-art models are often computationally expensive and rely on\nextensive video pre-training. In parallel, large-scale vision-language models\nlike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot\ncapabilities on static images, while motion vectors (MV) provide highly\nefficient temporal information directly from compressed video streams. To\nsynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple\nyet powerful two-stream late fusion framework for efficient video recognition.\nOur approach combines features from a frozen CLIP image encoder with features\nfrom a lightweight, supervised network trained on raw MV. During fusion, both\nbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is\ntrained, ensuring extreme efficiency. Through comprehensive experiments on the\nUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,\nsignificantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)\nbaselines. Our work provides a new, highly efficient baseline for video\nunderstanding that effectively bridges the gap between large static models and\ndynamic, low-cost motion cues. Our code and models are available at\nhttps://github.com/microa/MoCLIP-Lite.\n", "link": "http://arxiv.org/abs/2509.17084v2", "date": "2025-09-25", "relevancy": 2.3369, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6028}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCLIP-Lite%3A%20Efficient%20Video%20Recognition%20by%20Fusing%20CLIP%20with%20Motion%0A%20%20Vectors&body=Title%3A%20MoCLIP-Lite%3A%20Efficient%20Video%20Recognition%20by%20Fusing%20CLIP%20with%20Motion%0A%20%20Vectors%0AAuthor%3A%20Binhua%20Huang%20and%20Ni%20Wang%20and%20Arjun%20Pakrashi%20and%20Soumyabrata%20Dev%0AAbstract%3A%20%20%20Video%20action%20recognition%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20but%0Astate-of-the-art%20models%20are%20often%20computationally%20expensive%20and%20rely%20on%0Aextensive%20video%20pre-training.%20In%20parallel%2C%20large-scale%20vision-language%20models%0Alike%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20offer%20powerful%20zero-shot%0Acapabilities%20on%20static%20images%2C%20while%20motion%20vectors%20%28MV%29%20provide%20highly%0Aefficient%20temporal%20information%20directly%20from%20compressed%20video%20streams.%20To%0Asynergize%20the%20strengths%20of%20these%20paradigms%2C%20we%20propose%20MoCLIP-Lite%2C%20a%20simple%0Ayet%20powerful%20two-stream%20late%20fusion%20framework%20for%20efficient%20video%20recognition.%0AOur%20approach%20combines%20features%20from%20a%20frozen%20CLIP%20image%20encoder%20with%20features%0Afrom%20a%20lightweight%2C%20supervised%20network%20trained%20on%20raw%20MV.%20During%20fusion%2C%20both%0Abackbones%20are%20frozen%2C%20and%20only%20a%20tiny%20Multi-Layer%20Perceptron%20%28MLP%29%20head%20is%0Atrained%2C%20ensuring%20extreme%20efficiency.%20Through%20comprehensive%20experiments%20on%20the%0AUCF101%20dataset%2C%20our%20method%20achieves%20a%20remarkable%2089.2%25%20Top-1%20accuracy%2C%0Asignificantly%20outperforming%20strong%20zero-shot%20%2865.0%25%29%20and%20MV-only%20%2866.5%25%29%0Abaselines.%20Our%20work%20provides%20a%20new%2C%20highly%20efficient%20baseline%20for%20video%0Aunderstanding%20that%20effectively%20bridges%20the%20gap%20between%20large%20static%20models%20and%0Adynamic%2C%20low-cost%20motion%20cues.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/microa/MoCLIP-Lite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCLIP-Lite%253A%2520Efficient%2520Video%2520Recognition%2520by%2520Fusing%2520CLIP%2520with%2520Motion%250A%2520%2520Vectors%26entry.906535625%3DBinhua%2520Huang%2520and%2520Ni%2520Wang%2520and%2520Arjun%2520Pakrashi%2520and%2520Soumyabrata%2520Dev%26entry.1292438233%3D%2520%2520Video%2520action%2520recognition%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520but%250Astate-of-the-art%2520models%2520are%2520often%2520computationally%2520expensive%2520and%2520rely%2520on%250Aextensive%2520video%2520pre-training.%2520In%2520parallel%252C%2520large-scale%2520vision-language%2520models%250Alike%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520offer%2520powerful%2520zero-shot%250Acapabilities%2520on%2520static%2520images%252C%2520while%2520motion%2520vectors%2520%2528MV%2529%2520provide%2520highly%250Aefficient%2520temporal%2520information%2520directly%2520from%2520compressed%2520video%2520streams.%2520To%250Asynergize%2520the%2520strengths%2520of%2520these%2520paradigms%252C%2520we%2520propose%2520MoCLIP-Lite%252C%2520a%2520simple%250Ayet%2520powerful%2520two-stream%2520late%2520fusion%2520framework%2520for%2520efficient%2520video%2520recognition.%250AOur%2520approach%2520combines%2520features%2520from%2520a%2520frozen%2520CLIP%2520image%2520encoder%2520with%2520features%250Afrom%2520a%2520lightweight%252C%2520supervised%2520network%2520trained%2520on%2520raw%2520MV.%2520During%2520fusion%252C%2520both%250Abackbones%2520are%2520frozen%252C%2520and%2520only%2520a%2520tiny%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520head%2520is%250Atrained%252C%2520ensuring%2520extreme%2520efficiency.%2520Through%2520comprehensive%2520experiments%2520on%2520the%250AUCF101%2520dataset%252C%2520our%2520method%2520achieves%2520a%2520remarkable%252089.2%2525%2520Top-1%2520accuracy%252C%250Asignificantly%2520outperforming%2520strong%2520zero-shot%2520%252865.0%2525%2529%2520and%2520MV-only%2520%252866.5%2525%2529%250Abaselines.%2520Our%2520work%2520provides%2520a%2520new%252C%2520highly%2520efficient%2520baseline%2520for%2520video%250Aunderstanding%2520that%2520effectively%2520bridges%2520the%2520gap%2520between%2520large%2520static%2520models%2520and%250Adynamic%252C%2520low-cost%2520motion%2520cues.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/microa/MoCLIP-Lite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCLIP-Lite%3A%20Efficient%20Video%20Recognition%20by%20Fusing%20CLIP%20with%20Motion%0A%20%20Vectors&entry.906535625=Binhua%20Huang%20and%20Ni%20Wang%20and%20Arjun%20Pakrashi%20and%20Soumyabrata%20Dev&entry.1292438233=%20%20Video%20action%20recognition%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20but%0Astate-of-the-art%20models%20are%20often%20computationally%20expensive%20and%20rely%20on%0Aextensive%20video%20pre-training.%20In%20parallel%2C%20large-scale%20vision-language%20models%0Alike%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20offer%20powerful%20zero-shot%0Acapabilities%20on%20static%20images%2C%20while%20motion%20vectors%20%28MV%29%20provide%20highly%0Aefficient%20temporal%20information%20directly%20from%20compressed%20video%20streams.%20To%0Asynergize%20the%20strengths%20of%20these%20paradigms%2C%20we%20propose%20MoCLIP-Lite%2C%20a%20simple%0Ayet%20powerful%20two-stream%20late%20fusion%20framework%20for%20efficient%20video%20recognition.%0AOur%20approach%20combines%20features%20from%20a%20frozen%20CLIP%20image%20encoder%20with%20features%0Afrom%20a%20lightweight%2C%20supervised%20network%20trained%20on%20raw%20MV.%20During%20fusion%2C%20both%0Abackbones%20are%20frozen%2C%20and%20only%20a%20tiny%20Multi-Layer%20Perceptron%20%28MLP%29%20head%20is%0Atrained%2C%20ensuring%20extreme%20efficiency.%20Through%20comprehensive%20experiments%20on%20the%0AUCF101%20dataset%2C%20our%20method%20achieves%20a%20remarkable%2089.2%25%20Top-1%20accuracy%2C%0Asignificantly%20outperforming%20strong%20zero-shot%20%2865.0%25%29%20and%20MV-only%20%2866.5%25%29%0Abaselines.%20Our%20work%20provides%20a%20new%2C%20highly%20efficient%20baseline%20for%20video%0Aunderstanding%20that%20effectively%20bridges%20the%20gap%20between%20large%20static%20models%20and%0Adynamic%2C%20low-cost%20motion%20cues.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/microa/MoCLIP-Lite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17084v2&entry.124074799=Read"},
{"title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios", "author": "Yang Shi and Huanqian Wang and Wulin Xie and Huanyao Zhang and Lijie Zhao and Yi-Fan Zhang and Xinfeng Li and Chaoyou Fu and Zhuoer Wen and Wenting Liu and Zhuoran Zhang and Xinlong Chen and Bohan Zeng and Sihan Yang and Yushuo Guan and Zhang Zhang and Liang Wang and Haoxuan Li and Zhouchen Lin and Yuanxing Zhang and Pengfei Wan and Haotian Wang and Wenjing Yang", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.\n", "link": "http://arxiv.org/abs/2505.21333v2", "date": "2025-09-25", "relevancy": 2.3257, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios&body=Title%3A%20MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios%0AAuthor%3A%20Yang%20Shi%20and%20Huanqian%20Wang%20and%20Wulin%20Xie%20and%20Huanyao%20Zhang%20and%20Lijie%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Xinfeng%20Li%20and%20Chaoyou%20Fu%20and%20Zhuoer%20Wen%20and%20Wenting%20Liu%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Bohan%20Zeng%20and%20Sihan%20Yang%20and%20Yushuo%20Guan%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Haoxuan%20Li%20and%20Zhouchen%20Lin%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20accuracy%0Ain%20Optical%20Character%20Recognition%20%28OCR%29%20from%20static%20images.%20However%2C%20their%0Aefficacy%20in%20video%20OCR%20is%20significantly%20diminished%20due%20to%20factors%20such%20as%20motion%0Ablur%2C%20temporal%20variations%2C%20and%20visual%20effects%20inherent%20in%20video%20content.%20To%0Aprovide%20clearer%20guidance%20for%20training%20practical%20MLLMs%2C%20we%20introduce%20the%0AMME-VideoOCR%20benchmark%2C%20which%20encompasses%20a%20comprehensive%20range%20of%20video%20OCR%0Aapplication%20scenarios.%20MME-VideoOCR%20features%2010%20task%20categories%20comprising%2025%0Aindividual%20tasks%20and%20spans%2044%20diverse%20scenarios.%20These%20tasks%20extend%20beyond%20text%0Arecognition%20to%20incorporate%20deeper%20comprehension%20and%20reasoning%20of%20textual%0Acontent%20within%20videos.%20The%20benchmark%20consists%20of%201%2C464%20videos%20with%20varying%0Aresolutions%2C%20aspect%20ratios%2C%20and%20durations%2C%20along%20with%202%2C000%20meticulously%0Acurated%2C%20manually%20annotated%20question-answer%20pairs.%20We%20evaluate%2018%0Astate-of-the-art%20MLLMs%20on%20MME-VideoOCR%2C%20revealing%20that%20even%20the%20best-performing%0Amodel%20%28Gemini-2.5%20Pro%29%20achieves%20an%20accuracy%20of%20only%2073.7%25.%20Fine-grained%0Aanalysis%20indicates%20that%20while%20existing%20MLLMs%20demonstrate%20strong%20performance%20on%0Atasks%20where%20relevant%20texts%20are%20contained%20within%20a%20single%20or%20few%20frames%2C%20they%0Aexhibit%20limited%20capability%20in%20effectively%20handling%20tasks%20that%20demand%20holistic%0Avideo%20comprehension.%20These%20limitations%20are%20especially%20evident%20in%20scenarios%20that%0Arequire%20spatio-temporal%20reasoning%2C%20cross-frame%20information%20integration%2C%20or%0Aresistance%20to%20language%20prior%20bias.%20Our%20findings%20also%20highlight%20the%20importance%0Aof%20high-resolution%20visual%20input%20and%20sufficient%20temporal%20coverage%20for%20reliable%0AOCR%20in%20dynamic%20video%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-VideoOCR%253A%2520Evaluating%2520OCR-Based%2520Capabilities%2520of%2520Multimodal%2520LLMs%2520in%250A%2520%2520Video%2520Scenarios%26entry.906535625%3DYang%2520Shi%2520and%2520Huanqian%2520Wang%2520and%2520Wulin%2520Xie%2520and%2520Huanyao%2520Zhang%2520and%2520Lijie%2520Zhao%2520and%2520Yi-Fan%2520Zhang%2520and%2520Xinfeng%2520Li%2520and%2520Chaoyou%2520Fu%2520and%2520Zhuoer%2520Wen%2520and%2520Wenting%2520Liu%2520and%2520Zhuoran%2520Zhang%2520and%2520Xinlong%2520Chen%2520and%2520Bohan%2520Zeng%2520and%2520Sihan%2520Yang%2520and%2520Yushuo%2520Guan%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%2520and%2520Haoxuan%2520Li%2520and%2520Zhouchen%2520Lin%2520and%2520Yuanxing%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Haotian%2520Wang%2520and%2520Wenjing%2520Yang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520considerable%2520accuracy%250Ain%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520from%2520static%2520images.%2520However%252C%2520their%250Aefficacy%2520in%2520video%2520OCR%2520is%2520significantly%2520diminished%2520due%2520to%2520factors%2520such%2520as%2520motion%250Ablur%252C%2520temporal%2520variations%252C%2520and%2520visual%2520effects%2520inherent%2520in%2520video%2520content.%2520To%250Aprovide%2520clearer%2520guidance%2520for%2520training%2520practical%2520MLLMs%252C%2520we%2520introduce%2520the%250AMME-VideoOCR%2520benchmark%252C%2520which%2520encompasses%2520a%2520comprehensive%2520range%2520of%2520video%2520OCR%250Aapplication%2520scenarios.%2520MME-VideoOCR%2520features%252010%2520task%2520categories%2520comprising%252025%250Aindividual%2520tasks%2520and%2520spans%252044%2520diverse%2520scenarios.%2520These%2520tasks%2520extend%2520beyond%2520text%250Arecognition%2520to%2520incorporate%2520deeper%2520comprehension%2520and%2520reasoning%2520of%2520textual%250Acontent%2520within%2520videos.%2520The%2520benchmark%2520consists%2520of%25201%252C464%2520videos%2520with%2520varying%250Aresolutions%252C%2520aspect%2520ratios%252C%2520and%2520durations%252C%2520along%2520with%25202%252C000%2520meticulously%250Acurated%252C%2520manually%2520annotated%2520question-answer%2520pairs.%2520We%2520evaluate%252018%250Astate-of-the-art%2520MLLMs%2520on%2520MME-VideoOCR%252C%2520revealing%2520that%2520even%2520the%2520best-performing%250Amodel%2520%2528Gemini-2.5%2520Pro%2529%2520achieves%2520an%2520accuracy%2520of%2520only%252073.7%2525.%2520Fine-grained%250Aanalysis%2520indicates%2520that%2520while%2520existing%2520MLLMs%2520demonstrate%2520strong%2520performance%2520on%250Atasks%2520where%2520relevant%2520texts%2520are%2520contained%2520within%2520a%2520single%2520or%2520few%2520frames%252C%2520they%250Aexhibit%2520limited%2520capability%2520in%2520effectively%2520handling%2520tasks%2520that%2520demand%2520holistic%250Avideo%2520comprehension.%2520These%2520limitations%2520are%2520especially%2520evident%2520in%2520scenarios%2520that%250Arequire%2520spatio-temporal%2520reasoning%252C%2520cross-frame%2520information%2520integration%252C%2520or%250Aresistance%2520to%2520language%2520prior%2520bias.%2520Our%2520findings%2520also%2520highlight%2520the%2520importance%250Aof%2520high-resolution%2520visual%2520input%2520and%2520sufficient%2520temporal%2520coverage%2520for%2520reliable%250AOCR%2520in%2520dynamic%2520video%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios&entry.906535625=Yang%20Shi%20and%20Huanqian%20Wang%20and%20Wulin%20Xie%20and%20Huanyao%20Zhang%20and%20Lijie%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Xinfeng%20Li%20and%20Chaoyou%20Fu%20and%20Zhuoer%20Wen%20and%20Wenting%20Liu%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Bohan%20Zeng%20and%20Sihan%20Yang%20and%20Yushuo%20Guan%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Haoxuan%20Li%20and%20Zhouchen%20Lin%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Haotian%20Wang%20and%20Wenjing%20Yang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20accuracy%0Ain%20Optical%20Character%20Recognition%20%28OCR%29%20from%20static%20images.%20However%2C%20their%0Aefficacy%20in%20video%20OCR%20is%20significantly%20diminished%20due%20to%20factors%20such%20as%20motion%0Ablur%2C%20temporal%20variations%2C%20and%20visual%20effects%20inherent%20in%20video%20content.%20To%0Aprovide%20clearer%20guidance%20for%20training%20practical%20MLLMs%2C%20we%20introduce%20the%0AMME-VideoOCR%20benchmark%2C%20which%20encompasses%20a%20comprehensive%20range%20of%20video%20OCR%0Aapplication%20scenarios.%20MME-VideoOCR%20features%2010%20task%20categories%20comprising%2025%0Aindividual%20tasks%20and%20spans%2044%20diverse%20scenarios.%20These%20tasks%20extend%20beyond%20text%0Arecognition%20to%20incorporate%20deeper%20comprehension%20and%20reasoning%20of%20textual%0Acontent%20within%20videos.%20The%20benchmark%20consists%20of%201%2C464%20videos%20with%20varying%0Aresolutions%2C%20aspect%20ratios%2C%20and%20durations%2C%20along%20with%202%2C000%20meticulously%0Acurated%2C%20manually%20annotated%20question-answer%20pairs.%20We%20evaluate%2018%0Astate-of-the-art%20MLLMs%20on%20MME-VideoOCR%2C%20revealing%20that%20even%20the%20best-performing%0Amodel%20%28Gemini-2.5%20Pro%29%20achieves%20an%20accuracy%20of%20only%2073.7%25.%20Fine-grained%0Aanalysis%20indicates%20that%20while%20existing%20MLLMs%20demonstrate%20strong%20performance%20on%0Atasks%20where%20relevant%20texts%20are%20contained%20within%20a%20single%20or%20few%20frames%2C%20they%0Aexhibit%20limited%20capability%20in%20effectively%20handling%20tasks%20that%20demand%20holistic%0Avideo%20comprehension.%20These%20limitations%20are%20especially%20evident%20in%20scenarios%20that%0Arequire%20spatio-temporal%20reasoning%2C%20cross-frame%20information%20integration%2C%20or%0Aresistance%20to%20language%20prior%20bias.%20Our%20findings%20also%20highlight%20the%20importance%0Aof%20high-resolution%20visual%20input%20and%20sufficient%20temporal%20coverage%20for%20reliable%0AOCR%20in%20dynamic%20video%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21333v2&entry.124074799=Read"},
{"title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs", "author": "Yunheng Li and Jing Cheng and Shaoyong Jia and Hangyi Kuang and Shaohui Jiao and Qibin Hou and Ming-Ming Cheng", "abstract": "  This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1\n", "link": "http://arxiv.org/abs/2509.18056v2", "date": "2025-09-25", "relevancy": 2.3207, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5781}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs&body=Title%3A%20TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs%0AAuthor%3A%20Yunheng%20Li%20and%20Jing%20Cheng%20and%20Shaoyong%20Jia%20and%20Hangyi%20Kuang%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20This%20paper%20introduces%20TempSamp-R1%2C%20a%20new%20reinforcement%20fine-tuning%20framework%0Adesigned%20to%20improve%20the%20effectiveness%20of%20adapting%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20video%20temporal%20grounding%20tasks.%20We%20reveal%20that%20existing%0Areinforcement%20learning%20methods%2C%20such%20as%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20rely%20on%20on-policy%20sampling%20for%20policy%20updates.%20However%2C%20in%20tasks%20with%0Alarge%20temporal%20search%20spaces%2C%20this%20strategy%20becomes%20both%20inefficient%20and%0Alimited%20in%20performance%2C%20as%20it%20often%20fails%20to%20identify%20temporally%20accurate%0Asolutions.%20To%20address%20this%20limitation%2C%20TempSamp-R1%20leverages%20ground-truth%0Aannotations%20as%20off-policy%20supervision%20to%20provide%20temporally%20precise%20guidance%2C%0Aeffectively%20compensating%20for%20the%20sparsity%20and%20misalignment%20in%20on-policy%0Asolutions.%20To%20further%20stabilize%20training%20and%20reduce%20variance%20in%20reward-based%0Aupdates%2C%20TempSamp-R1%20provides%20a%20non-linear%20soft%20advantage%20computation%20method%0Athat%20dynamically%20reshapes%20the%20reward%20feedback%20via%20an%20asymmetric%20transformation.%0ABy%20employing%20a%20hybrid%20Chain-of-Thought%20%28CoT%29%20training%20paradigm%2C%20TempSamp-R1%0Aoptimizes%20a%20single%20unified%20model%20to%20support%20both%20CoT%20and%20non-CoT%20inference%0Amodes%2C%20enabling%20efficient%20handling%20of%20queries%20with%20varying%20reasoning%0Acomplexity.%20Experimental%20results%20demonstrate%20that%20TempSamp-R1%20outperforms%0AGRPO-based%20baselines%2C%20establishing%20new%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%3A%20Charades-STA%20%28R1%400.7%3A%2052.9%25%2C%20%2B2.7%25%29%2C%20ActivityNet%20Captions%0A%28R1%400.5%3A%2056.0%25%2C%20%2B5.3%25%29%2C%20and%20QVHighlights%20%28mAP%3A%2030.0%25%2C%20%2B3.0%25%29.%20Moreover%2C%0ATempSamp-R1%20shows%20robust%20few-shot%20generalization%20capabilities%20under%20limited%0Adata.%20Code%3A%20https%3A//github.com/HVision-NKU/TempSamp-R1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempSamp-R1%253A%2520Effective%2520Temporal%2520Sampling%2520with%2520Reinforcement%2520Fine-Tuning%250A%2520%2520for%2520Video%2520LLMs%26entry.906535625%3DYunheng%2520Li%2520and%2520Jing%2520Cheng%2520and%2520Shaoyong%2520Jia%2520and%2520Hangyi%2520Kuang%2520and%2520Shaohui%2520Jiao%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520TempSamp-R1%252C%2520a%2520new%2520reinforcement%2520fine-tuning%2520framework%250Adesigned%2520to%2520improve%2520the%2520effectiveness%2520of%2520adapting%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%2520to%2520video%2520temporal%2520grounding%2520tasks.%2520We%2520reveal%2520that%2520existing%250Areinforcement%2520learning%2520methods%252C%2520such%2520as%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520rely%2520on%2520on-policy%2520sampling%2520for%2520policy%2520updates.%2520However%252C%2520in%2520tasks%2520with%250Alarge%2520temporal%2520search%2520spaces%252C%2520this%2520strategy%2520becomes%2520both%2520inefficient%2520and%250Alimited%2520in%2520performance%252C%2520as%2520it%2520often%2520fails%2520to%2520identify%2520temporally%2520accurate%250Asolutions.%2520To%2520address%2520this%2520limitation%252C%2520TempSamp-R1%2520leverages%2520ground-truth%250Aannotations%2520as%2520off-policy%2520supervision%2520to%2520provide%2520temporally%2520precise%2520guidance%252C%250Aeffectively%2520compensating%2520for%2520the%2520sparsity%2520and%2520misalignment%2520in%2520on-policy%250Asolutions.%2520To%2520further%2520stabilize%2520training%2520and%2520reduce%2520variance%2520in%2520reward-based%250Aupdates%252C%2520TempSamp-R1%2520provides%2520a%2520non-linear%2520soft%2520advantage%2520computation%2520method%250Athat%2520dynamically%2520reshapes%2520the%2520reward%2520feedback%2520via%2520an%2520asymmetric%2520transformation.%250ABy%2520employing%2520a%2520hybrid%2520Chain-of-Thought%2520%2528CoT%2529%2520training%2520paradigm%252C%2520TempSamp-R1%250Aoptimizes%2520a%2520single%2520unified%2520model%2520to%2520support%2520both%2520CoT%2520and%2520non-CoT%2520inference%250Amodes%252C%2520enabling%2520efficient%2520handling%2520of%2520queries%2520with%2520varying%2520reasoning%250Acomplexity.%2520Experimental%2520results%2520demonstrate%2520that%2520TempSamp-R1%2520outperforms%250AGRPO-based%2520baselines%252C%2520establishing%2520new%2520state-of-the-art%2520performance%2520on%250Abenchmark%2520datasets%253A%2520Charades-STA%2520%2528R1%25400.7%253A%252052.9%2525%252C%2520%252B2.7%2525%2529%252C%2520ActivityNet%2520Captions%250A%2528R1%25400.5%253A%252056.0%2525%252C%2520%252B5.3%2525%2529%252C%2520and%2520QVHighlights%2520%2528mAP%253A%252030.0%2525%252C%2520%252B3.0%2525%2529.%2520Moreover%252C%250ATempSamp-R1%2520shows%2520robust%2520few-shot%2520generalization%2520capabilities%2520under%2520limited%250Adata.%2520Code%253A%2520https%253A//github.com/HVision-NKU/TempSamp-R1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempSamp-R1%3A%20Effective%20Temporal%20Sampling%20with%20Reinforcement%20Fine-Tuning%0A%20%20for%20Video%20LLMs&entry.906535625=Yunheng%20Li%20and%20Jing%20Cheng%20and%20Shaoyong%20Jia%20and%20Hangyi%20Kuang%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20This%20paper%20introduces%20TempSamp-R1%2C%20a%20new%20reinforcement%20fine-tuning%20framework%0Adesigned%20to%20improve%20the%20effectiveness%20of%20adapting%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20to%20video%20temporal%20grounding%20tasks.%20We%20reveal%20that%20existing%0Areinforcement%20learning%20methods%2C%20such%20as%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20rely%20on%20on-policy%20sampling%20for%20policy%20updates.%20However%2C%20in%20tasks%20with%0Alarge%20temporal%20search%20spaces%2C%20this%20strategy%20becomes%20both%20inefficient%20and%0Alimited%20in%20performance%2C%20as%20it%20often%20fails%20to%20identify%20temporally%20accurate%0Asolutions.%20To%20address%20this%20limitation%2C%20TempSamp-R1%20leverages%20ground-truth%0Aannotations%20as%20off-policy%20supervision%20to%20provide%20temporally%20precise%20guidance%2C%0Aeffectively%20compensating%20for%20the%20sparsity%20and%20misalignment%20in%20on-policy%0Asolutions.%20To%20further%20stabilize%20training%20and%20reduce%20variance%20in%20reward-based%0Aupdates%2C%20TempSamp-R1%20provides%20a%20non-linear%20soft%20advantage%20computation%20method%0Athat%20dynamically%20reshapes%20the%20reward%20feedback%20via%20an%20asymmetric%20transformation.%0ABy%20employing%20a%20hybrid%20Chain-of-Thought%20%28CoT%29%20training%20paradigm%2C%20TempSamp-R1%0Aoptimizes%20a%20single%20unified%20model%20to%20support%20both%20CoT%20and%20non-CoT%20inference%0Amodes%2C%20enabling%20efficient%20handling%20of%20queries%20with%20varying%20reasoning%0Acomplexity.%20Experimental%20results%20demonstrate%20that%20TempSamp-R1%20outperforms%0AGRPO-based%20baselines%2C%20establishing%20new%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%3A%20Charades-STA%20%28R1%400.7%3A%2052.9%25%2C%20%2B2.7%25%29%2C%20ActivityNet%20Captions%0A%28R1%400.5%3A%2056.0%25%2C%20%2B5.3%25%29%2C%20and%20QVHighlights%20%28mAP%3A%2030.0%25%2C%20%2B3.0%25%29.%20Moreover%2C%0ATempSamp-R1%20shows%20robust%20few-shot%20generalization%20capabilities%20under%20limited%0Adata.%20Code%3A%20https%3A//github.com/HVision-NKU/TempSamp-R1%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18056v2&entry.124074799=Read"},
{"title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "author": "Dapeng Zhang and Jing Sun and Chenghui Hu and Xiaoyan Wu and Zhenlong Yuan and Rui Zhou and Fei Shen and Qingguo Zhou", "abstract": "  The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.\n", "link": "http://arxiv.org/abs/2509.19012v2", "date": "2025-09-25", "relevancy": 2.3163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Dapeng%20Zhang%20and%20Jing%20Sun%20and%20Chenghui%20Hu%20and%20Xiaoyan%20Wu%20and%20Zhenlong%20Yuan%20and%20Rui%20Zhou%20and%20Fei%20Shen%20and%20Qingguo%20Zhou%0AAbstract%3A%20%20%20The%20emergence%20of%20Vision%20Language%20Action%20%28VLA%29%20models%20marks%20a%20paradigm%20shift%0Afrom%20traditional%20policy-based%20control%20to%20generalized%20robotics%2C%20reframing%20Vision%0ALanguage%20Models%20%28VLMs%29%20from%20passive%20sequence%20generators%20into%20active%20agents%20for%0Amanipulation%20and%20decision-making%20in%20complex%2C%20dynamic%20environments.%20This%20survey%0Adelves%20into%20advanced%20VLA%20methods%2C%20aiming%20to%20provide%20a%20clear%20taxonomy%20and%20a%0Asystematic%2C%20comprehensive%20review%20of%20existing%20research.%20It%20presents%20a%0Acomprehensive%20analysis%20of%20VLA%20applications%20across%20different%20scenarios%20and%0Aclassifies%20VLA%20approaches%20into%20several%20paradigms%3A%20autoregression-based%2C%0Adiffusion-based%2C%20reinforcement-based%2C%20hybrid%2C%20and%20specialized%20methods%3B%20while%0Aexamining%20their%20motivations%2C%20core%20strategies%2C%20and%20implementations%20in%20detail.%20In%0Aaddition%2C%20foundational%20datasets%2C%20benchmarks%2C%20and%20simulation%20platforms%20are%0Aintroduced.%20Building%20on%20the%20current%20VLA%20landscape%2C%20the%20review%20further%20proposes%0Aperspectives%20on%20key%20challenges%20and%20future%20directions%20to%20advance%20research%20in%20VLA%0Amodels%20and%20generalizable%20robotics.%20By%20synthesizing%20insights%20from%20over%20three%0Ahundred%20recent%20studies%2C%20this%20survey%20maps%20the%20contours%20of%20this%20rapidly%20evolving%0Afield%20and%20highlights%20the%20opportunities%20and%20challenges%20that%20will%20shape%20the%0Adevelopment%20of%20scalable%2C%20general-purpose%20VLA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPure%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520Models%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DDapeng%2520Zhang%2520and%2520Jing%2520Sun%2520and%2520Chenghui%2520Hu%2520and%2520Xiaoyan%2520Wu%2520and%2520Zhenlong%2520Yuan%2520and%2520Rui%2520Zhou%2520and%2520Fei%2520Shen%2520and%2520Qingguo%2520Zhou%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520models%2520marks%2520a%2520paradigm%2520shift%250Afrom%2520traditional%2520policy-based%2520control%2520to%2520generalized%2520robotics%252C%2520reframing%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520from%2520passive%2520sequence%2520generators%2520into%2520active%2520agents%2520for%250Amanipulation%2520and%2520decision-making%2520in%2520complex%252C%2520dynamic%2520environments.%2520This%2520survey%250Adelves%2520into%2520advanced%2520VLA%2520methods%252C%2520aiming%2520to%2520provide%2520a%2520clear%2520taxonomy%2520and%2520a%250Asystematic%252C%2520comprehensive%2520review%2520of%2520existing%2520research.%2520It%2520presents%2520a%250Acomprehensive%2520analysis%2520of%2520VLA%2520applications%2520across%2520different%2520scenarios%2520and%250Aclassifies%2520VLA%2520approaches%2520into%2520several%2520paradigms%253A%2520autoregression-based%252C%250Adiffusion-based%252C%2520reinforcement-based%252C%2520hybrid%252C%2520and%2520specialized%2520methods%253B%2520while%250Aexamining%2520their%2520motivations%252C%2520core%2520strategies%252C%2520and%2520implementations%2520in%2520detail.%2520In%250Aaddition%252C%2520foundational%2520datasets%252C%2520benchmarks%252C%2520and%2520simulation%2520platforms%2520are%250Aintroduced.%2520Building%2520on%2520the%2520current%2520VLA%2520landscape%252C%2520the%2520review%2520further%2520proposes%250Aperspectives%2520on%2520key%2520challenges%2520and%2520future%2520directions%2520to%2520advance%2520research%2520in%2520VLA%250Amodels%2520and%2520generalizable%2520robotics.%2520By%2520synthesizing%2520insights%2520from%2520over%2520three%250Ahundred%2520recent%2520studies%252C%2520this%2520survey%2520maps%2520the%2520contours%2520of%2520this%2520rapidly%2520evolving%250Afield%2520and%2520highlights%2520the%2520opportunities%2520and%2520challenges%2520that%2520will%2520shape%2520the%250Adevelopment%2520of%2520scalable%252C%2520general-purpose%2520VLA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Dapeng%20Zhang%20and%20Jing%20Sun%20and%20Chenghui%20Hu%20and%20Xiaoyan%20Wu%20and%20Zhenlong%20Yuan%20and%20Rui%20Zhou%20and%20Fei%20Shen%20and%20Qingguo%20Zhou&entry.1292438233=%20%20The%20emergence%20of%20Vision%20Language%20Action%20%28VLA%29%20models%20marks%20a%20paradigm%20shift%0Afrom%20traditional%20policy-based%20control%20to%20generalized%20robotics%2C%20reframing%20Vision%0ALanguage%20Models%20%28VLMs%29%20from%20passive%20sequence%20generators%20into%20active%20agents%20for%0Amanipulation%20and%20decision-making%20in%20complex%2C%20dynamic%20environments.%20This%20survey%0Adelves%20into%20advanced%20VLA%20methods%2C%20aiming%20to%20provide%20a%20clear%20taxonomy%20and%20a%0Asystematic%2C%20comprehensive%20review%20of%20existing%20research.%20It%20presents%20a%0Acomprehensive%20analysis%20of%20VLA%20applications%20across%20different%20scenarios%20and%0Aclassifies%20VLA%20approaches%20into%20several%20paradigms%3A%20autoregression-based%2C%0Adiffusion-based%2C%20reinforcement-based%2C%20hybrid%2C%20and%20specialized%20methods%3B%20while%0Aexamining%20their%20motivations%2C%20core%20strategies%2C%20and%20implementations%20in%20detail.%20In%0Aaddition%2C%20foundational%20datasets%2C%20benchmarks%2C%20and%20simulation%20platforms%20are%0Aintroduced.%20Building%20on%20the%20current%20VLA%20landscape%2C%20the%20review%20further%20proposes%0Aperspectives%20on%20key%20challenges%20and%20future%20directions%20to%20advance%20research%20in%20VLA%0Amodels%20and%20generalizable%20robotics.%20By%20synthesizing%20insights%20from%20over%20three%0Ahundred%20recent%20studies%2C%20this%20survey%20maps%20the%20contours%20of%20this%20rapidly%20evolving%0Afield%20and%20highlights%20the%20opportunities%20and%20challenges%20that%20will%20shape%20the%0Adevelopment%20of%20scalable%2C%20general-purpose%20VLA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19012v2&entry.124074799=Read"},
{"title": "Expressiveness of Multi-Neuron Convex Relaxations in Neural Network\n  Certification", "author": "Yuhao Mao and Yani Zhang and Martin Vechev", "abstract": "  Neural network certification methods heavily rely on convex relaxations to\nprovide robustness guarantees. However, these relaxations are often imprecise:\neven the most accurate single-neuron relaxation is incomplete for general ReLU\nnetworks, a limitation known as the \\emph{single-neuron convex barrier}. While\nmulti-neuron relaxations have been heuristically applied to address this issue,\ntwo central questions arise: (i) whether they overcome the convex barrier, and\nif not, (ii) whether they offer theoretical capabilities beyond those of\nsingle-neuron relaxations. In this work, we present the first rigorous analysis\nof the expressiveness of multi-neuron relaxations. Perhaps surprisingly, we\nshow that they are inherently incomplete, even when allocated sufficient\nresources to capture finitely many neurons and layers optimally. This result\nextends the single-neuron barrier to a \\textit{universal convex barrier} for\nneural network certification. On the positive side, we show that completeness\ncan be achieved by either (i) augmenting the network with a polynomial number\nof carefully designed ReLU neurons or (ii) partitioning the input domain into\nconvex sub-polytopes, thereby distinguishing multi-neuron relaxations from\nsingle-neuron ones which are unable to realize the former and have worse\npartition complexity for the latter. Our findings establish a foundation for\nmulti-neuron relaxations and point to new directions for certified robustness,\nincluding training methods tailored to multi-neuron relaxations and\nverification methods with multi-neuron relaxations as the main subroutine.\n", "link": "http://arxiv.org/abs/2410.06816v3", "date": "2025-09-25", "relevancy": 2.3114, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expressiveness%20of%20Multi-Neuron%20Convex%20Relaxations%20in%20Neural%20Network%0A%20%20Certification&body=Title%3A%20Expressiveness%20of%20Multi-Neuron%20Convex%20Relaxations%20in%20Neural%20Network%0A%20%20Certification%0AAuthor%3A%20Yuhao%20Mao%20and%20Yani%20Zhang%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Neural%20network%20certification%20methods%20heavily%20rely%20on%20convex%20relaxations%20to%0Aprovide%20robustness%20guarantees.%20However%2C%20these%20relaxations%20are%20often%20imprecise%3A%0Aeven%20the%20most%20accurate%20single-neuron%20relaxation%20is%20incomplete%20for%20general%20ReLU%0Anetworks%2C%20a%20limitation%20known%20as%20the%20%5Cemph%7Bsingle-neuron%20convex%20barrier%7D.%20While%0Amulti-neuron%20relaxations%20have%20been%20heuristically%20applied%20to%20address%20this%20issue%2C%0Atwo%20central%20questions%20arise%3A%20%28i%29%20whether%20they%20overcome%20the%20convex%20barrier%2C%20and%0Aif%20not%2C%20%28ii%29%20whether%20they%20offer%20theoretical%20capabilities%20beyond%20those%20of%0Asingle-neuron%20relaxations.%20In%20this%20work%2C%20we%20present%20the%20first%20rigorous%20analysis%0Aof%20the%20expressiveness%20of%20multi-neuron%20relaxations.%20Perhaps%20surprisingly%2C%20we%0Ashow%20that%20they%20are%20inherently%20incomplete%2C%20even%20when%20allocated%20sufficient%0Aresources%20to%20capture%20finitely%20many%20neurons%20and%20layers%20optimally.%20This%20result%0Aextends%20the%20single-neuron%20barrier%20to%20a%20%5Ctextit%7Buniversal%20convex%20barrier%7D%20for%0Aneural%20network%20certification.%20On%20the%20positive%20side%2C%20we%20show%20that%20completeness%0Acan%20be%20achieved%20by%20either%20%28i%29%20augmenting%20the%20network%20with%20a%20polynomial%20number%0Aof%20carefully%20designed%20ReLU%20neurons%20or%20%28ii%29%20partitioning%20the%20input%20domain%20into%0Aconvex%20sub-polytopes%2C%20thereby%20distinguishing%20multi-neuron%20relaxations%20from%0Asingle-neuron%20ones%20which%20are%20unable%20to%20realize%20the%20former%20and%20have%20worse%0Apartition%20complexity%20for%20the%20latter.%20Our%20findings%20establish%20a%20foundation%20for%0Amulti-neuron%20relaxations%20and%20point%20to%20new%20directions%20for%20certified%20robustness%2C%0Aincluding%20training%20methods%20tailored%20to%20multi-neuron%20relaxations%20and%0Averification%20methods%20with%20multi-neuron%20relaxations%20as%20the%20main%20subroutine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06816v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpressiveness%2520of%2520Multi-Neuron%2520Convex%2520Relaxations%2520in%2520Neural%2520Network%250A%2520%2520Certification%26entry.906535625%3DYuhao%2520Mao%2520and%2520Yani%2520Zhang%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Neural%2520network%2520certification%2520methods%2520heavily%2520rely%2520on%2520convex%2520relaxations%2520to%250Aprovide%2520robustness%2520guarantees.%2520However%252C%2520these%2520relaxations%2520are%2520often%2520imprecise%253A%250Aeven%2520the%2520most%2520accurate%2520single-neuron%2520relaxation%2520is%2520incomplete%2520for%2520general%2520ReLU%250Anetworks%252C%2520a%2520limitation%2520known%2520as%2520the%2520%255Cemph%257Bsingle-neuron%2520convex%2520barrier%257D.%2520While%250Amulti-neuron%2520relaxations%2520have%2520been%2520heuristically%2520applied%2520to%2520address%2520this%2520issue%252C%250Atwo%2520central%2520questions%2520arise%253A%2520%2528i%2529%2520whether%2520they%2520overcome%2520the%2520convex%2520barrier%252C%2520and%250Aif%2520not%252C%2520%2528ii%2529%2520whether%2520they%2520offer%2520theoretical%2520capabilities%2520beyond%2520those%2520of%250Asingle-neuron%2520relaxations.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520rigorous%2520analysis%250Aof%2520the%2520expressiveness%2520of%2520multi-neuron%2520relaxations.%2520Perhaps%2520surprisingly%252C%2520we%250Ashow%2520that%2520they%2520are%2520inherently%2520incomplete%252C%2520even%2520when%2520allocated%2520sufficient%250Aresources%2520to%2520capture%2520finitely%2520many%2520neurons%2520and%2520layers%2520optimally.%2520This%2520result%250Aextends%2520the%2520single-neuron%2520barrier%2520to%2520a%2520%255Ctextit%257Buniversal%2520convex%2520barrier%257D%2520for%250Aneural%2520network%2520certification.%2520On%2520the%2520positive%2520side%252C%2520we%2520show%2520that%2520completeness%250Acan%2520be%2520achieved%2520by%2520either%2520%2528i%2529%2520augmenting%2520the%2520network%2520with%2520a%2520polynomial%2520number%250Aof%2520carefully%2520designed%2520ReLU%2520neurons%2520or%2520%2528ii%2529%2520partitioning%2520the%2520input%2520domain%2520into%250Aconvex%2520sub-polytopes%252C%2520thereby%2520distinguishing%2520multi-neuron%2520relaxations%2520from%250Asingle-neuron%2520ones%2520which%2520are%2520unable%2520to%2520realize%2520the%2520former%2520and%2520have%2520worse%250Apartition%2520complexity%2520for%2520the%2520latter.%2520Our%2520findings%2520establish%2520a%2520foundation%2520for%250Amulti-neuron%2520relaxations%2520and%2520point%2520to%2520new%2520directions%2520for%2520certified%2520robustness%252C%250Aincluding%2520training%2520methods%2520tailored%2520to%2520multi-neuron%2520relaxations%2520and%250Averification%2520methods%2520with%2520multi-neuron%2520relaxations%2520as%2520the%2520main%2520subroutine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06816v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expressiveness%20of%20Multi-Neuron%20Convex%20Relaxations%20in%20Neural%20Network%0A%20%20Certification&entry.906535625=Yuhao%20Mao%20and%20Yani%20Zhang%20and%20Martin%20Vechev&entry.1292438233=%20%20Neural%20network%20certification%20methods%20heavily%20rely%20on%20convex%20relaxations%20to%0Aprovide%20robustness%20guarantees.%20However%2C%20these%20relaxations%20are%20often%20imprecise%3A%0Aeven%20the%20most%20accurate%20single-neuron%20relaxation%20is%20incomplete%20for%20general%20ReLU%0Anetworks%2C%20a%20limitation%20known%20as%20the%20%5Cemph%7Bsingle-neuron%20convex%20barrier%7D.%20While%0Amulti-neuron%20relaxations%20have%20been%20heuristically%20applied%20to%20address%20this%20issue%2C%0Atwo%20central%20questions%20arise%3A%20%28i%29%20whether%20they%20overcome%20the%20convex%20barrier%2C%20and%0Aif%20not%2C%20%28ii%29%20whether%20they%20offer%20theoretical%20capabilities%20beyond%20those%20of%0Asingle-neuron%20relaxations.%20In%20this%20work%2C%20we%20present%20the%20first%20rigorous%20analysis%0Aof%20the%20expressiveness%20of%20multi-neuron%20relaxations.%20Perhaps%20surprisingly%2C%20we%0Ashow%20that%20they%20are%20inherently%20incomplete%2C%20even%20when%20allocated%20sufficient%0Aresources%20to%20capture%20finitely%20many%20neurons%20and%20layers%20optimally.%20This%20result%0Aextends%20the%20single-neuron%20barrier%20to%20a%20%5Ctextit%7Buniversal%20convex%20barrier%7D%20for%0Aneural%20network%20certification.%20On%20the%20positive%20side%2C%20we%20show%20that%20completeness%0Acan%20be%20achieved%20by%20either%20%28i%29%20augmenting%20the%20network%20with%20a%20polynomial%20number%0Aof%20carefully%20designed%20ReLU%20neurons%20or%20%28ii%29%20partitioning%20the%20input%20domain%20into%0Aconvex%20sub-polytopes%2C%20thereby%20distinguishing%20multi-neuron%20relaxations%20from%0Asingle-neuron%20ones%20which%20are%20unable%20to%20realize%20the%20former%20and%20have%20worse%0Apartition%20complexity%20for%20the%20latter.%20Our%20findings%20establish%20a%20foundation%20for%0Amulti-neuron%20relaxations%20and%20point%20to%20new%20directions%20for%20certified%20robustness%2C%0Aincluding%20training%20methods%20tailored%20to%20multi-neuron%20relaxations%20and%0Averification%20methods%20with%20multi-neuron%20relaxations%20as%20the%20main%20subroutine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06816v3&entry.124074799=Read"},
{"title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection", "author": "Yidan Zhang and Mutian Xu and Yiming Hao and Kun Zhou and Jiahao Chang and Xiaoqiang Liu and Pengfei Wan and Hongbo Fu and Xiaoguang Han", "abstract": "  Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/.\n", "link": "http://arxiv.org/abs/2509.21291v1", "date": "2025-09-25", "relevancy": 2.2911, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5818}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5699}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VC-Agent%3A%20An%20Interactive%20Agent%20for%20Customized%20Video%20Dataset%20Collection&body=Title%3A%20VC-Agent%3A%20An%20Interactive%20Agent%20for%20Customized%20Video%20Dataset%20Collection%0AAuthor%3A%20Yidan%20Zhang%20and%20Mutian%20Xu%20and%20Yiming%20Hao%20and%20Kun%20Zhou%20and%20Jiahao%20Chang%20and%20Xiaoqiang%20Liu%20and%20Pengfei%20Wan%20and%20Hongbo%20Fu%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20Facing%20scaling%20laws%2C%20video%20data%20from%20the%20internet%20becomes%20increasingly%0Aimportant.%20However%2C%20collecting%20extensive%20videos%20that%20meet%20specific%20needs%20is%0Aextremely%20labor-intensive%20and%20time-consuming.%20In%20this%20work%2C%20we%20study%20the%20way%20to%0Aexpedite%20this%20collection%20process%20and%20propose%20VC-Agent%2C%20the%20first%20interactive%0Aagent%20that%20is%20able%20to%20understand%20users%27%20queries%20and%20feedback%2C%20and%20accordingly%0Aretrieve/scale%20up%20relevant%20video%20clips%20with%20minimal%20user%20input.%20Specifically%2C%0Aconsidering%20the%20user%20interface%2C%20our%20agent%20defines%20various%20user-friendly%20ways%0Afor%20the%20user%20to%20specify%20requirements%20based%20on%20textual%20descriptions%20and%0Aconfirmations.%20As%20for%20agent%20functions%2C%20we%20leverage%20existing%20multi-modal%20large%0Alanguage%20models%20to%20connect%20the%20user%27s%20requirements%20with%20the%20video%20content.%20More%0Aimportantly%2C%20we%20propose%20two%20novel%20filtering%20policies%20that%20can%20be%20updated%20when%0Auser%20interaction%20is%20continually%20performed.%20Finally%2C%20we%20provide%20a%20new%20benchmark%0Afor%20personalized%20video%20dataset%20collection%2C%20and%20carefully%20conduct%20the%20user%20study%0Ato%20verify%20our%20agent%27s%20usage%20in%20various%20real%20scenarios.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20agent%20for%20customized%20video%0Adataset%20collection.%20Project%20page%3A%20https%3A//allenyidan.github.io/vcagent_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVC-Agent%253A%2520An%2520Interactive%2520Agent%2520for%2520Customized%2520Video%2520Dataset%2520Collection%26entry.906535625%3DYidan%2520Zhang%2520and%2520Mutian%2520Xu%2520and%2520Yiming%2520Hao%2520and%2520Kun%2520Zhou%2520and%2520Jiahao%2520Chang%2520and%2520Xiaoqiang%2520Liu%2520and%2520Pengfei%2520Wan%2520and%2520Hongbo%2520Fu%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520Facing%2520scaling%2520laws%252C%2520video%2520data%2520from%2520the%2520internet%2520becomes%2520increasingly%250Aimportant.%2520However%252C%2520collecting%2520extensive%2520videos%2520that%2520meet%2520specific%2520needs%2520is%250Aextremely%2520labor-intensive%2520and%2520time-consuming.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520way%2520to%250Aexpedite%2520this%2520collection%2520process%2520and%2520propose%2520VC-Agent%252C%2520the%2520first%2520interactive%250Aagent%2520that%2520is%2520able%2520to%2520understand%2520users%2527%2520queries%2520and%2520feedback%252C%2520and%2520accordingly%250Aretrieve/scale%2520up%2520relevant%2520video%2520clips%2520with%2520minimal%2520user%2520input.%2520Specifically%252C%250Aconsidering%2520the%2520user%2520interface%252C%2520our%2520agent%2520defines%2520various%2520user-friendly%2520ways%250Afor%2520the%2520user%2520to%2520specify%2520requirements%2520based%2520on%2520textual%2520descriptions%2520and%250Aconfirmations.%2520As%2520for%2520agent%2520functions%252C%2520we%2520leverage%2520existing%2520multi-modal%2520large%250Alanguage%2520models%2520to%2520connect%2520the%2520user%2527s%2520requirements%2520with%2520the%2520video%2520content.%2520More%250Aimportantly%252C%2520we%2520propose%2520two%2520novel%2520filtering%2520policies%2520that%2520can%2520be%2520updated%2520when%250Auser%2520interaction%2520is%2520continually%2520performed.%2520Finally%252C%2520we%2520provide%2520a%2520new%2520benchmark%250Afor%2520personalized%2520video%2520dataset%2520collection%252C%2520and%2520carefully%2520conduct%2520the%2520user%2520study%250Ato%2520verify%2520our%2520agent%2527s%2520usage%2520in%2520various%2520real%2520scenarios.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520agent%2520for%2520customized%2520video%250Adataset%2520collection.%2520Project%2520page%253A%2520https%253A//allenyidan.github.io/vcagent_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VC-Agent%3A%20An%20Interactive%20Agent%20for%20Customized%20Video%20Dataset%20Collection&entry.906535625=Yidan%20Zhang%20and%20Mutian%20Xu%20and%20Yiming%20Hao%20and%20Kun%20Zhou%20and%20Jiahao%20Chang%20and%20Xiaoqiang%20Liu%20and%20Pengfei%20Wan%20and%20Hongbo%20Fu%20and%20Xiaoguang%20Han&entry.1292438233=%20%20Facing%20scaling%20laws%2C%20video%20data%20from%20the%20internet%20becomes%20increasingly%0Aimportant.%20However%2C%20collecting%20extensive%20videos%20that%20meet%20specific%20needs%20is%0Aextremely%20labor-intensive%20and%20time-consuming.%20In%20this%20work%2C%20we%20study%20the%20way%20to%0Aexpedite%20this%20collection%20process%20and%20propose%20VC-Agent%2C%20the%20first%20interactive%0Aagent%20that%20is%20able%20to%20understand%20users%27%20queries%20and%20feedback%2C%20and%20accordingly%0Aretrieve/scale%20up%20relevant%20video%20clips%20with%20minimal%20user%20input.%20Specifically%2C%0Aconsidering%20the%20user%20interface%2C%20our%20agent%20defines%20various%20user-friendly%20ways%0Afor%20the%20user%20to%20specify%20requirements%20based%20on%20textual%20descriptions%20and%0Aconfirmations.%20As%20for%20agent%20functions%2C%20we%20leverage%20existing%20multi-modal%20large%0Alanguage%20models%20to%20connect%20the%20user%27s%20requirements%20with%20the%20video%20content.%20More%0Aimportantly%2C%20we%20propose%20two%20novel%20filtering%20policies%20that%20can%20be%20updated%20when%0Auser%20interaction%20is%20continually%20performed.%20Finally%2C%20we%20provide%20a%20new%20benchmark%0Afor%20personalized%20video%20dataset%20collection%2C%20and%20carefully%20conduct%20the%20user%20study%0Ato%20verify%20our%20agent%27s%20usage%20in%20various%20real%20scenarios.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20agent%20for%20customized%20video%0Adataset%20collection.%20Project%20page%3A%20https%3A//allenyidan.github.io/vcagent_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21291v1&entry.124074799=Read"},
{"title": "UNO: Unlearning via Orthogonalization in Generative models", "author": "Pinak Mandal and Georg A. Gottwald", "abstract": "  As generative models become increasingly powerful and pervasive, the ability\nto unlearn specific data, whether due to privacy concerns, legal requirements,\nor the correction of harmful content, has become increasingly important. Unlike\nin conventional training, where data are accumulated and knowledge is\nreinforced, unlearning aims to selectively remove the influence of particular\ndata points without costly retraining from scratch. To be effective and\nreliable, such algorithms need to achieve (i) forgetting of the undesired data,\n(ii) preservation of the quality of the generation, (iii) preservation of the\ninfluence of the desired training data on the model parameters, and (iv) small\nnumber of training steps. We propose fast unlearning algorithms based on loss\ngradient orthogonalization for unconditional and conditional generative models.\nWe show that our algorithms are able to forget data while maintaining the\nfidelity of the original model. On standard image benchmarks, our algorithms\nachieve orders of magnitude faster unlearning times than their predecessors,\nsuch as gradient surgery. We demonstrate our algorithms with datasets of\nincreasing complexity (MNIST, CelebA and ImageNet-1K) and for generative models\nof increasing complexity (VAEs and diffusion transformers).\n", "link": "http://arxiv.org/abs/2506.04712v2", "date": "2025-09-25", "relevancy": 2.2882, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5866}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5783}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNO%3A%20Unlearning%20via%20Orthogonalization%20in%20Generative%20models&body=Title%3A%20UNO%3A%20Unlearning%20via%20Orthogonalization%20in%20Generative%20models%0AAuthor%3A%20Pinak%20Mandal%20and%20Georg%20A.%20Gottwald%0AAbstract%3A%20%20%20As%20generative%20models%20become%20increasingly%20powerful%20and%20pervasive%2C%20the%20ability%0Ato%20unlearn%20specific%20data%2C%20whether%20due%20to%20privacy%20concerns%2C%20legal%20requirements%2C%0Aor%20the%20correction%20of%20harmful%20content%2C%20has%20become%20increasingly%20important.%20Unlike%0Ain%20conventional%20training%2C%20where%20data%20are%20accumulated%20and%20knowledge%20is%0Areinforced%2C%20unlearning%20aims%20to%20selectively%20remove%20the%20influence%20of%20particular%0Adata%20points%20without%20costly%20retraining%20from%20scratch.%20To%20be%20effective%20and%0Areliable%2C%20such%20algorithms%20need%20to%20achieve%20%28i%29%20forgetting%20of%20the%20undesired%20data%2C%0A%28ii%29%20preservation%20of%20the%20quality%20of%20the%20generation%2C%20%28iii%29%20preservation%20of%20the%0Ainfluence%20of%20the%20desired%20training%20data%20on%20the%20model%20parameters%2C%20and%20%28iv%29%20small%0Anumber%20of%20training%20steps.%20We%20propose%20fast%20unlearning%20algorithms%20based%20on%20loss%0Agradient%20orthogonalization%20for%20unconditional%20and%20conditional%20generative%20models.%0AWe%20show%20that%20our%20algorithms%20are%20able%20to%20forget%20data%20while%20maintaining%20the%0Afidelity%20of%20the%20original%20model.%20On%20standard%20image%20benchmarks%2C%20our%20algorithms%0Aachieve%20orders%20of%20magnitude%20faster%20unlearning%20times%20than%20their%20predecessors%2C%0Asuch%20as%20gradient%20surgery.%20We%20demonstrate%20our%20algorithms%20with%20datasets%20of%0Aincreasing%20complexity%20%28MNIST%2C%20CelebA%20and%20ImageNet-1K%29%20and%20for%20generative%20models%0Aof%20increasing%20complexity%20%28VAEs%20and%20diffusion%20transformers%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNO%253A%2520Unlearning%2520via%2520Orthogonalization%2520in%2520Generative%2520models%26entry.906535625%3DPinak%2520Mandal%2520and%2520Georg%2520A.%2520Gottwald%26entry.1292438233%3D%2520%2520As%2520generative%2520models%2520become%2520increasingly%2520powerful%2520and%2520pervasive%252C%2520the%2520ability%250Ato%2520unlearn%2520specific%2520data%252C%2520whether%2520due%2520to%2520privacy%2520concerns%252C%2520legal%2520requirements%252C%250Aor%2520the%2520correction%2520of%2520harmful%2520content%252C%2520has%2520become%2520increasingly%2520important.%2520Unlike%250Ain%2520conventional%2520training%252C%2520where%2520data%2520are%2520accumulated%2520and%2520knowledge%2520is%250Areinforced%252C%2520unlearning%2520aims%2520to%2520selectively%2520remove%2520the%2520influence%2520of%2520particular%250Adata%2520points%2520without%2520costly%2520retraining%2520from%2520scratch.%2520To%2520be%2520effective%2520and%250Areliable%252C%2520such%2520algorithms%2520need%2520to%2520achieve%2520%2528i%2529%2520forgetting%2520of%2520the%2520undesired%2520data%252C%250A%2528ii%2529%2520preservation%2520of%2520the%2520quality%2520of%2520the%2520generation%252C%2520%2528iii%2529%2520preservation%2520of%2520the%250Ainfluence%2520of%2520the%2520desired%2520training%2520data%2520on%2520the%2520model%2520parameters%252C%2520and%2520%2528iv%2529%2520small%250Anumber%2520of%2520training%2520steps.%2520We%2520propose%2520fast%2520unlearning%2520algorithms%2520based%2520on%2520loss%250Agradient%2520orthogonalization%2520for%2520unconditional%2520and%2520conditional%2520generative%2520models.%250AWe%2520show%2520that%2520our%2520algorithms%2520are%2520able%2520to%2520forget%2520data%2520while%2520maintaining%2520the%250Afidelity%2520of%2520the%2520original%2520model.%2520On%2520standard%2520image%2520benchmarks%252C%2520our%2520algorithms%250Aachieve%2520orders%2520of%2520magnitude%2520faster%2520unlearning%2520times%2520than%2520their%2520predecessors%252C%250Asuch%2520as%2520gradient%2520surgery.%2520We%2520demonstrate%2520our%2520algorithms%2520with%2520datasets%2520of%250Aincreasing%2520complexity%2520%2528MNIST%252C%2520CelebA%2520and%2520ImageNet-1K%2529%2520and%2520for%2520generative%2520models%250Aof%2520increasing%2520complexity%2520%2528VAEs%2520and%2520diffusion%2520transformers%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNO%3A%20Unlearning%20via%20Orthogonalization%20in%20Generative%20models&entry.906535625=Pinak%20Mandal%20and%20Georg%20A.%20Gottwald&entry.1292438233=%20%20As%20generative%20models%20become%20increasingly%20powerful%20and%20pervasive%2C%20the%20ability%0Ato%20unlearn%20specific%20data%2C%20whether%20due%20to%20privacy%20concerns%2C%20legal%20requirements%2C%0Aor%20the%20correction%20of%20harmful%20content%2C%20has%20become%20increasingly%20important.%20Unlike%0Ain%20conventional%20training%2C%20where%20data%20are%20accumulated%20and%20knowledge%20is%0Areinforced%2C%20unlearning%20aims%20to%20selectively%20remove%20the%20influence%20of%20particular%0Adata%20points%20without%20costly%20retraining%20from%20scratch.%20To%20be%20effective%20and%0Areliable%2C%20such%20algorithms%20need%20to%20achieve%20%28i%29%20forgetting%20of%20the%20undesired%20data%2C%0A%28ii%29%20preservation%20of%20the%20quality%20of%20the%20generation%2C%20%28iii%29%20preservation%20of%20the%0Ainfluence%20of%20the%20desired%20training%20data%20on%20the%20model%20parameters%2C%20and%20%28iv%29%20small%0Anumber%20of%20training%20steps.%20We%20propose%20fast%20unlearning%20algorithms%20based%20on%20loss%0Agradient%20orthogonalization%20for%20unconditional%20and%20conditional%20generative%20models.%0AWe%20show%20that%20our%20algorithms%20are%20able%20to%20forget%20data%20while%20maintaining%20the%0Afidelity%20of%20the%20original%20model.%20On%20standard%20image%20benchmarks%2C%20our%20algorithms%0Aachieve%20orders%20of%20magnitude%20faster%20unlearning%20times%20than%20their%20predecessors%2C%0Asuch%20as%20gradient%20surgery.%20We%20demonstrate%20our%20algorithms%20with%20datasets%20of%0Aincreasing%20complexity%20%28MNIST%2C%20CelebA%20and%20ImageNet-1K%29%20and%20for%20generative%20models%0Aof%20increasing%20complexity%20%28VAEs%20and%20diffusion%20transformers%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04712v2&entry.124074799=Read"},
{"title": "Compositional-ARC: Assessing Systematic Generalization in Abstract\n  Spatial Reasoning", "author": "Philipp Mondorf and Shijia Zhou and Monica Riedler and Barbara Plank", "abstract": "  Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend\nmeta-learning for compositionality to the domain of abstract spatial reasoning.\nTo this end, we introduce $\\textit{Compositional-ARC}-$a dataset designed to\nevaluate the capacity of models to systematically generalize from known\ngeometric transformations (e.g., translation, rotation) of abstract\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a small transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions.\nNotably, despite having only 5.7M parameters, this model significantly\noutperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0\nFlash, which fail to exhibit similar systematic behavior$-$and performs on par\nwith the winning model of the ARC prize 2024, an 8B-parameter LLM trained via\ntest-time training. Our findings highlight the effectiveness of meta-learning\nin promoting systematicity beyond linguistic tasks, suggesting a promising\ndirection toward more robust and generalizable models.\n", "link": "http://arxiv.org/abs/2504.01445v2", "date": "2025-09-25", "relevancy": 2.2653, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5829}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional-ARC%3A%20Assessing%20Systematic%20Generalization%20in%20Abstract%0A%20%20Spatial%20Reasoning&body=Title%3A%20Compositional-ARC%3A%20Assessing%20Systematic%20Generalization%20in%20Abstract%0A%20%20Spatial%20Reasoning%0AAuthor%3A%20Philipp%20Mondorf%20and%20Shijia%20Zhou%20and%20Monica%20Riedler%20and%20Barbara%20Plank%0AAbstract%3A%20%20%20Systematic%20generalization%20refers%20to%20the%20capacity%20to%20understand%20and%20generate%0Anovel%20combinations%20from%20known%20components.%20Despite%20recent%20progress%20by%20large%0Alanguage%20models%20%28LLMs%29%20across%20various%20domains%2C%20these%20models%20often%20fail%20to%0Aextend%20their%20knowledge%20to%20novel%20compositional%20scenarios%2C%20revealing%20notable%0Alimitations%20in%20systematic%20generalization.%20There%20has%20been%20an%20ongoing%20debate%0Aabout%20whether%20neural%20networks%20possess%20the%20capacity%20for%20systematic%0Ageneralization%2C%20with%20recent%20studies%20suggesting%20that%20meta-learning%20approaches%0Adesigned%20for%20compositionality%20can%20significantly%20enhance%20this%20ability.%20However%2C%0Athese%20insights%20have%20largely%20been%20confined%20to%20linguistic%20problems%2C%20leaving%20their%0Aapplicability%20to%20other%20tasks%20an%20open%20question.%20In%20this%20study%2C%20we%20extend%0Ameta-learning%20for%20compositionality%20to%20the%20domain%20of%20abstract%20spatial%20reasoning.%0ATo%20this%20end%2C%20we%20introduce%20%24%5Ctextit%7BCompositional-ARC%7D-%24a%20dataset%20designed%20to%0Aevaluate%20the%20capacity%20of%20models%20to%20systematically%20generalize%20from%20known%0Ageometric%20transformations%20%28e.g.%2C%20translation%2C%20rotation%29%20of%20abstract%0Atwo-dimensional%20objects%20to%20novel%20combinations%20of%20these%20transformations%20%28e.g.%2C%0Atranslation%2Brotation%29.%20Our%20results%20show%20that%20a%20small%20transformer-based%0Aencoder-decoder%20model%2C%20trained%20via%20meta-learning%20for%20compositionality%2C%20can%0Asystematically%20generalize%20to%20previously%20unseen%20transformation%20compositions.%0ANotably%2C%20despite%20having%20only%205.7M%20parameters%2C%20this%20model%20significantly%0Aoutperforms%20state-of-the-art%20LLMs%24-%24including%20o3-mini%2C%20GPT-4o%2C%20and%20Gemini%202.0%0AFlash%2C%20which%20fail%20to%20exhibit%20similar%20systematic%20behavior%24-%24and%20performs%20on%20par%0Awith%20the%20winning%20model%20of%20the%20ARC%20prize%202024%2C%20an%208B-parameter%20LLM%20trained%20via%0Atest-time%20training.%20Our%20findings%20highlight%20the%20effectiveness%20of%20meta-learning%0Ain%20promoting%20systematicity%20beyond%20linguistic%20tasks%2C%20suggesting%20a%20promising%0Adirection%20toward%20more%20robust%20and%20generalizable%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional-ARC%253A%2520Assessing%2520Systematic%2520Generalization%2520in%2520Abstract%250A%2520%2520Spatial%2520Reasoning%26entry.906535625%3DPhilipp%2520Mondorf%2520and%2520Shijia%2520Zhou%2520and%2520Monica%2520Riedler%2520and%2520Barbara%2520Plank%26entry.1292438233%3D%2520%2520Systematic%2520generalization%2520refers%2520to%2520the%2520capacity%2520to%2520understand%2520and%2520generate%250Anovel%2520combinations%2520from%2520known%2520components.%2520Despite%2520recent%2520progress%2520by%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520across%2520various%2520domains%252C%2520these%2520models%2520often%2520fail%2520to%250Aextend%2520their%2520knowledge%2520to%2520novel%2520compositional%2520scenarios%252C%2520revealing%2520notable%250Alimitations%2520in%2520systematic%2520generalization.%2520There%2520has%2520been%2520an%2520ongoing%2520debate%250Aabout%2520whether%2520neural%2520networks%2520possess%2520the%2520capacity%2520for%2520systematic%250Ageneralization%252C%2520with%2520recent%2520studies%2520suggesting%2520that%2520meta-learning%2520approaches%250Adesigned%2520for%2520compositionality%2520can%2520significantly%2520enhance%2520this%2520ability.%2520However%252C%250Athese%2520insights%2520have%2520largely%2520been%2520confined%2520to%2520linguistic%2520problems%252C%2520leaving%2520their%250Aapplicability%2520to%2520other%2520tasks%2520an%2520open%2520question.%2520In%2520this%2520study%252C%2520we%2520extend%250Ameta-learning%2520for%2520compositionality%2520to%2520the%2520domain%2520of%2520abstract%2520spatial%2520reasoning.%250ATo%2520this%2520end%252C%2520we%2520introduce%2520%2524%255Ctextit%257BCompositional-ARC%257D-%2524a%2520dataset%2520designed%2520to%250Aevaluate%2520the%2520capacity%2520of%2520models%2520to%2520systematically%2520generalize%2520from%2520known%250Ageometric%2520transformations%2520%2528e.g.%252C%2520translation%252C%2520rotation%2529%2520of%2520abstract%250Atwo-dimensional%2520objects%2520to%2520novel%2520combinations%2520of%2520these%2520transformations%2520%2528e.g.%252C%250Atranslation%252Brotation%2529.%2520Our%2520results%2520show%2520that%2520a%2520small%2520transformer-based%250Aencoder-decoder%2520model%252C%2520trained%2520via%2520meta-learning%2520for%2520compositionality%252C%2520can%250Asystematically%2520generalize%2520to%2520previously%2520unseen%2520transformation%2520compositions.%250ANotably%252C%2520despite%2520having%2520only%25205.7M%2520parameters%252C%2520this%2520model%2520significantly%250Aoutperforms%2520state-of-the-art%2520LLMs%2524-%2524including%2520o3-mini%252C%2520GPT-4o%252C%2520and%2520Gemini%25202.0%250AFlash%252C%2520which%2520fail%2520to%2520exhibit%2520similar%2520systematic%2520behavior%2524-%2524and%2520performs%2520on%2520par%250Awith%2520the%2520winning%2520model%2520of%2520the%2520ARC%2520prize%25202024%252C%2520an%25208B-parameter%2520LLM%2520trained%2520via%250Atest-time%2520training.%2520Our%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520meta-learning%250Ain%2520promoting%2520systematicity%2520beyond%2520linguistic%2520tasks%252C%2520suggesting%2520a%2520promising%250Adirection%2520toward%2520more%2520robust%2520and%2520generalizable%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional-ARC%3A%20Assessing%20Systematic%20Generalization%20in%20Abstract%0A%20%20Spatial%20Reasoning&entry.906535625=Philipp%20Mondorf%20and%20Shijia%20Zhou%20and%20Monica%20Riedler%20and%20Barbara%20Plank&entry.1292438233=%20%20Systematic%20generalization%20refers%20to%20the%20capacity%20to%20understand%20and%20generate%0Anovel%20combinations%20from%20known%20components.%20Despite%20recent%20progress%20by%20large%0Alanguage%20models%20%28LLMs%29%20across%20various%20domains%2C%20these%20models%20often%20fail%20to%0Aextend%20their%20knowledge%20to%20novel%20compositional%20scenarios%2C%20revealing%20notable%0Alimitations%20in%20systematic%20generalization.%20There%20has%20been%20an%20ongoing%20debate%0Aabout%20whether%20neural%20networks%20possess%20the%20capacity%20for%20systematic%0Ageneralization%2C%20with%20recent%20studies%20suggesting%20that%20meta-learning%20approaches%0Adesigned%20for%20compositionality%20can%20significantly%20enhance%20this%20ability.%20However%2C%0Athese%20insights%20have%20largely%20been%20confined%20to%20linguistic%20problems%2C%20leaving%20their%0Aapplicability%20to%20other%20tasks%20an%20open%20question.%20In%20this%20study%2C%20we%20extend%0Ameta-learning%20for%20compositionality%20to%20the%20domain%20of%20abstract%20spatial%20reasoning.%0ATo%20this%20end%2C%20we%20introduce%20%24%5Ctextit%7BCompositional-ARC%7D-%24a%20dataset%20designed%20to%0Aevaluate%20the%20capacity%20of%20models%20to%20systematically%20generalize%20from%20known%0Ageometric%20transformations%20%28e.g.%2C%20translation%2C%20rotation%29%20of%20abstract%0Atwo-dimensional%20objects%20to%20novel%20combinations%20of%20these%20transformations%20%28e.g.%2C%0Atranslation%2Brotation%29.%20Our%20results%20show%20that%20a%20small%20transformer-based%0Aencoder-decoder%20model%2C%20trained%20via%20meta-learning%20for%20compositionality%2C%20can%0Asystematically%20generalize%20to%20previously%20unseen%20transformation%20compositions.%0ANotably%2C%20despite%20having%20only%205.7M%20parameters%2C%20this%20model%20significantly%0Aoutperforms%20state-of-the-art%20LLMs%24-%24including%20o3-mini%2C%20GPT-4o%2C%20and%20Gemini%202.0%0AFlash%2C%20which%20fail%20to%20exhibit%20similar%20systematic%20behavior%24-%24and%20performs%20on%20par%0Awith%20the%20winning%20model%20of%20the%20ARC%20prize%202024%2C%20an%208B-parameter%20LLM%20trained%20via%0Atest-time%20training.%20Our%20findings%20highlight%20the%20effectiveness%20of%20meta-learning%0Ain%20promoting%20systematicity%20beyond%20linguistic%20tasks%2C%20suggesting%20a%20promising%0Adirection%20toward%20more%20robust%20and%20generalizable%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01445v2&entry.124074799=Read"},
{"title": "On the Dynamic Regret of Following the Regularized Leader: Optimism with\n  History Pruning", "author": "Naram Mhaisen and George Iosifidis", "abstract": "  We revisit the Follow the Regularized Leader (FTRL) framework for Online\nConvex Optimization (OCO) over compact sets, focusing on achieving dynamic\nregret guarantees. Prior work has highlighted the framework's limitations in\ndynamic environments due to its tendency to produce \"lazy\" iterates. However,\nbuilding on insights showing FTRL's ability to produce \"agile\" iterates, we\nshow that it can indeed recover known dynamic regret bounds through optimistic\ncomposition of future costs and careful linearization of past costs, which can\nlead to pruning some of them. This new analysis of FTRL against dynamic\ncomparators yields a principled way to interpolate between greedy and agile\nupdates and offers several benefits, including refined control over regret\nterms, optimism without cyclic dependence, and the application of minimal\nrecursive regularization akin to AdaFTRL. More broadly, we show that it is not\nthe \"lazy\" projection style of FTRL that hinders (optimistic) dynamic regret,\nbut the decoupling of the algorithm's state (linearized history) from its\niterates, allowing the state to grow arbitrarily. Instead, pruning synchronizes\nthese two when necessary.\n", "link": "http://arxiv.org/abs/2505.22899v2", "date": "2025-09-25", "relevancy": 2.2264, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4604}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4397}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Dynamic%20Regret%20of%20Following%20the%20Regularized%20Leader%3A%20Optimism%20with%0A%20%20History%20Pruning&body=Title%3A%20On%20the%20Dynamic%20Regret%20of%20Following%20the%20Regularized%20Leader%3A%20Optimism%20with%0A%20%20History%20Pruning%0AAuthor%3A%20Naram%20Mhaisen%20and%20George%20Iosifidis%0AAbstract%3A%20%20%20We%20revisit%20the%20Follow%20the%20Regularized%20Leader%20%28FTRL%29%20framework%20for%20Online%0AConvex%20Optimization%20%28OCO%29%20over%20compact%20sets%2C%20focusing%20on%20achieving%20dynamic%0Aregret%20guarantees.%20Prior%20work%20has%20highlighted%20the%20framework%27s%20limitations%20in%0Adynamic%20environments%20due%20to%20its%20tendency%20to%20produce%20%22lazy%22%20iterates.%20However%2C%0Abuilding%20on%20insights%20showing%20FTRL%27s%20ability%20to%20produce%20%22agile%22%20iterates%2C%20we%0Ashow%20that%20it%20can%20indeed%20recover%20known%20dynamic%20regret%20bounds%20through%20optimistic%0Acomposition%20of%20future%20costs%20and%20careful%20linearization%20of%20past%20costs%2C%20which%20can%0Alead%20to%20pruning%20some%20of%20them.%20This%20new%20analysis%20of%20FTRL%20against%20dynamic%0Acomparators%20yields%20a%20principled%20way%20to%20interpolate%20between%20greedy%20and%20agile%0Aupdates%20and%20offers%20several%20benefits%2C%20including%20refined%20control%20over%20regret%0Aterms%2C%20optimism%20without%20cyclic%20dependence%2C%20and%20the%20application%20of%20minimal%0Arecursive%20regularization%20akin%20to%20AdaFTRL.%20More%20broadly%2C%20we%20show%20that%20it%20is%20not%0Athe%20%22lazy%22%20projection%20style%20of%20FTRL%20that%20hinders%20%28optimistic%29%20dynamic%20regret%2C%0Abut%20the%20decoupling%20of%20the%20algorithm%27s%20state%20%28linearized%20history%29%20from%20its%0Aiterates%2C%20allowing%20the%20state%20to%20grow%20arbitrarily.%20Instead%2C%20pruning%20synchronizes%0Athese%20two%20when%20necessary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Dynamic%2520Regret%2520of%2520Following%2520the%2520Regularized%2520Leader%253A%2520Optimism%2520with%250A%2520%2520History%2520Pruning%26entry.906535625%3DNaram%2520Mhaisen%2520and%2520George%2520Iosifidis%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520Follow%2520the%2520Regularized%2520Leader%2520%2528FTRL%2529%2520framework%2520for%2520Online%250AConvex%2520Optimization%2520%2528OCO%2529%2520over%2520compact%2520sets%252C%2520focusing%2520on%2520achieving%2520dynamic%250Aregret%2520guarantees.%2520Prior%2520work%2520has%2520highlighted%2520the%2520framework%2527s%2520limitations%2520in%250Adynamic%2520environments%2520due%2520to%2520its%2520tendency%2520to%2520produce%2520%2522lazy%2522%2520iterates.%2520However%252C%250Abuilding%2520on%2520insights%2520showing%2520FTRL%2527s%2520ability%2520to%2520produce%2520%2522agile%2522%2520iterates%252C%2520we%250Ashow%2520that%2520it%2520can%2520indeed%2520recover%2520known%2520dynamic%2520regret%2520bounds%2520through%2520optimistic%250Acomposition%2520of%2520future%2520costs%2520and%2520careful%2520linearization%2520of%2520past%2520costs%252C%2520which%2520can%250Alead%2520to%2520pruning%2520some%2520of%2520them.%2520This%2520new%2520analysis%2520of%2520FTRL%2520against%2520dynamic%250Acomparators%2520yields%2520a%2520principled%2520way%2520to%2520interpolate%2520between%2520greedy%2520and%2520agile%250Aupdates%2520and%2520offers%2520several%2520benefits%252C%2520including%2520refined%2520control%2520over%2520regret%250Aterms%252C%2520optimism%2520without%2520cyclic%2520dependence%252C%2520and%2520the%2520application%2520of%2520minimal%250Arecursive%2520regularization%2520akin%2520to%2520AdaFTRL.%2520More%2520broadly%252C%2520we%2520show%2520that%2520it%2520is%2520not%250Athe%2520%2522lazy%2522%2520projection%2520style%2520of%2520FTRL%2520that%2520hinders%2520%2528optimistic%2529%2520dynamic%2520regret%252C%250Abut%2520the%2520decoupling%2520of%2520the%2520algorithm%2527s%2520state%2520%2528linearized%2520history%2529%2520from%2520its%250Aiterates%252C%2520allowing%2520the%2520state%2520to%2520grow%2520arbitrarily.%2520Instead%252C%2520pruning%2520synchronizes%250Athese%2520two%2520when%2520necessary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Dynamic%20Regret%20of%20Following%20the%20Regularized%20Leader%3A%20Optimism%20with%0A%20%20History%20Pruning&entry.906535625=Naram%20Mhaisen%20and%20George%20Iosifidis&entry.1292438233=%20%20We%20revisit%20the%20Follow%20the%20Regularized%20Leader%20%28FTRL%29%20framework%20for%20Online%0AConvex%20Optimization%20%28OCO%29%20over%20compact%20sets%2C%20focusing%20on%20achieving%20dynamic%0Aregret%20guarantees.%20Prior%20work%20has%20highlighted%20the%20framework%27s%20limitations%20in%0Adynamic%20environments%20due%20to%20its%20tendency%20to%20produce%20%22lazy%22%20iterates.%20However%2C%0Abuilding%20on%20insights%20showing%20FTRL%27s%20ability%20to%20produce%20%22agile%22%20iterates%2C%20we%0Ashow%20that%20it%20can%20indeed%20recover%20known%20dynamic%20regret%20bounds%20through%20optimistic%0Acomposition%20of%20future%20costs%20and%20careful%20linearization%20of%20past%20costs%2C%20which%20can%0Alead%20to%20pruning%20some%20of%20them.%20This%20new%20analysis%20of%20FTRL%20against%20dynamic%0Acomparators%20yields%20a%20principled%20way%20to%20interpolate%20between%20greedy%20and%20agile%0Aupdates%20and%20offers%20several%20benefits%2C%20including%20refined%20control%20over%20regret%0Aterms%2C%20optimism%20without%20cyclic%20dependence%2C%20and%20the%20application%20of%20minimal%0Arecursive%20regularization%20akin%20to%20AdaFTRL.%20More%20broadly%2C%20we%20show%20that%20it%20is%20not%0Athe%20%22lazy%22%20projection%20style%20of%20FTRL%20that%20hinders%20%28optimistic%29%20dynamic%20regret%2C%0Abut%20the%20decoupling%20of%20the%20algorithm%27s%20state%20%28linearized%20history%29%20from%20its%0Aiterates%2C%20allowing%20the%20state%20to%20grow%20arbitrarily.%20Instead%2C%20pruning%20synchronizes%0Athese%20two%20when%20necessary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22899v2&entry.124074799=Read"},
{"title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "author": "Maksim Kuprashevich and Grigorii Alekseenko and Irina Tolstykh and Georgii Fedorov and Bulat Suleimanov and Vladimir Dokholyan and Aleksandr Gordeev", "abstract": "  Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets (original image, instruction,\nedited image), yet mining pixel-accurate examples is hard. Each edit must\naffect only prompt-specified regions, preserve stylistic coherence, respect\nphysical plausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approx. 2.6x, enabling\nlarge-scale high-fidelity training data. By automating the most repetitive\nannotation steps, the approach allows a new scale of training without human\nlabeling effort. To democratize research in this resource-intensive area, we\nrelease NHR-Edit, an open dataset of 720k high-quality triplets, curated at\nindustrial scale via millions of guided generations and validator passes, and\nwe analyze the pipeline's stage-wise survival rates, providing a framework for\nestimating computational effort across different model stacks. In the largest\ncross-dataset evaluation, it surpasses all public alternatives. We also release\nBagel-NHR-Edit, a fine-tuned Bagel model with state-of-the-art metrics.\n", "link": "http://arxiv.org/abs/2507.14119v2", "date": "2025-09-25", "relevancy": 2.2104, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5615}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5486}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoHumansRequired%3A%20Autonomous%20High-Quality%20Image%20Editing%20Triplet%20Mining&body=Title%3A%20NoHumansRequired%3A%20Autonomous%20High-Quality%20Image%20Editing%20Triplet%20Mining%0AAuthor%3A%20Maksim%20Kuprashevich%20and%20Grigorii%20Alekseenko%20and%20Irina%20Tolstykh%20and%20Georgii%20Fedorov%20and%20Bulat%20Suleimanov%20and%20Vladimir%20Dokholyan%20and%20Aleksandr%20Gordeev%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20modeling%20enable%20image%20editing%20assistants%20that%0Afollow%20natural%20language%20instructions%20without%20additional%20user%20input.%20Their%0Asupervised%20training%20requires%20millions%20of%20triplets%20%28original%20image%2C%20instruction%2C%0Aedited%20image%29%2C%20yet%20mining%20pixel-accurate%20examples%20is%20hard.%20Each%20edit%20must%0Aaffect%20only%20prompt-specified%20regions%2C%20preserve%20stylistic%20coherence%2C%20respect%0Aphysical%20plausibility%2C%20and%20retain%20visual%20appeal.%20The%20lack%20of%20robust%20automated%0Aedit-quality%20metrics%20hinders%20reliable%20automation%20at%20scale.%20We%20present%20an%0Aautomated%2C%20modular%20pipeline%20that%20mines%20high-fidelity%20triplets%20across%20domains%2C%0Aresolutions%2C%20instruction%20complexities%2C%20and%20styles.%20Built%20on%20public%20generative%0Amodels%20and%20running%20without%20human%20intervention%2C%20our%20system%20uses%20a%20task-tuned%0AGemini%20validator%20to%20score%20instruction%20adherence%20and%20aesthetics%20directly%2C%0Aremoving%20any%20need%20for%20segmentation%20or%20grounding%20models.%20Inversion%20and%0Acompositional%20bootstrapping%20enlarge%20the%20mined%20set%20by%20approx.%202.6x%2C%20enabling%0Alarge-scale%20high-fidelity%20training%20data.%20By%20automating%20the%20most%20repetitive%0Aannotation%20steps%2C%20the%20approach%20allows%20a%20new%20scale%20of%20training%20without%20human%0Alabeling%20effort.%20To%20democratize%20research%20in%20this%20resource-intensive%20area%2C%20we%0Arelease%20NHR-Edit%2C%20an%20open%20dataset%20of%20720k%20high-quality%20triplets%2C%20curated%20at%0Aindustrial%20scale%20via%20millions%20of%20guided%20generations%20and%20validator%20passes%2C%20and%0Awe%20analyze%20the%20pipeline%27s%20stage-wise%20survival%20rates%2C%20providing%20a%20framework%20for%0Aestimating%20computational%20effort%20across%20different%20model%20stacks.%20In%20the%20largest%0Across-dataset%20evaluation%2C%20it%20surpasses%20all%20public%20alternatives.%20We%20also%20release%0ABagel-NHR-Edit%2C%20a%20fine-tuned%20Bagel%20model%20with%20state-of-the-art%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoHumansRequired%253A%2520Autonomous%2520High-Quality%2520Image%2520Editing%2520Triplet%2520Mining%26entry.906535625%3DMaksim%2520Kuprashevich%2520and%2520Grigorii%2520Alekseenko%2520and%2520Irina%2520Tolstykh%2520and%2520Georgii%2520Fedorov%2520and%2520Bulat%2520Suleimanov%2520and%2520Vladimir%2520Dokholyan%2520and%2520Aleksandr%2520Gordeev%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520modeling%2520enable%2520image%2520editing%2520assistants%2520that%250Afollow%2520natural%2520language%2520instructions%2520without%2520additional%2520user%2520input.%2520Their%250Asupervised%2520training%2520requires%2520millions%2520of%2520triplets%2520%2528original%2520image%252C%2520instruction%252C%250Aedited%2520image%2529%252C%2520yet%2520mining%2520pixel-accurate%2520examples%2520is%2520hard.%2520Each%2520edit%2520must%250Aaffect%2520only%2520prompt-specified%2520regions%252C%2520preserve%2520stylistic%2520coherence%252C%2520respect%250Aphysical%2520plausibility%252C%2520and%2520retain%2520visual%2520appeal.%2520The%2520lack%2520of%2520robust%2520automated%250Aedit-quality%2520metrics%2520hinders%2520reliable%2520automation%2520at%2520scale.%2520We%2520present%2520an%250Aautomated%252C%2520modular%2520pipeline%2520that%2520mines%2520high-fidelity%2520triplets%2520across%2520domains%252C%250Aresolutions%252C%2520instruction%2520complexities%252C%2520and%2520styles.%2520Built%2520on%2520public%2520generative%250Amodels%2520and%2520running%2520without%2520human%2520intervention%252C%2520our%2520system%2520uses%2520a%2520task-tuned%250AGemini%2520validator%2520to%2520score%2520instruction%2520adherence%2520and%2520aesthetics%2520directly%252C%250Aremoving%2520any%2520need%2520for%2520segmentation%2520or%2520grounding%2520models.%2520Inversion%2520and%250Acompositional%2520bootstrapping%2520enlarge%2520the%2520mined%2520set%2520by%2520approx.%25202.6x%252C%2520enabling%250Alarge-scale%2520high-fidelity%2520training%2520data.%2520By%2520automating%2520the%2520most%2520repetitive%250Aannotation%2520steps%252C%2520the%2520approach%2520allows%2520a%2520new%2520scale%2520of%2520training%2520without%2520human%250Alabeling%2520effort.%2520To%2520democratize%2520research%2520in%2520this%2520resource-intensive%2520area%252C%2520we%250Arelease%2520NHR-Edit%252C%2520an%2520open%2520dataset%2520of%2520720k%2520high-quality%2520triplets%252C%2520curated%2520at%250Aindustrial%2520scale%2520via%2520millions%2520of%2520guided%2520generations%2520and%2520validator%2520passes%252C%2520and%250Awe%2520analyze%2520the%2520pipeline%2527s%2520stage-wise%2520survival%2520rates%252C%2520providing%2520a%2520framework%2520for%250Aestimating%2520computational%2520effort%2520across%2520different%2520model%2520stacks.%2520In%2520the%2520largest%250Across-dataset%2520evaluation%252C%2520it%2520surpasses%2520all%2520public%2520alternatives.%2520We%2520also%2520release%250ABagel-NHR-Edit%252C%2520a%2520fine-tuned%2520Bagel%2520model%2520with%2520state-of-the-art%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoHumansRequired%3A%20Autonomous%20High-Quality%20Image%20Editing%20Triplet%20Mining&entry.906535625=Maksim%20Kuprashevich%20and%20Grigorii%20Alekseenko%20and%20Irina%20Tolstykh%20and%20Georgii%20Fedorov%20and%20Bulat%20Suleimanov%20and%20Vladimir%20Dokholyan%20and%20Aleksandr%20Gordeev&entry.1292438233=%20%20Recent%20advances%20in%20generative%20modeling%20enable%20image%20editing%20assistants%20that%0Afollow%20natural%20language%20instructions%20without%20additional%20user%20input.%20Their%0Asupervised%20training%20requires%20millions%20of%20triplets%20%28original%20image%2C%20instruction%2C%0Aedited%20image%29%2C%20yet%20mining%20pixel-accurate%20examples%20is%20hard.%20Each%20edit%20must%0Aaffect%20only%20prompt-specified%20regions%2C%20preserve%20stylistic%20coherence%2C%20respect%0Aphysical%20plausibility%2C%20and%20retain%20visual%20appeal.%20The%20lack%20of%20robust%20automated%0Aedit-quality%20metrics%20hinders%20reliable%20automation%20at%20scale.%20We%20present%20an%0Aautomated%2C%20modular%20pipeline%20that%20mines%20high-fidelity%20triplets%20across%20domains%2C%0Aresolutions%2C%20instruction%20complexities%2C%20and%20styles.%20Built%20on%20public%20generative%0Amodels%20and%20running%20without%20human%20intervention%2C%20our%20system%20uses%20a%20task-tuned%0AGemini%20validator%20to%20score%20instruction%20adherence%20and%20aesthetics%20directly%2C%0Aremoving%20any%20need%20for%20segmentation%20or%20grounding%20models.%20Inversion%20and%0Acompositional%20bootstrapping%20enlarge%20the%20mined%20set%20by%20approx.%202.6x%2C%20enabling%0Alarge-scale%20high-fidelity%20training%20data.%20By%20automating%20the%20most%20repetitive%0Aannotation%20steps%2C%20the%20approach%20allows%20a%20new%20scale%20of%20training%20without%20human%0Alabeling%20effort.%20To%20democratize%20research%20in%20this%20resource-intensive%20area%2C%20we%0Arelease%20NHR-Edit%2C%20an%20open%20dataset%20of%20720k%20high-quality%20triplets%2C%20curated%20at%0Aindustrial%20scale%20via%20millions%20of%20guided%20generations%20and%20validator%20passes%2C%20and%0Awe%20analyze%20the%20pipeline%27s%20stage-wise%20survival%20rates%2C%20providing%20a%20framework%20for%0Aestimating%20computational%20effort%20across%20different%20model%20stacks.%20In%20the%20largest%0Across-dataset%20evaluation%2C%20it%20surpasses%20all%20public%20alternatives.%20We%20also%20release%0ABagel-NHR-Edit%2C%20a%20fine-tuned%20Bagel%20model%20with%20state-of-the-art%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14119v2&entry.124074799=Read"},
{"title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\n  Training", "author": "Shiju Wang and Yujie Wang and Ao Sun and Fangcheng Fu and Zijian Zhu and Bin Cui and Xu Han and Kaisheng Ma", "abstract": "  Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.\n", "link": "http://arxiv.org/abs/2509.21275v1", "date": "2025-09-25", "relevancy": 2.21, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4387}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Centric%20Elastic%20Pipeline%20Parallelism%20for%20Efficient%20Long-Context%20LLM%0A%20%20Training&body=Title%3A%20Data-Centric%20Elastic%20Pipeline%20Parallelism%20for%20Efficient%20Long-Context%20LLM%0A%20%20Training%0AAuthor%3A%20Shiju%20Wang%20and%20Yujie%20Wang%20and%20Ao%20Sun%20and%20Fangcheng%20Fu%20and%20Zijian%20Zhu%20and%20Bin%20Cui%20and%20Xu%20Han%20and%20Kaisheng%20Ma%0AAbstract%3A%20%20%20Long%20context%20training%20is%20crucial%20for%20LLM%27s%20context%20extension.%20Existing%0Aschemes%2C%20such%20as%20sequence%20parallelism%2C%20incur%20substantial%20communication%0Aoverhead.%20Pipeline%20parallelism%20%28PP%29%20reduces%20this%20cost%2C%20but%20its%20effectiveness%0Ahinges%20on%20partitioning%20granularity.%20Batch-level%20PP%20dividing%20input%20samples%0Aexhibits%20high%20memory%20consumption%20in%20long-context%20scenario%2C%20whereas%20token-level%0APP%20splitting%20sequences%20into%20slices%20alleviates%20memory%20overhead%20but%20may%20incur%0Ahardware%20under-utilization.%20This%20trade-off%20motivates%20adaptively%20selecting%20PP%0Agranularity%20to%20match%20resource%20and%20workload%20characteristics.%20Moreover%2C%20sequence%0Alength%20distribution%20of%20the%20real-world%20dataset%20exhibits%20skewness%2C%20posing%20a%0Achallenge%20on%20PP%27s%20workload%20balance%20and%20efficient%20scheduling.%20Current%20static%20PP%0Ascheduling%20methods%20overlook%20the%20variance%20of%20sequence%20length%2C%20leading%20to%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20propose%20Elastic%20Pipeline%20Parallelism%0A%28EPP%29%20that%20orchestrates%20token-level%20PP%20and%20batch-level%20PP%20to%20adapt%20to%20resource%0Aand%20workload%20heterogeneity.%20We%20build%20InfiniPipe%2C%20a%20distributed%20training%20system%0Athat%20unleashes%20the%20potential%20of%20EPP%20via%20%281%29%20a%20resource-aware%20and%0Aworkload-balanced%20sequence%20processor%20that%20splits%20long%20sequences%20and%20packs%20short%0Aones%3B%20and%20%282%29%20a%20co-optimization%20methodology%20that%20jointly%20optimizes%20pipeline%0Aschedule%20and%20gradient%20checkpointing%20via%20a%20mechanism%20named%20stage-aware%0Achunk-level%20adaptive%20checkpointing.%20Comprehensive%20experiments%20demonstrate%20that%0AInfiniPipe%20achieves%20a%201.69x%20speedup%20over%20state-of-the-art%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Centric%2520Elastic%2520Pipeline%2520Parallelism%2520for%2520Efficient%2520Long-Context%2520LLM%250A%2520%2520Training%26entry.906535625%3DShiju%2520Wang%2520and%2520Yujie%2520Wang%2520and%2520Ao%2520Sun%2520and%2520Fangcheng%2520Fu%2520and%2520Zijian%2520Zhu%2520and%2520Bin%2520Cui%2520and%2520Xu%2520Han%2520and%2520Kaisheng%2520Ma%26entry.1292438233%3D%2520%2520Long%2520context%2520training%2520is%2520crucial%2520for%2520LLM%2527s%2520context%2520extension.%2520Existing%250Aschemes%252C%2520such%2520as%2520sequence%2520parallelism%252C%2520incur%2520substantial%2520communication%250Aoverhead.%2520Pipeline%2520parallelism%2520%2528PP%2529%2520reduces%2520this%2520cost%252C%2520but%2520its%2520effectiveness%250Ahinges%2520on%2520partitioning%2520granularity.%2520Batch-level%2520PP%2520dividing%2520input%2520samples%250Aexhibits%2520high%2520memory%2520consumption%2520in%2520long-context%2520scenario%252C%2520whereas%2520token-level%250APP%2520splitting%2520sequences%2520into%2520slices%2520alleviates%2520memory%2520overhead%2520but%2520may%2520incur%250Ahardware%2520under-utilization.%2520This%2520trade-off%2520motivates%2520adaptively%2520selecting%2520PP%250Agranularity%2520to%2520match%2520resource%2520and%2520workload%2520characteristics.%2520Moreover%252C%2520sequence%250Alength%2520distribution%2520of%2520the%2520real-world%2520dataset%2520exhibits%2520skewness%252C%2520posing%2520a%250Achallenge%2520on%2520PP%2527s%2520workload%2520balance%2520and%2520efficient%2520scheduling.%2520Current%2520static%2520PP%250Ascheduling%2520methods%2520overlook%2520the%2520variance%2520of%2520sequence%2520length%252C%2520leading%2520to%250Asuboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Elastic%2520Pipeline%2520Parallelism%250A%2528EPP%2529%2520that%2520orchestrates%2520token-level%2520PP%2520and%2520batch-level%2520PP%2520to%2520adapt%2520to%2520resource%250Aand%2520workload%2520heterogeneity.%2520We%2520build%2520InfiniPipe%252C%2520a%2520distributed%2520training%2520system%250Athat%2520unleashes%2520the%2520potential%2520of%2520EPP%2520via%2520%25281%2529%2520a%2520resource-aware%2520and%250Aworkload-balanced%2520sequence%2520processor%2520that%2520splits%2520long%2520sequences%2520and%2520packs%2520short%250Aones%253B%2520and%2520%25282%2529%2520a%2520co-optimization%2520methodology%2520that%2520jointly%2520optimizes%2520pipeline%250Aschedule%2520and%2520gradient%2520checkpointing%2520via%2520a%2520mechanism%2520named%2520stage-aware%250Achunk-level%2520adaptive%2520checkpointing.%2520Comprehensive%2520experiments%2520demonstrate%2520that%250AInfiniPipe%2520achieves%2520a%25201.69x%2520speedup%2520over%2520state-of-the-art%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Centric%20Elastic%20Pipeline%20Parallelism%20for%20Efficient%20Long-Context%20LLM%0A%20%20Training&entry.906535625=Shiju%20Wang%20and%20Yujie%20Wang%20and%20Ao%20Sun%20and%20Fangcheng%20Fu%20and%20Zijian%20Zhu%20and%20Bin%20Cui%20and%20Xu%20Han%20and%20Kaisheng%20Ma&entry.1292438233=%20%20Long%20context%20training%20is%20crucial%20for%20LLM%27s%20context%20extension.%20Existing%0Aschemes%2C%20such%20as%20sequence%20parallelism%2C%20incur%20substantial%20communication%0Aoverhead.%20Pipeline%20parallelism%20%28PP%29%20reduces%20this%20cost%2C%20but%20its%20effectiveness%0Ahinges%20on%20partitioning%20granularity.%20Batch-level%20PP%20dividing%20input%20samples%0Aexhibits%20high%20memory%20consumption%20in%20long-context%20scenario%2C%20whereas%20token-level%0APP%20splitting%20sequences%20into%20slices%20alleviates%20memory%20overhead%20but%20may%20incur%0Ahardware%20under-utilization.%20This%20trade-off%20motivates%20adaptively%20selecting%20PP%0Agranularity%20to%20match%20resource%20and%20workload%20characteristics.%20Moreover%2C%20sequence%0Alength%20distribution%20of%20the%20real-world%20dataset%20exhibits%20skewness%2C%20posing%20a%0Achallenge%20on%20PP%27s%20workload%20balance%20and%20efficient%20scheduling.%20Current%20static%20PP%0Ascheduling%20methods%20overlook%20the%20variance%20of%20sequence%20length%2C%20leading%20to%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20propose%20Elastic%20Pipeline%20Parallelism%0A%28EPP%29%20that%20orchestrates%20token-level%20PP%20and%20batch-level%20PP%20to%20adapt%20to%20resource%0Aand%20workload%20heterogeneity.%20We%20build%20InfiniPipe%2C%20a%20distributed%20training%20system%0Athat%20unleashes%20the%20potential%20of%20EPP%20via%20%281%29%20a%20resource-aware%20and%0Aworkload-balanced%20sequence%20processor%20that%20splits%20long%20sequences%20and%20packs%20short%0Aones%3B%20and%20%282%29%20a%20co-optimization%20methodology%20that%20jointly%20optimizes%20pipeline%0Aschedule%20and%20gradient%20checkpointing%20via%20a%20mechanism%20named%20stage-aware%0Achunk-level%20adaptive%20checkpointing.%20Comprehensive%20experiments%20demonstrate%20that%0AInfiniPipe%20achieves%20a%201.69x%20speedup%20over%20state-of-the-art%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21275v1&entry.124074799=Read"},
{"title": "Differential Gated Self-Attention", "author": "Elpiniki Maria Lygizou and M\u00f3nika Farsang and Radu Grosu", "abstract": "  Transformers excel across a large variety of tasks but remain susceptible to\ncorrupted inputs, since standard self-attention treats all query-key\ninteractions uniformly. Inspired by lateral inhibition in biological neural\ncircuits and building on the recent use by the Differential Transformer's use\nof two parallel softmax subtraction for noise cancellation, we propose\nMultihead Differential Gated Self-Attention (M-DGSA) that learns per-head\ninput-dependent gating to dynamically suppress attention noise. Each head\nsplits into excitatory and inhibitory branches whose dual softmax maps are\nfused by a sigmoid gate predicted from the token embedding, yielding a\ncontext-aware contrast enhancement. M-DGSA integrates seamlessly into existing\nTransformer stacks with minimal computational overhead. We evaluate on both\nvision and language benchmarks, demonstrating consistent robustness gains over\nvanilla Transformer, Vision Transformer, and Differential Transformer\nbaselines. Our contributions are (i) a novel input-dependent gating mechanism\nfor self-attention grounded in lateral inhibition, (ii) a principled synthesis\nof biological contrast-enhancement and self-attention theory, and (iii)\ncomprehensive experiments demonstrating noise resilience and cross-domain\napplicability.\n", "link": "http://arxiv.org/abs/2505.24054v2", "date": "2025-09-25", "relevancy": 2.1958, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5874}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5478}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Gated%20Self-Attention&body=Title%3A%20Differential%20Gated%20Self-Attention%0AAuthor%3A%20Elpiniki%20Maria%20Lygizou%20and%20M%C3%B3nika%20Farsang%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20Transformers%20excel%20across%20a%20large%20variety%20of%20tasks%20but%20remain%20susceptible%20to%0Acorrupted%20inputs%2C%20since%20standard%20self-attention%20treats%20all%20query-key%0Ainteractions%20uniformly.%20Inspired%20by%20lateral%20inhibition%20in%20biological%20neural%0Acircuits%20and%20building%20on%20the%20recent%20use%20by%20the%20Differential%20Transformer%27s%20use%0Aof%20two%20parallel%20softmax%20subtraction%20for%20noise%20cancellation%2C%20we%20propose%0AMultihead%20Differential%20Gated%20Self-Attention%20%28M-DGSA%29%20that%20learns%20per-head%0Ainput-dependent%20gating%20to%20dynamically%20suppress%20attention%20noise.%20Each%20head%0Asplits%20into%20excitatory%20and%20inhibitory%20branches%20whose%20dual%20softmax%20maps%20are%0Afused%20by%20a%20sigmoid%20gate%20predicted%20from%20the%20token%20embedding%2C%20yielding%20a%0Acontext-aware%20contrast%20enhancement.%20M-DGSA%20integrates%20seamlessly%20into%20existing%0ATransformer%20stacks%20with%20minimal%20computational%20overhead.%20We%20evaluate%20on%20both%0Avision%20and%20language%20benchmarks%2C%20demonstrating%20consistent%20robustness%20gains%20over%0Avanilla%20Transformer%2C%20Vision%20Transformer%2C%20and%20Differential%20Transformer%0Abaselines.%20Our%20contributions%20are%20%28i%29%20a%20novel%20input-dependent%20gating%20mechanism%0Afor%20self-attention%20grounded%20in%20lateral%20inhibition%2C%20%28ii%29%20a%20principled%20synthesis%0Aof%20biological%20contrast-enhancement%20and%20self-attention%20theory%2C%20and%20%28iii%29%0Acomprehensive%20experiments%20demonstrating%20noise%20resilience%20and%20cross-domain%0Aapplicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Gated%2520Self-Attention%26entry.906535625%3DElpiniki%2520Maria%2520Lygizou%2520and%2520M%25C3%25B3nika%2520Farsang%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520Transformers%2520excel%2520across%2520a%2520large%2520variety%2520of%2520tasks%2520but%2520remain%2520susceptible%2520to%250Acorrupted%2520inputs%252C%2520since%2520standard%2520self-attention%2520treats%2520all%2520query-key%250Ainteractions%2520uniformly.%2520Inspired%2520by%2520lateral%2520inhibition%2520in%2520biological%2520neural%250Acircuits%2520and%2520building%2520on%2520the%2520recent%2520use%2520by%2520the%2520Differential%2520Transformer%2527s%2520use%250Aof%2520two%2520parallel%2520softmax%2520subtraction%2520for%2520noise%2520cancellation%252C%2520we%2520propose%250AMultihead%2520Differential%2520Gated%2520Self-Attention%2520%2528M-DGSA%2529%2520that%2520learns%2520per-head%250Ainput-dependent%2520gating%2520to%2520dynamically%2520suppress%2520attention%2520noise.%2520Each%2520head%250Asplits%2520into%2520excitatory%2520and%2520inhibitory%2520branches%2520whose%2520dual%2520softmax%2520maps%2520are%250Afused%2520by%2520a%2520sigmoid%2520gate%2520predicted%2520from%2520the%2520token%2520embedding%252C%2520yielding%2520a%250Acontext-aware%2520contrast%2520enhancement.%2520M-DGSA%2520integrates%2520seamlessly%2520into%2520existing%250ATransformer%2520stacks%2520with%2520minimal%2520computational%2520overhead.%2520We%2520evaluate%2520on%2520both%250Avision%2520and%2520language%2520benchmarks%252C%2520demonstrating%2520consistent%2520robustness%2520gains%2520over%250Avanilla%2520Transformer%252C%2520Vision%2520Transformer%252C%2520and%2520Differential%2520Transformer%250Abaselines.%2520Our%2520contributions%2520are%2520%2528i%2529%2520a%2520novel%2520input-dependent%2520gating%2520mechanism%250Afor%2520self-attention%2520grounded%2520in%2520lateral%2520inhibition%252C%2520%2528ii%2529%2520a%2520principled%2520synthesis%250Aof%2520biological%2520contrast-enhancement%2520and%2520self-attention%2520theory%252C%2520and%2520%2528iii%2529%250Acomprehensive%2520experiments%2520demonstrating%2520noise%2520resilience%2520and%2520cross-domain%250Aapplicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Gated%20Self-Attention&entry.906535625=Elpiniki%20Maria%20Lygizou%20and%20M%C3%B3nika%20Farsang%20and%20Radu%20Grosu&entry.1292438233=%20%20Transformers%20excel%20across%20a%20large%20variety%20of%20tasks%20but%20remain%20susceptible%20to%0Acorrupted%20inputs%2C%20since%20standard%20self-attention%20treats%20all%20query-key%0Ainteractions%20uniformly.%20Inspired%20by%20lateral%20inhibition%20in%20biological%20neural%0Acircuits%20and%20building%20on%20the%20recent%20use%20by%20the%20Differential%20Transformer%27s%20use%0Aof%20two%20parallel%20softmax%20subtraction%20for%20noise%20cancellation%2C%20we%20propose%0AMultihead%20Differential%20Gated%20Self-Attention%20%28M-DGSA%29%20that%20learns%20per-head%0Ainput-dependent%20gating%20to%20dynamically%20suppress%20attention%20noise.%20Each%20head%0Asplits%20into%20excitatory%20and%20inhibitory%20branches%20whose%20dual%20softmax%20maps%20are%0Afused%20by%20a%20sigmoid%20gate%20predicted%20from%20the%20token%20embedding%2C%20yielding%20a%0Acontext-aware%20contrast%20enhancement.%20M-DGSA%20integrates%20seamlessly%20into%20existing%0ATransformer%20stacks%20with%20minimal%20computational%20overhead.%20We%20evaluate%20on%20both%0Avision%20and%20language%20benchmarks%2C%20demonstrating%20consistent%20robustness%20gains%20over%0Avanilla%20Transformer%2C%20Vision%20Transformer%2C%20and%20Differential%20Transformer%0Abaselines.%20Our%20contributions%20are%20%28i%29%20a%20novel%20input-dependent%20gating%20mechanism%0Afor%20self-attention%20grounded%20in%20lateral%20inhibition%2C%20%28ii%29%20a%20principled%20synthesis%0Aof%20biological%20contrast-enhancement%20and%20self-attention%20theory%2C%20and%20%28iii%29%0Acomprehensive%20experiments%20demonstrating%20noise%20resilience%20and%20cross-domain%0Aapplicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24054v2&entry.124074799=Read"},
{"title": "Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into\n  Vision Transformers", "author": "Yuyang Shu and Michael E. Bain", "abstract": "  Humans see low spatial frequency components before high spatial frequency\ncomponents.\n  Drawing on this neuroscientific inspiration, we investigate the effect of\nintroducing patches from different spatial frequencies into Vision Transformers\n(ViTs).\n  We name this model Retina Vision Transformer (RetinaViT) due to its\ninspiration from the human visual system.\n  Our experiments on benchmark data show that RetinaViT exhibits a strong\ntendency to attend to low spatial frequency components in the early layers, and\nshifts its attention to high spatial frequency components as the network goes\ndeeper.\n  This tendency emerged by itself without any additional inductive bias, and\naligns with the visual processing order of the human visual system.\n  We hypothesise that RetinaViT captures structural features, or the gist of\nthe scene, in earlier layers, before attending to fine details in subsequent\nlayers, which is the reverse of the processing order of mainstream backbone\nvision models, such as CNNs.\n  We also observe that RetinaViT is more robust to significant reductions in\nmodel size compared to the original ViT, which we hypothesise to have come from\nits ability to capture the gist of the scene early.\n", "link": "http://arxiv.org/abs/2403.13677v2", "date": "2025-09-25", "relevancy": 2.1949, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5952}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers&body=Title%3A%20Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers%0AAuthor%3A%20Yuyang%20Shu%20and%20Michael%20E.%20Bain%0AAbstract%3A%20%20%20Humans%20see%20low%20spatial%20frequency%20components%20before%20high%20spatial%20frequency%0Acomponents.%0A%20%20Drawing%20on%20this%20neuroscientific%20inspiration%2C%20we%20investigate%20the%20effect%20of%0Aintroducing%20patches%20from%20different%20spatial%20frequencies%20into%20Vision%20Transformers%0A%28ViTs%29.%0A%20%20We%20name%20this%20model%20Retina%20Vision%20Transformer%20%28RetinaViT%29%20due%20to%20its%0Ainspiration%20from%20the%20human%20visual%20system.%0A%20%20Our%20experiments%20on%20benchmark%20data%20show%20that%20RetinaViT%20exhibits%20a%20strong%0Atendency%20to%20attend%20to%20low%20spatial%20frequency%20components%20in%20the%20early%20layers%2C%20and%0Ashifts%20its%20attention%20to%20high%20spatial%20frequency%20components%20as%20the%20network%20goes%0Adeeper.%0A%20%20This%20tendency%20emerged%20by%20itself%20without%20any%20additional%20inductive%20bias%2C%20and%0Aaligns%20with%20the%20visual%20processing%20order%20of%20the%20human%20visual%20system.%0A%20%20We%20hypothesise%20that%20RetinaViT%20captures%20structural%20features%2C%20or%20the%20gist%20of%0Athe%20scene%2C%20in%20earlier%20layers%2C%20before%20attending%20to%20fine%20details%20in%20subsequent%0Alayers%2C%20which%20is%20the%20reverse%20of%20the%20processing%20order%20of%20mainstream%20backbone%0Avision%20models%2C%20such%20as%20CNNs.%0A%20%20We%20also%20observe%20that%20RetinaViT%20is%20more%20robust%20to%20significant%20reductions%20in%0Amodel%20size%20compared%20to%20the%20original%20ViT%2C%20which%20we%20hypothesise%20to%20have%20come%20from%0Aits%20ability%20to%20capture%20the%20gist%20of%20the%20scene%20early.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetina%2520Vision%2520Transformer%2520%2528RetinaViT%2529%253A%2520Introducing%2520Scaled%2520Patches%2520into%250A%2520%2520Vision%2520Transformers%26entry.906535625%3DYuyang%2520Shu%2520and%2520Michael%2520E.%2520Bain%26entry.1292438233%3D%2520%2520Humans%2520see%2520low%2520spatial%2520frequency%2520components%2520before%2520high%2520spatial%2520frequency%250Acomponents.%250A%2520%2520Drawing%2520on%2520this%2520neuroscientific%2520inspiration%252C%2520we%2520investigate%2520the%2520effect%2520of%250Aintroducing%2520patches%2520from%2520different%2520spatial%2520frequencies%2520into%2520Vision%2520Transformers%250A%2528ViTs%2529.%250A%2520%2520We%2520name%2520this%2520model%2520Retina%2520Vision%2520Transformer%2520%2528RetinaViT%2529%2520due%2520to%2520its%250Ainspiration%2520from%2520the%2520human%2520visual%2520system.%250A%2520%2520Our%2520experiments%2520on%2520benchmark%2520data%2520show%2520that%2520RetinaViT%2520exhibits%2520a%2520strong%250Atendency%2520to%2520attend%2520to%2520low%2520spatial%2520frequency%2520components%2520in%2520the%2520early%2520layers%252C%2520and%250Ashifts%2520its%2520attention%2520to%2520high%2520spatial%2520frequency%2520components%2520as%2520the%2520network%2520goes%250Adeeper.%250A%2520%2520This%2520tendency%2520emerged%2520by%2520itself%2520without%2520any%2520additional%2520inductive%2520bias%252C%2520and%250Aaligns%2520with%2520the%2520visual%2520processing%2520order%2520of%2520the%2520human%2520visual%2520system.%250A%2520%2520We%2520hypothesise%2520that%2520RetinaViT%2520captures%2520structural%2520features%252C%2520or%2520the%2520gist%2520of%250Athe%2520scene%252C%2520in%2520earlier%2520layers%252C%2520before%2520attending%2520to%2520fine%2520details%2520in%2520subsequent%250Alayers%252C%2520which%2520is%2520the%2520reverse%2520of%2520the%2520processing%2520order%2520of%2520mainstream%2520backbone%250Avision%2520models%252C%2520such%2520as%2520CNNs.%250A%2520%2520We%2520also%2520observe%2520that%2520RetinaViT%2520is%2520more%2520robust%2520to%2520significant%2520reductions%2520in%250Amodel%2520size%2520compared%2520to%2520the%2520original%2520ViT%252C%2520which%2520we%2520hypothesise%2520to%2520have%2520come%2520from%250Aits%2520ability%2520to%2520capture%2520the%2520gist%2520of%2520the%2520scene%2520early.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers&entry.906535625=Yuyang%20Shu%20and%20Michael%20E.%20Bain&entry.1292438233=%20%20Humans%20see%20low%20spatial%20frequency%20components%20before%20high%20spatial%20frequency%0Acomponents.%0A%20%20Drawing%20on%20this%20neuroscientific%20inspiration%2C%20we%20investigate%20the%20effect%20of%0Aintroducing%20patches%20from%20different%20spatial%20frequencies%20into%20Vision%20Transformers%0A%28ViTs%29.%0A%20%20We%20name%20this%20model%20Retina%20Vision%20Transformer%20%28RetinaViT%29%20due%20to%20its%0Ainspiration%20from%20the%20human%20visual%20system.%0A%20%20Our%20experiments%20on%20benchmark%20data%20show%20that%20RetinaViT%20exhibits%20a%20strong%0Atendency%20to%20attend%20to%20low%20spatial%20frequency%20components%20in%20the%20early%20layers%2C%20and%0Ashifts%20its%20attention%20to%20high%20spatial%20frequency%20components%20as%20the%20network%20goes%0Adeeper.%0A%20%20This%20tendency%20emerged%20by%20itself%20without%20any%20additional%20inductive%20bias%2C%20and%0Aaligns%20with%20the%20visual%20processing%20order%20of%20the%20human%20visual%20system.%0A%20%20We%20hypothesise%20that%20RetinaViT%20captures%20structural%20features%2C%20or%20the%20gist%20of%0Athe%20scene%2C%20in%20earlier%20layers%2C%20before%20attending%20to%20fine%20details%20in%20subsequent%0Alayers%2C%20which%20is%20the%20reverse%20of%20the%20processing%20order%20of%20mainstream%20backbone%0Avision%20models%2C%20such%20as%20CNNs.%0A%20%20We%20also%20observe%20that%20RetinaViT%20is%20more%20robust%20to%20significant%20reductions%20in%0Amodel%20size%20compared%20to%20the%20original%20ViT%2C%20which%20we%20hypothesise%20to%20have%20come%20from%0Aits%20ability%20to%20capture%20the%20gist%20of%20the%20scene%20early.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13677v2&entry.124074799=Read"},
{"title": "Aegis: Automated Error Generation and Identification for Multi-Agent\n  Systems", "author": "Fanqi Kong and Ruijie Zhang and Huaxiao Yin and Guibin Zhang and Xiaofei Zhang and Ziang Chen and Zhaowei Zhang and Xiaoyuan Zhang and Song-Chun Zhu and Xue Feng", "abstract": "  As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.\n", "link": "http://arxiv.org/abs/2509.14295v3", "date": "2025-09-25", "relevancy": 2.1854, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5489}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aegis%3A%20Automated%20Error%20Generation%20and%20Identification%20for%20Multi-Agent%0A%20%20Systems&body=Title%3A%20Aegis%3A%20Automated%20Error%20Generation%20and%20Identification%20for%20Multi-Agent%0A%20%20Systems%0AAuthor%3A%20Fanqi%20Kong%20and%20Ruijie%20Zhang%20and%20Huaxiao%20Yin%20and%20Guibin%20Zhang%20and%20Xiaofei%20Zhang%20and%20Ziang%20Chen%20and%20Zhaowei%20Zhang%20and%20Xiaoyuan%20Zhang%20and%20Song-Chun%20Zhu%20and%20Xue%20Feng%0AAbstract%3A%20%20%20As%20Multi-Agent%20Systems%20%28MAS%29%20become%20increasingly%20autonomous%20and%20complex%2C%0Aunderstanding%20their%20error%20modes%20is%20critical%20for%20ensuring%20their%20reliability%20and%0Asafety.%20However%2C%20research%20in%20this%20area%20has%20been%20severely%20hampered%20by%20the%20lack%0Aof%20large-scale%2C%20diverse%20datasets%20with%20precise%2C%20ground-truth%20error%20labels.%20To%0Aaddress%20this%20bottleneck%2C%20we%20introduce%20%5Ctextbf%7BAEGIS%7D%2C%20a%20novel%20framework%20for%0A%5Ctextbf%7BA%7Dutomated%20%5Ctextbf%7BE%7Drror%20%5Ctextbf%7BG%7Deneration%20and%0A%5Ctextbf%7BI%7Ddentification%20for%20Multi-Agent%20%5Ctextbf%7BS%7Dystems.%20By%20systematically%0Ainjecting%20controllable%20and%20traceable%20errors%20into%20initially%20successful%0Atrajectories%2C%20we%20create%20a%20rich%20dataset%20of%20realistic%20failures.%20This%20is%20achieved%0Ausing%20a%20context-aware%2C%20LLM-based%20adaptive%20manipulator%20that%20performs%0Asophisticated%20attacks%20like%20prompt%20injection%20and%20response%20corruption%20to%20induce%0Aspecific%2C%20predefined%20error%20modes.%20We%20demonstrate%20the%20value%20of%20our%20dataset%20by%0Aexploring%20three%20distinct%20learning%20paradigms%20for%20the%20error%20identification%20task%3A%0ASupervised%20Fine-Tuning%2C%20Reinforcement%20Learning%2C%20and%20Contrastive%20Learning.%20Our%0Acomprehensive%20experiments%20show%20that%20models%20trained%20on%20AEGIS%20data%20achieve%0Asubstantial%20improvements%20across%20all%20three%20learning%20paradigms.%20Notably%2C%20several%0Aof%20our%20fine-tuned%20models%20demonstrate%20performance%20competitive%20with%20or%20superior%0Ato%20proprietary%20systems%20an%20order%20of%20magnitude%20larger%2C%20validating%20our%20automated%0Adata%20generation%20framework%20as%20a%20crucial%20resource%20for%20developing%20more%20robust%20and%0Ainterpretable%20multi-agent%20systems.%20Our%20project%20website%20is%20available%20at%0Ahttps%3A//kfq20.github.io/AEGIS-Website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAegis%253A%2520Automated%2520Error%2520Generation%2520and%2520Identification%2520for%2520Multi-Agent%250A%2520%2520Systems%26entry.906535625%3DFanqi%2520Kong%2520and%2520Ruijie%2520Zhang%2520and%2520Huaxiao%2520Yin%2520and%2520Guibin%2520Zhang%2520and%2520Xiaofei%2520Zhang%2520and%2520Ziang%2520Chen%2520and%2520Zhaowei%2520Zhang%2520and%2520Xiaoyuan%2520Zhang%2520and%2520Song-Chun%2520Zhu%2520and%2520Xue%2520Feng%26entry.1292438233%3D%2520%2520As%2520Multi-Agent%2520Systems%2520%2528MAS%2529%2520become%2520increasingly%2520autonomous%2520and%2520complex%252C%250Aunderstanding%2520their%2520error%2520modes%2520is%2520critical%2520for%2520ensuring%2520their%2520reliability%2520and%250Asafety.%2520However%252C%2520research%2520in%2520this%2520area%2520has%2520been%2520severely%2520hampered%2520by%2520the%2520lack%250Aof%2520large-scale%252C%2520diverse%2520datasets%2520with%2520precise%252C%2520ground-truth%2520error%2520labels.%2520To%250Aaddress%2520this%2520bottleneck%252C%2520we%2520introduce%2520%255Ctextbf%257BAEGIS%257D%252C%2520a%2520novel%2520framework%2520for%250A%255Ctextbf%257BA%257Dutomated%2520%255Ctextbf%257BE%257Drror%2520%255Ctextbf%257BG%257Deneration%2520and%250A%255Ctextbf%257BI%257Ddentification%2520for%2520Multi-Agent%2520%255Ctextbf%257BS%257Dystems.%2520By%2520systematically%250Ainjecting%2520controllable%2520and%2520traceable%2520errors%2520into%2520initially%2520successful%250Atrajectories%252C%2520we%2520create%2520a%2520rich%2520dataset%2520of%2520realistic%2520failures.%2520This%2520is%2520achieved%250Ausing%2520a%2520context-aware%252C%2520LLM-based%2520adaptive%2520manipulator%2520that%2520performs%250Asophisticated%2520attacks%2520like%2520prompt%2520injection%2520and%2520response%2520corruption%2520to%2520induce%250Aspecific%252C%2520predefined%2520error%2520modes.%2520We%2520demonstrate%2520the%2520value%2520of%2520our%2520dataset%2520by%250Aexploring%2520three%2520distinct%2520learning%2520paradigms%2520for%2520the%2520error%2520identification%2520task%253A%250ASupervised%2520Fine-Tuning%252C%2520Reinforcement%2520Learning%252C%2520and%2520Contrastive%2520Learning.%2520Our%250Acomprehensive%2520experiments%2520show%2520that%2520models%2520trained%2520on%2520AEGIS%2520data%2520achieve%250Asubstantial%2520improvements%2520across%2520all%2520three%2520learning%2520paradigms.%2520Notably%252C%2520several%250Aof%2520our%2520fine-tuned%2520models%2520demonstrate%2520performance%2520competitive%2520with%2520or%2520superior%250Ato%2520proprietary%2520systems%2520an%2520order%2520of%2520magnitude%2520larger%252C%2520validating%2520our%2520automated%250Adata%2520generation%2520framework%2520as%2520a%2520crucial%2520resource%2520for%2520developing%2520more%2520robust%2520and%250Ainterpretable%2520multi-agent%2520systems.%2520Our%2520project%2520website%2520is%2520available%2520at%250Ahttps%253A//kfq20.github.io/AEGIS-Website.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aegis%3A%20Automated%20Error%20Generation%20and%20Identification%20for%20Multi-Agent%0A%20%20Systems&entry.906535625=Fanqi%20Kong%20and%20Ruijie%20Zhang%20and%20Huaxiao%20Yin%20and%20Guibin%20Zhang%20and%20Xiaofei%20Zhang%20and%20Ziang%20Chen%20and%20Zhaowei%20Zhang%20and%20Xiaoyuan%20Zhang%20and%20Song-Chun%20Zhu%20and%20Xue%20Feng&entry.1292438233=%20%20As%20Multi-Agent%20Systems%20%28MAS%29%20become%20increasingly%20autonomous%20and%20complex%2C%0Aunderstanding%20their%20error%20modes%20is%20critical%20for%20ensuring%20their%20reliability%20and%0Asafety.%20However%2C%20research%20in%20this%20area%20has%20been%20severely%20hampered%20by%20the%20lack%0Aof%20large-scale%2C%20diverse%20datasets%20with%20precise%2C%20ground-truth%20error%20labels.%20To%0Aaddress%20this%20bottleneck%2C%20we%20introduce%20%5Ctextbf%7BAEGIS%7D%2C%20a%20novel%20framework%20for%0A%5Ctextbf%7BA%7Dutomated%20%5Ctextbf%7BE%7Drror%20%5Ctextbf%7BG%7Deneration%20and%0A%5Ctextbf%7BI%7Ddentification%20for%20Multi-Agent%20%5Ctextbf%7BS%7Dystems.%20By%20systematically%0Ainjecting%20controllable%20and%20traceable%20errors%20into%20initially%20successful%0Atrajectories%2C%20we%20create%20a%20rich%20dataset%20of%20realistic%20failures.%20This%20is%20achieved%0Ausing%20a%20context-aware%2C%20LLM-based%20adaptive%20manipulator%20that%20performs%0Asophisticated%20attacks%20like%20prompt%20injection%20and%20response%20corruption%20to%20induce%0Aspecific%2C%20predefined%20error%20modes.%20We%20demonstrate%20the%20value%20of%20our%20dataset%20by%0Aexploring%20three%20distinct%20learning%20paradigms%20for%20the%20error%20identification%20task%3A%0ASupervised%20Fine-Tuning%2C%20Reinforcement%20Learning%2C%20and%20Contrastive%20Learning.%20Our%0Acomprehensive%20experiments%20show%20that%20models%20trained%20on%20AEGIS%20data%20achieve%0Asubstantial%20improvements%20across%20all%20three%20learning%20paradigms.%20Notably%2C%20several%0Aof%20our%20fine-tuned%20models%20demonstrate%20performance%20competitive%20with%20or%20superior%0Ato%20proprietary%20systems%20an%20order%20of%20magnitude%20larger%2C%20validating%20our%20automated%0Adata%20generation%20framework%20as%20a%20crucial%20resource%20for%20developing%20more%20robust%20and%0Ainterpretable%20multi-agent%20systems.%20Our%20project%20website%20is%20available%20at%0Ahttps%3A//kfq20.github.io/AEGIS-Website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14295v3&entry.124074799=Read"},
{"title": "CodeBrain: Towards Decoupled Interpretability and Multi-Scale\n  Architecture for EEG Foundation Model", "author": "Jingying Ma and Feng Wu and Qika Lin and Yucheng Xing and Chenyu Liu and Ziyu Jia and Mengling Feng", "abstract": "  Electroencephalography (EEG) provides real-time insights into brain activity\nand supports diverse applications in neuroscience. While EEG foundation models\n(EFMs) have emerged to address the scalability issues of task-specific models,\ncurrent approaches still yield clinically uninterpretable and weakly\ndiscriminative representations, inefficiently capture global dependencies, and\nneglect important local neural events. We present CodeBrain, a two-stage EFM\ndesigned to fill this gap. In the first stage, we introduce the\nTFDual-Tokenizer, which decouples heterogeneous temporal and frequency EEG\nsignals into discrete tokens, quadratically expanding the representation space\nto enhance discriminative power and offering domain-specific interpretability\nby suggesting potential links to neural events and spectral rhythms. In the\nsecond stage, we propose the multi-scale EEGSSM architecture, which combines\nstructured global convolution with sliding window attention to efficiently\ncapture both sparse long-range and local dependencies, reflecting the brain's\nsmall-world topology. Pretrained on the largest public EEG corpus, CodeBrain\nachieves strong generalization across 8 downstream tasks and 10 datasets under\ndistribution shifts, supported by comprehensive ablations, scaling-law\nanalyses, and interpretability evaluations. Both code and pretraining weights\nwill be released in the future version.\n", "link": "http://arxiv.org/abs/2506.09110v2", "date": "2025-09-25", "relevancy": 2.1828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeBrain%3A%20Towards%20Decoupled%20Interpretability%20and%20Multi-Scale%0A%20%20Architecture%20for%20EEG%20Foundation%20Model&body=Title%3A%20CodeBrain%3A%20Towards%20Decoupled%20Interpretability%20and%20Multi-Scale%0A%20%20Architecture%20for%20EEG%20Foundation%20Model%0AAuthor%3A%20Jingying%20Ma%20and%20Feng%20Wu%20and%20Qika%20Lin%20and%20Yucheng%20Xing%20and%20Chenyu%20Liu%20and%20Ziyu%20Jia%20and%20Mengling%20Feng%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20provides%20real-time%20insights%20into%20brain%20activity%0Aand%20supports%20diverse%20applications%20in%20neuroscience.%20While%20EEG%20foundation%20models%0A%28EFMs%29%20have%20emerged%20to%20address%20the%20scalability%20issues%20of%20task-specific%20models%2C%0Acurrent%20approaches%20still%20yield%20clinically%20uninterpretable%20and%20weakly%0Adiscriminative%20representations%2C%20inefficiently%20capture%20global%20dependencies%2C%20and%0Aneglect%20important%20local%20neural%20events.%20We%20present%20CodeBrain%2C%20a%20two-stage%20EFM%0Adesigned%20to%20fill%20this%20gap.%20In%20the%20first%20stage%2C%20we%20introduce%20the%0ATFDual-Tokenizer%2C%20which%20decouples%20heterogeneous%20temporal%20and%20frequency%20EEG%0Asignals%20into%20discrete%20tokens%2C%20quadratically%20expanding%20the%20representation%20space%0Ato%20enhance%20discriminative%20power%20and%20offering%20domain-specific%20interpretability%0Aby%20suggesting%20potential%20links%20to%20neural%20events%20and%20spectral%20rhythms.%20In%20the%0Asecond%20stage%2C%20we%20propose%20the%20multi-scale%20EEGSSM%20architecture%2C%20which%20combines%0Astructured%20global%20convolution%20with%20sliding%20window%20attention%20to%20efficiently%0Acapture%20both%20sparse%20long-range%20and%20local%20dependencies%2C%20reflecting%20the%20brain%27s%0Asmall-world%20topology.%20Pretrained%20on%20the%20largest%20public%20EEG%20corpus%2C%20CodeBrain%0Aachieves%20strong%20generalization%20across%208%20downstream%20tasks%20and%2010%20datasets%20under%0Adistribution%20shifts%2C%20supported%20by%20comprehensive%20ablations%2C%20scaling-law%0Aanalyses%2C%20and%20interpretability%20evaluations.%20Both%20code%20and%20pretraining%20weights%0Awill%20be%20released%20in%20the%20future%20version.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeBrain%253A%2520Towards%2520Decoupled%2520Interpretability%2520and%2520Multi-Scale%250A%2520%2520Architecture%2520for%2520EEG%2520Foundation%2520Model%26entry.906535625%3DJingying%2520Ma%2520and%2520Feng%2520Wu%2520and%2520Qika%2520Lin%2520and%2520Yucheng%2520Xing%2520and%2520Chenyu%2520Liu%2520and%2520Ziyu%2520Jia%2520and%2520Mengling%2520Feng%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520provides%2520real-time%2520insights%2520into%2520brain%2520activity%250Aand%2520supports%2520diverse%2520applications%2520in%2520neuroscience.%2520While%2520EEG%2520foundation%2520models%250A%2528EFMs%2529%2520have%2520emerged%2520to%2520address%2520the%2520scalability%2520issues%2520of%2520task-specific%2520models%252C%250Acurrent%2520approaches%2520still%2520yield%2520clinically%2520uninterpretable%2520and%2520weakly%250Adiscriminative%2520representations%252C%2520inefficiently%2520capture%2520global%2520dependencies%252C%2520and%250Aneglect%2520important%2520local%2520neural%2520events.%2520We%2520present%2520CodeBrain%252C%2520a%2520two-stage%2520EFM%250Adesigned%2520to%2520fill%2520this%2520gap.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520the%250ATFDual-Tokenizer%252C%2520which%2520decouples%2520heterogeneous%2520temporal%2520and%2520frequency%2520EEG%250Asignals%2520into%2520discrete%2520tokens%252C%2520quadratically%2520expanding%2520the%2520representation%2520space%250Ato%2520enhance%2520discriminative%2520power%2520and%2520offering%2520domain-specific%2520interpretability%250Aby%2520suggesting%2520potential%2520links%2520to%2520neural%2520events%2520and%2520spectral%2520rhythms.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520propose%2520the%2520multi-scale%2520EEGSSM%2520architecture%252C%2520which%2520combines%250Astructured%2520global%2520convolution%2520with%2520sliding%2520window%2520attention%2520to%2520efficiently%250Acapture%2520both%2520sparse%2520long-range%2520and%2520local%2520dependencies%252C%2520reflecting%2520the%2520brain%2527s%250Asmall-world%2520topology.%2520Pretrained%2520on%2520the%2520largest%2520public%2520EEG%2520corpus%252C%2520CodeBrain%250Aachieves%2520strong%2520generalization%2520across%25208%2520downstream%2520tasks%2520and%252010%2520datasets%2520under%250Adistribution%2520shifts%252C%2520supported%2520by%2520comprehensive%2520ablations%252C%2520scaling-law%250Aanalyses%252C%2520and%2520interpretability%2520evaluations.%2520Both%2520code%2520and%2520pretraining%2520weights%250Awill%2520be%2520released%2520in%2520the%2520future%2520version.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeBrain%3A%20Towards%20Decoupled%20Interpretability%20and%20Multi-Scale%0A%20%20Architecture%20for%20EEG%20Foundation%20Model&entry.906535625=Jingying%20Ma%20and%20Feng%20Wu%20and%20Qika%20Lin%20and%20Yucheng%20Xing%20and%20Chenyu%20Liu%20and%20Ziyu%20Jia%20and%20Mengling%20Feng&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20provides%20real-time%20insights%20into%20brain%20activity%0Aand%20supports%20diverse%20applications%20in%20neuroscience.%20While%20EEG%20foundation%20models%0A%28EFMs%29%20have%20emerged%20to%20address%20the%20scalability%20issues%20of%20task-specific%20models%2C%0Acurrent%20approaches%20still%20yield%20clinically%20uninterpretable%20and%20weakly%0Adiscriminative%20representations%2C%20inefficiently%20capture%20global%20dependencies%2C%20and%0Aneglect%20important%20local%20neural%20events.%20We%20present%20CodeBrain%2C%20a%20two-stage%20EFM%0Adesigned%20to%20fill%20this%20gap.%20In%20the%20first%20stage%2C%20we%20introduce%20the%0ATFDual-Tokenizer%2C%20which%20decouples%20heterogeneous%20temporal%20and%20frequency%20EEG%0Asignals%20into%20discrete%20tokens%2C%20quadratically%20expanding%20the%20representation%20space%0Ato%20enhance%20discriminative%20power%20and%20offering%20domain-specific%20interpretability%0Aby%20suggesting%20potential%20links%20to%20neural%20events%20and%20spectral%20rhythms.%20In%20the%0Asecond%20stage%2C%20we%20propose%20the%20multi-scale%20EEGSSM%20architecture%2C%20which%20combines%0Astructured%20global%20convolution%20with%20sliding%20window%20attention%20to%20efficiently%0Acapture%20both%20sparse%20long-range%20and%20local%20dependencies%2C%20reflecting%20the%20brain%27s%0Asmall-world%20topology.%20Pretrained%20on%20the%20largest%20public%20EEG%20corpus%2C%20CodeBrain%0Aachieves%20strong%20generalization%20across%208%20downstream%20tasks%20and%2010%20datasets%20under%0Adistribution%20shifts%2C%20supported%20by%20comprehensive%20ablations%2C%20scaling-law%0Aanalyses%2C%20and%20interpretability%20evaluations.%20Both%20code%20and%20pretraining%20weights%0Awill%20be%20released%20in%20the%20future%20version.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09110v2&entry.124074799=Read"},
{"title": "Training-Free Stein Diffusion Guidance: Posterior Correction for\n  Sampling Beyond High-Density Regions", "author": "Van Khoa Nguyen and Lionel Blond\u00e9 and Alexandros Kalousis", "abstract": "  Training free diffusion guidance provides a flexible way to leverage\noff-the-shelf classifiers without additional training. Yet, current approaches\nhinge on posterior approximations via Tweedie's formula, which often yield\nunreliable guidance, particularly in low-density regions. Stochastic optimal\ncontrol (SOC), in contrast, provides principled posterior simulation but is\nprohibitively expensive for fast sampling. In this work, we reconcile the\nstrengths of these paradigms by introducing Stein Diffusion Guidance (SDG), a\nnovel training-free framework grounded in a surrogate SOC objective. We\nestablish a theoretical bound on the value function, demonstrating the\nnecessity of correcting approximate posteriors to faithfully reflect true\ndiffusion dynamics. Leveraging Stein variational inference, SDG identifies the\nsteepest descent direction that minimizes the Kullback-Leibler divergence\nbetween approximate and true posteriors. By incorporating a principled Stein\ncorrection mechanism and a novel running cost functional, SDG enables effective\nguidance in low-density regions. Experiments on molecular low-density sampling\ntasks suggest that SDG consistently surpasses standard training-free guidance\nmethods, highlighting its potential for broader diffusion-based sampling beyond\nhigh-density regions.\n", "link": "http://arxiv.org/abs/2507.05482v2", "date": "2025-09-25", "relevancy": 2.1827, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5909}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5557}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Stein%20Diffusion%20Guidance%3A%20Posterior%20Correction%20for%0A%20%20Sampling%20Beyond%20High-Density%20Regions&body=Title%3A%20Training-Free%20Stein%20Diffusion%20Guidance%3A%20Posterior%20Correction%20for%0A%20%20Sampling%20Beyond%20High-Density%20Regions%0AAuthor%3A%20Van%20Khoa%20Nguyen%20and%20Lionel%20Blond%C3%A9%20and%20Alexandros%20Kalousis%0AAbstract%3A%20%20%20Training%20free%20diffusion%20guidance%20provides%20a%20flexible%20way%20to%20leverage%0Aoff-the-shelf%20classifiers%20without%20additional%20training.%20Yet%2C%20current%20approaches%0Ahinge%20on%20posterior%20approximations%20via%20Tweedie%27s%20formula%2C%20which%20often%20yield%0Aunreliable%20guidance%2C%20particularly%20in%20low-density%20regions.%20Stochastic%20optimal%0Acontrol%20%28SOC%29%2C%20in%20contrast%2C%20provides%20principled%20posterior%20simulation%20but%20is%0Aprohibitively%20expensive%20for%20fast%20sampling.%20In%20this%20work%2C%20we%20reconcile%20the%0Astrengths%20of%20these%20paradigms%20by%20introducing%20Stein%20Diffusion%20Guidance%20%28SDG%29%2C%20a%0Anovel%20training-free%20framework%20grounded%20in%20a%20surrogate%20SOC%20objective.%20We%0Aestablish%20a%20theoretical%20bound%20on%20the%20value%20function%2C%20demonstrating%20the%0Anecessity%20of%20correcting%20approximate%20posteriors%20to%20faithfully%20reflect%20true%0Adiffusion%20dynamics.%20Leveraging%20Stein%20variational%20inference%2C%20SDG%20identifies%20the%0Asteepest%20descent%20direction%20that%20minimizes%20the%20Kullback-Leibler%20divergence%0Abetween%20approximate%20and%20true%20posteriors.%20By%20incorporating%20a%20principled%20Stein%0Acorrection%20mechanism%20and%20a%20novel%20running%20cost%20functional%2C%20SDG%20enables%20effective%0Aguidance%20in%20low-density%20regions.%20Experiments%20on%20molecular%20low-density%20sampling%0Atasks%20suggest%20that%20SDG%20consistently%20surpasses%20standard%20training-free%20guidance%0Amethods%2C%20highlighting%20its%20potential%20for%20broader%20diffusion-based%20sampling%20beyond%0Ahigh-density%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Stein%2520Diffusion%2520Guidance%253A%2520Posterior%2520Correction%2520for%250A%2520%2520Sampling%2520Beyond%2520High-Density%2520Regions%26entry.906535625%3DVan%2520Khoa%2520Nguyen%2520and%2520Lionel%2520Blond%25C3%25A9%2520and%2520Alexandros%2520Kalousis%26entry.1292438233%3D%2520%2520Training%2520free%2520diffusion%2520guidance%2520provides%2520a%2520flexible%2520way%2520to%2520leverage%250Aoff-the-shelf%2520classifiers%2520without%2520additional%2520training.%2520Yet%252C%2520current%2520approaches%250Ahinge%2520on%2520posterior%2520approximations%2520via%2520Tweedie%2527s%2520formula%252C%2520which%2520often%2520yield%250Aunreliable%2520guidance%252C%2520particularly%2520in%2520low-density%2520regions.%2520Stochastic%2520optimal%250Acontrol%2520%2528SOC%2529%252C%2520in%2520contrast%252C%2520provides%2520principled%2520posterior%2520simulation%2520but%2520is%250Aprohibitively%2520expensive%2520for%2520fast%2520sampling.%2520In%2520this%2520work%252C%2520we%2520reconcile%2520the%250Astrengths%2520of%2520these%2520paradigms%2520by%2520introducing%2520Stein%2520Diffusion%2520Guidance%2520%2528SDG%2529%252C%2520a%250Anovel%2520training-free%2520framework%2520grounded%2520in%2520a%2520surrogate%2520SOC%2520objective.%2520We%250Aestablish%2520a%2520theoretical%2520bound%2520on%2520the%2520value%2520function%252C%2520demonstrating%2520the%250Anecessity%2520of%2520correcting%2520approximate%2520posteriors%2520to%2520faithfully%2520reflect%2520true%250Adiffusion%2520dynamics.%2520Leveraging%2520Stein%2520variational%2520inference%252C%2520SDG%2520identifies%2520the%250Asteepest%2520descent%2520direction%2520that%2520minimizes%2520the%2520Kullback-Leibler%2520divergence%250Abetween%2520approximate%2520and%2520true%2520posteriors.%2520By%2520incorporating%2520a%2520principled%2520Stein%250Acorrection%2520mechanism%2520and%2520a%2520novel%2520running%2520cost%2520functional%252C%2520SDG%2520enables%2520effective%250Aguidance%2520in%2520low-density%2520regions.%2520Experiments%2520on%2520molecular%2520low-density%2520sampling%250Atasks%2520suggest%2520that%2520SDG%2520consistently%2520surpasses%2520standard%2520training-free%2520guidance%250Amethods%252C%2520highlighting%2520its%2520potential%2520for%2520broader%2520diffusion-based%2520sampling%2520beyond%250Ahigh-density%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Stein%20Diffusion%20Guidance%3A%20Posterior%20Correction%20for%0A%20%20Sampling%20Beyond%20High-Density%20Regions&entry.906535625=Van%20Khoa%20Nguyen%20and%20Lionel%20Blond%C3%A9%20and%20Alexandros%20Kalousis&entry.1292438233=%20%20Training%20free%20diffusion%20guidance%20provides%20a%20flexible%20way%20to%20leverage%0Aoff-the-shelf%20classifiers%20without%20additional%20training.%20Yet%2C%20current%20approaches%0Ahinge%20on%20posterior%20approximations%20via%20Tweedie%27s%20formula%2C%20which%20often%20yield%0Aunreliable%20guidance%2C%20particularly%20in%20low-density%20regions.%20Stochastic%20optimal%0Acontrol%20%28SOC%29%2C%20in%20contrast%2C%20provides%20principled%20posterior%20simulation%20but%20is%0Aprohibitively%20expensive%20for%20fast%20sampling.%20In%20this%20work%2C%20we%20reconcile%20the%0Astrengths%20of%20these%20paradigms%20by%20introducing%20Stein%20Diffusion%20Guidance%20%28SDG%29%2C%20a%0Anovel%20training-free%20framework%20grounded%20in%20a%20surrogate%20SOC%20objective.%20We%0Aestablish%20a%20theoretical%20bound%20on%20the%20value%20function%2C%20demonstrating%20the%0Anecessity%20of%20correcting%20approximate%20posteriors%20to%20faithfully%20reflect%20true%0Adiffusion%20dynamics.%20Leveraging%20Stein%20variational%20inference%2C%20SDG%20identifies%20the%0Asteepest%20descent%20direction%20that%20minimizes%20the%20Kullback-Leibler%20divergence%0Abetween%20approximate%20and%20true%20posteriors.%20By%20incorporating%20a%20principled%20Stein%0Acorrection%20mechanism%20and%20a%20novel%20running%20cost%20functional%2C%20SDG%20enables%20effective%0Aguidance%20in%20low-density%20regions.%20Experiments%20on%20molecular%20low-density%20sampling%0Atasks%20suggest%20that%20SDG%20consistently%20surpasses%20standard%20training-free%20guidance%0Amethods%2C%20highlighting%20its%20potential%20for%20broader%20diffusion-based%20sampling%20beyond%0Ahigh-density%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05482v2&entry.124074799=Read"},
{"title": "SOOD++: Leveraging Unlabeled Data to Boost Oriented Object Detection", "author": "Dingkang Liang and Wei Hua and Chunsheng Shi and Zhikang Zou and Xiaoqing Ye and Xiang Bai", "abstract": "  Semi-supervised object detection (SSOD), leveraging unlabeled data to boost\nobject detectors, has become a hot topic recently. However, existing SSOD\napproaches mainly focus on horizontal objects, leaving oriented objects common\nin aerial images unexplored. At the same time, the annotation cost of oriented\nobjects is significantly higher than that of their horizontal counterparts.\nTherefore, in this paper, we propose a simple yet effective Semi-supervised\nOriented Object Detection method termed SOOD++. Specifically, we observe that\nobjects from aerial images usually have arbitrary orientations, small scales,\nand dense distribution, which inspires the following core designs: a Simple\nInstance-aware Dense Sampling (SIDS) strategy is used to generate comprehensive\ndense pseudo-labels; the Geometry-aware Adaptive Weighting (GAW) loss\ndynamically modulates the importance of each pair between pseudo-label and\ncorresponding prediction by leveraging the intricate geometric information of\naerial objects; we treat aerial images as global layouts and explicitly build\nthe many-to-many relationship between the sets of pseudo-labels and predictions\nvia the proposed Noise-driven Global Consistency (NGC). Extensive experiments\nconducted on various oriented object datasets under various labeled settings\ndemonstrate the effectiveness of our method. For example, on the\nDOTA-V2.0/DOTA-V1.5 benchmark, the proposed method outperforms previous\nstate-of-the-art (SOTA) by a large margin (+2.90/2.14, +2.16/2.18, and\n+2.66/2.32) mAP under 10%, 20%, and 30% labeled data settings, respectively,\nwith single-scale training and testing. More importantly, it still improves\nupon a strong supervised baseline with 70.66 mAP, trained using the full\nDOTA-V1.5 train-val set, by +1.82 mAP, resulting in a 72.48 mAP, pushing the\nnew state-of-the-art. The project page is at https://dk-liang.github.io/SOODv2/\n", "link": "http://arxiv.org/abs/2407.01016v2", "date": "2025-09-25", "relevancy": 2.1804, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5418}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOOD%2B%2B%3A%20Leveraging%20Unlabeled%20Data%20to%20Boost%20Oriented%20Object%20Detection&body=Title%3A%20SOOD%2B%2B%3A%20Leveraging%20Unlabeled%20Data%20to%20Boost%20Oriented%20Object%20Detection%0AAuthor%3A%20Dingkang%20Liang%20and%20Wei%20Hua%20and%20Chunsheng%20Shi%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Semi-supervised%20object%20detection%20%28SSOD%29%2C%20leveraging%20unlabeled%20data%20to%20boost%0Aobject%20detectors%2C%20has%20become%20a%20hot%20topic%20recently.%20However%2C%20existing%20SSOD%0Aapproaches%20mainly%20focus%20on%20horizontal%20objects%2C%20leaving%20oriented%20objects%20common%0Ain%20aerial%20images%20unexplored.%20At%20the%20same%20time%2C%20the%20annotation%20cost%20of%20oriented%0Aobjects%20is%20significantly%20higher%20than%20that%20of%20their%20horizontal%20counterparts.%0ATherefore%2C%20in%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20Semi-supervised%0AOriented%20Object%20Detection%20method%20termed%20SOOD%2B%2B.%20Specifically%2C%20we%20observe%20that%0Aobjects%20from%20aerial%20images%20usually%20have%20arbitrary%20orientations%2C%20small%20scales%2C%0Aand%20dense%20distribution%2C%20which%20inspires%20the%20following%20core%20designs%3A%20a%20Simple%0AInstance-aware%20Dense%20Sampling%20%28SIDS%29%20strategy%20is%20used%20to%20generate%20comprehensive%0Adense%20pseudo-labels%3B%20the%20Geometry-aware%20Adaptive%20Weighting%20%28GAW%29%20loss%0Adynamically%20modulates%20the%20importance%20of%20each%20pair%20between%20pseudo-label%20and%0Acorresponding%20prediction%20by%20leveraging%20the%20intricate%20geometric%20information%20of%0Aaerial%20objects%3B%20we%20treat%20aerial%20images%20as%20global%20layouts%20and%20explicitly%20build%0Athe%20many-to-many%20relationship%20between%20the%20sets%20of%20pseudo-labels%20and%20predictions%0Avia%20the%20proposed%20Noise-driven%20Global%20Consistency%20%28NGC%29.%20Extensive%20experiments%0Aconducted%20on%20various%20oriented%20object%20datasets%20under%20various%20labeled%20settings%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%20For%20example%2C%20on%20the%0ADOTA-V2.0/DOTA-V1.5%20benchmark%2C%20the%20proposed%20method%20outperforms%20previous%0Astate-of-the-art%20%28SOTA%29%20by%20a%20large%20margin%20%28%2B2.90/2.14%2C%20%2B2.16/2.18%2C%20and%0A%2B2.66/2.32%29%20mAP%20under%2010%25%2C%2020%25%2C%20and%2030%25%20labeled%20data%20settings%2C%20respectively%2C%0Awith%20single-scale%20training%20and%20testing.%20More%20importantly%2C%20it%20still%20improves%0Aupon%20a%20strong%20supervised%20baseline%20with%2070.66%20mAP%2C%20trained%20using%20the%20full%0ADOTA-V1.5%20train-val%20set%2C%20by%20%2B1.82%20mAP%2C%20resulting%20in%20a%2072.48%20mAP%2C%20pushing%20the%0Anew%20state-of-the-art.%20The%20project%20page%20is%20at%20https%3A//dk-liang.github.io/SOODv2/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOOD%252B%252B%253A%2520Leveraging%2520Unlabeled%2520Data%2520to%2520Boost%2520Oriented%2520Object%2520Detection%26entry.906535625%3DDingkang%2520Liang%2520and%2520Wei%2520Hua%2520and%2520Chunsheng%2520Shi%2520and%2520Zhikang%2520Zou%2520and%2520Xiaoqing%2520Ye%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Semi-supervised%2520object%2520detection%2520%2528SSOD%2529%252C%2520leveraging%2520unlabeled%2520data%2520to%2520boost%250Aobject%2520detectors%252C%2520has%2520become%2520a%2520hot%2520topic%2520recently.%2520However%252C%2520existing%2520SSOD%250Aapproaches%2520mainly%2520focus%2520on%2520horizontal%2520objects%252C%2520leaving%2520oriented%2520objects%2520common%250Ain%2520aerial%2520images%2520unexplored.%2520At%2520the%2520same%2520time%252C%2520the%2520annotation%2520cost%2520of%2520oriented%250Aobjects%2520is%2520significantly%2520higher%2520than%2520that%2520of%2520their%2520horizontal%2520counterparts.%250ATherefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520Semi-supervised%250AOriented%2520Object%2520Detection%2520method%2520termed%2520SOOD%252B%252B.%2520Specifically%252C%2520we%2520observe%2520that%250Aobjects%2520from%2520aerial%2520images%2520usually%2520have%2520arbitrary%2520orientations%252C%2520small%2520scales%252C%250Aand%2520dense%2520distribution%252C%2520which%2520inspires%2520the%2520following%2520core%2520designs%253A%2520a%2520Simple%250AInstance-aware%2520Dense%2520Sampling%2520%2528SIDS%2529%2520strategy%2520is%2520used%2520to%2520generate%2520comprehensive%250Adense%2520pseudo-labels%253B%2520the%2520Geometry-aware%2520Adaptive%2520Weighting%2520%2528GAW%2529%2520loss%250Adynamically%2520modulates%2520the%2520importance%2520of%2520each%2520pair%2520between%2520pseudo-label%2520and%250Acorresponding%2520prediction%2520by%2520leveraging%2520the%2520intricate%2520geometric%2520information%2520of%250Aaerial%2520objects%253B%2520we%2520treat%2520aerial%2520images%2520as%2520global%2520layouts%2520and%2520explicitly%2520build%250Athe%2520many-to-many%2520relationship%2520between%2520the%2520sets%2520of%2520pseudo-labels%2520and%2520predictions%250Avia%2520the%2520proposed%2520Noise-driven%2520Global%2520Consistency%2520%2528NGC%2529.%2520Extensive%2520experiments%250Aconducted%2520on%2520various%2520oriented%2520object%2520datasets%2520under%2520various%2520labeled%2520settings%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520For%2520example%252C%2520on%2520the%250ADOTA-V2.0/DOTA-V1.5%2520benchmark%252C%2520the%2520proposed%2520method%2520outperforms%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520by%2520a%2520large%2520margin%2520%2528%252B2.90/2.14%252C%2520%252B2.16/2.18%252C%2520and%250A%252B2.66/2.32%2529%2520mAP%2520under%252010%2525%252C%252020%2525%252C%2520and%252030%2525%2520labeled%2520data%2520settings%252C%2520respectively%252C%250Awith%2520single-scale%2520training%2520and%2520testing.%2520More%2520importantly%252C%2520it%2520still%2520improves%250Aupon%2520a%2520strong%2520supervised%2520baseline%2520with%252070.66%2520mAP%252C%2520trained%2520using%2520the%2520full%250ADOTA-V1.5%2520train-val%2520set%252C%2520by%2520%252B1.82%2520mAP%252C%2520resulting%2520in%2520a%252072.48%2520mAP%252C%2520pushing%2520the%250Anew%2520state-of-the-art.%2520The%2520project%2520page%2520is%2520at%2520https%253A//dk-liang.github.io/SOODv2/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOOD%2B%2B%3A%20Leveraging%20Unlabeled%20Data%20to%20Boost%20Oriented%20Object%20Detection&entry.906535625=Dingkang%20Liang%20and%20Wei%20Hua%20and%20Chunsheng%20Shi%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Xiang%20Bai&entry.1292438233=%20%20Semi-supervised%20object%20detection%20%28SSOD%29%2C%20leveraging%20unlabeled%20data%20to%20boost%0Aobject%20detectors%2C%20has%20become%20a%20hot%20topic%20recently.%20However%2C%20existing%20SSOD%0Aapproaches%20mainly%20focus%20on%20horizontal%20objects%2C%20leaving%20oriented%20objects%20common%0Ain%20aerial%20images%20unexplored.%20At%20the%20same%20time%2C%20the%20annotation%20cost%20of%20oriented%0Aobjects%20is%20significantly%20higher%20than%20that%20of%20their%20horizontal%20counterparts.%0ATherefore%2C%20in%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20Semi-supervised%0AOriented%20Object%20Detection%20method%20termed%20SOOD%2B%2B.%20Specifically%2C%20we%20observe%20that%0Aobjects%20from%20aerial%20images%20usually%20have%20arbitrary%20orientations%2C%20small%20scales%2C%0Aand%20dense%20distribution%2C%20which%20inspires%20the%20following%20core%20designs%3A%20a%20Simple%0AInstance-aware%20Dense%20Sampling%20%28SIDS%29%20strategy%20is%20used%20to%20generate%20comprehensive%0Adense%20pseudo-labels%3B%20the%20Geometry-aware%20Adaptive%20Weighting%20%28GAW%29%20loss%0Adynamically%20modulates%20the%20importance%20of%20each%20pair%20between%20pseudo-label%20and%0Acorresponding%20prediction%20by%20leveraging%20the%20intricate%20geometric%20information%20of%0Aaerial%20objects%3B%20we%20treat%20aerial%20images%20as%20global%20layouts%20and%20explicitly%20build%0Athe%20many-to-many%20relationship%20between%20the%20sets%20of%20pseudo-labels%20and%20predictions%0Avia%20the%20proposed%20Noise-driven%20Global%20Consistency%20%28NGC%29.%20Extensive%20experiments%0Aconducted%20on%20various%20oriented%20object%20datasets%20under%20various%20labeled%20settings%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%20For%20example%2C%20on%20the%0ADOTA-V2.0/DOTA-V1.5%20benchmark%2C%20the%20proposed%20method%20outperforms%20previous%0Astate-of-the-art%20%28SOTA%29%20by%20a%20large%20margin%20%28%2B2.90/2.14%2C%20%2B2.16/2.18%2C%20and%0A%2B2.66/2.32%29%20mAP%20under%2010%25%2C%2020%25%2C%20and%2030%25%20labeled%20data%20settings%2C%20respectively%2C%0Awith%20single-scale%20training%20and%20testing.%20More%20importantly%2C%20it%20still%20improves%0Aupon%20a%20strong%20supervised%20baseline%20with%2070.66%20mAP%2C%20trained%20using%20the%20full%0ADOTA-V1.5%20train-val%20set%2C%20by%20%2B1.82%20mAP%2C%20resulting%20in%20a%2072.48%20mAP%2C%20pushing%20the%0Anew%20state-of-the-art.%20The%20project%20page%20is%20at%20https%3A//dk-liang.github.io/SOODv2/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01016v2&entry.124074799=Read"},
{"title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic\n  Surveillance with ViT and LLMs over Mobile Networks", "author": "Murat Arda Onsu and Poonam Lohan and Burak Kantarci and Aisha Syed and Matthew Andrews and Sean Kennedy", "abstract": "  Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance.\n", "link": "http://arxiv.org/abs/2509.21259v1", "date": "2025-09-25", "relevancy": 2.1713, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Edge-Cloud%20Communication%20for%20Real-Time%20Urban%20Traffic%0A%20%20Surveillance%20with%20ViT%20and%20LLMs%20over%20Mobile%20Networks&body=Title%3A%20Semantic%20Edge-Cloud%20Communication%20for%20Real-Time%20Urban%20Traffic%0A%20%20Surveillance%20with%20ViT%20and%20LLMs%20over%20Mobile%20Networks%0AAuthor%3A%20Murat%20Arda%20Onsu%20and%20Poonam%20Lohan%20and%20Burak%20Kantarci%20and%20Aisha%20Syed%20and%20Matthew%20Andrews%20and%20Sean%20Kennedy%0AAbstract%3A%20%20%20Real-time%20urban%20traffic%20surveillance%20is%20vital%20for%20Intelligent%20Transportation%0ASystems%20%28ITS%29%20to%20ensure%20road%20safety%2C%20optimize%20traffic%20flow%2C%20track%20vehicle%0Atrajectories%2C%20and%20prevent%20collisions%20in%20smart%20cities.%20Deploying%20edge%20cameras%0Aacross%20urban%20environments%20is%20a%20standard%20practice%20for%20monitoring%20road%0Aconditions.%20However%2C%20integrating%20these%20with%20intelligent%20models%20requires%20a%0Arobust%20understanding%20of%20dynamic%20traffic%20scenarios%20and%20a%20responsive%20interface%0Afor%20user%20interaction.%20Although%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20can%0Ainterpret%20traffic%20images%20and%20generate%20informative%20responses%2C%20their%20deployment%0Aon%20edge%20devices%20is%20infeasible%20due%20to%20high%20computational%20demands.%20Therefore%2C%20LLM%0Ainference%20must%20occur%20on%20the%20cloud%2C%20necessitating%20visual%20data%20transmission%20from%0Aedge%20to%20cloud%2C%20a%20process%20hindered%20by%20limited%20bandwidth%2C%20leading%20to%20potential%0Adelays%20that%20compromise%20real-time%20performance.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20semantic%20communication%20framework%20that%20significantly%20reduces%0Atransmission%20overhead.%20Our%20method%20involves%20detecting%20Regions%20of%20Interest%20%28RoIs%29%0Ausing%20YOLOv11%2C%20cropping%20relevant%20image%20segments%2C%20and%20converting%20them%20into%0Acompact%20embedding%20vectors%20using%20a%20Vision%20Transformer%20%28ViT%29.%20These%20embeddings%0Aare%20then%20transmitted%20to%20the%20cloud%2C%20where%20an%20image%20decoder%20reconstructs%20the%0Acropped%20images.%20The%20reconstructed%20images%20are%20processed%20by%20a%20multimodal%20LLM%20to%0Agenerate%20traffic%20condition%20descriptions.%20This%20approach%20achieves%20a%2099.9%25%0Areduction%20in%20data%20transmission%20size%20while%20maintaining%20an%20LLM%20response%20accuracy%0Aof%2089%25%20for%20reconstructed%20cropped%20images%2C%20compared%20to%2093%25%20accuracy%20with%20original%0Acropped%20images.%20Our%20results%20demonstrate%20the%20efficiency%20and%20practicality%20of%20ViT%0Aand%20LLM-assisted%20edge-cloud%20semantic%20communication%20for%20real-time%20traffic%0Asurveillance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Edge-Cloud%2520Communication%2520for%2520Real-Time%2520Urban%2520Traffic%250A%2520%2520Surveillance%2520with%2520ViT%2520and%2520LLMs%2520over%2520Mobile%2520Networks%26entry.906535625%3DMurat%2520Arda%2520Onsu%2520and%2520Poonam%2520Lohan%2520and%2520Burak%2520Kantarci%2520and%2520Aisha%2520Syed%2520and%2520Matthew%2520Andrews%2520and%2520Sean%2520Kennedy%26entry.1292438233%3D%2520%2520Real-time%2520urban%2520traffic%2520surveillance%2520is%2520vital%2520for%2520Intelligent%2520Transportation%250ASystems%2520%2528ITS%2529%2520to%2520ensure%2520road%2520safety%252C%2520optimize%2520traffic%2520flow%252C%2520track%2520vehicle%250Atrajectories%252C%2520and%2520prevent%2520collisions%2520in%2520smart%2520cities.%2520Deploying%2520edge%2520cameras%250Aacross%2520urban%2520environments%2520is%2520a%2520standard%2520practice%2520for%2520monitoring%2520road%250Aconditions.%2520However%252C%2520integrating%2520these%2520with%2520intelligent%2520models%2520requires%2520a%250Arobust%2520understanding%2520of%2520dynamic%2520traffic%2520scenarios%2520and%2520a%2520responsive%2520interface%250Afor%2520user%2520interaction.%2520Although%2520multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%250Ainterpret%2520traffic%2520images%2520and%2520generate%2520informative%2520responses%252C%2520their%2520deployment%250Aon%2520edge%2520devices%2520is%2520infeasible%2520due%2520to%2520high%2520computational%2520demands.%2520Therefore%252C%2520LLM%250Ainference%2520must%2520occur%2520on%2520the%2520cloud%252C%2520necessitating%2520visual%2520data%2520transmission%2520from%250Aedge%2520to%2520cloud%252C%2520a%2520process%2520hindered%2520by%2520limited%2520bandwidth%252C%2520leading%2520to%2520potential%250Adelays%2520that%2520compromise%2520real-time%2520performance.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520semantic%2520communication%2520framework%2520that%2520significantly%2520reduces%250Atransmission%2520overhead.%2520Our%2520method%2520involves%2520detecting%2520Regions%2520of%2520Interest%2520%2528RoIs%2529%250Ausing%2520YOLOv11%252C%2520cropping%2520relevant%2520image%2520segments%252C%2520and%2520converting%2520them%2520into%250Acompact%2520embedding%2520vectors%2520using%2520a%2520Vision%2520Transformer%2520%2528ViT%2529.%2520These%2520embeddings%250Aare%2520then%2520transmitted%2520to%2520the%2520cloud%252C%2520where%2520an%2520image%2520decoder%2520reconstructs%2520the%250Acropped%2520images.%2520The%2520reconstructed%2520images%2520are%2520processed%2520by%2520a%2520multimodal%2520LLM%2520to%250Agenerate%2520traffic%2520condition%2520descriptions.%2520This%2520approach%2520achieves%2520a%252099.9%2525%250Areduction%2520in%2520data%2520transmission%2520size%2520while%2520maintaining%2520an%2520LLM%2520response%2520accuracy%250Aof%252089%2525%2520for%2520reconstructed%2520cropped%2520images%252C%2520compared%2520to%252093%2525%2520accuracy%2520with%2520original%250Acropped%2520images.%2520Our%2520results%2520demonstrate%2520the%2520efficiency%2520and%2520practicality%2520of%2520ViT%250Aand%2520LLM-assisted%2520edge-cloud%2520semantic%2520communication%2520for%2520real-time%2520traffic%250Asurveillance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Edge-Cloud%20Communication%20for%20Real-Time%20Urban%20Traffic%0A%20%20Surveillance%20with%20ViT%20and%20LLMs%20over%20Mobile%20Networks&entry.906535625=Murat%20Arda%20Onsu%20and%20Poonam%20Lohan%20and%20Burak%20Kantarci%20and%20Aisha%20Syed%20and%20Matthew%20Andrews%20and%20Sean%20Kennedy&entry.1292438233=%20%20Real-time%20urban%20traffic%20surveillance%20is%20vital%20for%20Intelligent%20Transportation%0ASystems%20%28ITS%29%20to%20ensure%20road%20safety%2C%20optimize%20traffic%20flow%2C%20track%20vehicle%0Atrajectories%2C%20and%20prevent%20collisions%20in%20smart%20cities.%20Deploying%20edge%20cameras%0Aacross%20urban%20environments%20is%20a%20standard%20practice%20for%20monitoring%20road%0Aconditions.%20However%2C%20integrating%20these%20with%20intelligent%20models%20requires%20a%0Arobust%20understanding%20of%20dynamic%20traffic%20scenarios%20and%20a%20responsive%20interface%0Afor%20user%20interaction.%20Although%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20can%0Ainterpret%20traffic%20images%20and%20generate%20informative%20responses%2C%20their%20deployment%0Aon%20edge%20devices%20is%20infeasible%20due%20to%20high%20computational%20demands.%20Therefore%2C%20LLM%0Ainference%20must%20occur%20on%20the%20cloud%2C%20necessitating%20visual%20data%20transmission%20from%0Aedge%20to%20cloud%2C%20a%20process%20hindered%20by%20limited%20bandwidth%2C%20leading%20to%20potential%0Adelays%20that%20compromise%20real-time%20performance.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20semantic%20communication%20framework%20that%20significantly%20reduces%0Atransmission%20overhead.%20Our%20method%20involves%20detecting%20Regions%20of%20Interest%20%28RoIs%29%0Ausing%20YOLOv11%2C%20cropping%20relevant%20image%20segments%2C%20and%20converting%20them%20into%0Acompact%20embedding%20vectors%20using%20a%20Vision%20Transformer%20%28ViT%29.%20These%20embeddings%0Aare%20then%20transmitted%20to%20the%20cloud%2C%20where%20an%20image%20decoder%20reconstructs%20the%0Acropped%20images.%20The%20reconstructed%20images%20are%20processed%20by%20a%20multimodal%20LLM%20to%0Agenerate%20traffic%20condition%20descriptions.%20This%20approach%20achieves%20a%2099.9%25%0Areduction%20in%20data%20transmission%20size%20while%20maintaining%20an%20LLM%20response%20accuracy%0Aof%2089%25%20for%20reconstructed%20cropped%20images%2C%20compared%20to%2093%25%20accuracy%20with%20original%0Acropped%20images.%20Our%20results%20demonstrate%20the%20efficiency%20and%20practicality%20of%20ViT%0Aand%20LLM-assisted%20edge-cloud%20semantic%20communication%20for%20real-time%20traffic%0Asurveillance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21259v1&entry.124074799=Read"},
{"title": "FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware\n  Calibration", "author": "Yutong Li and Jieyi Zhang and Wenqiang Xu and Tutian Tang and Cewu Lu", "abstract": "  Accurate hand motion capture (MoCap) is vital for applications in robotics,\nvirtual reality, and biomechanics, yet existing systems face limitations in\ncapturing high-degree-of-freedom (DoF) joint kinematics and personalized hand\nshape. Commercial gloves offer up to 21 DoFs, which are insufficient for\ncomplex manipulations while neglecting shape variations that are critical for\ncontact-rich tasks. We present FSGlove, an inertial-based system that\nsimultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes\nvia DiffHCal, a novel calibration method. Each finger joint and the dorsum are\nequipped with IMUs, enabling high-resolution motion sensing. DiffHCal\nintegrates with the parametric MANO model through differentiable optimization,\nresolving joint kinematics, shape parameters, and sensor misalignment during a\nsingle streamlined calibration. The system achieves state-of-the-art accuracy,\nwith joint angle errors of less than 2.7 degree, and outperforms commercial\nalternatives in shape reconstruction and contact fidelity. FSGlove's\nopen-source hardware and software design ensures compatibility with current VR\nand robotics ecosystems, while its ability to capture subtle motions (e.g.,\nfingertip rubbing) bridges the gap between human dexterity and robotic\nimitation. Evaluated against Nokov optical MoCap, FSGlove advances hand\ntracking by unifying the kinematic and contact fidelity. Hardware design,\nsoftware, and more results are available at:\nhttps://sites.google.com/view/fsglove.\n", "link": "http://arxiv.org/abs/2509.21242v1", "date": "2025-09-25", "relevancy": 2.1654, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5441}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5437}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FSGlove%3A%20An%20Inertial-Based%20Hand%20Tracking%20System%20with%20Shape-Aware%0A%20%20Calibration&body=Title%3A%20FSGlove%3A%20An%20Inertial-Based%20Hand%20Tracking%20System%20with%20Shape-Aware%0A%20%20Calibration%0AAuthor%3A%20Yutong%20Li%20and%20Jieyi%20Zhang%20and%20Wenqiang%20Xu%20and%20Tutian%20Tang%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Accurate%20hand%20motion%20capture%20%28MoCap%29%20is%20vital%20for%20applications%20in%20robotics%2C%0Avirtual%20reality%2C%20and%20biomechanics%2C%20yet%20existing%20systems%20face%20limitations%20in%0Acapturing%20high-degree-of-freedom%20%28DoF%29%20joint%20kinematics%20and%20personalized%20hand%0Ashape.%20Commercial%20gloves%20offer%20up%20to%2021%20DoFs%2C%20which%20are%20insufficient%20for%0Acomplex%20manipulations%20while%20neglecting%20shape%20variations%20that%20are%20critical%20for%0Acontact-rich%20tasks.%20We%20present%20FSGlove%2C%20an%20inertial-based%20system%20that%0Asimultaneously%20tracks%20up%20to%2048%20DoFs%20and%20reconstructs%20personalized%20hand%20shapes%0Avia%20DiffHCal%2C%20a%20novel%20calibration%20method.%20Each%20finger%20joint%20and%20the%20dorsum%20are%0Aequipped%20with%20IMUs%2C%20enabling%20high-resolution%20motion%20sensing.%20DiffHCal%0Aintegrates%20with%20the%20parametric%20MANO%20model%20through%20differentiable%20optimization%2C%0Aresolving%20joint%20kinematics%2C%20shape%20parameters%2C%20and%20sensor%20misalignment%20during%20a%0Asingle%20streamlined%20calibration.%20The%20system%20achieves%20state-of-the-art%20accuracy%2C%0Awith%20joint%20angle%20errors%20of%20less%20than%202.7%20degree%2C%20and%20outperforms%20commercial%0Aalternatives%20in%20shape%20reconstruction%20and%20contact%20fidelity.%20FSGlove%27s%0Aopen-source%20hardware%20and%20software%20design%20ensures%20compatibility%20with%20current%20VR%0Aand%20robotics%20ecosystems%2C%20while%20its%20ability%20to%20capture%20subtle%20motions%20%28e.g.%2C%0Afingertip%20rubbing%29%20bridges%20the%20gap%20between%20human%20dexterity%20and%20robotic%0Aimitation.%20Evaluated%20against%20Nokov%20optical%20MoCap%2C%20FSGlove%20advances%20hand%0Atracking%20by%20unifying%20the%20kinematic%20and%20contact%20fidelity.%20Hardware%20design%2C%0Asoftware%2C%20and%20more%20results%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/fsglove.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFSGlove%253A%2520An%2520Inertial-Based%2520Hand%2520Tracking%2520System%2520with%2520Shape-Aware%250A%2520%2520Calibration%26entry.906535625%3DYutong%2520Li%2520and%2520Jieyi%2520Zhang%2520and%2520Wenqiang%2520Xu%2520and%2520Tutian%2520Tang%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Accurate%2520hand%2520motion%2520capture%2520%2528MoCap%2529%2520is%2520vital%2520for%2520applications%2520in%2520robotics%252C%250Avirtual%2520reality%252C%2520and%2520biomechanics%252C%2520yet%2520existing%2520systems%2520face%2520limitations%2520in%250Acapturing%2520high-degree-of-freedom%2520%2528DoF%2529%2520joint%2520kinematics%2520and%2520personalized%2520hand%250Ashape.%2520Commercial%2520gloves%2520offer%2520up%2520to%252021%2520DoFs%252C%2520which%2520are%2520insufficient%2520for%250Acomplex%2520manipulations%2520while%2520neglecting%2520shape%2520variations%2520that%2520are%2520critical%2520for%250Acontact-rich%2520tasks.%2520We%2520present%2520FSGlove%252C%2520an%2520inertial-based%2520system%2520that%250Asimultaneously%2520tracks%2520up%2520to%252048%2520DoFs%2520and%2520reconstructs%2520personalized%2520hand%2520shapes%250Avia%2520DiffHCal%252C%2520a%2520novel%2520calibration%2520method.%2520Each%2520finger%2520joint%2520and%2520the%2520dorsum%2520are%250Aequipped%2520with%2520IMUs%252C%2520enabling%2520high-resolution%2520motion%2520sensing.%2520DiffHCal%250Aintegrates%2520with%2520the%2520parametric%2520MANO%2520model%2520through%2520differentiable%2520optimization%252C%250Aresolving%2520joint%2520kinematics%252C%2520shape%2520parameters%252C%2520and%2520sensor%2520misalignment%2520during%2520a%250Asingle%2520streamlined%2520calibration.%2520The%2520system%2520achieves%2520state-of-the-art%2520accuracy%252C%250Awith%2520joint%2520angle%2520errors%2520of%2520less%2520than%25202.7%2520degree%252C%2520and%2520outperforms%2520commercial%250Aalternatives%2520in%2520shape%2520reconstruction%2520and%2520contact%2520fidelity.%2520FSGlove%2527s%250Aopen-source%2520hardware%2520and%2520software%2520design%2520ensures%2520compatibility%2520with%2520current%2520VR%250Aand%2520robotics%2520ecosystems%252C%2520while%2520its%2520ability%2520to%2520capture%2520subtle%2520motions%2520%2528e.g.%252C%250Afingertip%2520rubbing%2529%2520bridges%2520the%2520gap%2520between%2520human%2520dexterity%2520and%2520robotic%250Aimitation.%2520Evaluated%2520against%2520Nokov%2520optical%2520MoCap%252C%2520FSGlove%2520advances%2520hand%250Atracking%2520by%2520unifying%2520the%2520kinematic%2520and%2520contact%2520fidelity.%2520Hardware%2520design%252C%250Asoftware%252C%2520and%2520more%2520results%2520are%2520available%2520at%253A%250Ahttps%253A//sites.google.com/view/fsglove.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSGlove%3A%20An%20Inertial-Based%20Hand%20Tracking%20System%20with%20Shape-Aware%0A%20%20Calibration&entry.906535625=Yutong%20Li%20and%20Jieyi%20Zhang%20and%20Wenqiang%20Xu%20and%20Tutian%20Tang%20and%20Cewu%20Lu&entry.1292438233=%20%20Accurate%20hand%20motion%20capture%20%28MoCap%29%20is%20vital%20for%20applications%20in%20robotics%2C%0Avirtual%20reality%2C%20and%20biomechanics%2C%20yet%20existing%20systems%20face%20limitations%20in%0Acapturing%20high-degree-of-freedom%20%28DoF%29%20joint%20kinematics%20and%20personalized%20hand%0Ashape.%20Commercial%20gloves%20offer%20up%20to%2021%20DoFs%2C%20which%20are%20insufficient%20for%0Acomplex%20manipulations%20while%20neglecting%20shape%20variations%20that%20are%20critical%20for%0Acontact-rich%20tasks.%20We%20present%20FSGlove%2C%20an%20inertial-based%20system%20that%0Asimultaneously%20tracks%20up%20to%2048%20DoFs%20and%20reconstructs%20personalized%20hand%20shapes%0Avia%20DiffHCal%2C%20a%20novel%20calibration%20method.%20Each%20finger%20joint%20and%20the%20dorsum%20are%0Aequipped%20with%20IMUs%2C%20enabling%20high-resolution%20motion%20sensing.%20DiffHCal%0Aintegrates%20with%20the%20parametric%20MANO%20model%20through%20differentiable%20optimization%2C%0Aresolving%20joint%20kinematics%2C%20shape%20parameters%2C%20and%20sensor%20misalignment%20during%20a%0Asingle%20streamlined%20calibration.%20The%20system%20achieves%20state-of-the-art%20accuracy%2C%0Awith%20joint%20angle%20errors%20of%20less%20than%202.7%20degree%2C%20and%20outperforms%20commercial%0Aalternatives%20in%20shape%20reconstruction%20and%20contact%20fidelity.%20FSGlove%27s%0Aopen-source%20hardware%20and%20software%20design%20ensures%20compatibility%20with%20current%20VR%0Aand%20robotics%20ecosystems%2C%20while%20its%20ability%20to%20capture%20subtle%20motions%20%28e.g.%2C%0Afingertip%20rubbing%29%20bridges%20the%20gap%20between%20human%20dexterity%20and%20robotic%0Aimitation.%20Evaluated%20against%20Nokov%20optical%20MoCap%2C%20FSGlove%20advances%20hand%0Atracking%20by%20unifying%20the%20kinematic%20and%20contact%20fidelity.%20Hardware%20design%2C%0Asoftware%2C%20and%20more%20results%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/fsglove.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21242v1&entry.124074799=Read"},
{"title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining", "author": "Seonwu Kim and Yohan Na and Kihun Kim and Hanhee Cho and Geun Lim and Mintae Kim and Seongik Park and Ki Hyun Kim and Youngsub Han and Byoung-Ki Jeon", "abstract": "  The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.\n", "link": "http://arxiv.org/abs/2507.06795v3", "date": "2025-09-25", "relevancy": 2.1636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ixi-GEN%3A%20Efficient%20Industrial%20sLLMs%20through%20Domain%20Adaptive%20Continual%0A%20%20Pretraining&body=Title%3A%20ixi-GEN%3A%20Efficient%20Industrial%20sLLMs%20through%20Domain%20Adaptive%20Continual%0A%20%20Pretraining%0AAuthor%3A%20Seonwu%20Kim%20and%20Yohan%20Na%20and%20Kihun%20Kim%20and%20Hanhee%20Cho%20and%20Geun%20Lim%20and%20Mintae%20Kim%20and%20Seongik%20Park%20and%20Ki%20Hyun%20Kim%20and%20Youngsub%20Han%20and%20Byoung-Ki%20Jeon%0AAbstract%3A%20%20%20The%20emergence%20of%20open-source%20large%20language%20models%20%28LLMs%29%20has%20expanded%0Aopportunities%20for%20enterprise%20applications%3B%20however%2C%20many%20organizations%20still%0Alack%20the%20infrastructure%20to%20deploy%20and%20maintain%20large-scale%20models.%20As%20a%20result%2C%0Asmall%20LLMs%20%28sLLMs%29%20have%20become%20a%20practical%20alternative%2C%20despite%20their%20inherent%0Aperformance%20limitations.%20While%20Domain%20Adaptive%20Continual%20Pretraining%20%28DACP%29%20has%0Abeen%20previously%20explored%20as%20a%20method%20for%20domain%20adaptation%2C%20its%20utility%20in%0Acommercial%20applications%20remains%20under-examined.%20In%20this%20study%2C%20we%20validate%20the%0Aeffectiveness%20of%20applying%20a%20DACP-based%20recipe%20across%20diverse%20foundation%20models%0Aand%20service%20domains.%20Through%20extensive%20experiments%20and%20real-world%20evaluations%2C%0Awe%20demonstrate%20that%20DACP-applied%20sLLMs%20achieve%20substantial%20gains%20in%20target%0Adomain%20performance%20while%20preserving%20general%20capabilities%2C%20offering%20a%0Acost-efficient%20and%20scalable%20solution%20for%20enterprise-level%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dixi-GEN%253A%2520Efficient%2520Industrial%2520sLLMs%2520through%2520Domain%2520Adaptive%2520Continual%250A%2520%2520Pretraining%26entry.906535625%3DSeonwu%2520Kim%2520and%2520Yohan%2520Na%2520and%2520Kihun%2520Kim%2520and%2520Hanhee%2520Cho%2520and%2520Geun%2520Lim%2520and%2520Mintae%2520Kim%2520and%2520Seongik%2520Park%2520and%2520Ki%2520Hyun%2520Kim%2520and%2520Youngsub%2520Han%2520and%2520Byoung-Ki%2520Jeon%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520open-source%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520expanded%250Aopportunities%2520for%2520enterprise%2520applications%253B%2520however%252C%2520many%2520organizations%2520still%250Alack%2520the%2520infrastructure%2520to%2520deploy%2520and%2520maintain%2520large-scale%2520models.%2520As%2520a%2520result%252C%250Asmall%2520LLMs%2520%2528sLLMs%2529%2520have%2520become%2520a%2520practical%2520alternative%252C%2520despite%2520their%2520inherent%250Aperformance%2520limitations.%2520While%2520Domain%2520Adaptive%2520Continual%2520Pretraining%2520%2528DACP%2529%2520has%250Abeen%2520previously%2520explored%2520as%2520a%2520method%2520for%2520domain%2520adaptation%252C%2520its%2520utility%2520in%250Acommercial%2520applications%2520remains%2520under-examined.%2520In%2520this%2520study%252C%2520we%2520validate%2520the%250Aeffectiveness%2520of%2520applying%2520a%2520DACP-based%2520recipe%2520across%2520diverse%2520foundation%2520models%250Aand%2520service%2520domains.%2520Through%2520extensive%2520experiments%2520and%2520real-world%2520evaluations%252C%250Awe%2520demonstrate%2520that%2520DACP-applied%2520sLLMs%2520achieve%2520substantial%2520gains%2520in%2520target%250Adomain%2520performance%2520while%2520preserving%2520general%2520capabilities%252C%2520offering%2520a%250Acost-efficient%2520and%2520scalable%2520solution%2520for%2520enterprise-level%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ixi-GEN%3A%20Efficient%20Industrial%20sLLMs%20through%20Domain%20Adaptive%20Continual%0A%20%20Pretraining&entry.906535625=Seonwu%20Kim%20and%20Yohan%20Na%20and%20Kihun%20Kim%20and%20Hanhee%20Cho%20and%20Geun%20Lim%20and%20Mintae%20Kim%20and%20Seongik%20Park%20and%20Ki%20Hyun%20Kim%20and%20Youngsub%20Han%20and%20Byoung-Ki%20Jeon&entry.1292438233=%20%20The%20emergence%20of%20open-source%20large%20language%20models%20%28LLMs%29%20has%20expanded%0Aopportunities%20for%20enterprise%20applications%3B%20however%2C%20many%20organizations%20still%0Alack%20the%20infrastructure%20to%20deploy%20and%20maintain%20large-scale%20models.%20As%20a%20result%2C%0Asmall%20LLMs%20%28sLLMs%29%20have%20become%20a%20practical%20alternative%2C%20despite%20their%20inherent%0Aperformance%20limitations.%20While%20Domain%20Adaptive%20Continual%20Pretraining%20%28DACP%29%20has%0Abeen%20previously%20explored%20as%20a%20method%20for%20domain%20adaptation%2C%20its%20utility%20in%0Acommercial%20applications%20remains%20under-examined.%20In%20this%20study%2C%20we%20validate%20the%0Aeffectiveness%20of%20applying%20a%20DACP-based%20recipe%20across%20diverse%20foundation%20models%0Aand%20service%20domains.%20Through%20extensive%20experiments%20and%20real-world%20evaluations%2C%0Awe%20demonstrate%20that%20DACP-applied%20sLLMs%20achieve%20substantial%20gains%20in%20target%0Adomain%20performance%20while%20preserving%20general%20capabilities%2C%20offering%20a%0Acost-efficient%20and%20scalable%20solution%20for%20enterprise-level%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06795v3&entry.124074799=Read"},
{"title": "Hallucination as an Upper Bound: A New Perspective on Text-to-Image\n  Evaluation", "author": "Seyed Amir Kasaei and Mohammad Hossein Rohban", "abstract": "  In language and vision-language models, hallucination is broadly understood\nas content generated from a model's prior knowledge or biases rather than from\nthe given input. While this phenomenon has been studied in those domains, it\nhas not been clearly framed for text-to-image (T2I) generative models. Existing\nevaluations mainly focus on alignment, checking whether prompt-specified\nelements appear, but overlook what the model generates beyond the prompt. We\nargue for defining hallucination in T2I as bias-driven deviations and propose a\ntaxonomy with three categories: attribute, relation, and object hallucinations.\nThis framing introduces an upper bound for evaluation and surfaces hidden\nbiases, providing a foundation for richer assessment of T2I models.\n", "link": "http://arxiv.org/abs/2509.21257v1", "date": "2025-09-25", "relevancy": 2.1583, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucination%20as%20an%20Upper%20Bound%3A%20A%20New%20Perspective%20on%20Text-to-Image%0A%20%20Evaluation&body=Title%3A%20Hallucination%20as%20an%20Upper%20Bound%3A%20A%20New%20Perspective%20on%20Text-to-Image%0A%20%20Evaluation%0AAuthor%3A%20Seyed%20Amir%20Kasaei%20and%20Mohammad%20Hossein%20Rohban%0AAbstract%3A%20%20%20In%20language%20and%20vision-language%20models%2C%20hallucination%20is%20broadly%20understood%0Aas%20content%20generated%20from%20a%20model%27s%20prior%20knowledge%20or%20biases%20rather%20than%20from%0Athe%20given%20input.%20While%20this%20phenomenon%20has%20been%20studied%20in%20those%20domains%2C%20it%0Ahas%20not%20been%20clearly%20framed%20for%20text-to-image%20%28T2I%29%20generative%20models.%20Existing%0Aevaluations%20mainly%20focus%20on%20alignment%2C%20checking%20whether%20prompt-specified%0Aelements%20appear%2C%20but%20overlook%20what%20the%20model%20generates%20beyond%20the%20prompt.%20We%0Aargue%20for%20defining%20hallucination%20in%20T2I%20as%20bias-driven%20deviations%20and%20propose%20a%0Ataxonomy%20with%20three%20categories%3A%20attribute%2C%20relation%2C%20and%20object%20hallucinations.%0AThis%20framing%20introduces%20an%20upper%20bound%20for%20evaluation%20and%20surfaces%20hidden%0Abiases%2C%20providing%20a%20foundation%20for%20richer%20assessment%20of%20T2I%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucination%2520as%2520an%2520Upper%2520Bound%253A%2520A%2520New%2520Perspective%2520on%2520Text-to-Image%250A%2520%2520Evaluation%26entry.906535625%3DSeyed%2520Amir%2520Kasaei%2520and%2520Mohammad%2520Hossein%2520Rohban%26entry.1292438233%3D%2520%2520In%2520language%2520and%2520vision-language%2520models%252C%2520hallucination%2520is%2520broadly%2520understood%250Aas%2520content%2520generated%2520from%2520a%2520model%2527s%2520prior%2520knowledge%2520or%2520biases%2520rather%2520than%2520from%250Athe%2520given%2520input.%2520While%2520this%2520phenomenon%2520has%2520been%2520studied%2520in%2520those%2520domains%252C%2520it%250Ahas%2520not%2520been%2520clearly%2520framed%2520for%2520text-to-image%2520%2528T2I%2529%2520generative%2520models.%2520Existing%250Aevaluations%2520mainly%2520focus%2520on%2520alignment%252C%2520checking%2520whether%2520prompt-specified%250Aelements%2520appear%252C%2520but%2520overlook%2520what%2520the%2520model%2520generates%2520beyond%2520the%2520prompt.%2520We%250Aargue%2520for%2520defining%2520hallucination%2520in%2520T2I%2520as%2520bias-driven%2520deviations%2520and%2520propose%2520a%250Ataxonomy%2520with%2520three%2520categories%253A%2520attribute%252C%2520relation%252C%2520and%2520object%2520hallucinations.%250AThis%2520framing%2520introduces%2520an%2520upper%2520bound%2520for%2520evaluation%2520and%2520surfaces%2520hidden%250Abiases%252C%2520providing%2520a%2520foundation%2520for%2520richer%2520assessment%2520of%2520T2I%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucination%20as%20an%20Upper%20Bound%3A%20A%20New%20Perspective%20on%20Text-to-Image%0A%20%20Evaluation&entry.906535625=Seyed%20Amir%20Kasaei%20and%20Mohammad%20Hossein%20Rohban&entry.1292438233=%20%20In%20language%20and%20vision-language%20models%2C%20hallucination%20is%20broadly%20understood%0Aas%20content%20generated%20from%20a%20model%27s%20prior%20knowledge%20or%20biases%20rather%20than%20from%0Athe%20given%20input.%20While%20this%20phenomenon%20has%20been%20studied%20in%20those%20domains%2C%20it%0Ahas%20not%20been%20clearly%20framed%20for%20text-to-image%20%28T2I%29%20generative%20models.%20Existing%0Aevaluations%20mainly%20focus%20on%20alignment%2C%20checking%20whether%20prompt-specified%0Aelements%20appear%2C%20but%20overlook%20what%20the%20model%20generates%20beyond%20the%20prompt.%20We%0Aargue%20for%20defining%20hallucination%20in%20T2I%20as%20bias-driven%20deviations%20and%20propose%20a%0Ataxonomy%20with%20three%20categories%3A%20attribute%2C%20relation%2C%20and%20object%20hallucinations.%0AThis%20framing%20introduces%20an%20upper%20bound%20for%20evaluation%20and%20surfaces%20hidden%0Abiases%2C%20providing%20a%20foundation%20for%20richer%20assessment%20of%20T2I%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21257v1&entry.124074799=Read"},
{"title": "AbideGym: Turning Static RL Worlds into Adaptive Challenges", "author": "Abi Aryan and Zac Liu and Aaron Childress", "abstract": "  Agents trained with reinforcement learning often develop brittle policies\nthat fail when dynamics shift, a problem amplified by static benchmarks.\nAbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and\nscalable complexity to enforce intra-episode adaptation. By exposing weaknesses\nin static policies and promoting resilience, AbideGym provides a modular,\nreproducible evaluation framework for advancing research in curriculum\nlearning, continual learning, and robust generalization.\n", "link": "http://arxiv.org/abs/2509.21234v1", "date": "2025-09-25", "relevancy": 2.1527, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5424}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5417}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AbideGym%3A%20Turning%20Static%20RL%20Worlds%20into%20Adaptive%20Challenges&body=Title%3A%20AbideGym%3A%20Turning%20Static%20RL%20Worlds%20into%20Adaptive%20Challenges%0AAuthor%3A%20Abi%20Aryan%20and%20Zac%20Liu%20and%20Aaron%20Childress%0AAbstract%3A%20%20%20Agents%20trained%20with%20reinforcement%20learning%20often%20develop%20brittle%20policies%0Athat%20fail%20when%20dynamics%20shift%2C%20a%20problem%20amplified%20by%20static%20benchmarks.%0AAbideGym%2C%20a%20dynamic%20MiniGrid%20wrapper%2C%20introduces%20agent-aware%20perturbations%20and%0Ascalable%20complexity%20to%20enforce%20intra-episode%20adaptation.%20By%20exposing%20weaknesses%0Ain%20static%20policies%20and%20promoting%20resilience%2C%20AbideGym%20provides%20a%20modular%2C%0Areproducible%20evaluation%20framework%20for%20advancing%20research%20in%20curriculum%0Alearning%2C%20continual%20learning%2C%20and%20robust%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbideGym%253A%2520Turning%2520Static%2520RL%2520Worlds%2520into%2520Adaptive%2520Challenges%26entry.906535625%3DAbi%2520Aryan%2520and%2520Zac%2520Liu%2520and%2520Aaron%2520Childress%26entry.1292438233%3D%2520%2520Agents%2520trained%2520with%2520reinforcement%2520learning%2520often%2520develop%2520brittle%2520policies%250Athat%2520fail%2520when%2520dynamics%2520shift%252C%2520a%2520problem%2520amplified%2520by%2520static%2520benchmarks.%250AAbideGym%252C%2520a%2520dynamic%2520MiniGrid%2520wrapper%252C%2520introduces%2520agent-aware%2520perturbations%2520and%250Ascalable%2520complexity%2520to%2520enforce%2520intra-episode%2520adaptation.%2520By%2520exposing%2520weaknesses%250Ain%2520static%2520policies%2520and%2520promoting%2520resilience%252C%2520AbideGym%2520provides%2520a%2520modular%252C%250Areproducible%2520evaluation%2520framework%2520for%2520advancing%2520research%2520in%2520curriculum%250Alearning%252C%2520continual%2520learning%252C%2520and%2520robust%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbideGym%3A%20Turning%20Static%20RL%20Worlds%20into%20Adaptive%20Challenges&entry.906535625=Abi%20Aryan%20and%20Zac%20Liu%20and%20Aaron%20Childress&entry.1292438233=%20%20Agents%20trained%20with%20reinforcement%20learning%20often%20develop%20brittle%20policies%0Athat%20fail%20when%20dynamics%20shift%2C%20a%20problem%20amplified%20by%20static%20benchmarks.%0AAbideGym%2C%20a%20dynamic%20MiniGrid%20wrapper%2C%20introduces%20agent-aware%20perturbations%20and%0Ascalable%20complexity%20to%20enforce%20intra-episode%20adaptation.%20By%20exposing%20weaknesses%0Ain%20static%20policies%20and%20promoting%20resilience%2C%20AbideGym%20provides%20a%20modular%2C%0Areproducible%20evaluation%20framework%20for%20advancing%20research%20in%20curriculum%0Alearning%2C%20continual%20learning%2C%20and%20robust%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21234v1&entry.124074799=Read"},
{"title": "Strassen Attention, Split VC Dimension and Compositionality in\n  Transformers", "author": "Alexander Kozachinskiy and Felipe Urrutia and Hector Jimenez and Tomasz Steifer and Germ\u00e1n Pizarro and Mat\u00edas Fuentes and Francisco Meza and Cristian B. Calderon and Crist\u00f3bal Rojas", "abstract": "  We propose the first method to show theoretical limitations for one-layer\nsoftmax transformers with arbitrarily many precision bits (even infinite). We\nestablish those limitations for three tasks that require advanced reasoning.\nThe first task, Match 3 (Sanford et al., 2023), requires looking at all\npossible token triplets in an input sequence. The second and third tasks\naddress compositionality-based reasoning: function composition (Peng et al.,\n2024) and binary relations composition, respectively. We formally prove the\ninability of one-layer softmax Transformers to solve any of these tasks. To\novercome these limitations, we introduce Strassen attention and prove that,\nequipped with this mechanism, a one-layer transformer can in principle solve\nall these tasks. Importantly, we show that it enjoys sub-cubic running-time\ncomplexity, making it more scalable than similar previously proposed\nmechanisms, such as higher-order attention (Sanford et al., 2023). To\ncomplement our theoretical findings, we experimentally studied Strassen\nattention and compared it against standard (Vaswani et al, 2017), higher-order\nattention (Sanford et al., 2023), and triangular attention (Bergen et al.\n2021). Our results help to disentangle all these attention mechanisms,\nhighlighting their strengths and limitations. In particular, Strassen attention\noutperforms standard attention significantly on all the tasks. Altogether,\nunderstanding the theoretical limitations can guide research towards scalable\nattention mechanisms that improve the reasoning abilities of Transformers.\n", "link": "http://arxiv.org/abs/2501.19215v3", "date": "2025-09-25", "relevancy": 2.1527, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strassen%20Attention%2C%20Split%20VC%20Dimension%20and%20Compositionality%20in%0A%20%20Transformers&body=Title%3A%20Strassen%20Attention%2C%20Split%20VC%20Dimension%20and%20Compositionality%20in%0A%20%20Transformers%0AAuthor%3A%20Alexander%20Kozachinskiy%20and%20Felipe%20Urrutia%20and%20Hector%20Jimenez%20and%20Tomasz%20Steifer%20and%20Germ%C3%A1n%20Pizarro%20and%20Mat%C3%ADas%20Fuentes%20and%20Francisco%20Meza%20and%20Cristian%20B.%20Calderon%20and%20Crist%C3%B3bal%20Rojas%0AAbstract%3A%20%20%20We%20propose%20the%20first%20method%20to%20show%20theoretical%20limitations%20for%20one-layer%0Asoftmax%20transformers%20with%20arbitrarily%20many%20precision%20bits%20%28even%20infinite%29.%20We%0Aestablish%20those%20limitations%20for%20three%20tasks%20that%20require%20advanced%20reasoning.%0AThe%20first%20task%2C%20Match%203%20%28Sanford%20et%20al.%2C%202023%29%2C%20requires%20looking%20at%20all%0Apossible%20token%20triplets%20in%20an%20input%20sequence.%20The%20second%20and%20third%20tasks%0Aaddress%20compositionality-based%20reasoning%3A%20function%20composition%20%28Peng%20et%20al.%2C%0A2024%29%20and%20binary%20relations%20composition%2C%20respectively.%20We%20formally%20prove%20the%0Ainability%20of%20one-layer%20softmax%20Transformers%20to%20solve%20any%20of%20these%20tasks.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20Strassen%20attention%20and%20prove%20that%2C%0Aequipped%20with%20this%20mechanism%2C%20a%20one-layer%20transformer%20can%20in%20principle%20solve%0Aall%20these%20tasks.%20Importantly%2C%20we%20show%20that%20it%20enjoys%20sub-cubic%20running-time%0Acomplexity%2C%20making%20it%20more%20scalable%20than%20similar%20previously%20proposed%0Amechanisms%2C%20such%20as%20higher-order%20attention%20%28Sanford%20et%20al.%2C%202023%29.%20To%0Acomplement%20our%20theoretical%20findings%2C%20we%20experimentally%20studied%20Strassen%0Aattention%20and%20compared%20it%20against%20standard%20%28Vaswani%20et%20al%2C%202017%29%2C%20higher-order%0Aattention%20%28Sanford%20et%20al.%2C%202023%29%2C%20and%20triangular%20attention%20%28Bergen%20et%20al.%0A2021%29.%20Our%20results%20help%20to%20disentangle%20all%20these%20attention%20mechanisms%2C%0Ahighlighting%20their%20strengths%20and%20limitations.%20In%20particular%2C%20Strassen%20attention%0Aoutperforms%20standard%20attention%20significantly%20on%20all%20the%20tasks.%20Altogether%2C%0Aunderstanding%20the%20theoretical%20limitations%20can%20guide%20research%20towards%20scalable%0Aattention%20mechanisms%20that%20improve%20the%20reasoning%20abilities%20of%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19215v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrassen%2520Attention%252C%2520Split%2520VC%2520Dimension%2520and%2520Compositionality%2520in%250A%2520%2520Transformers%26entry.906535625%3DAlexander%2520Kozachinskiy%2520and%2520Felipe%2520Urrutia%2520and%2520Hector%2520Jimenez%2520and%2520Tomasz%2520Steifer%2520and%2520Germ%25C3%25A1n%2520Pizarro%2520and%2520Mat%25C3%25ADas%2520Fuentes%2520and%2520Francisco%2520Meza%2520and%2520Cristian%2520B.%2520Calderon%2520and%2520Crist%25C3%25B3bal%2520Rojas%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520first%2520method%2520to%2520show%2520theoretical%2520limitations%2520for%2520one-layer%250Asoftmax%2520transformers%2520with%2520arbitrarily%2520many%2520precision%2520bits%2520%2528even%2520infinite%2529.%2520We%250Aestablish%2520those%2520limitations%2520for%2520three%2520tasks%2520that%2520require%2520advanced%2520reasoning.%250AThe%2520first%2520task%252C%2520Match%25203%2520%2528Sanford%2520et%2520al.%252C%25202023%2529%252C%2520requires%2520looking%2520at%2520all%250Apossible%2520token%2520triplets%2520in%2520an%2520input%2520sequence.%2520The%2520second%2520and%2520third%2520tasks%250Aaddress%2520compositionality-based%2520reasoning%253A%2520function%2520composition%2520%2528Peng%2520et%2520al.%252C%250A2024%2529%2520and%2520binary%2520relations%2520composition%252C%2520respectively.%2520We%2520formally%2520prove%2520the%250Ainability%2520of%2520one-layer%2520softmax%2520Transformers%2520to%2520solve%2520any%2520of%2520these%2520tasks.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520introduce%2520Strassen%2520attention%2520and%2520prove%2520that%252C%250Aequipped%2520with%2520this%2520mechanism%252C%2520a%2520one-layer%2520transformer%2520can%2520in%2520principle%2520solve%250Aall%2520these%2520tasks.%2520Importantly%252C%2520we%2520show%2520that%2520it%2520enjoys%2520sub-cubic%2520running-time%250Acomplexity%252C%2520making%2520it%2520more%2520scalable%2520than%2520similar%2520previously%2520proposed%250Amechanisms%252C%2520such%2520as%2520higher-order%2520attention%2520%2528Sanford%2520et%2520al.%252C%25202023%2529.%2520To%250Acomplement%2520our%2520theoretical%2520findings%252C%2520we%2520experimentally%2520studied%2520Strassen%250Aattention%2520and%2520compared%2520it%2520against%2520standard%2520%2528Vaswani%2520et%2520al%252C%25202017%2529%252C%2520higher-order%250Aattention%2520%2528Sanford%2520et%2520al.%252C%25202023%2529%252C%2520and%2520triangular%2520attention%2520%2528Bergen%2520et%2520al.%250A2021%2529.%2520Our%2520results%2520help%2520to%2520disentangle%2520all%2520these%2520attention%2520mechanisms%252C%250Ahighlighting%2520their%2520strengths%2520and%2520limitations.%2520In%2520particular%252C%2520Strassen%2520attention%250Aoutperforms%2520standard%2520attention%2520significantly%2520on%2520all%2520the%2520tasks.%2520Altogether%252C%250Aunderstanding%2520the%2520theoretical%2520limitations%2520can%2520guide%2520research%2520towards%2520scalable%250Aattention%2520mechanisms%2520that%2520improve%2520the%2520reasoning%2520abilities%2520of%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19215v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strassen%20Attention%2C%20Split%20VC%20Dimension%20and%20Compositionality%20in%0A%20%20Transformers&entry.906535625=Alexander%20Kozachinskiy%20and%20Felipe%20Urrutia%20and%20Hector%20Jimenez%20and%20Tomasz%20Steifer%20and%20Germ%C3%A1n%20Pizarro%20and%20Mat%C3%ADas%20Fuentes%20and%20Francisco%20Meza%20and%20Cristian%20B.%20Calderon%20and%20Crist%C3%B3bal%20Rojas&entry.1292438233=%20%20We%20propose%20the%20first%20method%20to%20show%20theoretical%20limitations%20for%20one-layer%0Asoftmax%20transformers%20with%20arbitrarily%20many%20precision%20bits%20%28even%20infinite%29.%20We%0Aestablish%20those%20limitations%20for%20three%20tasks%20that%20require%20advanced%20reasoning.%0AThe%20first%20task%2C%20Match%203%20%28Sanford%20et%20al.%2C%202023%29%2C%20requires%20looking%20at%20all%0Apossible%20token%20triplets%20in%20an%20input%20sequence.%20The%20second%20and%20third%20tasks%0Aaddress%20compositionality-based%20reasoning%3A%20function%20composition%20%28Peng%20et%20al.%2C%0A2024%29%20and%20binary%20relations%20composition%2C%20respectively.%20We%20formally%20prove%20the%0Ainability%20of%20one-layer%20softmax%20Transformers%20to%20solve%20any%20of%20these%20tasks.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20Strassen%20attention%20and%20prove%20that%2C%0Aequipped%20with%20this%20mechanism%2C%20a%20one-layer%20transformer%20can%20in%20principle%20solve%0Aall%20these%20tasks.%20Importantly%2C%20we%20show%20that%20it%20enjoys%20sub-cubic%20running-time%0Acomplexity%2C%20making%20it%20more%20scalable%20than%20similar%20previously%20proposed%0Amechanisms%2C%20such%20as%20higher-order%20attention%20%28Sanford%20et%20al.%2C%202023%29.%20To%0Acomplement%20our%20theoretical%20findings%2C%20we%20experimentally%20studied%20Strassen%0Aattention%20and%20compared%20it%20against%20standard%20%28Vaswani%20et%20al%2C%202017%29%2C%20higher-order%0Aattention%20%28Sanford%20et%20al.%2C%202023%29%2C%20and%20triangular%20attention%20%28Bergen%20et%20al.%0A2021%29.%20Our%20results%20help%20to%20disentangle%20all%20these%20attention%20mechanisms%2C%0Ahighlighting%20their%20strengths%20and%20limitations.%20In%20particular%2C%20Strassen%20attention%0Aoutperforms%20standard%20attention%20significantly%20on%20all%20the%20tasks.%20Altogether%2C%0Aunderstanding%20the%20theoretical%20limitations%20can%20guide%20research%20towards%20scalable%0Aattention%20mechanisms%20that%20improve%20the%20reasoning%20abilities%20of%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19215v3&entry.124074799=Read"},
{"title": "ProstaTD: Bridging Surgical Triplet from Classification to Fully\n  Supervised Detection", "author": "Yiliang Chen and Zhixi Li and Cheng Xu and Alex Qinyang Liu and Ruize Cui and Xuemiao Xu and Jeremy Yuen-Chun Teoh and Shengfeng He and Jing Qin", "abstract": "  Surgical triplet detection is a critical task in surgical video analysis.\nHowever, existing datasets like CholecT50 lack precise spatial bounding box\nannotations, rendering triplet classification at the image level insufficient\nfor practical applications. The inclusion of bounding box annotations is\nessential to make this task meaningful, as they provide the spatial context\nnecessary for accurate analysis and improved model generalizability. To address\nthese shortcomings, we introduce ProstaTD, a large-scale, multi-institutional\ndataset for surgical triplet detection, developed from the technically\ndemanding domain of robot-assisted prostatectomy. ProstaTD offers clinically\ndefined temporal boundaries and high-precision bounding box annotations for\neach structured triplet activity. The dataset comprises 71,775 video frames and\n196,490 annotated triplet instances, collected from 21 surgeries performed\nacross multiple institutions, reflecting a broad range of surgical practices\nand intraoperative conditions. The annotation process was conducted under\nrigorous medical supervision and involved more than 60 contributors, including\npracticing surgeons and medically trained annotators, through multiple\niterative phases of labeling and verification. To further facilitate future\ngeneral-purpose surgical annotation, we developed two tailored labeling tools\nto improve efficiency and scalability in our annotation workflows. In addition,\nwe created a surgical triplet detection evaluation toolkit that enables\nstandardized and reproducible performance assessment across studies. ProstaTD\nis the largest and most diverse surgical triplet dataset to date, moving the\nfield from simple classification to full detection with precise spatial and\ntemporal boundaries and thereby providing a robust foundation for fair\nbenchmarking.\n", "link": "http://arxiv.org/abs/2506.01130v2", "date": "2025-09-25", "relevancy": 2.1446, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5391}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProstaTD%3A%20Bridging%20Surgical%20Triplet%20from%20Classification%20to%20Fully%0A%20%20Supervised%20Detection&body=Title%3A%20ProstaTD%3A%20Bridging%20Surgical%20Triplet%20from%20Classification%20to%20Fully%0A%20%20Supervised%20Detection%0AAuthor%3A%20Yiliang%20Chen%20and%20Zhixi%20Li%20and%20Cheng%20Xu%20and%20Alex%20Qinyang%20Liu%20and%20Ruize%20Cui%20and%20Xuemiao%20Xu%20and%20Jeremy%20Yuen-Chun%20Teoh%20and%20Shengfeng%20He%20and%20Jing%20Qin%0AAbstract%3A%20%20%20Surgical%20triplet%20detection%20is%20a%20critical%20task%20in%20surgical%20video%20analysis.%0AHowever%2C%20existing%20datasets%20like%20CholecT50%20lack%20precise%20spatial%20bounding%20box%0Aannotations%2C%20rendering%20triplet%20classification%20at%20the%20image%20level%20insufficient%0Afor%20practical%20applications.%20The%20inclusion%20of%20bounding%20box%20annotations%20is%0Aessential%20to%20make%20this%20task%20meaningful%2C%20as%20they%20provide%20the%20spatial%20context%0Anecessary%20for%20accurate%20analysis%20and%20improved%20model%20generalizability.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20ProstaTD%2C%20a%20large-scale%2C%20multi-institutional%0Adataset%20for%20surgical%20triplet%20detection%2C%20developed%20from%20the%20technically%0Ademanding%20domain%20of%20robot-assisted%20prostatectomy.%20ProstaTD%20offers%20clinically%0Adefined%20temporal%20boundaries%20and%20high-precision%20bounding%20box%20annotations%20for%0Aeach%20structured%20triplet%20activity.%20The%20dataset%20comprises%2071%2C775%20video%20frames%20and%0A196%2C490%20annotated%20triplet%20instances%2C%20collected%20from%2021%20surgeries%20performed%0Aacross%20multiple%20institutions%2C%20reflecting%20a%20broad%20range%20of%20surgical%20practices%0Aand%20intraoperative%20conditions.%20The%20annotation%20process%20was%20conducted%20under%0Arigorous%20medical%20supervision%20and%20involved%20more%20than%2060%20contributors%2C%20including%0Apracticing%20surgeons%20and%20medically%20trained%20annotators%2C%20through%20multiple%0Aiterative%20phases%20of%20labeling%20and%20verification.%20To%20further%20facilitate%20future%0Ageneral-purpose%20surgical%20annotation%2C%20we%20developed%20two%20tailored%20labeling%20tools%0Ato%20improve%20efficiency%20and%20scalability%20in%20our%20annotation%20workflows.%20In%20addition%2C%0Awe%20created%20a%20surgical%20triplet%20detection%20evaluation%20toolkit%20that%20enables%0Astandardized%20and%20reproducible%20performance%20assessment%20across%20studies.%20ProstaTD%0Ais%20the%20largest%20and%20most%20diverse%20surgical%20triplet%20dataset%20to%20date%2C%20moving%20the%0Afield%20from%20simple%20classification%20to%20full%20detection%20with%20precise%20spatial%20and%0Atemporal%20boundaries%20and%20thereby%20providing%20a%20robust%20foundation%20for%20fair%0Abenchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProstaTD%253A%2520Bridging%2520Surgical%2520Triplet%2520from%2520Classification%2520to%2520Fully%250A%2520%2520Supervised%2520Detection%26entry.906535625%3DYiliang%2520Chen%2520and%2520Zhixi%2520Li%2520and%2520Cheng%2520Xu%2520and%2520Alex%2520Qinyang%2520Liu%2520and%2520Ruize%2520Cui%2520and%2520Xuemiao%2520Xu%2520and%2520Jeremy%2520Yuen-Chun%2520Teoh%2520and%2520Shengfeng%2520He%2520and%2520Jing%2520Qin%26entry.1292438233%3D%2520%2520Surgical%2520triplet%2520detection%2520is%2520a%2520critical%2520task%2520in%2520surgical%2520video%2520analysis.%250AHowever%252C%2520existing%2520datasets%2520like%2520CholecT50%2520lack%2520precise%2520spatial%2520bounding%2520box%250Aannotations%252C%2520rendering%2520triplet%2520classification%2520at%2520the%2520image%2520level%2520insufficient%250Afor%2520practical%2520applications.%2520The%2520inclusion%2520of%2520bounding%2520box%2520annotations%2520is%250Aessential%2520to%2520make%2520this%2520task%2520meaningful%252C%2520as%2520they%2520provide%2520the%2520spatial%2520context%250Anecessary%2520for%2520accurate%2520analysis%2520and%2520improved%2520model%2520generalizability.%2520To%2520address%250Athese%2520shortcomings%252C%2520we%2520introduce%2520ProstaTD%252C%2520a%2520large-scale%252C%2520multi-institutional%250Adataset%2520for%2520surgical%2520triplet%2520detection%252C%2520developed%2520from%2520the%2520technically%250Ademanding%2520domain%2520of%2520robot-assisted%2520prostatectomy.%2520ProstaTD%2520offers%2520clinically%250Adefined%2520temporal%2520boundaries%2520and%2520high-precision%2520bounding%2520box%2520annotations%2520for%250Aeach%2520structured%2520triplet%2520activity.%2520The%2520dataset%2520comprises%252071%252C775%2520video%2520frames%2520and%250A196%252C490%2520annotated%2520triplet%2520instances%252C%2520collected%2520from%252021%2520surgeries%2520performed%250Aacross%2520multiple%2520institutions%252C%2520reflecting%2520a%2520broad%2520range%2520of%2520surgical%2520practices%250Aand%2520intraoperative%2520conditions.%2520The%2520annotation%2520process%2520was%2520conducted%2520under%250Arigorous%2520medical%2520supervision%2520and%2520involved%2520more%2520than%252060%2520contributors%252C%2520including%250Apracticing%2520surgeons%2520and%2520medically%2520trained%2520annotators%252C%2520through%2520multiple%250Aiterative%2520phases%2520of%2520labeling%2520and%2520verification.%2520To%2520further%2520facilitate%2520future%250Ageneral-purpose%2520surgical%2520annotation%252C%2520we%2520developed%2520two%2520tailored%2520labeling%2520tools%250Ato%2520improve%2520efficiency%2520and%2520scalability%2520in%2520our%2520annotation%2520workflows.%2520In%2520addition%252C%250Awe%2520created%2520a%2520surgical%2520triplet%2520detection%2520evaluation%2520toolkit%2520that%2520enables%250Astandardized%2520and%2520reproducible%2520performance%2520assessment%2520across%2520studies.%2520ProstaTD%250Ais%2520the%2520largest%2520and%2520most%2520diverse%2520surgical%2520triplet%2520dataset%2520to%2520date%252C%2520moving%2520the%250Afield%2520from%2520simple%2520classification%2520to%2520full%2520detection%2520with%2520precise%2520spatial%2520and%250Atemporal%2520boundaries%2520and%2520thereby%2520providing%2520a%2520robust%2520foundation%2520for%2520fair%250Abenchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProstaTD%3A%20Bridging%20Surgical%20Triplet%20from%20Classification%20to%20Fully%0A%20%20Supervised%20Detection&entry.906535625=Yiliang%20Chen%20and%20Zhixi%20Li%20and%20Cheng%20Xu%20and%20Alex%20Qinyang%20Liu%20and%20Ruize%20Cui%20and%20Xuemiao%20Xu%20and%20Jeremy%20Yuen-Chun%20Teoh%20and%20Shengfeng%20He%20and%20Jing%20Qin&entry.1292438233=%20%20Surgical%20triplet%20detection%20is%20a%20critical%20task%20in%20surgical%20video%20analysis.%0AHowever%2C%20existing%20datasets%20like%20CholecT50%20lack%20precise%20spatial%20bounding%20box%0Aannotations%2C%20rendering%20triplet%20classification%20at%20the%20image%20level%20insufficient%0Afor%20practical%20applications.%20The%20inclusion%20of%20bounding%20box%20annotations%20is%0Aessential%20to%20make%20this%20task%20meaningful%2C%20as%20they%20provide%20the%20spatial%20context%0Anecessary%20for%20accurate%20analysis%20and%20improved%20model%20generalizability.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20ProstaTD%2C%20a%20large-scale%2C%20multi-institutional%0Adataset%20for%20surgical%20triplet%20detection%2C%20developed%20from%20the%20technically%0Ademanding%20domain%20of%20robot-assisted%20prostatectomy.%20ProstaTD%20offers%20clinically%0Adefined%20temporal%20boundaries%20and%20high-precision%20bounding%20box%20annotations%20for%0Aeach%20structured%20triplet%20activity.%20The%20dataset%20comprises%2071%2C775%20video%20frames%20and%0A196%2C490%20annotated%20triplet%20instances%2C%20collected%20from%2021%20surgeries%20performed%0Aacross%20multiple%20institutions%2C%20reflecting%20a%20broad%20range%20of%20surgical%20practices%0Aand%20intraoperative%20conditions.%20The%20annotation%20process%20was%20conducted%20under%0Arigorous%20medical%20supervision%20and%20involved%20more%20than%2060%20contributors%2C%20including%0Apracticing%20surgeons%20and%20medically%20trained%20annotators%2C%20through%20multiple%0Aiterative%20phases%20of%20labeling%20and%20verification.%20To%20further%20facilitate%20future%0Ageneral-purpose%20surgical%20annotation%2C%20we%20developed%20two%20tailored%20labeling%20tools%0Ato%20improve%20efficiency%20and%20scalability%20in%20our%20annotation%20workflows.%20In%20addition%2C%0Awe%20created%20a%20surgical%20triplet%20detection%20evaluation%20toolkit%20that%20enables%0Astandardized%20and%20reproducible%20performance%20assessment%20across%20studies.%20ProstaTD%0Ais%20the%20largest%20and%20most%20diverse%20surgical%20triplet%20dataset%20to%20date%2C%20moving%20the%0Afield%20from%20simple%20classification%20to%20full%20detection%20with%20precise%20spatial%20and%0Atemporal%20boundaries%20and%20thereby%20providing%20a%20robust%20foundation%20for%20fair%0Abenchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01130v2&entry.124074799=Read"},
{"title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP", "author": "Moshe Kimhi and Erez Koifman and Ehud Rivlin and Eli Schwartz and Chaim Baskin", "abstract": "  We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.\n", "link": "http://arxiv.org/abs/2509.21153v1", "date": "2025-09-25", "relevancy": 2.1418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5211}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WAVECLIP%3A%20Wavelet%20Tokenization%20for%20Adaptive-Resolution%20CLIP&body=Title%3A%20WAVECLIP%3A%20Wavelet%20Tokenization%20for%20Adaptive-Resolution%20CLIP%0AAuthor%3A%20Moshe%20Kimhi%20and%20Erez%20Koifman%20and%20Ehud%20Rivlin%20and%20Eli%20Schwartz%20and%20Chaim%20Baskin%0AAbstract%3A%20%20%20We%20introduce%20WAVECLIP%2C%20a%20single%20unified%20model%20for%20adaptive%20resolution%0Ainference%20in%20CLIP%2C%20enabled%20by%20wavelet-based%20tokenization.%20WAVECLIP%20replaces%0Astandard%20patch%20embeddings%20with%20a%20multi-level%20wavelet%20decomposition%2C%20enabling%0Athe%20model%20to%20process%20images%20coarse%20to%20fine%20while%20naturally%20supporting%20multiple%0Aresolutions%20within%20the%20same%20model.%20At%20inference%20time%2C%20the%20model%20begins%20with%20low%0Aresolution%20tokens%20and%20refines%20only%20when%20needed%2C%20using%20key-value%20caching%20and%0Acausal%20cross-level%20attention%20to%20reuse%20computation%2C%20effectively%20introducing%20to%0Athe%20model%20only%20new%20information%20when%20needed.%20We%20evaluate%20WAVECLIP%20in%20zero-shot%0Aclassification%2C%20demonstrating%20that%20a%20simple%20confidence-based%20gating%20mechanism%0Aenables%20adaptive%20early%20exits.%20This%20allows%20users%20to%20dynamically%20choose%20a%0Acompute-accuracy%20trade-off%20using%20a%20single%20deployed%20model.%20Our%20approach%20requires%0Aonly%20lightweight%20distillation%20from%20a%20frozen%20CLIP%20teacher%20and%20achieves%0Acompetitive%20accuracy%20with%20significant%20computational%20savings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWAVECLIP%253A%2520Wavelet%2520Tokenization%2520for%2520Adaptive-Resolution%2520CLIP%26entry.906535625%3DMoshe%2520Kimhi%2520and%2520Erez%2520Koifman%2520and%2520Ehud%2520Rivlin%2520and%2520Eli%2520Schwartz%2520and%2520Chaim%2520Baskin%26entry.1292438233%3D%2520%2520We%2520introduce%2520WAVECLIP%252C%2520a%2520single%2520unified%2520model%2520for%2520adaptive%2520resolution%250Ainference%2520in%2520CLIP%252C%2520enabled%2520by%2520wavelet-based%2520tokenization.%2520WAVECLIP%2520replaces%250Astandard%2520patch%2520embeddings%2520with%2520a%2520multi-level%2520wavelet%2520decomposition%252C%2520enabling%250Athe%2520model%2520to%2520process%2520images%2520coarse%2520to%2520fine%2520while%2520naturally%2520supporting%2520multiple%250Aresolutions%2520within%2520the%2520same%2520model.%2520At%2520inference%2520time%252C%2520the%2520model%2520begins%2520with%2520low%250Aresolution%2520tokens%2520and%2520refines%2520only%2520when%2520needed%252C%2520using%2520key-value%2520caching%2520and%250Acausal%2520cross-level%2520attention%2520to%2520reuse%2520computation%252C%2520effectively%2520introducing%2520to%250Athe%2520model%2520only%2520new%2520information%2520when%2520needed.%2520We%2520evaluate%2520WAVECLIP%2520in%2520zero-shot%250Aclassification%252C%2520demonstrating%2520that%2520a%2520simple%2520confidence-based%2520gating%2520mechanism%250Aenables%2520adaptive%2520early%2520exits.%2520This%2520allows%2520users%2520to%2520dynamically%2520choose%2520a%250Acompute-accuracy%2520trade-off%2520using%2520a%2520single%2520deployed%2520model.%2520Our%2520approach%2520requires%250Aonly%2520lightweight%2520distillation%2520from%2520a%2520frozen%2520CLIP%2520teacher%2520and%2520achieves%250Acompetitive%2520accuracy%2520with%2520significant%2520computational%2520savings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WAVECLIP%3A%20Wavelet%20Tokenization%20for%20Adaptive-Resolution%20CLIP&entry.906535625=Moshe%20Kimhi%20and%20Erez%20Koifman%20and%20Ehud%20Rivlin%20and%20Eli%20Schwartz%20and%20Chaim%20Baskin&entry.1292438233=%20%20We%20introduce%20WAVECLIP%2C%20a%20single%20unified%20model%20for%20adaptive%20resolution%0Ainference%20in%20CLIP%2C%20enabled%20by%20wavelet-based%20tokenization.%20WAVECLIP%20replaces%0Astandard%20patch%20embeddings%20with%20a%20multi-level%20wavelet%20decomposition%2C%20enabling%0Athe%20model%20to%20process%20images%20coarse%20to%20fine%20while%20naturally%20supporting%20multiple%0Aresolutions%20within%20the%20same%20model.%20At%20inference%20time%2C%20the%20model%20begins%20with%20low%0Aresolution%20tokens%20and%20refines%20only%20when%20needed%2C%20using%20key-value%20caching%20and%0Acausal%20cross-level%20attention%20to%20reuse%20computation%2C%20effectively%20introducing%20to%0Athe%20model%20only%20new%20information%20when%20needed.%20We%20evaluate%20WAVECLIP%20in%20zero-shot%0Aclassification%2C%20demonstrating%20that%20a%20simple%20confidence-based%20gating%20mechanism%0Aenables%20adaptive%20early%20exits.%20This%20allows%20users%20to%20dynamically%20choose%20a%0Acompute-accuracy%20trade-off%20using%20a%20single%20deployed%20model.%20Our%20approach%20requires%0Aonly%20lightweight%20distillation%20from%20a%20frozen%20CLIP%20teacher%20and%20achieves%0Acompetitive%20accuracy%20with%20significant%20computational%20savings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21153v1&entry.124074799=Read"},
{"title": "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources", "author": "Sicong Leng and Jing Wang and Jiaxi Li and Hao Zhang and Zhiqiang Hu and Boqiang Zhang and Yuming Jiang and Hang Zhang and Xin Li and Lidong Bing and Deli Zhao and Wei Lu and Yu Rong and Aixin Sun and Shijian Lu", "abstract": "  Large multimodal reasoning models have achieved rapid progress, but their\nadvancement is constrained by two major limitations: the absence of open,\nlarge-scale, high-quality long chain-of-thought (CoT) data, and the instability\nof reinforcement learning (RL) algorithms in post-training. Group Relative\nPolicy Optimization (GRPO), the standard framework for RL fine-tuning, is prone\nto gradient vanishing when reward variance is low, which weakens optimization\nsignals and impairs convergence. This work makes three contributions: (1) We\npropose Variance-Aware Sampling (VAS), a data selection strategy guided by\nVariance Promotion Score (VPS) that combines outcome variance and trajectory\ndiversity to promote reward variance and stabilize policy optimization. (2) We\nrelease large-scale, carefully curated resources containing ~1.6M long CoT\ncold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,\nand diversity, along with a fully reproducible end-to-end training codebase.\n(3) We open-source a family of multimodal reasoning models in multiple scales,\nestablishing standardized baselines for the community. Experiments across\nmathematical reasoning benchmarks demonstrate the effectiveness of both the\ncurated data and the proposed VAS. Comprehensive ablation studies and analyses\nprovide further insight into the contributions of each component. In addition,\nwe theoretically establish that reward variance lower-bounds the expected\npolicy gradient magnitude, with VAS serving as a practical mechanism to realize\nthis guarantee. Our code, data, and checkpoints are available at\nhttps://github.com/LengSicong/MMR1.\n", "link": "http://arxiv.org/abs/2509.21268v1", "date": "2025-09-25", "relevancy": 2.1406, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMR1%3A%20Enhancing%20Multimodal%20Reasoning%20with%20Variance-Aware%20Sampling%20and%0A%20%20Open%20Resources&body=Title%3A%20MMR1%3A%20Enhancing%20Multimodal%20Reasoning%20with%20Variance-Aware%20Sampling%20and%0A%20%20Open%20Resources%0AAuthor%3A%20Sicong%20Leng%20and%20Jing%20Wang%20and%20Jiaxi%20Li%20and%20Hao%20Zhang%20and%20Zhiqiang%20Hu%20and%20Boqiang%20Zhang%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Lidong%20Bing%20and%20Deli%20Zhao%20and%20Wei%20Lu%20and%20Yu%20Rong%20and%20Aixin%20Sun%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Large%20multimodal%20reasoning%20models%20have%20achieved%20rapid%20progress%2C%20but%20their%0Aadvancement%20is%20constrained%20by%20two%20major%20limitations%3A%20the%20absence%20of%20open%2C%0Alarge-scale%2C%20high-quality%20long%20chain-of-thought%20%28CoT%29%20data%2C%20and%20the%20instability%0Aof%20reinforcement%20learning%20%28RL%29%20algorithms%20in%20post-training.%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20the%20standard%20framework%20for%20RL%20fine-tuning%2C%20is%20prone%0Ato%20gradient%20vanishing%20when%20reward%20variance%20is%20low%2C%20which%20weakens%20optimization%0Asignals%20and%20impairs%20convergence.%20This%20work%20makes%20three%20contributions%3A%20%281%29%20We%0Apropose%20Variance-Aware%20Sampling%20%28VAS%29%2C%20a%20data%20selection%20strategy%20guided%20by%0AVariance%20Promotion%20Score%20%28VPS%29%20that%20combines%20outcome%20variance%20and%20trajectory%0Adiversity%20to%20promote%20reward%20variance%20and%20stabilize%20policy%20optimization.%20%282%29%20We%0Arelease%20large-scale%2C%20carefully%20curated%20resources%20containing%20~1.6M%20long%20CoT%0Acold-start%20data%20and%20~15k%20RL%20QA%20pairs%2C%20designed%20to%20ensure%20quality%2C%20difficulty%2C%0Aand%20diversity%2C%20along%20with%20a%20fully%20reproducible%20end-to-end%20training%20codebase.%0A%283%29%20We%20open-source%20a%20family%20of%20multimodal%20reasoning%20models%20in%20multiple%20scales%2C%0Aestablishing%20standardized%20baselines%20for%20the%20community.%20Experiments%20across%0Amathematical%20reasoning%20benchmarks%20demonstrate%20the%20effectiveness%20of%20both%20the%0Acurated%20data%20and%20the%20proposed%20VAS.%20Comprehensive%20ablation%20studies%20and%20analyses%0Aprovide%20further%20insight%20into%20the%20contributions%20of%20each%20component.%20In%20addition%2C%0Awe%20theoretically%20establish%20that%20reward%20variance%20lower-bounds%20the%20expected%0Apolicy%20gradient%20magnitude%2C%20with%20VAS%20serving%20as%20a%20practical%20mechanism%20to%20realize%0Athis%20guarantee.%20Our%20code%2C%20data%2C%20and%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/LengSicong/MMR1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMR1%253A%2520Enhancing%2520Multimodal%2520Reasoning%2520with%2520Variance-Aware%2520Sampling%2520and%250A%2520%2520Open%2520Resources%26entry.906535625%3DSicong%2520Leng%2520and%2520Jing%2520Wang%2520and%2520Jiaxi%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Zhiqiang%2520Hu%2520and%2520Boqiang%2520Zhang%2520and%2520Yuming%2520Jiang%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Lidong%2520Bing%2520and%2520Deli%2520Zhao%2520and%2520Wei%2520Lu%2520and%2520Yu%2520Rong%2520and%2520Aixin%2520Sun%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520reasoning%2520models%2520have%2520achieved%2520rapid%2520progress%252C%2520but%2520their%250Aadvancement%2520is%2520constrained%2520by%2520two%2520major%2520limitations%253A%2520the%2520absence%2520of%2520open%252C%250Alarge-scale%252C%2520high-quality%2520long%2520chain-of-thought%2520%2528CoT%2529%2520data%252C%2520and%2520the%2520instability%250Aof%2520reinforcement%2520learning%2520%2528RL%2529%2520algorithms%2520in%2520post-training.%2520Group%2520Relative%250APolicy%2520Optimization%2520%2528GRPO%2529%252C%2520the%2520standard%2520framework%2520for%2520RL%2520fine-tuning%252C%2520is%2520prone%250Ato%2520gradient%2520vanishing%2520when%2520reward%2520variance%2520is%2520low%252C%2520which%2520weakens%2520optimization%250Asignals%2520and%2520impairs%2520convergence.%2520This%2520work%2520makes%2520three%2520contributions%253A%2520%25281%2529%2520We%250Apropose%2520Variance-Aware%2520Sampling%2520%2528VAS%2529%252C%2520a%2520data%2520selection%2520strategy%2520guided%2520by%250AVariance%2520Promotion%2520Score%2520%2528VPS%2529%2520that%2520combines%2520outcome%2520variance%2520and%2520trajectory%250Adiversity%2520to%2520promote%2520reward%2520variance%2520and%2520stabilize%2520policy%2520optimization.%2520%25282%2529%2520We%250Arelease%2520large-scale%252C%2520carefully%2520curated%2520resources%2520containing%2520~1.6M%2520long%2520CoT%250Acold-start%2520data%2520and%2520~15k%2520RL%2520QA%2520pairs%252C%2520designed%2520to%2520ensure%2520quality%252C%2520difficulty%252C%250Aand%2520diversity%252C%2520along%2520with%2520a%2520fully%2520reproducible%2520end-to-end%2520training%2520codebase.%250A%25283%2529%2520We%2520open-source%2520a%2520family%2520of%2520multimodal%2520reasoning%2520models%2520in%2520multiple%2520scales%252C%250Aestablishing%2520standardized%2520baselines%2520for%2520the%2520community.%2520Experiments%2520across%250Amathematical%2520reasoning%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520both%2520the%250Acurated%2520data%2520and%2520the%2520proposed%2520VAS.%2520Comprehensive%2520ablation%2520studies%2520and%2520analyses%250Aprovide%2520further%2520insight%2520into%2520the%2520contributions%2520of%2520each%2520component.%2520In%2520addition%252C%250Awe%2520theoretically%2520establish%2520that%2520reward%2520variance%2520lower-bounds%2520the%2520expected%250Apolicy%2520gradient%2520magnitude%252C%2520with%2520VAS%2520serving%2520as%2520a%2520practical%2520mechanism%2520to%2520realize%250Athis%2520guarantee.%2520Our%2520code%252C%2520data%252C%2520and%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/LengSicong/MMR1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMR1%3A%20Enhancing%20Multimodal%20Reasoning%20with%20Variance-Aware%20Sampling%20and%0A%20%20Open%20Resources&entry.906535625=Sicong%20Leng%20and%20Jing%20Wang%20and%20Jiaxi%20Li%20and%20Hao%20Zhang%20and%20Zhiqiang%20Hu%20and%20Boqiang%20Zhang%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Lidong%20Bing%20and%20Deli%20Zhao%20and%20Wei%20Lu%20and%20Yu%20Rong%20and%20Aixin%20Sun%20and%20Shijian%20Lu&entry.1292438233=%20%20Large%20multimodal%20reasoning%20models%20have%20achieved%20rapid%20progress%2C%20but%20their%0Aadvancement%20is%20constrained%20by%20two%20major%20limitations%3A%20the%20absence%20of%20open%2C%0Alarge-scale%2C%20high-quality%20long%20chain-of-thought%20%28CoT%29%20data%2C%20and%20the%20instability%0Aof%20reinforcement%20learning%20%28RL%29%20algorithms%20in%20post-training.%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20the%20standard%20framework%20for%20RL%20fine-tuning%2C%20is%20prone%0Ato%20gradient%20vanishing%20when%20reward%20variance%20is%20low%2C%20which%20weakens%20optimization%0Asignals%20and%20impairs%20convergence.%20This%20work%20makes%20three%20contributions%3A%20%281%29%20We%0Apropose%20Variance-Aware%20Sampling%20%28VAS%29%2C%20a%20data%20selection%20strategy%20guided%20by%0AVariance%20Promotion%20Score%20%28VPS%29%20that%20combines%20outcome%20variance%20and%20trajectory%0Adiversity%20to%20promote%20reward%20variance%20and%20stabilize%20policy%20optimization.%20%282%29%20We%0Arelease%20large-scale%2C%20carefully%20curated%20resources%20containing%20~1.6M%20long%20CoT%0Acold-start%20data%20and%20~15k%20RL%20QA%20pairs%2C%20designed%20to%20ensure%20quality%2C%20difficulty%2C%0Aand%20diversity%2C%20along%20with%20a%20fully%20reproducible%20end-to-end%20training%20codebase.%0A%283%29%20We%20open-source%20a%20family%20of%20multimodal%20reasoning%20models%20in%20multiple%20scales%2C%0Aestablishing%20standardized%20baselines%20for%20the%20community.%20Experiments%20across%0Amathematical%20reasoning%20benchmarks%20demonstrate%20the%20effectiveness%20of%20both%20the%0Acurated%20data%20and%20the%20proposed%20VAS.%20Comprehensive%20ablation%20studies%20and%20analyses%0Aprovide%20further%20insight%20into%20the%20contributions%20of%20each%20component.%20In%20addition%2C%0Awe%20theoretically%20establish%20that%20reward%20variance%20lower-bounds%20the%20expected%0Apolicy%20gradient%20magnitude%2C%20with%20VAS%20serving%20as%20a%20practical%20mechanism%20to%20realize%0Athis%20guarantee.%20Our%20code%2C%20data%2C%20and%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/LengSicong/MMR1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21268v1&entry.124074799=Read"},
{"title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse\n  Chains of Thought Patterns", "author": "Xuemiao Zhang and Can Ren and Chengying Tu and Rongxiang Weng and Shuo Wang and Hongfei Yan and Jingang Wang and Xunliang Cai", "abstract": "  Recent progress in large reasoning models for challenging mathematical\nreasoning has been driven by reinforcement learning (RL). Incorporating long\nchain-of-thought (CoT) data during mid-training has also been shown to\nsubstantially improve reasoning depth. However, current approaches often\nutilize CoT data indiscriminately, leaving open the critical question of which\ndata types most effectively enhance model reasoning capabilities. In this\npaper, we define the foundation model's reasoning potential for the first time\nas the inverse of the number of independent attempts required to correctly\nanswer the question, which is strongly correlated with the final model\nperformance. We then propose utilizing diverse data enriched with high-value\nreasoning patterns to expand the reasoning potential. Specifically, we abstract\natomic reasoning patterns from CoT sequences, characterized by commonality and\ninductive capabilities, and use them to construct a core reference set enriched\nwith valuable reasoning patterns. Furthermore, we propose a dual-granularity\nalgorithm involving chains of reasoning patterns and token entropy, efficiently\nselecting high-value CoT data (CoTP) from the data pool that aligns with the\ncore set, thereby training models to master reasoning effectively. Only\n10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve\nby 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of\ndownstream RL performance by 7.81%.\n", "link": "http://arxiv.org/abs/2509.21124v1", "date": "2025-09-25", "relevancy": 2.1362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20Reasoning%20Potential%20in%20Foundation%20Model%20by%20Learning%20Diverse%0A%20%20Chains%20of%20Thought%20Patterns&body=Title%3A%20Expanding%20Reasoning%20Potential%20in%20Foundation%20Model%20by%20Learning%20Diverse%0A%20%20Chains%20of%20Thought%20Patterns%0AAuthor%3A%20Xuemiao%20Zhang%20and%20Can%20Ren%20and%20Chengying%20Tu%20and%20Rongxiang%20Weng%20and%20Shuo%20Wang%20and%20Hongfei%20Yan%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%0AAbstract%3A%20%20%20Recent%20progress%20in%20large%20reasoning%20models%20for%20challenging%20mathematical%0Areasoning%20has%20been%20driven%20by%20reinforcement%20learning%20%28RL%29.%20Incorporating%20long%0Achain-of-thought%20%28CoT%29%20data%20during%20mid-training%20has%20also%20been%20shown%20to%0Asubstantially%20improve%20reasoning%20depth.%20However%2C%20current%20approaches%20often%0Autilize%20CoT%20data%20indiscriminately%2C%20leaving%20open%20the%20critical%20question%20of%20which%0Adata%20types%20most%20effectively%20enhance%20model%20reasoning%20capabilities.%20In%20this%0Apaper%2C%20we%20define%20the%20foundation%20model%27s%20reasoning%20potential%20for%20the%20first%20time%0Aas%20the%20inverse%20of%20the%20number%20of%20independent%20attempts%20required%20to%20correctly%0Aanswer%20the%20question%2C%20which%20is%20strongly%20correlated%20with%20the%20final%20model%0Aperformance.%20We%20then%20propose%20utilizing%20diverse%20data%20enriched%20with%20high-value%0Areasoning%20patterns%20to%20expand%20the%20reasoning%20potential.%20Specifically%2C%20we%20abstract%0Aatomic%20reasoning%20patterns%20from%20CoT%20sequences%2C%20characterized%20by%20commonality%20and%0Ainductive%20capabilities%2C%20and%20use%20them%20to%20construct%20a%20core%20reference%20set%20enriched%0Awith%20valuable%20reasoning%20patterns.%20Furthermore%2C%20we%20propose%20a%20dual-granularity%0Aalgorithm%20involving%20chains%20of%20reasoning%20patterns%20and%20token%20entropy%2C%20efficiently%0Aselecting%20high-value%20CoT%20data%20%28CoTP%29%20from%20the%20data%20pool%20that%20aligns%20with%20the%0Acore%20set%2C%20thereby%20training%20models%20to%20master%20reasoning%20effectively.%20Only%0A10B-token%20CoTP%20data%20enables%20the%2085A6B%20Mixture-of-Experts%20%28MoE%29%20model%20to%20improve%0Aby%209.58%25%20on%20the%20challenging%20AIME%202024%20and%202025%2C%20and%20to%20raise%20the%20upper%20bound%20of%0Adownstream%20RL%20performance%20by%207.81%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520Reasoning%2520Potential%2520in%2520Foundation%2520Model%2520by%2520Learning%2520Diverse%250A%2520%2520Chains%2520of%2520Thought%2520Patterns%26entry.906535625%3DXuemiao%2520Zhang%2520and%2520Can%2520Ren%2520and%2520Chengying%2520Tu%2520and%2520Rongxiang%2520Weng%2520and%2520Shuo%2520Wang%2520and%2520Hongfei%2520Yan%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520large%2520reasoning%2520models%2520for%2520challenging%2520mathematical%250Areasoning%2520has%2520been%2520driven%2520by%2520reinforcement%2520learning%2520%2528RL%2529.%2520Incorporating%2520long%250Achain-of-thought%2520%2528CoT%2529%2520data%2520during%2520mid-training%2520has%2520also%2520been%2520shown%2520to%250Asubstantially%2520improve%2520reasoning%2520depth.%2520However%252C%2520current%2520approaches%2520often%250Autilize%2520CoT%2520data%2520indiscriminately%252C%2520leaving%2520open%2520the%2520critical%2520question%2520of%2520which%250Adata%2520types%2520most%2520effectively%2520enhance%2520model%2520reasoning%2520capabilities.%2520In%2520this%250Apaper%252C%2520we%2520define%2520the%2520foundation%2520model%2527s%2520reasoning%2520potential%2520for%2520the%2520first%2520time%250Aas%2520the%2520inverse%2520of%2520the%2520number%2520of%2520independent%2520attempts%2520required%2520to%2520correctly%250Aanswer%2520the%2520question%252C%2520which%2520is%2520strongly%2520correlated%2520with%2520the%2520final%2520model%250Aperformance.%2520We%2520then%2520propose%2520utilizing%2520diverse%2520data%2520enriched%2520with%2520high-value%250Areasoning%2520patterns%2520to%2520expand%2520the%2520reasoning%2520potential.%2520Specifically%252C%2520we%2520abstract%250Aatomic%2520reasoning%2520patterns%2520from%2520CoT%2520sequences%252C%2520characterized%2520by%2520commonality%2520and%250Ainductive%2520capabilities%252C%2520and%2520use%2520them%2520to%2520construct%2520a%2520core%2520reference%2520set%2520enriched%250Awith%2520valuable%2520reasoning%2520patterns.%2520Furthermore%252C%2520we%2520propose%2520a%2520dual-granularity%250Aalgorithm%2520involving%2520chains%2520of%2520reasoning%2520patterns%2520and%2520token%2520entropy%252C%2520efficiently%250Aselecting%2520high-value%2520CoT%2520data%2520%2528CoTP%2529%2520from%2520the%2520data%2520pool%2520that%2520aligns%2520with%2520the%250Acore%2520set%252C%2520thereby%2520training%2520models%2520to%2520master%2520reasoning%2520effectively.%2520Only%250A10B-token%2520CoTP%2520data%2520enables%2520the%252085A6B%2520Mixture-of-Experts%2520%2528MoE%2529%2520model%2520to%2520improve%250Aby%25209.58%2525%2520on%2520the%2520challenging%2520AIME%25202024%2520and%25202025%252C%2520and%2520to%2520raise%2520the%2520upper%2520bound%2520of%250Adownstream%2520RL%2520performance%2520by%25207.81%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20Reasoning%20Potential%20in%20Foundation%20Model%20by%20Learning%20Diverse%0A%20%20Chains%20of%20Thought%20Patterns&entry.906535625=Xuemiao%20Zhang%20and%20Can%20Ren%20and%20Chengying%20Tu%20and%20Rongxiang%20Weng%20and%20Shuo%20Wang%20and%20Hongfei%20Yan%20and%20Jingang%20Wang%20and%20Xunliang%20Cai&entry.1292438233=%20%20Recent%20progress%20in%20large%20reasoning%20models%20for%20challenging%20mathematical%0Areasoning%20has%20been%20driven%20by%20reinforcement%20learning%20%28RL%29.%20Incorporating%20long%0Achain-of-thought%20%28CoT%29%20data%20during%20mid-training%20has%20also%20been%20shown%20to%0Asubstantially%20improve%20reasoning%20depth.%20However%2C%20current%20approaches%20often%0Autilize%20CoT%20data%20indiscriminately%2C%20leaving%20open%20the%20critical%20question%20of%20which%0Adata%20types%20most%20effectively%20enhance%20model%20reasoning%20capabilities.%20In%20this%0Apaper%2C%20we%20define%20the%20foundation%20model%27s%20reasoning%20potential%20for%20the%20first%20time%0Aas%20the%20inverse%20of%20the%20number%20of%20independent%20attempts%20required%20to%20correctly%0Aanswer%20the%20question%2C%20which%20is%20strongly%20correlated%20with%20the%20final%20model%0Aperformance.%20We%20then%20propose%20utilizing%20diverse%20data%20enriched%20with%20high-value%0Areasoning%20patterns%20to%20expand%20the%20reasoning%20potential.%20Specifically%2C%20we%20abstract%0Aatomic%20reasoning%20patterns%20from%20CoT%20sequences%2C%20characterized%20by%20commonality%20and%0Ainductive%20capabilities%2C%20and%20use%20them%20to%20construct%20a%20core%20reference%20set%20enriched%0Awith%20valuable%20reasoning%20patterns.%20Furthermore%2C%20we%20propose%20a%20dual-granularity%0Aalgorithm%20involving%20chains%20of%20reasoning%20patterns%20and%20token%20entropy%2C%20efficiently%0Aselecting%20high-value%20CoT%20data%20%28CoTP%29%20from%20the%20data%20pool%20that%20aligns%20with%20the%0Acore%20set%2C%20thereby%20training%20models%20to%20master%20reasoning%20effectively.%20Only%0A10B-token%20CoTP%20data%20enables%20the%2085A6B%20Mixture-of-Experts%20%28MoE%29%20model%20to%20improve%0Aby%209.58%25%20on%20the%20challenging%20AIME%202024%20and%202025%2C%20and%20to%20raise%20the%20upper%20bound%20of%0Adownstream%20RL%20performance%20by%207.81%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21124v1&entry.124074799=Read"},
{"title": "SAGE: A Realistic Benchmark for Semantic Understanding", "author": "Samarth Goel and Reagan J. Lee and Kannan Ramchandran", "abstract": "  As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment.\n", "link": "http://arxiv.org/abs/2509.21310v1", "date": "2025-09-25", "relevancy": 2.1349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGE%3A%20A%20Realistic%20Benchmark%20for%20Semantic%20Understanding&body=Title%3A%20SAGE%3A%20A%20Realistic%20Benchmark%20for%20Semantic%20Understanding%0AAuthor%3A%20Samarth%20Goel%20and%20Reagan%20J.%20Lee%20and%20Kannan%20Ramchandran%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20on%20traditional%0Abenchmarks%2C%20there%20is%20an%20urgent%20need%20for%20more%20challenging%20evaluation%20frameworks%0Athat%20probe%20deeper%20aspects%20of%20semantic%20understanding.%20We%20introduce%20SAGE%0A%28Semantic%20Alignment%20%26%20Generalization%20Evaluation%29%2C%20a%20rigorous%20benchmark%20designed%0Ato%20assess%20both%20embedding%20models%20and%20similarity%20metrics%20across%20five%20categories%3A%0AHuman%20Preference%20Alignment%2C%20Transformation%20Robustness%2C%20Information%20Sensitivity%2C%0AClustering%20Performance%2C%20and%20Retrieval%20Robustness.%20Unlike%20existing%20benchmarks%0Athat%20focus%20on%20isolated%20capabilities%2C%20SAGE%20evaluates%20semantic%20understanding%0Athrough%20adversarial%20conditions%2C%20noisy%20transformations%2C%20and%20nuanced%20human%0Ajudgment%20tasks%20across%2030%2B%20datasets.%20Our%20comprehensive%20evaluation%20of%209%20embedding%0Amodels%20and%20classical%20metrics%20reveals%20significant%20performance%20gaps%2C%20with%20no%0Asingle%20approach%20excelling%20across%20all%20dimensions.%20For%20instance%2C%20while%0Astate-of-the-art%20embedding%20models%20like%20OpenAI%27s%20text-embedding-3-large%20dominate%0Ain%20aligning%20with%20human%20preferences%20%280.682%20vs.%200.591%20for%20the%20best%20classical%0Ametric%29%2C%20they%20are%20significantly%20outperformed%20by%20classical%20metrics%20on%0Ainformation%20sensitivity%20tasks%2C%20where%20Jaccard%20Similarity%20achieves%20a%20score%20of%0A0.905%20compared%20to%20the%20top%20embedding%20score%20of%200.794.%20SAGE%20further%20uncovers%0Acritical%20trade-offs%3A%20OpenAI%27s%20text-embedding-3-small%20achieves%20the%20highest%0Aclustering%20performance%20%280.483%29%20but%20demonstrates%20extreme%20brittleness%20with%20the%0Alowest%20robustness%20score%20%280.011%29.%20SAGE%20exposes%20critical%20limitations%20in%20current%0Asemantic%20understanding%20capabilities%20and%20provides%20a%20more%20realistic%20assessment%20of%0Amodel%20robustness%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGE%253A%2520A%2520Realistic%2520Benchmark%2520for%2520Semantic%2520Understanding%26entry.906535625%3DSamarth%2520Goel%2520and%2520Reagan%2520J.%2520Lee%2520and%2520Kannan%2520Ramchandran%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520strong%2520performance%2520on%2520traditional%250Abenchmarks%252C%2520there%2520is%2520an%2520urgent%2520need%2520for%2520more%2520challenging%2520evaluation%2520frameworks%250Athat%2520probe%2520deeper%2520aspects%2520of%2520semantic%2520understanding.%2520We%2520introduce%2520SAGE%250A%2528Semantic%2520Alignment%2520%2526%2520Generalization%2520Evaluation%2529%252C%2520a%2520rigorous%2520benchmark%2520designed%250Ato%2520assess%2520both%2520embedding%2520models%2520and%2520similarity%2520metrics%2520across%2520five%2520categories%253A%250AHuman%2520Preference%2520Alignment%252C%2520Transformation%2520Robustness%252C%2520Information%2520Sensitivity%252C%250AClustering%2520Performance%252C%2520and%2520Retrieval%2520Robustness.%2520Unlike%2520existing%2520benchmarks%250Athat%2520focus%2520on%2520isolated%2520capabilities%252C%2520SAGE%2520evaluates%2520semantic%2520understanding%250Athrough%2520adversarial%2520conditions%252C%2520noisy%2520transformations%252C%2520and%2520nuanced%2520human%250Ajudgment%2520tasks%2520across%252030%252B%2520datasets.%2520Our%2520comprehensive%2520evaluation%2520of%25209%2520embedding%250Amodels%2520and%2520classical%2520metrics%2520reveals%2520significant%2520performance%2520gaps%252C%2520with%2520no%250Asingle%2520approach%2520excelling%2520across%2520all%2520dimensions.%2520For%2520instance%252C%2520while%250Astate-of-the-art%2520embedding%2520models%2520like%2520OpenAI%2527s%2520text-embedding-3-large%2520dominate%250Ain%2520aligning%2520with%2520human%2520preferences%2520%25280.682%2520vs.%25200.591%2520for%2520the%2520best%2520classical%250Ametric%2529%252C%2520they%2520are%2520significantly%2520outperformed%2520by%2520classical%2520metrics%2520on%250Ainformation%2520sensitivity%2520tasks%252C%2520where%2520Jaccard%2520Similarity%2520achieves%2520a%2520score%2520of%250A0.905%2520compared%2520to%2520the%2520top%2520embedding%2520score%2520of%25200.794.%2520SAGE%2520further%2520uncovers%250Acritical%2520trade-offs%253A%2520OpenAI%2527s%2520text-embedding-3-small%2520achieves%2520the%2520highest%250Aclustering%2520performance%2520%25280.483%2529%2520but%2520demonstrates%2520extreme%2520brittleness%2520with%2520the%250Alowest%2520robustness%2520score%2520%25280.011%2529.%2520SAGE%2520exposes%2520critical%2520limitations%2520in%2520current%250Asemantic%2520understanding%2520capabilities%2520and%2520provides%2520a%2520more%2520realistic%2520assessment%2520of%250Amodel%2520robustness%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGE%3A%20A%20Realistic%20Benchmark%20for%20Semantic%20Understanding&entry.906535625=Samarth%20Goel%20and%20Reagan%20J.%20Lee%20and%20Kannan%20Ramchandran&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20on%20traditional%0Abenchmarks%2C%20there%20is%20an%20urgent%20need%20for%20more%20challenging%20evaluation%20frameworks%0Athat%20probe%20deeper%20aspects%20of%20semantic%20understanding.%20We%20introduce%20SAGE%0A%28Semantic%20Alignment%20%26%20Generalization%20Evaluation%29%2C%20a%20rigorous%20benchmark%20designed%0Ato%20assess%20both%20embedding%20models%20and%20similarity%20metrics%20across%20five%20categories%3A%0AHuman%20Preference%20Alignment%2C%20Transformation%20Robustness%2C%20Information%20Sensitivity%2C%0AClustering%20Performance%2C%20and%20Retrieval%20Robustness.%20Unlike%20existing%20benchmarks%0Athat%20focus%20on%20isolated%20capabilities%2C%20SAGE%20evaluates%20semantic%20understanding%0Athrough%20adversarial%20conditions%2C%20noisy%20transformations%2C%20and%20nuanced%20human%0Ajudgment%20tasks%20across%2030%2B%20datasets.%20Our%20comprehensive%20evaluation%20of%209%20embedding%0Amodels%20and%20classical%20metrics%20reveals%20significant%20performance%20gaps%2C%20with%20no%0Asingle%20approach%20excelling%20across%20all%20dimensions.%20For%20instance%2C%20while%0Astate-of-the-art%20embedding%20models%20like%20OpenAI%27s%20text-embedding-3-large%20dominate%0Ain%20aligning%20with%20human%20preferences%20%280.682%20vs.%200.591%20for%20the%20best%20classical%0Ametric%29%2C%20they%20are%20significantly%20outperformed%20by%20classical%20metrics%20on%0Ainformation%20sensitivity%20tasks%2C%20where%20Jaccard%20Similarity%20achieves%20a%20score%20of%0A0.905%20compared%20to%20the%20top%20embedding%20score%20of%200.794.%20SAGE%20further%20uncovers%0Acritical%20trade-offs%3A%20OpenAI%27s%20text-embedding-3-small%20achieves%20the%20highest%0Aclustering%20performance%20%280.483%29%20but%20demonstrates%20extreme%20brittleness%20with%20the%0Alowest%20robustness%20score%20%280.011%29.%20SAGE%20exposes%20critical%20limitations%20in%20current%0Asemantic%20understanding%20capabilities%20and%20provides%20a%20more%20realistic%20assessment%20of%0Amodel%20robustness%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21310v1&entry.124074799=Read"},
{"title": "MedVSR: Medical Video Super-Resolution with Cross State-Space\n  Propagation", "author": "Xinyu Liu and Guolei Sun and Cheng Wang and Yixuan Yuan and Ender Konukoglu", "abstract": "  High-resolution (HR) medical videos are vital for accurate diagnosis, yet are\nhard to acquire due to hardware limitations and physiological constraints.\nClinically, the collected low-resolution (LR) medical videos present unique\nchallenges for video super-resolution (VSR) models, including camera shake,\nnoise, and abrupt frame transitions, which result in significant optical flow\nerrors and alignment difficulties. Additionally, tissues and organs exhibit\ncontinuous and nuanced structures, but current VSR models are prone to\nintroducing artifacts and distorted features that can mislead doctors. To this\nend, we propose MedVSR, a tailored framework for medical VSR. It first employs\nCross State-Space Propagation (CSSP) to address the imprecise alignment by\nprojecting distant frames as control matrices within state-space models,\nenabling the selective propagation of consistent and informative features to\nneighboring frames for effective alignment. Moreover, we design an Inner\nState-Space Reconstruction (ISSR) module that enhances tissue structures and\nreduces artifacts with joint long-range spatial feature learning and\nlarge-kernel short-range information aggregation. Experiments across four\ndatasets in diverse medical scenarios, including endoscopy and cataract\nsurgeries, show that MedVSR significantly outperforms existing VSR models in\nreconstruction performance and efficiency. Code released at\nhttps://github.com/CUHK-AIM-Group/MedVSR.\n", "link": "http://arxiv.org/abs/2509.21265v1", "date": "2025-09-25", "relevancy": 2.1285, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5496}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5349}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedVSR%3A%20Medical%20Video%20Super-Resolution%20with%20Cross%20State-Space%0A%20%20Propagation&body=Title%3A%20MedVSR%3A%20Medical%20Video%20Super-Resolution%20with%20Cross%20State-Space%0A%20%20Propagation%0AAuthor%3A%20Xinyu%20Liu%20and%20Guolei%20Sun%20and%20Cheng%20Wang%20and%20Yixuan%20Yuan%20and%20Ender%20Konukoglu%0AAbstract%3A%20%20%20High-resolution%20%28HR%29%20medical%20videos%20are%20vital%20for%20accurate%20diagnosis%2C%20yet%20are%0Ahard%20to%20acquire%20due%20to%20hardware%20limitations%20and%20physiological%20constraints.%0AClinically%2C%20the%20collected%20low-resolution%20%28LR%29%20medical%20videos%20present%20unique%0Achallenges%20for%20video%20super-resolution%20%28VSR%29%20models%2C%20including%20camera%20shake%2C%0Anoise%2C%20and%20abrupt%20frame%20transitions%2C%20which%20result%20in%20significant%20optical%20flow%0Aerrors%20and%20alignment%20difficulties.%20Additionally%2C%20tissues%20and%20organs%20exhibit%0Acontinuous%20and%20nuanced%20structures%2C%20but%20current%20VSR%20models%20are%20prone%20to%0Aintroducing%20artifacts%20and%20distorted%20features%20that%20can%20mislead%20doctors.%20To%20this%0Aend%2C%20we%20propose%20MedVSR%2C%20a%20tailored%20framework%20for%20medical%20VSR.%20It%20first%20employs%0ACross%20State-Space%20Propagation%20%28CSSP%29%20to%20address%20the%20imprecise%20alignment%20by%0Aprojecting%20distant%20frames%20as%20control%20matrices%20within%20state-space%20models%2C%0Aenabling%20the%20selective%20propagation%20of%20consistent%20and%20informative%20features%20to%0Aneighboring%20frames%20for%20effective%20alignment.%20Moreover%2C%20we%20design%20an%20Inner%0AState-Space%20Reconstruction%20%28ISSR%29%20module%20that%20enhances%20tissue%20structures%20and%0Areduces%20artifacts%20with%20joint%20long-range%20spatial%20feature%20learning%20and%0Alarge-kernel%20short-range%20information%20aggregation.%20Experiments%20across%20four%0Adatasets%20in%20diverse%20medical%20scenarios%2C%20including%20endoscopy%20and%20cataract%0Asurgeries%2C%20show%20that%20MedVSR%20significantly%20outperforms%20existing%20VSR%20models%20in%0Areconstruction%20performance%20and%20efficiency.%20Code%20released%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/MedVSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedVSR%253A%2520Medical%2520Video%2520Super-Resolution%2520with%2520Cross%2520State-Space%250A%2520%2520Propagation%26entry.906535625%3DXinyu%2520Liu%2520and%2520Guolei%2520Sun%2520and%2520Cheng%2520Wang%2520and%2520Yixuan%2520Yuan%2520and%2520Ender%2520Konukoglu%26entry.1292438233%3D%2520%2520High-resolution%2520%2528HR%2529%2520medical%2520videos%2520are%2520vital%2520for%2520accurate%2520diagnosis%252C%2520yet%2520are%250Ahard%2520to%2520acquire%2520due%2520to%2520hardware%2520limitations%2520and%2520physiological%2520constraints.%250AClinically%252C%2520the%2520collected%2520low-resolution%2520%2528LR%2529%2520medical%2520videos%2520present%2520unique%250Achallenges%2520for%2520video%2520super-resolution%2520%2528VSR%2529%2520models%252C%2520including%2520camera%2520shake%252C%250Anoise%252C%2520and%2520abrupt%2520frame%2520transitions%252C%2520which%2520result%2520in%2520significant%2520optical%2520flow%250Aerrors%2520and%2520alignment%2520difficulties.%2520Additionally%252C%2520tissues%2520and%2520organs%2520exhibit%250Acontinuous%2520and%2520nuanced%2520structures%252C%2520but%2520current%2520VSR%2520models%2520are%2520prone%2520to%250Aintroducing%2520artifacts%2520and%2520distorted%2520features%2520that%2520can%2520mislead%2520doctors.%2520To%2520this%250Aend%252C%2520we%2520propose%2520MedVSR%252C%2520a%2520tailored%2520framework%2520for%2520medical%2520VSR.%2520It%2520first%2520employs%250ACross%2520State-Space%2520Propagation%2520%2528CSSP%2529%2520to%2520address%2520the%2520imprecise%2520alignment%2520by%250Aprojecting%2520distant%2520frames%2520as%2520control%2520matrices%2520within%2520state-space%2520models%252C%250Aenabling%2520the%2520selective%2520propagation%2520of%2520consistent%2520and%2520informative%2520features%2520to%250Aneighboring%2520frames%2520for%2520effective%2520alignment.%2520Moreover%252C%2520we%2520design%2520an%2520Inner%250AState-Space%2520Reconstruction%2520%2528ISSR%2529%2520module%2520that%2520enhances%2520tissue%2520structures%2520and%250Areduces%2520artifacts%2520with%2520joint%2520long-range%2520spatial%2520feature%2520learning%2520and%250Alarge-kernel%2520short-range%2520information%2520aggregation.%2520Experiments%2520across%2520four%250Adatasets%2520in%2520diverse%2520medical%2520scenarios%252C%2520including%2520endoscopy%2520and%2520cataract%250Asurgeries%252C%2520show%2520that%2520MedVSR%2520significantly%2520outperforms%2520existing%2520VSR%2520models%2520in%250Areconstruction%2520performance%2520and%2520efficiency.%2520Code%2520released%2520at%250Ahttps%253A//github.com/CUHK-AIM-Group/MedVSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedVSR%3A%20Medical%20Video%20Super-Resolution%20with%20Cross%20State-Space%0A%20%20Propagation&entry.906535625=Xinyu%20Liu%20and%20Guolei%20Sun%20and%20Cheng%20Wang%20and%20Yixuan%20Yuan%20and%20Ender%20Konukoglu&entry.1292438233=%20%20High-resolution%20%28HR%29%20medical%20videos%20are%20vital%20for%20accurate%20diagnosis%2C%20yet%20are%0Ahard%20to%20acquire%20due%20to%20hardware%20limitations%20and%20physiological%20constraints.%0AClinically%2C%20the%20collected%20low-resolution%20%28LR%29%20medical%20videos%20present%20unique%0Achallenges%20for%20video%20super-resolution%20%28VSR%29%20models%2C%20including%20camera%20shake%2C%0Anoise%2C%20and%20abrupt%20frame%20transitions%2C%20which%20result%20in%20significant%20optical%20flow%0Aerrors%20and%20alignment%20difficulties.%20Additionally%2C%20tissues%20and%20organs%20exhibit%0Acontinuous%20and%20nuanced%20structures%2C%20but%20current%20VSR%20models%20are%20prone%20to%0Aintroducing%20artifacts%20and%20distorted%20features%20that%20can%20mislead%20doctors.%20To%20this%0Aend%2C%20we%20propose%20MedVSR%2C%20a%20tailored%20framework%20for%20medical%20VSR.%20It%20first%20employs%0ACross%20State-Space%20Propagation%20%28CSSP%29%20to%20address%20the%20imprecise%20alignment%20by%0Aprojecting%20distant%20frames%20as%20control%20matrices%20within%20state-space%20models%2C%0Aenabling%20the%20selective%20propagation%20of%20consistent%20and%20informative%20features%20to%0Aneighboring%20frames%20for%20effective%20alignment.%20Moreover%2C%20we%20design%20an%20Inner%0AState-Space%20Reconstruction%20%28ISSR%29%20module%20that%20enhances%20tissue%20structures%20and%0Areduces%20artifacts%20with%20joint%20long-range%20spatial%20feature%20learning%20and%0Alarge-kernel%20short-range%20information%20aggregation.%20Experiments%20across%20four%0Adatasets%20in%20diverse%20medical%20scenarios%2C%20including%20endoscopy%20and%20cataract%0Asurgeries%2C%20show%20that%20MedVSR%20significantly%20outperforms%20existing%20VSR%20models%20in%0Areconstruction%20performance%20and%20efficiency.%20Code%20released%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/MedVSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21265v1&entry.124074799=Read"},
{"title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection:\n  Leveraging Synthetic Data and Relative Context Discrepancy", "author": "Tian Lan and Hao Duong Le and Jinbo Li and Wenjun He and Meng Wang and Chenghao Liu and Chen Zhang", "abstract": "  Time series anomaly detection (TSAD) is a critical task, but developing\nmodels that generalize to unseen data in a zero-shot manner remains a major\nchallenge. Prevailing foundation models for TSAD predominantly rely on\nreconstruction-based objectives, which suffer from a fundamental objective\nmismatch: they struggle to identify subtle anomalies while often\nmisinterpreting complex normal patterns, leading to high rates of false\nnegatives and positives. To overcome these limitations, we introduce\n\\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new\npre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning\nto reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify\nanomalies by detecting significant discrepancies between adjacent time windows.\nThis relational approach, implemented with a standard Transformer architecture,\nenables the model to capture contextual shifts indicative of anomalies that\nreconstruction-based methods often miss. To facilitate this paradigm, we\ndevelop a large-scale, diverse synthetic corpus with token-level anomaly\nlabels, providing the rich supervisory signal necessary for effective\npre-training. Extensive experiments demonstrate that \\texttt{TimeRCD}\nsignificantly outperforms existing general-purpose and anomaly-specific\nfoundation models in zero-shot TSAD across diverse datasets. Our results\nvalidate the superiority of the RCD paradigm and establish a new, effective\npath toward building robust and generalizable foundation models for time series\nanomaly detection.\n", "link": "http://arxiv.org/abs/2509.21190v1", "date": "2025-09-25", "relevancy": 2.1214, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20for%20Zero-Shot%20Time%20Series%20Anomaly%20Detection%3A%0A%20%20Leveraging%20Synthetic%20Data%20and%20Relative%20Context%20Discrepancy&body=Title%3A%20Towards%20Foundation%20Models%20for%20Zero-Shot%20Time%20Series%20Anomaly%20Detection%3A%0A%20%20Leveraging%20Synthetic%20Data%20and%20Relative%20Context%20Discrepancy%0AAuthor%3A%20Tian%20Lan%20and%20Hao%20Duong%20Le%20and%20Jinbo%20Li%20and%20Wenjun%20He%20and%20Meng%20Wang%20and%20Chenghao%20Liu%20and%20Chen%20Zhang%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20%28TSAD%29%20is%20a%20critical%20task%2C%20but%20developing%0Amodels%20that%20generalize%20to%20unseen%20data%20in%20a%20zero-shot%20manner%20remains%20a%20major%0Achallenge.%20Prevailing%20foundation%20models%20for%20TSAD%20predominantly%20rely%20on%0Areconstruction-based%20objectives%2C%20which%20suffer%20from%20a%20fundamental%20objective%0Amismatch%3A%20they%20struggle%20to%20identify%20subtle%20anomalies%20while%20often%0Amisinterpreting%20complex%20normal%20patterns%2C%20leading%20to%20high%20rates%20of%20false%0Anegatives%20and%20positives.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0A%5Ctexttt%7BTimeRCD%7D%2C%20a%20novel%20foundation%20model%20for%20TSAD%20built%20upon%20a%20new%0Apre-training%20paradigm%3A%20Relative%20Context%20Discrepancy%20%28RCD%29.%20Instead%20of%20learning%0Ato%20reconstruct%20inputs%2C%20%5Ctexttt%7BTimeRCD%7D%20is%20explicitly%20trained%20to%20identify%0Aanomalies%20by%20detecting%20significant%20discrepancies%20between%20adjacent%20time%20windows.%0AThis%20relational%20approach%2C%20implemented%20with%20a%20standard%20Transformer%20architecture%2C%0Aenables%20the%20model%20to%20capture%20contextual%20shifts%20indicative%20of%20anomalies%20that%0Areconstruction-based%20methods%20often%20miss.%20To%20facilitate%20this%20paradigm%2C%20we%0Adevelop%20a%20large-scale%2C%20diverse%20synthetic%20corpus%20with%20token-level%20anomaly%0Alabels%2C%20providing%20the%20rich%20supervisory%20signal%20necessary%20for%20effective%0Apre-training.%20Extensive%20experiments%20demonstrate%20that%20%5Ctexttt%7BTimeRCD%7D%0Asignificantly%20outperforms%20existing%20general-purpose%20and%20anomaly-specific%0Afoundation%20models%20in%20zero-shot%20TSAD%20across%20diverse%20datasets.%20Our%20results%0Avalidate%20the%20superiority%20of%20the%20RCD%20paradigm%20and%20establish%20a%20new%2C%20effective%0Apath%20toward%20building%20robust%20and%20generalizable%20foundation%20models%20for%20time%20series%0Aanomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520for%2520Zero-Shot%2520Time%2520Series%2520Anomaly%2520Detection%253A%250A%2520%2520Leveraging%2520Synthetic%2520Data%2520and%2520Relative%2520Context%2520Discrepancy%26entry.906535625%3DTian%2520Lan%2520and%2520Hao%2520Duong%2520Le%2520and%2520Jinbo%2520Li%2520and%2520Wenjun%2520He%2520and%2520Meng%2520Wang%2520and%2520Chenghao%2520Liu%2520and%2520Chen%2520Zhang%26entry.1292438233%3D%2520%2520Time%2520series%2520anomaly%2520detection%2520%2528TSAD%2529%2520is%2520a%2520critical%2520task%252C%2520but%2520developing%250Amodels%2520that%2520generalize%2520to%2520unseen%2520data%2520in%2520a%2520zero-shot%2520manner%2520remains%2520a%2520major%250Achallenge.%2520Prevailing%2520foundation%2520models%2520for%2520TSAD%2520predominantly%2520rely%2520on%250Areconstruction-based%2520objectives%252C%2520which%2520suffer%2520from%2520a%2520fundamental%2520objective%250Amismatch%253A%2520they%2520struggle%2520to%2520identify%2520subtle%2520anomalies%2520while%2520often%250Amisinterpreting%2520complex%2520normal%2520patterns%252C%2520leading%2520to%2520high%2520rates%2520of%2520false%250Anegatives%2520and%2520positives.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%250A%255Ctexttt%257BTimeRCD%257D%252C%2520a%2520novel%2520foundation%2520model%2520for%2520TSAD%2520built%2520upon%2520a%2520new%250Apre-training%2520paradigm%253A%2520Relative%2520Context%2520Discrepancy%2520%2528RCD%2529.%2520Instead%2520of%2520learning%250Ato%2520reconstruct%2520inputs%252C%2520%255Ctexttt%257BTimeRCD%257D%2520is%2520explicitly%2520trained%2520to%2520identify%250Aanomalies%2520by%2520detecting%2520significant%2520discrepancies%2520between%2520adjacent%2520time%2520windows.%250AThis%2520relational%2520approach%252C%2520implemented%2520with%2520a%2520standard%2520Transformer%2520architecture%252C%250Aenables%2520the%2520model%2520to%2520capture%2520contextual%2520shifts%2520indicative%2520of%2520anomalies%2520that%250Areconstruction-based%2520methods%2520often%2520miss.%2520To%2520facilitate%2520this%2520paradigm%252C%2520we%250Adevelop%2520a%2520large-scale%252C%2520diverse%2520synthetic%2520corpus%2520with%2520token-level%2520anomaly%250Alabels%252C%2520providing%2520the%2520rich%2520supervisory%2520signal%2520necessary%2520for%2520effective%250Apre-training.%2520Extensive%2520experiments%2520demonstrate%2520that%2520%255Ctexttt%257BTimeRCD%257D%250Asignificantly%2520outperforms%2520existing%2520general-purpose%2520and%2520anomaly-specific%250Afoundation%2520models%2520in%2520zero-shot%2520TSAD%2520across%2520diverse%2520datasets.%2520Our%2520results%250Avalidate%2520the%2520superiority%2520of%2520the%2520RCD%2520paradigm%2520and%2520establish%2520a%2520new%252C%2520effective%250Apath%2520toward%2520building%2520robust%2520and%2520generalizable%2520foundation%2520models%2520for%2520time%2520series%250Aanomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20for%20Zero-Shot%20Time%20Series%20Anomaly%20Detection%3A%0A%20%20Leveraging%20Synthetic%20Data%20and%20Relative%20Context%20Discrepancy&entry.906535625=Tian%20Lan%20and%20Hao%20Duong%20Le%20and%20Jinbo%20Li%20and%20Wenjun%20He%20and%20Meng%20Wang%20and%20Chenghao%20Liu%20and%20Chen%20Zhang&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20%28TSAD%29%20is%20a%20critical%20task%2C%20but%20developing%0Amodels%20that%20generalize%20to%20unseen%20data%20in%20a%20zero-shot%20manner%20remains%20a%20major%0Achallenge.%20Prevailing%20foundation%20models%20for%20TSAD%20predominantly%20rely%20on%0Areconstruction-based%20objectives%2C%20which%20suffer%20from%20a%20fundamental%20objective%0Amismatch%3A%20they%20struggle%20to%20identify%20subtle%20anomalies%20while%20often%0Amisinterpreting%20complex%20normal%20patterns%2C%20leading%20to%20high%20rates%20of%20false%0Anegatives%20and%20positives.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0A%5Ctexttt%7BTimeRCD%7D%2C%20a%20novel%20foundation%20model%20for%20TSAD%20built%20upon%20a%20new%0Apre-training%20paradigm%3A%20Relative%20Context%20Discrepancy%20%28RCD%29.%20Instead%20of%20learning%0Ato%20reconstruct%20inputs%2C%20%5Ctexttt%7BTimeRCD%7D%20is%20explicitly%20trained%20to%20identify%0Aanomalies%20by%20detecting%20significant%20discrepancies%20between%20adjacent%20time%20windows.%0AThis%20relational%20approach%2C%20implemented%20with%20a%20standard%20Transformer%20architecture%2C%0Aenables%20the%20model%20to%20capture%20contextual%20shifts%20indicative%20of%20anomalies%20that%0Areconstruction-based%20methods%20often%20miss.%20To%20facilitate%20this%20paradigm%2C%20we%0Adevelop%20a%20large-scale%2C%20diverse%20synthetic%20corpus%20with%20token-level%20anomaly%0Alabels%2C%20providing%20the%20rich%20supervisory%20signal%20necessary%20for%20effective%0Apre-training.%20Extensive%20experiments%20demonstrate%20that%20%5Ctexttt%7BTimeRCD%7D%0Asignificantly%20outperforms%20existing%20general-purpose%20and%20anomaly-specific%0Afoundation%20models%20in%20zero-shot%20TSAD%20across%20diverse%20datasets.%20Our%20results%0Avalidate%20the%20superiority%20of%20the%20RCD%20paradigm%20and%20establish%20a%20new%2C%20effective%0Apath%20toward%20building%20robust%20and%20generalizable%20foundation%20models%20for%20time%20series%0Aanomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21190v1&entry.124074799=Read"},
{"title": "GraphUniverse: Enabling Systematic Evaluation of Inductive\n  Generalization", "author": "Louis Van Langendonck and Guillermo Bern\u00e1rdez and Nina Miolane and Pere Barlet-Ros", "abstract": "  A fundamental challenge in graph learning is understanding how models\ngeneralize to new, unseen graphs. While synthetic benchmarks offer controlled\nsettings for analysis, existing approaches are confined to single-graph,\ntransductive settings where models train and test on the same graph structure.\nAddressing this gap, we introduce GraphUniverse, a framework for generating\nentire families of graphs to enable the first systematic evaluation of\ninductive generalization at scale. Our core innovation is the generation of\ngraphs with persistent semantic communities, ensuring conceptual consistency\nwhile allowing fine-grained control over structural properties like homophily\nand degree distributions. This enables crucial but underexplored robustness\ntests, such as performance under controlled distribution shifts. Benchmarking a\nwide range of architectures -- from GNNs to graph transformers and topological\narchitectures -- reveals that strong transductive performance is a poor\npredictor of inductive generalization. Furthermore, we find that robustness to\ndistribution shift is highly sensitive not only to model architecture choice\nbut also to the initial graph regime (e.g., high vs. low homophily). Beyond\nbenchmarking, GraphUniverse's flexibility and scalability can facilitate the\ndevelopment of robust and truly generalizable architectures -- including\nnext-generation graph foundation models. An interactive demo is available at\nhttps://graphuniverse.streamlit.app.\n", "link": "http://arxiv.org/abs/2509.21097v1", "date": "2025-09-25", "relevancy": 2.1178, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5572}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5105}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphUniverse%3A%20Enabling%20Systematic%20Evaluation%20of%20Inductive%0A%20%20Generalization&body=Title%3A%20GraphUniverse%3A%20Enabling%20Systematic%20Evaluation%20of%20Inductive%0A%20%20Generalization%0AAuthor%3A%20Louis%20Van%20Langendonck%20and%20Guillermo%20Bern%C3%A1rdez%20and%20Nina%20Miolane%20and%20Pere%20Barlet-Ros%0AAbstract%3A%20%20%20A%20fundamental%20challenge%20in%20graph%20learning%20is%20understanding%20how%20models%0Ageneralize%20to%20new%2C%20unseen%20graphs.%20While%20synthetic%20benchmarks%20offer%20controlled%0Asettings%20for%20analysis%2C%20existing%20approaches%20are%20confined%20to%20single-graph%2C%0Atransductive%20settings%20where%20models%20train%20and%20test%20on%20the%20same%20graph%20structure.%0AAddressing%20this%20gap%2C%20we%20introduce%20GraphUniverse%2C%20a%20framework%20for%20generating%0Aentire%20families%20of%20graphs%20to%20enable%20the%20first%20systematic%20evaluation%20of%0Ainductive%20generalization%20at%20scale.%20Our%20core%20innovation%20is%20the%20generation%20of%0Agraphs%20with%20persistent%20semantic%20communities%2C%20ensuring%20conceptual%20consistency%0Awhile%20allowing%20fine-grained%20control%20over%20structural%20properties%20like%20homophily%0Aand%20degree%20distributions.%20This%20enables%20crucial%20but%20underexplored%20robustness%0Atests%2C%20such%20as%20performance%20under%20controlled%20distribution%20shifts.%20Benchmarking%20a%0Awide%20range%20of%20architectures%20--%20from%20GNNs%20to%20graph%20transformers%20and%20topological%0Aarchitectures%20--%20reveals%20that%20strong%20transductive%20performance%20is%20a%20poor%0Apredictor%20of%20inductive%20generalization.%20Furthermore%2C%20we%20find%20that%20robustness%20to%0Adistribution%20shift%20is%20highly%20sensitive%20not%20only%20to%20model%20architecture%20choice%0Abut%20also%20to%20the%20initial%20graph%20regime%20%28e.g.%2C%20high%20vs.%20low%20homophily%29.%20Beyond%0Abenchmarking%2C%20GraphUniverse%27s%20flexibility%20and%20scalability%20can%20facilitate%20the%0Adevelopment%20of%20robust%20and%20truly%20generalizable%20architectures%20--%20including%0Anext-generation%20graph%20foundation%20models.%20An%20interactive%20demo%20is%20available%20at%0Ahttps%3A//graphuniverse.streamlit.app.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphUniverse%253A%2520Enabling%2520Systematic%2520Evaluation%2520of%2520Inductive%250A%2520%2520Generalization%26entry.906535625%3DLouis%2520Van%2520Langendonck%2520and%2520Guillermo%2520Bern%25C3%25A1rdez%2520and%2520Nina%2520Miolane%2520and%2520Pere%2520Barlet-Ros%26entry.1292438233%3D%2520%2520A%2520fundamental%2520challenge%2520in%2520graph%2520learning%2520is%2520understanding%2520how%2520models%250Ageneralize%2520to%2520new%252C%2520unseen%2520graphs.%2520While%2520synthetic%2520benchmarks%2520offer%2520controlled%250Asettings%2520for%2520analysis%252C%2520existing%2520approaches%2520are%2520confined%2520to%2520single-graph%252C%250Atransductive%2520settings%2520where%2520models%2520train%2520and%2520test%2520on%2520the%2520same%2520graph%2520structure.%250AAddressing%2520this%2520gap%252C%2520we%2520introduce%2520GraphUniverse%252C%2520a%2520framework%2520for%2520generating%250Aentire%2520families%2520of%2520graphs%2520to%2520enable%2520the%2520first%2520systematic%2520evaluation%2520of%250Ainductive%2520generalization%2520at%2520scale.%2520Our%2520core%2520innovation%2520is%2520the%2520generation%2520of%250Agraphs%2520with%2520persistent%2520semantic%2520communities%252C%2520ensuring%2520conceptual%2520consistency%250Awhile%2520allowing%2520fine-grained%2520control%2520over%2520structural%2520properties%2520like%2520homophily%250Aand%2520degree%2520distributions.%2520This%2520enables%2520crucial%2520but%2520underexplored%2520robustness%250Atests%252C%2520such%2520as%2520performance%2520under%2520controlled%2520distribution%2520shifts.%2520Benchmarking%2520a%250Awide%2520range%2520of%2520architectures%2520--%2520from%2520GNNs%2520to%2520graph%2520transformers%2520and%2520topological%250Aarchitectures%2520--%2520reveals%2520that%2520strong%2520transductive%2520performance%2520is%2520a%2520poor%250Apredictor%2520of%2520inductive%2520generalization.%2520Furthermore%252C%2520we%2520find%2520that%2520robustness%2520to%250Adistribution%2520shift%2520is%2520highly%2520sensitive%2520not%2520only%2520to%2520model%2520architecture%2520choice%250Abut%2520also%2520to%2520the%2520initial%2520graph%2520regime%2520%2528e.g.%252C%2520high%2520vs.%2520low%2520homophily%2529.%2520Beyond%250Abenchmarking%252C%2520GraphUniverse%2527s%2520flexibility%2520and%2520scalability%2520can%2520facilitate%2520the%250Adevelopment%2520of%2520robust%2520and%2520truly%2520generalizable%2520architectures%2520--%2520including%250Anext-generation%2520graph%2520foundation%2520models.%2520An%2520interactive%2520demo%2520is%2520available%2520at%250Ahttps%253A//graphuniverse.streamlit.app.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphUniverse%3A%20Enabling%20Systematic%20Evaluation%20of%20Inductive%0A%20%20Generalization&entry.906535625=Louis%20Van%20Langendonck%20and%20Guillermo%20Bern%C3%A1rdez%20and%20Nina%20Miolane%20and%20Pere%20Barlet-Ros&entry.1292438233=%20%20A%20fundamental%20challenge%20in%20graph%20learning%20is%20understanding%20how%20models%0Ageneralize%20to%20new%2C%20unseen%20graphs.%20While%20synthetic%20benchmarks%20offer%20controlled%0Asettings%20for%20analysis%2C%20existing%20approaches%20are%20confined%20to%20single-graph%2C%0Atransductive%20settings%20where%20models%20train%20and%20test%20on%20the%20same%20graph%20structure.%0AAddressing%20this%20gap%2C%20we%20introduce%20GraphUniverse%2C%20a%20framework%20for%20generating%0Aentire%20families%20of%20graphs%20to%20enable%20the%20first%20systematic%20evaluation%20of%0Ainductive%20generalization%20at%20scale.%20Our%20core%20innovation%20is%20the%20generation%20of%0Agraphs%20with%20persistent%20semantic%20communities%2C%20ensuring%20conceptual%20consistency%0Awhile%20allowing%20fine-grained%20control%20over%20structural%20properties%20like%20homophily%0Aand%20degree%20distributions.%20This%20enables%20crucial%20but%20underexplored%20robustness%0Atests%2C%20such%20as%20performance%20under%20controlled%20distribution%20shifts.%20Benchmarking%20a%0Awide%20range%20of%20architectures%20--%20from%20GNNs%20to%20graph%20transformers%20and%20topological%0Aarchitectures%20--%20reveals%20that%20strong%20transductive%20performance%20is%20a%20poor%0Apredictor%20of%20inductive%20generalization.%20Furthermore%2C%20we%20find%20that%20robustness%20to%0Adistribution%20shift%20is%20highly%20sensitive%20not%20only%20to%20model%20architecture%20choice%0Abut%20also%20to%20the%20initial%20graph%20regime%20%28e.g.%2C%20high%20vs.%20low%20homophily%29.%20Beyond%0Abenchmarking%2C%20GraphUniverse%27s%20flexibility%20and%20scalability%20can%20facilitate%20the%0Adevelopment%20of%20robust%20and%20truly%20generalizable%20architectures%20--%20including%0Anext-generation%20graph%20foundation%20models.%20An%20interactive%20demo%20is%20available%20at%0Ahttps%3A//graphuniverse.streamlit.app.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21097v1&entry.124074799=Read"},
{"title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs", "author": "Kohsei Matsutani and Shota Takashiro and Gouki Minegishi and Takeshi Kojima and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches.\n", "link": "http://arxiv.org/abs/2509.21128v1", "date": "2025-09-25", "relevancy": 2.1138, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL%20Squeezes%2C%20SFT%20Expands%3A%20A%20Comparative%20Study%20of%20Reasoning%20LLMs&body=Title%3A%20RL%20Squeezes%2C%20SFT%20Expands%3A%20A%20Comparative%20Study%20of%20Reasoning%20LLMs%0AAuthor%3A%20Kohsei%20Matsutani%20and%20Shota%20Takashiro%20and%20Gouki%20Minegishi%20and%20Takeshi%20Kojima%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20typically%20trained%20by%20reinforcement%20learning%0A%28RL%29%20with%20verifiable%20rewards%20%28RLVR%29%20and%20supervised%20fine-tuning%20%28SFT%29%20on%0Areasoning%20traces%20to%20improve%20their%20reasoning%20abilities.%20However%2C%20how%20these%0Amethods%20shape%20reasoning%20capabilities%20remains%20largely%20elusive.%20Going%20beyond%20an%0Aaccuracy-based%20investigation%20of%20how%20these%20two%20components%20sculpt%20the%20reasoning%0Aprocess%2C%20this%20paper%20introduces%20a%20novel%20analysis%20framework%20that%20quantifies%0Areasoning%20paths%20and%20captures%20their%20qualitative%20changes%20under%20each%20training%0Aprocess%20%28with%20models%20of%201.5B%2C%207B%2C%20and%2014B%20parameters%20on%20mathematical%20domains%29.%0ASpecifically%2C%20we%20investigate%20the%20reasoning%20process%20at%20two%20levels%20of%0Agranularity%3A%20the%20trajectory-level%2C%20which%20examines%20complete%20reasoning%20outputs%2C%0Aand%20the%20step-level%2C%20which%20analyzes%20reasoning%20graphs%20whose%20nodes%20correspond%20to%0Aindividual%20reasoning%20steps.%20Notably%2C%20clustering%20of%20unique%20reasoning%0Atrajectories%20shows%20complementary%20effects%3A%20RL%20compresses%20incorrect%20trajectories%2C%0Awhereas%20SFT%20expands%20correct%20ones.%20Step-level%20analysis%20reveals%20that%20RL%20steepens%0A%28about%202.5%20times%29%2C%20while%20SFT%20flattens%20%28reduced%20to%20about%20one-third%29%2C%20the%20decay%0Arates%20of%20node%20visitation%20frequency%2C%20degree%2C%20and%20betweenness%20centrality%0Adistributions%20in%20the%20reasoning%20graph.%20This%20indicates%20that%20RL%20concentrates%0Areasoning%20functionality%20into%20a%20small%20subset%20of%20steps%2C%20while%20SFT%20homogenizes%20it%0Aacross%20many%20steps.%20Furthermore%2C%20by%20evaluating%20the%20reasoning%20graph%20topologies%0Afrom%20multiple%20perspectives%2C%20we%20delineate%20the%20shared%20and%20distinct%0Acharacteristics%20of%20RL%20and%20SFT.%20Our%20work%20presents%20a%20novel%20reasoning%20path%0Aperspective%20that%20explains%20why%20the%20current%20best%20practice%20of%20two-stage%20training%2C%0Awith%20SFT%20followed%20by%20RL%2C%20is%20successful%2C%20and%20offers%20practical%20implications%20for%0Adata%20construction%20and%20more%20efficient%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL%2520Squeezes%252C%2520SFT%2520Expands%253A%2520A%2520Comparative%2520Study%2520of%2520Reasoning%2520LLMs%26entry.906535625%3DKohsei%2520Matsutani%2520and%2520Shota%2520Takashiro%2520and%2520Gouki%2520Minegishi%2520and%2520Takeshi%2520Kojima%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520typically%2520trained%2520by%2520reinforcement%2520learning%250A%2528RL%2529%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520and%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%250Areasoning%2520traces%2520to%2520improve%2520their%2520reasoning%2520abilities.%2520However%252C%2520how%2520these%250Amethods%2520shape%2520reasoning%2520capabilities%2520remains%2520largely%2520elusive.%2520Going%2520beyond%2520an%250Aaccuracy-based%2520investigation%2520of%2520how%2520these%2520two%2520components%2520sculpt%2520the%2520reasoning%250Aprocess%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520analysis%2520framework%2520that%2520quantifies%250Areasoning%2520paths%2520and%2520captures%2520their%2520qualitative%2520changes%2520under%2520each%2520training%250Aprocess%2520%2528with%2520models%2520of%25201.5B%252C%25207B%252C%2520and%252014B%2520parameters%2520on%2520mathematical%2520domains%2529.%250ASpecifically%252C%2520we%2520investigate%2520the%2520reasoning%2520process%2520at%2520two%2520levels%2520of%250Agranularity%253A%2520the%2520trajectory-level%252C%2520which%2520examines%2520complete%2520reasoning%2520outputs%252C%250Aand%2520the%2520step-level%252C%2520which%2520analyzes%2520reasoning%2520graphs%2520whose%2520nodes%2520correspond%2520to%250Aindividual%2520reasoning%2520steps.%2520Notably%252C%2520clustering%2520of%2520unique%2520reasoning%250Atrajectories%2520shows%2520complementary%2520effects%253A%2520RL%2520compresses%2520incorrect%2520trajectories%252C%250Awhereas%2520SFT%2520expands%2520correct%2520ones.%2520Step-level%2520analysis%2520reveals%2520that%2520RL%2520steepens%250A%2528about%25202.5%2520times%2529%252C%2520while%2520SFT%2520flattens%2520%2528reduced%2520to%2520about%2520one-third%2529%252C%2520the%2520decay%250Arates%2520of%2520node%2520visitation%2520frequency%252C%2520degree%252C%2520and%2520betweenness%2520centrality%250Adistributions%2520in%2520the%2520reasoning%2520graph.%2520This%2520indicates%2520that%2520RL%2520concentrates%250Areasoning%2520functionality%2520into%2520a%2520small%2520subset%2520of%2520steps%252C%2520while%2520SFT%2520homogenizes%2520it%250Aacross%2520many%2520steps.%2520Furthermore%252C%2520by%2520evaluating%2520the%2520reasoning%2520graph%2520topologies%250Afrom%2520multiple%2520perspectives%252C%2520we%2520delineate%2520the%2520shared%2520and%2520distinct%250Acharacteristics%2520of%2520RL%2520and%2520SFT.%2520Our%2520work%2520presents%2520a%2520novel%2520reasoning%2520path%250Aperspective%2520that%2520explains%2520why%2520the%2520current%2520best%2520practice%2520of%2520two-stage%2520training%252C%250Awith%2520SFT%2520followed%2520by%2520RL%252C%2520is%2520successful%252C%2520and%2520offers%2520practical%2520implications%2520for%250Adata%2520construction%2520and%2520more%2520efficient%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL%20Squeezes%2C%20SFT%20Expands%3A%20A%20Comparative%20Study%20of%20Reasoning%20LLMs&entry.906535625=Kohsei%20Matsutani%20and%20Shota%20Takashiro%20and%20Gouki%20Minegishi%20and%20Takeshi%20Kojima%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20typically%20trained%20by%20reinforcement%20learning%0A%28RL%29%20with%20verifiable%20rewards%20%28RLVR%29%20and%20supervised%20fine-tuning%20%28SFT%29%20on%0Areasoning%20traces%20to%20improve%20their%20reasoning%20abilities.%20However%2C%20how%20these%0Amethods%20shape%20reasoning%20capabilities%20remains%20largely%20elusive.%20Going%20beyond%20an%0Aaccuracy-based%20investigation%20of%20how%20these%20two%20components%20sculpt%20the%20reasoning%0Aprocess%2C%20this%20paper%20introduces%20a%20novel%20analysis%20framework%20that%20quantifies%0Areasoning%20paths%20and%20captures%20their%20qualitative%20changes%20under%20each%20training%0Aprocess%20%28with%20models%20of%201.5B%2C%207B%2C%20and%2014B%20parameters%20on%20mathematical%20domains%29.%0ASpecifically%2C%20we%20investigate%20the%20reasoning%20process%20at%20two%20levels%20of%0Agranularity%3A%20the%20trajectory-level%2C%20which%20examines%20complete%20reasoning%20outputs%2C%0Aand%20the%20step-level%2C%20which%20analyzes%20reasoning%20graphs%20whose%20nodes%20correspond%20to%0Aindividual%20reasoning%20steps.%20Notably%2C%20clustering%20of%20unique%20reasoning%0Atrajectories%20shows%20complementary%20effects%3A%20RL%20compresses%20incorrect%20trajectories%2C%0Awhereas%20SFT%20expands%20correct%20ones.%20Step-level%20analysis%20reveals%20that%20RL%20steepens%0A%28about%202.5%20times%29%2C%20while%20SFT%20flattens%20%28reduced%20to%20about%20one-third%29%2C%20the%20decay%0Arates%20of%20node%20visitation%20frequency%2C%20degree%2C%20and%20betweenness%20centrality%0Adistributions%20in%20the%20reasoning%20graph.%20This%20indicates%20that%20RL%20concentrates%0Areasoning%20functionality%20into%20a%20small%20subset%20of%20steps%2C%20while%20SFT%20homogenizes%20it%0Aacross%20many%20steps.%20Furthermore%2C%20by%20evaluating%20the%20reasoning%20graph%20topologies%0Afrom%20multiple%20perspectives%2C%20we%20delineate%20the%20shared%20and%20distinct%0Acharacteristics%20of%20RL%20and%20SFT.%20Our%20work%20presents%20a%20novel%20reasoning%20path%0Aperspective%20that%20explains%20why%20the%20current%20best%20practice%20of%20two-stage%20training%2C%0Awith%20SFT%20followed%20by%20RL%2C%20is%20successful%2C%20and%20offers%20practical%20implications%20for%0Adata%20construction%20and%20more%20efficient%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21128v1&entry.124074799=Read"},
{"title": "Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned\n  Latent Space", "author": "Aditya Sengar and Jiying Zhang and Pierre Vandergheynst and Patrick Barth", "abstract": "  Simulating the long-timescale dynamics of biomolecules is a central challenge\nin computational science. While enhanced sampling methods can accelerate these\nsimulations, they rely on pre-defined collective variables that are often\ndifficult to identify. A recent generative model, LD-FPG, demonstrated that\nthis problem could be bypassed by learning to sample the static equilibrium\nensemble as all-atom deformations from a reference structure, establishing a\npowerful method for all-atom ensemble generation. However, while this approach\nsuccessfully captures a system's probable conformations, it does not model the\ntemporal evolution between them. We introduce the Graph Latent Dynamics\nPropagator (GLDP), a modular component for simulating dynamics within the\nlearned latent space of LD-FPG. We then compare three classes of propagators:\n(i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and\n(iii) autoregressive neural networks. Within a unified\nencoder-propagator-decoder framework, we evaluate long-horizon stability,\nbackbone and side-chain ensemble fidelity, and functional free-energy\nlandscapes. Autoregressive neural networks deliver the most robust long\nrollouts; score-guided Langevin best recovers side-chain thermodynamics when\nthe score is well learned; and Koopman provides an interpretable, lightweight\nbaseline that tends to damp fluctuations. These results clarify the trade-offs\namong propagators and offer practical guidance for latent-space simulators of\nall-atom protein dynamics.\n", "link": "http://arxiv.org/abs/2509.02196v3", "date": "2025-09-25", "relevancy": 2.0989, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5217}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Ensembles%3A%20Simulating%20All-Atom%20Protein%20Dynamics%20in%20a%20Learned%0A%20%20Latent%20Space&body=Title%3A%20Beyond%20Ensembles%3A%20Simulating%20All-Atom%20Protein%20Dynamics%20in%20a%20Learned%0A%20%20Latent%20Space%0AAuthor%3A%20Aditya%20Sengar%20and%20Jiying%20Zhang%20and%20Pierre%20Vandergheynst%20and%20Patrick%20Barth%0AAbstract%3A%20%20%20Simulating%20the%20long-timescale%20dynamics%20of%20biomolecules%20is%20a%20central%20challenge%0Ain%20computational%20science.%20While%20enhanced%20sampling%20methods%20can%20accelerate%20these%0Asimulations%2C%20they%20rely%20on%20pre-defined%20collective%20variables%20that%20are%20often%0Adifficult%20to%20identify.%20A%20recent%20generative%20model%2C%20LD-FPG%2C%20demonstrated%20that%0Athis%20problem%20could%20be%20bypassed%20by%20learning%20to%20sample%20the%20static%20equilibrium%0Aensemble%20as%20all-atom%20deformations%20from%20a%20reference%20structure%2C%20establishing%20a%0Apowerful%20method%20for%20all-atom%20ensemble%20generation.%20However%2C%20while%20this%20approach%0Asuccessfully%20captures%20a%20system%27s%20probable%20conformations%2C%20it%20does%20not%20model%20the%0Atemporal%20evolution%20between%20them.%20We%20introduce%20the%20Graph%20Latent%20Dynamics%0APropagator%20%28GLDP%29%2C%20a%20modular%20component%20for%20simulating%20dynamics%20within%20the%0Alearned%20latent%20space%20of%20LD-FPG.%20We%20then%20compare%20three%20classes%20of%20propagators%3A%0A%28i%29%20score-guided%20Langevin%20dynamics%2C%20%28ii%29%20Koopman-based%20linear%20operators%2C%20and%0A%28iii%29%20autoregressive%20neural%20networks.%20Within%20a%20unified%0Aencoder-propagator-decoder%20framework%2C%20we%20evaluate%20long-horizon%20stability%2C%0Abackbone%20and%20side-chain%20ensemble%20fidelity%2C%20and%20functional%20free-energy%0Alandscapes.%20Autoregressive%20neural%20networks%20deliver%20the%20most%20robust%20long%0Arollouts%3B%20score-guided%20Langevin%20best%20recovers%20side-chain%20thermodynamics%20when%0Athe%20score%20is%20well%20learned%3B%20and%20Koopman%20provides%20an%20interpretable%2C%20lightweight%0Abaseline%20that%20tends%20to%20damp%20fluctuations.%20These%20results%20clarify%20the%20trade-offs%0Aamong%20propagators%20and%20offer%20practical%20guidance%20for%20latent-space%20simulators%20of%0Aall-atom%20protein%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Ensembles%253A%2520Simulating%2520All-Atom%2520Protein%2520Dynamics%2520in%2520a%2520Learned%250A%2520%2520Latent%2520Space%26entry.906535625%3DAditya%2520Sengar%2520and%2520Jiying%2520Zhang%2520and%2520Pierre%2520Vandergheynst%2520and%2520Patrick%2520Barth%26entry.1292438233%3D%2520%2520Simulating%2520the%2520long-timescale%2520dynamics%2520of%2520biomolecules%2520is%2520a%2520central%2520challenge%250Ain%2520computational%2520science.%2520While%2520enhanced%2520sampling%2520methods%2520can%2520accelerate%2520these%250Asimulations%252C%2520they%2520rely%2520on%2520pre-defined%2520collective%2520variables%2520that%2520are%2520often%250Adifficult%2520to%2520identify.%2520A%2520recent%2520generative%2520model%252C%2520LD-FPG%252C%2520demonstrated%2520that%250Athis%2520problem%2520could%2520be%2520bypassed%2520by%2520learning%2520to%2520sample%2520the%2520static%2520equilibrium%250Aensemble%2520as%2520all-atom%2520deformations%2520from%2520a%2520reference%2520structure%252C%2520establishing%2520a%250Apowerful%2520method%2520for%2520all-atom%2520ensemble%2520generation.%2520However%252C%2520while%2520this%2520approach%250Asuccessfully%2520captures%2520a%2520system%2527s%2520probable%2520conformations%252C%2520it%2520does%2520not%2520model%2520the%250Atemporal%2520evolution%2520between%2520them.%2520We%2520introduce%2520the%2520Graph%2520Latent%2520Dynamics%250APropagator%2520%2528GLDP%2529%252C%2520a%2520modular%2520component%2520for%2520simulating%2520dynamics%2520within%2520the%250Alearned%2520latent%2520space%2520of%2520LD-FPG.%2520We%2520then%2520compare%2520three%2520classes%2520of%2520propagators%253A%250A%2528i%2529%2520score-guided%2520Langevin%2520dynamics%252C%2520%2528ii%2529%2520Koopman-based%2520linear%2520operators%252C%2520and%250A%2528iii%2529%2520autoregressive%2520neural%2520networks.%2520Within%2520a%2520unified%250Aencoder-propagator-decoder%2520framework%252C%2520we%2520evaluate%2520long-horizon%2520stability%252C%250Abackbone%2520and%2520side-chain%2520ensemble%2520fidelity%252C%2520and%2520functional%2520free-energy%250Alandscapes.%2520Autoregressive%2520neural%2520networks%2520deliver%2520the%2520most%2520robust%2520long%250Arollouts%253B%2520score-guided%2520Langevin%2520best%2520recovers%2520side-chain%2520thermodynamics%2520when%250Athe%2520score%2520is%2520well%2520learned%253B%2520and%2520Koopman%2520provides%2520an%2520interpretable%252C%2520lightweight%250Abaseline%2520that%2520tends%2520to%2520damp%2520fluctuations.%2520These%2520results%2520clarify%2520the%2520trade-offs%250Aamong%2520propagators%2520and%2520offer%2520practical%2520guidance%2520for%2520latent-space%2520simulators%2520of%250Aall-atom%2520protein%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Ensembles%3A%20Simulating%20All-Atom%20Protein%20Dynamics%20in%20a%20Learned%0A%20%20Latent%20Space&entry.906535625=Aditya%20Sengar%20and%20Jiying%20Zhang%20and%20Pierre%20Vandergheynst%20and%20Patrick%20Barth&entry.1292438233=%20%20Simulating%20the%20long-timescale%20dynamics%20of%20biomolecules%20is%20a%20central%20challenge%0Ain%20computational%20science.%20While%20enhanced%20sampling%20methods%20can%20accelerate%20these%0Asimulations%2C%20they%20rely%20on%20pre-defined%20collective%20variables%20that%20are%20often%0Adifficult%20to%20identify.%20A%20recent%20generative%20model%2C%20LD-FPG%2C%20demonstrated%20that%0Athis%20problem%20could%20be%20bypassed%20by%20learning%20to%20sample%20the%20static%20equilibrium%0Aensemble%20as%20all-atom%20deformations%20from%20a%20reference%20structure%2C%20establishing%20a%0Apowerful%20method%20for%20all-atom%20ensemble%20generation.%20However%2C%20while%20this%20approach%0Asuccessfully%20captures%20a%20system%27s%20probable%20conformations%2C%20it%20does%20not%20model%20the%0Atemporal%20evolution%20between%20them.%20We%20introduce%20the%20Graph%20Latent%20Dynamics%0APropagator%20%28GLDP%29%2C%20a%20modular%20component%20for%20simulating%20dynamics%20within%20the%0Alearned%20latent%20space%20of%20LD-FPG.%20We%20then%20compare%20three%20classes%20of%20propagators%3A%0A%28i%29%20score-guided%20Langevin%20dynamics%2C%20%28ii%29%20Koopman-based%20linear%20operators%2C%20and%0A%28iii%29%20autoregressive%20neural%20networks.%20Within%20a%20unified%0Aencoder-propagator-decoder%20framework%2C%20we%20evaluate%20long-horizon%20stability%2C%0Abackbone%20and%20side-chain%20ensemble%20fidelity%2C%20and%20functional%20free-energy%0Alandscapes.%20Autoregressive%20neural%20networks%20deliver%20the%20most%20robust%20long%0Arollouts%3B%20score-guided%20Langevin%20best%20recovers%20side-chain%20thermodynamics%20when%0Athe%20score%20is%20well%20learned%3B%20and%20Koopman%20provides%20an%20interpretable%2C%20lightweight%0Abaseline%20that%20tends%20to%20damp%20fluctuations.%20These%20results%20clarify%20the%20trade-offs%0Aamong%20propagators%20and%20offer%20practical%20guidance%20for%20latent-space%20simulators%20of%0Aall-atom%20protein%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02196v3&entry.124074799=Read"},
{"title": "Embodied Representation Alignment with Mirror Neurons", "author": "Wentao Zhu and Zhining Zhang and Yuwei Ren and Yin Huang and Hao Xu and Yizhou Wang", "abstract": "  Mirror neurons are a class of neurons that activate both when an individual\nobserves an action and when they perform the same action. This mechanism\nreveals a fundamental interplay between action understanding and embodied\nexecution, suggesting that these two abilities are inherently connected.\nNonetheless, existing machine learning methods largely overlook this interplay,\ntreating these abilities as separate tasks. In this study, we provide a unified\nperspective in modeling them through the lens of representation learning. We\nfirst observe that their intermediate representations spontaneously align.\nInspired by mirror neurons, we further introduce an approach that explicitly\naligns the representations of observed and executed actions. Specifically, we\nemploy two linear layers to map the representations to a shared latent space,\nwhere contrastive learning enforces the alignment of corresponding\nrepresentations, effectively maximizing their mutual information. Experiments\ndemonstrate that this simple approach fosters mutual synergy between the two\ntasks, effectively improving representation quality and generalization.\n", "link": "http://arxiv.org/abs/2509.21136v1", "date": "2025-09-25", "relevancy": 2.0943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Representation%20Alignment%20with%20Mirror%20Neurons&body=Title%3A%20Embodied%20Representation%20Alignment%20with%20Mirror%20Neurons%0AAuthor%3A%20Wentao%20Zhu%20and%20Zhining%20Zhang%20and%20Yuwei%20Ren%20and%20Yin%20Huang%20and%20Hao%20Xu%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Mirror%20neurons%20are%20a%20class%20of%20neurons%20that%20activate%20both%20when%20an%20individual%0Aobserves%20an%20action%20and%20when%20they%20perform%20the%20same%20action.%20This%20mechanism%0Areveals%20a%20fundamental%20interplay%20between%20action%20understanding%20and%20embodied%0Aexecution%2C%20suggesting%20that%20these%20two%20abilities%20are%20inherently%20connected.%0ANonetheless%2C%20existing%20machine%20learning%20methods%20largely%20overlook%20this%20interplay%2C%0Atreating%20these%20abilities%20as%20separate%20tasks.%20In%20this%20study%2C%20we%20provide%20a%20unified%0Aperspective%20in%20modeling%20them%20through%20the%20lens%20of%20representation%20learning.%20We%0Afirst%20observe%20that%20their%20intermediate%20representations%20spontaneously%20align.%0AInspired%20by%20mirror%20neurons%2C%20we%20further%20introduce%20an%20approach%20that%20explicitly%0Aaligns%20the%20representations%20of%20observed%20and%20executed%20actions.%20Specifically%2C%20we%0Aemploy%20two%20linear%20layers%20to%20map%20the%20representations%20to%20a%20shared%20latent%20space%2C%0Awhere%20contrastive%20learning%20enforces%20the%20alignment%20of%20corresponding%0Arepresentations%2C%20effectively%20maximizing%20their%20mutual%20information.%20Experiments%0Ademonstrate%20that%20this%20simple%20approach%20fosters%20mutual%20synergy%20between%20the%20two%0Atasks%2C%20effectively%20improving%20representation%20quality%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Representation%2520Alignment%2520with%2520Mirror%2520Neurons%26entry.906535625%3DWentao%2520Zhu%2520and%2520Zhining%2520Zhang%2520and%2520Yuwei%2520Ren%2520and%2520Yin%2520Huang%2520and%2520Hao%2520Xu%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Mirror%2520neurons%2520are%2520a%2520class%2520of%2520neurons%2520that%2520activate%2520both%2520when%2520an%2520individual%250Aobserves%2520an%2520action%2520and%2520when%2520they%2520perform%2520the%2520same%2520action.%2520This%2520mechanism%250Areveals%2520a%2520fundamental%2520interplay%2520between%2520action%2520understanding%2520and%2520embodied%250Aexecution%252C%2520suggesting%2520that%2520these%2520two%2520abilities%2520are%2520inherently%2520connected.%250ANonetheless%252C%2520existing%2520machine%2520learning%2520methods%2520largely%2520overlook%2520this%2520interplay%252C%250Atreating%2520these%2520abilities%2520as%2520separate%2520tasks.%2520In%2520this%2520study%252C%2520we%2520provide%2520a%2520unified%250Aperspective%2520in%2520modeling%2520them%2520through%2520the%2520lens%2520of%2520representation%2520learning.%2520We%250Afirst%2520observe%2520that%2520their%2520intermediate%2520representations%2520spontaneously%2520align.%250AInspired%2520by%2520mirror%2520neurons%252C%2520we%2520further%2520introduce%2520an%2520approach%2520that%2520explicitly%250Aaligns%2520the%2520representations%2520of%2520observed%2520and%2520executed%2520actions.%2520Specifically%252C%2520we%250Aemploy%2520two%2520linear%2520layers%2520to%2520map%2520the%2520representations%2520to%2520a%2520shared%2520latent%2520space%252C%250Awhere%2520contrastive%2520learning%2520enforces%2520the%2520alignment%2520of%2520corresponding%250Arepresentations%252C%2520effectively%2520maximizing%2520their%2520mutual%2520information.%2520Experiments%250Ademonstrate%2520that%2520this%2520simple%2520approach%2520fosters%2520mutual%2520synergy%2520between%2520the%2520two%250Atasks%252C%2520effectively%2520improving%2520representation%2520quality%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Representation%20Alignment%20with%20Mirror%20Neurons&entry.906535625=Wentao%20Zhu%20and%20Zhining%20Zhang%20and%20Yuwei%20Ren%20and%20Yin%20Huang%20and%20Hao%20Xu%20and%20Yizhou%20Wang&entry.1292438233=%20%20Mirror%20neurons%20are%20a%20class%20of%20neurons%20that%20activate%20both%20when%20an%20individual%0Aobserves%20an%20action%20and%20when%20they%20perform%20the%20same%20action.%20This%20mechanism%0Areveals%20a%20fundamental%20interplay%20between%20action%20understanding%20and%20embodied%0Aexecution%2C%20suggesting%20that%20these%20two%20abilities%20are%20inherently%20connected.%0ANonetheless%2C%20existing%20machine%20learning%20methods%20largely%20overlook%20this%20interplay%2C%0Atreating%20these%20abilities%20as%20separate%20tasks.%20In%20this%20study%2C%20we%20provide%20a%20unified%0Aperspective%20in%20modeling%20them%20through%20the%20lens%20of%20representation%20learning.%20We%0Afirst%20observe%20that%20their%20intermediate%20representations%20spontaneously%20align.%0AInspired%20by%20mirror%20neurons%2C%20we%20further%20introduce%20an%20approach%20that%20explicitly%0Aaligns%20the%20representations%20of%20observed%20and%20executed%20actions.%20Specifically%2C%20we%0Aemploy%20two%20linear%20layers%20to%20map%20the%20representations%20to%20a%20shared%20latent%20space%2C%0Awhere%20contrastive%20learning%20enforces%20the%20alignment%20of%20corresponding%0Arepresentations%2C%20effectively%20maximizing%20their%20mutual%20information.%20Experiments%0Ademonstrate%20that%20this%20simple%20approach%20fosters%20mutual%20synergy%20between%20the%20two%0Atasks%2C%20effectively%20improving%20representation%20quality%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21136v1&entry.124074799=Read"},
{"title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective", "author": "Yiwen Zhang and Ziang Chen and Fanqi Kong and Yizhe Huang and Xue Feng", "abstract": "  Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.\n", "link": "http://arxiv.org/abs/2509.21134v1", "date": "2025-09-25", "relevancy": 2.0925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToMPO%3A%20Training%20LLM%20Strategic%20Decision%20Making%20from%20a%20Multi-Agent%0A%20%20Perspective&body=Title%3A%20ToMPO%3A%20Training%20LLM%20Strategic%20Decision%20Making%20from%20a%20Multi-Agent%0A%20%20Perspective%0AAuthor%3A%20Yiwen%20Zhang%20and%20Ziang%20Chen%20and%20Fanqi%20Kong%20and%20Yizhe%20Huang%20and%20Xue%20Feng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20to%20make%20decisions%20in%20complex%0Ascenarios%2C%20where%20they%20need%20models%20to%20think%20deeply%2C%20reason%20logically%2C%20and%20decide%0Awisely.%20Many%20existing%20studies%20focus%20solely%20on%20multi-round%20conversations%20in%0Asocial%20tasks%20or%20simulated%20environments%2C%20neglecting%20the%20various%20types%20of%0Adecisions%20and%20their%20interdependence.%20Current%20reinforcement%20learning%20methods%0Astruggle%20to%20consider%20the%20strategies%20of%20others%20during%20training.%20To%20address%20these%0Aissues%2C%20we%20first%20define%20a%20strategic%20decision-making%20problem%20that%20includes%20two%0Atypes%20of%20decisions%20and%20their%20temporal%20dependencies.%20Furthermore%2C%20we%20propose%0A%2A%2AT%2A%2Aheory%20%2A%2Ao%2A%2Af%20%2A%2AM%2A%2Aind%20%2A%2AP%2A%2Aolicy%20%2A%2AO%2A%2Aptimization%20%2A%2A%28ToMPO%29%2A%2A%20algorithm%20to%0Aoptimize%20the%20perception%20of%20other%20individual%20strategies%20and%20the%20game%20situation%0Atrends.%20Compared%20to%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%2C%0AToMPO%20enhances%20the%20LLM%27s%20strategic%20decision-making%20mainly%20by%3A%201%29%20generating%0Arollouts%20based%20on%20reasoning%20the%20strategies%20of%20other%20individuals%2C%202%29%20estimating%0Aadvantages%20at%20both%20the%20graph-level%20and%20sample-level%2C%20and%203%29%20balancing%20global%0Aand%20partial%20rewards.%20The%20ToMPO%20algorithm%20outperforms%20the%20GRPO%20method%20by%2035%25%20in%0Aterms%20of%20model%20output%20compliance%20and%20cooperative%20outcomes.%20Additionally%2C%20when%0Acompared%20to%20models%20with%20parameter%20sizes%20100%20times%20larger%2C%20it%20shows%20an%2018%25%0Aimprovement.%20This%20demonstrates%20the%20effectiveness%20of%20the%20ToMPO%20algorithm%20in%0Aenhancing%20the%20model%27s%20strategic%20decision-making%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToMPO%253A%2520Training%2520LLM%2520Strategic%2520Decision%2520Making%2520from%2520a%2520Multi-Agent%250A%2520%2520Perspective%26entry.906535625%3DYiwen%2520Zhang%2520and%2520Ziang%2520Chen%2520and%2520Fanqi%2520Kong%2520and%2520Yizhe%2520Huang%2520and%2520Xue%2520Feng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520used%2520to%2520make%2520decisions%2520in%2520complex%250Ascenarios%252C%2520where%2520they%2520need%2520models%2520to%2520think%2520deeply%252C%2520reason%2520logically%252C%2520and%2520decide%250Awisely.%2520Many%2520existing%2520studies%2520focus%2520solely%2520on%2520multi-round%2520conversations%2520in%250Asocial%2520tasks%2520or%2520simulated%2520environments%252C%2520neglecting%2520the%2520various%2520types%2520of%250Adecisions%2520and%2520their%2520interdependence.%2520Current%2520reinforcement%2520learning%2520methods%250Astruggle%2520to%2520consider%2520the%2520strategies%2520of%2520others%2520during%2520training.%2520To%2520address%2520these%250Aissues%252C%2520we%2520first%2520define%2520a%2520strategic%2520decision-making%2520problem%2520that%2520includes%2520two%250Atypes%2520of%2520decisions%2520and%2520their%2520temporal%2520dependencies.%2520Furthermore%252C%2520we%2520propose%250A%252A%252AT%252A%252Aheory%2520%252A%252Ao%252A%252Af%2520%252A%252AM%252A%252Aind%2520%252A%252AP%252A%252Aolicy%2520%252A%252AO%252A%252Aptimization%2520%252A%252A%2528ToMPO%2529%252A%252A%2520algorithm%2520to%250Aoptimize%2520the%2520perception%2520of%2520other%2520individual%2520strategies%2520and%2520the%2520game%2520situation%250Atrends.%2520Compared%2520to%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520algorithm%252C%250AToMPO%2520enhances%2520the%2520LLM%2527s%2520strategic%2520decision-making%2520mainly%2520by%253A%25201%2529%2520generating%250Arollouts%2520based%2520on%2520reasoning%2520the%2520strategies%2520of%2520other%2520individuals%252C%25202%2529%2520estimating%250Aadvantages%2520at%2520both%2520the%2520graph-level%2520and%2520sample-level%252C%2520and%25203%2529%2520balancing%2520global%250Aand%2520partial%2520rewards.%2520The%2520ToMPO%2520algorithm%2520outperforms%2520the%2520GRPO%2520method%2520by%252035%2525%2520in%250Aterms%2520of%2520model%2520output%2520compliance%2520and%2520cooperative%2520outcomes.%2520Additionally%252C%2520when%250Acompared%2520to%2520models%2520with%2520parameter%2520sizes%2520100%2520times%2520larger%252C%2520it%2520shows%2520an%252018%2525%250Aimprovement.%2520This%2520demonstrates%2520the%2520effectiveness%2520of%2520the%2520ToMPO%2520algorithm%2520in%250Aenhancing%2520the%2520model%2527s%2520strategic%2520decision-making%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToMPO%3A%20Training%20LLM%20Strategic%20Decision%20Making%20from%20a%20Multi-Agent%0A%20%20Perspective&entry.906535625=Yiwen%20Zhang%20and%20Ziang%20Chen%20and%20Fanqi%20Kong%20and%20Yizhe%20Huang%20and%20Xue%20Feng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20to%20make%20decisions%20in%20complex%0Ascenarios%2C%20where%20they%20need%20models%20to%20think%20deeply%2C%20reason%20logically%2C%20and%20decide%0Awisely.%20Many%20existing%20studies%20focus%20solely%20on%20multi-round%20conversations%20in%0Asocial%20tasks%20or%20simulated%20environments%2C%20neglecting%20the%20various%20types%20of%0Adecisions%20and%20their%20interdependence.%20Current%20reinforcement%20learning%20methods%0Astruggle%20to%20consider%20the%20strategies%20of%20others%20during%20training.%20To%20address%20these%0Aissues%2C%20we%20first%20define%20a%20strategic%20decision-making%20problem%20that%20includes%20two%0Atypes%20of%20decisions%20and%20their%20temporal%20dependencies.%20Furthermore%2C%20we%20propose%0A%2A%2AT%2A%2Aheory%20%2A%2Ao%2A%2Af%20%2A%2AM%2A%2Aind%20%2A%2AP%2A%2Aolicy%20%2A%2AO%2A%2Aptimization%20%2A%2A%28ToMPO%29%2A%2A%20algorithm%20to%0Aoptimize%20the%20perception%20of%20other%20individual%20strategies%20and%20the%20game%20situation%0Atrends.%20Compared%20to%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%2C%0AToMPO%20enhances%20the%20LLM%27s%20strategic%20decision-making%20mainly%20by%3A%201%29%20generating%0Arollouts%20based%20on%20reasoning%20the%20strategies%20of%20other%20individuals%2C%202%29%20estimating%0Aadvantages%20at%20both%20the%20graph-level%20and%20sample-level%2C%20and%203%29%20balancing%20global%0Aand%20partial%20rewards.%20The%20ToMPO%20algorithm%20outperforms%20the%20GRPO%20method%20by%2035%25%20in%0Aterms%20of%20model%20output%20compliance%20and%20cooperative%20outcomes.%20Additionally%2C%20when%0Acompared%20to%20models%20with%20parameter%20sizes%20100%20times%20larger%2C%20it%20shows%20an%2018%25%0Aimprovement.%20This%20demonstrates%20the%20effectiveness%20of%20the%20ToMPO%20algorithm%20in%0Aenhancing%20the%20model%27s%20strategic%20decision-making%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21134v1&entry.124074799=Read"},
{"title": "Bayesian Attention Mechanism: A Probabilistic Framework for Positional\n  Encoding and Context Length Extrapolation", "author": "Arthur S. Bianchessi and Yasmin C. Aguirre and Rodrigo C. Barros and Lucas S. Kupssinsk\u00fc", "abstract": "  Transformer-based language models rely on positional encoding (PE) to handle\ntoken order and support context length extrapolation. However, existing PE\nmethods lack theoretical clarity and rely on limited evaluation metrics to\nsubstantiate their extrapolation claims. We propose the Bayesian Attention\nMechanism (BAM), a theoretical framework that formulates positional encoding as\na prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE\nand ALiBi) and motivates a new Generalized Gaussian positional prior that\nsubstantially improves long-context generalization. Empirically, BAM enables\naccurate information retrieval at $500\\times$ the training context length,\noutperforming previous state-of-the-art context length generalization in long\ncontext retrieval accuracy while maintaining comparable perplexity and\nintroducing minimal additional parameters.\n", "link": "http://arxiv.org/abs/2505.22842v2", "date": "2025-09-25", "relevancy": 2.0924, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.54}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Attention%20Mechanism%3A%20A%20Probabilistic%20Framework%20for%20Positional%0A%20%20Encoding%20and%20Context%20Length%20Extrapolation&body=Title%3A%20Bayesian%20Attention%20Mechanism%3A%20A%20Probabilistic%20Framework%20for%20Positional%0A%20%20Encoding%20and%20Context%20Length%20Extrapolation%0AAuthor%3A%20Arthur%20S.%20Bianchessi%20and%20Yasmin%20C.%20Aguirre%20and%20Rodrigo%20C.%20Barros%20and%20Lucas%20S.%20Kupssinsk%C3%BC%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20rely%20on%20positional%20encoding%20%28PE%29%20to%20handle%0Atoken%20order%20and%20support%20context%20length%20extrapolation.%20However%2C%20existing%20PE%0Amethods%20lack%20theoretical%20clarity%20and%20rely%20on%20limited%20evaluation%20metrics%20to%0Asubstantiate%20their%20extrapolation%20claims.%20We%20propose%20the%20Bayesian%20Attention%0AMechanism%20%28BAM%29%2C%20a%20theoretical%20framework%20that%20formulates%20positional%20encoding%20as%0Aa%20prior%20within%20a%20probabilistic%20model.%20BAM%20unifies%20existing%20methods%20%28e.g.%2C%20NoPE%0Aand%20ALiBi%29%20and%20motivates%20a%20new%20Generalized%20Gaussian%20positional%20prior%20that%0Asubstantially%20improves%20long-context%20generalization.%20Empirically%2C%20BAM%20enables%0Aaccurate%20information%20retrieval%20at%20%24500%5Ctimes%24%20the%20training%20context%20length%2C%0Aoutperforming%20previous%20state-of-the-art%20context%20length%20generalization%20in%20long%0Acontext%20retrieval%20accuracy%20while%20maintaining%20comparable%20perplexity%20and%0Aintroducing%20minimal%20additional%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Attention%2520Mechanism%253A%2520A%2520Probabilistic%2520Framework%2520for%2520Positional%250A%2520%2520Encoding%2520and%2520Context%2520Length%2520Extrapolation%26entry.906535625%3DArthur%2520S.%2520Bianchessi%2520and%2520Yasmin%2520C.%2520Aguirre%2520and%2520Rodrigo%2520C.%2520Barros%2520and%2520Lucas%2520S.%2520Kupssinsk%25C3%25BC%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520rely%2520on%2520positional%2520encoding%2520%2528PE%2529%2520to%2520handle%250Atoken%2520order%2520and%2520support%2520context%2520length%2520extrapolation.%2520However%252C%2520existing%2520PE%250Amethods%2520lack%2520theoretical%2520clarity%2520and%2520rely%2520on%2520limited%2520evaluation%2520metrics%2520to%250Asubstantiate%2520their%2520extrapolation%2520claims.%2520We%2520propose%2520the%2520Bayesian%2520Attention%250AMechanism%2520%2528BAM%2529%252C%2520a%2520theoretical%2520framework%2520that%2520formulates%2520positional%2520encoding%2520as%250Aa%2520prior%2520within%2520a%2520probabilistic%2520model.%2520BAM%2520unifies%2520existing%2520methods%2520%2528e.g.%252C%2520NoPE%250Aand%2520ALiBi%2529%2520and%2520motivates%2520a%2520new%2520Generalized%2520Gaussian%2520positional%2520prior%2520that%250Asubstantially%2520improves%2520long-context%2520generalization.%2520Empirically%252C%2520BAM%2520enables%250Aaccurate%2520information%2520retrieval%2520at%2520%2524500%255Ctimes%2524%2520the%2520training%2520context%2520length%252C%250Aoutperforming%2520previous%2520state-of-the-art%2520context%2520length%2520generalization%2520in%2520long%250Acontext%2520retrieval%2520accuracy%2520while%2520maintaining%2520comparable%2520perplexity%2520and%250Aintroducing%2520minimal%2520additional%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Attention%20Mechanism%3A%20A%20Probabilistic%20Framework%20for%20Positional%0A%20%20Encoding%20and%20Context%20Length%20Extrapolation&entry.906535625=Arthur%20S.%20Bianchessi%20and%20Yasmin%20C.%20Aguirre%20and%20Rodrigo%20C.%20Barros%20and%20Lucas%20S.%20Kupssinsk%C3%BC&entry.1292438233=%20%20Transformer-based%20language%20models%20rely%20on%20positional%20encoding%20%28PE%29%20to%20handle%0Atoken%20order%20and%20support%20context%20length%20extrapolation.%20However%2C%20existing%20PE%0Amethods%20lack%20theoretical%20clarity%20and%20rely%20on%20limited%20evaluation%20metrics%20to%0Asubstantiate%20their%20extrapolation%20claims.%20We%20propose%20the%20Bayesian%20Attention%0AMechanism%20%28BAM%29%2C%20a%20theoretical%20framework%20that%20formulates%20positional%20encoding%20as%0Aa%20prior%20within%20a%20probabilistic%20model.%20BAM%20unifies%20existing%20methods%20%28e.g.%2C%20NoPE%0Aand%20ALiBi%29%20and%20motivates%20a%20new%20Generalized%20Gaussian%20positional%20prior%20that%0Asubstantially%20improves%20long-context%20generalization.%20Empirically%2C%20BAM%20enables%0Aaccurate%20information%20retrieval%20at%20%24500%5Ctimes%24%20the%20training%20context%20length%2C%0Aoutperforming%20previous%20state-of-the-art%20context%20length%20generalization%20in%20long%0Acontext%20retrieval%20accuracy%20while%20maintaining%20comparable%20perplexity%20and%0Aintroducing%20minimal%20additional%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22842v2&entry.124074799=Read"},
{"title": "SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced\n  Representation Learning in Digital Pathology", "author": "Shakib Khan and Fariba Dambandkhameneh and Nazim Shaikh and Yao Nie and Raghavan Venugopal and Xiao Li", "abstract": "  Advances in computational pathology increasingly rely on extracting\nmeaningful representations from Whole Slide Images (WSIs) to support various\nclinical and biological tasks. In this study, we propose a generalizable deep\nlearning framework that integrates the Mamba architecture with Graph Neural\nNetworks (GNNs) for enhanced WSI analysis. Our method is designed to capture\nboth local spatial relationships and long-range contextual dependencies,\noffering a flexible architecture for digital pathology analysis. Mamba modules\nexcels in capturing long-range global dependencies, while GNNs emphasize\nfine-grained short-range spatial interactions. To effectively combine these\ncomplementary signals, we introduce an adaptive fusion strategy that uses an\nentropy-based confidence weighting mechanism. This approach dynamically\nbalances contributions from both branches by assigning higher weight to the\nbranch with more confident (lower-entropy) predictions, depending on the\ncontextual importance of local versus global information for different\ndownstream tasks. We demonstrate the utility of our approach on a\nrepresentative task: predicting gene fusion and mutation status from WSIs. Our\nframework, SlideMamba, achieves an area under the precision recall curve\n(PRAUC) of 0.751 \\pm 0.05, outperforming MIL (0.491 \\pm 0.042), Trans-MIL (0.39\n\\pm 0.017), Mamba-only (0.664 \\pm 0.063), GNN-only (0.748 \\pm 0.091), and a\nprior similar work GAT-Mamba (0.703 \\pm 0.075). SlideMamba also achieves\ncompetitive results across ROC AUC (0.738 \\pm 0.055), sensitivity (0.662 \\pm\n0.083), and specificity (0.725 \\pm 0.094). These results highlight the strength\nof the integrated architecture, enhanced by the proposed entropy-based adaptive\nfusion strategy, and suggest promising potential for application of\nspatially-resolved predictive modeling tasks in computational pathology.\n", "link": "http://arxiv.org/abs/2509.21239v1", "date": "2025-09-25", "relevancy": 2.0909, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideMamba%3A%20Entropy-Based%20Adaptive%20Fusion%20of%20GNN%20and%20Mamba%20for%20Enhanced%0A%20%20Representation%20Learning%20in%20Digital%20Pathology&body=Title%3A%20SlideMamba%3A%20Entropy-Based%20Adaptive%20Fusion%20of%20GNN%20and%20Mamba%20for%20Enhanced%0A%20%20Representation%20Learning%20in%20Digital%20Pathology%0AAuthor%3A%20Shakib%20Khan%20and%20Fariba%20Dambandkhameneh%20and%20Nazim%20Shaikh%20and%20Yao%20Nie%20and%20Raghavan%20Venugopal%20and%20Xiao%20Li%0AAbstract%3A%20%20%20Advances%20in%20computational%20pathology%20increasingly%20rely%20on%20extracting%0Ameaningful%20representations%20from%20Whole%20Slide%20Images%20%28WSIs%29%20to%20support%20various%0Aclinical%20and%20biological%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20generalizable%20deep%0Alearning%20framework%20that%20integrates%20the%20Mamba%20architecture%20with%20Graph%20Neural%0ANetworks%20%28GNNs%29%20for%20enhanced%20WSI%20analysis.%20Our%20method%20is%20designed%20to%20capture%0Aboth%20local%20spatial%20relationships%20and%20long-range%20contextual%20dependencies%2C%0Aoffering%20a%20flexible%20architecture%20for%20digital%20pathology%20analysis.%20Mamba%20modules%0Aexcels%20in%20capturing%20long-range%20global%20dependencies%2C%20while%20GNNs%20emphasize%0Afine-grained%20short-range%20spatial%20interactions.%20To%20effectively%20combine%20these%0Acomplementary%20signals%2C%20we%20introduce%20an%20adaptive%20fusion%20strategy%20that%20uses%20an%0Aentropy-based%20confidence%20weighting%20mechanism.%20This%20approach%20dynamically%0Abalances%20contributions%20from%20both%20branches%20by%20assigning%20higher%20weight%20to%20the%0Abranch%20with%20more%20confident%20%28lower-entropy%29%20predictions%2C%20depending%20on%20the%0Acontextual%20importance%20of%20local%20versus%20global%20information%20for%20different%0Adownstream%20tasks.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20on%20a%0Arepresentative%20task%3A%20predicting%20gene%20fusion%20and%20mutation%20status%20from%20WSIs.%20Our%0Aframework%2C%20SlideMamba%2C%20achieves%20an%20area%20under%20the%20precision%20recall%20curve%0A%28PRAUC%29%20of%200.751%20%5Cpm%200.05%2C%20outperforming%20MIL%20%280.491%20%5Cpm%200.042%29%2C%20Trans-MIL%20%280.39%0A%5Cpm%200.017%29%2C%20Mamba-only%20%280.664%20%5Cpm%200.063%29%2C%20GNN-only%20%280.748%20%5Cpm%200.091%29%2C%20and%20a%0Aprior%20similar%20work%20GAT-Mamba%20%280.703%20%5Cpm%200.075%29.%20SlideMamba%20also%20achieves%0Acompetitive%20results%20across%20ROC%20AUC%20%280.738%20%5Cpm%200.055%29%2C%20sensitivity%20%280.662%20%5Cpm%0A0.083%29%2C%20and%20specificity%20%280.725%20%5Cpm%200.094%29.%20These%20results%20highlight%20the%20strength%0Aof%20the%20integrated%20architecture%2C%20enhanced%20by%20the%20proposed%20entropy-based%20adaptive%0Afusion%20strategy%2C%20and%20suggest%20promising%20potential%20for%20application%20of%0Aspatially-resolved%20predictive%20modeling%20tasks%20in%20computational%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideMamba%253A%2520Entropy-Based%2520Adaptive%2520Fusion%2520of%2520GNN%2520and%2520Mamba%2520for%2520Enhanced%250A%2520%2520Representation%2520Learning%2520in%2520Digital%2520Pathology%26entry.906535625%3DShakib%2520Khan%2520and%2520Fariba%2520Dambandkhameneh%2520and%2520Nazim%2520Shaikh%2520and%2520Yao%2520Nie%2520and%2520Raghavan%2520Venugopal%2520and%2520Xiao%2520Li%26entry.1292438233%3D%2520%2520Advances%2520in%2520computational%2520pathology%2520increasingly%2520rely%2520on%2520extracting%250Ameaningful%2520representations%2520from%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520to%2520support%2520various%250Aclinical%2520and%2520biological%2520tasks.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520generalizable%2520deep%250Alearning%2520framework%2520that%2520integrates%2520the%2520Mamba%2520architecture%2520with%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520for%2520enhanced%2520WSI%2520analysis.%2520Our%2520method%2520is%2520designed%2520to%2520capture%250Aboth%2520local%2520spatial%2520relationships%2520and%2520long-range%2520contextual%2520dependencies%252C%250Aoffering%2520a%2520flexible%2520architecture%2520for%2520digital%2520pathology%2520analysis.%2520Mamba%2520modules%250Aexcels%2520in%2520capturing%2520long-range%2520global%2520dependencies%252C%2520while%2520GNNs%2520emphasize%250Afine-grained%2520short-range%2520spatial%2520interactions.%2520To%2520effectively%2520combine%2520these%250Acomplementary%2520signals%252C%2520we%2520introduce%2520an%2520adaptive%2520fusion%2520strategy%2520that%2520uses%2520an%250Aentropy-based%2520confidence%2520weighting%2520mechanism.%2520This%2520approach%2520dynamically%250Abalances%2520contributions%2520from%2520both%2520branches%2520by%2520assigning%2520higher%2520weight%2520to%2520the%250Abranch%2520with%2520more%2520confident%2520%2528lower-entropy%2529%2520predictions%252C%2520depending%2520on%2520the%250Acontextual%2520importance%2520of%2520local%2520versus%2520global%2520information%2520for%2520different%250Adownstream%2520tasks.%2520We%2520demonstrate%2520the%2520utility%2520of%2520our%2520approach%2520on%2520a%250Arepresentative%2520task%253A%2520predicting%2520gene%2520fusion%2520and%2520mutation%2520status%2520from%2520WSIs.%2520Our%250Aframework%252C%2520SlideMamba%252C%2520achieves%2520an%2520area%2520under%2520the%2520precision%2520recall%2520curve%250A%2528PRAUC%2529%2520of%25200.751%2520%255Cpm%25200.05%252C%2520outperforming%2520MIL%2520%25280.491%2520%255Cpm%25200.042%2529%252C%2520Trans-MIL%2520%25280.39%250A%255Cpm%25200.017%2529%252C%2520Mamba-only%2520%25280.664%2520%255Cpm%25200.063%2529%252C%2520GNN-only%2520%25280.748%2520%255Cpm%25200.091%2529%252C%2520and%2520a%250Aprior%2520similar%2520work%2520GAT-Mamba%2520%25280.703%2520%255Cpm%25200.075%2529.%2520SlideMamba%2520also%2520achieves%250Acompetitive%2520results%2520across%2520ROC%2520AUC%2520%25280.738%2520%255Cpm%25200.055%2529%252C%2520sensitivity%2520%25280.662%2520%255Cpm%250A0.083%2529%252C%2520and%2520specificity%2520%25280.725%2520%255Cpm%25200.094%2529.%2520These%2520results%2520highlight%2520the%2520strength%250Aof%2520the%2520integrated%2520architecture%252C%2520enhanced%2520by%2520the%2520proposed%2520entropy-based%2520adaptive%250Afusion%2520strategy%252C%2520and%2520suggest%2520promising%2520potential%2520for%2520application%2520of%250Aspatially-resolved%2520predictive%2520modeling%2520tasks%2520in%2520computational%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideMamba%3A%20Entropy-Based%20Adaptive%20Fusion%20of%20GNN%20and%20Mamba%20for%20Enhanced%0A%20%20Representation%20Learning%20in%20Digital%20Pathology&entry.906535625=Shakib%20Khan%20and%20Fariba%20Dambandkhameneh%20and%20Nazim%20Shaikh%20and%20Yao%20Nie%20and%20Raghavan%20Venugopal%20and%20Xiao%20Li&entry.1292438233=%20%20Advances%20in%20computational%20pathology%20increasingly%20rely%20on%20extracting%0Ameaningful%20representations%20from%20Whole%20Slide%20Images%20%28WSIs%29%20to%20support%20various%0Aclinical%20and%20biological%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20generalizable%20deep%0Alearning%20framework%20that%20integrates%20the%20Mamba%20architecture%20with%20Graph%20Neural%0ANetworks%20%28GNNs%29%20for%20enhanced%20WSI%20analysis.%20Our%20method%20is%20designed%20to%20capture%0Aboth%20local%20spatial%20relationships%20and%20long-range%20contextual%20dependencies%2C%0Aoffering%20a%20flexible%20architecture%20for%20digital%20pathology%20analysis.%20Mamba%20modules%0Aexcels%20in%20capturing%20long-range%20global%20dependencies%2C%20while%20GNNs%20emphasize%0Afine-grained%20short-range%20spatial%20interactions.%20To%20effectively%20combine%20these%0Acomplementary%20signals%2C%20we%20introduce%20an%20adaptive%20fusion%20strategy%20that%20uses%20an%0Aentropy-based%20confidence%20weighting%20mechanism.%20This%20approach%20dynamically%0Abalances%20contributions%20from%20both%20branches%20by%20assigning%20higher%20weight%20to%20the%0Abranch%20with%20more%20confident%20%28lower-entropy%29%20predictions%2C%20depending%20on%20the%0Acontextual%20importance%20of%20local%20versus%20global%20information%20for%20different%0Adownstream%20tasks.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20on%20a%0Arepresentative%20task%3A%20predicting%20gene%20fusion%20and%20mutation%20status%20from%20WSIs.%20Our%0Aframework%2C%20SlideMamba%2C%20achieves%20an%20area%20under%20the%20precision%20recall%20curve%0A%28PRAUC%29%20of%200.751%20%5Cpm%200.05%2C%20outperforming%20MIL%20%280.491%20%5Cpm%200.042%29%2C%20Trans-MIL%20%280.39%0A%5Cpm%200.017%29%2C%20Mamba-only%20%280.664%20%5Cpm%200.063%29%2C%20GNN-only%20%280.748%20%5Cpm%200.091%29%2C%20and%20a%0Aprior%20similar%20work%20GAT-Mamba%20%280.703%20%5Cpm%200.075%29.%20SlideMamba%20also%20achieves%0Acompetitive%20results%20across%20ROC%20AUC%20%280.738%20%5Cpm%200.055%29%2C%20sensitivity%20%280.662%20%5Cpm%0A0.083%29%2C%20and%20specificity%20%280.725%20%5Cpm%200.094%29.%20These%20results%20highlight%20the%20strength%0Aof%20the%20integrated%20architecture%2C%20enhanced%20by%20the%20proposed%20entropy-based%20adaptive%0Afusion%20strategy%2C%20and%20suggest%20promising%20potential%20for%20application%20of%0Aspatially-resolved%20predictive%20modeling%20tasks%20in%20computational%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21239v1&entry.124074799=Read"},
{"title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute", "author": "Junpei Komiyama and Daisuke Oba and Masafumi Oyamada", "abstract": "  We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach.\n", "link": "http://arxiv.org/abs/2509.21091v1", "date": "2025-09-25", "relevancy": 2.0849, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4231}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best-of-%24%5Cinfty%24%20--%20Asymptotic%20Performance%20of%20Test-Time%20Compute&body=Title%3A%20Best-of-%24%5Cinfty%24%20--%20Asymptotic%20Performance%20of%20Test-Time%20Compute%0AAuthor%3A%20Junpei%20Komiyama%20and%20Daisuke%20Oba%20and%20Masafumi%20Oyamada%0AAbstract%3A%20%20%20We%20study%20best-of-%24N%24%20for%20large%20language%20models%20%28LLMs%29%20where%20the%20selection%20is%0Abased%20on%20majority%20voting.%20In%20particular%2C%20we%20analyze%20the%20limit%20%24N%20%5Cto%20%5Cinfty%24%2C%0Awhich%20we%20denote%20as%20Best-of-%24%5Cinfty%24.%20While%20this%20approach%20achieves%20impressive%0Aperformance%20in%20the%20limit%2C%20it%20requires%20an%20infinite%20test-time%20budget.%20To%20address%0Athis%2C%20we%20propose%20an%20adaptive%20generation%20scheme%20that%20selects%20%24N%24%20based%20on%20answer%0Aagreement%2C%20thereby%20efficiently%20allocating%20inference-time%20computation.%20Beyond%0Aadaptivity%2C%20we%20extend%20the%20framework%20to%20weighted%20ensembles%20of%20multiple%20LLMs%2C%0Ashowing%20that%20such%20mixtures%20can%20outperform%20any%20individual%20model.%20The%20optimal%0Aensemble%20weighting%20is%20formulated%20and%20efficiently%20computed%20as%20a%20mixed-integer%0Alinear%20program.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest-of-%2524%255Cinfty%2524%2520--%2520Asymptotic%2520Performance%2520of%2520Test-Time%2520Compute%26entry.906535625%3DJunpei%2520Komiyama%2520and%2520Daisuke%2520Oba%2520and%2520Masafumi%2520Oyamada%26entry.1292438233%3D%2520%2520We%2520study%2520best-of-%2524N%2524%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520where%2520the%2520selection%2520is%250Abased%2520on%2520majority%2520voting.%2520In%2520particular%252C%2520we%2520analyze%2520the%2520limit%2520%2524N%2520%255Cto%2520%255Cinfty%2524%252C%250Awhich%2520we%2520denote%2520as%2520Best-of-%2524%255Cinfty%2524.%2520While%2520this%2520approach%2520achieves%2520impressive%250Aperformance%2520in%2520the%2520limit%252C%2520it%2520requires%2520an%2520infinite%2520test-time%2520budget.%2520To%2520address%250Athis%252C%2520we%2520propose%2520an%2520adaptive%2520generation%2520scheme%2520that%2520selects%2520%2524N%2524%2520based%2520on%2520answer%250Aagreement%252C%2520thereby%2520efficiently%2520allocating%2520inference-time%2520computation.%2520Beyond%250Aadaptivity%252C%2520we%2520extend%2520the%2520framework%2520to%2520weighted%2520ensembles%2520of%2520multiple%2520LLMs%252C%250Ashowing%2520that%2520such%2520mixtures%2520can%2520outperform%2520any%2520individual%2520model.%2520The%2520optimal%250Aensemble%2520weighting%2520is%2520formulated%2520and%2520efficiently%2520computed%2520as%2520a%2520mixed-integer%250Alinear%2520program.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best-of-%24%5Cinfty%24%20--%20Asymptotic%20Performance%20of%20Test-Time%20Compute&entry.906535625=Junpei%20Komiyama%20and%20Daisuke%20Oba%20and%20Masafumi%20Oyamada&entry.1292438233=%20%20We%20study%20best-of-%24N%24%20for%20large%20language%20models%20%28LLMs%29%20where%20the%20selection%20is%0Abased%20on%20majority%20voting.%20In%20particular%2C%20we%20analyze%20the%20limit%20%24N%20%5Cto%20%5Cinfty%24%2C%0Awhich%20we%20denote%20as%20Best-of-%24%5Cinfty%24.%20While%20this%20approach%20achieves%20impressive%0Aperformance%20in%20the%20limit%2C%20it%20requires%20an%20infinite%20test-time%20budget.%20To%20address%0Athis%2C%20we%20propose%20an%20adaptive%20generation%20scheme%20that%20selects%20%24N%24%20based%20on%20answer%0Aagreement%2C%20thereby%20efficiently%20allocating%20inference-time%20computation.%20Beyond%0Aadaptivity%2C%20we%20extend%20the%20framework%20to%20weighted%20ensembles%20of%20multiple%20LLMs%2C%0Ashowing%20that%20such%20mixtures%20can%20outperform%20any%20individual%20model.%20The%20optimal%0Aensemble%20weighting%20is%20formulated%20and%20efficiently%20computed%20as%20a%20mixed-integer%0Alinear%20program.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21091v1&entry.124074799=Read"},
{"title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "author": "Zhibo Hu and Chen Wang and Yanfeng Shu and Hye-Young Paik and Liming Zhu", "abstract": "  Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.\n", "link": "http://arxiv.org/abs/2505.11679v2", "date": "2025-09-25", "relevancy": 2.0795, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5241}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ambiguity%20Resolution%20in%20Text-to-Structured%20Data%20Mapping&body=Title%3A%20Ambiguity%20Resolution%20in%20Text-to-Structured%20Data%20Mapping%0AAuthor%3A%20Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-Young%20Paik%20and%20Liming%20Zhu%0AAbstract%3A%20%20%20Ambiguity%20in%20natural%20language%20is%20a%20significant%20obstacle%20for%20achieving%0Aaccurate%20text%20to%20structured%20data%20mapping%20through%20large%20language%20models%20%28LLMs%29%2C%0Awhich%20affects%20the%20performance%20of%20tasks%20such%20as%20mapping%20text%20to%20agentic%20tool%0Acalling%20and%20text-to-SQL%20queries.%20Existing%20methods%20to%20ambiguity%20handling%20either%0Arely%20on%20the%20ReACT%20framework%20to%20obtain%20correct%20mappings%20through%20trial%20and%20error%2C%0Aor%20on%20supervised%20fine-tuning%20to%20bias%20models%20toward%20specific%20tasks.%20In%20this%0Apaper%2C%20we%20adopt%20a%20different%20approach%20that%20characterizes%20representation%0Adifferences%20of%20ambiguous%20text%20in%20the%20latent%20space%20and%20leverages%20these%0Adifferences%20to%20identify%20ambiguity%20before%20mapping%20them%20to%20structured%20data.%20To%0Adetect%20sentence-level%20ambiguity%2C%20we%20focus%20on%20the%20relationship%20between%20ambiguous%0Aquestions%20and%20their%20interpretations.%20Unlike%20distances%20calculated%20by%20dense%0Aembeddings%2C%20we%20introduce%20a%20new%20distance%20measure%20based%20on%20a%20path%20kernel%20over%0Aconcepts.%20With%20this%20measurement%2C%20we%20identify%20patterns%20to%20distinguish%20ambiguous%0Afrom%20unambiguous%20questions.%20Furthermore%2C%20we%20propose%20a%20method%20for%20improving%20LLM%0Aperformance%20on%20ambiguous%20agentic%20tool%20calling%20through%20missing%20concept%0Aprediction.%20Both%20achieve%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmbiguity%2520Resolution%2520in%2520Text-to-Structured%2520Data%2520Mapping%26entry.906535625%3DZhibo%2520Hu%2520and%2520Chen%2520Wang%2520and%2520Yanfeng%2520Shu%2520and%2520Hye-Young%2520Paik%2520and%2520Liming%2520Zhu%26entry.1292438233%3D%2520%2520Ambiguity%2520in%2520natural%2520language%2520is%2520a%2520significant%2520obstacle%2520for%2520achieving%250Aaccurate%2520text%2520to%2520structured%2520data%2520mapping%2520through%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Awhich%2520affects%2520the%2520performance%2520of%2520tasks%2520such%2520as%2520mapping%2520text%2520to%2520agentic%2520tool%250Acalling%2520and%2520text-to-SQL%2520queries.%2520Existing%2520methods%2520to%2520ambiguity%2520handling%2520either%250Arely%2520on%2520the%2520ReACT%2520framework%2520to%2520obtain%2520correct%2520mappings%2520through%2520trial%2520and%2520error%252C%250Aor%2520on%2520supervised%2520fine-tuning%2520to%2520bias%2520models%2520toward%2520specific%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520adopt%2520a%2520different%2520approach%2520that%2520characterizes%2520representation%250Adifferences%2520of%2520ambiguous%2520text%2520in%2520the%2520latent%2520space%2520and%2520leverages%2520these%250Adifferences%2520to%2520identify%2520ambiguity%2520before%2520mapping%2520them%2520to%2520structured%2520data.%2520To%250Adetect%2520sentence-level%2520ambiguity%252C%2520we%2520focus%2520on%2520the%2520relationship%2520between%2520ambiguous%250Aquestions%2520and%2520their%2520interpretations.%2520Unlike%2520distances%2520calculated%2520by%2520dense%250Aembeddings%252C%2520we%2520introduce%2520a%2520new%2520distance%2520measure%2520based%2520on%2520a%2520path%2520kernel%2520over%250Aconcepts.%2520With%2520this%2520measurement%252C%2520we%2520identify%2520patterns%2520to%2520distinguish%2520ambiguous%250Afrom%2520unambiguous%2520questions.%2520Furthermore%252C%2520we%2520propose%2520a%2520method%2520for%2520improving%2520LLM%250Aperformance%2520on%2520ambiguous%2520agentic%2520tool%2520calling%2520through%2520missing%2520concept%250Aprediction.%2520Both%2520achieve%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ambiguity%20Resolution%20in%20Text-to-Structured%20Data%20Mapping&entry.906535625=Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-Young%20Paik%20and%20Liming%20Zhu&entry.1292438233=%20%20Ambiguity%20in%20natural%20language%20is%20a%20significant%20obstacle%20for%20achieving%0Aaccurate%20text%20to%20structured%20data%20mapping%20through%20large%20language%20models%20%28LLMs%29%2C%0Awhich%20affects%20the%20performance%20of%20tasks%20such%20as%20mapping%20text%20to%20agentic%20tool%0Acalling%20and%20text-to-SQL%20queries.%20Existing%20methods%20to%20ambiguity%20handling%20either%0Arely%20on%20the%20ReACT%20framework%20to%20obtain%20correct%20mappings%20through%20trial%20and%20error%2C%0Aor%20on%20supervised%20fine-tuning%20to%20bias%20models%20toward%20specific%20tasks.%20In%20this%0Apaper%2C%20we%20adopt%20a%20different%20approach%20that%20characterizes%20representation%0Adifferences%20of%20ambiguous%20text%20in%20the%20latent%20space%20and%20leverages%20these%0Adifferences%20to%20identify%20ambiguity%20before%20mapping%20them%20to%20structured%20data.%20To%0Adetect%20sentence-level%20ambiguity%2C%20we%20focus%20on%20the%20relationship%20between%20ambiguous%0Aquestions%20and%20their%20interpretations.%20Unlike%20distances%20calculated%20by%20dense%0Aembeddings%2C%20we%20introduce%20a%20new%20distance%20measure%20based%20on%20a%20path%20kernel%20over%0Aconcepts.%20With%20this%20measurement%2C%20we%20identify%20patterns%20to%20distinguish%20ambiguous%0Afrom%20unambiguous%20questions.%20Furthermore%2C%20we%20propose%20a%20method%20for%20improving%20LLM%0Aperformance%20on%20ambiguous%20agentic%20tool%20calling%20through%20missing%20concept%0Aprediction.%20Both%20achieve%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11679v2&entry.124074799=Read"},
{"title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large\n  Language Models", "author": "Yuchen Yan and Jin Jiang and Zhenbang Ren and Yijun Li and Xudong Cai and Yang Liu and Xin Xu and Mengdi Zhang and Jian Shao and Yongliang Shen and Jun Xiao and Yueting Zhuang", "abstract": "  Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.15801v3", "date": "2025-09-25", "relevancy": 2.0743, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VerifyBench%3A%20Benchmarking%20Reference-based%20Reward%20Systems%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20VerifyBench%3A%20Benchmarking%20Reference-based%20Reward%20Systems%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuchen%20Yan%20and%20Jin%20Jiang%20and%20Zhenbang%20Ren%20and%20Yijun%20Li%20and%20Xudong%20Cai%20and%20Yang%20Liu%20and%20Xin%20Xu%20and%20Mengdi%20Zhang%20and%20Jian%20Shao%20and%20Yongliang%20Shen%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20reasoning%20models%20such%20as%20OpenAI%20o1%20and%20DeepSeek-R1%20have%20achieved%0Aremarkable%20performance%20in%20the%20domain%20of%20reasoning.%20A%20key%20component%20of%20their%0Atraining%20is%20the%20incorporation%20of%20verifiable%20rewards%20within%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20existing%20reward%20benchmarks%20do%20not%20evaluate%0Areference-based%20reward%20systems%2C%20leaving%20researchers%20with%20limited%20understanding%0Aof%20the%20accuracy%20of%20verifiers%20used%20in%20RL.%20In%20this%20paper%2C%20we%20introduce%20two%0Abenchmarks%2C%20VerifyBench%20and%20VerifyBench-Hard%2C%20designed%20to%20assess%20the%0Aperformance%20of%20reference-based%20reward%20systems.%20These%20benchmarks%20are%20constructed%0Athrough%20meticulous%20data%20collection%20and%20curation%2C%20followed%20by%20careful%20human%0Aannotation%20to%20ensure%20high%20quality.%20Current%20models%20still%20show%20considerable%20room%0Afor%20improvement%20on%20both%20VerifyBench%20and%20VerifyBench-Hard%2C%20especially%0Asmaller-scale%20models.%20Furthermore%2C%20we%20conduct%20a%20thorough%20and%20comprehensive%0Aanalysis%20of%20evaluation%20results%2C%20offering%20insights%20for%20understanding%20and%0Adeveloping%20reference-based%20reward%20systems.%20Our%20proposed%20benchmarks%20serve%20as%0Aeffective%20tools%20for%20guiding%20the%20development%20of%20verifier%20accuracy%20and%20the%0Areasoning%20capabilities%20of%20models%20trained%20via%20RL%20in%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15801v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifyBench%253A%2520Benchmarking%2520Reference-based%2520Reward%2520Systems%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuchen%2520Yan%2520and%2520Jin%2520Jiang%2520and%2520Zhenbang%2520Ren%2520and%2520Yijun%2520Li%2520and%2520Xudong%2520Cai%2520and%2520Yang%2520Liu%2520and%2520Xin%2520Xu%2520and%2520Mengdi%2520Zhang%2520and%2520Jian%2520Shao%2520and%2520Yongliang%2520Shen%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520such%2520as%2520OpenAI%2520o1%2520and%2520DeepSeek-R1%2520have%2520achieved%250Aremarkable%2520performance%2520in%2520the%2520domain%2520of%2520reasoning.%2520A%2520key%2520component%2520of%2520their%250Atraining%2520is%2520the%2520incorporation%2520of%2520verifiable%2520rewards%2520within%2520reinforcement%250Alearning%2520%2528RL%2529.%2520However%252C%2520existing%2520reward%2520benchmarks%2520do%2520not%2520evaluate%250Areference-based%2520reward%2520systems%252C%2520leaving%2520researchers%2520with%2520limited%2520understanding%250Aof%2520the%2520accuracy%2520of%2520verifiers%2520used%2520in%2520RL.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520two%250Abenchmarks%252C%2520VerifyBench%2520and%2520VerifyBench-Hard%252C%2520designed%2520to%2520assess%2520the%250Aperformance%2520of%2520reference-based%2520reward%2520systems.%2520These%2520benchmarks%2520are%2520constructed%250Athrough%2520meticulous%2520data%2520collection%2520and%2520curation%252C%2520followed%2520by%2520careful%2520human%250Aannotation%2520to%2520ensure%2520high%2520quality.%2520Current%2520models%2520still%2520show%2520considerable%2520room%250Afor%2520improvement%2520on%2520both%2520VerifyBench%2520and%2520VerifyBench-Hard%252C%2520especially%250Asmaller-scale%2520models.%2520Furthermore%252C%2520we%2520conduct%2520a%2520thorough%2520and%2520comprehensive%250Aanalysis%2520of%2520evaluation%2520results%252C%2520offering%2520insights%2520for%2520understanding%2520and%250Adeveloping%2520reference-based%2520reward%2520systems.%2520Our%2520proposed%2520benchmarks%2520serve%2520as%250Aeffective%2520tools%2520for%2520guiding%2520the%2520development%2520of%2520verifier%2520accuracy%2520and%2520the%250Areasoning%2520capabilities%2520of%2520models%2520trained%2520via%2520RL%2520in%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15801v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VerifyBench%3A%20Benchmarking%20Reference-based%20Reward%20Systems%20for%20Large%0A%20%20Language%20Models&entry.906535625=Yuchen%20Yan%20and%20Jin%20Jiang%20and%20Zhenbang%20Ren%20and%20Yijun%20Li%20and%20Xudong%20Cai%20and%20Yang%20Liu%20and%20Xin%20Xu%20and%20Mengdi%20Zhang%20and%20Jian%20Shao%20and%20Yongliang%20Shen%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20reasoning%20models%20such%20as%20OpenAI%20o1%20and%20DeepSeek-R1%20have%20achieved%0Aremarkable%20performance%20in%20the%20domain%20of%20reasoning.%20A%20key%20component%20of%20their%0Atraining%20is%20the%20incorporation%20of%20verifiable%20rewards%20within%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20existing%20reward%20benchmarks%20do%20not%20evaluate%0Areference-based%20reward%20systems%2C%20leaving%20researchers%20with%20limited%20understanding%0Aof%20the%20accuracy%20of%20verifiers%20used%20in%20RL.%20In%20this%20paper%2C%20we%20introduce%20two%0Abenchmarks%2C%20VerifyBench%20and%20VerifyBench-Hard%2C%20designed%20to%20assess%20the%0Aperformance%20of%20reference-based%20reward%20systems.%20These%20benchmarks%20are%20constructed%0Athrough%20meticulous%20data%20collection%20and%20curation%2C%20followed%20by%20careful%20human%0Aannotation%20to%20ensure%20high%20quality.%20Current%20models%20still%20show%20considerable%20room%0Afor%20improvement%20on%20both%20VerifyBench%20and%20VerifyBench-Hard%2C%20especially%0Asmaller-scale%20models.%20Furthermore%2C%20we%20conduct%20a%20thorough%20and%20comprehensive%0Aanalysis%20of%20evaluation%20results%2C%20offering%20insights%20for%20understanding%20and%0Adeveloping%20reference-based%20reward%20systems.%20Our%20proposed%20benchmarks%20serve%20as%0Aeffective%20tools%20for%20guiding%20the%20development%20of%20verifier%20accuracy%20and%20the%0Areasoning%20capabilities%20of%20models%20trained%20via%20RL%20in%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15801v3&entry.124074799=Read"},
{"title": "Technical report on label-informed logit redistribution for better\n  domain generalization in low-shot classification with foundation models", "author": "Behraj Khan and Tahir Syed", "abstract": "  Confidence calibration is an emerging challenge in real-world decision\nsystems based on foundations models when used for downstream vision\nclassification tasks. Due to various reasons exposed, logit scores on the CLIP\nhead remain large irrespective of whether the image-language pairs reconcile.\nIt is difficult to address in data space, given the few-shot regime. We propose\na penalty incorporated into loss objective that penalizes incorrect\nclassifications whenever one is made during finetuning, by moving an amount of\nlog-likelihood to the true class commensurate to the relative amplitudes of the\ntwo likelihoods. We refer to it as \\textit{confidence misalignment penalty\n(CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain\ngeneralization datasets supports the calibration performance of our method\nagainst stat-of-the-art. CMP outperforms the benchmarked prompt learning\nmethods, demonstrating average improvement in Expected Calibration Error (ECE)\nby average $6.01$\\%, $4.01$ \\% at minimum and $9.72$\\% at maximum.\n", "link": "http://arxiv.org/abs/2501.17595v3", "date": "2025-09-25", "relevancy": 2.0654, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5332}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20report%20on%20label-informed%20logit%20redistribution%20for%20better%0A%20%20domain%20generalization%20in%20low-shot%20classification%20with%20foundation%20models&body=Title%3A%20Technical%20report%20on%20label-informed%20logit%20redistribution%20for%20better%0A%20%20domain%20generalization%20in%20low-shot%20classification%20with%20foundation%20models%0AAuthor%3A%20Behraj%20Khan%20and%20Tahir%20Syed%0AAbstract%3A%20%20%20Confidence%20calibration%20is%20an%20emerging%20challenge%20in%20real-world%20decision%0Asystems%20based%20on%20foundations%20models%20when%20used%20for%20downstream%20vision%0Aclassification%20tasks.%20Due%20to%20various%20reasons%20exposed%2C%20logit%20scores%20on%20the%20CLIP%0Ahead%20remain%20large%20irrespective%20of%20whether%20the%20image-language%20pairs%20reconcile.%0AIt%20is%20difficult%20to%20address%20in%20data%20space%2C%20given%20the%20few-shot%20regime.%20We%20propose%0Aa%20penalty%20incorporated%20into%20loss%20objective%20that%20penalizes%20incorrect%0Aclassifications%20whenever%20one%20is%20made%20during%20finetuning%2C%20by%20moving%20an%20amount%20of%0Alog-likelihood%20to%20the%20true%20class%20commensurate%20to%20the%20relative%20amplitudes%20of%20the%0Atwo%20likelihoods.%20We%20refer%20to%20it%20as%20%5Ctextit%7Bconfidence%20misalignment%20penalty%0A%28CMP%29%7D.%20Extensive%20experiments%20on%20%2412%24%20vision%20datasets%20and%20%245%24%20domain%0Ageneralization%20datasets%20supports%20the%20calibration%20performance%20of%20our%20method%0Aagainst%20stat-of-the-art.%20CMP%20outperforms%20the%20benchmarked%20prompt%20learning%0Amethods%2C%20demonstrating%20average%20improvement%20in%20Expected%20Calibration%20Error%20%28ECE%29%0Aby%20average%20%246.01%24%5C%25%2C%20%244.01%24%20%5C%25%20at%20minimum%20and%20%249.72%24%5C%25%20at%20maximum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17595v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520report%2520on%2520label-informed%2520logit%2520redistribution%2520for%2520better%250A%2520%2520domain%2520generalization%2520in%2520low-shot%2520classification%2520with%2520foundation%2520models%26entry.906535625%3DBehraj%2520Khan%2520and%2520Tahir%2520Syed%26entry.1292438233%3D%2520%2520Confidence%2520calibration%2520is%2520an%2520emerging%2520challenge%2520in%2520real-world%2520decision%250Asystems%2520based%2520on%2520foundations%2520models%2520when%2520used%2520for%2520downstream%2520vision%250Aclassification%2520tasks.%2520Due%2520to%2520various%2520reasons%2520exposed%252C%2520logit%2520scores%2520on%2520the%2520CLIP%250Ahead%2520remain%2520large%2520irrespective%2520of%2520whether%2520the%2520image-language%2520pairs%2520reconcile.%250AIt%2520is%2520difficult%2520to%2520address%2520in%2520data%2520space%252C%2520given%2520the%2520few-shot%2520regime.%2520We%2520propose%250Aa%2520penalty%2520incorporated%2520into%2520loss%2520objective%2520that%2520penalizes%2520incorrect%250Aclassifications%2520whenever%2520one%2520is%2520made%2520during%2520finetuning%252C%2520by%2520moving%2520an%2520amount%2520of%250Alog-likelihood%2520to%2520the%2520true%2520class%2520commensurate%2520to%2520the%2520relative%2520amplitudes%2520of%2520the%250Atwo%2520likelihoods.%2520We%2520refer%2520to%2520it%2520as%2520%255Ctextit%257Bconfidence%2520misalignment%2520penalty%250A%2528CMP%2529%257D.%2520Extensive%2520experiments%2520on%2520%252412%2524%2520vision%2520datasets%2520and%2520%25245%2524%2520domain%250Ageneralization%2520datasets%2520supports%2520the%2520calibration%2520performance%2520of%2520our%2520method%250Aagainst%2520stat-of-the-art.%2520CMP%2520outperforms%2520the%2520benchmarked%2520prompt%2520learning%250Amethods%252C%2520demonstrating%2520average%2520improvement%2520in%2520Expected%2520Calibration%2520Error%2520%2528ECE%2529%250Aby%2520average%2520%25246.01%2524%255C%2525%252C%2520%25244.01%2524%2520%255C%2525%2520at%2520minimum%2520and%2520%25249.72%2524%255C%2525%2520at%2520maximum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17595v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20report%20on%20label-informed%20logit%20redistribution%20for%20better%0A%20%20domain%20generalization%20in%20low-shot%20classification%20with%20foundation%20models&entry.906535625=Behraj%20Khan%20and%20Tahir%20Syed&entry.1292438233=%20%20Confidence%20calibration%20is%20an%20emerging%20challenge%20in%20real-world%20decision%0Asystems%20based%20on%20foundations%20models%20when%20used%20for%20downstream%20vision%0Aclassification%20tasks.%20Due%20to%20various%20reasons%20exposed%2C%20logit%20scores%20on%20the%20CLIP%0Ahead%20remain%20large%20irrespective%20of%20whether%20the%20image-language%20pairs%20reconcile.%0AIt%20is%20difficult%20to%20address%20in%20data%20space%2C%20given%20the%20few-shot%20regime.%20We%20propose%0Aa%20penalty%20incorporated%20into%20loss%20objective%20that%20penalizes%20incorrect%0Aclassifications%20whenever%20one%20is%20made%20during%20finetuning%2C%20by%20moving%20an%20amount%20of%0Alog-likelihood%20to%20the%20true%20class%20commensurate%20to%20the%20relative%20amplitudes%20of%20the%0Atwo%20likelihoods.%20We%20refer%20to%20it%20as%20%5Ctextit%7Bconfidence%20misalignment%20penalty%0A%28CMP%29%7D.%20Extensive%20experiments%20on%20%2412%24%20vision%20datasets%20and%20%245%24%20domain%0Ageneralization%20datasets%20supports%20the%20calibration%20performance%20of%20our%20method%0Aagainst%20stat-of-the-art.%20CMP%20outperforms%20the%20benchmarked%20prompt%20learning%0Amethods%2C%20demonstrating%20average%20improvement%20in%20Expected%20Calibration%20Error%20%28ECE%29%0Aby%20average%20%246.01%24%5C%25%2C%20%244.01%24%20%5C%25%20at%20minimum%20and%20%249.72%24%5C%25%20at%20maximum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17595v3&entry.124074799=Read"},
{"title": "The Unwinnable Arms Race of AI Image Detection", "author": "Till Aczel and Lorenzo Vettor and Andreas Plesner and Roger Wattenhofer", "abstract": "  The rapid progress of image generative AI has blurred the boundary between\nsynthetic and real images, fueling an arms race between generators and\ndiscriminators. This paper investigates the conditions under which\ndiscriminators are most disadvantaged in this competition. We analyze two key\nfactors: data dimensionality and data complexity. While increased\ndimensionality often strengthens the discriminators ability to detect subtle\ninconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov\ncomplexity as a measure of intrinsic dataset structure, we show that both very\nsimple and highly complex datasets reduce the detectability of synthetic\nimages; generators can learn simple datasets almost perfectly, whereas extreme\ndiversity masks imperfections. In contrast, intermediate-complexity datasets\ncreate the most favorable conditions for detection, as generators fail to fully\ncapture the distribution and their errors remain visible.\n", "link": "http://arxiv.org/abs/2509.21135v1", "date": "2025-09-25", "relevancy": 2.0629, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5179}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Unwinnable%20Arms%20Race%20of%20AI%20Image%20Detection&body=Title%3A%20The%20Unwinnable%20Arms%20Race%20of%20AI%20Image%20Detection%0AAuthor%3A%20Till%20Aczel%20and%20Lorenzo%20Vettor%20and%20Andreas%20Plesner%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20image%20generative%20AI%20has%20blurred%20the%20boundary%20between%0Asynthetic%20and%20real%20images%2C%20fueling%20an%20arms%20race%20between%20generators%20and%0Adiscriminators.%20This%20paper%20investigates%20the%20conditions%20under%20which%0Adiscriminators%20are%20most%20disadvantaged%20in%20this%20competition.%20We%20analyze%20two%20key%0Afactors%3A%20data%20dimensionality%20and%20data%20complexity.%20While%20increased%0Adimensionality%20often%20strengthens%20the%20discriminators%20ability%20to%20detect%20subtle%0Ainconsistencies%2C%20complexity%20introduces%20a%20more%20nuanced%20effect.%20Using%20Kolmogorov%0Acomplexity%20as%20a%20measure%20of%20intrinsic%20dataset%20structure%2C%20we%20show%20that%20both%20very%0Asimple%20and%20highly%20complex%20datasets%20reduce%20the%20detectability%20of%20synthetic%0Aimages%3B%20generators%20can%20learn%20simple%20datasets%20almost%20perfectly%2C%20whereas%20extreme%0Adiversity%20masks%20imperfections.%20In%20contrast%2C%20intermediate-complexity%20datasets%0Acreate%20the%20most%20favorable%20conditions%20for%20detection%2C%20as%20generators%20fail%20to%20fully%0Acapture%20the%20distribution%20and%20their%20errors%20remain%20visible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Unwinnable%2520Arms%2520Race%2520of%2520AI%2520Image%2520Detection%26entry.906535625%3DTill%2520Aczel%2520and%2520Lorenzo%2520Vettor%2520and%2520Andreas%2520Plesner%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520image%2520generative%2520AI%2520has%2520blurred%2520the%2520boundary%2520between%250Asynthetic%2520and%2520real%2520images%252C%2520fueling%2520an%2520arms%2520race%2520between%2520generators%2520and%250Adiscriminators.%2520This%2520paper%2520investigates%2520the%2520conditions%2520under%2520which%250Adiscriminators%2520are%2520most%2520disadvantaged%2520in%2520this%2520competition.%2520We%2520analyze%2520two%2520key%250Afactors%253A%2520data%2520dimensionality%2520and%2520data%2520complexity.%2520While%2520increased%250Adimensionality%2520often%2520strengthens%2520the%2520discriminators%2520ability%2520to%2520detect%2520subtle%250Ainconsistencies%252C%2520complexity%2520introduces%2520a%2520more%2520nuanced%2520effect.%2520Using%2520Kolmogorov%250Acomplexity%2520as%2520a%2520measure%2520of%2520intrinsic%2520dataset%2520structure%252C%2520we%2520show%2520that%2520both%2520very%250Asimple%2520and%2520highly%2520complex%2520datasets%2520reduce%2520the%2520detectability%2520of%2520synthetic%250Aimages%253B%2520generators%2520can%2520learn%2520simple%2520datasets%2520almost%2520perfectly%252C%2520whereas%2520extreme%250Adiversity%2520masks%2520imperfections.%2520In%2520contrast%252C%2520intermediate-complexity%2520datasets%250Acreate%2520the%2520most%2520favorable%2520conditions%2520for%2520detection%252C%2520as%2520generators%2520fail%2520to%2520fully%250Acapture%2520the%2520distribution%2520and%2520their%2520errors%2520remain%2520visible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unwinnable%20Arms%20Race%20of%20AI%20Image%20Detection&entry.906535625=Till%20Aczel%20and%20Lorenzo%20Vettor%20and%20Andreas%20Plesner%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20The%20rapid%20progress%20of%20image%20generative%20AI%20has%20blurred%20the%20boundary%20between%0Asynthetic%20and%20real%20images%2C%20fueling%20an%20arms%20race%20between%20generators%20and%0Adiscriminators.%20This%20paper%20investigates%20the%20conditions%20under%20which%0Adiscriminators%20are%20most%20disadvantaged%20in%20this%20competition.%20We%20analyze%20two%20key%0Afactors%3A%20data%20dimensionality%20and%20data%20complexity.%20While%20increased%0Adimensionality%20often%20strengthens%20the%20discriminators%20ability%20to%20detect%20subtle%0Ainconsistencies%2C%20complexity%20introduces%20a%20more%20nuanced%20effect.%20Using%20Kolmogorov%0Acomplexity%20as%20a%20measure%20of%20intrinsic%20dataset%20structure%2C%20we%20show%20that%20both%20very%0Asimple%20and%20highly%20complex%20datasets%20reduce%20the%20detectability%20of%20synthetic%0Aimages%3B%20generators%20can%20learn%20simple%20datasets%20almost%20perfectly%2C%20whereas%20extreme%0Adiversity%20masks%20imperfections.%20In%20contrast%2C%20intermediate-complexity%20datasets%0Acreate%20the%20most%20favorable%20conditions%20for%20detection%2C%20as%20generators%20fail%20to%20fully%0Acapture%20the%20distribution%20and%20their%20errors%20remain%20visible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21135v1&entry.124074799=Read"},
{"title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image\n  Augmentation", "author": "Hugo Carlesso and Maria Eliza Patulea and Moncef Garouani and Radu Tudor Ionescu and Josiane Mothe", "abstract": "  Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.\n", "link": "http://arxiv.org/abs/2507.15577v2", "date": "2025-09-25", "relevancy": 2.0621, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5463}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5113}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation&body=Title%3A%20GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation%0AAuthor%3A%20Hugo%20Carlesso%20and%20Maria%20Eliza%20Patulea%20and%20Moncef%20Garouani%20and%20Radu%20Tudor%20Ionescu%20and%20Josiane%20Mothe%0AAbstract%3A%20%20%20Mixup%20has%20become%20a%20popular%20augmentation%20strategy%20for%20image%20classification%2C%0Ayet%20its%20naive%20pixel-wise%20interpolation%20often%20produces%20unrealistic%20images%20that%0Acan%20hinder%20learning%2C%20particularly%20in%20high-stakes%20medical%20applications.%20We%0Apropose%20GeMix%2C%20a%20two-stage%20framework%20that%20replaces%20heuristic%20blending%20with%20a%0Alearned%2C%20label-aware%20interpolation%20powered%20by%20class-conditional%20GANs.%20First%2C%20a%0AStyleGAN2-ADA%20generator%20is%20trained%20on%20the%20target%20dataset.%20During%20augmentation%2C%0Awe%20sample%20two%20label%20vectors%20from%20Dirichlet%20priors%20biased%20toward%20different%0Aclasses%20and%20blend%20them%20via%20a%20Beta-distributed%20coefficient.%20Then%2C%20we%20condition%0Athe%20generator%20on%20this%20soft%20label%20to%20synthesize%20visually%20coherent%20images%20that%0Alie%20along%20a%20continuous%20class%20manifold.%20We%20benchmark%20GeMix%20on%20the%20large-scale%0ACOVIDx-CT-3%20dataset%20using%20three%20backbones%20%28ResNet-50%2C%20ResNet-101%2C%0AEfficientNet-B0%29.%20When%20combined%20with%20real%20data%2C%20our%20method%20increases%20macro-F1%0Aover%20traditional%20mixup%20for%20all%20backbones%2C%20reducing%20the%20false%20negative%20rate%20for%0ACOVID-19%20detection.%20GeMix%20is%20thus%20a%20drop-in%20replacement%20for%20pixel-space%20mixup%2C%0Adelivering%20stronger%20regularization%20and%20greater%20semantic%20fidelity%2C%20without%0Adisrupting%20existing%20training%20pipelines.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/hugocarlesso/GeMix%20to%20foster%20reproducibility%20and%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeMix%253A%2520Conditional%2520GAN-Based%2520Mixup%2520for%2520Improved%2520Medical%2520Image%250A%2520%2520Augmentation%26entry.906535625%3DHugo%2520Carlesso%2520and%2520Maria%2520Eliza%2520Patulea%2520and%2520Moncef%2520Garouani%2520and%2520Radu%2520Tudor%2520Ionescu%2520and%2520Josiane%2520Mothe%26entry.1292438233%3D%2520%2520Mixup%2520has%2520become%2520a%2520popular%2520augmentation%2520strategy%2520for%2520image%2520classification%252C%250Ayet%2520its%2520naive%2520pixel-wise%2520interpolation%2520often%2520produces%2520unrealistic%2520images%2520that%250Acan%2520hinder%2520learning%252C%2520particularly%2520in%2520high-stakes%2520medical%2520applications.%2520We%250Apropose%2520GeMix%252C%2520a%2520two-stage%2520framework%2520that%2520replaces%2520heuristic%2520blending%2520with%2520a%250Alearned%252C%2520label-aware%2520interpolation%2520powered%2520by%2520class-conditional%2520GANs.%2520First%252C%2520a%250AStyleGAN2-ADA%2520generator%2520is%2520trained%2520on%2520the%2520target%2520dataset.%2520During%2520augmentation%252C%250Awe%2520sample%2520two%2520label%2520vectors%2520from%2520Dirichlet%2520priors%2520biased%2520toward%2520different%250Aclasses%2520and%2520blend%2520them%2520via%2520a%2520Beta-distributed%2520coefficient.%2520Then%252C%2520we%2520condition%250Athe%2520generator%2520on%2520this%2520soft%2520label%2520to%2520synthesize%2520visually%2520coherent%2520images%2520that%250Alie%2520along%2520a%2520continuous%2520class%2520manifold.%2520We%2520benchmark%2520GeMix%2520on%2520the%2520large-scale%250ACOVIDx-CT-3%2520dataset%2520using%2520three%2520backbones%2520%2528ResNet-50%252C%2520ResNet-101%252C%250AEfficientNet-B0%2529.%2520When%2520combined%2520with%2520real%2520data%252C%2520our%2520method%2520increases%2520macro-F1%250Aover%2520traditional%2520mixup%2520for%2520all%2520backbones%252C%2520reducing%2520the%2520false%2520negative%2520rate%2520for%250ACOVID-19%2520detection.%2520GeMix%2520is%2520thus%2520a%2520drop-in%2520replacement%2520for%2520pixel-space%2520mixup%252C%250Adelivering%2520stronger%2520regularization%2520and%2520greater%2520semantic%2520fidelity%252C%2520without%250Adisrupting%2520existing%2520training%2520pipelines.%2520We%2520publicly%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/hugocarlesso/GeMix%2520to%2520foster%2520reproducibility%2520and%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation&entry.906535625=Hugo%20Carlesso%20and%20Maria%20Eliza%20Patulea%20and%20Moncef%20Garouani%20and%20Radu%20Tudor%20Ionescu%20and%20Josiane%20Mothe&entry.1292438233=%20%20Mixup%20has%20become%20a%20popular%20augmentation%20strategy%20for%20image%20classification%2C%0Ayet%20its%20naive%20pixel-wise%20interpolation%20often%20produces%20unrealistic%20images%20that%0Acan%20hinder%20learning%2C%20particularly%20in%20high-stakes%20medical%20applications.%20We%0Apropose%20GeMix%2C%20a%20two-stage%20framework%20that%20replaces%20heuristic%20blending%20with%20a%0Alearned%2C%20label-aware%20interpolation%20powered%20by%20class-conditional%20GANs.%20First%2C%20a%0AStyleGAN2-ADA%20generator%20is%20trained%20on%20the%20target%20dataset.%20During%20augmentation%2C%0Awe%20sample%20two%20label%20vectors%20from%20Dirichlet%20priors%20biased%20toward%20different%0Aclasses%20and%20blend%20them%20via%20a%20Beta-distributed%20coefficient.%20Then%2C%20we%20condition%0Athe%20generator%20on%20this%20soft%20label%20to%20synthesize%20visually%20coherent%20images%20that%0Alie%20along%20a%20continuous%20class%20manifold.%20We%20benchmark%20GeMix%20on%20the%20large-scale%0ACOVIDx-CT-3%20dataset%20using%20three%20backbones%20%28ResNet-50%2C%20ResNet-101%2C%0AEfficientNet-B0%29.%20When%20combined%20with%20real%20data%2C%20our%20method%20increases%20macro-F1%0Aover%20traditional%20mixup%20for%20all%20backbones%2C%20reducing%20the%20false%20negative%20rate%20for%0ACOVID-19%20detection.%20GeMix%20is%20thus%20a%20drop-in%20replacement%20for%20pixel-space%20mixup%2C%0Adelivering%20stronger%20regularization%20and%20greater%20semantic%20fidelity%2C%20without%0Adisrupting%20existing%20training%20pipelines.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/hugocarlesso/GeMix%20to%20foster%20reproducibility%20and%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15577v2&entry.124074799=Read"},
{"title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models", "author": "Zhiteng Li and Mingyuan Xia and Jingyuan Zhang and Zheng Hui and Haotong Qin and Linghe Kong and Yulun Zhang and Xiaokang Yang", "abstract": "  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD.\n", "link": "http://arxiv.org/abs/2502.01403v4", "date": "2025-09-25", "relevancy": 2.0565, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaSVD%3A%20Adaptive%20Singular%20Value%20Decomposition%20for%20Large%20Language%20Models&body=Title%3A%20AdaSVD%3A%20Adaptive%20Singular%20Value%20Decomposition%20for%20Large%20Language%20Models%0AAuthor%3A%20Zhiteng%20Li%20and%20Mingyuan%20Xia%20and%20Jingyuan%20Zhang%20and%20Zheng%20Hui%20and%20Haotong%20Qin%20and%20Linghe%20Kong%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29%20tasks%2C%20yet%20their%20substantial%20memory%20requirements%0Apresent%20significant%20challenges%20for%20deployment%20on%20resource-constrained%20devices.%0ASingular%20Value%20Decomposition%20%28SVD%29%20has%20emerged%20as%20a%20promising%20compression%0Atechnique%20for%20LLMs%2C%20offering%20considerable%20reductions%20in%20memory%20overhead.%0AHowever%2C%20existing%20SVD-based%20methods%20often%20struggle%20to%20effectively%20mitigate%20the%0Aerrors%20introduced%20by%20SVD%20truncation%2C%20leading%20to%20a%20noticeable%20performance%20gap%0Awhen%20compared%20to%20the%20original%20models.%20Furthermore%2C%20applying%20a%20uniform%0Acompression%20ratio%20across%20all%20transformer%20layers%20fails%20to%20account%20for%20the%0Avarying%20importance%20of%20different%20layers.%20To%20address%20these%20challenges%2C%20we%20propose%0AAdaSVD%2C%20an%20adaptive%20SVD-based%20LLM%20compression%20approach.%20Specifically%2C%20AdaSVD%0Aintroduces%20adaComp%2C%20which%20adaptively%20compensates%20for%20SVD%20truncation%20errors%20by%0Aalternately%20updating%20the%20singular%20matrices%20%24%5Cmathcal%7BU%7D%24%20and%0A%24%5Cmathcal%7BV%7D%5E%5Ctop%24.%20Additionally%2C%20AdaSVD%20introduces%20adaCR%2C%20which%20adaptively%0Aassigns%20layer-specific%20compression%20ratios%20based%20on%20the%20relative%20importance%20of%0Aeach%20layer.%20Extensive%20experiments%20across%20multiple%20LLM/VLM%20families%20and%0Aevaluation%20metrics%20demonstrate%20that%20AdaSVD%20consistently%20outperforms%0Astate-of-the-art%20%28SOTA%29%20SVD-based%20methods%2C%20achieving%20superior%20performance%20with%0Asignificantly%20reduced%20memory%20requirements.%20Code%20and%20models%20of%20AdaSVD%20will%20be%0Aavailable%20at%20https%3A//github.com/ZHITENGLI/AdaSVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01403v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaSVD%253A%2520Adaptive%2520Singular%2520Value%2520Decomposition%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DZhiteng%2520Li%2520and%2520Mingyuan%2520Xia%2520and%2520Jingyuan%2520Zhang%2520and%2520Zheng%2520Hui%2520and%2520Haotong%2520Qin%2520and%2520Linghe%2520Kong%2520and%2520Yulun%2520Zhang%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520tasks%252C%2520yet%2520their%2520substantial%2520memory%2520requirements%250Apresent%2520significant%2520challenges%2520for%2520deployment%2520on%2520resource-constrained%2520devices.%250ASingular%2520Value%2520Decomposition%2520%2528SVD%2529%2520has%2520emerged%2520as%2520a%2520promising%2520compression%250Atechnique%2520for%2520LLMs%252C%2520offering%2520considerable%2520reductions%2520in%2520memory%2520overhead.%250AHowever%252C%2520existing%2520SVD-based%2520methods%2520often%2520struggle%2520to%2520effectively%2520mitigate%2520the%250Aerrors%2520introduced%2520by%2520SVD%2520truncation%252C%2520leading%2520to%2520a%2520noticeable%2520performance%2520gap%250Awhen%2520compared%2520to%2520the%2520original%2520models.%2520Furthermore%252C%2520applying%2520a%2520uniform%250Acompression%2520ratio%2520across%2520all%2520transformer%2520layers%2520fails%2520to%2520account%2520for%2520the%250Avarying%2520importance%2520of%2520different%2520layers.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AAdaSVD%252C%2520an%2520adaptive%2520SVD-based%2520LLM%2520compression%2520approach.%2520Specifically%252C%2520AdaSVD%250Aintroduces%2520adaComp%252C%2520which%2520adaptively%2520compensates%2520for%2520SVD%2520truncation%2520errors%2520by%250Aalternately%2520updating%2520the%2520singular%2520matrices%2520%2524%255Cmathcal%257BU%257D%2524%2520and%250A%2524%255Cmathcal%257BV%257D%255E%255Ctop%2524.%2520Additionally%252C%2520AdaSVD%2520introduces%2520adaCR%252C%2520which%2520adaptively%250Aassigns%2520layer-specific%2520compression%2520ratios%2520based%2520on%2520the%2520relative%2520importance%2520of%250Aeach%2520layer.%2520Extensive%2520experiments%2520across%2520multiple%2520LLM/VLM%2520families%2520and%250Aevaluation%2520metrics%2520demonstrate%2520that%2520AdaSVD%2520consistently%2520outperforms%250Astate-of-the-art%2520%2528SOTA%2529%2520SVD-based%2520methods%252C%2520achieving%2520superior%2520performance%2520with%250Asignificantly%2520reduced%2520memory%2520requirements.%2520Code%2520and%2520models%2520of%2520AdaSVD%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/ZHITENGLI/AdaSVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01403v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaSVD%3A%20Adaptive%20Singular%20Value%20Decomposition%20for%20Large%20Language%20Models&entry.906535625=Zhiteng%20Li%20and%20Mingyuan%20Xia%20and%20Jingyuan%20Zhang%20and%20Zheng%20Hui%20and%20Haotong%20Qin%20and%20Linghe%20Kong%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29%20tasks%2C%20yet%20their%20substantial%20memory%20requirements%0Apresent%20significant%20challenges%20for%20deployment%20on%20resource-constrained%20devices.%0ASingular%20Value%20Decomposition%20%28SVD%29%20has%20emerged%20as%20a%20promising%20compression%0Atechnique%20for%20LLMs%2C%20offering%20considerable%20reductions%20in%20memory%20overhead.%0AHowever%2C%20existing%20SVD-based%20methods%20often%20struggle%20to%20effectively%20mitigate%20the%0Aerrors%20introduced%20by%20SVD%20truncation%2C%20leading%20to%20a%20noticeable%20performance%20gap%0Awhen%20compared%20to%20the%20original%20models.%20Furthermore%2C%20applying%20a%20uniform%0Acompression%20ratio%20across%20all%20transformer%20layers%20fails%20to%20account%20for%20the%0Avarying%20importance%20of%20different%20layers.%20To%20address%20these%20challenges%2C%20we%20propose%0AAdaSVD%2C%20an%20adaptive%20SVD-based%20LLM%20compression%20approach.%20Specifically%2C%20AdaSVD%0Aintroduces%20adaComp%2C%20which%20adaptively%20compensates%20for%20SVD%20truncation%20errors%20by%0Aalternately%20updating%20the%20singular%20matrices%20%24%5Cmathcal%7BU%7D%24%20and%0A%24%5Cmathcal%7BV%7D%5E%5Ctop%24.%20Additionally%2C%20AdaSVD%20introduces%20adaCR%2C%20which%20adaptively%0Aassigns%20layer-specific%20compression%20ratios%20based%20on%20the%20relative%20importance%20of%0Aeach%20layer.%20Extensive%20experiments%20across%20multiple%20LLM/VLM%20families%20and%0Aevaluation%20metrics%20demonstrate%20that%20AdaSVD%20consistently%20outperforms%0Astate-of-the-art%20%28SOTA%29%20SVD-based%20methods%2C%20achieving%20superior%20performance%20with%0Asignificantly%20reduced%20memory%20requirements.%20Code%20and%20models%20of%20AdaSVD%20will%20be%0Aavailable%20at%20https%3A//github.com/ZHITENGLI/AdaSVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01403v4&entry.124074799=Read"},
{"title": "Understanding Optimization in Deep Learning with Central Flows", "author": "Jeremy M. Cohen and Alex Damian and Ameet Talwalkar and J. Zico Kolter and Jason D. Lee", "abstract": "  Traditional theories of optimization cannot describe the dynamics of\noptimization in deep learning, even in the simple setting of deterministic\ntraining. The challenge is that optimizers typically operate in a complex,\noscillatory regime called the \"edge of stability.\" In this paper, we develop\ntheory that can describe the dynamics of optimization in this regime. Our key\ninsight is that while the *exact* trajectory of an oscillatory optimizer may be\nchallenging to analyze, the *time-averaged* (i.e. smoothed) trajectory is often\nmuch more tractable. To analyze an optimizer, we derive a differential equation\ncalled a \"central flow\" that characterizes this time-averaged trajectory. We\nempirically show that these central flows can predict long-term optimization\ntrajectories for generic neural networks with a high degree of numerical\naccuracy. By interpreting these central flows, we are able to understand how\ngradient descent makes progress even as the loss sometimes goes up; how\nadaptive optimizers \"adapt\" to the local loss landscape; and how adaptive\noptimizers implicitly navigate towards regions where they can take larger\nsteps. Our results suggest that central flows can be a valuable theoretical\ntool for reasoning about optimization in deep learning.\n", "link": "http://arxiv.org/abs/2410.24206v2", "date": "2025-09-25", "relevancy": 2.0527, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5458}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Optimization%20in%20Deep%20Learning%20with%20Central%20Flows&body=Title%3A%20Understanding%20Optimization%20in%20Deep%20Learning%20with%20Central%20Flows%0AAuthor%3A%20Jeremy%20M.%20Cohen%20and%20Alex%20Damian%20and%20Ameet%20Talwalkar%20and%20J.%20Zico%20Kolter%20and%20Jason%20D.%20Lee%0AAbstract%3A%20%20%20Traditional%20theories%20of%20optimization%20cannot%20describe%20the%20dynamics%20of%0Aoptimization%20in%20deep%20learning%2C%20even%20in%20the%20simple%20setting%20of%20deterministic%0Atraining.%20The%20challenge%20is%20that%20optimizers%20typically%20operate%20in%20a%20complex%2C%0Aoscillatory%20regime%20called%20the%20%22edge%20of%20stability.%22%20In%20this%20paper%2C%20we%20develop%0Atheory%20that%20can%20describe%20the%20dynamics%20of%20optimization%20in%20this%20regime.%20Our%20key%0Ainsight%20is%20that%20while%20the%20%2Aexact%2A%20trajectory%20of%20an%20oscillatory%20optimizer%20may%20be%0Achallenging%20to%20analyze%2C%20the%20%2Atime-averaged%2A%20%28i.e.%20smoothed%29%20trajectory%20is%20often%0Amuch%20more%20tractable.%20To%20analyze%20an%20optimizer%2C%20we%20derive%20a%20differential%20equation%0Acalled%20a%20%22central%20flow%22%20that%20characterizes%20this%20time-averaged%20trajectory.%20We%0Aempirically%20show%20that%20these%20central%20flows%20can%20predict%20long-term%20optimization%0Atrajectories%20for%20generic%20neural%20networks%20with%20a%20high%20degree%20of%20numerical%0Aaccuracy.%20By%20interpreting%20these%20central%20flows%2C%20we%20are%20able%20to%20understand%20how%0Agradient%20descent%20makes%20progress%20even%20as%20the%20loss%20sometimes%20goes%20up%3B%20how%0Aadaptive%20optimizers%20%22adapt%22%20to%20the%20local%20loss%20landscape%3B%20and%20how%20adaptive%0Aoptimizers%20implicitly%20navigate%20towards%20regions%20where%20they%20can%20take%20larger%0Asteps.%20Our%20results%20suggest%20that%20central%20flows%20can%20be%20a%20valuable%20theoretical%0Atool%20for%20reasoning%20about%20optimization%20in%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Optimization%2520in%2520Deep%2520Learning%2520with%2520Central%2520Flows%26entry.906535625%3DJeremy%2520M.%2520Cohen%2520and%2520Alex%2520Damian%2520and%2520Ameet%2520Talwalkar%2520and%2520J.%2520Zico%2520Kolter%2520and%2520Jason%2520D.%2520Lee%26entry.1292438233%3D%2520%2520Traditional%2520theories%2520of%2520optimization%2520cannot%2520describe%2520the%2520dynamics%2520of%250Aoptimization%2520in%2520deep%2520learning%252C%2520even%2520in%2520the%2520simple%2520setting%2520of%2520deterministic%250Atraining.%2520The%2520challenge%2520is%2520that%2520optimizers%2520typically%2520operate%2520in%2520a%2520complex%252C%250Aoscillatory%2520regime%2520called%2520the%2520%2522edge%2520of%2520stability.%2522%2520In%2520this%2520paper%252C%2520we%2520develop%250Atheory%2520that%2520can%2520describe%2520the%2520dynamics%2520of%2520optimization%2520in%2520this%2520regime.%2520Our%2520key%250Ainsight%2520is%2520that%2520while%2520the%2520%252Aexact%252A%2520trajectory%2520of%2520an%2520oscillatory%2520optimizer%2520may%2520be%250Achallenging%2520to%2520analyze%252C%2520the%2520%252Atime-averaged%252A%2520%2528i.e.%2520smoothed%2529%2520trajectory%2520is%2520often%250Amuch%2520more%2520tractable.%2520To%2520analyze%2520an%2520optimizer%252C%2520we%2520derive%2520a%2520differential%2520equation%250Acalled%2520a%2520%2522central%2520flow%2522%2520that%2520characterizes%2520this%2520time-averaged%2520trajectory.%2520We%250Aempirically%2520show%2520that%2520these%2520central%2520flows%2520can%2520predict%2520long-term%2520optimization%250Atrajectories%2520for%2520generic%2520neural%2520networks%2520with%2520a%2520high%2520degree%2520of%2520numerical%250Aaccuracy.%2520By%2520interpreting%2520these%2520central%2520flows%252C%2520we%2520are%2520able%2520to%2520understand%2520how%250Agradient%2520descent%2520makes%2520progress%2520even%2520as%2520the%2520loss%2520sometimes%2520goes%2520up%253B%2520how%250Aadaptive%2520optimizers%2520%2522adapt%2522%2520to%2520the%2520local%2520loss%2520landscape%253B%2520and%2520how%2520adaptive%250Aoptimizers%2520implicitly%2520navigate%2520towards%2520regions%2520where%2520they%2520can%2520take%2520larger%250Asteps.%2520Our%2520results%2520suggest%2520that%2520central%2520flows%2520can%2520be%2520a%2520valuable%2520theoretical%250Atool%2520for%2520reasoning%2520about%2520optimization%2520in%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Optimization%20in%20Deep%20Learning%20with%20Central%20Flows&entry.906535625=Jeremy%20M.%20Cohen%20and%20Alex%20Damian%20and%20Ameet%20Talwalkar%20and%20J.%20Zico%20Kolter%20and%20Jason%20D.%20Lee&entry.1292438233=%20%20Traditional%20theories%20of%20optimization%20cannot%20describe%20the%20dynamics%20of%0Aoptimization%20in%20deep%20learning%2C%20even%20in%20the%20simple%20setting%20of%20deterministic%0Atraining.%20The%20challenge%20is%20that%20optimizers%20typically%20operate%20in%20a%20complex%2C%0Aoscillatory%20regime%20called%20the%20%22edge%20of%20stability.%22%20In%20this%20paper%2C%20we%20develop%0Atheory%20that%20can%20describe%20the%20dynamics%20of%20optimization%20in%20this%20regime.%20Our%20key%0Ainsight%20is%20that%20while%20the%20%2Aexact%2A%20trajectory%20of%20an%20oscillatory%20optimizer%20may%20be%0Achallenging%20to%20analyze%2C%20the%20%2Atime-averaged%2A%20%28i.e.%20smoothed%29%20trajectory%20is%20often%0Amuch%20more%20tractable.%20To%20analyze%20an%20optimizer%2C%20we%20derive%20a%20differential%20equation%0Acalled%20a%20%22central%20flow%22%20that%20characterizes%20this%20time-averaged%20trajectory.%20We%0Aempirically%20show%20that%20these%20central%20flows%20can%20predict%20long-term%20optimization%0Atrajectories%20for%20generic%20neural%20networks%20with%20a%20high%20degree%20of%20numerical%0Aaccuracy.%20By%20interpreting%20these%20central%20flows%2C%20we%20are%20able%20to%20understand%20how%0Agradient%20descent%20makes%20progress%20even%20as%20the%20loss%20sometimes%20goes%20up%3B%20how%0Aadaptive%20optimizers%20%22adapt%22%20to%20the%20local%20loss%20landscape%3B%20and%20how%20adaptive%0Aoptimizers%20implicitly%20navigate%20towards%20regions%20where%20they%20can%20take%20larger%0Asteps.%20Our%20results%20suggest%20that%20central%20flows%20can%20be%20a%20valuable%20theoretical%0Atool%20for%20reasoning%20about%20optimization%20in%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24206v2&entry.124074799=Read"},
{"title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven\n  Framework", "author": "Yucheng Wang and Ziyang Chen and Md Faisal Kabir", "abstract": "  The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI.\n", "link": "http://arxiv.org/abs/2509.21241v1", "date": "2025-09-25", "relevancy": 2.0515, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Fine%20Tuned%20LLMs%20via%20Counterfactuals%20A%20Knowledge%20Graph%20Driven%0A%20%20Framework&body=Title%3A%20Explaining%20Fine%20Tuned%20LLMs%20via%20Counterfactuals%20A%20Knowledge%20Graph%20Driven%0A%20%20Framework%0AAuthor%3A%20Yucheng%20Wang%20and%20Ziyang%20Chen%20and%20Md%20Faisal%20Kabir%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20enabled%20large%0Alanguage%20models%20%28LLMs%29%20to%20acquire%20domain-specific%20knowledge%20with%20remarkable%0Aefficiency.%20However%2C%20understanding%20how%20such%20a%20fine-tuning%20mechanism%20alters%20a%0Amodel%27s%20structural%20reasoning%20and%20semantic%20behavior%20remains%20an%20open%20challenge.%0AThis%20work%20introduces%20a%20novel%20framework%20that%20explains%20fine-tuned%20LLMs%20via%0Acounterfactuals%20grounded%20in%20knowledge%20graphs.%20Specifically%2C%20we%20construct%0ABioToolKG%2C%20a%20domain-specific%20heterogeneous%20knowledge%20graph%20in%20bioinformatics%0Atools%20and%20design%20a%20counterfactual-based%20fine-tuned%20LLMs%20explainer%0A%28CFFTLLMExplainer%29%20that%20learns%20soft%20masks%20over%20graph%20nodes%20and%20edges%20to%0Agenerate%20minimal%20structural%20perturbations%20that%20induce%20maximum%20semantic%0Adivergence.%20Our%20method%20jointly%20optimizes%20structural%20sparsity%20and%20semantic%0Adivergence%20while%20enforcing%20interpretability%20preserving%20constraints%20such%20as%0Aentropy%20regularization%20and%20edge%20smoothness.%20We%20apply%20this%20framework%20to%20a%0Afine-tuned%20LLaMA-based%20LLM%20and%20reveal%20that%20counterfactual%20masking%20exposes%20the%0Amodel%27s%20structural%20dependencies%20and%20aligns%20with%20LoRA-induced%20parameter%20shifts.%0AThis%20work%20provides%20new%20insights%20into%20the%20internal%20mechanisms%20of%20fine-tuned%20LLMs%0Aand%20highlights%20counterfactual%20graphs%20as%20a%20potential%20tool%20for%20interpretable%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Fine%2520Tuned%2520LLMs%2520via%2520Counterfactuals%2520A%2520Knowledge%2520Graph%2520Driven%250A%2520%2520Framework%26entry.906535625%3DYucheng%2520Wang%2520and%2520Ziyang%2520Chen%2520and%2520Md%2520Faisal%2520Kabir%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520enabled%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520acquire%2520domain-specific%2520knowledge%2520with%2520remarkable%250Aefficiency.%2520However%252C%2520understanding%2520how%2520such%2520a%2520fine-tuning%2520mechanism%2520alters%2520a%250Amodel%2527s%2520structural%2520reasoning%2520and%2520semantic%2520behavior%2520remains%2520an%2520open%2520challenge.%250AThis%2520work%2520introduces%2520a%2520novel%2520framework%2520that%2520explains%2520fine-tuned%2520LLMs%2520via%250Acounterfactuals%2520grounded%2520in%2520knowledge%2520graphs.%2520Specifically%252C%2520we%2520construct%250ABioToolKG%252C%2520a%2520domain-specific%2520heterogeneous%2520knowledge%2520graph%2520in%2520bioinformatics%250Atools%2520and%2520design%2520a%2520counterfactual-based%2520fine-tuned%2520LLMs%2520explainer%250A%2528CFFTLLMExplainer%2529%2520that%2520learns%2520soft%2520masks%2520over%2520graph%2520nodes%2520and%2520edges%2520to%250Agenerate%2520minimal%2520structural%2520perturbations%2520that%2520induce%2520maximum%2520semantic%250Adivergence.%2520Our%2520method%2520jointly%2520optimizes%2520structural%2520sparsity%2520and%2520semantic%250Adivergence%2520while%2520enforcing%2520interpretability%2520preserving%2520constraints%2520such%2520as%250Aentropy%2520regularization%2520and%2520edge%2520smoothness.%2520We%2520apply%2520this%2520framework%2520to%2520a%250Afine-tuned%2520LLaMA-based%2520LLM%2520and%2520reveal%2520that%2520counterfactual%2520masking%2520exposes%2520the%250Amodel%2527s%2520structural%2520dependencies%2520and%2520aligns%2520with%2520LoRA-induced%2520parameter%2520shifts.%250AThis%2520work%2520provides%2520new%2520insights%2520into%2520the%2520internal%2520mechanisms%2520of%2520fine-tuned%2520LLMs%250Aand%2520highlights%2520counterfactual%2520graphs%2520as%2520a%2520potential%2520tool%2520for%2520interpretable%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Fine%20Tuned%20LLMs%20via%20Counterfactuals%20A%20Knowledge%20Graph%20Driven%0A%20%20Framework&entry.906535625=Yucheng%20Wang%20and%20Ziyang%20Chen%20and%20Md%20Faisal%20Kabir&entry.1292438233=%20%20The%20widespread%20adoption%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20enabled%20large%0Alanguage%20models%20%28LLMs%29%20to%20acquire%20domain-specific%20knowledge%20with%20remarkable%0Aefficiency.%20However%2C%20understanding%20how%20such%20a%20fine-tuning%20mechanism%20alters%20a%0Amodel%27s%20structural%20reasoning%20and%20semantic%20behavior%20remains%20an%20open%20challenge.%0AThis%20work%20introduces%20a%20novel%20framework%20that%20explains%20fine-tuned%20LLMs%20via%0Acounterfactuals%20grounded%20in%20knowledge%20graphs.%20Specifically%2C%20we%20construct%0ABioToolKG%2C%20a%20domain-specific%20heterogeneous%20knowledge%20graph%20in%20bioinformatics%0Atools%20and%20design%20a%20counterfactual-based%20fine-tuned%20LLMs%20explainer%0A%28CFFTLLMExplainer%29%20that%20learns%20soft%20masks%20over%20graph%20nodes%20and%20edges%20to%0Agenerate%20minimal%20structural%20perturbations%20that%20induce%20maximum%20semantic%0Adivergence.%20Our%20method%20jointly%20optimizes%20structural%20sparsity%20and%20semantic%0Adivergence%20while%20enforcing%20interpretability%20preserving%20constraints%20such%20as%0Aentropy%20regularization%20and%20edge%20smoothness.%20We%20apply%20this%20framework%20to%20a%0Afine-tuned%20LLaMA-based%20LLM%20and%20reveal%20that%20counterfactual%20masking%20exposes%20the%0Amodel%27s%20structural%20dependencies%20and%20aligns%20with%20LoRA-induced%20parameter%20shifts.%0AThis%20work%20provides%20new%20insights%20into%20the%20internal%20mechanisms%20of%20fine-tuned%20LLMs%0Aand%20highlights%20counterfactual%20graphs%20as%20a%20potential%20tool%20for%20interpretable%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21241v1&entry.124074799=Read"},
{"title": "Vision Transformers: the threat of realistic adversarial patches", "author": "Kasper Cools and Clara Maathuis and Alexander M. van Oers and Claudia S. H\u00fcbner and Nikos Deligiannis and Marijke Vandewal and Geert De Cubber", "abstract": "  The increasing reliance on machine learning systems has made their security a\ncritical concern. Evasion attacks enable adversaries to manipulate the\ndecision-making processes of AI systems, potentially causing security breaches\nor misclassification of targets. Vision Transformers (ViTs) have gained\nsignificant traction in modern machine learning due to increased 1) performance\ncompared to Convolutional Neural Networks (CNNs) and 2) robustness against\nadversarial perturbations. However, ViTs remain vulnerable to evasion attacks,\nparticularly to adversarial patches, unique patterns designed to manipulate AI\nclassification systems. These vulnerabilities are investigated by designing\nrealistic adversarial patches to cause misclassification in person vs.\nnon-person classification tasks using the Creases Transformation (CT)\ntechnique, which adds subtle geometric distortions similar to those occurring\nnaturally when wearing clothing. This study investigates the transferability of\nadversarial attack techniques used in CNNs when applied to ViT classification\nmodels. Experimental evaluation across four fine-tuned ViT models on a binary\nperson classification task reveals significant vulnerability variations: attack\nsuccess rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%\n(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and\nfacebook/dinov3-vitb16 reaching 65.17%. These results confirm the\ncross-architectural transferability of adversarial patches from CNNs to ViTs,\nwith pre-training dataset scale and methodology strongly influencing model\nresilience to adversarial attacks.\n", "link": "http://arxiv.org/abs/2509.21084v1", "date": "2025-09-25", "relevancy": 2.0514, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5292}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5071}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformers%3A%20the%20threat%20of%20realistic%20adversarial%20patches&body=Title%3A%20Vision%20Transformers%3A%20the%20threat%20of%20realistic%20adversarial%20patches%0AAuthor%3A%20Kasper%20Cools%20and%20Clara%20Maathuis%20and%20Alexander%20M.%20van%20Oers%20and%20Claudia%20S.%20H%C3%BCbner%20and%20Nikos%20Deligiannis%20and%20Marijke%20Vandewal%20and%20Geert%20De%20Cubber%0AAbstract%3A%20%20%20The%20increasing%20reliance%20on%20machine%20learning%20systems%20has%20made%20their%20security%20a%0Acritical%20concern.%20Evasion%20attacks%20enable%20adversaries%20to%20manipulate%20the%0Adecision-making%20processes%20of%20AI%20systems%2C%20potentially%20causing%20security%20breaches%0Aor%20misclassification%20of%20targets.%20Vision%20Transformers%20%28ViTs%29%20have%20gained%0Asignificant%20traction%20in%20modern%20machine%20learning%20due%20to%20increased%201%29%20performance%0Acompared%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%202%29%20robustness%20against%0Aadversarial%20perturbations.%20However%2C%20ViTs%20remain%20vulnerable%20to%20evasion%20attacks%2C%0Aparticularly%20to%20adversarial%20patches%2C%20unique%20patterns%20designed%20to%20manipulate%20AI%0Aclassification%20systems.%20These%20vulnerabilities%20are%20investigated%20by%20designing%0Arealistic%20adversarial%20patches%20to%20cause%20misclassification%20in%20person%20vs.%0Anon-person%20classification%20tasks%20using%20the%20Creases%20Transformation%20%28CT%29%0Atechnique%2C%20which%20adds%20subtle%20geometric%20distortions%20similar%20to%20those%20occurring%0Anaturally%20when%20wearing%20clothing.%20This%20study%20investigates%20the%20transferability%20of%0Aadversarial%20attack%20techniques%20used%20in%20CNNs%20when%20applied%20to%20ViT%20classification%0Amodels.%20Experimental%20evaluation%20across%20four%20fine-tuned%20ViT%20models%20on%20a%20binary%0Aperson%20classification%20task%20reveals%20significant%20vulnerability%20variations%3A%20attack%0Asuccess%20rates%20ranged%20from%2040.04%25%20%28google/vit-base-patch16-224-in21k%29%20to%2099.97%25%0A%28facebook/dino-vitb16%29%2C%20with%20google/vit-base-patch16-224%20achieving%2066.40%25%20and%0Afacebook/dinov3-vitb16%20reaching%2065.17%25.%20These%20results%20confirm%20the%0Across-architectural%20transferability%20of%20adversarial%20patches%20from%20CNNs%20to%20ViTs%2C%0Awith%20pre-training%20dataset%20scale%20and%20methodology%20strongly%20influencing%20model%0Aresilience%20to%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformers%253A%2520the%2520threat%2520of%2520realistic%2520adversarial%2520patches%26entry.906535625%3DKasper%2520Cools%2520and%2520Clara%2520Maathuis%2520and%2520Alexander%2520M.%2520van%2520Oers%2520and%2520Claudia%2520S.%2520H%25C3%25BCbner%2520and%2520Nikos%2520Deligiannis%2520and%2520Marijke%2520Vandewal%2520and%2520Geert%2520De%2520Cubber%26entry.1292438233%3D%2520%2520The%2520increasing%2520reliance%2520on%2520machine%2520learning%2520systems%2520has%2520made%2520their%2520security%2520a%250Acritical%2520concern.%2520Evasion%2520attacks%2520enable%2520adversaries%2520to%2520manipulate%2520the%250Adecision-making%2520processes%2520of%2520AI%2520systems%252C%2520potentially%2520causing%2520security%2520breaches%250Aor%2520misclassification%2520of%2520targets.%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520gained%250Asignificant%2520traction%2520in%2520modern%2520machine%2520learning%2520due%2520to%2520increased%25201%2529%2520performance%250Acompared%2520to%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%25202%2529%2520robustness%2520against%250Aadversarial%2520perturbations.%2520However%252C%2520ViTs%2520remain%2520vulnerable%2520to%2520evasion%2520attacks%252C%250Aparticularly%2520to%2520adversarial%2520patches%252C%2520unique%2520patterns%2520designed%2520to%2520manipulate%2520AI%250Aclassification%2520systems.%2520These%2520vulnerabilities%2520are%2520investigated%2520by%2520designing%250Arealistic%2520adversarial%2520patches%2520to%2520cause%2520misclassification%2520in%2520person%2520vs.%250Anon-person%2520classification%2520tasks%2520using%2520the%2520Creases%2520Transformation%2520%2528CT%2529%250Atechnique%252C%2520which%2520adds%2520subtle%2520geometric%2520distortions%2520similar%2520to%2520those%2520occurring%250Anaturally%2520when%2520wearing%2520clothing.%2520This%2520study%2520investigates%2520the%2520transferability%2520of%250Aadversarial%2520attack%2520techniques%2520used%2520in%2520CNNs%2520when%2520applied%2520to%2520ViT%2520classification%250Amodels.%2520Experimental%2520evaluation%2520across%2520four%2520fine-tuned%2520ViT%2520models%2520on%2520a%2520binary%250Aperson%2520classification%2520task%2520reveals%2520significant%2520vulnerability%2520variations%253A%2520attack%250Asuccess%2520rates%2520ranged%2520from%252040.04%2525%2520%2528google/vit-base-patch16-224-in21k%2529%2520to%252099.97%2525%250A%2528facebook/dino-vitb16%2529%252C%2520with%2520google/vit-base-patch16-224%2520achieving%252066.40%2525%2520and%250Afacebook/dinov3-vitb16%2520reaching%252065.17%2525.%2520These%2520results%2520confirm%2520the%250Across-architectural%2520transferability%2520of%2520adversarial%2520patches%2520from%2520CNNs%2520to%2520ViTs%252C%250Awith%2520pre-training%2520dataset%2520scale%2520and%2520methodology%2520strongly%2520influencing%2520model%250Aresilience%2520to%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformers%3A%20the%20threat%20of%20realistic%20adversarial%20patches&entry.906535625=Kasper%20Cools%20and%20Clara%20Maathuis%20and%20Alexander%20M.%20van%20Oers%20and%20Claudia%20S.%20H%C3%BCbner%20and%20Nikos%20Deligiannis%20and%20Marijke%20Vandewal%20and%20Geert%20De%20Cubber&entry.1292438233=%20%20The%20increasing%20reliance%20on%20machine%20learning%20systems%20has%20made%20their%20security%20a%0Acritical%20concern.%20Evasion%20attacks%20enable%20adversaries%20to%20manipulate%20the%0Adecision-making%20processes%20of%20AI%20systems%2C%20potentially%20causing%20security%20breaches%0Aor%20misclassification%20of%20targets.%20Vision%20Transformers%20%28ViTs%29%20have%20gained%0Asignificant%20traction%20in%20modern%20machine%20learning%20due%20to%20increased%201%29%20performance%0Acompared%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%202%29%20robustness%20against%0Aadversarial%20perturbations.%20However%2C%20ViTs%20remain%20vulnerable%20to%20evasion%20attacks%2C%0Aparticularly%20to%20adversarial%20patches%2C%20unique%20patterns%20designed%20to%20manipulate%20AI%0Aclassification%20systems.%20These%20vulnerabilities%20are%20investigated%20by%20designing%0Arealistic%20adversarial%20patches%20to%20cause%20misclassification%20in%20person%20vs.%0Anon-person%20classification%20tasks%20using%20the%20Creases%20Transformation%20%28CT%29%0Atechnique%2C%20which%20adds%20subtle%20geometric%20distortions%20similar%20to%20those%20occurring%0Anaturally%20when%20wearing%20clothing.%20This%20study%20investigates%20the%20transferability%20of%0Aadversarial%20attack%20techniques%20used%20in%20CNNs%20when%20applied%20to%20ViT%20classification%0Amodels.%20Experimental%20evaluation%20across%20four%20fine-tuned%20ViT%20models%20on%20a%20binary%0Aperson%20classification%20task%20reveals%20significant%20vulnerability%20variations%3A%20attack%0Asuccess%20rates%20ranged%20from%2040.04%25%20%28google/vit-base-patch16-224-in21k%29%20to%2099.97%25%0A%28facebook/dino-vitb16%29%2C%20with%20google/vit-base-patch16-224%20achieving%2066.40%25%20and%0Afacebook/dinov3-vitb16%20reaching%2065.17%25.%20These%20results%20confirm%20the%0Across-architectural%20transferability%20of%20adversarial%20patches%20from%20CNNs%20to%20ViTs%2C%0Awith%20pre-training%20dataset%20scale%20and%20methodology%20strongly%20influencing%20model%0Aresilience%20to%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21084v1&entry.124074799=Read"},
{"title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards", "author": "Zhilin Wang and Jiaqi Zeng and Olivier Delalleau and Ellie Evans and Daniel Egert and Hoo-Chang Shin and Felipe Soares and Yi Dong and Oleksii Kuchaiev", "abstract": "  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).\n", "link": "http://arxiv.org/abs/2509.21319v1", "date": "2025-09-25", "relevancy": 2.0356, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5136}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards&body=Title%3A%20RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards%0AAuthor%3A%20Zhilin%20Wang%20and%20Jiaqi%20Zeng%20and%20Olivier%20Delalleau%20and%20Ellie%20Evans%20and%20Daniel%20Egert%20and%20Hoo-Chang%20Shin%20and%20Felipe%20Soares%20and%20Yi%20Dong%20and%20Oleksii%20Kuchaiev%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20and%20Reinforcement%20Learning%0Awith%20Verifiable%20Rewards%20%28RLVR%29%20are%20the%20main%20RL%20paradigms%20used%20in%20LLM%0Apost-training%2C%20each%20offering%20distinct%20advantages.%20However%2C%20RLHF%20struggles%20with%0Ainterpretability%20and%20reward%20hacking%20because%20it%20relies%20on%20human%20judgments%20that%0Ausually%20lack%20explicit%20criteria%2C%20whereas%20RLVR%20is%20limited%20in%20scope%20by%20its%20focus%0Aon%20correctness-based%20verifiers.%20We%20propose%20Reinforcement%20Learning%20with%20Binary%0AFlexible%20Feedback%20%28RLBFF%29%2C%20which%20combines%20the%20versatility%20of%20human-driven%0Apreferences%20with%20the%20precision%20of%20rule-based%20verification%2C%20enabling%20reward%0Amodels%20to%20capture%20nuanced%20aspects%20of%20response%20quality%20beyond%20mere%20correctness.%0ARLBFF%20extracts%20principles%20that%20can%20be%20answered%20in%20a%20binary%20fashion%20%28e.g.%0Aaccuracy%20of%20information%3A%20yes%2C%20or%20code%20readability%3A%20no%29%20from%20natural%20language%0Afeedback.%20Such%20principles%20can%20then%20be%20used%20to%20ground%20Reward%20Model%20training%20as%0Aan%20entailment%20task%20%28response%20satisfies%20or%20does%20not%20satisfy%20an%20arbitrary%0Aprinciple%29.%20We%20show%20that%20Reward%20Models%20trained%20in%20this%20manner%20can%20outperform%0ABradley-Terry%20models%20when%20matched%20for%20data%20and%20achieve%20top%20performance%20on%0ARM-Bench%20%2886.2%25%29%20and%20JudgeBench%20%2881.4%25%2C%20%231%20on%20leaderboard%20as%20of%20September%2024%2C%0A2025%29.%20Additionally%2C%20users%20can%20specify%20principles%20of%20interest%20at%20inference%20time%0Ato%20customize%20the%20focus%20of%20our%20reward%20models%2C%20in%20contrast%20to%20Bradley-Terry%0Amodels.%20Finally%2C%20we%20present%20a%20fully%20open%20source%20recipe%20%28including%20data%29%20to%0Aalign%20Qwen3-32B%20using%20RLBFF%20and%20our%20Reward%20Model%2C%20to%20match%20or%20exceed%20the%0Aperformance%20of%20o3-mini%20and%20DeepSeek%20R1%20on%20general%20alignment%20benchmarks%20of%0AMT-Bench%2C%20WildBench%2C%20and%20Arena%20Hard%20v2%20%28at%20%3C5%25%20of%20the%20inference%20cost%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLBFF%253A%2520Binary%2520Flexible%2520Feedback%2520to%2520bridge%2520between%2520Human%2520Feedback%2520%2526%250A%2520%2520Verifiable%2520Rewards%26entry.906535625%3DZhilin%2520Wang%2520and%2520Jiaqi%2520Zeng%2520and%2520Olivier%2520Delalleau%2520and%2520Ellie%2520Evans%2520and%2520Daniel%2520Egert%2520and%2520Hoo-Chang%2520Shin%2520and%2520Felipe%2520Soares%2520and%2520Yi%2520Dong%2520and%2520Oleksii%2520Kuchaiev%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Reinforcement%2520Learning%250Awith%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520are%2520the%2520main%2520RL%2520paradigms%2520used%2520in%2520LLM%250Apost-training%252C%2520each%2520offering%2520distinct%2520advantages.%2520However%252C%2520RLHF%2520struggles%2520with%250Ainterpretability%2520and%2520reward%2520hacking%2520because%2520it%2520relies%2520on%2520human%2520judgments%2520that%250Ausually%2520lack%2520explicit%2520criteria%252C%2520whereas%2520RLVR%2520is%2520limited%2520in%2520scope%2520by%2520its%2520focus%250Aon%2520correctness-based%2520verifiers.%2520We%2520propose%2520Reinforcement%2520Learning%2520with%2520Binary%250AFlexible%2520Feedback%2520%2528RLBFF%2529%252C%2520which%2520combines%2520the%2520versatility%2520of%2520human-driven%250Apreferences%2520with%2520the%2520precision%2520of%2520rule-based%2520verification%252C%2520enabling%2520reward%250Amodels%2520to%2520capture%2520nuanced%2520aspects%2520of%2520response%2520quality%2520beyond%2520mere%2520correctness.%250ARLBFF%2520extracts%2520principles%2520that%2520can%2520be%2520answered%2520in%2520a%2520binary%2520fashion%2520%2528e.g.%250Aaccuracy%2520of%2520information%253A%2520yes%252C%2520or%2520code%2520readability%253A%2520no%2529%2520from%2520natural%2520language%250Afeedback.%2520Such%2520principles%2520can%2520then%2520be%2520used%2520to%2520ground%2520Reward%2520Model%2520training%2520as%250Aan%2520entailment%2520task%2520%2528response%2520satisfies%2520or%2520does%2520not%2520satisfy%2520an%2520arbitrary%250Aprinciple%2529.%2520We%2520show%2520that%2520Reward%2520Models%2520trained%2520in%2520this%2520manner%2520can%2520outperform%250ABradley-Terry%2520models%2520when%2520matched%2520for%2520data%2520and%2520achieve%2520top%2520performance%2520on%250ARM-Bench%2520%252886.2%2525%2529%2520and%2520JudgeBench%2520%252881.4%2525%252C%2520%25231%2520on%2520leaderboard%2520as%2520of%2520September%252024%252C%250A2025%2529.%2520Additionally%252C%2520users%2520can%2520specify%2520principles%2520of%2520interest%2520at%2520inference%2520time%250Ato%2520customize%2520the%2520focus%2520of%2520our%2520reward%2520models%252C%2520in%2520contrast%2520to%2520Bradley-Terry%250Amodels.%2520Finally%252C%2520we%2520present%2520a%2520fully%2520open%2520source%2520recipe%2520%2528including%2520data%2529%2520to%250Aalign%2520Qwen3-32B%2520using%2520RLBFF%2520and%2520our%2520Reward%2520Model%252C%2520to%2520match%2520or%2520exceed%2520the%250Aperformance%2520of%2520o3-mini%2520and%2520DeepSeek%2520R1%2520on%2520general%2520alignment%2520benchmarks%2520of%250AMT-Bench%252C%2520WildBench%252C%2520and%2520Arena%2520Hard%2520v2%2520%2528at%2520%253C5%2525%2520of%2520the%2520inference%2520cost%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLBFF%3A%20Binary%20Flexible%20Feedback%20to%20bridge%20between%20Human%20Feedback%20%26%0A%20%20Verifiable%20Rewards&entry.906535625=Zhilin%20Wang%20and%20Jiaqi%20Zeng%20and%20Olivier%20Delalleau%20and%20Ellie%20Evans%20and%20Daniel%20Egert%20and%20Hoo-Chang%20Shin%20and%20Felipe%20Soares%20and%20Yi%20Dong%20and%20Oleksii%20Kuchaiev&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20and%20Reinforcement%20Learning%0Awith%20Verifiable%20Rewards%20%28RLVR%29%20are%20the%20main%20RL%20paradigms%20used%20in%20LLM%0Apost-training%2C%20each%20offering%20distinct%20advantages.%20However%2C%20RLHF%20struggles%20with%0Ainterpretability%20and%20reward%20hacking%20because%20it%20relies%20on%20human%20judgments%20that%0Ausually%20lack%20explicit%20criteria%2C%20whereas%20RLVR%20is%20limited%20in%20scope%20by%20its%20focus%0Aon%20correctness-based%20verifiers.%20We%20propose%20Reinforcement%20Learning%20with%20Binary%0AFlexible%20Feedback%20%28RLBFF%29%2C%20which%20combines%20the%20versatility%20of%20human-driven%0Apreferences%20with%20the%20precision%20of%20rule-based%20verification%2C%20enabling%20reward%0Amodels%20to%20capture%20nuanced%20aspects%20of%20response%20quality%20beyond%20mere%20correctness.%0ARLBFF%20extracts%20principles%20that%20can%20be%20answered%20in%20a%20binary%20fashion%20%28e.g.%0Aaccuracy%20of%20information%3A%20yes%2C%20or%20code%20readability%3A%20no%29%20from%20natural%20language%0Afeedback.%20Such%20principles%20can%20then%20be%20used%20to%20ground%20Reward%20Model%20training%20as%0Aan%20entailment%20task%20%28response%20satisfies%20or%20does%20not%20satisfy%20an%20arbitrary%0Aprinciple%29.%20We%20show%20that%20Reward%20Models%20trained%20in%20this%20manner%20can%20outperform%0ABradley-Terry%20models%20when%20matched%20for%20data%20and%20achieve%20top%20performance%20on%0ARM-Bench%20%2886.2%25%29%20and%20JudgeBench%20%2881.4%25%2C%20%231%20on%20leaderboard%20as%20of%20September%2024%2C%0A2025%29.%20Additionally%2C%20users%20can%20specify%20principles%20of%20interest%20at%20inference%20time%0Ato%20customize%20the%20focus%20of%20our%20reward%20models%2C%20in%20contrast%20to%20Bradley-Terry%0Amodels.%20Finally%2C%20we%20present%20a%20fully%20open%20source%20recipe%20%28including%20data%29%20to%0Aalign%20Qwen3-32B%20using%20RLBFF%20and%20our%20Reward%20Model%2C%20to%20match%20or%20exceed%20the%0Aperformance%20of%20o3-mini%20and%20DeepSeek%20R1%20on%20general%20alignment%20benchmarks%20of%0AMT-Bench%2C%20WildBench%2C%20and%20Arena%20Hard%20v2%20%28at%20%3C5%25%20of%20the%20inference%20cost%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21319v1&entry.124074799=Read"},
{"title": "MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for\n  Video Temporal Reasoning", "author": "Sicheng Tao and Jungang Li and Yibo Yan and Junyan Zhang and Yubo Gao and Hanqian Li and ShuHang Xun and Yuxuan Fan and Hong Chen and Jianxiang He and Xuming Hu", "abstract": "  Video reasoning has emerged as a critical capability for multimodal large\nlanguage models (MLLMs), requiring models to move beyond static perception\ntoward coherent understanding of temporal dynamics in complex scenes. Yet\nexisting MLLMs often exhibit process inconsistency, where intermediate\nreasoning drifts from video dynamics even when the final answer is correct,\nundermining interpretability and robustness. To address this issue, we\nintroduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time\nWarping (DTW)-based process reward. This rule-based reward aligns reasoning\ntraces with temporally grounded references, enabling efficient process\nsupervision without auxiliary reward models. We further identify dynamic state\nprediction as a key measure of video reasoning and construct MOSS-Video, a\nbenchmark with annotated reasoning traces, where the training split is used to\nfine-tune MOSS-ChatV and the held-out split is reserved for evaluation.\nMOSS-ChatV achieves 87.2\\% on MOSS-Video (test) and improves performance on\ngeneral video benchmarks such as MVBench and MMVU. The framework consistently\nyields gains across different architectures, including Qwen2.5-VL and Phi-2,\nconfirming its broad applicability. Evaluations with GPT-4o-as-judge further\nshow that MOSS-ChatV produces more consistent and stable reasoning traces.\n", "link": "http://arxiv.org/abs/2509.21113v1", "date": "2025-09-25", "relevancy": 2.0338, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5164}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5072}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSS-ChatV%3A%20Reinforcement%20Learning%20with%20Process%20Reasoning%20Reward%20for%0A%20%20Video%20Temporal%20Reasoning&body=Title%3A%20MOSS-ChatV%3A%20Reinforcement%20Learning%20with%20Process%20Reasoning%20Reward%20for%0A%20%20Video%20Temporal%20Reasoning%0AAuthor%3A%20Sicheng%20Tao%20and%20Jungang%20Li%20and%20Yibo%20Yan%20and%20Junyan%20Zhang%20and%20Yubo%20Gao%20and%20Hanqian%20Li%20and%20ShuHang%20Xun%20and%20Yuxuan%20Fan%20and%20Hong%20Chen%20and%20Jianxiang%20He%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Video%20reasoning%20has%20emerged%20as%20a%20critical%20capability%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%2C%20requiring%20models%20to%20move%20beyond%20static%20perception%0Atoward%20coherent%20understanding%20of%20temporal%20dynamics%20in%20complex%20scenes.%20Yet%0Aexisting%20MLLMs%20often%20exhibit%20process%20inconsistency%2C%20where%20intermediate%0Areasoning%20drifts%20from%20video%20dynamics%20even%20when%20the%20final%20answer%20is%20correct%2C%0Aundermining%20interpretability%20and%20robustness.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20MOSS-ChatV%2C%20a%20reinforcement%20learning%20framework%20with%20a%20Dynamic%20Time%0AWarping%20%28DTW%29-based%20process%20reward.%20This%20rule-based%20reward%20aligns%20reasoning%0Atraces%20with%20temporally%20grounded%20references%2C%20enabling%20efficient%20process%0Asupervision%20without%20auxiliary%20reward%20models.%20We%20further%20identify%20dynamic%20state%0Aprediction%20as%20a%20key%20measure%20of%20video%20reasoning%20and%20construct%20MOSS-Video%2C%20a%0Abenchmark%20with%20annotated%20reasoning%20traces%2C%20where%20the%20training%20split%20is%20used%20to%0Afine-tune%20MOSS-ChatV%20and%20the%20held-out%20split%20is%20reserved%20for%20evaluation.%0AMOSS-ChatV%20achieves%2087.2%5C%25%20on%20MOSS-Video%20%28test%29%20and%20improves%20performance%20on%0Ageneral%20video%20benchmarks%20such%20as%20MVBench%20and%20MMVU.%20The%20framework%20consistently%0Ayields%20gains%20across%20different%20architectures%2C%20including%20Qwen2.5-VL%20and%20Phi-2%2C%0Aconfirming%20its%20broad%20applicability.%20Evaluations%20with%20GPT-4o-as-judge%20further%0Ashow%20that%20MOSS-ChatV%20produces%20more%20consistent%20and%20stable%20reasoning%20traces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSS-ChatV%253A%2520Reinforcement%2520Learning%2520with%2520Process%2520Reasoning%2520Reward%2520for%250A%2520%2520Video%2520Temporal%2520Reasoning%26entry.906535625%3DSicheng%2520Tao%2520and%2520Jungang%2520Li%2520and%2520Yibo%2520Yan%2520and%2520Junyan%2520Zhang%2520and%2520Yubo%2520Gao%2520and%2520Hanqian%2520Li%2520and%2520ShuHang%2520Xun%2520and%2520Yuxuan%2520Fan%2520and%2520Hong%2520Chen%2520and%2520Jianxiang%2520He%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Video%2520reasoning%2520has%2520emerged%2520as%2520a%2520critical%2520capability%2520for%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%252C%2520requiring%2520models%2520to%2520move%2520beyond%2520static%2520perception%250Atoward%2520coherent%2520understanding%2520of%2520temporal%2520dynamics%2520in%2520complex%2520scenes.%2520Yet%250Aexisting%2520MLLMs%2520often%2520exhibit%2520process%2520inconsistency%252C%2520where%2520intermediate%250Areasoning%2520drifts%2520from%2520video%2520dynamics%2520even%2520when%2520the%2520final%2520answer%2520is%2520correct%252C%250Aundermining%2520interpretability%2520and%2520robustness.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520MOSS-ChatV%252C%2520a%2520reinforcement%2520learning%2520framework%2520with%2520a%2520Dynamic%2520Time%250AWarping%2520%2528DTW%2529-based%2520process%2520reward.%2520This%2520rule-based%2520reward%2520aligns%2520reasoning%250Atraces%2520with%2520temporally%2520grounded%2520references%252C%2520enabling%2520efficient%2520process%250Asupervision%2520without%2520auxiliary%2520reward%2520models.%2520We%2520further%2520identify%2520dynamic%2520state%250Aprediction%2520as%2520a%2520key%2520measure%2520of%2520video%2520reasoning%2520and%2520construct%2520MOSS-Video%252C%2520a%250Abenchmark%2520with%2520annotated%2520reasoning%2520traces%252C%2520where%2520the%2520training%2520split%2520is%2520used%2520to%250Afine-tune%2520MOSS-ChatV%2520and%2520the%2520held-out%2520split%2520is%2520reserved%2520for%2520evaluation.%250AMOSS-ChatV%2520achieves%252087.2%255C%2525%2520on%2520MOSS-Video%2520%2528test%2529%2520and%2520improves%2520performance%2520on%250Ageneral%2520video%2520benchmarks%2520such%2520as%2520MVBench%2520and%2520MMVU.%2520The%2520framework%2520consistently%250Ayields%2520gains%2520across%2520different%2520architectures%252C%2520including%2520Qwen2.5-VL%2520and%2520Phi-2%252C%250Aconfirming%2520its%2520broad%2520applicability.%2520Evaluations%2520with%2520GPT-4o-as-judge%2520further%250Ashow%2520that%2520MOSS-ChatV%2520produces%2520more%2520consistent%2520and%2520stable%2520reasoning%2520traces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSS-ChatV%3A%20Reinforcement%20Learning%20with%20Process%20Reasoning%20Reward%20for%0A%20%20Video%20Temporal%20Reasoning&entry.906535625=Sicheng%20Tao%20and%20Jungang%20Li%20and%20Yibo%20Yan%20and%20Junyan%20Zhang%20and%20Yubo%20Gao%20and%20Hanqian%20Li%20and%20ShuHang%20Xun%20and%20Yuxuan%20Fan%20and%20Hong%20Chen%20and%20Jianxiang%20He%20and%20Xuming%20Hu&entry.1292438233=%20%20Video%20reasoning%20has%20emerged%20as%20a%20critical%20capability%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%2C%20requiring%20models%20to%20move%20beyond%20static%20perception%0Atoward%20coherent%20understanding%20of%20temporal%20dynamics%20in%20complex%20scenes.%20Yet%0Aexisting%20MLLMs%20often%20exhibit%20process%20inconsistency%2C%20where%20intermediate%0Areasoning%20drifts%20from%20video%20dynamics%20even%20when%20the%20final%20answer%20is%20correct%2C%0Aundermining%20interpretability%20and%20robustness.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20MOSS-ChatV%2C%20a%20reinforcement%20learning%20framework%20with%20a%20Dynamic%20Time%0AWarping%20%28DTW%29-based%20process%20reward.%20This%20rule-based%20reward%20aligns%20reasoning%0Atraces%20with%20temporally%20grounded%20references%2C%20enabling%20efficient%20process%0Asupervision%20without%20auxiliary%20reward%20models.%20We%20further%20identify%20dynamic%20state%0Aprediction%20as%20a%20key%20measure%20of%20video%20reasoning%20and%20construct%20MOSS-Video%2C%20a%0Abenchmark%20with%20annotated%20reasoning%20traces%2C%20where%20the%20training%20split%20is%20used%20to%0Afine-tune%20MOSS-ChatV%20and%20the%20held-out%20split%20is%20reserved%20for%20evaluation.%0AMOSS-ChatV%20achieves%2087.2%5C%25%20on%20MOSS-Video%20%28test%29%20and%20improves%20performance%20on%0Ageneral%20video%20benchmarks%20such%20as%20MVBench%20and%20MMVU.%20The%20framework%20consistently%0Ayields%20gains%20across%20different%20architectures%2C%20including%20Qwen2.5-VL%20and%20Phi-2%2C%0Aconfirming%20its%20broad%20applicability.%20Evaluations%20with%20GPT-4o-as-judge%20further%0Ashow%20that%20MOSS-ChatV%20produces%20more%20consistent%20and%20stable%20reasoning%20traces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21113v1&entry.124074799=Read"},
{"title": "Sparse Representations Improve Adversarial Robustness of Neural Network\n  Classifiers", "author": "Killian Steunou and Sigurd Saue and Th\u00e9o Druilhe", "abstract": "  Deep neural networks perform remarkably well on image classification tasks\nbut remain vulnerable to carefully crafted adversarial perturbations. This work\nrevisits linear dimensionality reduction as a simple, data-adapted defense. We\nempirically compare standard Principal Component Analysis (PCA) with its sparse\nvariant (SPCA) as front-end feature extractors for downstream classifiers, and\nwe complement these experiments with a theoretical analysis. On the theory\nside, we derive exact robustness certificates for linear heads applied to SPCA\nfeatures: for both $\\ell_\\infty$ and $\\ell_2$ threat models (binary and\nmulticlass), the certified radius grows as the dual norms of $W^\\top u$ shrink,\nwhere $W$ is the projection and $u$ the head weights. We further show that for\ngeneral (non-linear) heads, sparsity reduces operator-norm bounds through a\nLipschitz composition argument, predicting lower input sensitivity.\nEmpirically, with a small non-linear network after the projection, SPCA\nconsistently degrades more gracefully than PCA under strong white-box and\nblack-box attacks while maintaining competitive clean accuracy. Taken together,\nthe theory identifies the mechanism (sparser projections reduce adversarial\nleverage) and the experiments verify that this benefit persists beyond the\nlinear setting. Our code is available at\nhttps://github.com/killian31/SPCARobustness.\n", "link": "http://arxiv.org/abs/2509.21130v1", "date": "2025-09-25", "relevancy": 2.0314, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5337}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Representations%20Improve%20Adversarial%20Robustness%20of%20Neural%20Network%0A%20%20Classifiers&body=Title%3A%20Sparse%20Representations%20Improve%20Adversarial%20Robustness%20of%20Neural%20Network%0A%20%20Classifiers%0AAuthor%3A%20Killian%20Steunou%20and%20Sigurd%20Saue%20and%20Th%C3%A9o%20Druilhe%0AAbstract%3A%20%20%20Deep%20neural%20networks%20perform%20remarkably%20well%20on%20image%20classification%20tasks%0Abut%20remain%20vulnerable%20to%20carefully%20crafted%20adversarial%20perturbations.%20This%20work%0Arevisits%20linear%20dimensionality%20reduction%20as%20a%20simple%2C%20data-adapted%20defense.%20We%0Aempirically%20compare%20standard%20Principal%20Component%20Analysis%20%28PCA%29%20with%20its%20sparse%0Avariant%20%28SPCA%29%20as%20front-end%20feature%20extractors%20for%20downstream%20classifiers%2C%20and%0Awe%20complement%20these%20experiments%20with%20a%20theoretical%20analysis.%20On%20the%20theory%0Aside%2C%20we%20derive%20exact%20robustness%20certificates%20for%20linear%20heads%20applied%20to%20SPCA%0Afeatures%3A%20for%20both%20%24%5Cell_%5Cinfty%24%20and%20%24%5Cell_2%24%20threat%20models%20%28binary%20and%0Amulticlass%29%2C%20the%20certified%20radius%20grows%20as%20the%20dual%20norms%20of%20%24W%5E%5Ctop%20u%24%20shrink%2C%0Awhere%20%24W%24%20is%20the%20projection%20and%20%24u%24%20the%20head%20weights.%20We%20further%20show%20that%20for%0Ageneral%20%28non-linear%29%20heads%2C%20sparsity%20reduces%20operator-norm%20bounds%20through%20a%0ALipschitz%20composition%20argument%2C%20predicting%20lower%20input%20sensitivity.%0AEmpirically%2C%20with%20a%20small%20non-linear%20network%20after%20the%20projection%2C%20SPCA%0Aconsistently%20degrades%20more%20gracefully%20than%20PCA%20under%20strong%20white-box%20and%0Ablack-box%20attacks%20while%20maintaining%20competitive%20clean%20accuracy.%20Taken%20together%2C%0Athe%20theory%20identifies%20the%20mechanism%20%28sparser%20projections%20reduce%20adversarial%0Aleverage%29%20and%20the%20experiments%20verify%20that%20this%20benefit%20persists%20beyond%20the%0Alinear%20setting.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/killian31/SPCARobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Representations%2520Improve%2520Adversarial%2520Robustness%2520of%2520Neural%2520Network%250A%2520%2520Classifiers%26entry.906535625%3DKillian%2520Steunou%2520and%2520Sigurd%2520Saue%2520and%2520Th%25C3%25A9o%2520Druilhe%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520perform%2520remarkably%2520well%2520on%2520image%2520classification%2520tasks%250Abut%2520remain%2520vulnerable%2520to%2520carefully%2520crafted%2520adversarial%2520perturbations.%2520This%2520work%250Arevisits%2520linear%2520dimensionality%2520reduction%2520as%2520a%2520simple%252C%2520data-adapted%2520defense.%2520We%250Aempirically%2520compare%2520standard%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520with%2520its%2520sparse%250Avariant%2520%2528SPCA%2529%2520as%2520front-end%2520feature%2520extractors%2520for%2520downstream%2520classifiers%252C%2520and%250Awe%2520complement%2520these%2520experiments%2520with%2520a%2520theoretical%2520analysis.%2520On%2520the%2520theory%250Aside%252C%2520we%2520derive%2520exact%2520robustness%2520certificates%2520for%2520linear%2520heads%2520applied%2520to%2520SPCA%250Afeatures%253A%2520for%2520both%2520%2524%255Cell_%255Cinfty%2524%2520and%2520%2524%255Cell_2%2524%2520threat%2520models%2520%2528binary%2520and%250Amulticlass%2529%252C%2520the%2520certified%2520radius%2520grows%2520as%2520the%2520dual%2520norms%2520of%2520%2524W%255E%255Ctop%2520u%2524%2520shrink%252C%250Awhere%2520%2524W%2524%2520is%2520the%2520projection%2520and%2520%2524u%2524%2520the%2520head%2520weights.%2520We%2520further%2520show%2520that%2520for%250Ageneral%2520%2528non-linear%2529%2520heads%252C%2520sparsity%2520reduces%2520operator-norm%2520bounds%2520through%2520a%250ALipschitz%2520composition%2520argument%252C%2520predicting%2520lower%2520input%2520sensitivity.%250AEmpirically%252C%2520with%2520a%2520small%2520non-linear%2520network%2520after%2520the%2520projection%252C%2520SPCA%250Aconsistently%2520degrades%2520more%2520gracefully%2520than%2520PCA%2520under%2520strong%2520white-box%2520and%250Ablack-box%2520attacks%2520while%2520maintaining%2520competitive%2520clean%2520accuracy.%2520Taken%2520together%252C%250Athe%2520theory%2520identifies%2520the%2520mechanism%2520%2528sparser%2520projections%2520reduce%2520adversarial%250Aleverage%2529%2520and%2520the%2520experiments%2520verify%2520that%2520this%2520benefit%2520persists%2520beyond%2520the%250Alinear%2520setting.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/killian31/SPCARobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Representations%20Improve%20Adversarial%20Robustness%20of%20Neural%20Network%0A%20%20Classifiers&entry.906535625=Killian%20Steunou%20and%20Sigurd%20Saue%20and%20Th%C3%A9o%20Druilhe&entry.1292438233=%20%20Deep%20neural%20networks%20perform%20remarkably%20well%20on%20image%20classification%20tasks%0Abut%20remain%20vulnerable%20to%20carefully%20crafted%20adversarial%20perturbations.%20This%20work%0Arevisits%20linear%20dimensionality%20reduction%20as%20a%20simple%2C%20data-adapted%20defense.%20We%0Aempirically%20compare%20standard%20Principal%20Component%20Analysis%20%28PCA%29%20with%20its%20sparse%0Avariant%20%28SPCA%29%20as%20front-end%20feature%20extractors%20for%20downstream%20classifiers%2C%20and%0Awe%20complement%20these%20experiments%20with%20a%20theoretical%20analysis.%20On%20the%20theory%0Aside%2C%20we%20derive%20exact%20robustness%20certificates%20for%20linear%20heads%20applied%20to%20SPCA%0Afeatures%3A%20for%20both%20%24%5Cell_%5Cinfty%24%20and%20%24%5Cell_2%24%20threat%20models%20%28binary%20and%0Amulticlass%29%2C%20the%20certified%20radius%20grows%20as%20the%20dual%20norms%20of%20%24W%5E%5Ctop%20u%24%20shrink%2C%0Awhere%20%24W%24%20is%20the%20projection%20and%20%24u%24%20the%20head%20weights.%20We%20further%20show%20that%20for%0Ageneral%20%28non-linear%29%20heads%2C%20sparsity%20reduces%20operator-norm%20bounds%20through%20a%0ALipschitz%20composition%20argument%2C%20predicting%20lower%20input%20sensitivity.%0AEmpirically%2C%20with%20a%20small%20non-linear%20network%20after%20the%20projection%2C%20SPCA%0Aconsistently%20degrades%20more%20gracefully%20than%20PCA%20under%20strong%20white-box%20and%0Ablack-box%20attacks%20while%20maintaining%20competitive%20clean%20accuracy.%20Taken%20together%2C%0Athe%20theory%20identifies%20the%20mechanism%20%28sparser%20projections%20reduce%20adversarial%0Aleverage%29%20and%20the%20experiments%20verify%20that%20this%20benefit%20persists%20beyond%20the%0Alinear%20setting.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/killian31/SPCARobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21130v1&entry.124074799=Read"},
{"title": "LAVA: Explainability for Unsupervised Latent Embeddings", "author": "Ivan Stresec and Joana P. Gon\u00e7alves", "abstract": "  Unsupervised black-box models can be drivers of scientific discovery, but\nremain difficult to interpret. Crucially, discovery hinges on understanding the\nmodel output, which is often a multi-dimensional latent embedding rather than a\nwell-defined target. While explainability for supervised learning usually seeks\nto uncover how input features are used to predict a target, its unsupervised\ncounterpart should relate input features to the structure of the learned latent\nspace. Adaptations of supervised model explainability for unsupervised learning\nprovide either single-sample or dataset-wide summary explanations. However,\nwithout automated strategies of relating similar samples to one another guided\nby their latent proximity, explanations remain either too fine-grained or too\nreductive to be meaningful. This is especially relevant for manifold learning\nmethods that produce no mapping function, leaving us only with the relative\nspatial organization of their embeddings. We introduce Locality-Aware Variable\nAssociations (LAVA), a post-hoc model-agnostic method designed to explain local\nembedding organization through its relationship with the input features. To\nachieve this, LAVA represents the latent space as a series of localities\n(neighborhoods) described in terms of correlations between the original\nfeatures, and then reveals reoccurring patterns of correlations across the\nentire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney\ndataset, we show that LAVA captures relevant feature associations, with\nvisually and biologically relevant local patterns shared among seemingly\ndistant regions of the latent spaces.\n", "link": "http://arxiv.org/abs/2509.21149v1", "date": "2025-09-25", "relevancy": 2.0307, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5029}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings&body=Title%3A%20LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings%0AAuthor%3A%20Ivan%20Stresec%20and%20Joana%20P.%20Gon%C3%A7alves%0AAbstract%3A%20%20%20Unsupervised%20black-box%20models%20can%20be%20drivers%20of%20scientific%20discovery%2C%20but%0Aremain%20difficult%20to%20interpret.%20Crucially%2C%20discovery%20hinges%20on%20understanding%20the%0Amodel%20output%2C%20which%20is%20often%20a%20multi-dimensional%20latent%20embedding%20rather%20than%20a%0Awell-defined%20target.%20While%20explainability%20for%20supervised%20learning%20usually%20seeks%0Ato%20uncover%20how%20input%20features%20are%20used%20to%20predict%20a%20target%2C%20its%20unsupervised%0Acounterpart%20should%20relate%20input%20features%20to%20the%20structure%20of%20the%20learned%20latent%0Aspace.%20Adaptations%20of%20supervised%20model%20explainability%20for%20unsupervised%20learning%0Aprovide%20either%20single-sample%20or%20dataset-wide%20summary%20explanations.%20However%2C%0Awithout%20automated%20strategies%20of%20relating%20similar%20samples%20to%20one%20another%20guided%0Aby%20their%20latent%20proximity%2C%20explanations%20remain%20either%20too%20fine-grained%20or%20too%0Areductive%20to%20be%20meaningful.%20This%20is%20especially%20relevant%20for%20manifold%20learning%0Amethods%20that%20produce%20no%20mapping%20function%2C%20leaving%20us%20only%20with%20the%20relative%0Aspatial%20organization%20of%20their%20embeddings.%20We%20introduce%20Locality-Aware%20Variable%0AAssociations%20%28LAVA%29%2C%20a%20post-hoc%20model-agnostic%20method%20designed%20to%20explain%20local%0Aembedding%20organization%20through%20its%20relationship%20with%20the%20input%20features.%20To%0Aachieve%20this%2C%20LAVA%20represents%20the%20latent%20space%20as%20a%20series%20of%20localities%0A%28neighborhoods%29%20described%20in%20terms%20of%20correlations%20between%20the%20original%0Afeatures%2C%20and%20then%20reveals%20reoccurring%20patterns%20of%20correlations%20across%20the%0Aentire%20latent%20space.%20Based%20on%20UMAP%20embeddings%20of%20MNIST%20and%20a%20single-cell%20kidney%0Adataset%2C%20we%20show%20that%20LAVA%20captures%20relevant%20feature%20associations%2C%20with%0Avisually%20and%20biologically%20relevant%20local%20patterns%20shared%20among%20seemingly%0Adistant%20regions%20of%20the%20latent%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAVA%253A%2520Explainability%2520for%2520Unsupervised%2520Latent%2520Embeddings%26entry.906535625%3DIvan%2520Stresec%2520and%2520Joana%2520P.%2520Gon%25C3%25A7alves%26entry.1292438233%3D%2520%2520Unsupervised%2520black-box%2520models%2520can%2520be%2520drivers%2520of%2520scientific%2520discovery%252C%2520but%250Aremain%2520difficult%2520to%2520interpret.%2520Crucially%252C%2520discovery%2520hinges%2520on%2520understanding%2520the%250Amodel%2520output%252C%2520which%2520is%2520often%2520a%2520multi-dimensional%2520latent%2520embedding%2520rather%2520than%2520a%250Awell-defined%2520target.%2520While%2520explainability%2520for%2520supervised%2520learning%2520usually%2520seeks%250Ato%2520uncover%2520how%2520input%2520features%2520are%2520used%2520to%2520predict%2520a%2520target%252C%2520its%2520unsupervised%250Acounterpart%2520should%2520relate%2520input%2520features%2520to%2520the%2520structure%2520of%2520the%2520learned%2520latent%250Aspace.%2520Adaptations%2520of%2520supervised%2520model%2520explainability%2520for%2520unsupervised%2520learning%250Aprovide%2520either%2520single-sample%2520or%2520dataset-wide%2520summary%2520explanations.%2520However%252C%250Awithout%2520automated%2520strategies%2520of%2520relating%2520similar%2520samples%2520to%2520one%2520another%2520guided%250Aby%2520their%2520latent%2520proximity%252C%2520explanations%2520remain%2520either%2520too%2520fine-grained%2520or%2520too%250Areductive%2520to%2520be%2520meaningful.%2520This%2520is%2520especially%2520relevant%2520for%2520manifold%2520learning%250Amethods%2520that%2520produce%2520no%2520mapping%2520function%252C%2520leaving%2520us%2520only%2520with%2520the%2520relative%250Aspatial%2520organization%2520of%2520their%2520embeddings.%2520We%2520introduce%2520Locality-Aware%2520Variable%250AAssociations%2520%2528LAVA%2529%252C%2520a%2520post-hoc%2520model-agnostic%2520method%2520designed%2520to%2520explain%2520local%250Aembedding%2520organization%2520through%2520its%2520relationship%2520with%2520the%2520input%2520features.%2520To%250Aachieve%2520this%252C%2520LAVA%2520represents%2520the%2520latent%2520space%2520as%2520a%2520series%2520of%2520localities%250A%2528neighborhoods%2529%2520described%2520in%2520terms%2520of%2520correlations%2520between%2520the%2520original%250Afeatures%252C%2520and%2520then%2520reveals%2520reoccurring%2520patterns%2520of%2520correlations%2520across%2520the%250Aentire%2520latent%2520space.%2520Based%2520on%2520UMAP%2520embeddings%2520of%2520MNIST%2520and%2520a%2520single-cell%2520kidney%250Adataset%252C%2520we%2520show%2520that%2520LAVA%2520captures%2520relevant%2520feature%2520associations%252C%2520with%250Avisually%2520and%2520biologically%2520relevant%2520local%2520patterns%2520shared%2520among%2520seemingly%250Adistant%2520regions%2520of%2520the%2520latent%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings&entry.906535625=Ivan%20Stresec%20and%20Joana%20P.%20Gon%C3%A7alves&entry.1292438233=%20%20Unsupervised%20black-box%20models%20can%20be%20drivers%20of%20scientific%20discovery%2C%20but%0Aremain%20difficult%20to%20interpret.%20Crucially%2C%20discovery%20hinges%20on%20understanding%20the%0Amodel%20output%2C%20which%20is%20often%20a%20multi-dimensional%20latent%20embedding%20rather%20than%20a%0Awell-defined%20target.%20While%20explainability%20for%20supervised%20learning%20usually%20seeks%0Ato%20uncover%20how%20input%20features%20are%20used%20to%20predict%20a%20target%2C%20its%20unsupervised%0Acounterpart%20should%20relate%20input%20features%20to%20the%20structure%20of%20the%20learned%20latent%0Aspace.%20Adaptations%20of%20supervised%20model%20explainability%20for%20unsupervised%20learning%0Aprovide%20either%20single-sample%20or%20dataset-wide%20summary%20explanations.%20However%2C%0Awithout%20automated%20strategies%20of%20relating%20similar%20samples%20to%20one%20another%20guided%0Aby%20their%20latent%20proximity%2C%20explanations%20remain%20either%20too%20fine-grained%20or%20too%0Areductive%20to%20be%20meaningful.%20This%20is%20especially%20relevant%20for%20manifold%20learning%0Amethods%20that%20produce%20no%20mapping%20function%2C%20leaving%20us%20only%20with%20the%20relative%0Aspatial%20organization%20of%20their%20embeddings.%20We%20introduce%20Locality-Aware%20Variable%0AAssociations%20%28LAVA%29%2C%20a%20post-hoc%20model-agnostic%20method%20designed%20to%20explain%20local%0Aembedding%20organization%20through%20its%20relationship%20with%20the%20input%20features.%20To%0Aachieve%20this%2C%20LAVA%20represents%20the%20latent%20space%20as%20a%20series%20of%20localities%0A%28neighborhoods%29%20described%20in%20terms%20of%20correlations%20between%20the%20original%0Afeatures%2C%20and%20then%20reveals%20reoccurring%20patterns%20of%20correlations%20across%20the%0Aentire%20latent%20space.%20Based%20on%20UMAP%20embeddings%20of%20MNIST%20and%20a%20single-cell%20kidney%0Adataset%2C%20we%20show%20that%20LAVA%20captures%20relevant%20feature%20associations%2C%20with%0Avisually%20and%20biologically%20relevant%20local%20patterns%20shared%20among%20seemingly%0Adistant%20regions%20of%20the%20latent%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21149v1&entry.124074799=Read"},
{"title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A\n  Drone Ball Balancing Study", "author": "Mingjiang Liu and Hailong Huang", "abstract": "  This paper addresses a drone ball-balancing task, in which a drone stabilizes\na ball atop a movable beam through cable-based interaction. We propose a\nhierarchical control framework that decouples high-level balancing policy from\nlow-level drone control, and train a reinforcement learning (RL) policy to\nhandle the high-level decision-making. Simulation results show that the RL\npolicy achieves superior performance compared to carefully tuned PID\ncontrollers within the same hierarchical structure. Through systematic\ncomparative analysis, we demonstrate that RL's advantage stems not from\nimproved parameter tuning or inherent nonlinear mapping capabilities, but from\nits ability to effectively utilize richer state observations. These findings\nunderscore the critical role of comprehensive state representation in\nlearning-based systems and suggest that enhanced sensing could be instrumental\nin improving controller performance.\n", "link": "http://arxiv.org/abs/2509.21122v1", "date": "2025-09-25", "relevancy": 2.0258, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rich%20State%20Observations%20Empower%20Reinforcement%20Learning%20to%20Surpass%20PID%3A%20A%0A%20%20Drone%20Ball%20Balancing%20Study&body=Title%3A%20Rich%20State%20Observations%20Empower%20Reinforcement%20Learning%20to%20Surpass%20PID%3A%20A%0A%20%20Drone%20Ball%20Balancing%20Study%0AAuthor%3A%20Mingjiang%20Liu%20and%20Hailong%20Huang%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20drone%20ball-balancing%20task%2C%20in%20which%20a%20drone%20stabilizes%0Aa%20ball%20atop%20a%20movable%20beam%20through%20cable-based%20interaction.%20We%20propose%20a%0Ahierarchical%20control%20framework%20that%20decouples%20high-level%20balancing%20policy%20from%0Alow-level%20drone%20control%2C%20and%20train%20a%20reinforcement%20learning%20%28RL%29%20policy%20to%0Ahandle%20the%20high-level%20decision-making.%20Simulation%20results%20show%20that%20the%20RL%0Apolicy%20achieves%20superior%20performance%20compared%20to%20carefully%20tuned%20PID%0Acontrollers%20within%20the%20same%20hierarchical%20structure.%20Through%20systematic%0Acomparative%20analysis%2C%20we%20demonstrate%20that%20RL%27s%20advantage%20stems%20not%20from%0Aimproved%20parameter%20tuning%20or%20inherent%20nonlinear%20mapping%20capabilities%2C%20but%20from%0Aits%20ability%20to%20effectively%20utilize%20richer%20state%20observations.%20These%20findings%0Aunderscore%20the%20critical%20role%20of%20comprehensive%20state%20representation%20in%0Alearning-based%20systems%20and%20suggest%20that%20enhanced%20sensing%20could%20be%20instrumental%0Ain%20improving%20controller%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRich%2520State%2520Observations%2520Empower%2520Reinforcement%2520Learning%2520to%2520Surpass%2520PID%253A%2520A%250A%2520%2520Drone%2520Ball%2520Balancing%2520Study%26entry.906535625%3DMingjiang%2520Liu%2520and%2520Hailong%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520a%2520drone%2520ball-balancing%2520task%252C%2520in%2520which%2520a%2520drone%2520stabilizes%250Aa%2520ball%2520atop%2520a%2520movable%2520beam%2520through%2520cable-based%2520interaction.%2520We%2520propose%2520a%250Ahierarchical%2520control%2520framework%2520that%2520decouples%2520high-level%2520balancing%2520policy%2520from%250Alow-level%2520drone%2520control%252C%2520and%2520train%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520policy%2520to%250Ahandle%2520the%2520high-level%2520decision-making.%2520Simulation%2520results%2520show%2520that%2520the%2520RL%250Apolicy%2520achieves%2520superior%2520performance%2520compared%2520to%2520carefully%2520tuned%2520PID%250Acontrollers%2520within%2520the%2520same%2520hierarchical%2520structure.%2520Through%2520systematic%250Acomparative%2520analysis%252C%2520we%2520demonstrate%2520that%2520RL%2527s%2520advantage%2520stems%2520not%2520from%250Aimproved%2520parameter%2520tuning%2520or%2520inherent%2520nonlinear%2520mapping%2520capabilities%252C%2520but%2520from%250Aits%2520ability%2520to%2520effectively%2520utilize%2520richer%2520state%2520observations.%2520These%2520findings%250Aunderscore%2520the%2520critical%2520role%2520of%2520comprehensive%2520state%2520representation%2520in%250Alearning-based%2520systems%2520and%2520suggest%2520that%2520enhanced%2520sensing%2520could%2520be%2520instrumental%250Ain%2520improving%2520controller%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rich%20State%20Observations%20Empower%20Reinforcement%20Learning%20to%20Surpass%20PID%3A%20A%0A%20%20Drone%20Ball%20Balancing%20Study&entry.906535625=Mingjiang%20Liu%20and%20Hailong%20Huang&entry.1292438233=%20%20This%20paper%20addresses%20a%20drone%20ball-balancing%20task%2C%20in%20which%20a%20drone%20stabilizes%0Aa%20ball%20atop%20a%20movable%20beam%20through%20cable-based%20interaction.%20We%20propose%20a%0Ahierarchical%20control%20framework%20that%20decouples%20high-level%20balancing%20policy%20from%0Alow-level%20drone%20control%2C%20and%20train%20a%20reinforcement%20learning%20%28RL%29%20policy%20to%0Ahandle%20the%20high-level%20decision-making.%20Simulation%20results%20show%20that%20the%20RL%0Apolicy%20achieves%20superior%20performance%20compared%20to%20carefully%20tuned%20PID%0Acontrollers%20within%20the%20same%20hierarchical%20structure.%20Through%20systematic%0Acomparative%20analysis%2C%20we%20demonstrate%20that%20RL%27s%20advantage%20stems%20not%20from%0Aimproved%20parameter%20tuning%20or%20inherent%20nonlinear%20mapping%20capabilities%2C%20but%20from%0Aits%20ability%20to%20effectively%20utilize%20richer%20state%20observations.%20These%20findings%0Aunderscore%20the%20critical%20role%20of%20comprehensive%20state%20representation%20in%0Alearning-based%20systems%20and%20suggest%20that%20enhanced%20sensing%20could%20be%20instrumental%0Ain%20improving%20controller%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21122v1&entry.124074799=Read"},
{"title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning", "author": "Yang Zhou and Sunzhu Li and Shunyu Liu and Wenkai Fang and Kongcheng Zhang and Jiale Zhao and Jingwen Yang and Yihe Zhou and Jianwei Lv and Tongya Zheng and Hengtong Lu and Wei Chen and Yan Xie and Mingli Song", "abstract": "  Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.\n", "link": "http://arxiv.org/abs/2508.16949v3", "date": "2025-09-25", "relevancy": 2.0257, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&body=Title%3A%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning%0AAuthor%3A%20Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16949v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Exploration%2520Bottleneck%253A%2520Rubric-Scaffolded%2520Reinforcement%250A%2520%2520Learning%2520for%2520General%2520LLM%2520Reasoning%26entry.906535625%3DYang%2520Zhou%2520and%2520Sunzhu%2520Li%2520and%2520Shunyu%2520Liu%2520and%2520Wenkai%2520Fang%2520and%2520Kongcheng%2520Zhang%2520and%2520Jiale%2520Zhao%2520and%2520Jingwen%2520Yang%2520and%2520Yihe%2520Zhou%2520and%2520Jianwei%2520Lv%2520and%2520Tongya%2520Zheng%2520and%2520Hengtong%2520Lu%2520and%2520Wei%2520Chen%2520and%2520Yan%2520Xie%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520underscored%2520the%250Apotential%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520facilitate%2520the%2520emergence%2520of%250Areasoning%2520capabilities.%2520Despite%2520the%2520encouraging%2520results%252C%2520a%2520fundamental%2520dilemma%250Apersists%2520as%2520RL%2520improvement%2520relies%2520on%2520learning%2520from%2520high-quality%2520samples%252C%2520yet%250Athe%2520exploration%2520for%2520such%2520samples%2520remains%2520bounded%2520by%2520the%2520inherent%2520limitations%2520of%250ALLMs.%2520This%252C%2520in%2520effect%252C%2520creates%2520an%2520undesirable%2520cycle%2520in%2520which%2520what%2520cannot%2520be%250Aexplored%2520cannot%2520be%2520learned.%2520In%2520this%2520work%252C%2520we%2520propose%2520Rubric-Scaffolded%250AReinforcement%2520Learning%2520%2528RuscaRL%2529%252C%2520a%2520novel%2520instructional%2520scaffolding%2520framework%250Adesigned%2520to%2520break%2520the%2520exploration%2520bottleneck%2520for%2520general%2520LLM%2520reasoning.%250ASpecifically%252C%2520RuscaRL%2520introduces%2520checklist-style%2520rubrics%2520as%2520%25281%2529%2520explicit%250Ascaffolding%2520for%2520exploration%2520during%2520rollout%2520generation%252C%2520where%2520different%2520rubrics%250Aare%2520provided%2520as%2520external%2520guidance%2520within%2520task%2520instructions%2520to%2520steer%2520diverse%250Ahigh-quality%2520responses.%2520This%2520guidance%2520is%2520gradually%2520decayed%2520over%2520time%252C%250Aencouraging%2520the%2520model%2520to%2520internalize%2520the%2520underlying%2520reasoning%2520patterns%253B%2520%25282%2529%250Averifiable%2520rewards%2520for%2520exploitation%2520during%2520model%2520training%252C%2520where%2520we%2520can%2520obtain%250Arobust%2520LLM-as-a-Judge%2520scores%2520using%2520rubrics%2520as%2520references%252C%2520enabling%2520effective%2520RL%250Aon%2520general%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520the%2520proposed%2520RuscaRL%2520across%2520various%2520benchmarks%252C%2520effectively%2520expanding%250Areasoning%2520boundaries%2520under%2520the%2520Best-of-N%2520evaluation.%2520Notably%252C%2520RuscaRL%250Asignificantly%2520boosts%2520Qwen2.5-7B-Instruct%2520from%252023.6%2520to%252050.3%2520on%2520HealthBench-500%252C%250Asurpassing%2520GPT-4.1.%2520Furthermore%252C%2520our%2520fine-tuned%2520variant%2520on%250AQwen3-30B-A3B-Instruct%2520achieves%252061.1%2520on%2520HealthBench-500%252C%2520outperforming%2520leading%250ALLMs%2520including%2520OpenAI-o3.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IANNXANG/RuscaRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16949v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&entry.906535625=Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16949v3&entry.124074799=Read"},
{"title": "Background Prompt for Few-Shot Out-of-Distribution Detection", "author": "Songyue Cai and Zongqian Wu and Yujie Mo and Liang Peng and Ping Hu and Xiaoshuang Shi and Xiaofeng Zhu", "abstract": "  Existing foreground-background (FG-BG) decomposition methods for the few-shot\nout-of-distribution (FS-OOD) detection often suffer from low robustness due to\nover-reliance on the local class similarity and a fixed background patch\nextraction strategy. To address these challenges, we propose a new FG-BG\ndecomposition framework, namely Mambo, for FS-OOD detection. Specifically, we\npropose to first learn a background prompt to obtain the local background\nsimilarity containing both the background and image semantic information, and\nthen refine the local background similarity using the local class similarity.\nAs a result, we use both the refined local background similarity and the local\nclass similarity to conduct background extraction, reducing the dependence of\nthe local class similarity in previous methods. Furthermore, we propose the\npatch self-calibrated tuning to consider the sample diversity to flexibly\nselect numbers of background patches for different samples, and thus exploring\nthe issue of fixed background extraction strategies in previous methods.\nExtensive experiments on real-world datasets demonstrate that our proposed\nMambo achieves the best performance, compared to SOTA methods in terms of OOD\ndetection and near OOD detection setting. The source code will be released at\nhttps://github.com/YuzunoKawori/Mambo.\n", "link": "http://arxiv.org/abs/2509.21055v1", "date": "2025-09-25", "relevancy": 2.025, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5065}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Background%20Prompt%20for%20Few-Shot%20Out-of-Distribution%20Detection&body=Title%3A%20Background%20Prompt%20for%20Few-Shot%20Out-of-Distribution%20Detection%0AAuthor%3A%20Songyue%20Cai%20and%20Zongqian%20Wu%20and%20Yujie%20Mo%20and%20Liang%20Peng%20and%20Ping%20Hu%20and%20Xiaoshuang%20Shi%20and%20Xiaofeng%20Zhu%0AAbstract%3A%20%20%20Existing%20foreground-background%20%28FG-BG%29%20decomposition%20methods%20for%20the%20few-shot%0Aout-of-distribution%20%28FS-OOD%29%20detection%20often%20suffer%20from%20low%20robustness%20due%20to%0Aover-reliance%20on%20the%20local%20class%20similarity%20and%20a%20fixed%20background%20patch%0Aextraction%20strategy.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20new%20FG-BG%0Adecomposition%20framework%2C%20namely%20Mambo%2C%20for%20FS-OOD%20detection.%20Specifically%2C%20we%0Apropose%20to%20first%20learn%20a%20background%20prompt%20to%20obtain%20the%20local%20background%0Asimilarity%20containing%20both%20the%20background%20and%20image%20semantic%20information%2C%20and%0Athen%20refine%20the%20local%20background%20similarity%20using%20the%20local%20class%20similarity.%0AAs%20a%20result%2C%20we%20use%20both%20the%20refined%20local%20background%20similarity%20and%20the%20local%0Aclass%20similarity%20to%20conduct%20background%20extraction%2C%20reducing%20the%20dependence%20of%0Athe%20local%20class%20similarity%20in%20previous%20methods.%20Furthermore%2C%20we%20propose%20the%0Apatch%20self-calibrated%20tuning%20to%20consider%20the%20sample%20diversity%20to%20flexibly%0Aselect%20numbers%20of%20background%20patches%20for%20different%20samples%2C%20and%20thus%20exploring%0Athe%20issue%20of%20fixed%20background%20extraction%20strategies%20in%20previous%20methods.%0AExtensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20proposed%0AMambo%20achieves%20the%20best%20performance%2C%20compared%20to%20SOTA%20methods%20in%20terms%20of%20OOD%0Adetection%20and%20near%20OOD%20detection%20setting.%20The%20source%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/YuzunoKawori/Mambo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackground%2520Prompt%2520for%2520Few-Shot%2520Out-of-Distribution%2520Detection%26entry.906535625%3DSongyue%2520Cai%2520and%2520Zongqian%2520Wu%2520and%2520Yujie%2520Mo%2520and%2520Liang%2520Peng%2520and%2520Ping%2520Hu%2520and%2520Xiaoshuang%2520Shi%2520and%2520Xiaofeng%2520Zhu%26entry.1292438233%3D%2520%2520Existing%2520foreground-background%2520%2528FG-BG%2529%2520decomposition%2520methods%2520for%2520the%2520few-shot%250Aout-of-distribution%2520%2528FS-OOD%2529%2520detection%2520often%2520suffer%2520from%2520low%2520robustness%2520due%2520to%250Aover-reliance%2520on%2520the%2520local%2520class%2520similarity%2520and%2520a%2520fixed%2520background%2520patch%250Aextraction%2520strategy.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520new%2520FG-BG%250Adecomposition%2520framework%252C%2520namely%2520Mambo%252C%2520for%2520FS-OOD%2520detection.%2520Specifically%252C%2520we%250Apropose%2520to%2520first%2520learn%2520a%2520background%2520prompt%2520to%2520obtain%2520the%2520local%2520background%250Asimilarity%2520containing%2520both%2520the%2520background%2520and%2520image%2520semantic%2520information%252C%2520and%250Athen%2520refine%2520the%2520local%2520background%2520similarity%2520using%2520the%2520local%2520class%2520similarity.%250AAs%2520a%2520result%252C%2520we%2520use%2520both%2520the%2520refined%2520local%2520background%2520similarity%2520and%2520the%2520local%250Aclass%2520similarity%2520to%2520conduct%2520background%2520extraction%252C%2520reducing%2520the%2520dependence%2520of%250Athe%2520local%2520class%2520similarity%2520in%2520previous%2520methods.%2520Furthermore%252C%2520we%2520propose%2520the%250Apatch%2520self-calibrated%2520tuning%2520to%2520consider%2520the%2520sample%2520diversity%2520to%2520flexibly%250Aselect%2520numbers%2520of%2520background%2520patches%2520for%2520different%2520samples%252C%2520and%2520thus%2520exploring%250Athe%2520issue%2520of%2520fixed%2520background%2520extraction%2520strategies%2520in%2520previous%2520methods.%250AExtensive%2520experiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520proposed%250AMambo%2520achieves%2520the%2520best%2520performance%252C%2520compared%2520to%2520SOTA%2520methods%2520in%2520terms%2520of%2520OOD%250Adetection%2520and%2520near%2520OOD%2520detection%2520setting.%2520The%2520source%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/YuzunoKawori/Mambo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Background%20Prompt%20for%20Few-Shot%20Out-of-Distribution%20Detection&entry.906535625=Songyue%20Cai%20and%20Zongqian%20Wu%20and%20Yujie%20Mo%20and%20Liang%20Peng%20and%20Ping%20Hu%20and%20Xiaoshuang%20Shi%20and%20Xiaofeng%20Zhu&entry.1292438233=%20%20Existing%20foreground-background%20%28FG-BG%29%20decomposition%20methods%20for%20the%20few-shot%0Aout-of-distribution%20%28FS-OOD%29%20detection%20often%20suffer%20from%20low%20robustness%20due%20to%0Aover-reliance%20on%20the%20local%20class%20similarity%20and%20a%20fixed%20background%20patch%0Aextraction%20strategy.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20new%20FG-BG%0Adecomposition%20framework%2C%20namely%20Mambo%2C%20for%20FS-OOD%20detection.%20Specifically%2C%20we%0Apropose%20to%20first%20learn%20a%20background%20prompt%20to%20obtain%20the%20local%20background%0Asimilarity%20containing%20both%20the%20background%20and%20image%20semantic%20information%2C%20and%0Athen%20refine%20the%20local%20background%20similarity%20using%20the%20local%20class%20similarity.%0AAs%20a%20result%2C%20we%20use%20both%20the%20refined%20local%20background%20similarity%20and%20the%20local%0Aclass%20similarity%20to%20conduct%20background%20extraction%2C%20reducing%20the%20dependence%20of%0Athe%20local%20class%20similarity%20in%20previous%20methods.%20Furthermore%2C%20we%20propose%20the%0Apatch%20self-calibrated%20tuning%20to%20consider%20the%20sample%20diversity%20to%20flexibly%0Aselect%20numbers%20of%20background%20patches%20for%20different%20samples%2C%20and%20thus%20exploring%0Athe%20issue%20of%20fixed%20background%20extraction%20strategies%20in%20previous%20methods.%0AExtensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20proposed%0AMambo%20achieves%20the%20best%20performance%2C%20compared%20to%20SOTA%20methods%20in%20terms%20of%20OOD%0Adetection%20and%20near%20OOD%20detection%20setting.%20The%20source%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/YuzunoKawori/Mambo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21055v1&entry.124074799=Read"},
{"title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect\n  for Accurate Edge Detection", "author": "Chenyu Zhao and Jingao Xu and Ciyu Ruan and Haoyang Wang and Shengbo Wang and Jiaqi Li and Jirong Zha and Weijie Hong and Zheng Yang and Yunhao Liu and Xiao-Ping Zhang and Xinlei Chen", "abstract": "  Drone-based rapid and accurate environmental edge detection is highly\nadvantageous for tasks such as disaster relief and autonomous navigation.\nCurrent methods, using radars or cameras, raise deployment costs and burden\nlightweight drones with high computational demands. In this paper, we propose\nAirTouch, a system that transforms the ground effect from a stability \"foe\" in\ntraditional flight control views, into a \"friend\" for accurate and efficient\nedge detection. Our key insight is that analyzing drone basic attitude sensor\nreadings and flight commands allows us to detect ground effect changes. Such\nchanges typically indicate the drone flying over a boundary of two materials,\nmaking this information valuable for edge detection. We approach this insight\nthrough theoretical analysis, algorithm design, and implementation, fully\nleveraging the ground effect as a new sensing modality without compromising\ndrone flight stability, thereby achieving accurate and efficient scene edge\ndetection. We also compare this new sensing modality with vision-based methods\nto clarify its exclusive advantages in resource efficiency and detection\ncapability. Extensive evaluations demonstrate that our system achieves a high\ndetection accuracy with mean detection distance errors of 0.051m, outperforming\nthe baseline method performance by 86%. With such detection performance, our\nsystem requires only 43 mW power consumption, contributing to this new sensing\nmodality for low-cost and highly efficient edge detection.\n", "link": "http://arxiv.org/abs/2509.21085v1", "date": "2025-09-25", "relevancy": 1.4859, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5031}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flight%20Dynamics%20to%20Sensing%20Modalities%3A%20Exploiting%20Drone%20Ground%20Effect%0A%20%20for%20Accurate%20Edge%20Detection&body=Title%3A%20Flight%20Dynamics%20to%20Sensing%20Modalities%3A%20Exploiting%20Drone%20Ground%20Effect%0A%20%20for%20Accurate%20Edge%20Detection%0AAuthor%3A%20Chenyu%20Zhao%20and%20Jingao%20Xu%20and%20Ciyu%20Ruan%20and%20Haoyang%20Wang%20and%20Shengbo%20Wang%20and%20Jiaqi%20Li%20and%20Jirong%20Zha%20and%20Weijie%20Hong%20and%20Zheng%20Yang%20and%20Yunhao%20Liu%20and%20Xiao-Ping%20Zhang%20and%20Xinlei%20Chen%0AAbstract%3A%20%20%20Drone-based%20rapid%20and%20accurate%20environmental%20edge%20detection%20is%20highly%0Aadvantageous%20for%20tasks%20such%20as%20disaster%20relief%20and%20autonomous%20navigation.%0ACurrent%20methods%2C%20using%20radars%20or%20cameras%2C%20raise%20deployment%20costs%20and%20burden%0Alightweight%20drones%20with%20high%20computational%20demands.%20In%20this%20paper%2C%20we%20propose%0AAirTouch%2C%20a%20system%20that%20transforms%20the%20ground%20effect%20from%20a%20stability%20%22foe%22%20in%0Atraditional%20flight%20control%20views%2C%20into%20a%20%22friend%22%20for%20accurate%20and%20efficient%0Aedge%20detection.%20Our%20key%20insight%20is%20that%20analyzing%20drone%20basic%20attitude%20sensor%0Areadings%20and%20flight%20commands%20allows%20us%20to%20detect%20ground%20effect%20changes.%20Such%0Achanges%20typically%20indicate%20the%20drone%20flying%20over%20a%20boundary%20of%20two%20materials%2C%0Amaking%20this%20information%20valuable%20for%20edge%20detection.%20We%20approach%20this%20insight%0Athrough%20theoretical%20analysis%2C%20algorithm%20design%2C%20and%20implementation%2C%20fully%0Aleveraging%20the%20ground%20effect%20as%20a%20new%20sensing%20modality%20without%20compromising%0Adrone%20flight%20stability%2C%20thereby%20achieving%20accurate%20and%20efficient%20scene%20edge%0Adetection.%20We%20also%20compare%20this%20new%20sensing%20modality%20with%20vision-based%20methods%0Ato%20clarify%20its%20exclusive%20advantages%20in%20resource%20efficiency%20and%20detection%0Acapability.%20Extensive%20evaluations%20demonstrate%20that%20our%20system%20achieves%20a%20high%0Adetection%20accuracy%20with%20mean%20detection%20distance%20errors%20of%200.051m%2C%20outperforming%0Athe%20baseline%20method%20performance%20by%2086%25.%20With%20such%20detection%20performance%2C%20our%0Asystem%20requires%20only%2043%20mW%20power%20consumption%2C%20contributing%20to%20this%20new%20sensing%0Amodality%20for%20low-cost%20and%20highly%20efficient%20edge%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlight%2520Dynamics%2520to%2520Sensing%2520Modalities%253A%2520Exploiting%2520Drone%2520Ground%2520Effect%250A%2520%2520for%2520Accurate%2520Edge%2520Detection%26entry.906535625%3DChenyu%2520Zhao%2520and%2520Jingao%2520Xu%2520and%2520Ciyu%2520Ruan%2520and%2520Haoyang%2520Wang%2520and%2520Shengbo%2520Wang%2520and%2520Jiaqi%2520Li%2520and%2520Jirong%2520Zha%2520and%2520Weijie%2520Hong%2520and%2520Zheng%2520Yang%2520and%2520Yunhao%2520Liu%2520and%2520Xiao-Ping%2520Zhang%2520and%2520Xinlei%2520Chen%26entry.1292438233%3D%2520%2520Drone-based%2520rapid%2520and%2520accurate%2520environmental%2520edge%2520detection%2520is%2520highly%250Aadvantageous%2520for%2520tasks%2520such%2520as%2520disaster%2520relief%2520and%2520autonomous%2520navigation.%250ACurrent%2520methods%252C%2520using%2520radars%2520or%2520cameras%252C%2520raise%2520deployment%2520costs%2520and%2520burden%250Alightweight%2520drones%2520with%2520high%2520computational%2520demands.%2520In%2520this%2520paper%252C%2520we%2520propose%250AAirTouch%252C%2520a%2520system%2520that%2520transforms%2520the%2520ground%2520effect%2520from%2520a%2520stability%2520%2522foe%2522%2520in%250Atraditional%2520flight%2520control%2520views%252C%2520into%2520a%2520%2522friend%2522%2520for%2520accurate%2520and%2520efficient%250Aedge%2520detection.%2520Our%2520key%2520insight%2520is%2520that%2520analyzing%2520drone%2520basic%2520attitude%2520sensor%250Areadings%2520and%2520flight%2520commands%2520allows%2520us%2520to%2520detect%2520ground%2520effect%2520changes.%2520Such%250Achanges%2520typically%2520indicate%2520the%2520drone%2520flying%2520over%2520a%2520boundary%2520of%2520two%2520materials%252C%250Amaking%2520this%2520information%2520valuable%2520for%2520edge%2520detection.%2520We%2520approach%2520this%2520insight%250Athrough%2520theoretical%2520analysis%252C%2520algorithm%2520design%252C%2520and%2520implementation%252C%2520fully%250Aleveraging%2520the%2520ground%2520effect%2520as%2520a%2520new%2520sensing%2520modality%2520without%2520compromising%250Adrone%2520flight%2520stability%252C%2520thereby%2520achieving%2520accurate%2520and%2520efficient%2520scene%2520edge%250Adetection.%2520We%2520also%2520compare%2520this%2520new%2520sensing%2520modality%2520with%2520vision-based%2520methods%250Ato%2520clarify%2520its%2520exclusive%2520advantages%2520in%2520resource%2520efficiency%2520and%2520detection%250Acapability.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520our%2520system%2520achieves%2520a%2520high%250Adetection%2520accuracy%2520with%2520mean%2520detection%2520distance%2520errors%2520of%25200.051m%252C%2520outperforming%250Athe%2520baseline%2520method%2520performance%2520by%252086%2525.%2520With%2520such%2520detection%2520performance%252C%2520our%250Asystem%2520requires%2520only%252043%2520mW%2520power%2520consumption%252C%2520contributing%2520to%2520this%2520new%2520sensing%250Amodality%2520for%2520low-cost%2520and%2520highly%2520efficient%2520edge%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flight%20Dynamics%20to%20Sensing%20Modalities%3A%20Exploiting%20Drone%20Ground%20Effect%0A%20%20for%20Accurate%20Edge%20Detection&entry.906535625=Chenyu%20Zhao%20and%20Jingao%20Xu%20and%20Ciyu%20Ruan%20and%20Haoyang%20Wang%20and%20Shengbo%20Wang%20and%20Jiaqi%20Li%20and%20Jirong%20Zha%20and%20Weijie%20Hong%20and%20Zheng%20Yang%20and%20Yunhao%20Liu%20and%20Xiao-Ping%20Zhang%20and%20Xinlei%20Chen&entry.1292438233=%20%20Drone-based%20rapid%20and%20accurate%20environmental%20edge%20detection%20is%20highly%0Aadvantageous%20for%20tasks%20such%20as%20disaster%20relief%20and%20autonomous%20navigation.%0ACurrent%20methods%2C%20using%20radars%20or%20cameras%2C%20raise%20deployment%20costs%20and%20burden%0Alightweight%20drones%20with%20high%20computational%20demands.%20In%20this%20paper%2C%20we%20propose%0AAirTouch%2C%20a%20system%20that%20transforms%20the%20ground%20effect%20from%20a%20stability%20%22foe%22%20in%0Atraditional%20flight%20control%20views%2C%20into%20a%20%22friend%22%20for%20accurate%20and%20efficient%0Aedge%20detection.%20Our%20key%20insight%20is%20that%20analyzing%20drone%20basic%20attitude%20sensor%0Areadings%20and%20flight%20commands%20allows%20us%20to%20detect%20ground%20effect%20changes.%20Such%0Achanges%20typically%20indicate%20the%20drone%20flying%20over%20a%20boundary%20of%20two%20materials%2C%0Amaking%20this%20information%20valuable%20for%20edge%20detection.%20We%20approach%20this%20insight%0Athrough%20theoretical%20analysis%2C%20algorithm%20design%2C%20and%20implementation%2C%20fully%0Aleveraging%20the%20ground%20effect%20as%20a%20new%20sensing%20modality%20without%20compromising%0Adrone%20flight%20stability%2C%20thereby%20achieving%20accurate%20and%20efficient%20scene%20edge%0Adetection.%20We%20also%20compare%20this%20new%20sensing%20modality%20with%20vision-based%20methods%0Ato%20clarify%20its%20exclusive%20advantages%20in%20resource%20efficiency%20and%20detection%0Acapability.%20Extensive%20evaluations%20demonstrate%20that%20our%20system%20achieves%20a%20high%0Adetection%20accuracy%20with%20mean%20detection%20distance%20errors%20of%200.051m%2C%20outperforming%0Athe%20baseline%20method%20performance%20by%2086%25.%20With%20such%20detection%20performance%2C%20our%0Asystem%20requires%20only%2043%20mW%20power%20consumption%2C%20contributing%20to%20this%20new%20sensing%0Amodality%20for%20low-cost%20and%20highly%20efficient%20edge%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21085v1&entry.124074799=Read"},
{"title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary\n  Results", "author": "Carlos N\u00fa\u00f1ez-Molina and Vicen\u00e7 G\u00f3mez and Hector Geffner", "abstract": "  We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.\n", "link": "http://arxiv.org/abs/2509.13389v2", "date": "2025-09-25", "relevancy": 1.5545, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5302}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Next%20Token%20Prediction%20to%20%28STRIPS%29%20World%20Models%20--%20Preliminary%0A%20%20Results&body=Title%3A%20From%20Next%20Token%20Prediction%20to%20%28STRIPS%29%20World%20Models%20--%20Preliminary%0A%20%20Results%0AAuthor%3A%20Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Vicen%C3%A7%20G%C3%B3mez%20and%20Hector%20Geffner%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20propositional%20STRIPS%20world%20models%20from%0Aaction%20traces%20alone%2C%20using%20a%20deep%20learning%20architecture%20%28transformers%29%20and%0Agradient%20descent.%20The%20task%20is%20cast%20as%20a%20supervised%20next%20token%20prediction%0Aproblem%20where%20the%20tokens%20are%20the%20actions%2C%20and%20an%20action%20%24a%24%20may%20follow%20an%0Aaction%20sequence%20if%20the%20hidden%20effects%20of%20the%20previous%20actions%20do%20not%20make%20an%0Aaction%20precondition%20of%20%24a%24%20false.%20We%20show%20that%20a%20suitable%20transformer%0Aarchitecture%20can%20faithfully%20represent%20propositional%20STRIPS%20world%20models%2C%20and%0Athat%20the%20models%20can%20be%20learned%20from%20sets%20of%20random%20valid%20%28positive%29%20and%20invalid%0A%28negative%29%20action%20sequences%20alone.%20A%20number%20of%20experiments%20are%20reported.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Next%2520Token%2520Prediction%2520to%2520%2528STRIPS%2529%2520World%2520Models%2520--%2520Preliminary%250A%2520%2520Results%26entry.906535625%3DCarlos%2520N%25C3%25BA%25C3%25B1ez-Molina%2520and%2520Vicen%25C3%25A7%2520G%25C3%25B3mez%2520and%2520Hector%2520Geffner%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520propositional%2520STRIPS%2520world%2520models%2520from%250Aaction%2520traces%2520alone%252C%2520using%2520a%2520deep%2520learning%2520architecture%2520%2528transformers%2529%2520and%250Agradient%2520descent.%2520The%2520task%2520is%2520cast%2520as%2520a%2520supervised%2520next%2520token%2520prediction%250Aproblem%2520where%2520the%2520tokens%2520are%2520the%2520actions%252C%2520and%2520an%2520action%2520%2524a%2524%2520may%2520follow%2520an%250Aaction%2520sequence%2520if%2520the%2520hidden%2520effects%2520of%2520the%2520previous%2520actions%2520do%2520not%2520make%2520an%250Aaction%2520precondition%2520of%2520%2524a%2524%2520false.%2520We%2520show%2520that%2520a%2520suitable%2520transformer%250Aarchitecture%2520can%2520faithfully%2520represent%2520propositional%2520STRIPS%2520world%2520models%252C%2520and%250Athat%2520the%2520models%2520can%2520be%2520learned%2520from%2520sets%2520of%2520random%2520valid%2520%2528positive%2529%2520and%2520invalid%250A%2528negative%2529%2520action%2520sequences%2520alone.%2520A%2520number%2520of%2520experiments%2520are%2520reported.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Next%20Token%20Prediction%20to%20%28STRIPS%29%20World%20Models%20--%20Preliminary%0A%20%20Results&entry.906535625=Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Vicen%C3%A7%20G%C3%B3mez%20and%20Hector%20Geffner&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20propositional%20STRIPS%20world%20models%20from%0Aaction%20traces%20alone%2C%20using%20a%20deep%20learning%20architecture%20%28transformers%29%20and%0Agradient%20descent.%20The%20task%20is%20cast%20as%20a%20supervised%20next%20token%20prediction%0Aproblem%20where%20the%20tokens%20are%20the%20actions%2C%20and%20an%20action%20%24a%24%20may%20follow%20an%0Aaction%20sequence%20if%20the%20hidden%20effects%20of%20the%20previous%20actions%20do%20not%20make%20an%0Aaction%20precondition%20of%20%24a%24%20false.%20We%20show%20that%20a%20suitable%20transformer%0Aarchitecture%20can%20faithfully%20represent%20propositional%20STRIPS%20world%20models%2C%20and%0Athat%20the%20models%20can%20be%20learned%20from%20sets%20of%20random%20valid%20%28positive%29%20and%20invalid%0A%28negative%29%20action%20sequences%20alone.%20A%20number%20of%20experiments%20are%20reported.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13389v2&entry.124074799=Read"},
{"title": "Generalizing while preserving monotonicity in comparison-based\n  preference learning models", "author": "Julien Fageot and Peva Blanchard and Gilles Bareilles and L\u00ea-Nguy\u00ean Hoang", "abstract": "  If you tell a learning model that you prefer an alternative $a$ over another\nalternative $b$, then you probably expect the model to be monotone, that is,\nthe valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps\nsurprisingly, many widely deployed comparison-based preference learning models,\nincluding large language models, fail to have this guarantee. Until now, the\nonly comparison-based preference learning algorithms that were proved to be\nmonotone are the Generalized Bradley-Terry models. Yet, these models are unable\nto generalize to uncompared data. In this paper, we advance the understanding\nof the set of models with generalization ability that are monotone. Namely, we\npropose a new class of Linear Generalized Bradley-Terry models with Diffusion\nPriors, and identify sufficient conditions on alternatives' embeddings that\nguarantee monotonicity. Our experiments show that this monotonicity is far from\nbeing a general guarantee, and that our new class of generalizing models\nimproves accuracy, especially when the dataset is limited.\n", "link": "http://arxiv.org/abs/2506.08616v2", "date": "2025-09-25", "relevancy": 1.4115, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20while%20preserving%20monotonicity%20in%20comparison-based%0A%20%20preference%20learning%20models&body=Title%3A%20Generalizing%20while%20preserving%20monotonicity%20in%20comparison-based%0A%20%20preference%20learning%20models%0AAuthor%3A%20Julien%20Fageot%20and%20Peva%20Blanchard%20and%20Gilles%20Bareilles%20and%20L%C3%AA-Nguy%C3%AAn%20Hoang%0AAbstract%3A%20%20%20If%20you%20tell%20a%20learning%20model%20that%20you%20prefer%20an%20alternative%20%24a%24%20over%20another%0Aalternative%20%24b%24%2C%20then%20you%20probably%20expect%20the%20model%20to%20be%20monotone%2C%20that%20is%2C%0Athe%20valuation%20of%20%24a%24%20increases%2C%20and%20that%20of%20%24b%24%20decreases.%20Yet%2C%20perhaps%0Asurprisingly%2C%20many%20widely%20deployed%20comparison-based%20preference%20learning%20models%2C%0Aincluding%20large%20language%20models%2C%20fail%20to%20have%20this%20guarantee.%20Until%20now%2C%20the%0Aonly%20comparison-based%20preference%20learning%20algorithms%20that%20were%20proved%20to%20be%0Amonotone%20are%20the%20Generalized%20Bradley-Terry%20models.%20Yet%2C%20these%20models%20are%20unable%0Ato%20generalize%20to%20uncompared%20data.%20In%20this%20paper%2C%20we%20advance%20the%20understanding%0Aof%20the%20set%20of%20models%20with%20generalization%20ability%20that%20are%20monotone.%20Namely%2C%20we%0Apropose%20a%20new%20class%20of%20Linear%20Generalized%20Bradley-Terry%20models%20with%20Diffusion%0APriors%2C%20and%20identify%20sufficient%20conditions%20on%20alternatives%27%20embeddings%20that%0Aguarantee%20monotonicity.%20Our%20experiments%20show%20that%20this%20monotonicity%20is%20far%20from%0Abeing%20a%20general%20guarantee%2C%20and%20that%20our%20new%20class%20of%20generalizing%20models%0Aimproves%20accuracy%2C%20especially%20when%20the%20dataset%20is%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520while%2520preserving%2520monotonicity%2520in%2520comparison-based%250A%2520%2520preference%2520learning%2520models%26entry.906535625%3DJulien%2520Fageot%2520and%2520Peva%2520Blanchard%2520and%2520Gilles%2520Bareilles%2520and%2520L%25C3%25AA-Nguy%25C3%25AAn%2520Hoang%26entry.1292438233%3D%2520%2520If%2520you%2520tell%2520a%2520learning%2520model%2520that%2520you%2520prefer%2520an%2520alternative%2520%2524a%2524%2520over%2520another%250Aalternative%2520%2524b%2524%252C%2520then%2520you%2520probably%2520expect%2520the%2520model%2520to%2520be%2520monotone%252C%2520that%2520is%252C%250Athe%2520valuation%2520of%2520%2524a%2524%2520increases%252C%2520and%2520that%2520of%2520%2524b%2524%2520decreases.%2520Yet%252C%2520perhaps%250Asurprisingly%252C%2520many%2520widely%2520deployed%2520comparison-based%2520preference%2520learning%2520models%252C%250Aincluding%2520large%2520language%2520models%252C%2520fail%2520to%2520have%2520this%2520guarantee.%2520Until%2520now%252C%2520the%250Aonly%2520comparison-based%2520preference%2520learning%2520algorithms%2520that%2520were%2520proved%2520to%2520be%250Amonotone%2520are%2520the%2520Generalized%2520Bradley-Terry%2520models.%2520Yet%252C%2520these%2520models%2520are%2520unable%250Ato%2520generalize%2520to%2520uncompared%2520data.%2520In%2520this%2520paper%252C%2520we%2520advance%2520the%2520understanding%250Aof%2520the%2520set%2520of%2520models%2520with%2520generalization%2520ability%2520that%2520are%2520monotone.%2520Namely%252C%2520we%250Apropose%2520a%2520new%2520class%2520of%2520Linear%2520Generalized%2520Bradley-Terry%2520models%2520with%2520Diffusion%250APriors%252C%2520and%2520identify%2520sufficient%2520conditions%2520on%2520alternatives%2527%2520embeddings%2520that%250Aguarantee%2520monotonicity.%2520Our%2520experiments%2520show%2520that%2520this%2520monotonicity%2520is%2520far%2520from%250Abeing%2520a%2520general%2520guarantee%252C%2520and%2520that%2520our%2520new%2520class%2520of%2520generalizing%2520models%250Aimproves%2520accuracy%252C%2520especially%2520when%2520the%2520dataset%2520is%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20while%20preserving%20monotonicity%20in%20comparison-based%0A%20%20preference%20learning%20models&entry.906535625=Julien%20Fageot%20and%20Peva%20Blanchard%20and%20Gilles%20Bareilles%20and%20L%C3%AA-Nguy%C3%AAn%20Hoang&entry.1292438233=%20%20If%20you%20tell%20a%20learning%20model%20that%20you%20prefer%20an%20alternative%20%24a%24%20over%20another%0Aalternative%20%24b%24%2C%20then%20you%20probably%20expect%20the%20model%20to%20be%20monotone%2C%20that%20is%2C%0Athe%20valuation%20of%20%24a%24%20increases%2C%20and%20that%20of%20%24b%24%20decreases.%20Yet%2C%20perhaps%0Asurprisingly%2C%20many%20widely%20deployed%20comparison-based%20preference%20learning%20models%2C%0Aincluding%20large%20language%20models%2C%20fail%20to%20have%20this%20guarantee.%20Until%20now%2C%20the%0Aonly%20comparison-based%20preference%20learning%20algorithms%20that%20were%20proved%20to%20be%0Amonotone%20are%20the%20Generalized%20Bradley-Terry%20models.%20Yet%2C%20these%20models%20are%20unable%0Ato%20generalize%20to%20uncompared%20data.%20In%20this%20paper%2C%20we%20advance%20the%20understanding%0Aof%20the%20set%20of%20models%20with%20generalization%20ability%20that%20are%20monotone.%20Namely%2C%20we%0Apropose%20a%20new%20class%20of%20Linear%20Generalized%20Bradley-Terry%20models%20with%20Diffusion%0APriors%2C%20and%20identify%20sufficient%20conditions%20on%20alternatives%27%20embeddings%20that%0Aguarantee%20monotonicity.%20Our%20experiments%20show%20that%20this%20monotonicity%20is%20far%20from%0Abeing%20a%20general%20guarantee%2C%20and%20that%20our%20new%20class%20of%20generalizing%20models%0Aimproves%20accuracy%2C%20especially%20when%20the%20dataset%20is%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08616v2&entry.124074799=Read"},
{"title": "Inverse Reinforcement Learning Using Just Classification and a Few\n  Regressions", "author": "Lars van der Laan and Nathan Kallus and Aur\u00e9lien Bibaut", "abstract": "  Inverse reinforcement learning (IRL) aims to explain observed behavior by\nuncovering an underlying reward. In the maximum-entropy or\nGumbel-shocks-to-reward frameworks, this amounts to fitting a reward function\nand a soft value function that together satisfy the soft Bellman consistency\ncondition and maximize the likelihood of observed actions. While this\nperspective has had enormous impact in imitation learning for robotics and\nunderstanding dynamic choices in economics, practical learning algorithms often\ninvolve delicate inner-loop optimization, repeated dynamic programming, or\nadversarial training, all of which complicate the use of modern, highly\nexpressive function approximators like neural nets and boosting. We revisit\nsoftmax IRL and show that the population maximum-likelihood solution is\ncharacterized by a linear fixed-point equation involving the behavior policy.\nThis observation reduces IRL to two off-the-shelf supervised learning problems:\nprobabilistic classification to estimate the behavior policy, and iterative\nregression to solve the fixed point. The resulting method is simple and modular\nacross function approximation classes and algorithms. We provide a precise\ncharacterization of the optimal solution, a generic oracle-based algorithm,\nfinite-sample error bounds, and empirical results showing competitive or\nsuperior performance to MaxEnt IRL.\n", "link": "http://arxiv.org/abs/2509.21172v1", "date": "2025-09-25", "relevancy": 1.8375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4821}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4619}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Reinforcement%20Learning%20Using%20Just%20Classification%20and%20a%20Few%0A%20%20Regressions&body=Title%3A%20Inverse%20Reinforcement%20Learning%20Using%20Just%20Classification%20and%20a%20Few%0A%20%20Regressions%0AAuthor%3A%20Lars%20van%20der%20Laan%20and%20Nathan%20Kallus%20and%20Aur%C3%A9lien%20Bibaut%0AAbstract%3A%20%20%20Inverse%20reinforcement%20learning%20%28IRL%29%20aims%20to%20explain%20observed%20behavior%20by%0Auncovering%20an%20underlying%20reward.%20In%20the%20maximum-entropy%20or%0AGumbel-shocks-to-reward%20frameworks%2C%20this%20amounts%20to%20fitting%20a%20reward%20function%0Aand%20a%20soft%20value%20function%20that%20together%20satisfy%20the%20soft%20Bellman%20consistency%0Acondition%20and%20maximize%20the%20likelihood%20of%20observed%20actions.%20While%20this%0Aperspective%20has%20had%20enormous%20impact%20in%20imitation%20learning%20for%20robotics%20and%0Aunderstanding%20dynamic%20choices%20in%20economics%2C%20practical%20learning%20algorithms%20often%0Ainvolve%20delicate%20inner-loop%20optimization%2C%20repeated%20dynamic%20programming%2C%20or%0Aadversarial%20training%2C%20all%20of%20which%20complicate%20the%20use%20of%20modern%2C%20highly%0Aexpressive%20function%20approximators%20like%20neural%20nets%20and%20boosting.%20We%20revisit%0Asoftmax%20IRL%20and%20show%20that%20the%20population%20maximum-likelihood%20solution%20is%0Acharacterized%20by%20a%20linear%20fixed-point%20equation%20involving%20the%20behavior%20policy.%0AThis%20observation%20reduces%20IRL%20to%20two%20off-the-shelf%20supervised%20learning%20problems%3A%0Aprobabilistic%20classification%20to%20estimate%20the%20behavior%20policy%2C%20and%20iterative%0Aregression%20to%20solve%20the%20fixed%20point.%20The%20resulting%20method%20is%20simple%20and%20modular%0Aacross%20function%20approximation%20classes%20and%20algorithms.%20We%20provide%20a%20precise%0Acharacterization%20of%20the%20optimal%20solution%2C%20a%20generic%20oracle-based%20algorithm%2C%0Afinite-sample%20error%20bounds%2C%20and%20empirical%20results%20showing%20competitive%20or%0Asuperior%20performance%20to%20MaxEnt%20IRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Reinforcement%2520Learning%2520Using%2520Just%2520Classification%2520and%2520a%2520Few%250A%2520%2520Regressions%26entry.906535625%3DLars%2520van%2520der%2520Laan%2520and%2520Nathan%2520Kallus%2520and%2520Aur%25C3%25A9lien%2520Bibaut%26entry.1292438233%3D%2520%2520Inverse%2520reinforcement%2520learning%2520%2528IRL%2529%2520aims%2520to%2520explain%2520observed%2520behavior%2520by%250Auncovering%2520an%2520underlying%2520reward.%2520In%2520the%2520maximum-entropy%2520or%250AGumbel-shocks-to-reward%2520frameworks%252C%2520this%2520amounts%2520to%2520fitting%2520a%2520reward%2520function%250Aand%2520a%2520soft%2520value%2520function%2520that%2520together%2520satisfy%2520the%2520soft%2520Bellman%2520consistency%250Acondition%2520and%2520maximize%2520the%2520likelihood%2520of%2520observed%2520actions.%2520While%2520this%250Aperspective%2520has%2520had%2520enormous%2520impact%2520in%2520imitation%2520learning%2520for%2520robotics%2520and%250Aunderstanding%2520dynamic%2520choices%2520in%2520economics%252C%2520practical%2520learning%2520algorithms%2520often%250Ainvolve%2520delicate%2520inner-loop%2520optimization%252C%2520repeated%2520dynamic%2520programming%252C%2520or%250Aadversarial%2520training%252C%2520all%2520of%2520which%2520complicate%2520the%2520use%2520of%2520modern%252C%2520highly%250Aexpressive%2520function%2520approximators%2520like%2520neural%2520nets%2520and%2520boosting.%2520We%2520revisit%250Asoftmax%2520IRL%2520and%2520show%2520that%2520the%2520population%2520maximum-likelihood%2520solution%2520is%250Acharacterized%2520by%2520a%2520linear%2520fixed-point%2520equation%2520involving%2520the%2520behavior%2520policy.%250AThis%2520observation%2520reduces%2520IRL%2520to%2520two%2520off-the-shelf%2520supervised%2520learning%2520problems%253A%250Aprobabilistic%2520classification%2520to%2520estimate%2520the%2520behavior%2520policy%252C%2520and%2520iterative%250Aregression%2520to%2520solve%2520the%2520fixed%2520point.%2520The%2520resulting%2520method%2520is%2520simple%2520and%2520modular%250Aacross%2520function%2520approximation%2520classes%2520and%2520algorithms.%2520We%2520provide%2520a%2520precise%250Acharacterization%2520of%2520the%2520optimal%2520solution%252C%2520a%2520generic%2520oracle-based%2520algorithm%252C%250Afinite-sample%2520error%2520bounds%252C%2520and%2520empirical%2520results%2520showing%2520competitive%2520or%250Asuperior%2520performance%2520to%2520MaxEnt%2520IRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Reinforcement%20Learning%20Using%20Just%20Classification%20and%20a%20Few%0A%20%20Regressions&entry.906535625=Lars%20van%20der%20Laan%20and%20Nathan%20Kallus%20and%20Aur%C3%A9lien%20Bibaut&entry.1292438233=%20%20Inverse%20reinforcement%20learning%20%28IRL%29%20aims%20to%20explain%20observed%20behavior%20by%0Auncovering%20an%20underlying%20reward.%20In%20the%20maximum-entropy%20or%0AGumbel-shocks-to-reward%20frameworks%2C%20this%20amounts%20to%20fitting%20a%20reward%20function%0Aand%20a%20soft%20value%20function%20that%20together%20satisfy%20the%20soft%20Bellman%20consistency%0Acondition%20and%20maximize%20the%20likelihood%20of%20observed%20actions.%20While%20this%0Aperspective%20has%20had%20enormous%20impact%20in%20imitation%20learning%20for%20robotics%20and%0Aunderstanding%20dynamic%20choices%20in%20economics%2C%20practical%20learning%20algorithms%20often%0Ainvolve%20delicate%20inner-loop%20optimization%2C%20repeated%20dynamic%20programming%2C%20or%0Aadversarial%20training%2C%20all%20of%20which%20complicate%20the%20use%20of%20modern%2C%20highly%0Aexpressive%20function%20approximators%20like%20neural%20nets%20and%20boosting.%20We%20revisit%0Asoftmax%20IRL%20and%20show%20that%20the%20population%20maximum-likelihood%20solution%20is%0Acharacterized%20by%20a%20linear%20fixed-point%20equation%20involving%20the%20behavior%20policy.%0AThis%20observation%20reduces%20IRL%20to%20two%20off-the-shelf%20supervised%20learning%20problems%3A%0Aprobabilistic%20classification%20to%20estimate%20the%20behavior%20policy%2C%20and%20iterative%0Aregression%20to%20solve%20the%20fixed%20point.%20The%20resulting%20method%20is%20simple%20and%20modular%0Aacross%20function%20approximation%20classes%20and%20algorithms.%20We%20provide%20a%20precise%0Acharacterization%20of%20the%20optimal%20solution%2C%20a%20generic%20oracle-based%20algorithm%2C%0Afinite-sample%20error%20bounds%2C%20and%20empirical%20results%20showing%20competitive%20or%0Asuperior%20performance%20to%20MaxEnt%20IRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21172v1&entry.124074799=Read"},
{"title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms", "author": "Bingxin Xu and Zhen Dong and Oussama Elachqar and Yuzhang Shang", "abstract": "  Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.\n", "link": "http://arxiv.org/abs/2509.09679v2", "date": "2025-09-25", "relevancy": 1.8994, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4893}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ButterflyQuant%3A%20Ultra-low-bit%20LLM%20Quantization%20through%20Learnable%0A%20%20Orthogonal%20Butterfly%20Transforms&body=Title%3A%20ButterflyQuant%3A%20Ultra-low-bit%20LLM%20Quantization%20through%20Learnable%0A%20%20Orthogonal%20Butterfly%20Transforms%0AAuthor%3A%20Bingxin%20Xu%20and%20Zhen%20Dong%20and%20Oussama%20Elachqar%20and%20Yuzhang%20Shang%0AAbstract%3A%20%20%20Large%20language%20models%20require%20massive%20memory%20footprints%2C%20severely%20limiting%0Adeployment%20on%20consumer%20hardware.%20Quantization%20reduces%20memory%20through%20lower%0Anumerical%20precision%2C%20but%20extreme%202-bit%20quantization%20suffers%20from%20catastrophic%0Aperformance%20loss%20due%20to%20outliers%20in%20activations.%20Rotation-based%20methods%20such%20as%0AQuIP%20and%20QuaRot%20apply%20orthogonal%20transforms%20to%20eliminate%20outliers%20before%0Aquantization%2C%20using%20computational%20invariance%3A%20%24%5Cmathbf%7By%7D%20%3D%20%5Cmathbf%7BWx%7D%20%3D%0A%28%5Cmathbf%7BWQ%7D%5ET%29%28%5Cmathbf%7BQx%7D%29%24%20for%20orthogonal%20%24%5Cmathbf%7BQ%7D%24.%20However%2C%20these%0Amethods%20use%20fixed%20transforms--Hadamard%20matrices%20achieving%20optimal%20worst-case%0Acoherence%20%24%5Cmu%20%3D%201/%5Csqrt%7Bn%7D%24--that%20cannot%20adapt%20to%20specific%20weight%0Adistributions.%20We%20identify%20that%20different%20transformer%20layers%20exhibit%20distinct%0Aoutlier%20patterns%2C%20motivating%20layer-adaptive%20rotations%20rather%20than%0Aone-size-fits-all%20approaches.%20In%20this%20work%2C%20we%20propose%20ButterflyQuant%2C%20which%0Areplaces%20Hadamard%20rotations%20with%20learnable%20butterfly%20transforms%20parameterized%0Aby%20continuous%20Givens%20rotation%20angles.%20Unlike%20Hadamard%27s%20discrete%20%24%5C%7B%2B1%2C%20-1%5C%7D%24%0Aentries%20that%20are%20non-differentiable%20and%20thus%20prohibit%20gradient-based%20learning%2C%0Abutterfly%20transforms%27%20continuous%20parameterization%20enables%20smooth%20optimization%0Awhile%20guaranteeing%20orthogonality%20by%20construction.%20This%20orthogonal%20constraint%0Aensures%20theoretical%20guarantees%20in%20outlier%20suppression%20while%20achieving%20%24O%28n%20%5Clog%0An%29%24%20computational%20complexity%20with%20only%20%24%5Cfrac%7Bn%20%5Clog%20n%7D%7B2%7D%24%20learnable%0Aparameters.%20We%20further%20introduce%20a%20uniformity%20regularization%20on%0Apost-transformation%20activations%20to%20promote%20smoother%20distributions%20amenable%20to%0Aquantization.%20Learning%20requires%20only%20128%20calibration%20samples%20and%20converges%20in%0Aminutes%20on%20a%20single%20GPU--a%20negligible%20one-time%20cost.%20For%20LLaMA-2-7B%20with%202-bit%0Aquantization%2C%20ButterflyQuant%20achieves%2015.4%20perplexity%20versus%2037.3%20for%20QuIP.%0A%5Chref%7Bhttps%3A//github.com/42Shawn/Butterflyquant-llm%7D%7BCodes%7D%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DButterflyQuant%253A%2520Ultra-low-bit%2520LLM%2520Quantization%2520through%2520Learnable%250A%2520%2520Orthogonal%2520Butterfly%2520Transforms%26entry.906535625%3DBingxin%2520Xu%2520and%2520Zhen%2520Dong%2520and%2520Oussama%2520Elachqar%2520and%2520Yuzhang%2520Shang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520require%2520massive%2520memory%2520footprints%252C%2520severely%2520limiting%250Adeployment%2520on%2520consumer%2520hardware.%2520Quantization%2520reduces%2520memory%2520through%2520lower%250Anumerical%2520precision%252C%2520but%2520extreme%25202-bit%2520quantization%2520suffers%2520from%2520catastrophic%250Aperformance%2520loss%2520due%2520to%2520outliers%2520in%2520activations.%2520Rotation-based%2520methods%2520such%2520as%250AQuIP%2520and%2520QuaRot%2520apply%2520orthogonal%2520transforms%2520to%2520eliminate%2520outliers%2520before%250Aquantization%252C%2520using%2520computational%2520invariance%253A%2520%2524%255Cmathbf%257By%257D%2520%253D%2520%255Cmathbf%257BWx%257D%2520%253D%250A%2528%255Cmathbf%257BWQ%257D%255ET%2529%2528%255Cmathbf%257BQx%257D%2529%2524%2520for%2520orthogonal%2520%2524%255Cmathbf%257BQ%257D%2524.%2520However%252C%2520these%250Amethods%2520use%2520fixed%2520transforms--Hadamard%2520matrices%2520achieving%2520optimal%2520worst-case%250Acoherence%2520%2524%255Cmu%2520%253D%25201/%255Csqrt%257Bn%257D%2524--that%2520cannot%2520adapt%2520to%2520specific%2520weight%250Adistributions.%2520We%2520identify%2520that%2520different%2520transformer%2520layers%2520exhibit%2520distinct%250Aoutlier%2520patterns%252C%2520motivating%2520layer-adaptive%2520rotations%2520rather%2520than%250Aone-size-fits-all%2520approaches.%2520In%2520this%2520work%252C%2520we%2520propose%2520ButterflyQuant%252C%2520which%250Areplaces%2520Hadamard%2520rotations%2520with%2520learnable%2520butterfly%2520transforms%2520parameterized%250Aby%2520continuous%2520Givens%2520rotation%2520angles.%2520Unlike%2520Hadamard%2527s%2520discrete%2520%2524%255C%257B%252B1%252C%2520-1%255C%257D%2524%250Aentries%2520that%2520are%2520non-differentiable%2520and%2520thus%2520prohibit%2520gradient-based%2520learning%252C%250Abutterfly%2520transforms%2527%2520continuous%2520parameterization%2520enables%2520smooth%2520optimization%250Awhile%2520guaranteeing%2520orthogonality%2520by%2520construction.%2520This%2520orthogonal%2520constraint%250Aensures%2520theoretical%2520guarantees%2520in%2520outlier%2520suppression%2520while%2520achieving%2520%2524O%2528n%2520%255Clog%250An%2529%2524%2520computational%2520complexity%2520with%2520only%2520%2524%255Cfrac%257Bn%2520%255Clog%2520n%257D%257B2%257D%2524%2520learnable%250Aparameters.%2520We%2520further%2520introduce%2520a%2520uniformity%2520regularization%2520on%250Apost-transformation%2520activations%2520to%2520promote%2520smoother%2520distributions%2520amenable%2520to%250Aquantization.%2520Learning%2520requires%2520only%2520128%2520calibration%2520samples%2520and%2520converges%2520in%250Aminutes%2520on%2520a%2520single%2520GPU--a%2520negligible%2520one-time%2520cost.%2520For%2520LLaMA-2-7B%2520with%25202-bit%250Aquantization%252C%2520ButterflyQuant%2520achieves%252015.4%2520perplexity%2520versus%252037.3%2520for%2520QuIP.%250A%255Chref%257Bhttps%253A//github.com/42Shawn/Butterflyquant-llm%257D%257BCodes%257D%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ButterflyQuant%3A%20Ultra-low-bit%20LLM%20Quantization%20through%20Learnable%0A%20%20Orthogonal%20Butterfly%20Transforms&entry.906535625=Bingxin%20Xu%20and%20Zhen%20Dong%20and%20Oussama%20Elachqar%20and%20Yuzhang%20Shang&entry.1292438233=%20%20Large%20language%20models%20require%20massive%20memory%20footprints%2C%20severely%20limiting%0Adeployment%20on%20consumer%20hardware.%20Quantization%20reduces%20memory%20through%20lower%0Anumerical%20precision%2C%20but%20extreme%202-bit%20quantization%20suffers%20from%20catastrophic%0Aperformance%20loss%20due%20to%20outliers%20in%20activations.%20Rotation-based%20methods%20such%20as%0AQuIP%20and%20QuaRot%20apply%20orthogonal%20transforms%20to%20eliminate%20outliers%20before%0Aquantization%2C%20using%20computational%20invariance%3A%20%24%5Cmathbf%7By%7D%20%3D%20%5Cmathbf%7BWx%7D%20%3D%0A%28%5Cmathbf%7BWQ%7D%5ET%29%28%5Cmathbf%7BQx%7D%29%24%20for%20orthogonal%20%24%5Cmathbf%7BQ%7D%24.%20However%2C%20these%0Amethods%20use%20fixed%20transforms--Hadamard%20matrices%20achieving%20optimal%20worst-case%0Acoherence%20%24%5Cmu%20%3D%201/%5Csqrt%7Bn%7D%24--that%20cannot%20adapt%20to%20specific%20weight%0Adistributions.%20We%20identify%20that%20different%20transformer%20layers%20exhibit%20distinct%0Aoutlier%20patterns%2C%20motivating%20layer-adaptive%20rotations%20rather%20than%0Aone-size-fits-all%20approaches.%20In%20this%20work%2C%20we%20propose%20ButterflyQuant%2C%20which%0Areplaces%20Hadamard%20rotations%20with%20learnable%20butterfly%20transforms%20parameterized%0Aby%20continuous%20Givens%20rotation%20angles.%20Unlike%20Hadamard%27s%20discrete%20%24%5C%7B%2B1%2C%20-1%5C%7D%24%0Aentries%20that%20are%20non-differentiable%20and%20thus%20prohibit%20gradient-based%20learning%2C%0Abutterfly%20transforms%27%20continuous%20parameterization%20enables%20smooth%20optimization%0Awhile%20guaranteeing%20orthogonality%20by%20construction.%20This%20orthogonal%20constraint%0Aensures%20theoretical%20guarantees%20in%20outlier%20suppression%20while%20achieving%20%24O%28n%20%5Clog%0An%29%24%20computational%20complexity%20with%20only%20%24%5Cfrac%7Bn%20%5Clog%20n%7D%7B2%7D%24%20learnable%0Aparameters.%20We%20further%20introduce%20a%20uniformity%20regularization%20on%0Apost-transformation%20activations%20to%20promote%20smoother%20distributions%20amenable%20to%0Aquantization.%20Learning%20requires%20only%20128%20calibration%20samples%20and%20converges%20in%0Aminutes%20on%20a%20single%20GPU--a%20negligible%20one-time%20cost.%20For%20LLaMA-2-7B%20with%202-bit%0Aquantization%2C%20ButterflyQuant%20achieves%2015.4%20perplexity%20versus%2037.3%20for%20QuIP.%0A%5Chref%7Bhttps%3A//github.com/42Shawn/Butterflyquant-llm%7D%7BCodes%7D%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09679v2&entry.124074799=Read"},
{"title": "Differential-Integral Neural Operator for Long-Term Turbulence\n  Forecasting", "author": "Hao Wu and Yuan Gao and Fan Xu and Fan Zhang and Qingsong Wen and Kun Wang and Xiaomeng Huang and Xian Wu", "abstract": "  Accurately forecasting the long-term evolution of turbulence represents a\ngrand challenge in scientific computing and is crucial for applications ranging\nfrom climate modeling to aerospace engineering. Existing deep learning methods,\nparticularly neural operators, often fail in long-term autoregressive\npredictions, suffering from catastrophic error accumulation and a loss of\nphysical fidelity. This failure stems from their inability to simultaneously\ncapture the distinct mathematical structures that govern turbulent dynamics:\nlocal, dissipative effects and global, non-local interactions. In this paper,\nwe propose the\n{\\textbf{\\underline{D}}}ifferential-{\\textbf{\\underline{I}}}ntegral\n{\\textbf{\\underline{N}}}eural {\\textbf{\\underline{O}}}perator (\\method{}), a\nnovel framework designed from a first-principles approach of operator\ndecomposition. \\method{} explicitly models the turbulent evolution through\nparallel branches that learn distinct physical operators: a local differential\noperator, realized by a constrained convolutional network that provably\nconverges to a derivative, and a global integral operator, captured by a\nTransformer architecture that learns a data-driven global kernel. This\nphysics-based decomposition endows \\method{} with exceptional stability and\nrobustness. Through extensive experiments on the challenging 2D Kolmogorov flow\nbenchmark, we demonstrate that \\method{} significantly outperforms\nstate-of-the-art models in long-term forecasting. It successfully suppresses\nerror accumulation over hundreds of timesteps, maintains high fidelity in both\nthe vorticity fields and energy spectra, and establishes a new benchmark for\nphysically consistent, long-range turbulence forecast.\n", "link": "http://arxiv.org/abs/2509.21196v1", "date": "2025-09-25", "relevancy": 1.4924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5465}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4907}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential-Integral%20Neural%20Operator%20for%20Long-Term%20Turbulence%0A%20%20Forecasting&body=Title%3A%20Differential-Integral%20Neural%20Operator%20for%20Long-Term%20Turbulence%0A%20%20Forecasting%0AAuthor%3A%20Hao%20Wu%20and%20Yuan%20Gao%20and%20Fan%20Xu%20and%20Fan%20Zhang%20and%20Qingsong%20Wen%20and%20Kun%20Wang%20and%20Xiaomeng%20Huang%20and%20Xian%20Wu%0AAbstract%3A%20%20%20Accurately%20forecasting%20the%20long-term%20evolution%20of%20turbulence%20represents%20a%0Agrand%20challenge%20in%20scientific%20computing%20and%20is%20crucial%20for%20applications%20ranging%0Afrom%20climate%20modeling%20to%20aerospace%20engineering.%20Existing%20deep%20learning%20methods%2C%0Aparticularly%20neural%20operators%2C%20often%20fail%20in%20long-term%20autoregressive%0Apredictions%2C%20suffering%20from%20catastrophic%20error%20accumulation%20and%20a%20loss%20of%0Aphysical%20fidelity.%20This%20failure%20stems%20from%20their%20inability%20to%20simultaneously%0Acapture%20the%20distinct%20mathematical%20structures%20that%20govern%20turbulent%20dynamics%3A%0Alocal%2C%20dissipative%20effects%20and%20global%2C%20non-local%20interactions.%20In%20this%20paper%2C%0Awe%20propose%20the%0A%7B%5Ctextbf%7B%5Cunderline%7BD%7D%7D%7Differential-%7B%5Ctextbf%7B%5Cunderline%7BI%7D%7D%7Dntegral%0A%7B%5Ctextbf%7B%5Cunderline%7BN%7D%7D%7Deural%20%7B%5Ctextbf%7B%5Cunderline%7BO%7D%7D%7Dperator%20%28%5Cmethod%7B%7D%29%2C%20a%0Anovel%20framework%20designed%20from%20a%20first-principles%20approach%20of%20operator%0Adecomposition.%20%5Cmethod%7B%7D%20explicitly%20models%20the%20turbulent%20evolution%20through%0Aparallel%20branches%20that%20learn%20distinct%20physical%20operators%3A%20a%20local%20differential%0Aoperator%2C%20realized%20by%20a%20constrained%20convolutional%20network%20that%20provably%0Aconverges%20to%20a%20derivative%2C%20and%20a%20global%20integral%20operator%2C%20captured%20by%20a%0ATransformer%20architecture%20that%20learns%20a%20data-driven%20global%20kernel.%20This%0Aphysics-based%20decomposition%20endows%20%5Cmethod%7B%7D%20with%20exceptional%20stability%20and%0Arobustness.%20Through%20extensive%20experiments%20on%20the%20challenging%202D%20Kolmogorov%20flow%0Abenchmark%2C%20we%20demonstrate%20that%20%5Cmethod%7B%7D%20significantly%20outperforms%0Astate-of-the-art%20models%20in%20long-term%20forecasting.%20It%20successfully%20suppresses%0Aerror%20accumulation%20over%20hundreds%20of%20timesteps%2C%20maintains%20high%20fidelity%20in%20both%0Athe%20vorticity%20fields%20and%20energy%20spectra%2C%20and%20establishes%20a%20new%20benchmark%20for%0Aphysically%20consistent%2C%20long-range%20turbulence%20forecast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential-Integral%2520Neural%2520Operator%2520for%2520Long-Term%2520Turbulence%250A%2520%2520Forecasting%26entry.906535625%3DHao%2520Wu%2520and%2520Yuan%2520Gao%2520and%2520Fan%2520Xu%2520and%2520Fan%2520Zhang%2520and%2520Qingsong%2520Wen%2520and%2520Kun%2520Wang%2520and%2520Xiaomeng%2520Huang%2520and%2520Xian%2520Wu%26entry.1292438233%3D%2520%2520Accurately%2520forecasting%2520the%2520long-term%2520evolution%2520of%2520turbulence%2520represents%2520a%250Agrand%2520challenge%2520in%2520scientific%2520computing%2520and%2520is%2520crucial%2520for%2520applications%2520ranging%250Afrom%2520climate%2520modeling%2520to%2520aerospace%2520engineering.%2520Existing%2520deep%2520learning%2520methods%252C%250Aparticularly%2520neural%2520operators%252C%2520often%2520fail%2520in%2520long-term%2520autoregressive%250Apredictions%252C%2520suffering%2520from%2520catastrophic%2520error%2520accumulation%2520and%2520a%2520loss%2520of%250Aphysical%2520fidelity.%2520This%2520failure%2520stems%2520from%2520their%2520inability%2520to%2520simultaneously%250Acapture%2520the%2520distinct%2520mathematical%2520structures%2520that%2520govern%2520turbulent%2520dynamics%253A%250Alocal%252C%2520dissipative%2520effects%2520and%2520global%252C%2520non-local%2520interactions.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%250A%257B%255Ctextbf%257B%255Cunderline%257BD%257D%257D%257Differential-%257B%255Ctextbf%257B%255Cunderline%257BI%257D%257D%257Dntegral%250A%257B%255Ctextbf%257B%255Cunderline%257BN%257D%257D%257Deural%2520%257B%255Ctextbf%257B%255Cunderline%257BO%257D%257D%257Dperator%2520%2528%255Cmethod%257B%257D%2529%252C%2520a%250Anovel%2520framework%2520designed%2520from%2520a%2520first-principles%2520approach%2520of%2520operator%250Adecomposition.%2520%255Cmethod%257B%257D%2520explicitly%2520models%2520the%2520turbulent%2520evolution%2520through%250Aparallel%2520branches%2520that%2520learn%2520distinct%2520physical%2520operators%253A%2520a%2520local%2520differential%250Aoperator%252C%2520realized%2520by%2520a%2520constrained%2520convolutional%2520network%2520that%2520provably%250Aconverges%2520to%2520a%2520derivative%252C%2520and%2520a%2520global%2520integral%2520operator%252C%2520captured%2520by%2520a%250ATransformer%2520architecture%2520that%2520learns%2520a%2520data-driven%2520global%2520kernel.%2520This%250Aphysics-based%2520decomposition%2520endows%2520%255Cmethod%257B%257D%2520with%2520exceptional%2520stability%2520and%250Arobustness.%2520Through%2520extensive%2520experiments%2520on%2520the%2520challenging%25202D%2520Kolmogorov%2520flow%250Abenchmark%252C%2520we%2520demonstrate%2520that%2520%255Cmethod%257B%257D%2520significantly%2520outperforms%250Astate-of-the-art%2520models%2520in%2520long-term%2520forecasting.%2520It%2520successfully%2520suppresses%250Aerror%2520accumulation%2520over%2520hundreds%2520of%2520timesteps%252C%2520maintains%2520high%2520fidelity%2520in%2520both%250Athe%2520vorticity%2520fields%2520and%2520energy%2520spectra%252C%2520and%2520establishes%2520a%2520new%2520benchmark%2520for%250Aphysically%2520consistent%252C%2520long-range%2520turbulence%2520forecast.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential-Integral%20Neural%20Operator%20for%20Long-Term%20Turbulence%0A%20%20Forecasting&entry.906535625=Hao%20Wu%20and%20Yuan%20Gao%20and%20Fan%20Xu%20and%20Fan%20Zhang%20and%20Qingsong%20Wen%20and%20Kun%20Wang%20and%20Xiaomeng%20Huang%20and%20Xian%20Wu&entry.1292438233=%20%20Accurately%20forecasting%20the%20long-term%20evolution%20of%20turbulence%20represents%20a%0Agrand%20challenge%20in%20scientific%20computing%20and%20is%20crucial%20for%20applications%20ranging%0Afrom%20climate%20modeling%20to%20aerospace%20engineering.%20Existing%20deep%20learning%20methods%2C%0Aparticularly%20neural%20operators%2C%20often%20fail%20in%20long-term%20autoregressive%0Apredictions%2C%20suffering%20from%20catastrophic%20error%20accumulation%20and%20a%20loss%20of%0Aphysical%20fidelity.%20This%20failure%20stems%20from%20their%20inability%20to%20simultaneously%0Acapture%20the%20distinct%20mathematical%20structures%20that%20govern%20turbulent%20dynamics%3A%0Alocal%2C%20dissipative%20effects%20and%20global%2C%20non-local%20interactions.%20In%20this%20paper%2C%0Awe%20propose%20the%0A%7B%5Ctextbf%7B%5Cunderline%7BD%7D%7D%7Differential-%7B%5Ctextbf%7B%5Cunderline%7BI%7D%7D%7Dntegral%0A%7B%5Ctextbf%7B%5Cunderline%7BN%7D%7D%7Deural%20%7B%5Ctextbf%7B%5Cunderline%7BO%7D%7D%7Dperator%20%28%5Cmethod%7B%7D%29%2C%20a%0Anovel%20framework%20designed%20from%20a%20first-principles%20approach%20of%20operator%0Adecomposition.%20%5Cmethod%7B%7D%20explicitly%20models%20the%20turbulent%20evolution%20through%0Aparallel%20branches%20that%20learn%20distinct%20physical%20operators%3A%20a%20local%20differential%0Aoperator%2C%20realized%20by%20a%20constrained%20convolutional%20network%20that%20provably%0Aconverges%20to%20a%20derivative%2C%20and%20a%20global%20integral%20operator%2C%20captured%20by%20a%0ATransformer%20architecture%20that%20learns%20a%20data-driven%20global%20kernel.%20This%0Aphysics-based%20decomposition%20endows%20%5Cmethod%7B%7D%20with%20exceptional%20stability%20and%0Arobustness.%20Through%20extensive%20experiments%20on%20the%20challenging%202D%20Kolmogorov%20flow%0Abenchmark%2C%20we%20demonstrate%20that%20%5Cmethod%7B%7D%20significantly%20outperforms%0Astate-of-the-art%20models%20in%20long-term%20forecasting.%20It%20successfully%20suppresses%0Aerror%20accumulation%20over%20hundreds%20of%20timesteps%2C%20maintains%20high%20fidelity%20in%20both%0Athe%20vorticity%20fields%20and%20energy%20spectra%2C%20and%20establishes%20a%20new%20benchmark%20for%0Aphysically%20consistent%2C%20long-range%20turbulence%20forecast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21196v1&entry.124074799=Read"},
{"title": "WISER: Segmenting watermarked region - an epidemic change-point\n  perspective", "author": "Soham Bonnerjee and Sayar Karmakar and Subhrajyoty Roy", "abstract": "  With the increasing popularity of large language models, concerns over\ncontent authenticity have led to the development of myriad watermarking\nschemes. These schemes can be used to detect a machine-generated text via an\nappropriate key, while being imperceptible to readers with no such keys. The\ncorresponding detection mechanisms usually take the form of statistical\nhypothesis testing for the existence of watermarks, spurring extensive research\nin this direction. However, the finer-grained problem of identifying which\nsegments of a mixed-source text are actually watermarked, is much less\nexplored; the existing approaches either lack scalability or theoretical\nguarantees robust to paraphrase and post-editing. In this work, we introduce a\nunique perspective to such watermark segmentation problems through the lens of\nepidemic change-points. By highlighting the similarities as well as differences\nof these two problems, we motivate and propose WISER: a novel, computationally\nefficient, watermark segmentation algorithm. We theoretically validate our\nalgorithm by deriving finite sample error-bounds, and establishing its\nconsistency in detecting multiple watermarked segments in a single text.\nComplementing these theoretical results, our extensive numerical experiments\nshow that WISER outperforms state-of-the-art baseline methods, both in terms of\ncomputational speed as well as accuracy, on various benchmark datasets embedded\nwith diverse watermarking schemes. Our theoretical and empirical findings\nestablish WISER as an effective tool for watermark localization in most\nsettings. It also shows how insights from a classical statistical problem can\nlead to a theoretically valid and computationally efficient solution of a\nmodern and pertinent problem.\n", "link": "http://arxiv.org/abs/2509.21160v1", "date": "2025-09-25", "relevancy": 1.3631, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4699}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4504}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WISER%3A%20Segmenting%20watermarked%20region%20-%20an%20epidemic%20change-point%0A%20%20perspective&body=Title%3A%20WISER%3A%20Segmenting%20watermarked%20region%20-%20an%20epidemic%20change-point%0A%20%20perspective%0AAuthor%3A%20Soham%20Bonnerjee%20and%20Sayar%20Karmakar%20and%20Subhrajyoty%20Roy%0AAbstract%3A%20%20%20With%20the%20increasing%20popularity%20of%20large%20language%20models%2C%20concerns%20over%0Acontent%20authenticity%20have%20led%20to%20the%20development%20of%20myriad%20watermarking%0Aschemes.%20These%20schemes%20can%20be%20used%20to%20detect%20a%20machine-generated%20text%20via%20an%0Aappropriate%20key%2C%20while%20being%20imperceptible%20to%20readers%20with%20no%20such%20keys.%20The%0Acorresponding%20detection%20mechanisms%20usually%20take%20the%20form%20of%20statistical%0Ahypothesis%20testing%20for%20the%20existence%20of%20watermarks%2C%20spurring%20extensive%20research%0Ain%20this%20direction.%20However%2C%20the%20finer-grained%20problem%20of%20identifying%20which%0Asegments%20of%20a%20mixed-source%20text%20are%20actually%20watermarked%2C%20is%20much%20less%0Aexplored%3B%20the%20existing%20approaches%20either%20lack%20scalability%20or%20theoretical%0Aguarantees%20robust%20to%20paraphrase%20and%20post-editing.%20In%20this%20work%2C%20we%20introduce%20a%0Aunique%20perspective%20to%20such%20watermark%20segmentation%20problems%20through%20the%20lens%20of%0Aepidemic%20change-points.%20By%20highlighting%20the%20similarities%20as%20well%20as%20differences%0Aof%20these%20two%20problems%2C%20we%20motivate%20and%20propose%20WISER%3A%20a%20novel%2C%20computationally%0Aefficient%2C%20watermark%20segmentation%20algorithm.%20We%20theoretically%20validate%20our%0Aalgorithm%20by%20deriving%20finite%20sample%20error-bounds%2C%20and%20establishing%20its%0Aconsistency%20in%20detecting%20multiple%20watermarked%20segments%20in%20a%20single%20text.%0AComplementing%20these%20theoretical%20results%2C%20our%20extensive%20numerical%20experiments%0Ashow%20that%20WISER%20outperforms%20state-of-the-art%20baseline%20methods%2C%20both%20in%20terms%20of%0Acomputational%20speed%20as%20well%20as%20accuracy%2C%20on%20various%20benchmark%20datasets%20embedded%0Awith%20diverse%20watermarking%20schemes.%20Our%20theoretical%20and%20empirical%20findings%0Aestablish%20WISER%20as%20an%20effective%20tool%20for%20watermark%20localization%20in%20most%0Asettings.%20It%20also%20shows%20how%20insights%20from%20a%20classical%20statistical%20problem%20can%0Alead%20to%20a%20theoretically%20valid%20and%20computationally%20efficient%20solution%20of%20a%0Amodern%20and%20pertinent%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWISER%253A%2520Segmenting%2520watermarked%2520region%2520-%2520an%2520epidemic%2520change-point%250A%2520%2520perspective%26entry.906535625%3DSoham%2520Bonnerjee%2520and%2520Sayar%2520Karmakar%2520and%2520Subhrajyoty%2520Roy%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520popularity%2520of%2520large%2520language%2520models%252C%2520concerns%2520over%250Acontent%2520authenticity%2520have%2520led%2520to%2520the%2520development%2520of%2520myriad%2520watermarking%250Aschemes.%2520These%2520schemes%2520can%2520be%2520used%2520to%2520detect%2520a%2520machine-generated%2520text%2520via%2520an%250Aappropriate%2520key%252C%2520while%2520being%2520imperceptible%2520to%2520readers%2520with%2520no%2520such%2520keys.%2520The%250Acorresponding%2520detection%2520mechanisms%2520usually%2520take%2520the%2520form%2520of%2520statistical%250Ahypothesis%2520testing%2520for%2520the%2520existence%2520of%2520watermarks%252C%2520spurring%2520extensive%2520research%250Ain%2520this%2520direction.%2520However%252C%2520the%2520finer-grained%2520problem%2520of%2520identifying%2520which%250Asegments%2520of%2520a%2520mixed-source%2520text%2520are%2520actually%2520watermarked%252C%2520is%2520much%2520less%250Aexplored%253B%2520the%2520existing%2520approaches%2520either%2520lack%2520scalability%2520or%2520theoretical%250Aguarantees%2520robust%2520to%2520paraphrase%2520and%2520post-editing.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Aunique%2520perspective%2520to%2520such%2520watermark%2520segmentation%2520problems%2520through%2520the%2520lens%2520of%250Aepidemic%2520change-points.%2520By%2520highlighting%2520the%2520similarities%2520as%2520well%2520as%2520differences%250Aof%2520these%2520two%2520problems%252C%2520we%2520motivate%2520and%2520propose%2520WISER%253A%2520a%2520novel%252C%2520computationally%250Aefficient%252C%2520watermark%2520segmentation%2520algorithm.%2520We%2520theoretically%2520validate%2520our%250Aalgorithm%2520by%2520deriving%2520finite%2520sample%2520error-bounds%252C%2520and%2520establishing%2520its%250Aconsistency%2520in%2520detecting%2520multiple%2520watermarked%2520segments%2520in%2520a%2520single%2520text.%250AComplementing%2520these%2520theoretical%2520results%252C%2520our%2520extensive%2520numerical%2520experiments%250Ashow%2520that%2520WISER%2520outperforms%2520state-of-the-art%2520baseline%2520methods%252C%2520both%2520in%2520terms%2520of%250Acomputational%2520speed%2520as%2520well%2520as%2520accuracy%252C%2520on%2520various%2520benchmark%2520datasets%2520embedded%250Awith%2520diverse%2520watermarking%2520schemes.%2520Our%2520theoretical%2520and%2520empirical%2520findings%250Aestablish%2520WISER%2520as%2520an%2520effective%2520tool%2520for%2520watermark%2520localization%2520in%2520most%250Asettings.%2520It%2520also%2520shows%2520how%2520insights%2520from%2520a%2520classical%2520statistical%2520problem%2520can%250Alead%2520to%2520a%2520theoretically%2520valid%2520and%2520computationally%2520efficient%2520solution%2520of%2520a%250Amodern%2520and%2520pertinent%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WISER%3A%20Segmenting%20watermarked%20region%20-%20an%20epidemic%20change-point%0A%20%20perspective&entry.906535625=Soham%20Bonnerjee%20and%20Sayar%20Karmakar%20and%20Subhrajyoty%20Roy&entry.1292438233=%20%20With%20the%20increasing%20popularity%20of%20large%20language%20models%2C%20concerns%20over%0Acontent%20authenticity%20have%20led%20to%20the%20development%20of%20myriad%20watermarking%0Aschemes.%20These%20schemes%20can%20be%20used%20to%20detect%20a%20machine-generated%20text%20via%20an%0Aappropriate%20key%2C%20while%20being%20imperceptible%20to%20readers%20with%20no%20such%20keys.%20The%0Acorresponding%20detection%20mechanisms%20usually%20take%20the%20form%20of%20statistical%0Ahypothesis%20testing%20for%20the%20existence%20of%20watermarks%2C%20spurring%20extensive%20research%0Ain%20this%20direction.%20However%2C%20the%20finer-grained%20problem%20of%20identifying%20which%0Asegments%20of%20a%20mixed-source%20text%20are%20actually%20watermarked%2C%20is%20much%20less%0Aexplored%3B%20the%20existing%20approaches%20either%20lack%20scalability%20or%20theoretical%0Aguarantees%20robust%20to%20paraphrase%20and%20post-editing.%20In%20this%20work%2C%20we%20introduce%20a%0Aunique%20perspective%20to%20such%20watermark%20segmentation%20problems%20through%20the%20lens%20of%0Aepidemic%20change-points.%20By%20highlighting%20the%20similarities%20as%20well%20as%20differences%0Aof%20these%20two%20problems%2C%20we%20motivate%20and%20propose%20WISER%3A%20a%20novel%2C%20computationally%0Aefficient%2C%20watermark%20segmentation%20algorithm.%20We%20theoretically%20validate%20our%0Aalgorithm%20by%20deriving%20finite%20sample%20error-bounds%2C%20and%20establishing%20its%0Aconsistency%20in%20detecting%20multiple%20watermarked%20segments%20in%20a%20single%20text.%0AComplementing%20these%20theoretical%20results%2C%20our%20extensive%20numerical%20experiments%0Ashow%20that%20WISER%20outperforms%20state-of-the-art%20baseline%20methods%2C%20both%20in%20terms%20of%0Acomputational%20speed%20as%20well%20as%20accuracy%2C%20on%20various%20benchmark%20datasets%20embedded%0Awith%20diverse%20watermarking%20schemes.%20Our%20theoretical%20and%20empirical%20findings%0Aestablish%20WISER%20as%20an%20effective%20tool%20for%20watermark%20localization%20in%20most%0Asettings.%20It%20also%20shows%20how%20insights%20from%20a%20classical%20statistical%20problem%20can%0Alead%20to%20a%20theoretically%20valid%20and%20computationally%20efficient%20solution%20of%20a%0Amodern%20and%20pertinent%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21160v1&entry.124074799=Read"},
{"title": "Fractal Graph Contrastive Learning", "author": "Nero Z. Li and Xuehao Zhai and Zhichao Shi and Boshen Shi and Xuhui Jiang", "abstract": "  While Graph Contrastive Learning (GCL) has attracted considerable attention\nin the field of graph self-supervised learning, its performance heavily relies\non data augmentations that are expected to generate semantically consistent\npositive pairs. Existing strategies typically resort to random perturbations or\nlocal structure preservation, yet lack explicit control over global structural\nconsistency between augmented views. To address this limitation, we propose\nFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework\nintroducing two key innovations: a renormalisation-based augmentation that\ngenerates structurally aligned positive views via box coverings; and a\nfractal-dimension-aware contrastive loss that aligns graph embeddings according\nto their fractal dimensions, equipping the method with a fallback mechanism\nguaranteeing a performance lower bound even on non-fractal graphs. While\ncombining the two innovations markedly boosts graph-representation quality, it\nalso adds non-trivial computational overhead. To mitigate the computational\noverhead of fractal dimension estimation, we derive a one-shot estimator by\nproving that the dimension discrepancy between original and renormalised graphs\nconverges weakly to a centred Gaussian distribution. This theoretical insight\nenables a reduction in dimension computation cost by an order of magnitude,\ncutting overall training time by approximately 61\\%. The experiments show that\nFractalGCL not only delivers state-of-the-art results on standard benchmarks\nbut also outperforms traditional and latest baselines on traffic networks by an\naverage margin of about remarkably 4\\%. Codes are available at\n(https://anonymous.4open.science/r/FractalGCL-0511/).\n", "link": "http://arxiv.org/abs/2505.11356v3", "date": "2025-09-25", "relevancy": 1.5716, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5401}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5161}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fractal%20Graph%20Contrastive%20Learning&body=Title%3A%20Fractal%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Nero%20Z.%20Li%20and%20Xuehao%20Zhai%20and%20Zhichao%20Shi%20and%20Boshen%20Shi%20and%20Xuhui%20Jiang%0AAbstract%3A%20%20%20While%20Graph%20Contrastive%20Learning%20%28GCL%29%20has%20attracted%20considerable%20attention%0Ain%20the%20field%20of%20graph%20self-supervised%20learning%2C%20its%20performance%20heavily%20relies%0Aon%20data%20augmentations%20that%20are%20expected%20to%20generate%20semantically%20consistent%0Apositive%20pairs.%20Existing%20strategies%20typically%20resort%20to%20random%20perturbations%20or%0Alocal%20structure%20preservation%2C%20yet%20lack%20explicit%20control%20over%20global%20structural%0Aconsistency%20between%20augmented%20views.%20To%20address%20this%20limitation%2C%20we%20propose%0AFractal%20Graph%20Contrastive%20Learning%20%28FractalGCL%29%2C%20a%20theory-driven%20framework%0Aintroducing%20two%20key%20innovations%3A%20a%20renormalisation-based%20augmentation%20that%0Agenerates%20structurally%20aligned%20positive%20views%20via%20box%20coverings%3B%20and%20a%0Afractal-dimension-aware%20contrastive%20loss%20that%20aligns%20graph%20embeddings%20according%0Ato%20their%20fractal%20dimensions%2C%20equipping%20the%20method%20with%20a%20fallback%20mechanism%0Aguaranteeing%20a%20performance%20lower%20bound%20even%20on%20non-fractal%20graphs.%20While%0Acombining%20the%20two%20innovations%20markedly%20boosts%20graph-representation%20quality%2C%20it%0Aalso%20adds%20non-trivial%20computational%20overhead.%20To%20mitigate%20the%20computational%0Aoverhead%20of%20fractal%20dimension%20estimation%2C%20we%20derive%20a%20one-shot%20estimator%20by%0Aproving%20that%20the%20dimension%20discrepancy%20between%20original%20and%20renormalised%20graphs%0Aconverges%20weakly%20to%20a%20centred%20Gaussian%20distribution.%20This%20theoretical%20insight%0Aenables%20a%20reduction%20in%20dimension%20computation%20cost%20by%20an%20order%20of%20magnitude%2C%0Acutting%20overall%20training%20time%20by%20approximately%2061%5C%25.%20The%20experiments%20show%20that%0AFractalGCL%20not%20only%20delivers%20state-of-the-art%20results%20on%20standard%20benchmarks%0Abut%20also%20outperforms%20traditional%20and%20latest%20baselines%20on%20traffic%20networks%20by%20an%0Aaverage%20margin%20of%20about%20remarkably%204%5C%25.%20Codes%20are%20available%20at%0A%28https%3A//anonymous.4open.science/r/FractalGCL-0511/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11356v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFractal%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DNero%2520Z.%2520Li%2520and%2520Xuehao%2520Zhai%2520and%2520Zhichao%2520Shi%2520and%2520Boshen%2520Shi%2520and%2520Xuhui%2520Jiang%26entry.1292438233%3D%2520%2520While%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520has%2520attracted%2520considerable%2520attention%250Ain%2520the%2520field%2520of%2520graph%2520self-supervised%2520learning%252C%2520its%2520performance%2520heavily%2520relies%250Aon%2520data%2520augmentations%2520that%2520are%2520expected%2520to%2520generate%2520semantically%2520consistent%250Apositive%2520pairs.%2520Existing%2520strategies%2520typically%2520resort%2520to%2520random%2520perturbations%2520or%250Alocal%2520structure%2520preservation%252C%2520yet%2520lack%2520explicit%2520control%2520over%2520global%2520structural%250Aconsistency%2520between%2520augmented%2520views.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AFractal%2520Graph%2520Contrastive%2520Learning%2520%2528FractalGCL%2529%252C%2520a%2520theory-driven%2520framework%250Aintroducing%2520two%2520key%2520innovations%253A%2520a%2520renormalisation-based%2520augmentation%2520that%250Agenerates%2520structurally%2520aligned%2520positive%2520views%2520via%2520box%2520coverings%253B%2520and%2520a%250Afractal-dimension-aware%2520contrastive%2520loss%2520that%2520aligns%2520graph%2520embeddings%2520according%250Ato%2520their%2520fractal%2520dimensions%252C%2520equipping%2520the%2520method%2520with%2520a%2520fallback%2520mechanism%250Aguaranteeing%2520a%2520performance%2520lower%2520bound%2520even%2520on%2520non-fractal%2520graphs.%2520While%250Acombining%2520the%2520two%2520innovations%2520markedly%2520boosts%2520graph-representation%2520quality%252C%2520it%250Aalso%2520adds%2520non-trivial%2520computational%2520overhead.%2520To%2520mitigate%2520the%2520computational%250Aoverhead%2520of%2520fractal%2520dimension%2520estimation%252C%2520we%2520derive%2520a%2520one-shot%2520estimator%2520by%250Aproving%2520that%2520the%2520dimension%2520discrepancy%2520between%2520original%2520and%2520renormalised%2520graphs%250Aconverges%2520weakly%2520to%2520a%2520centred%2520Gaussian%2520distribution.%2520This%2520theoretical%2520insight%250Aenables%2520a%2520reduction%2520in%2520dimension%2520computation%2520cost%2520by%2520an%2520order%2520of%2520magnitude%252C%250Acutting%2520overall%2520training%2520time%2520by%2520approximately%252061%255C%2525.%2520The%2520experiments%2520show%2520that%250AFractalGCL%2520not%2520only%2520delivers%2520state-of-the-art%2520results%2520on%2520standard%2520benchmarks%250Abut%2520also%2520outperforms%2520traditional%2520and%2520latest%2520baselines%2520on%2520traffic%2520networks%2520by%2520an%250Aaverage%2520margin%2520of%2520about%2520remarkably%25204%255C%2525.%2520Codes%2520are%2520available%2520at%250A%2528https%253A//anonymous.4open.science/r/FractalGCL-0511/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11356v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fractal%20Graph%20Contrastive%20Learning&entry.906535625=Nero%20Z.%20Li%20and%20Xuehao%20Zhai%20and%20Zhichao%20Shi%20and%20Boshen%20Shi%20and%20Xuhui%20Jiang&entry.1292438233=%20%20While%20Graph%20Contrastive%20Learning%20%28GCL%29%20has%20attracted%20considerable%20attention%0Ain%20the%20field%20of%20graph%20self-supervised%20learning%2C%20its%20performance%20heavily%20relies%0Aon%20data%20augmentations%20that%20are%20expected%20to%20generate%20semantically%20consistent%0Apositive%20pairs.%20Existing%20strategies%20typically%20resort%20to%20random%20perturbations%20or%0Alocal%20structure%20preservation%2C%20yet%20lack%20explicit%20control%20over%20global%20structural%0Aconsistency%20between%20augmented%20views.%20To%20address%20this%20limitation%2C%20we%20propose%0AFractal%20Graph%20Contrastive%20Learning%20%28FractalGCL%29%2C%20a%20theory-driven%20framework%0Aintroducing%20two%20key%20innovations%3A%20a%20renormalisation-based%20augmentation%20that%0Agenerates%20structurally%20aligned%20positive%20views%20via%20box%20coverings%3B%20and%20a%0Afractal-dimension-aware%20contrastive%20loss%20that%20aligns%20graph%20embeddings%20according%0Ato%20their%20fractal%20dimensions%2C%20equipping%20the%20method%20with%20a%20fallback%20mechanism%0Aguaranteeing%20a%20performance%20lower%20bound%20even%20on%20non-fractal%20graphs.%20While%0Acombining%20the%20two%20innovations%20markedly%20boosts%20graph-representation%20quality%2C%20it%0Aalso%20adds%20non-trivial%20computational%20overhead.%20To%20mitigate%20the%20computational%0Aoverhead%20of%20fractal%20dimension%20estimation%2C%20we%20derive%20a%20one-shot%20estimator%20by%0Aproving%20that%20the%20dimension%20discrepancy%20between%20original%20and%20renormalised%20graphs%0Aconverges%20weakly%20to%20a%20centred%20Gaussian%20distribution.%20This%20theoretical%20insight%0Aenables%20a%20reduction%20in%20dimension%20computation%20cost%20by%20an%20order%20of%20magnitude%2C%0Acutting%20overall%20training%20time%20by%20approximately%2061%5C%25.%20The%20experiments%20show%20that%0AFractalGCL%20not%20only%20delivers%20state-of-the-art%20results%20on%20standard%20benchmarks%0Abut%20also%20outperforms%20traditional%20and%20latest%20baselines%20on%20traffic%20networks%20by%20an%0Aaverage%20margin%20of%20about%20remarkably%204%5C%25.%20Codes%20are%20available%20at%0A%28https%3A//anonymous.4open.science/r/FractalGCL-0511/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11356v3&entry.124074799=Read"},
{"title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface\n  Systems", "author": "Junfeng Yan and Biao Wu and Meng Fang and Ling Chen", "abstract": "  Multimodal agents have demonstrated strong performance in general GUI\ninteractions, but their application in automotive systems has been largely\nunexplored. In-vehicle GUIs present distinct challenges: drivers' limited\nattention, strict safety requirements, and complex location-based interaction\npatterns. To address these challenges, we introduce Automotive-ENV, the first\nhigh-fidelity benchmark and interaction environment tailored for vehicle GUIs.\nThis platform defines 185 parameterized tasks spanning explicit control,\nimplicit intent understanding, and safety-aware tasks, and provides structured\nmultimodal observations with precise programmatic checks for reproducible\nevaluation. Building on this benchmark, we propose ASURADA, a geo-aware\nmultimodal agent that integrates GPS-informed context to dynamically adjust\nactions based on location, environmental conditions, and regional driving\nnorms. Experiments show that geo-aware information significantly improves\nsuccess on safety-aware tasks, highlighting the importance of location-based\ncontext in automotive environments. We will release Automotive-ENV, complete\nwith all tasks and benchmarking tools, to further the development of safe and\nadaptive in-vehicle agents.\n", "link": "http://arxiv.org/abs/2509.21143v1", "date": "2025-09-25", "relevancy": 1.6092, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5474}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5355}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automotive-ENV%3A%20Benchmarking%20Multimodal%20Agents%20in%20Vehicle%20Interface%0A%20%20Systems&body=Title%3A%20Automotive-ENV%3A%20Benchmarking%20Multimodal%20Agents%20in%20Vehicle%20Interface%0A%20%20Systems%0AAuthor%3A%20Junfeng%20Yan%20and%20Biao%20Wu%20and%20Meng%20Fang%20and%20Ling%20Chen%0AAbstract%3A%20%20%20Multimodal%20agents%20have%20demonstrated%20strong%20performance%20in%20general%20GUI%0Ainteractions%2C%20but%20their%20application%20in%20automotive%20systems%20has%20been%20largely%0Aunexplored.%20In-vehicle%20GUIs%20present%20distinct%20challenges%3A%20drivers%27%20limited%0Aattention%2C%20strict%20safety%20requirements%2C%20and%20complex%20location-based%20interaction%0Apatterns.%20To%20address%20these%20challenges%2C%20we%20introduce%20Automotive-ENV%2C%20the%20first%0Ahigh-fidelity%20benchmark%20and%20interaction%20environment%20tailored%20for%20vehicle%20GUIs.%0AThis%20platform%20defines%20185%20parameterized%20tasks%20spanning%20explicit%20control%2C%0Aimplicit%20intent%20understanding%2C%20and%20safety-aware%20tasks%2C%20and%20provides%20structured%0Amultimodal%20observations%20with%20precise%20programmatic%20checks%20for%20reproducible%0Aevaluation.%20Building%20on%20this%20benchmark%2C%20we%20propose%20ASURADA%2C%20a%20geo-aware%0Amultimodal%20agent%20that%20integrates%20GPS-informed%20context%20to%20dynamically%20adjust%0Aactions%20based%20on%20location%2C%20environmental%20conditions%2C%20and%20regional%20driving%0Anorms.%20Experiments%20show%20that%20geo-aware%20information%20significantly%20improves%0Asuccess%20on%20safety-aware%20tasks%2C%20highlighting%20the%20importance%20of%20location-based%0Acontext%20in%20automotive%20environments.%20We%20will%20release%20Automotive-ENV%2C%20complete%0Awith%20all%20tasks%20and%20benchmarking%20tools%2C%20to%20further%20the%20development%20of%20safe%20and%0Aadaptive%20in-vehicle%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomotive-ENV%253A%2520Benchmarking%2520Multimodal%2520Agents%2520in%2520Vehicle%2520Interface%250A%2520%2520Systems%26entry.906535625%3DJunfeng%2520Yan%2520and%2520Biao%2520Wu%2520and%2520Meng%2520Fang%2520and%2520Ling%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520agents%2520have%2520demonstrated%2520strong%2520performance%2520in%2520general%2520GUI%250Ainteractions%252C%2520but%2520their%2520application%2520in%2520automotive%2520systems%2520has%2520been%2520largely%250Aunexplored.%2520In-vehicle%2520GUIs%2520present%2520distinct%2520challenges%253A%2520drivers%2527%2520limited%250Aattention%252C%2520strict%2520safety%2520requirements%252C%2520and%2520complex%2520location-based%2520interaction%250Apatterns.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Automotive-ENV%252C%2520the%2520first%250Ahigh-fidelity%2520benchmark%2520and%2520interaction%2520environment%2520tailored%2520for%2520vehicle%2520GUIs.%250AThis%2520platform%2520defines%2520185%2520parameterized%2520tasks%2520spanning%2520explicit%2520control%252C%250Aimplicit%2520intent%2520understanding%252C%2520and%2520safety-aware%2520tasks%252C%2520and%2520provides%2520structured%250Amultimodal%2520observations%2520with%2520precise%2520programmatic%2520checks%2520for%2520reproducible%250Aevaluation.%2520Building%2520on%2520this%2520benchmark%252C%2520we%2520propose%2520ASURADA%252C%2520a%2520geo-aware%250Amultimodal%2520agent%2520that%2520integrates%2520GPS-informed%2520context%2520to%2520dynamically%2520adjust%250Aactions%2520based%2520on%2520location%252C%2520environmental%2520conditions%252C%2520and%2520regional%2520driving%250Anorms.%2520Experiments%2520show%2520that%2520geo-aware%2520information%2520significantly%2520improves%250Asuccess%2520on%2520safety-aware%2520tasks%252C%2520highlighting%2520the%2520importance%2520of%2520location-based%250Acontext%2520in%2520automotive%2520environments.%2520We%2520will%2520release%2520Automotive-ENV%252C%2520complete%250Awith%2520all%2520tasks%2520and%2520benchmarking%2520tools%252C%2520to%2520further%2520the%2520development%2520of%2520safe%2520and%250Aadaptive%2520in-vehicle%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automotive-ENV%3A%20Benchmarking%20Multimodal%20Agents%20in%20Vehicle%20Interface%0A%20%20Systems&entry.906535625=Junfeng%20Yan%20and%20Biao%20Wu%20and%20Meng%20Fang%20and%20Ling%20Chen&entry.1292438233=%20%20Multimodal%20agents%20have%20demonstrated%20strong%20performance%20in%20general%20GUI%0Ainteractions%2C%20but%20their%20application%20in%20automotive%20systems%20has%20been%20largely%0Aunexplored.%20In-vehicle%20GUIs%20present%20distinct%20challenges%3A%20drivers%27%20limited%0Aattention%2C%20strict%20safety%20requirements%2C%20and%20complex%20location-based%20interaction%0Apatterns.%20To%20address%20these%20challenges%2C%20we%20introduce%20Automotive-ENV%2C%20the%20first%0Ahigh-fidelity%20benchmark%20and%20interaction%20environment%20tailored%20for%20vehicle%20GUIs.%0AThis%20platform%20defines%20185%20parameterized%20tasks%20spanning%20explicit%20control%2C%0Aimplicit%20intent%20understanding%2C%20and%20safety-aware%20tasks%2C%20and%20provides%20structured%0Amultimodal%20observations%20with%20precise%20programmatic%20checks%20for%20reproducible%0Aevaluation.%20Building%20on%20this%20benchmark%2C%20we%20propose%20ASURADA%2C%20a%20geo-aware%0Amultimodal%20agent%20that%20integrates%20GPS-informed%20context%20to%20dynamically%20adjust%0Aactions%20based%20on%20location%2C%20environmental%20conditions%2C%20and%20regional%20driving%0Anorms.%20Experiments%20show%20that%20geo-aware%20information%20significantly%20improves%0Asuccess%20on%20safety-aware%20tasks%2C%20highlighting%20the%20importance%20of%20location-based%0Acontext%20in%20automotive%20environments.%20We%20will%20release%20Automotive-ENV%2C%20complete%0Awith%20all%20tasks%20and%20benchmarking%20tools%2C%20to%20further%20the%20development%20of%20safe%20and%0Aadaptive%20in-vehicle%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21143v1&entry.124074799=Read"},
{"title": "Adoption, usability and perceived clinical value of a UK AI clinical\n  reference platform (iatroX): a mixed-methods formative evaluation of\n  real-world usage and a 1,223-respondent user survey", "author": "Kolawole Tytler", "abstract": "  Clinicians face growing information overload from biomedical literature and\nguidelines, hindering evidence-based care. Retrieval-augmented generation (RAG)\nwith large language models may provide fast, provenance-linked answers, but\nrequires real-world evaluation. We describe iatroX, a UK-centred RAG-based\nclinical reference platform, and report early adoption, usability, and\nperceived clinical value from a formative implementation evaluation. Methods\ncomprised a retrospective analysis of usage across web, iOS, and Android over\n16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage\nmetrics were drawn from web and app analytics with bot filtering. A client-side\nscript randomized single-item prompts to approx. 10% of web sessions from a\npredefined battery assessing usefulness, reliability, and adoption intent.\nProportions were summarized with Wilson 95% confidence intervals; free-text\ncomments underwent thematic content analysis. iatroX reached 19,269 unique web\nusers, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile\nuptake included 1,960 iOS downloads and Android growth (peak >750 daily active\nusers). The survey yielded 1,223 item-level responses: perceived usefulness\n86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%;\n14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived\naccuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI\n62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK\nspecificity. Early real-world use suggests iatroX can mitigate information\noverload and support timely answers for UK clinicians. Limitations include\nsmall per-item samples and early-adopter bias; future work will include\naccuracy audits and prospective studies on workflow and care quality.\n", "link": "http://arxiv.org/abs/2509.21188v1", "date": "2025-09-25", "relevancy": 1.1982, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4063}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4042}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adoption%2C%20usability%20and%20perceived%20clinical%20value%20of%20a%20UK%20AI%20clinical%0A%20%20reference%20platform%20%28iatroX%29%3A%20a%20mixed-methods%20formative%20evaluation%20of%0A%20%20real-world%20usage%20and%20a%201%2C223-respondent%20user%20survey&body=Title%3A%20Adoption%2C%20usability%20and%20perceived%20clinical%20value%20of%20a%20UK%20AI%20clinical%0A%20%20reference%20platform%20%28iatroX%29%3A%20a%20mixed-methods%20formative%20evaluation%20of%0A%20%20real-world%20usage%20and%20a%201%2C223-respondent%20user%20survey%0AAuthor%3A%20Kolawole%20Tytler%0AAbstract%3A%20%20%20Clinicians%20face%20growing%20information%20overload%20from%20biomedical%20literature%20and%0Aguidelines%2C%20hindering%20evidence-based%20care.%20Retrieval-augmented%20generation%20%28RAG%29%0Awith%20large%20language%20models%20may%20provide%20fast%2C%20provenance-linked%20answers%2C%20but%0Arequires%20real-world%20evaluation.%20We%20describe%20iatroX%2C%20a%20UK-centred%20RAG-based%0Aclinical%20reference%20platform%2C%20and%20report%20early%20adoption%2C%20usability%2C%20and%0Aperceived%20clinical%20value%20from%20a%20formative%20implementation%20evaluation.%20Methods%0Acomprised%20a%20retrospective%20analysis%20of%20usage%20across%20web%2C%20iOS%2C%20and%20Android%20over%0A16%20weeks%20%288%20April-31%20July%202025%29%20and%20an%20in-product%20intercept%20survey.%20Usage%0Ametrics%20were%20drawn%20from%20web%20and%20app%20analytics%20with%20bot%20filtering.%20A%20client-side%0Ascript%20randomized%20single-item%20prompts%20to%20approx.%2010%25%20of%20web%20sessions%20from%20a%0Apredefined%20battery%20assessing%20usefulness%2C%20reliability%2C%20and%20adoption%20intent.%0AProportions%20were%20summarized%20with%20Wilson%2095%25%20confidence%20intervals%3B%20free-text%0Acomments%20underwent%20thematic%20content%20analysis.%20iatroX%20reached%2019%2C269%20unique%20web%0Ausers%2C%20202%2C660%20engagement%20events%2C%20and%20approx.%2040%2C000%20clinical%20queries.%20Mobile%0Auptake%20included%201%2C960%20iOS%20downloads%20and%20Android%20growth%20%28peak%20%3E750%20daily%20active%0Ausers%29.%20The%20survey%20yielded%201%2C223%20item-level%20responses%3A%20perceived%20usefulness%0A86.2%25%20%2895%25%20CI%2074.8-93.9%25%3B%2050/58%29%3B%20would%20use%20again%2093.3%25%20%2895%25%20CI%2068.1-99.8%25%3B%0A14/15%29%3B%20recommend%20to%20a%20colleague%2088.4%25%20%2895%25%20CI%2075.1-95.9%25%3B%2038/43%29%3B%20perceived%0Aaccuracy%2075.0%25%20%2895%25%20CI%2058.8-87.3%25%3B%2030/40%29%3B%20reliability%2079.4%25%20%2895%25%20CI%0A62.1-91.3%25%3B%2027/34%29.%20Themes%20highlighted%20speed%2C%20guideline-linked%20answers%2C%20and%20UK%0Aspecificity.%20Early%20real-world%20use%20suggests%20iatroX%20can%20mitigate%20information%0Aoverload%20and%20support%20timely%20answers%20for%20UK%20clinicians.%20Limitations%20include%0Asmall%20per-item%20samples%20and%20early-adopter%20bias%3B%20future%20work%20will%20include%0Aaccuracy%20audits%20and%20prospective%20studies%20on%20workflow%20and%20care%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdoption%252C%2520usability%2520and%2520perceived%2520clinical%2520value%2520of%2520a%2520UK%2520AI%2520clinical%250A%2520%2520reference%2520platform%2520%2528iatroX%2529%253A%2520a%2520mixed-methods%2520formative%2520evaluation%2520of%250A%2520%2520real-world%2520usage%2520and%2520a%25201%252C223-respondent%2520user%2520survey%26entry.906535625%3DKolawole%2520Tytler%26entry.1292438233%3D%2520%2520Clinicians%2520face%2520growing%2520information%2520overload%2520from%2520biomedical%2520literature%2520and%250Aguidelines%252C%2520hindering%2520evidence-based%2520care.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%250Awith%2520large%2520language%2520models%2520may%2520provide%2520fast%252C%2520provenance-linked%2520answers%252C%2520but%250Arequires%2520real-world%2520evaluation.%2520We%2520describe%2520iatroX%252C%2520a%2520UK-centred%2520RAG-based%250Aclinical%2520reference%2520platform%252C%2520and%2520report%2520early%2520adoption%252C%2520usability%252C%2520and%250Aperceived%2520clinical%2520value%2520from%2520a%2520formative%2520implementation%2520evaluation.%2520Methods%250Acomprised%2520a%2520retrospective%2520analysis%2520of%2520usage%2520across%2520web%252C%2520iOS%252C%2520and%2520Android%2520over%250A16%2520weeks%2520%25288%2520April-31%2520July%25202025%2529%2520and%2520an%2520in-product%2520intercept%2520survey.%2520Usage%250Ametrics%2520were%2520drawn%2520from%2520web%2520and%2520app%2520analytics%2520with%2520bot%2520filtering.%2520A%2520client-side%250Ascript%2520randomized%2520single-item%2520prompts%2520to%2520approx.%252010%2525%2520of%2520web%2520sessions%2520from%2520a%250Apredefined%2520battery%2520assessing%2520usefulness%252C%2520reliability%252C%2520and%2520adoption%2520intent.%250AProportions%2520were%2520summarized%2520with%2520Wilson%252095%2525%2520confidence%2520intervals%253B%2520free-text%250Acomments%2520underwent%2520thematic%2520content%2520analysis.%2520iatroX%2520reached%252019%252C269%2520unique%2520web%250Ausers%252C%2520202%252C660%2520engagement%2520events%252C%2520and%2520approx.%252040%252C000%2520clinical%2520queries.%2520Mobile%250Auptake%2520included%25201%252C960%2520iOS%2520downloads%2520and%2520Android%2520growth%2520%2528peak%2520%253E750%2520daily%2520active%250Ausers%2529.%2520The%2520survey%2520yielded%25201%252C223%2520item-level%2520responses%253A%2520perceived%2520usefulness%250A86.2%2525%2520%252895%2525%2520CI%252074.8-93.9%2525%253B%252050/58%2529%253B%2520would%2520use%2520again%252093.3%2525%2520%252895%2525%2520CI%252068.1-99.8%2525%253B%250A14/15%2529%253B%2520recommend%2520to%2520a%2520colleague%252088.4%2525%2520%252895%2525%2520CI%252075.1-95.9%2525%253B%252038/43%2529%253B%2520perceived%250Aaccuracy%252075.0%2525%2520%252895%2525%2520CI%252058.8-87.3%2525%253B%252030/40%2529%253B%2520reliability%252079.4%2525%2520%252895%2525%2520CI%250A62.1-91.3%2525%253B%252027/34%2529.%2520Themes%2520highlighted%2520speed%252C%2520guideline-linked%2520answers%252C%2520and%2520UK%250Aspecificity.%2520Early%2520real-world%2520use%2520suggests%2520iatroX%2520can%2520mitigate%2520information%250Aoverload%2520and%2520support%2520timely%2520answers%2520for%2520UK%2520clinicians.%2520Limitations%2520include%250Asmall%2520per-item%2520samples%2520and%2520early-adopter%2520bias%253B%2520future%2520work%2520will%2520include%250Aaccuracy%2520audits%2520and%2520prospective%2520studies%2520on%2520workflow%2520and%2520care%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adoption%2C%20usability%20and%20perceived%20clinical%20value%20of%20a%20UK%20AI%20clinical%0A%20%20reference%20platform%20%28iatroX%29%3A%20a%20mixed-methods%20formative%20evaluation%20of%0A%20%20real-world%20usage%20and%20a%201%2C223-respondent%20user%20survey&entry.906535625=Kolawole%20Tytler&entry.1292438233=%20%20Clinicians%20face%20growing%20information%20overload%20from%20biomedical%20literature%20and%0Aguidelines%2C%20hindering%20evidence-based%20care.%20Retrieval-augmented%20generation%20%28RAG%29%0Awith%20large%20language%20models%20may%20provide%20fast%2C%20provenance-linked%20answers%2C%20but%0Arequires%20real-world%20evaluation.%20We%20describe%20iatroX%2C%20a%20UK-centred%20RAG-based%0Aclinical%20reference%20platform%2C%20and%20report%20early%20adoption%2C%20usability%2C%20and%0Aperceived%20clinical%20value%20from%20a%20formative%20implementation%20evaluation.%20Methods%0Acomprised%20a%20retrospective%20analysis%20of%20usage%20across%20web%2C%20iOS%2C%20and%20Android%20over%0A16%20weeks%20%288%20April-31%20July%202025%29%20and%20an%20in-product%20intercept%20survey.%20Usage%0Ametrics%20were%20drawn%20from%20web%20and%20app%20analytics%20with%20bot%20filtering.%20A%20client-side%0Ascript%20randomized%20single-item%20prompts%20to%20approx.%2010%25%20of%20web%20sessions%20from%20a%0Apredefined%20battery%20assessing%20usefulness%2C%20reliability%2C%20and%20adoption%20intent.%0AProportions%20were%20summarized%20with%20Wilson%2095%25%20confidence%20intervals%3B%20free-text%0Acomments%20underwent%20thematic%20content%20analysis.%20iatroX%20reached%2019%2C269%20unique%20web%0Ausers%2C%20202%2C660%20engagement%20events%2C%20and%20approx.%2040%2C000%20clinical%20queries.%20Mobile%0Auptake%20included%201%2C960%20iOS%20downloads%20and%20Android%20growth%20%28peak%20%3E750%20daily%20active%0Ausers%29.%20The%20survey%20yielded%201%2C223%20item-level%20responses%3A%20perceived%20usefulness%0A86.2%25%20%2895%25%20CI%2074.8-93.9%25%3B%2050/58%29%3B%20would%20use%20again%2093.3%25%20%2895%25%20CI%2068.1-99.8%25%3B%0A14/15%29%3B%20recommend%20to%20a%20colleague%2088.4%25%20%2895%25%20CI%2075.1-95.9%25%3B%2038/43%29%3B%20perceived%0Aaccuracy%2075.0%25%20%2895%25%20CI%2058.8-87.3%25%3B%2030/40%29%3B%20reliability%2079.4%25%20%2895%25%20CI%0A62.1-91.3%25%3B%2027/34%29.%20Themes%20highlighted%20speed%2C%20guideline-linked%20answers%2C%20and%20UK%0Aspecificity.%20Early%20real-world%20use%20suggests%20iatroX%20can%20mitigate%20information%0Aoverload%20and%20support%20timely%20answers%20for%20UK%20clinicians.%20Limitations%20include%0Asmall%20per-item%20samples%20and%20early-adopter%20bias%3B%20future%20work%20will%20include%0Aaccuracy%20audits%20and%20prospective%20studies%20on%20workflow%20and%20care%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21188v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


