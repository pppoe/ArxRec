<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250907.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting", "author": "Yangming Li and Chaoyu Liu and Lihao Liu and Simon Masnou and Carola-Bibian Sch\u00f6nlieb", "abstract": "  A few recent works explored incorporating geometric priors to regularize the\noptimization of Gaussian splatting, further improving its performance. However,\nthose early studies mainly focused on the use of low-order geometric priors\n(e.g., normal vector), and they are also unreliably estimated by\nnoise-sensitive methods, like local principal component analysis. To address\ntheir limitations, we first present GeoSplat, a general geometry-constrained\noptimization framework that exploits both first-order and second-order\ngeometric quantities to improve the entire training pipeline of Gaussian\nsplatting, including Gaussian initialization, gradient update, and\ndensification. As an example, we initialize the scales of 3D Gaussian\nprimitives in terms of principal curvatures, leading to a better coverage of\nthe object surface than random initialization. Secondly, based on certain\ngeometric structures (e.g., local manifold), we introduce efficient and\nnoise-robust estimation methods that provide dynamic geometric priors for our\nframework. We conduct extensive experiments on multiple datasets for novel view\nsynthesis, showing that our framework: GeoSplat, significantly improves the\nperformance of Gaussian splatting and outperforms previous baselines.\n", "link": "http://arxiv.org/abs/2509.05075v1", "date": "2025-09-05", "relevancy": 3.2622, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6697}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6587}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSplat%3A%20A%20Deep%20Dive%20into%20Geometry-Constrained%20Gaussian%20Splatting&body=Title%3A%20GeoSplat%3A%20A%20Deep%20Dive%20into%20Geometry-Constrained%20Gaussian%20Splatting%0AAuthor%3A%20Yangming%20Li%20and%20Chaoyu%20Liu%20and%20Lihao%20Liu%20and%20Simon%20Masnou%20and%20Carola-Bibian%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20A%20few%20recent%20works%20explored%20incorporating%20geometric%20priors%20to%20regularize%20the%0Aoptimization%20of%20Gaussian%20splatting%2C%20further%20improving%20its%20performance.%20However%2C%0Athose%20early%20studies%20mainly%20focused%20on%20the%20use%20of%20low-order%20geometric%20priors%0A%28e.g.%2C%20normal%20vector%29%2C%20and%20they%20are%20also%20unreliably%20estimated%20by%0Anoise-sensitive%20methods%2C%20like%20local%20principal%20component%20analysis.%20To%20address%0Atheir%20limitations%2C%20we%20first%20present%20GeoSplat%2C%20a%20general%20geometry-constrained%0Aoptimization%20framework%20that%20exploits%20both%20first-order%20and%20second-order%0Ageometric%20quantities%20to%20improve%20the%20entire%20training%20pipeline%20of%20Gaussian%0Asplatting%2C%20including%20Gaussian%20initialization%2C%20gradient%20update%2C%20and%0Adensification.%20As%20an%20example%2C%20we%20initialize%20the%20scales%20of%203D%20Gaussian%0Aprimitives%20in%20terms%20of%20principal%20curvatures%2C%20leading%20to%20a%20better%20coverage%20of%0Athe%20object%20surface%20than%20random%20initialization.%20Secondly%2C%20based%20on%20certain%0Ageometric%20structures%20%28e.g.%2C%20local%20manifold%29%2C%20we%20introduce%20efficient%20and%0Anoise-robust%20estimation%20methods%20that%20provide%20dynamic%20geometric%20priors%20for%20our%0Aframework.%20We%20conduct%20extensive%20experiments%20on%20multiple%20datasets%20for%20novel%20view%0Asynthesis%2C%20showing%20that%20our%20framework%3A%20GeoSplat%2C%20significantly%20improves%20the%0Aperformance%20of%20Gaussian%20splatting%20and%20outperforms%20previous%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSplat%253A%2520A%2520Deep%2520Dive%2520into%2520Geometry-Constrained%2520Gaussian%2520Splatting%26entry.906535625%3DYangming%2520Li%2520and%2520Chaoyu%2520Liu%2520and%2520Lihao%2520Liu%2520and%2520Simon%2520Masnou%2520and%2520Carola-Bibian%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520A%2520few%2520recent%2520works%2520explored%2520incorporating%2520geometric%2520priors%2520to%2520regularize%2520the%250Aoptimization%2520of%2520Gaussian%2520splatting%252C%2520further%2520improving%2520its%2520performance.%2520However%252C%250Athose%2520early%2520studies%2520mainly%2520focused%2520on%2520the%2520use%2520of%2520low-order%2520geometric%2520priors%250A%2528e.g.%252C%2520normal%2520vector%2529%252C%2520and%2520they%2520are%2520also%2520unreliably%2520estimated%2520by%250Anoise-sensitive%2520methods%252C%2520like%2520local%2520principal%2520component%2520analysis.%2520To%2520address%250Atheir%2520limitations%252C%2520we%2520first%2520present%2520GeoSplat%252C%2520a%2520general%2520geometry-constrained%250Aoptimization%2520framework%2520that%2520exploits%2520both%2520first-order%2520and%2520second-order%250Ageometric%2520quantities%2520to%2520improve%2520the%2520entire%2520training%2520pipeline%2520of%2520Gaussian%250Asplatting%252C%2520including%2520Gaussian%2520initialization%252C%2520gradient%2520update%252C%2520and%250Adensification.%2520As%2520an%2520example%252C%2520we%2520initialize%2520the%2520scales%2520of%25203D%2520Gaussian%250Aprimitives%2520in%2520terms%2520of%2520principal%2520curvatures%252C%2520leading%2520to%2520a%2520better%2520coverage%2520of%250Athe%2520object%2520surface%2520than%2520random%2520initialization.%2520Secondly%252C%2520based%2520on%2520certain%250Ageometric%2520structures%2520%2528e.g.%252C%2520local%2520manifold%2529%252C%2520we%2520introduce%2520efficient%2520and%250Anoise-robust%2520estimation%2520methods%2520that%2520provide%2520dynamic%2520geometric%2520priors%2520for%2520our%250Aframework.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520multiple%2520datasets%2520for%2520novel%2520view%250Asynthesis%252C%2520showing%2520that%2520our%2520framework%253A%2520GeoSplat%252C%2520significantly%2520improves%2520the%250Aperformance%2520of%2520Gaussian%2520splatting%2520and%2520outperforms%2520previous%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSplat%3A%20A%20Deep%20Dive%20into%20Geometry-Constrained%20Gaussian%20Splatting&entry.906535625=Yangming%20Li%20and%20Chaoyu%20Liu%20and%20Lihao%20Liu%20and%20Simon%20Masnou%20and%20Carola-Bibian%20Sch%C3%B6nlieb&entry.1292438233=%20%20A%20few%20recent%20works%20explored%20incorporating%20geometric%20priors%20to%20regularize%20the%0Aoptimization%20of%20Gaussian%20splatting%2C%20further%20improving%20its%20performance.%20However%2C%0Athose%20early%20studies%20mainly%20focused%20on%20the%20use%20of%20low-order%20geometric%20priors%0A%28e.g.%2C%20normal%20vector%29%2C%20and%20they%20are%20also%20unreliably%20estimated%20by%0Anoise-sensitive%20methods%2C%20like%20local%20principal%20component%20analysis.%20To%20address%0Atheir%20limitations%2C%20we%20first%20present%20GeoSplat%2C%20a%20general%20geometry-constrained%0Aoptimization%20framework%20that%20exploits%20both%20first-order%20and%20second-order%0Ageometric%20quantities%20to%20improve%20the%20entire%20training%20pipeline%20of%20Gaussian%0Asplatting%2C%20including%20Gaussian%20initialization%2C%20gradient%20update%2C%20and%0Adensification.%20As%20an%20example%2C%20we%20initialize%20the%20scales%20of%203D%20Gaussian%0Aprimitives%20in%20terms%20of%20principal%20curvatures%2C%20leading%20to%20a%20better%20coverage%20of%0Athe%20object%20surface%20than%20random%20initialization.%20Secondly%2C%20based%20on%20certain%0Ageometric%20structures%20%28e.g.%2C%20local%20manifold%29%2C%20we%20introduce%20efficient%20and%0Anoise-robust%20estimation%20methods%20that%20provide%20dynamic%20geometric%20priors%20for%20our%0Aframework.%20We%20conduct%20extensive%20experiments%20on%20multiple%20datasets%20for%20novel%20view%0Asynthesis%2C%20showing%20that%20our%20framework%3A%20GeoSplat%2C%20significantly%20improves%20the%0Aperformance%20of%20Gaussian%20splatting%20and%20outperforms%20previous%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05075v1&entry.124074799=Read"},
{"title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic\n  Mask Splitting and Growing", "author": "Chaolei Wang and Yang Luo and Jing Du and Siyu Chen and Yiping Chen and Ting Han", "abstract": "  Accurate 3D instance segmentation is crucial for high-quality scene\nunderstanding in the 3D vision domain. However, 3D instance segmentation based\non 2D-to-3D lifting approaches struggle to produce precise instance-level\nsegmentation, due to accumulated errors introduced during the lifting process\nfrom ambiguous semantic guidance and insufficient depth constraints. To tackle\nthese challenges, we propose splitting and growing reliable semantic mask for\nhigh-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\"\nframework that first purifies and splits ambiguous lifted masks using geometric\nprimitives, and then grows them into complete instances within the scene.\nUnlike existing approaches that directly rely on raw lifted masks and sacrifice\nsegmentation accuracy, SGS-3D serves as a training-free refinement method that\njointly fuses semantic and geometric information, enabling effective\ncooperation between the two levels of representation. Specifically, for\nsemantic guidance, we introduce a mask filtering strategy that leverages the\nco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,\nthereby ensuring more reliable semantic consistency with the 3D object\ninstances. For the geometric refinement, we construct fine-grained object\ninstances by exploiting both spatial continuity and high-level features,\nparticularly in the case of semantic ambiguity between distinct objects.\nExperimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that\nSGS-3D substantially improves segmentation accuracy and robustness against\ninaccurate masks from pre-trained models, yielding high-fidelity object\ninstances while maintaining strong generalization across diverse indoor and\noutdoor environments. Code is available in the supplementary materials.\n", "link": "http://arxiv.org/abs/2509.05144v1", "date": "2025-09-05", "relevancy": 3.0971, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6595}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%0A%20%20Mask%20Splitting%20and%20Growing&body=Title%3A%20SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%0A%20%20Mask%20Splitting%20and%20Growing%0AAuthor%3A%20Chaolei%20Wang%20and%20Yang%20Luo%20and%20Jing%20Du%20and%20Siyu%20Chen%20and%20Yiping%20Chen%20and%20Ting%20Han%0AAbstract%3A%20%20%20Accurate%203D%20instance%20segmentation%20is%20crucial%20for%20high-quality%20scene%0Aunderstanding%20in%20the%203D%20vision%20domain.%20However%2C%203D%20instance%20segmentation%20based%0Aon%202D-to-3D%20lifting%20approaches%20struggle%20to%20produce%20precise%20instance-level%0Asegmentation%2C%20due%20to%20accumulated%20errors%20introduced%20during%20the%20lifting%20process%0Afrom%20ambiguous%20semantic%20guidance%20and%20insufficient%20depth%20constraints.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20splitting%20and%20growing%20reliable%20semantic%20mask%20for%0Ahigh-fidelity%203D%20instance%20segmentation%20%28SGS-3D%29%2C%20a%20novel%20%22split-then-grow%22%0Aframework%20that%20first%20purifies%20and%20splits%20ambiguous%20lifted%20masks%20using%20geometric%0Aprimitives%2C%20and%20then%20grows%20them%20into%20complete%20instances%20within%20the%20scene.%0AUnlike%20existing%20approaches%20that%20directly%20rely%20on%20raw%20lifted%20masks%20and%20sacrifice%0Asegmentation%20accuracy%2C%20SGS-3D%20serves%20as%20a%20training-free%20refinement%20method%20that%0Ajointly%20fuses%20semantic%20and%20geometric%20information%2C%20enabling%20effective%0Acooperation%20between%20the%20two%20levels%20of%20representation.%20Specifically%2C%20for%0Asemantic%20guidance%2C%20we%20introduce%20a%20mask%20filtering%20strategy%20that%20leverages%20the%0Aco-occurrence%20of%203D%20geometry%20primitives%20to%20identify%20and%20remove%20ambiguous%20masks%2C%0Athereby%20ensuring%20more%20reliable%20semantic%20consistency%20with%20the%203D%20object%0Ainstances.%20For%20the%20geometric%20refinement%2C%20we%20construct%20fine-grained%20object%0Ainstances%20by%20exploiting%20both%20spatial%20continuity%20and%20high-level%20features%2C%0Aparticularly%20in%20the%20case%20of%20semantic%20ambiguity%20between%20distinct%20objects.%0AExperimental%20results%20on%20ScanNet200%2C%20ScanNet%2B%2B%2C%20and%20KITTI-360%20demonstrate%20that%0ASGS-3D%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%20against%0Ainaccurate%20masks%20from%20pre-trained%20models%2C%20yielding%20high-fidelity%20object%0Ainstances%20while%20maintaining%20strong%20generalization%20across%20diverse%20indoor%20and%0Aoutdoor%20environments.%20Code%20is%20available%20in%20the%20supplementary%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGS-3D%253A%2520High-Fidelity%25203D%2520Instance%2520Segmentation%2520via%2520Reliable%2520Semantic%250A%2520%2520Mask%2520Splitting%2520and%2520Growing%26entry.906535625%3DChaolei%2520Wang%2520and%2520Yang%2520Luo%2520and%2520Jing%2520Du%2520and%2520Siyu%2520Chen%2520and%2520Yiping%2520Chen%2520and%2520Ting%2520Han%26entry.1292438233%3D%2520%2520Accurate%25203D%2520instance%2520segmentation%2520is%2520crucial%2520for%2520high-quality%2520scene%250Aunderstanding%2520in%2520the%25203D%2520vision%2520domain.%2520However%252C%25203D%2520instance%2520segmentation%2520based%250Aon%25202D-to-3D%2520lifting%2520approaches%2520struggle%2520to%2520produce%2520precise%2520instance-level%250Asegmentation%252C%2520due%2520to%2520accumulated%2520errors%2520introduced%2520during%2520the%2520lifting%2520process%250Afrom%2520ambiguous%2520semantic%2520guidance%2520and%2520insufficient%2520depth%2520constraints.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520propose%2520splitting%2520and%2520growing%2520reliable%2520semantic%2520mask%2520for%250Ahigh-fidelity%25203D%2520instance%2520segmentation%2520%2528SGS-3D%2529%252C%2520a%2520novel%2520%2522split-then-grow%2522%250Aframework%2520that%2520first%2520purifies%2520and%2520splits%2520ambiguous%2520lifted%2520masks%2520using%2520geometric%250Aprimitives%252C%2520and%2520then%2520grows%2520them%2520into%2520complete%2520instances%2520within%2520the%2520scene.%250AUnlike%2520existing%2520approaches%2520that%2520directly%2520rely%2520on%2520raw%2520lifted%2520masks%2520and%2520sacrifice%250Asegmentation%2520accuracy%252C%2520SGS-3D%2520serves%2520as%2520a%2520training-free%2520refinement%2520method%2520that%250Ajointly%2520fuses%2520semantic%2520and%2520geometric%2520information%252C%2520enabling%2520effective%250Acooperation%2520between%2520the%2520two%2520levels%2520of%2520representation.%2520Specifically%252C%2520for%250Asemantic%2520guidance%252C%2520we%2520introduce%2520a%2520mask%2520filtering%2520strategy%2520that%2520leverages%2520the%250Aco-occurrence%2520of%25203D%2520geometry%2520primitives%2520to%2520identify%2520and%2520remove%2520ambiguous%2520masks%252C%250Athereby%2520ensuring%2520more%2520reliable%2520semantic%2520consistency%2520with%2520the%25203D%2520object%250Ainstances.%2520For%2520the%2520geometric%2520refinement%252C%2520we%2520construct%2520fine-grained%2520object%250Ainstances%2520by%2520exploiting%2520both%2520spatial%2520continuity%2520and%2520high-level%2520features%252C%250Aparticularly%2520in%2520the%2520case%2520of%2520semantic%2520ambiguity%2520between%2520distinct%2520objects.%250AExperimental%2520results%2520on%2520ScanNet200%252C%2520ScanNet%252B%252B%252C%2520and%2520KITTI-360%2520demonstrate%2520that%250ASGS-3D%2520substantially%2520improves%2520segmentation%2520accuracy%2520and%2520robustness%2520against%250Ainaccurate%2520masks%2520from%2520pre-trained%2520models%252C%2520yielding%2520high-fidelity%2520object%250Ainstances%2520while%2520maintaining%2520strong%2520generalization%2520across%2520diverse%2520indoor%2520and%250Aoutdoor%2520environments.%2520Code%2520is%2520available%2520in%2520the%2520supplementary%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%0A%20%20Mask%20Splitting%20and%20Growing&entry.906535625=Chaolei%20Wang%20and%20Yang%20Luo%20and%20Jing%20Du%20and%20Siyu%20Chen%20and%20Yiping%20Chen%20and%20Ting%20Han&entry.1292438233=%20%20Accurate%203D%20instance%20segmentation%20is%20crucial%20for%20high-quality%20scene%0Aunderstanding%20in%20the%203D%20vision%20domain.%20However%2C%203D%20instance%20segmentation%20based%0Aon%202D-to-3D%20lifting%20approaches%20struggle%20to%20produce%20precise%20instance-level%0Asegmentation%2C%20due%20to%20accumulated%20errors%20introduced%20during%20the%20lifting%20process%0Afrom%20ambiguous%20semantic%20guidance%20and%20insufficient%20depth%20constraints.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20splitting%20and%20growing%20reliable%20semantic%20mask%20for%0Ahigh-fidelity%203D%20instance%20segmentation%20%28SGS-3D%29%2C%20a%20novel%20%22split-then-grow%22%0Aframework%20that%20first%20purifies%20and%20splits%20ambiguous%20lifted%20masks%20using%20geometric%0Aprimitives%2C%20and%20then%20grows%20them%20into%20complete%20instances%20within%20the%20scene.%0AUnlike%20existing%20approaches%20that%20directly%20rely%20on%20raw%20lifted%20masks%20and%20sacrifice%0Asegmentation%20accuracy%2C%20SGS-3D%20serves%20as%20a%20training-free%20refinement%20method%20that%0Ajointly%20fuses%20semantic%20and%20geometric%20information%2C%20enabling%20effective%0Acooperation%20between%20the%20two%20levels%20of%20representation.%20Specifically%2C%20for%0Asemantic%20guidance%2C%20we%20introduce%20a%20mask%20filtering%20strategy%20that%20leverages%20the%0Aco-occurrence%20of%203D%20geometry%20primitives%20to%20identify%20and%20remove%20ambiguous%20masks%2C%0Athereby%20ensuring%20more%20reliable%20semantic%20consistency%20with%20the%203D%20object%0Ainstances.%20For%20the%20geometric%20refinement%2C%20we%20construct%20fine-grained%20object%0Ainstances%20by%20exploiting%20both%20spatial%20continuity%20and%20high-level%20features%2C%0Aparticularly%20in%20the%20case%20of%20semantic%20ambiguity%20between%20distinct%20objects.%0AExperimental%20results%20on%20ScanNet200%2C%20ScanNet%2B%2B%2C%20and%20KITTI-360%20demonstrate%20that%0ASGS-3D%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%20against%0Ainaccurate%20masks%20from%20pre-trained%20models%2C%20yielding%20high-fidelity%20object%0Ainstances%20while%20maintaining%20strong%20generalization%20across%20diverse%20indoor%20and%0Aoutdoor%20environments.%20Code%20is%20available%20in%20the%20supplementary%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05144v1&entry.124074799=Read"},
{"title": "LUIVITON: Learned Universal Interoperable VIrtual Try-ON", "author": "Cong Cao and Xianhang Cheng and Jingyuan Liu and Yujian Zheng and Zhenhui Lin and Meriem Chkir and Hao Li", "abstract": "  We present LUIVITON, an end-to-end system for fully automated virtual try-on,\ncapable of draping complex, multi-layer clothing onto diverse and arbitrarily\nposed humanoid characters. To address the challenge of aligning complex\ngarments with arbitrary and highly diverse body shapes, we use SMPL as a proxy\nrepresentation and separate the clothing-to-body draping problem into two\ncorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,\nwhere each has its unique challenges. While we address the clothing-to-SMPL\nfitting problem using a geometric learning-based approach for\npartial-to-complete shape correspondence prediction, we introduce a diffusion\nmodel-based approach for body-to-SMPL correspondence using multi-view\nconsistent appearance features and a pre-trained 2D foundation model. Our\nmethod can handle complex geometries, non-manifold meshes, and generalizes\neffectively to a wide range of humanoid characters -- including humans, robots,\ncartoon subjects, creatures, and aliens, while maintaining computational\nefficiency for practical adoption. In addition to offering a fully automatic\nfitting solution, LUIVITON supports fast customization of clothing size,\nallowing users to adjust clothing sizes and material properties after they have\nbeen draped. We show that our system can produce high-quality 3D clothing\nfittings without any human labor, even when 2D clothing sewing patterns are not\navailable.\n", "link": "http://arxiv.org/abs/2509.05030v1", "date": "2025-09-05", "relevancy": 3.0251, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6116}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6071}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUIVITON%3A%20Learned%20Universal%20Interoperable%20VIrtual%20Try-ON&body=Title%3A%20LUIVITON%3A%20Learned%20Universal%20Interoperable%20VIrtual%20Try-ON%0AAuthor%3A%20Cong%20Cao%20and%20Xianhang%20Cheng%20and%20Jingyuan%20Liu%20and%20Yujian%20Zheng%20and%20Zhenhui%20Lin%20and%20Meriem%20Chkir%20and%20Hao%20Li%0AAbstract%3A%20%20%20We%20present%20LUIVITON%2C%20an%20end-to-end%20system%20for%20fully%20automated%20virtual%20try-on%2C%0Acapable%20of%20draping%20complex%2C%20multi-layer%20clothing%20onto%20diverse%20and%20arbitrarily%0Aposed%20humanoid%20characters.%20To%20address%20the%20challenge%20of%20aligning%20complex%0Agarments%20with%20arbitrary%20and%20highly%20diverse%20body%20shapes%2C%20we%20use%20SMPL%20as%20a%20proxy%0Arepresentation%20and%20separate%20the%20clothing-to-body%20draping%20problem%20into%20two%0Acorrespondence%20tasks%3A%201%29%20clothing-to-SMPL%20and%202%29%20body-to-SMPL%20correspondence%2C%0Awhere%20each%20has%20its%20unique%20challenges.%20While%20we%20address%20the%20clothing-to-SMPL%0Afitting%20problem%20using%20a%20geometric%20learning-based%20approach%20for%0Apartial-to-complete%20shape%20correspondence%20prediction%2C%20we%20introduce%20a%20diffusion%0Amodel-based%20approach%20for%20body-to-SMPL%20correspondence%20using%20multi-view%0Aconsistent%20appearance%20features%20and%20a%20pre-trained%202D%20foundation%20model.%20Our%0Amethod%20can%20handle%20complex%20geometries%2C%20non-manifold%20meshes%2C%20and%20generalizes%0Aeffectively%20to%20a%20wide%20range%20of%20humanoid%20characters%20--%20including%20humans%2C%20robots%2C%0Acartoon%20subjects%2C%20creatures%2C%20and%20aliens%2C%20while%20maintaining%20computational%0Aefficiency%20for%20practical%20adoption.%20In%20addition%20to%20offering%20a%20fully%20automatic%0Afitting%20solution%2C%20LUIVITON%20supports%20fast%20customization%20of%20clothing%20size%2C%0Aallowing%20users%20to%20adjust%20clothing%20sizes%20and%20material%20properties%20after%20they%20have%0Abeen%20draped.%20We%20show%20that%20our%20system%20can%20produce%20high-quality%203D%20clothing%0Afittings%20without%20any%20human%20labor%2C%20even%20when%202D%20clothing%20sewing%20patterns%20are%20not%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUIVITON%253A%2520Learned%2520Universal%2520Interoperable%2520VIrtual%2520Try-ON%26entry.906535625%3DCong%2520Cao%2520and%2520Xianhang%2520Cheng%2520and%2520Jingyuan%2520Liu%2520and%2520Yujian%2520Zheng%2520and%2520Zhenhui%2520Lin%2520and%2520Meriem%2520Chkir%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520LUIVITON%252C%2520an%2520end-to-end%2520system%2520for%2520fully%2520automated%2520virtual%2520try-on%252C%250Acapable%2520of%2520draping%2520complex%252C%2520multi-layer%2520clothing%2520onto%2520diverse%2520and%2520arbitrarily%250Aposed%2520humanoid%2520characters.%2520To%2520address%2520the%2520challenge%2520of%2520aligning%2520complex%250Agarments%2520with%2520arbitrary%2520and%2520highly%2520diverse%2520body%2520shapes%252C%2520we%2520use%2520SMPL%2520as%2520a%2520proxy%250Arepresentation%2520and%2520separate%2520the%2520clothing-to-body%2520draping%2520problem%2520into%2520two%250Acorrespondence%2520tasks%253A%25201%2529%2520clothing-to-SMPL%2520and%25202%2529%2520body-to-SMPL%2520correspondence%252C%250Awhere%2520each%2520has%2520its%2520unique%2520challenges.%2520While%2520we%2520address%2520the%2520clothing-to-SMPL%250Afitting%2520problem%2520using%2520a%2520geometric%2520learning-based%2520approach%2520for%250Apartial-to-complete%2520shape%2520correspondence%2520prediction%252C%2520we%2520introduce%2520a%2520diffusion%250Amodel-based%2520approach%2520for%2520body-to-SMPL%2520correspondence%2520using%2520multi-view%250Aconsistent%2520appearance%2520features%2520and%2520a%2520pre-trained%25202D%2520foundation%2520model.%2520Our%250Amethod%2520can%2520handle%2520complex%2520geometries%252C%2520non-manifold%2520meshes%252C%2520and%2520generalizes%250Aeffectively%2520to%2520a%2520wide%2520range%2520of%2520humanoid%2520characters%2520--%2520including%2520humans%252C%2520robots%252C%250Acartoon%2520subjects%252C%2520creatures%252C%2520and%2520aliens%252C%2520while%2520maintaining%2520computational%250Aefficiency%2520for%2520practical%2520adoption.%2520In%2520addition%2520to%2520offering%2520a%2520fully%2520automatic%250Afitting%2520solution%252C%2520LUIVITON%2520supports%2520fast%2520customization%2520of%2520clothing%2520size%252C%250Aallowing%2520users%2520to%2520adjust%2520clothing%2520sizes%2520and%2520material%2520properties%2520after%2520they%2520have%250Abeen%2520draped.%2520We%2520show%2520that%2520our%2520system%2520can%2520produce%2520high-quality%25203D%2520clothing%250Afittings%2520without%2520any%2520human%2520labor%252C%2520even%2520when%25202D%2520clothing%2520sewing%2520patterns%2520are%2520not%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUIVITON%3A%20Learned%20Universal%20Interoperable%20VIrtual%20Try-ON&entry.906535625=Cong%20Cao%20and%20Xianhang%20Cheng%20and%20Jingyuan%20Liu%20and%20Yujian%20Zheng%20and%20Zhenhui%20Lin%20and%20Meriem%20Chkir%20and%20Hao%20Li&entry.1292438233=%20%20We%20present%20LUIVITON%2C%20an%20end-to-end%20system%20for%20fully%20automated%20virtual%20try-on%2C%0Acapable%20of%20draping%20complex%2C%20multi-layer%20clothing%20onto%20diverse%20and%20arbitrarily%0Aposed%20humanoid%20characters.%20To%20address%20the%20challenge%20of%20aligning%20complex%0Agarments%20with%20arbitrary%20and%20highly%20diverse%20body%20shapes%2C%20we%20use%20SMPL%20as%20a%20proxy%0Arepresentation%20and%20separate%20the%20clothing-to-body%20draping%20problem%20into%20two%0Acorrespondence%20tasks%3A%201%29%20clothing-to-SMPL%20and%202%29%20body-to-SMPL%20correspondence%2C%0Awhere%20each%20has%20its%20unique%20challenges.%20While%20we%20address%20the%20clothing-to-SMPL%0Afitting%20problem%20using%20a%20geometric%20learning-based%20approach%20for%0Apartial-to-complete%20shape%20correspondence%20prediction%2C%20we%20introduce%20a%20diffusion%0Amodel-based%20approach%20for%20body-to-SMPL%20correspondence%20using%20multi-view%0Aconsistent%20appearance%20features%20and%20a%20pre-trained%202D%20foundation%20model.%20Our%0Amethod%20can%20handle%20complex%20geometries%2C%20non-manifold%20meshes%2C%20and%20generalizes%0Aeffectively%20to%20a%20wide%20range%20of%20humanoid%20characters%20--%20including%20humans%2C%20robots%2C%0Acartoon%20subjects%2C%20creatures%2C%20and%20aliens%2C%20while%20maintaining%20computational%0Aefficiency%20for%20practical%20adoption.%20In%20addition%20to%20offering%20a%20fully%20automatic%0Afitting%20solution%2C%20LUIVITON%20supports%20fast%20customization%20of%20clothing%20size%2C%0Aallowing%20users%20to%20adjust%20clothing%20sizes%20and%20material%20properties%20after%20they%20have%0Abeen%20draped.%20We%20show%20that%20our%20system%20can%20produce%20high-quality%203D%20clothing%0Afittings%20without%20any%20human%20labor%2C%20even%20when%202D%20clothing%20sewing%20patterns%20are%20not%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05030v1&entry.124074799=Read"},
{"title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced\n  Medical Image Segmentation", "author": "Julia Dietlmeier and Oluwabukola Grace Adegboro and Vayangi Ganepola and Claudia Mazo and Noel E. O'Connor", "abstract": "  Vision-language models and their adaptations to image segmentation tasks\npresent enormous potential for producing highly accurate and interpretable\nresults. However, implementations based on CLIP and BiomedCLIP are still\nlagging behind more sophisticated architectures such as CRIS. In this work,\ninstead of focusing on text prompt engineering as is the norm, we attempt to\nnarrow this gap by showing how to ensemble vision-language segmentation models\n(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice\nscore improvement of 6.3% on the BKAI polyp dataset using the ensembled\nBiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.\nFurthermore, we provide initial results on additional four radiology and\nnon-radiology datasets. We conclude that ensembling works differently across\nthese datasets (from outperforming to underperforming the CRIS model),\nindicating a topic for future investigation by the community. The code is\navailable at https://github.com/juliadietlmeier/VLSM-Ensemble.\n", "link": "http://arxiv.org/abs/2509.05154v1", "date": "2025-09-05", "relevancy": 2.9878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLSM-Ensemble%3A%20Ensembling%20CLIP-based%20Vision-Language%20Models%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20VLSM-Ensemble%3A%20Ensembling%20CLIP-based%20Vision-Language%20Models%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Julia%20Dietlmeier%20and%20Oluwabukola%20Grace%20Adegboro%20and%20Vayangi%20Ganepola%20and%20Claudia%20Mazo%20and%20Noel%20E.%20O%27Connor%0AAbstract%3A%20%20%20Vision-language%20models%20and%20their%20adaptations%20to%20image%20segmentation%20tasks%0Apresent%20enormous%20potential%20for%20producing%20highly%20accurate%20and%20interpretable%0Aresults.%20However%2C%20implementations%20based%20on%20CLIP%20and%20BiomedCLIP%20are%20still%0Alagging%20behind%20more%20sophisticated%20architectures%20such%20as%20CRIS.%20In%20this%20work%2C%0Ainstead%20of%20focusing%20on%20text%20prompt%20engineering%20as%20is%20the%20norm%2C%20we%20attempt%20to%0Anarrow%20this%20gap%20by%20showing%20how%20to%20ensemble%20vision-language%20segmentation%20models%0A%28VLSMs%29%20with%20a%20low-complexity%20CNN.%20By%20doing%20so%2C%20we%20achieve%20a%20significant%20Dice%0Ascore%20improvement%20of%206.3%25%20on%20the%20BKAI%20polyp%20dataset%20using%20the%20ensembled%0ABiomedCLIPSeg%2C%20while%20other%20datasets%20exhibit%20gains%20ranging%20from%201%25%20to%206%25.%0AFurthermore%2C%20we%20provide%20initial%20results%20on%20additional%20four%20radiology%20and%0Anon-radiology%20datasets.%20We%20conclude%20that%20ensembling%20works%20differently%20across%0Athese%20datasets%20%28from%20outperforming%20to%20underperforming%20the%20CRIS%20model%29%2C%0Aindicating%20a%20topic%20for%20future%20investigation%20by%20the%20community.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/juliadietlmeier/VLSM-Ensemble.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLSM-Ensemble%253A%2520Ensembling%2520CLIP-based%2520Vision-Language%2520Models%2520for%2520Enhanced%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJulia%2520Dietlmeier%2520and%2520Oluwabukola%2520Grace%2520Adegboro%2520and%2520Vayangi%2520Ganepola%2520and%2520Claudia%2520Mazo%2520and%2520Noel%2520E.%2520O%2527Connor%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520and%2520their%2520adaptations%2520to%2520image%2520segmentation%2520tasks%250Apresent%2520enormous%2520potential%2520for%2520producing%2520highly%2520accurate%2520and%2520interpretable%250Aresults.%2520However%252C%2520implementations%2520based%2520on%2520CLIP%2520and%2520BiomedCLIP%2520are%2520still%250Alagging%2520behind%2520more%2520sophisticated%2520architectures%2520such%2520as%2520CRIS.%2520In%2520this%2520work%252C%250Ainstead%2520of%2520focusing%2520on%2520text%2520prompt%2520engineering%2520as%2520is%2520the%2520norm%252C%2520we%2520attempt%2520to%250Anarrow%2520this%2520gap%2520by%2520showing%2520how%2520to%2520ensemble%2520vision-language%2520segmentation%2520models%250A%2528VLSMs%2529%2520with%2520a%2520low-complexity%2520CNN.%2520By%2520doing%2520so%252C%2520we%2520achieve%2520a%2520significant%2520Dice%250Ascore%2520improvement%2520of%25206.3%2525%2520on%2520the%2520BKAI%2520polyp%2520dataset%2520using%2520the%2520ensembled%250ABiomedCLIPSeg%252C%2520while%2520other%2520datasets%2520exhibit%2520gains%2520ranging%2520from%25201%2525%2520to%25206%2525.%250AFurthermore%252C%2520we%2520provide%2520initial%2520results%2520on%2520additional%2520four%2520radiology%2520and%250Anon-radiology%2520datasets.%2520We%2520conclude%2520that%2520ensembling%2520works%2520differently%2520across%250Athese%2520datasets%2520%2528from%2520outperforming%2520to%2520underperforming%2520the%2520CRIS%2520model%2529%252C%250Aindicating%2520a%2520topic%2520for%2520future%2520investigation%2520by%2520the%2520community.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/juliadietlmeier/VLSM-Ensemble.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLSM-Ensemble%3A%20Ensembling%20CLIP-based%20Vision-Language%20Models%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Julia%20Dietlmeier%20and%20Oluwabukola%20Grace%20Adegboro%20and%20Vayangi%20Ganepola%20and%20Claudia%20Mazo%20and%20Noel%20E.%20O%27Connor&entry.1292438233=%20%20Vision-language%20models%20and%20their%20adaptations%20to%20image%20segmentation%20tasks%0Apresent%20enormous%20potential%20for%20producing%20highly%20accurate%20and%20interpretable%0Aresults.%20However%2C%20implementations%20based%20on%20CLIP%20and%20BiomedCLIP%20are%20still%0Alagging%20behind%20more%20sophisticated%20architectures%20such%20as%20CRIS.%20In%20this%20work%2C%0Ainstead%20of%20focusing%20on%20text%20prompt%20engineering%20as%20is%20the%20norm%2C%20we%20attempt%20to%0Anarrow%20this%20gap%20by%20showing%20how%20to%20ensemble%20vision-language%20segmentation%20models%0A%28VLSMs%29%20with%20a%20low-complexity%20CNN.%20By%20doing%20so%2C%20we%20achieve%20a%20significant%20Dice%0Ascore%20improvement%20of%206.3%25%20on%20the%20BKAI%20polyp%20dataset%20using%20the%20ensembled%0ABiomedCLIPSeg%2C%20while%20other%20datasets%20exhibit%20gains%20ranging%20from%201%25%20to%206%25.%0AFurthermore%2C%20we%20provide%20initial%20results%20on%20additional%20four%20radiology%20and%0Anon-radiology%20datasets.%20We%20conclude%20that%20ensembling%20works%20differently%20across%0Athese%20datasets%20%28from%20outperforming%20to%20underperforming%20the%20CRIS%20model%29%2C%0Aindicating%20a%20topic%20for%20future%20investigation%20by%20the%20community.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/juliadietlmeier/VLSM-Ensemble.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05154v1&entry.124074799=Read"},
{"title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation", "author": "Yinglin Duan and Zhengxia Zou and Tongwei Gu and Wei Jia and Zhan Zhao and Luyi Xu and Xinzhu Liu and Hao Jiang and Kang Chen and Shuang Qiu", "abstract": "  Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18\n", "link": "http://arxiv.org/abs/2509.05263v1", "date": "2025-09-05", "relevancy": 2.8921, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation&body=Title%3A%20LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation%0AAuthor%3A%20Yinglin%20Duan%20and%20Zhengxia%20Zou%20and%20Tongwei%20Gu%20and%20Wei%20Jia%20and%20Zhan%20Zhao%20and%20Luyi%20Xu%20and%20Xinzhu%20Liu%20and%20Hao%20Jiang%20and%20Kang%20Chen%20and%20Shuang%20Qiu%0AAbstract%3A%20%20%20Recent%20research%20has%20been%20increasingly%20focusing%20on%20developing%203D%20world%20models%0Athat%20simulate%20complex%20real-world%20scenarios.%20World%20models%20have%20found%20broad%0Aapplications%20across%20various%20domains%2C%20including%20embodied%20AI%2C%20autonomous%20driving%2C%0Aentertainment%2C%20etc.%20A%20more%20realistic%20simulation%20with%20accurate%20physics%20will%0Aeffectively%20narrow%20the%20sim-to-real%20gap%20and%20allow%20us%20to%20gather%20rich%20information%0Aabout%20the%20real%20world%20conveniently.%20While%20traditional%20manual%20modeling%20has%0Aenabled%20the%20creation%20of%20virtual%203D%20scenes%2C%20modern%20approaches%20have%20leveraged%0Aadvanced%20machine%20learning%20algorithms%20for%203D%20world%20generation%2C%20with%20most%20recent%0Aadvances%20focusing%20on%20generative%20methods%20that%20can%20create%20virtual%20worlds%20based%20on%0Auser%20instructions.%20This%20work%20explores%20such%20a%20research%20direction%20by%20proposing%0ALatticeWorld%2C%20a%20simple%20yet%20effective%203D%20world%20generation%20framework%20that%0Astreamlines%20the%20industrial%20production%20pipeline%20of%203D%20environments.%20LatticeWorld%0Aleverages%20lightweight%20LLMs%20%28LLaMA-2-7B%29%20alongside%20the%20industry-grade%20rendering%0Aengine%20%28e.g.%2C%20Unreal%20Engine%205%29%20to%20generate%20a%20dynamic%20environment.%20Our%20proposed%0Aframework%20accepts%20textual%20descriptions%20and%20visual%20instructions%20as%20multimodal%0Ainputs%20and%20creates%20large-scale%203D%20interactive%20worlds%20with%20dynamic%20agents%2C%0Afeaturing%20competitive%20multi-agent%20interaction%2C%20high-fidelity%20physics%0Asimulation%2C%20and%20real-time%20rendering.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20LatticeWorld%2C%20showing%20that%20it%20achieves%20superior%20accuracy%20in%20scene%0Alayout%20generation%20and%20visual%20fidelity.%20Moreover%2C%20LatticeWorld%20achieves%20over%20a%0A%2490%5Ctimes%24%20increase%20in%20industrial%20production%20efficiency%20while%20maintaining%20high%0Acreative%20quality%20compared%20with%20traditional%20manual%20production%20methods.%20Our%20demo%0Avideo%20is%20available%20at%20https%3A//youtu.be/8VWZXpERR18%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatticeWorld%253A%2520A%2520Multimodal%2520Large%2520Language%2520Model-Empowered%2520Framework%2520for%250A%2520%2520Interactive%2520Complex%2520World%2520Generation%26entry.906535625%3DYinglin%2520Duan%2520and%2520Zhengxia%2520Zou%2520and%2520Tongwei%2520Gu%2520and%2520Wei%2520Jia%2520and%2520Zhan%2520Zhao%2520and%2520Luyi%2520Xu%2520and%2520Xinzhu%2520Liu%2520and%2520Hao%2520Jiang%2520and%2520Kang%2520Chen%2520and%2520Shuang%2520Qiu%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520been%2520increasingly%2520focusing%2520on%2520developing%25203D%2520world%2520models%250Athat%2520simulate%2520complex%2520real-world%2520scenarios.%2520World%2520models%2520have%2520found%2520broad%250Aapplications%2520across%2520various%2520domains%252C%2520including%2520embodied%2520AI%252C%2520autonomous%2520driving%252C%250Aentertainment%252C%2520etc.%2520A%2520more%2520realistic%2520simulation%2520with%2520accurate%2520physics%2520will%250Aeffectively%2520narrow%2520the%2520sim-to-real%2520gap%2520and%2520allow%2520us%2520to%2520gather%2520rich%2520information%250Aabout%2520the%2520real%2520world%2520conveniently.%2520While%2520traditional%2520manual%2520modeling%2520has%250Aenabled%2520the%2520creation%2520of%2520virtual%25203D%2520scenes%252C%2520modern%2520approaches%2520have%2520leveraged%250Aadvanced%2520machine%2520learning%2520algorithms%2520for%25203D%2520world%2520generation%252C%2520with%2520most%2520recent%250Aadvances%2520focusing%2520on%2520generative%2520methods%2520that%2520can%2520create%2520virtual%2520worlds%2520based%2520on%250Auser%2520instructions.%2520This%2520work%2520explores%2520such%2520a%2520research%2520direction%2520by%2520proposing%250ALatticeWorld%252C%2520a%2520simple%2520yet%2520effective%25203D%2520world%2520generation%2520framework%2520that%250Astreamlines%2520the%2520industrial%2520production%2520pipeline%2520of%25203D%2520environments.%2520LatticeWorld%250Aleverages%2520lightweight%2520LLMs%2520%2528LLaMA-2-7B%2529%2520alongside%2520the%2520industry-grade%2520rendering%250Aengine%2520%2528e.g.%252C%2520Unreal%2520Engine%25205%2529%2520to%2520generate%2520a%2520dynamic%2520environment.%2520Our%2520proposed%250Aframework%2520accepts%2520textual%2520descriptions%2520and%2520visual%2520instructions%2520as%2520multimodal%250Ainputs%2520and%2520creates%2520large-scale%25203D%2520interactive%2520worlds%2520with%2520dynamic%2520agents%252C%250Afeaturing%2520competitive%2520multi-agent%2520interaction%252C%2520high-fidelity%2520physics%250Asimulation%252C%2520and%2520real-time%2520rendering.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%250Aevaluate%2520LatticeWorld%252C%2520showing%2520that%2520it%2520achieves%2520superior%2520accuracy%2520in%2520scene%250Alayout%2520generation%2520and%2520visual%2520fidelity.%2520Moreover%252C%2520LatticeWorld%2520achieves%2520over%2520a%250A%252490%255Ctimes%2524%2520increase%2520in%2520industrial%2520production%2520efficiency%2520while%2520maintaining%2520high%250Acreative%2520quality%2520compared%2520with%2520traditional%2520manual%2520production%2520methods.%2520Our%2520demo%250Avideo%2520is%2520available%2520at%2520https%253A//youtu.be/8VWZXpERR18%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation&entry.906535625=Yinglin%20Duan%20and%20Zhengxia%20Zou%20and%20Tongwei%20Gu%20and%20Wei%20Jia%20and%20Zhan%20Zhao%20and%20Luyi%20Xu%20and%20Xinzhu%20Liu%20and%20Hao%20Jiang%20and%20Kang%20Chen%20and%20Shuang%20Qiu&entry.1292438233=%20%20Recent%20research%20has%20been%20increasingly%20focusing%20on%20developing%203D%20world%20models%0Athat%20simulate%20complex%20real-world%20scenarios.%20World%20models%20have%20found%20broad%0Aapplications%20across%20various%20domains%2C%20including%20embodied%20AI%2C%20autonomous%20driving%2C%0Aentertainment%2C%20etc.%20A%20more%20realistic%20simulation%20with%20accurate%20physics%20will%0Aeffectively%20narrow%20the%20sim-to-real%20gap%20and%20allow%20us%20to%20gather%20rich%20information%0Aabout%20the%20real%20world%20conveniently.%20While%20traditional%20manual%20modeling%20has%0Aenabled%20the%20creation%20of%20virtual%203D%20scenes%2C%20modern%20approaches%20have%20leveraged%0Aadvanced%20machine%20learning%20algorithms%20for%203D%20world%20generation%2C%20with%20most%20recent%0Aadvances%20focusing%20on%20generative%20methods%20that%20can%20create%20virtual%20worlds%20based%20on%0Auser%20instructions.%20This%20work%20explores%20such%20a%20research%20direction%20by%20proposing%0ALatticeWorld%2C%20a%20simple%20yet%20effective%203D%20world%20generation%20framework%20that%0Astreamlines%20the%20industrial%20production%20pipeline%20of%203D%20environments.%20LatticeWorld%0Aleverages%20lightweight%20LLMs%20%28LLaMA-2-7B%29%20alongside%20the%20industry-grade%20rendering%0Aengine%20%28e.g.%2C%20Unreal%20Engine%205%29%20to%20generate%20a%20dynamic%20environment.%20Our%20proposed%0Aframework%20accepts%20textual%20descriptions%20and%20visual%20instructions%20as%20multimodal%0Ainputs%20and%20creates%20large-scale%203D%20interactive%20worlds%20with%20dynamic%20agents%2C%0Afeaturing%20competitive%20multi-agent%20interaction%2C%20high-fidelity%20physics%0Asimulation%2C%20and%20real-time%20rendering.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20LatticeWorld%2C%20showing%20that%20it%20achieves%20superior%20accuracy%20in%20scene%0Alayout%20generation%20and%20visual%20fidelity.%20Moreover%2C%20LatticeWorld%20achieves%20over%20a%0A%2490%5Ctimes%24%20increase%20in%20industrial%20production%20efficiency%20while%20maintaining%20high%0Acreative%20quality%20compared%20with%20traditional%20manual%20production%20methods.%20Our%20demo%0Avideo%20is%20available%20at%20https%3A//youtu.be/8VWZXpERR18%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05263v1&entry.124074799=Read"},
{"title": "Beyond the Linear Separability Ceiling: Aligning Representations in VLMs", "author": "Enrico Vompa and Tanel Tammet and Mohit Vaishnav", "abstract": "  A challenge in advancing Visual-Language Models (VLMs) is determining whether\ntheir failures on abstract reasoning tasks, such as Bongard problems, stem from\nflawed perception or faulty top-down reasoning. To disentangle these factors,\nwe introduce a diagnostic framework centered on the Linear Separability Ceiling\n(LSC), the performance achievable by a linear classifier on a VLM's raw visual\nembeddings. Applying this framework to state-of-the-art VLMs, we uncover a\npervasive \"alignment gap\", where most models fail to generatively outperform\nthe linear separability of their own representations. We find that the few\nmodels surpassing this ceiling do so via two mechanisms: by further refining\nvisual representations into a more linearly separable format or by executing\nnon-linear decision logic. We demonstrate that this bottleneck is not a\nfundamental limitation but a solvable alignment issue. By augmenting standard\nnext-token prediction with a contrastive objective, our fine-tuning method\nactivates dormant reasoning pathways, systematically improving the linear\nstructure of representations to significantly surpass the LSC.\n", "link": "http://arxiv.org/abs/2507.07574v2", "date": "2025-09-05", "relevancy": 2.8816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Linear%20Separability%20Ceiling%3A%20Aligning%20Representations%20in%20VLMs&body=Title%3A%20Beyond%20the%20Linear%20Separability%20Ceiling%3A%20Aligning%20Representations%20in%20VLMs%0AAuthor%3A%20Enrico%20Vompa%20and%20Tanel%20Tammet%20and%20Mohit%20Vaishnav%0AAbstract%3A%20%20%20A%20challenge%20in%20advancing%20Visual-Language%20Models%20%28VLMs%29%20is%20determining%20whether%0Atheir%20failures%20on%20abstract%20reasoning%20tasks%2C%20such%20as%20Bongard%20problems%2C%20stem%20from%0Aflawed%20perception%20or%20faulty%20top-down%20reasoning.%20To%20disentangle%20these%20factors%2C%0Awe%20introduce%20a%20diagnostic%20framework%20centered%20on%20the%20Linear%20Separability%20Ceiling%0A%28LSC%29%2C%20the%20performance%20achievable%20by%20a%20linear%20classifier%20on%20a%20VLM%27s%20raw%20visual%0Aembeddings.%20Applying%20this%20framework%20to%20state-of-the-art%20VLMs%2C%20we%20uncover%20a%0Apervasive%20%22alignment%20gap%22%2C%20where%20most%20models%20fail%20to%20generatively%20outperform%0Athe%20linear%20separability%20of%20their%20own%20representations.%20We%20find%20that%20the%20few%0Amodels%20surpassing%20this%20ceiling%20do%20so%20via%20two%20mechanisms%3A%20by%20further%20refining%0Avisual%20representations%20into%20a%20more%20linearly%20separable%20format%20or%20by%20executing%0Anon-linear%20decision%20logic.%20We%20demonstrate%20that%20this%20bottleneck%20is%20not%20a%0Afundamental%20limitation%20but%20a%20solvable%20alignment%20issue.%20By%20augmenting%20standard%0Anext-token%20prediction%20with%20a%20contrastive%20objective%2C%20our%20fine-tuning%20method%0Aactivates%20dormant%20reasoning%20pathways%2C%20systematically%20improving%20the%20linear%0Astructure%20of%20representations%20to%20significantly%20surpass%20the%20LSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Linear%2520Separability%2520Ceiling%253A%2520Aligning%2520Representations%2520in%2520VLMs%26entry.906535625%3DEnrico%2520Vompa%2520and%2520Tanel%2520Tammet%2520and%2520Mohit%2520Vaishnav%26entry.1292438233%3D%2520%2520A%2520challenge%2520in%2520advancing%2520Visual-Language%2520Models%2520%2528VLMs%2529%2520is%2520determining%2520whether%250Atheir%2520failures%2520on%2520abstract%2520reasoning%2520tasks%252C%2520such%2520as%2520Bongard%2520problems%252C%2520stem%2520from%250Aflawed%2520perception%2520or%2520faulty%2520top-down%2520reasoning.%2520To%2520disentangle%2520these%2520factors%252C%250Awe%2520introduce%2520a%2520diagnostic%2520framework%2520centered%2520on%2520the%2520Linear%2520Separability%2520Ceiling%250A%2528LSC%2529%252C%2520the%2520performance%2520achievable%2520by%2520a%2520linear%2520classifier%2520on%2520a%2520VLM%2527s%2520raw%2520visual%250Aembeddings.%2520Applying%2520this%2520framework%2520to%2520state-of-the-art%2520VLMs%252C%2520we%2520uncover%2520a%250Apervasive%2520%2522alignment%2520gap%2522%252C%2520where%2520most%2520models%2520fail%2520to%2520generatively%2520outperform%250Athe%2520linear%2520separability%2520of%2520their%2520own%2520representations.%2520We%2520find%2520that%2520the%2520few%250Amodels%2520surpassing%2520this%2520ceiling%2520do%2520so%2520via%2520two%2520mechanisms%253A%2520by%2520further%2520refining%250Avisual%2520representations%2520into%2520a%2520more%2520linearly%2520separable%2520format%2520or%2520by%2520executing%250Anon-linear%2520decision%2520logic.%2520We%2520demonstrate%2520that%2520this%2520bottleneck%2520is%2520not%2520a%250Afundamental%2520limitation%2520but%2520a%2520solvable%2520alignment%2520issue.%2520By%2520augmenting%2520standard%250Anext-token%2520prediction%2520with%2520a%2520contrastive%2520objective%252C%2520our%2520fine-tuning%2520method%250Aactivates%2520dormant%2520reasoning%2520pathways%252C%2520systematically%2520improving%2520the%2520linear%250Astructure%2520of%2520representations%2520to%2520significantly%2520surpass%2520the%2520LSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Linear%20Separability%20Ceiling%3A%20Aligning%20Representations%20in%20VLMs&entry.906535625=Enrico%20Vompa%20and%20Tanel%20Tammet%20and%20Mohit%20Vaishnav&entry.1292438233=%20%20A%20challenge%20in%20advancing%20Visual-Language%20Models%20%28VLMs%29%20is%20determining%20whether%0Atheir%20failures%20on%20abstract%20reasoning%20tasks%2C%20such%20as%20Bongard%20problems%2C%20stem%20from%0Aflawed%20perception%20or%20faulty%20top-down%20reasoning.%20To%20disentangle%20these%20factors%2C%0Awe%20introduce%20a%20diagnostic%20framework%20centered%20on%20the%20Linear%20Separability%20Ceiling%0A%28LSC%29%2C%20the%20performance%20achievable%20by%20a%20linear%20classifier%20on%20a%20VLM%27s%20raw%20visual%0Aembeddings.%20Applying%20this%20framework%20to%20state-of-the-art%20VLMs%2C%20we%20uncover%20a%0Apervasive%20%22alignment%20gap%22%2C%20where%20most%20models%20fail%20to%20generatively%20outperform%0Athe%20linear%20separability%20of%20their%20own%20representations.%20We%20find%20that%20the%20few%0Amodels%20surpassing%20this%20ceiling%20do%20so%20via%20two%20mechanisms%3A%20by%20further%20refining%0Avisual%20representations%20into%20a%20more%20linearly%20separable%20format%20or%20by%20executing%0Anon-linear%20decision%20logic.%20We%20demonstrate%20that%20this%20bottleneck%20is%20not%20a%0Afundamental%20limitation%20but%20a%20solvable%20alignment%20issue.%20By%20augmenting%20standard%0Anext-token%20prediction%20with%20a%20contrastive%20objective%2C%20our%20fine-tuning%20method%0Aactivates%20dormant%20reasoning%20pathways%2C%20systematically%20improving%20the%20linear%0Astructure%20of%20representations%20to%20significantly%20surpass%20the%20LSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07574v2&entry.124074799=Read"},
{"title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining", "author": "Deniz Bayazit and Aaron Mueller and Antoine Bosselut", "abstract": "  Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.\n", "link": "http://arxiv.org/abs/2509.05291v1", "date": "2025-09-05", "relevancy": 2.8762, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crosscoding%20Through%20Time%3A%20Tracking%20Emergence%20%26%20Consolidation%20Of%0A%20%20Linguistic%20Representations%20Throughout%20LLM%20Pretraining&body=Title%3A%20Crosscoding%20Through%20Time%3A%20Tracking%20Emergence%20%26%20Consolidation%20Of%0A%20%20Linguistic%20Representations%20Throughout%20LLM%20Pretraining%0AAuthor%3A%20Deniz%20Bayazit%20and%20Aaron%20Mueller%20and%20Antoine%20Bosselut%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20learn%20non-trivial%20abstractions%20during%0Apretraining%2C%20like%20detecting%20irregular%20plural%20noun%20subjects.%20However%2C%20it%20is%20not%0Awell%20understood%20when%20and%20how%20specific%20linguistic%20abilities%20emerge%20as%0Atraditional%20evaluation%20methods%20such%20as%20benchmarking%20fail%20to%20reveal%20how%20models%0Aacquire%20concepts%20and%20capabilities.%20To%20bridge%20this%20gap%20and%20better%20understand%0Amodel%20training%20at%20the%20concept%20level%2C%20we%20use%20sparse%20crosscoders%20to%20discover%20and%0Aalign%20features%20across%20model%20checkpoints.%20Using%20this%20approach%2C%20we%20track%20the%0Aevolution%20of%20linguistic%20features%20during%20pretraining.%20We%20train%20crosscoders%0Abetween%20open-sourced%20checkpoint%20triplets%20with%20significant%20performance%20and%0Arepresentation%20shifts%2C%20and%20introduce%20a%20novel%20metric%2C%20Relative%20Indirect%20Effects%0A%28RelIE%29%2C%20to%20trace%20training%20stages%20at%20which%20individual%20features%20become%20causally%0Aimportant%20for%20task%20performance.%20We%20show%20that%20crosscoders%20can%20detect%20feature%0Aemergence%2C%20maintenance%2C%20and%20discontinuation%20during%20pretraining.%20Our%20approach%20is%0Aarchitecture-agnostic%20and%20scalable%2C%20offering%20a%20promising%20path%20toward%20more%0Ainterpretable%20and%20fine-grained%20analysis%20of%20representation%20learning%20throughout%0Apretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrosscoding%2520Through%2520Time%253A%2520Tracking%2520Emergence%2520%2526%2520Consolidation%2520Of%250A%2520%2520Linguistic%2520Representations%2520Throughout%2520LLM%2520Pretraining%26entry.906535625%3DDeniz%2520Bayazit%2520and%2520Aaron%2520Mueller%2520and%2520Antoine%2520Bosselut%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520learn%2520non-trivial%2520abstractions%2520during%250Apretraining%252C%2520like%2520detecting%2520irregular%2520plural%2520noun%2520subjects.%2520However%252C%2520it%2520is%2520not%250Awell%2520understood%2520when%2520and%2520how%2520specific%2520linguistic%2520abilities%2520emerge%2520as%250Atraditional%2520evaluation%2520methods%2520such%2520as%2520benchmarking%2520fail%2520to%2520reveal%2520how%2520models%250Aacquire%2520concepts%2520and%2520capabilities.%2520To%2520bridge%2520this%2520gap%2520and%2520better%2520understand%250Amodel%2520training%2520at%2520the%2520concept%2520level%252C%2520we%2520use%2520sparse%2520crosscoders%2520to%2520discover%2520and%250Aalign%2520features%2520across%2520model%2520checkpoints.%2520Using%2520this%2520approach%252C%2520we%2520track%2520the%250Aevolution%2520of%2520linguistic%2520features%2520during%2520pretraining.%2520We%2520train%2520crosscoders%250Abetween%2520open-sourced%2520checkpoint%2520triplets%2520with%2520significant%2520performance%2520and%250Arepresentation%2520shifts%252C%2520and%2520introduce%2520a%2520novel%2520metric%252C%2520Relative%2520Indirect%2520Effects%250A%2528RelIE%2529%252C%2520to%2520trace%2520training%2520stages%2520at%2520which%2520individual%2520features%2520become%2520causally%250Aimportant%2520for%2520task%2520performance.%2520We%2520show%2520that%2520crosscoders%2520can%2520detect%2520feature%250Aemergence%252C%2520maintenance%252C%2520and%2520discontinuation%2520during%2520pretraining.%2520Our%2520approach%2520is%250Aarchitecture-agnostic%2520and%2520scalable%252C%2520offering%2520a%2520promising%2520path%2520toward%2520more%250Ainterpretable%2520and%2520fine-grained%2520analysis%2520of%2520representation%2520learning%2520throughout%250Apretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crosscoding%20Through%20Time%3A%20Tracking%20Emergence%20%26%20Consolidation%20Of%0A%20%20Linguistic%20Representations%20Throughout%20LLM%20Pretraining&entry.906535625=Deniz%20Bayazit%20and%20Aaron%20Mueller%20and%20Antoine%20Bosselut&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20learn%20non-trivial%20abstractions%20during%0Apretraining%2C%20like%20detecting%20irregular%20plural%20noun%20subjects.%20However%2C%20it%20is%20not%0Awell%20understood%20when%20and%20how%20specific%20linguistic%20abilities%20emerge%20as%0Atraditional%20evaluation%20methods%20such%20as%20benchmarking%20fail%20to%20reveal%20how%20models%0Aacquire%20concepts%20and%20capabilities.%20To%20bridge%20this%20gap%20and%20better%20understand%0Amodel%20training%20at%20the%20concept%20level%2C%20we%20use%20sparse%20crosscoders%20to%20discover%20and%0Aalign%20features%20across%20model%20checkpoints.%20Using%20this%20approach%2C%20we%20track%20the%0Aevolution%20of%20linguistic%20features%20during%20pretraining.%20We%20train%20crosscoders%0Abetween%20open-sourced%20checkpoint%20triplets%20with%20significant%20performance%20and%0Arepresentation%20shifts%2C%20and%20introduce%20a%20novel%20metric%2C%20Relative%20Indirect%20Effects%0A%28RelIE%29%2C%20to%20trace%20training%20stages%20at%20which%20individual%20features%20become%20causally%0Aimportant%20for%20task%20performance.%20We%20show%20that%20crosscoders%20can%20detect%20feature%0Aemergence%2C%20maintenance%2C%20and%20discontinuation%20during%20pretraining.%20Our%20approach%20is%0Aarchitecture-agnostic%20and%20scalable%2C%20offering%20a%20promising%20path%20toward%20more%0Ainterpretable%20and%20fine-grained%20analysis%20of%20representation%20learning%20throughout%0Apretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05291v1&entry.124074799=Read"},
{"title": "GeoArena: An Open Platform for Benchmarking Large Vision-language Models\n  on WorldWide Image Geolocalization", "author": "Pengyue Jia and Yingyi Zhang and Xiangyu Zhao and Yixuan Li", "abstract": "  Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.\n", "link": "http://arxiv.org/abs/2509.04334v2", "date": "2025-09-05", "relevancy": 2.8584, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization&body=Title%3A%20GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization%0AAuthor%3A%20Pengyue%20Jia%20and%20Yingyi%20Zhang%20and%20Xiangyu%20Zhao%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Image%20geolocalization%20aims%20to%20predict%20the%20geographic%20location%20of%20images%0Acaptured%20anywhere%20on%20Earth%2C%20but%20its%20global%20nature%20presents%20significant%0Achallenges.%20Current%20evaluation%20methodologies%20suffer%20from%20two%20major%20limitations.%0AFirst%2C%20data%20leakage%3A%20advanced%20approaches%20often%20rely%20on%20large%20vision-language%0Amodels%20%28LVLMs%29%20to%20predict%20image%20locations%2C%20yet%20these%20models%20are%20frequently%0Apretrained%20on%20the%20test%20datasets%2C%20compromising%20the%20accuracy%20of%20evaluating%20a%0Amodel%27s%20actual%20geolocalization%20capability.%20Second%2C%20existing%20metrics%20primarily%0Arely%20on%20exact%20geographic%20coordinates%20to%20assess%20predictions%2C%20which%20not%20only%0Aneglects%20the%20reasoning%20process%20but%20also%20raises%20privacy%20concerns%20when%20user-level%0Alocation%20data%20is%20required.%20To%20address%20these%20issues%2C%20we%20propose%20GeoArena%2C%20a%0Afirst%20open%20platform%20for%20evaluating%20LVLMs%20on%20worldwide%20image%20geolocalization%0Atasks%2C%20offering%20true%20in-the-wild%20and%20human-centered%20benchmarking.%20GeoArena%0Aenables%20users%20to%20upload%20in-the-wild%20images%20for%20a%20more%20diverse%20evaluation%0Acorpus%2C%20and%20it%20leverages%20pairwise%20human%20judgments%20to%20determine%20which%20model%0Aoutput%20better%20aligns%20with%20human%20expectations.%20Our%20platform%20has%20been%20deployed%0Aonline%20for%20two%20months%2C%20during%20which%20we%20collected%20over%20thousands%20voting%20records.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20detailed%20analysis%20and%20establish%20a%20leaderboard%0Aof%20different%20LVLMs%20on%20the%20image%20geolocalization%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoArena%253A%2520An%2520Open%2520Platform%2520for%2520Benchmarking%2520Large%2520Vision-language%2520Models%250A%2520%2520on%2520WorldWide%2520Image%2520Geolocalization%26entry.906535625%3DPengyue%2520Jia%2520and%2520Yingyi%2520Zhang%2520and%2520Xiangyu%2520Zhao%2520and%2520Yixuan%2520Li%26entry.1292438233%3D%2520%2520Image%2520geolocalization%2520aims%2520to%2520predict%2520the%2520geographic%2520location%2520of%2520images%250Acaptured%2520anywhere%2520on%2520Earth%252C%2520but%2520its%2520global%2520nature%2520presents%2520significant%250Achallenges.%2520Current%2520evaluation%2520methodologies%2520suffer%2520from%2520two%2520major%2520limitations.%250AFirst%252C%2520data%2520leakage%253A%2520advanced%2520approaches%2520often%2520rely%2520on%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520to%2520predict%2520image%2520locations%252C%2520yet%2520these%2520models%2520are%2520frequently%250Apretrained%2520on%2520the%2520test%2520datasets%252C%2520compromising%2520the%2520accuracy%2520of%2520evaluating%2520a%250Amodel%2527s%2520actual%2520geolocalization%2520capability.%2520Second%252C%2520existing%2520metrics%2520primarily%250Arely%2520on%2520exact%2520geographic%2520coordinates%2520to%2520assess%2520predictions%252C%2520which%2520not%2520only%250Aneglects%2520the%2520reasoning%2520process%2520but%2520also%2520raises%2520privacy%2520concerns%2520when%2520user-level%250Alocation%2520data%2520is%2520required.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520GeoArena%252C%2520a%250Afirst%2520open%2520platform%2520for%2520evaluating%2520LVLMs%2520on%2520worldwide%2520image%2520geolocalization%250Atasks%252C%2520offering%2520true%2520in-the-wild%2520and%2520human-centered%2520benchmarking.%2520GeoArena%250Aenables%2520users%2520to%2520upload%2520in-the-wild%2520images%2520for%2520a%2520more%2520diverse%2520evaluation%250Acorpus%252C%2520and%2520it%2520leverages%2520pairwise%2520human%2520judgments%2520to%2520determine%2520which%2520model%250Aoutput%2520better%2520aligns%2520with%2520human%2520expectations.%2520Our%2520platform%2520has%2520been%2520deployed%250Aonline%2520for%2520two%2520months%252C%2520during%2520which%2520we%2520collected%2520over%2520thousands%2520voting%2520records.%250ABased%2520on%2520this%2520data%252C%2520we%2520conduct%2520a%2520detailed%2520analysis%2520and%2520establish%2520a%2520leaderboard%250Aof%2520different%2520LVLMs%2520on%2520the%2520image%2520geolocalization%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization&entry.906535625=Pengyue%20Jia%20and%20Yingyi%20Zhang%20and%20Xiangyu%20Zhao%20and%20Yixuan%20Li&entry.1292438233=%20%20Image%20geolocalization%20aims%20to%20predict%20the%20geographic%20location%20of%20images%0Acaptured%20anywhere%20on%20Earth%2C%20but%20its%20global%20nature%20presents%20significant%0Achallenges.%20Current%20evaluation%20methodologies%20suffer%20from%20two%20major%20limitations.%0AFirst%2C%20data%20leakage%3A%20advanced%20approaches%20often%20rely%20on%20large%20vision-language%0Amodels%20%28LVLMs%29%20to%20predict%20image%20locations%2C%20yet%20these%20models%20are%20frequently%0Apretrained%20on%20the%20test%20datasets%2C%20compromising%20the%20accuracy%20of%20evaluating%20a%0Amodel%27s%20actual%20geolocalization%20capability.%20Second%2C%20existing%20metrics%20primarily%0Arely%20on%20exact%20geographic%20coordinates%20to%20assess%20predictions%2C%20which%20not%20only%0Aneglects%20the%20reasoning%20process%20but%20also%20raises%20privacy%20concerns%20when%20user-level%0Alocation%20data%20is%20required.%20To%20address%20these%20issues%2C%20we%20propose%20GeoArena%2C%20a%0Afirst%20open%20platform%20for%20evaluating%20LVLMs%20on%20worldwide%20image%20geolocalization%0Atasks%2C%20offering%20true%20in-the-wild%20and%20human-centered%20benchmarking.%20GeoArena%0Aenables%20users%20to%20upload%20in-the-wild%20images%20for%20a%20more%20diverse%20evaluation%0Acorpus%2C%20and%20it%20leverages%20pairwise%20human%20judgments%20to%20determine%20which%20model%0Aoutput%20better%20aligns%20with%20human%20expectations.%20Our%20platform%20has%20been%20deployed%0Aonline%20for%20two%20months%2C%20during%20which%20we%20collected%20over%20thousands%20voting%20records.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20detailed%20analysis%20and%20establish%20a%20leaderboard%0Aof%20different%20LVLMs%20on%20the%20image%20geolocalization%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04334v2&entry.124074799=Read"},
{"title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS\n  Alignment Framework", "author": "Furong Jia and Lanxin Liu and Ce Hou and Fan Zhang and Xinyan Liu and Yu Liu", "abstract": "  Worldwide geo-localization involves determining the exact geographic location\nof images captured globally, typically guided by geographic cues such as\nclimate, landmarks, and architectural styles. Despite advancements in\ngeo-localization models like GeoCLIP, which leverages images and location\nalignment via contrastive learning for accurate predictions, the\ninterpretability of these models remains insufficiently explored. Current\nconcept-based interpretability methods fail to align effectively with\nGeo-alignment image-location embedding objectives, resulting in suboptimal\ninterpretability and performance. To address this gap, we propose a novel\nframework integrating global geo-localization with concept bottlenecks. Our\nmethod inserts a Concept-Aware Alignment Module that jointly projects image and\nlocation embeddings onto a shared bank of geographic concepts (e.g., tropical\nclimate, mountain, cathedral) and minimizes a concept-level loss, enhancing\nalignment in a concept-specific subspace and enabling robust interpretability.\nTo our knowledge, this is the first work to introduce interpretability into\ngeo-localization. Extensive experiments demonstrate that our approach surpasses\nGeoCLIP in geo-localization accuracy and boosts performance across diverse\ngeospatial prediction tasks, revealing richer semantic insights into geographic\ndecision-making processes.\n", "link": "http://arxiv.org/abs/2509.01910v2", "date": "2025-09-05", "relevancy": 2.8006, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5894}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20Geo-localization%3A%20a%20Concept-Aware%20Global%20Image-GPS%0A%20%20Alignment%20Framework&body=Title%3A%20Towards%20Interpretable%20Geo-localization%3A%20a%20Concept-Aware%20Global%20Image-GPS%0A%20%20Alignment%20Framework%0AAuthor%3A%20Furong%20Jia%20and%20Lanxin%20Liu%20and%20Ce%20Hou%20and%20Fan%20Zhang%20and%20Xinyan%20Liu%20and%20Yu%20Liu%0AAbstract%3A%20%20%20Worldwide%20geo-localization%20involves%20determining%20the%20exact%20geographic%20location%0Aof%20images%20captured%20globally%2C%20typically%20guided%20by%20geographic%20cues%20such%20as%0Aclimate%2C%20landmarks%2C%20and%20architectural%20styles.%20Despite%20advancements%20in%0Ageo-localization%20models%20like%20GeoCLIP%2C%20which%20leverages%20images%20and%20location%0Aalignment%20via%20contrastive%20learning%20for%20accurate%20predictions%2C%20the%0Ainterpretability%20of%20these%20models%20remains%20insufficiently%20explored.%20Current%0Aconcept-based%20interpretability%20methods%20fail%20to%20align%20effectively%20with%0AGeo-alignment%20image-location%20embedding%20objectives%2C%20resulting%20in%20suboptimal%0Ainterpretability%20and%20performance.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%0Aframework%20integrating%20global%20geo-localization%20with%20concept%20bottlenecks.%20Our%0Amethod%20inserts%20a%20Concept-Aware%20Alignment%20Module%20that%20jointly%20projects%20image%20and%0Alocation%20embeddings%20onto%20a%20shared%20bank%20of%20geographic%20concepts%20%28e.g.%2C%20tropical%0Aclimate%2C%20mountain%2C%20cathedral%29%20and%20minimizes%20a%20concept-level%20loss%2C%20enhancing%0Aalignment%20in%20a%20concept-specific%20subspace%20and%20enabling%20robust%20interpretability.%0ATo%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20introduce%20interpretability%20into%0Ageo-localization.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20surpasses%0AGeoCLIP%20in%20geo-localization%20accuracy%20and%20boosts%20performance%20across%20diverse%0Ageospatial%20prediction%20tasks%2C%20revealing%20richer%20semantic%20insights%20into%20geographic%0Adecision-making%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520Geo-localization%253A%2520a%2520Concept-Aware%2520Global%2520Image-GPS%250A%2520%2520Alignment%2520Framework%26entry.906535625%3DFurong%2520Jia%2520and%2520Lanxin%2520Liu%2520and%2520Ce%2520Hou%2520and%2520Fan%2520Zhang%2520and%2520Xinyan%2520Liu%2520and%2520Yu%2520Liu%26entry.1292438233%3D%2520%2520Worldwide%2520geo-localization%2520involves%2520determining%2520the%2520exact%2520geographic%2520location%250Aof%2520images%2520captured%2520globally%252C%2520typically%2520guided%2520by%2520geographic%2520cues%2520such%2520as%250Aclimate%252C%2520landmarks%252C%2520and%2520architectural%2520styles.%2520Despite%2520advancements%2520in%250Ageo-localization%2520models%2520like%2520GeoCLIP%252C%2520which%2520leverages%2520images%2520and%2520location%250Aalignment%2520via%2520contrastive%2520learning%2520for%2520accurate%2520predictions%252C%2520the%250Ainterpretability%2520of%2520these%2520models%2520remains%2520insufficiently%2520explored.%2520Current%250Aconcept-based%2520interpretability%2520methods%2520fail%2520to%2520align%2520effectively%2520with%250AGeo-alignment%2520image-location%2520embedding%2520objectives%252C%2520resulting%2520in%2520suboptimal%250Ainterpretability%2520and%2520performance.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520integrating%2520global%2520geo-localization%2520with%2520concept%2520bottlenecks.%2520Our%250Amethod%2520inserts%2520a%2520Concept-Aware%2520Alignment%2520Module%2520that%2520jointly%2520projects%2520image%2520and%250Alocation%2520embeddings%2520onto%2520a%2520shared%2520bank%2520of%2520geographic%2520concepts%2520%2528e.g.%252C%2520tropical%250Aclimate%252C%2520mountain%252C%2520cathedral%2529%2520and%2520minimizes%2520a%2520concept-level%2520loss%252C%2520enhancing%250Aalignment%2520in%2520a%2520concept-specific%2520subspace%2520and%2520enabling%2520robust%2520interpretability.%250ATo%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520introduce%2520interpretability%2520into%250Ageo-localization.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520surpasses%250AGeoCLIP%2520in%2520geo-localization%2520accuracy%2520and%2520boosts%2520performance%2520across%2520diverse%250Ageospatial%2520prediction%2520tasks%252C%2520revealing%2520richer%2520semantic%2520insights%2520into%2520geographic%250Adecision-making%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20Geo-localization%3A%20a%20Concept-Aware%20Global%20Image-GPS%0A%20%20Alignment%20Framework&entry.906535625=Furong%20Jia%20and%20Lanxin%20Liu%20and%20Ce%20Hou%20and%20Fan%20Zhang%20and%20Xinyan%20Liu%20and%20Yu%20Liu&entry.1292438233=%20%20Worldwide%20geo-localization%20involves%20determining%20the%20exact%20geographic%20location%0Aof%20images%20captured%20globally%2C%20typically%20guided%20by%20geographic%20cues%20such%20as%0Aclimate%2C%20landmarks%2C%20and%20architectural%20styles.%20Despite%20advancements%20in%0Ageo-localization%20models%20like%20GeoCLIP%2C%20which%20leverages%20images%20and%20location%0Aalignment%20via%20contrastive%20learning%20for%20accurate%20predictions%2C%20the%0Ainterpretability%20of%20these%20models%20remains%20insufficiently%20explored.%20Current%0Aconcept-based%20interpretability%20methods%20fail%20to%20align%20effectively%20with%0AGeo-alignment%20image-location%20embedding%20objectives%2C%20resulting%20in%20suboptimal%0Ainterpretability%20and%20performance.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%0Aframework%20integrating%20global%20geo-localization%20with%20concept%20bottlenecks.%20Our%0Amethod%20inserts%20a%20Concept-Aware%20Alignment%20Module%20that%20jointly%20projects%20image%20and%0Alocation%20embeddings%20onto%20a%20shared%20bank%20of%20geographic%20concepts%20%28e.g.%2C%20tropical%0Aclimate%2C%20mountain%2C%20cathedral%29%20and%20minimizes%20a%20concept-level%20loss%2C%20enhancing%0Aalignment%20in%20a%20concept-specific%20subspace%20and%20enabling%20robust%20interpretability.%0ATo%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20introduce%20interpretability%20into%0Ageo-localization.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20surpasses%0AGeoCLIP%20in%20geo-localization%20accuracy%20and%20boosts%20performance%20across%20diverse%0Ageospatial%20prediction%20tasks%2C%20revealing%20richer%20semantic%20insights%20into%20geographic%0Adecision-making%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01910v2&entry.124074799=Read"},
{"title": "BEDTime: A Unified Benchmark for Automatically Describing Time Series", "author": "Medhasweta Sen and Zachary Gottesman and Jiaxing Qiu and C. Bayan Bruss and Nam Nguyen and Tom Hartvigsen", "abstract": "  Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems.\n", "link": "http://arxiv.org/abs/2509.05215v1", "date": "2025-09-05", "relevancy": 2.7902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEDTime%3A%20A%20Unified%20Benchmark%20for%20Automatically%20Describing%20Time%20Series&body=Title%3A%20BEDTime%3A%20A%20Unified%20Benchmark%20for%20Automatically%20Describing%20Time%20Series%0AAuthor%3A%20Medhasweta%20Sen%20and%20Zachary%20Gottesman%20and%20Jiaxing%20Qiu%20and%20C.%20Bayan%20Bruss%20and%20Nam%20Nguyen%20and%20Tom%20Hartvigsen%0AAbstract%3A%20%20%20Many%20recent%20studies%20have%20proposed%20general-purpose%20foundation%20models%20designed%0Afor%20a%20variety%20of%20time%20series%20analysis%20tasks.%20While%20several%20established%20datasets%0Aalready%20exist%20for%20evaluating%20these%20models%2C%20previous%20works%20frequently%20introduce%0Atheir%20models%20in%20conjunction%20with%20new%20datasets%2C%20limiting%20opportunities%20for%0Adirect%2C%20independent%20comparisons%20and%20obscuring%20insights%20into%20the%20relative%0Astrengths%20of%20different%20methods.%20Additionally%2C%20prior%20evaluations%20often%20cover%0Anumerous%20tasks%20simultaneously%2C%20assessing%20a%20broad%20range%20of%20model%20abilities%0Awithout%20clearly%20pinpointing%20which%20capabilities%20contribute%20to%20overall%0Aperformance.%20To%20address%20these%20gaps%2C%20we%20formalize%20and%20evaluate%203%20tasks%20that%20test%0Aa%20model%27s%20ability%20to%20describe%20time%20series%20using%20generic%20natural%20language%3A%20%281%29%0Arecognition%20%28True/False%20question-answering%29%2C%20%282%29%20differentiation%20%28multiple%0Achoice%20question-answering%29%2C%20and%20%283%29%20generation%20%28open-ended%20natural%20language%0Adescription%29.%20We%20then%20unify%204%20recent%20datasets%20to%20enable%20head-to-head%20model%0Acomparisons%20on%20each%20task.%20Experimentally%2C%20in%20evaluating%2013%20state-of-the-art%0Alanguage%2C%20vision--language%2C%20and%20time%20series--language%20models%2C%20we%20find%20that%20%281%29%0Apopular%20language-only%20methods%20largely%20underperform%2C%20indicating%20a%20need%20for%20time%0Aseries-specific%20architectures%2C%20%282%29%20VLMs%20are%20quite%20successful%2C%20as%20expected%2C%0Aidentifying%20the%20value%20of%20vision%20models%20for%20these%20tasks%20and%20%283%29%20pretrained%0Amultimodal%20time%20series--language%20models%20successfully%20outperform%20LLMs%2C%20but%20still%0Ahave%20significant%20room%20for%20improvement.%20We%20also%20find%20that%20all%20approaches%20exhibit%0Aclear%20fragility%20in%20a%20range%20of%20robustness%20tests.%20Overall%2C%20our%20benchmark%20provides%0Aa%20standardized%20evaluation%20on%20a%20task%20necessary%20for%20time%20series%20reasoning%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEDTime%253A%2520A%2520Unified%2520Benchmark%2520for%2520Automatically%2520Describing%2520Time%2520Series%26entry.906535625%3DMedhasweta%2520Sen%2520and%2520Zachary%2520Gottesman%2520and%2520Jiaxing%2520Qiu%2520and%2520C.%2520Bayan%2520Bruss%2520and%2520Nam%2520Nguyen%2520and%2520Tom%2520Hartvigsen%26entry.1292438233%3D%2520%2520Many%2520recent%2520studies%2520have%2520proposed%2520general-purpose%2520foundation%2520models%2520designed%250Afor%2520a%2520variety%2520of%2520time%2520series%2520analysis%2520tasks.%2520While%2520several%2520established%2520datasets%250Aalready%2520exist%2520for%2520evaluating%2520these%2520models%252C%2520previous%2520works%2520frequently%2520introduce%250Atheir%2520models%2520in%2520conjunction%2520with%2520new%2520datasets%252C%2520limiting%2520opportunities%2520for%250Adirect%252C%2520independent%2520comparisons%2520and%2520obscuring%2520insights%2520into%2520the%2520relative%250Astrengths%2520of%2520different%2520methods.%2520Additionally%252C%2520prior%2520evaluations%2520often%2520cover%250Anumerous%2520tasks%2520simultaneously%252C%2520assessing%2520a%2520broad%2520range%2520of%2520model%2520abilities%250Awithout%2520clearly%2520pinpointing%2520which%2520capabilities%2520contribute%2520to%2520overall%250Aperformance.%2520To%2520address%2520these%2520gaps%252C%2520we%2520formalize%2520and%2520evaluate%25203%2520tasks%2520that%2520test%250Aa%2520model%2527s%2520ability%2520to%2520describe%2520time%2520series%2520using%2520generic%2520natural%2520language%253A%2520%25281%2529%250Arecognition%2520%2528True/False%2520question-answering%2529%252C%2520%25282%2529%2520differentiation%2520%2528multiple%250Achoice%2520question-answering%2529%252C%2520and%2520%25283%2529%2520generation%2520%2528open-ended%2520natural%2520language%250Adescription%2529.%2520We%2520then%2520unify%25204%2520recent%2520datasets%2520to%2520enable%2520head-to-head%2520model%250Acomparisons%2520on%2520each%2520task.%2520Experimentally%252C%2520in%2520evaluating%252013%2520state-of-the-art%250Alanguage%252C%2520vision--language%252C%2520and%2520time%2520series--language%2520models%252C%2520we%2520find%2520that%2520%25281%2529%250Apopular%2520language-only%2520methods%2520largely%2520underperform%252C%2520indicating%2520a%2520need%2520for%2520time%250Aseries-specific%2520architectures%252C%2520%25282%2529%2520VLMs%2520are%2520quite%2520successful%252C%2520as%2520expected%252C%250Aidentifying%2520the%2520value%2520of%2520vision%2520models%2520for%2520these%2520tasks%2520and%2520%25283%2529%2520pretrained%250Amultimodal%2520time%2520series--language%2520models%2520successfully%2520outperform%2520LLMs%252C%2520but%2520still%250Ahave%2520significant%2520room%2520for%2520improvement.%2520We%2520also%2520find%2520that%2520all%2520approaches%2520exhibit%250Aclear%2520fragility%2520in%2520a%2520range%2520of%2520robustness%2520tests.%2520Overall%252C%2520our%2520benchmark%2520provides%250Aa%2520standardized%2520evaluation%2520on%2520a%2520task%2520necessary%2520for%2520time%2520series%2520reasoning%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEDTime%3A%20A%20Unified%20Benchmark%20for%20Automatically%20Describing%20Time%20Series&entry.906535625=Medhasweta%20Sen%20and%20Zachary%20Gottesman%20and%20Jiaxing%20Qiu%20and%20C.%20Bayan%20Bruss%20and%20Nam%20Nguyen%20and%20Tom%20Hartvigsen&entry.1292438233=%20%20Many%20recent%20studies%20have%20proposed%20general-purpose%20foundation%20models%20designed%0Afor%20a%20variety%20of%20time%20series%20analysis%20tasks.%20While%20several%20established%20datasets%0Aalready%20exist%20for%20evaluating%20these%20models%2C%20previous%20works%20frequently%20introduce%0Atheir%20models%20in%20conjunction%20with%20new%20datasets%2C%20limiting%20opportunities%20for%0Adirect%2C%20independent%20comparisons%20and%20obscuring%20insights%20into%20the%20relative%0Astrengths%20of%20different%20methods.%20Additionally%2C%20prior%20evaluations%20often%20cover%0Anumerous%20tasks%20simultaneously%2C%20assessing%20a%20broad%20range%20of%20model%20abilities%0Awithout%20clearly%20pinpointing%20which%20capabilities%20contribute%20to%20overall%0Aperformance.%20To%20address%20these%20gaps%2C%20we%20formalize%20and%20evaluate%203%20tasks%20that%20test%0Aa%20model%27s%20ability%20to%20describe%20time%20series%20using%20generic%20natural%20language%3A%20%281%29%0Arecognition%20%28True/False%20question-answering%29%2C%20%282%29%20differentiation%20%28multiple%0Achoice%20question-answering%29%2C%20and%20%283%29%20generation%20%28open-ended%20natural%20language%0Adescription%29.%20We%20then%20unify%204%20recent%20datasets%20to%20enable%20head-to-head%20model%0Acomparisons%20on%20each%20task.%20Experimentally%2C%20in%20evaluating%2013%20state-of-the-art%0Alanguage%2C%20vision--language%2C%20and%20time%20series--language%20models%2C%20we%20find%20that%20%281%29%0Apopular%20language-only%20methods%20largely%20underperform%2C%20indicating%20a%20need%20for%20time%0Aseries-specific%20architectures%2C%20%282%29%20VLMs%20are%20quite%20successful%2C%20as%20expected%2C%0Aidentifying%20the%20value%20of%20vision%20models%20for%20these%20tasks%20and%20%283%29%20pretrained%0Amultimodal%20time%20series--language%20models%20successfully%20outperform%20LLMs%2C%20but%20still%0Ahave%20significant%20room%20for%20improvement.%20We%20also%20find%20that%20all%20approaches%20exhibit%0Aclear%20fragility%20in%20a%20range%20of%20robustness%20tests.%20Overall%2C%20our%20benchmark%20provides%0Aa%20standardized%20evaluation%20on%20a%20task%20necessary%20for%20time%20series%20reasoning%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05215v1&entry.124074799=Read"},
{"title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language\n  Translation", "author": "Marshall Thomas and Edward Fish and Richard Bowden", "abstract": "  Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.\n", "link": "http://arxiv.org/abs/2509.00030v2", "date": "2025-09-05", "relevancy": 2.7738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiStream-LLM%3A%20Bridging%20Modalities%20for%20Robust%20Sign%20Language%0A%20%20Translation&body=Title%3A%20MultiStream-LLM%3A%20Bridging%20Modalities%20for%20Robust%20Sign%20Language%0A%20%20Translation%0AAuthor%3A%20Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Despite%20progress%20in%20gloss-free%20Sign%20Language%20Translation%20%28SLT%29%2C%20monolithic%0Aend-to-end%20models%20consistently%20fail%20on%20two%20critical%20components%20of%20natural%0Asigning%3A%20the%20precise%20recognition%20of%20high-speed%20fingerspelling%20and%20the%0Aintegration%20of%20asynchronous%20non-manual%20cues%20from%20the%20face.%20Recent%20progress%20in%0AAutomated%20Sign%20Language%20Translation%20with%20Large%20Language%20Models%20has%20side%20stepped%0Athis%20challenge%2C%20forcing%20a%20single%20network%20to%20learn%20these%20simultaneously%0Aresulting%20in%20poor%20performance%20when%20tasked%20with%20translating%20crucial%20information%0Asuch%20as%20names%2Cplaces%2C%20and%20technical%20terms.%20We%20introduce%20MultiStream-LLM%2C%20a%0Amodular%20framework%20designed%20to%20overcome%20these%20limitations.%20Our%20approach%20employs%0Aseparate%2C%20specialized%20predictors%20for%20continuous%20signing%2C%20fingerspelling%2C%20and%0Alipreading.%20Each%20expert%20network%20first%20decodes%20its%20specific%20modality%20into%20a%0Asequence%20of%20tokens.%20These%20parallel%20streams%20are%20then%20fused%20by%20a%20lightweight%0Atransformer%20that%20resolves%20temporal%20misalignments%20before%20passing%20the%20combined%0Arepresentation%20to%20a%20Large%20Language%20Model%20%28LLM%29%20for%20final%20sentence%20generation.%0AOur%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20How2Sign%20benchmark%20with%20a%0ABLEU-4%20score%20of%2023.5%20and%20achieves%2073.2%25%20letter%20accuracy%20on%20the%20challenging%0AChicagoFSWildPlus%20fingerspelling%20dataset.%20These%20results%20validate%20our%20core%0Ahypothesis%3A%20by%20isolating%20and%20solving%20distinct%20recogni%20tion%20tasks%20before%20fusion%2C%0Aour%20multi-expert%20approach%20provides%20a%20more%20powerful%20and%20effective%20pathway%20to%0Arobust%2C%20high-fidelity%20sign%20language%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00030v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiStream-LLM%253A%2520Bridging%2520Modalities%2520for%2520Robust%2520Sign%2520Language%250A%2520%2520Translation%26entry.906535625%3DMarshall%2520Thomas%2520and%2520Edward%2520Fish%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Despite%2520progress%2520in%2520gloss-free%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%252C%2520monolithic%250Aend-to-end%2520models%2520consistently%2520fail%2520on%2520two%2520critical%2520components%2520of%2520natural%250Asigning%253A%2520the%2520precise%2520recognition%2520of%2520high-speed%2520fingerspelling%2520and%2520the%250Aintegration%2520of%2520asynchronous%2520non-manual%2520cues%2520from%2520the%2520face.%2520Recent%2520progress%2520in%250AAutomated%2520Sign%2520Language%2520Translation%2520with%2520Large%2520Language%2520Models%2520has%2520side%2520stepped%250Athis%2520challenge%252C%2520forcing%2520a%2520single%2520network%2520to%2520learn%2520these%2520simultaneously%250Aresulting%2520in%2520poor%2520performance%2520when%2520tasked%2520with%2520translating%2520crucial%2520information%250Asuch%2520as%2520names%252Cplaces%252C%2520and%2520technical%2520terms.%2520We%2520introduce%2520MultiStream-LLM%252C%2520a%250Amodular%2520framework%2520designed%2520to%2520overcome%2520these%2520limitations.%2520Our%2520approach%2520employs%250Aseparate%252C%2520specialized%2520predictors%2520for%2520continuous%2520signing%252C%2520fingerspelling%252C%2520and%250Alipreading.%2520Each%2520expert%2520network%2520first%2520decodes%2520its%2520specific%2520modality%2520into%2520a%250Asequence%2520of%2520tokens.%2520These%2520parallel%2520streams%2520are%2520then%2520fused%2520by%2520a%2520lightweight%250Atransformer%2520that%2520resolves%2520temporal%2520misalignments%2520before%2520passing%2520the%2520combined%250Arepresentation%2520to%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520for%2520final%2520sentence%2520generation.%250AOur%2520method%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520the%2520How2Sign%2520benchmark%2520with%2520a%250ABLEU-4%2520score%2520of%252023.5%2520and%2520achieves%252073.2%2525%2520letter%2520accuracy%2520on%2520the%2520challenging%250AChicagoFSWildPlus%2520fingerspelling%2520dataset.%2520These%2520results%2520validate%2520our%2520core%250Ahypothesis%253A%2520by%2520isolating%2520and%2520solving%2520distinct%2520recogni%2520tion%2520tasks%2520before%2520fusion%252C%250Aour%2520multi-expert%2520approach%2520provides%2520a%2520more%2520powerful%2520and%2520effective%2520pathway%2520to%250Arobust%252C%2520high-fidelity%2520sign%2520language%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00030v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiStream-LLM%3A%20Bridging%20Modalities%20for%20Robust%20Sign%20Language%0A%20%20Translation&entry.906535625=Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden&entry.1292438233=%20%20Despite%20progress%20in%20gloss-free%20Sign%20Language%20Translation%20%28SLT%29%2C%20monolithic%0Aend-to-end%20models%20consistently%20fail%20on%20two%20critical%20components%20of%20natural%0Asigning%3A%20the%20precise%20recognition%20of%20high-speed%20fingerspelling%20and%20the%0Aintegration%20of%20asynchronous%20non-manual%20cues%20from%20the%20face.%20Recent%20progress%20in%0AAutomated%20Sign%20Language%20Translation%20with%20Large%20Language%20Models%20has%20side%20stepped%0Athis%20challenge%2C%20forcing%20a%20single%20network%20to%20learn%20these%20simultaneously%0Aresulting%20in%20poor%20performance%20when%20tasked%20with%20translating%20crucial%20information%0Asuch%20as%20names%2Cplaces%2C%20and%20technical%20terms.%20We%20introduce%20MultiStream-LLM%2C%20a%0Amodular%20framework%20designed%20to%20overcome%20these%20limitations.%20Our%20approach%20employs%0Aseparate%2C%20specialized%20predictors%20for%20continuous%20signing%2C%20fingerspelling%2C%20and%0Alipreading.%20Each%20expert%20network%20first%20decodes%20its%20specific%20modality%20into%20a%0Asequence%20of%20tokens.%20These%20parallel%20streams%20are%20then%20fused%20by%20a%20lightweight%0Atransformer%20that%20resolves%20temporal%20misalignments%20before%20passing%20the%20combined%0Arepresentation%20to%20a%20Large%20Language%20Model%20%28LLM%29%20for%20final%20sentence%20generation.%0AOur%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20How2Sign%20benchmark%20with%20a%0ABLEU-4%20score%20of%2023.5%20and%20achieves%2073.2%25%20letter%20accuracy%20on%20the%20challenging%0AChicagoFSWildPlus%20fingerspelling%20dataset.%20These%20results%20validate%20our%20core%0Ahypothesis%3A%20by%20isolating%20and%20solving%20distinct%20recogni%20tion%20tasks%20before%20fusion%2C%0Aour%20multi-expert%20approach%20provides%20a%20more%20powerful%20and%20effective%20pathway%20to%0Arobust%2C%20high-fidelity%20sign%20language%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00030v2&entry.124074799=Read"},
{"title": "SL-SLR: Self-Supervised Representation Learning for Sign Language\n  Recognition", "author": "Ariel Basso Madjoukeng and J\u00e9r\u00f4me Fink and Pierre Poitier and Edith Belise Kenmogne and Benoit Frenay", "abstract": "  Sign language recognition (SLR) is a machine learning task aiming to identify\nsigns in videos. Due to the scarcity of annotated data, unsupervised methods\nlike contrastive learning have become promising in this field. They learn\nmeaningful representations by pulling positive pairs (two augmented versions of\nthe same instance) closer and pushing negative pairs (different from the\npositive pairs) apart. In SLR, in a sign video, only certain parts provide\ninformation that is truly useful for its recognition. Applying contrastive\nmethods to SLR raises two issues: (i) contrastive learning methods treat all\nparts of a video in the same way, without taking into account the relevance of\ncertain parts over others; (ii) shared movements between different signs make\nnegative pairs highly similar, complicating sign discrimination. These issues\nlead to learning non-discriminative features for sign recognition and poor\nresults in downstream tasks. In response, this paper proposes a self-supervised\nlearning framework designed to learn meaningful representations for SLR. This\nframework consists of two key components designed to work together: (i) a new\nself-supervised approach with free-negative pairs; (ii) a new data augmentation\ntechnique. This approach shows a considerable gain in accuracy compared to\nseveral contrastive and self-supervised methods, across linear evaluation,\nsemi-supervised learning, and transferability between sign languages.\n", "link": "http://arxiv.org/abs/2509.05188v1", "date": "2025-09-05", "relevancy": 2.7487, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SL-SLR%3A%20Self-Supervised%20Representation%20Learning%20for%20Sign%20Language%0A%20%20Recognition&body=Title%3A%20SL-SLR%3A%20Self-Supervised%20Representation%20Learning%20for%20Sign%20Language%0A%20%20Recognition%0AAuthor%3A%20Ariel%20Basso%20Madjoukeng%20and%20J%C3%A9r%C3%B4me%20Fink%20and%20Pierre%20Poitier%20and%20Edith%20Belise%20Kenmogne%20and%20Benoit%20Frenay%0AAbstract%3A%20%20%20Sign%20language%20recognition%20%28SLR%29%20is%20a%20machine%20learning%20task%20aiming%20to%20identify%0Asigns%20in%20videos.%20Due%20to%20the%20scarcity%20of%20annotated%20data%2C%20unsupervised%20methods%0Alike%20contrastive%20learning%20have%20become%20promising%20in%20this%20field.%20They%20learn%0Ameaningful%20representations%20by%20pulling%20positive%20pairs%20%28two%20augmented%20versions%20of%0Athe%20same%20instance%29%20closer%20and%20pushing%20negative%20pairs%20%28different%20from%20the%0Apositive%20pairs%29%20apart.%20In%20SLR%2C%20in%20a%20sign%20video%2C%20only%20certain%20parts%20provide%0Ainformation%20that%20is%20truly%20useful%20for%20its%20recognition.%20Applying%20contrastive%0Amethods%20to%20SLR%20raises%20two%20issues%3A%20%28i%29%20contrastive%20learning%20methods%20treat%20all%0Aparts%20of%20a%20video%20in%20the%20same%20way%2C%20without%20taking%20into%20account%20the%20relevance%20of%0Acertain%20parts%20over%20others%3B%20%28ii%29%20shared%20movements%20between%20different%20signs%20make%0Anegative%20pairs%20highly%20similar%2C%20complicating%20sign%20discrimination.%20These%20issues%0Alead%20to%20learning%20non-discriminative%20features%20for%20sign%20recognition%20and%20poor%0Aresults%20in%20downstream%20tasks.%20In%20response%2C%20this%20paper%20proposes%20a%20self-supervised%0Alearning%20framework%20designed%20to%20learn%20meaningful%20representations%20for%20SLR.%20This%0Aframework%20consists%20of%20two%20key%20components%20designed%20to%20work%20together%3A%20%28i%29%20a%20new%0Aself-supervised%20approach%20with%20free-negative%20pairs%3B%20%28ii%29%20a%20new%20data%20augmentation%0Atechnique.%20This%20approach%20shows%20a%20considerable%20gain%20in%20accuracy%20compared%20to%0Aseveral%20contrastive%20and%20self-supervised%20methods%2C%20across%20linear%20evaluation%2C%0Asemi-supervised%20learning%2C%20and%20transferability%20between%20sign%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSL-SLR%253A%2520Self-Supervised%2520Representation%2520Learning%2520for%2520Sign%2520Language%250A%2520%2520Recognition%26entry.906535625%3DAriel%2520Basso%2520Madjoukeng%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Fink%2520and%2520Pierre%2520Poitier%2520and%2520Edith%2520Belise%2520Kenmogne%2520and%2520Benoit%2520Frenay%26entry.1292438233%3D%2520%2520Sign%2520language%2520recognition%2520%2528SLR%2529%2520is%2520a%2520machine%2520learning%2520task%2520aiming%2520to%2520identify%250Asigns%2520in%2520videos.%2520Due%2520to%2520the%2520scarcity%2520of%2520annotated%2520data%252C%2520unsupervised%2520methods%250Alike%2520contrastive%2520learning%2520have%2520become%2520promising%2520in%2520this%2520field.%2520They%2520learn%250Ameaningful%2520representations%2520by%2520pulling%2520positive%2520pairs%2520%2528two%2520augmented%2520versions%2520of%250Athe%2520same%2520instance%2529%2520closer%2520and%2520pushing%2520negative%2520pairs%2520%2528different%2520from%2520the%250Apositive%2520pairs%2529%2520apart.%2520In%2520SLR%252C%2520in%2520a%2520sign%2520video%252C%2520only%2520certain%2520parts%2520provide%250Ainformation%2520that%2520is%2520truly%2520useful%2520for%2520its%2520recognition.%2520Applying%2520contrastive%250Amethods%2520to%2520SLR%2520raises%2520two%2520issues%253A%2520%2528i%2529%2520contrastive%2520learning%2520methods%2520treat%2520all%250Aparts%2520of%2520a%2520video%2520in%2520the%2520same%2520way%252C%2520without%2520taking%2520into%2520account%2520the%2520relevance%2520of%250Acertain%2520parts%2520over%2520others%253B%2520%2528ii%2529%2520shared%2520movements%2520between%2520different%2520signs%2520make%250Anegative%2520pairs%2520highly%2520similar%252C%2520complicating%2520sign%2520discrimination.%2520These%2520issues%250Alead%2520to%2520learning%2520non-discriminative%2520features%2520for%2520sign%2520recognition%2520and%2520poor%250Aresults%2520in%2520downstream%2520tasks.%2520In%2520response%252C%2520this%2520paper%2520proposes%2520a%2520self-supervised%250Alearning%2520framework%2520designed%2520to%2520learn%2520meaningful%2520representations%2520for%2520SLR.%2520This%250Aframework%2520consists%2520of%2520two%2520key%2520components%2520designed%2520to%2520work%2520together%253A%2520%2528i%2529%2520a%2520new%250Aself-supervised%2520approach%2520with%2520free-negative%2520pairs%253B%2520%2528ii%2529%2520a%2520new%2520data%2520augmentation%250Atechnique.%2520This%2520approach%2520shows%2520a%2520considerable%2520gain%2520in%2520accuracy%2520compared%2520to%250Aseveral%2520contrastive%2520and%2520self-supervised%2520methods%252C%2520across%2520linear%2520evaluation%252C%250Asemi-supervised%2520learning%252C%2520and%2520transferability%2520between%2520sign%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SL-SLR%3A%20Self-Supervised%20Representation%20Learning%20for%20Sign%20Language%0A%20%20Recognition&entry.906535625=Ariel%20Basso%20Madjoukeng%20and%20J%C3%A9r%C3%B4me%20Fink%20and%20Pierre%20Poitier%20and%20Edith%20Belise%20Kenmogne%20and%20Benoit%20Frenay&entry.1292438233=%20%20Sign%20language%20recognition%20%28SLR%29%20is%20a%20machine%20learning%20task%20aiming%20to%20identify%0Asigns%20in%20videos.%20Due%20to%20the%20scarcity%20of%20annotated%20data%2C%20unsupervised%20methods%0Alike%20contrastive%20learning%20have%20become%20promising%20in%20this%20field.%20They%20learn%0Ameaningful%20representations%20by%20pulling%20positive%20pairs%20%28two%20augmented%20versions%20of%0Athe%20same%20instance%29%20closer%20and%20pushing%20negative%20pairs%20%28different%20from%20the%0Apositive%20pairs%29%20apart.%20In%20SLR%2C%20in%20a%20sign%20video%2C%20only%20certain%20parts%20provide%0Ainformation%20that%20is%20truly%20useful%20for%20its%20recognition.%20Applying%20contrastive%0Amethods%20to%20SLR%20raises%20two%20issues%3A%20%28i%29%20contrastive%20learning%20methods%20treat%20all%0Aparts%20of%20a%20video%20in%20the%20same%20way%2C%20without%20taking%20into%20account%20the%20relevance%20of%0Acertain%20parts%20over%20others%3B%20%28ii%29%20shared%20movements%20between%20different%20signs%20make%0Anegative%20pairs%20highly%20similar%2C%20complicating%20sign%20discrimination.%20These%20issues%0Alead%20to%20learning%20non-discriminative%20features%20for%20sign%20recognition%20and%20poor%0Aresults%20in%20downstream%20tasks.%20In%20response%2C%20this%20paper%20proposes%20a%20self-supervised%0Alearning%20framework%20designed%20to%20learn%20meaningful%20representations%20for%20SLR.%20This%0Aframework%20consists%20of%20two%20key%20components%20designed%20to%20work%20together%3A%20%28i%29%20a%20new%0Aself-supervised%20approach%20with%20free-negative%20pairs%3B%20%28ii%29%20a%20new%20data%20augmentation%0Atechnique.%20This%20approach%20shows%20a%20considerable%20gain%20in%20accuracy%20compared%20to%0Aseveral%20contrastive%20and%20self-supervised%20methods%2C%20across%20linear%20evaluation%2C%0Asemi-supervised%20learning%2C%20and%20transferability%20between%20sign%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05188v1&entry.124074799=Read"},
{"title": "Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified\n  Synthesis Framework", "author": "Wang Wang and Mingyu Shi and Jun Jiang and Wenqian Ma and Chong Liu and Yasutaka Narazaki and Xuguang Wang", "abstract": "  As critical transportation infrastructure, bridges face escalating challenges\nfrom aging and deterioration, while traditional manual inspection methods\nsuffer from low efficiency. Although 3D point cloud technology provides a new\ndata-driven paradigm, its application potential is often constrained by the\nincompleteness of real-world data, which results from missing labels and\nscanning occlusions. To overcome the bottleneck of insufficient generalization\nin existing synthetic data methods, this paper proposes a systematic framework\nfor generating 3D bridge data.\n  This framework can automatically generate complete point clouds featuring\ncomponent-level instance annotations, high-fidelity color, and precise normal\nvectors. It can be further extended to simulate the creation of diverse and\nphysically realistic incomplete point clouds, designed to support the training\nof segmentation and completion networks, respectively. Experiments demonstrate\nthat a PointNet++ model trained with our synthetic data achieves a mean\nIntersection over Union (mIoU) of 84.2% in real-world bridge semantic\nsegmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance\non the component completion task.\n  This research offers an innovative methodology and a foundational dataset for\nthe 3D visual analysis of bridge structures, holding significant implications\nfor advancing the automated management and maintenance of infrastructure.\n", "link": "http://arxiv.org/abs/2507.05814v3", "date": "2025-09-05", "relevancy": 2.7472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Bridge%20Digital%20Twins%20by%20Bridging%20the%20Data%20Gap%20with%20a%20Unified%0A%20%20Synthesis%20Framework&body=Title%3A%20Empowering%20Bridge%20Digital%20Twins%20by%20Bridging%20the%20Data%20Gap%20with%20a%20Unified%0A%20%20Synthesis%20Framework%0AAuthor%3A%20Wang%20Wang%20and%20Mingyu%20Shi%20and%20Jun%20Jiang%20and%20Wenqian%20Ma%20and%20Chong%20Liu%20and%20Yasutaka%20Narazaki%20and%20Xuguang%20Wang%0AAbstract%3A%20%20%20As%20critical%20transportation%20infrastructure%2C%20bridges%20face%20escalating%20challenges%0Afrom%20aging%20and%20deterioration%2C%20while%20traditional%20manual%20inspection%20methods%0Asuffer%20from%20low%20efficiency.%20Although%203D%20point%20cloud%20technology%20provides%20a%20new%0Adata-driven%20paradigm%2C%20its%20application%20potential%20is%20often%20constrained%20by%20the%0Aincompleteness%20of%20real-world%20data%2C%20which%20results%20from%20missing%20labels%20and%0Ascanning%20occlusions.%20To%20overcome%20the%20bottleneck%20of%20insufficient%20generalization%0Ain%20existing%20synthetic%20data%20methods%2C%20this%20paper%20proposes%20a%20systematic%20framework%0Afor%20generating%203D%20bridge%20data.%0A%20%20This%20framework%20can%20automatically%20generate%20complete%20point%20clouds%20featuring%0Acomponent-level%20instance%20annotations%2C%20high-fidelity%20color%2C%20and%20precise%20normal%0Avectors.%20It%20can%20be%20further%20extended%20to%20simulate%20the%20creation%20of%20diverse%20and%0Aphysically%20realistic%20incomplete%20point%20clouds%2C%20designed%20to%20support%20the%20training%0Aof%20segmentation%20and%20completion%20networks%2C%20respectively.%20Experiments%20demonstrate%0Athat%20a%20PointNet%2B%2B%20model%20trained%20with%20our%20synthetic%20data%20achieves%20a%20mean%0AIntersection%20over%20Union%20%28mIoU%29%20of%2084.2%25%20in%20real-world%20bridge%20semantic%0Asegmentation.%20Concurrently%2C%20a%20fine-tuned%20KT-Net%20exhibits%20superior%20performance%0Aon%20the%20component%20completion%20task.%0A%20%20This%20research%20offers%20an%20innovative%20methodology%20and%20a%20foundational%20dataset%20for%0Athe%203D%20visual%20analysis%20of%20bridge%20structures%2C%20holding%20significant%20implications%0Afor%20advancing%20the%20automated%20management%20and%20maintenance%20of%20infrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05814v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Bridge%2520Digital%2520Twins%2520by%2520Bridging%2520the%2520Data%2520Gap%2520with%2520a%2520Unified%250A%2520%2520Synthesis%2520Framework%26entry.906535625%3DWang%2520Wang%2520and%2520Mingyu%2520Shi%2520and%2520Jun%2520Jiang%2520and%2520Wenqian%2520Ma%2520and%2520Chong%2520Liu%2520and%2520Yasutaka%2520Narazaki%2520and%2520Xuguang%2520Wang%26entry.1292438233%3D%2520%2520As%2520critical%2520transportation%2520infrastructure%252C%2520bridges%2520face%2520escalating%2520challenges%250Afrom%2520aging%2520and%2520deterioration%252C%2520while%2520traditional%2520manual%2520inspection%2520methods%250Asuffer%2520from%2520low%2520efficiency.%2520Although%25203D%2520point%2520cloud%2520technology%2520provides%2520a%2520new%250Adata-driven%2520paradigm%252C%2520its%2520application%2520potential%2520is%2520often%2520constrained%2520by%2520the%250Aincompleteness%2520of%2520real-world%2520data%252C%2520which%2520results%2520from%2520missing%2520labels%2520and%250Ascanning%2520occlusions.%2520To%2520overcome%2520the%2520bottleneck%2520of%2520insufficient%2520generalization%250Ain%2520existing%2520synthetic%2520data%2520methods%252C%2520this%2520paper%2520proposes%2520a%2520systematic%2520framework%250Afor%2520generating%25203D%2520bridge%2520data.%250A%2520%2520This%2520framework%2520can%2520automatically%2520generate%2520complete%2520point%2520clouds%2520featuring%250Acomponent-level%2520instance%2520annotations%252C%2520high-fidelity%2520color%252C%2520and%2520precise%2520normal%250Avectors.%2520It%2520can%2520be%2520further%2520extended%2520to%2520simulate%2520the%2520creation%2520of%2520diverse%2520and%250Aphysically%2520realistic%2520incomplete%2520point%2520clouds%252C%2520designed%2520to%2520support%2520the%2520training%250Aof%2520segmentation%2520and%2520completion%2520networks%252C%2520respectively.%2520Experiments%2520demonstrate%250Athat%2520a%2520PointNet%252B%252B%2520model%2520trained%2520with%2520our%2520synthetic%2520data%2520achieves%2520a%2520mean%250AIntersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252084.2%2525%2520in%2520real-world%2520bridge%2520semantic%250Asegmentation.%2520Concurrently%252C%2520a%2520fine-tuned%2520KT-Net%2520exhibits%2520superior%2520performance%250Aon%2520the%2520component%2520completion%2520task.%250A%2520%2520This%2520research%2520offers%2520an%2520innovative%2520methodology%2520and%2520a%2520foundational%2520dataset%2520for%250Athe%25203D%2520visual%2520analysis%2520of%2520bridge%2520structures%252C%2520holding%2520significant%2520implications%250Afor%2520advancing%2520the%2520automated%2520management%2520and%2520maintenance%2520of%2520infrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05814v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Bridge%20Digital%20Twins%20by%20Bridging%20the%20Data%20Gap%20with%20a%20Unified%0A%20%20Synthesis%20Framework&entry.906535625=Wang%20Wang%20and%20Mingyu%20Shi%20and%20Jun%20Jiang%20and%20Wenqian%20Ma%20and%20Chong%20Liu%20and%20Yasutaka%20Narazaki%20and%20Xuguang%20Wang&entry.1292438233=%20%20As%20critical%20transportation%20infrastructure%2C%20bridges%20face%20escalating%20challenges%0Afrom%20aging%20and%20deterioration%2C%20while%20traditional%20manual%20inspection%20methods%0Asuffer%20from%20low%20efficiency.%20Although%203D%20point%20cloud%20technology%20provides%20a%20new%0Adata-driven%20paradigm%2C%20its%20application%20potential%20is%20often%20constrained%20by%20the%0Aincompleteness%20of%20real-world%20data%2C%20which%20results%20from%20missing%20labels%20and%0Ascanning%20occlusions.%20To%20overcome%20the%20bottleneck%20of%20insufficient%20generalization%0Ain%20existing%20synthetic%20data%20methods%2C%20this%20paper%20proposes%20a%20systematic%20framework%0Afor%20generating%203D%20bridge%20data.%0A%20%20This%20framework%20can%20automatically%20generate%20complete%20point%20clouds%20featuring%0Acomponent-level%20instance%20annotations%2C%20high-fidelity%20color%2C%20and%20precise%20normal%0Avectors.%20It%20can%20be%20further%20extended%20to%20simulate%20the%20creation%20of%20diverse%20and%0Aphysically%20realistic%20incomplete%20point%20clouds%2C%20designed%20to%20support%20the%20training%0Aof%20segmentation%20and%20completion%20networks%2C%20respectively.%20Experiments%20demonstrate%0Athat%20a%20PointNet%2B%2B%20model%20trained%20with%20our%20synthetic%20data%20achieves%20a%20mean%0AIntersection%20over%20Union%20%28mIoU%29%20of%2084.2%25%20in%20real-world%20bridge%20semantic%0Asegmentation.%20Concurrently%2C%20a%20fine-tuned%20KT-Net%20exhibits%20superior%20performance%0Aon%20the%20component%20completion%20task.%0A%20%20This%20research%20offers%20an%20innovative%20methodology%20and%20a%20foundational%20dataset%20for%0Athe%203D%20visual%20analysis%20of%20bridge%20structures%2C%20holding%20significant%20implications%0Afor%20advancing%20the%20automated%20management%20and%20maintenance%20of%20infrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05814v3&entry.124074799=Read"},
{"title": "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive\n  Visual Perception", "author": "Mengqi Lei and Siqi Li and Yihong Wu and Han Hu and You Zhou and Xinhu Zheng and Guiguang Ding and Shaoyi Du and Zongze Wu and Yue Gao", "abstract": "  The YOLO series models reign supreme in real-time object detection due to\ntheir superior accuracy and computational efficiency. However, both the\nconvolutional architectures of YOLO11 and earlier versions and the area-based\nself-attention mechanism introduced in YOLOv12 are limited to local information\naggregation and pairwise correlation modeling, lacking the capability to\ncapture global multi-to-multi high-order correlations, which limits detection\nperformance in complex scenarios. In this paper, we propose YOLOv13, an\naccurate and lightweight object detector. To address the above-mentioned\nchallenges, we propose a Hypergraph-based Adaptive Correlation Enhancement\n(HyperACE) mechanism that adaptively exploits latent high-order correlations\nand overcomes the limitation of previous methods that are restricted to\npairwise correlation modeling based on hypergraph computation, achieving\nefficient global cross-location and cross-scale feature fusion and enhancement.\nSubsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)\nparadigm based on HyperACE, which effectively achieves fine-grained information\nflow and representation synergy within the entire network by distributing\ncorrelation-enhanced features to the full pipeline. Finally, we propose to\nleverage depthwise separable convolutions to replace vanilla large-kernel\nconvolutions, and design a series of blocks that significantly reduce\nparameters and computational complexity without sacrificing performance. We\nconduct extensive experiments on the widely used MS COCO benchmark, and the\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance with fewer parameters and FLOPs. Specifically, our YOLOv13-N\nimproves mAP by 3.0\\% over YOLO11-N and by 1.5\\% over YOLOv12-N. The code and\nmodels of our YOLOv13 model are available at:\nhttps://github.com/iMoonLab/yolov13.\n", "link": "http://arxiv.org/abs/2506.17733v2", "date": "2025-09-05", "relevancy": 2.7293, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLOv13%3A%20Real-Time%20Object%20Detection%20with%20Hypergraph-Enhanced%20Adaptive%0A%20%20Visual%20Perception&body=Title%3A%20YOLOv13%3A%20Real-Time%20Object%20Detection%20with%20Hypergraph-Enhanced%20Adaptive%0A%20%20Visual%20Perception%0AAuthor%3A%20Mengqi%20Lei%20and%20Siqi%20Li%20and%20Yihong%20Wu%20and%20Han%20Hu%20and%20You%20Zhou%20and%20Xinhu%20Zheng%20and%20Guiguang%20Ding%20and%20Shaoyi%20Du%20and%20Zongze%20Wu%20and%20Yue%20Gao%0AAbstract%3A%20%20%20The%20YOLO%20series%20models%20reign%20supreme%20in%20real-time%20object%20detection%20due%20to%0Atheir%20superior%20accuracy%20and%20computational%20efficiency.%20However%2C%20both%20the%0Aconvolutional%20architectures%20of%20YOLO11%20and%20earlier%20versions%20and%20the%20area-based%0Aself-attention%20mechanism%20introduced%20in%20YOLOv12%20are%20limited%20to%20local%20information%0Aaggregation%20and%20pairwise%20correlation%20modeling%2C%20lacking%20the%20capability%20to%0Acapture%20global%20multi-to-multi%20high-order%20correlations%2C%20which%20limits%20detection%0Aperformance%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20YOLOv13%2C%20an%0Aaccurate%20and%20lightweight%20object%20detector.%20To%20address%20the%20above-mentioned%0Achallenges%2C%20we%20propose%20a%20Hypergraph-based%20Adaptive%20Correlation%20Enhancement%0A%28HyperACE%29%20mechanism%20that%20adaptively%20exploits%20latent%20high-order%20correlations%0Aand%20overcomes%20the%20limitation%20of%20previous%20methods%20that%20are%20restricted%20to%0Apairwise%20correlation%20modeling%20based%20on%20hypergraph%20computation%2C%20achieving%0Aefficient%20global%20cross-location%20and%20cross-scale%20feature%20fusion%20and%20enhancement.%0ASubsequently%2C%20we%20propose%20a%20Full-Pipeline%20Aggregation-and-Distribution%20%28FullPAD%29%0Aparadigm%20based%20on%20HyperACE%2C%20which%20effectively%20achieves%20fine-grained%20information%0Aflow%20and%20representation%20synergy%20within%20the%20entire%20network%20by%20distributing%0Acorrelation-enhanced%20features%20to%20the%20full%20pipeline.%20Finally%2C%20we%20propose%20to%0Aleverage%20depthwise%20separable%20convolutions%20to%20replace%20vanilla%20large-kernel%0Aconvolutions%2C%20and%20design%20a%20series%20of%20blocks%20that%20significantly%20reduce%0Aparameters%20and%20computational%20complexity%20without%20sacrificing%20performance.%20We%0Aconduct%20extensive%20experiments%20on%20the%20widely%20used%20MS%20COCO%20benchmark%2C%20and%20the%0Aexperimental%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20with%20fewer%20parameters%20and%20FLOPs.%20Specifically%2C%20our%20YOLOv13-N%0Aimproves%20mAP%20by%203.0%5C%25%20over%20YOLO11-N%20and%20by%201.5%5C%25%20over%20YOLOv12-N.%20The%20code%20and%0Amodels%20of%20our%20YOLOv13%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/iMoonLab/yolov13.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.17733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLOv13%253A%2520Real-Time%2520Object%2520Detection%2520with%2520Hypergraph-Enhanced%2520Adaptive%250A%2520%2520Visual%2520Perception%26entry.906535625%3DMengqi%2520Lei%2520and%2520Siqi%2520Li%2520and%2520Yihong%2520Wu%2520and%2520Han%2520Hu%2520and%2520You%2520Zhou%2520and%2520Xinhu%2520Zheng%2520and%2520Guiguang%2520Ding%2520and%2520Shaoyi%2520Du%2520and%2520Zongze%2520Wu%2520and%2520Yue%2520Gao%26entry.1292438233%3D%2520%2520The%2520YOLO%2520series%2520models%2520reign%2520supreme%2520in%2520real-time%2520object%2520detection%2520due%2520to%250Atheir%2520superior%2520accuracy%2520and%2520computational%2520efficiency.%2520However%252C%2520both%2520the%250Aconvolutional%2520architectures%2520of%2520YOLO11%2520and%2520earlier%2520versions%2520and%2520the%2520area-based%250Aself-attention%2520mechanism%2520introduced%2520in%2520YOLOv12%2520are%2520limited%2520to%2520local%2520information%250Aaggregation%2520and%2520pairwise%2520correlation%2520modeling%252C%2520lacking%2520the%2520capability%2520to%250Acapture%2520global%2520multi-to-multi%2520high-order%2520correlations%252C%2520which%2520limits%2520detection%250Aperformance%2520in%2520complex%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520YOLOv13%252C%2520an%250Aaccurate%2520and%2520lightweight%2520object%2520detector.%2520To%2520address%2520the%2520above-mentioned%250Achallenges%252C%2520we%2520propose%2520a%2520Hypergraph-based%2520Adaptive%2520Correlation%2520Enhancement%250A%2528HyperACE%2529%2520mechanism%2520that%2520adaptively%2520exploits%2520latent%2520high-order%2520correlations%250Aand%2520overcomes%2520the%2520limitation%2520of%2520previous%2520methods%2520that%2520are%2520restricted%2520to%250Apairwise%2520correlation%2520modeling%2520based%2520on%2520hypergraph%2520computation%252C%2520achieving%250Aefficient%2520global%2520cross-location%2520and%2520cross-scale%2520feature%2520fusion%2520and%2520enhancement.%250ASubsequently%252C%2520we%2520propose%2520a%2520Full-Pipeline%2520Aggregation-and-Distribution%2520%2528FullPAD%2529%250Aparadigm%2520based%2520on%2520HyperACE%252C%2520which%2520effectively%2520achieves%2520fine-grained%2520information%250Aflow%2520and%2520representation%2520synergy%2520within%2520the%2520entire%2520network%2520by%2520distributing%250Acorrelation-enhanced%2520features%2520to%2520the%2520full%2520pipeline.%2520Finally%252C%2520we%2520propose%2520to%250Aleverage%2520depthwise%2520separable%2520convolutions%2520to%2520replace%2520vanilla%2520large-kernel%250Aconvolutions%252C%2520and%2520design%2520a%2520series%2520of%2520blocks%2520that%2520significantly%2520reduce%250Aparameters%2520and%2520computational%2520complexity%2520without%2520sacrificing%2520performance.%2520We%250Aconduct%2520extensive%2520experiments%2520on%2520the%2520widely%2520used%2520MS%2520COCO%2520benchmark%252C%2520and%2520the%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520fewer%2520parameters%2520and%2520FLOPs.%2520Specifically%252C%2520our%2520YOLOv13-N%250Aimproves%2520mAP%2520by%25203.0%255C%2525%2520over%2520YOLO11-N%2520and%2520by%25201.5%255C%2525%2520over%2520YOLOv12-N.%2520The%2520code%2520and%250Amodels%2520of%2520our%2520YOLOv13%2520model%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/iMoonLab/yolov13.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOv13%3A%20Real-Time%20Object%20Detection%20with%20Hypergraph-Enhanced%20Adaptive%0A%20%20Visual%20Perception&entry.906535625=Mengqi%20Lei%20and%20Siqi%20Li%20and%20Yihong%20Wu%20and%20Han%20Hu%20and%20You%20Zhou%20and%20Xinhu%20Zheng%20and%20Guiguang%20Ding%20and%20Shaoyi%20Du%20and%20Zongze%20Wu%20and%20Yue%20Gao&entry.1292438233=%20%20The%20YOLO%20series%20models%20reign%20supreme%20in%20real-time%20object%20detection%20due%20to%0Atheir%20superior%20accuracy%20and%20computational%20efficiency.%20However%2C%20both%20the%0Aconvolutional%20architectures%20of%20YOLO11%20and%20earlier%20versions%20and%20the%20area-based%0Aself-attention%20mechanism%20introduced%20in%20YOLOv12%20are%20limited%20to%20local%20information%0Aaggregation%20and%20pairwise%20correlation%20modeling%2C%20lacking%20the%20capability%20to%0Acapture%20global%20multi-to-multi%20high-order%20correlations%2C%20which%20limits%20detection%0Aperformance%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20YOLOv13%2C%20an%0Aaccurate%20and%20lightweight%20object%20detector.%20To%20address%20the%20above-mentioned%0Achallenges%2C%20we%20propose%20a%20Hypergraph-based%20Adaptive%20Correlation%20Enhancement%0A%28HyperACE%29%20mechanism%20that%20adaptively%20exploits%20latent%20high-order%20correlations%0Aand%20overcomes%20the%20limitation%20of%20previous%20methods%20that%20are%20restricted%20to%0Apairwise%20correlation%20modeling%20based%20on%20hypergraph%20computation%2C%20achieving%0Aefficient%20global%20cross-location%20and%20cross-scale%20feature%20fusion%20and%20enhancement.%0ASubsequently%2C%20we%20propose%20a%20Full-Pipeline%20Aggregation-and-Distribution%20%28FullPAD%29%0Aparadigm%20based%20on%20HyperACE%2C%20which%20effectively%20achieves%20fine-grained%20information%0Aflow%20and%20representation%20synergy%20within%20the%20entire%20network%20by%20distributing%0Acorrelation-enhanced%20features%20to%20the%20full%20pipeline.%20Finally%2C%20we%20propose%20to%0Aleverage%20depthwise%20separable%20convolutions%20to%20replace%20vanilla%20large-kernel%0Aconvolutions%2C%20and%20design%20a%20series%20of%20blocks%20that%20significantly%20reduce%0Aparameters%20and%20computational%20complexity%20without%20sacrificing%20performance.%20We%0Aconduct%20extensive%20experiments%20on%20the%20widely%20used%20MS%20COCO%20benchmark%2C%20and%20the%0Aexperimental%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20with%20fewer%20parameters%20and%20FLOPs.%20Specifically%2C%20our%20YOLOv13-N%0Aimproves%20mAP%20by%203.0%5C%25%20over%20YOLO11-N%20and%20by%201.5%5C%25%20over%20YOLOv12-N.%20The%20code%20and%0Amodels%20of%20our%20YOLOv13%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/iMoonLab/yolov13.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.17733v2&entry.124074799=Read"},
{"title": "RAVEN: Query-Guided Representation Alignment for Question Answering over\n  Audio, Video, Embedded Sensors, and Natural Language", "author": "Subrata Biswas and Mohammad Nur Hossain Khan and Bashima Islam", "abstract": "  Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.\n", "link": "http://arxiv.org/abs/2505.17114v3", "date": "2025-09-05", "relevancy": 2.7206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVEN%3A%20Query-Guided%20Representation%20Alignment%20for%20Question%20Answering%20over%0A%20%20Audio%2C%20Video%2C%20Embedded%20Sensors%2C%20and%20Natural%20Language&body=Title%3A%20RAVEN%3A%20Query-Guided%20Representation%20Alignment%20for%20Question%20Answering%20over%0A%20%20Audio%2C%20Video%2C%20Embedded%20Sensors%2C%20and%20Natural%20Language%0AAuthor%3A%20Subrata%20Biswas%20and%20Mohammad%20Nur%20Hossain%20Khan%20and%20Bashima%20Islam%0AAbstract%3A%20%20%20Multimodal%20question%20answering%20%28QA%29%20often%20requires%20identifying%20which%20video%2C%0Aaudio%2C%20or%20sensor%20tokens%20are%20relevant%20to%20the%20question.%20Yet%20modality%0Adisagreements%20are%20common%3A%20off-camera%20speech%2C%20background%20noise%2C%20or%20motion%0Aoutside%20the%20field%20of%20view%20often%20mislead%20fusion%20models%20that%20weight%20all%20streams%0Aequally.%20We%20present%20RAVEN%2C%20a%20unified%20QA%20architecture%20whose%20core%20is%20QuART%2C%20a%0Aquery-conditioned%20cross-modal%20gating%20module%20that%20assigns%20scalar%20relevance%0Ascores%20to%20each%20token%20across%20modalities%2C%20enabling%20the%20model%20to%20amplify%0Ainformative%20signals%20and%20suppress%20distractors%20before%20fusion.%20RAVEN%20is%20trained%0Athrough%20a%20three-stage%20pipeline%20comprising%20unimodal%20pretraining%2C%20query-aligned%0Afusion%2C%20and%20disagreement-oriented%20fine-tuning%20--%20each%20stage%20targeting%20a%0Adistinct%20challenge%20in%20multi-modal%20reasoning%3A%20representation%20quality%2C%0Across-modal%20relevance%2C%20and%20robustness%20to%20modality%20mismatch.%20To%20support%20training%0Aand%20evaluation%2C%20we%20release%20AVS-QA%2C%20a%20dataset%20of%20300K%20synchronized%0AAudio--Video-Sensor%20streams%20paired%20with%20automatically%20generated%20question-answer%0Apairs.%20Experimental%20results%20on%20seven%20multi-modal%20QA%20benchmarks%20--%20including%0Aegocentric%20and%20exocentric%20tasks%20--%20show%20that%20RAVEN%20achieves%20up%20to%2014.5%5C%25%20and%0A8.0%5C%25%20gains%20in%20accuracy%20compared%20to%20state-of-the-art%20multi-modal%20large%20language%0Amodels%2C%20respectively.%20Incorporating%20sensor%20data%20provides%20an%20additional%2016.4%5C%25%0Aboost%2C%20and%20the%20model%20remains%20robust%20under%20modality%20corruption%2C%20outperforming%0ASOTA%20baselines%20by%2050.23%5C%25.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/BASHLab/RAVEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVEN%253A%2520Query-Guided%2520Representation%2520Alignment%2520for%2520Question%2520Answering%2520over%250A%2520%2520Audio%252C%2520Video%252C%2520Embedded%2520Sensors%252C%2520and%2520Natural%2520Language%26entry.906535625%3DSubrata%2520Biswas%2520and%2520Mohammad%2520Nur%2520Hossain%2520Khan%2520and%2520Bashima%2520Islam%26entry.1292438233%3D%2520%2520Multimodal%2520question%2520answering%2520%2528QA%2529%2520often%2520requires%2520identifying%2520which%2520video%252C%250Aaudio%252C%2520or%2520sensor%2520tokens%2520are%2520relevant%2520to%2520the%2520question.%2520Yet%2520modality%250Adisagreements%2520are%2520common%253A%2520off-camera%2520speech%252C%2520background%2520noise%252C%2520or%2520motion%250Aoutside%2520the%2520field%2520of%2520view%2520often%2520mislead%2520fusion%2520models%2520that%2520weight%2520all%2520streams%250Aequally.%2520We%2520present%2520RAVEN%252C%2520a%2520unified%2520QA%2520architecture%2520whose%2520core%2520is%2520QuART%252C%2520a%250Aquery-conditioned%2520cross-modal%2520gating%2520module%2520that%2520assigns%2520scalar%2520relevance%250Ascores%2520to%2520each%2520token%2520across%2520modalities%252C%2520enabling%2520the%2520model%2520to%2520amplify%250Ainformative%2520signals%2520and%2520suppress%2520distractors%2520before%2520fusion.%2520RAVEN%2520is%2520trained%250Athrough%2520a%2520three-stage%2520pipeline%2520comprising%2520unimodal%2520pretraining%252C%2520query-aligned%250Afusion%252C%2520and%2520disagreement-oriented%2520fine-tuning%2520--%2520each%2520stage%2520targeting%2520a%250Adistinct%2520challenge%2520in%2520multi-modal%2520reasoning%253A%2520representation%2520quality%252C%250Across-modal%2520relevance%252C%2520and%2520robustness%2520to%2520modality%2520mismatch.%2520To%2520support%2520training%250Aand%2520evaluation%252C%2520we%2520release%2520AVS-QA%252C%2520a%2520dataset%2520of%2520300K%2520synchronized%250AAudio--Video-Sensor%2520streams%2520paired%2520with%2520automatically%2520generated%2520question-answer%250Apairs.%2520Experimental%2520results%2520on%2520seven%2520multi-modal%2520QA%2520benchmarks%2520--%2520including%250Aegocentric%2520and%2520exocentric%2520tasks%2520--%2520show%2520that%2520RAVEN%2520achieves%2520up%2520to%252014.5%255C%2525%2520and%250A8.0%255C%2525%2520gains%2520in%2520accuracy%2520compared%2520to%2520state-of-the-art%2520multi-modal%2520large%2520language%250Amodels%252C%2520respectively.%2520Incorporating%2520sensor%2520data%2520provides%2520an%2520additional%252016.4%255C%2525%250Aboost%252C%2520and%2520the%2520model%2520remains%2520robust%2520under%2520modality%2520corruption%252C%2520outperforming%250ASOTA%2520baselines%2520by%252050.23%255C%2525.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/BASHLab/RAVEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVEN%3A%20Query-Guided%20Representation%20Alignment%20for%20Question%20Answering%20over%0A%20%20Audio%2C%20Video%2C%20Embedded%20Sensors%2C%20and%20Natural%20Language&entry.906535625=Subrata%20Biswas%20and%20Mohammad%20Nur%20Hossain%20Khan%20and%20Bashima%20Islam&entry.1292438233=%20%20Multimodal%20question%20answering%20%28QA%29%20often%20requires%20identifying%20which%20video%2C%0Aaudio%2C%20or%20sensor%20tokens%20are%20relevant%20to%20the%20question.%20Yet%20modality%0Adisagreements%20are%20common%3A%20off-camera%20speech%2C%20background%20noise%2C%20or%20motion%0Aoutside%20the%20field%20of%20view%20often%20mislead%20fusion%20models%20that%20weight%20all%20streams%0Aequally.%20We%20present%20RAVEN%2C%20a%20unified%20QA%20architecture%20whose%20core%20is%20QuART%2C%20a%0Aquery-conditioned%20cross-modal%20gating%20module%20that%20assigns%20scalar%20relevance%0Ascores%20to%20each%20token%20across%20modalities%2C%20enabling%20the%20model%20to%20amplify%0Ainformative%20signals%20and%20suppress%20distractors%20before%20fusion.%20RAVEN%20is%20trained%0Athrough%20a%20three-stage%20pipeline%20comprising%20unimodal%20pretraining%2C%20query-aligned%0Afusion%2C%20and%20disagreement-oriented%20fine-tuning%20--%20each%20stage%20targeting%20a%0Adistinct%20challenge%20in%20multi-modal%20reasoning%3A%20representation%20quality%2C%0Across-modal%20relevance%2C%20and%20robustness%20to%20modality%20mismatch.%20To%20support%20training%0Aand%20evaluation%2C%20we%20release%20AVS-QA%2C%20a%20dataset%20of%20300K%20synchronized%0AAudio--Video-Sensor%20streams%20paired%20with%20automatically%20generated%20question-answer%0Apairs.%20Experimental%20results%20on%20seven%20multi-modal%20QA%20benchmarks%20--%20including%0Aegocentric%20and%20exocentric%20tasks%20--%20show%20that%20RAVEN%20achieves%20up%20to%2014.5%5C%25%20and%0A8.0%5C%25%20gains%20in%20accuracy%20compared%20to%20state-of-the-art%20multi-modal%20large%20language%0Amodels%2C%20respectively.%20Incorporating%20sensor%20data%20provides%20an%20additional%2016.4%5C%25%0Aboost%2C%20and%20the%20model%20remains%20robust%20under%20modality%20corruption%2C%20outperforming%0ASOTA%20baselines%20by%2050.23%5C%25.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/BASHLab/RAVEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17114v3&entry.124074799=Read"},
{"title": "Enhancing 3D Point Cloud Classification with ModelNet-R and\n  Point-SkipNet", "author": "Mohammad Saeid and Amir Salarpour and Pedram MohajerAnsari", "abstract": "  The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.\n", "link": "http://arxiv.org/abs/2509.05198v1", "date": "2025-09-05", "relevancy": 2.6832, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5185}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%203D%20Point%20Cloud%20Classification%20with%20ModelNet-R%20and%0A%20%20Point-SkipNet&body=Title%3A%20Enhancing%203D%20Point%20Cloud%20Classification%20with%20ModelNet-R%20and%0A%20%20Point-SkipNet%0AAuthor%3A%20Mohammad%20Saeid%20and%20Amir%20Salarpour%20and%20Pedram%20MohajerAnsari%0AAbstract%3A%20%20%20The%20classification%20of%203D%20point%20clouds%20is%20crucial%20for%20applications%20such%20as%0Aautonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20However%2C%20the%20commonly%20used%0AModelNet40%20dataset%20suffers%20from%20limitations%20such%20as%20inconsistent%20labeling%2C%202D%0Adata%2C%20size%20mismatches%2C%20and%20inadequate%20class%20differentiation%2C%20which%20hinder%20model%0Aperformance.%20This%20paper%20introduces%20ModelNet-R%2C%20a%20meticulously%20refined%20version%0Aof%20ModelNet40%20designed%20to%20address%20these%20issues%20and%20serve%20as%20a%20more%20reliable%0Abenchmark.%20Additionally%2C%20this%20paper%20proposes%20Point-SkipNet%2C%20a%20lightweight%0Agraph-based%20neural%20network%20that%20leverages%20efficient%20sampling%2C%20neighborhood%0Agrouping%2C%20and%20skip%20connections%20to%20achieve%20high%20classification%20accuracy%20with%0Areduced%20computational%20overhead.%20Extensive%20experiments%20demonstrate%20that%20models%0Atrained%20in%20ModelNet-R%20exhibit%20significant%20performance%20improvements.%20Notably%2C%0APoint-SkipNet%20achieves%20state-of-the-art%20accuracy%20on%20ModelNet-R%20with%20a%0Asubstantially%20lower%20parameter%20count%20compared%20to%20contemporary%20models.%20This%0Aresearch%20highlights%20the%20crucial%20role%20of%20dataset%20quality%20in%20optimizing%20model%0Aefficiency%20for%203D%20point%20cloud%20classification.%20For%20more%20details%2C%20see%20the%20code%0Aat%3A%20https%3A//github.com/m-saeid/ModeNetR_PointSkipNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%25203D%2520Point%2520Cloud%2520Classification%2520with%2520ModelNet-R%2520and%250A%2520%2520Point-SkipNet%26entry.906535625%3DMohammad%2520Saeid%2520and%2520Amir%2520Salarpour%2520and%2520Pedram%2520MohajerAnsari%26entry.1292438233%3D%2520%2520The%2520classification%2520of%25203D%2520point%2520clouds%2520is%2520crucial%2520for%2520applications%2520such%2520as%250Aautonomous%2520driving%252C%2520robotics%252C%2520and%2520augmented%2520reality.%2520However%252C%2520the%2520commonly%2520used%250AModelNet40%2520dataset%2520suffers%2520from%2520limitations%2520such%2520as%2520inconsistent%2520labeling%252C%25202D%250Adata%252C%2520size%2520mismatches%252C%2520and%2520inadequate%2520class%2520differentiation%252C%2520which%2520hinder%2520model%250Aperformance.%2520This%2520paper%2520introduces%2520ModelNet-R%252C%2520a%2520meticulously%2520refined%2520version%250Aof%2520ModelNet40%2520designed%2520to%2520address%2520these%2520issues%2520and%2520serve%2520as%2520a%2520more%2520reliable%250Abenchmark.%2520Additionally%252C%2520this%2520paper%2520proposes%2520Point-SkipNet%252C%2520a%2520lightweight%250Agraph-based%2520neural%2520network%2520that%2520leverages%2520efficient%2520sampling%252C%2520neighborhood%250Agrouping%252C%2520and%2520skip%2520connections%2520to%2520achieve%2520high%2520classification%2520accuracy%2520with%250Areduced%2520computational%2520overhead.%2520Extensive%2520experiments%2520demonstrate%2520that%2520models%250Atrained%2520in%2520ModelNet-R%2520exhibit%2520significant%2520performance%2520improvements.%2520Notably%252C%250APoint-SkipNet%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520ModelNet-R%2520with%2520a%250Asubstantially%2520lower%2520parameter%2520count%2520compared%2520to%2520contemporary%2520models.%2520This%250Aresearch%2520highlights%2520the%2520crucial%2520role%2520of%2520dataset%2520quality%2520in%2520optimizing%2520model%250Aefficiency%2520for%25203D%2520point%2520cloud%2520classification.%2520For%2520more%2520details%252C%2520see%2520the%2520code%250Aat%253A%2520https%253A//github.com/m-saeid/ModeNetR_PointSkipNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%203D%20Point%20Cloud%20Classification%20with%20ModelNet-R%20and%0A%20%20Point-SkipNet&entry.906535625=Mohammad%20Saeid%20and%20Amir%20Salarpour%20and%20Pedram%20MohajerAnsari&entry.1292438233=%20%20The%20classification%20of%203D%20point%20clouds%20is%20crucial%20for%20applications%20such%20as%0Aautonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20However%2C%20the%20commonly%20used%0AModelNet40%20dataset%20suffers%20from%20limitations%20such%20as%20inconsistent%20labeling%2C%202D%0Adata%2C%20size%20mismatches%2C%20and%20inadequate%20class%20differentiation%2C%20which%20hinder%20model%0Aperformance.%20This%20paper%20introduces%20ModelNet-R%2C%20a%20meticulously%20refined%20version%0Aof%20ModelNet40%20designed%20to%20address%20these%20issues%20and%20serve%20as%20a%20more%20reliable%0Abenchmark.%20Additionally%2C%20this%20paper%20proposes%20Point-SkipNet%2C%20a%20lightweight%0Agraph-based%20neural%20network%20that%20leverages%20efficient%20sampling%2C%20neighborhood%0Agrouping%2C%20and%20skip%20connections%20to%20achieve%20high%20classification%20accuracy%20with%0Areduced%20computational%20overhead.%20Extensive%20experiments%20demonstrate%20that%20models%0Atrained%20in%20ModelNet-R%20exhibit%20significant%20performance%20improvements.%20Notably%2C%0APoint-SkipNet%20achieves%20state-of-the-art%20accuracy%20on%20ModelNet-R%20with%20a%0Asubstantially%20lower%20parameter%20count%20compared%20to%20contemporary%20models.%20This%0Aresearch%20highlights%20the%20crucial%20role%20of%20dataset%20quality%20in%20optimizing%20model%0Aefficiency%20for%203D%20point%20cloud%20classification.%20For%20more%20details%2C%20see%20the%20code%0Aat%3A%20https%3A//github.com/m-saeid/ModeNetR_PointSkipNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05198v1&entry.124074799=Read"},
{"title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks", "author": "Arefin Niam and Tevfik Kosar and M S Q Zulkar Nine", "abstract": "  Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.\n", "link": "http://arxiv.org/abs/2509.05207v1", "date": "2025-09-05", "relevancy": 2.664, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.543}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RapidGNN%3A%20Energy%20and%20Communication-Efficient%20Distributed%20Training%20on%0A%20%20Large-Scale%20Graph%20Neural%20Networks&body=Title%3A%20RapidGNN%3A%20Energy%20and%20Communication-Efficient%20Distributed%20Training%20on%0A%20%20Large-Scale%20Graph%20Neural%20Networks%0AAuthor%3A%20Arefin%20Niam%20and%20Tevfik%20Kosar%20and%20M%20S%20Q%20Zulkar%20Nine%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20popular%20across%20a%20diverse%20set%20of%0Atasks%20in%20exploring%20structural%20relationships%20between%20entities.%20However%2C%20due%20to%0Athe%20highly%20connected%20structure%20of%20the%20datasets%2C%20distributed%20training%20of%20GNNs%20on%0Alarge-scale%20graphs%20poses%20significant%20challenges.%20Traditional%20sampling-based%0Aapproaches%20mitigate%20the%20computational%20loads%2C%20yet%20the%20communication%20overhead%0Aremains%20a%20challenge.%20This%20paper%20presents%20RapidGNN%2C%20a%20distributed%20GNN%20training%0Aframework%20with%20deterministic%20sampling-based%20scheduling%20to%20enable%20efficient%0Acache%20construction%20and%20prefetching%20of%20remote%20features.%20Evaluation%20on%20benchmark%0Agraph%20datasets%20demonstrates%20RapidGNN%27s%20effectiveness%20across%20different%20scales%0Aand%20topologies.%20RapidGNN%20improves%20end-to-end%20training%20throughput%20by%202.46x%20to%0A3.00x%20on%20average%20over%20baseline%20methods%20across%20the%20benchmark%20datasets%2C%20while%0Acutting%20remote%20feature%20fetches%20by%20over%209.70x%20to%2015.39x.%20RapidGNN%20further%0Ademonstrates%20near-linear%20scalability%20with%20an%20increasing%20number%20of%20computing%0Aunits%20efficiently.%20Furthermore%2C%20it%20achieves%20increased%20energy%20efficiency%20over%0Athe%20baseline%20methods%20for%20both%20CPU%20and%20GPU%20by%2044%25%20and%2032%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapidGNN%253A%2520Energy%2520and%2520Communication-Efficient%2520Distributed%2520Training%2520on%250A%2520%2520Large-Scale%2520Graph%2520Neural%2520Networks%26entry.906535625%3DArefin%2520Niam%2520and%2520Tevfik%2520Kosar%2520and%2520M%2520S%2520Q%2520Zulkar%2520Nine%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520popular%2520across%2520a%2520diverse%2520set%2520of%250Atasks%2520in%2520exploring%2520structural%2520relationships%2520between%2520entities.%2520However%252C%2520due%2520to%250Athe%2520highly%2520connected%2520structure%2520of%2520the%2520datasets%252C%2520distributed%2520training%2520of%2520GNNs%2520on%250Alarge-scale%2520graphs%2520poses%2520significant%2520challenges.%2520Traditional%2520sampling-based%250Aapproaches%2520mitigate%2520the%2520computational%2520loads%252C%2520yet%2520the%2520communication%2520overhead%250Aremains%2520a%2520challenge.%2520This%2520paper%2520presents%2520RapidGNN%252C%2520a%2520distributed%2520GNN%2520training%250Aframework%2520with%2520deterministic%2520sampling-based%2520scheduling%2520to%2520enable%2520efficient%250Acache%2520construction%2520and%2520prefetching%2520of%2520remote%2520features.%2520Evaluation%2520on%2520benchmark%250Agraph%2520datasets%2520demonstrates%2520RapidGNN%2527s%2520effectiveness%2520across%2520different%2520scales%250Aand%2520topologies.%2520RapidGNN%2520improves%2520end-to-end%2520training%2520throughput%2520by%25202.46x%2520to%250A3.00x%2520on%2520average%2520over%2520baseline%2520methods%2520across%2520the%2520benchmark%2520datasets%252C%2520while%250Acutting%2520remote%2520feature%2520fetches%2520by%2520over%25209.70x%2520to%252015.39x.%2520RapidGNN%2520further%250Ademonstrates%2520near-linear%2520scalability%2520with%2520an%2520increasing%2520number%2520of%2520computing%250Aunits%2520efficiently.%2520Furthermore%252C%2520it%2520achieves%2520increased%2520energy%2520efficiency%2520over%250Athe%2520baseline%2520methods%2520for%2520both%2520CPU%2520and%2520GPU%2520by%252044%2525%2520and%252032%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RapidGNN%3A%20Energy%20and%20Communication-Efficient%20Distributed%20Training%20on%0A%20%20Large-Scale%20Graph%20Neural%20Networks&entry.906535625=Arefin%20Niam%20and%20Tevfik%20Kosar%20and%20M%20S%20Q%20Zulkar%20Nine&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20popular%20across%20a%20diverse%20set%20of%0Atasks%20in%20exploring%20structural%20relationships%20between%20entities.%20However%2C%20due%20to%0Athe%20highly%20connected%20structure%20of%20the%20datasets%2C%20distributed%20training%20of%20GNNs%20on%0Alarge-scale%20graphs%20poses%20significant%20challenges.%20Traditional%20sampling-based%0Aapproaches%20mitigate%20the%20computational%20loads%2C%20yet%20the%20communication%20overhead%0Aremains%20a%20challenge.%20This%20paper%20presents%20RapidGNN%2C%20a%20distributed%20GNN%20training%0Aframework%20with%20deterministic%20sampling-based%20scheduling%20to%20enable%20efficient%0Acache%20construction%20and%20prefetching%20of%20remote%20features.%20Evaluation%20on%20benchmark%0Agraph%20datasets%20demonstrates%20RapidGNN%27s%20effectiveness%20across%20different%20scales%0Aand%20topologies.%20RapidGNN%20improves%20end-to-end%20training%20throughput%20by%202.46x%20to%0A3.00x%20on%20average%20over%20baseline%20methods%20across%20the%20benchmark%20datasets%2C%20while%0Acutting%20remote%20feature%20fetches%20by%20over%209.70x%20to%2015.39x.%20RapidGNN%20further%0Ademonstrates%20near-linear%20scalability%20with%20an%20increasing%20number%20of%20computing%0Aunits%20efficiently.%20Furthermore%2C%20it%20achieves%20increased%20energy%20efficiency%20over%0Athe%20baseline%20methods%20for%20both%20CPU%20and%20GPU%20by%2044%25%20and%2032%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05207v1&entry.124074799=Read"},
{"title": "Symbolic Graphics Programming with Large Language Models", "author": "Yamei Chen and Haoquan Zhang and Yangyi Huang and Zeju Qiu and Kaipeng Zhang and Yandong Wen and Weiyang Liu", "abstract": "  Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.\n", "link": "http://arxiv.org/abs/2509.05208v1", "date": "2025-09-05", "relevancy": 2.6586, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Graphics%20Programming%20with%20Large%20Language%20Models&body=Title%3A%20Symbolic%20Graphics%20Programming%20with%20Large%20Language%20Models%0AAuthor%3A%20Yamei%20Chen%20and%20Haoquan%20Zhang%20and%20Yangyi%20Huang%20and%20Zeju%20Qiu%20and%20Kaipeng%20Zhang%20and%20Yandong%20Wen%20and%20Weiyang%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20program%20synthesis%2C%20yet%20their%20ability%20to%0Aproduce%20symbolic%20graphics%20programs%20%28SGPs%29%20that%20render%20into%20precise%20visual%0Acontent%20remains%20underexplored.%20We%20study%20symbolic%20graphics%20programming%2C%20where%0Athe%20goal%20is%20to%20generate%20an%20SGP%20from%20a%20natural-language%20description.%20This%20task%0Aalso%20serves%20as%20a%20lens%20into%20how%20LLMs%20understand%20the%20visual%20world%20by%20prompting%0Athem%20to%20generate%20images%20rendered%20from%20SGPs.%20Among%20various%20SGPs%2C%20our%20paper%0Asticks%20to%20scalable%20vector%20graphics%20%28SVGs%29.%20We%20begin%20by%20examining%20the%20extent%20to%0Awhich%20LLMs%20can%20generate%20SGPs.%20To%20this%20end%2C%20we%20introduce%20SGP-GenBench%2C%20a%0Acomprehensive%20benchmark%20covering%20object%20fidelity%2C%20scene%20fidelity%2C%20and%0Acompositionality%20%28attribute%20binding%2C%20spatial%20relations%2C%20numeracy%29.%20On%0ASGP-GenBench%2C%20we%20discover%20that%20frontier%20proprietary%20models%20substantially%0Aoutperform%20open-source%20models%2C%20and%20performance%20correlates%20well%20with%20general%0Acoding%20capabilities.%20Motivated%20by%20this%20gap%2C%20we%20aim%20to%20improve%20LLMs%27%20ability%20to%0Agenerate%20SGPs.%20We%20propose%20a%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20rewards%0Aapproach%2C%20where%20a%20format-validity%20gate%20ensures%20renderable%20SVG%2C%20and%20a%0Across-modal%20reward%20aligns%20text%20and%20the%20rendered%20image%20via%20strong%20vision%0Aencoders%20%28e.g.%2C%20SigLIP%20for%20text-image%20and%20DINO%20for%20image-image%29.%20Applied%20to%0AQwen-2.5-7B%2C%20our%20method%20substantially%20improves%20SVG%20generation%20quality%20and%0Asemantics%2C%20achieving%20performance%20on%20par%20with%20frontier%20systems.%20We%20further%0Aanalyze%20training%20dynamics%2C%20showing%20that%20RL%20induces%20%28i%29%20finer%20decomposition%20of%0Aobjects%20into%20controllable%20primitives%20and%20%28ii%29%20contextual%20details%20that%20improve%0Ascene%20coherence.%20Our%20results%20demonstrate%20that%20symbolic%20graphics%20programming%0Aoffers%20a%20precise%20and%20interpretable%20lens%20on%20cross-modal%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520Graphics%2520Programming%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DYamei%2520Chen%2520and%2520Haoquan%2520Zhang%2520and%2520Yangyi%2520Huang%2520and%2520Zeju%2520Qiu%2520and%2520Kaipeng%2520Zhang%2520and%2520Yandong%2520Wen%2520and%2520Weiyang%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520program%2520synthesis%252C%2520yet%2520their%2520ability%2520to%250Aproduce%2520symbolic%2520graphics%2520programs%2520%2528SGPs%2529%2520that%2520render%2520into%2520precise%2520visual%250Acontent%2520remains%2520underexplored.%2520We%2520study%2520symbolic%2520graphics%2520programming%252C%2520where%250Athe%2520goal%2520is%2520to%2520generate%2520an%2520SGP%2520from%2520a%2520natural-language%2520description.%2520This%2520task%250Aalso%2520serves%2520as%2520a%2520lens%2520into%2520how%2520LLMs%2520understand%2520the%2520visual%2520world%2520by%2520prompting%250Athem%2520to%2520generate%2520images%2520rendered%2520from%2520SGPs.%2520Among%2520various%2520SGPs%252C%2520our%2520paper%250Asticks%2520to%2520scalable%2520vector%2520graphics%2520%2528SVGs%2529.%2520We%2520begin%2520by%2520examining%2520the%2520extent%2520to%250Awhich%2520LLMs%2520can%2520generate%2520SGPs.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SGP-GenBench%252C%2520a%250Acomprehensive%2520benchmark%2520covering%2520object%2520fidelity%252C%2520scene%2520fidelity%252C%2520and%250Acompositionality%2520%2528attribute%2520binding%252C%2520spatial%2520relations%252C%2520numeracy%2529.%2520On%250ASGP-GenBench%252C%2520we%2520discover%2520that%2520frontier%2520proprietary%2520models%2520substantially%250Aoutperform%2520open-source%2520models%252C%2520and%2520performance%2520correlates%2520well%2520with%2520general%250Acoding%2520capabilities.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520aim%2520to%2520improve%2520LLMs%2527%2520ability%2520to%250Agenerate%2520SGPs.%2520We%2520propose%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520verifiable%2520rewards%250Aapproach%252C%2520where%2520a%2520format-validity%2520gate%2520ensures%2520renderable%2520SVG%252C%2520and%2520a%250Across-modal%2520reward%2520aligns%2520text%2520and%2520the%2520rendered%2520image%2520via%2520strong%2520vision%250Aencoders%2520%2528e.g.%252C%2520SigLIP%2520for%2520text-image%2520and%2520DINO%2520for%2520image-image%2529.%2520Applied%2520to%250AQwen-2.5-7B%252C%2520our%2520method%2520substantially%2520improves%2520SVG%2520generation%2520quality%2520and%250Asemantics%252C%2520achieving%2520performance%2520on%2520par%2520with%2520frontier%2520systems.%2520We%2520further%250Aanalyze%2520training%2520dynamics%252C%2520showing%2520that%2520RL%2520induces%2520%2528i%2529%2520finer%2520decomposition%2520of%250Aobjects%2520into%2520controllable%2520primitives%2520and%2520%2528ii%2529%2520contextual%2520details%2520that%2520improve%250Ascene%2520coherence.%2520Our%2520results%2520demonstrate%2520that%2520symbolic%2520graphics%2520programming%250Aoffers%2520a%2520precise%2520and%2520interpretable%2520lens%2520on%2520cross-modal%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Graphics%20Programming%20with%20Large%20Language%20Models&entry.906535625=Yamei%20Chen%20and%20Haoquan%20Zhang%20and%20Yangyi%20Huang%20and%20Zeju%20Qiu%20and%20Kaipeng%20Zhang%20and%20Yandong%20Wen%20and%20Weiyang%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20program%20synthesis%2C%20yet%20their%20ability%20to%0Aproduce%20symbolic%20graphics%20programs%20%28SGPs%29%20that%20render%20into%20precise%20visual%0Acontent%20remains%20underexplored.%20We%20study%20symbolic%20graphics%20programming%2C%20where%0Athe%20goal%20is%20to%20generate%20an%20SGP%20from%20a%20natural-language%20description.%20This%20task%0Aalso%20serves%20as%20a%20lens%20into%20how%20LLMs%20understand%20the%20visual%20world%20by%20prompting%0Athem%20to%20generate%20images%20rendered%20from%20SGPs.%20Among%20various%20SGPs%2C%20our%20paper%0Asticks%20to%20scalable%20vector%20graphics%20%28SVGs%29.%20We%20begin%20by%20examining%20the%20extent%20to%0Awhich%20LLMs%20can%20generate%20SGPs.%20To%20this%20end%2C%20we%20introduce%20SGP-GenBench%2C%20a%0Acomprehensive%20benchmark%20covering%20object%20fidelity%2C%20scene%20fidelity%2C%20and%0Acompositionality%20%28attribute%20binding%2C%20spatial%20relations%2C%20numeracy%29.%20On%0ASGP-GenBench%2C%20we%20discover%20that%20frontier%20proprietary%20models%20substantially%0Aoutperform%20open-source%20models%2C%20and%20performance%20correlates%20well%20with%20general%0Acoding%20capabilities.%20Motivated%20by%20this%20gap%2C%20we%20aim%20to%20improve%20LLMs%27%20ability%20to%0Agenerate%20SGPs.%20We%20propose%20a%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20rewards%0Aapproach%2C%20where%20a%20format-validity%20gate%20ensures%20renderable%20SVG%2C%20and%20a%0Across-modal%20reward%20aligns%20text%20and%20the%20rendered%20image%20via%20strong%20vision%0Aencoders%20%28e.g.%2C%20SigLIP%20for%20text-image%20and%20DINO%20for%20image-image%29.%20Applied%20to%0AQwen-2.5-7B%2C%20our%20method%20substantially%20improves%20SVG%20generation%20quality%20and%0Asemantics%2C%20achieving%20performance%20on%20par%20with%20frontier%20systems.%20We%20further%0Aanalyze%20training%20dynamics%2C%20showing%20that%20RL%20induces%20%28i%29%20finer%20decomposition%20of%0Aobjects%20into%20controllable%20primitives%20and%20%28ii%29%20contextual%20details%20that%20improve%0Ascene%20coherence.%20Our%20results%20demonstrate%20that%20symbolic%20graphics%20programming%0Aoffers%20a%20precise%20and%20interpretable%20lens%20on%20cross-modal%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05208v1&entry.124074799=Read"},
{"title": "Global-to-Local or Local-to-Global? Enhancing Image Retrieval with\n  Efficient Local Search and Effective Global Re-ranking", "author": "Dror Aiger and Bingyi Cao and Kaifeng Chen and Andre Araujo", "abstract": "  The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.\n", "link": "http://arxiv.org/abs/2509.04351v2", "date": "2025-09-05", "relevancy": 2.6251, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking&body=Title%3A%20Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking%0AAuthor%3A%20Dror%20Aiger%20and%20Bingyi%20Cao%20and%20Kaifeng%20Chen%20and%20Andre%20Araujo%0AAbstract%3A%20%20%20The%20dominant%20paradigm%20in%20image%20retrieval%20systems%20today%20is%20to%20search%20large%0Adatabases%20using%20global%20image%20features%2C%20and%20re-rank%20those%20initial%20results%20with%0Alocal%20image%20feature%20matching%20techniques.%20This%20design%2C%20dubbed%20global-to-local%2C%0Astems%20from%20the%20computational%20cost%20of%20local%20matching%20approaches%2C%20which%20can%20only%0Abe%20afforded%20for%20a%20small%20number%20of%20retrieved%20images.%20However%2C%20emerging%20efficient%0Alocal%20feature%20search%20approaches%20have%20opened%20up%20new%20possibilities%2C%20in%20particular%0Aenabling%20detailed%20retrieval%20at%20large%20scale%2C%20to%20find%20partial%20matches%20which%20are%0Aoften%20missed%20by%20global%20feature%20search.%20In%20parallel%2C%20global%20feature-based%0Are-ranking%20has%20shown%20promising%20results%20with%20high%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20leverage%20these%20building%20blocks%20to%20introduce%20a%20local-to-global%0Aretrieval%20paradigm%2C%20where%20efficient%20local%20feature%20search%20meets%20effective%20global%0Afeature%20re-ranking.%20Critically%2C%20we%20propose%20a%20re-ranking%20method%20where%20global%0Afeatures%20are%20computed%20on-the-fly%2C%20based%20on%20the%20local%20feature%20retrieval%0Asimilarities.%20Such%20re-ranking-only%20global%20features%20leverage%20multidimensional%0Ascaling%20techniques%20to%20create%20embeddings%20which%20respect%20the%20local%20similarities%0Aobtained%20during%20search%2C%20enabling%20a%20significant%20re-ranking%20boost.%0AExperimentally%2C%20we%20demonstrate%20solid%20retrieval%20performance%2C%20setting%20new%0Astate-of-the-art%20results%20on%20the%20Revisited%20Oxford%20and%20Paris%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-to-Local%2520or%2520Local-to-Global%253F%2520Enhancing%2520Image%2520Retrieval%2520with%250A%2520%2520Efficient%2520Local%2520Search%2520and%2520Effective%2520Global%2520Re-ranking%26entry.906535625%3DDror%2520Aiger%2520and%2520Bingyi%2520Cao%2520and%2520Kaifeng%2520Chen%2520and%2520Andre%2520Araujo%26entry.1292438233%3D%2520%2520The%2520dominant%2520paradigm%2520in%2520image%2520retrieval%2520systems%2520today%2520is%2520to%2520search%2520large%250Adatabases%2520using%2520global%2520image%2520features%252C%2520and%2520re-rank%2520those%2520initial%2520results%2520with%250Alocal%2520image%2520feature%2520matching%2520techniques.%2520This%2520design%252C%2520dubbed%2520global-to-local%252C%250Astems%2520from%2520the%2520computational%2520cost%2520of%2520local%2520matching%2520approaches%252C%2520which%2520can%2520only%250Abe%2520afforded%2520for%2520a%2520small%2520number%2520of%2520retrieved%2520images.%2520However%252C%2520emerging%2520efficient%250Alocal%2520feature%2520search%2520approaches%2520have%2520opened%2520up%2520new%2520possibilities%252C%2520in%2520particular%250Aenabling%2520detailed%2520retrieval%2520at%2520large%2520scale%252C%2520to%2520find%2520partial%2520matches%2520which%2520are%250Aoften%2520missed%2520by%2520global%2520feature%2520search.%2520In%2520parallel%252C%2520global%2520feature-based%250Are-ranking%2520has%2520shown%2520promising%2520results%2520with%2520high%2520computational%2520efficiency.%2520In%250Athis%2520work%252C%2520we%2520leverage%2520these%2520building%2520blocks%2520to%2520introduce%2520a%2520local-to-global%250Aretrieval%2520paradigm%252C%2520where%2520efficient%2520local%2520feature%2520search%2520meets%2520effective%2520global%250Afeature%2520re-ranking.%2520Critically%252C%2520we%2520propose%2520a%2520re-ranking%2520method%2520where%2520global%250Afeatures%2520are%2520computed%2520on-the-fly%252C%2520based%2520on%2520the%2520local%2520feature%2520retrieval%250Asimilarities.%2520Such%2520re-ranking-only%2520global%2520features%2520leverage%2520multidimensional%250Ascaling%2520techniques%2520to%2520create%2520embeddings%2520which%2520respect%2520the%2520local%2520similarities%250Aobtained%2520during%2520search%252C%2520enabling%2520a%2520significant%2520re-ranking%2520boost.%250AExperimentally%252C%2520we%2520demonstrate%2520solid%2520retrieval%2520performance%252C%2520setting%2520new%250Astate-of-the-art%2520results%2520on%2520the%2520Revisited%2520Oxford%2520and%2520Paris%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking&entry.906535625=Dror%20Aiger%20and%20Bingyi%20Cao%20and%20Kaifeng%20Chen%20and%20Andre%20Araujo&entry.1292438233=%20%20The%20dominant%20paradigm%20in%20image%20retrieval%20systems%20today%20is%20to%20search%20large%0Adatabases%20using%20global%20image%20features%2C%20and%20re-rank%20those%20initial%20results%20with%0Alocal%20image%20feature%20matching%20techniques.%20This%20design%2C%20dubbed%20global-to-local%2C%0Astems%20from%20the%20computational%20cost%20of%20local%20matching%20approaches%2C%20which%20can%20only%0Abe%20afforded%20for%20a%20small%20number%20of%20retrieved%20images.%20However%2C%20emerging%20efficient%0Alocal%20feature%20search%20approaches%20have%20opened%20up%20new%20possibilities%2C%20in%20particular%0Aenabling%20detailed%20retrieval%20at%20large%20scale%2C%20to%20find%20partial%20matches%20which%20are%0Aoften%20missed%20by%20global%20feature%20search.%20In%20parallel%2C%20global%20feature-based%0Are-ranking%20has%20shown%20promising%20results%20with%20high%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20leverage%20these%20building%20blocks%20to%20introduce%20a%20local-to-global%0Aretrieval%20paradigm%2C%20where%20efficient%20local%20feature%20search%20meets%20effective%20global%0Afeature%20re-ranking.%20Critically%2C%20we%20propose%20a%20re-ranking%20method%20where%20global%0Afeatures%20are%20computed%20on-the-fly%2C%20based%20on%20the%20local%20feature%20retrieval%0Asimilarities.%20Such%20re-ranking-only%20global%20features%20leverage%20multidimensional%0Ascaling%20techniques%20to%20create%20embeddings%20which%20respect%20the%20local%20similarities%0Aobtained%20during%20search%2C%20enabling%20a%20significant%20re-ranking%20boost.%0AExperimentally%2C%20we%20demonstrate%20solid%20retrieval%20performance%2C%20setting%20new%0Astate-of-the-art%20results%20on%20the%20Revisited%20Oxford%20and%20Paris%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04351v2&entry.124074799=Read"},
{"title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating\n  Conceptual Shortcuts in Pre-Trained Language Models", "author": "Aysenur Kocak and Shuo Yang and Bardh Prenkaj and Gjergji Kasneci", "abstract": "  Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems.\n", "link": "http://arxiv.org/abs/2509.05230v1", "date": "2025-09-05", "relevancy": 2.622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models&body=Title%3A%20CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models%0AAuthor%3A%20Aysenur%20Kocak%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications%20but%20remain%20susceptible%20to%20spurious%2C%20concept-driven%20correlations%0Athat%20impair%20robustness%20and%20fairness.%20In%20this%20work%2C%20we%20introduce%20CURE%2C%20a%20novel%0Aand%20lightweight%20framework%20that%20systematically%20disentangles%20and%20suppresses%0Aconceptual%20shortcuts%20while%20preserving%20essential%20content%20information.%20Our%20method%0Afirst%20extracts%20concept-irrelevant%20representations%20via%20a%20dedicated%20content%0Aextractor%20reinforced%20by%20a%20reversal%20network%2C%20ensuring%20minimal%20loss%20of%0Atask-relevant%20information.%20A%20subsequent%20controllable%20debiasing%20module%20employs%0Acontrastive%20learning%20to%20finely%20adjust%20the%20influence%20of%20residual%20conceptual%0Acues%2C%20enabling%20the%20model%20to%20either%20diminish%20harmful%20biases%20or%20harness%0Abeneficial%20correlations%20as%20appropriate%20for%20the%20target%20task.%20Evaluated%20on%20the%0AIMDB%20and%20Yelp%20datasets%20using%20three%20pre-trained%20architectures%2C%20CURE%20achieves%20an%0Aabsolute%20improvement%20of%20%2B10%20points%20in%20F1%20score%20on%20IMDB%20and%20%2B2%20points%20on%20Yelp%2C%0Awhile%20introducing%20minimal%20computational%20overhead.%20Our%20approach%20establishes%20a%0Aflexible%2C%20unsupervised%20blueprint%20for%20combating%20conceptual%20biases%2C%20paving%20the%0Away%20for%20more%20reliable%20and%20fair%20language%20understanding%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURE%253A%2520Controlled%2520Unlearning%2520for%2520Robust%2520Embeddings%2520--%2520Mitigating%250A%2520%2520Conceptual%2520Shortcuts%2520in%2520Pre-Trained%2520Language%2520Models%26entry.906535625%3DAysenur%2520Kocak%2520and%2520Shuo%2520Yang%2520and%2520Bardh%2520Prenkaj%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%250Aapplications%2520but%2520remain%2520susceptible%2520to%2520spurious%252C%2520concept-driven%2520correlations%250Athat%2520impair%2520robustness%2520and%2520fairness.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CURE%252C%2520a%2520novel%250Aand%2520lightweight%2520framework%2520that%2520systematically%2520disentangles%2520and%2520suppresses%250Aconceptual%2520shortcuts%2520while%2520preserving%2520essential%2520content%2520information.%2520Our%2520method%250Afirst%2520extracts%2520concept-irrelevant%2520representations%2520via%2520a%2520dedicated%2520content%250Aextractor%2520reinforced%2520by%2520a%2520reversal%2520network%252C%2520ensuring%2520minimal%2520loss%2520of%250Atask-relevant%2520information.%2520A%2520subsequent%2520controllable%2520debiasing%2520module%2520employs%250Acontrastive%2520learning%2520to%2520finely%2520adjust%2520the%2520influence%2520of%2520residual%2520conceptual%250Acues%252C%2520enabling%2520the%2520model%2520to%2520either%2520diminish%2520harmful%2520biases%2520or%2520harness%250Abeneficial%2520correlations%2520as%2520appropriate%2520for%2520the%2520target%2520task.%2520Evaluated%2520on%2520the%250AIMDB%2520and%2520Yelp%2520datasets%2520using%2520three%2520pre-trained%2520architectures%252C%2520CURE%2520achieves%2520an%250Aabsolute%2520improvement%2520of%2520%252B10%2520points%2520in%2520F1%2520score%2520on%2520IMDB%2520and%2520%252B2%2520points%2520on%2520Yelp%252C%250Awhile%2520introducing%2520minimal%2520computational%2520overhead.%2520Our%2520approach%2520establishes%2520a%250Aflexible%252C%2520unsupervised%2520blueprint%2520for%2520combating%2520conceptual%2520biases%252C%2520paving%2520the%250Away%2520for%2520more%2520reliable%2520and%2520fair%2520language%2520understanding%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models&entry.906535625=Aysenur%20Kocak%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications%20but%20remain%20susceptible%20to%20spurious%2C%20concept-driven%20correlations%0Athat%20impair%20robustness%20and%20fairness.%20In%20this%20work%2C%20we%20introduce%20CURE%2C%20a%20novel%0Aand%20lightweight%20framework%20that%20systematically%20disentangles%20and%20suppresses%0Aconceptual%20shortcuts%20while%20preserving%20essential%20content%20information.%20Our%20method%0Afirst%20extracts%20concept-irrelevant%20representations%20via%20a%20dedicated%20content%0Aextractor%20reinforced%20by%20a%20reversal%20network%2C%20ensuring%20minimal%20loss%20of%0Atask-relevant%20information.%20A%20subsequent%20controllable%20debiasing%20module%20employs%0Acontrastive%20learning%20to%20finely%20adjust%20the%20influence%20of%20residual%20conceptual%0Acues%2C%20enabling%20the%20model%20to%20either%20diminish%20harmful%20biases%20or%20harness%0Abeneficial%20correlations%20as%20appropriate%20for%20the%20target%20task.%20Evaluated%20on%20the%0AIMDB%20and%20Yelp%20datasets%20using%20three%20pre-trained%20architectures%2C%20CURE%20achieves%20an%0Aabsolute%20improvement%20of%20%2B10%20points%20in%20F1%20score%20on%20IMDB%20and%20%2B2%20points%20on%20Yelp%2C%0Awhile%20introducing%20minimal%20computational%20overhead.%20Our%20approach%20establishes%20a%0Aflexible%2C%20unsupervised%20blueprint%20for%20combating%20conceptual%20biases%2C%20paving%20the%0Away%20for%20more%20reliable%20and%20fair%20language%20understanding%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05230v1&entry.124074799=Read"},
{"title": "PRIM: Towards Practical In-Image Multilingual Machine Translation", "author": "Yanzhi Tian and Zeming Liu and Zhengyang Liu and Chong Feng and Xin Li and Heyan Huang and Yuhang Guo", "abstract": "  In-Image Machine Translation (IIMT) aims to translate images containing texts\nfrom one language to another. Current research of end-to-end IIMT mainly\nconducts on synthetic data, with simple background, single font, fixed text\nposition, and bilingual translation, which can not fully reflect real world,\ncausing a significant gap between the research and practical conditions. To\nfacilitate research of IIMT in real-world scenarios, we explore Practical\nIn-Image Multilingual Machine Translation (IIMMT). In order to convince the\nlack of publicly available data, we annotate the PRIM dataset, which contains\nreal-world captured one-line text images with complex background, various\nfonts, diverse text positions, and supports multilingual translation\ndirections. We propose an end-to-end model VisTrans to handle the challenge of\npractical conditions in PRIM, which processes visual text and background\ninformation in the image separately, ensuring the capability of multilingual\ntranslation while improving the visual quality. Experimental results indicate\nthe VisTrans achieves a better translation quality and visual effect compared\nto other models. The code and dataset are available at:\nhttps://github.com/BITHLP/PRIM.\n", "link": "http://arxiv.org/abs/2509.05146v1", "date": "2025-09-05", "relevancy": 2.6094, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIM%3A%20Towards%20Practical%20In-Image%20Multilingual%20Machine%20Translation&body=Title%3A%20PRIM%3A%20Towards%20Practical%20In-Image%20Multilingual%20Machine%20Translation%0AAuthor%3A%20Yanzhi%20Tian%20and%20Zeming%20Liu%20and%20Zhengyang%20Liu%20and%20Chong%20Feng%20and%20Xin%20Li%20and%20Heyan%20Huang%20and%20Yuhang%20Guo%0AAbstract%3A%20%20%20In-Image%20Machine%20Translation%20%28IIMT%29%20aims%20to%20translate%20images%20containing%20texts%0Afrom%20one%20language%20to%20another.%20Current%20research%20of%20end-to-end%20IIMT%20mainly%0Aconducts%20on%20synthetic%20data%2C%20with%20simple%20background%2C%20single%20font%2C%20fixed%20text%0Aposition%2C%20and%20bilingual%20translation%2C%20which%20can%20not%20fully%20reflect%20real%20world%2C%0Acausing%20a%20significant%20gap%20between%20the%20research%20and%20practical%20conditions.%20To%0Afacilitate%20research%20of%20IIMT%20in%20real-world%20scenarios%2C%20we%20explore%20Practical%0AIn-Image%20Multilingual%20Machine%20Translation%20%28IIMMT%29.%20In%20order%20to%20convince%20the%0Alack%20of%20publicly%20available%20data%2C%20we%20annotate%20the%20PRIM%20dataset%2C%20which%20contains%0Areal-world%20captured%20one-line%20text%20images%20with%20complex%20background%2C%20various%0Afonts%2C%20diverse%20text%20positions%2C%20and%20supports%20multilingual%20translation%0Adirections.%20We%20propose%20an%20end-to-end%20model%20VisTrans%20to%20handle%20the%20challenge%20of%0Apractical%20conditions%20in%20PRIM%2C%20which%20processes%20visual%20text%20and%20background%0Ainformation%20in%20the%20image%20separately%2C%20ensuring%20the%20capability%20of%20multilingual%0Atranslation%20while%20improving%20the%20visual%20quality.%20Experimental%20results%20indicate%0Athe%20VisTrans%20achieves%20a%20better%20translation%20quality%20and%20visual%20effect%20compared%0Ato%20other%20models.%20The%20code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/BITHLP/PRIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIM%253A%2520Towards%2520Practical%2520In-Image%2520Multilingual%2520Machine%2520Translation%26entry.906535625%3DYanzhi%2520Tian%2520and%2520Zeming%2520Liu%2520and%2520Zhengyang%2520Liu%2520and%2520Chong%2520Feng%2520and%2520Xin%2520Li%2520and%2520Heyan%2520Huang%2520and%2520Yuhang%2520Guo%26entry.1292438233%3D%2520%2520In-Image%2520Machine%2520Translation%2520%2528IIMT%2529%2520aims%2520to%2520translate%2520images%2520containing%2520texts%250Afrom%2520one%2520language%2520to%2520another.%2520Current%2520research%2520of%2520end-to-end%2520IIMT%2520mainly%250Aconducts%2520on%2520synthetic%2520data%252C%2520with%2520simple%2520background%252C%2520single%2520font%252C%2520fixed%2520text%250Aposition%252C%2520and%2520bilingual%2520translation%252C%2520which%2520can%2520not%2520fully%2520reflect%2520real%2520world%252C%250Acausing%2520a%2520significant%2520gap%2520between%2520the%2520research%2520and%2520practical%2520conditions.%2520To%250Afacilitate%2520research%2520of%2520IIMT%2520in%2520real-world%2520scenarios%252C%2520we%2520explore%2520Practical%250AIn-Image%2520Multilingual%2520Machine%2520Translation%2520%2528IIMMT%2529.%2520In%2520order%2520to%2520convince%2520the%250Alack%2520of%2520publicly%2520available%2520data%252C%2520we%2520annotate%2520the%2520PRIM%2520dataset%252C%2520which%2520contains%250Areal-world%2520captured%2520one-line%2520text%2520images%2520with%2520complex%2520background%252C%2520various%250Afonts%252C%2520diverse%2520text%2520positions%252C%2520and%2520supports%2520multilingual%2520translation%250Adirections.%2520We%2520propose%2520an%2520end-to-end%2520model%2520VisTrans%2520to%2520handle%2520the%2520challenge%2520of%250Apractical%2520conditions%2520in%2520PRIM%252C%2520which%2520processes%2520visual%2520text%2520and%2520background%250Ainformation%2520in%2520the%2520image%2520separately%252C%2520ensuring%2520the%2520capability%2520of%2520multilingual%250Atranslation%2520while%2520improving%2520the%2520visual%2520quality.%2520Experimental%2520results%2520indicate%250Athe%2520VisTrans%2520achieves%2520a%2520better%2520translation%2520quality%2520and%2520visual%2520effect%2520compared%250Ato%2520other%2520models.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/BITHLP/PRIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIM%3A%20Towards%20Practical%20In-Image%20Multilingual%20Machine%20Translation&entry.906535625=Yanzhi%20Tian%20and%20Zeming%20Liu%20and%20Zhengyang%20Liu%20and%20Chong%20Feng%20and%20Xin%20Li%20and%20Heyan%20Huang%20and%20Yuhang%20Guo&entry.1292438233=%20%20In-Image%20Machine%20Translation%20%28IIMT%29%20aims%20to%20translate%20images%20containing%20texts%0Afrom%20one%20language%20to%20another.%20Current%20research%20of%20end-to-end%20IIMT%20mainly%0Aconducts%20on%20synthetic%20data%2C%20with%20simple%20background%2C%20single%20font%2C%20fixed%20text%0Aposition%2C%20and%20bilingual%20translation%2C%20which%20can%20not%20fully%20reflect%20real%20world%2C%0Acausing%20a%20significant%20gap%20between%20the%20research%20and%20practical%20conditions.%20To%0Afacilitate%20research%20of%20IIMT%20in%20real-world%20scenarios%2C%20we%20explore%20Practical%0AIn-Image%20Multilingual%20Machine%20Translation%20%28IIMMT%29.%20In%20order%20to%20convince%20the%0Alack%20of%20publicly%20available%20data%2C%20we%20annotate%20the%20PRIM%20dataset%2C%20which%20contains%0Areal-world%20captured%20one-line%20text%20images%20with%20complex%20background%2C%20various%0Afonts%2C%20diverse%20text%20positions%2C%20and%20supports%20multilingual%20translation%0Adirections.%20We%20propose%20an%20end-to-end%20model%20VisTrans%20to%20handle%20the%20challenge%20of%0Apractical%20conditions%20in%20PRIM%2C%20which%20processes%20visual%20text%20and%20background%0Ainformation%20in%20the%20image%20separately%2C%20ensuring%20the%20capability%20of%20multilingual%0Atranslation%20while%20improving%20the%20visual%20quality.%20Experimental%20results%20indicate%0Athe%20VisTrans%20achieves%20a%20better%20translation%20quality%20and%20visual%20effect%20compared%0Ato%20other%20models.%20The%20code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/BITHLP/PRIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05146v1&entry.124074799=Read"},
{"title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens", "author": "Dmitry Akulov and Mohamed Sana and Antonio De Domenico and Tareq Si Salem and Nicola Piovesan and Fadhel Ayed", "abstract": "  Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.\n", "link": "http://arxiv.org/abs/2509.05165v1", "date": "2025-09-05", "relevancy": 2.6029, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens&body=Title%3A%20KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens%0AAuthor%3A%20Dmitry%20Akulov%20and%20Mohamed%20Sana%20and%20Antonio%20De%20Domenico%20and%20Tareq%20Si%20Salem%20and%20Nicola%20Piovesan%20and%20Fadhel%20Ayed%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20%28KV%29%20caches%20for%20efficient%0Aautoregressive%20decoding%3B%20however%2C%20cache%20size%20grows%20linearly%20with%20context%20length%0Aand%20model%20depth%2C%20becoming%20a%20major%20bottleneck%20in%20long-context%20inference.%20Prior%0AKV%20cache%20compression%20methods%20either%20enforce%20rigid%20heuristics%2C%20disrupt%20tensor%0Alayouts%20with%20per-attention-head%20variability%2C%20or%20require%20specialized%20compute%0Akernels.%0A%20%20We%20propose%20a%20simple%2C%20yet%20effective%2C%20KV%20cache%20compression%20framework%20based%20on%0Aattention-guided%2C%20layer-adaptive%20composite%20tokens.%20Our%20method%20aggregates%0Aattention%20scores%20to%20estimate%20token%20importance%2C%20selects%20head-specific%20tokens%0Aindependently%2C%20and%20aligns%20them%20into%20composite%20tokens%20that%20respect%20the%20uniform%0Acache%20structure%20required%20by%20existing%20inference%20engines.%20A%20global%20allocation%0Amechanism%20further%20adapts%20retention%20budgets%20across%20layers%2C%20assigning%20more%0Acapacity%20to%20layers%20with%20informative%20tokens.%20This%20approach%20achieves%20significant%0Amemory%20reduction%20while%20preserving%20accuracy%2C%20consistently%20outperforming%20prior%0Astructured%20and%20semi-structured%20methods.%20Crucially%2C%20our%20approach%20remains%20fully%0Acompatible%20with%20standard%20inference%20pipelines%2C%20offering%20a%20practical%20and%20scalable%0Asolution%20for%20efficient%20long-context%20LLM%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKVCompose%253A%2520Efficient%2520Structured%2520KV%2520Cache%2520Compression%2520with%2520Composite%250A%2520%2520Tokens%26entry.906535625%3DDmitry%2520Akulov%2520and%2520Mohamed%2520Sana%2520and%2520Antonio%2520De%2520Domenico%2520and%2520Tareq%2520Si%2520Salem%2520and%2520Nicola%2520Piovesan%2520and%2520Fadhel%2520Ayed%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520rely%2520on%2520key-value%2520%2528KV%2529%2520caches%2520for%2520efficient%250Aautoregressive%2520decoding%253B%2520however%252C%2520cache%2520size%2520grows%2520linearly%2520with%2520context%2520length%250Aand%2520model%2520depth%252C%2520becoming%2520a%2520major%2520bottleneck%2520in%2520long-context%2520inference.%2520Prior%250AKV%2520cache%2520compression%2520methods%2520either%2520enforce%2520rigid%2520heuristics%252C%2520disrupt%2520tensor%250Alayouts%2520with%2520per-attention-head%2520variability%252C%2520or%2520require%2520specialized%2520compute%250Akernels.%250A%2520%2520We%2520propose%2520a%2520simple%252C%2520yet%2520effective%252C%2520KV%2520cache%2520compression%2520framework%2520based%2520on%250Aattention-guided%252C%2520layer-adaptive%2520composite%2520tokens.%2520Our%2520method%2520aggregates%250Aattention%2520scores%2520to%2520estimate%2520token%2520importance%252C%2520selects%2520head-specific%2520tokens%250Aindependently%252C%2520and%2520aligns%2520them%2520into%2520composite%2520tokens%2520that%2520respect%2520the%2520uniform%250Acache%2520structure%2520required%2520by%2520existing%2520inference%2520engines.%2520A%2520global%2520allocation%250Amechanism%2520further%2520adapts%2520retention%2520budgets%2520across%2520layers%252C%2520assigning%2520more%250Acapacity%2520to%2520layers%2520with%2520informative%2520tokens.%2520This%2520approach%2520achieves%2520significant%250Amemory%2520reduction%2520while%2520preserving%2520accuracy%252C%2520consistently%2520outperforming%2520prior%250Astructured%2520and%2520semi-structured%2520methods.%2520Crucially%252C%2520our%2520approach%2520remains%2520fully%250Acompatible%2520with%2520standard%2520inference%2520pipelines%252C%2520offering%2520a%2520practical%2520and%2520scalable%250Asolution%2520for%2520efficient%2520long-context%2520LLM%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens&entry.906535625=Dmitry%20Akulov%20and%20Mohamed%20Sana%20and%20Antonio%20De%20Domenico%20and%20Tareq%20Si%20Salem%20and%20Nicola%20Piovesan%20and%20Fadhel%20Ayed&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20%28KV%29%20caches%20for%20efficient%0Aautoregressive%20decoding%3B%20however%2C%20cache%20size%20grows%20linearly%20with%20context%20length%0Aand%20model%20depth%2C%20becoming%20a%20major%20bottleneck%20in%20long-context%20inference.%20Prior%0AKV%20cache%20compression%20methods%20either%20enforce%20rigid%20heuristics%2C%20disrupt%20tensor%0Alayouts%20with%20per-attention-head%20variability%2C%20or%20require%20specialized%20compute%0Akernels.%0A%20%20We%20propose%20a%20simple%2C%20yet%20effective%2C%20KV%20cache%20compression%20framework%20based%20on%0Aattention-guided%2C%20layer-adaptive%20composite%20tokens.%20Our%20method%20aggregates%0Aattention%20scores%20to%20estimate%20token%20importance%2C%20selects%20head-specific%20tokens%0Aindependently%2C%20and%20aligns%20them%20into%20composite%20tokens%20that%20respect%20the%20uniform%0Acache%20structure%20required%20by%20existing%20inference%20engines.%20A%20global%20allocation%0Amechanism%20further%20adapts%20retention%20budgets%20across%20layers%2C%20assigning%20more%0Acapacity%20to%20layers%20with%20informative%20tokens.%20This%20approach%20achieves%20significant%0Amemory%20reduction%20while%20preserving%20accuracy%2C%20consistently%20outperforming%20prior%0Astructured%20and%20semi-structured%20methods.%20Crucially%2C%20our%20approach%20remains%20fully%0Acompatible%20with%20standard%20inference%20pipelines%2C%20offering%20a%20practical%20and%20scalable%0Asolution%20for%20efficient%20long-context%20LLM%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05165v1&entry.124074799=Read"},
{"title": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse\n  Mixture-of-Experts Layers", "author": "Svetlana Pavlitska and Haixi Fan and Konstantin Ditschuneit and J. Marius Z\u00f6llner", "abstract": "  Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.\n", "link": "http://arxiv.org/abs/2509.05086v1", "date": "2025-09-05", "relevancy": 2.595, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5341}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Experts%3A%20the%20Effect%20of%20Adversarial%20Training%20on%20CNNs%20with%20Sparse%0A%20%20Mixture-of-Experts%20Layers&body=Title%3A%20Robust%20Experts%3A%20the%20Effect%20of%20Adversarial%20Training%20on%20CNNs%20with%20Sparse%0A%20%20Mixture-of-Experts%20Layers%0AAuthor%3A%20Svetlana%20Pavlitska%20and%20Haixi%20Fan%20and%20Konstantin%20Ditschuneit%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Robustifying%20convolutional%20neural%20networks%20%28CNNs%29%20against%20adversarial%20attacks%0Aremains%20challenging%20and%20often%20requires%20resource-intensive%20countermeasures.%20We%0Aexplore%20the%20use%20of%20sparse%20mixture-of-experts%20%28MoE%29%20layers%20to%20improve%20robustness%0Aby%20replacing%20selected%20residual%20blocks%20or%20convolutional%20layers%2C%20thereby%0Aincreasing%20model%20capacity%20without%20additional%20inference%20cost.%20On%20ResNet%0Aarchitectures%20trained%20on%20CIFAR-100%2C%20we%20find%20that%20inserting%20a%20single%20MoE%20layer%0Ain%20the%20deeper%20stages%20leads%20to%20consistent%20improvements%20in%20robustness%20under%20PGD%0Aand%20AutoPGD%20attacks%20when%20combined%20with%20adversarial%20training.%20Furthermore%2C%20we%0Adiscover%20that%20when%20switch%20loss%20is%20used%20for%20balancing%2C%20it%20causes%20routing%20to%0Acollapse%20onto%20a%20small%20set%20of%20overused%20experts%2C%20thereby%20concentrating%0Aadversarial%20training%20on%20these%20paths%20and%20inadvertently%20making%20them%20more%20robust.%0AAs%20a%20result%2C%20some%20individual%20experts%20outperform%20the%20gated%20MoE%20model%20in%0Arobustness%2C%20suggesting%20that%20robust%20subpaths%20emerge%20through%20specialization.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/KASTEL-MobilityLab/robust-sparse-moes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Experts%253A%2520the%2520Effect%2520of%2520Adversarial%2520Training%2520on%2520CNNs%2520with%2520Sparse%250A%2520%2520Mixture-of-Experts%2520Layers%26entry.906535625%3DSvetlana%2520Pavlitska%2520and%2520Haixi%2520Fan%2520and%2520Konstantin%2520Ditschuneit%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Robustifying%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520against%2520adversarial%2520attacks%250Aremains%2520challenging%2520and%2520often%2520requires%2520resource-intensive%2520countermeasures.%2520We%250Aexplore%2520the%2520use%2520of%2520sparse%2520mixture-of-experts%2520%2528MoE%2529%2520layers%2520to%2520improve%2520robustness%250Aby%2520replacing%2520selected%2520residual%2520blocks%2520or%2520convolutional%2520layers%252C%2520thereby%250Aincreasing%2520model%2520capacity%2520without%2520additional%2520inference%2520cost.%2520On%2520ResNet%250Aarchitectures%2520trained%2520on%2520CIFAR-100%252C%2520we%2520find%2520that%2520inserting%2520a%2520single%2520MoE%2520layer%250Ain%2520the%2520deeper%2520stages%2520leads%2520to%2520consistent%2520improvements%2520in%2520robustness%2520under%2520PGD%250Aand%2520AutoPGD%2520attacks%2520when%2520combined%2520with%2520adversarial%2520training.%2520Furthermore%252C%2520we%250Adiscover%2520that%2520when%2520switch%2520loss%2520is%2520used%2520for%2520balancing%252C%2520it%2520causes%2520routing%2520to%250Acollapse%2520onto%2520a%2520small%2520set%2520of%2520overused%2520experts%252C%2520thereby%2520concentrating%250Aadversarial%2520training%2520on%2520these%2520paths%2520and%2520inadvertently%2520making%2520them%2520more%2520robust.%250AAs%2520a%2520result%252C%2520some%2520individual%2520experts%2520outperform%2520the%2520gated%2520MoE%2520model%2520in%250Arobustness%252C%2520suggesting%2520that%2520robust%2520subpaths%2520emerge%2520through%2520specialization.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/KASTEL-MobilityLab/robust-sparse-moes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Experts%3A%20the%20Effect%20of%20Adversarial%20Training%20on%20CNNs%20with%20Sparse%0A%20%20Mixture-of-Experts%20Layers&entry.906535625=Svetlana%20Pavlitska%20and%20Haixi%20Fan%20and%20Konstantin%20Ditschuneit%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Robustifying%20convolutional%20neural%20networks%20%28CNNs%29%20against%20adversarial%20attacks%0Aremains%20challenging%20and%20often%20requires%20resource-intensive%20countermeasures.%20We%0Aexplore%20the%20use%20of%20sparse%20mixture-of-experts%20%28MoE%29%20layers%20to%20improve%20robustness%0Aby%20replacing%20selected%20residual%20blocks%20or%20convolutional%20layers%2C%20thereby%0Aincreasing%20model%20capacity%20without%20additional%20inference%20cost.%20On%20ResNet%0Aarchitectures%20trained%20on%20CIFAR-100%2C%20we%20find%20that%20inserting%20a%20single%20MoE%20layer%0Ain%20the%20deeper%20stages%20leads%20to%20consistent%20improvements%20in%20robustness%20under%20PGD%0Aand%20AutoPGD%20attacks%20when%20combined%20with%20adversarial%20training.%20Furthermore%2C%20we%0Adiscover%20that%20when%20switch%20loss%20is%20used%20for%20balancing%2C%20it%20causes%20routing%20to%0Acollapse%20onto%20a%20small%20set%20of%20overused%20experts%2C%20thereby%20concentrating%0Aadversarial%20training%20on%20these%20paths%20and%20inadvertently%20making%20them%20more%20robust.%0AAs%20a%20result%2C%20some%20individual%20experts%20outperform%20the%20gated%20MoE%20model%20in%0Arobustness%2C%20suggesting%20that%20robust%20subpaths%20emerge%20through%20specialization.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/KASTEL-MobilityLab/robust-sparse-moes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05086v1&entry.124074799=Read"},
{"title": "A Scalable Attention-Based Approach for Image-to-3D Texture Mapping", "author": "Arianna Rampini and Kanika Madan and Bruno Roy and AmirHossein Zamani and Derek Cheung", "abstract": "  High-quality textures are critical for realistic 3D content creation, yet\nexisting generative methods are slow, rely on UV maps, and often fail to remain\nfaithful to a reference image. To address these challenges, we propose a\ntransformer-based framework that predicts a 3D texture field directly from a\nsingle image and a mesh, eliminating the need for UV mapping and differentiable\nrendering, and enabling faster texture generation. Our method integrates a\ntriplane representation with depth-based backprojection losses, enabling\nefficient training and faster inference. Once trained, it generates\nhigh-fidelity textures in a single forward pass, requiring only 0.2s per shape.\nExtensive qualitative, quantitative, and user preference evaluations\ndemonstrate that our method outperforms state-of-the-art baselines on\nsingle-image texture reconstruction in terms of both fidelity to the input\nimage and perceptual quality, highlighting its practicality for scalable,\nhigh-quality, and controllable 3D content creation.\n", "link": "http://arxiv.org/abs/2509.05131v1", "date": "2025-09-05", "relevancy": 2.5722, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6515}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6371}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Attention-Based%20Approach%20for%20Image-to-3D%20Texture%20Mapping&body=Title%3A%20A%20Scalable%20Attention-Based%20Approach%20for%20Image-to-3D%20Texture%20Mapping%0AAuthor%3A%20Arianna%20Rampini%20and%20Kanika%20Madan%20and%20Bruno%20Roy%20and%20AmirHossein%20Zamani%20and%20Derek%20Cheung%0AAbstract%3A%20%20%20High-quality%20textures%20are%20critical%20for%20realistic%203D%20content%20creation%2C%20yet%0Aexisting%20generative%20methods%20are%20slow%2C%20rely%20on%20UV%20maps%2C%20and%20often%20fail%20to%20remain%0Afaithful%20to%20a%20reference%20image.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atransformer-based%20framework%20that%20predicts%20a%203D%20texture%20field%20directly%20from%20a%0Asingle%20image%20and%20a%20mesh%2C%20eliminating%20the%20need%20for%20UV%20mapping%20and%20differentiable%0Arendering%2C%20and%20enabling%20faster%20texture%20generation.%20Our%20method%20integrates%20a%0Atriplane%20representation%20with%20depth-based%20backprojection%20losses%2C%20enabling%0Aefficient%20training%20and%20faster%20inference.%20Once%20trained%2C%20it%20generates%0Ahigh-fidelity%20textures%20in%20a%20single%20forward%20pass%2C%20requiring%20only%200.2s%20per%20shape.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20user%20preference%20evaluations%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%0Asingle-image%20texture%20reconstruction%20in%20terms%20of%20both%20fidelity%20to%20the%20input%0Aimage%20and%20perceptual%20quality%2C%20highlighting%20its%20practicality%20for%20scalable%2C%0Ahigh-quality%2C%20and%20controllable%203D%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Attention-Based%2520Approach%2520for%2520Image-to-3D%2520Texture%2520Mapping%26entry.906535625%3DArianna%2520Rampini%2520and%2520Kanika%2520Madan%2520and%2520Bruno%2520Roy%2520and%2520AmirHossein%2520Zamani%2520and%2520Derek%2520Cheung%26entry.1292438233%3D%2520%2520High-quality%2520textures%2520are%2520critical%2520for%2520realistic%25203D%2520content%2520creation%252C%2520yet%250Aexisting%2520generative%2520methods%2520are%2520slow%252C%2520rely%2520on%2520UV%2520maps%252C%2520and%2520often%2520fail%2520to%2520remain%250Afaithful%2520to%2520a%2520reference%2520image.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Atransformer-based%2520framework%2520that%2520predicts%2520a%25203D%2520texture%2520field%2520directly%2520from%2520a%250Asingle%2520image%2520and%2520a%2520mesh%252C%2520eliminating%2520the%2520need%2520for%2520UV%2520mapping%2520and%2520differentiable%250Arendering%252C%2520and%2520enabling%2520faster%2520texture%2520generation.%2520Our%2520method%2520integrates%2520a%250Atriplane%2520representation%2520with%2520depth-based%2520backprojection%2520losses%252C%2520enabling%250Aefficient%2520training%2520and%2520faster%2520inference.%2520Once%2520trained%252C%2520it%2520generates%250Ahigh-fidelity%2520textures%2520in%2520a%2520single%2520forward%2520pass%252C%2520requiring%2520only%25200.2s%2520per%2520shape.%250AExtensive%2520qualitative%252C%2520quantitative%252C%2520and%2520user%2520preference%2520evaluations%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520on%250Asingle-image%2520texture%2520reconstruction%2520in%2520terms%2520of%2520both%2520fidelity%2520to%2520the%2520input%250Aimage%2520and%2520perceptual%2520quality%252C%2520highlighting%2520its%2520practicality%2520for%2520scalable%252C%250Ahigh-quality%252C%2520and%2520controllable%25203D%2520content%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Attention-Based%20Approach%20for%20Image-to-3D%20Texture%20Mapping&entry.906535625=Arianna%20Rampini%20and%20Kanika%20Madan%20and%20Bruno%20Roy%20and%20AmirHossein%20Zamani%20and%20Derek%20Cheung&entry.1292438233=%20%20High-quality%20textures%20are%20critical%20for%20realistic%203D%20content%20creation%2C%20yet%0Aexisting%20generative%20methods%20are%20slow%2C%20rely%20on%20UV%20maps%2C%20and%20often%20fail%20to%20remain%0Afaithful%20to%20a%20reference%20image.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atransformer-based%20framework%20that%20predicts%20a%203D%20texture%20field%20directly%20from%20a%0Asingle%20image%20and%20a%20mesh%2C%20eliminating%20the%20need%20for%20UV%20mapping%20and%20differentiable%0Arendering%2C%20and%20enabling%20faster%20texture%20generation.%20Our%20method%20integrates%20a%0Atriplane%20representation%20with%20depth-based%20backprojection%20losses%2C%20enabling%0Aefficient%20training%20and%20faster%20inference.%20Once%20trained%2C%20it%20generates%0Ahigh-fidelity%20textures%20in%20a%20single%20forward%20pass%2C%20requiring%20only%200.2s%20per%20shape.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20user%20preference%20evaluations%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%0Asingle-image%20texture%20reconstruction%20in%20terms%20of%20both%20fidelity%20to%20the%20input%0Aimage%20and%20perceptual%20quality%2C%20highlighting%20its%20practicality%20for%20scalable%2C%0Ahigh-quality%2C%20and%20controllable%203D%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05131v1&entry.124074799=Read"},
{"title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation", "author": "Zhongyu Yang and Jun Chen and Dannong Xu and Junjie Fei and Xiaoqian Shen and Liangbing Zhao and Chun-Mei Feng and Mohamed Elhoseiny", "abstract": "  Knowledge discovery and collection are intelligence-intensive tasks that\ntraditionally require significant human effort to ensure high-quality outputs.\nRecent research has explored multi-agent frameworks for automating\nWikipedia-style article generation by retrieving and synthesizing information\nfrom the internet. However, these methods primarily focus on text-only\ngeneration, overlooking the importance of multimodal content in enhancing\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\nsystem for automated multimodal Wikipedia-style article generation. Unlike\nprior approaches, WikiAutoGen retrieves and integrates relevant images\nalongside text, enriching both the depth and visual appeal of generated\ncontent. To further improve factual accuracy and comprehensiveness, we propose\na multi-perspective self-reflection mechanism, which critically assesses\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\nWikipedia articles with topics paired with both textual and image-based\nrepresentations, designed to evaluate multimodal knowledge generation on more\nchallenging topics. Experimental results show that WikiAutoGen outperforms\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\ncoherent, and visually enriched Wikipedia-style articles. Our code and examples\nare available at https://wikiautogen.github.io/\n", "link": "http://arxiv.org/abs/2503.19065v3", "date": "2025-09-05", "relevancy": 2.561, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5551}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4909}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WikiAutoGen%3A%20Towards%20Multi-Modal%20Wikipedia-Style%20Article%20Generation&body=Title%3A%20WikiAutoGen%3A%20Towards%20Multi-Modal%20Wikipedia-Style%20Article%20Generation%0AAuthor%3A%20Zhongyu%20Yang%20and%20Jun%20Chen%20and%20Dannong%20Xu%20and%20Junjie%20Fei%20and%20Xiaoqian%20Shen%20and%20Liangbing%20Zhao%20and%20Chun-Mei%20Feng%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Knowledge%20discovery%20and%20collection%20are%20intelligence-intensive%20tasks%20that%0Atraditionally%20require%20significant%20human%20effort%20to%20ensure%20high-quality%20outputs.%0ARecent%20research%20has%20explored%20multi-agent%20frameworks%20for%20automating%0AWikipedia-style%20article%20generation%20by%20retrieving%20and%20synthesizing%20information%0Afrom%20the%20internet.%20However%2C%20these%20methods%20primarily%20focus%20on%20text-only%0Ageneration%2C%20overlooking%20the%20importance%20of%20multimodal%20content%20in%20enhancing%0Ainformativeness%20and%20engagement.%20In%20this%20work%2C%20we%20introduce%20WikiAutoGen%2C%20a%20novel%0Asystem%20for%20automated%20multimodal%20Wikipedia-style%20article%20generation.%20Unlike%0Aprior%20approaches%2C%20WikiAutoGen%20retrieves%20and%20integrates%20relevant%20images%0Aalongside%20text%2C%20enriching%20both%20the%20depth%20and%20visual%20appeal%20of%20generated%0Acontent.%20To%20further%20improve%20factual%20accuracy%20and%20comprehensiveness%2C%20we%20propose%0Aa%20multi-perspective%20self-reflection%20mechanism%2C%20which%20critically%20assesses%0Aretrieved%20content%20from%20diverse%20viewpoints%20to%20enhance%20reliability%2C%20breadth%2C%20and%0Acoherence%2C%20etc.%20Additionally%2C%20we%20introduce%20WikiSeek%2C%20a%20benchmark%20comprising%0AWikipedia%20articles%20with%20topics%20paired%20with%20both%20textual%20and%20image-based%0Arepresentations%2C%20designed%20to%20evaluate%20multimodal%20knowledge%20generation%20on%20more%0Achallenging%20topics.%20Experimental%20results%20show%20that%20WikiAutoGen%20outperforms%0Aprevious%20methods%20by%208%25-29%25%20on%20our%20WikiSeek%20benchmark%2C%20producing%20more%20accurate%2C%0Acoherent%2C%20and%20visually%20enriched%20Wikipedia-style%20articles.%20Our%20code%20and%20examples%0Aare%20available%20at%20https%3A//wikiautogen.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19065v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWikiAutoGen%253A%2520Towards%2520Multi-Modal%2520Wikipedia-Style%2520Article%2520Generation%26entry.906535625%3DZhongyu%2520Yang%2520and%2520Jun%2520Chen%2520and%2520Dannong%2520Xu%2520and%2520Junjie%2520Fei%2520and%2520Xiaoqian%2520Shen%2520and%2520Liangbing%2520Zhao%2520and%2520Chun-Mei%2520Feng%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Knowledge%2520discovery%2520and%2520collection%2520are%2520intelligence-intensive%2520tasks%2520that%250Atraditionally%2520require%2520significant%2520human%2520effort%2520to%2520ensure%2520high-quality%2520outputs.%250ARecent%2520research%2520has%2520explored%2520multi-agent%2520frameworks%2520for%2520automating%250AWikipedia-style%2520article%2520generation%2520by%2520retrieving%2520and%2520synthesizing%2520information%250Afrom%2520the%2520internet.%2520However%252C%2520these%2520methods%2520primarily%2520focus%2520on%2520text-only%250Ageneration%252C%2520overlooking%2520the%2520importance%2520of%2520multimodal%2520content%2520in%2520enhancing%250Ainformativeness%2520and%2520engagement.%2520In%2520this%2520work%252C%2520we%2520introduce%2520WikiAutoGen%252C%2520a%2520novel%250Asystem%2520for%2520automated%2520multimodal%2520Wikipedia-style%2520article%2520generation.%2520Unlike%250Aprior%2520approaches%252C%2520WikiAutoGen%2520retrieves%2520and%2520integrates%2520relevant%2520images%250Aalongside%2520text%252C%2520enriching%2520both%2520the%2520depth%2520and%2520visual%2520appeal%2520of%2520generated%250Acontent.%2520To%2520further%2520improve%2520factual%2520accuracy%2520and%2520comprehensiveness%252C%2520we%2520propose%250Aa%2520multi-perspective%2520self-reflection%2520mechanism%252C%2520which%2520critically%2520assesses%250Aretrieved%2520content%2520from%2520diverse%2520viewpoints%2520to%2520enhance%2520reliability%252C%2520breadth%252C%2520and%250Acoherence%252C%2520etc.%2520Additionally%252C%2520we%2520introduce%2520WikiSeek%252C%2520a%2520benchmark%2520comprising%250AWikipedia%2520articles%2520with%2520topics%2520paired%2520with%2520both%2520textual%2520and%2520image-based%250Arepresentations%252C%2520designed%2520to%2520evaluate%2520multimodal%2520knowledge%2520generation%2520on%2520more%250Achallenging%2520topics.%2520Experimental%2520results%2520show%2520that%2520WikiAutoGen%2520outperforms%250Aprevious%2520methods%2520by%25208%2525-29%2525%2520on%2520our%2520WikiSeek%2520benchmark%252C%2520producing%2520more%2520accurate%252C%250Acoherent%252C%2520and%2520visually%2520enriched%2520Wikipedia-style%2520articles.%2520Our%2520code%2520and%2520examples%250Aare%2520available%2520at%2520https%253A//wikiautogen.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19065v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WikiAutoGen%3A%20Towards%20Multi-Modal%20Wikipedia-Style%20Article%20Generation&entry.906535625=Zhongyu%20Yang%20and%20Jun%20Chen%20and%20Dannong%20Xu%20and%20Junjie%20Fei%20and%20Xiaoqian%20Shen%20and%20Liangbing%20Zhao%20and%20Chun-Mei%20Feng%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Knowledge%20discovery%20and%20collection%20are%20intelligence-intensive%20tasks%20that%0Atraditionally%20require%20significant%20human%20effort%20to%20ensure%20high-quality%20outputs.%0ARecent%20research%20has%20explored%20multi-agent%20frameworks%20for%20automating%0AWikipedia-style%20article%20generation%20by%20retrieving%20and%20synthesizing%20information%0Afrom%20the%20internet.%20However%2C%20these%20methods%20primarily%20focus%20on%20text-only%0Ageneration%2C%20overlooking%20the%20importance%20of%20multimodal%20content%20in%20enhancing%0Ainformativeness%20and%20engagement.%20In%20this%20work%2C%20we%20introduce%20WikiAutoGen%2C%20a%20novel%0Asystem%20for%20automated%20multimodal%20Wikipedia-style%20article%20generation.%20Unlike%0Aprior%20approaches%2C%20WikiAutoGen%20retrieves%20and%20integrates%20relevant%20images%0Aalongside%20text%2C%20enriching%20both%20the%20depth%20and%20visual%20appeal%20of%20generated%0Acontent.%20To%20further%20improve%20factual%20accuracy%20and%20comprehensiveness%2C%20we%20propose%0Aa%20multi-perspective%20self-reflection%20mechanism%2C%20which%20critically%20assesses%0Aretrieved%20content%20from%20diverse%20viewpoints%20to%20enhance%20reliability%2C%20breadth%2C%20and%0Acoherence%2C%20etc.%20Additionally%2C%20we%20introduce%20WikiSeek%2C%20a%20benchmark%20comprising%0AWikipedia%20articles%20with%20topics%20paired%20with%20both%20textual%20and%20image-based%0Arepresentations%2C%20designed%20to%20evaluate%20multimodal%20knowledge%20generation%20on%20more%0Achallenging%20topics.%20Experimental%20results%20show%20that%20WikiAutoGen%20outperforms%0Aprevious%20methods%20by%208%25-29%25%20on%20our%20WikiSeek%20benchmark%2C%20producing%20more%20accurate%2C%0Acoherent%2C%20and%20visually%20enriched%20Wikipedia-style%20articles.%20Our%20code%20and%20examples%0Aare%20available%20at%20https%3A//wikiautogen.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19065v3&entry.124074799=Read"},
{"title": "GenAI-based test case generation and execution in SDV platform", "author": "Denesa Zyberaj and Lukasz Mazur and Nenad Petrovic and Pankhuri Verma and Pascal Hirmer and Dirk Slama and Xiangwei Cheng and Alois Knoll", "abstract": "  This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.\n", "link": "http://arxiv.org/abs/2509.05112v1", "date": "2025-09-05", "relevancy": 2.5405, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5287}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5024}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI-based%20test%20case%20generation%20and%20execution%20in%20SDV%20platform&body=Title%3A%20GenAI-based%20test%20case%20generation%20and%20execution%20in%20SDV%20platform%0AAuthor%3A%20Denesa%20Zyberaj%20and%20Lukasz%20Mazur%20and%20Nenad%20Petrovic%20and%20Pankhuri%20Verma%20and%20Pascal%20Hirmer%20and%20Dirk%20Slama%20and%20Xiangwei%20Cheng%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20GenAI-driven%20approach%20for%20automated%20test%20case%0Ageneration%2C%20leveraging%20Large%20Language%20Models%20and%20Vision-Language%20Models%20to%0Atranslate%20natural%20language%20requirements%20and%20system%20diagrams%20into%20structured%0AGherkin%20test%20cases.%20The%20methodology%20integrates%20Vehicle%20Signal%20Specification%0Amodeling%20to%20standardize%20vehicle%20signal%20definitions%2C%20improve%20compatibility%0Aacross%20automotive%20subsystems%2C%20and%20streamline%20integration%20with%20third-party%0Atesting%20tools.%20Generated%20test%20cases%20are%20executed%20within%20the%20digital.auto%0Aplayground%2C%20an%20open%20and%20vendor-neutral%20environment%20designed%20to%20facilitate%20rapid%0Avalidation%20of%20software-defined%20vehicle%20functionalities.%20We%20evaluate%20our%0Aapproach%20using%20the%20Child%20Presence%20Detection%20System%20use%20case%2C%20demonstrating%0Asubstantial%20reductions%20in%20manual%20test%20specification%20effort%20and%20rapid%20execution%0Aof%20generated%20tests.%20Despite%20significant%20automation%2C%20the%20generation%20of%20test%0Acases%20and%20test%20scripts%20still%20requires%20manual%20intervention%20due%20to%20current%0Alimitations%20in%20the%20GenAI%20pipeline%20and%20constraints%20of%20the%20digital.auto%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI-based%2520test%2520case%2520generation%2520and%2520execution%2520in%2520SDV%2520platform%26entry.906535625%3DDenesa%2520Zyberaj%2520and%2520Lukasz%2520Mazur%2520and%2520Nenad%2520Petrovic%2520and%2520Pankhuri%2520Verma%2520and%2520Pascal%2520Hirmer%2520and%2520Dirk%2520Slama%2520and%2520Xiangwei%2520Cheng%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520GenAI-driven%2520approach%2520for%2520automated%2520test%2520case%250Ageneration%252C%2520leveraging%2520Large%2520Language%2520Models%2520and%2520Vision-Language%2520Models%2520to%250Atranslate%2520natural%2520language%2520requirements%2520and%2520system%2520diagrams%2520into%2520structured%250AGherkin%2520test%2520cases.%2520The%2520methodology%2520integrates%2520Vehicle%2520Signal%2520Specification%250Amodeling%2520to%2520standardize%2520vehicle%2520signal%2520definitions%252C%2520improve%2520compatibility%250Aacross%2520automotive%2520subsystems%252C%2520and%2520streamline%2520integration%2520with%2520third-party%250Atesting%2520tools.%2520Generated%2520test%2520cases%2520are%2520executed%2520within%2520the%2520digital.auto%250Aplayground%252C%2520an%2520open%2520and%2520vendor-neutral%2520environment%2520designed%2520to%2520facilitate%2520rapid%250Avalidation%2520of%2520software-defined%2520vehicle%2520functionalities.%2520We%2520evaluate%2520our%250Aapproach%2520using%2520the%2520Child%2520Presence%2520Detection%2520System%2520use%2520case%252C%2520demonstrating%250Asubstantial%2520reductions%2520in%2520manual%2520test%2520specification%2520effort%2520and%2520rapid%2520execution%250Aof%2520generated%2520tests.%2520Despite%2520significant%2520automation%252C%2520the%2520generation%2520of%2520test%250Acases%2520and%2520test%2520scripts%2520still%2520requires%2520manual%2520intervention%2520due%2520to%2520current%250Alimitations%2520in%2520the%2520GenAI%2520pipeline%2520and%2520constraints%2520of%2520the%2520digital.auto%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI-based%20test%20case%20generation%20and%20execution%20in%20SDV%20platform&entry.906535625=Denesa%20Zyberaj%20and%20Lukasz%20Mazur%20and%20Nenad%20Petrovic%20and%20Pankhuri%20Verma%20and%20Pascal%20Hirmer%20and%20Dirk%20Slama%20and%20Xiangwei%20Cheng%20and%20Alois%20Knoll&entry.1292438233=%20%20This%20paper%20introduces%20a%20GenAI-driven%20approach%20for%20automated%20test%20case%0Ageneration%2C%20leveraging%20Large%20Language%20Models%20and%20Vision-Language%20Models%20to%0Atranslate%20natural%20language%20requirements%20and%20system%20diagrams%20into%20structured%0AGherkin%20test%20cases.%20The%20methodology%20integrates%20Vehicle%20Signal%20Specification%0Amodeling%20to%20standardize%20vehicle%20signal%20definitions%2C%20improve%20compatibility%0Aacross%20automotive%20subsystems%2C%20and%20streamline%20integration%20with%20third-party%0Atesting%20tools.%20Generated%20test%20cases%20are%20executed%20within%20the%20digital.auto%0Aplayground%2C%20an%20open%20and%20vendor-neutral%20environment%20designed%20to%20facilitate%20rapid%0Avalidation%20of%20software-defined%20vehicle%20functionalities.%20We%20evaluate%20our%0Aapproach%20using%20the%20Child%20Presence%20Detection%20System%20use%20case%2C%20demonstrating%0Asubstantial%20reductions%20in%20manual%20test%20specification%20effort%20and%20rapid%20execution%0Aof%20generated%20tests.%20Despite%20significant%20automation%2C%20the%20generation%20of%20test%0Acases%20and%20test%20scripts%20still%20requires%20manual%20intervention%20due%20to%20current%0Alimitations%20in%20the%20GenAI%20pipeline%20and%20constraints%20of%20the%20digital.auto%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05112v1&entry.124074799=Read"},
{"title": "Do Sparse Autoencoders Generalize? A Case Study of Answerability", "author": "Lovis Heindrich and Philip Torr and Fazl Barez and Veronika Thost", "abstract": "  Sparse autoencoders (SAEs) have emerged as a promising approach in language\nmodel interpretability, offering unsupervised extraction of sparse features.\nFor interpretability methods to succeed, they must identify abstract features\nacross domains, and these features can often manifest differently in each\ncontext. We examine this through \"answerability\" - a model's ability to\nrecognize answerable questions. We extensively evaluate SAE feature\ngeneralization across diverse, partly self-constructed answerability datasets\nfor Gemma 2 SAEs. Our analysis reveals that residual stream probes outperform\nSAE features within domains, but generalization performance differs sharply.\nSAE features show inconsistent out-of-domain transfer, with performance varying\nfrom almost random to outperforming residual stream probes. Overall, this\ndemonstrates the need for robust evaluation methods and quantitative approaches\nto predict feature generalization in SAE-based interpretability.\n", "link": "http://arxiv.org/abs/2502.19964v2", "date": "2025-09-05", "relevancy": 2.5352, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Sparse%20Autoencoders%20Generalize%3F%20A%20Case%20Study%20of%20Answerability&body=Title%3A%20Do%20Sparse%20Autoencoders%20Generalize%3F%20A%20Case%20Study%20of%20Answerability%0AAuthor%3A%20Lovis%20Heindrich%20and%20Philip%20Torr%20and%20Fazl%20Barez%20and%20Veronika%20Thost%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20promising%20approach%20in%20language%0Amodel%20interpretability%2C%20offering%20unsupervised%20extraction%20of%20sparse%20features.%0AFor%20interpretability%20methods%20to%20succeed%2C%20they%20must%20identify%20abstract%20features%0Aacross%20domains%2C%20and%20these%20features%20can%20often%20manifest%20differently%20in%20each%0Acontext.%20We%20examine%20this%20through%20%22answerability%22%20-%20a%20model%27s%20ability%20to%0Arecognize%20answerable%20questions.%20We%20extensively%20evaluate%20SAE%20feature%0Ageneralization%20across%20diverse%2C%20partly%20self-constructed%20answerability%20datasets%0Afor%20Gemma%202%20SAEs.%20Our%20analysis%20reveals%20that%20residual%20stream%20probes%20outperform%0ASAE%20features%20within%20domains%2C%20but%20generalization%20performance%20differs%20sharply.%0ASAE%20features%20show%20inconsistent%20out-of-domain%20transfer%2C%20with%20performance%20varying%0Afrom%20almost%20random%20to%20outperforming%20residual%20stream%20probes.%20Overall%2C%20this%0Ademonstrates%20the%20need%20for%20robust%20evaluation%20methods%20and%20quantitative%20approaches%0Ato%20predict%20feature%20generalization%20in%20SAE-based%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Sparse%2520Autoencoders%2520Generalize%253F%2520A%2520Case%2520Study%2520of%2520Answerability%26entry.906535625%3DLovis%2520Heindrich%2520and%2520Philip%2520Torr%2520and%2520Fazl%2520Barez%2520and%2520Veronika%2520Thost%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520in%2520language%250Amodel%2520interpretability%252C%2520offering%2520unsupervised%2520extraction%2520of%2520sparse%2520features.%250AFor%2520interpretability%2520methods%2520to%2520succeed%252C%2520they%2520must%2520identify%2520abstract%2520features%250Aacross%2520domains%252C%2520and%2520these%2520features%2520can%2520often%2520manifest%2520differently%2520in%2520each%250Acontext.%2520We%2520examine%2520this%2520through%2520%2522answerability%2522%2520-%2520a%2520model%2527s%2520ability%2520to%250Arecognize%2520answerable%2520questions.%2520We%2520extensively%2520evaluate%2520SAE%2520feature%250Ageneralization%2520across%2520diverse%252C%2520partly%2520self-constructed%2520answerability%2520datasets%250Afor%2520Gemma%25202%2520SAEs.%2520Our%2520analysis%2520reveals%2520that%2520residual%2520stream%2520probes%2520outperform%250ASAE%2520features%2520within%2520domains%252C%2520but%2520generalization%2520performance%2520differs%2520sharply.%250ASAE%2520features%2520show%2520inconsistent%2520out-of-domain%2520transfer%252C%2520with%2520performance%2520varying%250Afrom%2520almost%2520random%2520to%2520outperforming%2520residual%2520stream%2520probes.%2520Overall%252C%2520this%250Ademonstrates%2520the%2520need%2520for%2520robust%2520evaluation%2520methods%2520and%2520quantitative%2520approaches%250Ato%2520predict%2520feature%2520generalization%2520in%2520SAE-based%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Sparse%20Autoencoders%20Generalize%3F%20A%20Case%20Study%20of%20Answerability&entry.906535625=Lovis%20Heindrich%20and%20Philip%20Torr%20and%20Fazl%20Barez%20and%20Veronika%20Thost&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20promising%20approach%20in%20language%0Amodel%20interpretability%2C%20offering%20unsupervised%20extraction%20of%20sparse%20features.%0AFor%20interpretability%20methods%20to%20succeed%2C%20they%20must%20identify%20abstract%20features%0Aacross%20domains%2C%20and%20these%20features%20can%20often%20manifest%20differently%20in%20each%0Acontext.%20We%20examine%20this%20through%20%22answerability%22%20-%20a%20model%27s%20ability%20to%0Arecognize%20answerable%20questions.%20We%20extensively%20evaluate%20SAE%20feature%0Ageneralization%20across%20diverse%2C%20partly%20self-constructed%20answerability%20datasets%0Afor%20Gemma%202%20SAEs.%20Our%20analysis%20reveals%20that%20residual%20stream%20probes%20outperform%0ASAE%20features%20within%20domains%2C%20but%20generalization%20performance%20differs%20sharply.%0ASAE%20features%20show%20inconsistent%20out-of-domain%20transfer%2C%20with%20performance%20varying%0Afrom%20almost%20random%20to%20outperforming%20residual%20stream%20probes.%20Overall%2C%20this%0Ademonstrates%20the%20need%20for%20robust%20evaluation%20methods%20and%20quantitative%20approaches%0Ato%20predict%20feature%20generalization%20in%20SAE-based%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19964v2&entry.124074799=Read"},
{"title": "Learning to accelerate distributed ADMM using graph neural networks", "author": "Henri Doerks and Paul H\u00e4usner and Daniel Hern\u00e1ndez Escobar and Jens Sj\u00f6lund", "abstract": "  Distributed optimization is fundamental in large-scale machine learning and\ncontrol applications. Among existing methods, the Alternating Direction Method\nof Multipliers (ADMM) has gained popularity due to its strong convergence\nguarantees and suitability for decentralized computation. However, ADMM often\nsuffers from slow convergence and sensitivity to hyperparameter choices. In\nthis work, we show that distributed ADMM iterations can be naturally\nrepresented within the message-passing framework of graph neural networks\n(GNNs). Building on this connection, we propose to learn adaptive step sizes\nand communication weights by a graph neural network that predicts the\nhyperparameters based on the iterates. By unrolling ADMM for a fixed number of\niterations, we train the network parameters end-to-end to minimize the final\niterates error for a given problem class, while preserving the algorithm's\nconvergence properties. Numerical experiments demonstrate that our learned\nvariant consistently improves convergence speed and solution quality compared\nto standard ADMM. The code is available at\nhttps://github.com/paulhausner/learning-distributed-admm.\n", "link": "http://arxiv.org/abs/2509.05288v1", "date": "2025-09-05", "relevancy": 2.516, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5357}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4939}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20accelerate%20distributed%20ADMM%20using%20graph%20neural%20networks&body=Title%3A%20Learning%20to%20accelerate%20distributed%20ADMM%20using%20graph%20neural%20networks%0AAuthor%3A%20Henri%20Doerks%20and%20Paul%20H%C3%A4usner%20and%20Daniel%20Hern%C3%A1ndez%20Escobar%20and%20Jens%20Sj%C3%B6lund%0AAbstract%3A%20%20%20Distributed%20optimization%20is%20fundamental%20in%20large-scale%20machine%20learning%20and%0Acontrol%20applications.%20Among%20existing%20methods%2C%20the%20Alternating%20Direction%20Method%0Aof%20Multipliers%20%28ADMM%29%20has%20gained%20popularity%20due%20to%20its%20strong%20convergence%0Aguarantees%20and%20suitability%20for%20decentralized%20computation.%20However%2C%20ADMM%20often%0Asuffers%20from%20slow%20convergence%20and%20sensitivity%20to%20hyperparameter%20choices.%20In%0Athis%20work%2C%20we%20show%20that%20distributed%20ADMM%20iterations%20can%20be%20naturally%0Arepresented%20within%20the%20message-passing%20framework%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Building%20on%20this%20connection%2C%20we%20propose%20to%20learn%20adaptive%20step%20sizes%0Aand%20communication%20weights%20by%20a%20graph%20neural%20network%20that%20predicts%20the%0Ahyperparameters%20based%20on%20the%20iterates.%20By%20unrolling%20ADMM%20for%20a%20fixed%20number%20of%0Aiterations%2C%20we%20train%20the%20network%20parameters%20end-to-end%20to%20minimize%20the%20final%0Aiterates%20error%20for%20a%20given%20problem%20class%2C%20while%20preserving%20the%20algorithm%27s%0Aconvergence%20properties.%20Numerical%20experiments%20demonstrate%20that%20our%20learned%0Avariant%20consistently%20improves%20convergence%20speed%20and%20solution%20quality%20compared%0Ato%20standard%20ADMM.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/paulhausner/learning-distributed-admm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520accelerate%2520distributed%2520ADMM%2520using%2520graph%2520neural%2520networks%26entry.906535625%3DHenri%2520Doerks%2520and%2520Paul%2520H%25C3%25A4usner%2520and%2520Daniel%2520Hern%25C3%25A1ndez%2520Escobar%2520and%2520Jens%2520Sj%25C3%25B6lund%26entry.1292438233%3D%2520%2520Distributed%2520optimization%2520is%2520fundamental%2520in%2520large-scale%2520machine%2520learning%2520and%250Acontrol%2520applications.%2520Among%2520existing%2520methods%252C%2520the%2520Alternating%2520Direction%2520Method%250Aof%2520Multipliers%2520%2528ADMM%2529%2520has%2520gained%2520popularity%2520due%2520to%2520its%2520strong%2520convergence%250Aguarantees%2520and%2520suitability%2520for%2520decentralized%2520computation.%2520However%252C%2520ADMM%2520often%250Asuffers%2520from%2520slow%2520convergence%2520and%2520sensitivity%2520to%2520hyperparameter%2520choices.%2520In%250Athis%2520work%252C%2520we%2520show%2520that%2520distributed%2520ADMM%2520iterations%2520can%2520be%2520naturally%250Arepresented%2520within%2520the%2520message-passing%2520framework%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529.%2520Building%2520on%2520this%2520connection%252C%2520we%2520propose%2520to%2520learn%2520adaptive%2520step%2520sizes%250Aand%2520communication%2520weights%2520by%2520a%2520graph%2520neural%2520network%2520that%2520predicts%2520the%250Ahyperparameters%2520based%2520on%2520the%2520iterates.%2520By%2520unrolling%2520ADMM%2520for%2520a%2520fixed%2520number%2520of%250Aiterations%252C%2520we%2520train%2520the%2520network%2520parameters%2520end-to-end%2520to%2520minimize%2520the%2520final%250Aiterates%2520error%2520for%2520a%2520given%2520problem%2520class%252C%2520while%2520preserving%2520the%2520algorithm%2527s%250Aconvergence%2520properties.%2520Numerical%2520experiments%2520demonstrate%2520that%2520our%2520learned%250Avariant%2520consistently%2520improves%2520convergence%2520speed%2520and%2520solution%2520quality%2520compared%250Ato%2520standard%2520ADMM.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/paulhausner/learning-distributed-admm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20accelerate%20distributed%20ADMM%20using%20graph%20neural%20networks&entry.906535625=Henri%20Doerks%20and%20Paul%20H%C3%A4usner%20and%20Daniel%20Hern%C3%A1ndez%20Escobar%20and%20Jens%20Sj%C3%B6lund&entry.1292438233=%20%20Distributed%20optimization%20is%20fundamental%20in%20large-scale%20machine%20learning%20and%0Acontrol%20applications.%20Among%20existing%20methods%2C%20the%20Alternating%20Direction%20Method%0Aof%20Multipliers%20%28ADMM%29%20has%20gained%20popularity%20due%20to%20its%20strong%20convergence%0Aguarantees%20and%20suitability%20for%20decentralized%20computation.%20However%2C%20ADMM%20often%0Asuffers%20from%20slow%20convergence%20and%20sensitivity%20to%20hyperparameter%20choices.%20In%0Athis%20work%2C%20we%20show%20that%20distributed%20ADMM%20iterations%20can%20be%20naturally%0Arepresented%20within%20the%20message-passing%20framework%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Building%20on%20this%20connection%2C%20we%20propose%20to%20learn%20adaptive%20step%20sizes%0Aand%20communication%20weights%20by%20a%20graph%20neural%20network%20that%20predicts%20the%0Ahyperparameters%20based%20on%20the%20iterates.%20By%20unrolling%20ADMM%20for%20a%20fixed%20number%20of%0Aiterations%2C%20we%20train%20the%20network%20parameters%20end-to-end%20to%20minimize%20the%20final%0Aiterates%20error%20for%20a%20given%20problem%20class%2C%20while%20preserving%20the%20algorithm%27s%0Aconvergence%20properties.%20Numerical%20experiments%20demonstrate%20that%20our%20learned%0Avariant%20consistently%20improves%20convergence%20speed%20and%20solution%20quality%20compared%0Ato%20standard%20ADMM.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/paulhausner/learning-distributed-admm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05288v1&entry.124074799=Read"},
{"title": "EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding", "author": "Bruno Aristimunha and Dung Truong and Pierre Guetschel and Seyed Yahya Shirazi and Isabelle Guyon and Alexandre R. Franco and Michael P. Milham and Aviv Dotan and Scott Makeig and Alexandre Gramfort and Jean-Remi King and Marie-Constance Corsi and Pedro A. Vald\u00e9s-Sosa and Amit Majumdar and Alan Evans and Terrence J Sejnowski and Oren Shriki and Sylvain Chevallier and Arnaud Delorme", "abstract": "  Current electroencephalogram (EEG) decoding models are typically trained on\nsmall numbers of subjects performing a single task. Here, we introduce a\nlarge-scale, code-submission-based competition comprising two challenges.\nFirst, the Transfer Challenge asks participants to build and test a model that\ncan zero-shot decode new tasks and new subjects from their EEG data. Second,\nthe Psychopathology factor prediction Challenge asks participants to infer\nsubject measures of mental health from EEG data. For this, we use an\nunprecedented, multi-terabyte dataset of high-density EEG signals (128\nchannels) recorded from over 3,000 child to young adult subjects engaged in\nmultiple active and passive tasks. We provide several tunable neural network\nbaselines for each of these two challenges, including a simple network and\ndemographic-based regression models. Developing models that generalise across\ntasks and individuals will pave the way for ML network architectures capable of\nadapting to EEG data collected from diverse tasks and individuals. Similarly,\npredicting mental health-relevant personality trait values from EEG might\nidentify objective biomarkers useful for clinical diagnosis and design of\npersonalised treatment for psychological conditions. Ultimately, the advances\nspurred by this challenge could contribute to the development of computational\npsychiatry and useful neurotechnology, and contribute to breakthroughs in both\nfundamental neuroscience and applied clinical research.\n", "link": "http://arxiv.org/abs/2506.19141v2", "date": "2025-09-05", "relevancy": 2.5026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG%20Foundation%20Challenge%3A%20From%20Cross-Task%20to%20Cross-Subject%20EEG%20Decoding&body=Title%3A%20EEG%20Foundation%20Challenge%3A%20From%20Cross-Task%20to%20Cross-Subject%20EEG%20Decoding%0AAuthor%3A%20Bruno%20Aristimunha%20and%20Dung%20Truong%20and%20Pierre%20Guetschel%20and%20Seyed%20Yahya%20Shirazi%20and%20Isabelle%20Guyon%20and%20Alexandre%20R.%20Franco%20and%20Michael%20P.%20Milham%20and%20Aviv%20Dotan%20and%20Scott%20Makeig%20and%20Alexandre%20Gramfort%20and%20Jean-Remi%20King%20and%20Marie-Constance%20Corsi%20and%20Pedro%20A.%20Vald%C3%A9s-Sosa%20and%20Amit%20Majumdar%20and%20Alan%20Evans%20and%20Terrence%20J%20Sejnowski%20and%20Oren%20Shriki%20and%20Sylvain%20Chevallier%20and%20Arnaud%20Delorme%0AAbstract%3A%20%20%20Current%20electroencephalogram%20%28EEG%29%20decoding%20models%20are%20typically%20trained%20on%0Asmall%20numbers%20of%20subjects%20performing%20a%20single%20task.%20Here%2C%20we%20introduce%20a%0Alarge-scale%2C%20code-submission-based%20competition%20comprising%20two%20challenges.%0AFirst%2C%20the%20Transfer%20Challenge%20asks%20participants%20to%20build%20and%20test%20a%20model%20that%0Acan%20zero-shot%20decode%20new%20tasks%20and%20new%20subjects%20from%20their%20EEG%20data.%20Second%2C%0Athe%20Psychopathology%20factor%20prediction%20Challenge%20asks%20participants%20to%20infer%0Asubject%20measures%20of%20mental%20health%20from%20EEG%20data.%20For%20this%2C%20we%20use%20an%0Aunprecedented%2C%20multi-terabyte%20dataset%20of%20high-density%20EEG%20signals%20%28128%0Achannels%29%20recorded%20from%20over%203%2C000%20child%20to%20young%20adult%20subjects%20engaged%20in%0Amultiple%20active%20and%20passive%20tasks.%20We%20provide%20several%20tunable%20neural%20network%0Abaselines%20for%20each%20of%20these%20two%20challenges%2C%20including%20a%20simple%20network%20and%0Ademographic-based%20regression%20models.%20Developing%20models%20that%20generalise%20across%0Atasks%20and%20individuals%20will%20pave%20the%20way%20for%20ML%20network%20architectures%20capable%20of%0Aadapting%20to%20EEG%20data%20collected%20from%20diverse%20tasks%20and%20individuals.%20Similarly%2C%0Apredicting%20mental%20health-relevant%20personality%20trait%20values%20from%20EEG%20might%0Aidentify%20objective%20biomarkers%20useful%20for%20clinical%20diagnosis%20and%20design%20of%0Apersonalised%20treatment%20for%20psychological%20conditions.%20Ultimately%2C%20the%20advances%0Aspurred%20by%20this%20challenge%20could%20contribute%20to%20the%20development%20of%20computational%0Apsychiatry%20and%20useful%20neurotechnology%2C%20and%20contribute%20to%20breakthroughs%20in%20both%0Afundamental%20neuroscience%20and%20applied%20clinical%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG%2520Foundation%2520Challenge%253A%2520From%2520Cross-Task%2520to%2520Cross-Subject%2520EEG%2520Decoding%26entry.906535625%3DBruno%2520Aristimunha%2520and%2520Dung%2520Truong%2520and%2520Pierre%2520Guetschel%2520and%2520Seyed%2520Yahya%2520Shirazi%2520and%2520Isabelle%2520Guyon%2520and%2520Alexandre%2520R.%2520Franco%2520and%2520Michael%2520P.%2520Milham%2520and%2520Aviv%2520Dotan%2520and%2520Scott%2520Makeig%2520and%2520Alexandre%2520Gramfort%2520and%2520Jean-Remi%2520King%2520and%2520Marie-Constance%2520Corsi%2520and%2520Pedro%2520A.%2520Vald%25C3%25A9s-Sosa%2520and%2520Amit%2520Majumdar%2520and%2520Alan%2520Evans%2520and%2520Terrence%2520J%2520Sejnowski%2520and%2520Oren%2520Shriki%2520and%2520Sylvain%2520Chevallier%2520and%2520Arnaud%2520Delorme%26entry.1292438233%3D%2520%2520Current%2520electroencephalogram%2520%2528EEG%2529%2520decoding%2520models%2520are%2520typically%2520trained%2520on%250Asmall%2520numbers%2520of%2520subjects%2520performing%2520a%2520single%2520task.%2520Here%252C%2520we%2520introduce%2520a%250Alarge-scale%252C%2520code-submission-based%2520competition%2520comprising%2520two%2520challenges.%250AFirst%252C%2520the%2520Transfer%2520Challenge%2520asks%2520participants%2520to%2520build%2520and%2520test%2520a%2520model%2520that%250Acan%2520zero-shot%2520decode%2520new%2520tasks%2520and%2520new%2520subjects%2520from%2520their%2520EEG%2520data.%2520Second%252C%250Athe%2520Psychopathology%2520factor%2520prediction%2520Challenge%2520asks%2520participants%2520to%2520infer%250Asubject%2520measures%2520of%2520mental%2520health%2520from%2520EEG%2520data.%2520For%2520this%252C%2520we%2520use%2520an%250Aunprecedented%252C%2520multi-terabyte%2520dataset%2520of%2520high-density%2520EEG%2520signals%2520%2528128%250Achannels%2529%2520recorded%2520from%2520over%25203%252C000%2520child%2520to%2520young%2520adult%2520subjects%2520engaged%2520in%250Amultiple%2520active%2520and%2520passive%2520tasks.%2520We%2520provide%2520several%2520tunable%2520neural%2520network%250Abaselines%2520for%2520each%2520of%2520these%2520two%2520challenges%252C%2520including%2520a%2520simple%2520network%2520and%250Ademographic-based%2520regression%2520models.%2520Developing%2520models%2520that%2520generalise%2520across%250Atasks%2520and%2520individuals%2520will%2520pave%2520the%2520way%2520for%2520ML%2520network%2520architectures%2520capable%2520of%250Aadapting%2520to%2520EEG%2520data%2520collected%2520from%2520diverse%2520tasks%2520and%2520individuals.%2520Similarly%252C%250Apredicting%2520mental%2520health-relevant%2520personality%2520trait%2520values%2520from%2520EEG%2520might%250Aidentify%2520objective%2520biomarkers%2520useful%2520for%2520clinical%2520diagnosis%2520and%2520design%2520of%250Apersonalised%2520treatment%2520for%2520psychological%2520conditions.%2520Ultimately%252C%2520the%2520advances%250Aspurred%2520by%2520this%2520challenge%2520could%2520contribute%2520to%2520the%2520development%2520of%2520computational%250Apsychiatry%2520and%2520useful%2520neurotechnology%252C%2520and%2520contribute%2520to%2520breakthroughs%2520in%2520both%250Afundamental%2520neuroscience%2520and%2520applied%2520clinical%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG%20Foundation%20Challenge%3A%20From%20Cross-Task%20to%20Cross-Subject%20EEG%20Decoding&entry.906535625=Bruno%20Aristimunha%20and%20Dung%20Truong%20and%20Pierre%20Guetschel%20and%20Seyed%20Yahya%20Shirazi%20and%20Isabelle%20Guyon%20and%20Alexandre%20R.%20Franco%20and%20Michael%20P.%20Milham%20and%20Aviv%20Dotan%20and%20Scott%20Makeig%20and%20Alexandre%20Gramfort%20and%20Jean-Remi%20King%20and%20Marie-Constance%20Corsi%20and%20Pedro%20A.%20Vald%C3%A9s-Sosa%20and%20Amit%20Majumdar%20and%20Alan%20Evans%20and%20Terrence%20J%20Sejnowski%20and%20Oren%20Shriki%20and%20Sylvain%20Chevallier%20and%20Arnaud%20Delorme&entry.1292438233=%20%20Current%20electroencephalogram%20%28EEG%29%20decoding%20models%20are%20typically%20trained%20on%0Asmall%20numbers%20of%20subjects%20performing%20a%20single%20task.%20Here%2C%20we%20introduce%20a%0Alarge-scale%2C%20code-submission-based%20competition%20comprising%20two%20challenges.%0AFirst%2C%20the%20Transfer%20Challenge%20asks%20participants%20to%20build%20and%20test%20a%20model%20that%0Acan%20zero-shot%20decode%20new%20tasks%20and%20new%20subjects%20from%20their%20EEG%20data.%20Second%2C%0Athe%20Psychopathology%20factor%20prediction%20Challenge%20asks%20participants%20to%20infer%0Asubject%20measures%20of%20mental%20health%20from%20EEG%20data.%20For%20this%2C%20we%20use%20an%0Aunprecedented%2C%20multi-terabyte%20dataset%20of%20high-density%20EEG%20signals%20%28128%0Achannels%29%20recorded%20from%20over%203%2C000%20child%20to%20young%20adult%20subjects%20engaged%20in%0Amultiple%20active%20and%20passive%20tasks.%20We%20provide%20several%20tunable%20neural%20network%0Abaselines%20for%20each%20of%20these%20two%20challenges%2C%20including%20a%20simple%20network%20and%0Ademographic-based%20regression%20models.%20Developing%20models%20that%20generalise%20across%0Atasks%20and%20individuals%20will%20pave%20the%20way%20for%20ML%20network%20architectures%20capable%20of%0Aadapting%20to%20EEG%20data%20collected%20from%20diverse%20tasks%20and%20individuals.%20Similarly%2C%0Apredicting%20mental%20health-relevant%20personality%20trait%20values%20from%20EEG%20might%0Aidentify%20objective%20biomarkers%20useful%20for%20clinical%20diagnosis%20and%20design%20of%0Apersonalised%20treatment%20for%20psychological%20conditions.%20Ultimately%2C%20the%20advances%0Aspurred%20by%20this%20challenge%20could%20contribute%20to%20the%20development%20of%20computational%0Apsychiatry%20and%20useful%20neurotechnology%2C%20and%20contribute%20to%20breakthroughs%20in%20both%0Afundamental%20neuroscience%20and%20applied%20clinical%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19141v2&entry.124074799=Read"},
{"title": "Recurrent State Encoders for Efficient Neural Combinatorial Optimization", "author": "Tim Dernedde and Daniela Thyssens and Lars Schmidt-Thieme", "abstract": "  The primary paradigm in Neural Combinatorial Optimization (NCO) are\nconstruction methods, where a neural network is trained to sequentially add one\nsolution component at a time until a complete solution is constructed. We\nobserve that the typical changes to the state between two steps are small,\nsince usually only the node that gets added to the solution is removed from the\nstate. An efficient model should be able to reuse computation done in prior\nsteps. To that end, we propose to train a recurrent encoder that computes the\nstate embeddings not only based on the state but also the embeddings of the\nstep before. We show that the recurrent encoder can achieve equivalent or\nbetter performance than a non-recurrent encoder even if it consists of\n$3\\times$ fewer layers, thus significantly improving on latency. We demonstrate\nour findings on three different problems: the Traveling Salesman Problem (TSP),\nthe Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem\n(OP) and integrate the models into a large neighborhood search algorithm, to\nshowcase the practical relevance of our findings.\n", "link": "http://arxiv.org/abs/2509.05084v1", "date": "2025-09-05", "relevancy": 2.4825, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20State%20Encoders%20for%20Efficient%20Neural%20Combinatorial%20Optimization&body=Title%3A%20Recurrent%20State%20Encoders%20for%20Efficient%20Neural%20Combinatorial%20Optimization%0AAuthor%3A%20Tim%20Dernedde%20and%20Daniela%20Thyssens%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20%20%20The%20primary%20paradigm%20in%20Neural%20Combinatorial%20Optimization%20%28NCO%29%20are%0Aconstruction%20methods%2C%20where%20a%20neural%20network%20is%20trained%20to%20sequentially%20add%20one%0Asolution%20component%20at%20a%20time%20until%20a%20complete%20solution%20is%20constructed.%20We%0Aobserve%20that%20the%20typical%20changes%20to%20the%20state%20between%20two%20steps%20are%20small%2C%0Asince%20usually%20only%20the%20node%20that%20gets%20added%20to%20the%20solution%20is%20removed%20from%20the%0Astate.%20An%20efficient%20model%20should%20be%20able%20to%20reuse%20computation%20done%20in%20prior%0Asteps.%20To%20that%20end%2C%20we%20propose%20to%20train%20a%20recurrent%20encoder%20that%20computes%20the%0Astate%20embeddings%20not%20only%20based%20on%20the%20state%20but%20also%20the%20embeddings%20of%20the%0Astep%20before.%20We%20show%20that%20the%20recurrent%20encoder%20can%20achieve%20equivalent%20or%0Abetter%20performance%20than%20a%20non-recurrent%20encoder%20even%20if%20it%20consists%20of%0A%243%5Ctimes%24%20fewer%20layers%2C%20thus%20significantly%20improving%20on%20latency.%20We%20demonstrate%0Aour%20findings%20on%20three%20different%20problems%3A%20the%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%0Athe%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29%2C%20and%20the%20Orienteering%20Problem%0A%28OP%29%20and%20integrate%20the%20models%20into%20a%20large%20neighborhood%20search%20algorithm%2C%20to%0Ashowcase%20the%20practical%20relevance%20of%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520State%2520Encoders%2520for%2520Efficient%2520Neural%2520Combinatorial%2520Optimization%26entry.906535625%3DTim%2520Dernedde%2520and%2520Daniela%2520Thyssens%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3D%2520%2520The%2520primary%2520paradigm%2520in%2520Neural%2520Combinatorial%2520Optimization%2520%2528NCO%2529%2520are%250Aconstruction%2520methods%252C%2520where%2520a%2520neural%2520network%2520is%2520trained%2520to%2520sequentially%2520add%2520one%250Asolution%2520component%2520at%2520a%2520time%2520until%2520a%2520complete%2520solution%2520is%2520constructed.%2520We%250Aobserve%2520that%2520the%2520typical%2520changes%2520to%2520the%2520state%2520between%2520two%2520steps%2520are%2520small%252C%250Asince%2520usually%2520only%2520the%2520node%2520that%2520gets%2520added%2520to%2520the%2520solution%2520is%2520removed%2520from%2520the%250Astate.%2520An%2520efficient%2520model%2520should%2520be%2520able%2520to%2520reuse%2520computation%2520done%2520in%2520prior%250Asteps.%2520To%2520that%2520end%252C%2520we%2520propose%2520to%2520train%2520a%2520recurrent%2520encoder%2520that%2520computes%2520the%250Astate%2520embeddings%2520not%2520only%2520based%2520on%2520the%2520state%2520but%2520also%2520the%2520embeddings%2520of%2520the%250Astep%2520before.%2520We%2520show%2520that%2520the%2520recurrent%2520encoder%2520can%2520achieve%2520equivalent%2520or%250Abetter%2520performance%2520than%2520a%2520non-recurrent%2520encoder%2520even%2520if%2520it%2520consists%2520of%250A%25243%255Ctimes%2524%2520fewer%2520layers%252C%2520thus%2520significantly%2520improving%2520on%2520latency.%2520We%2520demonstrate%250Aour%2520findings%2520on%2520three%2520different%2520problems%253A%2520the%2520Traveling%2520Salesman%2520Problem%2520%2528TSP%2529%252C%250Athe%2520Capacitated%2520Vehicle%2520Routing%2520Problem%2520%2528CVRP%2529%252C%2520and%2520the%2520Orienteering%2520Problem%250A%2528OP%2529%2520and%2520integrate%2520the%2520models%2520into%2520a%2520large%2520neighborhood%2520search%2520algorithm%252C%2520to%250Ashowcase%2520the%2520practical%2520relevance%2520of%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20State%20Encoders%20for%20Efficient%20Neural%20Combinatorial%20Optimization&entry.906535625=Tim%20Dernedde%20and%20Daniela%20Thyssens%20and%20Lars%20Schmidt-Thieme&entry.1292438233=%20%20The%20primary%20paradigm%20in%20Neural%20Combinatorial%20Optimization%20%28NCO%29%20are%0Aconstruction%20methods%2C%20where%20a%20neural%20network%20is%20trained%20to%20sequentially%20add%20one%0Asolution%20component%20at%20a%20time%20until%20a%20complete%20solution%20is%20constructed.%20We%0Aobserve%20that%20the%20typical%20changes%20to%20the%20state%20between%20two%20steps%20are%20small%2C%0Asince%20usually%20only%20the%20node%20that%20gets%20added%20to%20the%20solution%20is%20removed%20from%20the%0Astate.%20An%20efficient%20model%20should%20be%20able%20to%20reuse%20computation%20done%20in%20prior%0Asteps.%20To%20that%20end%2C%20we%20propose%20to%20train%20a%20recurrent%20encoder%20that%20computes%20the%0Astate%20embeddings%20not%20only%20based%20on%20the%20state%20but%20also%20the%20embeddings%20of%20the%0Astep%20before.%20We%20show%20that%20the%20recurrent%20encoder%20can%20achieve%20equivalent%20or%0Abetter%20performance%20than%20a%20non-recurrent%20encoder%20even%20if%20it%20consists%20of%0A%243%5Ctimes%24%20fewer%20layers%2C%20thus%20significantly%20improving%20on%20latency.%20We%20demonstrate%0Aour%20findings%20on%20three%20different%20problems%3A%20the%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%0Athe%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29%2C%20and%20the%20Orienteering%20Problem%0A%28OP%29%20and%20integrate%20the%20models%20into%20a%20large%20neighborhood%20search%20algorithm%2C%20to%0Ashowcase%20the%20practical%20relevance%20of%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05084v1&entry.124074799=Read"},
{"title": "CEHR-XGPT: A Scalable Multi-Task Foundation Model for Electronic Health\n  Records", "author": "Chao Pang and Jiheum Park and Xinzhuo Jiang and Nishanth Parameshwar Pavinkurve and Krishna S. Kalluri and Shalmali Joshi and No\u00e9mie Elhadad and Karthik Natarajan", "abstract": "  Electronic Health Records (EHRs) provide a rich, longitudinal view of patient\nhealth and hold significant potential for advancing clinical decision support,\nrisk prediction, and data-driven healthcare research. However, most artificial\nintelligence (AI) models for EHRs are designed for narrow, single-purpose\ntasks, limiting their generalizability and utility in real-world settings.\nHere, we present CEHR-XGPT, a general-purpose foundation model for EHR data\nthat unifies three essential capabilities - feature representation, zero-shot\nprediction, and synthetic data generation - within a single architecture. To\nsupport temporal reasoning over clinical sequences, CEHR-XGPT incorporates a\nnovel time-token-based learning framework that explicitly encodes patients'\ndynamic timelines into the model structure. CEHR-XGPT demonstrates strong\nperformance across all three tasks and generalizes effectively to external\ndatasets through vocabulary expansion and fine-tuning. Its versatility enables\nrapid model development, cohort discovery, and patient outcome forecasting\nwithout the need for task-specific retraining.\n", "link": "http://arxiv.org/abs/2509.03643v2", "date": "2025-09-05", "relevancy": 2.4614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CEHR-XGPT%3A%20A%20Scalable%20Multi-Task%20Foundation%20Model%20for%20Electronic%20Health%0A%20%20Records&body=Title%3A%20CEHR-XGPT%3A%20A%20Scalable%20Multi-Task%20Foundation%20Model%20for%20Electronic%20Health%0A%20%20Records%0AAuthor%3A%20Chao%20Pang%20and%20Jiheum%20Park%20and%20Xinzhuo%20Jiang%20and%20Nishanth%20Parameshwar%20Pavinkurve%20and%20Krishna%20S.%20Kalluri%20and%20Shalmali%20Joshi%20and%20No%C3%A9mie%20Elhadad%20and%20Karthik%20Natarajan%0AAbstract%3A%20%20%20Electronic%20Health%20Records%20%28EHRs%29%20provide%20a%20rich%2C%20longitudinal%20view%20of%20patient%0Ahealth%20and%20hold%20significant%20potential%20for%20advancing%20clinical%20decision%20support%2C%0Arisk%20prediction%2C%20and%20data-driven%20healthcare%20research.%20However%2C%20most%20artificial%0Aintelligence%20%28AI%29%20models%20for%20EHRs%20are%20designed%20for%20narrow%2C%20single-purpose%0Atasks%2C%20limiting%20their%20generalizability%20and%20utility%20in%20real-world%20settings.%0AHere%2C%20we%20present%20CEHR-XGPT%2C%20a%20general-purpose%20foundation%20model%20for%20EHR%20data%0Athat%20unifies%20three%20essential%20capabilities%20-%20feature%20representation%2C%20zero-shot%0Aprediction%2C%20and%20synthetic%20data%20generation%20-%20within%20a%20single%20architecture.%20To%0Asupport%20temporal%20reasoning%20over%20clinical%20sequences%2C%20CEHR-XGPT%20incorporates%20a%0Anovel%20time-token-based%20learning%20framework%20that%20explicitly%20encodes%20patients%27%0Adynamic%20timelines%20into%20the%20model%20structure.%20CEHR-XGPT%20demonstrates%20strong%0Aperformance%20across%20all%20three%20tasks%20and%20generalizes%20effectively%20to%20external%0Adatasets%20through%20vocabulary%20expansion%20and%20fine-tuning.%20Its%20versatility%20enables%0Arapid%20model%20development%2C%20cohort%20discovery%2C%20and%20patient%20outcome%20forecasting%0Awithout%20the%20need%20for%20task-specific%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCEHR-XGPT%253A%2520A%2520Scalable%2520Multi-Task%2520Foundation%2520Model%2520for%2520Electronic%2520Health%250A%2520%2520Records%26entry.906535625%3DChao%2520Pang%2520and%2520Jiheum%2520Park%2520and%2520Xinzhuo%2520Jiang%2520and%2520Nishanth%2520Parameshwar%2520Pavinkurve%2520and%2520Krishna%2520S.%2520Kalluri%2520and%2520Shalmali%2520Joshi%2520and%2520No%25C3%25A9mie%2520Elhadad%2520and%2520Karthik%2520Natarajan%26entry.1292438233%3D%2520%2520Electronic%2520Health%2520Records%2520%2528EHRs%2529%2520provide%2520a%2520rich%252C%2520longitudinal%2520view%2520of%2520patient%250Ahealth%2520and%2520hold%2520significant%2520potential%2520for%2520advancing%2520clinical%2520decision%2520support%252C%250Arisk%2520prediction%252C%2520and%2520data-driven%2520healthcare%2520research.%2520However%252C%2520most%2520artificial%250Aintelligence%2520%2528AI%2529%2520models%2520for%2520EHRs%2520are%2520designed%2520for%2520narrow%252C%2520single-purpose%250Atasks%252C%2520limiting%2520their%2520generalizability%2520and%2520utility%2520in%2520real-world%2520settings.%250AHere%252C%2520we%2520present%2520CEHR-XGPT%252C%2520a%2520general-purpose%2520foundation%2520model%2520for%2520EHR%2520data%250Athat%2520unifies%2520three%2520essential%2520capabilities%2520-%2520feature%2520representation%252C%2520zero-shot%250Aprediction%252C%2520and%2520synthetic%2520data%2520generation%2520-%2520within%2520a%2520single%2520architecture.%2520To%250Asupport%2520temporal%2520reasoning%2520over%2520clinical%2520sequences%252C%2520CEHR-XGPT%2520incorporates%2520a%250Anovel%2520time-token-based%2520learning%2520framework%2520that%2520explicitly%2520encodes%2520patients%2527%250Adynamic%2520timelines%2520into%2520the%2520model%2520structure.%2520CEHR-XGPT%2520demonstrates%2520strong%250Aperformance%2520across%2520all%2520three%2520tasks%2520and%2520generalizes%2520effectively%2520to%2520external%250Adatasets%2520through%2520vocabulary%2520expansion%2520and%2520fine-tuning.%2520Its%2520versatility%2520enables%250Arapid%2520model%2520development%252C%2520cohort%2520discovery%252C%2520and%2520patient%2520outcome%2520forecasting%250Awithout%2520the%2520need%2520for%2520task-specific%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEHR-XGPT%3A%20A%20Scalable%20Multi-Task%20Foundation%20Model%20for%20Electronic%20Health%0A%20%20Records&entry.906535625=Chao%20Pang%20and%20Jiheum%20Park%20and%20Xinzhuo%20Jiang%20and%20Nishanth%20Parameshwar%20Pavinkurve%20and%20Krishna%20S.%20Kalluri%20and%20Shalmali%20Joshi%20and%20No%C3%A9mie%20Elhadad%20and%20Karthik%20Natarajan&entry.1292438233=%20%20Electronic%20Health%20Records%20%28EHRs%29%20provide%20a%20rich%2C%20longitudinal%20view%20of%20patient%0Ahealth%20and%20hold%20significant%20potential%20for%20advancing%20clinical%20decision%20support%2C%0Arisk%20prediction%2C%20and%20data-driven%20healthcare%20research.%20However%2C%20most%20artificial%0Aintelligence%20%28AI%29%20models%20for%20EHRs%20are%20designed%20for%20narrow%2C%20single-purpose%0Atasks%2C%20limiting%20their%20generalizability%20and%20utility%20in%20real-world%20settings.%0AHere%2C%20we%20present%20CEHR-XGPT%2C%20a%20general-purpose%20foundation%20model%20for%20EHR%20data%0Athat%20unifies%20three%20essential%20capabilities%20-%20feature%20representation%2C%20zero-shot%0Aprediction%2C%20and%20synthetic%20data%20generation%20-%20within%20a%20single%20architecture.%20To%0Asupport%20temporal%20reasoning%20over%20clinical%20sequences%2C%20CEHR-XGPT%20incorporates%20a%0Anovel%20time-token-based%20learning%20framework%20that%20explicitly%20encodes%20patients%27%0Adynamic%20timelines%20into%20the%20model%20structure.%20CEHR-XGPT%20demonstrates%20strong%0Aperformance%20across%20all%20three%20tasks%20and%20generalizes%20effectively%20to%20external%0Adatasets%20through%20vocabulary%20expansion%20and%20fine-tuning.%20Its%20versatility%20enables%0Arapid%20model%20development%2C%20cohort%20discovery%2C%20and%20patient%20outcome%20forecasting%0Awithout%20the%20need%20for%20task-specific%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03643v2&entry.124074799=Read"},
{"title": "Do Large Language Models Need Intent? Revisiting Response Generation\n  Strategies for Service Assistant", "author": "Inbal Bolshinsky and Shani Kupiec and Almog Sasson and Yehudit Aperstein and Alexander Apartsin", "abstract": "  In the era of conversational AI, generating accurate and contextually\nappropriate service responses remains a critical challenge. A central question\nremains: Is explicit intent recognition a prerequisite for generating\nhigh-quality service responses, or can models bypass this step and produce\neffective replies directly? This paper conducts a rigorous comparative study to\naddress this fundamental design dilemma. Leveraging two publicly available\nservice interaction datasets, we benchmark several state-of-the-art language\nmodels, including a fine-tuned T5 variant, across both paradigms: Intent-First\nResponse Generation and Direct Response Generation. Evaluation metrics\nencompass both linguistic quality and task success rates, revealing surprising\ninsights into the necessity or redundancy of explicit intent modelling. Our\nfindings challenge conventional assumptions in conversational AI pipelines,\noffering actionable guidelines for designing more efficient and effective\nresponse generation systems.\n", "link": "http://arxiv.org/abs/2509.05006v1", "date": "2025-09-05", "relevancy": 2.4389, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20Need%20Intent%3F%20Revisiting%20Response%20Generation%0A%20%20Strategies%20for%20Service%20Assistant&body=Title%3A%20Do%20Large%20Language%20Models%20Need%20Intent%3F%20Revisiting%20Response%20Generation%0A%20%20Strategies%20for%20Service%20Assistant%0AAuthor%3A%20Inbal%20Bolshinsky%20and%20Shani%20Kupiec%20and%20Almog%20Sasson%20and%20Yehudit%20Aperstein%20and%20Alexander%20Apartsin%0AAbstract%3A%20%20%20In%20the%20era%20of%20conversational%20AI%2C%20generating%20accurate%20and%20contextually%0Aappropriate%20service%20responses%20remains%20a%20critical%20challenge.%20A%20central%20question%0Aremains%3A%20Is%20explicit%20intent%20recognition%20a%20prerequisite%20for%20generating%0Ahigh-quality%20service%20responses%2C%20or%20can%20models%20bypass%20this%20step%20and%20produce%0Aeffective%20replies%20directly%3F%20This%20paper%20conducts%20a%20rigorous%20comparative%20study%20to%0Aaddress%20this%20fundamental%20design%20dilemma.%20Leveraging%20two%20publicly%20available%0Aservice%20interaction%20datasets%2C%20we%20benchmark%20several%20state-of-the-art%20language%0Amodels%2C%20including%20a%20fine-tuned%20T5%20variant%2C%20across%20both%20paradigms%3A%20Intent-First%0AResponse%20Generation%20and%20Direct%20Response%20Generation.%20Evaluation%20metrics%0Aencompass%20both%20linguistic%20quality%20and%20task%20success%20rates%2C%20revealing%20surprising%0Ainsights%20into%20the%20necessity%20or%20redundancy%20of%20explicit%20intent%20modelling.%20Our%0Afindings%20challenge%20conventional%20assumptions%20in%20conversational%20AI%20pipelines%2C%0Aoffering%20actionable%20guidelines%20for%20designing%20more%20efficient%20and%20effective%0Aresponse%20generation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520Need%2520Intent%253F%2520Revisiting%2520Response%2520Generation%250A%2520%2520Strategies%2520for%2520Service%2520Assistant%26entry.906535625%3DInbal%2520Bolshinsky%2520and%2520Shani%2520Kupiec%2520and%2520Almog%2520Sasson%2520and%2520Yehudit%2520Aperstein%2520and%2520Alexander%2520Apartsin%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520conversational%2520AI%252C%2520generating%2520accurate%2520and%2520contextually%250Aappropriate%2520service%2520responses%2520remains%2520a%2520critical%2520challenge.%2520A%2520central%2520question%250Aremains%253A%2520Is%2520explicit%2520intent%2520recognition%2520a%2520prerequisite%2520for%2520generating%250Ahigh-quality%2520service%2520responses%252C%2520or%2520can%2520models%2520bypass%2520this%2520step%2520and%2520produce%250Aeffective%2520replies%2520directly%253F%2520This%2520paper%2520conducts%2520a%2520rigorous%2520comparative%2520study%2520to%250Aaddress%2520this%2520fundamental%2520design%2520dilemma.%2520Leveraging%2520two%2520publicly%2520available%250Aservice%2520interaction%2520datasets%252C%2520we%2520benchmark%2520several%2520state-of-the-art%2520language%250Amodels%252C%2520including%2520a%2520fine-tuned%2520T5%2520variant%252C%2520across%2520both%2520paradigms%253A%2520Intent-First%250AResponse%2520Generation%2520and%2520Direct%2520Response%2520Generation.%2520Evaluation%2520metrics%250Aencompass%2520both%2520linguistic%2520quality%2520and%2520task%2520success%2520rates%252C%2520revealing%2520surprising%250Ainsights%2520into%2520the%2520necessity%2520or%2520redundancy%2520of%2520explicit%2520intent%2520modelling.%2520Our%250Afindings%2520challenge%2520conventional%2520assumptions%2520in%2520conversational%2520AI%2520pipelines%252C%250Aoffering%2520actionable%2520guidelines%2520for%2520designing%2520more%2520efficient%2520and%2520effective%250Aresponse%2520generation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20Need%20Intent%3F%20Revisiting%20Response%20Generation%0A%20%20Strategies%20for%20Service%20Assistant&entry.906535625=Inbal%20Bolshinsky%20and%20Shani%20Kupiec%20and%20Almog%20Sasson%20and%20Yehudit%20Aperstein%20and%20Alexander%20Apartsin&entry.1292438233=%20%20In%20the%20era%20of%20conversational%20AI%2C%20generating%20accurate%20and%20contextually%0Aappropriate%20service%20responses%20remains%20a%20critical%20challenge.%20A%20central%20question%0Aremains%3A%20Is%20explicit%20intent%20recognition%20a%20prerequisite%20for%20generating%0Ahigh-quality%20service%20responses%2C%20or%20can%20models%20bypass%20this%20step%20and%20produce%0Aeffective%20replies%20directly%3F%20This%20paper%20conducts%20a%20rigorous%20comparative%20study%20to%0Aaddress%20this%20fundamental%20design%20dilemma.%20Leveraging%20two%20publicly%20available%0Aservice%20interaction%20datasets%2C%20we%20benchmark%20several%20state-of-the-art%20language%0Amodels%2C%20including%20a%20fine-tuned%20T5%20variant%2C%20across%20both%20paradigms%3A%20Intent-First%0AResponse%20Generation%20and%20Direct%20Response%20Generation.%20Evaluation%20metrics%0Aencompass%20both%20linguistic%20quality%20and%20task%20success%20rates%2C%20revealing%20surprising%0Ainsights%20into%20the%20necessity%20or%20redundancy%20of%20explicit%20intent%20modelling.%20Our%0Afindings%20challenge%20conventional%20assumptions%20in%20conversational%20AI%20pipelines%2C%0Aoffering%20actionable%20guidelines%20for%20designing%20more%20efficient%20and%20effective%0Aresponse%20generation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05006v1&entry.124074799=Read"},
{"title": "3D Densification for Multi-Map Monocular VSLAM in Endoscopy", "author": "X. Anad\u00f3n and Javier Rodr\u00edguez-Puigvert and J. M. M. Montiel", "abstract": "  Multi-map Sparse Monocular visual Simultaneous Localization and Mapping\napplied to monocular endoscopic sequences has proven efficient to robustly\nrecover tracking after the frequent losses in endoscopy due to motion blur,\ntemporal occlusion, tools interaction or water jets. The sparse multi-maps are\nadequate for robust camera localization, however they are very poor for\nenvironment representation, they are noisy, with a high percentage of\ninaccurately reconstructed 3D points, including significant outliers, and more\nimportantly with an unacceptable low density for clinical applications.\n  We propose a method to remove outliers and densify the maps of the state of\nthe art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for\nup-to-scale depth dense predictions are aligned with the sparse CudaSIFT\nsubmaps by means of the robust to spurious LMedS. Our system mitigates the\ninherent scale ambiguity in monocular depth estimation while filtering\noutliers, leading to reliable densified 3D maps.\n  We provide experimental evidence of accurate densified maps 4.15 mm RMS\naccuracy at affordable computing time in the C3VD phantom colon dataset. We\nreport qualitative results on the real colonoscopy from the Endomapper dataset.\n", "link": "http://arxiv.org/abs/2503.14346v2", "date": "2025-09-05", "relevancy": 2.4371, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6175}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6145}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Densification%20for%20Multi-Map%20Monocular%20VSLAM%20in%20Endoscopy&body=Title%3A%203D%20Densification%20for%20Multi-Map%20Monocular%20VSLAM%20in%20Endoscopy%0AAuthor%3A%20X.%20Anad%C3%B3n%20and%20Javier%20Rodr%C3%ADguez-Puigvert%20and%20J.%20M.%20M.%20Montiel%0AAbstract%3A%20%20%20Multi-map%20Sparse%20Monocular%20visual%20Simultaneous%20Localization%20and%20Mapping%0Aapplied%20to%20monocular%20endoscopic%20sequences%20has%20proven%20efficient%20to%20robustly%0Arecover%20tracking%20after%20the%20frequent%20losses%20in%20endoscopy%20due%20to%20motion%20blur%2C%0Atemporal%20occlusion%2C%20tools%20interaction%20or%20water%20jets.%20The%20sparse%20multi-maps%20are%0Aadequate%20for%20robust%20camera%20localization%2C%20however%20they%20are%20very%20poor%20for%0Aenvironment%20representation%2C%20they%20are%20noisy%2C%20with%20a%20high%20percentage%20of%0Ainaccurately%20reconstructed%203D%20points%2C%20including%20significant%20outliers%2C%20and%20more%0Aimportantly%20with%20an%20unacceptable%20low%20density%20for%20clinical%20applications.%0A%20%20We%20propose%20a%20method%20to%20remove%20outliers%20and%20densify%20the%20maps%20of%20the%20state%20of%0Athe%20art%20for%20sparse%20endoscopy%20multi-map%20CudaSIFT-SLAM.%20The%20NN%20LightDepth%20for%0Aup-to-scale%20depth%20dense%20predictions%20are%20aligned%20with%20the%20sparse%20CudaSIFT%0Asubmaps%20by%20means%20of%20the%20robust%20to%20spurious%20LMedS.%20Our%20system%20mitigates%20the%0Ainherent%20scale%20ambiguity%20in%20monocular%20depth%20estimation%20while%20filtering%0Aoutliers%2C%20leading%20to%20reliable%20densified%203D%20maps.%0A%20%20We%20provide%20experimental%20evidence%20of%20accurate%20densified%20maps%204.15%20mm%20RMS%0Aaccuracy%20at%20affordable%20computing%20time%20in%20the%20C3VD%20phantom%20colon%20dataset.%20We%0Areport%20qualitative%20results%20on%20the%20real%20colonoscopy%20from%20the%20Endomapper%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Densification%2520for%2520Multi-Map%2520Monocular%2520VSLAM%2520in%2520Endoscopy%26entry.906535625%3DX.%2520Anad%25C3%25B3n%2520and%2520Javier%2520Rodr%25C3%25ADguez-Puigvert%2520and%2520J.%2520M.%2520M.%2520Montiel%26entry.1292438233%3D%2520%2520Multi-map%2520Sparse%2520Monocular%2520visual%2520Simultaneous%2520Localization%2520and%2520Mapping%250Aapplied%2520to%2520monocular%2520endoscopic%2520sequences%2520has%2520proven%2520efficient%2520to%2520robustly%250Arecover%2520tracking%2520after%2520the%2520frequent%2520losses%2520in%2520endoscopy%2520due%2520to%2520motion%2520blur%252C%250Atemporal%2520occlusion%252C%2520tools%2520interaction%2520or%2520water%2520jets.%2520The%2520sparse%2520multi-maps%2520are%250Aadequate%2520for%2520robust%2520camera%2520localization%252C%2520however%2520they%2520are%2520very%2520poor%2520for%250Aenvironment%2520representation%252C%2520they%2520are%2520noisy%252C%2520with%2520a%2520high%2520percentage%2520of%250Ainaccurately%2520reconstructed%25203D%2520points%252C%2520including%2520significant%2520outliers%252C%2520and%2520more%250Aimportantly%2520with%2520an%2520unacceptable%2520low%2520density%2520for%2520clinical%2520applications.%250A%2520%2520We%2520propose%2520a%2520method%2520to%2520remove%2520outliers%2520and%2520densify%2520the%2520maps%2520of%2520the%2520state%2520of%250Athe%2520art%2520for%2520sparse%2520endoscopy%2520multi-map%2520CudaSIFT-SLAM.%2520The%2520NN%2520LightDepth%2520for%250Aup-to-scale%2520depth%2520dense%2520predictions%2520are%2520aligned%2520with%2520the%2520sparse%2520CudaSIFT%250Asubmaps%2520by%2520means%2520of%2520the%2520robust%2520to%2520spurious%2520LMedS.%2520Our%2520system%2520mitigates%2520the%250Ainherent%2520scale%2520ambiguity%2520in%2520monocular%2520depth%2520estimation%2520while%2520filtering%250Aoutliers%252C%2520leading%2520to%2520reliable%2520densified%25203D%2520maps.%250A%2520%2520We%2520provide%2520experimental%2520evidence%2520of%2520accurate%2520densified%2520maps%25204.15%2520mm%2520RMS%250Aaccuracy%2520at%2520affordable%2520computing%2520time%2520in%2520the%2520C3VD%2520phantom%2520colon%2520dataset.%2520We%250Areport%2520qualitative%2520results%2520on%2520the%2520real%2520colonoscopy%2520from%2520the%2520Endomapper%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Densification%20for%20Multi-Map%20Monocular%20VSLAM%20in%20Endoscopy&entry.906535625=X.%20Anad%C3%B3n%20and%20Javier%20Rodr%C3%ADguez-Puigvert%20and%20J.%20M.%20M.%20Montiel&entry.1292438233=%20%20Multi-map%20Sparse%20Monocular%20visual%20Simultaneous%20Localization%20and%20Mapping%0Aapplied%20to%20monocular%20endoscopic%20sequences%20has%20proven%20efficient%20to%20robustly%0Arecover%20tracking%20after%20the%20frequent%20losses%20in%20endoscopy%20due%20to%20motion%20blur%2C%0Atemporal%20occlusion%2C%20tools%20interaction%20or%20water%20jets.%20The%20sparse%20multi-maps%20are%0Aadequate%20for%20robust%20camera%20localization%2C%20however%20they%20are%20very%20poor%20for%0Aenvironment%20representation%2C%20they%20are%20noisy%2C%20with%20a%20high%20percentage%20of%0Ainaccurately%20reconstructed%203D%20points%2C%20including%20significant%20outliers%2C%20and%20more%0Aimportantly%20with%20an%20unacceptable%20low%20density%20for%20clinical%20applications.%0A%20%20We%20propose%20a%20method%20to%20remove%20outliers%20and%20densify%20the%20maps%20of%20the%20state%20of%0Athe%20art%20for%20sparse%20endoscopy%20multi-map%20CudaSIFT-SLAM.%20The%20NN%20LightDepth%20for%0Aup-to-scale%20depth%20dense%20predictions%20are%20aligned%20with%20the%20sparse%20CudaSIFT%0Asubmaps%20by%20means%20of%20the%20robust%20to%20spurious%20LMedS.%20Our%20system%20mitigates%20the%0Ainherent%20scale%20ambiguity%20in%20monocular%20depth%20estimation%20while%20filtering%0Aoutliers%2C%20leading%20to%20reliable%20densified%203D%20maps.%0A%20%20We%20provide%20experimental%20evidence%20of%20accurate%20densified%20maps%204.15%20mm%20RMS%0Aaccuracy%20at%20affordable%20computing%20time%20in%20the%20C3VD%20phantom%20colon%20dataset.%20We%0Areport%20qualitative%20results%20on%20the%20real%20colonoscopy%20from%20the%20Endomapper%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14346v2&entry.124074799=Read"},
{"title": "Landmark-Based Node Representations for Shortest Path Distance\n  Approximations in Random Graphs", "author": "My Le and Luana Ruiz and Souvik Dhara", "abstract": "  Learning node representations is a fundamental problem in graph machine\nlearning. While existing embedding methods effectively preserve local\nsimilarity measures, they often fail to capture global functions like graph\ndistances. Inspired by Bourgain's seminal work on Hilbert space embeddings of\nmetric spaces (1985), we study the performance of local distance-preserving\nnode embeddings. Known as landmark-based algorithms, these embeddings\napproximate pairwise distances by computing shortest paths from a small subset\nof reference nodes called landmarks. Our main theoretical contribution shows\nthat random graphs, such as Erdos-Renyi random graphs, require lower dimensions\nin landmark-based embeddings compared to worst-case graphs. Empirically, we\ndemonstrate that the GNN-based approximations for the distances to landmarks\ngeneralize well to larger real-world networks, offering a scalable and\ntransferable alternative for graph representation learning.\n", "link": "http://arxiv.org/abs/2504.08216v2", "date": "2025-09-05", "relevancy": 2.4327, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5307}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4772}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Landmark-Based%20Node%20Representations%20for%20Shortest%20Path%20Distance%0A%20%20Approximations%20in%20Random%20Graphs&body=Title%3A%20Landmark-Based%20Node%20Representations%20for%20Shortest%20Path%20Distance%0A%20%20Approximations%20in%20Random%20Graphs%0AAuthor%3A%20My%20Le%20and%20Luana%20Ruiz%20and%20Souvik%20Dhara%0AAbstract%3A%20%20%20Learning%20node%20representations%20is%20a%20fundamental%20problem%20in%20graph%20machine%0Alearning.%20While%20existing%20embedding%20methods%20effectively%20preserve%20local%0Asimilarity%20measures%2C%20they%20often%20fail%20to%20capture%20global%20functions%20like%20graph%0Adistances.%20Inspired%20by%20Bourgain%27s%20seminal%20work%20on%20Hilbert%20space%20embeddings%20of%0Ametric%20spaces%20%281985%29%2C%20we%20study%20the%20performance%20of%20local%20distance-preserving%0Anode%20embeddings.%20Known%20as%20landmark-based%20algorithms%2C%20these%20embeddings%0Aapproximate%20pairwise%20distances%20by%20computing%20shortest%20paths%20from%20a%20small%20subset%0Aof%20reference%20nodes%20called%20landmarks.%20Our%20main%20theoretical%20contribution%20shows%0Athat%20random%20graphs%2C%20such%20as%20Erdos-Renyi%20random%20graphs%2C%20require%20lower%20dimensions%0Ain%20landmark-based%20embeddings%20compared%20to%20worst-case%20graphs.%20Empirically%2C%20we%0Ademonstrate%20that%20the%20GNN-based%20approximations%20for%20the%20distances%20to%20landmarks%0Ageneralize%20well%20to%20larger%20real-world%20networks%2C%20offering%20a%20scalable%20and%0Atransferable%20alternative%20for%20graph%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLandmark-Based%2520Node%2520Representations%2520for%2520Shortest%2520Path%2520Distance%250A%2520%2520Approximations%2520in%2520Random%2520Graphs%26entry.906535625%3DMy%2520Le%2520and%2520Luana%2520Ruiz%2520and%2520Souvik%2520Dhara%26entry.1292438233%3D%2520%2520Learning%2520node%2520representations%2520is%2520a%2520fundamental%2520problem%2520in%2520graph%2520machine%250Alearning.%2520While%2520existing%2520embedding%2520methods%2520effectively%2520preserve%2520local%250Asimilarity%2520measures%252C%2520they%2520often%2520fail%2520to%2520capture%2520global%2520functions%2520like%2520graph%250Adistances.%2520Inspired%2520by%2520Bourgain%2527s%2520seminal%2520work%2520on%2520Hilbert%2520space%2520embeddings%2520of%250Ametric%2520spaces%2520%25281985%2529%252C%2520we%2520study%2520the%2520performance%2520of%2520local%2520distance-preserving%250Anode%2520embeddings.%2520Known%2520as%2520landmark-based%2520algorithms%252C%2520these%2520embeddings%250Aapproximate%2520pairwise%2520distances%2520by%2520computing%2520shortest%2520paths%2520from%2520a%2520small%2520subset%250Aof%2520reference%2520nodes%2520called%2520landmarks.%2520Our%2520main%2520theoretical%2520contribution%2520shows%250Athat%2520random%2520graphs%252C%2520such%2520as%2520Erdos-Renyi%2520random%2520graphs%252C%2520require%2520lower%2520dimensions%250Ain%2520landmark-based%2520embeddings%2520compared%2520to%2520worst-case%2520graphs.%2520Empirically%252C%2520we%250Ademonstrate%2520that%2520the%2520GNN-based%2520approximations%2520for%2520the%2520distances%2520to%2520landmarks%250Ageneralize%2520well%2520to%2520larger%2520real-world%2520networks%252C%2520offering%2520a%2520scalable%2520and%250Atransferable%2520alternative%2520for%2520graph%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Landmark-Based%20Node%20Representations%20for%20Shortest%20Path%20Distance%0A%20%20Approximations%20in%20Random%20Graphs&entry.906535625=My%20Le%20and%20Luana%20Ruiz%20and%20Souvik%20Dhara&entry.1292438233=%20%20Learning%20node%20representations%20is%20a%20fundamental%20problem%20in%20graph%20machine%0Alearning.%20While%20existing%20embedding%20methods%20effectively%20preserve%20local%0Asimilarity%20measures%2C%20they%20often%20fail%20to%20capture%20global%20functions%20like%20graph%0Adistances.%20Inspired%20by%20Bourgain%27s%20seminal%20work%20on%20Hilbert%20space%20embeddings%20of%0Ametric%20spaces%20%281985%29%2C%20we%20study%20the%20performance%20of%20local%20distance-preserving%0Anode%20embeddings.%20Known%20as%20landmark-based%20algorithms%2C%20these%20embeddings%0Aapproximate%20pairwise%20distances%20by%20computing%20shortest%20paths%20from%20a%20small%20subset%0Aof%20reference%20nodes%20called%20landmarks.%20Our%20main%20theoretical%20contribution%20shows%0Athat%20random%20graphs%2C%20such%20as%20Erdos-Renyi%20random%20graphs%2C%20require%20lower%20dimensions%0Ain%20landmark-based%20embeddings%20compared%20to%20worst-case%20graphs.%20Empirically%2C%20we%0Ademonstrate%20that%20the%20GNN-based%20approximations%20for%20the%20distances%20to%20landmarks%0Ageneralize%20well%20to%20larger%20real-world%20networks%2C%20offering%20a%20scalable%20and%0Atransferable%20alternative%20for%20graph%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08216v2&entry.124074799=Read"},
{"title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection", "author": "Yung-Hsu Yang and Luigi Piccinelli and Mattia Segu and Siyuan Li and Rui Huang and Yuqian Fu and Marc Pollefeys and Hermann Blum and Zuria Bauer", "abstract": "  Monocular 3D object detection is valuable for various applications such as\nrobotics and AR/VR. Existing methods are confined to closed-set settings, where\nthe training and testing sets consist of the same scenes and/or object\ncategories. However, real-world applications often introduce new environments\nand novel object categories, posing a challenge to these methods. In this\npaper, we address monocular 3D object detection in an open-set setting and\nintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).\nWe propose to lift the open-set 2D detection into 3D space through our designed\n3D bounding box head, enabling end-to-end joint training for both 2D and 3D\ntasks to yield better overall performance. We condition the object queries with\ngeometry prior and overcome the generalization for 3D estimation across diverse\nscenes. To further improve performance, we design the canonical image space for\nmore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set\nsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and\nachieve new state-of-the-art results. Code and models are available at\nroyyang0714.github.io/3D-MOOD.\n", "link": "http://arxiv.org/abs/2507.23567v2", "date": "2025-09-05", "relevancy": 2.3839, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6133}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection&body=Title%3A%203D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection%0AAuthor%3A%20Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Siyuan%20Li%20and%20Rui%20Huang%20and%20Yuqian%20Fu%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer%0AAbstract%3A%20%20%20Monocular%203D%20object%20detection%20is%20valuable%20for%20various%20applications%20such%20as%0Arobotics%20and%20AR/VR.%20Existing%20methods%20are%20confined%20to%20closed-set%20settings%2C%20where%0Athe%20training%20and%20testing%20sets%20consist%20of%20the%20same%20scenes%20and/or%20object%0Acategories.%20However%2C%20real-world%20applications%20often%20introduce%20new%20environments%0Aand%20novel%20object%20categories%2C%20posing%20a%20challenge%20to%20these%20methods.%20In%20this%0Apaper%2C%20we%20address%20monocular%203D%20object%20detection%20in%20an%20open-set%20setting%20and%0Aintroduce%20the%20first%20end-to-end%203D%20Monocular%20Open-set%20Object%20Detector%20%283D-MOOD%29.%0AWe%20propose%20to%20lift%20the%20open-set%202D%20detection%20into%203D%20space%20through%20our%20designed%0A3D%20bounding%20box%20head%2C%20enabling%20end-to-end%20joint%20training%20for%20both%202D%20and%203D%0Atasks%20to%20yield%20better%20overall%20performance.%20We%20condition%20the%20object%20queries%20with%0Ageometry%20prior%20and%20overcome%20the%20generalization%20for%203D%20estimation%20across%20diverse%0Ascenes.%20To%20further%20improve%20performance%2C%20we%20design%20the%20canonical%20image%20space%20for%0Amore%20efficient%20cross-dataset%20training.%20We%20evaluate%203D-MOOD%20on%20both%20closed-set%0Asettings%20%28Omni3D%29%20and%20open-set%20settings%20%28Omni3D%20to%20Argoverse%202%2C%20ScanNet%29%2C%20and%0Aachieve%20new%20state-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%0Aroyyang0714.github.io/3D-MOOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-MOOD%253A%2520Lifting%25202D%2520to%25203D%2520for%2520Monocular%2520Open-Set%2520Object%2520Detection%26entry.906535625%3DYung-Hsu%2520Yang%2520and%2520Luigi%2520Piccinelli%2520and%2520Mattia%2520Segu%2520and%2520Siyuan%2520Li%2520and%2520Rui%2520Huang%2520and%2520Yuqian%2520Fu%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%2520and%2520Zuria%2520Bauer%26entry.1292438233%3D%2520%2520Monocular%25203D%2520object%2520detection%2520is%2520valuable%2520for%2520various%2520applications%2520such%2520as%250Arobotics%2520and%2520AR/VR.%2520Existing%2520methods%2520are%2520confined%2520to%2520closed-set%2520settings%252C%2520where%250Athe%2520training%2520and%2520testing%2520sets%2520consist%2520of%2520the%2520same%2520scenes%2520and/or%2520object%250Acategories.%2520However%252C%2520real-world%2520applications%2520often%2520introduce%2520new%2520environments%250Aand%2520novel%2520object%2520categories%252C%2520posing%2520a%2520challenge%2520to%2520these%2520methods.%2520In%2520this%250Apaper%252C%2520we%2520address%2520monocular%25203D%2520object%2520detection%2520in%2520an%2520open-set%2520setting%2520and%250Aintroduce%2520the%2520first%2520end-to-end%25203D%2520Monocular%2520Open-set%2520Object%2520Detector%2520%25283D-MOOD%2529.%250AWe%2520propose%2520to%2520lift%2520the%2520open-set%25202D%2520detection%2520into%25203D%2520space%2520through%2520our%2520designed%250A3D%2520bounding%2520box%2520head%252C%2520enabling%2520end-to-end%2520joint%2520training%2520for%2520both%25202D%2520and%25203D%250Atasks%2520to%2520yield%2520better%2520overall%2520performance.%2520We%2520condition%2520the%2520object%2520queries%2520with%250Ageometry%2520prior%2520and%2520overcome%2520the%2520generalization%2520for%25203D%2520estimation%2520across%2520diverse%250Ascenes.%2520To%2520further%2520improve%2520performance%252C%2520we%2520design%2520the%2520canonical%2520image%2520space%2520for%250Amore%2520efficient%2520cross-dataset%2520training.%2520We%2520evaluate%25203D-MOOD%2520on%2520both%2520closed-set%250Asettings%2520%2528Omni3D%2529%2520and%2520open-set%2520settings%2520%2528Omni3D%2520to%2520Argoverse%25202%252C%2520ScanNet%2529%252C%2520and%250Aachieve%2520new%2520state-of-the-art%2520results.%2520Code%2520and%2520models%2520are%2520available%2520at%250Aroyyang0714.github.io/3D-MOOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection&entry.906535625=Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Siyuan%20Li%20and%20Rui%20Huang%20and%20Yuqian%20Fu%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer&entry.1292438233=%20%20Monocular%203D%20object%20detection%20is%20valuable%20for%20various%20applications%20such%20as%0Arobotics%20and%20AR/VR.%20Existing%20methods%20are%20confined%20to%20closed-set%20settings%2C%20where%0Athe%20training%20and%20testing%20sets%20consist%20of%20the%20same%20scenes%20and/or%20object%0Acategories.%20However%2C%20real-world%20applications%20often%20introduce%20new%20environments%0Aand%20novel%20object%20categories%2C%20posing%20a%20challenge%20to%20these%20methods.%20In%20this%0Apaper%2C%20we%20address%20monocular%203D%20object%20detection%20in%20an%20open-set%20setting%20and%0Aintroduce%20the%20first%20end-to-end%203D%20Monocular%20Open-set%20Object%20Detector%20%283D-MOOD%29.%0AWe%20propose%20to%20lift%20the%20open-set%202D%20detection%20into%203D%20space%20through%20our%20designed%0A3D%20bounding%20box%20head%2C%20enabling%20end-to-end%20joint%20training%20for%20both%202D%20and%203D%0Atasks%20to%20yield%20better%20overall%20performance.%20We%20condition%20the%20object%20queries%20with%0Ageometry%20prior%20and%20overcome%20the%20generalization%20for%203D%20estimation%20across%20diverse%0Ascenes.%20To%20further%20improve%20performance%2C%20we%20design%20the%20canonical%20image%20space%20for%0Amore%20efficient%20cross-dataset%20training.%20We%20evaluate%203D-MOOD%20on%20both%20closed-set%0Asettings%20%28Omni3D%29%20and%20open-set%20settings%20%28Omni3D%20to%20Argoverse%202%2C%20ScanNet%29%2C%20and%0Aachieve%20new%20state-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%0Aroyyang0714.github.io/3D-MOOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23567v2&entry.124074799=Read"},
{"title": "Leveraging Transfer Learning and Mobile-enabled Convolutional Neural\n  Networks for Improved Arabic Handwritten Character Recognition", "author": "Mohsine El Khayati and Ayyad Maafiri and Yassine Himeur and Hamzah Ali Alkhazaleh and Shadi Atalla and Wathiq Mansoor", "abstract": "  The study explores the integration of transfer learning (TL) with\nmobile-enabled convolutional neural networks (MbNets) to enhance Arabic\nHandwritten Character Recognition (AHCR). Addressing challenges like extensive\ncomputational requirements and dataset scarcity, this research evaluates three\nTL strategies--full fine-tuning, partial fine-tuning, and training from\nscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and\nShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,\nHIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently\nachieving superior accuracy, robustness, and efficiency, with ShuffleNet\nexcelling in generalization, particularly under full fine-tuning. The IFHCDB\ndataset yielded the highest results, with 99% accuracy using MnasNet under full\nfine-tuning, highlighting its suitability for robust character recognition. The\nAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA\nposed significant challenges due to its variability, achieving a peak accuracy\nof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall\nperformance, balancing accuracy and convergence speed, while partial\nfine-tuning underperformed across metrics. These findings underscore the\npotential of combining TL and MbNets for resource-efficient AHCR, paving the\nway for further optimizations and broader applications. Future work will\nexplore architectural modifications, in-depth dataset feature analysis, data\naugmentation, and advanced sensitivity analysis to enhance model robustness and\ngeneralizability.\n", "link": "http://arxiv.org/abs/2509.05019v1", "date": "2025-09-05", "relevancy": 2.367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.471}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Transfer%20Learning%20and%20Mobile-enabled%20Convolutional%20Neural%0A%20%20Networks%20for%20Improved%20Arabic%20Handwritten%20Character%20Recognition&body=Title%3A%20Leveraging%20Transfer%20Learning%20and%20Mobile-enabled%20Convolutional%20Neural%0A%20%20Networks%20for%20Improved%20Arabic%20Handwritten%20Character%20Recognition%0AAuthor%3A%20Mohsine%20El%20Khayati%20and%20Ayyad%20Maafiri%20and%20Yassine%20Himeur%20and%20Hamzah%20Ali%20Alkhazaleh%20and%20Shadi%20Atalla%20and%20Wathiq%20Mansoor%0AAbstract%3A%20%20%20The%20study%20explores%20the%20integration%20of%20transfer%20learning%20%28TL%29%20with%0Amobile-enabled%20convolutional%20neural%20networks%20%28MbNets%29%20to%20enhance%20Arabic%0AHandwritten%20Character%20Recognition%20%28AHCR%29.%20Addressing%20challenges%20like%20extensive%0Acomputational%20requirements%20and%20dataset%20scarcity%2C%20this%20research%20evaluates%20three%0ATL%20strategies--full%20fine-tuning%2C%20partial%20fine-tuning%2C%20and%20training%20from%0Ascratch--using%20four%20lightweight%20MbNets%3A%20MobileNet%2C%20SqueezeNet%2C%20MnasNet%2C%20and%0AShuffleNet.%20Experiments%20were%20conducted%20on%20three%20benchmark%20datasets%3A%20AHCD%2C%0AHIJJA%2C%20and%20IFHCDB.%20MobileNet%20emerged%20as%20the%20top-performing%20model%2C%20consistently%0Aachieving%20superior%20accuracy%2C%20robustness%2C%20and%20efficiency%2C%20with%20ShuffleNet%0Aexcelling%20in%20generalization%2C%20particularly%20under%20full%20fine-tuning.%20The%20IFHCDB%0Adataset%20yielded%20the%20highest%20results%2C%20with%2099%25%20accuracy%20using%20MnasNet%20under%20full%0Afine-tuning%2C%20highlighting%20its%20suitability%20for%20robust%20character%20recognition.%20The%0AAHCD%20dataset%20achieved%20competitive%20accuracy%20%2897%25%29%20with%20ShuffleNet%2C%20while%20HIJJA%0Aposed%20significant%20challenges%20due%20to%20its%20variability%2C%20achieving%20a%20peak%20accuracy%0Aof%2092%25%20with%20ShuffleNet.%20Notably%2C%20full%20fine-tuning%20demonstrated%20the%20best%20overall%0Aperformance%2C%20balancing%20accuracy%20and%20convergence%20speed%2C%20while%20partial%0Afine-tuning%20underperformed%20across%20metrics.%20These%20findings%20underscore%20the%0Apotential%20of%20combining%20TL%20and%20MbNets%20for%20resource-efficient%20AHCR%2C%20paving%20the%0Away%20for%20further%20optimizations%20and%20broader%20applications.%20Future%20work%20will%0Aexplore%20architectural%20modifications%2C%20in-depth%20dataset%20feature%20analysis%2C%20data%0Aaugmentation%2C%20and%20advanced%20sensitivity%20analysis%20to%20enhance%20model%20robustness%20and%0Ageneralizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Transfer%2520Learning%2520and%2520Mobile-enabled%2520Convolutional%2520Neural%250A%2520%2520Networks%2520for%2520Improved%2520Arabic%2520Handwritten%2520Character%2520Recognition%26entry.906535625%3DMohsine%2520El%2520Khayati%2520and%2520Ayyad%2520Maafiri%2520and%2520Yassine%2520Himeur%2520and%2520Hamzah%2520Ali%2520Alkhazaleh%2520and%2520Shadi%2520Atalla%2520and%2520Wathiq%2520Mansoor%26entry.1292438233%3D%2520%2520The%2520study%2520explores%2520the%2520integration%2520of%2520transfer%2520learning%2520%2528TL%2529%2520with%250Amobile-enabled%2520convolutional%2520neural%2520networks%2520%2528MbNets%2529%2520to%2520enhance%2520Arabic%250AHandwritten%2520Character%2520Recognition%2520%2528AHCR%2529.%2520Addressing%2520challenges%2520like%2520extensive%250Acomputational%2520requirements%2520and%2520dataset%2520scarcity%252C%2520this%2520research%2520evaluates%2520three%250ATL%2520strategies--full%2520fine-tuning%252C%2520partial%2520fine-tuning%252C%2520and%2520training%2520from%250Ascratch--using%2520four%2520lightweight%2520MbNets%253A%2520MobileNet%252C%2520SqueezeNet%252C%2520MnasNet%252C%2520and%250AShuffleNet.%2520Experiments%2520were%2520conducted%2520on%2520three%2520benchmark%2520datasets%253A%2520AHCD%252C%250AHIJJA%252C%2520and%2520IFHCDB.%2520MobileNet%2520emerged%2520as%2520the%2520top-performing%2520model%252C%2520consistently%250Aachieving%2520superior%2520accuracy%252C%2520robustness%252C%2520and%2520efficiency%252C%2520with%2520ShuffleNet%250Aexcelling%2520in%2520generalization%252C%2520particularly%2520under%2520full%2520fine-tuning.%2520The%2520IFHCDB%250Adataset%2520yielded%2520the%2520highest%2520results%252C%2520with%252099%2525%2520accuracy%2520using%2520MnasNet%2520under%2520full%250Afine-tuning%252C%2520highlighting%2520its%2520suitability%2520for%2520robust%2520character%2520recognition.%2520The%250AAHCD%2520dataset%2520achieved%2520competitive%2520accuracy%2520%252897%2525%2529%2520with%2520ShuffleNet%252C%2520while%2520HIJJA%250Aposed%2520significant%2520challenges%2520due%2520to%2520its%2520variability%252C%2520achieving%2520a%2520peak%2520accuracy%250Aof%252092%2525%2520with%2520ShuffleNet.%2520Notably%252C%2520full%2520fine-tuning%2520demonstrated%2520the%2520best%2520overall%250Aperformance%252C%2520balancing%2520accuracy%2520and%2520convergence%2520speed%252C%2520while%2520partial%250Afine-tuning%2520underperformed%2520across%2520metrics.%2520These%2520findings%2520underscore%2520the%250Apotential%2520of%2520combining%2520TL%2520and%2520MbNets%2520for%2520resource-efficient%2520AHCR%252C%2520paving%2520the%250Away%2520for%2520further%2520optimizations%2520and%2520broader%2520applications.%2520Future%2520work%2520will%250Aexplore%2520architectural%2520modifications%252C%2520in-depth%2520dataset%2520feature%2520analysis%252C%2520data%250Aaugmentation%252C%2520and%2520advanced%2520sensitivity%2520analysis%2520to%2520enhance%2520model%2520robustness%2520and%250Ageneralizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Transfer%20Learning%20and%20Mobile-enabled%20Convolutional%20Neural%0A%20%20Networks%20for%20Improved%20Arabic%20Handwritten%20Character%20Recognition&entry.906535625=Mohsine%20El%20Khayati%20and%20Ayyad%20Maafiri%20and%20Yassine%20Himeur%20and%20Hamzah%20Ali%20Alkhazaleh%20and%20Shadi%20Atalla%20and%20Wathiq%20Mansoor&entry.1292438233=%20%20The%20study%20explores%20the%20integration%20of%20transfer%20learning%20%28TL%29%20with%0Amobile-enabled%20convolutional%20neural%20networks%20%28MbNets%29%20to%20enhance%20Arabic%0AHandwritten%20Character%20Recognition%20%28AHCR%29.%20Addressing%20challenges%20like%20extensive%0Acomputational%20requirements%20and%20dataset%20scarcity%2C%20this%20research%20evaluates%20three%0ATL%20strategies--full%20fine-tuning%2C%20partial%20fine-tuning%2C%20and%20training%20from%0Ascratch--using%20four%20lightweight%20MbNets%3A%20MobileNet%2C%20SqueezeNet%2C%20MnasNet%2C%20and%0AShuffleNet.%20Experiments%20were%20conducted%20on%20three%20benchmark%20datasets%3A%20AHCD%2C%0AHIJJA%2C%20and%20IFHCDB.%20MobileNet%20emerged%20as%20the%20top-performing%20model%2C%20consistently%0Aachieving%20superior%20accuracy%2C%20robustness%2C%20and%20efficiency%2C%20with%20ShuffleNet%0Aexcelling%20in%20generalization%2C%20particularly%20under%20full%20fine-tuning.%20The%20IFHCDB%0Adataset%20yielded%20the%20highest%20results%2C%20with%2099%25%20accuracy%20using%20MnasNet%20under%20full%0Afine-tuning%2C%20highlighting%20its%20suitability%20for%20robust%20character%20recognition.%20The%0AAHCD%20dataset%20achieved%20competitive%20accuracy%20%2897%25%29%20with%20ShuffleNet%2C%20while%20HIJJA%0Aposed%20significant%20challenges%20due%20to%20its%20variability%2C%20achieving%20a%20peak%20accuracy%0Aof%2092%25%20with%20ShuffleNet.%20Notably%2C%20full%20fine-tuning%20demonstrated%20the%20best%20overall%0Aperformance%2C%20balancing%20accuracy%20and%20convergence%20speed%2C%20while%20partial%0Afine-tuning%20underperformed%20across%20metrics.%20These%20findings%20underscore%20the%0Apotential%20of%20combining%20TL%20and%20MbNets%20for%20resource-efficient%20AHCR%2C%20paving%20the%0Away%20for%20further%20optimizations%20and%20broader%20applications.%20Future%20work%20will%0Aexplore%20architectural%20modifications%2C%20in-depth%20dataset%20feature%20analysis%2C%20data%0Aaugmentation%2C%20and%20advanced%20sensitivity%20analysis%20to%20enhance%20model%20robustness%20and%0Ageneralizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05019v1&entry.124074799=Read"},
{"title": "An Efficient Subspace Algorithm for Federated Learning on Heterogeneous\n  Data", "author": "Jiaojiao Zhang and Yuqi Xu and Kun Yuan", "abstract": "  This work addresses the key challenges of applying federated learning to\nlarge-scale deep neural networks, particularly the issue of client drift due to\ndata heterogeneity across clients and the high costs of communication,\ncomputation, and memory. We propose FedSub, an efficient subspace algorithm for\nfederated learning on heterogeneous data. Specifically, FedSub utilizes\nsubspace projection to guarantee local updates of each client within\nlow-dimensional subspaces, thereby reducing communication, computation, and\nmemory costs. Additionally, it incorporates low-dimensional dual variables to\nmitigate client drift. We provide convergence analysis that reveals the impact\nof key factors such as step size and subspace projection matrices on\nconvergence. Experimental results demonstrate its efficiency.\n", "link": "http://arxiv.org/abs/2509.05213v1", "date": "2025-09-05", "relevancy": 2.3492, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4749}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4712}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Subspace%20Algorithm%20for%20Federated%20Learning%20on%20Heterogeneous%0A%20%20Data&body=Title%3A%20An%20Efficient%20Subspace%20Algorithm%20for%20Federated%20Learning%20on%20Heterogeneous%0A%20%20Data%0AAuthor%3A%20Jiaojiao%20Zhang%20and%20Yuqi%20Xu%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20key%20challenges%20of%20applying%20federated%20learning%20to%0Alarge-scale%20deep%20neural%20networks%2C%20particularly%20the%20issue%20of%20client%20drift%20due%20to%0Adata%20heterogeneity%20across%20clients%20and%20the%20high%20costs%20of%20communication%2C%0Acomputation%2C%20and%20memory.%20We%20propose%20FedSub%2C%20an%20efficient%20subspace%20algorithm%20for%0Afederated%20learning%20on%20heterogeneous%20data.%20Specifically%2C%20FedSub%20utilizes%0Asubspace%20projection%20to%20guarantee%20local%20updates%20of%20each%20client%20within%0Alow-dimensional%20subspaces%2C%20thereby%20reducing%20communication%2C%20computation%2C%20and%0Amemory%20costs.%20Additionally%2C%20it%20incorporates%20low-dimensional%20dual%20variables%20to%0Amitigate%20client%20drift.%20We%20provide%20convergence%20analysis%20that%20reveals%20the%20impact%0Aof%20key%20factors%20such%20as%20step%20size%20and%20subspace%20projection%20matrices%20on%0Aconvergence.%20Experimental%20results%20demonstrate%20its%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Subspace%2520Algorithm%2520for%2520Federated%2520Learning%2520on%2520Heterogeneous%250A%2520%2520Data%26entry.906535625%3DJiaojiao%2520Zhang%2520and%2520Yuqi%2520Xu%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520key%2520challenges%2520of%2520applying%2520federated%2520learning%2520to%250Alarge-scale%2520deep%2520neural%2520networks%252C%2520particularly%2520the%2520issue%2520of%2520client%2520drift%2520due%2520to%250Adata%2520heterogeneity%2520across%2520clients%2520and%2520the%2520high%2520costs%2520of%2520communication%252C%250Acomputation%252C%2520and%2520memory.%2520We%2520propose%2520FedSub%252C%2520an%2520efficient%2520subspace%2520algorithm%2520for%250Afederated%2520learning%2520on%2520heterogeneous%2520data.%2520Specifically%252C%2520FedSub%2520utilizes%250Asubspace%2520projection%2520to%2520guarantee%2520local%2520updates%2520of%2520each%2520client%2520within%250Alow-dimensional%2520subspaces%252C%2520thereby%2520reducing%2520communication%252C%2520computation%252C%2520and%250Amemory%2520costs.%2520Additionally%252C%2520it%2520incorporates%2520low-dimensional%2520dual%2520variables%2520to%250Amitigate%2520client%2520drift.%2520We%2520provide%2520convergence%2520analysis%2520that%2520reveals%2520the%2520impact%250Aof%2520key%2520factors%2520such%2520as%2520step%2520size%2520and%2520subspace%2520projection%2520matrices%2520on%250Aconvergence.%2520Experimental%2520results%2520demonstrate%2520its%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Subspace%20Algorithm%20for%20Federated%20Learning%20on%20Heterogeneous%0A%20%20Data&entry.906535625=Jiaojiao%20Zhang%20and%20Yuqi%20Xu%20and%20Kun%20Yuan&entry.1292438233=%20%20This%20work%20addresses%20the%20key%20challenges%20of%20applying%20federated%20learning%20to%0Alarge-scale%20deep%20neural%20networks%2C%20particularly%20the%20issue%20of%20client%20drift%20due%20to%0Adata%20heterogeneity%20across%20clients%20and%20the%20high%20costs%20of%20communication%2C%0Acomputation%2C%20and%20memory.%20We%20propose%20FedSub%2C%20an%20efficient%20subspace%20algorithm%20for%0Afederated%20learning%20on%20heterogeneous%20data.%20Specifically%2C%20FedSub%20utilizes%0Asubspace%20projection%20to%20guarantee%20local%20updates%20of%20each%20client%20within%0Alow-dimensional%20subspaces%2C%20thereby%20reducing%20communication%2C%20computation%2C%20and%0Amemory%20costs.%20Additionally%2C%20it%20incorporates%20low-dimensional%20dual%20variables%20to%0Amitigate%20client%20drift.%20We%20provide%20convergence%20analysis%20that%20reveals%20the%20impact%0Aof%20key%20factors%20such%20as%20step%20size%20and%20subspace%20projection%20matrices%20on%0Aconvergence.%20Experimental%20results%20demonstrate%20its%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05213v1&entry.124074799=Read"},
{"title": "Should We Always Train Models on Fine-Grained Classes?", "author": "Davide Pirovano and Federico Milanesio and Michele Caselle and Piero Fariselli and Matteo Osella", "abstract": "  In classification problems, models must predict a class label based on the\ninput data features. However, class labels are organized hierarchically in many\ndatasets. While a classification task is often defined at a specific level of\nthis hierarchy, training can utilize a finer granularity of labels. Empirical\nevidence suggests that such fine-grained training can enhance performance. In\nthis work, we investigate the generality of this observation and explore its\nunderlying causes using both real and synthetic datasets. We show that training\non fine-grained labels does not universally improve classification accuracy.\nInstead, the effectiveness of this strategy depends critically on the geometric\nstructure of the data and its relations with the label hierarchy. Additionally,\nfactors such as dataset size and model capacity significantly influence whether\nfine-grained labels provide a performance benefit.\n", "link": "http://arxiv.org/abs/2509.05130v1", "date": "2025-09-05", "relevancy": 2.3467, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4717}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4691}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Should%20We%20Always%20Train%20Models%20on%20Fine-Grained%20Classes%3F&body=Title%3A%20Should%20We%20Always%20Train%20Models%20on%20Fine-Grained%20Classes%3F%0AAuthor%3A%20Davide%20Pirovano%20and%20Federico%20Milanesio%20and%20Michele%20Caselle%20and%20Piero%20Fariselli%20and%20Matteo%20Osella%0AAbstract%3A%20%20%20In%20classification%20problems%2C%20models%20must%20predict%20a%20class%20label%20based%20on%20the%0Ainput%20data%20features.%20However%2C%20class%20labels%20are%20organized%20hierarchically%20in%20many%0Adatasets.%20While%20a%20classification%20task%20is%20often%20defined%20at%20a%20specific%20level%20of%0Athis%20hierarchy%2C%20training%20can%20utilize%20a%20finer%20granularity%20of%20labels.%20Empirical%0Aevidence%20suggests%20that%20such%20fine-grained%20training%20can%20enhance%20performance.%20In%0Athis%20work%2C%20we%20investigate%20the%20generality%20of%20this%20observation%20and%20explore%20its%0Aunderlying%20causes%20using%20both%20real%20and%20synthetic%20datasets.%20We%20show%20that%20training%0Aon%20fine-grained%20labels%20does%20not%20universally%20improve%20classification%20accuracy.%0AInstead%2C%20the%20effectiveness%20of%20this%20strategy%20depends%20critically%20on%20the%20geometric%0Astructure%20of%20the%20data%20and%20its%20relations%20with%20the%20label%20hierarchy.%20Additionally%2C%0Afactors%20such%20as%20dataset%20size%20and%20model%20capacity%20significantly%20influence%20whether%0Afine-grained%20labels%20provide%20a%20performance%20benefit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShould%2520We%2520Always%2520Train%2520Models%2520on%2520Fine-Grained%2520Classes%253F%26entry.906535625%3DDavide%2520Pirovano%2520and%2520Federico%2520Milanesio%2520and%2520Michele%2520Caselle%2520and%2520Piero%2520Fariselli%2520and%2520Matteo%2520Osella%26entry.1292438233%3D%2520%2520In%2520classification%2520problems%252C%2520models%2520must%2520predict%2520a%2520class%2520label%2520based%2520on%2520the%250Ainput%2520data%2520features.%2520However%252C%2520class%2520labels%2520are%2520organized%2520hierarchically%2520in%2520many%250Adatasets.%2520While%2520a%2520classification%2520task%2520is%2520often%2520defined%2520at%2520a%2520specific%2520level%2520of%250Athis%2520hierarchy%252C%2520training%2520can%2520utilize%2520a%2520finer%2520granularity%2520of%2520labels.%2520Empirical%250Aevidence%2520suggests%2520that%2520such%2520fine-grained%2520training%2520can%2520enhance%2520performance.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520the%2520generality%2520of%2520this%2520observation%2520and%2520explore%2520its%250Aunderlying%2520causes%2520using%2520both%2520real%2520and%2520synthetic%2520datasets.%2520We%2520show%2520that%2520training%250Aon%2520fine-grained%2520labels%2520does%2520not%2520universally%2520improve%2520classification%2520accuracy.%250AInstead%252C%2520the%2520effectiveness%2520of%2520this%2520strategy%2520depends%2520critically%2520on%2520the%2520geometric%250Astructure%2520of%2520the%2520data%2520and%2520its%2520relations%2520with%2520the%2520label%2520hierarchy.%2520Additionally%252C%250Afactors%2520such%2520as%2520dataset%2520size%2520and%2520model%2520capacity%2520significantly%2520influence%2520whether%250Afine-grained%2520labels%2520provide%2520a%2520performance%2520benefit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Should%20We%20Always%20Train%20Models%20on%20Fine-Grained%20Classes%3F&entry.906535625=Davide%20Pirovano%20and%20Federico%20Milanesio%20and%20Michele%20Caselle%20and%20Piero%20Fariselli%20and%20Matteo%20Osella&entry.1292438233=%20%20In%20classification%20problems%2C%20models%20must%20predict%20a%20class%20label%20based%20on%20the%0Ainput%20data%20features.%20However%2C%20class%20labels%20are%20organized%20hierarchically%20in%20many%0Adatasets.%20While%20a%20classification%20task%20is%20often%20defined%20at%20a%20specific%20level%20of%0Athis%20hierarchy%2C%20training%20can%20utilize%20a%20finer%20granularity%20of%20labels.%20Empirical%0Aevidence%20suggests%20that%20such%20fine-grained%20training%20can%20enhance%20performance.%20In%0Athis%20work%2C%20we%20investigate%20the%20generality%20of%20this%20observation%20and%20explore%20its%0Aunderlying%20causes%20using%20both%20real%20and%20synthetic%20datasets.%20We%20show%20that%20training%0Aon%20fine-grained%20labels%20does%20not%20universally%20improve%20classification%20accuracy.%0AInstead%2C%20the%20effectiveness%20of%20this%20strategy%20depends%20critically%20on%20the%20geometric%0Astructure%20of%20the%20data%20and%20its%20relations%20with%20the%20label%20hierarchy.%20Additionally%2C%0Afactors%20such%20as%20dataset%20size%20and%20model%20capacity%20significantly%20influence%20whether%0Afine-grained%20labels%20provide%20a%20performance%20benefit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05130v1&entry.124074799=Read"},
{"title": "Robust Model Predictive Control Design for Autonomous Vehicles with\n  Perception-based Observers", "author": "Nariman Niknejad and Gokul S. Sankar and Bahare Kiumarsi and Hamidreza Modares", "abstract": "  This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.\n", "link": "http://arxiv.org/abs/2509.05201v1", "date": "2025-09-05", "relevancy": 2.3359, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6271}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6127}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Model%20Predictive%20Control%20Design%20for%20Autonomous%20Vehicles%20with%0A%20%20Perception-based%20Observers&body=Title%3A%20Robust%20Model%20Predictive%20Control%20Design%20for%20Autonomous%20Vehicles%20with%0A%20%20Perception-based%20Observers%0AAuthor%3A%20Nariman%20Niknejad%20and%20Gokul%20S.%20Sankar%20and%20Bahare%20Kiumarsi%20and%20Hamidreza%20Modares%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20robust%20model%20predictive%20control%20%28MPC%29%20framework%20that%0Aexplicitly%20addresses%20the%20non-Gaussian%20noise%20inherent%20in%20deep%20learning-based%0Aperception%20modules%20used%20for%20state%20estimation.%20Recognizing%20that%20accurate%0Auncertainty%20quantification%20of%20the%20perception%20module%20is%20essential%20for%20safe%0Afeedback%20control%2C%20our%20approach%20departs%20from%20the%20conventional%20assumption%20of%0Azero-mean%20noise%20quantification%20of%20the%20perception%20error.%20Instead%2C%20it%20employs%0Aset-based%20state%20estimation%20with%20constrained%20zonotopes%20to%20capture%20biased%2C%0Aheavy-tailed%20uncertainties%20while%20maintaining%20bounded%20estimation%20errors.%20To%0Aimprove%20computational%20efficiency%2C%20the%20robust%20MPC%20is%20reformulated%20as%20a%20linear%0Aprogram%20%28LP%29%2C%20using%20a%20Minkowski-Lyapunov-based%20cost%20function%20with%20an%20added%0Aslack%20variable%20to%20prevent%20degenerate%20solutions.%20Closed-loop%20stability%20is%0Aensured%20through%20Minkowski-Lyapunov%20inequalities%20and%20contractive%20zonotopic%0Ainvariant%20sets.%20The%20largest%20stabilizing%20terminal%20set%20and%20its%20corresponding%0Afeedback%20gain%20are%20then%20derived%20via%20an%20ellipsoidal%20approximation%20of%20the%0Azonotopes.%20The%20proposed%20framework%20is%20validated%20through%20both%20simulations%20and%0Ahardware%20experiments%20on%20an%20omnidirectional%20mobile%20robot%20along%20with%20a%20camera%20and%0Aa%20convolutional%20neural%20network-based%20perception%20module%20implemented%20within%20a%0AROS2%20framework.%20The%20results%20demonstrate%20that%20the%20perception-aware%20MPC%20provides%0Astable%20and%20accurate%20control%20performance%20under%20heavy-tailed%20noise%20conditions%2C%0Asignificantly%20outperforming%20traditional%20Gaussian-noise-based%20designs%20in%20terms%0Aof%20both%20state%20estimation%20error%20bounding%20and%20overall%20control%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Model%2520Predictive%2520Control%2520Design%2520for%2520Autonomous%2520Vehicles%2520with%250A%2520%2520Perception-based%2520Observers%26entry.906535625%3DNariman%2520Niknejad%2520and%2520Gokul%2520S.%2520Sankar%2520and%2520Bahare%2520Kiumarsi%2520and%2520Hamidreza%2520Modares%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520robust%2520model%2520predictive%2520control%2520%2528MPC%2529%2520framework%2520that%250Aexplicitly%2520addresses%2520the%2520non-Gaussian%2520noise%2520inherent%2520in%2520deep%2520learning-based%250Aperception%2520modules%2520used%2520for%2520state%2520estimation.%2520Recognizing%2520that%2520accurate%250Auncertainty%2520quantification%2520of%2520the%2520perception%2520module%2520is%2520essential%2520for%2520safe%250Afeedback%2520control%252C%2520our%2520approach%2520departs%2520from%2520the%2520conventional%2520assumption%2520of%250Azero-mean%2520noise%2520quantification%2520of%2520the%2520perception%2520error.%2520Instead%252C%2520it%2520employs%250Aset-based%2520state%2520estimation%2520with%2520constrained%2520zonotopes%2520to%2520capture%2520biased%252C%250Aheavy-tailed%2520uncertainties%2520while%2520maintaining%2520bounded%2520estimation%2520errors.%2520To%250Aimprove%2520computational%2520efficiency%252C%2520the%2520robust%2520MPC%2520is%2520reformulated%2520as%2520a%2520linear%250Aprogram%2520%2528LP%2529%252C%2520using%2520a%2520Minkowski-Lyapunov-based%2520cost%2520function%2520with%2520an%2520added%250Aslack%2520variable%2520to%2520prevent%2520degenerate%2520solutions.%2520Closed-loop%2520stability%2520is%250Aensured%2520through%2520Minkowski-Lyapunov%2520inequalities%2520and%2520contractive%2520zonotopic%250Ainvariant%2520sets.%2520The%2520largest%2520stabilizing%2520terminal%2520set%2520and%2520its%2520corresponding%250Afeedback%2520gain%2520are%2520then%2520derived%2520via%2520an%2520ellipsoidal%2520approximation%2520of%2520the%250Azonotopes.%2520The%2520proposed%2520framework%2520is%2520validated%2520through%2520both%2520simulations%2520and%250Ahardware%2520experiments%2520on%2520an%2520omnidirectional%2520mobile%2520robot%2520along%2520with%2520a%2520camera%2520and%250Aa%2520convolutional%2520neural%2520network-based%2520perception%2520module%2520implemented%2520within%2520a%250AROS2%2520framework.%2520The%2520results%2520demonstrate%2520that%2520the%2520perception-aware%2520MPC%2520provides%250Astable%2520and%2520accurate%2520control%2520performance%2520under%2520heavy-tailed%2520noise%2520conditions%252C%250Asignificantly%2520outperforming%2520traditional%2520Gaussian-noise-based%2520designs%2520in%2520terms%250Aof%2520both%2520state%2520estimation%2520error%2520bounding%2520and%2520overall%2520control%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Model%20Predictive%20Control%20Design%20for%20Autonomous%20Vehicles%20with%0A%20%20Perception-based%20Observers&entry.906535625=Nariman%20Niknejad%20and%20Gokul%20S.%20Sankar%20and%20Bahare%20Kiumarsi%20and%20Hamidreza%20Modares&entry.1292438233=%20%20This%20paper%20presents%20a%20robust%20model%20predictive%20control%20%28MPC%29%20framework%20that%0Aexplicitly%20addresses%20the%20non-Gaussian%20noise%20inherent%20in%20deep%20learning-based%0Aperception%20modules%20used%20for%20state%20estimation.%20Recognizing%20that%20accurate%0Auncertainty%20quantification%20of%20the%20perception%20module%20is%20essential%20for%20safe%0Afeedback%20control%2C%20our%20approach%20departs%20from%20the%20conventional%20assumption%20of%0Azero-mean%20noise%20quantification%20of%20the%20perception%20error.%20Instead%2C%20it%20employs%0Aset-based%20state%20estimation%20with%20constrained%20zonotopes%20to%20capture%20biased%2C%0Aheavy-tailed%20uncertainties%20while%20maintaining%20bounded%20estimation%20errors.%20To%0Aimprove%20computational%20efficiency%2C%20the%20robust%20MPC%20is%20reformulated%20as%20a%20linear%0Aprogram%20%28LP%29%2C%20using%20a%20Minkowski-Lyapunov-based%20cost%20function%20with%20an%20added%0Aslack%20variable%20to%20prevent%20degenerate%20solutions.%20Closed-loop%20stability%20is%0Aensured%20through%20Minkowski-Lyapunov%20inequalities%20and%20contractive%20zonotopic%0Ainvariant%20sets.%20The%20largest%20stabilizing%20terminal%20set%20and%20its%20corresponding%0Afeedback%20gain%20are%20then%20derived%20via%20an%20ellipsoidal%20approximation%20of%20the%0Azonotopes.%20The%20proposed%20framework%20is%20validated%20through%20both%20simulations%20and%0Ahardware%20experiments%20on%20an%20omnidirectional%20mobile%20robot%20along%20with%20a%20camera%20and%0Aa%20convolutional%20neural%20network-based%20perception%20module%20implemented%20within%20a%0AROS2%20framework.%20The%20results%20demonstrate%20that%20the%20perception-aware%20MPC%20provides%0Astable%20and%20accurate%20control%20performance%20under%20heavy-tailed%20noise%20conditions%2C%0Asignificantly%20outperforming%20traditional%20Gaussian-noise-based%20designs%20in%20terms%0Aof%20both%20state%20estimation%20error%20bounding%20and%20overall%20control%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05201v1&entry.124074799=Read"},
{"title": "Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based\n  Seizure Detection", "author": "Mounvik K and N Harshit", "abstract": "  Deep learning models, especially convolutional neural networks (CNNs), have\nshown considerable promise for biomedical signals such as EEG-based seizure\ndetection. However, these models come with challenges, primarily due to their\nsize and compute requirements in environments where real-time detection or\nlimited resources are available. In this study, we present a lightweight\none-dimensional CNN model with structured pruning to improve efficiency and\nreliability. The model was trained with mild early stopping to address possible\noverfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.\nStructured pruning of the baseline CNN involved removing 50% of the\nconvolutional kernels based on their importance to model predictions.\nSurprisingly, after pruning the weights and memory by 50%, the new network was\nstill able to maintain predictive capabilities, while modestly increasing\nprecision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we\npresent a convincing case that structured pruning removes redundancy, improves\ngeneralization, and, in combination with mild early stopping, achieves a\npromising way forward to improve seizure detection efficiency and reliability,\nwhich is clear motivation for resource-limited settings.\n", "link": "http://arxiv.org/abs/2509.05190v1", "date": "2025-09-05", "relevancy": 2.326, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4849}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4561}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy-Constrained%20CNN%20Pruning%20for%20Efficient%20and%20Reliable%20EEG-Based%0A%20%20Seizure%20Detection&body=Title%3A%20Accuracy-Constrained%20CNN%20Pruning%20for%20Efficient%20and%20Reliable%20EEG-Based%0A%20%20Seizure%20Detection%0AAuthor%3A%20Mounvik%20K%20and%20N%20Harshit%0AAbstract%3A%20%20%20Deep%20learning%20models%2C%20especially%20convolutional%20neural%20networks%20%28CNNs%29%2C%20have%0Ashown%20considerable%20promise%20for%20biomedical%20signals%20such%20as%20EEG-based%20seizure%0Adetection.%20However%2C%20these%20models%20come%20with%20challenges%2C%20primarily%20due%20to%20their%0Asize%20and%20compute%20requirements%20in%20environments%20where%20real-time%20detection%20or%0Alimited%20resources%20are%20available.%20In%20this%20study%2C%20we%20present%20a%20lightweight%0Aone-dimensional%20CNN%20model%20with%20structured%20pruning%20to%20improve%20efficiency%20and%0Areliability.%20The%20model%20was%20trained%20with%20mild%20early%20stopping%20to%20address%20possible%0Aoverfitting%2C%20achieving%20an%20accuracy%20of%2092.78%25%20and%20a%20macro-F1%20score%20of%200.8686.%0AStructured%20pruning%20of%20the%20baseline%20CNN%20involved%20removing%2050%25%20of%20the%0Aconvolutional%20kernels%20based%20on%20their%20importance%20to%20model%20predictions.%0ASurprisingly%2C%20after%20pruning%20the%20weights%20and%20memory%20by%2050%25%2C%20the%20new%20network%20was%0Astill%20able%20to%20maintain%20predictive%20capabilities%2C%20while%20modestly%20increasing%0Aprecision%20to%2092.87%25%20and%20improving%20the%20macro-F1%20score%20to%200.8707.%20Overall%2C%20we%0Apresent%20a%20convincing%20case%20that%20structured%20pruning%20removes%20redundancy%2C%20improves%0Ageneralization%2C%20and%2C%20in%20combination%20with%20mild%20early%20stopping%2C%20achieves%20a%0Apromising%20way%20forward%20to%20improve%20seizure%20detection%20efficiency%20and%20reliability%2C%0Awhich%20is%20clear%20motivation%20for%20resource-limited%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy-Constrained%2520CNN%2520Pruning%2520for%2520Efficient%2520and%2520Reliable%2520EEG-Based%250A%2520%2520Seizure%2520Detection%26entry.906535625%3DMounvik%2520K%2520and%2520N%2520Harshit%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%252C%2520especially%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520have%250Ashown%2520considerable%2520promise%2520for%2520biomedical%2520signals%2520such%2520as%2520EEG-based%2520seizure%250Adetection.%2520However%252C%2520these%2520models%2520come%2520with%2520challenges%252C%2520primarily%2520due%2520to%2520their%250Asize%2520and%2520compute%2520requirements%2520in%2520environments%2520where%2520real-time%2520detection%2520or%250Alimited%2520resources%2520are%2520available.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520lightweight%250Aone-dimensional%2520CNN%2520model%2520with%2520structured%2520pruning%2520to%2520improve%2520efficiency%2520and%250Areliability.%2520The%2520model%2520was%2520trained%2520with%2520mild%2520early%2520stopping%2520to%2520address%2520possible%250Aoverfitting%252C%2520achieving%2520an%2520accuracy%2520of%252092.78%2525%2520and%2520a%2520macro-F1%2520score%2520of%25200.8686.%250AStructured%2520pruning%2520of%2520the%2520baseline%2520CNN%2520involved%2520removing%252050%2525%2520of%2520the%250Aconvolutional%2520kernels%2520based%2520on%2520their%2520importance%2520to%2520model%2520predictions.%250ASurprisingly%252C%2520after%2520pruning%2520the%2520weights%2520and%2520memory%2520by%252050%2525%252C%2520the%2520new%2520network%2520was%250Astill%2520able%2520to%2520maintain%2520predictive%2520capabilities%252C%2520while%2520modestly%2520increasing%250Aprecision%2520to%252092.87%2525%2520and%2520improving%2520the%2520macro-F1%2520score%2520to%25200.8707.%2520Overall%252C%2520we%250Apresent%2520a%2520convincing%2520case%2520that%2520structured%2520pruning%2520removes%2520redundancy%252C%2520improves%250Ageneralization%252C%2520and%252C%2520in%2520combination%2520with%2520mild%2520early%2520stopping%252C%2520achieves%2520a%250Apromising%2520way%2520forward%2520to%2520improve%2520seizure%2520detection%2520efficiency%2520and%2520reliability%252C%250Awhich%2520is%2520clear%2520motivation%2520for%2520resource-limited%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy-Constrained%20CNN%20Pruning%20for%20Efficient%20and%20Reliable%20EEG-Based%0A%20%20Seizure%20Detection&entry.906535625=Mounvik%20K%20and%20N%20Harshit&entry.1292438233=%20%20Deep%20learning%20models%2C%20especially%20convolutional%20neural%20networks%20%28CNNs%29%2C%20have%0Ashown%20considerable%20promise%20for%20biomedical%20signals%20such%20as%20EEG-based%20seizure%0Adetection.%20However%2C%20these%20models%20come%20with%20challenges%2C%20primarily%20due%20to%20their%0Asize%20and%20compute%20requirements%20in%20environments%20where%20real-time%20detection%20or%0Alimited%20resources%20are%20available.%20In%20this%20study%2C%20we%20present%20a%20lightweight%0Aone-dimensional%20CNN%20model%20with%20structured%20pruning%20to%20improve%20efficiency%20and%0Areliability.%20The%20model%20was%20trained%20with%20mild%20early%20stopping%20to%20address%20possible%0Aoverfitting%2C%20achieving%20an%20accuracy%20of%2092.78%25%20and%20a%20macro-F1%20score%20of%200.8686.%0AStructured%20pruning%20of%20the%20baseline%20CNN%20involved%20removing%2050%25%20of%20the%0Aconvolutional%20kernels%20based%20on%20their%20importance%20to%20model%20predictions.%0ASurprisingly%2C%20after%20pruning%20the%20weights%20and%20memory%20by%2050%25%2C%20the%20new%20network%20was%0Astill%20able%20to%20maintain%20predictive%20capabilities%2C%20while%20modestly%20increasing%0Aprecision%20to%2092.87%25%20and%20improving%20the%20macro-F1%20score%20to%200.8707.%20Overall%2C%20we%0Apresent%20a%20convincing%20case%20that%20structured%20pruning%20removes%20redundancy%2C%20improves%0Ageneralization%2C%20and%2C%20in%20combination%20with%20mild%20early%20stopping%2C%20achieves%20a%0Apromising%20way%20forward%20to%20improve%20seizure%20detection%20efficiency%20and%20reliability%2C%0Awhich%20is%20clear%20motivation%20for%20resource-limited%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05190v1&entry.124074799=Read"},
{"title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and\n  Interpretability in Manipulation", "author": "Tien Pham and Xinyun Chi and Khang Nguyen and Manfred Huber and Angelo Cangelosi", "abstract": "  Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input\n", "link": "http://arxiv.org/abs/2509.04970v1", "date": "2025-09-05", "relevancy": 2.3126, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5993}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeGuV%3A%20Depth-Guided%20Visual%20Reinforcement%20Learning%20for%20Generalization%20and%0A%20%20Interpretability%20in%20Manipulation&body=Title%3A%20DeGuV%3A%20Depth-Guided%20Visual%20Reinforcement%20Learning%20for%20Generalization%20and%0A%20%20Interpretability%20in%20Manipulation%0AAuthor%3A%20Tien%20Pham%20and%20Xinyun%20Chi%20and%20Khang%20Nguyen%20and%20Manfred%20Huber%20and%20Angelo%20Cangelosi%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20agents%20can%20learn%20to%20solve%20complex%20tasks%20from%0Avisual%20inputs%2C%20but%20generalizing%20these%20learned%20skills%20to%20new%20environments%0Aremains%20a%20major%20challenge%20in%20RL%20application%2C%20especially%20robotics.%20While%20data%0Aaugmentation%20can%20improve%20generalization%2C%20it%20often%20compromises%20sample%20efficiency%0Aand%20training%20stability.%20This%20paper%20introduces%20DeGuV%2C%20an%20RL%20framework%20that%0Aenhances%20both%20generalization%20and%20sample%20efficiency.%20In%20specific%2C%20we%20leverage%20a%0Alearnable%20masker%20network%20that%20produces%20a%20mask%20from%20the%20depth%20input%2C%20preserving%0Aonly%20critical%20visual%20information%20while%20discarding%20irrelevant%20pixels.%20Through%0Athis%2C%20we%20ensure%20that%20our%20RL%20agents%20focus%20on%20essential%20features%2C%20improving%0Arobustness%20under%20data%20augmentation.%20In%20addition%2C%20we%20incorporate%20contrastive%0Alearning%20and%20stabilize%20Q-value%20estimation%20under%20augmentation%20to%20further%20enhance%0Asample%20efficiency%20and%20training%20stability.%20We%20evaluate%20our%20proposed%20method%20on%0Athe%20RL-ViGen%20benchmark%20using%20the%20Franka%20Emika%20robot%20and%20demonstrate%20its%0Aeffectiveness%20in%20zero-shot%20sim-to-real%20transfer.%20Our%20results%20show%20that%20DeGuV%0Aoutperforms%20state-of-the-art%20methods%20in%20both%20generalization%20and%20sample%0Aefficiency%20while%20also%20improving%20interpretability%20by%20highlighting%20the%20most%0Arelevant%20regions%20in%20the%20visual%20input%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeGuV%253A%2520Depth-Guided%2520Visual%2520Reinforcement%2520Learning%2520for%2520Generalization%2520and%250A%2520%2520Interpretability%2520in%2520Manipulation%26entry.906535625%3DTien%2520Pham%2520and%2520Xinyun%2520Chi%2520and%2520Khang%2520Nguyen%2520and%2520Manfred%2520Huber%2520and%2520Angelo%2520Cangelosi%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520agents%2520can%2520learn%2520to%2520solve%2520complex%2520tasks%2520from%250Avisual%2520inputs%252C%2520but%2520generalizing%2520these%2520learned%2520skills%2520to%2520new%2520environments%250Aremains%2520a%2520major%2520challenge%2520in%2520RL%2520application%252C%2520especially%2520robotics.%2520While%2520data%250Aaugmentation%2520can%2520improve%2520generalization%252C%2520it%2520often%2520compromises%2520sample%2520efficiency%250Aand%2520training%2520stability.%2520This%2520paper%2520introduces%2520DeGuV%252C%2520an%2520RL%2520framework%2520that%250Aenhances%2520both%2520generalization%2520and%2520sample%2520efficiency.%2520In%2520specific%252C%2520we%2520leverage%2520a%250Alearnable%2520masker%2520network%2520that%2520produces%2520a%2520mask%2520from%2520the%2520depth%2520input%252C%2520preserving%250Aonly%2520critical%2520visual%2520information%2520while%2520discarding%2520irrelevant%2520pixels.%2520Through%250Athis%252C%2520we%2520ensure%2520that%2520our%2520RL%2520agents%2520focus%2520on%2520essential%2520features%252C%2520improving%250Arobustness%2520under%2520data%2520augmentation.%2520In%2520addition%252C%2520we%2520incorporate%2520contrastive%250Alearning%2520and%2520stabilize%2520Q-value%2520estimation%2520under%2520augmentation%2520to%2520further%2520enhance%250Asample%2520efficiency%2520and%2520training%2520stability.%2520We%2520evaluate%2520our%2520proposed%2520method%2520on%250Athe%2520RL-ViGen%2520benchmark%2520using%2520the%2520Franka%2520Emika%2520robot%2520and%2520demonstrate%2520its%250Aeffectiveness%2520in%2520zero-shot%2520sim-to-real%2520transfer.%2520Our%2520results%2520show%2520that%2520DeGuV%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520both%2520generalization%2520and%2520sample%250Aefficiency%2520while%2520also%2520improving%2520interpretability%2520by%2520highlighting%2520the%2520most%250Arelevant%2520regions%2520in%2520the%2520visual%2520input%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeGuV%3A%20Depth-Guided%20Visual%20Reinforcement%20Learning%20for%20Generalization%20and%0A%20%20Interpretability%20in%20Manipulation&entry.906535625=Tien%20Pham%20and%20Xinyun%20Chi%20and%20Khang%20Nguyen%20and%20Manfred%20Huber%20and%20Angelo%20Cangelosi&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20agents%20can%20learn%20to%20solve%20complex%20tasks%20from%0Avisual%20inputs%2C%20but%20generalizing%20these%20learned%20skills%20to%20new%20environments%0Aremains%20a%20major%20challenge%20in%20RL%20application%2C%20especially%20robotics.%20While%20data%0Aaugmentation%20can%20improve%20generalization%2C%20it%20often%20compromises%20sample%20efficiency%0Aand%20training%20stability.%20This%20paper%20introduces%20DeGuV%2C%20an%20RL%20framework%20that%0Aenhances%20both%20generalization%20and%20sample%20efficiency.%20In%20specific%2C%20we%20leverage%20a%0Alearnable%20masker%20network%20that%20produces%20a%20mask%20from%20the%20depth%20input%2C%20preserving%0Aonly%20critical%20visual%20information%20while%20discarding%20irrelevant%20pixels.%20Through%0Athis%2C%20we%20ensure%20that%20our%20RL%20agents%20focus%20on%20essential%20features%2C%20improving%0Arobustness%20under%20data%20augmentation.%20In%20addition%2C%20we%20incorporate%20contrastive%0Alearning%20and%20stabilize%20Q-value%20estimation%20under%20augmentation%20to%20further%20enhance%0Asample%20efficiency%20and%20training%20stability.%20We%20evaluate%20our%20proposed%20method%20on%0Athe%20RL-ViGen%20benchmark%20using%20the%20Franka%20Emika%20robot%20and%20demonstrate%20its%0Aeffectiveness%20in%20zero-shot%20sim-to-real%20transfer.%20Our%20results%20show%20that%20DeGuV%0Aoutperforms%20state-of-the-art%20methods%20in%20both%20generalization%20and%20sample%0Aefficiency%20while%20also%20improving%20interpretability%20by%20highlighting%20the%20most%0Arelevant%20regions%20in%20the%20visual%20input%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04970v1&entry.124074799=Read"},
{"title": "A biologically inspired separable learning vision model for real-time\n  traffic object perception in Dark", "author": "Hulin Li and Qiliang Ren and Jun Li and Hanbing Wei and Zheng Liu and Linfang Fan", "abstract": "  Fast and accurate object perception in low-light traffic scenes has attracted\nincreasing attention. However, due to severe illumination degradation and the\nlack of reliable visual cues, existing perception models and methods struggle\nto quickly adapt to and accurately predict in low-light environments. Moreover,\nthere is the absence of available large-scale benchmark specifically focused on\nlow-light traffic scenes. To bridge this gap, we introduce a physically\ngrounded illumination degradation method tailored to real-world low-light\nsettings and construct Dark-traffic, the largest densely annotated dataset to\ndate for low-light traffic scenes, supporting object detection, instance\nsegmentation, and optical flow estimation. We further propose the Separable\nLearning Vision Model (SLVM), a biologically inspired framework designed to\nenhance perception under adverse lighting. SLVM integrates four key components:\na light-adaptive pupillary mechanism for illumination-sensitive feature\nextraction, a feature-level separable learning strategy for efficient\nrepresentation, task-specific decoupled branches for multi-task separable\nlearning, and a spatial misalignment-aware fusion module for precise\nmulti-feature alignment. Extensive experiments demonstrate that SLVM achieves\nstate-of-the-art performance with reduced computational overhead. Notably, it\noutperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1\npercentage points in instance segmentation, and reduces endpoint error (EPE) of\nbaseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end\ntrained SLVM surpasses Swin Transformer+EnlightenGAN and\nConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key\nmetrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage\npoints. The Dark-traffic dataset and complete code is released at\nhttps://github.com/alanli1997/slvm.\n", "link": "http://arxiv.org/abs/2509.05012v1", "date": "2025-09-05", "relevancy": 2.3036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20biologically%20inspired%20separable%20learning%20vision%20model%20for%20real-time%0A%20%20traffic%20object%20perception%20in%20Dark&body=Title%3A%20A%20biologically%20inspired%20separable%20learning%20vision%20model%20for%20real-time%0A%20%20traffic%20object%20perception%20in%20Dark%0AAuthor%3A%20Hulin%20Li%20and%20Qiliang%20Ren%20and%20Jun%20Li%20and%20Hanbing%20Wei%20and%20Zheng%20Liu%20and%20Linfang%20Fan%0AAbstract%3A%20%20%20Fast%20and%20accurate%20object%20perception%20in%20low-light%20traffic%20scenes%20has%20attracted%0Aincreasing%20attention.%20However%2C%20due%20to%20severe%20illumination%20degradation%20and%20the%0Alack%20of%20reliable%20visual%20cues%2C%20existing%20perception%20models%20and%20methods%20struggle%0Ato%20quickly%20adapt%20to%20and%20accurately%20predict%20in%20low-light%20environments.%20Moreover%2C%0Athere%20is%20the%20absence%20of%20available%20large-scale%20benchmark%20specifically%20focused%20on%0Alow-light%20traffic%20scenes.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20physically%0Agrounded%20illumination%20degradation%20method%20tailored%20to%20real-world%20low-light%0Asettings%20and%20construct%20Dark-traffic%2C%20the%20largest%20densely%20annotated%20dataset%20to%0Adate%20for%20low-light%20traffic%20scenes%2C%20supporting%20object%20detection%2C%20instance%0Asegmentation%2C%20and%20optical%20flow%20estimation.%20We%20further%20propose%20the%20Separable%0ALearning%20Vision%20Model%20%28SLVM%29%2C%20a%20biologically%20inspired%20framework%20designed%20to%0Aenhance%20perception%20under%20adverse%20lighting.%20SLVM%20integrates%20four%20key%20components%3A%0Aa%20light-adaptive%20pupillary%20mechanism%20for%20illumination-sensitive%20feature%0Aextraction%2C%20a%20feature-level%20separable%20learning%20strategy%20for%20efficient%0Arepresentation%2C%20task-specific%20decoupled%20branches%20for%20multi-task%20separable%0Alearning%2C%20and%20a%20spatial%20misalignment-aware%20fusion%20module%20for%20precise%0Amulti-feature%20alignment.%20Extensive%20experiments%20demonstrate%20that%20SLVM%20achieves%0Astate-of-the-art%20performance%20with%20reduced%20computational%20overhead.%20Notably%2C%20it%0Aoutperforms%20RT-DETR%20by%2011.2%20percentage%20points%20in%20detection%2C%20YOLOv12%20by%206.1%0Apercentage%20points%20in%20instance%20segmentation%2C%20and%20reduces%20endpoint%20error%20%28EPE%29%20of%0Abaseline%20by%2012.37%25%20on%20Dark-traffic.%20On%20the%20LIS%20benchmark%2C%20the%20end-to-end%0Atrained%20SLVM%20surpasses%20Swin%20Transformer%2BEnlightenGAN%20and%0AConvNeXt-T%2BEnlightenGAN%20by%20an%20average%20of%2011%20percentage%20points%20across%20key%0Ametrics%2C%20and%20exceeds%20Mask%20RCNN%20%28with%20light%20enhancement%29%20by%203.1%20percentage%0Apoints.%20The%20Dark-traffic%20dataset%20and%20complete%20code%20is%20released%20at%0Ahttps%3A//github.com/alanli1997/slvm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520biologically%2520inspired%2520separable%2520learning%2520vision%2520model%2520for%2520real-time%250A%2520%2520traffic%2520object%2520perception%2520in%2520Dark%26entry.906535625%3DHulin%2520Li%2520and%2520Qiliang%2520Ren%2520and%2520Jun%2520Li%2520and%2520Hanbing%2520Wei%2520and%2520Zheng%2520Liu%2520and%2520Linfang%2520Fan%26entry.1292438233%3D%2520%2520Fast%2520and%2520accurate%2520object%2520perception%2520in%2520low-light%2520traffic%2520scenes%2520has%2520attracted%250Aincreasing%2520attention.%2520However%252C%2520due%2520to%2520severe%2520illumination%2520degradation%2520and%2520the%250Alack%2520of%2520reliable%2520visual%2520cues%252C%2520existing%2520perception%2520models%2520and%2520methods%2520struggle%250Ato%2520quickly%2520adapt%2520to%2520and%2520accurately%2520predict%2520in%2520low-light%2520environments.%2520Moreover%252C%250Athere%2520is%2520the%2520absence%2520of%2520available%2520large-scale%2520benchmark%2520specifically%2520focused%2520on%250Alow-light%2520traffic%2520scenes.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520physically%250Agrounded%2520illumination%2520degradation%2520method%2520tailored%2520to%2520real-world%2520low-light%250Asettings%2520and%2520construct%2520Dark-traffic%252C%2520the%2520largest%2520densely%2520annotated%2520dataset%2520to%250Adate%2520for%2520low-light%2520traffic%2520scenes%252C%2520supporting%2520object%2520detection%252C%2520instance%250Asegmentation%252C%2520and%2520optical%2520flow%2520estimation.%2520We%2520further%2520propose%2520the%2520Separable%250ALearning%2520Vision%2520Model%2520%2528SLVM%2529%252C%2520a%2520biologically%2520inspired%2520framework%2520designed%2520to%250Aenhance%2520perception%2520under%2520adverse%2520lighting.%2520SLVM%2520integrates%2520four%2520key%2520components%253A%250Aa%2520light-adaptive%2520pupillary%2520mechanism%2520for%2520illumination-sensitive%2520feature%250Aextraction%252C%2520a%2520feature-level%2520separable%2520learning%2520strategy%2520for%2520efficient%250Arepresentation%252C%2520task-specific%2520decoupled%2520branches%2520for%2520multi-task%2520separable%250Alearning%252C%2520and%2520a%2520spatial%2520misalignment-aware%2520fusion%2520module%2520for%2520precise%250Amulti-feature%2520alignment.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SLVM%2520achieves%250Astate-of-the-art%2520performance%2520with%2520reduced%2520computational%2520overhead.%2520Notably%252C%2520it%250Aoutperforms%2520RT-DETR%2520by%252011.2%2520percentage%2520points%2520in%2520detection%252C%2520YOLOv12%2520by%25206.1%250Apercentage%2520points%2520in%2520instance%2520segmentation%252C%2520and%2520reduces%2520endpoint%2520error%2520%2528EPE%2529%2520of%250Abaseline%2520by%252012.37%2525%2520on%2520Dark-traffic.%2520On%2520the%2520LIS%2520benchmark%252C%2520the%2520end-to-end%250Atrained%2520SLVM%2520surpasses%2520Swin%2520Transformer%252BEnlightenGAN%2520and%250AConvNeXt-T%252BEnlightenGAN%2520by%2520an%2520average%2520of%252011%2520percentage%2520points%2520across%2520key%250Ametrics%252C%2520and%2520exceeds%2520Mask%2520RCNN%2520%2528with%2520light%2520enhancement%2529%2520by%25203.1%2520percentage%250Apoints.%2520The%2520Dark-traffic%2520dataset%2520and%2520complete%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/alanli1997/slvm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20biologically%20inspired%20separable%20learning%20vision%20model%20for%20real-time%0A%20%20traffic%20object%20perception%20in%20Dark&entry.906535625=Hulin%20Li%20and%20Qiliang%20Ren%20and%20Jun%20Li%20and%20Hanbing%20Wei%20and%20Zheng%20Liu%20and%20Linfang%20Fan&entry.1292438233=%20%20Fast%20and%20accurate%20object%20perception%20in%20low-light%20traffic%20scenes%20has%20attracted%0Aincreasing%20attention.%20However%2C%20due%20to%20severe%20illumination%20degradation%20and%20the%0Alack%20of%20reliable%20visual%20cues%2C%20existing%20perception%20models%20and%20methods%20struggle%0Ato%20quickly%20adapt%20to%20and%20accurately%20predict%20in%20low-light%20environments.%20Moreover%2C%0Athere%20is%20the%20absence%20of%20available%20large-scale%20benchmark%20specifically%20focused%20on%0Alow-light%20traffic%20scenes.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20physically%0Agrounded%20illumination%20degradation%20method%20tailored%20to%20real-world%20low-light%0Asettings%20and%20construct%20Dark-traffic%2C%20the%20largest%20densely%20annotated%20dataset%20to%0Adate%20for%20low-light%20traffic%20scenes%2C%20supporting%20object%20detection%2C%20instance%0Asegmentation%2C%20and%20optical%20flow%20estimation.%20We%20further%20propose%20the%20Separable%0ALearning%20Vision%20Model%20%28SLVM%29%2C%20a%20biologically%20inspired%20framework%20designed%20to%0Aenhance%20perception%20under%20adverse%20lighting.%20SLVM%20integrates%20four%20key%20components%3A%0Aa%20light-adaptive%20pupillary%20mechanism%20for%20illumination-sensitive%20feature%0Aextraction%2C%20a%20feature-level%20separable%20learning%20strategy%20for%20efficient%0Arepresentation%2C%20task-specific%20decoupled%20branches%20for%20multi-task%20separable%0Alearning%2C%20and%20a%20spatial%20misalignment-aware%20fusion%20module%20for%20precise%0Amulti-feature%20alignment.%20Extensive%20experiments%20demonstrate%20that%20SLVM%20achieves%0Astate-of-the-art%20performance%20with%20reduced%20computational%20overhead.%20Notably%2C%20it%0Aoutperforms%20RT-DETR%20by%2011.2%20percentage%20points%20in%20detection%2C%20YOLOv12%20by%206.1%0Apercentage%20points%20in%20instance%20segmentation%2C%20and%20reduces%20endpoint%20error%20%28EPE%29%20of%0Abaseline%20by%2012.37%25%20on%20Dark-traffic.%20On%20the%20LIS%20benchmark%2C%20the%20end-to-end%0Atrained%20SLVM%20surpasses%20Swin%20Transformer%2BEnlightenGAN%20and%0AConvNeXt-T%2BEnlightenGAN%20by%20an%20average%20of%2011%20percentage%20points%20across%20key%0Ametrics%2C%20and%20exceeds%20Mask%20RCNN%20%28with%20light%20enhancement%29%20by%203.1%20percentage%0Apoints.%20The%20Dark-traffic%20dataset%20and%20complete%20code%20is%20released%20at%0Ahttps%3A//github.com/alanli1997/slvm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05012v1&entry.124074799=Read"},
{"title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool", "author": "Zizun Li and Jianjun Zhou and Yifan Wang and Haoyu Guo and Wenzheng Chang and Yang Zhou and Haoyi Zhu and Junyi Chen and Chunhua Shen and Tong He", "abstract": "  We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.\n", "link": "http://arxiv.org/abs/2509.05296v1", "date": "2025-09-05", "relevancy": 2.2963, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6009}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WinT3R%3A%20Window-Based%20Streaming%20Reconstruction%20with%20Camera%20Token%20Pool&body=Title%3A%20WinT3R%3A%20Window-Based%20Streaming%20Reconstruction%20with%20Camera%20Token%20Pool%0AAuthor%3A%20Zizun%20Li%20and%20Jianjun%20Zhou%20and%20Yifan%20Wang%20and%20Haoyu%20Guo%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Haoyi%20Zhu%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Tong%20He%0AAbstract%3A%20%20%20We%20present%20WinT3R%2C%20a%20feed-forward%20reconstruction%20model%20capable%20of%20online%0Aprediction%20of%20precise%20camera%20poses%20and%20high-quality%20point%20maps.%20Previous%0Amethods%20suffer%20from%20a%20trade-off%20between%20reconstruction%20quality%20and%20real-time%0Aperformance.%20To%20address%20this%2C%20we%20first%20introduce%20a%20sliding%20window%20mechanism%0Athat%20ensures%20sufficient%20information%20exchange%20among%20frames%20within%20the%20window%2C%0Athereby%20improving%20the%20quality%20of%20geometric%20predictions%20without%20large%0Acomputation.%20In%20addition%2C%20we%20leverage%20a%20compact%20representation%20of%20cameras%20and%0Amaintain%20a%20global%20camera%20token%20pool%2C%20which%20enhances%20the%20reliability%20of%20camera%0Apose%20estimation%20without%20sacrificing%20efficiency.%20These%20designs%20enable%20WinT3R%20to%0Aachieve%20state-of-the-art%20performance%20in%20terms%20of%20online%20reconstruction%20quality%2C%0Acamera%20pose%20estimation%2C%20and%20reconstruction%20speed%2C%20as%20validated%20by%20extensive%0Aexperiments%20on%20diverse%20datasets.%20Code%20and%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/LiZizun/WinT3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWinT3R%253A%2520Window-Based%2520Streaming%2520Reconstruction%2520with%2520Camera%2520Token%2520Pool%26entry.906535625%3DZizun%2520Li%2520and%2520Jianjun%2520Zhou%2520and%2520Yifan%2520Wang%2520and%2520Haoyu%2520Guo%2520and%2520Wenzheng%2520Chang%2520and%2520Yang%2520Zhou%2520and%2520Haoyi%2520Zhu%2520and%2520Junyi%2520Chen%2520and%2520Chunhua%2520Shen%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520We%2520present%2520WinT3R%252C%2520a%2520feed-forward%2520reconstruction%2520model%2520capable%2520of%2520online%250Aprediction%2520of%2520precise%2520camera%2520poses%2520and%2520high-quality%2520point%2520maps.%2520Previous%250Amethods%2520suffer%2520from%2520a%2520trade-off%2520between%2520reconstruction%2520quality%2520and%2520real-time%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520first%2520introduce%2520a%2520sliding%2520window%2520mechanism%250Athat%2520ensures%2520sufficient%2520information%2520exchange%2520among%2520frames%2520within%2520the%2520window%252C%250Athereby%2520improving%2520the%2520quality%2520of%2520geometric%2520predictions%2520without%2520large%250Acomputation.%2520In%2520addition%252C%2520we%2520leverage%2520a%2520compact%2520representation%2520of%2520cameras%2520and%250Amaintain%2520a%2520global%2520camera%2520token%2520pool%252C%2520which%2520enhances%2520the%2520reliability%2520of%2520camera%250Apose%2520estimation%2520without%2520sacrificing%2520efficiency.%2520These%2520designs%2520enable%2520WinT3R%2520to%250Aachieve%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520online%2520reconstruction%2520quality%252C%250Acamera%2520pose%2520estimation%252C%2520and%2520reconstruction%2520speed%252C%2520as%2520validated%2520by%2520extensive%250Aexperiments%2520on%2520diverse%2520datasets.%2520Code%2520and%2520model%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/LiZizun/WinT3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WinT3R%3A%20Window-Based%20Streaming%20Reconstruction%20with%20Camera%20Token%20Pool&entry.906535625=Zizun%20Li%20and%20Jianjun%20Zhou%20and%20Yifan%20Wang%20and%20Haoyu%20Guo%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Haoyi%20Zhu%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Tong%20He&entry.1292438233=%20%20We%20present%20WinT3R%2C%20a%20feed-forward%20reconstruction%20model%20capable%20of%20online%0Aprediction%20of%20precise%20camera%20poses%20and%20high-quality%20point%20maps.%20Previous%0Amethods%20suffer%20from%20a%20trade-off%20between%20reconstruction%20quality%20and%20real-time%0Aperformance.%20To%20address%20this%2C%20we%20first%20introduce%20a%20sliding%20window%20mechanism%0Athat%20ensures%20sufficient%20information%20exchange%20among%20frames%20within%20the%20window%2C%0Athereby%20improving%20the%20quality%20of%20geometric%20predictions%20without%20large%0Acomputation.%20In%20addition%2C%20we%20leverage%20a%20compact%20representation%20of%20cameras%20and%0Amaintain%20a%20global%20camera%20token%20pool%2C%20which%20enhances%20the%20reliability%20of%20camera%0Apose%20estimation%20without%20sacrificing%20efficiency.%20These%20designs%20enable%20WinT3R%20to%0Aachieve%20state-of-the-art%20performance%20in%20terms%20of%20online%20reconstruction%20quality%2C%0Acamera%20pose%20estimation%2C%20and%20reconstruction%20speed%2C%20as%20validated%20by%20extensive%0Aexperiments%20on%20diverse%20datasets.%20Code%20and%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/LiZizun/WinT3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05296v1&entry.124074799=Read"},
{"title": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM\n  Uncertainty Quantification", "author": "Maya Kruse and Majid Afshar and Saksham Khatwani and Anoop Mayampurath and Guanhua Chen and Yanjun Gao", "abstract": "  Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE.\n", "link": "http://arxiv.org/abs/2507.07236v2", "date": "2025-09-05", "relevancy": 2.2469, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5513}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Yet%20Effective%3A%20An%20Information-Theoretic%20Approach%20to%20Multi-LLM%0A%20%20Uncertainty%20Quantification&body=Title%3A%20Simple%20Yet%20Effective%3A%20An%20Information-Theoretic%20Approach%20to%20Multi-LLM%0A%20%20Uncertainty%20Quantification%0AAuthor%3A%20Maya%20Kruse%20and%20Majid%20Afshar%20and%20Saksham%20Khatwani%20and%20Anoop%20Mayampurath%20and%20Guanhua%20Chen%20and%20Yanjun%20Gao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20behave%20inconsistently%20across%20inputs%2C%0Aindicating%20uncertainty%20and%20motivating%20the%20need%20for%20its%20quantification%20in%0Ahigh-stakes%20settings.%20Prior%20work%20on%20calibration%20and%20uncertainty%20quantification%0Aoften%20focuses%20on%20individual%20models%2C%20overlooking%20the%20potential%20of%20model%0Adiversity.%20We%20hypothesize%20that%20LLMs%20make%20complementary%20predictions%20due%20to%0Adifferences%20in%20training%20and%20the%20Zipfian%20nature%20of%20language%2C%20and%20that%0Aaggregating%20their%20outputs%20leads%20to%20more%20reliable%20uncertainty%20estimates.%20To%0Aleverage%20this%2C%20we%20propose%20MUSE%20%28Multi-LLM%20Uncertainty%20via%20Subset%20Ensembles%29%2C%20a%0Asimple%20information-theoretic%20method%20that%20uses%20Jensen-Shannon%20Divergence%20to%0Aidentify%20and%20aggregate%20well-calibrated%20subsets%20of%20LLMs.%20Experiments%20on%20binary%0Aprediction%20tasks%20demonstrate%20improved%20calibration%20and%20predictive%20performance%0Acompared%20to%20single-model%20and%20na%5C%22ive%20ensemble%20baselines.%20In%20addition%2C%20we%0Aexplore%20using%20MUSE%20as%20guided%20signals%20with%20chain-of-thought%20distillation%20to%0Afine-tune%20LLMs%20for%20calibration.%20MUSE%20is%20available%0Aat%3Ahttps%3A//github.com/LARK-NLP-Lab/MUSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Yet%2520Effective%253A%2520An%2520Information-Theoretic%2520Approach%2520to%2520Multi-LLM%250A%2520%2520Uncertainty%2520Quantification%26entry.906535625%3DMaya%2520Kruse%2520and%2520Majid%2520Afshar%2520and%2520Saksham%2520Khatwani%2520and%2520Anoop%2520Mayampurath%2520and%2520Guanhua%2520Chen%2520and%2520Yanjun%2520Gao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520behave%2520inconsistently%2520across%2520inputs%252C%250Aindicating%2520uncertainty%2520and%2520motivating%2520the%2520need%2520for%2520its%2520quantification%2520in%250Ahigh-stakes%2520settings.%2520Prior%2520work%2520on%2520calibration%2520and%2520uncertainty%2520quantification%250Aoften%2520focuses%2520on%2520individual%2520models%252C%2520overlooking%2520the%2520potential%2520of%2520model%250Adiversity.%2520We%2520hypothesize%2520that%2520LLMs%2520make%2520complementary%2520predictions%2520due%2520to%250Adifferences%2520in%2520training%2520and%2520the%2520Zipfian%2520nature%2520of%2520language%252C%2520and%2520that%250Aaggregating%2520their%2520outputs%2520leads%2520to%2520more%2520reliable%2520uncertainty%2520estimates.%2520To%250Aleverage%2520this%252C%2520we%2520propose%2520MUSE%2520%2528Multi-LLM%2520Uncertainty%2520via%2520Subset%2520Ensembles%2529%252C%2520a%250Asimple%2520information-theoretic%2520method%2520that%2520uses%2520Jensen-Shannon%2520Divergence%2520to%250Aidentify%2520and%2520aggregate%2520well-calibrated%2520subsets%2520of%2520LLMs.%2520Experiments%2520on%2520binary%250Aprediction%2520tasks%2520demonstrate%2520improved%2520calibration%2520and%2520predictive%2520performance%250Acompared%2520to%2520single-model%2520and%2520na%255C%2522ive%2520ensemble%2520baselines.%2520In%2520addition%252C%2520we%250Aexplore%2520using%2520MUSE%2520as%2520guided%2520signals%2520with%2520chain-of-thought%2520distillation%2520to%250Afine-tune%2520LLMs%2520for%2520calibration.%2520MUSE%2520is%2520available%250Aat%253Ahttps%253A//github.com/LARK-NLP-Lab/MUSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Yet%20Effective%3A%20An%20Information-Theoretic%20Approach%20to%20Multi-LLM%0A%20%20Uncertainty%20Quantification&entry.906535625=Maya%20Kruse%20and%20Majid%20Afshar%20and%20Saksham%20Khatwani%20and%20Anoop%20Mayampurath%20and%20Guanhua%20Chen%20and%20Yanjun%20Gao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20behave%20inconsistently%20across%20inputs%2C%0Aindicating%20uncertainty%20and%20motivating%20the%20need%20for%20its%20quantification%20in%0Ahigh-stakes%20settings.%20Prior%20work%20on%20calibration%20and%20uncertainty%20quantification%0Aoften%20focuses%20on%20individual%20models%2C%20overlooking%20the%20potential%20of%20model%0Adiversity.%20We%20hypothesize%20that%20LLMs%20make%20complementary%20predictions%20due%20to%0Adifferences%20in%20training%20and%20the%20Zipfian%20nature%20of%20language%2C%20and%20that%0Aaggregating%20their%20outputs%20leads%20to%20more%20reliable%20uncertainty%20estimates.%20To%0Aleverage%20this%2C%20we%20propose%20MUSE%20%28Multi-LLM%20Uncertainty%20via%20Subset%20Ensembles%29%2C%20a%0Asimple%20information-theoretic%20method%20that%20uses%20Jensen-Shannon%20Divergence%20to%0Aidentify%20and%20aggregate%20well-calibrated%20subsets%20of%20LLMs.%20Experiments%20on%20binary%0Aprediction%20tasks%20demonstrate%20improved%20calibration%20and%20predictive%20performance%0Acompared%20to%20single-model%20and%20na%5C%22ive%20ensemble%20baselines.%20In%20addition%2C%20we%0Aexplore%20using%20MUSE%20as%20guided%20signals%20with%20chain-of-thought%20distillation%20to%0Afine-tune%20LLMs%20for%20calibration.%20MUSE%20is%20available%0Aat%3Ahttps%3A//github.com/LARK-NLP-Lab/MUSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07236v2&entry.124074799=Read"},
{"title": "Pointing-Guided Target Estimation via Transformer-Based Attention", "author": "Luca M\u00fcller and Hassan Ali and Philipp Allgeuer and Luk\u00e1\u0161 Gajdo\u0161ech and Stefan Wermter", "abstract": "  Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.\n", "link": "http://arxiv.org/abs/2509.05031v1", "date": "2025-09-05", "relevancy": 2.2429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5776}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointing-Guided%20Target%20Estimation%20via%20Transformer-Based%20Attention&body=Title%3A%20Pointing-Guided%20Target%20Estimation%20via%20Transformer-Based%20Attention%0AAuthor%3A%20Luca%20M%C3%BCller%20and%20Hassan%20Ali%20and%20Philipp%20Allgeuer%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Deictic%20gestures%2C%20like%20pointing%2C%20are%20a%20fundamental%20form%20of%20non-verbal%0Acommunication%2C%20enabling%20humans%20to%20direct%20attention%20to%20specific%20objects%20or%0Alocations.%20This%20capability%20is%20essential%20in%20Human-Robot%20Interaction%20%28HRI%29%2C%20where%0Arobots%20should%20be%20able%20to%20predict%20human%20intent%20and%20anticipate%20appropriate%0Aresponses.%20In%20this%20work%2C%20we%20propose%20the%20Multi-Modality%20Inter-TransFormer%0A%28MM-ITF%29%2C%20a%20modular%20architecture%20to%20predict%20objects%20in%20a%20controlled%20tabletop%0Ascenario%20with%20the%20NICOL%20robot%2C%20where%20humans%20indicate%20targets%20through%20natural%0Apointing%20gestures.%20Leveraging%20inter-modality%20attention%2C%20MM-ITF%20maps%202D%20pointing%0Agestures%20to%20object%20locations%2C%20assigns%20a%20likelihood%20score%20to%20each%2C%20and%0Aidentifies%20the%20most%20likely%20target.%20Our%20results%20demonstrate%20that%20the%20method%20can%0Aaccurately%20predict%20the%20intended%20object%20using%20monocular%20RGB%20data%2C%20thus%20enabling%0Aintuitive%20and%20accessible%20human-robot%20collaboration.%20To%20evaluate%20the%0Aperformance%2C%20we%20introduce%20a%20patch%20confusion%20matrix%2C%20providing%20insights%20into%20the%0Amodel%27s%20predictions%20across%20candidate%20object%20locations.%20Code%20available%20at%3A%0Ahttps%3A//github.com/lucamuellercode/MMITF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointing-Guided%2520Target%2520Estimation%2520via%2520Transformer-Based%2520Attention%26entry.906535625%3DLuca%2520M%25C3%25BCller%2520and%2520Hassan%2520Ali%2520and%2520Philipp%2520Allgeuer%2520and%2520Luk%25C3%25A1%25C5%25A1%2520Gajdo%25C5%25A1ech%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520Deictic%2520gestures%252C%2520like%2520pointing%252C%2520are%2520a%2520fundamental%2520form%2520of%2520non-verbal%250Acommunication%252C%2520enabling%2520humans%2520to%2520direct%2520attention%2520to%2520specific%2520objects%2520or%250Alocations.%2520This%2520capability%2520is%2520essential%2520in%2520Human-Robot%2520Interaction%2520%2528HRI%2529%252C%2520where%250Arobots%2520should%2520be%2520able%2520to%2520predict%2520human%2520intent%2520and%2520anticipate%2520appropriate%250Aresponses.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Multi-Modality%2520Inter-TransFormer%250A%2528MM-ITF%2529%252C%2520a%2520modular%2520architecture%2520to%2520predict%2520objects%2520in%2520a%2520controlled%2520tabletop%250Ascenario%2520with%2520the%2520NICOL%2520robot%252C%2520where%2520humans%2520indicate%2520targets%2520through%2520natural%250Apointing%2520gestures.%2520Leveraging%2520inter-modality%2520attention%252C%2520MM-ITF%2520maps%25202D%2520pointing%250Agestures%2520to%2520object%2520locations%252C%2520assigns%2520a%2520likelihood%2520score%2520to%2520each%252C%2520and%250Aidentifies%2520the%2520most%2520likely%2520target.%2520Our%2520results%2520demonstrate%2520that%2520the%2520method%2520can%250Aaccurately%2520predict%2520the%2520intended%2520object%2520using%2520monocular%2520RGB%2520data%252C%2520thus%2520enabling%250Aintuitive%2520and%2520accessible%2520human-robot%2520collaboration.%2520To%2520evaluate%2520the%250Aperformance%252C%2520we%2520introduce%2520a%2520patch%2520confusion%2520matrix%252C%2520providing%2520insights%2520into%2520the%250Amodel%2527s%2520predictions%2520across%2520candidate%2520object%2520locations.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/lucamuellercode/MMITF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointing-Guided%20Target%20Estimation%20via%20Transformer-Based%20Attention&entry.906535625=Luca%20M%C3%BCller%20and%20Hassan%20Ali%20and%20Philipp%20Allgeuer%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Stefan%20Wermter&entry.1292438233=%20%20Deictic%20gestures%2C%20like%20pointing%2C%20are%20a%20fundamental%20form%20of%20non-verbal%0Acommunication%2C%20enabling%20humans%20to%20direct%20attention%20to%20specific%20objects%20or%0Alocations.%20This%20capability%20is%20essential%20in%20Human-Robot%20Interaction%20%28HRI%29%2C%20where%0Arobots%20should%20be%20able%20to%20predict%20human%20intent%20and%20anticipate%20appropriate%0Aresponses.%20In%20this%20work%2C%20we%20propose%20the%20Multi-Modality%20Inter-TransFormer%0A%28MM-ITF%29%2C%20a%20modular%20architecture%20to%20predict%20objects%20in%20a%20controlled%20tabletop%0Ascenario%20with%20the%20NICOL%20robot%2C%20where%20humans%20indicate%20targets%20through%20natural%0Apointing%20gestures.%20Leveraging%20inter-modality%20attention%2C%20MM-ITF%20maps%202D%20pointing%0Agestures%20to%20object%20locations%2C%20assigns%20a%20likelihood%20score%20to%20each%2C%20and%0Aidentifies%20the%20most%20likely%20target.%20Our%20results%20demonstrate%20that%20the%20method%20can%0Aaccurately%20predict%20the%20intended%20object%20using%20monocular%20RGB%20data%2C%20thus%20enabling%0Aintuitive%20and%20accessible%20human-robot%20collaboration.%20To%20evaluate%20the%0Aperformance%2C%20we%20introduce%20a%20patch%20confusion%20matrix%2C%20providing%20insights%20into%20the%0Amodel%27s%20predictions%20across%20candidate%20object%20locations.%20Code%20available%20at%3A%0Ahttps%3A//github.com/lucamuellercode/MMITF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05031v1&entry.124074799=Read"},
{"title": "MAIA: An Inpainting-Based Approach for Music Adversarial Attacks", "author": "Yuxuan Liu and Peihong Zhang and Rui Sang and Zhixin Li and Shengchen Li", "abstract": "  Music adversarial attacks have garnered significant interest in the field of\nMusic Information Retrieval (MIR). In this paper, we present Music Adversarial\nInpainting Attack (MAIA), a novel adversarial attack framework that supports\nboth white-box and black-box attack scenarios. MAIA begins with an importance\nanalysis to identify critical audio segments, which are then targeted for\nmodification. Utilizing generative inpainting models, these segments are\nreconstructed with guidance from the output of the attacked model, ensuring\nsubtle and effective adversarial perturbations. We evaluate MAIA on multiple\nMIR tasks, demonstrating high attack success rates in both white-box and\nblack-box settings while maintaining minimal perceptual distortion.\nAdditionally, subjective listening tests confirm the high audio fidelity of the\nadversarial samples. Our findings highlight vulnerabilities in current MIR\nsystems and emphasize the need for more robust and secure models.\n", "link": "http://arxiv.org/abs/2509.04980v1", "date": "2025-09-05", "relevancy": 2.2394, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4622}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAIA%3A%20An%20Inpainting-Based%20Approach%20for%20Music%20Adversarial%20Attacks&body=Title%3A%20MAIA%3A%20An%20Inpainting-Based%20Approach%20for%20Music%20Adversarial%20Attacks%0AAuthor%3A%20Yuxuan%20Liu%20and%20Peihong%20Zhang%20and%20Rui%20Sang%20and%20Zhixin%20Li%20and%20Shengchen%20Li%0AAbstract%3A%20%20%20Music%20adversarial%20attacks%20have%20garnered%20significant%20interest%20in%20the%20field%20of%0AMusic%20Information%20Retrieval%20%28MIR%29.%20In%20this%20paper%2C%20we%20present%20Music%20Adversarial%0AInpainting%20Attack%20%28MAIA%29%2C%20a%20novel%20adversarial%20attack%20framework%20that%20supports%0Aboth%20white-box%20and%20black-box%20attack%20scenarios.%20MAIA%20begins%20with%20an%20importance%0Aanalysis%20to%20identify%20critical%20audio%20segments%2C%20which%20are%20then%20targeted%20for%0Amodification.%20Utilizing%20generative%20inpainting%20models%2C%20these%20segments%20are%0Areconstructed%20with%20guidance%20from%20the%20output%20of%20the%20attacked%20model%2C%20ensuring%0Asubtle%20and%20effective%20adversarial%20perturbations.%20We%20evaluate%20MAIA%20on%20multiple%0AMIR%20tasks%2C%20demonstrating%20high%20attack%20success%20rates%20in%20both%20white-box%20and%0Ablack-box%20settings%20while%20maintaining%20minimal%20perceptual%20distortion.%0AAdditionally%2C%20subjective%20listening%20tests%20confirm%20the%20high%20audio%20fidelity%20of%20the%0Aadversarial%20samples.%20Our%20findings%20highlight%20vulnerabilities%20in%20current%20MIR%0Asystems%20and%20emphasize%20the%20need%20for%20more%20robust%20and%20secure%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAIA%253A%2520An%2520Inpainting-Based%2520Approach%2520for%2520Music%2520Adversarial%2520Attacks%26entry.906535625%3DYuxuan%2520Liu%2520and%2520Peihong%2520Zhang%2520and%2520Rui%2520Sang%2520and%2520Zhixin%2520Li%2520and%2520Shengchen%2520Li%26entry.1292438233%3D%2520%2520Music%2520adversarial%2520attacks%2520have%2520garnered%2520significant%2520interest%2520in%2520the%2520field%2520of%250AMusic%2520Information%2520Retrieval%2520%2528MIR%2529.%2520In%2520this%2520paper%252C%2520we%2520present%2520Music%2520Adversarial%250AInpainting%2520Attack%2520%2528MAIA%2529%252C%2520a%2520novel%2520adversarial%2520attack%2520framework%2520that%2520supports%250Aboth%2520white-box%2520and%2520black-box%2520attack%2520scenarios.%2520MAIA%2520begins%2520with%2520an%2520importance%250Aanalysis%2520to%2520identify%2520critical%2520audio%2520segments%252C%2520which%2520are%2520then%2520targeted%2520for%250Amodification.%2520Utilizing%2520generative%2520inpainting%2520models%252C%2520these%2520segments%2520are%250Areconstructed%2520with%2520guidance%2520from%2520the%2520output%2520of%2520the%2520attacked%2520model%252C%2520ensuring%250Asubtle%2520and%2520effective%2520adversarial%2520perturbations.%2520We%2520evaluate%2520MAIA%2520on%2520multiple%250AMIR%2520tasks%252C%2520demonstrating%2520high%2520attack%2520success%2520rates%2520in%2520both%2520white-box%2520and%250Ablack-box%2520settings%2520while%2520maintaining%2520minimal%2520perceptual%2520distortion.%250AAdditionally%252C%2520subjective%2520listening%2520tests%2520confirm%2520the%2520high%2520audio%2520fidelity%2520of%2520the%250Aadversarial%2520samples.%2520Our%2520findings%2520highlight%2520vulnerabilities%2520in%2520current%2520MIR%250Asystems%2520and%2520emphasize%2520the%2520need%2520for%2520more%2520robust%2520and%2520secure%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAIA%3A%20An%20Inpainting-Based%20Approach%20for%20Music%20Adversarial%20Attacks&entry.906535625=Yuxuan%20Liu%20and%20Peihong%20Zhang%20and%20Rui%20Sang%20and%20Zhixin%20Li%20and%20Shengchen%20Li&entry.1292438233=%20%20Music%20adversarial%20attacks%20have%20garnered%20significant%20interest%20in%20the%20field%20of%0AMusic%20Information%20Retrieval%20%28MIR%29.%20In%20this%20paper%2C%20we%20present%20Music%20Adversarial%0AInpainting%20Attack%20%28MAIA%29%2C%20a%20novel%20adversarial%20attack%20framework%20that%20supports%0Aboth%20white-box%20and%20black-box%20attack%20scenarios.%20MAIA%20begins%20with%20an%20importance%0Aanalysis%20to%20identify%20critical%20audio%20segments%2C%20which%20are%20then%20targeted%20for%0Amodification.%20Utilizing%20generative%20inpainting%20models%2C%20these%20segments%20are%0Areconstructed%20with%20guidance%20from%20the%20output%20of%20the%20attacked%20model%2C%20ensuring%0Asubtle%20and%20effective%20adversarial%20perturbations.%20We%20evaluate%20MAIA%20on%20multiple%0AMIR%20tasks%2C%20demonstrating%20high%20attack%20success%20rates%20in%20both%20white-box%20and%0Ablack-box%20settings%20while%20maintaining%20minimal%20perceptual%20distortion.%0AAdditionally%2C%20subjective%20listening%20tests%20confirm%20the%20high%20audio%20fidelity%20of%20the%0Aadversarial%20samples.%20Our%20findings%20highlight%20vulnerabilities%20in%20current%20MIR%0Asystems%20and%20emphasize%20the%20need%20for%20more%20robust%20and%20secure%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04980v1&entry.124074799=Read"},
{"title": "On the Learnability of Distribution Classes with Adaptive Adversaries", "author": "Tosca Lechner and Alex Bie and Gautam Kamath", "abstract": "  We consider the question of learnability of distribution classes in the\npresence of adaptive adversaries -- that is, adversaries capable of\nintercepting the samples requested by a learner and applying manipulations with\nfull knowledge of the samples before passing it on to the learner. This stands\nin contrast to oblivious adversaries, who can only modify the underlying\ndistribution the samples come from but not their i.i.d.\\ nature. We formulate a\ngeneral notion of learnability with respect to adaptive adversaries, taking\ninto account the budget of the adversary. We show that learnability with\nrespect to additive adaptive adversaries is a strictly stronger condition than\nlearnability with respect to additive oblivious adversaries.\n", "link": "http://arxiv.org/abs/2509.05137v1", "date": "2025-09-05", "relevancy": 2.2383, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4623}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Learnability%20of%20Distribution%20Classes%20with%20Adaptive%20Adversaries&body=Title%3A%20On%20the%20Learnability%20of%20Distribution%20Classes%20with%20Adaptive%20Adversaries%0AAuthor%3A%20Tosca%20Lechner%20and%20Alex%20Bie%20and%20Gautam%20Kamath%0AAbstract%3A%20%20%20We%20consider%20the%20question%20of%20learnability%20of%20distribution%20classes%20in%20the%0Apresence%20of%20adaptive%20adversaries%20--%20that%20is%2C%20adversaries%20capable%20of%0Aintercepting%20the%20samples%20requested%20by%20a%20learner%20and%20applying%20manipulations%20with%0Afull%20knowledge%20of%20the%20samples%20before%20passing%20it%20on%20to%20the%20learner.%20This%20stands%0Ain%20contrast%20to%20oblivious%20adversaries%2C%20who%20can%20only%20modify%20the%20underlying%0Adistribution%20the%20samples%20come%20from%20but%20not%20their%20i.i.d.%5C%20nature.%20We%20formulate%20a%0Ageneral%20notion%20of%20learnability%20with%20respect%20to%20adaptive%20adversaries%2C%20taking%0Ainto%20account%20the%20budget%20of%20the%20adversary.%20We%20show%20that%20learnability%20with%0Arespect%20to%20additive%20adaptive%20adversaries%20is%20a%20strictly%20stronger%20condition%20than%0Alearnability%20with%20respect%20to%20additive%20oblivious%20adversaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Learnability%2520of%2520Distribution%2520Classes%2520with%2520Adaptive%2520Adversaries%26entry.906535625%3DTosca%2520Lechner%2520and%2520Alex%2520Bie%2520and%2520Gautam%2520Kamath%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520question%2520of%2520learnability%2520of%2520distribution%2520classes%2520in%2520the%250Apresence%2520of%2520adaptive%2520adversaries%2520--%2520that%2520is%252C%2520adversaries%2520capable%2520of%250Aintercepting%2520the%2520samples%2520requested%2520by%2520a%2520learner%2520and%2520applying%2520manipulations%2520with%250Afull%2520knowledge%2520of%2520the%2520samples%2520before%2520passing%2520it%2520on%2520to%2520the%2520learner.%2520This%2520stands%250Ain%2520contrast%2520to%2520oblivious%2520adversaries%252C%2520who%2520can%2520only%2520modify%2520the%2520underlying%250Adistribution%2520the%2520samples%2520come%2520from%2520but%2520not%2520their%2520i.i.d.%255C%2520nature.%2520We%2520formulate%2520a%250Ageneral%2520notion%2520of%2520learnability%2520with%2520respect%2520to%2520adaptive%2520adversaries%252C%2520taking%250Ainto%2520account%2520the%2520budget%2520of%2520the%2520adversary.%2520We%2520show%2520that%2520learnability%2520with%250Arespect%2520to%2520additive%2520adaptive%2520adversaries%2520is%2520a%2520strictly%2520stronger%2520condition%2520than%250Alearnability%2520with%2520respect%2520to%2520additive%2520oblivious%2520adversaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Learnability%20of%20Distribution%20Classes%20with%20Adaptive%20Adversaries&entry.906535625=Tosca%20Lechner%20and%20Alex%20Bie%20and%20Gautam%20Kamath&entry.1292438233=%20%20We%20consider%20the%20question%20of%20learnability%20of%20distribution%20classes%20in%20the%0Apresence%20of%20adaptive%20adversaries%20--%20that%20is%2C%20adversaries%20capable%20of%0Aintercepting%20the%20samples%20requested%20by%20a%20learner%20and%20applying%20manipulations%20with%0Afull%20knowledge%20of%20the%20samples%20before%20passing%20it%20on%20to%20the%20learner.%20This%20stands%0Ain%20contrast%20to%20oblivious%20adversaries%2C%20who%20can%20only%20modify%20the%20underlying%0Adistribution%20the%20samples%20come%20from%20but%20not%20their%20i.i.d.%5C%20nature.%20We%20formulate%20a%0Ageneral%20notion%20of%20learnability%20with%20respect%20to%20adaptive%20adversaries%2C%20taking%0Ainto%20account%20the%20budget%20of%20the%20adversary.%20We%20show%20that%20learnability%20with%0Arespect%20to%20additive%20adaptive%20adversaries%20is%20a%20strictly%20stronger%20condition%20than%0Alearnability%20with%20respect%20to%20additive%20oblivious%20adversaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05137v1&entry.124074799=Read"},
{"title": "Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital\n  Twins", "author": "Jialin Chen and Jeremie Clos and Dominic Price and Praminda Caleb-Solly", "abstract": "  To advance the development of assistive and rehabilitation robots, it is\nessential to conduct experiments early in the design cycle. However, testing\nearly prototypes directly with users can pose safety risks. To address this, we\nexplore the use of condition-specific simulation suits worn by healthy\nparticipants in controlled environments as a means to study gait changes\nassociated with various impairments and support rapid prototyping. This paper\npresents a study analyzing the impact of a hemiplegia simulation suit on gait.\nWe collected biomechanical data using a Vicon motion capture system and Delsys\nTrigno EMG and IMU sensors under four walking conditions: with and without a\nrollator, and with and without the simulation suit. The gait data was\nintegrated into a digital twin model, enabling machine learning analyses to\ndetect the use of the simulation suit and rollator, identify turning behavior,\nand evaluate how the suit affects gait over time. Our findings show that the\nsimulation suit significantly alters movement and muscle activation patterns,\nprompting users to compensate with more abrupt motions. We also identify key\nfeatures and sensor modalities that are most informative for accurately\ncapturing gait dynamics and modeling human-rollator interaction within the\ndigital twin framework.\n", "link": "http://arxiv.org/abs/2509.05116v1", "date": "2025-09-05", "relevancy": 2.2024, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5785}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Gait%20Adaptation%20with%20Hemiplegia%20Simulation%20Suits%20and%20Digital%0A%20%20Twins&body=Title%3A%20Analyzing%20Gait%20Adaptation%20with%20Hemiplegia%20Simulation%20Suits%20and%20Digital%0A%20%20Twins%0AAuthor%3A%20Jialin%20Chen%20and%20Jeremie%20Clos%20and%20Dominic%20Price%20and%20Praminda%20Caleb-Solly%0AAbstract%3A%20%20%20To%20advance%20the%20development%20of%20assistive%20and%20rehabilitation%20robots%2C%20it%20is%0Aessential%20to%20conduct%20experiments%20early%20in%20the%20design%20cycle.%20However%2C%20testing%0Aearly%20prototypes%20directly%20with%20users%20can%20pose%20safety%20risks.%20To%20address%20this%2C%20we%0Aexplore%20the%20use%20of%20condition-specific%20simulation%20suits%20worn%20by%20healthy%0Aparticipants%20in%20controlled%20environments%20as%20a%20means%20to%20study%20gait%20changes%0Aassociated%20with%20various%20impairments%20and%20support%20rapid%20prototyping.%20This%20paper%0Apresents%20a%20study%20analyzing%20the%20impact%20of%20a%20hemiplegia%20simulation%20suit%20on%20gait.%0AWe%20collected%20biomechanical%20data%20using%20a%20Vicon%20motion%20capture%20system%20and%20Delsys%0ATrigno%20EMG%20and%20IMU%20sensors%20under%20four%20walking%20conditions%3A%20with%20and%20without%20a%0Arollator%2C%20and%20with%20and%20without%20the%20simulation%20suit.%20The%20gait%20data%20was%0Aintegrated%20into%20a%20digital%20twin%20model%2C%20enabling%20machine%20learning%20analyses%20to%0Adetect%20the%20use%20of%20the%20simulation%20suit%20and%20rollator%2C%20identify%20turning%20behavior%2C%0Aand%20evaluate%20how%20the%20suit%20affects%20gait%20over%20time.%20Our%20findings%20show%20that%20the%0Asimulation%20suit%20significantly%20alters%20movement%20and%20muscle%20activation%20patterns%2C%0Aprompting%20users%20to%20compensate%20with%20more%20abrupt%20motions.%20We%20also%20identify%20key%0Afeatures%20and%20sensor%20modalities%20that%20are%20most%20informative%20for%20accurately%0Acapturing%20gait%20dynamics%20and%20modeling%20human-rollator%20interaction%20within%20the%0Adigital%20twin%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Gait%2520Adaptation%2520with%2520Hemiplegia%2520Simulation%2520Suits%2520and%2520Digital%250A%2520%2520Twins%26entry.906535625%3DJialin%2520Chen%2520and%2520Jeremie%2520Clos%2520and%2520Dominic%2520Price%2520and%2520Praminda%2520Caleb-Solly%26entry.1292438233%3D%2520%2520To%2520advance%2520the%2520development%2520of%2520assistive%2520and%2520rehabilitation%2520robots%252C%2520it%2520is%250Aessential%2520to%2520conduct%2520experiments%2520early%2520in%2520the%2520design%2520cycle.%2520However%252C%2520testing%250Aearly%2520prototypes%2520directly%2520with%2520users%2520can%2520pose%2520safety%2520risks.%2520To%2520address%2520this%252C%2520we%250Aexplore%2520the%2520use%2520of%2520condition-specific%2520simulation%2520suits%2520worn%2520by%2520healthy%250Aparticipants%2520in%2520controlled%2520environments%2520as%2520a%2520means%2520to%2520study%2520gait%2520changes%250Aassociated%2520with%2520various%2520impairments%2520and%2520support%2520rapid%2520prototyping.%2520This%2520paper%250Apresents%2520a%2520study%2520analyzing%2520the%2520impact%2520of%2520a%2520hemiplegia%2520simulation%2520suit%2520on%2520gait.%250AWe%2520collected%2520biomechanical%2520data%2520using%2520a%2520Vicon%2520motion%2520capture%2520system%2520and%2520Delsys%250ATrigno%2520EMG%2520and%2520IMU%2520sensors%2520under%2520four%2520walking%2520conditions%253A%2520with%2520and%2520without%2520a%250Arollator%252C%2520and%2520with%2520and%2520without%2520the%2520simulation%2520suit.%2520The%2520gait%2520data%2520was%250Aintegrated%2520into%2520a%2520digital%2520twin%2520model%252C%2520enabling%2520machine%2520learning%2520analyses%2520to%250Adetect%2520the%2520use%2520of%2520the%2520simulation%2520suit%2520and%2520rollator%252C%2520identify%2520turning%2520behavior%252C%250Aand%2520evaluate%2520how%2520the%2520suit%2520affects%2520gait%2520over%2520time.%2520Our%2520findings%2520show%2520that%2520the%250Asimulation%2520suit%2520significantly%2520alters%2520movement%2520and%2520muscle%2520activation%2520patterns%252C%250Aprompting%2520users%2520to%2520compensate%2520with%2520more%2520abrupt%2520motions.%2520We%2520also%2520identify%2520key%250Afeatures%2520and%2520sensor%2520modalities%2520that%2520are%2520most%2520informative%2520for%2520accurately%250Acapturing%2520gait%2520dynamics%2520and%2520modeling%2520human-rollator%2520interaction%2520within%2520the%250Adigital%2520twin%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Gait%20Adaptation%20with%20Hemiplegia%20Simulation%20Suits%20and%20Digital%0A%20%20Twins&entry.906535625=Jialin%20Chen%20and%20Jeremie%20Clos%20and%20Dominic%20Price%20and%20Praminda%20Caleb-Solly&entry.1292438233=%20%20To%20advance%20the%20development%20of%20assistive%20and%20rehabilitation%20robots%2C%20it%20is%0Aessential%20to%20conduct%20experiments%20early%20in%20the%20design%20cycle.%20However%2C%20testing%0Aearly%20prototypes%20directly%20with%20users%20can%20pose%20safety%20risks.%20To%20address%20this%2C%20we%0Aexplore%20the%20use%20of%20condition-specific%20simulation%20suits%20worn%20by%20healthy%0Aparticipants%20in%20controlled%20environments%20as%20a%20means%20to%20study%20gait%20changes%0Aassociated%20with%20various%20impairments%20and%20support%20rapid%20prototyping.%20This%20paper%0Apresents%20a%20study%20analyzing%20the%20impact%20of%20a%20hemiplegia%20simulation%20suit%20on%20gait.%0AWe%20collected%20biomechanical%20data%20using%20a%20Vicon%20motion%20capture%20system%20and%20Delsys%0ATrigno%20EMG%20and%20IMU%20sensors%20under%20four%20walking%20conditions%3A%20with%20and%20without%20a%0Arollator%2C%20and%20with%20and%20without%20the%20simulation%20suit.%20The%20gait%20data%20was%0Aintegrated%20into%20a%20digital%20twin%20model%2C%20enabling%20machine%20learning%20analyses%20to%0Adetect%20the%20use%20of%20the%20simulation%20suit%20and%20rollator%2C%20identify%20turning%20behavior%2C%0Aand%20evaluate%20how%20the%20suit%20affects%20gait%20over%20time.%20Our%20findings%20show%20that%20the%0Asimulation%20suit%20significantly%20alters%20movement%20and%20muscle%20activation%20patterns%2C%0Aprompting%20users%20to%20compensate%20with%20more%20abrupt%20motions.%20We%20also%20identify%20key%0Afeatures%20and%20sensor%20modalities%20that%20are%20most%20informative%20for%20accurately%0Acapturing%20gait%20dynamics%20and%20modeling%20human-rollator%20interaction%20within%20the%0Adigital%20twin%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05116v1&entry.124074799=Read"},
{"title": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation", "author": "In\u00e9s Gonzalez-Pepe and Vinuyan Sivakolunthu and Yohan Chatelain and Tristan Glatard", "abstract": "  Deep learning (DL) is rapidly advancing neuroimaging by achieving\nstate-of-the-art performance with reduced computation times. Yet the numerical\nstability of DL models -- particularly during training -- remains\nunderexplored. While inference with DL is relatively stable, training\nintroduces additional variability primarily through iterative stochastic\noptimization. We investigate this training-time variability using FastSurfer, a\nCNN-based whole-brain segmentation pipeline. Controlled perturbations are\nintroduced via floating point perturbations and random seeds. We find that: (i)\nFastSurfer exhibits higher variability compared to that of a traditional\nneuroimaging pipeline, suggesting that DL inherits and is particularly\nsusceptible to sources of instability present in its predecessors; (ii)\nensembles generated with perturbations achieve performance similar to an\nunperturbed baseline; and (iii) variability effectively produces ensembles of\nnumerical model families that can be repurposed for downstream applications. As\na proof of concept, we demonstrate that numerical ensembles can be used as a\ndata augmentation strategy for brain age regression. These findings position\ntraining-time variability not only as a reproducibility concern but also as a\nresource that can be harnessed to improve robustness and enable new\napplications in neuroimaging.\n", "link": "http://arxiv.org/abs/2509.05238v1", "date": "2025-09-05", "relevancy": 2.1993, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5519}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5496}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertain%20but%20Useful%3A%20Leveraging%20CNN%20Variability%20into%20Data%20Augmentation&body=Title%3A%20Uncertain%20but%20Useful%3A%20Leveraging%20CNN%20Variability%20into%20Data%20Augmentation%0AAuthor%3A%20In%C3%A9s%20Gonzalez-Pepe%20and%20Vinuyan%20Sivakolunthu%20and%20Yohan%20Chatelain%20and%20Tristan%20Glatard%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20is%20rapidly%20advancing%20neuroimaging%20by%20achieving%0Astate-of-the-art%20performance%20with%20reduced%20computation%20times.%20Yet%20the%20numerical%0Astability%20of%20DL%20models%20--%20particularly%20during%20training%20--%20remains%0Aunderexplored.%20While%20inference%20with%20DL%20is%20relatively%20stable%2C%20training%0Aintroduces%20additional%20variability%20primarily%20through%20iterative%20stochastic%0Aoptimization.%20We%20investigate%20this%20training-time%20variability%20using%20FastSurfer%2C%20a%0ACNN-based%20whole-brain%20segmentation%20pipeline.%20Controlled%20perturbations%20are%0Aintroduced%20via%20floating%20point%20perturbations%20and%20random%20seeds.%20We%20find%20that%3A%20%28i%29%0AFastSurfer%20exhibits%20higher%20variability%20compared%20to%20that%20of%20a%20traditional%0Aneuroimaging%20pipeline%2C%20suggesting%20that%20DL%20inherits%20and%20is%20particularly%0Asusceptible%20to%20sources%20of%20instability%20present%20in%20its%20predecessors%3B%20%28ii%29%0Aensembles%20generated%20with%20perturbations%20achieve%20performance%20similar%20to%20an%0Aunperturbed%20baseline%3B%20and%20%28iii%29%20variability%20effectively%20produces%20ensembles%20of%0Anumerical%20model%20families%20that%20can%20be%20repurposed%20for%20downstream%20applications.%20As%0Aa%20proof%20of%20concept%2C%20we%20demonstrate%20that%20numerical%20ensembles%20can%20be%20used%20as%20a%0Adata%20augmentation%20strategy%20for%20brain%20age%20regression.%20These%20findings%20position%0Atraining-time%20variability%20not%20only%20as%20a%20reproducibility%20concern%20but%20also%20as%20a%0Aresource%20that%20can%20be%20harnessed%20to%20improve%20robustness%20and%20enable%20new%0Aapplications%20in%20neuroimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertain%2520but%2520Useful%253A%2520Leveraging%2520CNN%2520Variability%2520into%2520Data%2520Augmentation%26entry.906535625%3DIn%25C3%25A9s%2520Gonzalez-Pepe%2520and%2520Vinuyan%2520Sivakolunthu%2520and%2520Yohan%2520Chatelain%2520and%2520Tristan%2520Glatard%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520is%2520rapidly%2520advancing%2520neuroimaging%2520by%2520achieving%250Astate-of-the-art%2520performance%2520with%2520reduced%2520computation%2520times.%2520Yet%2520the%2520numerical%250Astability%2520of%2520DL%2520models%2520--%2520particularly%2520during%2520training%2520--%2520remains%250Aunderexplored.%2520While%2520inference%2520with%2520DL%2520is%2520relatively%2520stable%252C%2520training%250Aintroduces%2520additional%2520variability%2520primarily%2520through%2520iterative%2520stochastic%250Aoptimization.%2520We%2520investigate%2520this%2520training-time%2520variability%2520using%2520FastSurfer%252C%2520a%250ACNN-based%2520whole-brain%2520segmentation%2520pipeline.%2520Controlled%2520perturbations%2520are%250Aintroduced%2520via%2520floating%2520point%2520perturbations%2520and%2520random%2520seeds.%2520We%2520find%2520that%253A%2520%2528i%2529%250AFastSurfer%2520exhibits%2520higher%2520variability%2520compared%2520to%2520that%2520of%2520a%2520traditional%250Aneuroimaging%2520pipeline%252C%2520suggesting%2520that%2520DL%2520inherits%2520and%2520is%2520particularly%250Asusceptible%2520to%2520sources%2520of%2520instability%2520present%2520in%2520its%2520predecessors%253B%2520%2528ii%2529%250Aensembles%2520generated%2520with%2520perturbations%2520achieve%2520performance%2520similar%2520to%2520an%250Aunperturbed%2520baseline%253B%2520and%2520%2528iii%2529%2520variability%2520effectively%2520produces%2520ensembles%2520of%250Anumerical%2520model%2520families%2520that%2520can%2520be%2520repurposed%2520for%2520downstream%2520applications.%2520As%250Aa%2520proof%2520of%2520concept%252C%2520we%2520demonstrate%2520that%2520numerical%2520ensembles%2520can%2520be%2520used%2520as%2520a%250Adata%2520augmentation%2520strategy%2520for%2520brain%2520age%2520regression.%2520These%2520findings%2520position%250Atraining-time%2520variability%2520not%2520only%2520as%2520a%2520reproducibility%2520concern%2520but%2520also%2520as%2520a%250Aresource%2520that%2520can%2520be%2520harnessed%2520to%2520improve%2520robustness%2520and%2520enable%2520new%250Aapplications%2520in%2520neuroimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertain%20but%20Useful%3A%20Leveraging%20CNN%20Variability%20into%20Data%20Augmentation&entry.906535625=In%C3%A9s%20Gonzalez-Pepe%20and%20Vinuyan%20Sivakolunthu%20and%20Yohan%20Chatelain%20and%20Tristan%20Glatard&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20is%20rapidly%20advancing%20neuroimaging%20by%20achieving%0Astate-of-the-art%20performance%20with%20reduced%20computation%20times.%20Yet%20the%20numerical%0Astability%20of%20DL%20models%20--%20particularly%20during%20training%20--%20remains%0Aunderexplored.%20While%20inference%20with%20DL%20is%20relatively%20stable%2C%20training%0Aintroduces%20additional%20variability%20primarily%20through%20iterative%20stochastic%0Aoptimization.%20We%20investigate%20this%20training-time%20variability%20using%20FastSurfer%2C%20a%0ACNN-based%20whole-brain%20segmentation%20pipeline.%20Controlled%20perturbations%20are%0Aintroduced%20via%20floating%20point%20perturbations%20and%20random%20seeds.%20We%20find%20that%3A%20%28i%29%0AFastSurfer%20exhibits%20higher%20variability%20compared%20to%20that%20of%20a%20traditional%0Aneuroimaging%20pipeline%2C%20suggesting%20that%20DL%20inherits%20and%20is%20particularly%0Asusceptible%20to%20sources%20of%20instability%20present%20in%20its%20predecessors%3B%20%28ii%29%0Aensembles%20generated%20with%20perturbations%20achieve%20performance%20similar%20to%20an%0Aunperturbed%20baseline%3B%20and%20%28iii%29%20variability%20effectively%20produces%20ensembles%20of%0Anumerical%20model%20families%20that%20can%20be%20repurposed%20for%20downstream%20applications.%20As%0Aa%20proof%20of%20concept%2C%20we%20demonstrate%20that%20numerical%20ensembles%20can%20be%20used%20as%20a%0Adata%20augmentation%20strategy%20for%20brain%20age%20regression.%20These%20findings%20position%0Atraining-time%20variability%20not%20only%20as%20a%20reproducibility%20concern%20but%20also%20as%20a%0Aresource%20that%20can%20be%20harnessed%20to%20improve%20robustness%20and%20enable%20new%0Aapplications%20in%20neuroimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05238v1&entry.124074799=Read"},
{"title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly\n  Detection", "author": "Sidahmed Benabderrahmane and Talal Rahwan", "abstract": "  Advanced Persistent Threats (APTs) present a considerable challenge to\ncybersecurity due to their stealthy, long-duration nature. Traditional\nsupervised learning methods typically require large amounts of labeled data,\nwhich is often scarce in real-world scenarios. This paper introduces a novel\napproach that combines AutoEncoders for anomaly detection with active learning\nto iteratively enhance APT detection. By selectively querying an oracle for\nlabels on uncertain or ambiguous samples, our method reduces labeling costs\nwhile improving detection accuracy, enabling the model to effectively learn\nwith minimal data and reduce reliance on extensive manual labeling. We present\na comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based\nanomaly detection framework and demonstrate how the active learning loop\nprogressively enhances the model's performance. The framework is evaluated on\nreal-world, imbalanced provenance trace data from the DARPA Transparent\nComputing program, where APT-like attacks account for just 0.004\\% of the data.\nThe datasets, which cover multiple operating systems including Android, Linux,\nBSD, and Windows, are tested in two attack scenarios. The results show\nsubstantial improvements in detection rates during active learning,\noutperforming existing methods.\n", "link": "http://arxiv.org/abs/2509.04999v1", "date": "2025-09-05", "relevancy": 2.1938, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5908}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5287}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Augmentation%20and%20Active%20Sampling%20for%20Robust%20Cyber%20Anomaly%0A%20%20Detection&body=Title%3A%20Adversarial%20Augmentation%20and%20Active%20Sampling%20for%20Robust%20Cyber%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan%0AAbstract%3A%20%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20present%20a%20considerable%20challenge%20to%0Acybersecurity%20due%20to%20their%20stealthy%2C%20long-duration%20nature.%20Traditional%0Asupervised%20learning%20methods%20typically%20require%20large%20amounts%20of%20labeled%20data%2C%0Awhich%20is%20often%20scarce%20in%20real-world%20scenarios.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20combines%20AutoEncoders%20for%20anomaly%20detection%20with%20active%20learning%0Ato%20iteratively%20enhance%20APT%20detection.%20By%20selectively%20querying%20an%20oracle%20for%0Alabels%20on%20uncertain%20or%20ambiguous%20samples%2C%20our%20method%20reduces%20labeling%20costs%0Awhile%20improving%20detection%20accuracy%2C%20enabling%20the%20model%20to%20effectively%20learn%0Awith%20minimal%20data%20and%20reduce%20reliance%20on%20extensive%20manual%20labeling.%20We%20present%0Aa%20comprehensive%20formulation%20of%20the%20Attention%20Adversarial%20Dual%20AutoEncoder-based%0Aanomaly%20detection%20framework%20and%20demonstrate%20how%20the%20active%20learning%20loop%0Aprogressively%20enhances%20the%20model%27s%20performance.%20The%20framework%20is%20evaluated%20on%0Areal-world%2C%20imbalanced%20provenance%20trace%20data%20from%20the%20DARPA%20Transparent%0AComputing%20program%2C%20where%20APT-like%20attacks%20account%20for%20just%200.004%5C%25%20of%20the%20data.%0AThe%20datasets%2C%20which%20cover%20multiple%20operating%20systems%20including%20Android%2C%20Linux%2C%0ABSD%2C%20and%20Windows%2C%20are%20tested%20in%20two%20attack%20scenarios.%20The%20results%20show%0Asubstantial%20improvements%20in%20detection%20rates%20during%20active%20learning%2C%0Aoutperforming%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Augmentation%2520and%2520Active%2520Sampling%2520for%2520Robust%2520Cyber%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DSidahmed%2520Benabderrahmane%2520and%2520Talal%2520Rahwan%26entry.1292438233%3D%2520%2520Advanced%2520Persistent%2520Threats%2520%2528APTs%2529%2520present%2520a%2520considerable%2520challenge%2520to%250Acybersecurity%2520due%2520to%2520their%2520stealthy%252C%2520long-duration%2520nature.%2520Traditional%250Asupervised%2520learning%2520methods%2520typically%2520require%2520large%2520amounts%2520of%2520labeled%2520data%252C%250Awhich%2520is%2520often%2520scarce%2520in%2520real-world%2520scenarios.%2520This%2520paper%2520introduces%2520a%2520novel%250Aapproach%2520that%2520combines%2520AutoEncoders%2520for%2520anomaly%2520detection%2520with%2520active%2520learning%250Ato%2520iteratively%2520enhance%2520APT%2520detection.%2520By%2520selectively%2520querying%2520an%2520oracle%2520for%250Alabels%2520on%2520uncertain%2520or%2520ambiguous%2520samples%252C%2520our%2520method%2520reduces%2520labeling%2520costs%250Awhile%2520improving%2520detection%2520accuracy%252C%2520enabling%2520the%2520model%2520to%2520effectively%2520learn%250Awith%2520minimal%2520data%2520and%2520reduce%2520reliance%2520on%2520extensive%2520manual%2520labeling.%2520We%2520present%250Aa%2520comprehensive%2520formulation%2520of%2520the%2520Attention%2520Adversarial%2520Dual%2520AutoEncoder-based%250Aanomaly%2520detection%2520framework%2520and%2520demonstrate%2520how%2520the%2520active%2520learning%2520loop%250Aprogressively%2520enhances%2520the%2520model%2527s%2520performance.%2520The%2520framework%2520is%2520evaluated%2520on%250Areal-world%252C%2520imbalanced%2520provenance%2520trace%2520data%2520from%2520the%2520DARPA%2520Transparent%250AComputing%2520program%252C%2520where%2520APT-like%2520attacks%2520account%2520for%2520just%25200.004%255C%2525%2520of%2520the%2520data.%250AThe%2520datasets%252C%2520which%2520cover%2520multiple%2520operating%2520systems%2520including%2520Android%252C%2520Linux%252C%250ABSD%252C%2520and%2520Windows%252C%2520are%2520tested%2520in%2520two%2520attack%2520scenarios.%2520The%2520results%2520show%250Asubstantial%2520improvements%2520in%2520detection%2520rates%2520during%2520active%2520learning%252C%250Aoutperforming%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Augmentation%20and%20Active%20Sampling%20for%20Robust%20Cyber%20Anomaly%0A%20%20Detection&entry.906535625=Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan&entry.1292438233=%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20present%20a%20considerable%20challenge%20to%0Acybersecurity%20due%20to%20their%20stealthy%2C%20long-duration%20nature.%20Traditional%0Asupervised%20learning%20methods%20typically%20require%20large%20amounts%20of%20labeled%20data%2C%0Awhich%20is%20often%20scarce%20in%20real-world%20scenarios.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20combines%20AutoEncoders%20for%20anomaly%20detection%20with%20active%20learning%0Ato%20iteratively%20enhance%20APT%20detection.%20By%20selectively%20querying%20an%20oracle%20for%0Alabels%20on%20uncertain%20or%20ambiguous%20samples%2C%20our%20method%20reduces%20labeling%20costs%0Awhile%20improving%20detection%20accuracy%2C%20enabling%20the%20model%20to%20effectively%20learn%0Awith%20minimal%20data%20and%20reduce%20reliance%20on%20extensive%20manual%20labeling.%20We%20present%0Aa%20comprehensive%20formulation%20of%20the%20Attention%20Adversarial%20Dual%20AutoEncoder-based%0Aanomaly%20detection%20framework%20and%20demonstrate%20how%20the%20active%20learning%20loop%0Aprogressively%20enhances%20the%20model%27s%20performance.%20The%20framework%20is%20evaluated%20on%0Areal-world%2C%20imbalanced%20provenance%20trace%20data%20from%20the%20DARPA%20Transparent%0AComputing%20program%2C%20where%20APT-like%20attacks%20account%20for%20just%200.004%5C%25%20of%20the%20data.%0AThe%20datasets%2C%20which%20cover%20multiple%20operating%20systems%20including%20Android%2C%20Linux%2C%0ABSD%2C%20and%20Windows%2C%20are%20tested%20in%20two%20attack%20scenarios.%20The%20results%20show%0Asubstantial%20improvements%20in%20detection%20rates%20during%20active%20learning%2C%0Aoutperforming%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04999v1&entry.124074799=Read"},
{"title": "TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect\n  Classification", "author": "Darya Taratynova and Alya Almsouti and Beknur Kalmakhanbet and Numan Saeed and Mohammad Yaqub", "abstract": "  Congenital heart defect (CHD) detection in ultrasound videos is hindered by\nimage noise and probe positioning variability. While automated methods can\nreduce operator dependence, current machine learning approaches often neglect\ntemporal information, limit themselves to binary classification, and do not\naccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),\na method leveraging foundation image-text model and prompt-aware contrastive\nlearning to classify fetal CHD on cardiac ultrasound videos. TPA extracts\nfeatures from each frame of video subclips using an image encoder, aggregates\nthem with a trainable temporal extractor to capture heart motion, and aligns\nthe video representation with class-specific text prompts via a margin-hinge\ncontrastive loss. To enhance calibration for clinical reliability, we introduce\na Conditional Variational Autoencoder Style Modulation (CVAESM) module, which\nlearns a latent style vector to modulate embeddings and quantifies\nclassification uncertainty. Evaluated on a private dataset for CHD detection\nand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA\nachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while\nalso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On\nEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to\n58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital\nheart defect (CHD) classification in ultrasound videos that integrates temporal\nmodeling, prompt-aware contrastive learning, and uncertainty quantification.\n", "link": "http://arxiv.org/abs/2508.15298v5", "date": "2025-09-05", "relevancy": 2.189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.544}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification&body=Title%3A%20TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification%0AAuthor%3A%20Darya%20Taratynova%20and%20Alya%20Almsouti%20and%20Beknur%20Kalmakhanbet%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Congenital%20heart%20defect%20%28CHD%29%20detection%20in%20ultrasound%20videos%20is%20hindered%20by%0Aimage%20noise%20and%20probe%20positioning%20variability.%20While%20automated%20methods%20can%0Areduce%20operator%20dependence%2C%20current%20machine%20learning%20approaches%20often%20neglect%0Atemporal%20information%2C%20limit%20themselves%20to%20binary%20classification%2C%20and%20do%20not%0Aaccount%20for%20prediction%20calibration.%20We%20propose%20Temporal%20Prompt%20Alignment%20%28TPA%29%2C%0Aa%20method%20leveraging%20foundation%20image-text%20model%20and%20prompt-aware%20contrastive%0Alearning%20to%20classify%20fetal%20CHD%20on%20cardiac%20ultrasound%20videos.%20TPA%20extracts%0Afeatures%20from%20each%20frame%20of%20video%20subclips%20using%20an%20image%20encoder%2C%20aggregates%0Athem%20with%20a%20trainable%20temporal%20extractor%20to%20capture%20heart%20motion%2C%20and%20aligns%0Athe%20video%20representation%20with%20class-specific%20text%20prompts%20via%20a%20margin-hinge%0Acontrastive%20loss.%20To%20enhance%20calibration%20for%20clinical%20reliability%2C%20we%20introduce%0Aa%20Conditional%20Variational%20Autoencoder%20Style%20Modulation%20%28CVAESM%29%20module%2C%20which%0Alearns%20a%20latent%20style%20vector%20to%20modulate%20embeddings%20and%20quantifies%0Aclassification%20uncertainty.%20Evaluated%20on%20a%20private%20dataset%20for%20CHD%20detection%0Aand%20on%20a%20large%20public%20dataset%2C%20EchoNet-Dynamic%2C%20for%20systolic%20dysfunction%2C%20TPA%0Aachieves%20state-of-the-art%20macro%20F1%20scores%20of%2085.40%25%20for%20CHD%20diagnosis%2C%20while%0Aalso%20reducing%20expected%20calibration%20error%20by%205.38%25%20and%20adaptive%20ECE%20by%206.8%25.%20On%0AEchoNet-Dynamic%27s%20three-class%20task%2C%20it%20boosts%20macro%20F1%20by%204.73%25%20%28from%2053.89%25%20to%0A58.62%25%29.%20Temporal%20Prompt%20Alignment%20%28TPA%29%20is%20a%20framework%20for%20fetal%20congenital%0Aheart%20defect%20%28CHD%29%20classification%20in%20ultrasound%20videos%20that%20integrates%20temporal%0Amodeling%2C%20prompt-aware%20contrastive%20learning%2C%20and%20uncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15298v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPA%253A%2520Temporal%2520Prompt%2520Alignment%2520for%2520Fetal%2520Congenital%2520Heart%2520Defect%250A%2520%2520Classification%26entry.906535625%3DDarya%2520Taratynova%2520and%2520Alya%2520Almsouti%2520and%2520Beknur%2520Kalmakhanbet%2520and%2520Numan%2520Saeed%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Congenital%2520heart%2520defect%2520%2528CHD%2529%2520detection%2520in%2520ultrasound%2520videos%2520is%2520hindered%2520by%250Aimage%2520noise%2520and%2520probe%2520positioning%2520variability.%2520While%2520automated%2520methods%2520can%250Areduce%2520operator%2520dependence%252C%2520current%2520machine%2520learning%2520approaches%2520often%2520neglect%250Atemporal%2520information%252C%2520limit%2520themselves%2520to%2520binary%2520classification%252C%2520and%2520do%2520not%250Aaccount%2520for%2520prediction%2520calibration.%2520We%2520propose%2520Temporal%2520Prompt%2520Alignment%2520%2528TPA%2529%252C%250Aa%2520method%2520leveraging%2520foundation%2520image-text%2520model%2520and%2520prompt-aware%2520contrastive%250Alearning%2520to%2520classify%2520fetal%2520CHD%2520on%2520cardiac%2520ultrasound%2520videos.%2520TPA%2520extracts%250Afeatures%2520from%2520each%2520frame%2520of%2520video%2520subclips%2520using%2520an%2520image%2520encoder%252C%2520aggregates%250Athem%2520with%2520a%2520trainable%2520temporal%2520extractor%2520to%2520capture%2520heart%2520motion%252C%2520and%2520aligns%250Athe%2520video%2520representation%2520with%2520class-specific%2520text%2520prompts%2520via%2520a%2520margin-hinge%250Acontrastive%2520loss.%2520To%2520enhance%2520calibration%2520for%2520clinical%2520reliability%252C%2520we%2520introduce%250Aa%2520Conditional%2520Variational%2520Autoencoder%2520Style%2520Modulation%2520%2528CVAESM%2529%2520module%252C%2520which%250Alearns%2520a%2520latent%2520style%2520vector%2520to%2520modulate%2520embeddings%2520and%2520quantifies%250Aclassification%2520uncertainty.%2520Evaluated%2520on%2520a%2520private%2520dataset%2520for%2520CHD%2520detection%250Aand%2520on%2520a%2520large%2520public%2520dataset%252C%2520EchoNet-Dynamic%252C%2520for%2520systolic%2520dysfunction%252C%2520TPA%250Aachieves%2520state-of-the-art%2520macro%2520F1%2520scores%2520of%252085.40%2525%2520for%2520CHD%2520diagnosis%252C%2520while%250Aalso%2520reducing%2520expected%2520calibration%2520error%2520by%25205.38%2525%2520and%2520adaptive%2520ECE%2520by%25206.8%2525.%2520On%250AEchoNet-Dynamic%2527s%2520three-class%2520task%252C%2520it%2520boosts%2520macro%2520F1%2520by%25204.73%2525%2520%2528from%252053.89%2525%2520to%250A58.62%2525%2529.%2520Temporal%2520Prompt%2520Alignment%2520%2528TPA%2529%2520is%2520a%2520framework%2520for%2520fetal%2520congenital%250Aheart%2520defect%2520%2528CHD%2529%2520classification%2520in%2520ultrasound%2520videos%2520that%2520integrates%2520temporal%250Amodeling%252C%2520prompt-aware%2520contrastive%2520learning%252C%2520and%2520uncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15298v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification&entry.906535625=Darya%20Taratynova%20and%20Alya%20Almsouti%20and%20Beknur%20Kalmakhanbet%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Congenital%20heart%20defect%20%28CHD%29%20detection%20in%20ultrasound%20videos%20is%20hindered%20by%0Aimage%20noise%20and%20probe%20positioning%20variability.%20While%20automated%20methods%20can%0Areduce%20operator%20dependence%2C%20current%20machine%20learning%20approaches%20often%20neglect%0Atemporal%20information%2C%20limit%20themselves%20to%20binary%20classification%2C%20and%20do%20not%0Aaccount%20for%20prediction%20calibration.%20We%20propose%20Temporal%20Prompt%20Alignment%20%28TPA%29%2C%0Aa%20method%20leveraging%20foundation%20image-text%20model%20and%20prompt-aware%20contrastive%0Alearning%20to%20classify%20fetal%20CHD%20on%20cardiac%20ultrasound%20videos.%20TPA%20extracts%0Afeatures%20from%20each%20frame%20of%20video%20subclips%20using%20an%20image%20encoder%2C%20aggregates%0Athem%20with%20a%20trainable%20temporal%20extractor%20to%20capture%20heart%20motion%2C%20and%20aligns%0Athe%20video%20representation%20with%20class-specific%20text%20prompts%20via%20a%20margin-hinge%0Acontrastive%20loss.%20To%20enhance%20calibration%20for%20clinical%20reliability%2C%20we%20introduce%0Aa%20Conditional%20Variational%20Autoencoder%20Style%20Modulation%20%28CVAESM%29%20module%2C%20which%0Alearns%20a%20latent%20style%20vector%20to%20modulate%20embeddings%20and%20quantifies%0Aclassification%20uncertainty.%20Evaluated%20on%20a%20private%20dataset%20for%20CHD%20detection%0Aand%20on%20a%20large%20public%20dataset%2C%20EchoNet-Dynamic%2C%20for%20systolic%20dysfunction%2C%20TPA%0Aachieves%20state-of-the-art%20macro%20F1%20scores%20of%2085.40%25%20for%20CHD%20diagnosis%2C%20while%0Aalso%20reducing%20expected%20calibration%20error%20by%205.38%25%20and%20adaptive%20ECE%20by%206.8%25.%20On%0AEchoNet-Dynamic%27s%20three-class%20task%2C%20it%20boosts%20macro%20F1%20by%204.73%25%20%28from%2053.89%25%20to%0A58.62%25%29.%20Temporal%20Prompt%20Alignment%20%28TPA%29%20is%20a%20framework%20for%20fetal%20congenital%0Aheart%20defect%20%28CHD%29%20classification%20in%20ultrasound%20videos%20that%20integrates%20temporal%0Amodeling%2C%20prompt-aware%20contrastive%20learning%2C%20and%20uncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15298v5&entry.124074799=Read"},
{"title": "Net2Brain: A Toolbox to compare artificial vision models with human\n  brain responses", "author": "Domenic Bersch and Kshitij Dwivedi and Martina Vilas and Radoslaw M. Cichy and Gemma Roig", "abstract": "  We introduce Net2Brain, a graphical and command-line user interface toolbox\nfor comparing the representational spaces of artificial deep neural networks\n(DNNs) and human brain recordings. While different toolboxes facilitate only\nsingle functionalities or only focus on a small subset of supervised image\nclassification models, Net2Brain allows the extraction of activations of more\nthan 600 DNNs trained to perform a diverse range of vision-related tasks (e.g\nsemantic segmentation, depth estimation, action recognition, etc.), over both\nimage and video datasets. The toolbox computes the representational\ndissimilarity matrices (RDMs) over those activations and compares them to brain\nrecordings using representational similarity analysis (RSA), weighted RSA, both\nin specific ROIs and with searchlight search. In addition, it is possible to\nadd a new data set of stimuli and brain recordings to the toolbox for\nevaluation. We demonstrate the functionality and advantages of Net2Brain with\nan example showcasing how it can be used to test hypotheses of cognitive\ncomputational neuroscience.\n", "link": "http://arxiv.org/abs/2208.09677v4", "date": "2025-09-05", "relevancy": 2.17, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Net2Brain%3A%20A%20Toolbox%20to%20compare%20artificial%20vision%20models%20with%20human%0A%20%20brain%20responses&body=Title%3A%20Net2Brain%3A%20A%20Toolbox%20to%20compare%20artificial%20vision%20models%20with%20human%0A%20%20brain%20responses%0AAuthor%3A%20Domenic%20Bersch%20and%20Kshitij%20Dwivedi%20and%20Martina%20Vilas%20and%20Radoslaw%20M.%20Cichy%20and%20Gemma%20Roig%0AAbstract%3A%20%20%20We%20introduce%20Net2Brain%2C%20a%20graphical%20and%20command-line%20user%20interface%20toolbox%0Afor%20comparing%20the%20representational%20spaces%20of%20artificial%20deep%20neural%20networks%0A%28DNNs%29%20and%20human%20brain%20recordings.%20While%20different%20toolboxes%20facilitate%20only%0Asingle%20functionalities%20or%20only%20focus%20on%20a%20small%20subset%20of%20supervised%20image%0Aclassification%20models%2C%20Net2Brain%20allows%20the%20extraction%20of%20activations%20of%20more%0Athan%20600%20DNNs%20trained%20to%20perform%20a%20diverse%20range%20of%20vision-related%20tasks%20%28e.g%0Asemantic%20segmentation%2C%20depth%20estimation%2C%20action%20recognition%2C%20etc.%29%2C%20over%20both%0Aimage%20and%20video%20datasets.%20The%20toolbox%20computes%20the%20representational%0Adissimilarity%20matrices%20%28RDMs%29%20over%20those%20activations%20and%20compares%20them%20to%20brain%0Arecordings%20using%20representational%20similarity%20analysis%20%28RSA%29%2C%20weighted%20RSA%2C%20both%0Ain%20specific%20ROIs%20and%20with%20searchlight%20search.%20In%20addition%2C%20it%20is%20possible%20to%0Aadd%20a%20new%20data%20set%20of%20stimuli%20and%20brain%20recordings%20to%20the%20toolbox%20for%0Aevaluation.%20We%20demonstrate%20the%20functionality%20and%20advantages%20of%20Net2Brain%20with%0Aan%20example%20showcasing%20how%20it%20can%20be%20used%20to%20test%20hypotheses%20of%20cognitive%0Acomputational%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.09677v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNet2Brain%253A%2520A%2520Toolbox%2520to%2520compare%2520artificial%2520vision%2520models%2520with%2520human%250A%2520%2520brain%2520responses%26entry.906535625%3DDomenic%2520Bersch%2520and%2520Kshitij%2520Dwivedi%2520and%2520Martina%2520Vilas%2520and%2520Radoslaw%2520M.%2520Cichy%2520and%2520Gemma%2520Roig%26entry.1292438233%3D%2520%2520We%2520introduce%2520Net2Brain%252C%2520a%2520graphical%2520and%2520command-line%2520user%2520interface%2520toolbox%250Afor%2520comparing%2520the%2520representational%2520spaces%2520of%2520artificial%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520and%2520human%2520brain%2520recordings.%2520While%2520different%2520toolboxes%2520facilitate%2520only%250Asingle%2520functionalities%2520or%2520only%2520focus%2520on%2520a%2520small%2520subset%2520of%2520supervised%2520image%250Aclassification%2520models%252C%2520Net2Brain%2520allows%2520the%2520extraction%2520of%2520activations%2520of%2520more%250Athan%2520600%2520DNNs%2520trained%2520to%2520perform%2520a%2520diverse%2520range%2520of%2520vision-related%2520tasks%2520%2528e.g%250Asemantic%2520segmentation%252C%2520depth%2520estimation%252C%2520action%2520recognition%252C%2520etc.%2529%252C%2520over%2520both%250Aimage%2520and%2520video%2520datasets.%2520The%2520toolbox%2520computes%2520the%2520representational%250Adissimilarity%2520matrices%2520%2528RDMs%2529%2520over%2520those%2520activations%2520and%2520compares%2520them%2520to%2520brain%250Arecordings%2520using%2520representational%2520similarity%2520analysis%2520%2528RSA%2529%252C%2520weighted%2520RSA%252C%2520both%250Ain%2520specific%2520ROIs%2520and%2520with%2520searchlight%2520search.%2520In%2520addition%252C%2520it%2520is%2520possible%2520to%250Aadd%2520a%2520new%2520data%2520set%2520of%2520stimuli%2520and%2520brain%2520recordings%2520to%2520the%2520toolbox%2520for%250Aevaluation.%2520We%2520demonstrate%2520the%2520functionality%2520and%2520advantages%2520of%2520Net2Brain%2520with%250Aan%2520example%2520showcasing%2520how%2520it%2520can%2520be%2520used%2520to%2520test%2520hypotheses%2520of%2520cognitive%250Acomputational%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.09677v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Net2Brain%3A%20A%20Toolbox%20to%20compare%20artificial%20vision%20models%20with%20human%0A%20%20brain%20responses&entry.906535625=Domenic%20Bersch%20and%20Kshitij%20Dwivedi%20and%20Martina%20Vilas%20and%20Radoslaw%20M.%20Cichy%20and%20Gemma%20Roig&entry.1292438233=%20%20We%20introduce%20Net2Brain%2C%20a%20graphical%20and%20command-line%20user%20interface%20toolbox%0Afor%20comparing%20the%20representational%20spaces%20of%20artificial%20deep%20neural%20networks%0A%28DNNs%29%20and%20human%20brain%20recordings.%20While%20different%20toolboxes%20facilitate%20only%0Asingle%20functionalities%20or%20only%20focus%20on%20a%20small%20subset%20of%20supervised%20image%0Aclassification%20models%2C%20Net2Brain%20allows%20the%20extraction%20of%20activations%20of%20more%0Athan%20600%20DNNs%20trained%20to%20perform%20a%20diverse%20range%20of%20vision-related%20tasks%20%28e.g%0Asemantic%20segmentation%2C%20depth%20estimation%2C%20action%20recognition%2C%20etc.%29%2C%20over%20both%0Aimage%20and%20video%20datasets.%20The%20toolbox%20computes%20the%20representational%0Adissimilarity%20matrices%20%28RDMs%29%20over%20those%20activations%20and%20compares%20them%20to%20brain%0Arecordings%20using%20representational%20similarity%20analysis%20%28RSA%29%2C%20weighted%20RSA%2C%20both%0Ain%20specific%20ROIs%20and%20with%20searchlight%20search.%20In%20addition%2C%20it%20is%20possible%20to%0Aadd%20a%20new%20data%20set%20of%20stimuli%20and%20brain%20recordings%20to%20the%20toolbox%20for%0Aevaluation.%20We%20demonstrate%20the%20functionality%20and%20advantages%20of%20Net2Brain%20with%0Aan%20example%20showcasing%20how%20it%20can%20be%20used%20to%20test%20hypotheses%20of%20cognitive%0Acomputational%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.09677v4&entry.124074799=Read"},
{"title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding", "author": "Shuai Wang and Ivona Najdenkoska and Hongyi Zhu and Stevan Rudinac and Monika Kackovic and Nachoem Wijnberg and Marcel Worring", "abstract": "  Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.\n", "link": "http://arxiv.org/abs/2505.06020v2", "date": "2025-09-05", "relevancy": 2.1443, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding&body=Title%3A%20ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding%0AAuthor%3A%20Shuai%20Wang%20and%20Ivona%20Najdenkoska%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Monika%20Kackovic%20and%20Nachoem%20Wijnberg%20and%20Marcel%20Worring%0AAbstract%3A%20%20%20Understanding%20visual%20art%20requires%20reasoning%20across%20multiple%20perspectives%20--%0Acultural%2C%20historical%2C%20and%20stylistic%20--%20beyond%20mere%20object%20recognition.%20While%0Arecent%20multimodal%20large%20language%20models%20%28MLLMs%29%20perform%20well%20on%20general%20image%0Acaptioning%2C%20they%20often%20fail%20to%20capture%20the%20nuanced%20interpretations%20that%20fine%0Aart%20demands.%20We%20propose%20ArtRAG%2C%20a%20novel%2C%20training-free%20framework%20that%20combines%0Astructured%20knowledge%20with%20retrieval-augmented%20generation%20%28RAG%29%20for%0Amulti-perspective%20artwork%20explanation.%20ArtRAG%20automatically%20constructs%20an%20Art%0AContext%20Knowledge%20Graph%20%28ACKG%29%20from%20domain-specific%20textual%20sources%2C%20organizing%0Aentities%20such%20as%20artists%2C%20movements%2C%20themes%2C%20and%20historical%20events%20into%20a%20rich%2C%0Ainterpretable%20graph.%20At%20inference%20time%2C%20a%20multi-granular%20structured%20retriever%0Aselects%20semantically%20and%20topologically%20relevant%20subgraphs%20to%20guide%20generation.%0AThis%20enables%20MLLMs%20to%20produce%20contextually%20grounded%2C%20culturally%20informed%20art%0Adescriptions.%20Experiments%20on%20the%20SemArt%20and%20Artpedia%20datasets%20show%20that%20ArtRAG%0Aoutperforms%20several%20heavily%20trained%20baselines.%20Human%20evaluations%20further%0Aconfirm%20that%20ArtRAG%20generates%20coherent%2C%20insightful%2C%20and%20culturally%20enriched%0Ainterpretations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtRAG%253A%2520Retrieval-Augmented%2520Generation%2520with%2520Structured%2520Context%2520for%250A%2520%2520Visual%2520Art%2520Understanding%26entry.906535625%3DShuai%2520Wang%2520and%2520Ivona%2520Najdenkoska%2520and%2520Hongyi%2520Zhu%2520and%2520Stevan%2520Rudinac%2520and%2520Monika%2520Kackovic%2520and%2520Nachoem%2520Wijnberg%2520and%2520Marcel%2520Worring%26entry.1292438233%3D%2520%2520Understanding%2520visual%2520art%2520requires%2520reasoning%2520across%2520multiple%2520perspectives%2520--%250Acultural%252C%2520historical%252C%2520and%2520stylistic%2520--%2520beyond%2520mere%2520object%2520recognition.%2520While%250Arecent%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520perform%2520well%2520on%2520general%2520image%250Acaptioning%252C%2520they%2520often%2520fail%2520to%2520capture%2520the%2520nuanced%2520interpretations%2520that%2520fine%250Aart%2520demands.%2520We%2520propose%2520ArtRAG%252C%2520a%2520novel%252C%2520training-free%2520framework%2520that%2520combines%250Astructured%2520knowledge%2520with%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520for%250Amulti-perspective%2520artwork%2520explanation.%2520ArtRAG%2520automatically%2520constructs%2520an%2520Art%250AContext%2520Knowledge%2520Graph%2520%2528ACKG%2529%2520from%2520domain-specific%2520textual%2520sources%252C%2520organizing%250Aentities%2520such%2520as%2520artists%252C%2520movements%252C%2520themes%252C%2520and%2520historical%2520events%2520into%2520a%2520rich%252C%250Ainterpretable%2520graph.%2520At%2520inference%2520time%252C%2520a%2520multi-granular%2520structured%2520retriever%250Aselects%2520semantically%2520and%2520topologically%2520relevant%2520subgraphs%2520to%2520guide%2520generation.%250AThis%2520enables%2520MLLMs%2520to%2520produce%2520contextually%2520grounded%252C%2520culturally%2520informed%2520art%250Adescriptions.%2520Experiments%2520on%2520the%2520SemArt%2520and%2520Artpedia%2520datasets%2520show%2520that%2520ArtRAG%250Aoutperforms%2520several%2520heavily%2520trained%2520baselines.%2520Human%2520evaluations%2520further%250Aconfirm%2520that%2520ArtRAG%2520generates%2520coherent%252C%2520insightful%252C%2520and%2520culturally%2520enriched%250Ainterpretations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding&entry.906535625=Shuai%20Wang%20and%20Ivona%20Najdenkoska%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Monika%20Kackovic%20and%20Nachoem%20Wijnberg%20and%20Marcel%20Worring&entry.1292438233=%20%20Understanding%20visual%20art%20requires%20reasoning%20across%20multiple%20perspectives%20--%0Acultural%2C%20historical%2C%20and%20stylistic%20--%20beyond%20mere%20object%20recognition.%20While%0Arecent%20multimodal%20large%20language%20models%20%28MLLMs%29%20perform%20well%20on%20general%20image%0Acaptioning%2C%20they%20often%20fail%20to%20capture%20the%20nuanced%20interpretations%20that%20fine%0Aart%20demands.%20We%20propose%20ArtRAG%2C%20a%20novel%2C%20training-free%20framework%20that%20combines%0Astructured%20knowledge%20with%20retrieval-augmented%20generation%20%28RAG%29%20for%0Amulti-perspective%20artwork%20explanation.%20ArtRAG%20automatically%20constructs%20an%20Art%0AContext%20Knowledge%20Graph%20%28ACKG%29%20from%20domain-specific%20textual%20sources%2C%20organizing%0Aentities%20such%20as%20artists%2C%20movements%2C%20themes%2C%20and%20historical%20events%20into%20a%20rich%2C%0Ainterpretable%20graph.%20At%20inference%20time%2C%20a%20multi-granular%20structured%20retriever%0Aselects%20semantically%20and%20topologically%20relevant%20subgraphs%20to%20guide%20generation.%0AThis%20enables%20MLLMs%20to%20produce%20contextually%20grounded%2C%20culturally%20informed%20art%0Adescriptions.%20Experiments%20on%20the%20SemArt%20and%20Artpedia%20datasets%20show%20that%20ArtRAG%0Aoutperforms%20several%20heavily%20trained%20baselines.%20Human%20evaluations%20further%0Aconfirm%20that%20ArtRAG%20generates%20coherent%2C%20insightful%2C%20and%20culturally%20enriched%0Ainterpretations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06020v2&entry.124074799=Read"},
{"title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal\n  Comprehension Capability of MLLMs", "author": "Zitian Wang and Yue Liao and Kang Rong and Fengyun Rao and Yibo Yang and Si Liu", "abstract": "  Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.\n", "link": "http://arxiv.org/abs/2503.20309v2", "date": "2025-09-05", "relevancy": 2.1248, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Oriented%20Preference%20Alignment%20for%20Enhancing%20Multi-Modal%0A%20%20Comprehension%20Capability%20of%20MLLMs&body=Title%3A%20Instruction-Oriented%20Preference%20Alignment%20for%20Enhancing%20Multi-Modal%0A%20%20Comprehension%20Capability%20of%20MLLMs%0AAuthor%3A%20Zitian%20Wang%20and%20Yue%20Liao%20and%20Kang%20Rong%20and%20Fengyun%20Rao%20and%20Yibo%20Yang%20and%20Si%20Liu%0AAbstract%3A%20%20%20Preference%20alignment%20has%20emerged%20as%20an%20effective%20strategy%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20following%20supervised%0Afine-tuning.%20While%20existing%20preference%20alignment%20methods%20predominantly%20target%0Ahallucination%20factors%2C%20they%20overlook%20the%20factors%20essential%20for%20multi-modal%0Acomprehension%20capabilities%2C%20often%20narrowing%20their%20improvements%20on%20hallucination%0Amitigation.%20To%20bridge%20this%20gap%2C%20we%20propose%20Instruction-oriented%20Preference%0AAlignment%20%28IPA%29%2C%20a%20scalable%20framework%20designed%20to%20automatically%20construct%0Aalignment%20preferences%20grounded%20in%20instruction%20fulfillment%20efficacy.%20Our%20method%0Ainvolves%20an%20automated%20preference%20construction%20coupled%20with%20a%20dedicated%0Averification%20process%20that%20identifies%20instruction-oriented%20factors%2C%20avoiding%0Asignificant%20variability%20in%20response%20representations.%20Additionally%2C%20IPA%0Aincorporates%20a%20progressive%20preference%20collection%20pipeline%2C%20further%20recalling%0Achallenging%20samples%20through%20model%20self-evolution%20and%20reference-guided%0Arefinement.%20Experiments%20conducted%20on%20Qwen2VL-7B%20demonstrate%20IPA%27s%20effectiveness%0Aacross%20multiple%20benchmarks%2C%20including%20hallucination%20evaluation%2C%20visual%20question%0Aanswering%2C%20and%20text%20understanding%20tasks%2C%20highlighting%20its%20capability%20to%20enhance%0Ageneral%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Oriented%2520Preference%2520Alignment%2520for%2520Enhancing%2520Multi-Modal%250A%2520%2520Comprehension%2520Capability%2520of%2520MLLMs%26entry.906535625%3DZitian%2520Wang%2520and%2520Yue%2520Liao%2520and%2520Kang%2520Rong%2520and%2520Fengyun%2520Rao%2520and%2520Yibo%2520Yang%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520Preference%2520alignment%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520enhance%2520the%250Aperformance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520following%2520supervised%250Afine-tuning.%2520While%2520existing%2520preference%2520alignment%2520methods%2520predominantly%2520target%250Ahallucination%2520factors%252C%2520they%2520overlook%2520the%2520factors%2520essential%2520for%2520multi-modal%250Acomprehension%2520capabilities%252C%2520often%2520narrowing%2520their%2520improvements%2520on%2520hallucination%250Amitigation.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Instruction-oriented%2520Preference%250AAlignment%2520%2528IPA%2529%252C%2520a%2520scalable%2520framework%2520designed%2520to%2520automatically%2520construct%250Aalignment%2520preferences%2520grounded%2520in%2520instruction%2520fulfillment%2520efficacy.%2520Our%2520method%250Ainvolves%2520an%2520automated%2520preference%2520construction%2520coupled%2520with%2520a%2520dedicated%250Averification%2520process%2520that%2520identifies%2520instruction-oriented%2520factors%252C%2520avoiding%250Asignificant%2520variability%2520in%2520response%2520representations.%2520Additionally%252C%2520IPA%250Aincorporates%2520a%2520progressive%2520preference%2520collection%2520pipeline%252C%2520further%2520recalling%250Achallenging%2520samples%2520through%2520model%2520self-evolution%2520and%2520reference-guided%250Arefinement.%2520Experiments%2520conducted%2520on%2520Qwen2VL-7B%2520demonstrate%2520IPA%2527s%2520effectiveness%250Aacross%2520multiple%2520benchmarks%252C%2520including%2520hallucination%2520evaluation%252C%2520visual%2520question%250Aanswering%252C%2520and%2520text%2520understanding%2520tasks%252C%2520highlighting%2520its%2520capability%2520to%2520enhance%250Ageneral%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Oriented%20Preference%20Alignment%20for%20Enhancing%20Multi-Modal%0A%20%20Comprehension%20Capability%20of%20MLLMs&entry.906535625=Zitian%20Wang%20and%20Yue%20Liao%20and%20Kang%20Rong%20and%20Fengyun%20Rao%20and%20Yibo%20Yang%20and%20Si%20Liu&entry.1292438233=%20%20Preference%20alignment%20has%20emerged%20as%20an%20effective%20strategy%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20following%20supervised%0Afine-tuning.%20While%20existing%20preference%20alignment%20methods%20predominantly%20target%0Ahallucination%20factors%2C%20they%20overlook%20the%20factors%20essential%20for%20multi-modal%0Acomprehension%20capabilities%2C%20often%20narrowing%20their%20improvements%20on%20hallucination%0Amitigation.%20To%20bridge%20this%20gap%2C%20we%20propose%20Instruction-oriented%20Preference%0AAlignment%20%28IPA%29%2C%20a%20scalable%20framework%20designed%20to%20automatically%20construct%0Aalignment%20preferences%20grounded%20in%20instruction%20fulfillment%20efficacy.%20Our%20method%0Ainvolves%20an%20automated%20preference%20construction%20coupled%20with%20a%20dedicated%0Averification%20process%20that%20identifies%20instruction-oriented%20factors%2C%20avoiding%0Asignificant%20variability%20in%20response%20representations.%20Additionally%2C%20IPA%0Aincorporates%20a%20progressive%20preference%20collection%20pipeline%2C%20further%20recalling%0Achallenging%20samples%20through%20model%20self-evolution%20and%20reference-guided%0Arefinement.%20Experiments%20conducted%20on%20Qwen2VL-7B%20demonstrate%20IPA%27s%20effectiveness%0Aacross%20multiple%20benchmarks%2C%20including%20hallucination%20evaluation%2C%20visual%20question%0Aanswering%2C%20and%20text%20understanding%20tasks%2C%20highlighting%20its%20capability%20to%20enhance%0Ageneral%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20309v2&entry.124074799=Read"},
{"title": "Imitating and Finetuning Model Predictive Control for Robust and\n  Symmetric Quadrupedal Locomotion", "author": "Donghoon Youm and Hyunyoung Jung and Hyeongjun Kim and Jemin Hwangbo and Hae-Won Park and Sehoon Ha", "abstract": "  Control of legged robots is a challenging problem that has been investigated\nby different approaches, such as model-based control and learning algorithms.\nThis work proposes a novel Imitating and Finetuning Model Predictive Control\n(IFM) framework to take the strengths of both approaches. Our framework first\ndevelops a conventional model predictive controller (MPC) using Differential\nDynamic Programming and Raibert heuristic, which serves as an expert policy.\nThen we train a clone of the MPC using imitation learning to make the\ncontroller learnable. Finally, we leverage deep reinforcement learning with\nlimited exploration for further finetuning the policy on more challenging\nterrains. By conducting comprehensive simulation and hardware experiments, we\ndemonstrate that the proposed IFM framework can significantly improve the\nperformance of the given MPC controller on rough, slippery, and conveyor\nterrains that require careful coordination of footsteps. We also showcase that\nIFM can efficiently produce more symmetric, periodic, and energy-efficient\ngaits compared to Vanilla RL with a minimal burden of reward shaping.\n", "link": "http://arxiv.org/abs/2311.02304v2", "date": "2025-09-05", "relevancy": 2.114, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6034}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5171}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitating%20and%20Finetuning%20Model%20Predictive%20Control%20for%20Robust%20and%0A%20%20Symmetric%20Quadrupedal%20Locomotion&body=Title%3A%20Imitating%20and%20Finetuning%20Model%20Predictive%20Control%20for%20Robust%20and%0A%20%20Symmetric%20Quadrupedal%20Locomotion%0AAuthor%3A%20Donghoon%20Youm%20and%20Hyunyoung%20Jung%20and%20Hyeongjun%20Kim%20and%20Jemin%20Hwangbo%20and%20Hae-Won%20Park%20and%20Sehoon%20Ha%0AAbstract%3A%20%20%20Control%20of%20legged%20robots%20is%20a%20challenging%20problem%20that%20has%20been%20investigated%0Aby%20different%20approaches%2C%20such%20as%20model-based%20control%20and%20learning%20algorithms.%0AThis%20work%20proposes%20a%20novel%20Imitating%20and%20Finetuning%20Model%20Predictive%20Control%0A%28IFM%29%20framework%20to%20take%20the%20strengths%20of%20both%20approaches.%20Our%20framework%20first%0Adevelops%20a%20conventional%20model%20predictive%20controller%20%28MPC%29%20using%20Differential%0ADynamic%20Programming%20and%20Raibert%20heuristic%2C%20which%20serves%20as%20an%20expert%20policy.%0AThen%20we%20train%20a%20clone%20of%20the%20MPC%20using%20imitation%20learning%20to%20make%20the%0Acontroller%20learnable.%20Finally%2C%20we%20leverage%20deep%20reinforcement%20learning%20with%0Alimited%20exploration%20for%20further%20finetuning%20the%20policy%20on%20more%20challenging%0Aterrains.%20By%20conducting%20comprehensive%20simulation%20and%20hardware%20experiments%2C%20we%0Ademonstrate%20that%20the%20proposed%20IFM%20framework%20can%20significantly%20improve%20the%0Aperformance%20of%20the%20given%20MPC%20controller%20on%20rough%2C%20slippery%2C%20and%20conveyor%0Aterrains%20that%20require%20careful%20coordination%20of%20footsteps.%20We%20also%20showcase%20that%0AIFM%20can%20efficiently%20produce%20more%20symmetric%2C%20periodic%2C%20and%20energy-efficient%0Agaits%20compared%20to%20Vanilla%20RL%20with%20a%20minimal%20burden%20of%20reward%20shaping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitating%2520and%2520Finetuning%2520Model%2520Predictive%2520Control%2520for%2520Robust%2520and%250A%2520%2520Symmetric%2520Quadrupedal%2520Locomotion%26entry.906535625%3DDonghoon%2520Youm%2520and%2520Hyunyoung%2520Jung%2520and%2520Hyeongjun%2520Kim%2520and%2520Jemin%2520Hwangbo%2520and%2520Hae-Won%2520Park%2520and%2520Sehoon%2520Ha%26entry.1292438233%3D%2520%2520Control%2520of%2520legged%2520robots%2520is%2520a%2520challenging%2520problem%2520that%2520has%2520been%2520investigated%250Aby%2520different%2520approaches%252C%2520such%2520as%2520model-based%2520control%2520and%2520learning%2520algorithms.%250AThis%2520work%2520proposes%2520a%2520novel%2520Imitating%2520and%2520Finetuning%2520Model%2520Predictive%2520Control%250A%2528IFM%2529%2520framework%2520to%2520take%2520the%2520strengths%2520of%2520both%2520approaches.%2520Our%2520framework%2520first%250Adevelops%2520a%2520conventional%2520model%2520predictive%2520controller%2520%2528MPC%2529%2520using%2520Differential%250ADynamic%2520Programming%2520and%2520Raibert%2520heuristic%252C%2520which%2520serves%2520as%2520an%2520expert%2520policy.%250AThen%2520we%2520train%2520a%2520clone%2520of%2520the%2520MPC%2520using%2520imitation%2520learning%2520to%2520make%2520the%250Acontroller%2520learnable.%2520Finally%252C%2520we%2520leverage%2520deep%2520reinforcement%2520learning%2520with%250Alimited%2520exploration%2520for%2520further%2520finetuning%2520the%2520policy%2520on%2520more%2520challenging%250Aterrains.%2520By%2520conducting%2520comprehensive%2520simulation%2520and%2520hardware%2520experiments%252C%2520we%250Ademonstrate%2520that%2520the%2520proposed%2520IFM%2520framework%2520can%2520significantly%2520improve%2520the%250Aperformance%2520of%2520the%2520given%2520MPC%2520controller%2520on%2520rough%252C%2520slippery%252C%2520and%2520conveyor%250Aterrains%2520that%2520require%2520careful%2520coordination%2520of%2520footsteps.%2520We%2520also%2520showcase%2520that%250AIFM%2520can%2520efficiently%2520produce%2520more%2520symmetric%252C%2520periodic%252C%2520and%2520energy-efficient%250Agaits%2520compared%2520to%2520Vanilla%2520RL%2520with%2520a%2520minimal%2520burden%2520of%2520reward%2520shaping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitating%20and%20Finetuning%20Model%20Predictive%20Control%20for%20Robust%20and%0A%20%20Symmetric%20Quadrupedal%20Locomotion&entry.906535625=Donghoon%20Youm%20and%20Hyunyoung%20Jung%20and%20Hyeongjun%20Kim%20and%20Jemin%20Hwangbo%20and%20Hae-Won%20Park%20and%20Sehoon%20Ha&entry.1292438233=%20%20Control%20of%20legged%20robots%20is%20a%20challenging%20problem%20that%20has%20been%20investigated%0Aby%20different%20approaches%2C%20such%20as%20model-based%20control%20and%20learning%20algorithms.%0AThis%20work%20proposes%20a%20novel%20Imitating%20and%20Finetuning%20Model%20Predictive%20Control%0A%28IFM%29%20framework%20to%20take%20the%20strengths%20of%20both%20approaches.%20Our%20framework%20first%0Adevelops%20a%20conventional%20model%20predictive%20controller%20%28MPC%29%20using%20Differential%0ADynamic%20Programming%20and%20Raibert%20heuristic%2C%20which%20serves%20as%20an%20expert%20policy.%0AThen%20we%20train%20a%20clone%20of%20the%20MPC%20using%20imitation%20learning%20to%20make%20the%0Acontroller%20learnable.%20Finally%2C%20we%20leverage%20deep%20reinforcement%20learning%20with%0Alimited%20exploration%20for%20further%20finetuning%20the%20policy%20on%20more%20challenging%0Aterrains.%20By%20conducting%20comprehensive%20simulation%20and%20hardware%20experiments%2C%20we%0Ademonstrate%20that%20the%20proposed%20IFM%20framework%20can%20significantly%20improve%20the%0Aperformance%20of%20the%20given%20MPC%20controller%20on%20rough%2C%20slippery%2C%20and%20conveyor%0Aterrains%20that%20require%20careful%20coordination%20of%20footsteps.%20We%20also%20showcase%20that%0AIFM%20can%20efficiently%20produce%20more%20symmetric%2C%20periodic%2C%20and%20energy-efficient%0Agaits%20compared%20to%20Vanilla%20RL%20with%20a%20minimal%20burden%20of%20reward%20shaping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02304v2&entry.124074799=Read"},
{"title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning", "author": "Haoming Wang and Haoyang Zou and Huatong Song and Jiazhan Feng and Junjie Fang and Junting Lu and Longxiang Liu and Qinyu Luo and Shihao Liang and Shijue Huang and Wanjun Zhong and Yining Ye and Yujia Qin and Yuwen Xiong and Yuxin Song and Zhiyong Wu and Aoyan Li and Bo Li and Chen Dun and Chong Liu and Daoguang Zan and Fuxing Leng and Hanbin Wang and Hao Yu and Haobin Chen and Hongyi Guo and Jing Su and Jingjia Huang and Kai Shen and Kaiyu Shi and Lin Yan and Peiyao Zhao and Pengfei Liu and Qinghao Ye and Renjie Zheng and Shulin Xin and Wayne Xin Zhao and Wen Heng and Wenhao Huang and Wenqian Wang and Xiaobo Qin and Yi Lin and Youbin Wu and Zehui Chen and Zihao Wang and Baoquan Zhong and Xinchun Zhang and Xujing Li and Yuanfan Li and Zhongkai Zhao and Chengquan Jiang and Faming Wu and Haotian Zhou and Jinlin Pang and Li Han and Qi Liu and Qianli Ma and Siyao Liu and Songhua Cai and Wenqi Fu and Xin Liu and Yaohui Wang and Zhi Zhang and Bo Zhou and Guoliang Li and Jiajun Shi and Jiale Yang and Jie Tang and Li Li and Qihua Han and Taoran Lu and Woyu Lin and Xiaokang Tong and Xinyao Li and Yichi Zhang and Yu Miao and Zhengxuan Jiang and Zili Li and Ziyuan Zhao and Chenxin Li and Dehua Ma and Feng Lin and Ge Zhang and Haihua Yang and Hangyu Guo and Hongda Zhu and Jiaheng Liu and Junda Du and Kai Cai and Kuanye Li and Lichen Yuan and Meilan Han and Minchao Wang and Shuyue Guo and Tianhao Cheng and Xiaobo Ma and Xiaojun Xiao and Xiaolong Huang and Xinjie Chen and Yidi Du and Yilin Chen and Yiwen Wang and Zhaojian Li and Zhenzhu Yang and Zhiyuan Zeng and Chaolin Jin and Chen Li and Hao Chen and Haoli Chen and Jian Chen and Qinghao Zhao and Guang Shi", "abstract": "  The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios.\n", "link": "http://arxiv.org/abs/2509.02544v2", "date": "2025-09-05", "relevancy": 2.1088, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5448}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5247}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-TARS-2%20Technical%20Report%3A%20Advancing%20GUI%20Agent%20with%20Multi-Turn%0A%20%20Reinforcement%20Learning&body=Title%3A%20UI-TARS-2%20Technical%20Report%3A%20Advancing%20GUI%20Agent%20with%20Multi-Turn%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Haoming%20Wang%20and%20Haoyang%20Zou%20and%20Huatong%20Song%20and%20Jiazhan%20Feng%20and%20Junjie%20Fang%20and%20Junting%20Lu%20and%20Longxiang%20Liu%20and%20Qinyu%20Luo%20and%20Shihao%20Liang%20and%20Shijue%20Huang%20and%20Wanjun%20Zhong%20and%20Yining%20Ye%20and%20Yujia%20Qin%20and%20Yuwen%20Xiong%20and%20Yuxin%20Song%20and%20Zhiyong%20Wu%20and%20Aoyan%20Li%20and%20Bo%20Li%20and%20Chen%20Dun%20and%20Chong%20Liu%20and%20Daoguang%20Zan%20and%20Fuxing%20Leng%20and%20Hanbin%20Wang%20and%20Hao%20Yu%20and%20Haobin%20Chen%20and%20Hongyi%20Guo%20and%20Jing%20Su%20and%20Jingjia%20Huang%20and%20Kai%20Shen%20and%20Kaiyu%20Shi%20and%20Lin%20Yan%20and%20Peiyao%20Zhao%20and%20Pengfei%20Liu%20and%20Qinghao%20Ye%20and%20Renjie%20Zheng%20and%20Shulin%20Xin%20and%20Wayne%20Xin%20Zhao%20and%20Wen%20Heng%20and%20Wenhao%20Huang%20and%20Wenqian%20Wang%20and%20Xiaobo%20Qin%20and%20Yi%20Lin%20and%20Youbin%20Wu%20and%20Zehui%20Chen%20and%20Zihao%20Wang%20and%20Baoquan%20Zhong%20and%20Xinchun%20Zhang%20and%20Xujing%20Li%20and%20Yuanfan%20Li%20and%20Zhongkai%20Zhao%20and%20Chengquan%20Jiang%20and%20Faming%20Wu%20and%20Haotian%20Zhou%20and%20Jinlin%20Pang%20and%20Li%20Han%20and%20Qi%20Liu%20and%20Qianli%20Ma%20and%20Siyao%20Liu%20and%20Songhua%20Cai%20and%20Wenqi%20Fu%20and%20Xin%20Liu%20and%20Yaohui%20Wang%20and%20Zhi%20Zhang%20and%20Bo%20Zhou%20and%20Guoliang%20Li%20and%20Jiajun%20Shi%20and%20Jiale%20Yang%20and%20Jie%20Tang%20and%20Li%20Li%20and%20Qihua%20Han%20and%20Taoran%20Lu%20and%20Woyu%20Lin%20and%20Xiaokang%20Tong%20and%20Xinyao%20Li%20and%20Yichi%20Zhang%20and%20Yu%20Miao%20and%20Zhengxuan%20Jiang%20and%20Zili%20Li%20and%20Ziyuan%20Zhao%20and%20Chenxin%20Li%20and%20Dehua%20Ma%20and%20Feng%20Lin%20and%20Ge%20Zhang%20and%20Haihua%20Yang%20and%20Hangyu%20Guo%20and%20Hongda%20Zhu%20and%20Jiaheng%20Liu%20and%20Junda%20Du%20and%20Kai%20Cai%20and%20Kuanye%20Li%20and%20Lichen%20Yuan%20and%20Meilan%20Han%20and%20Minchao%20Wang%20and%20Shuyue%20Guo%20and%20Tianhao%20Cheng%20and%20Xiaobo%20Ma%20and%20Xiaojun%20Xiao%20and%20Xiaolong%20Huang%20and%20Xinjie%20Chen%20and%20Yidi%20Du%20and%20Yilin%20Chen%20and%20Yiwen%20Wang%20and%20Zhaojian%20Li%20and%20Zhenzhu%20Yang%20and%20Zhiyuan%20Zeng%20and%20Chaolin%20Jin%20and%20Chen%20Li%20and%20Hao%20Chen%20and%20Haoli%20Chen%20and%20Jian%20Chen%20and%20Qinghao%20Zhao%20and%20Guang%20Shi%0AAbstract%3A%20%20%20The%20development%20of%20autonomous%20agents%20for%20graphical%20user%20interfaces%20%28GUIs%29%0Apresents%20major%20challenges%20in%20artificial%20intelligence.%20While%20recent%20advances%20in%0Anative%20agent%20models%20have%20shown%20promise%20by%20unifying%20perception%2C%20reasoning%2C%0Aaction%2C%20and%20memory%20through%20end-to-end%20learning%2C%20open%20problems%20remain%20in%20data%0Ascalability%2C%20multi-turn%20reinforcement%20learning%20%28RL%29%2C%20the%20limitations%20of%0AGUI-only%20operation%2C%20and%20environment%20stability.%20In%20this%20technical%20report%2C%20we%0Apresent%20UI-TARS-2%2C%20a%20native%20GUI-centered%20agent%20model%20that%20addresses%20these%0Achallenges%20through%20a%20systematic%20training%20methodology%3A%20a%20data%20flywheel%20for%0Ascalable%20data%20generation%2C%20a%20stabilized%20multi-turn%20RL%20framework%2C%20a%20hybrid%20GUI%0Aenvironment%20that%20integrates%20file%20systems%20and%20terminals%2C%20and%20a%20unified%20sandbox%0Aplatform%20for%20large-scale%20rollouts.%20Empirical%20evaluation%20demonstrates%20that%0AUI-TARS-2%20achieves%20significant%20improvements%20over%20its%20predecessor%20UI-TARS-1.5.%0AOn%20GUI%20benchmarks%2C%20it%20reaches%2088.2%20on%20Online-Mind2Web%2C%2047.5%20on%20OSWorld%2C%2050.6%20on%0AWindowsAgentArena%2C%20and%2073.3%20on%20AndroidWorld%2C%20outperforming%20strong%20baselines%0Asuch%20as%20Claude%20and%20OpenAI%20agents.%20In%20game%20environments%2C%20it%20attains%20a%20mean%0Anormalized%20score%20of%2059.8%20across%20a%2015-game%20suite-roughly%2060%25%20of%20human-level%0Aperformance-and%20remains%20competitive%20with%20frontier%20proprietary%20models%20%28e.g.%2C%0AOpenAI%20o3%29%20on%20LMGame-Bench.%20Additionally%2C%20the%20model%20can%20generalize%20to%0Along-horizon%20information-seeking%20tasks%20and%20software%20engineering%20benchmarks%2C%0Ahighlighting%20its%20robustness%20across%20diverse%20agent%20tasks.%20Detailed%20analyses%20of%0Atraining%20dynamics%20further%20provide%20insights%20into%20achieving%20stability%20and%0Aefficiency%20in%20large-scale%20agent%20RL.%20These%20results%20underscore%20UI-TARS-2%27s%0Apotential%20to%20advance%20the%20state%20of%20GUI%20agents%20and%20exhibit%20strong%20generalization%0Ato%20real-world%20interactive%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-TARS-2%2520Technical%2520Report%253A%2520Advancing%2520GUI%2520Agent%2520with%2520Multi-Turn%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DHaoming%2520Wang%2520and%2520Haoyang%2520Zou%2520and%2520Huatong%2520Song%2520and%2520Jiazhan%2520Feng%2520and%2520Junjie%2520Fang%2520and%2520Junting%2520Lu%2520and%2520Longxiang%2520Liu%2520and%2520Qinyu%2520Luo%2520and%2520Shihao%2520Liang%2520and%2520Shijue%2520Huang%2520and%2520Wanjun%2520Zhong%2520and%2520Yining%2520Ye%2520and%2520Yujia%2520Qin%2520and%2520Yuwen%2520Xiong%2520and%2520Yuxin%2520Song%2520and%2520Zhiyong%2520Wu%2520and%2520Aoyan%2520Li%2520and%2520Bo%2520Li%2520and%2520Chen%2520Dun%2520and%2520Chong%2520Liu%2520and%2520Daoguang%2520Zan%2520and%2520Fuxing%2520Leng%2520and%2520Hanbin%2520Wang%2520and%2520Hao%2520Yu%2520and%2520Haobin%2520Chen%2520and%2520Hongyi%2520Guo%2520and%2520Jing%2520Su%2520and%2520Jingjia%2520Huang%2520and%2520Kai%2520Shen%2520and%2520Kaiyu%2520Shi%2520and%2520Lin%2520Yan%2520and%2520Peiyao%2520Zhao%2520and%2520Pengfei%2520Liu%2520and%2520Qinghao%2520Ye%2520and%2520Renjie%2520Zheng%2520and%2520Shulin%2520Xin%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Wen%2520Heng%2520and%2520Wenhao%2520Huang%2520and%2520Wenqian%2520Wang%2520and%2520Xiaobo%2520Qin%2520and%2520Yi%2520Lin%2520and%2520Youbin%2520Wu%2520and%2520Zehui%2520Chen%2520and%2520Zihao%2520Wang%2520and%2520Baoquan%2520Zhong%2520and%2520Xinchun%2520Zhang%2520and%2520Xujing%2520Li%2520and%2520Yuanfan%2520Li%2520and%2520Zhongkai%2520Zhao%2520and%2520Chengquan%2520Jiang%2520and%2520Faming%2520Wu%2520and%2520Haotian%2520Zhou%2520and%2520Jinlin%2520Pang%2520and%2520Li%2520Han%2520and%2520Qi%2520Liu%2520and%2520Qianli%2520Ma%2520and%2520Siyao%2520Liu%2520and%2520Songhua%2520Cai%2520and%2520Wenqi%2520Fu%2520and%2520Xin%2520Liu%2520and%2520Yaohui%2520Wang%2520and%2520Zhi%2520Zhang%2520and%2520Bo%2520Zhou%2520and%2520Guoliang%2520Li%2520and%2520Jiajun%2520Shi%2520and%2520Jiale%2520Yang%2520and%2520Jie%2520Tang%2520and%2520Li%2520Li%2520and%2520Qihua%2520Han%2520and%2520Taoran%2520Lu%2520and%2520Woyu%2520Lin%2520and%2520Xiaokang%2520Tong%2520and%2520Xinyao%2520Li%2520and%2520Yichi%2520Zhang%2520and%2520Yu%2520Miao%2520and%2520Zhengxuan%2520Jiang%2520and%2520Zili%2520Li%2520and%2520Ziyuan%2520Zhao%2520and%2520Chenxin%2520Li%2520and%2520Dehua%2520Ma%2520and%2520Feng%2520Lin%2520and%2520Ge%2520Zhang%2520and%2520Haihua%2520Yang%2520and%2520Hangyu%2520Guo%2520and%2520Hongda%2520Zhu%2520and%2520Jiaheng%2520Liu%2520and%2520Junda%2520Du%2520and%2520Kai%2520Cai%2520and%2520Kuanye%2520Li%2520and%2520Lichen%2520Yuan%2520and%2520Meilan%2520Han%2520and%2520Minchao%2520Wang%2520and%2520Shuyue%2520Guo%2520and%2520Tianhao%2520Cheng%2520and%2520Xiaobo%2520Ma%2520and%2520Xiaojun%2520Xiao%2520and%2520Xiaolong%2520Huang%2520and%2520Xinjie%2520Chen%2520and%2520Yidi%2520Du%2520and%2520Yilin%2520Chen%2520and%2520Yiwen%2520Wang%2520and%2520Zhaojian%2520Li%2520and%2520Zhenzhu%2520Yang%2520and%2520Zhiyuan%2520Zeng%2520and%2520Chaolin%2520Jin%2520and%2520Chen%2520Li%2520and%2520Hao%2520Chen%2520and%2520Haoli%2520Chen%2520and%2520Jian%2520Chen%2520and%2520Qinghao%2520Zhao%2520and%2520Guang%2520Shi%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520autonomous%2520agents%2520for%2520graphical%2520user%2520interfaces%2520%2528GUIs%2529%250Apresents%2520major%2520challenges%2520in%2520artificial%2520intelligence.%2520While%2520recent%2520advances%2520in%250Anative%2520agent%2520models%2520have%2520shown%2520promise%2520by%2520unifying%2520perception%252C%2520reasoning%252C%250Aaction%252C%2520and%2520memory%2520through%2520end-to-end%2520learning%252C%2520open%2520problems%2520remain%2520in%2520data%250Ascalability%252C%2520multi-turn%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520the%2520limitations%2520of%250AGUI-only%2520operation%252C%2520and%2520environment%2520stability.%2520In%2520this%2520technical%2520report%252C%2520we%250Apresent%2520UI-TARS-2%252C%2520a%2520native%2520GUI-centered%2520agent%2520model%2520that%2520addresses%2520these%250Achallenges%2520through%2520a%2520systematic%2520training%2520methodology%253A%2520a%2520data%2520flywheel%2520for%250Ascalable%2520data%2520generation%252C%2520a%2520stabilized%2520multi-turn%2520RL%2520framework%252C%2520a%2520hybrid%2520GUI%250Aenvironment%2520that%2520integrates%2520file%2520systems%2520and%2520terminals%252C%2520and%2520a%2520unified%2520sandbox%250Aplatform%2520for%2520large-scale%2520rollouts.%2520Empirical%2520evaluation%2520demonstrates%2520that%250AUI-TARS-2%2520achieves%2520significant%2520improvements%2520over%2520its%2520predecessor%2520UI-TARS-1.5.%250AOn%2520GUI%2520benchmarks%252C%2520it%2520reaches%252088.2%2520on%2520Online-Mind2Web%252C%252047.5%2520on%2520OSWorld%252C%252050.6%2520on%250AWindowsAgentArena%252C%2520and%252073.3%2520on%2520AndroidWorld%252C%2520outperforming%2520strong%2520baselines%250Asuch%2520as%2520Claude%2520and%2520OpenAI%2520agents.%2520In%2520game%2520environments%252C%2520it%2520attains%2520a%2520mean%250Anormalized%2520score%2520of%252059.8%2520across%2520a%252015-game%2520suite-roughly%252060%2525%2520of%2520human-level%250Aperformance-and%2520remains%2520competitive%2520with%2520frontier%2520proprietary%2520models%2520%2528e.g.%252C%250AOpenAI%2520o3%2529%2520on%2520LMGame-Bench.%2520Additionally%252C%2520the%2520model%2520can%2520generalize%2520to%250Along-horizon%2520information-seeking%2520tasks%2520and%2520software%2520engineering%2520benchmarks%252C%250Ahighlighting%2520its%2520robustness%2520across%2520diverse%2520agent%2520tasks.%2520Detailed%2520analyses%2520of%250Atraining%2520dynamics%2520further%2520provide%2520insights%2520into%2520achieving%2520stability%2520and%250Aefficiency%2520in%2520large-scale%2520agent%2520RL.%2520These%2520results%2520underscore%2520UI-TARS-2%2527s%250Apotential%2520to%2520advance%2520the%2520state%2520of%2520GUI%2520agents%2520and%2520exhibit%2520strong%2520generalization%250Ato%2520real-world%2520interactive%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-TARS-2%20Technical%20Report%3A%20Advancing%20GUI%20Agent%20with%20Multi-Turn%0A%20%20Reinforcement%20Learning&entry.906535625=Haoming%20Wang%20and%20Haoyang%20Zou%20and%20Huatong%20Song%20and%20Jiazhan%20Feng%20and%20Junjie%20Fang%20and%20Junting%20Lu%20and%20Longxiang%20Liu%20and%20Qinyu%20Luo%20and%20Shihao%20Liang%20and%20Shijue%20Huang%20and%20Wanjun%20Zhong%20and%20Yining%20Ye%20and%20Yujia%20Qin%20and%20Yuwen%20Xiong%20and%20Yuxin%20Song%20and%20Zhiyong%20Wu%20and%20Aoyan%20Li%20and%20Bo%20Li%20and%20Chen%20Dun%20and%20Chong%20Liu%20and%20Daoguang%20Zan%20and%20Fuxing%20Leng%20and%20Hanbin%20Wang%20and%20Hao%20Yu%20and%20Haobin%20Chen%20and%20Hongyi%20Guo%20and%20Jing%20Su%20and%20Jingjia%20Huang%20and%20Kai%20Shen%20and%20Kaiyu%20Shi%20and%20Lin%20Yan%20and%20Peiyao%20Zhao%20and%20Pengfei%20Liu%20and%20Qinghao%20Ye%20and%20Renjie%20Zheng%20and%20Shulin%20Xin%20and%20Wayne%20Xin%20Zhao%20and%20Wen%20Heng%20and%20Wenhao%20Huang%20and%20Wenqian%20Wang%20and%20Xiaobo%20Qin%20and%20Yi%20Lin%20and%20Youbin%20Wu%20and%20Zehui%20Chen%20and%20Zihao%20Wang%20and%20Baoquan%20Zhong%20and%20Xinchun%20Zhang%20and%20Xujing%20Li%20and%20Yuanfan%20Li%20and%20Zhongkai%20Zhao%20and%20Chengquan%20Jiang%20and%20Faming%20Wu%20and%20Haotian%20Zhou%20and%20Jinlin%20Pang%20and%20Li%20Han%20and%20Qi%20Liu%20and%20Qianli%20Ma%20and%20Siyao%20Liu%20and%20Songhua%20Cai%20and%20Wenqi%20Fu%20and%20Xin%20Liu%20and%20Yaohui%20Wang%20and%20Zhi%20Zhang%20and%20Bo%20Zhou%20and%20Guoliang%20Li%20and%20Jiajun%20Shi%20and%20Jiale%20Yang%20and%20Jie%20Tang%20and%20Li%20Li%20and%20Qihua%20Han%20and%20Taoran%20Lu%20and%20Woyu%20Lin%20and%20Xiaokang%20Tong%20and%20Xinyao%20Li%20and%20Yichi%20Zhang%20and%20Yu%20Miao%20and%20Zhengxuan%20Jiang%20and%20Zili%20Li%20and%20Ziyuan%20Zhao%20and%20Chenxin%20Li%20and%20Dehua%20Ma%20and%20Feng%20Lin%20and%20Ge%20Zhang%20and%20Haihua%20Yang%20and%20Hangyu%20Guo%20and%20Hongda%20Zhu%20and%20Jiaheng%20Liu%20and%20Junda%20Du%20and%20Kai%20Cai%20and%20Kuanye%20Li%20and%20Lichen%20Yuan%20and%20Meilan%20Han%20and%20Minchao%20Wang%20and%20Shuyue%20Guo%20and%20Tianhao%20Cheng%20and%20Xiaobo%20Ma%20and%20Xiaojun%20Xiao%20and%20Xiaolong%20Huang%20and%20Xinjie%20Chen%20and%20Yidi%20Du%20and%20Yilin%20Chen%20and%20Yiwen%20Wang%20and%20Zhaojian%20Li%20and%20Zhenzhu%20Yang%20and%20Zhiyuan%20Zeng%20and%20Chaolin%20Jin%20and%20Chen%20Li%20and%20Hao%20Chen%20and%20Haoli%20Chen%20and%20Jian%20Chen%20and%20Qinghao%20Zhao%20and%20Guang%20Shi&entry.1292438233=%20%20The%20development%20of%20autonomous%20agents%20for%20graphical%20user%20interfaces%20%28GUIs%29%0Apresents%20major%20challenges%20in%20artificial%20intelligence.%20While%20recent%20advances%20in%0Anative%20agent%20models%20have%20shown%20promise%20by%20unifying%20perception%2C%20reasoning%2C%0Aaction%2C%20and%20memory%20through%20end-to-end%20learning%2C%20open%20problems%20remain%20in%20data%0Ascalability%2C%20multi-turn%20reinforcement%20learning%20%28RL%29%2C%20the%20limitations%20of%0AGUI-only%20operation%2C%20and%20environment%20stability.%20In%20this%20technical%20report%2C%20we%0Apresent%20UI-TARS-2%2C%20a%20native%20GUI-centered%20agent%20model%20that%20addresses%20these%0Achallenges%20through%20a%20systematic%20training%20methodology%3A%20a%20data%20flywheel%20for%0Ascalable%20data%20generation%2C%20a%20stabilized%20multi-turn%20RL%20framework%2C%20a%20hybrid%20GUI%0Aenvironment%20that%20integrates%20file%20systems%20and%20terminals%2C%20and%20a%20unified%20sandbox%0Aplatform%20for%20large-scale%20rollouts.%20Empirical%20evaluation%20demonstrates%20that%0AUI-TARS-2%20achieves%20significant%20improvements%20over%20its%20predecessor%20UI-TARS-1.5.%0AOn%20GUI%20benchmarks%2C%20it%20reaches%2088.2%20on%20Online-Mind2Web%2C%2047.5%20on%20OSWorld%2C%2050.6%20on%0AWindowsAgentArena%2C%20and%2073.3%20on%20AndroidWorld%2C%20outperforming%20strong%20baselines%0Asuch%20as%20Claude%20and%20OpenAI%20agents.%20In%20game%20environments%2C%20it%20attains%20a%20mean%0Anormalized%20score%20of%2059.8%20across%20a%2015-game%20suite-roughly%2060%25%20of%20human-level%0Aperformance-and%20remains%20competitive%20with%20frontier%20proprietary%20models%20%28e.g.%2C%0AOpenAI%20o3%29%20on%20LMGame-Bench.%20Additionally%2C%20the%20model%20can%20generalize%20to%0Along-horizon%20information-seeking%20tasks%20and%20software%20engineering%20benchmarks%2C%0Ahighlighting%20its%20robustness%20across%20diverse%20agent%20tasks.%20Detailed%20analyses%20of%0Atraining%20dynamics%20further%20provide%20insights%20into%20achieving%20stability%20and%0Aefficiency%20in%20large-scale%20agent%20RL.%20These%20results%20underscore%20UI-TARS-2%27s%0Apotential%20to%20advance%20the%20state%20of%20GUI%20agents%20and%20exhibit%20strong%20generalization%0Ato%20real-world%20interactive%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02544v2&entry.124074799=Read"},
{"title": "Learning to Coordinate: Distributed Meta-Trajectory Optimization Via\n  Differentiable ADMM-DDP", "author": "Bingheng Wang and Yichao Gao and Tianchen Sun and Lin Zhao", "abstract": "  Distributed trajectory optimization via ADMM-DDP is a powerful approach for\ncoordinating multi-agent systems, but it requires extensive tuning of tightly\ncoupled hyperparameters that jointly govern local task performance and global\ncoordination. In this paper, we propose Learning to Coordinate (L2C), a general\nframework that meta-learns these hyperparameters, modeled by lightweight\nagent-wise neural networks, to adapt across diverse tasks and agent\nconfigurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in\na distributed manner. It also enables efficient meta-gradient computation by\nreusing DDP components such as Riccati recursions and feedback gains. These\ngradients correspond to the optimal solutions of distributed matrix-valued LQR\nproblems, coordinated across agents via an auxiliary ADMM framework that\nbecomes convex under mild assumptions. Training is further accelerated by\ntruncating iterations and meta-learning ADMM penalty parameters optimized for\nrapid residual reduction, with provable Lipschitz-bounded gradient errors. On a\nchallenging cooperative aerial transport task, L2C generates dynamically\nfeasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures\nquadrotor formations for safe 6-DoF load manipulation in tight spaces, and\nadapts robustly to varying team sizes and task conditions, while achieving up\nto $88\\%$ faster gradient computation than state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2509.01630v2", "date": "2025-09-05", "relevancy": 2.0904, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5269}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5264}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Coordinate%3A%20Distributed%20Meta-Trajectory%20Optimization%20Via%0A%20%20Differentiable%20ADMM-DDP&body=Title%3A%20Learning%20to%20Coordinate%3A%20Distributed%20Meta-Trajectory%20Optimization%20Via%0A%20%20Differentiable%20ADMM-DDP%0AAuthor%3A%20Bingheng%20Wang%20and%20Yichao%20Gao%20and%20Tianchen%20Sun%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Distributed%20trajectory%20optimization%20via%20ADMM-DDP%20is%20a%20powerful%20approach%20for%0Acoordinating%20multi-agent%20systems%2C%20but%20it%20requires%20extensive%20tuning%20of%20tightly%0Acoupled%20hyperparameters%20that%20jointly%20govern%20local%20task%20performance%20and%20global%0Acoordination.%20In%20this%20paper%2C%20we%20propose%20Learning%20to%20Coordinate%20%28L2C%29%2C%20a%20general%0Aframework%20that%20meta-learns%20these%20hyperparameters%2C%20modeled%20by%20lightweight%0Aagent-wise%20neural%20networks%2C%20to%20adapt%20across%20diverse%20tasks%20and%20agent%0Aconfigurations.%20L2C%20differentiates%20end-to-end%20through%20the%20ADMM-DDP%20pipeline%20in%0Aa%20distributed%20manner.%20It%20also%20enables%20efficient%20meta-gradient%20computation%20by%0Areusing%20DDP%20components%20such%20as%20Riccati%20recursions%20and%20feedback%20gains.%20These%0Agradients%20correspond%20to%20the%20optimal%20solutions%20of%20distributed%20matrix-valued%20LQR%0Aproblems%2C%20coordinated%20across%20agents%20via%20an%20auxiliary%20ADMM%20framework%20that%0Abecomes%20convex%20under%20mild%20assumptions.%20Training%20is%20further%20accelerated%20by%0Atruncating%20iterations%20and%20meta-learning%20ADMM%20penalty%20parameters%20optimized%20for%0Arapid%20residual%20reduction%2C%20with%20provable%20Lipschitz-bounded%20gradient%20errors.%20On%20a%0Achallenging%20cooperative%20aerial%20transport%20task%2C%20L2C%20generates%20dynamically%0Afeasible%20trajectories%20in%20high-fidelity%20simulation%20using%20IsaacSIM%2C%20reconfigures%0Aquadrotor%20formations%20for%20safe%206-DoF%20load%20manipulation%20in%20tight%20spaces%2C%20and%0Aadapts%20robustly%20to%20varying%20team%20sizes%20and%20task%20conditions%2C%20while%20achieving%20up%0Ato%20%2488%5C%25%24%20faster%20gradient%20computation%20than%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Coordinate%253A%2520Distributed%2520Meta-Trajectory%2520Optimization%2520Via%250A%2520%2520Differentiable%2520ADMM-DDP%26entry.906535625%3DBingheng%2520Wang%2520and%2520Yichao%2520Gao%2520and%2520Tianchen%2520Sun%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Distributed%2520trajectory%2520optimization%2520via%2520ADMM-DDP%2520is%2520a%2520powerful%2520approach%2520for%250Acoordinating%2520multi-agent%2520systems%252C%2520but%2520it%2520requires%2520extensive%2520tuning%2520of%2520tightly%250Acoupled%2520hyperparameters%2520that%2520jointly%2520govern%2520local%2520task%2520performance%2520and%2520global%250Acoordination.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Learning%2520to%2520Coordinate%2520%2528L2C%2529%252C%2520a%2520general%250Aframework%2520that%2520meta-learns%2520these%2520hyperparameters%252C%2520modeled%2520by%2520lightweight%250Aagent-wise%2520neural%2520networks%252C%2520to%2520adapt%2520across%2520diverse%2520tasks%2520and%2520agent%250Aconfigurations.%2520L2C%2520differentiates%2520end-to-end%2520through%2520the%2520ADMM-DDP%2520pipeline%2520in%250Aa%2520distributed%2520manner.%2520It%2520also%2520enables%2520efficient%2520meta-gradient%2520computation%2520by%250Areusing%2520DDP%2520components%2520such%2520as%2520Riccati%2520recursions%2520and%2520feedback%2520gains.%2520These%250Agradients%2520correspond%2520to%2520the%2520optimal%2520solutions%2520of%2520distributed%2520matrix-valued%2520LQR%250Aproblems%252C%2520coordinated%2520across%2520agents%2520via%2520an%2520auxiliary%2520ADMM%2520framework%2520that%250Abecomes%2520convex%2520under%2520mild%2520assumptions.%2520Training%2520is%2520further%2520accelerated%2520by%250Atruncating%2520iterations%2520and%2520meta-learning%2520ADMM%2520penalty%2520parameters%2520optimized%2520for%250Arapid%2520residual%2520reduction%252C%2520with%2520provable%2520Lipschitz-bounded%2520gradient%2520errors.%2520On%2520a%250Achallenging%2520cooperative%2520aerial%2520transport%2520task%252C%2520L2C%2520generates%2520dynamically%250Afeasible%2520trajectories%2520in%2520high-fidelity%2520simulation%2520using%2520IsaacSIM%252C%2520reconfigures%250Aquadrotor%2520formations%2520for%2520safe%25206-DoF%2520load%2520manipulation%2520in%2520tight%2520spaces%252C%2520and%250Aadapts%2520robustly%2520to%2520varying%2520team%2520sizes%2520and%2520task%2520conditions%252C%2520while%2520achieving%2520up%250Ato%2520%252488%255C%2525%2524%2520faster%2520gradient%2520computation%2520than%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Coordinate%3A%20Distributed%20Meta-Trajectory%20Optimization%20Via%0A%20%20Differentiable%20ADMM-DDP&entry.906535625=Bingheng%20Wang%20and%20Yichao%20Gao%20and%20Tianchen%20Sun%20and%20Lin%20Zhao&entry.1292438233=%20%20Distributed%20trajectory%20optimization%20via%20ADMM-DDP%20is%20a%20powerful%20approach%20for%0Acoordinating%20multi-agent%20systems%2C%20but%20it%20requires%20extensive%20tuning%20of%20tightly%0Acoupled%20hyperparameters%20that%20jointly%20govern%20local%20task%20performance%20and%20global%0Acoordination.%20In%20this%20paper%2C%20we%20propose%20Learning%20to%20Coordinate%20%28L2C%29%2C%20a%20general%0Aframework%20that%20meta-learns%20these%20hyperparameters%2C%20modeled%20by%20lightweight%0Aagent-wise%20neural%20networks%2C%20to%20adapt%20across%20diverse%20tasks%20and%20agent%0Aconfigurations.%20L2C%20differentiates%20end-to-end%20through%20the%20ADMM-DDP%20pipeline%20in%0Aa%20distributed%20manner.%20It%20also%20enables%20efficient%20meta-gradient%20computation%20by%0Areusing%20DDP%20components%20such%20as%20Riccati%20recursions%20and%20feedback%20gains.%20These%0Agradients%20correspond%20to%20the%20optimal%20solutions%20of%20distributed%20matrix-valued%20LQR%0Aproblems%2C%20coordinated%20across%20agents%20via%20an%20auxiliary%20ADMM%20framework%20that%0Abecomes%20convex%20under%20mild%20assumptions.%20Training%20is%20further%20accelerated%20by%0Atruncating%20iterations%20and%20meta-learning%20ADMM%20penalty%20parameters%20optimized%20for%0Arapid%20residual%20reduction%2C%20with%20provable%20Lipschitz-bounded%20gradient%20errors.%20On%20a%0Achallenging%20cooperative%20aerial%20transport%20task%2C%20L2C%20generates%20dynamically%0Afeasible%20trajectories%20in%20high-fidelity%20simulation%20using%20IsaacSIM%2C%20reconfigures%0Aquadrotor%20formations%20for%20safe%206-DoF%20load%20manipulation%20in%20tight%20spaces%2C%20and%0Aadapts%20robustly%20to%20varying%20team%20sizes%20and%20task%20conditions%2C%20while%20achieving%20up%0Ato%20%2488%5C%25%24%20faster%20gradient%20computation%20than%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01630v2&entry.124074799=Read"},
{"title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range\n  Dependency Modeling in Large Language Models", "author": "Chang Dai and Hongyu Shan and Mingyang Song and Di Liang", "abstract": "  Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available.\n", "link": "http://arxiv.org/abs/2509.05218v1", "date": "2025-09-05", "relevancy": 2.0866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoPE%3A%20Hyperbolic%20Rotary%20Positional%20Encoding%20for%20Stable%20Long-Range%0A%20%20Dependency%20Modeling%20in%20Large%20Language%20Models&body=Title%3A%20HoPE%3A%20Hyperbolic%20Rotary%20Positional%20Encoding%20for%20Stable%20Long-Range%0A%20%20Dependency%20Modeling%20in%20Large%20Language%20Models%0AAuthor%3A%20Chang%20Dai%20and%20Hongyu%20Shan%20and%20Mingyang%20Song%20and%20Di%20Liang%0AAbstract%3A%20%20%20Positional%20encoding%20mechanisms%20enable%20Transformers%20to%20model%20sequential%0Astructure%20and%20long-range%20dependencies%20in%20text.%20While%20absolute%20positional%0Aencodings%20struggle%20with%20extrapolation%20to%20longer%20sequences%20due%20to%20fixed%0Apositional%20representations%2C%20and%20relative%20approaches%20like%20Alibi%20exhibit%0Aperformance%20degradation%20on%20extremely%20long%20contexts%2C%20the%20widely-used%20Rotary%0APositional%20Encoding%20%28RoPE%29%20introduces%20oscillatory%20attention%20patterns%20that%0Ahinder%20stable%20long-distance%20dependency%20modelling.%20We%20address%20these%20limitations%0Athrough%20a%20geometric%20reformulation%20of%20positional%20encoding.%20Drawing%20inspiration%0Afrom%20Lorentz%20transformations%20in%20hyperbolic%20geometry%2C%20we%20propose%20Hyperbolic%0ARotary%20Positional%20Encoding%20%28HoPE%29%2C%20which%20leverages%20hyperbolic%20functions%20to%0Aimplement%20Lorentz%20rotations%20on%20token%20representations.%20Theoretical%20analysis%0Ademonstrates%20that%20RoPE%20is%20a%20special%20case%20of%20our%20generalized%20formulation.%20HoPE%0Afundamentally%20resolves%20RoPE%27s%20slation%20issues%20by%20enforcing%20monotonic%20decay%20of%0Aattention%20weights%20with%20increasing%20token%20distances.%20Extensive%20experimental%0Aresults%2C%20including%20perplexity%20evaluations%20under%20several%20extended%20sequence%0Abenchmarks%2C%20show%20that%20HoPE%20consistently%20exceeds%20existing%20positional%20encoding%0Amethods.%20These%20findings%20underscore%20HoPE%27s%20enhanced%20capacity%20for%20representing%0Aand%20generalizing%20long-range%20dependencies.%20Data%20and%20code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoPE%253A%2520Hyperbolic%2520Rotary%2520Positional%2520Encoding%2520for%2520Stable%2520Long-Range%250A%2520%2520Dependency%2520Modeling%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DChang%2520Dai%2520and%2520Hongyu%2520Shan%2520and%2520Mingyang%2520Song%2520and%2520Di%2520Liang%26entry.1292438233%3D%2520%2520Positional%2520encoding%2520mechanisms%2520enable%2520Transformers%2520to%2520model%2520sequential%250Astructure%2520and%2520long-range%2520dependencies%2520in%2520text.%2520While%2520absolute%2520positional%250Aencodings%2520struggle%2520with%2520extrapolation%2520to%2520longer%2520sequences%2520due%2520to%2520fixed%250Apositional%2520representations%252C%2520and%2520relative%2520approaches%2520like%2520Alibi%2520exhibit%250Aperformance%2520degradation%2520on%2520extremely%2520long%2520contexts%252C%2520the%2520widely-used%2520Rotary%250APositional%2520Encoding%2520%2528RoPE%2529%2520introduces%2520oscillatory%2520attention%2520patterns%2520that%250Ahinder%2520stable%2520long-distance%2520dependency%2520modelling.%2520We%2520address%2520these%2520limitations%250Athrough%2520a%2520geometric%2520reformulation%2520of%2520positional%2520encoding.%2520Drawing%2520inspiration%250Afrom%2520Lorentz%2520transformations%2520in%2520hyperbolic%2520geometry%252C%2520we%2520propose%2520Hyperbolic%250ARotary%2520Positional%2520Encoding%2520%2528HoPE%2529%252C%2520which%2520leverages%2520hyperbolic%2520functions%2520to%250Aimplement%2520Lorentz%2520rotations%2520on%2520token%2520representations.%2520Theoretical%2520analysis%250Ademonstrates%2520that%2520RoPE%2520is%2520a%2520special%2520case%2520of%2520our%2520generalized%2520formulation.%2520HoPE%250Afundamentally%2520resolves%2520RoPE%2527s%2520slation%2520issues%2520by%2520enforcing%2520monotonic%2520decay%2520of%250Aattention%2520weights%2520with%2520increasing%2520token%2520distances.%2520Extensive%2520experimental%250Aresults%252C%2520including%2520perplexity%2520evaluations%2520under%2520several%2520extended%2520sequence%250Abenchmarks%252C%2520show%2520that%2520HoPE%2520consistently%2520exceeds%2520existing%2520positional%2520encoding%250Amethods.%2520These%2520findings%2520underscore%2520HoPE%2527s%2520enhanced%2520capacity%2520for%2520representing%250Aand%2520generalizing%2520long-range%2520dependencies.%2520Data%2520and%2520code%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoPE%3A%20Hyperbolic%20Rotary%20Positional%20Encoding%20for%20Stable%20Long-Range%0A%20%20Dependency%20Modeling%20in%20Large%20Language%20Models&entry.906535625=Chang%20Dai%20and%20Hongyu%20Shan%20and%20Mingyang%20Song%20and%20Di%20Liang&entry.1292438233=%20%20Positional%20encoding%20mechanisms%20enable%20Transformers%20to%20model%20sequential%0Astructure%20and%20long-range%20dependencies%20in%20text.%20While%20absolute%20positional%0Aencodings%20struggle%20with%20extrapolation%20to%20longer%20sequences%20due%20to%20fixed%0Apositional%20representations%2C%20and%20relative%20approaches%20like%20Alibi%20exhibit%0Aperformance%20degradation%20on%20extremely%20long%20contexts%2C%20the%20widely-used%20Rotary%0APositional%20Encoding%20%28RoPE%29%20introduces%20oscillatory%20attention%20patterns%20that%0Ahinder%20stable%20long-distance%20dependency%20modelling.%20We%20address%20these%20limitations%0Athrough%20a%20geometric%20reformulation%20of%20positional%20encoding.%20Drawing%20inspiration%0Afrom%20Lorentz%20transformations%20in%20hyperbolic%20geometry%2C%20we%20propose%20Hyperbolic%0ARotary%20Positional%20Encoding%20%28HoPE%29%2C%20which%20leverages%20hyperbolic%20functions%20to%0Aimplement%20Lorentz%20rotations%20on%20token%20representations.%20Theoretical%20analysis%0Ademonstrates%20that%20RoPE%20is%20a%20special%20case%20of%20our%20generalized%20formulation.%20HoPE%0Afundamentally%20resolves%20RoPE%27s%20slation%20issues%20by%20enforcing%20monotonic%20decay%20of%0Aattention%20weights%20with%20increasing%20token%20distances.%20Extensive%20experimental%0Aresults%2C%20including%20perplexity%20evaluations%20under%20several%20extended%20sequence%0Abenchmarks%2C%20show%20that%20HoPE%20consistently%20exceeds%20existing%20positional%20encoding%0Amethods.%20These%20findings%20underscore%20HoPE%27s%20enhanced%20capacity%20for%20representing%0Aand%20generalizing%20long-range%20dependencies.%20Data%20and%20code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05218v1&entry.124074799=Read"},
{"title": "Semi-supervised Deep Transfer for Regression without Domain Alignment", "author": "Mainak Biswas and Ambedkar Dukkipati and Devarajan Sridharan", "abstract": "  Deep learning models deployed in real-world applications (e.g., medicine)\nface challenges because source models do not generalize well to domain-shifted\ntarget data. Many successful domain adaptation (DA) approaches require full\naccess to source data. Yet, such requirements are unrealistic in scenarios\nwhere source data cannot be shared either because of privacy concerns or\nbecause it is too large and incurs prohibitive storage or computational costs.\nMoreover, resource constraints may limit the availability of labeled targets.\nWe illustrate this challenge in a neuroscience setting where source data are\nunavailable, labeled target data are meager, and predictions involve\ncontinuous-valued outputs. We build upon Contradistinguisher (CUDA), an\nefficient framework that learns a shared model across the labeled source and\nunlabeled target samples, without intermediate representation alignment. Yet,\nCUDA was designed for unsupervised DA, with full access to source data, and for\nclassification tasks. We develop CRAFT -- a Contradistinguisher-based\nRegularization Approach for Flexible Training -- for source-free (SF),\nsemi-supervised transfer of pretrained models in regression tasks. We showcase\nthe efficacy of CRAFT in two neuroscience settings: gaze prediction with\nelectroencephalography (EEG) data and ``brain age'' prediction with structural\nMRI data. For both datasets, CRAFT yielded up to 9% improvement in\nroot-mean-squared error (RMSE) over fine-tuned models when labeled training\nexamples were scarce. Moreover, CRAFT leveraged unlabeled target data and\noutperformed four competing state-of-the-art source-free domain adaptation\nmodels by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two\nother real-world regression benchmarks. We propose CRAFT as an efficient\napproach for source-free, semi-supervised deep transfer for regression that is\nubiquitous in biology and medicine.\n", "link": "http://arxiv.org/abs/2509.05092v1", "date": "2025-09-05", "relevancy": 2.0828, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%20Deep%20Transfer%20for%20Regression%20without%20Domain%20Alignment&body=Title%3A%20Semi-supervised%20Deep%20Transfer%20for%20Regression%20without%20Domain%20Alignment%0AAuthor%3A%20Mainak%20Biswas%20and%20Ambedkar%20Dukkipati%20and%20Devarajan%20Sridharan%0AAbstract%3A%20%20%20Deep%20learning%20models%20deployed%20in%20real-world%20applications%20%28e.g.%2C%20medicine%29%0Aface%20challenges%20because%20source%20models%20do%20not%20generalize%20well%20to%20domain-shifted%0Atarget%20data.%20Many%20successful%20domain%20adaptation%20%28DA%29%20approaches%20require%20full%0Aaccess%20to%20source%20data.%20Yet%2C%20such%20requirements%20are%20unrealistic%20in%20scenarios%0Awhere%20source%20data%20cannot%20be%20shared%20either%20because%20of%20privacy%20concerns%20or%0Abecause%20it%20is%20too%20large%20and%20incurs%20prohibitive%20storage%20or%20computational%20costs.%0AMoreover%2C%20resource%20constraints%20may%20limit%20the%20availability%20of%20labeled%20targets.%0AWe%20illustrate%20this%20challenge%20in%20a%20neuroscience%20setting%20where%20source%20data%20are%0Aunavailable%2C%20labeled%20target%20data%20are%20meager%2C%20and%20predictions%20involve%0Acontinuous-valued%20outputs.%20We%20build%20upon%20Contradistinguisher%20%28CUDA%29%2C%20an%0Aefficient%20framework%20that%20learns%20a%20shared%20model%20across%20the%20labeled%20source%20and%0Aunlabeled%20target%20samples%2C%20without%20intermediate%20representation%20alignment.%20Yet%2C%0ACUDA%20was%20designed%20for%20unsupervised%20DA%2C%20with%20full%20access%20to%20source%20data%2C%20and%20for%0Aclassification%20tasks.%20We%20develop%20CRAFT%20--%20a%20Contradistinguisher-based%0ARegularization%20Approach%20for%20Flexible%20Training%20--%20for%20source-free%20%28SF%29%2C%0Asemi-supervised%20transfer%20of%20pretrained%20models%20in%20regression%20tasks.%20We%20showcase%0Athe%20efficacy%20of%20CRAFT%20in%20two%20neuroscience%20settings%3A%20gaze%20prediction%20with%0Aelectroencephalography%20%28EEG%29%20data%20and%20%60%60brain%20age%27%27%20prediction%20with%20structural%0AMRI%20data.%20For%20both%20datasets%2C%20CRAFT%20yielded%20up%20to%209%25%20improvement%20in%0Aroot-mean-squared%20error%20%28RMSE%29%20over%20fine-tuned%20models%20when%20labeled%20training%0Aexamples%20were%20scarce.%20Moreover%2C%20CRAFT%20leveraged%20unlabeled%20target%20data%20and%0Aoutperformed%20four%20competing%20state-of-the-art%20source-free%20domain%20adaptation%0Amodels%20by%20more%20than%203%25.%20Lastly%2C%20we%20demonstrate%20the%20efficacy%20of%20CRAFT%20on%20two%0Aother%20real-world%20regression%20benchmarks.%20We%20propose%20CRAFT%20as%20an%20efficient%0Aapproach%20for%20source-free%2C%20semi-supervised%20deep%20transfer%20for%20regression%20that%20is%0Aubiquitous%20in%20biology%20and%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-supervised%2520Deep%2520Transfer%2520for%2520Regression%2520without%2520Domain%2520Alignment%26entry.906535625%3DMainak%2520Biswas%2520and%2520Ambedkar%2520Dukkipati%2520and%2520Devarajan%2520Sridharan%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520deployed%2520in%2520real-world%2520applications%2520%2528e.g.%252C%2520medicine%2529%250Aface%2520challenges%2520because%2520source%2520models%2520do%2520not%2520generalize%2520well%2520to%2520domain-shifted%250Atarget%2520data.%2520Many%2520successful%2520domain%2520adaptation%2520%2528DA%2529%2520approaches%2520require%2520full%250Aaccess%2520to%2520source%2520data.%2520Yet%252C%2520such%2520requirements%2520are%2520unrealistic%2520in%2520scenarios%250Awhere%2520source%2520data%2520cannot%2520be%2520shared%2520either%2520because%2520of%2520privacy%2520concerns%2520or%250Abecause%2520it%2520is%2520too%2520large%2520and%2520incurs%2520prohibitive%2520storage%2520or%2520computational%2520costs.%250AMoreover%252C%2520resource%2520constraints%2520may%2520limit%2520the%2520availability%2520of%2520labeled%2520targets.%250AWe%2520illustrate%2520this%2520challenge%2520in%2520a%2520neuroscience%2520setting%2520where%2520source%2520data%2520are%250Aunavailable%252C%2520labeled%2520target%2520data%2520are%2520meager%252C%2520and%2520predictions%2520involve%250Acontinuous-valued%2520outputs.%2520We%2520build%2520upon%2520Contradistinguisher%2520%2528CUDA%2529%252C%2520an%250Aefficient%2520framework%2520that%2520learns%2520a%2520shared%2520model%2520across%2520the%2520labeled%2520source%2520and%250Aunlabeled%2520target%2520samples%252C%2520without%2520intermediate%2520representation%2520alignment.%2520Yet%252C%250ACUDA%2520was%2520designed%2520for%2520unsupervised%2520DA%252C%2520with%2520full%2520access%2520to%2520source%2520data%252C%2520and%2520for%250Aclassification%2520tasks.%2520We%2520develop%2520CRAFT%2520--%2520a%2520Contradistinguisher-based%250ARegularization%2520Approach%2520for%2520Flexible%2520Training%2520--%2520for%2520source-free%2520%2528SF%2529%252C%250Asemi-supervised%2520transfer%2520of%2520pretrained%2520models%2520in%2520regression%2520tasks.%2520We%2520showcase%250Athe%2520efficacy%2520of%2520CRAFT%2520in%2520two%2520neuroscience%2520settings%253A%2520gaze%2520prediction%2520with%250Aelectroencephalography%2520%2528EEG%2529%2520data%2520and%2520%2560%2560brain%2520age%2527%2527%2520prediction%2520with%2520structural%250AMRI%2520data.%2520For%2520both%2520datasets%252C%2520CRAFT%2520yielded%2520up%2520to%25209%2525%2520improvement%2520in%250Aroot-mean-squared%2520error%2520%2528RMSE%2529%2520over%2520fine-tuned%2520models%2520when%2520labeled%2520training%250Aexamples%2520were%2520scarce.%2520Moreover%252C%2520CRAFT%2520leveraged%2520unlabeled%2520target%2520data%2520and%250Aoutperformed%2520four%2520competing%2520state-of-the-art%2520source-free%2520domain%2520adaptation%250Amodels%2520by%2520more%2520than%25203%2525.%2520Lastly%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520CRAFT%2520on%2520two%250Aother%2520real-world%2520regression%2520benchmarks.%2520We%2520propose%2520CRAFT%2520as%2520an%2520efficient%250Aapproach%2520for%2520source-free%252C%2520semi-supervised%2520deep%2520transfer%2520for%2520regression%2520that%2520is%250Aubiquitous%2520in%2520biology%2520and%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Deep%20Transfer%20for%20Regression%20without%20Domain%20Alignment&entry.906535625=Mainak%20Biswas%20and%20Ambedkar%20Dukkipati%20and%20Devarajan%20Sridharan&entry.1292438233=%20%20Deep%20learning%20models%20deployed%20in%20real-world%20applications%20%28e.g.%2C%20medicine%29%0Aface%20challenges%20because%20source%20models%20do%20not%20generalize%20well%20to%20domain-shifted%0Atarget%20data.%20Many%20successful%20domain%20adaptation%20%28DA%29%20approaches%20require%20full%0Aaccess%20to%20source%20data.%20Yet%2C%20such%20requirements%20are%20unrealistic%20in%20scenarios%0Awhere%20source%20data%20cannot%20be%20shared%20either%20because%20of%20privacy%20concerns%20or%0Abecause%20it%20is%20too%20large%20and%20incurs%20prohibitive%20storage%20or%20computational%20costs.%0AMoreover%2C%20resource%20constraints%20may%20limit%20the%20availability%20of%20labeled%20targets.%0AWe%20illustrate%20this%20challenge%20in%20a%20neuroscience%20setting%20where%20source%20data%20are%0Aunavailable%2C%20labeled%20target%20data%20are%20meager%2C%20and%20predictions%20involve%0Acontinuous-valued%20outputs.%20We%20build%20upon%20Contradistinguisher%20%28CUDA%29%2C%20an%0Aefficient%20framework%20that%20learns%20a%20shared%20model%20across%20the%20labeled%20source%20and%0Aunlabeled%20target%20samples%2C%20without%20intermediate%20representation%20alignment.%20Yet%2C%0ACUDA%20was%20designed%20for%20unsupervised%20DA%2C%20with%20full%20access%20to%20source%20data%2C%20and%20for%0Aclassification%20tasks.%20We%20develop%20CRAFT%20--%20a%20Contradistinguisher-based%0ARegularization%20Approach%20for%20Flexible%20Training%20--%20for%20source-free%20%28SF%29%2C%0Asemi-supervised%20transfer%20of%20pretrained%20models%20in%20regression%20tasks.%20We%20showcase%0Athe%20efficacy%20of%20CRAFT%20in%20two%20neuroscience%20settings%3A%20gaze%20prediction%20with%0Aelectroencephalography%20%28EEG%29%20data%20and%20%60%60brain%20age%27%27%20prediction%20with%20structural%0AMRI%20data.%20For%20both%20datasets%2C%20CRAFT%20yielded%20up%20to%209%25%20improvement%20in%0Aroot-mean-squared%20error%20%28RMSE%29%20over%20fine-tuned%20models%20when%20labeled%20training%0Aexamples%20were%20scarce.%20Moreover%2C%20CRAFT%20leveraged%20unlabeled%20target%20data%20and%0Aoutperformed%20four%20competing%20state-of-the-art%20source-free%20domain%20adaptation%0Amodels%20by%20more%20than%203%25.%20Lastly%2C%20we%20demonstrate%20the%20efficacy%20of%20CRAFT%20on%20two%0Aother%20real-world%20regression%20benchmarks.%20We%20propose%20CRAFT%20as%20an%20efficient%0Aapproach%20for%20source-free%2C%20semi-supervised%20deep%20transfer%20for%20regression%20that%20is%0Aubiquitous%20in%20biology%20and%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05092v1&entry.124074799=Read"},
{"title": "Optimizing Small Transformer-Based Language Models for Multi-Label\n  Sentiment Analysis in Short Texts", "author": "Julius Neumann and Robert Lange and Yuni Susanti and Michael F\u00e4rber", "abstract": "  Sentiment classification in short text datasets faces significant challenges\nsuch as class imbalance, limited training samples, and the inherent\nsubjectivity of sentiment labels -- issues that are further intensified by the\nlimited context in short texts. These factors make it difficult to resolve\nambiguity and exacerbate data sparsity, hindering effective learning. In this\npaper, we evaluate the effectiveness of small Transformer-based models (i.e.,\nBERT and RoBERTa, with fewer than 1 billion parameters) for multi-label\nsentiment classification, with a particular focus on short-text settings.\nSpecifically, we evaluated three key factors influencing model performance: (1)\ncontinued domain-specific pre-training, (2) data augmentation using\nautomatically generated examples, specifically generative data augmentation,\nand (3) architectural variations of the classification head. Our experiment\nresults show that data augmentation improves classification performance, while\ncontinued pre-training on augmented datasets can introduce noise rather than\nboost accuracy. Furthermore, we confirm that modifications to the\nclassification head yield only marginal benefits. These findings provide\npractical guidance for optimizing BERT-based models in resource-constrained\nsettings and refining strategies for sentiment classification in short-text\ndatasets.\n", "link": "http://arxiv.org/abs/2509.04982v1", "date": "2025-09-05", "relevancy": 2.0807, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5436}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Small%20Transformer-Based%20Language%20Models%20for%20Multi-Label%0A%20%20Sentiment%20Analysis%20in%20Short%20Texts&body=Title%3A%20Optimizing%20Small%20Transformer-Based%20Language%20Models%20for%20Multi-Label%0A%20%20Sentiment%20Analysis%20in%20Short%20Texts%0AAuthor%3A%20Julius%20Neumann%20and%20Robert%20Lange%20and%20Yuni%20Susanti%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Sentiment%20classification%20in%20short%20text%20datasets%20faces%20significant%20challenges%0Asuch%20as%20class%20imbalance%2C%20limited%20training%20samples%2C%20and%20the%20inherent%0Asubjectivity%20of%20sentiment%20labels%20--%20issues%20that%20are%20further%20intensified%20by%20the%0Alimited%20context%20in%20short%20texts.%20These%20factors%20make%20it%20difficult%20to%20resolve%0Aambiguity%20and%20exacerbate%20data%20sparsity%2C%20hindering%20effective%20learning.%20In%20this%0Apaper%2C%20we%20evaluate%20the%20effectiveness%20of%20small%20Transformer-based%20models%20%28i.e.%2C%0ABERT%20and%20RoBERTa%2C%20with%20fewer%20than%201%20billion%20parameters%29%20for%20multi-label%0Asentiment%20classification%2C%20with%20a%20particular%20focus%20on%20short-text%20settings.%0ASpecifically%2C%20we%20evaluated%20three%20key%20factors%20influencing%20model%20performance%3A%20%281%29%0Acontinued%20domain-specific%20pre-training%2C%20%282%29%20data%20augmentation%20using%0Aautomatically%20generated%20examples%2C%20specifically%20generative%20data%20augmentation%2C%0Aand%20%283%29%20architectural%20variations%20of%20the%20classification%20head.%20Our%20experiment%0Aresults%20show%20that%20data%20augmentation%20improves%20classification%20performance%2C%20while%0Acontinued%20pre-training%20on%20augmented%20datasets%20can%20introduce%20noise%20rather%20than%0Aboost%20accuracy.%20Furthermore%2C%20we%20confirm%20that%20modifications%20to%20the%0Aclassification%20head%20yield%20only%20marginal%20benefits.%20These%20findings%20provide%0Apractical%20guidance%20for%20optimizing%20BERT-based%20models%20in%20resource-constrained%0Asettings%20and%20refining%20strategies%20for%20sentiment%20classification%20in%20short-text%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Small%2520Transformer-Based%2520Language%2520Models%2520for%2520Multi-Label%250A%2520%2520Sentiment%2520Analysis%2520in%2520Short%2520Texts%26entry.906535625%3DJulius%2520Neumann%2520and%2520Robert%2520Lange%2520and%2520Yuni%2520Susanti%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Sentiment%2520classification%2520in%2520short%2520text%2520datasets%2520faces%2520significant%2520challenges%250Asuch%2520as%2520class%2520imbalance%252C%2520limited%2520training%2520samples%252C%2520and%2520the%2520inherent%250Asubjectivity%2520of%2520sentiment%2520labels%2520--%2520issues%2520that%2520are%2520further%2520intensified%2520by%2520the%250Alimited%2520context%2520in%2520short%2520texts.%2520These%2520factors%2520make%2520it%2520difficult%2520to%2520resolve%250Aambiguity%2520and%2520exacerbate%2520data%2520sparsity%252C%2520hindering%2520effective%2520learning.%2520In%2520this%250Apaper%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520small%2520Transformer-based%2520models%2520%2528i.e.%252C%250ABERT%2520and%2520RoBERTa%252C%2520with%2520fewer%2520than%25201%2520billion%2520parameters%2529%2520for%2520multi-label%250Asentiment%2520classification%252C%2520with%2520a%2520particular%2520focus%2520on%2520short-text%2520settings.%250ASpecifically%252C%2520we%2520evaluated%2520three%2520key%2520factors%2520influencing%2520model%2520performance%253A%2520%25281%2529%250Acontinued%2520domain-specific%2520pre-training%252C%2520%25282%2529%2520data%2520augmentation%2520using%250Aautomatically%2520generated%2520examples%252C%2520specifically%2520generative%2520data%2520augmentation%252C%250Aand%2520%25283%2529%2520architectural%2520variations%2520of%2520the%2520classification%2520head.%2520Our%2520experiment%250Aresults%2520show%2520that%2520data%2520augmentation%2520improves%2520classification%2520performance%252C%2520while%250Acontinued%2520pre-training%2520on%2520augmented%2520datasets%2520can%2520introduce%2520noise%2520rather%2520than%250Aboost%2520accuracy.%2520Furthermore%252C%2520we%2520confirm%2520that%2520modifications%2520to%2520the%250Aclassification%2520head%2520yield%2520only%2520marginal%2520benefits.%2520These%2520findings%2520provide%250Apractical%2520guidance%2520for%2520optimizing%2520BERT-based%2520models%2520in%2520resource-constrained%250Asettings%2520and%2520refining%2520strategies%2520for%2520sentiment%2520classification%2520in%2520short-text%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Small%20Transformer-Based%20Language%20Models%20for%20Multi-Label%0A%20%20Sentiment%20Analysis%20in%20Short%20Texts&entry.906535625=Julius%20Neumann%20and%20Robert%20Lange%20and%20Yuni%20Susanti%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Sentiment%20classification%20in%20short%20text%20datasets%20faces%20significant%20challenges%0Asuch%20as%20class%20imbalance%2C%20limited%20training%20samples%2C%20and%20the%20inherent%0Asubjectivity%20of%20sentiment%20labels%20--%20issues%20that%20are%20further%20intensified%20by%20the%0Alimited%20context%20in%20short%20texts.%20These%20factors%20make%20it%20difficult%20to%20resolve%0Aambiguity%20and%20exacerbate%20data%20sparsity%2C%20hindering%20effective%20learning.%20In%20this%0Apaper%2C%20we%20evaluate%20the%20effectiveness%20of%20small%20Transformer-based%20models%20%28i.e.%2C%0ABERT%20and%20RoBERTa%2C%20with%20fewer%20than%201%20billion%20parameters%29%20for%20multi-label%0Asentiment%20classification%2C%20with%20a%20particular%20focus%20on%20short-text%20settings.%0ASpecifically%2C%20we%20evaluated%20three%20key%20factors%20influencing%20model%20performance%3A%20%281%29%0Acontinued%20domain-specific%20pre-training%2C%20%282%29%20data%20augmentation%20using%0Aautomatically%20generated%20examples%2C%20specifically%20generative%20data%20augmentation%2C%0Aand%20%283%29%20architectural%20variations%20of%20the%20classification%20head.%20Our%20experiment%0Aresults%20show%20that%20data%20augmentation%20improves%20classification%20performance%2C%20while%0Acontinued%20pre-training%20on%20augmented%20datasets%20can%20introduce%20noise%20rather%20than%0Aboost%20accuracy.%20Furthermore%2C%20we%20confirm%20that%20modifications%20to%20the%0Aclassification%20head%20yield%20only%20marginal%20benefits.%20These%20findings%20provide%0Apractical%20guidance%20for%20optimizing%20BERT-based%20models%20in%20resource-constrained%0Asettings%20and%20refining%20strategies%20for%20sentiment%20classification%20in%20short-text%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04982v1&entry.124074799=Read"},
{"title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via\n  Modality-Aware Visual Reasoning", "author": "Hang Wu and Hongkai Chen and Yujun Cai and Chang Liu and Qingwen Ye and Ming-Hsuan Yang and Yiwei Wang", "abstract": "  Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.\n", "link": "http://arxiv.org/abs/2507.00008v2", "date": "2025-09-05", "relevancy": 2.0776, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiMo-GUI%3A%20Advancing%20Test-time%20Scaling%20in%20GUI%20Grounding%20via%0A%20%20Modality-Aware%20Visual%20Reasoning&body=Title%3A%20DiMo-GUI%3A%20Advancing%20Test-time%20Scaling%20in%20GUI%20Grounding%20via%0A%20%20Modality-Aware%20Visual%20Reasoning%0AAuthor%3A%20Hang%20Wu%20and%20Hongkai%20Chen%20and%20Yujun%20Cai%20and%20Chang%20Liu%20and%20Qingwen%20Ye%20and%20Ming-Hsuan%20Yang%20and%20Yiwei%20Wang%0AAbstract%3A%20%20%20Grounding%20natural%20language%20queries%20in%20graphical%20user%20interfaces%20%28GUIs%29%20poses%0Aunique%20challenges%20due%20to%20the%20diversity%20of%20visual%20elements%2C%20spatial%20clutter%2C%20and%0Athe%20ambiguity%20of%20language.%20In%20this%20paper%2C%20we%20introduce%20DiMo-GUI%2C%20a%0Atraining-free%20framework%20for%20GUI%20grounding%20that%20leverages%20two%20core%20strategies%3A%0Adynamic%20visual%20grounding%20and%20modality-aware%20optimization.%20Instead%20of%20treating%0Athe%20GUI%20as%20a%20monolithic%20image%2C%20our%20method%20splits%20the%20input%20into%20textual%0Aelements%20and%20iconic%20elements%2C%20allowing%20the%20model%20to%20reason%20over%20each%20modality%0Aindependently%20using%20general-purpose%20vision-language%20models.%20When%20predictions%0Aare%20ambiguous%20or%20incorrect%2C%20DiMo-GUI%20dynamically%20focuses%20attention%20by%0Agenerating%20candidate%20focal%20regions%20centered%20on%20the%20model%27s%20initial%20predictions%0Aand%20incrementally%20zooms%20into%20subregions%20to%20refine%20the%20grounding%20result.%20This%0Ahierarchical%20refinement%20process%20helps%20disambiguate%20visually%20crowded%20layouts%0Awithout%20the%20need%20for%20additional%20training%20or%20annotations.%20We%20evaluate%20our%0Aapproach%20on%20standard%20GUI%20grounding%20benchmarks%20and%20demonstrate%20consistent%0Aimprovements%20over%20baseline%20inference%20pipelines%2C%20highlighting%20the%20effectiveness%0Aof%20combining%20modality%20separation%20with%20region-focused%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiMo-GUI%253A%2520Advancing%2520Test-time%2520Scaling%2520in%2520GUI%2520Grounding%2520via%250A%2520%2520Modality-Aware%2520Visual%2520Reasoning%26entry.906535625%3DHang%2520Wu%2520and%2520Hongkai%2520Chen%2520and%2520Yujun%2520Cai%2520and%2520Chang%2520Liu%2520and%2520Qingwen%2520Ye%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Yiwei%2520Wang%26entry.1292438233%3D%2520%2520Grounding%2520natural%2520language%2520queries%2520in%2520graphical%2520user%2520interfaces%2520%2528GUIs%2529%2520poses%250Aunique%2520challenges%2520due%2520to%2520the%2520diversity%2520of%2520visual%2520elements%252C%2520spatial%2520clutter%252C%2520and%250Athe%2520ambiguity%2520of%2520language.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DiMo-GUI%252C%2520a%250Atraining-free%2520framework%2520for%2520GUI%2520grounding%2520that%2520leverages%2520two%2520core%2520strategies%253A%250Adynamic%2520visual%2520grounding%2520and%2520modality-aware%2520optimization.%2520Instead%2520of%2520treating%250Athe%2520GUI%2520as%2520a%2520monolithic%2520image%252C%2520our%2520method%2520splits%2520the%2520input%2520into%2520textual%250Aelements%2520and%2520iconic%2520elements%252C%2520allowing%2520the%2520model%2520to%2520reason%2520over%2520each%2520modality%250Aindependently%2520using%2520general-purpose%2520vision-language%2520models.%2520When%2520predictions%250Aare%2520ambiguous%2520or%2520incorrect%252C%2520DiMo-GUI%2520dynamically%2520focuses%2520attention%2520by%250Agenerating%2520candidate%2520focal%2520regions%2520centered%2520on%2520the%2520model%2527s%2520initial%2520predictions%250Aand%2520incrementally%2520zooms%2520into%2520subregions%2520to%2520refine%2520the%2520grounding%2520result.%2520This%250Ahierarchical%2520refinement%2520process%2520helps%2520disambiguate%2520visually%2520crowded%2520layouts%250Awithout%2520the%2520need%2520for%2520additional%2520training%2520or%2520annotations.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520standard%2520GUI%2520grounding%2520benchmarks%2520and%2520demonstrate%2520consistent%250Aimprovements%2520over%2520baseline%2520inference%2520pipelines%252C%2520highlighting%2520the%2520effectiveness%250Aof%2520combining%2520modality%2520separation%2520with%2520region-focused%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiMo-GUI%3A%20Advancing%20Test-time%20Scaling%20in%20GUI%20Grounding%20via%0A%20%20Modality-Aware%20Visual%20Reasoning&entry.906535625=Hang%20Wu%20and%20Hongkai%20Chen%20and%20Yujun%20Cai%20and%20Chang%20Liu%20and%20Qingwen%20Ye%20and%20Ming-Hsuan%20Yang%20and%20Yiwei%20Wang&entry.1292438233=%20%20Grounding%20natural%20language%20queries%20in%20graphical%20user%20interfaces%20%28GUIs%29%20poses%0Aunique%20challenges%20due%20to%20the%20diversity%20of%20visual%20elements%2C%20spatial%20clutter%2C%20and%0Athe%20ambiguity%20of%20language.%20In%20this%20paper%2C%20we%20introduce%20DiMo-GUI%2C%20a%0Atraining-free%20framework%20for%20GUI%20grounding%20that%20leverages%20two%20core%20strategies%3A%0Adynamic%20visual%20grounding%20and%20modality-aware%20optimization.%20Instead%20of%20treating%0Athe%20GUI%20as%20a%20monolithic%20image%2C%20our%20method%20splits%20the%20input%20into%20textual%0Aelements%20and%20iconic%20elements%2C%20allowing%20the%20model%20to%20reason%20over%20each%20modality%0Aindependently%20using%20general-purpose%20vision-language%20models.%20When%20predictions%0Aare%20ambiguous%20or%20incorrect%2C%20DiMo-GUI%20dynamically%20focuses%20attention%20by%0Agenerating%20candidate%20focal%20regions%20centered%20on%20the%20model%27s%20initial%20predictions%0Aand%20incrementally%20zooms%20into%20subregions%20to%20refine%20the%20grounding%20result.%20This%0Ahierarchical%20refinement%20process%20helps%20disambiguate%20visually%20crowded%20layouts%0Awithout%20the%20need%20for%20additional%20training%20or%20annotations.%20We%20evaluate%20our%0Aapproach%20on%20standard%20GUI%20grounding%20benchmarks%20and%20demonstrate%20consistent%0Aimprovements%20over%20baseline%20inference%20pipelines%2C%20highlighting%20the%20effectiveness%0Aof%20combining%20modality%20separation%20with%20region-focused%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00008v2&entry.124074799=Read"},
{"title": "Dynamical Learning in Deep Asymmetric Recurrent Neural Networks", "author": "Davide Badalotti and Carlo Baldassi and Marc M\u00e9zard and Mattia Scardecchia and Riccardo Zecchina", "abstract": "  We show that asymmetric deep recurrent neural networks, enhanced with\nadditional sparse excitatory couplings, give rise to an exponentially large,\ndense accessible manifold of internal representations which can be found by\ndifferent algorithms, including simple iterative dynamics. Building on the\ngeometrical properties of the stable configurations, we propose a distributed\nlearning scheme in which input-output associations emerge naturally from the\nrecurrent dynamics, without any need of gradient evaluation. A critical feature\nenabling the learning process is the stability of the configurations reached at\nconvergence, even after removal of the supervisory output signal. Extensive\nsimulations demonstrate that this approach performs competitively on standard\nAI benchmarks. The model can be generalized in multiple directions, both\ncomputational and biological, potentially contributing to narrowing the gap\nbetween AI and computational neuroscience.\n", "link": "http://arxiv.org/abs/2509.05041v1", "date": "2025-09-05", "relevancy": 2.0687, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4918}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20Learning%20in%20Deep%20Asymmetric%20Recurrent%20Neural%20Networks&body=Title%3A%20Dynamical%20Learning%20in%20Deep%20Asymmetric%20Recurrent%20Neural%20Networks%0AAuthor%3A%20Davide%20Badalotti%20and%20Carlo%20Baldassi%20and%20Marc%20M%C3%A9zard%20and%20Mattia%20Scardecchia%20and%20Riccardo%20Zecchina%0AAbstract%3A%20%20%20We%20show%20that%20asymmetric%20deep%20recurrent%20neural%20networks%2C%20enhanced%20with%0Aadditional%20sparse%20excitatory%20couplings%2C%20give%20rise%20to%20an%20exponentially%20large%2C%0Adense%20accessible%20manifold%20of%20internal%20representations%20which%20can%20be%20found%20by%0Adifferent%20algorithms%2C%20including%20simple%20iterative%20dynamics.%20Building%20on%20the%0Ageometrical%20properties%20of%20the%20stable%20configurations%2C%20we%20propose%20a%20distributed%0Alearning%20scheme%20in%20which%20input-output%20associations%20emerge%20naturally%20from%20the%0Arecurrent%20dynamics%2C%20without%20any%20need%20of%20gradient%20evaluation.%20A%20critical%20feature%0Aenabling%20the%20learning%20process%20is%20the%20stability%20of%20the%20configurations%20reached%20at%0Aconvergence%2C%20even%20after%20removal%20of%20the%20supervisory%20output%20signal.%20Extensive%0Asimulations%20demonstrate%20that%20this%20approach%20performs%20competitively%20on%20standard%0AAI%20benchmarks.%20The%20model%20can%20be%20generalized%20in%20multiple%20directions%2C%20both%0Acomputational%20and%20biological%2C%20potentially%20contributing%20to%20narrowing%20the%20gap%0Abetween%20AI%20and%20computational%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520Learning%2520in%2520Deep%2520Asymmetric%2520Recurrent%2520Neural%2520Networks%26entry.906535625%3DDavide%2520Badalotti%2520and%2520Carlo%2520Baldassi%2520and%2520Marc%2520M%25C3%25A9zard%2520and%2520Mattia%2520Scardecchia%2520and%2520Riccardo%2520Zecchina%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520asymmetric%2520deep%2520recurrent%2520neural%2520networks%252C%2520enhanced%2520with%250Aadditional%2520sparse%2520excitatory%2520couplings%252C%2520give%2520rise%2520to%2520an%2520exponentially%2520large%252C%250Adense%2520accessible%2520manifold%2520of%2520internal%2520representations%2520which%2520can%2520be%2520found%2520by%250Adifferent%2520algorithms%252C%2520including%2520simple%2520iterative%2520dynamics.%2520Building%2520on%2520the%250Ageometrical%2520properties%2520of%2520the%2520stable%2520configurations%252C%2520we%2520propose%2520a%2520distributed%250Alearning%2520scheme%2520in%2520which%2520input-output%2520associations%2520emerge%2520naturally%2520from%2520the%250Arecurrent%2520dynamics%252C%2520without%2520any%2520need%2520of%2520gradient%2520evaluation.%2520A%2520critical%2520feature%250Aenabling%2520the%2520learning%2520process%2520is%2520the%2520stability%2520of%2520the%2520configurations%2520reached%2520at%250Aconvergence%252C%2520even%2520after%2520removal%2520of%2520the%2520supervisory%2520output%2520signal.%2520Extensive%250Asimulations%2520demonstrate%2520that%2520this%2520approach%2520performs%2520competitively%2520on%2520standard%250AAI%2520benchmarks.%2520The%2520model%2520can%2520be%2520generalized%2520in%2520multiple%2520directions%252C%2520both%250Acomputational%2520and%2520biological%252C%2520potentially%2520contributing%2520to%2520narrowing%2520the%2520gap%250Abetween%2520AI%2520and%2520computational%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20Learning%20in%20Deep%20Asymmetric%20Recurrent%20Neural%20Networks&entry.906535625=Davide%20Badalotti%20and%20Carlo%20Baldassi%20and%20Marc%20M%C3%A9zard%20and%20Mattia%20Scardecchia%20and%20Riccardo%20Zecchina&entry.1292438233=%20%20We%20show%20that%20asymmetric%20deep%20recurrent%20neural%20networks%2C%20enhanced%20with%0Aadditional%20sparse%20excitatory%20couplings%2C%20give%20rise%20to%20an%20exponentially%20large%2C%0Adense%20accessible%20manifold%20of%20internal%20representations%20which%20can%20be%20found%20by%0Adifferent%20algorithms%2C%20including%20simple%20iterative%20dynamics.%20Building%20on%20the%0Ageometrical%20properties%20of%20the%20stable%20configurations%2C%20we%20propose%20a%20distributed%0Alearning%20scheme%20in%20which%20input-output%20associations%20emerge%20naturally%20from%20the%0Arecurrent%20dynamics%2C%20without%20any%20need%20of%20gradient%20evaluation.%20A%20critical%20feature%0Aenabling%20the%20learning%20process%20is%20the%20stability%20of%20the%20configurations%20reached%20at%0Aconvergence%2C%20even%20after%20removal%20of%20the%20supervisory%20output%20signal.%20Extensive%0Asimulations%20demonstrate%20that%20this%20approach%20performs%20competitively%20on%20standard%0AAI%20benchmarks.%20The%20model%20can%20be%20generalized%20in%20multiple%20directions%2C%20both%0Acomputational%20and%20biological%2C%20potentially%20contributing%20to%20narrowing%20the%20gap%0Abetween%20AI%20and%20computational%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05041v1&entry.124074799=Read"},
{"title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs", "author": "Yilin Tao and Jiashui Huang and Huaze Xu and Ling Shao", "abstract": "  Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2509.04378v2", "date": "2025-09-05", "relevancy": 2.0498, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5462}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs&body=Title%3A%20Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs%0AAuthor%3A%20Yilin%20Tao%20and%20Jiashui%20Huang%20and%20Huaze%20Xu%20and%20Ling%20Shao%0AAbstract%3A%20%20%20Aesthetic%20Image%20Captioning%20%28AIC%29%20aims%20to%20generate%20textual%20descriptions%20of%0Aimage%20aesthetics%2C%20becoming%20a%20key%20research%20direction%20in%20the%20field%20of%0Acomputational%20aesthetics.%20In%20recent%20years%2C%20pretrained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20advanced%20rapidly%2C%20leading%20to%20a%20significant%20increase%20in%0Aimage%20aesthetics%20research%20that%20integrates%20both%20visual%20and%20textual%20modalities.%0AHowever%2C%20most%20existing%20studies%20on%20image%20aesthetics%20primarily%20focus%20on%0Apredicting%20aesthetic%20ratings%20and%20have%20shown%20limited%20application%20in%20AIC.%0AExisting%20AIC%20works%20leveraging%20MLLMs%20predominantly%20rely%20on%20fine-tuning%20methods%0Awithout%20specifically%20adapting%20MLLMs%20to%20focus%20on%20target%20aesthetic%20content.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20the%20Aesthetic%20Saliency%20Enhanced%20Multimodal%0ALarge%20Language%20Model%20%28ASE-MLLM%29%2C%20an%20end-to-end%20framework%20that%20explicitly%0Aincorporates%20aesthetic%20saliency%20into%20MLLMs.%20Within%20this%20framework%2C%20we%20introduce%0Athe%20Image%20Aesthetic%20Saliency%20Module%20%28IASM%29%2C%20which%20efficiently%20and%20effectively%0Aextracts%20aesthetic%20saliency%20features%20from%20images.%20Additionally%2C%20we%20design%0AIAS-ViT%20as%20the%20image%20encoder%20for%20MLLMs%2C%20this%20module%20fuses%20aesthetic%20saliency%0Afeatures%20with%20original%20image%20features%20via%20a%20cross-attention%20mechanism.%20To%20the%0Abest%20of%20our%20knowledge%2C%20ASE-MLLM%20is%20the%20first%20framework%20to%20integrate%20image%0Aaesthetic%20saliency%20into%20MLLMs%20specifically%20for%20AIC%20tasks.%20Extensive%20experiments%0Ademonstrated%20that%20our%20approach%20significantly%20outperformed%20traditional%20methods%0Aand%20generic%20MLLMs%20on%20current%20mainstream%20AIC%20benchmarks%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAesthetic%2520Image%2520Captioning%2520with%2520Saliency%2520Enhanced%2520MLLMs%26entry.906535625%3DYilin%2520Tao%2520and%2520Jiashui%2520Huang%2520and%2520Huaze%2520Xu%2520and%2520Ling%2520Shao%26entry.1292438233%3D%2520%2520Aesthetic%2520Image%2520Captioning%2520%2528AIC%2529%2520aims%2520to%2520generate%2520textual%2520descriptions%2520of%250Aimage%2520aesthetics%252C%2520becoming%2520a%2520key%2520research%2520direction%2520in%2520the%2520field%2520of%250Acomputational%2520aesthetics.%2520In%2520recent%2520years%252C%2520pretrained%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520have%2520advanced%2520rapidly%252C%2520leading%2520to%2520a%2520significant%2520increase%2520in%250Aimage%2520aesthetics%2520research%2520that%2520integrates%2520both%2520visual%2520and%2520textual%2520modalities.%250AHowever%252C%2520most%2520existing%2520studies%2520on%2520image%2520aesthetics%2520primarily%2520focus%2520on%250Apredicting%2520aesthetic%2520ratings%2520and%2520have%2520shown%2520limited%2520application%2520in%2520AIC.%250AExisting%2520AIC%2520works%2520leveraging%2520MLLMs%2520predominantly%2520rely%2520on%2520fine-tuning%2520methods%250Awithout%2520specifically%2520adapting%2520MLLMs%2520to%2520focus%2520on%2520target%2520aesthetic%2520content.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520the%2520Aesthetic%2520Saliency%2520Enhanced%2520Multimodal%250ALarge%2520Language%2520Model%2520%2528ASE-MLLM%2529%252C%2520an%2520end-to-end%2520framework%2520that%2520explicitly%250Aincorporates%2520aesthetic%2520saliency%2520into%2520MLLMs.%2520Within%2520this%2520framework%252C%2520we%2520introduce%250Athe%2520Image%2520Aesthetic%2520Saliency%2520Module%2520%2528IASM%2529%252C%2520which%2520efficiently%2520and%2520effectively%250Aextracts%2520aesthetic%2520saliency%2520features%2520from%2520images.%2520Additionally%252C%2520we%2520design%250AIAS-ViT%2520as%2520the%2520image%2520encoder%2520for%2520MLLMs%252C%2520this%2520module%2520fuses%2520aesthetic%2520saliency%250Afeatures%2520with%2520original%2520image%2520features%2520via%2520a%2520cross-attention%2520mechanism.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520ASE-MLLM%2520is%2520the%2520first%2520framework%2520to%2520integrate%2520image%250Aaesthetic%2520saliency%2520into%2520MLLMs%2520specifically%2520for%2520AIC%2520tasks.%2520Extensive%2520experiments%250Ademonstrated%2520that%2520our%2520approach%2520significantly%2520outperformed%2520traditional%2520methods%250Aand%2520generic%2520MLLMs%2520on%2520current%2520mainstream%2520AIC%2520benchmarks%252C%2520achieving%250Astate-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs&entry.906535625=Yilin%20Tao%20and%20Jiashui%20Huang%20and%20Huaze%20Xu%20and%20Ling%20Shao&entry.1292438233=%20%20Aesthetic%20Image%20Captioning%20%28AIC%29%20aims%20to%20generate%20textual%20descriptions%20of%0Aimage%20aesthetics%2C%20becoming%20a%20key%20research%20direction%20in%20the%20field%20of%0Acomputational%20aesthetics.%20In%20recent%20years%2C%20pretrained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20advanced%20rapidly%2C%20leading%20to%20a%20significant%20increase%20in%0Aimage%20aesthetics%20research%20that%20integrates%20both%20visual%20and%20textual%20modalities.%0AHowever%2C%20most%20existing%20studies%20on%20image%20aesthetics%20primarily%20focus%20on%0Apredicting%20aesthetic%20ratings%20and%20have%20shown%20limited%20application%20in%20AIC.%0AExisting%20AIC%20works%20leveraging%20MLLMs%20predominantly%20rely%20on%20fine-tuning%20methods%0Awithout%20specifically%20adapting%20MLLMs%20to%20focus%20on%20target%20aesthetic%20content.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20the%20Aesthetic%20Saliency%20Enhanced%20Multimodal%0ALarge%20Language%20Model%20%28ASE-MLLM%29%2C%20an%20end-to-end%20framework%20that%20explicitly%0Aincorporates%20aesthetic%20saliency%20into%20MLLMs.%20Within%20this%20framework%2C%20we%20introduce%0Athe%20Image%20Aesthetic%20Saliency%20Module%20%28IASM%29%2C%20which%20efficiently%20and%20effectively%0Aextracts%20aesthetic%20saliency%20features%20from%20images.%20Additionally%2C%20we%20design%0AIAS-ViT%20as%20the%20image%20encoder%20for%20MLLMs%2C%20this%20module%20fuses%20aesthetic%20saliency%0Afeatures%20with%20original%20image%20features%20via%20a%20cross-attention%20mechanism.%20To%20the%0Abest%20of%20our%20knowledge%2C%20ASE-MLLM%20is%20the%20first%20framework%20to%20integrate%20image%0Aaesthetic%20saliency%20into%20MLLMs%20specifically%20for%20AIC%20tasks.%20Extensive%20experiments%0Ademonstrated%20that%20our%20approach%20significantly%20outperformed%20traditional%20methods%0Aand%20generic%20MLLMs%20on%20current%20mainstream%20AIC%20benchmarks%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04378v2&entry.124074799=Read"},
{"title": "QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent\n  Reinforcement Learning", "author": "Aaron Mark Thomas and Yu-Cheng Chen and Hubert Okadome Valencia and Sharu Theresa Jose and Ronin Wu", "abstract": "  Navigating the vast chemical space of molecular structures to design novel\ndrug molecules with desired target properties remains a central challenge in\ndrug discovery. Recent advances in generative models offer promising solutions.\nThis work presents a novel quantum circuit Born machine (QCBM)-enabled\nGenerative Adversarial Network (GAN), called QCA-MolGAN, for generating\ndrug-like molecules. The QCBM serves as a learnable prior distribution, which\nis associatively trained to define a latent space aligning with high-level\nfeatures captured by the GANs discriminator. Additionally, we integrate a novel\nmulti-agent reinforcement learning network to guide molecular generation with\ndesired targeted properties, optimising key metrics such as quantitative\nestimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and\nsynthetic accessibility (SA) scores in conjunction with one another.\nExperimental results demonstrate that our approach enhances the property\nalignment of generated molecules with the multi-agent reinforcement learning\nagents effectively balancing chemical properties.\n", "link": "http://arxiv.org/abs/2509.05051v1", "date": "2025-09-05", "relevancy": 2.049, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5212}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QCA-MolGAN%3A%20Quantum%20Circuit%20Associative%20Molecular%20GAN%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning&body=Title%3A%20QCA-MolGAN%3A%20Quantum%20Circuit%20Associative%20Molecular%20GAN%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Aaron%20Mark%20Thomas%20and%20Yu-Cheng%20Chen%20and%20Hubert%20Okadome%20Valencia%20and%20Sharu%20Theresa%20Jose%20and%20Ronin%20Wu%0AAbstract%3A%20%20%20Navigating%20the%20vast%20chemical%20space%20of%20molecular%20structures%20to%20design%20novel%0Adrug%20molecules%20with%20desired%20target%20properties%20remains%20a%20central%20challenge%20in%0Adrug%20discovery.%20Recent%20advances%20in%20generative%20models%20offer%20promising%20solutions.%0AThis%20work%20presents%20a%20novel%20quantum%20circuit%20Born%20machine%20%28QCBM%29-enabled%0AGenerative%20Adversarial%20Network%20%28GAN%29%2C%20called%20QCA-MolGAN%2C%20for%20generating%0Adrug-like%20molecules.%20The%20QCBM%20serves%20as%20a%20learnable%20prior%20distribution%2C%20which%0Ais%20associatively%20trained%20to%20define%20a%20latent%20space%20aligning%20with%20high-level%0Afeatures%20captured%20by%20the%20GANs%20discriminator.%20Additionally%2C%20we%20integrate%20a%20novel%0Amulti-agent%20reinforcement%20learning%20network%20to%20guide%20molecular%20generation%20with%0Adesired%20targeted%20properties%2C%20optimising%20key%20metrics%20such%20as%20quantitative%0Aestimate%20of%20drug-likeness%20%28QED%29%2C%20octanol-water%20partition%20coefficient%20%28LogP%29%20and%0Asynthetic%20accessibility%20%28SA%29%20scores%20in%20conjunction%20with%20one%20another.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20enhances%20the%20property%0Aalignment%20of%20generated%20molecules%20with%20the%20multi-agent%20reinforcement%20learning%0Aagents%20effectively%20balancing%20chemical%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQCA-MolGAN%253A%2520Quantum%2520Circuit%2520Associative%2520Molecular%2520GAN%2520with%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAaron%2520Mark%2520Thomas%2520and%2520Yu-Cheng%2520Chen%2520and%2520Hubert%2520Okadome%2520Valencia%2520and%2520Sharu%2520Theresa%2520Jose%2520and%2520Ronin%2520Wu%26entry.1292438233%3D%2520%2520Navigating%2520the%2520vast%2520chemical%2520space%2520of%2520molecular%2520structures%2520to%2520design%2520novel%250Adrug%2520molecules%2520with%2520desired%2520target%2520properties%2520remains%2520a%2520central%2520challenge%2520in%250Adrug%2520discovery.%2520Recent%2520advances%2520in%2520generative%2520models%2520offer%2520promising%2520solutions.%250AThis%2520work%2520presents%2520a%2520novel%2520quantum%2520circuit%2520Born%2520machine%2520%2528QCBM%2529-enabled%250AGenerative%2520Adversarial%2520Network%2520%2528GAN%2529%252C%2520called%2520QCA-MolGAN%252C%2520for%2520generating%250Adrug-like%2520molecules.%2520The%2520QCBM%2520serves%2520as%2520a%2520learnable%2520prior%2520distribution%252C%2520which%250Ais%2520associatively%2520trained%2520to%2520define%2520a%2520latent%2520space%2520aligning%2520with%2520high-level%250Afeatures%2520captured%2520by%2520the%2520GANs%2520discriminator.%2520Additionally%252C%2520we%2520integrate%2520a%2520novel%250Amulti-agent%2520reinforcement%2520learning%2520network%2520to%2520guide%2520molecular%2520generation%2520with%250Adesired%2520targeted%2520properties%252C%2520optimising%2520key%2520metrics%2520such%2520as%2520quantitative%250Aestimate%2520of%2520drug-likeness%2520%2528QED%2529%252C%2520octanol-water%2520partition%2520coefficient%2520%2528LogP%2529%2520and%250Asynthetic%2520accessibility%2520%2528SA%2529%2520scores%2520in%2520conjunction%2520with%2520one%2520another.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520enhances%2520the%2520property%250Aalignment%2520of%2520generated%2520molecules%2520with%2520the%2520multi-agent%2520reinforcement%2520learning%250Aagents%2520effectively%2520balancing%2520chemical%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QCA-MolGAN%3A%20Quantum%20Circuit%20Associative%20Molecular%20GAN%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning&entry.906535625=Aaron%20Mark%20Thomas%20and%20Yu-Cheng%20Chen%20and%20Hubert%20Okadome%20Valencia%20and%20Sharu%20Theresa%20Jose%20and%20Ronin%20Wu&entry.1292438233=%20%20Navigating%20the%20vast%20chemical%20space%20of%20molecular%20structures%20to%20design%20novel%0Adrug%20molecules%20with%20desired%20target%20properties%20remains%20a%20central%20challenge%20in%0Adrug%20discovery.%20Recent%20advances%20in%20generative%20models%20offer%20promising%20solutions.%0AThis%20work%20presents%20a%20novel%20quantum%20circuit%20Born%20machine%20%28QCBM%29-enabled%0AGenerative%20Adversarial%20Network%20%28GAN%29%2C%20called%20QCA-MolGAN%2C%20for%20generating%0Adrug-like%20molecules.%20The%20QCBM%20serves%20as%20a%20learnable%20prior%20distribution%2C%20which%0Ais%20associatively%20trained%20to%20define%20a%20latent%20space%20aligning%20with%20high-level%0Afeatures%20captured%20by%20the%20GANs%20discriminator.%20Additionally%2C%20we%20integrate%20a%20novel%0Amulti-agent%20reinforcement%20learning%20network%20to%20guide%20molecular%20generation%20with%0Adesired%20targeted%20properties%2C%20optimising%20key%20metrics%20such%20as%20quantitative%0Aestimate%20of%20drug-likeness%20%28QED%29%2C%20octanol-water%20partition%20coefficient%20%28LogP%29%20and%0Asynthetic%20accessibility%20%28SA%29%20scores%20in%20conjunction%20with%20one%20another.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20enhances%20the%20property%0Aalignment%20of%20generated%20molecules%20with%20the%20multi-agent%20reinforcement%20learning%0Aagents%20effectively%20balancing%20chemical%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05051v1&entry.124074799=Read"},
{"title": "Demystifying Chains, Trees, and Graphs of Thoughts", "author": "Maciej Besta and Florim Memedi and Zhenyu Zhang and Robert Gerstenberger and Guangyuan Piao and Nils Blach and Piotr Nyczyk and Marcin Copik and Grzegorz Kwa\u015bniewski and J\u00fcrgen M\u00fcller and Lukas Gianinazzi and Ales Kubicek and Hubert Niewiadomski and Aidan O'Mahony and Onur Mutlu and Torsten Hoefler", "abstract": "  The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.\n", "link": "http://arxiv.org/abs/2401.14295v5", "date": "2025-09-05", "relevancy": 2.044, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts&body=Title%3A%20Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts%0AAuthor%3A%20Maciej%20Besta%20and%20Florim%20Memedi%20and%20Zhenyu%20Zhang%20and%20Robert%20Gerstenberger%20and%20Guangyuan%20Piao%20and%20Nils%20Blach%20and%20Piotr%20Nyczyk%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Lukas%20Gianinazzi%20and%20Ales%20Kubicek%20and%20Hubert%20Niewiadomski%20and%20Aidan%20O%27Mahony%20and%20Onur%20Mutlu%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20witnessed%20significant%0Aprogress%20in%20recent%20years%2C%20with%20a%20notable%20focus%20on%20improving%20large%20language%0Amodels%27%20%28LLM%29%20performance%20through%20innovative%20prompting%20techniques.%20Among%20these%2C%0Aprompt%20engineering%20coupled%20with%20structures%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Awith%20designs%20such%20as%20Chain-of-Thought%2C%20Tree%20of%20Thoughts%2C%20or%20Graph%20of%20Thoughts%2C%0Ain%20which%20the%20overall%20LLM%20reasoning%20is%20guided%20by%20a%20structure%20such%20as%20a%20graph.%20As%0Aillustrated%20with%20numerous%20examples%2C%20this%20paradigm%20significantly%20enhances%20the%0ALLM%27s%20capability%20to%20solve%20numerous%20tasks%2C%20ranging%20from%20logical%20or%20mathematical%0Areasoning%20to%20planning%20or%20creative%20writing.%20To%20facilitate%20the%20understanding%20of%0Athis%20growing%20field%20and%20pave%20the%20way%20for%20future%20developments%2C%20we%20devise%20a%0Ageneral%20blueprint%20for%20effective%20and%20efficient%20LLM%20reasoning%20schemes.%20For%20this%2C%0Awe%20conduct%20an%20in-depth%20analysis%20of%20the%20prompt%20execution%20pipeline%2C%20clarifying%0Aand%20clearly%20defining%20different%20concepts.%20We%20then%20build%20the%20first%20taxonomy%20of%0Astructure-enhanced%20LLM%20reasoning%20schemes.%20We%20focus%20on%20identifying%20fundamental%0Aclasses%20of%20harnessed%20structures%2C%20and%20we%20analyze%20the%20representations%20of%20these%0Astructures%2C%20algorithms%20executed%20with%20these%20structures%2C%20and%20many%20others.%20We%0Arefer%20to%20these%20structures%20as%20reasoning%20topologies%2C%20because%20their%20representation%0Abecomes%20to%20a%20degree%20spatial%2C%20as%20they%20are%20contained%20within%20the%20LLM%20context.%20Our%0Astudy%20compares%20existing%20prompting%20schemes%20using%20the%20proposed%20taxonomy%2C%0Adiscussing%20how%20certain%20design%20choices%20lead%20to%20different%20patterns%20in%20performance%0Aand%20cost.%20We%20also%20outline%20theoretical%20underpinnings%2C%20relationships%20between%0Aprompting%20and%20other%20parts%20of%20the%20LLM%20ecosystem%20such%20as%20knowledge%20bases%2C%20and%20the%0Aassociated%20research%20challenges.%20Our%20work%20will%20help%20to%20advance%20future%20prompt%0Aengineering%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14295v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520Chains%252C%2520Trees%252C%2520and%2520Graphs%2520of%2520Thoughts%26entry.906535625%3DMaciej%2520Besta%2520and%2520Florim%2520Memedi%2520and%2520Zhenyu%2520Zhang%2520and%2520Robert%2520Gerstenberger%2520and%2520Guangyuan%2520Piao%2520and%2520Nils%2520Blach%2520and%2520Piotr%2520Nyczyk%2520and%2520Marcin%2520Copik%2520and%2520Grzegorz%2520Kwa%25C5%259Bniewski%2520and%2520J%25C3%25BCrgen%2520M%25C3%25BCller%2520and%2520Lukas%2520Gianinazzi%2520and%2520Ales%2520Kubicek%2520and%2520Hubert%2520Niewiadomski%2520and%2520Aidan%2520O%2527Mahony%2520and%2520Onur%2520Mutlu%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520has%2520witnessed%2520significant%250Aprogress%2520in%2520recent%2520years%252C%2520with%2520a%2520notable%2520focus%2520on%2520improving%2520large%2520language%250Amodels%2527%2520%2528LLM%2529%2520performance%2520through%2520innovative%2520prompting%2520techniques.%2520Among%2520these%252C%250Aprompt%2520engineering%2520coupled%2520with%2520structures%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%252C%250Awith%2520designs%2520such%2520as%2520Chain-of-Thought%252C%2520Tree%2520of%2520Thoughts%252C%2520or%2520Graph%2520of%2520Thoughts%252C%250Ain%2520which%2520the%2520overall%2520LLM%2520reasoning%2520is%2520guided%2520by%2520a%2520structure%2520such%2520as%2520a%2520graph.%2520As%250Aillustrated%2520with%2520numerous%2520examples%252C%2520this%2520paradigm%2520significantly%2520enhances%2520the%250ALLM%2527s%2520capability%2520to%2520solve%2520numerous%2520tasks%252C%2520ranging%2520from%2520logical%2520or%2520mathematical%250Areasoning%2520to%2520planning%2520or%2520creative%2520writing.%2520To%2520facilitate%2520the%2520understanding%2520of%250Athis%2520growing%2520field%2520and%2520pave%2520the%2520way%2520for%2520future%2520developments%252C%2520we%2520devise%2520a%250Ageneral%2520blueprint%2520for%2520effective%2520and%2520efficient%2520LLM%2520reasoning%2520schemes.%2520For%2520this%252C%250Awe%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520the%2520prompt%2520execution%2520pipeline%252C%2520clarifying%250Aand%2520clearly%2520defining%2520different%2520concepts.%2520We%2520then%2520build%2520the%2520first%2520taxonomy%2520of%250Astructure-enhanced%2520LLM%2520reasoning%2520schemes.%2520We%2520focus%2520on%2520identifying%2520fundamental%250Aclasses%2520of%2520harnessed%2520structures%252C%2520and%2520we%2520analyze%2520the%2520representations%2520of%2520these%250Astructures%252C%2520algorithms%2520executed%2520with%2520these%2520structures%252C%2520and%2520many%2520others.%2520We%250Arefer%2520to%2520these%2520structures%2520as%2520reasoning%2520topologies%252C%2520because%2520their%2520representation%250Abecomes%2520to%2520a%2520degree%2520spatial%252C%2520as%2520they%2520are%2520contained%2520within%2520the%2520LLM%2520context.%2520Our%250Astudy%2520compares%2520existing%2520prompting%2520schemes%2520using%2520the%2520proposed%2520taxonomy%252C%250Adiscussing%2520how%2520certain%2520design%2520choices%2520lead%2520to%2520different%2520patterns%2520in%2520performance%250Aand%2520cost.%2520We%2520also%2520outline%2520theoretical%2520underpinnings%252C%2520relationships%2520between%250Aprompting%2520and%2520other%2520parts%2520of%2520the%2520LLM%2520ecosystem%2520such%2520as%2520knowledge%2520bases%252C%2520and%2520the%250Aassociated%2520research%2520challenges.%2520Our%2520work%2520will%2520help%2520to%2520advance%2520future%2520prompt%250Aengineering%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14295v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts&entry.906535625=Maciej%20Besta%20and%20Florim%20Memedi%20and%20Zhenyu%20Zhang%20and%20Robert%20Gerstenberger%20and%20Guangyuan%20Piao%20and%20Nils%20Blach%20and%20Piotr%20Nyczyk%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Lukas%20Gianinazzi%20and%20Ales%20Kubicek%20and%20Hubert%20Niewiadomski%20and%20Aidan%20O%27Mahony%20and%20Onur%20Mutlu%20and%20Torsten%20Hoefler&entry.1292438233=%20%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20witnessed%20significant%0Aprogress%20in%20recent%20years%2C%20with%20a%20notable%20focus%20on%20improving%20large%20language%0Amodels%27%20%28LLM%29%20performance%20through%20innovative%20prompting%20techniques.%20Among%20these%2C%0Aprompt%20engineering%20coupled%20with%20structures%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Awith%20designs%20such%20as%20Chain-of-Thought%2C%20Tree%20of%20Thoughts%2C%20or%20Graph%20of%20Thoughts%2C%0Ain%20which%20the%20overall%20LLM%20reasoning%20is%20guided%20by%20a%20structure%20such%20as%20a%20graph.%20As%0Aillustrated%20with%20numerous%20examples%2C%20this%20paradigm%20significantly%20enhances%20the%0ALLM%27s%20capability%20to%20solve%20numerous%20tasks%2C%20ranging%20from%20logical%20or%20mathematical%0Areasoning%20to%20planning%20or%20creative%20writing.%20To%20facilitate%20the%20understanding%20of%0Athis%20growing%20field%20and%20pave%20the%20way%20for%20future%20developments%2C%20we%20devise%20a%0Ageneral%20blueprint%20for%20effective%20and%20efficient%20LLM%20reasoning%20schemes.%20For%20this%2C%0Awe%20conduct%20an%20in-depth%20analysis%20of%20the%20prompt%20execution%20pipeline%2C%20clarifying%0Aand%20clearly%20defining%20different%20concepts.%20We%20then%20build%20the%20first%20taxonomy%20of%0Astructure-enhanced%20LLM%20reasoning%20schemes.%20We%20focus%20on%20identifying%20fundamental%0Aclasses%20of%20harnessed%20structures%2C%20and%20we%20analyze%20the%20representations%20of%20these%0Astructures%2C%20algorithms%20executed%20with%20these%20structures%2C%20and%20many%20others.%20We%0Arefer%20to%20these%20structures%20as%20reasoning%20topologies%2C%20because%20their%20representation%0Abecomes%20to%20a%20degree%20spatial%2C%20as%20they%20are%20contained%20within%20the%20LLM%20context.%20Our%0Astudy%20compares%20existing%20prompting%20schemes%20using%20the%20proposed%20taxonomy%2C%0Adiscussing%20how%20certain%20design%20choices%20lead%20to%20different%20patterns%20in%20performance%0Aand%20cost.%20We%20also%20outline%20theoretical%20underpinnings%2C%20relationships%20between%0Aprompting%20and%20other%20parts%20of%20the%20LLM%20ecosystem%20such%20as%20knowledge%20bases%2C%20and%20the%0Aassociated%20research%20challenges.%20Our%20work%20will%20help%20to%20advance%20future%20prompt%0Aengineering%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14295v5&entry.124074799=Read"},
{"title": "Foundational Models and Federated Learning: Survey, Taxonomy, Challenges\n  and Practical Insights", "author": "Cosmin-Andrei Hatfaludi and Alex Serban", "abstract": "  Federated learning has the potential to unlock siloed data and distributed\nresources by enabling collaborative model training without sharing private\ndata. As more complex foundational models gain widespread use, the need to\nexpand training resources and integrate privately owned data grows as well. In\nthis article, we explore the intersection of federated learning and\nfoundational models, aiming to identify, categorize, and characterize technical\nmethods that integrate the two paradigms. As a unified survey is currently\nunavailable, we present a literature survey structured around a novel taxonomy\nthat follows the development life-cycle stages, along with a technical\ncomparison of available methods. Additionally, we provide practical insights\nand guidelines for implementing and evolving these methods, with a specific\nfocus on the healthcare domain as a case study, where the potential impact of\nfederated learning and foundational models is considered significant. Our\nsurvey covers multiple intersecting topics, including but not limited to\nfederated learning, self-supervised learning, fine-tuning, distillation, and\ntransfer learning. Initially, we retrieved and reviewed a set of over 4,200\narticles. This collection was narrowed to more than 250 thoroughly reviewed\narticles through inclusion criteria, featuring 42 unique methods. The methods\nwere used to construct the taxonomy and enabled their comparison based on\ncomplexity, efficiency, and scalability. We present these results as a\nself-contained overview that not only summarizes the state of the field but\nalso provides insights into the practical aspects of adopting, evolving, and\nintegrating foundational models with federated learning.\n", "link": "http://arxiv.org/abs/2509.05142v1", "date": "2025-09-05", "relevancy": 2.0431, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20Models%20and%20Federated%20Learning%3A%20Survey%2C%20Taxonomy%2C%20Challenges%0A%20%20and%20Practical%20Insights&body=Title%3A%20Foundational%20Models%20and%20Federated%20Learning%3A%20Survey%2C%20Taxonomy%2C%20Challenges%0A%20%20and%20Practical%20Insights%0AAuthor%3A%20Cosmin-Andrei%20Hatfaludi%20and%20Alex%20Serban%0AAbstract%3A%20%20%20Federated%20learning%20has%20the%20potential%20to%20unlock%20siloed%20data%20and%20distributed%0Aresources%20by%20enabling%20collaborative%20model%20training%20without%20sharing%20private%0Adata.%20As%20more%20complex%20foundational%20models%20gain%20widespread%20use%2C%20the%20need%20to%0Aexpand%20training%20resources%20and%20integrate%20privately%20owned%20data%20grows%20as%20well.%20In%0Athis%20article%2C%20we%20explore%20the%20intersection%20of%20federated%20learning%20and%0Afoundational%20models%2C%20aiming%20to%20identify%2C%20categorize%2C%20and%20characterize%20technical%0Amethods%20that%20integrate%20the%20two%20paradigms.%20As%20a%20unified%20survey%20is%20currently%0Aunavailable%2C%20we%20present%20a%20literature%20survey%20structured%20around%20a%20novel%20taxonomy%0Athat%20follows%20the%20development%20life-cycle%20stages%2C%20along%20with%20a%20technical%0Acomparison%20of%20available%20methods.%20Additionally%2C%20we%20provide%20practical%20insights%0Aand%20guidelines%20for%20implementing%20and%20evolving%20these%20methods%2C%20with%20a%20specific%0Afocus%20on%20the%20healthcare%20domain%20as%20a%20case%20study%2C%20where%20the%20potential%20impact%20of%0Afederated%20learning%20and%20foundational%20models%20is%20considered%20significant.%20Our%0Asurvey%20covers%20multiple%20intersecting%20topics%2C%20including%20but%20not%20limited%20to%0Afederated%20learning%2C%20self-supervised%20learning%2C%20fine-tuning%2C%20distillation%2C%20and%0Atransfer%20learning.%20Initially%2C%20we%20retrieved%20and%20reviewed%20a%20set%20of%20over%204%2C200%0Aarticles.%20This%20collection%20was%20narrowed%20to%20more%20than%20250%20thoroughly%20reviewed%0Aarticles%20through%20inclusion%20criteria%2C%20featuring%2042%20unique%20methods.%20The%20methods%0Awere%20used%20to%20construct%20the%20taxonomy%20and%20enabled%20their%20comparison%20based%20on%0Acomplexity%2C%20efficiency%2C%20and%20scalability.%20We%20present%20these%20results%20as%20a%0Aself-contained%20overview%20that%20not%20only%20summarizes%20the%20state%20of%20the%20field%20but%0Aalso%20provides%20insights%20into%20the%20practical%20aspects%20of%20adopting%2C%20evolving%2C%20and%0Aintegrating%20foundational%20models%20with%20federated%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520Models%2520and%2520Federated%2520Learning%253A%2520Survey%252C%2520Taxonomy%252C%2520Challenges%250A%2520%2520and%2520Practical%2520Insights%26entry.906535625%3DCosmin-Andrei%2520Hatfaludi%2520and%2520Alex%2520Serban%26entry.1292438233%3D%2520%2520Federated%2520learning%2520has%2520the%2520potential%2520to%2520unlock%2520siloed%2520data%2520and%2520distributed%250Aresources%2520by%2520enabling%2520collaborative%2520model%2520training%2520without%2520sharing%2520private%250Adata.%2520As%2520more%2520complex%2520foundational%2520models%2520gain%2520widespread%2520use%252C%2520the%2520need%2520to%250Aexpand%2520training%2520resources%2520and%2520integrate%2520privately%2520owned%2520data%2520grows%2520as%2520well.%2520In%250Athis%2520article%252C%2520we%2520explore%2520the%2520intersection%2520of%2520federated%2520learning%2520and%250Afoundational%2520models%252C%2520aiming%2520to%2520identify%252C%2520categorize%252C%2520and%2520characterize%2520technical%250Amethods%2520that%2520integrate%2520the%2520two%2520paradigms.%2520As%2520a%2520unified%2520survey%2520is%2520currently%250Aunavailable%252C%2520we%2520present%2520a%2520literature%2520survey%2520structured%2520around%2520a%2520novel%2520taxonomy%250Athat%2520follows%2520the%2520development%2520life-cycle%2520stages%252C%2520along%2520with%2520a%2520technical%250Acomparison%2520of%2520available%2520methods.%2520Additionally%252C%2520we%2520provide%2520practical%2520insights%250Aand%2520guidelines%2520for%2520implementing%2520and%2520evolving%2520these%2520methods%252C%2520with%2520a%2520specific%250Afocus%2520on%2520the%2520healthcare%2520domain%2520as%2520a%2520case%2520study%252C%2520where%2520the%2520potential%2520impact%2520of%250Afederated%2520learning%2520and%2520foundational%2520models%2520is%2520considered%2520significant.%2520Our%250Asurvey%2520covers%2520multiple%2520intersecting%2520topics%252C%2520including%2520but%2520not%2520limited%2520to%250Afederated%2520learning%252C%2520self-supervised%2520learning%252C%2520fine-tuning%252C%2520distillation%252C%2520and%250Atransfer%2520learning.%2520Initially%252C%2520we%2520retrieved%2520and%2520reviewed%2520a%2520set%2520of%2520over%25204%252C200%250Aarticles.%2520This%2520collection%2520was%2520narrowed%2520to%2520more%2520than%2520250%2520thoroughly%2520reviewed%250Aarticles%2520through%2520inclusion%2520criteria%252C%2520featuring%252042%2520unique%2520methods.%2520The%2520methods%250Awere%2520used%2520to%2520construct%2520the%2520taxonomy%2520and%2520enabled%2520their%2520comparison%2520based%2520on%250Acomplexity%252C%2520efficiency%252C%2520and%2520scalability.%2520We%2520present%2520these%2520results%2520as%2520a%250Aself-contained%2520overview%2520that%2520not%2520only%2520summarizes%2520the%2520state%2520of%2520the%2520field%2520but%250Aalso%2520provides%2520insights%2520into%2520the%2520practical%2520aspects%2520of%2520adopting%252C%2520evolving%252C%2520and%250Aintegrating%2520foundational%2520models%2520with%2520federated%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20Models%20and%20Federated%20Learning%3A%20Survey%2C%20Taxonomy%2C%20Challenges%0A%20%20and%20Practical%20Insights&entry.906535625=Cosmin-Andrei%20Hatfaludi%20and%20Alex%20Serban&entry.1292438233=%20%20Federated%20learning%20has%20the%20potential%20to%20unlock%20siloed%20data%20and%20distributed%0Aresources%20by%20enabling%20collaborative%20model%20training%20without%20sharing%20private%0Adata.%20As%20more%20complex%20foundational%20models%20gain%20widespread%20use%2C%20the%20need%20to%0Aexpand%20training%20resources%20and%20integrate%20privately%20owned%20data%20grows%20as%20well.%20In%0Athis%20article%2C%20we%20explore%20the%20intersection%20of%20federated%20learning%20and%0Afoundational%20models%2C%20aiming%20to%20identify%2C%20categorize%2C%20and%20characterize%20technical%0Amethods%20that%20integrate%20the%20two%20paradigms.%20As%20a%20unified%20survey%20is%20currently%0Aunavailable%2C%20we%20present%20a%20literature%20survey%20structured%20around%20a%20novel%20taxonomy%0Athat%20follows%20the%20development%20life-cycle%20stages%2C%20along%20with%20a%20technical%0Acomparison%20of%20available%20methods.%20Additionally%2C%20we%20provide%20practical%20insights%0Aand%20guidelines%20for%20implementing%20and%20evolving%20these%20methods%2C%20with%20a%20specific%0Afocus%20on%20the%20healthcare%20domain%20as%20a%20case%20study%2C%20where%20the%20potential%20impact%20of%0Afederated%20learning%20and%20foundational%20models%20is%20considered%20significant.%20Our%0Asurvey%20covers%20multiple%20intersecting%20topics%2C%20including%20but%20not%20limited%20to%0Afederated%20learning%2C%20self-supervised%20learning%2C%20fine-tuning%2C%20distillation%2C%20and%0Atransfer%20learning.%20Initially%2C%20we%20retrieved%20and%20reviewed%20a%20set%20of%20over%204%2C200%0Aarticles.%20This%20collection%20was%20narrowed%20to%20more%20than%20250%20thoroughly%20reviewed%0Aarticles%20through%20inclusion%20criteria%2C%20featuring%2042%20unique%20methods.%20The%20methods%0Awere%20used%20to%20construct%20the%20taxonomy%20and%20enabled%20their%20comparison%20based%20on%0Acomplexity%2C%20efficiency%2C%20and%20scalability.%20We%20present%20these%20results%20as%20a%0Aself-contained%20overview%20that%20not%20only%20summarizes%20the%20state%20of%20the%20field%20but%0Aalso%20provides%20insights%20into%20the%20practical%20aspects%20of%20adopting%2C%20evolving%2C%20and%0Aintegrating%20foundational%20models%20with%20federated%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05142v1&entry.124074799=Read"},
{"title": "Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and\n  Feature Regularization", "author": "Shuaicheng Niu and Guohao Chen and Deyu Chen and Yifan Zhang and Jiaxiang Wu and Zhiquan Wen and Yaofo Chen and Peilin Zhao and Chunyan Miao and Mingkui Tan", "abstract": "  Test-time adaptation (TTA) may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, 3) online imbalanced label distribution shifts. This is often a key\nobstacle preventing existing TTA methods from being deployed in the real world.\nIn this paper, we investigate the unstable reasons and find that the batch norm\nlayer is a crucial factor hindering TTA stability. Conversely, TTA can perform\nmore stably with batch-agnostic norm layers, i.e., group or layer norm.\nHowever, we observe that TTA with group and layer norms does not always succeed\nand still suffers many failure cases, i.e., the model collapses into trivial\nsolutions by assigning the same class label for all samples. By digging into\nthis, we find that, during the collapse process: 1) the model gradients often\nundergo an initial explosion followed by rapid degradation, suggesting that\ncertain noisy test samples with large gradients may disrupt adaptation; and 2)\nthe model representations tend to exhibit high correlations and classification\nbias. To address this, we first propose a sharpness-aware and reliable entropy\nminimization method, called SAR, for stabilizing TTA from two aspects: 1)\nremove partial noisy samples with large gradients, 2) encourage model weights\nto go to a flat minimum so that the model is robust to the remaining noisy\nsamples. Based on SAR, we further introduce SAR^2 to prevent representation\ncollapse with two regularizers: 1) a redundancy regularizer to reduce\ninter-dimensional correlations among centroid-invariant features; and 2) an\ninequity regularizer to maximize the prediction entropy of a prototype\ncentroid, thereby penalizing biased representations toward any specific class.\nPromising results demonstrate that our methods perform more stably over prior\nmethods and are computationally efficient under the above wild test scenarios.\n", "link": "http://arxiv.org/abs/2509.04977v1", "date": "2025-09-05", "relevancy": 2.0281, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt%20in%20the%20Wild%3A%20Test-Time%20Entropy%20Minimization%20with%20Sharpness%20and%0A%20%20Feature%20Regularization&body=Title%3A%20Adapt%20in%20the%20Wild%3A%20Test-Time%20Entropy%20Minimization%20with%20Sharpness%20and%0A%20%20Feature%20Regularization%0AAuthor%3A%20Shuaicheng%20Niu%20and%20Guohao%20Chen%20and%20Deyu%20Chen%20and%20Yifan%20Zhang%20and%20Jiaxiang%20Wu%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Peilin%20Zhao%20and%20Chunyan%20Miao%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20may%20fail%20to%20improve%20or%20even%20harm%20the%20model%0Aperformance%20when%20test%20data%20have%3A%201%29%20mixed%20distribution%20shifts%2C%202%29%20small%20batch%0Asizes%2C%203%29%20online%20imbalanced%20label%20distribution%20shifts.%20This%20is%20often%20a%20key%0Aobstacle%20preventing%20existing%20TTA%20methods%20from%20being%20deployed%20in%20the%20real%20world.%0AIn%20this%20paper%2C%20we%20investigate%20the%20unstable%20reasons%20and%20find%20that%20the%20batch%20norm%0Alayer%20is%20a%20crucial%20factor%20hindering%20TTA%20stability.%20Conversely%2C%20TTA%20can%20perform%0Amore%20stably%20with%20batch-agnostic%20norm%20layers%2C%20i.e.%2C%20group%20or%20layer%20norm.%0AHowever%2C%20we%20observe%20that%20TTA%20with%20group%20and%20layer%20norms%20does%20not%20always%20succeed%0Aand%20still%20suffers%20many%20failure%20cases%2C%20i.e.%2C%20the%20model%20collapses%20into%20trivial%0Asolutions%20by%20assigning%20the%20same%20class%20label%20for%20all%20samples.%20By%20digging%20into%0Athis%2C%20we%20find%20that%2C%20during%20the%20collapse%20process%3A%201%29%20the%20model%20gradients%20often%0Aundergo%20an%20initial%20explosion%20followed%20by%20rapid%20degradation%2C%20suggesting%20that%0Acertain%20noisy%20test%20samples%20with%20large%20gradients%20may%20disrupt%20adaptation%3B%20and%202%29%0Athe%20model%20representations%20tend%20to%20exhibit%20high%20correlations%20and%20classification%0Abias.%20To%20address%20this%2C%20we%20first%20propose%20a%20sharpness-aware%20and%20reliable%20entropy%0Aminimization%20method%2C%20called%20SAR%2C%20for%20stabilizing%20TTA%20from%20two%20aspects%3A%201%29%0Aremove%20partial%20noisy%20samples%20with%20large%20gradients%2C%202%29%20encourage%20model%20weights%0Ato%20go%20to%20a%20flat%20minimum%20so%20that%20the%20model%20is%20robust%20to%20the%20remaining%20noisy%0Asamples.%20Based%20on%20SAR%2C%20we%20further%20introduce%20SAR%5E2%20to%20prevent%20representation%0Acollapse%20with%20two%20regularizers%3A%201%29%20a%20redundancy%20regularizer%20to%20reduce%0Ainter-dimensional%20correlations%20among%20centroid-invariant%20features%3B%20and%202%29%20an%0Ainequity%20regularizer%20to%20maximize%20the%20prediction%20entropy%20of%20a%20prototype%0Acentroid%2C%20thereby%20penalizing%20biased%20representations%20toward%20any%20specific%20class.%0APromising%20results%20demonstrate%20that%20our%20methods%20perform%20more%20stably%20over%20prior%0Amethods%20and%20are%20computationally%20efficient%20under%20the%20above%20wild%20test%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt%2520in%2520the%2520Wild%253A%2520Test-Time%2520Entropy%2520Minimization%2520with%2520Sharpness%2520and%250A%2520%2520Feature%2520Regularization%26entry.906535625%3DShuaicheng%2520Niu%2520and%2520Guohao%2520Chen%2520and%2520Deyu%2520Chen%2520and%2520Yifan%2520Zhang%2520and%2520Jiaxiang%2520Wu%2520and%2520Zhiquan%2520Wen%2520and%2520Yaofo%2520Chen%2520and%2520Peilin%2520Zhao%2520and%2520Chunyan%2520Miao%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520%2528TTA%2529%2520may%2520fail%2520to%2520improve%2520or%2520even%2520harm%2520the%2520model%250Aperformance%2520when%2520test%2520data%2520have%253A%25201%2529%2520mixed%2520distribution%2520shifts%252C%25202%2529%2520small%2520batch%250Asizes%252C%25203%2529%2520online%2520imbalanced%2520label%2520distribution%2520shifts.%2520This%2520is%2520often%2520a%2520key%250Aobstacle%2520preventing%2520existing%2520TTA%2520methods%2520from%2520being%2520deployed%2520in%2520the%2520real%2520world.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520the%2520unstable%2520reasons%2520and%2520find%2520that%2520the%2520batch%2520norm%250Alayer%2520is%2520a%2520crucial%2520factor%2520hindering%2520TTA%2520stability.%2520Conversely%252C%2520TTA%2520can%2520perform%250Amore%2520stably%2520with%2520batch-agnostic%2520norm%2520layers%252C%2520i.e.%252C%2520group%2520or%2520layer%2520norm.%250AHowever%252C%2520we%2520observe%2520that%2520TTA%2520with%2520group%2520and%2520layer%2520norms%2520does%2520not%2520always%2520succeed%250Aand%2520still%2520suffers%2520many%2520failure%2520cases%252C%2520i.e.%252C%2520the%2520model%2520collapses%2520into%2520trivial%250Asolutions%2520by%2520assigning%2520the%2520same%2520class%2520label%2520for%2520all%2520samples.%2520By%2520digging%2520into%250Athis%252C%2520we%2520find%2520that%252C%2520during%2520the%2520collapse%2520process%253A%25201%2529%2520the%2520model%2520gradients%2520often%250Aundergo%2520an%2520initial%2520explosion%2520followed%2520by%2520rapid%2520degradation%252C%2520suggesting%2520that%250Acertain%2520noisy%2520test%2520samples%2520with%2520large%2520gradients%2520may%2520disrupt%2520adaptation%253B%2520and%25202%2529%250Athe%2520model%2520representations%2520tend%2520to%2520exhibit%2520high%2520correlations%2520and%2520classification%250Abias.%2520To%2520address%2520this%252C%2520we%2520first%2520propose%2520a%2520sharpness-aware%2520and%2520reliable%2520entropy%250Aminimization%2520method%252C%2520called%2520SAR%252C%2520for%2520stabilizing%2520TTA%2520from%2520two%2520aspects%253A%25201%2529%250Aremove%2520partial%2520noisy%2520samples%2520with%2520large%2520gradients%252C%25202%2529%2520encourage%2520model%2520weights%250Ato%2520go%2520to%2520a%2520flat%2520minimum%2520so%2520that%2520the%2520model%2520is%2520robust%2520to%2520the%2520remaining%2520noisy%250Asamples.%2520Based%2520on%2520SAR%252C%2520we%2520further%2520introduce%2520SAR%255E2%2520to%2520prevent%2520representation%250Acollapse%2520with%2520two%2520regularizers%253A%25201%2529%2520a%2520redundancy%2520regularizer%2520to%2520reduce%250Ainter-dimensional%2520correlations%2520among%2520centroid-invariant%2520features%253B%2520and%25202%2529%2520an%250Ainequity%2520regularizer%2520to%2520maximize%2520the%2520prediction%2520entropy%2520of%2520a%2520prototype%250Acentroid%252C%2520thereby%2520penalizing%2520biased%2520representations%2520toward%2520any%2520specific%2520class.%250APromising%2520results%2520demonstrate%2520that%2520our%2520methods%2520perform%2520more%2520stably%2520over%2520prior%250Amethods%2520and%2520are%2520computationally%2520efficient%2520under%2520the%2520above%2520wild%2520test%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt%20in%20the%20Wild%3A%20Test-Time%20Entropy%20Minimization%20with%20Sharpness%20and%0A%20%20Feature%20Regularization&entry.906535625=Shuaicheng%20Niu%20and%20Guohao%20Chen%20and%20Deyu%20Chen%20and%20Yifan%20Zhang%20and%20Jiaxiang%20Wu%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Peilin%20Zhao%20and%20Chunyan%20Miao%20and%20Mingkui%20Tan&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20may%20fail%20to%20improve%20or%20even%20harm%20the%20model%0Aperformance%20when%20test%20data%20have%3A%201%29%20mixed%20distribution%20shifts%2C%202%29%20small%20batch%0Asizes%2C%203%29%20online%20imbalanced%20label%20distribution%20shifts.%20This%20is%20often%20a%20key%0Aobstacle%20preventing%20existing%20TTA%20methods%20from%20being%20deployed%20in%20the%20real%20world.%0AIn%20this%20paper%2C%20we%20investigate%20the%20unstable%20reasons%20and%20find%20that%20the%20batch%20norm%0Alayer%20is%20a%20crucial%20factor%20hindering%20TTA%20stability.%20Conversely%2C%20TTA%20can%20perform%0Amore%20stably%20with%20batch-agnostic%20norm%20layers%2C%20i.e.%2C%20group%20or%20layer%20norm.%0AHowever%2C%20we%20observe%20that%20TTA%20with%20group%20and%20layer%20norms%20does%20not%20always%20succeed%0Aand%20still%20suffers%20many%20failure%20cases%2C%20i.e.%2C%20the%20model%20collapses%20into%20trivial%0Asolutions%20by%20assigning%20the%20same%20class%20label%20for%20all%20samples.%20By%20digging%20into%0Athis%2C%20we%20find%20that%2C%20during%20the%20collapse%20process%3A%201%29%20the%20model%20gradients%20often%0Aundergo%20an%20initial%20explosion%20followed%20by%20rapid%20degradation%2C%20suggesting%20that%0Acertain%20noisy%20test%20samples%20with%20large%20gradients%20may%20disrupt%20adaptation%3B%20and%202%29%0Athe%20model%20representations%20tend%20to%20exhibit%20high%20correlations%20and%20classification%0Abias.%20To%20address%20this%2C%20we%20first%20propose%20a%20sharpness-aware%20and%20reliable%20entropy%0Aminimization%20method%2C%20called%20SAR%2C%20for%20stabilizing%20TTA%20from%20two%20aspects%3A%201%29%0Aremove%20partial%20noisy%20samples%20with%20large%20gradients%2C%202%29%20encourage%20model%20weights%0Ato%20go%20to%20a%20flat%20minimum%20so%20that%20the%20model%20is%20robust%20to%20the%20remaining%20noisy%0Asamples.%20Based%20on%20SAR%2C%20we%20further%20introduce%20SAR%5E2%20to%20prevent%20representation%0Acollapse%20with%20two%20regularizers%3A%201%29%20a%20redundancy%20regularizer%20to%20reduce%0Ainter-dimensional%20correlations%20among%20centroid-invariant%20features%3B%20and%202%29%20an%0Ainequity%20regularizer%20to%20maximize%20the%20prediction%20entropy%20of%20a%20prototype%0Acentroid%2C%20thereby%20penalizing%20biased%20representations%20toward%20any%20specific%20class.%0APromising%20results%20demonstrate%20that%20our%20methods%20perform%20more%20stably%20over%20prior%0Amethods%20and%20are%20computationally%20efficient%20under%20the%20above%20wild%20test%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04977v1&entry.124074799=Read"},
{"title": "Lightweight DNN for Full-Band Speech Denoising on Mobile Devices:\n  Exploiting Long and Short Temporal Patterns", "author": "Konstantinos Drossos and Mikko Heikkinen and Paschalis Tsiaflakis", "abstract": "  Speech denoising (SD) is an important task of many, if not all, modern signal\nprocessing chains used in devices and for everyday-life applications. While\nthere are many published and powerful deep neural network (DNN)-based methods\nfor SD, few are optimized for resource-constrained platforms such as mobile\ndevices. Additionally, most DNN-based methods for SD are not focusing on\nfull-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency\ncases. In this paper we present a causal, low latency, and lightweight\nDNN-based method for full-band SD, leveraging both short and long temporal\npatterns. The method is based on a modified UNet architecture employing\nlook-back frames, temporal spanning of convolutional kernels, and recurrent\nneural networks for exploiting short and long temporal patterns in the signal\nand estimated denoising mask. The DNN operates on a causal frame-by-frame basis\ntaking as an input the STFT magnitude, utilizes inverted bottlenecks inspired\nby MobileNet, employs causal instance normalization for channel-wise\nnormalization, and achieves a real-time factor below 0.02 when deployed on a\nmodern mobile phone. The proposed method is evaluated using established speech\ndenoising metrics and publicly available datasets, demonstrating its\neffectiveness in achieving an (SI-)SDR value that outperforms existing FB and\nlow latency SD methods.\n", "link": "http://arxiv.org/abs/2509.05079v1", "date": "2025-09-05", "relevancy": 2.0277, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5436}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20DNN%20for%20Full-Band%20Speech%20Denoising%20on%20Mobile%20Devices%3A%0A%20%20Exploiting%20Long%20and%20Short%20Temporal%20Patterns&body=Title%3A%20Lightweight%20DNN%20for%20Full-Band%20Speech%20Denoising%20on%20Mobile%20Devices%3A%0A%20%20Exploiting%20Long%20and%20Short%20Temporal%20Patterns%0AAuthor%3A%20Konstantinos%20Drossos%20and%20Mikko%20Heikkinen%20and%20Paschalis%20Tsiaflakis%0AAbstract%3A%20%20%20Speech%20denoising%20%28SD%29%20is%20an%20important%20task%20of%20many%2C%20if%20not%20all%2C%20modern%20signal%0Aprocessing%20chains%20used%20in%20devices%20and%20for%20everyday-life%20applications.%20While%0Athere%20are%20many%20published%20and%20powerful%20deep%20neural%20network%20%28DNN%29-based%20methods%0Afor%20SD%2C%20few%20are%20optimized%20for%20resource-constrained%20platforms%20such%20as%20mobile%0Adevices.%20Additionally%2C%20most%20DNN-based%20methods%20for%20SD%20are%20not%20focusing%20on%0Afull-band%20%28FB%29%20signals%2C%20i.e.%20having%2048%20kHz%20sampling%20rate%2C%20and/or%20low%20latency%0Acases.%20In%20this%20paper%20we%20present%20a%20causal%2C%20low%20latency%2C%20and%20lightweight%0ADNN-based%20method%20for%20full-band%20SD%2C%20leveraging%20both%20short%20and%20long%20temporal%0Apatterns.%20The%20method%20is%20based%20on%20a%20modified%20UNet%20architecture%20employing%0Alook-back%20frames%2C%20temporal%20spanning%20of%20convolutional%20kernels%2C%20and%20recurrent%0Aneural%20networks%20for%20exploiting%20short%20and%20long%20temporal%20patterns%20in%20the%20signal%0Aand%20estimated%20denoising%20mask.%20The%20DNN%20operates%20on%20a%20causal%20frame-by-frame%20basis%0Ataking%20as%20an%20input%20the%20STFT%20magnitude%2C%20utilizes%20inverted%20bottlenecks%20inspired%0Aby%20MobileNet%2C%20employs%20causal%20instance%20normalization%20for%20channel-wise%0Anormalization%2C%20and%20achieves%20a%20real-time%20factor%20below%200.02%20when%20deployed%20on%20a%0Amodern%20mobile%20phone.%20The%20proposed%20method%20is%20evaluated%20using%20established%20speech%0Adenoising%20metrics%20and%20publicly%20available%20datasets%2C%20demonstrating%20its%0Aeffectiveness%20in%20achieving%20an%20%28SI-%29SDR%20value%20that%20outperforms%20existing%20FB%20and%0Alow%20latency%20SD%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520DNN%2520for%2520Full-Band%2520Speech%2520Denoising%2520on%2520Mobile%2520Devices%253A%250A%2520%2520Exploiting%2520Long%2520and%2520Short%2520Temporal%2520Patterns%26entry.906535625%3DKonstantinos%2520Drossos%2520and%2520Mikko%2520Heikkinen%2520and%2520Paschalis%2520Tsiaflakis%26entry.1292438233%3D%2520%2520Speech%2520denoising%2520%2528SD%2529%2520is%2520an%2520important%2520task%2520of%2520many%252C%2520if%2520not%2520all%252C%2520modern%2520signal%250Aprocessing%2520chains%2520used%2520in%2520devices%2520and%2520for%2520everyday-life%2520applications.%2520While%250Athere%2520are%2520many%2520published%2520and%2520powerful%2520deep%2520neural%2520network%2520%2528DNN%2529-based%2520methods%250Afor%2520SD%252C%2520few%2520are%2520optimized%2520for%2520resource-constrained%2520platforms%2520such%2520as%2520mobile%250Adevices.%2520Additionally%252C%2520most%2520DNN-based%2520methods%2520for%2520SD%2520are%2520not%2520focusing%2520on%250Afull-band%2520%2528FB%2529%2520signals%252C%2520i.e.%2520having%252048%2520kHz%2520sampling%2520rate%252C%2520and/or%2520low%2520latency%250Acases.%2520In%2520this%2520paper%2520we%2520present%2520a%2520causal%252C%2520low%2520latency%252C%2520and%2520lightweight%250ADNN-based%2520method%2520for%2520full-band%2520SD%252C%2520leveraging%2520both%2520short%2520and%2520long%2520temporal%250Apatterns.%2520The%2520method%2520is%2520based%2520on%2520a%2520modified%2520UNet%2520architecture%2520employing%250Alook-back%2520frames%252C%2520temporal%2520spanning%2520of%2520convolutional%2520kernels%252C%2520and%2520recurrent%250Aneural%2520networks%2520for%2520exploiting%2520short%2520and%2520long%2520temporal%2520patterns%2520in%2520the%2520signal%250Aand%2520estimated%2520denoising%2520mask.%2520The%2520DNN%2520operates%2520on%2520a%2520causal%2520frame-by-frame%2520basis%250Ataking%2520as%2520an%2520input%2520the%2520STFT%2520magnitude%252C%2520utilizes%2520inverted%2520bottlenecks%2520inspired%250Aby%2520MobileNet%252C%2520employs%2520causal%2520instance%2520normalization%2520for%2520channel-wise%250Anormalization%252C%2520and%2520achieves%2520a%2520real-time%2520factor%2520below%25200.02%2520when%2520deployed%2520on%2520a%250Amodern%2520mobile%2520phone.%2520The%2520proposed%2520method%2520is%2520evaluated%2520using%2520established%2520speech%250Adenoising%2520metrics%2520and%2520publicly%2520available%2520datasets%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520achieving%2520an%2520%2528SI-%2529SDR%2520value%2520that%2520outperforms%2520existing%2520FB%2520and%250Alow%2520latency%2520SD%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20DNN%20for%20Full-Band%20Speech%20Denoising%20on%20Mobile%20Devices%3A%0A%20%20Exploiting%20Long%20and%20Short%20Temporal%20Patterns&entry.906535625=Konstantinos%20Drossos%20and%20Mikko%20Heikkinen%20and%20Paschalis%20Tsiaflakis&entry.1292438233=%20%20Speech%20denoising%20%28SD%29%20is%20an%20important%20task%20of%20many%2C%20if%20not%20all%2C%20modern%20signal%0Aprocessing%20chains%20used%20in%20devices%20and%20for%20everyday-life%20applications.%20While%0Athere%20are%20many%20published%20and%20powerful%20deep%20neural%20network%20%28DNN%29-based%20methods%0Afor%20SD%2C%20few%20are%20optimized%20for%20resource-constrained%20platforms%20such%20as%20mobile%0Adevices.%20Additionally%2C%20most%20DNN-based%20methods%20for%20SD%20are%20not%20focusing%20on%0Afull-band%20%28FB%29%20signals%2C%20i.e.%20having%2048%20kHz%20sampling%20rate%2C%20and/or%20low%20latency%0Acases.%20In%20this%20paper%20we%20present%20a%20causal%2C%20low%20latency%2C%20and%20lightweight%0ADNN-based%20method%20for%20full-band%20SD%2C%20leveraging%20both%20short%20and%20long%20temporal%0Apatterns.%20The%20method%20is%20based%20on%20a%20modified%20UNet%20architecture%20employing%0Alook-back%20frames%2C%20temporal%20spanning%20of%20convolutional%20kernels%2C%20and%20recurrent%0Aneural%20networks%20for%20exploiting%20short%20and%20long%20temporal%20patterns%20in%20the%20signal%0Aand%20estimated%20denoising%20mask.%20The%20DNN%20operates%20on%20a%20causal%20frame-by-frame%20basis%0Ataking%20as%20an%20input%20the%20STFT%20magnitude%2C%20utilizes%20inverted%20bottlenecks%20inspired%0Aby%20MobileNet%2C%20employs%20causal%20instance%20normalization%20for%20channel-wise%0Anormalization%2C%20and%20achieves%20a%20real-time%20factor%20below%200.02%20when%20deployed%20on%20a%0Amodern%20mobile%20phone.%20The%20proposed%20method%20is%20evaluated%20using%20established%20speech%0Adenoising%20metrics%20and%20publicly%20available%20datasets%2C%20demonstrating%20its%0Aeffectiveness%20in%20achieving%20an%20%28SI-%29SDR%20value%20that%20outperforms%20existing%20FB%20and%0Alow%20latency%20SD%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05079v1&entry.124074799=Read"},
{"title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and\n  Localization", "author": "Jingqi Wu and Hanxi Li and Lin Yuanbo Wu and Hao Chen and Deyin Liu and Peng Wang", "abstract": "  Industrial product inspection is often performed using Anomaly Detection (AD)\nframeworks trained solely on non-defective samples. Although defective samples\ncan be collected during production, leveraging them usually requires\npixel-level annotations, limiting scalability. To address this, we propose\nADClick, an Interactive Image Segmentation (IIS) algorithm for industrial\nanomaly detection. ADClick generates pixel-wise anomaly annotations from only a\nfew user clicks and a brief textual description, enabling precise and efficient\nlabeling that significantly improves AD model performance (e.g., AP = 96.1\\% on\nMVTec AD). We further introduce ADClick-Seg, a cross-modal framework that\naligns visual features and textual prompts via a prototype-based approach for\nanomaly detection and localization. By combining pixel-level priors with\nlanguage-guided cues, ADClick-Seg achieves state-of-the-art results on the\nchallenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC =\n99.1\\% on MVTec AD).\n", "link": "http://arxiv.org/abs/2509.05034v1", "date": "2025-09-05", "relevancy": 2.0193, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5333}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5021}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization&body=Title%3A%20Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization%0AAuthor%3A%20Jingqi%20Wu%20and%20Hanxi%20Li%20and%20Lin%20Yuanbo%20Wu%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Industrial%20product%20inspection%20is%20often%20performed%20using%20Anomaly%20Detection%20%28AD%29%0Aframeworks%20trained%20solely%20on%20non-defective%20samples.%20Although%20defective%20samples%0Acan%20be%20collected%20during%20production%2C%20leveraging%20them%20usually%20requires%0Apixel-level%20annotations%2C%20limiting%20scalability.%20To%20address%20this%2C%20we%20propose%0AADClick%2C%20an%20Interactive%20Image%20Segmentation%20%28IIS%29%20algorithm%20for%20industrial%0Aanomaly%20detection.%20ADClick%20generates%20pixel-wise%20anomaly%20annotations%20from%20only%20a%0Afew%20user%20clicks%20and%20a%20brief%20textual%20description%2C%20enabling%20precise%20and%20efficient%0Alabeling%20that%20significantly%20improves%20AD%20model%20performance%20%28e.g.%2C%20AP%20%3D%2096.1%5C%25%20on%0AMVTec%20AD%29.%20We%20further%20introduce%20ADClick-Seg%2C%20a%20cross-modal%20framework%20that%0Aaligns%20visual%20features%20and%20textual%20prompts%20via%20a%20prototype-based%20approach%20for%0Aanomaly%20detection%20and%20localization.%20By%20combining%20pixel-level%20priors%20with%0Alanguage-guided%20cues%2C%20ADClick-Seg%20achieves%20state-of-the-art%20results%20on%20the%0Achallenging%20%60%60Multi-class%27%27%20AD%20task%20%28AP%20%3D%2080.0%5C%25%2C%20PRO%20%3D%2097.5%5C%25%2C%20Pixel-AUROC%20%3D%0A99.1%5C%25%20on%20MVTec%20AD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Pixel%2520Labeling%2520for%2520Industrial%2520Anomaly%2520Detection%2520and%250A%2520%2520Localization%26entry.906535625%3DJingqi%2520Wu%2520and%2520Hanxi%2520Li%2520and%2520Lin%2520Yuanbo%2520Wu%2520and%2520Hao%2520Chen%2520and%2520Deyin%2520Liu%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%2520Industrial%2520product%2520inspection%2520is%2520often%2520performed%2520using%2520Anomaly%2520Detection%2520%2528AD%2529%250Aframeworks%2520trained%2520solely%2520on%2520non-defective%2520samples.%2520Although%2520defective%2520samples%250Acan%2520be%2520collected%2520during%2520production%252C%2520leveraging%2520them%2520usually%2520requires%250Apixel-level%2520annotations%252C%2520limiting%2520scalability.%2520To%2520address%2520this%252C%2520we%2520propose%250AADClick%252C%2520an%2520Interactive%2520Image%2520Segmentation%2520%2528IIS%2529%2520algorithm%2520for%2520industrial%250Aanomaly%2520detection.%2520ADClick%2520generates%2520pixel-wise%2520anomaly%2520annotations%2520from%2520only%2520a%250Afew%2520user%2520clicks%2520and%2520a%2520brief%2520textual%2520description%252C%2520enabling%2520precise%2520and%2520efficient%250Alabeling%2520that%2520significantly%2520improves%2520AD%2520model%2520performance%2520%2528e.g.%252C%2520AP%2520%253D%252096.1%255C%2525%2520on%250AMVTec%2520AD%2529.%2520We%2520further%2520introduce%2520ADClick-Seg%252C%2520a%2520cross-modal%2520framework%2520that%250Aaligns%2520visual%2520features%2520and%2520textual%2520prompts%2520via%2520a%2520prototype-based%2520approach%2520for%250Aanomaly%2520detection%2520and%2520localization.%2520By%2520combining%2520pixel-level%2520priors%2520with%250Alanguage-guided%2520cues%252C%2520ADClick-Seg%2520achieves%2520state-of-the-art%2520results%2520on%2520the%250Achallenging%2520%2560%2560Multi-class%2527%2527%2520AD%2520task%2520%2528AP%2520%253D%252080.0%255C%2525%252C%2520PRO%2520%253D%252097.5%255C%2525%252C%2520Pixel-AUROC%2520%253D%250A99.1%255C%2525%2520on%2520MVTec%2520AD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization&entry.906535625=Jingqi%20Wu%20and%20Hanxi%20Li%20and%20Lin%20Yuanbo%20Wu%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Peng%20Wang&entry.1292438233=%20%20Industrial%20product%20inspection%20is%20often%20performed%20using%20Anomaly%20Detection%20%28AD%29%0Aframeworks%20trained%20solely%20on%20non-defective%20samples.%20Although%20defective%20samples%0Acan%20be%20collected%20during%20production%2C%20leveraging%20them%20usually%20requires%0Apixel-level%20annotations%2C%20limiting%20scalability.%20To%20address%20this%2C%20we%20propose%0AADClick%2C%20an%20Interactive%20Image%20Segmentation%20%28IIS%29%20algorithm%20for%20industrial%0Aanomaly%20detection.%20ADClick%20generates%20pixel-wise%20anomaly%20annotations%20from%20only%20a%0Afew%20user%20clicks%20and%20a%20brief%20textual%20description%2C%20enabling%20precise%20and%20efficient%0Alabeling%20that%20significantly%20improves%20AD%20model%20performance%20%28e.g.%2C%20AP%20%3D%2096.1%5C%25%20on%0AMVTec%20AD%29.%20We%20further%20introduce%20ADClick-Seg%2C%20a%20cross-modal%20framework%20that%0Aaligns%20visual%20features%20and%20textual%20prompts%20via%20a%20prototype-based%20approach%20for%0Aanomaly%20detection%20and%20localization.%20By%20combining%20pixel-level%20priors%20with%0Alanguage-guided%20cues%2C%20ADClick-Seg%20achieves%20state-of-the-art%20results%20on%20the%0Achallenging%20%60%60Multi-class%27%27%20AD%20task%20%28AP%20%3D%2080.0%5C%25%2C%20PRO%20%3D%2097.5%5C%25%2C%20Pixel-AUROC%20%3D%0A99.1%5C%25%20on%20MVTec%20AD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05034v1&entry.124074799=Read"},
{"title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution\n  Alignment Using GFlowNets", "author": "Chenlin Liu and Minghui Fang and Patrick Zhang and Wei Zhou and Jie Gao and Jiqing Han", "abstract": "  Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness.\n", "link": "http://arxiv.org/abs/2508.15442v3", "date": "2025-09-05", "relevancy": 2.0192, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5636}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets&body=Title%3A%20Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets%0AAuthor%3A%20Chenlin%20Liu%20and%20Minghui%20Fang%20and%20Patrick%20Zhang%20and%20Wei%20Zhou%20and%20Jie%20Gao%20and%20Jiqing%20Han%0AAbstract%3A%20%20%20Language%20Model%20%28LM%29-based%20Text-to-Speech%20%28TTS%29%20systems%20often%20generate%0Ahallucinated%20speech%20that%20deviates%20from%20input%20text.%20Existing%20mitigation%0Astrategies%20either%20demand%20excessive%20training%20resources%20or%20introduce%20significant%0Ainference%20latency.%20In%20this%20paper%2C%20we%20propose%20GFlOwNet-guided%20distribution%0AAlignmenT%20%28GOAT%29%20for%20LM-based%20TTS%2C%20a%20post-training%20framework%20that%20mitigates%0Ahallucinations%20without%20relying%20on%20massive%20resources%20or%20inference%20cost.%0ASpecifically%2C%20we%20first%20conduct%20an%20uncertainty%20analysis%2C%20revealing%20a%20strong%0Apositive%20correlation%20between%20hallucination%20and%20model%20uncertainty.%20Based%20on%0Athis%2C%20we%20reformulate%20TTS%20generation%20as%20a%20trajectory%20flow%20optimization%20problem%0Aand%20introduce%20an%20enhanced%20Subtrajectory%20Balance%20objective%20together%20with%20a%0Asharpened%20internal%20reward%20as%20target%20distribution.%20We%20further%20integrate%20reward%0Atemperature%20decay%20and%20learning%20rate%20optimization%20for%20stability%20and%20performance%0Abalance.%20Extensive%20experiments%20show%20that%20GOAT%20reduce%20over%2050%25%20character%20error%0Arates%20on%20challenging%20test%20cases%20and%20lowering%20uncertainty%20by%20up%20to%2058%25%2C%0Ademonstrating%20its%20strong%20generalization%20ability%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15442v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucinations%2520in%2520LM-Based%2520TTS%2520Models%2520via%2520Distribution%250A%2520%2520Alignment%2520Using%2520GFlowNets%26entry.906535625%3DChenlin%2520Liu%2520and%2520Minghui%2520Fang%2520and%2520Patrick%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Jie%2520Gao%2520and%2520Jiqing%2520Han%26entry.1292438233%3D%2520%2520Language%2520Model%2520%2528LM%2529-based%2520Text-to-Speech%2520%2528TTS%2529%2520systems%2520often%2520generate%250Ahallucinated%2520speech%2520that%2520deviates%2520from%2520input%2520text.%2520Existing%2520mitigation%250Astrategies%2520either%2520demand%2520excessive%2520training%2520resources%2520or%2520introduce%2520significant%250Ainference%2520latency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GFlOwNet-guided%2520distribution%250AAlignmenT%2520%2528GOAT%2529%2520for%2520LM-based%2520TTS%252C%2520a%2520post-training%2520framework%2520that%2520mitigates%250Ahallucinations%2520without%2520relying%2520on%2520massive%2520resources%2520or%2520inference%2520cost.%250ASpecifically%252C%2520we%2520first%2520conduct%2520an%2520uncertainty%2520analysis%252C%2520revealing%2520a%2520strong%250Apositive%2520correlation%2520between%2520hallucination%2520and%2520model%2520uncertainty.%2520Based%2520on%250Athis%252C%2520we%2520reformulate%2520TTS%2520generation%2520as%2520a%2520trajectory%2520flow%2520optimization%2520problem%250Aand%2520introduce%2520an%2520enhanced%2520Subtrajectory%2520Balance%2520objective%2520together%2520with%2520a%250Asharpened%2520internal%2520reward%2520as%2520target%2520distribution.%2520We%2520further%2520integrate%2520reward%250Atemperature%2520decay%2520and%2520learning%2520rate%2520optimization%2520for%2520stability%2520and%2520performance%250Abalance.%2520Extensive%2520experiments%2520show%2520that%2520GOAT%2520reduce%2520over%252050%2525%2520character%2520error%250Arates%2520on%2520challenging%2520test%2520cases%2520and%2520lowering%2520uncertainty%2520by%2520up%2520to%252058%2525%252C%250Ademonstrating%2520its%2520strong%2520generalization%2520ability%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15442v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets&entry.906535625=Chenlin%20Liu%20and%20Minghui%20Fang%20and%20Patrick%20Zhang%20and%20Wei%20Zhou%20and%20Jie%20Gao%20and%20Jiqing%20Han&entry.1292438233=%20%20Language%20Model%20%28LM%29-based%20Text-to-Speech%20%28TTS%29%20systems%20often%20generate%0Ahallucinated%20speech%20that%20deviates%20from%20input%20text.%20Existing%20mitigation%0Astrategies%20either%20demand%20excessive%20training%20resources%20or%20introduce%20significant%0Ainference%20latency.%20In%20this%20paper%2C%20we%20propose%20GFlOwNet-guided%20distribution%0AAlignmenT%20%28GOAT%29%20for%20LM-based%20TTS%2C%20a%20post-training%20framework%20that%20mitigates%0Ahallucinations%20without%20relying%20on%20massive%20resources%20or%20inference%20cost.%0ASpecifically%2C%20we%20first%20conduct%20an%20uncertainty%20analysis%2C%20revealing%20a%20strong%0Apositive%20correlation%20between%20hallucination%20and%20model%20uncertainty.%20Based%20on%0Athis%2C%20we%20reformulate%20TTS%20generation%20as%20a%20trajectory%20flow%20optimization%20problem%0Aand%20introduce%20an%20enhanced%20Subtrajectory%20Balance%20objective%20together%20with%20a%0Asharpened%20internal%20reward%20as%20target%20distribution.%20We%20further%20integrate%20reward%0Atemperature%20decay%20and%20learning%20rate%20optimization%20for%20stability%20and%20performance%0Abalance.%20Extensive%20experiments%20show%20that%20GOAT%20reduce%20over%2050%25%20character%20error%0Arates%20on%20challenging%20test%20cases%20and%20lowering%20uncertainty%20by%20up%20to%2058%25%2C%0Ademonstrating%20its%20strong%20generalization%20ability%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15442v3&entry.124074799=Read"},
{"title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning", "author": "William F. Shen and Xinchi Qiu and Nicola Cancedda and Nicholas D. Lane", "abstract": "  Existing work on mitigating catastrophic forgetting during large language\nmodels (LLMs) fine-tuning for new knowledge instances has primarily focused on\npreserving performance on previously seen data, while critically overlooking\nthe collapse of essential capabilities instilled through alignment, most\nnotably the model's ability to faithfully express epistemic uncertainty (a\nproperty we term 'Ignorance Awareness'). In this work, we formalize the notion\nof Ignorance Awareness and illustrate that conventional fine-tuning methods can\nresult in substantial activation displacement. This displacement undermines the\ncritical capability of ignorance awareness, leading to undesirable behaviors\nsuch as hallucinations. To address this challenge, we introduce SEAT, a simple\nand principled fine-tuning approach that not only enables the model to\neffectively acquire new knowledge instances but also preserves its aligned\nignorance awareness. SEAT integrates two key components: (1) sparse tuning that\nconstrains activation drift, and (2) a novel entity perturbation method\ndesigned to counter knowledge entanglement. Experimental results demonstrate\nthat, across both real-world and synthetic datasets, SEAT significantly\noutperforms baselines in preserving ignorance awareness while retaining optimal\nfine-tuning performance, offering a more robust solution for LLM fine-tuning.\n", "link": "http://arxiv.org/abs/2506.14387v2", "date": "2025-09-05", "relevancy": 2.0058, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5269}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Make%20It%20Up%3A%20Preserving%20Ignorance%20Awareness%20in%20LLM%20Fine-Tuning&body=Title%3A%20Don%27t%20Make%20It%20Up%3A%20Preserving%20Ignorance%20Awareness%20in%20LLM%20Fine-Tuning%0AAuthor%3A%20William%20F.%20Shen%20and%20Xinchi%20Qiu%20and%20Nicola%20Cancedda%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Existing%20work%20on%20mitigating%20catastrophic%20forgetting%20during%20large%20language%0Amodels%20%28LLMs%29%20fine-tuning%20for%20new%20knowledge%20instances%20has%20primarily%20focused%20on%0Apreserving%20performance%20on%20previously%20seen%20data%2C%20while%20critically%20overlooking%0Athe%20collapse%20of%20essential%20capabilities%20instilled%20through%20alignment%2C%20most%0Anotably%20the%20model%27s%20ability%20to%20faithfully%20express%20epistemic%20uncertainty%20%28a%0Aproperty%20we%20term%20%27Ignorance%20Awareness%27%29.%20In%20this%20work%2C%20we%20formalize%20the%20notion%0Aof%20Ignorance%20Awareness%20and%20illustrate%20that%20conventional%20fine-tuning%20methods%20can%0Aresult%20in%20substantial%20activation%20displacement.%20This%20displacement%20undermines%20the%0Acritical%20capability%20of%20ignorance%20awareness%2C%20leading%20to%20undesirable%20behaviors%0Asuch%20as%20hallucinations.%20To%20address%20this%20challenge%2C%20we%20introduce%20SEAT%2C%20a%20simple%0Aand%20principled%20fine-tuning%20approach%20that%20not%20only%20enables%20the%20model%20to%0Aeffectively%20acquire%20new%20knowledge%20instances%20but%20also%20preserves%20its%20aligned%0Aignorance%20awareness.%20SEAT%20integrates%20two%20key%20components%3A%20%281%29%20sparse%20tuning%20that%0Aconstrains%20activation%20drift%2C%20and%20%282%29%20a%20novel%20entity%20perturbation%20method%0Adesigned%20to%20counter%20knowledge%20entanglement.%20Experimental%20results%20demonstrate%0Athat%2C%20across%20both%20real-world%20and%20synthetic%20datasets%2C%20SEAT%20significantly%0Aoutperforms%20baselines%20in%20preserving%20ignorance%20awareness%20while%20retaining%20optimal%0Afine-tuning%20performance%2C%20offering%20a%20more%20robust%20solution%20for%20LLM%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Make%2520It%2520Up%253A%2520Preserving%2520Ignorance%2520Awareness%2520in%2520LLM%2520Fine-Tuning%26entry.906535625%3DWilliam%2520F.%2520Shen%2520and%2520Xinchi%2520Qiu%2520and%2520Nicola%2520Cancedda%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Existing%2520work%2520on%2520mitigating%2520catastrophic%2520forgetting%2520during%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520fine-tuning%2520for%2520new%2520knowledge%2520instances%2520has%2520primarily%2520focused%2520on%250Apreserving%2520performance%2520on%2520previously%2520seen%2520data%252C%2520while%2520critically%2520overlooking%250Athe%2520collapse%2520of%2520essential%2520capabilities%2520instilled%2520through%2520alignment%252C%2520most%250Anotably%2520the%2520model%2527s%2520ability%2520to%2520faithfully%2520express%2520epistemic%2520uncertainty%2520%2528a%250Aproperty%2520we%2520term%2520%2527Ignorance%2520Awareness%2527%2529.%2520In%2520this%2520work%252C%2520we%2520formalize%2520the%2520notion%250Aof%2520Ignorance%2520Awareness%2520and%2520illustrate%2520that%2520conventional%2520fine-tuning%2520methods%2520can%250Aresult%2520in%2520substantial%2520activation%2520displacement.%2520This%2520displacement%2520undermines%2520the%250Acritical%2520capability%2520of%2520ignorance%2520awareness%252C%2520leading%2520to%2520undesirable%2520behaviors%250Asuch%2520as%2520hallucinations.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SEAT%252C%2520a%2520simple%250Aand%2520principled%2520fine-tuning%2520approach%2520that%2520not%2520only%2520enables%2520the%2520model%2520to%250Aeffectively%2520acquire%2520new%2520knowledge%2520instances%2520but%2520also%2520preserves%2520its%2520aligned%250Aignorance%2520awareness.%2520SEAT%2520integrates%2520two%2520key%2520components%253A%2520%25281%2529%2520sparse%2520tuning%2520that%250Aconstrains%2520activation%2520drift%252C%2520and%2520%25282%2529%2520a%2520novel%2520entity%2520perturbation%2520method%250Adesigned%2520to%2520counter%2520knowledge%2520entanglement.%2520Experimental%2520results%2520demonstrate%250Athat%252C%2520across%2520both%2520real-world%2520and%2520synthetic%2520datasets%252C%2520SEAT%2520significantly%250Aoutperforms%2520baselines%2520in%2520preserving%2520ignorance%2520awareness%2520while%2520retaining%2520optimal%250Afine-tuning%2520performance%252C%2520offering%2520a%2520more%2520robust%2520solution%2520for%2520LLM%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Make%20It%20Up%3A%20Preserving%20Ignorance%20Awareness%20in%20LLM%20Fine-Tuning&entry.906535625=William%20F.%20Shen%20and%20Xinchi%20Qiu%20and%20Nicola%20Cancedda%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Existing%20work%20on%20mitigating%20catastrophic%20forgetting%20during%20large%20language%0Amodels%20%28LLMs%29%20fine-tuning%20for%20new%20knowledge%20instances%20has%20primarily%20focused%20on%0Apreserving%20performance%20on%20previously%20seen%20data%2C%20while%20critically%20overlooking%0Athe%20collapse%20of%20essential%20capabilities%20instilled%20through%20alignment%2C%20most%0Anotably%20the%20model%27s%20ability%20to%20faithfully%20express%20epistemic%20uncertainty%20%28a%0Aproperty%20we%20term%20%27Ignorance%20Awareness%27%29.%20In%20this%20work%2C%20we%20formalize%20the%20notion%0Aof%20Ignorance%20Awareness%20and%20illustrate%20that%20conventional%20fine-tuning%20methods%20can%0Aresult%20in%20substantial%20activation%20displacement.%20This%20displacement%20undermines%20the%0Acritical%20capability%20of%20ignorance%20awareness%2C%20leading%20to%20undesirable%20behaviors%0Asuch%20as%20hallucinations.%20To%20address%20this%20challenge%2C%20we%20introduce%20SEAT%2C%20a%20simple%0Aand%20principled%20fine-tuning%20approach%20that%20not%20only%20enables%20the%20model%20to%0Aeffectively%20acquire%20new%20knowledge%20instances%20but%20also%20preserves%20its%20aligned%0Aignorance%20awareness.%20SEAT%20integrates%20two%20key%20components%3A%20%281%29%20sparse%20tuning%20that%0Aconstrains%20activation%20drift%2C%20and%20%282%29%20a%20novel%20entity%20perturbation%20method%0Adesigned%20to%20counter%20knowledge%20entanglement.%20Experimental%20results%20demonstrate%0Athat%2C%20across%20both%20real-world%20and%20synthetic%20datasets%2C%20SEAT%20significantly%0Aoutperforms%20baselines%20in%20preserving%20ignorance%20awareness%20while%20retaining%20optimal%0Afine-tuning%20performance%2C%20offering%20a%20more%20robust%20solution%20for%20LLM%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14387v2&entry.124074799=Read"},
{"title": "Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian", "author": "Koji Matsuno and Chien Chern Cheah", "abstract": "  Deep learning, with its exceptional learning capabilities and flexibility,\nhas been widely applied in various applications. However, its black-box nature\nposes a significant challenge in real-time robotic applications, particularly\nin robot control, where trustworthiness and robustness are critical in ensuring\nsafety. In robot motion control, it is essential to analyze and ensure system\nstability, necessitating the establishment of methodologies that address this\nneed. This paper aims to develop a theoretical framework for end-to-end deep\nlearning control that can be integrated into existing robot control theories.\nThe proposed control algorithm leverages a modular learning approach to update\nthe weights of all layers in real time, ensuring system stability based on\nLyapunov-like analysis. Experimental results on industrial robots are presented\nto illustrate the performance of the proposed deep learning controller. The\nproposed method offers an effective solution to the black-box problem in deep\nlearning, demonstrating the possibility of deploying real-time deep learning\nstrategies for robot kinematic control in a stable manner. This achievement\nprovides a critical foundation for future advancements in deep learning based\nreal-time robotic applications.\n", "link": "http://arxiv.org/abs/2509.04984v1", "date": "2025-09-05", "relevancy": 1.9974, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4982}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyapunov-Based%20Deep%20Learning%20Control%20for%20Robots%20with%20Unknown%20Jacobian&body=Title%3A%20Lyapunov-Based%20Deep%20Learning%20Control%20for%20Robots%20with%20Unknown%20Jacobian%0AAuthor%3A%20Koji%20Matsuno%20and%20Chien%20Chern%20Cheah%0AAbstract%3A%20%20%20Deep%20learning%2C%20with%20its%20exceptional%20learning%20capabilities%20and%20flexibility%2C%0Ahas%20been%20widely%20applied%20in%20various%20applications.%20However%2C%20its%20black-box%20nature%0Aposes%20a%20significant%20challenge%20in%20real-time%20robotic%20applications%2C%20particularly%0Ain%20robot%20control%2C%20where%20trustworthiness%20and%20robustness%20are%20critical%20in%20ensuring%0Asafety.%20In%20robot%20motion%20control%2C%20it%20is%20essential%20to%20analyze%20and%20ensure%20system%0Astability%2C%20necessitating%20the%20establishment%20of%20methodologies%20that%20address%20this%0Aneed.%20This%20paper%20aims%20to%20develop%20a%20theoretical%20framework%20for%20end-to-end%20deep%0Alearning%20control%20that%20can%20be%20integrated%20into%20existing%20robot%20control%20theories.%0AThe%20proposed%20control%20algorithm%20leverages%20a%20modular%20learning%20approach%20to%20update%0Athe%20weights%20of%20all%20layers%20in%20real%20time%2C%20ensuring%20system%20stability%20based%20on%0ALyapunov-like%20analysis.%20Experimental%20results%20on%20industrial%20robots%20are%20presented%0Ato%20illustrate%20the%20performance%20of%20the%20proposed%20deep%20learning%20controller.%20The%0Aproposed%20method%20offers%20an%20effective%20solution%20to%20the%20black-box%20problem%20in%20deep%0Alearning%2C%20demonstrating%20the%20possibility%20of%20deploying%20real-time%20deep%20learning%0Astrategies%20for%20robot%20kinematic%20control%20in%20a%20stable%20manner.%20This%20achievement%0Aprovides%20a%20critical%20foundation%20for%20future%20advancements%20in%20deep%20learning%20based%0Areal-time%20robotic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyapunov-Based%2520Deep%2520Learning%2520Control%2520for%2520Robots%2520with%2520Unknown%2520Jacobian%26entry.906535625%3DKoji%2520Matsuno%2520and%2520Chien%2520Chern%2520Cheah%26entry.1292438233%3D%2520%2520Deep%2520learning%252C%2520with%2520its%2520exceptional%2520learning%2520capabilities%2520and%2520flexibility%252C%250Ahas%2520been%2520widely%2520applied%2520in%2520various%2520applications.%2520However%252C%2520its%2520black-box%2520nature%250Aposes%2520a%2520significant%2520challenge%2520in%2520real-time%2520robotic%2520applications%252C%2520particularly%250Ain%2520robot%2520control%252C%2520where%2520trustworthiness%2520and%2520robustness%2520are%2520critical%2520in%2520ensuring%250Asafety.%2520In%2520robot%2520motion%2520control%252C%2520it%2520is%2520essential%2520to%2520analyze%2520and%2520ensure%2520system%250Astability%252C%2520necessitating%2520the%2520establishment%2520of%2520methodologies%2520that%2520address%2520this%250Aneed.%2520This%2520paper%2520aims%2520to%2520develop%2520a%2520theoretical%2520framework%2520for%2520end-to-end%2520deep%250Alearning%2520control%2520that%2520can%2520be%2520integrated%2520into%2520existing%2520robot%2520control%2520theories.%250AThe%2520proposed%2520control%2520algorithm%2520leverages%2520a%2520modular%2520learning%2520approach%2520to%2520update%250Athe%2520weights%2520of%2520all%2520layers%2520in%2520real%2520time%252C%2520ensuring%2520system%2520stability%2520based%2520on%250ALyapunov-like%2520analysis.%2520Experimental%2520results%2520on%2520industrial%2520robots%2520are%2520presented%250Ato%2520illustrate%2520the%2520performance%2520of%2520the%2520proposed%2520deep%2520learning%2520controller.%2520The%250Aproposed%2520method%2520offers%2520an%2520effective%2520solution%2520to%2520the%2520black-box%2520problem%2520in%2520deep%250Alearning%252C%2520demonstrating%2520the%2520possibility%2520of%2520deploying%2520real-time%2520deep%2520learning%250Astrategies%2520for%2520robot%2520kinematic%2520control%2520in%2520a%2520stable%2520manner.%2520This%2520achievement%250Aprovides%2520a%2520critical%2520foundation%2520for%2520future%2520advancements%2520in%2520deep%2520learning%2520based%250Areal-time%2520robotic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyapunov-Based%20Deep%20Learning%20Control%20for%20Robots%20with%20Unknown%20Jacobian&entry.906535625=Koji%20Matsuno%20and%20Chien%20Chern%20Cheah&entry.1292438233=%20%20Deep%20learning%2C%20with%20its%20exceptional%20learning%20capabilities%20and%20flexibility%2C%0Ahas%20been%20widely%20applied%20in%20various%20applications.%20However%2C%20its%20black-box%20nature%0Aposes%20a%20significant%20challenge%20in%20real-time%20robotic%20applications%2C%20particularly%0Ain%20robot%20control%2C%20where%20trustworthiness%20and%20robustness%20are%20critical%20in%20ensuring%0Asafety.%20In%20robot%20motion%20control%2C%20it%20is%20essential%20to%20analyze%20and%20ensure%20system%0Astability%2C%20necessitating%20the%20establishment%20of%20methodologies%20that%20address%20this%0Aneed.%20This%20paper%20aims%20to%20develop%20a%20theoretical%20framework%20for%20end-to-end%20deep%0Alearning%20control%20that%20can%20be%20integrated%20into%20existing%20robot%20control%20theories.%0AThe%20proposed%20control%20algorithm%20leverages%20a%20modular%20learning%20approach%20to%20update%0Athe%20weights%20of%20all%20layers%20in%20real%20time%2C%20ensuring%20system%20stability%20based%20on%0ALyapunov-like%20analysis.%20Experimental%20results%20on%20industrial%20robots%20are%20presented%0Ato%20illustrate%20the%20performance%20of%20the%20proposed%20deep%20learning%20controller.%20The%0Aproposed%20method%20offers%20an%20effective%20solution%20to%20the%20black-box%20problem%20in%20deep%0Alearning%2C%20demonstrating%20the%20possibility%20of%20deploying%20real-time%20deep%20learning%0Astrategies%20for%20robot%20kinematic%20control%20in%20a%20stable%20manner.%20This%20achievement%0Aprovides%20a%20critical%20foundation%20for%20future%20advancements%20in%20deep%20learning%20based%0Areal-time%20robotic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04984v1&entry.124074799=Read"},
{"title": "Dual-Branch Convolutional Framework for Spatial and Frequency-Based\n  Image Forgery Detection", "author": "Naman Tyagi", "abstract": "  With a very rapid increase in deepfakes and digital image forgeries, ensuring\nthe authenticity of images is becoming increasingly challenging. This report\nintroduces a forgery detection framework that combines spatial and\nfrequency-based features for detecting forgeries. We propose a dual branch\nconvolution neural network that operates on features extracted from spatial and\nfrequency domains. Features from both branches are fused and compared within a\nSiamese network, yielding 64 dimensional embeddings for classification. When\nbenchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,\noutperforming traditional statistical methods. Despite its relatively weaker\nperformance compared to larger, more complex forgery detection pipelines, our\napproach balances computational complexity and detection reliability, making it\nready for practical deployment. It provides a strong methodology for forensic\nscrutiny of digital images. In a broader sense, it advances the state of the\nart in visual forensics, addressing an urgent requirement in media\nverification, law enforcement and digital content reliability.\n", "link": "http://arxiv.org/abs/2509.05281v1", "date": "2025-09-05", "relevancy": 1.9925, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5024}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4978}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Branch%20Convolutional%20Framework%20for%20Spatial%20and%20Frequency-Based%0A%20%20Image%20Forgery%20Detection&body=Title%3A%20Dual-Branch%20Convolutional%20Framework%20for%20Spatial%20and%20Frequency-Based%0A%20%20Image%20Forgery%20Detection%0AAuthor%3A%20Naman%20Tyagi%0AAbstract%3A%20%20%20With%20a%20very%20rapid%20increase%20in%20deepfakes%20and%20digital%20image%20forgeries%2C%20ensuring%0Athe%20authenticity%20of%20images%20is%20becoming%20increasingly%20challenging.%20This%20report%0Aintroduces%20a%20forgery%20detection%20framework%20that%20combines%20spatial%20and%0Afrequency-based%20features%20for%20detecting%20forgeries.%20We%20propose%20a%20dual%20branch%0Aconvolution%20neural%20network%20that%20operates%20on%20features%20extracted%20from%20spatial%20and%0Afrequency%20domains.%20Features%20from%20both%20branches%20are%20fused%20and%20compared%20within%20a%0ASiamese%20network%2C%20yielding%2064%20dimensional%20embeddings%20for%20classification.%20When%0Abenchmarked%20on%20CASIA%202.0%20dataset%2C%20our%20method%20achieves%20an%20accuracy%20of%2077.9%25%2C%0Aoutperforming%20traditional%20statistical%20methods.%20Despite%20its%20relatively%20weaker%0Aperformance%20compared%20to%20larger%2C%20more%20complex%20forgery%20detection%20pipelines%2C%20our%0Aapproach%20balances%20computational%20complexity%20and%20detection%20reliability%2C%20making%20it%0Aready%20for%20practical%20deployment.%20It%20provides%20a%20strong%20methodology%20for%20forensic%0Ascrutiny%20of%20digital%20images.%20In%20a%20broader%20sense%2C%20it%20advances%20the%20state%20of%20the%0Aart%20in%20visual%20forensics%2C%20addressing%20an%20urgent%20requirement%20in%20media%0Averification%2C%20law%20enforcement%20and%20digital%20content%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Branch%2520Convolutional%2520Framework%2520for%2520Spatial%2520and%2520Frequency-Based%250A%2520%2520Image%2520Forgery%2520Detection%26entry.906535625%3DNaman%2520Tyagi%26entry.1292438233%3D%2520%2520With%2520a%2520very%2520rapid%2520increase%2520in%2520deepfakes%2520and%2520digital%2520image%2520forgeries%252C%2520ensuring%250Athe%2520authenticity%2520of%2520images%2520is%2520becoming%2520increasingly%2520challenging.%2520This%2520report%250Aintroduces%2520a%2520forgery%2520detection%2520framework%2520that%2520combines%2520spatial%2520and%250Afrequency-based%2520features%2520for%2520detecting%2520forgeries.%2520We%2520propose%2520a%2520dual%2520branch%250Aconvolution%2520neural%2520network%2520that%2520operates%2520on%2520features%2520extracted%2520from%2520spatial%2520and%250Afrequency%2520domains.%2520Features%2520from%2520both%2520branches%2520are%2520fused%2520and%2520compared%2520within%2520a%250ASiamese%2520network%252C%2520yielding%252064%2520dimensional%2520embeddings%2520for%2520classification.%2520When%250Abenchmarked%2520on%2520CASIA%25202.0%2520dataset%252C%2520our%2520method%2520achieves%2520an%2520accuracy%2520of%252077.9%2525%252C%250Aoutperforming%2520traditional%2520statistical%2520methods.%2520Despite%2520its%2520relatively%2520weaker%250Aperformance%2520compared%2520to%2520larger%252C%2520more%2520complex%2520forgery%2520detection%2520pipelines%252C%2520our%250Aapproach%2520balances%2520computational%2520complexity%2520and%2520detection%2520reliability%252C%2520making%2520it%250Aready%2520for%2520practical%2520deployment.%2520It%2520provides%2520a%2520strong%2520methodology%2520for%2520forensic%250Ascrutiny%2520of%2520digital%2520images.%2520In%2520a%2520broader%2520sense%252C%2520it%2520advances%2520the%2520state%2520of%2520the%250Aart%2520in%2520visual%2520forensics%252C%2520addressing%2520an%2520urgent%2520requirement%2520in%2520media%250Averification%252C%2520law%2520enforcement%2520and%2520digital%2520content%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Branch%20Convolutional%20Framework%20for%20Spatial%20and%20Frequency-Based%0A%20%20Image%20Forgery%20Detection&entry.906535625=Naman%20Tyagi&entry.1292438233=%20%20With%20a%20very%20rapid%20increase%20in%20deepfakes%20and%20digital%20image%20forgeries%2C%20ensuring%0Athe%20authenticity%20of%20images%20is%20becoming%20increasingly%20challenging.%20This%20report%0Aintroduces%20a%20forgery%20detection%20framework%20that%20combines%20spatial%20and%0Afrequency-based%20features%20for%20detecting%20forgeries.%20We%20propose%20a%20dual%20branch%0Aconvolution%20neural%20network%20that%20operates%20on%20features%20extracted%20from%20spatial%20and%0Afrequency%20domains.%20Features%20from%20both%20branches%20are%20fused%20and%20compared%20within%20a%0ASiamese%20network%2C%20yielding%2064%20dimensional%20embeddings%20for%20classification.%20When%0Abenchmarked%20on%20CASIA%202.0%20dataset%2C%20our%20method%20achieves%20an%20accuracy%20of%2077.9%25%2C%0Aoutperforming%20traditional%20statistical%20methods.%20Despite%20its%20relatively%20weaker%0Aperformance%20compared%20to%20larger%2C%20more%20complex%20forgery%20detection%20pipelines%2C%20our%0Aapproach%20balances%20computational%20complexity%20and%20detection%20reliability%2C%20making%20it%0Aready%20for%20practical%20deployment.%20It%20provides%20a%20strong%20methodology%20for%20forensic%0Ascrutiny%20of%20digital%20images.%20In%20a%20broader%20sense%2C%20it%20advances%20the%20state%20of%20the%0Aart%20in%20visual%20forensics%2C%20addressing%20an%20urgent%20requirement%20in%20media%0Averification%2C%20law%20enforcement%20and%20digital%20content%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05281v1&entry.124074799=Read"},
{"title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &\n  Dragons Gameplay", "author": "Andrew Zhu and Evan Osgood and Chris Callison-Burch", "abstract": "  Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.\n", "link": "http://arxiv.org/abs/2505.22809v2", "date": "2025-09-05", "relevancy": 1.9889, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%20Steps%20Towards%20Overhearing%20LLM%20Agents%3A%20A%20Case%20Study%20With%20Dungeons%20%26%0A%20%20Dragons%20Gameplay&body=Title%3A%20First%20Steps%20Towards%20Overhearing%20LLM%20Agents%3A%20A%20Case%20Study%20With%20Dungeons%20%26%0A%20%20Dragons%20Gameplay%0AAuthor%3A%20Andrew%20Zhu%20and%20Evan%20Osgood%20and%20Chris%20Callison-Burch%0AAbstract%3A%20%20%20Much%20work%20has%20been%20done%20on%20conversational%20LLM%20agents%20which%20directly%20assist%0Ahuman%20users%20with%20tasks.%20We%20present%20an%20alternative%20paradigm%20for%20interacting%20with%0ALLM%20agents%2C%20which%20we%20call%20%22overhearing%20agents%22.%20These%20overhearing%20agents%20do%20not%0Aactively%20participate%20in%20conversation%20--%20instead%2C%20they%20%22listen%20in%22%20on%0Ahuman-to-human%20conversations%20and%20perform%20background%20tasks%20or%20provide%0Asuggestions%20to%20assist%20the%20user.%20In%20this%20work%2C%20we%20explore%20the%20overhearing%20agents%0Aparadigm%20through%20the%20lens%20of%20Dungeons%20%26%20Dragons%20gameplay.%20We%20present%20an%0Ain-depth%20study%20using%20large%20multimodal%20audio-language%20models%20as%20overhearing%0Aagents%20to%20assist%20a%20Dungeon%20Master.%20We%20perform%20a%20human%20evaluation%20to%20examine%20the%0Ahelpfulness%20of%20such%20agents%20and%20find%20that%20some%20large%20audio-language%20models%20have%0Athe%20emergent%20ability%20to%20perform%20overhearing%20agent%20tasks%20using%20implicit%20audio%0Acues.%20Finally%2C%20we%20release%20Python%20libraries%20and%20our%20project%20code%20to%20support%0Afurther%20research%20into%20the%20overhearing%20agents%20paradigm%20at%0Ahttps%3A//github.com/zhudotexe/overhearing_agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%2520Steps%2520Towards%2520Overhearing%2520LLM%2520Agents%253A%2520A%2520Case%2520Study%2520With%2520Dungeons%2520%2526%250A%2520%2520Dragons%2520Gameplay%26entry.906535625%3DAndrew%2520Zhu%2520and%2520Evan%2520Osgood%2520and%2520Chris%2520Callison-Burch%26entry.1292438233%3D%2520%2520Much%2520work%2520has%2520been%2520done%2520on%2520conversational%2520LLM%2520agents%2520which%2520directly%2520assist%250Ahuman%2520users%2520with%2520tasks.%2520We%2520present%2520an%2520alternative%2520paradigm%2520for%2520interacting%2520with%250ALLM%2520agents%252C%2520which%2520we%2520call%2520%2522overhearing%2520agents%2522.%2520These%2520overhearing%2520agents%2520do%2520not%250Aactively%2520participate%2520in%2520conversation%2520--%2520instead%252C%2520they%2520%2522listen%2520in%2522%2520on%250Ahuman-to-human%2520conversations%2520and%2520perform%2520background%2520tasks%2520or%2520provide%250Asuggestions%2520to%2520assist%2520the%2520user.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520overhearing%2520agents%250Aparadigm%2520through%2520the%2520lens%2520of%2520Dungeons%2520%2526%2520Dragons%2520gameplay.%2520We%2520present%2520an%250Ain-depth%2520study%2520using%2520large%2520multimodal%2520audio-language%2520models%2520as%2520overhearing%250Aagents%2520to%2520assist%2520a%2520Dungeon%2520Master.%2520We%2520perform%2520a%2520human%2520evaluation%2520to%2520examine%2520the%250Ahelpfulness%2520of%2520such%2520agents%2520and%2520find%2520that%2520some%2520large%2520audio-language%2520models%2520have%250Athe%2520emergent%2520ability%2520to%2520perform%2520overhearing%2520agent%2520tasks%2520using%2520implicit%2520audio%250Acues.%2520Finally%252C%2520we%2520release%2520Python%2520libraries%2520and%2520our%2520project%2520code%2520to%2520support%250Afurther%2520research%2520into%2520the%2520overhearing%2520agents%2520paradigm%2520at%250Ahttps%253A//github.com/zhudotexe/overhearing_agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%20Steps%20Towards%20Overhearing%20LLM%20Agents%3A%20A%20Case%20Study%20With%20Dungeons%20%26%0A%20%20Dragons%20Gameplay&entry.906535625=Andrew%20Zhu%20and%20Evan%20Osgood%20and%20Chris%20Callison-Burch&entry.1292438233=%20%20Much%20work%20has%20been%20done%20on%20conversational%20LLM%20agents%20which%20directly%20assist%0Ahuman%20users%20with%20tasks.%20We%20present%20an%20alternative%20paradigm%20for%20interacting%20with%0ALLM%20agents%2C%20which%20we%20call%20%22overhearing%20agents%22.%20These%20overhearing%20agents%20do%20not%0Aactively%20participate%20in%20conversation%20--%20instead%2C%20they%20%22listen%20in%22%20on%0Ahuman-to-human%20conversations%20and%20perform%20background%20tasks%20or%20provide%0Asuggestions%20to%20assist%20the%20user.%20In%20this%20work%2C%20we%20explore%20the%20overhearing%20agents%0Aparadigm%20through%20the%20lens%20of%20Dungeons%20%26%20Dragons%20gameplay.%20We%20present%20an%0Ain-depth%20study%20using%20large%20multimodal%20audio-language%20models%20as%20overhearing%0Aagents%20to%20assist%20a%20Dungeon%20Master.%20We%20perform%20a%20human%20evaluation%20to%20examine%20the%0Ahelpfulness%20of%20such%20agents%20and%20find%20that%20some%20large%20audio-language%20models%20have%0Athe%20emergent%20ability%20to%20perform%20overhearing%20agent%20tasks%20using%20implicit%20audio%0Acues.%20Finally%2C%20we%20release%20Python%20libraries%20and%20our%20project%20code%20to%20support%0Afurther%20research%20into%20the%20overhearing%20agents%20paradigm%20at%0Ahttps%3A//github.com/zhudotexe/overhearing_agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22809v2&entry.124074799=Read"},
{"title": "Depth-Aware Initialization for Stable and Efficient Neural Network\n  Training", "author": "Vijay Pandey", "abstract": "  In past few years, various initialization schemes have been proposed. These\nschemes are glorot initialization, He initialization, initialization using\northogonal matrix, random walk method for initialization. Some of these methods\nstress on keeping unit variance of activation and gradient propagation through\nthe network layer. Few of these methods are independent of the depth\ninformation while some methods has considered the total network depth for\nbetter initialization. In this paper, comprehensive study has been done where\ndepth information of each layer as well as total network is incorporated for\nbetter initialization scheme. It has also been studied that for deeper networks\ntheoretical assumption of unit variance throughout the network does not perform\nwell. It requires the need to increase the variance of the network from first\nlayer activation to last layer activation. We proposed a novel way to increase\nthe variance of the network in flexible manner, which incorporates the\ninformation of each layer depth. Experiments shows that proposed method\nperforms better than the existing initialization scheme.\n", "link": "http://arxiv.org/abs/2509.05018v1", "date": "2025-09-05", "relevancy": 1.9889, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5238}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Aware%20Initialization%20for%20Stable%20and%20Efficient%20Neural%20Network%0A%20%20Training&body=Title%3A%20Depth-Aware%20Initialization%20for%20Stable%20and%20Efficient%20Neural%20Network%0A%20%20Training%0AAuthor%3A%20Vijay%20Pandey%0AAbstract%3A%20%20%20In%20past%20few%20years%2C%20various%20initialization%20schemes%20have%20been%20proposed.%20These%0Aschemes%20are%20glorot%20initialization%2C%20He%20initialization%2C%20initialization%20using%0Aorthogonal%20matrix%2C%20random%20walk%20method%20for%20initialization.%20Some%20of%20these%20methods%0Astress%20on%20keeping%20unit%20variance%20of%20activation%20and%20gradient%20propagation%20through%0Athe%20network%20layer.%20Few%20of%20these%20methods%20are%20independent%20of%20the%20depth%0Ainformation%20while%20some%20methods%20has%20considered%20the%20total%20network%20depth%20for%0Abetter%20initialization.%20In%20this%20paper%2C%20comprehensive%20study%20has%20been%20done%20where%0Adepth%20information%20of%20each%20layer%20as%20well%20as%20total%20network%20is%20incorporated%20for%0Abetter%20initialization%20scheme.%20It%20has%20also%20been%20studied%20that%20for%20deeper%20networks%0Atheoretical%20assumption%20of%20unit%20variance%20throughout%20the%20network%20does%20not%20perform%0Awell.%20It%20requires%20the%20need%20to%20increase%20the%20variance%20of%20the%20network%20from%20first%0Alayer%20activation%20to%20last%20layer%20activation.%20We%20proposed%20a%20novel%20way%20to%20increase%0Athe%20variance%20of%20the%20network%20in%20flexible%20manner%2C%20which%20incorporates%20the%0Ainformation%20of%20each%20layer%20depth.%20Experiments%20shows%20that%20proposed%20method%0Aperforms%20better%20than%20the%20existing%20initialization%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Aware%2520Initialization%2520for%2520Stable%2520and%2520Efficient%2520Neural%2520Network%250A%2520%2520Training%26entry.906535625%3DVijay%2520Pandey%26entry.1292438233%3D%2520%2520In%2520past%2520few%2520years%252C%2520various%2520initialization%2520schemes%2520have%2520been%2520proposed.%2520These%250Aschemes%2520are%2520glorot%2520initialization%252C%2520He%2520initialization%252C%2520initialization%2520using%250Aorthogonal%2520matrix%252C%2520random%2520walk%2520method%2520for%2520initialization.%2520Some%2520of%2520these%2520methods%250Astress%2520on%2520keeping%2520unit%2520variance%2520of%2520activation%2520and%2520gradient%2520propagation%2520through%250Athe%2520network%2520layer.%2520Few%2520of%2520these%2520methods%2520are%2520independent%2520of%2520the%2520depth%250Ainformation%2520while%2520some%2520methods%2520has%2520considered%2520the%2520total%2520network%2520depth%2520for%250Abetter%2520initialization.%2520In%2520this%2520paper%252C%2520comprehensive%2520study%2520has%2520been%2520done%2520where%250Adepth%2520information%2520of%2520each%2520layer%2520as%2520well%2520as%2520total%2520network%2520is%2520incorporated%2520for%250Abetter%2520initialization%2520scheme.%2520It%2520has%2520also%2520been%2520studied%2520that%2520for%2520deeper%2520networks%250Atheoretical%2520assumption%2520of%2520unit%2520variance%2520throughout%2520the%2520network%2520does%2520not%2520perform%250Awell.%2520It%2520requires%2520the%2520need%2520to%2520increase%2520the%2520variance%2520of%2520the%2520network%2520from%2520first%250Alayer%2520activation%2520to%2520last%2520layer%2520activation.%2520We%2520proposed%2520a%2520novel%2520way%2520to%2520increase%250Athe%2520variance%2520of%2520the%2520network%2520in%2520flexible%2520manner%252C%2520which%2520incorporates%2520the%250Ainformation%2520of%2520each%2520layer%2520depth.%2520Experiments%2520shows%2520that%2520proposed%2520method%250Aperforms%2520better%2520than%2520the%2520existing%2520initialization%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Aware%20Initialization%20for%20Stable%20and%20Efficient%20Neural%20Network%0A%20%20Training&entry.906535625=Vijay%20Pandey&entry.1292438233=%20%20In%20past%20few%20years%2C%20various%20initialization%20schemes%20have%20been%20proposed.%20These%0Aschemes%20are%20glorot%20initialization%2C%20He%20initialization%2C%20initialization%20using%0Aorthogonal%20matrix%2C%20random%20walk%20method%20for%20initialization.%20Some%20of%20these%20methods%0Astress%20on%20keeping%20unit%20variance%20of%20activation%20and%20gradient%20propagation%20through%0Athe%20network%20layer.%20Few%20of%20these%20methods%20are%20independent%20of%20the%20depth%0Ainformation%20while%20some%20methods%20has%20considered%20the%20total%20network%20depth%20for%0Abetter%20initialization.%20In%20this%20paper%2C%20comprehensive%20study%20has%20been%20done%20where%0Adepth%20information%20of%20each%20layer%20as%20well%20as%20total%20network%20is%20incorporated%20for%0Abetter%20initialization%20scheme.%20It%20has%20also%20been%20studied%20that%20for%20deeper%20networks%0Atheoretical%20assumption%20of%20unit%20variance%20throughout%20the%20network%20does%20not%20perform%0Awell.%20It%20requires%20the%20need%20to%20increase%20the%20variance%20of%20the%20network%20from%20first%0Alayer%20activation%20to%20last%20layer%20activation.%20We%20proposed%20a%20novel%20way%20to%20increase%0Athe%20variance%20of%20the%20network%20in%20flexible%20manner%2C%20which%20incorporates%20the%0Ainformation%20of%20each%20layer%20depth.%20Experiments%20shows%20that%20proposed%20method%0Aperforms%20better%20than%20the%20existing%20initialization%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05018v1&entry.124074799=Read"},
{"title": "RailGoerl24: G\u00f6rlitz Rail Test Center CV Dataset 2024", "author": "Rustam Tagiew and Ilkay Wunderlich and Mark Sastuba and Kilian G\u00f6ller and Steffen Seitz", "abstract": "  Driverless train operation for open tracks on urban guided transport and\nmainline railways requires, among other things automatic detection of actual\nand potential obstacles, especially humans, in the danger zone of the train's\npath. Machine learning algorithms have proven to be powerful state-of-the-art\ntools for this task. However, these algorithms require large amounts of\nhigh-quality annotated data containing human beings in railway-specific\nenvironments as training data. Unfortunately, the amount of publicly available\ndatasets is not yet sufficient and is significantly inferior to the datasets in\nthe road domain. Therefore, this paper presents RailGoerl24, an on-board visual\nlight Full HD camera dataset of 12205 frames recorded in a railway test center\nof T\\\"UV S\\\"UD Rail, in G\\\"orlitz, Germany. Its main purpose is to support the\ndevelopment of driverless train operation for guided transport. RailGoerl24\nalso includes a terrestrial LiDAR scan covering parts of the area used to\nacquire the RGB data. In addition to the raw data, the dataset contains 33556\nboxwise annotations in total for the object class 'person'. The faces of\nrecorded actors are not blurred or altered in any other way. RailGoerl24,\navailable at data.fid-move.de/dataset/railgoerl24, can also be used for tasks\nbeyond collision prediction.\n", "link": "http://arxiv.org/abs/2504.00204v2", "date": "2025-09-05", "relevancy": 1.9855, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5118}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4865}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RailGoerl24%3A%20G%C3%B6rlitz%20Rail%20Test%20Center%20CV%20Dataset%202024&body=Title%3A%20RailGoerl24%3A%20G%C3%B6rlitz%20Rail%20Test%20Center%20CV%20Dataset%202024%0AAuthor%3A%20Rustam%20Tagiew%20and%20Ilkay%20Wunderlich%20and%20Mark%20Sastuba%20and%20Kilian%20G%C3%B6ller%20and%20Steffen%20Seitz%0AAbstract%3A%20%20%20Driverless%20train%20operation%20for%20open%20tracks%20on%20urban%20guided%20transport%20and%0Amainline%20railways%20requires%2C%20among%20other%20things%20automatic%20detection%20of%20actual%0Aand%20potential%20obstacles%2C%20especially%20humans%2C%20in%20the%20danger%20zone%20of%20the%20train%27s%0Apath.%20Machine%20learning%20algorithms%20have%20proven%20to%20be%20powerful%20state-of-the-art%0Atools%20for%20this%20task.%20However%2C%20these%20algorithms%20require%20large%20amounts%20of%0Ahigh-quality%20annotated%20data%20containing%20human%20beings%20in%20railway-specific%0Aenvironments%20as%20training%20data.%20Unfortunately%2C%20the%20amount%20of%20publicly%20available%0Adatasets%20is%20not%20yet%20sufficient%20and%20is%20significantly%20inferior%20to%20the%20datasets%20in%0Athe%20road%20domain.%20Therefore%2C%20this%20paper%20presents%20RailGoerl24%2C%20an%20on-board%20visual%0Alight%20Full%20HD%20camera%20dataset%20of%2012205%20frames%20recorded%20in%20a%20railway%20test%20center%0Aof%20T%5C%22UV%20S%5C%22UD%20Rail%2C%20in%20G%5C%22orlitz%2C%20Germany.%20Its%20main%20purpose%20is%20to%20support%20the%0Adevelopment%20of%20driverless%20train%20operation%20for%20guided%20transport.%20RailGoerl24%0Aalso%20includes%20a%20terrestrial%20LiDAR%20scan%20covering%20parts%20of%20the%20area%20used%20to%0Aacquire%20the%20RGB%20data.%20In%20addition%20to%20the%20raw%20data%2C%20the%20dataset%20contains%2033556%0Aboxwise%20annotations%20in%20total%20for%20the%20object%20class%20%27person%27.%20The%20faces%20of%0Arecorded%20actors%20are%20not%20blurred%20or%20altered%20in%20any%20other%20way.%20RailGoerl24%2C%0Aavailable%20at%20data.fid-move.de/dataset/railgoerl24%2C%20can%20also%20be%20used%20for%20tasks%0Abeyond%20collision%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRailGoerl24%253A%2520G%25C3%25B6rlitz%2520Rail%2520Test%2520Center%2520CV%2520Dataset%25202024%26entry.906535625%3DRustam%2520Tagiew%2520and%2520Ilkay%2520Wunderlich%2520and%2520Mark%2520Sastuba%2520and%2520Kilian%2520G%25C3%25B6ller%2520and%2520Steffen%2520Seitz%26entry.1292438233%3D%2520%2520Driverless%2520train%2520operation%2520for%2520open%2520tracks%2520on%2520urban%2520guided%2520transport%2520and%250Amainline%2520railways%2520requires%252C%2520among%2520other%2520things%2520automatic%2520detection%2520of%2520actual%250Aand%2520potential%2520obstacles%252C%2520especially%2520humans%252C%2520in%2520the%2520danger%2520zone%2520of%2520the%2520train%2527s%250Apath.%2520Machine%2520learning%2520algorithms%2520have%2520proven%2520to%2520be%2520powerful%2520state-of-the-art%250Atools%2520for%2520this%2520task.%2520However%252C%2520these%2520algorithms%2520require%2520large%2520amounts%2520of%250Ahigh-quality%2520annotated%2520data%2520containing%2520human%2520beings%2520in%2520railway-specific%250Aenvironments%2520as%2520training%2520data.%2520Unfortunately%252C%2520the%2520amount%2520of%2520publicly%2520available%250Adatasets%2520is%2520not%2520yet%2520sufficient%2520and%2520is%2520significantly%2520inferior%2520to%2520the%2520datasets%2520in%250Athe%2520road%2520domain.%2520Therefore%252C%2520this%2520paper%2520presents%2520RailGoerl24%252C%2520an%2520on-board%2520visual%250Alight%2520Full%2520HD%2520camera%2520dataset%2520of%252012205%2520frames%2520recorded%2520in%2520a%2520railway%2520test%2520center%250Aof%2520T%255C%2522UV%2520S%255C%2522UD%2520Rail%252C%2520in%2520G%255C%2522orlitz%252C%2520Germany.%2520Its%2520main%2520purpose%2520is%2520to%2520support%2520the%250Adevelopment%2520of%2520driverless%2520train%2520operation%2520for%2520guided%2520transport.%2520RailGoerl24%250Aalso%2520includes%2520a%2520terrestrial%2520LiDAR%2520scan%2520covering%2520parts%2520of%2520the%2520area%2520used%2520to%250Aacquire%2520the%2520RGB%2520data.%2520In%2520addition%2520to%2520the%2520raw%2520data%252C%2520the%2520dataset%2520contains%252033556%250Aboxwise%2520annotations%2520in%2520total%2520for%2520the%2520object%2520class%2520%2527person%2527.%2520The%2520faces%2520of%250Arecorded%2520actors%2520are%2520not%2520blurred%2520or%2520altered%2520in%2520any%2520other%2520way.%2520RailGoerl24%252C%250Aavailable%2520at%2520data.fid-move.de/dataset/railgoerl24%252C%2520can%2520also%2520be%2520used%2520for%2520tasks%250Abeyond%2520collision%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RailGoerl24%3A%20G%C3%B6rlitz%20Rail%20Test%20Center%20CV%20Dataset%202024&entry.906535625=Rustam%20Tagiew%20and%20Ilkay%20Wunderlich%20and%20Mark%20Sastuba%20and%20Kilian%20G%C3%B6ller%20and%20Steffen%20Seitz&entry.1292438233=%20%20Driverless%20train%20operation%20for%20open%20tracks%20on%20urban%20guided%20transport%20and%0Amainline%20railways%20requires%2C%20among%20other%20things%20automatic%20detection%20of%20actual%0Aand%20potential%20obstacles%2C%20especially%20humans%2C%20in%20the%20danger%20zone%20of%20the%20train%27s%0Apath.%20Machine%20learning%20algorithms%20have%20proven%20to%20be%20powerful%20state-of-the-art%0Atools%20for%20this%20task.%20However%2C%20these%20algorithms%20require%20large%20amounts%20of%0Ahigh-quality%20annotated%20data%20containing%20human%20beings%20in%20railway-specific%0Aenvironments%20as%20training%20data.%20Unfortunately%2C%20the%20amount%20of%20publicly%20available%0Adatasets%20is%20not%20yet%20sufficient%20and%20is%20significantly%20inferior%20to%20the%20datasets%20in%0Athe%20road%20domain.%20Therefore%2C%20this%20paper%20presents%20RailGoerl24%2C%20an%20on-board%20visual%0Alight%20Full%20HD%20camera%20dataset%20of%2012205%20frames%20recorded%20in%20a%20railway%20test%20center%0Aof%20T%5C%22UV%20S%5C%22UD%20Rail%2C%20in%20G%5C%22orlitz%2C%20Germany.%20Its%20main%20purpose%20is%20to%20support%20the%0Adevelopment%20of%20driverless%20train%20operation%20for%20guided%20transport.%20RailGoerl24%0Aalso%20includes%20a%20terrestrial%20LiDAR%20scan%20covering%20parts%20of%20the%20area%20used%20to%0Aacquire%20the%20RGB%20data.%20In%20addition%20to%20the%20raw%20data%2C%20the%20dataset%20contains%2033556%0Aboxwise%20annotations%20in%20total%20for%20the%20object%20class%20%27person%27.%20The%20faces%20of%0Arecorded%20actors%20are%20not%20blurred%20or%20altered%20in%20any%20other%20way.%20RailGoerl24%2C%0Aavailable%20at%20data.fid-move.de/dataset/railgoerl24%2C%20can%20also%20be%20used%20for%20tasks%0Abeyond%20collision%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00204v2&entry.124074799=Read"},
{"title": "Scaling Performance of Large Language Model Pretraining", "author": "Alexander Interrante-Grant and Carla Varela-Rosa and Suhaas Narayan and Chris Connelly and Albert Reuther", "abstract": "  Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.\n", "link": "http://arxiv.org/abs/2509.05258v1", "date": "2025-09-05", "relevancy": 1.9843, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Performance%20of%20Large%20Language%20Model%20Pretraining&body=Title%3A%20Scaling%20Performance%20of%20Large%20Language%20Model%20Pretraining%0AAuthor%3A%20Alexander%20Interrante-Grant%20and%20Carla%20Varela-Rosa%20and%20Suhaas%20Narayan%20and%20Chris%20Connelly%20and%20Albert%20Reuther%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20show%20best-in-class%20performance%20across%20a%20wide%0Arange%20of%20natural%20language%20processing%20applications.%20Training%20these%20models%20is%20an%0Aextremely%20computationally%20expensive%20task%3B%20frontier%20Artificial%20Intelligence%20%28AI%29%0Aresearch%20companies%20are%20investing%20billions%20of%20dollars%20into%20supercomputing%0Ainfrastructure%20to%20train%20progressively%20larger%20models%20on%20increasingly%20massive%0Adatasets.%20Unfortunately%2C%20information%20about%20the%20scaling%20performance%20and%20training%0Aconsiderations%20of%20these%20large%20training%20pipelines%20is%20scarce%20in%20public%0Aliterature.%20Working%20with%20large-scale%20datasets%20and%20models%20can%20be%20complex%20and%0Apractical%20recommendations%20are%20scarce%20in%20the%20public%20literature%20for%20tuning%0Atraining%20performance%20when%20scaling%20up%20large%20language%20models.%20In%20this%20paper%2C%20we%0Aaim%20to%20demystify%20the%20large%20language%20model%20pretraining%20pipeline%20somewhat%20-%20in%0Aparticular%20with%20respect%20to%20distributed%20training%2C%20managing%20large%20datasets%20across%0Ahundreds%20of%20nodes%2C%20and%20scaling%20up%20data%20parallelism%20with%20an%20emphasis%20on%20fully%0Aleveraging%20available%20GPU%20compute%20capacity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Performance%2520of%2520Large%2520Language%2520Model%2520Pretraining%26entry.906535625%3DAlexander%2520Interrante-Grant%2520and%2520Carla%2520Varela-Rosa%2520and%2520Suhaas%2520Narayan%2520and%2520Chris%2520Connelly%2520and%2520Albert%2520Reuther%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520show%2520best-in-class%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520natural%2520language%2520processing%2520applications.%2520Training%2520these%2520models%2520is%2520an%250Aextremely%2520computationally%2520expensive%2520task%253B%2520frontier%2520Artificial%2520Intelligence%2520%2528AI%2529%250Aresearch%2520companies%2520are%2520investing%2520billions%2520of%2520dollars%2520into%2520supercomputing%250Ainfrastructure%2520to%2520train%2520progressively%2520larger%2520models%2520on%2520increasingly%2520massive%250Adatasets.%2520Unfortunately%252C%2520information%2520about%2520the%2520scaling%2520performance%2520and%2520training%250Aconsiderations%2520of%2520these%2520large%2520training%2520pipelines%2520is%2520scarce%2520in%2520public%250Aliterature.%2520Working%2520with%2520large-scale%2520datasets%2520and%2520models%2520can%2520be%2520complex%2520and%250Apractical%2520recommendations%2520are%2520scarce%2520in%2520the%2520public%2520literature%2520for%2520tuning%250Atraining%2520performance%2520when%2520scaling%2520up%2520large%2520language%2520models.%2520In%2520this%2520paper%252C%2520we%250Aaim%2520to%2520demystify%2520the%2520large%2520language%2520model%2520pretraining%2520pipeline%2520somewhat%2520-%2520in%250Aparticular%2520with%2520respect%2520to%2520distributed%2520training%252C%2520managing%2520large%2520datasets%2520across%250Ahundreds%2520of%2520nodes%252C%2520and%2520scaling%2520up%2520data%2520parallelism%2520with%2520an%2520emphasis%2520on%2520fully%250Aleveraging%2520available%2520GPU%2520compute%2520capacity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Performance%20of%20Large%20Language%20Model%20Pretraining&entry.906535625=Alexander%20Interrante-Grant%20and%20Carla%20Varela-Rosa%20and%20Suhaas%20Narayan%20and%20Chris%20Connelly%20and%20Albert%20Reuther&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20show%20best-in-class%20performance%20across%20a%20wide%0Arange%20of%20natural%20language%20processing%20applications.%20Training%20these%20models%20is%20an%0Aextremely%20computationally%20expensive%20task%3B%20frontier%20Artificial%20Intelligence%20%28AI%29%0Aresearch%20companies%20are%20investing%20billions%20of%20dollars%20into%20supercomputing%0Ainfrastructure%20to%20train%20progressively%20larger%20models%20on%20increasingly%20massive%0Adatasets.%20Unfortunately%2C%20information%20about%20the%20scaling%20performance%20and%20training%0Aconsiderations%20of%20these%20large%20training%20pipelines%20is%20scarce%20in%20public%0Aliterature.%20Working%20with%20large-scale%20datasets%20and%20models%20can%20be%20complex%20and%0Apractical%20recommendations%20are%20scarce%20in%20the%20public%20literature%20for%20tuning%0Atraining%20performance%20when%20scaling%20up%20large%20language%20models.%20In%20this%20paper%2C%20we%0Aaim%20to%20demystify%20the%20large%20language%20model%20pretraining%20pipeline%20somewhat%20-%20in%0Aparticular%20with%20respect%20to%20distributed%20training%2C%20managing%20large%20datasets%20across%0Ahundreds%20of%20nodes%2C%20and%20scaling%20up%20data%20parallelism%20with%20an%20emphasis%20on%20fully%0Aleveraging%20available%20GPU%20compute%20capacity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05258v1&entry.124074799=Read"},
{"title": "TECP: Token-Entropy Conformal Prediction for LLMs", "author": "Beining Xu and Yongming Lu", "abstract": "  Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings.\n", "link": "http://arxiv.org/abs/2509.00461v2", "date": "2025-09-05", "relevancy": 1.9838, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5301}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TECP%3A%20Token-Entropy%20Conformal%20Prediction%20for%20LLMs&body=Title%3A%20TECP%3A%20Token-Entropy%20Conformal%20Prediction%20for%20LLMs%0AAuthor%3A%20Beining%20Xu%20and%20Yongming%20Lu%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20for%20open-ended%20language%20generation%20remains%20a%0Acritical%20yet%20underexplored%20challenge%2C%20especially%20under%20black-box%20constraints%0Awhere%20internal%20model%20signals%20are%20inaccessible.%20In%20this%20paper%2C%20we%20introduce%0AToken-Entropy%20Conformal%20Prediction%20%28TECP%29%2C%20a%20novel%20framework%20that%20leverages%0Atoken-level%20entropy%20as%20a%20logit-free%2C%20reference-free%20uncertainty%20measure%20and%0Aintegrates%20it%20into%20a%20split%20conformal%20prediction%20%28CP%29%20pipeline%20to%20construct%0Aprediction%20sets%20with%20formal%20coverage%20guarantees.%20Unlike%20existing%20approaches%0Athat%20rely%20on%20semantic%20consistency%20heuristics%20or%20white-box%20features%2C%20TECP%0Adirectly%20estimates%20epistemic%20uncertainty%20from%20the%20token%20entropy%20structure%20of%0Asampled%20generations%20and%20calibrates%20uncertainty%20thresholds%20via%20CP%20quantiles%20to%0Aensure%20provable%20error%20control.%20Empirical%20evaluations%20across%20six%20large%20language%0Amodels%20and%20two%20benchmarks%20%28CoQA%20and%20TriviaQA%29%20demonstrate%20that%20TECP%0Aconsistently%20achieves%20reliable%20coverage%20and%20compact%20prediction%20sets%2C%0Aoutperforming%20prior%20self-consistency-based%20UQ%20methods.%20Our%20method%20provides%20a%0Aprincipled%20and%20efficient%20solution%20for%20trustworthy%20generation%20in%20black-box%20LLM%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTECP%253A%2520Token-Entropy%2520Conformal%2520Prediction%2520for%2520LLMs%26entry.906535625%3DBeining%2520Xu%2520and%2520Yongming%2520Lu%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520for%2520open-ended%2520language%2520generation%2520remains%2520a%250Acritical%2520yet%2520underexplored%2520challenge%252C%2520especially%2520under%2520black-box%2520constraints%250Awhere%2520internal%2520model%2520signals%2520are%2520inaccessible.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AToken-Entropy%2520Conformal%2520Prediction%2520%2528TECP%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%250Atoken-level%2520entropy%2520as%2520a%2520logit-free%252C%2520reference-free%2520uncertainty%2520measure%2520and%250Aintegrates%2520it%2520into%2520a%2520split%2520conformal%2520prediction%2520%2528CP%2529%2520pipeline%2520to%2520construct%250Aprediction%2520sets%2520with%2520formal%2520coverage%2520guarantees.%2520Unlike%2520existing%2520approaches%250Athat%2520rely%2520on%2520semantic%2520consistency%2520heuristics%2520or%2520white-box%2520features%252C%2520TECP%250Adirectly%2520estimates%2520epistemic%2520uncertainty%2520from%2520the%2520token%2520entropy%2520structure%2520of%250Asampled%2520generations%2520and%2520calibrates%2520uncertainty%2520thresholds%2520via%2520CP%2520quantiles%2520to%250Aensure%2520provable%2520error%2520control.%2520Empirical%2520evaluations%2520across%2520six%2520large%2520language%250Amodels%2520and%2520two%2520benchmarks%2520%2528CoQA%2520and%2520TriviaQA%2529%2520demonstrate%2520that%2520TECP%250Aconsistently%2520achieves%2520reliable%2520coverage%2520and%2520compact%2520prediction%2520sets%252C%250Aoutperforming%2520prior%2520self-consistency-based%2520UQ%2520methods.%2520Our%2520method%2520provides%2520a%250Aprincipled%2520and%2520efficient%2520solution%2520for%2520trustworthy%2520generation%2520in%2520black-box%2520LLM%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TECP%3A%20Token-Entropy%20Conformal%20Prediction%20for%20LLMs&entry.906535625=Beining%20Xu%20and%20Yongming%20Lu&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20for%20open-ended%20language%20generation%20remains%20a%0Acritical%20yet%20underexplored%20challenge%2C%20especially%20under%20black-box%20constraints%0Awhere%20internal%20model%20signals%20are%20inaccessible.%20In%20this%20paper%2C%20we%20introduce%0AToken-Entropy%20Conformal%20Prediction%20%28TECP%29%2C%20a%20novel%20framework%20that%20leverages%0Atoken-level%20entropy%20as%20a%20logit-free%2C%20reference-free%20uncertainty%20measure%20and%0Aintegrates%20it%20into%20a%20split%20conformal%20prediction%20%28CP%29%20pipeline%20to%20construct%0Aprediction%20sets%20with%20formal%20coverage%20guarantees.%20Unlike%20existing%20approaches%0Athat%20rely%20on%20semantic%20consistency%20heuristics%20or%20white-box%20features%2C%20TECP%0Adirectly%20estimates%20epistemic%20uncertainty%20from%20the%20token%20entropy%20structure%20of%0Asampled%20generations%20and%20calibrates%20uncertainty%20thresholds%20via%20CP%20quantiles%20to%0Aensure%20provable%20error%20control.%20Empirical%20evaluations%20across%20six%20large%20language%0Amodels%20and%20two%20benchmarks%20%28CoQA%20and%20TriviaQA%29%20demonstrate%20that%20TECP%0Aconsistently%20achieves%20reliable%20coverage%20and%20compact%20prediction%20sets%2C%0Aoutperforming%20prior%20self-consistency-based%20UQ%20methods.%20Our%20method%20provides%20a%0Aprincipled%20and%20efficient%20solution%20for%20trustworthy%20generation%20in%20black-box%20LLM%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00461v2&entry.124074799=Read"},
{"title": "Directed Evolution of Proteins via Bayesian Optimization in Embedding\n  Space", "author": "Matou\u0161 Sold\u00e1t and Ji\u0159\u00ed Kl\u00e9ma", "abstract": "  Directed evolution is an iterative laboratory process of designing proteins\nwith improved function by iteratively synthesizing new protein variants and\nevaluating their desired property with expensive and time-consuming biochemical\nscreening. Machine learning methods can help select informative or promising\nvariants for screening to increase their quality and reduce the amount of\nnecessary screening. In this paper, we present a novel method for\nmachine-learning-assisted directed evolution of proteins which combines\nBayesian optimization with informative representation of protein variants\nextracted from a pre-trained protein language model. We demonstrate that the\nnew representation based on the sequence embeddings significantly improves the\nperformance of Bayesian optimization yielding better results with the same\nnumber of conducted screening in total. At the same time, our method\noutperforms the state-of-the-art machine-learning-assisted directed evolution\nmethods with regression objective.\n", "link": "http://arxiv.org/abs/2509.04998v1", "date": "2025-09-05", "relevancy": 1.9807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directed%20Evolution%20of%20Proteins%20via%20Bayesian%20Optimization%20in%20Embedding%0A%20%20Space&body=Title%3A%20Directed%20Evolution%20of%20Proteins%20via%20Bayesian%20Optimization%20in%20Embedding%0A%20%20Space%0AAuthor%3A%20Matou%C5%A1%20Sold%C3%A1t%20and%20Ji%C5%99%C3%AD%20Kl%C3%A9ma%0AAbstract%3A%20%20%20Directed%20evolution%20is%20an%20iterative%20laboratory%20process%20of%20designing%20proteins%0Awith%20improved%20function%20by%20iteratively%20synthesizing%20new%20protein%20variants%20and%0Aevaluating%20their%20desired%20property%20with%20expensive%20and%20time-consuming%20biochemical%0Ascreening.%20Machine%20learning%20methods%20can%20help%20select%20informative%20or%20promising%0Avariants%20for%20screening%20to%20increase%20their%20quality%20and%20reduce%20the%20amount%20of%0Anecessary%20screening.%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20for%0Amachine-learning-assisted%20directed%20evolution%20of%20proteins%20which%20combines%0ABayesian%20optimization%20with%20informative%20representation%20of%20protein%20variants%0Aextracted%20from%20a%20pre-trained%20protein%20language%20model.%20We%20demonstrate%20that%20the%0Anew%20representation%20based%20on%20the%20sequence%20embeddings%20significantly%20improves%20the%0Aperformance%20of%20Bayesian%20optimization%20yielding%20better%20results%20with%20the%20same%0Anumber%20of%20conducted%20screening%20in%20total.%20At%20the%20same%20time%2C%20our%20method%0Aoutperforms%20the%20state-of-the-art%20machine-learning-assisted%20directed%20evolution%0Amethods%20with%20regression%20objective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirected%2520Evolution%2520of%2520Proteins%2520via%2520Bayesian%2520Optimization%2520in%2520Embedding%250A%2520%2520Space%26entry.906535625%3DMatou%25C5%25A1%2520Sold%25C3%25A1t%2520and%2520Ji%25C5%2599%25C3%25AD%2520Kl%25C3%25A9ma%26entry.1292438233%3D%2520%2520Directed%2520evolution%2520is%2520an%2520iterative%2520laboratory%2520process%2520of%2520designing%2520proteins%250Awith%2520improved%2520function%2520by%2520iteratively%2520synthesizing%2520new%2520protein%2520variants%2520and%250Aevaluating%2520their%2520desired%2520property%2520with%2520expensive%2520and%2520time-consuming%2520biochemical%250Ascreening.%2520Machine%2520learning%2520methods%2520can%2520help%2520select%2520informative%2520or%2520promising%250Avariants%2520for%2520screening%2520to%2520increase%2520their%2520quality%2520and%2520reduce%2520the%2520amount%2520of%250Anecessary%2520screening.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520method%2520for%250Amachine-learning-assisted%2520directed%2520evolution%2520of%2520proteins%2520which%2520combines%250ABayesian%2520optimization%2520with%2520informative%2520representation%2520of%2520protein%2520variants%250Aextracted%2520from%2520a%2520pre-trained%2520protein%2520language%2520model.%2520We%2520demonstrate%2520that%2520the%250Anew%2520representation%2520based%2520on%2520the%2520sequence%2520embeddings%2520significantly%2520improves%2520the%250Aperformance%2520of%2520Bayesian%2520optimization%2520yielding%2520better%2520results%2520with%2520the%2520same%250Anumber%2520of%2520conducted%2520screening%2520in%2520total.%2520At%2520the%2520same%2520time%252C%2520our%2520method%250Aoutperforms%2520the%2520state-of-the-art%2520machine-learning-assisted%2520directed%2520evolution%250Amethods%2520with%2520regression%2520objective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directed%20Evolution%20of%20Proteins%20via%20Bayesian%20Optimization%20in%20Embedding%0A%20%20Space&entry.906535625=Matou%C5%A1%20Sold%C3%A1t%20and%20Ji%C5%99%C3%AD%20Kl%C3%A9ma&entry.1292438233=%20%20Directed%20evolution%20is%20an%20iterative%20laboratory%20process%20of%20designing%20proteins%0Awith%20improved%20function%20by%20iteratively%20synthesizing%20new%20protein%20variants%20and%0Aevaluating%20their%20desired%20property%20with%20expensive%20and%20time-consuming%20biochemical%0Ascreening.%20Machine%20learning%20methods%20can%20help%20select%20informative%20or%20promising%0Avariants%20for%20screening%20to%20increase%20their%20quality%20and%20reduce%20the%20amount%20of%0Anecessary%20screening.%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20for%0Amachine-learning-assisted%20directed%20evolution%20of%20proteins%20which%20combines%0ABayesian%20optimization%20with%20informative%20representation%20of%20protein%20variants%0Aextracted%20from%20a%20pre-trained%20protein%20language%20model.%20We%20demonstrate%20that%20the%0Anew%20representation%20based%20on%20the%20sequence%20embeddings%20significantly%20improves%20the%0Aperformance%20of%20Bayesian%20optimization%20yielding%20better%20results%20with%20the%20same%0Anumber%20of%20conducted%20screening%20in%20total.%20At%20the%20same%20time%2C%20our%20method%0Aoutperforms%20the%20state-of-the-art%20machine-learning-assisted%20directed%20evolution%0Amethods%20with%20regression%20objective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04998v1&entry.124074799=Read"},
{"title": "Quality control in sublinear time: a case study via random graphs", "author": "Cassandra Marcussen and Ronitt Rubinfeld and Madhu Sudan", "abstract": "  Many algorithms are designed to work well on average over inputs. When\nrunning such an algorithm on an arbitrary input, we must ask: Can we trust the\nalgorithm on this input? We identify a new class of algorithmic problems\naddressing this, which we call \"Quality Control Problems.\" These problems are\nspecified by a (positive, real-valued) \"quality function\" $\\rho$ and a\ndistribution $D$ such that, with high probability, a sample drawn from $D$ is\n\"high quality,\" meaning its $\\rho$-value is near $1$. The goal is to accept\ninputs $x \\sim D$ and reject potentially adversarially generated inputs $x$\nwith $\\rho(x)$ far from $1$. The objective of quality control is thus weaker\nthan either component problem: testing for \"$\\rho(x) \\approx 1$\" or testing if\n$x \\sim D$, and offers the possibility of more efficient algorithms.\n  In this work, we consider the sublinear version of the quality control\nproblem, where $D \\in \\Delta(\\{0,1\\}^N)$ and the goal is to solve the $(D\n,\\rho)$-quality problem with $o(N)$ queries and time. As a case study, we\nconsider random graphs, i.e., $D = G_{n,p}$ (and $N = \\binom{n}2$), and the\n$k$-clique count function $\\rho_k := C_k(G)/\\mathbb{E}_{G' \\sim\nG_{n,p}}[C_k(G')]$, where $C_k(G)$ is the number of $k$-cliques in $G$. Testing\nif $G \\sim G_{n,p}$ with one sample, let alone with sublinear query access to\nthe sample, is of course impossible. Testing if $\\rho_k(G)\\approx 1$ requires\n$p^{-\\Omega(k^2)}$ samples. In contrast, we show that the quality control\nproblem for $G_{n,p}$ (with $n \\geq p^{-ck}$ for some constant $c$) with\nrespect to $\\rho_k$ can be tested with $p^{-O(k)}$ queries and time, showing\nquality control is provably superpolynomially more efficient in this setting.\nMore generally, for a motif $H$ of maximum degree $\\Delta(H)$, the respective\nquality control problem can be solved with $p^{-O(\\Delta(H))}$ queries and\nrunning time.\n", "link": "http://arxiv.org/abs/2508.16531v2", "date": "2025-09-05", "relevancy": 1.9776, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4077}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3952}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs&body=Title%3A%20Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs%0AAuthor%3A%20Cassandra%20Marcussen%20and%20Ronitt%20Rubinfeld%20and%20Madhu%20Sudan%0AAbstract%3A%20%20%20Many%20algorithms%20are%20designed%20to%20work%20well%20on%20average%20over%20inputs.%20When%0Arunning%20such%20an%20algorithm%20on%20an%20arbitrary%20input%2C%20we%20must%20ask%3A%20Can%20we%20trust%20the%0Aalgorithm%20on%20this%20input%3F%20We%20identify%20a%20new%20class%20of%20algorithmic%20problems%0Aaddressing%20this%2C%20which%20we%20call%20%22Quality%20Control%20Problems.%22%20These%20problems%20are%0Aspecified%20by%20a%20%28positive%2C%20real-valued%29%20%22quality%20function%22%20%24%5Crho%24%20and%20a%0Adistribution%20%24D%24%20such%20that%2C%20with%20high%20probability%2C%20a%20sample%20drawn%20from%20%24D%24%20is%0A%22high%20quality%2C%22%20meaning%20its%20%24%5Crho%24-value%20is%20near%20%241%24.%20The%20goal%20is%20to%20accept%0Ainputs%20%24x%20%5Csim%20D%24%20and%20reject%20potentially%20adversarially%20generated%20inputs%20%24x%24%0Awith%20%24%5Crho%28x%29%24%20far%20from%20%241%24.%20The%20objective%20of%20quality%20control%20is%20thus%20weaker%0Athan%20either%20component%20problem%3A%20testing%20for%20%22%24%5Crho%28x%29%20%5Capprox%201%24%22%20or%20testing%20if%0A%24x%20%5Csim%20D%24%2C%20and%20offers%20the%20possibility%20of%20more%20efficient%20algorithms.%0A%20%20In%20this%20work%2C%20we%20consider%20the%20sublinear%20version%20of%20the%20quality%20control%0Aproblem%2C%20where%20%24D%20%5Cin%20%5CDelta%28%5C%7B0%2C1%5C%7D%5EN%29%24%20and%20the%20goal%20is%20to%20solve%20the%20%24%28D%0A%2C%5Crho%29%24-quality%20problem%20with%20%24o%28N%29%24%20queries%20and%20time.%20As%20a%20case%20study%2C%20we%0Aconsider%20random%20graphs%2C%20i.e.%2C%20%24D%20%3D%20G_%7Bn%2Cp%7D%24%20%28and%20%24N%20%3D%20%5Cbinom%7Bn%7D2%24%29%2C%20and%20the%0A%24k%24-clique%20count%20function%20%24%5Crho_k%20%3A%3D%20C_k%28G%29/%5Cmathbb%7BE%7D_%7BG%27%20%5Csim%0AG_%7Bn%2Cp%7D%7D%5BC_k%28G%27%29%5D%24%2C%20where%20%24C_k%28G%29%24%20is%20the%20number%20of%20%24k%24-cliques%20in%20%24G%24.%20Testing%0Aif%20%24G%20%5Csim%20G_%7Bn%2Cp%7D%24%20with%20one%20sample%2C%20let%20alone%20with%20sublinear%20query%20access%20to%0Athe%20sample%2C%20is%20of%20course%20impossible.%20Testing%20if%20%24%5Crho_k%28G%29%5Capprox%201%24%20requires%0A%24p%5E%7B-%5COmega%28k%5E2%29%7D%24%20samples.%20In%20contrast%2C%20we%20show%20that%20the%20quality%20control%0Aproblem%20for%20%24G_%7Bn%2Cp%7D%24%20%28with%20%24n%20%5Cgeq%20p%5E%7B-ck%7D%24%20for%20some%20constant%20%24c%24%29%20with%0Arespect%20to%20%24%5Crho_k%24%20can%20be%20tested%20with%20%24p%5E%7B-O%28k%29%7D%24%20queries%20and%20time%2C%20showing%0Aquality%20control%20is%20provably%20superpolynomially%20more%20efficient%20in%20this%20setting.%0AMore%20generally%2C%20for%20a%20motif%20%24H%24%20of%20maximum%20degree%20%24%5CDelta%28H%29%24%2C%20the%20respective%0Aquality%20control%20problem%20can%20be%20solved%20with%20%24p%5E%7B-O%28%5CDelta%28H%29%29%7D%24%20queries%20and%0Arunning%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520control%2520in%2520sublinear%2520time%253A%2520a%2520case%2520study%2520via%2520random%2520graphs%26entry.906535625%3DCassandra%2520Marcussen%2520and%2520Ronitt%2520Rubinfeld%2520and%2520Madhu%2520Sudan%26entry.1292438233%3D%2520%2520Many%2520algorithms%2520are%2520designed%2520to%2520work%2520well%2520on%2520average%2520over%2520inputs.%2520When%250Arunning%2520such%2520an%2520algorithm%2520on%2520an%2520arbitrary%2520input%252C%2520we%2520must%2520ask%253A%2520Can%2520we%2520trust%2520the%250Aalgorithm%2520on%2520this%2520input%253F%2520We%2520identify%2520a%2520new%2520class%2520of%2520algorithmic%2520problems%250Aaddressing%2520this%252C%2520which%2520we%2520call%2520%2522Quality%2520Control%2520Problems.%2522%2520These%2520problems%2520are%250Aspecified%2520by%2520a%2520%2528positive%252C%2520real-valued%2529%2520%2522quality%2520function%2522%2520%2524%255Crho%2524%2520and%2520a%250Adistribution%2520%2524D%2524%2520such%2520that%252C%2520with%2520high%2520probability%252C%2520a%2520sample%2520drawn%2520from%2520%2524D%2524%2520is%250A%2522high%2520quality%252C%2522%2520meaning%2520its%2520%2524%255Crho%2524-value%2520is%2520near%2520%25241%2524.%2520The%2520goal%2520is%2520to%2520accept%250Ainputs%2520%2524x%2520%255Csim%2520D%2524%2520and%2520reject%2520potentially%2520adversarially%2520generated%2520inputs%2520%2524x%2524%250Awith%2520%2524%255Crho%2528x%2529%2524%2520far%2520from%2520%25241%2524.%2520The%2520objective%2520of%2520quality%2520control%2520is%2520thus%2520weaker%250Athan%2520either%2520component%2520problem%253A%2520testing%2520for%2520%2522%2524%255Crho%2528x%2529%2520%255Capprox%25201%2524%2522%2520or%2520testing%2520if%250A%2524x%2520%255Csim%2520D%2524%252C%2520and%2520offers%2520the%2520possibility%2520of%2520more%2520efficient%2520algorithms.%250A%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520sublinear%2520version%2520of%2520the%2520quality%2520control%250Aproblem%252C%2520where%2520%2524D%2520%255Cin%2520%255CDelta%2528%255C%257B0%252C1%255C%257D%255EN%2529%2524%2520and%2520the%2520goal%2520is%2520to%2520solve%2520the%2520%2524%2528D%250A%252C%255Crho%2529%2524-quality%2520problem%2520with%2520%2524o%2528N%2529%2524%2520queries%2520and%2520time.%2520As%2520a%2520case%2520study%252C%2520we%250Aconsider%2520random%2520graphs%252C%2520i.e.%252C%2520%2524D%2520%253D%2520G_%257Bn%252Cp%257D%2524%2520%2528and%2520%2524N%2520%253D%2520%255Cbinom%257Bn%257D2%2524%2529%252C%2520and%2520the%250A%2524k%2524-clique%2520count%2520function%2520%2524%255Crho_k%2520%253A%253D%2520C_k%2528G%2529/%255Cmathbb%257BE%257D_%257BG%2527%2520%255Csim%250AG_%257Bn%252Cp%257D%257D%255BC_k%2528G%2527%2529%255D%2524%252C%2520where%2520%2524C_k%2528G%2529%2524%2520is%2520the%2520number%2520of%2520%2524k%2524-cliques%2520in%2520%2524G%2524.%2520Testing%250Aif%2520%2524G%2520%255Csim%2520G_%257Bn%252Cp%257D%2524%2520with%2520one%2520sample%252C%2520let%2520alone%2520with%2520sublinear%2520query%2520access%2520to%250Athe%2520sample%252C%2520is%2520of%2520course%2520impossible.%2520Testing%2520if%2520%2524%255Crho_k%2528G%2529%255Capprox%25201%2524%2520requires%250A%2524p%255E%257B-%255COmega%2528k%255E2%2529%257D%2524%2520samples.%2520In%2520contrast%252C%2520we%2520show%2520that%2520the%2520quality%2520control%250Aproblem%2520for%2520%2524G_%257Bn%252Cp%257D%2524%2520%2528with%2520%2524n%2520%255Cgeq%2520p%255E%257B-ck%257D%2524%2520for%2520some%2520constant%2520%2524c%2524%2529%2520with%250Arespect%2520to%2520%2524%255Crho_k%2524%2520can%2520be%2520tested%2520with%2520%2524p%255E%257B-O%2528k%2529%257D%2524%2520queries%2520and%2520time%252C%2520showing%250Aquality%2520control%2520is%2520provably%2520superpolynomially%2520more%2520efficient%2520in%2520this%2520setting.%250AMore%2520generally%252C%2520for%2520a%2520motif%2520%2524H%2524%2520of%2520maximum%2520degree%2520%2524%255CDelta%2528H%2529%2524%252C%2520the%2520respective%250Aquality%2520control%2520problem%2520can%2520be%2520solved%2520with%2520%2524p%255E%257B-O%2528%255CDelta%2528H%2529%2529%257D%2524%2520queries%2520and%250Arunning%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs&entry.906535625=Cassandra%20Marcussen%20and%20Ronitt%20Rubinfeld%20and%20Madhu%20Sudan&entry.1292438233=%20%20Many%20algorithms%20are%20designed%20to%20work%20well%20on%20average%20over%20inputs.%20When%0Arunning%20such%20an%20algorithm%20on%20an%20arbitrary%20input%2C%20we%20must%20ask%3A%20Can%20we%20trust%20the%0Aalgorithm%20on%20this%20input%3F%20We%20identify%20a%20new%20class%20of%20algorithmic%20problems%0Aaddressing%20this%2C%20which%20we%20call%20%22Quality%20Control%20Problems.%22%20These%20problems%20are%0Aspecified%20by%20a%20%28positive%2C%20real-valued%29%20%22quality%20function%22%20%24%5Crho%24%20and%20a%0Adistribution%20%24D%24%20such%20that%2C%20with%20high%20probability%2C%20a%20sample%20drawn%20from%20%24D%24%20is%0A%22high%20quality%2C%22%20meaning%20its%20%24%5Crho%24-value%20is%20near%20%241%24.%20The%20goal%20is%20to%20accept%0Ainputs%20%24x%20%5Csim%20D%24%20and%20reject%20potentially%20adversarially%20generated%20inputs%20%24x%24%0Awith%20%24%5Crho%28x%29%24%20far%20from%20%241%24.%20The%20objective%20of%20quality%20control%20is%20thus%20weaker%0Athan%20either%20component%20problem%3A%20testing%20for%20%22%24%5Crho%28x%29%20%5Capprox%201%24%22%20or%20testing%20if%0A%24x%20%5Csim%20D%24%2C%20and%20offers%20the%20possibility%20of%20more%20efficient%20algorithms.%0A%20%20In%20this%20work%2C%20we%20consider%20the%20sublinear%20version%20of%20the%20quality%20control%0Aproblem%2C%20where%20%24D%20%5Cin%20%5CDelta%28%5C%7B0%2C1%5C%7D%5EN%29%24%20and%20the%20goal%20is%20to%20solve%20the%20%24%28D%0A%2C%5Crho%29%24-quality%20problem%20with%20%24o%28N%29%24%20queries%20and%20time.%20As%20a%20case%20study%2C%20we%0Aconsider%20random%20graphs%2C%20i.e.%2C%20%24D%20%3D%20G_%7Bn%2Cp%7D%24%20%28and%20%24N%20%3D%20%5Cbinom%7Bn%7D2%24%29%2C%20and%20the%0A%24k%24-clique%20count%20function%20%24%5Crho_k%20%3A%3D%20C_k%28G%29/%5Cmathbb%7BE%7D_%7BG%27%20%5Csim%0AG_%7Bn%2Cp%7D%7D%5BC_k%28G%27%29%5D%24%2C%20where%20%24C_k%28G%29%24%20is%20the%20number%20of%20%24k%24-cliques%20in%20%24G%24.%20Testing%0Aif%20%24G%20%5Csim%20G_%7Bn%2Cp%7D%24%20with%20one%20sample%2C%20let%20alone%20with%20sublinear%20query%20access%20to%0Athe%20sample%2C%20is%20of%20course%20impossible.%20Testing%20if%20%24%5Crho_k%28G%29%5Capprox%201%24%20requires%0A%24p%5E%7B-%5COmega%28k%5E2%29%7D%24%20samples.%20In%20contrast%2C%20we%20show%20that%20the%20quality%20control%0Aproblem%20for%20%24G_%7Bn%2Cp%7D%24%20%28with%20%24n%20%5Cgeq%20p%5E%7B-ck%7D%24%20for%20some%20constant%20%24c%24%29%20with%0Arespect%20to%20%24%5Crho_k%24%20can%20be%20tested%20with%20%24p%5E%7B-O%28k%29%7D%24%20queries%20and%20time%2C%20showing%0Aquality%20control%20is%20provably%20superpolynomially%20more%20efficient%20in%20this%20setting.%0AMore%20generally%2C%20for%20a%20motif%20%24H%24%20of%20maximum%20degree%20%24%5CDelta%28H%29%24%2C%20the%20respective%0Aquality%20control%20problem%20can%20be%20solved%20with%20%24p%5E%7B-O%28%5CDelta%28H%29%29%7D%24%20queries%20and%0Arunning%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16531v2&entry.124074799=Read"},
{"title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions", "author": "Matteo Bortoletto and Constantin Ruhdorfer and Andreas Bulling", "abstract": "  Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch.\n", "link": "http://arxiv.org/abs/2509.05066v1", "date": "2025-09-05", "relevancy": 1.9754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToM-SSI%3A%20Evaluating%20Theory%20of%20Mind%20in%20Situated%20Social%20Interactions&body=Title%3A%20ToM-SSI%3A%20Evaluating%20Theory%20of%20Mind%20in%20Situated%20Social%20Interactions%0AAuthor%3A%20Matteo%20Bortoletto%20and%20Constantin%20Ruhdorfer%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20Most%20existing%20Theory%20of%20Mind%20%28ToM%29%20benchmarks%20for%20foundation%20models%20rely%20on%0Avariations%20of%20the%20Sally-Anne%20test%2C%20offering%20only%20a%20very%20limited%20perspective%20on%0AToM%20and%20neglecting%20the%20complexity%20of%20human%20social%20interactions.%20To%20address%20this%0Agap%2C%20we%20propose%20ToM-SSI%3A%20a%20new%20benchmark%20specifically%20designed%20to%20test%20ToM%0Acapabilities%20in%20environments%20rich%20with%20social%20interactions%20and%20spatial%0Adynamics.%20While%20current%20ToM%20benchmarks%20are%20limited%20to%20text-only%20or%20dyadic%0Ainteractions%2C%20ToM-SSI%20is%20multimodal%20and%20includes%20group%20interactions%20of%20up%20to%0Afour%20agents%20that%20communicate%20and%20move%20in%20situated%20environments.%20This%20unique%0Adesign%20allows%20us%20to%20study%2C%20for%20the%20first%20time%2C%20mixed%20cooperative-obstructive%0Asettings%20and%20reasoning%20about%20multiple%20agents%27%20mental%20state%20in%20parallel%2C%20thus%0Acapturing%20a%20wider%20range%20of%20social%20cognition%20than%20existing%20benchmarks.%20Our%0Aevaluations%20reveal%20that%20the%20current%20models%27%20performance%20is%20still%20severely%0Alimited%2C%20especially%20in%20these%20new%20tasks%2C%20highlighting%20critical%20gaps%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToM-SSI%253A%2520Evaluating%2520Theory%2520of%2520Mind%2520in%2520Situated%2520Social%2520Interactions%26entry.906535625%3DMatteo%2520Bortoletto%2520and%2520Constantin%2520Ruhdorfer%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520Most%2520existing%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520benchmarks%2520for%2520foundation%2520models%2520rely%2520on%250Avariations%2520of%2520the%2520Sally-Anne%2520test%252C%2520offering%2520only%2520a%2520very%2520limited%2520perspective%2520on%250AToM%2520and%2520neglecting%2520the%2520complexity%2520of%2520human%2520social%2520interactions.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520ToM-SSI%253A%2520a%2520new%2520benchmark%2520specifically%2520designed%2520to%2520test%2520ToM%250Acapabilities%2520in%2520environments%2520rich%2520with%2520social%2520interactions%2520and%2520spatial%250Adynamics.%2520While%2520current%2520ToM%2520benchmarks%2520are%2520limited%2520to%2520text-only%2520or%2520dyadic%250Ainteractions%252C%2520ToM-SSI%2520is%2520multimodal%2520and%2520includes%2520group%2520interactions%2520of%2520up%2520to%250Afour%2520agents%2520that%2520communicate%2520and%2520move%2520in%2520situated%2520environments.%2520This%2520unique%250Adesign%2520allows%2520us%2520to%2520study%252C%2520for%2520the%2520first%2520time%252C%2520mixed%2520cooperative-obstructive%250Asettings%2520and%2520reasoning%2520about%2520multiple%2520agents%2527%2520mental%2520state%2520in%2520parallel%252C%2520thus%250Acapturing%2520a%2520wider%2520range%2520of%2520social%2520cognition%2520than%2520existing%2520benchmarks.%2520Our%250Aevaluations%2520reveal%2520that%2520the%2520current%2520models%2527%2520performance%2520is%2520still%2520severely%250Alimited%252C%2520especially%2520in%2520these%2520new%2520tasks%252C%2520highlighting%2520critical%2520gaps%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToM-SSI%3A%20Evaluating%20Theory%20of%20Mind%20in%20Situated%20Social%20Interactions&entry.906535625=Matteo%20Bortoletto%20and%20Constantin%20Ruhdorfer%20and%20Andreas%20Bulling&entry.1292438233=%20%20Most%20existing%20Theory%20of%20Mind%20%28ToM%29%20benchmarks%20for%20foundation%20models%20rely%20on%0Avariations%20of%20the%20Sally-Anne%20test%2C%20offering%20only%20a%20very%20limited%20perspective%20on%0AToM%20and%20neglecting%20the%20complexity%20of%20human%20social%20interactions.%20To%20address%20this%0Agap%2C%20we%20propose%20ToM-SSI%3A%20a%20new%20benchmark%20specifically%20designed%20to%20test%20ToM%0Acapabilities%20in%20environments%20rich%20with%20social%20interactions%20and%20spatial%0Adynamics.%20While%20current%20ToM%20benchmarks%20are%20limited%20to%20text-only%20or%20dyadic%0Ainteractions%2C%20ToM-SSI%20is%20multimodal%20and%20includes%20group%20interactions%20of%20up%20to%0Afour%20agents%20that%20communicate%20and%20move%20in%20situated%20environments.%20This%20unique%0Adesign%20allows%20us%20to%20study%2C%20for%20the%20first%20time%2C%20mixed%20cooperative-obstructive%0Asettings%20and%20reasoning%20about%20multiple%20agents%27%20mental%20state%20in%20parallel%2C%20thus%0Acapturing%20a%20wider%20range%20of%20social%20cognition%20than%20existing%20benchmarks.%20Our%0Aevaluations%20reveal%20that%20the%20current%20models%27%20performance%20is%20still%20severely%0Alimited%2C%20especially%20in%20these%20new%20tasks%2C%20highlighting%20critical%20gaps%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05066v1&entry.124074799=Read"},
{"title": "Interpretable Deep Transfer Learning for Breast Ultrasound Cancer\n  Detection: A Multi-Dataset Study", "author": "Mohammad Abbadi and Yassine Himeur and Shadi Atalla and Wathiq Mansoor", "abstract": "  Breast cancer remains a leading cause of cancer-related mortality among women\nworldwide. Ultrasound imaging, widely used due to its safety and\ncost-effectiveness, plays a key role in early detection, especially in patients\nwith dense breast tissue. This paper presents a comprehensive study on the\napplication of machine learning and deep learning techniques for breast cancer\nclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,\nand BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,\nKNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,\nGoogLeNet). Experimental results show that ResNet-18 achieves the highest\naccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML\nmodels, though outperformed by CNNs, achieve competitive performance when\nenhanced with deep feature extraction. Grad-CAM visualizations further improve\nmodel transparency by highlighting diagnostically relevant image regions. These\nfindings support the integration of AI-based diagnostic tools into clinical\nworkflows and demonstrate the feasibility of deploying high-performing,\ninterpretable systems for ultrasound-based breast cancer detection.\n", "link": "http://arxiv.org/abs/2509.05004v1", "date": "2025-09-05", "relevancy": 1.974, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Deep%20Transfer%20Learning%20for%20Breast%20Ultrasound%20Cancer%0A%20%20Detection%3A%20A%20Multi-Dataset%20Study&body=Title%3A%20Interpretable%20Deep%20Transfer%20Learning%20for%20Breast%20Ultrasound%20Cancer%0A%20%20Detection%3A%20A%20Multi-Dataset%20Study%0AAuthor%3A%20Mohammad%20Abbadi%20and%20Yassine%20Himeur%20and%20Shadi%20Atalla%20and%20Wathiq%20Mansoor%0AAbstract%3A%20%20%20Breast%20cancer%20remains%20a%20leading%20cause%20of%20cancer-related%20mortality%20among%20women%0Aworldwide.%20Ultrasound%20imaging%2C%20widely%20used%20due%20to%20its%20safety%20and%0Acost-effectiveness%2C%20plays%20a%20key%20role%20in%20early%20detection%2C%20especially%20in%20patients%0Awith%20dense%20breast%20tissue.%20This%20paper%20presents%20a%20comprehensive%20study%20on%20the%0Aapplication%20of%20machine%20learning%20and%20deep%20learning%20techniques%20for%20breast%20cancer%0Aclassification%20using%20ultrasound%20images.%20Using%20datasets%20such%20as%20BUSI%2C%20BUS-BRA%2C%0Aand%20BrEaST-Lesions%20USG%2C%20we%20evaluate%20classical%20machine%20learning%20models%20%28SVM%2C%0AKNN%29%20and%20deep%20convolutional%20neural%20networks%20%28ResNet-18%2C%20EfficientNet-B0%2C%0AGoogLeNet%29.%20Experimental%20results%20show%20that%20ResNet-18%20achieves%20the%20highest%0Aaccuracy%20%2899.7%25%29%20and%20perfect%20sensitivity%20for%20malignant%20lesions.%20Classical%20ML%0Amodels%2C%20though%20outperformed%20by%20CNNs%2C%20achieve%20competitive%20performance%20when%0Aenhanced%20with%20deep%20feature%20extraction.%20Grad-CAM%20visualizations%20further%20improve%0Amodel%20transparency%20by%20highlighting%20diagnostically%20relevant%20image%20regions.%20These%0Afindings%20support%20the%20integration%20of%20AI-based%20diagnostic%20tools%20into%20clinical%0Aworkflows%20and%20demonstrate%20the%20feasibility%20of%20deploying%20high-performing%2C%0Ainterpretable%20systems%20for%20ultrasound-based%20breast%20cancer%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Deep%2520Transfer%2520Learning%2520for%2520Breast%2520Ultrasound%2520Cancer%250A%2520%2520Detection%253A%2520A%2520Multi-Dataset%2520Study%26entry.906535625%3DMohammad%2520Abbadi%2520and%2520Yassine%2520Himeur%2520and%2520Shadi%2520Atalla%2520and%2520Wathiq%2520Mansoor%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520remains%2520a%2520leading%2520cause%2520of%2520cancer-related%2520mortality%2520among%2520women%250Aworldwide.%2520Ultrasound%2520imaging%252C%2520widely%2520used%2520due%2520to%2520its%2520safety%2520and%250Acost-effectiveness%252C%2520plays%2520a%2520key%2520role%2520in%2520early%2520detection%252C%2520especially%2520in%2520patients%250Awith%2520dense%2520breast%2520tissue.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520study%2520on%2520the%250Aapplication%2520of%2520machine%2520learning%2520and%2520deep%2520learning%2520techniques%2520for%2520breast%2520cancer%250Aclassification%2520using%2520ultrasound%2520images.%2520Using%2520datasets%2520such%2520as%2520BUSI%252C%2520BUS-BRA%252C%250Aand%2520BrEaST-Lesions%2520USG%252C%2520we%2520evaluate%2520classical%2520machine%2520learning%2520models%2520%2528SVM%252C%250AKNN%2529%2520and%2520deep%2520convolutional%2520neural%2520networks%2520%2528ResNet-18%252C%2520EfficientNet-B0%252C%250AGoogLeNet%2529.%2520Experimental%2520results%2520show%2520that%2520ResNet-18%2520achieves%2520the%2520highest%250Aaccuracy%2520%252899.7%2525%2529%2520and%2520perfect%2520sensitivity%2520for%2520malignant%2520lesions.%2520Classical%2520ML%250Amodels%252C%2520though%2520outperformed%2520by%2520CNNs%252C%2520achieve%2520competitive%2520performance%2520when%250Aenhanced%2520with%2520deep%2520feature%2520extraction.%2520Grad-CAM%2520visualizations%2520further%2520improve%250Amodel%2520transparency%2520by%2520highlighting%2520diagnostically%2520relevant%2520image%2520regions.%2520These%250Afindings%2520support%2520the%2520integration%2520of%2520AI-based%2520diagnostic%2520tools%2520into%2520clinical%250Aworkflows%2520and%2520demonstrate%2520the%2520feasibility%2520of%2520deploying%2520high-performing%252C%250Ainterpretable%2520systems%2520for%2520ultrasound-based%2520breast%2520cancer%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Deep%20Transfer%20Learning%20for%20Breast%20Ultrasound%20Cancer%0A%20%20Detection%3A%20A%20Multi-Dataset%20Study&entry.906535625=Mohammad%20Abbadi%20and%20Yassine%20Himeur%20and%20Shadi%20Atalla%20and%20Wathiq%20Mansoor&entry.1292438233=%20%20Breast%20cancer%20remains%20a%20leading%20cause%20of%20cancer-related%20mortality%20among%20women%0Aworldwide.%20Ultrasound%20imaging%2C%20widely%20used%20due%20to%20its%20safety%20and%0Acost-effectiveness%2C%20plays%20a%20key%20role%20in%20early%20detection%2C%20especially%20in%20patients%0Awith%20dense%20breast%20tissue.%20This%20paper%20presents%20a%20comprehensive%20study%20on%20the%0Aapplication%20of%20machine%20learning%20and%20deep%20learning%20techniques%20for%20breast%20cancer%0Aclassification%20using%20ultrasound%20images.%20Using%20datasets%20such%20as%20BUSI%2C%20BUS-BRA%2C%0Aand%20BrEaST-Lesions%20USG%2C%20we%20evaluate%20classical%20machine%20learning%20models%20%28SVM%2C%0AKNN%29%20and%20deep%20convolutional%20neural%20networks%20%28ResNet-18%2C%20EfficientNet-B0%2C%0AGoogLeNet%29.%20Experimental%20results%20show%20that%20ResNet-18%20achieves%20the%20highest%0Aaccuracy%20%2899.7%25%29%20and%20perfect%20sensitivity%20for%20malignant%20lesions.%20Classical%20ML%0Amodels%2C%20though%20outperformed%20by%20CNNs%2C%20achieve%20competitive%20performance%20when%0Aenhanced%20with%20deep%20feature%20extraction.%20Grad-CAM%20visualizations%20further%20improve%0Amodel%20transparency%20by%20highlighting%20diagnostically%20relevant%20image%20regions.%20These%0Afindings%20support%20the%20integration%20of%20AI-based%20diagnostic%20tools%20into%20clinical%0Aworkflows%20and%20demonstrate%20the%20feasibility%20of%20deploying%20high-performing%2C%0Ainterpretable%20systems%20for%20ultrasound-based%20breast%20cancer%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05004v1&entry.124074799=Read"},
{"title": "Efficient Exact Resistance Distance Computation on Small-Treewidth\n  Graphs: a Labelling Approach", "author": "Meihao Liao and Yueyang Pan and Rong-Hua Li and Guoren Wang", "abstract": "  Resistance distance computation is a fundamental problem in graph analysis,\nyet existing random walk-based methods are limited to approximate solutions and\nsuffer from poor efficiency on small-treewidth graphs (e.g., road networks). In\ncontrast, shortest-path distance computation achieves remarkable efficiency on\nsuch graphs by leveraging cut properties and tree decompositions. Motivated by\nthis disparity, we first analyze the cut property of resistance distance. While\na direct generalization proves impractical due to costly matrix operations, we\novercome this limitation by integrating tree decompositions, revealing that the\nresistance distance $r(s,t)$ depends only on labels along the paths from $s$\nand $t$ to the root of the decomposition. This insight enables compact\nlabelling structures. Based on this, we propose \\treeindex, a novel index\nmethod that constructs a resistance distance labelling of size $O(n \\cdot\nh_{\\mathcal{G}})$ in $O(n \\cdot h_{\\mathcal{G}}^2 \\cdot d_{\\max})$ time, where\n$h_{\\mathcal{G}}$ (tree height) and $d_{\\max}$ (maximum degree) behave as small\nconstants in many real-world small-treewidth graphs (e.g., road networks). Our\nlabelling supports exact single-pair queries in $O(h_{\\mathcal{G}})$ time and\nsingle-source queries in $O(n \\cdot h_{\\mathcal{G}})$ time. Extensive\nexperiments show that TreeIndex substantially outperforms state-of-the-art\napproaches. For instance, on the full USA road network, it constructs a $405$\nGB labelling in $7$ hours (single-threaded) and answers exact single-pair\nqueries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the\nfirst exact method scalable to such large graphs.\n", "link": "http://arxiv.org/abs/2509.05129v1", "date": "2025-09-05", "relevancy": 1.9727, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4112}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.394}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Exact%20Resistance%20Distance%20Computation%20on%20Small-Treewidth%0A%20%20Graphs%3A%20a%20Labelling%20Approach&body=Title%3A%20Efficient%20Exact%20Resistance%20Distance%20Computation%20on%20Small-Treewidth%0A%20%20Graphs%3A%20a%20Labelling%20Approach%0AAuthor%3A%20Meihao%20Liao%20and%20Yueyang%20Pan%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Resistance%20distance%20computation%20is%20a%20fundamental%20problem%20in%20graph%20analysis%2C%0Ayet%20existing%20random%20walk-based%20methods%20are%20limited%20to%20approximate%20solutions%20and%0Asuffer%20from%20poor%20efficiency%20on%20small-treewidth%20graphs%20%28e.g.%2C%20road%20networks%29.%20In%0Acontrast%2C%20shortest-path%20distance%20computation%20achieves%20remarkable%20efficiency%20on%0Asuch%20graphs%20by%20leveraging%20cut%20properties%20and%20tree%20decompositions.%20Motivated%20by%0Athis%20disparity%2C%20we%20first%20analyze%20the%20cut%20property%20of%20resistance%20distance.%20While%0Aa%20direct%20generalization%20proves%20impractical%20due%20to%20costly%20matrix%20operations%2C%20we%0Aovercome%20this%20limitation%20by%20integrating%20tree%20decompositions%2C%20revealing%20that%20the%0Aresistance%20distance%20%24r%28s%2Ct%29%24%20depends%20only%20on%20labels%20along%20the%20paths%20from%20%24s%24%0Aand%20%24t%24%20to%20the%20root%20of%20the%20decomposition.%20This%20insight%20enables%20compact%0Alabelling%20structures.%20Based%20on%20this%2C%20we%20propose%20%5Ctreeindex%2C%20a%20novel%20index%0Amethod%20that%20constructs%20a%20resistance%20distance%20labelling%20of%20size%20%24O%28n%20%5Ccdot%0Ah_%7B%5Cmathcal%7BG%7D%7D%29%24%20in%20%24O%28n%20%5Ccdot%20h_%7B%5Cmathcal%7BG%7D%7D%5E2%20%5Ccdot%20d_%7B%5Cmax%7D%29%24%20time%2C%20where%0A%24h_%7B%5Cmathcal%7BG%7D%7D%24%20%28tree%20height%29%20and%20%24d_%7B%5Cmax%7D%24%20%28maximum%20degree%29%20behave%20as%20small%0Aconstants%20in%20many%20real-world%20small-treewidth%20graphs%20%28e.g.%2C%20road%20networks%29.%20Our%0Alabelling%20supports%20exact%20single-pair%20queries%20in%20%24O%28h_%7B%5Cmathcal%7BG%7D%7D%29%24%20time%20and%0Asingle-source%20queries%20in%20%24O%28n%20%5Ccdot%20h_%7B%5Cmathcal%7BG%7D%7D%29%24%20time.%20Extensive%0Aexperiments%20show%20that%20TreeIndex%20substantially%20outperforms%20state-of-the-art%0Aapproaches.%20For%20instance%2C%20on%20the%20full%20USA%20road%20network%2C%20it%20constructs%20a%20%24405%24%0AGB%20labelling%20in%20%247%24%20hours%20%28single-threaded%29%20and%20answers%20exact%20single-pair%0Aqueries%20in%20%2410%5E%7B-3%7D%24%20seconds%20and%20single-source%20queries%20in%20%24190%24%20seconds--the%0Afirst%20exact%20method%20scalable%20to%20such%20large%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Exact%2520Resistance%2520Distance%2520Computation%2520on%2520Small-Treewidth%250A%2520%2520Graphs%253A%2520a%2520Labelling%2520Approach%26entry.906535625%3DMeihao%2520Liao%2520and%2520Yueyang%2520Pan%2520and%2520Rong-Hua%2520Li%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Resistance%2520distance%2520computation%2520is%2520a%2520fundamental%2520problem%2520in%2520graph%2520analysis%252C%250Ayet%2520existing%2520random%2520walk-based%2520methods%2520are%2520limited%2520to%2520approximate%2520solutions%2520and%250Asuffer%2520from%2520poor%2520efficiency%2520on%2520small-treewidth%2520graphs%2520%2528e.g.%252C%2520road%2520networks%2529.%2520In%250Acontrast%252C%2520shortest-path%2520distance%2520computation%2520achieves%2520remarkable%2520efficiency%2520on%250Asuch%2520graphs%2520by%2520leveraging%2520cut%2520properties%2520and%2520tree%2520decompositions.%2520Motivated%2520by%250Athis%2520disparity%252C%2520we%2520first%2520analyze%2520the%2520cut%2520property%2520of%2520resistance%2520distance.%2520While%250Aa%2520direct%2520generalization%2520proves%2520impractical%2520due%2520to%2520costly%2520matrix%2520operations%252C%2520we%250Aovercome%2520this%2520limitation%2520by%2520integrating%2520tree%2520decompositions%252C%2520revealing%2520that%2520the%250Aresistance%2520distance%2520%2524r%2528s%252Ct%2529%2524%2520depends%2520only%2520on%2520labels%2520along%2520the%2520paths%2520from%2520%2524s%2524%250Aand%2520%2524t%2524%2520to%2520the%2520root%2520of%2520the%2520decomposition.%2520This%2520insight%2520enables%2520compact%250Alabelling%2520structures.%2520Based%2520on%2520this%252C%2520we%2520propose%2520%255Ctreeindex%252C%2520a%2520novel%2520index%250Amethod%2520that%2520constructs%2520a%2520resistance%2520distance%2520labelling%2520of%2520size%2520%2524O%2528n%2520%255Ccdot%250Ah_%257B%255Cmathcal%257BG%257D%257D%2529%2524%2520in%2520%2524O%2528n%2520%255Ccdot%2520h_%257B%255Cmathcal%257BG%257D%257D%255E2%2520%255Ccdot%2520d_%257B%255Cmax%257D%2529%2524%2520time%252C%2520where%250A%2524h_%257B%255Cmathcal%257BG%257D%257D%2524%2520%2528tree%2520height%2529%2520and%2520%2524d_%257B%255Cmax%257D%2524%2520%2528maximum%2520degree%2529%2520behave%2520as%2520small%250Aconstants%2520in%2520many%2520real-world%2520small-treewidth%2520graphs%2520%2528e.g.%252C%2520road%2520networks%2529.%2520Our%250Alabelling%2520supports%2520exact%2520single-pair%2520queries%2520in%2520%2524O%2528h_%257B%255Cmathcal%257BG%257D%257D%2529%2524%2520time%2520and%250Asingle-source%2520queries%2520in%2520%2524O%2528n%2520%255Ccdot%2520h_%257B%255Cmathcal%257BG%257D%257D%2529%2524%2520time.%2520Extensive%250Aexperiments%2520show%2520that%2520TreeIndex%2520substantially%2520outperforms%2520state-of-the-art%250Aapproaches.%2520For%2520instance%252C%2520on%2520the%2520full%2520USA%2520road%2520network%252C%2520it%2520constructs%2520a%2520%2524405%2524%250AGB%2520labelling%2520in%2520%25247%2524%2520hours%2520%2528single-threaded%2529%2520and%2520answers%2520exact%2520single-pair%250Aqueries%2520in%2520%252410%255E%257B-3%257D%2524%2520seconds%2520and%2520single-source%2520queries%2520in%2520%2524190%2524%2520seconds--the%250Afirst%2520exact%2520method%2520scalable%2520to%2520such%2520large%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Exact%20Resistance%20Distance%20Computation%20on%20Small-Treewidth%0A%20%20Graphs%3A%20a%20Labelling%20Approach&entry.906535625=Meihao%20Liao%20and%20Yueyang%20Pan%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang&entry.1292438233=%20%20Resistance%20distance%20computation%20is%20a%20fundamental%20problem%20in%20graph%20analysis%2C%0Ayet%20existing%20random%20walk-based%20methods%20are%20limited%20to%20approximate%20solutions%20and%0Asuffer%20from%20poor%20efficiency%20on%20small-treewidth%20graphs%20%28e.g.%2C%20road%20networks%29.%20In%0Acontrast%2C%20shortest-path%20distance%20computation%20achieves%20remarkable%20efficiency%20on%0Asuch%20graphs%20by%20leveraging%20cut%20properties%20and%20tree%20decompositions.%20Motivated%20by%0Athis%20disparity%2C%20we%20first%20analyze%20the%20cut%20property%20of%20resistance%20distance.%20While%0Aa%20direct%20generalization%20proves%20impractical%20due%20to%20costly%20matrix%20operations%2C%20we%0Aovercome%20this%20limitation%20by%20integrating%20tree%20decompositions%2C%20revealing%20that%20the%0Aresistance%20distance%20%24r%28s%2Ct%29%24%20depends%20only%20on%20labels%20along%20the%20paths%20from%20%24s%24%0Aand%20%24t%24%20to%20the%20root%20of%20the%20decomposition.%20This%20insight%20enables%20compact%0Alabelling%20structures.%20Based%20on%20this%2C%20we%20propose%20%5Ctreeindex%2C%20a%20novel%20index%0Amethod%20that%20constructs%20a%20resistance%20distance%20labelling%20of%20size%20%24O%28n%20%5Ccdot%0Ah_%7B%5Cmathcal%7BG%7D%7D%29%24%20in%20%24O%28n%20%5Ccdot%20h_%7B%5Cmathcal%7BG%7D%7D%5E2%20%5Ccdot%20d_%7B%5Cmax%7D%29%24%20time%2C%20where%0A%24h_%7B%5Cmathcal%7BG%7D%7D%24%20%28tree%20height%29%20and%20%24d_%7B%5Cmax%7D%24%20%28maximum%20degree%29%20behave%20as%20small%0Aconstants%20in%20many%20real-world%20small-treewidth%20graphs%20%28e.g.%2C%20road%20networks%29.%20Our%0Alabelling%20supports%20exact%20single-pair%20queries%20in%20%24O%28h_%7B%5Cmathcal%7BG%7D%7D%29%24%20time%20and%0Asingle-source%20queries%20in%20%24O%28n%20%5Ccdot%20h_%7B%5Cmathcal%7BG%7D%7D%29%24%20time.%20Extensive%0Aexperiments%20show%20that%20TreeIndex%20substantially%20outperforms%20state-of-the-art%0Aapproaches.%20For%20instance%2C%20on%20the%20full%20USA%20road%20network%2C%20it%20constructs%20a%20%24405%24%0AGB%20labelling%20in%20%247%24%20hours%20%28single-threaded%29%20and%20answers%20exact%20single-pair%0Aqueries%20in%20%2410%5E%7B-3%7D%24%20seconds%20and%20single-source%20queries%20in%20%24190%24%20seconds--the%0Afirst%20exact%20method%20scalable%20to%20such%20large%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05129v1&entry.124074799=Read"},
{"title": "AutoPDL: Automatic Prompt Optimization for LLM Agents", "author": "Claudio Spiess and Mandana Vaziri and Louis Mandel and Martin Hirzel", "abstract": "  The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points),\nup to 67.5pp, and reveal that selected prompting strategies vary across models\nand tasks.\n", "link": "http://arxiv.org/abs/2504.04365v3", "date": "2025-09-05", "relevancy": 1.9442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPDL%3A%20Automatic%20Prompt%20Optimization%20for%20LLM%20Agents&body=Title%3A%20AutoPDL%3A%20Automatic%20Prompt%20Optimization%20for%20LLM%20Agents%0AAuthor%3A%20Claudio%20Spiess%20and%20Mandana%20Vaziri%20and%20Louis%20Mandel%20and%20Martin%20Hirzel%0AAbstract%3A%20%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20depends%20on%20how%20they%20are%0Aprompted%2C%20with%20choices%20spanning%20both%20the%20high-level%20prompting%20pattern%20%28e.g.%2C%0AZero-Shot%2C%20CoT%2C%20ReAct%2C%20ReWOO%29%20and%20the%20specific%20prompt%20content%20%28instructions%20and%0Afew-shot%20demonstrations%29.%20Manually%20tuning%20this%20combination%20is%20tedious%2C%0Aerror-prone%2C%20and%20specific%20to%20a%20given%20LLM%20and%20task.%20Therefore%2C%20this%20paper%0Aproposes%20AutoPDL%2C%20an%20automated%20approach%20to%20discovering%20good%20LLM%20agent%0Aconfigurations.%20Our%20approach%20frames%20this%20as%20a%20structured%20AutoML%20problem%20over%20a%0Acombinatorial%20space%20of%20agentic%20and%20non-agentic%20prompting%20patterns%20and%0Ademonstrations%2C%20using%20successive%20halving%20to%20efficiently%20navigate%20this%20space.%20We%0Aintroduce%20a%20library%20implementing%20common%20prompting%20patterns%20using%20the%20PDL%20prompt%0Aprogramming%20language.%20AutoPDL%20solutions%20are%20human-readable%2C%20editable%2C%20and%0Aexecutable%20PDL%20programs%20that%20use%20this%20library.%20This%20approach%20also%20enables%0Asource-to-source%20optimization%2C%20allowing%20human-in-the-loop%20refinement%20and%20reuse.%0AEvaluations%20across%20three%20tasks%20and%20seven%20LLMs%20%28ranging%20from%203B%20to%2070B%0Aparameters%29%20show%20consistent%20accuracy%20gains%20%28%249.21%5Cpm15.46%24%20percentage%20points%29%2C%0Aup%20to%2067.5pp%2C%20and%20reveal%20that%20selected%20prompting%20strategies%20vary%20across%20models%0Aand%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04365v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPDL%253A%2520Automatic%2520Prompt%2520Optimization%2520for%2520LLM%2520Agents%26entry.906535625%3DClaudio%2520Spiess%2520and%2520Mandana%2520Vaziri%2520and%2520Louis%2520Mandel%2520and%2520Martin%2520Hirzel%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520depends%2520on%2520how%2520they%2520are%250Aprompted%252C%2520with%2520choices%2520spanning%2520both%2520the%2520high-level%2520prompting%2520pattern%2520%2528e.g.%252C%250AZero-Shot%252C%2520CoT%252C%2520ReAct%252C%2520ReWOO%2529%2520and%2520the%2520specific%2520prompt%2520content%2520%2528instructions%2520and%250Afew-shot%2520demonstrations%2529.%2520Manually%2520tuning%2520this%2520combination%2520is%2520tedious%252C%250Aerror-prone%252C%2520and%2520specific%2520to%2520a%2520given%2520LLM%2520and%2520task.%2520Therefore%252C%2520this%2520paper%250Aproposes%2520AutoPDL%252C%2520an%2520automated%2520approach%2520to%2520discovering%2520good%2520LLM%2520agent%250Aconfigurations.%2520Our%2520approach%2520frames%2520this%2520as%2520a%2520structured%2520AutoML%2520problem%2520over%2520a%250Acombinatorial%2520space%2520of%2520agentic%2520and%2520non-agentic%2520prompting%2520patterns%2520and%250Ademonstrations%252C%2520using%2520successive%2520halving%2520to%2520efficiently%2520navigate%2520this%2520space.%2520We%250Aintroduce%2520a%2520library%2520implementing%2520common%2520prompting%2520patterns%2520using%2520the%2520PDL%2520prompt%250Aprogramming%2520language.%2520AutoPDL%2520solutions%2520are%2520human-readable%252C%2520editable%252C%2520and%250Aexecutable%2520PDL%2520programs%2520that%2520use%2520this%2520library.%2520This%2520approach%2520also%2520enables%250Asource-to-source%2520optimization%252C%2520allowing%2520human-in-the-loop%2520refinement%2520and%2520reuse.%250AEvaluations%2520across%2520three%2520tasks%2520and%2520seven%2520LLMs%2520%2528ranging%2520from%25203B%2520to%252070B%250Aparameters%2529%2520show%2520consistent%2520accuracy%2520gains%2520%2528%25249.21%255Cpm15.46%2524%2520percentage%2520points%2529%252C%250Aup%2520to%252067.5pp%252C%2520and%2520reveal%2520that%2520selected%2520prompting%2520strategies%2520vary%2520across%2520models%250Aand%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04365v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPDL%3A%20Automatic%20Prompt%20Optimization%20for%20LLM%20Agents&entry.906535625=Claudio%20Spiess%20and%20Mandana%20Vaziri%20and%20Louis%20Mandel%20and%20Martin%20Hirzel&entry.1292438233=%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20depends%20on%20how%20they%20are%0Aprompted%2C%20with%20choices%20spanning%20both%20the%20high-level%20prompting%20pattern%20%28e.g.%2C%0AZero-Shot%2C%20CoT%2C%20ReAct%2C%20ReWOO%29%20and%20the%20specific%20prompt%20content%20%28instructions%20and%0Afew-shot%20demonstrations%29.%20Manually%20tuning%20this%20combination%20is%20tedious%2C%0Aerror-prone%2C%20and%20specific%20to%20a%20given%20LLM%20and%20task.%20Therefore%2C%20this%20paper%0Aproposes%20AutoPDL%2C%20an%20automated%20approach%20to%20discovering%20good%20LLM%20agent%0Aconfigurations.%20Our%20approach%20frames%20this%20as%20a%20structured%20AutoML%20problem%20over%20a%0Acombinatorial%20space%20of%20agentic%20and%20non-agentic%20prompting%20patterns%20and%0Ademonstrations%2C%20using%20successive%20halving%20to%20efficiently%20navigate%20this%20space.%20We%0Aintroduce%20a%20library%20implementing%20common%20prompting%20patterns%20using%20the%20PDL%20prompt%0Aprogramming%20language.%20AutoPDL%20solutions%20are%20human-readable%2C%20editable%2C%20and%0Aexecutable%20PDL%20programs%20that%20use%20this%20library.%20This%20approach%20also%20enables%0Asource-to-source%20optimization%2C%20allowing%20human-in-the-loop%20refinement%20and%20reuse.%0AEvaluations%20across%20three%20tasks%20and%20seven%20LLMs%20%28ranging%20from%203B%20to%2070B%0Aparameters%29%20show%20consistent%20accuracy%20gains%20%28%249.21%5Cpm15.46%24%20percentage%20points%29%2C%0Aup%20to%2067.5pp%2C%20and%20reveal%20that%20selected%20prompting%20strategies%20vary%20across%20models%0Aand%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04365v3&entry.124074799=Read"},
{"title": "Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map", "author": "J\u00e9rome Eertmans and Enrico Maria Vitucci and Vittorio Degli-Esposti and Laurent Jacques and Claude Oestges", "abstract": "  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n", "link": "http://arxiv.org/abs/2410.14535v6", "date": "2025-09-05", "relevancy": 1.942, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map&body=Title%3A%20Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map%0AAuthor%3A%20J%C3%A9rome%20Eertmans%20and%20Enrico%20Maria%20Vitucci%20and%20Vittorio%20Degli-Esposti%20and%20Laurent%20Jacques%20and%20Claude%20Oestges%0AAbstract%3A%20%20%20With%20the%20increasing%20presence%20of%20dynamic%20scenarios%2C%20such%20as%20Vehicle-to-Vehicle%0Acommunications%2C%20radio%20propagation%20modeling%20tools%20must%20adapt%20to%20the%20rapidly%0Achanging%20nature%20of%20the%20radio%20channel.%20Recently%2C%20both%20Differentiable%20and%20Dynamic%0ARay%20Tracing%20frameworks%20have%20emerged%20to%20address%20these%20challenges.%20However%2C%20there%0Ais%20often%20confusion%20about%20how%20these%20approaches%20differ%20and%20which%20one%20should%20be%0Aused%20in%20specific%20contexts.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20these%20two%0Atechniques%20and%20a%20comparative%20analysis%20against%20two%20state-of-the-art%20tools%3A%0A3DSCAT%20from%20UniBo%20and%20Sionna%20from%20NVIDIA.%20To%20provide%20a%20more%20precise%0Acharacterization%20of%20the%20scope%20of%20these%20methods%2C%20we%20introduce%20a%20novel%0Asimulation-based%20metric%2C%20the%20Multipath%20Lifetime%20Map%2C%20which%20enables%20the%0Aevaluation%20of%20spatial%20and%20temporal%20coherence%20in%20radio%20channels%20only%20based%20on%0Athe%20geometrical%20description%20of%20the%20environment.%20Finally%2C%20our%20metrics%20are%0Aevaluated%20on%20a%20classic%20urban%20street%20canyon%20scenario%2C%20yielding%20similar%20results%0Ato%20those%20obtained%20from%20measurement%20campaigns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14535v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Differentiable%2520and%2520Dynamic%2520Ray%2520Tracing%253A%2520Introducing%2520the%250A%2520%2520Multipath%2520Lifetime%2520Map%26entry.906535625%3DJ%25C3%25A9rome%2520Eertmans%2520and%2520Enrico%2520Maria%2520Vitucci%2520and%2520Vittorio%2520Degli-Esposti%2520and%2520Laurent%2520Jacques%2520and%2520Claude%2520Oestges%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520presence%2520of%2520dynamic%2520scenarios%252C%2520such%2520as%2520Vehicle-to-Vehicle%250Acommunications%252C%2520radio%2520propagation%2520modeling%2520tools%2520must%2520adapt%2520to%2520the%2520rapidly%250Achanging%2520nature%2520of%2520the%2520radio%2520channel.%2520Recently%252C%2520both%2520Differentiable%2520and%2520Dynamic%250ARay%2520Tracing%2520frameworks%2520have%2520emerged%2520to%2520address%2520these%2520challenges.%2520However%252C%2520there%250Ais%2520often%2520confusion%2520about%2520how%2520these%2520approaches%2520differ%2520and%2520which%2520one%2520should%2520be%250Aused%2520in%2520specific%2520contexts.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520overview%2520of%2520these%2520two%250Atechniques%2520and%2520a%2520comparative%2520analysis%2520against%2520two%2520state-of-the-art%2520tools%253A%250A3DSCAT%2520from%2520UniBo%2520and%2520Sionna%2520from%2520NVIDIA.%2520To%2520provide%2520a%2520more%2520precise%250Acharacterization%2520of%2520the%2520scope%2520of%2520these%2520methods%252C%2520we%2520introduce%2520a%2520novel%250Asimulation-based%2520metric%252C%2520the%2520Multipath%2520Lifetime%2520Map%252C%2520which%2520enables%2520the%250Aevaluation%2520of%2520spatial%2520and%2520temporal%2520coherence%2520in%2520radio%2520channels%2520only%2520based%2520on%250Athe%2520geometrical%2520description%2520of%2520the%2520environment.%2520Finally%252C%2520our%2520metrics%2520are%250Aevaluated%2520on%2520a%2520classic%2520urban%2520street%2520canyon%2520scenario%252C%2520yielding%2520similar%2520results%250Ato%2520those%2520obtained%2520from%2520measurement%2520campaigns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14535v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map&entry.906535625=J%C3%A9rome%20Eertmans%20and%20Enrico%20Maria%20Vitucci%20and%20Vittorio%20Degli-Esposti%20and%20Laurent%20Jacques%20and%20Claude%20Oestges&entry.1292438233=%20%20With%20the%20increasing%20presence%20of%20dynamic%20scenarios%2C%20such%20as%20Vehicle-to-Vehicle%0Acommunications%2C%20radio%20propagation%20modeling%20tools%20must%20adapt%20to%20the%20rapidly%0Achanging%20nature%20of%20the%20radio%20channel.%20Recently%2C%20both%20Differentiable%20and%20Dynamic%0ARay%20Tracing%20frameworks%20have%20emerged%20to%20address%20these%20challenges.%20However%2C%20there%0Ais%20often%20confusion%20about%20how%20these%20approaches%20differ%20and%20which%20one%20should%20be%0Aused%20in%20specific%20contexts.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20these%20two%0Atechniques%20and%20a%20comparative%20analysis%20against%20two%20state-of-the-art%20tools%3A%0A3DSCAT%20from%20UniBo%20and%20Sionna%20from%20NVIDIA.%20To%20provide%20a%20more%20precise%0Acharacterization%20of%20the%20scope%20of%20these%20methods%2C%20we%20introduce%20a%20novel%0Asimulation-based%20metric%2C%20the%20Multipath%20Lifetime%20Map%2C%20which%20enables%20the%0Aevaluation%20of%20spatial%20and%20temporal%20coherence%20in%20radio%20channels%20only%20based%20on%0Athe%20geometrical%20description%20of%20the%20environment.%20Finally%2C%20our%20metrics%20are%0Aevaluated%20on%20a%20classic%20urban%20street%20canyon%20scenario%2C%20yielding%20similar%20results%0Ato%20those%20obtained%20from%20measurement%20campaigns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14535v6&entry.124074799=Read"},
{"title": "STADE: Standard Deviation as a Pruning Metric", "author": "Diego Coello de Portugal Mecke and Haya Alyoussef and Maximilian Stubbemann and Ilia Koloiarov and Tom Hanika and Lars Schmidt-Thieme", "abstract": "  Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/\n", "link": "http://arxiv.org/abs/2503.22451v2", "date": "2025-09-05", "relevancy": 1.9256, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STADE%3A%20Standard%20Deviation%20as%20a%20Pruning%20Metric&body=Title%3A%20STADE%3A%20Standard%20Deviation%20as%20a%20Pruning%20Metric%0AAuthor%3A%20Diego%20Coello%20de%20Portugal%20Mecke%20and%20Haya%20Alyoussef%20and%20Maximilian%20Stubbemann%20and%20Ilia%20Koloiarov%20and%20Tom%20Hanika%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20very%20widespread%20and%20are%0Aused%20to%20solve%20a%20wide%20variety%20of%20tasks.%20To%20successfully%20handle%20these%20tasks%2C%20LLMs%0Arequire%20longer%20training%20times%20and%20larger%20model%20sizes.%20This%20makes%20LLMs%20ideal%0Acandidates%20for%20pruning%20methods%20that%20reduce%20computational%20demands%20while%0Amaintaining%20performance.%20Previous%20methods%20require%20a%20retraining%20phase%20after%0Apruning%20to%20maintain%20the%20original%20model%27s%20performance.%20However%2C%20state-of-the-art%0Apruning%20methods%2C%20such%20as%20Wanda%2C%20prune%20the%20model%20without%20retraining%2C%20making%20the%0Apruning%20process%20faster%20and%20more%20efficient.%20Building%20upon%20Wanda%27s%20work%2C%20this%0Astudy%20provides%20a%20theoretical%20explanation%20of%20why%20the%20method%20is%20effective%20and%0Aleverages%20these%20insights%20to%20enhance%20the%20pruning%20process.%20Specifically%2C%20a%0Atheoretical%20analysis%20of%20the%20pruning%20problem%20reveals%20a%20common%20scenario%20in%0AMachine%20Learning%20where%20Wanda%20is%20the%20optimal%20pruning%20method.%20Furthermore%2C%20this%0Aanalysis%20is%20extended%20to%20cases%20where%20Wanda%20is%20no%20longer%20optimal%2C%20leading%20to%20the%0Adevelopment%20of%20a%20new%20method%2C%20STADE%2C%20based%20on%20the%20standard%20deviation%20of%20the%0Ainput.%20From%20a%20theoretical%20standpoint%2C%20STADE%20demonstrates%20better%20generality%0Aacross%20different%20scenarios.%20Finally%2C%20extensive%20experiments%20on%20Llama%20and%20Open%0APre-trained%20Transformers%20%28OPT%29%20models%20validate%20these%20theoretical%20findings%2C%0Ashowing%20that%20depending%20on%20the%20training%20conditions%2C%20Wanda%27s%20optimal%20performance%0Avaries%20as%20predicted%20by%20the%20theoretical%20framework.%20These%20insights%20contribute%20to%0Aa%20more%20robust%20understanding%20of%20pruning%20strategies%20and%20their%20practical%0Aimplications.%20Code%20is%20available%20at%3A%20https%3A//github.com/Coello-dev/STADE/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTADE%253A%2520Standard%2520Deviation%2520as%2520a%2520Pruning%2520Metric%26entry.906535625%3DDiego%2520Coello%2520de%2520Portugal%2520Mecke%2520and%2520Haya%2520Alyoussef%2520and%2520Maximilian%2520Stubbemann%2520and%2520Ilia%2520Koloiarov%2520and%2520Tom%2520Hanika%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520very%2520widespread%2520and%2520are%250Aused%2520to%2520solve%2520a%2520wide%2520variety%2520of%2520tasks.%2520To%2520successfully%2520handle%2520these%2520tasks%252C%2520LLMs%250Arequire%2520longer%2520training%2520times%2520and%2520larger%2520model%2520sizes.%2520This%2520makes%2520LLMs%2520ideal%250Acandidates%2520for%2520pruning%2520methods%2520that%2520reduce%2520computational%2520demands%2520while%250Amaintaining%2520performance.%2520Previous%2520methods%2520require%2520a%2520retraining%2520phase%2520after%250Apruning%2520to%2520maintain%2520the%2520original%2520model%2527s%2520performance.%2520However%252C%2520state-of-the-art%250Apruning%2520methods%252C%2520such%2520as%2520Wanda%252C%2520prune%2520the%2520model%2520without%2520retraining%252C%2520making%2520the%250Apruning%2520process%2520faster%2520and%2520more%2520efficient.%2520Building%2520upon%2520Wanda%2527s%2520work%252C%2520this%250Astudy%2520provides%2520a%2520theoretical%2520explanation%2520of%2520why%2520the%2520method%2520is%2520effective%2520and%250Aleverages%2520these%2520insights%2520to%2520enhance%2520the%2520pruning%2520process.%2520Specifically%252C%2520a%250Atheoretical%2520analysis%2520of%2520the%2520pruning%2520problem%2520reveals%2520a%2520common%2520scenario%2520in%250AMachine%2520Learning%2520where%2520Wanda%2520is%2520the%2520optimal%2520pruning%2520method.%2520Furthermore%252C%2520this%250Aanalysis%2520is%2520extended%2520to%2520cases%2520where%2520Wanda%2520is%2520no%2520longer%2520optimal%252C%2520leading%2520to%2520the%250Adevelopment%2520of%2520a%2520new%2520method%252C%2520STADE%252C%2520based%2520on%2520the%2520standard%2520deviation%2520of%2520the%250Ainput.%2520From%2520a%2520theoretical%2520standpoint%252C%2520STADE%2520demonstrates%2520better%2520generality%250Aacross%2520different%2520scenarios.%2520Finally%252C%2520extensive%2520experiments%2520on%2520Llama%2520and%2520Open%250APre-trained%2520Transformers%2520%2528OPT%2529%2520models%2520validate%2520these%2520theoretical%2520findings%252C%250Ashowing%2520that%2520depending%2520on%2520the%2520training%2520conditions%252C%2520Wanda%2527s%2520optimal%2520performance%250Avaries%2520as%2520predicted%2520by%2520the%2520theoretical%2520framework.%2520These%2520insights%2520contribute%2520to%250Aa%2520more%2520robust%2520understanding%2520of%2520pruning%2520strategies%2520and%2520their%2520practical%250Aimplications.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/Coello-dev/STADE/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STADE%3A%20Standard%20Deviation%20as%20a%20Pruning%20Metric&entry.906535625=Diego%20Coello%20de%20Portugal%20Mecke%20and%20Haya%20Alyoussef%20and%20Maximilian%20Stubbemann%20and%20Ilia%20Koloiarov%20and%20Tom%20Hanika%20and%20Lars%20Schmidt-Thieme&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20very%20widespread%20and%20are%0Aused%20to%20solve%20a%20wide%20variety%20of%20tasks.%20To%20successfully%20handle%20these%20tasks%2C%20LLMs%0Arequire%20longer%20training%20times%20and%20larger%20model%20sizes.%20This%20makes%20LLMs%20ideal%0Acandidates%20for%20pruning%20methods%20that%20reduce%20computational%20demands%20while%0Amaintaining%20performance.%20Previous%20methods%20require%20a%20retraining%20phase%20after%0Apruning%20to%20maintain%20the%20original%20model%27s%20performance.%20However%2C%20state-of-the-art%0Apruning%20methods%2C%20such%20as%20Wanda%2C%20prune%20the%20model%20without%20retraining%2C%20making%20the%0Apruning%20process%20faster%20and%20more%20efficient.%20Building%20upon%20Wanda%27s%20work%2C%20this%0Astudy%20provides%20a%20theoretical%20explanation%20of%20why%20the%20method%20is%20effective%20and%0Aleverages%20these%20insights%20to%20enhance%20the%20pruning%20process.%20Specifically%2C%20a%0Atheoretical%20analysis%20of%20the%20pruning%20problem%20reveals%20a%20common%20scenario%20in%0AMachine%20Learning%20where%20Wanda%20is%20the%20optimal%20pruning%20method.%20Furthermore%2C%20this%0Aanalysis%20is%20extended%20to%20cases%20where%20Wanda%20is%20no%20longer%20optimal%2C%20leading%20to%20the%0Adevelopment%20of%20a%20new%20method%2C%20STADE%2C%20based%20on%20the%20standard%20deviation%20of%20the%0Ainput.%20From%20a%20theoretical%20standpoint%2C%20STADE%20demonstrates%20better%20generality%0Aacross%20different%20scenarios.%20Finally%2C%20extensive%20experiments%20on%20Llama%20and%20Open%0APre-trained%20Transformers%20%28OPT%29%20models%20validate%20these%20theoretical%20findings%2C%0Ashowing%20that%20depending%20on%20the%20training%20conditions%2C%20Wanda%27s%20optimal%20performance%0Avaries%20as%20predicted%20by%20the%20theoretical%20framework.%20These%20insights%20contribute%20to%0Aa%20more%20robust%20understanding%20of%20pruning%20strategies%20and%20their%20practical%0Aimplications.%20Code%20is%20available%20at%3A%20https%3A//github.com/Coello-dev/STADE/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22451v2&entry.124074799=Read"},
{"title": "Q-learning with Posterior Sampling", "author": "Priyank Agrawal and Shipra Agrawal and Azmat Azati", "abstract": "  Bayesian posterior sampling techniques have demonstrated superior empirical\nperformance in many exploration-exploitation settings. However, their\ntheoretical analysis remains a challenge, especially in complex settings like\nreinforcement learning. In this paper, we introduce Q-Learning with Posterior\nSampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian\nposteriors on Q-values for exploration, akin to the popular Thompson Sampling\nalgorithm in the multi-armed bandit setting. We show that in the tabular\nepisodic MDP setting, PSQL achieves a regret bound of $\\tilde\nO(H^2\\sqrt{SAT})$, closely matching the known lower bound of\n$\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in\nthe underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the\nnumber of episodes and $H$ being the planning horizon. Our work provides\nseveral new technical insights into the core challenges in combining posterior\nsampling with dynamic programming and TD-learning-based RL algorithms, along\nwith novel ideas for resolving those difficulties. We hope this will form a\nstarting point for analyzing this efficient and important algorithmic technique\nin even more complex RL settings.\n", "link": "http://arxiv.org/abs/2506.00917v2", "date": "2025-09-05", "relevancy": 1.9201, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4963}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4823}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-learning%20with%20Posterior%20Sampling&body=Title%3A%20Q-learning%20with%20Posterior%20Sampling%0AAuthor%3A%20Priyank%20Agrawal%20and%20Shipra%20Agrawal%20and%20Azmat%20Azati%0AAbstract%3A%20%20%20Bayesian%20posterior%20sampling%20techniques%20have%20demonstrated%20superior%20empirical%0Aperformance%20in%20many%20exploration-exploitation%20settings.%20However%2C%20their%0Atheoretical%20analysis%20remains%20a%20challenge%2C%20especially%20in%20complex%20settings%20like%0Areinforcement%20learning.%20In%20this%20paper%2C%20we%20introduce%20Q-Learning%20with%20Posterior%0ASampling%20%28PSQL%29%2C%20a%20simple%20Q-learning-based%20algorithm%20that%20uses%20Gaussian%0Aposteriors%20on%20Q-values%20for%20exploration%2C%20akin%20to%20the%20popular%20Thompson%20Sampling%0Aalgorithm%20in%20the%20multi-armed%20bandit%20setting.%20We%20show%20that%20in%20the%20tabular%0Aepisodic%20MDP%20setting%2C%20PSQL%20achieves%20a%20regret%20bound%20of%20%24%5Ctilde%0AO%28H%5E2%5Csqrt%7BSAT%7D%29%24%2C%20closely%20matching%20the%20known%20lower%20bound%20of%0A%24%5COmega%28H%5Csqrt%7BSAT%7D%29%24.%20Here%2C%20S%2C%20A%20denote%20the%20number%20of%20states%20and%20actions%20in%0Athe%20underlying%20Markov%20Decision%20Process%20%28MDP%29%2C%20and%20%24T%3DKH%24%20with%20%24K%24%20being%20the%0Anumber%20of%20episodes%20and%20%24H%24%20being%20the%20planning%20horizon.%20Our%20work%20provides%0Aseveral%20new%20technical%20insights%20into%20the%20core%20challenges%20in%20combining%20posterior%0Asampling%20with%20dynamic%20programming%20and%20TD-learning-based%20RL%20algorithms%2C%20along%0Awith%20novel%20ideas%20for%20resolving%20those%20difficulties.%20We%20hope%20this%20will%20form%20a%0Astarting%20point%20for%20analyzing%20this%20efficient%20and%20important%20algorithmic%20technique%0Ain%20even%20more%20complex%20RL%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-learning%2520with%2520Posterior%2520Sampling%26entry.906535625%3DPriyank%2520Agrawal%2520and%2520Shipra%2520Agrawal%2520and%2520Azmat%2520Azati%26entry.1292438233%3D%2520%2520Bayesian%2520posterior%2520sampling%2520techniques%2520have%2520demonstrated%2520superior%2520empirical%250Aperformance%2520in%2520many%2520exploration-exploitation%2520settings.%2520However%252C%2520their%250Atheoretical%2520analysis%2520remains%2520a%2520challenge%252C%2520especially%2520in%2520complex%2520settings%2520like%250Areinforcement%2520learning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Q-Learning%2520with%2520Posterior%250ASampling%2520%2528PSQL%2529%252C%2520a%2520simple%2520Q-learning-based%2520algorithm%2520that%2520uses%2520Gaussian%250Aposteriors%2520on%2520Q-values%2520for%2520exploration%252C%2520akin%2520to%2520the%2520popular%2520Thompson%2520Sampling%250Aalgorithm%2520in%2520the%2520multi-armed%2520bandit%2520setting.%2520We%2520show%2520that%2520in%2520the%2520tabular%250Aepisodic%2520MDP%2520setting%252C%2520PSQL%2520achieves%2520a%2520regret%2520bound%2520of%2520%2524%255Ctilde%250AO%2528H%255E2%255Csqrt%257BSAT%257D%2529%2524%252C%2520closely%2520matching%2520the%2520known%2520lower%2520bound%2520of%250A%2524%255COmega%2528H%255Csqrt%257BSAT%257D%2529%2524.%2520Here%252C%2520S%252C%2520A%2520denote%2520the%2520number%2520of%2520states%2520and%2520actions%2520in%250Athe%2520underlying%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%252C%2520and%2520%2524T%253DKH%2524%2520with%2520%2524K%2524%2520being%2520the%250Anumber%2520of%2520episodes%2520and%2520%2524H%2524%2520being%2520the%2520planning%2520horizon.%2520Our%2520work%2520provides%250Aseveral%2520new%2520technical%2520insights%2520into%2520the%2520core%2520challenges%2520in%2520combining%2520posterior%250Asampling%2520with%2520dynamic%2520programming%2520and%2520TD-learning-based%2520RL%2520algorithms%252C%2520along%250Awith%2520novel%2520ideas%2520for%2520resolving%2520those%2520difficulties.%2520We%2520hope%2520this%2520will%2520form%2520a%250Astarting%2520point%2520for%2520analyzing%2520this%2520efficient%2520and%2520important%2520algorithmic%2520technique%250Ain%2520even%2520more%2520complex%2520RL%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-learning%20with%20Posterior%20Sampling&entry.906535625=Priyank%20Agrawal%20and%20Shipra%20Agrawal%20and%20Azmat%20Azati&entry.1292438233=%20%20Bayesian%20posterior%20sampling%20techniques%20have%20demonstrated%20superior%20empirical%0Aperformance%20in%20many%20exploration-exploitation%20settings.%20However%2C%20their%0Atheoretical%20analysis%20remains%20a%20challenge%2C%20especially%20in%20complex%20settings%20like%0Areinforcement%20learning.%20In%20this%20paper%2C%20we%20introduce%20Q-Learning%20with%20Posterior%0ASampling%20%28PSQL%29%2C%20a%20simple%20Q-learning-based%20algorithm%20that%20uses%20Gaussian%0Aposteriors%20on%20Q-values%20for%20exploration%2C%20akin%20to%20the%20popular%20Thompson%20Sampling%0Aalgorithm%20in%20the%20multi-armed%20bandit%20setting.%20We%20show%20that%20in%20the%20tabular%0Aepisodic%20MDP%20setting%2C%20PSQL%20achieves%20a%20regret%20bound%20of%20%24%5Ctilde%0AO%28H%5E2%5Csqrt%7BSAT%7D%29%24%2C%20closely%20matching%20the%20known%20lower%20bound%20of%0A%24%5COmega%28H%5Csqrt%7BSAT%7D%29%24.%20Here%2C%20S%2C%20A%20denote%20the%20number%20of%20states%20and%20actions%20in%0Athe%20underlying%20Markov%20Decision%20Process%20%28MDP%29%2C%20and%20%24T%3DKH%24%20with%20%24K%24%20being%20the%0Anumber%20of%20episodes%20and%20%24H%24%20being%20the%20planning%20horizon.%20Our%20work%20provides%0Aseveral%20new%20technical%20insights%20into%20the%20core%20challenges%20in%20combining%20posterior%0Asampling%20with%20dynamic%20programming%20and%20TD-learning-based%20RL%20algorithms%2C%20along%0Awith%20novel%20ideas%20for%20resolving%20those%20difficulties.%20We%20hope%20this%20will%20form%20a%0Astarting%20point%20for%20analyzing%20this%20efficient%20and%20important%20algorithmic%20technique%0Ain%20even%20more%20complex%20RL%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00917v2&entry.124074799=Read"},
{"title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack\n  in Federated Learning", "author": "Francesco Diana and Andr\u00e9 Nusser and Chuan Xu and Giovanni Neglia", "abstract": "  Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.\n", "link": "http://arxiv.org/abs/2505.10264v2", "date": "2025-09-05", "relevancy": 1.9136, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4824}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4766}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cutting%20Through%20Privacy%3A%20A%20Hyperplane-Based%20Data%20Reconstruction%20Attack%0A%20%20in%20Federated%20Learning&body=Title%3A%20Cutting%20Through%20Privacy%3A%20A%20Hyperplane-Based%20Data%20Reconstruction%20Attack%0A%20%20in%20Federated%20Learning%0AAuthor%3A%20Francesco%20Diana%20and%20Andr%C3%A9%20Nusser%20and%20Chuan%20Xu%20and%20Giovanni%20Neglia%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20machine%20learning%0Amodels%20across%20distributed%20clients%20without%20sharing%20raw%20data%2C%20ostensibly%0Apreserving%20data%20privacy.%20Nevertheless%2C%20recent%20studies%20have%20revealed%20critical%0Avulnerabilities%20in%20FL%2C%20showing%20that%20a%20malicious%20central%20server%20can%20manipulate%0Amodel%20updates%20to%20reconstruct%20clients%27%20private%20training%20data.%20Existing%20data%0Areconstruction%20attacks%20have%20important%20limitations%3A%20they%20often%20rely%20on%0Aassumptions%20about%20the%20clients%27%20data%20distribution%20or%20their%20efficiency%0Asignificantly%20degrades%20when%20batch%20sizes%20exceed%20just%20a%20few%20tens%20of%20samples.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20data%20reconstruction%20attack%20that%20overcomes%0Athese%20limitations.%20Our%20method%20leverages%20a%20new%20geometric%20perspective%20on%20fully%0Aconnected%20layers%20to%20craft%20malicious%20model%20parameters%2C%20enabling%20the%20perfect%0Arecovery%20of%20arbitrarily%20large%20data%20batches%20in%20classification%20tasks%20without%20any%0Aprior%20knowledge%20of%20clients%27%20data.%20Through%20extensive%20experiments%20on%20both%20image%0Aand%20tabular%20datasets%2C%20we%20demonstrate%20that%20our%20attack%20outperforms%20existing%0Amethods%20and%20achieves%20perfect%20reconstruction%20of%20data%20batches%20two%20orders%20of%0Amagnitude%20larger%20than%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCutting%2520Through%2520Privacy%253A%2520A%2520Hyperplane-Based%2520Data%2520Reconstruction%2520Attack%250A%2520%2520in%2520Federated%2520Learning%26entry.906535625%3DFrancesco%2520Diana%2520and%2520Andr%25C3%25A9%2520Nusser%2520and%2520Chuan%2520Xu%2520and%2520Giovanni%2520Neglia%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520of%2520machine%2520learning%250Amodels%2520across%2520distributed%2520clients%2520without%2520sharing%2520raw%2520data%252C%2520ostensibly%250Apreserving%2520data%2520privacy.%2520Nevertheless%252C%2520recent%2520studies%2520have%2520revealed%2520critical%250Avulnerabilities%2520in%2520FL%252C%2520showing%2520that%2520a%2520malicious%2520central%2520server%2520can%2520manipulate%250Amodel%2520updates%2520to%2520reconstruct%2520clients%2527%2520private%2520training%2520data.%2520Existing%2520data%250Areconstruction%2520attacks%2520have%2520important%2520limitations%253A%2520they%2520often%2520rely%2520on%250Aassumptions%2520about%2520the%2520clients%2527%2520data%2520distribution%2520or%2520their%2520efficiency%250Asignificantly%2520degrades%2520when%2520batch%2520sizes%2520exceed%2520just%2520a%2520few%2520tens%2520of%2520samples.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520data%2520reconstruction%2520attack%2520that%2520overcomes%250Athese%2520limitations.%2520Our%2520method%2520leverages%2520a%2520new%2520geometric%2520perspective%2520on%2520fully%250Aconnected%2520layers%2520to%2520craft%2520malicious%2520model%2520parameters%252C%2520enabling%2520the%2520perfect%250Arecovery%2520of%2520arbitrarily%2520large%2520data%2520batches%2520in%2520classification%2520tasks%2520without%2520any%250Aprior%2520knowledge%2520of%2520clients%2527%2520data.%2520Through%2520extensive%2520experiments%2520on%2520both%2520image%250Aand%2520tabular%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%2520attack%2520outperforms%2520existing%250Amethods%2520and%2520achieves%2520perfect%2520reconstruction%2520of%2520data%2520batches%2520two%2520orders%2520of%250Amagnitude%2520larger%2520than%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cutting%20Through%20Privacy%3A%20A%20Hyperplane-Based%20Data%20Reconstruction%20Attack%0A%20%20in%20Federated%20Learning&entry.906535625=Francesco%20Diana%20and%20Andr%C3%A9%20Nusser%20and%20Chuan%20Xu%20and%20Giovanni%20Neglia&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20machine%20learning%0Amodels%20across%20distributed%20clients%20without%20sharing%20raw%20data%2C%20ostensibly%0Apreserving%20data%20privacy.%20Nevertheless%2C%20recent%20studies%20have%20revealed%20critical%0Avulnerabilities%20in%20FL%2C%20showing%20that%20a%20malicious%20central%20server%20can%20manipulate%0Amodel%20updates%20to%20reconstruct%20clients%27%20private%20training%20data.%20Existing%20data%0Areconstruction%20attacks%20have%20important%20limitations%3A%20they%20often%20rely%20on%0Aassumptions%20about%20the%20clients%27%20data%20distribution%20or%20their%20efficiency%0Asignificantly%20degrades%20when%20batch%20sizes%20exceed%20just%20a%20few%20tens%20of%20samples.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20data%20reconstruction%20attack%20that%20overcomes%0Athese%20limitations.%20Our%20method%20leverages%20a%20new%20geometric%20perspective%20on%20fully%0Aconnected%20layers%20to%20craft%20malicious%20model%20parameters%2C%20enabling%20the%20perfect%0Arecovery%20of%20arbitrarily%20large%20data%20batches%20in%20classification%20tasks%20without%20any%0Aprior%20knowledge%20of%20clients%27%20data.%20Through%20extensive%20experiments%20on%20both%20image%0Aand%20tabular%20datasets%2C%20we%20demonstrate%20that%20our%20attack%20outperforms%20existing%0Amethods%20and%20achieves%20perfect%20reconstruction%20of%20data%20batches%20two%20orders%20of%0Amagnitude%20larger%20than%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10264v2&entry.124074799=Read"},
{"title": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of\n  Manufactured Solutions", "author": "Rafael Bischof and Michal Piovar\u010di and Michael A. Kraus and Siddhartha Mishra and Bernd Bickel", "abstract": "  We present HyPINO, a multi-physics neural operator designed for zero-shot\ngeneralization across a broad class of parametric PDEs without requiring\ntask-specific fine-tuning. Our approach combines a Swin Transformer-based\nhypernetwork with mixed supervision: (i) labeled data from analytical solutions\ngenerated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled\nsamples optimized using physics-informed objectives. The model maps PDE\nparametrizations to target Physics-Informed Neural Networks (PINNs) and can\nhandle linear elliptic, hyperbolic, and parabolic equations in two dimensions\nwith varying source terms, geometries, and mixed Dirichlet/Neumann boundary\nconditions, including interior boundaries. HyPINO achieves strong zero-shot\naccuracy on seven benchmark problems from PINN literature, outperforming\nU-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we\nintroduce an iterative refinement procedure that compares the physics of the\ngenerated PINN to the requested PDE and uses the discrepancy to generate a\n\"delta\" PINN. Summing their contributions and repeating this process forms an\nensemble whose combined solution progressively reduces the error on six\nbenchmarks and achieves over 100x gain in average $L_2$ loss in the best case,\nwhile retaining forward-only inference. Additionally, we evaluate the\nfine-tuning behavior of PINNs initialized by HyPINO and show that they converge\nfaster and to lower final error than both randomly initialized and\nReptile-meta-learned PINNs on five benchmarks, performing on par on the\nremaining two. Our results highlight the potential of this scalable approach as\na foundation for extending neural operators toward solving increasingly\ncomplex, nonlinear, and high-dimensional PDE problems with significantly\nimproved accuracy and reduced computational cost.\n", "link": "http://arxiv.org/abs/2509.05117v1", "date": "2025-09-05", "relevancy": 1.8957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyPINO%3A%20Multi-Physics%20Neural%20Operators%20via%20HyperPINNs%20and%20the%20Method%20of%0A%20%20Manufactured%20Solutions&body=Title%3A%20HyPINO%3A%20Multi-Physics%20Neural%20Operators%20via%20HyperPINNs%20and%20the%20Method%20of%0A%20%20Manufactured%20Solutions%0AAuthor%3A%20Rafael%20Bischof%20and%20Michal%20Piovar%C4%8Di%20and%20Michael%20A.%20Kraus%20and%20Siddhartha%20Mishra%20and%20Bernd%20Bickel%0AAbstract%3A%20%20%20We%20present%20HyPINO%2C%20a%20multi-physics%20neural%20operator%20designed%20for%20zero-shot%0Ageneralization%20across%20a%20broad%20class%20of%20parametric%20PDEs%20without%20requiring%0Atask-specific%20fine-tuning.%20Our%20approach%20combines%20a%20Swin%20Transformer-based%0Ahypernetwork%20with%20mixed%20supervision%3A%20%28i%29%20labeled%20data%20from%20analytical%20solutions%0Agenerated%20via%20the%20Method%20of%20Manufactured%20Solutions%20%28MMS%29%2C%20and%20%28ii%29%20unlabeled%0Asamples%20optimized%20using%20physics-informed%20objectives.%20The%20model%20maps%20PDE%0Aparametrizations%20to%20target%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20can%0Ahandle%20linear%20elliptic%2C%20hyperbolic%2C%20and%20parabolic%20equations%20in%20two%20dimensions%0Awith%20varying%20source%20terms%2C%20geometries%2C%20and%20mixed%20Dirichlet/Neumann%20boundary%0Aconditions%2C%20including%20interior%20boundaries.%20HyPINO%20achieves%20strong%20zero-shot%0Aaccuracy%20on%20seven%20benchmark%20problems%20from%20PINN%20literature%2C%20outperforming%0AU-Nets%2C%20Poseidon%2C%20and%20Physics-Informed%20Neural%20Operators%20%28PINO%29.%20Further%2C%20we%0Aintroduce%20an%20iterative%20refinement%20procedure%20that%20compares%20the%20physics%20of%20the%0Agenerated%20PINN%20to%20the%20requested%20PDE%20and%20uses%20the%20discrepancy%20to%20generate%20a%0A%22delta%22%20PINN.%20Summing%20their%20contributions%20and%20repeating%20this%20process%20forms%20an%0Aensemble%20whose%20combined%20solution%20progressively%20reduces%20the%20error%20on%20six%0Abenchmarks%20and%20achieves%20over%20100x%20gain%20in%20average%20%24L_2%24%20loss%20in%20the%20best%20case%2C%0Awhile%20retaining%20forward-only%20inference.%20Additionally%2C%20we%20evaluate%20the%0Afine-tuning%20behavior%20of%20PINNs%20initialized%20by%20HyPINO%20and%20show%20that%20they%20converge%0Afaster%20and%20to%20lower%20final%20error%20than%20both%20randomly%20initialized%20and%0AReptile-meta-learned%20PINNs%20on%20five%20benchmarks%2C%20performing%20on%20par%20on%20the%0Aremaining%20two.%20Our%20results%20highlight%20the%20potential%20of%20this%20scalable%20approach%20as%0Aa%20foundation%20for%20extending%20neural%20operators%20toward%20solving%20increasingly%0Acomplex%2C%20nonlinear%2C%20and%20high-dimensional%20PDE%20problems%20with%20significantly%0Aimproved%20accuracy%20and%20reduced%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyPINO%253A%2520Multi-Physics%2520Neural%2520Operators%2520via%2520HyperPINNs%2520and%2520the%2520Method%2520of%250A%2520%2520Manufactured%2520Solutions%26entry.906535625%3DRafael%2520Bischof%2520and%2520Michal%2520Piovar%25C4%258Di%2520and%2520Michael%2520A.%2520Kraus%2520and%2520Siddhartha%2520Mishra%2520and%2520Bernd%2520Bickel%26entry.1292438233%3D%2520%2520We%2520present%2520HyPINO%252C%2520a%2520multi-physics%2520neural%2520operator%2520designed%2520for%2520zero-shot%250Ageneralization%2520across%2520a%2520broad%2520class%2520of%2520parametric%2520PDEs%2520without%2520requiring%250Atask-specific%2520fine-tuning.%2520Our%2520approach%2520combines%2520a%2520Swin%2520Transformer-based%250Ahypernetwork%2520with%2520mixed%2520supervision%253A%2520%2528i%2529%2520labeled%2520data%2520from%2520analytical%2520solutions%250Agenerated%2520via%2520the%2520Method%2520of%2520Manufactured%2520Solutions%2520%2528MMS%2529%252C%2520and%2520%2528ii%2529%2520unlabeled%250Asamples%2520optimized%2520using%2520physics-informed%2520objectives.%2520The%2520model%2520maps%2520PDE%250Aparametrizations%2520to%2520target%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520and%2520can%250Ahandle%2520linear%2520elliptic%252C%2520hyperbolic%252C%2520and%2520parabolic%2520equations%2520in%2520two%2520dimensions%250Awith%2520varying%2520source%2520terms%252C%2520geometries%252C%2520and%2520mixed%2520Dirichlet/Neumann%2520boundary%250Aconditions%252C%2520including%2520interior%2520boundaries.%2520HyPINO%2520achieves%2520strong%2520zero-shot%250Aaccuracy%2520on%2520seven%2520benchmark%2520problems%2520from%2520PINN%2520literature%252C%2520outperforming%250AU-Nets%252C%2520Poseidon%252C%2520and%2520Physics-Informed%2520Neural%2520Operators%2520%2528PINO%2529.%2520Further%252C%2520we%250Aintroduce%2520an%2520iterative%2520refinement%2520procedure%2520that%2520compares%2520the%2520physics%2520of%2520the%250Agenerated%2520PINN%2520to%2520the%2520requested%2520PDE%2520and%2520uses%2520the%2520discrepancy%2520to%2520generate%2520a%250A%2522delta%2522%2520PINN.%2520Summing%2520their%2520contributions%2520and%2520repeating%2520this%2520process%2520forms%2520an%250Aensemble%2520whose%2520combined%2520solution%2520progressively%2520reduces%2520the%2520error%2520on%2520six%250Abenchmarks%2520and%2520achieves%2520over%2520100x%2520gain%2520in%2520average%2520%2524L_2%2524%2520loss%2520in%2520the%2520best%2520case%252C%250Awhile%2520retaining%2520forward-only%2520inference.%2520Additionally%252C%2520we%2520evaluate%2520the%250Afine-tuning%2520behavior%2520of%2520PINNs%2520initialized%2520by%2520HyPINO%2520and%2520show%2520that%2520they%2520converge%250Afaster%2520and%2520to%2520lower%2520final%2520error%2520than%2520both%2520randomly%2520initialized%2520and%250AReptile-meta-learned%2520PINNs%2520on%2520five%2520benchmarks%252C%2520performing%2520on%2520par%2520on%2520the%250Aremaining%2520two.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520this%2520scalable%2520approach%2520as%250Aa%2520foundation%2520for%2520extending%2520neural%2520operators%2520toward%2520solving%2520increasingly%250Acomplex%252C%2520nonlinear%252C%2520and%2520high-dimensional%2520PDE%2520problems%2520with%2520significantly%250Aimproved%2520accuracy%2520and%2520reduced%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyPINO%3A%20Multi-Physics%20Neural%20Operators%20via%20HyperPINNs%20and%20the%20Method%20of%0A%20%20Manufactured%20Solutions&entry.906535625=Rafael%20Bischof%20and%20Michal%20Piovar%C4%8Di%20and%20Michael%20A.%20Kraus%20and%20Siddhartha%20Mishra%20and%20Bernd%20Bickel&entry.1292438233=%20%20We%20present%20HyPINO%2C%20a%20multi-physics%20neural%20operator%20designed%20for%20zero-shot%0Ageneralization%20across%20a%20broad%20class%20of%20parametric%20PDEs%20without%20requiring%0Atask-specific%20fine-tuning.%20Our%20approach%20combines%20a%20Swin%20Transformer-based%0Ahypernetwork%20with%20mixed%20supervision%3A%20%28i%29%20labeled%20data%20from%20analytical%20solutions%0Agenerated%20via%20the%20Method%20of%20Manufactured%20Solutions%20%28MMS%29%2C%20and%20%28ii%29%20unlabeled%0Asamples%20optimized%20using%20physics-informed%20objectives.%20The%20model%20maps%20PDE%0Aparametrizations%20to%20target%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20can%0Ahandle%20linear%20elliptic%2C%20hyperbolic%2C%20and%20parabolic%20equations%20in%20two%20dimensions%0Awith%20varying%20source%20terms%2C%20geometries%2C%20and%20mixed%20Dirichlet/Neumann%20boundary%0Aconditions%2C%20including%20interior%20boundaries.%20HyPINO%20achieves%20strong%20zero-shot%0Aaccuracy%20on%20seven%20benchmark%20problems%20from%20PINN%20literature%2C%20outperforming%0AU-Nets%2C%20Poseidon%2C%20and%20Physics-Informed%20Neural%20Operators%20%28PINO%29.%20Further%2C%20we%0Aintroduce%20an%20iterative%20refinement%20procedure%20that%20compares%20the%20physics%20of%20the%0Agenerated%20PINN%20to%20the%20requested%20PDE%20and%20uses%20the%20discrepancy%20to%20generate%20a%0A%22delta%22%20PINN.%20Summing%20their%20contributions%20and%20repeating%20this%20process%20forms%20an%0Aensemble%20whose%20combined%20solution%20progressively%20reduces%20the%20error%20on%20six%0Abenchmarks%20and%20achieves%20over%20100x%20gain%20in%20average%20%24L_2%24%20loss%20in%20the%20best%20case%2C%0Awhile%20retaining%20forward-only%20inference.%20Additionally%2C%20we%20evaluate%20the%0Afine-tuning%20behavior%20of%20PINNs%20initialized%20by%20HyPINO%20and%20show%20that%20they%20converge%0Afaster%20and%20to%20lower%20final%20error%20than%20both%20randomly%20initialized%20and%0AReptile-meta-learned%20PINNs%20on%20five%20benchmarks%2C%20performing%20on%20par%20on%20the%0Aremaining%20two.%20Our%20results%20highlight%20the%20potential%20of%20this%20scalable%20approach%20as%0Aa%20foundation%20for%20extending%20neural%20operators%20toward%20solving%20increasingly%0Acomplex%2C%20nonlinear%2C%20and%20high-dimensional%20PDE%20problems%20with%20significantly%0Aimproved%20accuracy%20and%20reduced%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05117v1&entry.124074799=Read"},
{"title": "Automated detection of underdiagnosed medical conditions via\n  opportunistic imaging", "author": "Asad Aali and Andrew Johnston and Louis Blankemeier and Dave Van Veen and Laura T Derry and David Svec and Jason Hom and Robert D. Boutin and Akshay S. Chaudhari", "abstract": "  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n", "link": "http://arxiv.org/abs/2409.11686v4", "date": "2025-09-05", "relevancy": 1.8949, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging&body=Title%3A%20Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging%0AAuthor%3A%20Asad%20Aali%20and%20Andrew%20Johnston%20and%20Louis%20Blankemeier%20and%20Dave%20Van%20Veen%20and%20Laura%20T%20Derry%20and%20David%20Svec%20and%20Jason%20Hom%20and%20Robert%20D.%20Boutin%20and%20Akshay%20S.%20Chaudhari%0AAbstract%3A%20%20%20Abdominal%20computed%20tomography%20%28CT%29%20scans%20are%20frequently%20performed%20in%20clinical%0Asettings.%20Opportunistic%20CT%20involves%20repurposing%20routine%20CT%20images%20to%20extract%0Adiagnostic%20information%20and%20is%20an%20emerging%20tool%20for%20detecting%20underdiagnosed%0Aconditions%20such%20as%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites.%20This%20study%0Autilizes%20deep%20learning%20methods%20to%20promote%20accurate%20diagnosis%20and%20clinical%0Adocumentation.%20We%20analyze%202%2C674%20inpatient%20CT%20scans%20to%20identify%20discrepancies%0Abetween%20imaging%20phenotypes%20%28characteristics%20derived%20from%20opportunistic%20CT%0Ascans%29%20and%20their%20corresponding%20documentation%20in%20radiology%20reports%20and%20ICD%0Acoding.%20Through%20our%20analysis%2C%20we%20find%20that%20only%200.5%25%2C%203.2%25%2C%20and%2030.7%25%20of%20scans%0Adiagnosed%20with%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites%20%28respectively%29%0Athrough%20either%20opportunistic%20imaging%20or%20radiology%20reports%20were%20ICD-coded.%20Our%0Afindings%20demonstrate%20opportunistic%20CT%27s%20potential%20to%20enhance%20diagnostic%0Aprecision%20and%20accuracy%20of%20risk%20adjustment%20models%2C%20offering%20advancements%20in%0Aprecision%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11686v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520detection%2520of%2520underdiagnosed%2520medical%2520conditions%2520via%250A%2520%2520opportunistic%2520imaging%26entry.906535625%3DAsad%2520Aali%2520and%2520Andrew%2520Johnston%2520and%2520Louis%2520Blankemeier%2520and%2520Dave%2520Van%2520Veen%2520and%2520Laura%2520T%2520Derry%2520and%2520David%2520Svec%2520and%2520Jason%2520Hom%2520and%2520Robert%2520D.%2520Boutin%2520and%2520Akshay%2520S.%2520Chaudhari%26entry.1292438233%3D%2520%2520Abdominal%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520are%2520frequently%2520performed%2520in%2520clinical%250Asettings.%2520Opportunistic%2520CT%2520involves%2520repurposing%2520routine%2520CT%2520images%2520to%2520extract%250Adiagnostic%2520information%2520and%2520is%2520an%2520emerging%2520tool%2520for%2520detecting%2520underdiagnosed%250Aconditions%2520such%2520as%2520sarcopenia%252C%2520hepatic%2520steatosis%252C%2520and%2520ascites.%2520This%2520study%250Autilizes%2520deep%2520learning%2520methods%2520to%2520promote%2520accurate%2520diagnosis%2520and%2520clinical%250Adocumentation.%2520We%2520analyze%25202%252C674%2520inpatient%2520CT%2520scans%2520to%2520identify%2520discrepancies%250Abetween%2520imaging%2520phenotypes%2520%2528characteristics%2520derived%2520from%2520opportunistic%2520CT%250Ascans%2529%2520and%2520their%2520corresponding%2520documentation%2520in%2520radiology%2520reports%2520and%2520ICD%250Acoding.%2520Through%2520our%2520analysis%252C%2520we%2520find%2520that%2520only%25200.5%2525%252C%25203.2%2525%252C%2520and%252030.7%2525%2520of%2520scans%250Adiagnosed%2520with%2520sarcopenia%252C%2520hepatic%2520steatosis%252C%2520and%2520ascites%2520%2528respectively%2529%250Athrough%2520either%2520opportunistic%2520imaging%2520or%2520radiology%2520reports%2520were%2520ICD-coded.%2520Our%250Afindings%2520demonstrate%2520opportunistic%2520CT%2527s%2520potential%2520to%2520enhance%2520diagnostic%250Aprecision%2520and%2520accuracy%2520of%2520risk%2520adjustment%2520models%252C%2520offering%2520advancements%2520in%250Aprecision%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11686v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging&entry.906535625=Asad%20Aali%20and%20Andrew%20Johnston%20and%20Louis%20Blankemeier%20and%20Dave%20Van%20Veen%20and%20Laura%20T%20Derry%20and%20David%20Svec%20and%20Jason%20Hom%20and%20Robert%20D.%20Boutin%20and%20Akshay%20S.%20Chaudhari&entry.1292438233=%20%20Abdominal%20computed%20tomography%20%28CT%29%20scans%20are%20frequently%20performed%20in%20clinical%0Asettings.%20Opportunistic%20CT%20involves%20repurposing%20routine%20CT%20images%20to%20extract%0Adiagnostic%20information%20and%20is%20an%20emerging%20tool%20for%20detecting%20underdiagnosed%0Aconditions%20such%20as%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites.%20This%20study%0Autilizes%20deep%20learning%20methods%20to%20promote%20accurate%20diagnosis%20and%20clinical%0Adocumentation.%20We%20analyze%202%2C674%20inpatient%20CT%20scans%20to%20identify%20discrepancies%0Abetween%20imaging%20phenotypes%20%28characteristics%20derived%20from%20opportunistic%20CT%0Ascans%29%20and%20their%20corresponding%20documentation%20in%20radiology%20reports%20and%20ICD%0Acoding.%20Through%20our%20analysis%2C%20we%20find%20that%20only%200.5%25%2C%203.2%25%2C%20and%2030.7%25%20of%20scans%0Adiagnosed%20with%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites%20%28respectively%29%0Athrough%20either%20opportunistic%20imaging%20or%20radiology%20reports%20were%20ICD-coded.%20Our%0Afindings%20demonstrate%20opportunistic%20CT%27s%20potential%20to%20enhance%20diagnostic%0Aprecision%20and%20accuracy%20of%20risk%20adjustment%20models%2C%20offering%20advancements%20in%0Aprecision%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11686v4&entry.124074799=Read"},
{"title": "Behavior Synthesis via Contact-Aware Fisher Information Maximization", "author": "Hrishikesh Sathyanarayan and Ian Abraham", "abstract": "  Contact dynamics hold immense amounts of information that can improve a\nrobot's ability to characterize and learn about objects in their environment\nthrough interactions. However, collecting information-rich contact data is\nchallenging due to its inherent sparsity and non-smooth nature, requiring an\nactive approach to maximize the utility of contacts for learning. In this work,\nwe investigate an optimal experimental design approach to synthesize robot\nbehaviors that produce contact-rich data for learning. Our approach derives a\ncontact-aware Fisher information measure that characterizes information-rich\ncontact behaviors that improve parameter learning. We observe emergent robot\nbehaviors that are able to excite contact interactions that efficiently learns\nobject parameters across a range of parameter learning examples. Last, we\ndemonstrate the utility of contact-awareness for learning parameters through\ncontact-seeking behaviors on several robotic experiments.\n", "link": "http://arxiv.org/abs/2505.12214v2", "date": "2025-09-05", "relevancy": 1.5736, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5735}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5436}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behavior%20Synthesis%20via%20Contact-Aware%20Fisher%20Information%20Maximization&body=Title%3A%20Behavior%20Synthesis%20via%20Contact-Aware%20Fisher%20Information%20Maximization%0AAuthor%3A%20Hrishikesh%20Sathyanarayan%20and%20Ian%20Abraham%0AAbstract%3A%20%20%20Contact%20dynamics%20hold%20immense%20amounts%20of%20information%20that%20can%20improve%20a%0Arobot%27s%20ability%20to%20characterize%20and%20learn%20about%20objects%20in%20their%20environment%0Athrough%20interactions.%20However%2C%20collecting%20information-rich%20contact%20data%20is%0Achallenging%20due%20to%20its%20inherent%20sparsity%20and%20non-smooth%20nature%2C%20requiring%20an%0Aactive%20approach%20to%20maximize%20the%20utility%20of%20contacts%20for%20learning.%20In%20this%20work%2C%0Awe%20investigate%20an%20optimal%20experimental%20design%20approach%20to%20synthesize%20robot%0Abehaviors%20that%20produce%20contact-rich%20data%20for%20learning.%20Our%20approach%20derives%20a%0Acontact-aware%20Fisher%20information%20measure%20that%20characterizes%20information-rich%0Acontact%20behaviors%20that%20improve%20parameter%20learning.%20We%20observe%20emergent%20robot%0Abehaviors%20that%20are%20able%20to%20excite%20contact%20interactions%20that%20efficiently%20learns%0Aobject%20parameters%20across%20a%20range%20of%20parameter%20learning%20examples.%20Last%2C%20we%0Ademonstrate%20the%20utility%20of%20contact-awareness%20for%20learning%20parameters%20through%0Acontact-seeking%20behaviors%20on%20several%20robotic%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehavior%2520Synthesis%2520via%2520Contact-Aware%2520Fisher%2520Information%2520Maximization%26entry.906535625%3DHrishikesh%2520Sathyanarayan%2520and%2520Ian%2520Abraham%26entry.1292438233%3D%2520%2520Contact%2520dynamics%2520hold%2520immense%2520amounts%2520of%2520information%2520that%2520can%2520improve%2520a%250Arobot%2527s%2520ability%2520to%2520characterize%2520and%2520learn%2520about%2520objects%2520in%2520their%2520environment%250Athrough%2520interactions.%2520However%252C%2520collecting%2520information-rich%2520contact%2520data%2520is%250Achallenging%2520due%2520to%2520its%2520inherent%2520sparsity%2520and%2520non-smooth%2520nature%252C%2520requiring%2520an%250Aactive%2520approach%2520to%2520maximize%2520the%2520utility%2520of%2520contacts%2520for%2520learning.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520an%2520optimal%2520experimental%2520design%2520approach%2520to%2520synthesize%2520robot%250Abehaviors%2520that%2520produce%2520contact-rich%2520data%2520for%2520learning.%2520Our%2520approach%2520derives%2520a%250Acontact-aware%2520Fisher%2520information%2520measure%2520that%2520characterizes%2520information-rich%250Acontact%2520behaviors%2520that%2520improve%2520parameter%2520learning.%2520We%2520observe%2520emergent%2520robot%250Abehaviors%2520that%2520are%2520able%2520to%2520excite%2520contact%2520interactions%2520that%2520efficiently%2520learns%250Aobject%2520parameters%2520across%2520a%2520range%2520of%2520parameter%2520learning%2520examples.%2520Last%252C%2520we%250Ademonstrate%2520the%2520utility%2520of%2520contact-awareness%2520for%2520learning%2520parameters%2520through%250Acontact-seeking%2520behaviors%2520on%2520several%2520robotic%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavior%20Synthesis%20via%20Contact-Aware%20Fisher%20Information%20Maximization&entry.906535625=Hrishikesh%20Sathyanarayan%20and%20Ian%20Abraham&entry.1292438233=%20%20Contact%20dynamics%20hold%20immense%20amounts%20of%20information%20that%20can%20improve%20a%0Arobot%27s%20ability%20to%20characterize%20and%20learn%20about%20objects%20in%20their%20environment%0Athrough%20interactions.%20However%2C%20collecting%20information-rich%20contact%20data%20is%0Achallenging%20due%20to%20its%20inherent%20sparsity%20and%20non-smooth%20nature%2C%20requiring%20an%0Aactive%20approach%20to%20maximize%20the%20utility%20of%20contacts%20for%20learning.%20In%20this%20work%2C%0Awe%20investigate%20an%20optimal%20experimental%20design%20approach%20to%20synthesize%20robot%0Abehaviors%20that%20produce%20contact-rich%20data%20for%20learning.%20Our%20approach%20derives%20a%0Acontact-aware%20Fisher%20information%20measure%20that%20characterizes%20information-rich%0Acontact%20behaviors%20that%20improve%20parameter%20learning.%20We%20observe%20emergent%20robot%0Abehaviors%20that%20are%20able%20to%20excite%20contact%20interactions%20that%20efficiently%20learns%0Aobject%20parameters%20across%20a%20range%20of%20parameter%20learning%20examples.%20Last%2C%20we%0Ademonstrate%20the%20utility%20of%20contact-awareness%20for%20learning%20parameters%20through%0Acontact-seeking%20behaviors%20on%20several%20robotic%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12214v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for Ranking Utility Tuning in the Ad\n  Recommender System at Pinterest", "author": "Xiao Yang and Mehdi Ben Ayed and Longyu Zhao and Fan Zhou and Yuchen Shen and Abe Engle and Jinfeng Zhuang and Ling Leng and Jiajing Xu and Charles Rosenberg and Prathibha Deshikachar", "abstract": "  The ranking utility function in an ad recommender system, which linearly\ncombines predictions of various business goals, plays a central role in\nbalancing values across the platform, advertisers, and users. Traditional\nmanual tuning, while offering simplicity and interpretability, often yields\nsuboptimal results due to its unprincipled tuning objectives, the vast amount\nof parameter combinations, and its lack of personalization and adaptability to\nseasonality. In this work, we propose a general Deep Reinforcement Learning\nframework for Personalized Utility Tuning (DRL-PUT) to address the challenges\nof multi-objective optimization within ad recommender systems. Our key\ncontributions include: 1) Formulating the problem as a reinforcement learning\ntask: given the state of an ad request, we predict the optimal hyperparameters\nto maximize a pre-defined reward. 2) Developing an approach to directly learn\nan optimal policy model using online serving logs, avoiding the need to\nestimate a value function, which is inherently challenging due to the high\nvariance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT\nthrough an online A/B experiment in Pinterest's ad recommender system. Compared\nto the baseline manual utility tuning approach, DRL-PUT improved the\nclick-through rate by 9.7% and the long click-through rate by 7.7% on the\ntreated segment. We conducted a detailed ablation study on the impact of\ndifferent reward definitions and analyzed the personalization aspect of the\nlearned policy model.\n", "link": "http://arxiv.org/abs/2509.05292v1", "date": "2025-09-05", "relevancy": 1.3518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4534}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20for%20Ranking%20Utility%20Tuning%20in%20the%20Ad%0A%20%20Recommender%20System%20at%20Pinterest&body=Title%3A%20Deep%20Reinforcement%20Learning%20for%20Ranking%20Utility%20Tuning%20in%20the%20Ad%0A%20%20Recommender%20System%20at%20Pinterest%0AAuthor%3A%20Xiao%20Yang%20and%20Mehdi%20Ben%20Ayed%20and%20Longyu%20Zhao%20and%20Fan%20Zhou%20and%20Yuchen%20Shen%20and%20Abe%20Engle%20and%20Jinfeng%20Zhuang%20and%20Ling%20Leng%20and%20Jiajing%20Xu%20and%20Charles%20Rosenberg%20and%20Prathibha%20Deshikachar%0AAbstract%3A%20%20%20The%20ranking%20utility%20function%20in%20an%20ad%20recommender%20system%2C%20which%20linearly%0Acombines%20predictions%20of%20various%20business%20goals%2C%20plays%20a%20central%20role%20in%0Abalancing%20values%20across%20the%20platform%2C%20advertisers%2C%20and%20users.%20Traditional%0Amanual%20tuning%2C%20while%20offering%20simplicity%20and%20interpretability%2C%20often%20yields%0Asuboptimal%20results%20due%20to%20its%20unprincipled%20tuning%20objectives%2C%20the%20vast%20amount%0Aof%20parameter%20combinations%2C%20and%20its%20lack%20of%20personalization%20and%20adaptability%20to%0Aseasonality.%20In%20this%20work%2C%20we%20propose%20a%20general%20Deep%20Reinforcement%20Learning%0Aframework%20for%20Personalized%20Utility%20Tuning%20%28DRL-PUT%29%20to%20address%20the%20challenges%0Aof%20multi-objective%20optimization%20within%20ad%20recommender%20systems.%20Our%20key%0Acontributions%20include%3A%201%29%20Formulating%20the%20problem%20as%20a%20reinforcement%20learning%0Atask%3A%20given%20the%20state%20of%20an%20ad%20request%2C%20we%20predict%20the%20optimal%20hyperparameters%0Ato%20maximize%20a%20pre-defined%20reward.%202%29%20Developing%20an%20approach%20to%20directly%20learn%0Aan%20optimal%20policy%20model%20using%20online%20serving%20logs%2C%20avoiding%20the%20need%20to%0Aestimate%20a%20value%20function%2C%20which%20is%20inherently%20challenging%20due%20to%20the%20high%0Avariance%20and%20unbalanced%20distribution%20of%20immediate%20rewards.%20We%20evaluated%20DRL-PUT%0Athrough%20an%20online%20A/B%20experiment%20in%20Pinterest%27s%20ad%20recommender%20system.%20Compared%0Ato%20the%20baseline%20manual%20utility%20tuning%20approach%2C%20DRL-PUT%20improved%20the%0Aclick-through%20rate%20by%209.7%25%20and%20the%20long%20click-through%20rate%20by%207.7%25%20on%20the%0Atreated%20segment.%20We%20conducted%20a%20detailed%20ablation%20study%20on%20the%20impact%20of%0Adifferent%20reward%20definitions%20and%20analyzed%20the%20personalization%20aspect%20of%20the%0Alearned%20policy%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520for%2520Ranking%2520Utility%2520Tuning%2520in%2520the%2520Ad%250A%2520%2520Recommender%2520System%2520at%2520Pinterest%26entry.906535625%3DXiao%2520Yang%2520and%2520Mehdi%2520Ben%2520Ayed%2520and%2520Longyu%2520Zhao%2520and%2520Fan%2520Zhou%2520and%2520Yuchen%2520Shen%2520and%2520Abe%2520Engle%2520and%2520Jinfeng%2520Zhuang%2520and%2520Ling%2520Leng%2520and%2520Jiajing%2520Xu%2520and%2520Charles%2520Rosenberg%2520and%2520Prathibha%2520Deshikachar%26entry.1292438233%3D%2520%2520The%2520ranking%2520utility%2520function%2520in%2520an%2520ad%2520recommender%2520system%252C%2520which%2520linearly%250Acombines%2520predictions%2520of%2520various%2520business%2520goals%252C%2520plays%2520a%2520central%2520role%2520in%250Abalancing%2520values%2520across%2520the%2520platform%252C%2520advertisers%252C%2520and%2520users.%2520Traditional%250Amanual%2520tuning%252C%2520while%2520offering%2520simplicity%2520and%2520interpretability%252C%2520often%2520yields%250Asuboptimal%2520results%2520due%2520to%2520its%2520unprincipled%2520tuning%2520objectives%252C%2520the%2520vast%2520amount%250Aof%2520parameter%2520combinations%252C%2520and%2520its%2520lack%2520of%2520personalization%2520and%2520adaptability%2520to%250Aseasonality.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%2520Deep%2520Reinforcement%2520Learning%250Aframework%2520for%2520Personalized%2520Utility%2520Tuning%2520%2528DRL-PUT%2529%2520to%2520address%2520the%2520challenges%250Aof%2520multi-objective%2520optimization%2520within%2520ad%2520recommender%2520systems.%2520Our%2520key%250Acontributions%2520include%253A%25201%2529%2520Formulating%2520the%2520problem%2520as%2520a%2520reinforcement%2520learning%250Atask%253A%2520given%2520the%2520state%2520of%2520an%2520ad%2520request%252C%2520we%2520predict%2520the%2520optimal%2520hyperparameters%250Ato%2520maximize%2520a%2520pre-defined%2520reward.%25202%2529%2520Developing%2520an%2520approach%2520to%2520directly%2520learn%250Aan%2520optimal%2520policy%2520model%2520using%2520online%2520serving%2520logs%252C%2520avoiding%2520the%2520need%2520to%250Aestimate%2520a%2520value%2520function%252C%2520which%2520is%2520inherently%2520challenging%2520due%2520to%2520the%2520high%250Avariance%2520and%2520unbalanced%2520distribution%2520of%2520immediate%2520rewards.%2520We%2520evaluated%2520DRL-PUT%250Athrough%2520an%2520online%2520A/B%2520experiment%2520in%2520Pinterest%2527s%2520ad%2520recommender%2520system.%2520Compared%250Ato%2520the%2520baseline%2520manual%2520utility%2520tuning%2520approach%252C%2520DRL-PUT%2520improved%2520the%250Aclick-through%2520rate%2520by%25209.7%2525%2520and%2520the%2520long%2520click-through%2520rate%2520by%25207.7%2525%2520on%2520the%250Atreated%2520segment.%2520We%2520conducted%2520a%2520detailed%2520ablation%2520study%2520on%2520the%2520impact%2520of%250Adifferent%2520reward%2520definitions%2520and%2520analyzed%2520the%2520personalization%2520aspect%2520of%2520the%250Alearned%2520policy%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20Ranking%20Utility%20Tuning%20in%20the%20Ad%0A%20%20Recommender%20System%20at%20Pinterest&entry.906535625=Xiao%20Yang%20and%20Mehdi%20Ben%20Ayed%20and%20Longyu%20Zhao%20and%20Fan%20Zhou%20and%20Yuchen%20Shen%20and%20Abe%20Engle%20and%20Jinfeng%20Zhuang%20and%20Ling%20Leng%20and%20Jiajing%20Xu%20and%20Charles%20Rosenberg%20and%20Prathibha%20Deshikachar&entry.1292438233=%20%20The%20ranking%20utility%20function%20in%20an%20ad%20recommender%20system%2C%20which%20linearly%0Acombines%20predictions%20of%20various%20business%20goals%2C%20plays%20a%20central%20role%20in%0Abalancing%20values%20across%20the%20platform%2C%20advertisers%2C%20and%20users.%20Traditional%0Amanual%20tuning%2C%20while%20offering%20simplicity%20and%20interpretability%2C%20often%20yields%0Asuboptimal%20results%20due%20to%20its%20unprincipled%20tuning%20objectives%2C%20the%20vast%20amount%0Aof%20parameter%20combinations%2C%20and%20its%20lack%20of%20personalization%20and%20adaptability%20to%0Aseasonality.%20In%20this%20work%2C%20we%20propose%20a%20general%20Deep%20Reinforcement%20Learning%0Aframework%20for%20Personalized%20Utility%20Tuning%20%28DRL-PUT%29%20to%20address%20the%20challenges%0Aof%20multi-objective%20optimization%20within%20ad%20recommender%20systems.%20Our%20key%0Acontributions%20include%3A%201%29%20Formulating%20the%20problem%20as%20a%20reinforcement%20learning%0Atask%3A%20given%20the%20state%20of%20an%20ad%20request%2C%20we%20predict%20the%20optimal%20hyperparameters%0Ato%20maximize%20a%20pre-defined%20reward.%202%29%20Developing%20an%20approach%20to%20directly%20learn%0Aan%20optimal%20policy%20model%20using%20online%20serving%20logs%2C%20avoiding%20the%20need%20to%0Aestimate%20a%20value%20function%2C%20which%20is%20inherently%20challenging%20due%20to%20the%20high%0Avariance%20and%20unbalanced%20distribution%20of%20immediate%20rewards.%20We%20evaluated%20DRL-PUT%0Athrough%20an%20online%20A/B%20experiment%20in%20Pinterest%27s%20ad%20recommender%20system.%20Compared%0Ato%20the%20baseline%20manual%20utility%20tuning%20approach%2C%20DRL-PUT%20improved%20the%0Aclick-through%20rate%20by%209.7%25%20and%20the%20long%20click-through%20rate%20by%207.7%25%20on%20the%0Atreated%20segment.%20We%20conducted%20a%20detailed%20ablation%20study%20on%20the%20impact%20of%0Adifferent%20reward%20definitions%20and%20analyzed%20the%20personalization%20aspect%20of%20the%0Alearned%20policy%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05292v1&entry.124074799=Read"},
{"title": "MultiSurv: A Multimodal Deep Survival Framework for Prostrate and\n  Bladder Cancer", "author": "Noorul Wahab and Ethar Alzaid and Jiaqi Lv and Adam Shephard and Shan E Ahmed Raza", "abstract": "  Accurate prediction of time-to-event outcomes is a central challenge in\noncology, with significant implications for treatment planning and patient\nmanagement. In this work, we present MultiSurv, a multimodal deep survival\nmodel utilising DeepHit with a projection layer and inter-modality\ncross-attention, which integrates heterogeneous patient data, including\nclinical, MRI, RNA-seq and whole-slide pathology features. The model is\ndesigned to capture complementary prognostic signals across modalities and\nestimate individualised time-to-biochemical recurrence in prostate cancer and\ntime-to-cancer recurrence in bladder cancer. Our approach was evaluated in the\ncontext of the CHIMERA Grand Challenge, across two of the three provided tasks.\nFor Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed\nframework achieved a concordance index (C-index) of 0.843 on 5-folds\ncross-validation and 0.818 on CHIMERA development set, demonstrating robust\ndiscriminatory ability. For Task 3 (bladder cancer recurrence prediction), the\nmodel obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on\ndevelopment set, highlighting its adaptability and potential for clinical\ntranslation. These results suggest that leveraging multimodal integration with\ndeep survival learning provides a promising pathway toward personalised risk\nstratification in prostate and bladder cancer. Beyond the challenge setting,\nour framework is broadly applicable to survival prediction tasks involving\nheterogeneous biomedical data.\n", "link": "http://arxiv.org/abs/2509.05037v1", "date": "2025-09-05", "relevancy": 1.6122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostrate%20and%0A%20%20Bladder%20Cancer&body=Title%3A%20MultiSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostrate%20and%0A%20%20Bladder%20Cancer%0AAuthor%3A%20Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20time-to-event%20outcomes%20is%20a%20central%20challenge%20in%0Aoncology%2C%20with%20significant%20implications%20for%20treatment%20planning%20and%20patient%0Amanagement.%20In%20this%20work%2C%20we%20present%20MultiSurv%2C%20a%20multimodal%20deep%20survival%0Amodel%20utilising%20DeepHit%20with%20a%20projection%20layer%20and%20inter-modality%0Across-attention%2C%20which%20integrates%20heterogeneous%20patient%20data%2C%20including%0Aclinical%2C%20MRI%2C%20RNA-seq%20and%20whole-slide%20pathology%20features.%20The%20model%20is%0Adesigned%20to%20capture%20complementary%20prognostic%20signals%20across%20modalities%20and%0Aestimate%20individualised%20time-to-biochemical%20recurrence%20in%20prostate%20cancer%20and%0Atime-to-cancer%20recurrence%20in%20bladder%20cancer.%20Our%20approach%20was%20evaluated%20in%20the%0Acontext%20of%20the%20CHIMERA%20Grand%20Challenge%2C%20across%20two%20of%20the%20three%20provided%20tasks.%0AFor%20Task%201%20%28prostate%20cancer%20bio-chemical%20recurrence%20prediction%29%2C%20the%20proposed%0Aframework%20achieved%20a%20concordance%20index%20%28C-index%29%20of%200.843%20on%205-folds%0Across-validation%20and%200.818%20on%20CHIMERA%20development%20set%2C%20demonstrating%20robust%0Adiscriminatory%20ability.%20For%20Task%203%20%28bladder%20cancer%20recurrence%20prediction%29%2C%20the%0Amodel%20obtained%20a%20C-index%20of%200.662%20on%205-folds%20cross-validation%20and%200.457%20on%0Adevelopment%20set%2C%20highlighting%20its%20adaptability%20and%20potential%20for%20clinical%0Atranslation.%20These%20results%20suggest%20that%20leveraging%20multimodal%20integration%20with%0Adeep%20survival%20learning%20provides%20a%20promising%20pathway%20toward%20personalised%20risk%0Astratification%20in%20prostate%20and%20bladder%20cancer.%20Beyond%20the%20challenge%20setting%2C%0Aour%20framework%20is%20broadly%20applicable%20to%20survival%20prediction%20tasks%20involving%0Aheterogeneous%20biomedical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiSurv%253A%2520A%2520Multimodal%2520Deep%2520Survival%2520Framework%2520for%2520Prostrate%2520and%250A%2520%2520Bladder%2520Cancer%26entry.906535625%3DNoorul%2520Wahab%2520and%2520Ethar%2520Alzaid%2520and%2520Jiaqi%2520Lv%2520and%2520Adam%2520Shephard%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520time-to-event%2520outcomes%2520is%2520a%2520central%2520challenge%2520in%250Aoncology%252C%2520with%2520significant%2520implications%2520for%2520treatment%2520planning%2520and%2520patient%250Amanagement.%2520In%2520this%2520work%252C%2520we%2520present%2520MultiSurv%252C%2520a%2520multimodal%2520deep%2520survival%250Amodel%2520utilising%2520DeepHit%2520with%2520a%2520projection%2520layer%2520and%2520inter-modality%250Across-attention%252C%2520which%2520integrates%2520heterogeneous%2520patient%2520data%252C%2520including%250Aclinical%252C%2520MRI%252C%2520RNA-seq%2520and%2520whole-slide%2520pathology%2520features.%2520The%2520model%2520is%250Adesigned%2520to%2520capture%2520complementary%2520prognostic%2520signals%2520across%2520modalities%2520and%250Aestimate%2520individualised%2520time-to-biochemical%2520recurrence%2520in%2520prostate%2520cancer%2520and%250Atime-to-cancer%2520recurrence%2520in%2520bladder%2520cancer.%2520Our%2520approach%2520was%2520evaluated%2520in%2520the%250Acontext%2520of%2520the%2520CHIMERA%2520Grand%2520Challenge%252C%2520across%2520two%2520of%2520the%2520three%2520provided%2520tasks.%250AFor%2520Task%25201%2520%2528prostate%2520cancer%2520bio-chemical%2520recurrence%2520prediction%2529%252C%2520the%2520proposed%250Aframework%2520achieved%2520a%2520concordance%2520index%2520%2528C-index%2529%2520of%25200.843%2520on%25205-folds%250Across-validation%2520and%25200.818%2520on%2520CHIMERA%2520development%2520set%252C%2520demonstrating%2520robust%250Adiscriminatory%2520ability.%2520For%2520Task%25203%2520%2528bladder%2520cancer%2520recurrence%2520prediction%2529%252C%2520the%250Amodel%2520obtained%2520a%2520C-index%2520of%25200.662%2520on%25205-folds%2520cross-validation%2520and%25200.457%2520on%250Adevelopment%2520set%252C%2520highlighting%2520its%2520adaptability%2520and%2520potential%2520for%2520clinical%250Atranslation.%2520These%2520results%2520suggest%2520that%2520leveraging%2520multimodal%2520integration%2520with%250Adeep%2520survival%2520learning%2520provides%2520a%2520promising%2520pathway%2520toward%2520personalised%2520risk%250Astratification%2520in%2520prostate%2520and%2520bladder%2520cancer.%2520Beyond%2520the%2520challenge%2520setting%252C%250Aour%2520framework%2520is%2520broadly%2520applicable%2520to%2520survival%2520prediction%2520tasks%2520involving%250Aheterogeneous%2520biomedical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostrate%20and%0A%20%20Bladder%20Cancer&entry.906535625=Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Accurate%20prediction%20of%20time-to-event%20outcomes%20is%20a%20central%20challenge%20in%0Aoncology%2C%20with%20significant%20implications%20for%20treatment%20planning%20and%20patient%0Amanagement.%20In%20this%20work%2C%20we%20present%20MultiSurv%2C%20a%20multimodal%20deep%20survival%0Amodel%20utilising%20DeepHit%20with%20a%20projection%20layer%20and%20inter-modality%0Across-attention%2C%20which%20integrates%20heterogeneous%20patient%20data%2C%20including%0Aclinical%2C%20MRI%2C%20RNA-seq%20and%20whole-slide%20pathology%20features.%20The%20model%20is%0Adesigned%20to%20capture%20complementary%20prognostic%20signals%20across%20modalities%20and%0Aestimate%20individualised%20time-to-biochemical%20recurrence%20in%20prostate%20cancer%20and%0Atime-to-cancer%20recurrence%20in%20bladder%20cancer.%20Our%20approach%20was%20evaluated%20in%20the%0Acontext%20of%20the%20CHIMERA%20Grand%20Challenge%2C%20across%20two%20of%20the%20three%20provided%20tasks.%0AFor%20Task%201%20%28prostate%20cancer%20bio-chemical%20recurrence%20prediction%29%2C%20the%20proposed%0Aframework%20achieved%20a%20concordance%20index%20%28C-index%29%20of%200.843%20on%205-folds%0Across-validation%20and%200.818%20on%20CHIMERA%20development%20set%2C%20demonstrating%20robust%0Adiscriminatory%20ability.%20For%20Task%203%20%28bladder%20cancer%20recurrence%20prediction%29%2C%20the%0Amodel%20obtained%20a%20C-index%20of%200.662%20on%205-folds%20cross-validation%20and%200.457%20on%0Adevelopment%20set%2C%20highlighting%20its%20adaptability%20and%20potential%20for%20clinical%0Atranslation.%20These%20results%20suggest%20that%20leveraging%20multimodal%20integration%20with%0Adeep%20survival%20learning%20provides%20a%20promising%20pathway%20toward%20personalised%20risk%0Astratification%20in%20prostate%20and%20bladder%20cancer.%20Beyond%20the%20challenge%20setting%2C%0Aour%20framework%20is%20broadly%20applicable%20to%20survival%20prediction%20tasks%20involving%0Aheterogeneous%20biomedical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05037v1&entry.124074799=Read"},
{"title": "Survival Analysis with Adversarial Regularization", "author": "Michael Potter and Stefano Maxenti and Michael Everett", "abstract": "  Survival Analysis (SA) models the time until an event occurs, with\napplications in fields like medicine, defense, finance, and aerospace. Recent\nresearch indicates that Neural Networks (NNs) can effectively capture complex\ndata patterns in SA, whereas simple generalized linear models often fall short\nin this regard. However, dataset uncertainties (e.g., noisy measurements, human\nerror) can degrade NN model performance. To address this, we leverage advances\nin NN verification to develop training objectives for robust, fully-parametric\nSA models. Specifically, we propose an adversarially robust loss function based\non a Min-Max optimization problem. We employ CROWN-Interval Bound Propagation\n(CROWN-IBP) to tackle the computational challenges inherent in solving this\nMin-Max problem. Evaluated over 10 SurvSet datasets, our method, Survival\nAnalysis with Adversarial Regularization (SAWAR), consistently outperforms\nbaseline adversarial training methods and state-of-the-art (SOTA) deep SA\nmodels across various covariate perturbations with respect to Negative Log\nLikelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI)\nmetrics. Thus, we demonstrate that adversarial robustness enhances SA\npredictive performance and calibration, mitigating data uncertainty and\nimproving generalization across diverse datasets by up to 150% compared to\nbaselines.\n", "link": "http://arxiv.org/abs/2312.16019v5", "date": "2025-09-05", "relevancy": 1.8701, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survival%20Analysis%20with%20Adversarial%20Regularization&body=Title%3A%20Survival%20Analysis%20with%20Adversarial%20Regularization%0AAuthor%3A%20Michael%20Potter%20and%20Stefano%20Maxenti%20and%20Michael%20Everett%0AAbstract%3A%20%20%20Survival%20Analysis%20%28SA%29%20models%20the%20time%20until%20an%20event%20occurs%2C%20with%0Aapplications%20in%20fields%20like%20medicine%2C%20defense%2C%20finance%2C%20and%20aerospace.%20Recent%0Aresearch%20indicates%20that%20Neural%20Networks%20%28NNs%29%20can%20effectively%20capture%20complex%0Adata%20patterns%20in%20SA%2C%20whereas%20simple%20generalized%20linear%20models%20often%20fall%20short%0Ain%20this%20regard.%20However%2C%20dataset%20uncertainties%20%28e.g.%2C%20noisy%20measurements%2C%20human%0Aerror%29%20can%20degrade%20NN%20model%20performance.%20To%20address%20this%2C%20we%20leverage%20advances%0Ain%20NN%20verification%20to%20develop%20training%20objectives%20for%20robust%2C%20fully-parametric%0ASA%20models.%20Specifically%2C%20we%20propose%20an%20adversarially%20robust%20loss%20function%20based%0Aon%20a%20Min-Max%20optimization%20problem.%20We%20employ%20CROWN-Interval%20Bound%20Propagation%0A%28CROWN-IBP%29%20to%20tackle%20the%20computational%20challenges%20inherent%20in%20solving%20this%0AMin-Max%20problem.%20Evaluated%20over%2010%20SurvSet%20datasets%2C%20our%20method%2C%20Survival%0AAnalysis%20with%20Adversarial%20Regularization%20%28SAWAR%29%2C%20consistently%20outperforms%0Abaseline%20adversarial%20training%20methods%20and%20state-of-the-art%20%28SOTA%29%20deep%20SA%0Amodels%20across%20various%20covariate%20perturbations%20with%20respect%20to%20Negative%20Log%0ALikelihood%20%28NegLL%29%2C%20Integrated%20Brier%20Score%20%28IBS%29%2C%20and%20Concordance%20Index%20%28CI%29%0Ametrics.%20Thus%2C%20we%20demonstrate%20that%20adversarial%20robustness%20enhances%20SA%0Apredictive%20performance%20and%20calibration%2C%20mitigating%20data%20uncertainty%20and%0Aimproving%20generalization%20across%20diverse%20datasets%20by%20up%20to%20150%25%20compared%20to%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16019v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvival%2520Analysis%2520with%2520Adversarial%2520Regularization%26entry.906535625%3DMichael%2520Potter%2520and%2520Stefano%2520Maxenti%2520and%2520Michael%2520Everett%26entry.1292438233%3D%2520%2520Survival%2520Analysis%2520%2528SA%2529%2520models%2520the%2520time%2520until%2520an%2520event%2520occurs%252C%2520with%250Aapplications%2520in%2520fields%2520like%2520medicine%252C%2520defense%252C%2520finance%252C%2520and%2520aerospace.%2520Recent%250Aresearch%2520indicates%2520that%2520Neural%2520Networks%2520%2528NNs%2529%2520can%2520effectively%2520capture%2520complex%250Adata%2520patterns%2520in%2520SA%252C%2520whereas%2520simple%2520generalized%2520linear%2520models%2520often%2520fall%2520short%250Ain%2520this%2520regard.%2520However%252C%2520dataset%2520uncertainties%2520%2528e.g.%252C%2520noisy%2520measurements%252C%2520human%250Aerror%2529%2520can%2520degrade%2520NN%2520model%2520performance.%2520To%2520address%2520this%252C%2520we%2520leverage%2520advances%250Ain%2520NN%2520verification%2520to%2520develop%2520training%2520objectives%2520for%2520robust%252C%2520fully-parametric%250ASA%2520models.%2520Specifically%252C%2520we%2520propose%2520an%2520adversarially%2520robust%2520loss%2520function%2520based%250Aon%2520a%2520Min-Max%2520optimization%2520problem.%2520We%2520employ%2520CROWN-Interval%2520Bound%2520Propagation%250A%2528CROWN-IBP%2529%2520to%2520tackle%2520the%2520computational%2520challenges%2520inherent%2520in%2520solving%2520this%250AMin-Max%2520problem.%2520Evaluated%2520over%252010%2520SurvSet%2520datasets%252C%2520our%2520method%252C%2520Survival%250AAnalysis%2520with%2520Adversarial%2520Regularization%2520%2528SAWAR%2529%252C%2520consistently%2520outperforms%250Abaseline%2520adversarial%2520training%2520methods%2520and%2520state-of-the-art%2520%2528SOTA%2529%2520deep%2520SA%250Amodels%2520across%2520various%2520covariate%2520perturbations%2520with%2520respect%2520to%2520Negative%2520Log%250ALikelihood%2520%2528NegLL%2529%252C%2520Integrated%2520Brier%2520Score%2520%2528IBS%2529%252C%2520and%2520Concordance%2520Index%2520%2528CI%2529%250Ametrics.%2520Thus%252C%2520we%2520demonstrate%2520that%2520adversarial%2520robustness%2520enhances%2520SA%250Apredictive%2520performance%2520and%2520calibration%252C%2520mitigating%2520data%2520uncertainty%2520and%250Aimproving%2520generalization%2520across%2520diverse%2520datasets%2520by%2520up%2520to%2520150%2525%2520compared%2520to%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16019v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survival%20Analysis%20with%20Adversarial%20Regularization&entry.906535625=Michael%20Potter%20and%20Stefano%20Maxenti%20and%20Michael%20Everett&entry.1292438233=%20%20Survival%20Analysis%20%28SA%29%20models%20the%20time%20until%20an%20event%20occurs%2C%20with%0Aapplications%20in%20fields%20like%20medicine%2C%20defense%2C%20finance%2C%20and%20aerospace.%20Recent%0Aresearch%20indicates%20that%20Neural%20Networks%20%28NNs%29%20can%20effectively%20capture%20complex%0Adata%20patterns%20in%20SA%2C%20whereas%20simple%20generalized%20linear%20models%20often%20fall%20short%0Ain%20this%20regard.%20However%2C%20dataset%20uncertainties%20%28e.g.%2C%20noisy%20measurements%2C%20human%0Aerror%29%20can%20degrade%20NN%20model%20performance.%20To%20address%20this%2C%20we%20leverage%20advances%0Ain%20NN%20verification%20to%20develop%20training%20objectives%20for%20robust%2C%20fully-parametric%0ASA%20models.%20Specifically%2C%20we%20propose%20an%20adversarially%20robust%20loss%20function%20based%0Aon%20a%20Min-Max%20optimization%20problem.%20We%20employ%20CROWN-Interval%20Bound%20Propagation%0A%28CROWN-IBP%29%20to%20tackle%20the%20computational%20challenges%20inherent%20in%20solving%20this%0AMin-Max%20problem.%20Evaluated%20over%2010%20SurvSet%20datasets%2C%20our%20method%2C%20Survival%0AAnalysis%20with%20Adversarial%20Regularization%20%28SAWAR%29%2C%20consistently%20outperforms%0Abaseline%20adversarial%20training%20methods%20and%20state-of-the-art%20%28SOTA%29%20deep%20SA%0Amodels%20across%20various%20covariate%20perturbations%20with%20respect%20to%20Negative%20Log%0ALikelihood%20%28NegLL%29%2C%20Integrated%20Brier%20Score%20%28IBS%29%2C%20and%20Concordance%20Index%20%28CI%29%0Ametrics.%20Thus%2C%20we%20demonstrate%20that%20adversarial%20robustness%20enhances%20SA%0Apredictive%20performance%20and%20calibration%2C%20mitigating%20data%20uncertainty%20and%0Aimproving%20generalization%20across%20diverse%20datasets%20by%20up%20to%20150%25%20compared%20to%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16019v5&entry.124074799=Read"},
{"title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed\n  Feedback", "author": "Matteo Bortoletto and Yichao Zhou and Lance Ying and Tianmin Shu and Andreas Bulling", "abstract": "  While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.\n", "link": "http://arxiv.org/abs/2509.05091v1", "date": "2025-09-05", "relevancy": 1.5448, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5436}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5359}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProToM%3A%20Promoting%20Prosocial%20Behaviour%20via%20Theory%20of%20Mind-Informed%0A%20%20Feedback&body=Title%3A%20ProToM%3A%20Promoting%20Prosocial%20Behaviour%20via%20Theory%20of%20Mind-Informed%0A%20%20Feedback%0AAuthor%3A%20Matteo%20Bortoletto%20and%20Yichao%20Zhou%20and%20Lance%20Ying%20and%20Tianmin%20Shu%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20While%20humans%20are%20inherently%20social%20creatures%2C%20the%20challenge%20of%20identifying%0Awhen%20and%20how%20to%20assist%20and%20collaborate%20with%20others%20-%20particularly%20when%20pursuing%0Aindependent%20goals%20-%20can%20hinder%20cooperation.%20To%20address%20this%20challenge%2C%20we%20aim%0Ato%20develop%20an%20AI%20system%20that%20provides%20useful%20feedback%20to%20promote%20prosocial%0Abehaviour%20-%20actions%20that%20benefit%20others%2C%20even%20when%20not%20directly%20aligned%20with%0Aone%27s%20own%20goals.%20We%20introduce%20ProToM%2C%20a%20Theory%20of%20Mind-informed%20facilitator%0Athat%20promotes%20prosocial%20actions%20in%20multi-agent%20systems%20by%20providing%20targeted%2C%0Acontext-sensitive%20feedback%20to%20individual%20agents.%20ProToM%20first%20infers%20agents%27%0Agoals%20using%20Bayesian%20inverse%20planning%2C%20then%20selects%20feedback%20to%20communicate%20by%0Amaximising%20expected%20utility%2C%20conditioned%20on%20the%20inferred%20goal%20distribution.%20We%0Aevaluate%20our%20approach%20against%20baselines%20in%20two%20multi-agent%20environments%3A%20Doors%2C%0AKeys%2C%20and%20Gems%2C%20as%20well%20as%20Overcooked.%20Our%20results%20suggest%20that%0Astate-of-the-art%20large%20language%20and%20reasoning%20models%20fall%20short%20of%0Acommunicating%20feedback%20that%20is%20both%20contextually%20grounded%20and%20well-timed%20-%0Aleading%20to%20higher%20communication%20overhead%20and%20task%20speedup.%20In%20contrast%2C%20ProToM%0Aprovides%20targeted%20and%20helpful%20feedback%2C%20achieving%20a%20higher%20success%20rate%2C%0Ashorter%20task%20completion%20times%2C%20and%20is%20consistently%20preferred%20by%20human%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProToM%253A%2520Promoting%2520Prosocial%2520Behaviour%2520via%2520Theory%2520of%2520Mind-Informed%250A%2520%2520Feedback%26entry.906535625%3DMatteo%2520Bortoletto%2520and%2520Yichao%2520Zhou%2520and%2520Lance%2520Ying%2520and%2520Tianmin%2520Shu%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520While%2520humans%2520are%2520inherently%2520social%2520creatures%252C%2520the%2520challenge%2520of%2520identifying%250Awhen%2520and%2520how%2520to%2520assist%2520and%2520collaborate%2520with%2520others%2520-%2520particularly%2520when%2520pursuing%250Aindependent%2520goals%2520-%2520can%2520hinder%2520cooperation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520aim%250Ato%2520develop%2520an%2520AI%2520system%2520that%2520provides%2520useful%2520feedback%2520to%2520promote%2520prosocial%250Abehaviour%2520-%2520actions%2520that%2520benefit%2520others%252C%2520even%2520when%2520not%2520directly%2520aligned%2520with%250Aone%2527s%2520own%2520goals.%2520We%2520introduce%2520ProToM%252C%2520a%2520Theory%2520of%2520Mind-informed%2520facilitator%250Athat%2520promotes%2520prosocial%2520actions%2520in%2520multi-agent%2520systems%2520by%2520providing%2520targeted%252C%250Acontext-sensitive%2520feedback%2520to%2520individual%2520agents.%2520ProToM%2520first%2520infers%2520agents%2527%250Agoals%2520using%2520Bayesian%2520inverse%2520planning%252C%2520then%2520selects%2520feedback%2520to%2520communicate%2520by%250Amaximising%2520expected%2520utility%252C%2520conditioned%2520on%2520the%2520inferred%2520goal%2520distribution.%2520We%250Aevaluate%2520our%2520approach%2520against%2520baselines%2520in%2520two%2520multi-agent%2520environments%253A%2520Doors%252C%250AKeys%252C%2520and%2520Gems%252C%2520as%2520well%2520as%2520Overcooked.%2520Our%2520results%2520suggest%2520that%250Astate-of-the-art%2520large%2520language%2520and%2520reasoning%2520models%2520fall%2520short%2520of%250Acommunicating%2520feedback%2520that%2520is%2520both%2520contextually%2520grounded%2520and%2520well-timed%2520-%250Aleading%2520to%2520higher%2520communication%2520overhead%2520and%2520task%2520speedup.%2520In%2520contrast%252C%2520ProToM%250Aprovides%2520targeted%2520and%2520helpful%2520feedback%252C%2520achieving%2520a%2520higher%2520success%2520rate%252C%250Ashorter%2520task%2520completion%2520times%252C%2520and%2520is%2520consistently%2520preferred%2520by%2520human%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProToM%3A%20Promoting%20Prosocial%20Behaviour%20via%20Theory%20of%20Mind-Informed%0A%20%20Feedback&entry.906535625=Matteo%20Bortoletto%20and%20Yichao%20Zhou%20and%20Lance%20Ying%20and%20Tianmin%20Shu%20and%20Andreas%20Bulling&entry.1292438233=%20%20While%20humans%20are%20inherently%20social%20creatures%2C%20the%20challenge%20of%20identifying%0Awhen%20and%20how%20to%20assist%20and%20collaborate%20with%20others%20-%20particularly%20when%20pursuing%0Aindependent%20goals%20-%20can%20hinder%20cooperation.%20To%20address%20this%20challenge%2C%20we%20aim%0Ato%20develop%20an%20AI%20system%20that%20provides%20useful%20feedback%20to%20promote%20prosocial%0Abehaviour%20-%20actions%20that%20benefit%20others%2C%20even%20when%20not%20directly%20aligned%20with%0Aone%27s%20own%20goals.%20We%20introduce%20ProToM%2C%20a%20Theory%20of%20Mind-informed%20facilitator%0Athat%20promotes%20prosocial%20actions%20in%20multi-agent%20systems%20by%20providing%20targeted%2C%0Acontext-sensitive%20feedback%20to%20individual%20agents.%20ProToM%20first%20infers%20agents%27%0Agoals%20using%20Bayesian%20inverse%20planning%2C%20then%20selects%20feedback%20to%20communicate%20by%0Amaximising%20expected%20utility%2C%20conditioned%20on%20the%20inferred%20goal%20distribution.%20We%0Aevaluate%20our%20approach%20against%20baselines%20in%20two%20multi-agent%20environments%3A%20Doors%2C%0AKeys%2C%20and%20Gems%2C%20as%20well%20as%20Overcooked.%20Our%20results%20suggest%20that%0Astate-of-the-art%20large%20language%20and%20reasoning%20models%20fall%20short%20of%0Acommunicating%20feedback%20that%20is%20both%20contextually%20grounded%20and%20well-timed%20-%0Aleading%20to%20higher%20communication%20overhead%20and%20task%20speedup.%20In%20contrast%2C%20ProToM%0Aprovides%20targeted%20and%20helpful%20feedback%2C%20achieving%20a%20higher%20success%20rate%2C%0Ashorter%20task%20completion%20times%2C%20and%20is%20consistently%20preferred%20by%20human%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05091v1&entry.124074799=Read"},
{"title": "On Evaluating the Poisoning Robustness of Federated Learning under Local\n  Differential Privacy", "author": "Zijian Wang and Wei Tong and Tingxuan Han and Haoyu Chen and Tianling Zhang and Yunlong Mao and Sheng Zhong", "abstract": "  Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack\n", "link": "http://arxiv.org/abs/2509.05265v1", "date": "2025-09-05", "relevancy": 1.8121, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4653}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Evaluating%20the%20Poisoning%20Robustness%20of%20Federated%20Learning%20under%20Local%0A%20%20Differential%20Privacy&body=Title%3A%20On%20Evaluating%20the%20Poisoning%20Robustness%20of%20Federated%20Learning%20under%20Local%0A%20%20Differential%20Privacy%0AAuthor%3A%20Zijian%20Wang%20and%20Wei%20Tong%20and%20Tingxuan%20Han%20and%20Haoyu%20Chen%20and%20Tianling%20Zhang%20and%20Yunlong%20Mao%20and%20Sheng%20Zhong%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20combined%20with%20local%20differential%20privacy%20%28LDP%29%0Aenables%20privacy-preserving%20model%20training%20across%20decentralized%20data%20sources.%0AHowever%2C%20the%20decentralized%20data-management%20paradigm%20leaves%20LDPFL%20vulnerable%20to%0Aparticipants%20with%20malicious%20intent.%20The%20robustness%20of%20LDPFL%20protocols%2C%0Aparticularly%20against%20model%20poisoning%20attacks%20%28MPA%29%2C%20where%20adversaries%20inject%0Amalicious%20updates%20to%20disrupt%20global%20model%20convergence%2C%20remains%20insufficiently%0Astudied.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20extensible%20model%20poisoning%0Aattack%20framework%20tailored%20for%20LDPFL%20settings.%20Our%20approach%20is%20driven%20by%20the%0Aobjective%20of%20maximizing%20the%20global%20training%20loss%20while%20adhering%20to%20local%0Aprivacy%20constraints.%20To%20counter%20robust%20aggregation%20mechanisms%20such%20as%0AMulti-Krum%20and%20trimmed%20mean%2C%20we%20develop%20adaptive%20attacks%20that%20embed%20carefully%0Acrafted%20constraints%20into%20a%20reverse%20training%20process%2C%20enabling%20evasion%20of%20these%0Adefenses.%20We%20evaluate%20our%20framework%20across%20three%20representative%20LDPFL%0Aprotocols%2C%20three%20benchmark%20datasets%2C%20and%20two%20types%20of%20deep%20neural%20networks.%0AAdditionally%2C%20we%20investigate%20the%20influence%20of%20data%20heterogeneity%20and%20privacy%0Abudgets%20on%20attack%20effectiveness.%20Experimental%20results%20demonstrate%20that%20our%0Aadaptive%20attacks%20can%20significantly%20degrade%20the%20performance%20of%20the%20global%20model%2C%0Arevealing%20critical%20vulnerabilities%20and%20highlighting%20the%20need%20for%20more%20robust%0ALDPFL%20defense%20strategies%20against%20MPA.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZiJW/LDPFL-Attack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Evaluating%2520the%2520Poisoning%2520Robustness%2520of%2520Federated%2520Learning%2520under%2520Local%250A%2520%2520Differential%2520Privacy%26entry.906535625%3DZijian%2520Wang%2520and%2520Wei%2520Tong%2520and%2520Tingxuan%2520Han%2520and%2520Haoyu%2520Chen%2520and%2520Tianling%2520Zhang%2520and%2520Yunlong%2520Mao%2520and%2520Sheng%2520Zhong%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520combined%2520with%2520local%2520differential%2520privacy%2520%2528LDP%2529%250Aenables%2520privacy-preserving%2520model%2520training%2520across%2520decentralized%2520data%2520sources.%250AHowever%252C%2520the%2520decentralized%2520data-management%2520paradigm%2520leaves%2520LDPFL%2520vulnerable%2520to%250Aparticipants%2520with%2520malicious%2520intent.%2520The%2520robustness%2520of%2520LDPFL%2520protocols%252C%250Aparticularly%2520against%2520model%2520poisoning%2520attacks%2520%2528MPA%2529%252C%2520where%2520adversaries%2520inject%250Amalicious%2520updates%2520to%2520disrupt%2520global%2520model%2520convergence%252C%2520remains%2520insufficiently%250Astudied.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520extensible%2520model%2520poisoning%250Aattack%2520framework%2520tailored%2520for%2520LDPFL%2520settings.%2520Our%2520approach%2520is%2520driven%2520by%2520the%250Aobjective%2520of%2520maximizing%2520the%2520global%2520training%2520loss%2520while%2520adhering%2520to%2520local%250Aprivacy%2520constraints.%2520To%2520counter%2520robust%2520aggregation%2520mechanisms%2520such%2520as%250AMulti-Krum%2520and%2520trimmed%2520mean%252C%2520we%2520develop%2520adaptive%2520attacks%2520that%2520embed%2520carefully%250Acrafted%2520constraints%2520into%2520a%2520reverse%2520training%2520process%252C%2520enabling%2520evasion%2520of%2520these%250Adefenses.%2520We%2520evaluate%2520our%2520framework%2520across%2520three%2520representative%2520LDPFL%250Aprotocols%252C%2520three%2520benchmark%2520datasets%252C%2520and%2520two%2520types%2520of%2520deep%2520neural%2520networks.%250AAdditionally%252C%2520we%2520investigate%2520the%2520influence%2520of%2520data%2520heterogeneity%2520and%2520privacy%250Abudgets%2520on%2520attack%2520effectiveness.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aadaptive%2520attacks%2520can%2520significantly%2520degrade%2520the%2520performance%2520of%2520the%2520global%2520model%252C%250Arevealing%2520critical%2520vulnerabilities%2520and%2520highlighting%2520the%2520need%2520for%2520more%2520robust%250ALDPFL%2520defense%2520strategies%2520against%2520MPA.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZiJW/LDPFL-Attack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Evaluating%20the%20Poisoning%20Robustness%20of%20Federated%20Learning%20under%20Local%0A%20%20Differential%20Privacy&entry.906535625=Zijian%20Wang%20and%20Wei%20Tong%20and%20Tingxuan%20Han%20and%20Haoyu%20Chen%20and%20Tianling%20Zhang%20and%20Yunlong%20Mao%20and%20Sheng%20Zhong&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20combined%20with%20local%20differential%20privacy%20%28LDP%29%0Aenables%20privacy-preserving%20model%20training%20across%20decentralized%20data%20sources.%0AHowever%2C%20the%20decentralized%20data-management%20paradigm%20leaves%20LDPFL%20vulnerable%20to%0Aparticipants%20with%20malicious%20intent.%20The%20robustness%20of%20LDPFL%20protocols%2C%0Aparticularly%20against%20model%20poisoning%20attacks%20%28MPA%29%2C%20where%20adversaries%20inject%0Amalicious%20updates%20to%20disrupt%20global%20model%20convergence%2C%20remains%20insufficiently%0Astudied.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20extensible%20model%20poisoning%0Aattack%20framework%20tailored%20for%20LDPFL%20settings.%20Our%20approach%20is%20driven%20by%20the%0Aobjective%20of%20maximizing%20the%20global%20training%20loss%20while%20adhering%20to%20local%0Aprivacy%20constraints.%20To%20counter%20robust%20aggregation%20mechanisms%20such%20as%0AMulti-Krum%20and%20trimmed%20mean%2C%20we%20develop%20adaptive%20attacks%20that%20embed%20carefully%0Acrafted%20constraints%20into%20a%20reverse%20training%20process%2C%20enabling%20evasion%20of%20these%0Adefenses.%20We%20evaluate%20our%20framework%20across%20three%20representative%20LDPFL%0Aprotocols%2C%20three%20benchmark%20datasets%2C%20and%20two%20types%20of%20deep%20neural%20networks.%0AAdditionally%2C%20we%20investigate%20the%20influence%20of%20data%20heterogeneity%20and%20privacy%0Abudgets%20on%20attack%20effectiveness.%20Experimental%20results%20demonstrate%20that%20our%0Aadaptive%20attacks%20can%20significantly%20degrade%20the%20performance%20of%20the%20global%20model%2C%0Arevealing%20critical%20vulnerabilities%20and%20highlighting%20the%20need%20for%20more%20robust%0ALDPFL%20defense%20strategies%20against%20MPA.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZiJW/LDPFL-Attack%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05265v1&entry.124074799=Read"},
{"title": "Epistemic Skills: Reasoning about Knowledge and Oblivion", "author": "Xiaolong Liang and Y\u00ec N. W\u00e1ng", "abstract": "  This paper presents a class of epistemic logics that captures the dynamics of\nacquiring knowledge and descending into oblivion, while incorporating concepts\nof group knowledge. The approach is grounded in a system of weighted models,\nintroducing an ``epistemic skills'' metric to represent the epistemic\ncapacities tied to knowledge updates. Within this framework, knowledge\nacquisition is modeled as a process of upskilling, whereas oblivion is\nrepresented as a consequence of downskilling. The framework further enables\nexploration of ``knowability'' and ``forgettability,'' defined as the potential\nto gain knowledge through upskilling and to lapse into oblivion through\ndownskilling, respectively. Additionally, it supports a detailed analysis of\nthe distinctions between epistemic de re and de dicto expressions. The\ncomputational complexity of the model checking and satisfiability problems is\nexamined, offering insights into their theoretical foundations and practical\nimplications.\n", "link": "http://arxiv.org/abs/2504.01733v2", "date": "2025-09-05", "relevancy": 1.2789, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3892}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epistemic%20Skills%3A%20Reasoning%20about%20Knowledge%20and%20Oblivion&body=Title%3A%20Epistemic%20Skills%3A%20Reasoning%20about%20Knowledge%20and%20Oblivion%0AAuthor%3A%20Xiaolong%20Liang%20and%20Y%C3%AC%20N.%20W%C3%A1ng%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20class%20of%20epistemic%20logics%20that%20captures%20the%20dynamics%20of%0Aacquiring%20knowledge%20and%20descending%20into%20oblivion%2C%20while%20incorporating%20concepts%0Aof%20group%20knowledge.%20The%20approach%20is%20grounded%20in%20a%20system%20of%20weighted%20models%2C%0Aintroducing%20an%20%60%60epistemic%20skills%27%27%20metric%20to%20represent%20the%20epistemic%0Acapacities%20tied%20to%20knowledge%20updates.%20Within%20this%20framework%2C%20knowledge%0Aacquisition%20is%20modeled%20as%20a%20process%20of%20upskilling%2C%20whereas%20oblivion%20is%0Arepresented%20as%20a%20consequence%20of%20downskilling.%20The%20framework%20further%20enables%0Aexploration%20of%20%60%60knowability%27%27%20and%20%60%60forgettability%2C%27%27%20defined%20as%20the%20potential%0Ato%20gain%20knowledge%20through%20upskilling%20and%20to%20lapse%20into%20oblivion%20through%0Adownskilling%2C%20respectively.%20Additionally%2C%20it%20supports%20a%20detailed%20analysis%20of%0Athe%20distinctions%20between%20epistemic%20de%20re%20and%20de%20dicto%20expressions.%20The%0Acomputational%20complexity%20of%20the%20model%20checking%20and%20satisfiability%20problems%20is%0Aexamined%2C%20offering%20insights%20into%20their%20theoretical%20foundations%20and%20practical%0Aimplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpistemic%2520Skills%253A%2520Reasoning%2520about%2520Knowledge%2520and%2520Oblivion%26entry.906535625%3DXiaolong%2520Liang%2520and%2520Y%25C3%25AC%2520N.%2520W%25C3%25A1ng%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520class%2520of%2520epistemic%2520logics%2520that%2520captures%2520the%2520dynamics%2520of%250Aacquiring%2520knowledge%2520and%2520descending%2520into%2520oblivion%252C%2520while%2520incorporating%2520concepts%250Aof%2520group%2520knowledge.%2520The%2520approach%2520is%2520grounded%2520in%2520a%2520system%2520of%2520weighted%2520models%252C%250Aintroducing%2520an%2520%2560%2560epistemic%2520skills%2527%2527%2520metric%2520to%2520represent%2520the%2520epistemic%250Acapacities%2520tied%2520to%2520knowledge%2520updates.%2520Within%2520this%2520framework%252C%2520knowledge%250Aacquisition%2520is%2520modeled%2520as%2520a%2520process%2520of%2520upskilling%252C%2520whereas%2520oblivion%2520is%250Arepresented%2520as%2520a%2520consequence%2520of%2520downskilling.%2520The%2520framework%2520further%2520enables%250Aexploration%2520of%2520%2560%2560knowability%2527%2527%2520and%2520%2560%2560forgettability%252C%2527%2527%2520defined%2520as%2520the%2520potential%250Ato%2520gain%2520knowledge%2520through%2520upskilling%2520and%2520to%2520lapse%2520into%2520oblivion%2520through%250Adownskilling%252C%2520respectively.%2520Additionally%252C%2520it%2520supports%2520a%2520detailed%2520analysis%2520of%250Athe%2520distinctions%2520between%2520epistemic%2520de%2520re%2520and%2520de%2520dicto%2520expressions.%2520The%250Acomputational%2520complexity%2520of%2520the%2520model%2520checking%2520and%2520satisfiability%2520problems%2520is%250Aexamined%252C%2520offering%2520insights%2520into%2520their%2520theoretical%2520foundations%2520and%2520practical%250Aimplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epistemic%20Skills%3A%20Reasoning%20about%20Knowledge%20and%20Oblivion&entry.906535625=Xiaolong%20Liang%20and%20Y%C3%AC%20N.%20W%C3%A1ng&entry.1292438233=%20%20This%20paper%20presents%20a%20class%20of%20epistemic%20logics%20that%20captures%20the%20dynamics%20of%0Aacquiring%20knowledge%20and%20descending%20into%20oblivion%2C%20while%20incorporating%20concepts%0Aof%20group%20knowledge.%20The%20approach%20is%20grounded%20in%20a%20system%20of%20weighted%20models%2C%0Aintroducing%20an%20%60%60epistemic%20skills%27%27%20metric%20to%20represent%20the%20epistemic%0Acapacities%20tied%20to%20knowledge%20updates.%20Within%20this%20framework%2C%20knowledge%0Aacquisition%20is%20modeled%20as%20a%20process%20of%20upskilling%2C%20whereas%20oblivion%20is%0Arepresented%20as%20a%20consequence%20of%20downskilling.%20The%20framework%20further%20enables%0Aexploration%20of%20%60%60knowability%27%27%20and%20%60%60forgettability%2C%27%27%20defined%20as%20the%20potential%0Ato%20gain%20knowledge%20through%20upskilling%20and%20to%20lapse%20into%20oblivion%20through%0Adownskilling%2C%20respectively.%20Additionally%2C%20it%20supports%20a%20detailed%20analysis%20of%0Athe%20distinctions%20between%20epistemic%20de%20re%20and%20de%20dicto%20expressions.%20The%0Acomputational%20complexity%20of%20the%20model%20checking%20and%20satisfiability%20problems%20is%0Aexamined%2C%20offering%20insights%20into%20their%20theoretical%20foundations%20and%20practical%0Aimplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01733v2&entry.124074799=Read"},
{"title": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based\n  Mobile Agents", "author": "Xuan Wang and Siyuan Liang and Zhe Liu and Yi Yu and Aishan Liu and Yuliang Lu and Xitong Gao and Ee-Chien Chang", "abstract": "  Mobile agents powered by vision-language models (VLMs) are increasingly\nadopted for tasks such as UI automation and camera-based assistance. These\nagents are typically fine-tuned using small-scale, user-collected data, making\nthem susceptible to stealthy training-time threats. This work introduces VIBMA,\nthe first clean-text backdoor attack targeting VLM-based mobile agents. The\nattack injects malicious behaviors into the model by modifying only the visual\ninput while preserving textual prompts and instructions, achieving stealth\nthrough the complete absence of textual anomalies. Once the agent is fine-tuned\non this poisoned data, adding a predefined visual pattern (trigger) at\ninference time activates the attacker-specified behavior (backdoor). Our attack\naligns the training gradients of poisoned samples with those of an\nattacker-specified target instance, effectively embedding backdoor-specific\nfeatures into the poisoned data. To ensure the robustness and stealthiness of\nthe attack, we design three trigger variants that better resemble real-world\nscenarios: static patches, dynamic motion patterns, and low-opacity blended\ncontent. Extensive experiments on six Android applications and three\nmobile-compatible VLMs demonstrate that our attack achieves high success rates\n(ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We\nfurther conduct ablation studies to understand how key design factors impact\nattack reliability and stealth. These findings is the first to reveal the\nsecurity vulnerabilities of mobile agents and their susceptibility to backdoor\ninjection, underscoring the need for robust defenses in mobile agent adaptation\npipelines.\n", "link": "http://arxiv.org/abs/2506.13205v6", "date": "2025-09-05", "relevancy": 1.8493, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4717}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4592}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poison%20Once%2C%20Control%20Anywhere%3A%20Clean-Text%20Visual%20Backdoors%20in%20VLM-based%0A%20%20Mobile%20Agents&body=Title%3A%20Poison%20Once%2C%20Control%20Anywhere%3A%20Clean-Text%20Visual%20Backdoors%20in%20VLM-based%0A%20%20Mobile%20Agents%0AAuthor%3A%20Xuan%20Wang%20and%20Siyuan%20Liang%20and%20Zhe%20Liu%20and%20Yi%20Yu%20and%20Aishan%20Liu%20and%20Yuliang%20Lu%20and%20Xitong%20Gao%20and%20Ee-Chien%20Chang%0AAbstract%3A%20%20%20Mobile%20agents%20powered%20by%20vision-language%20models%20%28VLMs%29%20are%20increasingly%0Aadopted%20for%20tasks%20such%20as%20UI%20automation%20and%20camera-based%20assistance.%20These%0Aagents%20are%20typically%20fine-tuned%20using%20small-scale%2C%20user-collected%20data%2C%20making%0Athem%20susceptible%20to%20stealthy%20training-time%20threats.%20This%20work%20introduces%20VIBMA%2C%0Athe%20first%20clean-text%20backdoor%20attack%20targeting%20VLM-based%20mobile%20agents.%20The%0Aattack%20injects%20malicious%20behaviors%20into%20the%20model%20by%20modifying%20only%20the%20visual%0Ainput%20while%20preserving%20textual%20prompts%20and%20instructions%2C%20achieving%20stealth%0Athrough%20the%20complete%20absence%20of%20textual%20anomalies.%20Once%20the%20agent%20is%20fine-tuned%0Aon%20this%20poisoned%20data%2C%20adding%20a%20predefined%20visual%20pattern%20%28trigger%29%20at%0Ainference%20time%20activates%20the%20attacker-specified%20behavior%20%28backdoor%29.%20Our%20attack%0Aaligns%20the%20training%20gradients%20of%20poisoned%20samples%20with%20those%20of%20an%0Aattacker-specified%20target%20instance%2C%20effectively%20embedding%20backdoor-specific%0Afeatures%20into%20the%20poisoned%20data.%20To%20ensure%20the%20robustness%20and%20stealthiness%20of%0Athe%20attack%2C%20we%20design%20three%20trigger%20variants%20that%20better%20resemble%20real-world%0Ascenarios%3A%20static%20patches%2C%20dynamic%20motion%20patterns%2C%20and%20low-opacity%20blended%0Acontent.%20Extensive%20experiments%20on%20six%20Android%20applications%20and%20three%0Amobile-compatible%20VLMs%20demonstrate%20that%20our%20attack%20achieves%20high%20success%20rates%0A%28ASR%20up%20to%2094.67%25%29%20while%20preserving%20clean-task%20behavior%20%28FSR%20up%20to%2095.85%25%29.%20We%0Afurther%20conduct%20ablation%20studies%20to%20understand%20how%20key%20design%20factors%20impact%0Aattack%20reliability%20and%20stealth.%20These%20findings%20is%20the%20first%20to%20reveal%20the%0Asecurity%20vulnerabilities%20of%20mobile%20agents%20and%20their%20susceptibility%20to%20backdoor%0Ainjection%2C%20underscoring%20the%20need%20for%20robust%20defenses%20in%20mobile%20agent%20adaptation%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13205v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoison%2520Once%252C%2520Control%2520Anywhere%253A%2520Clean-Text%2520Visual%2520Backdoors%2520in%2520VLM-based%250A%2520%2520Mobile%2520Agents%26entry.906535625%3DXuan%2520Wang%2520and%2520Siyuan%2520Liang%2520and%2520Zhe%2520Liu%2520and%2520Yi%2520Yu%2520and%2520Aishan%2520Liu%2520and%2520Yuliang%2520Lu%2520and%2520Xitong%2520Gao%2520and%2520Ee-Chien%2520Chang%26entry.1292438233%3D%2520%2520Mobile%2520agents%2520powered%2520by%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520increasingly%250Aadopted%2520for%2520tasks%2520such%2520as%2520UI%2520automation%2520and%2520camera-based%2520assistance.%2520These%250Aagents%2520are%2520typically%2520fine-tuned%2520using%2520small-scale%252C%2520user-collected%2520data%252C%2520making%250Athem%2520susceptible%2520to%2520stealthy%2520training-time%2520threats.%2520This%2520work%2520introduces%2520VIBMA%252C%250Athe%2520first%2520clean-text%2520backdoor%2520attack%2520targeting%2520VLM-based%2520mobile%2520agents.%2520The%250Aattack%2520injects%2520malicious%2520behaviors%2520into%2520the%2520model%2520by%2520modifying%2520only%2520the%2520visual%250Ainput%2520while%2520preserving%2520textual%2520prompts%2520and%2520instructions%252C%2520achieving%2520stealth%250Athrough%2520the%2520complete%2520absence%2520of%2520textual%2520anomalies.%2520Once%2520the%2520agent%2520is%2520fine-tuned%250Aon%2520this%2520poisoned%2520data%252C%2520adding%2520a%2520predefined%2520visual%2520pattern%2520%2528trigger%2529%2520at%250Ainference%2520time%2520activates%2520the%2520attacker-specified%2520behavior%2520%2528backdoor%2529.%2520Our%2520attack%250Aaligns%2520the%2520training%2520gradients%2520of%2520poisoned%2520samples%2520with%2520those%2520of%2520an%250Aattacker-specified%2520target%2520instance%252C%2520effectively%2520embedding%2520backdoor-specific%250Afeatures%2520into%2520the%2520poisoned%2520data.%2520To%2520ensure%2520the%2520robustness%2520and%2520stealthiness%2520of%250Athe%2520attack%252C%2520we%2520design%2520three%2520trigger%2520variants%2520that%2520better%2520resemble%2520real-world%250Ascenarios%253A%2520static%2520patches%252C%2520dynamic%2520motion%2520patterns%252C%2520and%2520low-opacity%2520blended%250Acontent.%2520Extensive%2520experiments%2520on%2520six%2520Android%2520applications%2520and%2520three%250Amobile-compatible%2520VLMs%2520demonstrate%2520that%2520our%2520attack%2520achieves%2520high%2520success%2520rates%250A%2528ASR%2520up%2520to%252094.67%2525%2529%2520while%2520preserving%2520clean-task%2520behavior%2520%2528FSR%2520up%2520to%252095.85%2525%2529.%2520We%250Afurther%2520conduct%2520ablation%2520studies%2520to%2520understand%2520how%2520key%2520design%2520factors%2520impact%250Aattack%2520reliability%2520and%2520stealth.%2520These%2520findings%2520is%2520the%2520first%2520to%2520reveal%2520the%250Asecurity%2520vulnerabilities%2520of%2520mobile%2520agents%2520and%2520their%2520susceptibility%2520to%2520backdoor%250Ainjection%252C%2520underscoring%2520the%2520need%2520for%2520robust%2520defenses%2520in%2520mobile%2520agent%2520adaptation%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13205v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poison%20Once%2C%20Control%20Anywhere%3A%20Clean-Text%20Visual%20Backdoors%20in%20VLM-based%0A%20%20Mobile%20Agents&entry.906535625=Xuan%20Wang%20and%20Siyuan%20Liang%20and%20Zhe%20Liu%20and%20Yi%20Yu%20and%20Aishan%20Liu%20and%20Yuliang%20Lu%20and%20Xitong%20Gao%20and%20Ee-Chien%20Chang&entry.1292438233=%20%20Mobile%20agents%20powered%20by%20vision-language%20models%20%28VLMs%29%20are%20increasingly%0Aadopted%20for%20tasks%20such%20as%20UI%20automation%20and%20camera-based%20assistance.%20These%0Aagents%20are%20typically%20fine-tuned%20using%20small-scale%2C%20user-collected%20data%2C%20making%0Athem%20susceptible%20to%20stealthy%20training-time%20threats.%20This%20work%20introduces%20VIBMA%2C%0Athe%20first%20clean-text%20backdoor%20attack%20targeting%20VLM-based%20mobile%20agents.%20The%0Aattack%20injects%20malicious%20behaviors%20into%20the%20model%20by%20modifying%20only%20the%20visual%0Ainput%20while%20preserving%20textual%20prompts%20and%20instructions%2C%20achieving%20stealth%0Athrough%20the%20complete%20absence%20of%20textual%20anomalies.%20Once%20the%20agent%20is%20fine-tuned%0Aon%20this%20poisoned%20data%2C%20adding%20a%20predefined%20visual%20pattern%20%28trigger%29%20at%0Ainference%20time%20activates%20the%20attacker-specified%20behavior%20%28backdoor%29.%20Our%20attack%0Aaligns%20the%20training%20gradients%20of%20poisoned%20samples%20with%20those%20of%20an%0Aattacker-specified%20target%20instance%2C%20effectively%20embedding%20backdoor-specific%0Afeatures%20into%20the%20poisoned%20data.%20To%20ensure%20the%20robustness%20and%20stealthiness%20of%0Athe%20attack%2C%20we%20design%20three%20trigger%20variants%20that%20better%20resemble%20real-world%0Ascenarios%3A%20static%20patches%2C%20dynamic%20motion%20patterns%2C%20and%20low-opacity%20blended%0Acontent.%20Extensive%20experiments%20on%20six%20Android%20applications%20and%20three%0Amobile-compatible%20VLMs%20demonstrate%20that%20our%20attack%20achieves%20high%20success%20rates%0A%28ASR%20up%20to%2094.67%25%29%20while%20preserving%20clean-task%20behavior%20%28FSR%20up%20to%2095.85%25%29.%20We%0Afurther%20conduct%20ablation%20studies%20to%20understand%20how%20key%20design%20factors%20impact%0Aattack%20reliability%20and%20stealth.%20These%20findings%20is%20the%20first%20to%20reveal%20the%0Asecurity%20vulnerabilities%20of%20mobile%20agents%20and%20their%20susceptibility%20to%20backdoor%0Ainjection%2C%20underscoring%20the%20need%20for%20robust%20defenses%20in%20mobile%20agent%20adaptation%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13205v6&entry.124074799=Read"},
{"title": "Combining feature-based approaches with graph neural networks and\n  symbolic regression for synergistic performance and interpretability", "author": "Rog\u00e9rio Almeida Gouv\u00eaa and Pierre-Paul De Breuck and Tatiane Pretto and Gian-Marco Rignanese and Marcos Jos\u00e9 Leite Santos", "abstract": "  This study introduces MatterVial, an innovative hybrid framework for\nfeature-based machine learning in materials science. MatterVial expands the\nfeature space by integrating latent representations from a diverse suite of\npretrained graph neural network (GNN) models including: structure-based\n(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, with\ncomputationally efficient, GNN-approximated descriptors and novel features from\nsymbolic regression. Our approach combines the chemical transparency of\ntraditional feature-based models with the predictive power of deep learning\narchitectures. When augmenting the feature-based model MODNet on Matbench\ntasks, this method yields significant error reductions and elevates its\nperformance to be competitive with, and in several cases superior to,\nstate-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% for\nmultiple tasks. An integrated interpretability module, employing surrogate\nmodels and symbolic regression, decodes the latent GNN-derived descriptors into\nexplicit, physically meaningful formulas. This unified framework advances\nmaterials informatics by providing a high-performance, transparent tool that\naligns with the principles of explainable AI, paving the way for more targeted\nand autonomous materials discovery.\n", "link": "http://arxiv.org/abs/2509.03547v2", "date": "2025-09-05", "relevancy": 1.4772, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5243}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20feature-based%20approaches%20with%20graph%20neural%20networks%20and%0A%20%20symbolic%20regression%20for%20synergistic%20performance%20and%20interpretability&body=Title%3A%20Combining%20feature-based%20approaches%20with%20graph%20neural%20networks%20and%0A%20%20symbolic%20regression%20for%20synergistic%20performance%20and%20interpretability%0AAuthor%3A%20Rog%C3%A9rio%20Almeida%20Gouv%C3%AAa%20and%20Pierre-Paul%20De%20Breuck%20and%20Tatiane%20Pretto%20and%20Gian-Marco%20Rignanese%20and%20Marcos%20Jos%C3%A9%20Leite%20Santos%0AAbstract%3A%20%20%20This%20study%20introduces%20MatterVial%2C%20an%20innovative%20hybrid%20framework%20for%0Afeature-based%20machine%20learning%20in%20materials%20science.%20MatterVial%20expands%20the%0Afeature%20space%20by%20integrating%20latent%20representations%20from%20a%20diverse%20suite%20of%0Apretrained%20graph%20neural%20network%20%28GNN%29%20models%20including%3A%20structure-based%0A%28MEGNet%29%2C%20composition-based%20%28ROOST%29%2C%20and%20equivariant%20%28ORB%29%20graph%20networks%2C%20with%0Acomputationally%20efficient%2C%20GNN-approximated%20descriptors%20and%20novel%20features%20from%0Asymbolic%20regression.%20Our%20approach%20combines%20the%20chemical%20transparency%20of%0Atraditional%20feature-based%20models%20with%20the%20predictive%20power%20of%20deep%20learning%0Aarchitectures.%20When%20augmenting%20the%20feature-based%20model%20MODNet%20on%20Matbench%0Atasks%2C%20this%20method%20yields%20significant%20error%20reductions%20and%20elevates%20its%0Aperformance%20to%20be%20competitive%20with%2C%20and%20in%20several%20cases%20superior%20to%2C%0Astate-of-the-art%20end-to-end%20GNNs%2C%20with%20accuracy%20increases%20exceeding%2040%25%20for%0Amultiple%20tasks.%20An%20integrated%20interpretability%20module%2C%20employing%20surrogate%0Amodels%20and%20symbolic%20regression%2C%20decodes%20the%20latent%20GNN-derived%20descriptors%20into%0Aexplicit%2C%20physically%20meaningful%20formulas.%20This%20unified%20framework%20advances%0Amaterials%20informatics%20by%20providing%20a%20high-performance%2C%20transparent%20tool%20that%0Aaligns%20with%20the%20principles%20of%20explainable%20AI%2C%20paving%20the%20way%20for%20more%20targeted%0Aand%20autonomous%20materials%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520feature-based%2520approaches%2520with%2520graph%2520neural%2520networks%2520and%250A%2520%2520symbolic%2520regression%2520for%2520synergistic%2520performance%2520and%2520interpretability%26entry.906535625%3DRog%25C3%25A9rio%2520Almeida%2520Gouv%25C3%25AAa%2520and%2520Pierre-Paul%2520De%2520Breuck%2520and%2520Tatiane%2520Pretto%2520and%2520Gian-Marco%2520Rignanese%2520and%2520Marcos%2520Jos%25C3%25A9%2520Leite%2520Santos%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520MatterVial%252C%2520an%2520innovative%2520hybrid%2520framework%2520for%250Afeature-based%2520machine%2520learning%2520in%2520materials%2520science.%2520MatterVial%2520expands%2520the%250Afeature%2520space%2520by%2520integrating%2520latent%2520representations%2520from%2520a%2520diverse%2520suite%2520of%250Apretrained%2520graph%2520neural%2520network%2520%2528GNN%2529%2520models%2520including%253A%2520structure-based%250A%2528MEGNet%2529%252C%2520composition-based%2520%2528ROOST%2529%252C%2520and%2520equivariant%2520%2528ORB%2529%2520graph%2520networks%252C%2520with%250Acomputationally%2520efficient%252C%2520GNN-approximated%2520descriptors%2520and%2520novel%2520features%2520from%250Asymbolic%2520regression.%2520Our%2520approach%2520combines%2520the%2520chemical%2520transparency%2520of%250Atraditional%2520feature-based%2520models%2520with%2520the%2520predictive%2520power%2520of%2520deep%2520learning%250Aarchitectures.%2520When%2520augmenting%2520the%2520feature-based%2520model%2520MODNet%2520on%2520Matbench%250Atasks%252C%2520this%2520method%2520yields%2520significant%2520error%2520reductions%2520and%2520elevates%2520its%250Aperformance%2520to%2520be%2520competitive%2520with%252C%2520and%2520in%2520several%2520cases%2520superior%2520to%252C%250Astate-of-the-art%2520end-to-end%2520GNNs%252C%2520with%2520accuracy%2520increases%2520exceeding%252040%2525%2520for%250Amultiple%2520tasks.%2520An%2520integrated%2520interpretability%2520module%252C%2520employing%2520surrogate%250Amodels%2520and%2520symbolic%2520regression%252C%2520decodes%2520the%2520latent%2520GNN-derived%2520descriptors%2520into%250Aexplicit%252C%2520physically%2520meaningful%2520formulas.%2520This%2520unified%2520framework%2520advances%250Amaterials%2520informatics%2520by%2520providing%2520a%2520high-performance%252C%2520transparent%2520tool%2520that%250Aaligns%2520with%2520the%2520principles%2520of%2520explainable%2520AI%252C%2520paving%2520the%2520way%2520for%2520more%2520targeted%250Aand%2520autonomous%2520materials%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20feature-based%20approaches%20with%20graph%20neural%20networks%20and%0A%20%20symbolic%20regression%20for%20synergistic%20performance%20and%20interpretability&entry.906535625=Rog%C3%A9rio%20Almeida%20Gouv%C3%AAa%20and%20Pierre-Paul%20De%20Breuck%20and%20Tatiane%20Pretto%20and%20Gian-Marco%20Rignanese%20and%20Marcos%20Jos%C3%A9%20Leite%20Santos&entry.1292438233=%20%20This%20study%20introduces%20MatterVial%2C%20an%20innovative%20hybrid%20framework%20for%0Afeature-based%20machine%20learning%20in%20materials%20science.%20MatterVial%20expands%20the%0Afeature%20space%20by%20integrating%20latent%20representations%20from%20a%20diverse%20suite%20of%0Apretrained%20graph%20neural%20network%20%28GNN%29%20models%20including%3A%20structure-based%0A%28MEGNet%29%2C%20composition-based%20%28ROOST%29%2C%20and%20equivariant%20%28ORB%29%20graph%20networks%2C%20with%0Acomputationally%20efficient%2C%20GNN-approximated%20descriptors%20and%20novel%20features%20from%0Asymbolic%20regression.%20Our%20approach%20combines%20the%20chemical%20transparency%20of%0Atraditional%20feature-based%20models%20with%20the%20predictive%20power%20of%20deep%20learning%0Aarchitectures.%20When%20augmenting%20the%20feature-based%20model%20MODNet%20on%20Matbench%0Atasks%2C%20this%20method%20yields%20significant%20error%20reductions%20and%20elevates%20its%0Aperformance%20to%20be%20competitive%20with%2C%20and%20in%20several%20cases%20superior%20to%2C%0Astate-of-the-art%20end-to-end%20GNNs%2C%20with%20accuracy%20increases%20exceeding%2040%25%20for%0Amultiple%20tasks.%20An%20integrated%20interpretability%20module%2C%20employing%20surrogate%0Amodels%20and%20symbolic%20regression%2C%20decodes%20the%20latent%20GNN-derived%20descriptors%20into%0Aexplicit%2C%20physically%20meaningful%20formulas.%20This%20unified%20framework%20advances%0Amaterials%20informatics%20by%20providing%20a%20high-performance%2C%20transparent%20tool%20that%0Aaligns%20with%20the%20principles%20of%20explainable%20AI%2C%20paving%20the%20way%20for%20more%20targeted%0Aand%20autonomous%20materials%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03547v2&entry.124074799=Read"},
{"title": "Hierarchical Multi-agent Reinforcement Learning for Cyber Network\n  Defense", "author": "Aditya Vikram Singh and Ethan Rathbun and Emma Graham and Lisa Oakley and Simona Boboila and Alina Oprea and Peter Chin", "abstract": "  Recent advances in multi-agent reinforcement learning (MARL) have created\nopportunities to solve complex real-world tasks. Cybersecurity is a notable\napplication area, where defending networks against sophisticated adversaries\nremains a challenging task typically performed by teams of security operators.\nIn this work, we explore novel MARL strategies for building autonomous cyber\nnetwork defenses that address challenges such as large policy spaces, partial\nobservability, and stealthy, deceptive adversarial strategies. To facilitate\nefficient and generalized learning, we propose a hierarchical Proximal Policy\nOptimization (PPO) architecture that decomposes the cyber defense task into\nspecific sub-tasks like network investigation and host recovery. Our approach\ninvolves training sub-policies for each sub-task using PPO enhanced with\ncybersecurity domain expertise. These sub-policies are then leveraged by a\nmaster defense policy that coordinates their selection to solve complex network\ndefense tasks. Furthermore, the sub-policies can be fine-tuned and transferred\nwith minimal cost to defend against shifts in adversarial behavior or changes\nin network settings. We conduct extensive experiments using CybORG Cage 4, the\nstate-of-the-art MARL environment for cyber defense. Comparisons with multiple\nbaselines across different adversaries show that our hierarchical learning\napproach achieves top performance in terms of convergence speed, episodic\nreturn, and several interpretable metrics relevant to cybersecurity, including\nthe fraction of clean machines on the network, precision, and false positives.\n", "link": "http://arxiv.org/abs/2410.17351v3", "date": "2025-09-05", "relevancy": 1.5464, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5158}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multi-agent%20Reinforcement%20Learning%20for%20Cyber%20Network%0A%20%20Defense&body=Title%3A%20Hierarchical%20Multi-agent%20Reinforcement%20Learning%20for%20Cyber%20Network%0A%20%20Defense%0AAuthor%3A%20Aditya%20Vikram%20Singh%20and%20Ethan%20Rathbun%20and%20Emma%20Graham%20and%20Lisa%20Oakley%20and%20Simona%20Boboila%20and%20Alina%20Oprea%20and%20Peter%20Chin%0AAbstract%3A%20%20%20Recent%20advances%20in%20multi-agent%20reinforcement%20learning%20%28MARL%29%20have%20created%0Aopportunities%20to%20solve%20complex%20real-world%20tasks.%20Cybersecurity%20is%20a%20notable%0Aapplication%20area%2C%20where%20defending%20networks%20against%20sophisticated%20adversaries%0Aremains%20a%20challenging%20task%20typically%20performed%20by%20teams%20of%20security%20operators.%0AIn%20this%20work%2C%20we%20explore%20novel%20MARL%20strategies%20for%20building%20autonomous%20cyber%0Anetwork%20defenses%20that%20address%20challenges%20such%20as%20large%20policy%20spaces%2C%20partial%0Aobservability%2C%20and%20stealthy%2C%20deceptive%20adversarial%20strategies.%20To%20facilitate%0Aefficient%20and%20generalized%20learning%2C%20we%20propose%20a%20hierarchical%20Proximal%20Policy%0AOptimization%20%28PPO%29%20architecture%20that%20decomposes%20the%20cyber%20defense%20task%20into%0Aspecific%20sub-tasks%20like%20network%20investigation%20and%20host%20recovery.%20Our%20approach%0Ainvolves%20training%20sub-policies%20for%20each%20sub-task%20using%20PPO%20enhanced%20with%0Acybersecurity%20domain%20expertise.%20These%20sub-policies%20are%20then%20leveraged%20by%20a%0Amaster%20defense%20policy%20that%20coordinates%20their%20selection%20to%20solve%20complex%20network%0Adefense%20tasks.%20Furthermore%2C%20the%20sub-policies%20can%20be%20fine-tuned%20and%20transferred%0Awith%20minimal%20cost%20to%20defend%20against%20shifts%20in%20adversarial%20behavior%20or%20changes%0Ain%20network%20settings.%20We%20conduct%20extensive%20experiments%20using%20CybORG%20Cage%204%2C%20the%0Astate-of-the-art%20MARL%20environment%20for%20cyber%20defense.%20Comparisons%20with%20multiple%0Abaselines%20across%20different%20adversaries%20show%20that%20our%20hierarchical%20learning%0Aapproach%20achieves%20top%20performance%20in%20terms%20of%20convergence%20speed%2C%20episodic%0Areturn%2C%20and%20several%20interpretable%20metrics%20relevant%20to%20cybersecurity%2C%20including%0Athe%20fraction%20of%20clean%20machines%20on%20the%20network%2C%20precision%2C%20and%20false%20positives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multi-agent%2520Reinforcement%2520Learning%2520for%2520Cyber%2520Network%250A%2520%2520Defense%26entry.906535625%3DAditya%2520Vikram%2520Singh%2520and%2520Ethan%2520Rathbun%2520and%2520Emma%2520Graham%2520and%2520Lisa%2520Oakley%2520and%2520Simona%2520Boboila%2520and%2520Alina%2520Oprea%2520and%2520Peter%2520Chin%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520have%2520created%250Aopportunities%2520to%2520solve%2520complex%2520real-world%2520tasks.%2520Cybersecurity%2520is%2520a%2520notable%250Aapplication%2520area%252C%2520where%2520defending%2520networks%2520against%2520sophisticated%2520adversaries%250Aremains%2520a%2520challenging%2520task%2520typically%2520performed%2520by%2520teams%2520of%2520security%2520operators.%250AIn%2520this%2520work%252C%2520we%2520explore%2520novel%2520MARL%2520strategies%2520for%2520building%2520autonomous%2520cyber%250Anetwork%2520defenses%2520that%2520address%2520challenges%2520such%2520as%2520large%2520policy%2520spaces%252C%2520partial%250Aobservability%252C%2520and%2520stealthy%252C%2520deceptive%2520adversarial%2520strategies.%2520To%2520facilitate%250Aefficient%2520and%2520generalized%2520learning%252C%2520we%2520propose%2520a%2520hierarchical%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%2520architecture%2520that%2520decomposes%2520the%2520cyber%2520defense%2520task%2520into%250Aspecific%2520sub-tasks%2520like%2520network%2520investigation%2520and%2520host%2520recovery.%2520Our%2520approach%250Ainvolves%2520training%2520sub-policies%2520for%2520each%2520sub-task%2520using%2520PPO%2520enhanced%2520with%250Acybersecurity%2520domain%2520expertise.%2520These%2520sub-policies%2520are%2520then%2520leveraged%2520by%2520a%250Amaster%2520defense%2520policy%2520that%2520coordinates%2520their%2520selection%2520to%2520solve%2520complex%2520network%250Adefense%2520tasks.%2520Furthermore%252C%2520the%2520sub-policies%2520can%2520be%2520fine-tuned%2520and%2520transferred%250Awith%2520minimal%2520cost%2520to%2520defend%2520against%2520shifts%2520in%2520adversarial%2520behavior%2520or%2520changes%250Ain%2520network%2520settings.%2520We%2520conduct%2520extensive%2520experiments%2520using%2520CybORG%2520Cage%25204%252C%2520the%250Astate-of-the-art%2520MARL%2520environment%2520for%2520cyber%2520defense.%2520Comparisons%2520with%2520multiple%250Abaselines%2520across%2520different%2520adversaries%2520show%2520that%2520our%2520hierarchical%2520learning%250Aapproach%2520achieves%2520top%2520performance%2520in%2520terms%2520of%2520convergence%2520speed%252C%2520episodic%250Areturn%252C%2520and%2520several%2520interpretable%2520metrics%2520relevant%2520to%2520cybersecurity%252C%2520including%250Athe%2520fraction%2520of%2520clean%2520machines%2520on%2520the%2520network%252C%2520precision%252C%2520and%2520false%2520positives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multi-agent%20Reinforcement%20Learning%20for%20Cyber%20Network%0A%20%20Defense&entry.906535625=Aditya%20Vikram%20Singh%20and%20Ethan%20Rathbun%20and%20Emma%20Graham%20and%20Lisa%20Oakley%20and%20Simona%20Boboila%20and%20Alina%20Oprea%20and%20Peter%20Chin&entry.1292438233=%20%20Recent%20advances%20in%20multi-agent%20reinforcement%20learning%20%28MARL%29%20have%20created%0Aopportunities%20to%20solve%20complex%20real-world%20tasks.%20Cybersecurity%20is%20a%20notable%0Aapplication%20area%2C%20where%20defending%20networks%20against%20sophisticated%20adversaries%0Aremains%20a%20challenging%20task%20typically%20performed%20by%20teams%20of%20security%20operators.%0AIn%20this%20work%2C%20we%20explore%20novel%20MARL%20strategies%20for%20building%20autonomous%20cyber%0Anetwork%20defenses%20that%20address%20challenges%20such%20as%20large%20policy%20spaces%2C%20partial%0Aobservability%2C%20and%20stealthy%2C%20deceptive%20adversarial%20strategies.%20To%20facilitate%0Aefficient%20and%20generalized%20learning%2C%20we%20propose%20a%20hierarchical%20Proximal%20Policy%0AOptimization%20%28PPO%29%20architecture%20that%20decomposes%20the%20cyber%20defense%20task%20into%0Aspecific%20sub-tasks%20like%20network%20investigation%20and%20host%20recovery.%20Our%20approach%0Ainvolves%20training%20sub-policies%20for%20each%20sub-task%20using%20PPO%20enhanced%20with%0Acybersecurity%20domain%20expertise.%20These%20sub-policies%20are%20then%20leveraged%20by%20a%0Amaster%20defense%20policy%20that%20coordinates%20their%20selection%20to%20solve%20complex%20network%0Adefense%20tasks.%20Furthermore%2C%20the%20sub-policies%20can%20be%20fine-tuned%20and%20transferred%0Awith%20minimal%20cost%20to%20defend%20against%20shifts%20in%20adversarial%20behavior%20or%20changes%0Ain%20network%20settings.%20We%20conduct%20extensive%20experiments%20using%20CybORG%20Cage%204%2C%20the%0Astate-of-the-art%20MARL%20environment%20for%20cyber%20defense.%20Comparisons%20with%20multiple%0Abaselines%20across%20different%20adversaries%20show%20that%20our%20hierarchical%20learning%0Aapproach%20achieves%20top%20performance%20in%20terms%20of%20convergence%20speed%2C%20episodic%0Areturn%2C%20and%20several%20interpretable%20metrics%20relevant%20to%20cybersecurity%2C%20including%0Athe%20fraction%20of%20clean%20machines%20on%20the%20network%2C%20precision%2C%20and%20false%20positives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17351v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


