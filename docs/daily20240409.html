<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation", "author": "Khoi Do and Duong Nguyen and Nguyen H. Tran and Viet Dung Nguyen", "abstract": "  Beyond class frequency, we recognize the impact of class-wise relationships\namong various class-specific predictions and the imbalance in label masks on\nlong-tailed segmentation learning. To address these challenges, we propose an\ninnovative Pixel-wise Adaptive Training (PAT) technique tailored for\nlong-tailed segmentation. PAT has two key features: 1) class-wise gradient\nmagnitude homogenization, and 2) pixel-wise class-specific loss adaptation\n(PCLA). First, the class-wise gradient magnitude homogenization helps alleviate\nthe imbalance among label masks by ensuring equal consideration of the\nclass-wise impact on model updates. Second, PCLA tackles the detrimental impact\nof both rare classes within the long-tailed distribution and inaccurate\npredictions from previous training stages by encouraging learning classes with\nlow prediction confidence and guarding against forgetting classes with high\nconfidence. This combined approach fosters robust learning while preventing the\nmodel from forgetting previously learned knowledge. PAT exhibits significant\nperformance improvements, surpassing the current state-of-the-art by 2.2% in\nthe NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and\nintersection over union value by 2.07%, with a particularly notable declination\nof 0.39% in detecting rare classes compared to Balance Logits Variation, as\ndemonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and\nNYU.\n", "link": "http://arxiv.org/abs/2404.05393v1", "date": "2024-04-08", "relevancy": 2.8985, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation&body=Title%3A%20PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation%0AAuthor%3A%20Khoi%20Do%20and%20Duong%20Nguyen%20and%20Nguyen%20H.%20Tran%20and%20Viet%20Dung%20Nguyen%0AAbstract%3A%20%20%20Beyond%20class%20frequency%2C%20we%20recognize%20the%20impact%20of%20class-wise%20relationships%0Aamong%20various%20class-specific%20predictions%20and%20the%20imbalance%20in%20label%20masks%20on%0Along-tailed%20segmentation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20an%0Ainnovative%20Pixel-wise%20Adaptive%20Training%20%28PAT%29%20technique%20tailored%20for%0Along-tailed%20segmentation.%20PAT%20has%20two%20key%20features%3A%201%29%20class-wise%20gradient%0Amagnitude%20homogenization%2C%20and%202%29%20pixel-wise%20class-specific%20loss%20adaptation%0A%28PCLA%29.%20First%2C%20the%20class-wise%20gradient%20magnitude%20homogenization%20helps%20alleviate%0Athe%20imbalance%20among%20label%20masks%20by%20ensuring%20equal%20consideration%20of%20the%0Aclass-wise%20impact%20on%20model%20updates.%20Second%2C%20PCLA%20tackles%20the%20detrimental%20impact%0Aof%20both%20rare%20classes%20within%20the%20long-tailed%20distribution%20and%20inaccurate%0Apredictions%20from%20previous%20training%20stages%20by%20encouraging%20learning%20classes%20with%0Alow%20prediction%20confidence%20and%20guarding%20against%20forgetting%20classes%20with%20high%0Aconfidence.%20This%20combined%20approach%20fosters%20robust%20learning%20while%20preventing%20the%0Amodel%20from%20forgetting%20previously%20learned%20knowledge.%20PAT%20exhibits%20significant%0Aperformance%20improvements%2C%20surpassing%20the%20current%20state-of-the-art%20by%202.2%25%20in%0Athe%20NyU%20dataset.%20Moreover%2C%20it%20enhances%20overall%20pixel-wise%20accuracy%20by%202.85%25%20and%0Aintersection%20over%20union%20value%20by%202.07%25%2C%20with%20a%20particularly%20notable%20declination%0Aof%200.39%25%20in%20detecting%20rare%20classes%20compared%20to%20Balance%20Logits%20Variation%2C%20as%0Ademonstrated%20on%20the%20three%20popular%20datasets%2C%20i.e.%2C%20OxfordPetIII%2C%20CityScape%2C%20and%0ANYU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05393v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation&entry.906535625=Khoi%20Do%20and%20Duong%20Nguyen%20and%20Nguyen%20H.%20Tran%20and%20Viet%20Dung%20Nguyen&entry.1292438233=%20%20Beyond%20class%20frequency%2C%20we%20recognize%20the%20impact%20of%20class-wise%20relationships%0Aamong%20various%20class-specific%20predictions%20and%20the%20imbalance%20in%20label%20masks%20on%0Along-tailed%20segmentation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20an%0Ainnovative%20Pixel-wise%20Adaptive%20Training%20%28PAT%29%20technique%20tailored%20for%0Along-tailed%20segmentation.%20PAT%20has%20two%20key%20features%3A%201%29%20class-wise%20gradient%0Amagnitude%20homogenization%2C%20and%202%29%20pixel-wise%20class-specific%20loss%20adaptation%0A%28PCLA%29.%20First%2C%20the%20class-wise%20gradient%20magnitude%20homogenization%20helps%20alleviate%0Athe%20imbalance%20among%20label%20masks%20by%20ensuring%20equal%20consideration%20of%20the%0Aclass-wise%20impact%20on%20model%20updates.%20Second%2C%20PCLA%20tackles%20the%20detrimental%20impact%0Aof%20both%20rare%20classes%20within%20the%20long-tailed%20distribution%20and%20inaccurate%0Apredictions%20from%20previous%20training%20stages%20by%20encouraging%20learning%20classes%20with%0Alow%20prediction%20confidence%20and%20guarding%20against%20forgetting%20classes%20with%20high%0Aconfidence.%20This%20combined%20approach%20fosters%20robust%20learning%20while%20preventing%20the%0Amodel%20from%20forgetting%20previously%20learned%20knowledge.%20PAT%20exhibits%20significant%0Aperformance%20improvements%2C%20surpassing%20the%20current%20state-of-the-art%20by%202.2%25%20in%0Athe%20NyU%20dataset.%20Moreover%2C%20it%20enhances%20overall%20pixel-wise%20accuracy%20by%202.85%25%20and%0Aintersection%20over%20union%20value%20by%202.07%25%2C%20with%20a%20particularly%20notable%20declination%0Aof%200.39%25%20in%20detecting%20rare%20classes%20compared%20to%20Balance%20Logits%20Variation%2C%20as%0Ademonstrated%20on%20the%20three%20popular%20datasets%2C%20i.e.%2C%20OxfordPetIII%2C%20CityScape%2C%20and%0ANYU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05393v1&entry.124074799=Read"},
{"title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset", "author": "Chengjian Feng and Yujie Zhong and Zequn Jie and Weidi Xie and Lin Ma", "abstract": "  In this paper, we present a novel paradigm to enhance the ability of object\ndetector, e.g., expanding categories or improving detection performance, by\ntraining on synthetic dataset generated from diffusion models. Specifically, we\nintegrate an instance-level grounding head into a pre-trained, generative\ndiffusion model, to augment it with the ability of localising instances in the\ngenerated images. The grounding head is trained to align the text embedding of\ncategory names with the regional visual feature of the diffusion model, using\nsupervision from an off-the-shelf object detector, and a novel self-training\nscheme on (novel) categories not covered by the detector. We conduct thorough\nexperiments to show that, this enhanced version of diffusion model, termed as\nInstaGen, can serve as a data synthesizer, to enhance object detectors by\ntraining on its generated samples, demonstrating superior performance over\nexisting state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse\n(+1.2 to 5.2 AP) scenarios. Project page with code:\nhttps://fcjian.github.io/InstaGen.\n", "link": "http://arxiv.org/abs/2402.05937v3", "date": "2024-04-08", "relevancy": 2.8304, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5711}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5669}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5602}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstaGen%3A%20Enhancing%20Object%20Detection%20by%20Training%20on%20Synthetic%20Dataset&body=Title%3A%20InstaGen%3A%20Enhancing%20Object%20Detection%20by%20Training%20on%20Synthetic%20Dataset%0AAuthor%3A%20Chengjian%20Feng%20and%20Yujie%20Zhong%20and%20Zequn%20Jie%20and%20Weidi%20Xie%20and%20Lin%20Ma%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20paradigm%20to%20enhance%20the%20ability%20of%20object%0Adetector%2C%20e.g.%2C%20expanding%20categories%20or%20improving%20detection%20performance%2C%20by%0Atraining%20on%20synthetic%20dataset%20generated%20from%20diffusion%20models.%20Specifically%2C%20we%0Aintegrate%20an%20instance-level%20grounding%20head%20into%20a%20pre-trained%2C%20generative%0Adiffusion%20model%2C%20to%20augment%20it%20with%20the%20ability%20of%20localising%20instances%20in%20the%0Agenerated%20images.%20The%20grounding%20head%20is%20trained%20to%20align%20the%20text%20embedding%20of%0Acategory%20names%20with%20the%20regional%20visual%20feature%20of%20the%20diffusion%20model%2C%20using%0Asupervision%20from%20an%20off-the-shelf%20object%20detector%2C%20and%20a%20novel%20self-training%0Ascheme%20on%20%28novel%29%20categories%20not%20covered%20by%20the%20detector.%20We%20conduct%20thorough%0Aexperiments%20to%20show%20that%2C%20this%20enhanced%20version%20of%20diffusion%20model%2C%20termed%20as%0AInstaGen%2C%20can%20serve%20as%20a%20data%20synthesizer%2C%20to%20enhance%20object%20detectors%20by%0Atraining%20on%20its%20generated%20samples%2C%20demonstrating%20superior%20performance%20over%0Aexisting%20state-of-the-art%20methods%20in%20open-vocabulary%20%28%2B4.5%20AP%29%20and%20data-sparse%0A%28%2B1.2%20to%205.2%20AP%29%20scenarios.%20Project%20page%20with%20code%3A%0Ahttps%3A//fcjian.github.io/InstaGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05937v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstaGen%3A%20Enhancing%20Object%20Detection%20by%20Training%20on%20Synthetic%20Dataset&entry.906535625=Chengjian%20Feng%20and%20Yujie%20Zhong%20and%20Zequn%20Jie%20and%20Weidi%20Xie%20and%20Lin%20Ma&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20paradigm%20to%20enhance%20the%20ability%20of%20object%0Adetector%2C%20e.g.%2C%20expanding%20categories%20or%20improving%20detection%20performance%2C%20by%0Atraining%20on%20synthetic%20dataset%20generated%20from%20diffusion%20models.%20Specifically%2C%20we%0Aintegrate%20an%20instance-level%20grounding%20head%20into%20a%20pre-trained%2C%20generative%0Adiffusion%20model%2C%20to%20augment%20it%20with%20the%20ability%20of%20localising%20instances%20in%20the%0Agenerated%20images.%20The%20grounding%20head%20is%20trained%20to%20align%20the%20text%20embedding%20of%0Acategory%20names%20with%20the%20regional%20visual%20feature%20of%20the%20diffusion%20model%2C%20using%0Asupervision%20from%20an%20off-the-shelf%20object%20detector%2C%20and%20a%20novel%20self-training%0Ascheme%20on%20%28novel%29%20categories%20not%20covered%20by%20the%20detector.%20We%20conduct%20thorough%0Aexperiments%20to%20show%20that%2C%20this%20enhanced%20version%20of%20diffusion%20model%2C%20termed%20as%0AInstaGen%2C%20can%20serve%20as%20a%20data%20synthesizer%2C%20to%20enhance%20object%20detectors%20by%0Atraining%20on%20its%20generated%20samples%2C%20demonstrating%20superior%20performance%20over%0Aexisting%20state-of-the-art%20methods%20in%20open-vocabulary%20%28%2B4.5%20AP%29%20and%20data-sparse%0A%28%2B1.2%20to%205.2%20AP%29%20scenarios.%20Project%20page%20with%20code%3A%0Ahttps%3A//fcjian.github.io/InstaGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05937v3&entry.124074799=Read"},
{"title": "MESA: Matching Everything by Segmenting Anything", "author": "Yesheng Zhang and Xu Zhao", "abstract": "  Feature matching is a crucial task in the field of computer vision, which\ninvolves finding correspondences between images. Previous studies achieve\nremarkable performance using learning-based feature comparison. However, the\npervasive presence of matching redundancy between images gives rise to\nunnecessary and error-prone computations in these methods, imposing limitations\non their accuracy. To address this issue, we propose MESA, a novel approach to\nestablish precise area (or region) matches for efficient matching redundancy\nreduction. MESA first leverages the advanced image understanding capability of\nSAM, a state-of-the-art foundation model for image segmentation, to obtain\nimage areas with implicit semantic. Then, a multi-relational graph is proposed\nto model the spatial structure of these areas and construct their scale\nhierarchy. Based on graphical models derived from the graph, the area matching\nis reformulated as an energy minimization task and effectively resolved.\nExtensive experiments demonstrate that MESA yields substantial precision\nimprovement for multiple point matchers in indoor and outdoor downstream tasks,\ne.g. +13.61% for DKM in indoor pose estimation.\n", "link": "http://arxiv.org/abs/2401.16741v2", "date": "2024-04-08", "relevancy": 2.7115, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5088}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MESA%3A%20Matching%20Everything%20by%20Segmenting%20Anything&body=Title%3A%20MESA%3A%20Matching%20Everything%20by%20Segmenting%20Anything%0AAuthor%3A%20Yesheng%20Zhang%20and%20Xu%20Zhao%0AAbstract%3A%20%20%20Feature%20matching%20is%20a%20crucial%20task%20in%20the%20field%20of%20computer%20vision%2C%20which%0Ainvolves%20finding%20correspondences%20between%20images.%20Previous%20studies%20achieve%0Aremarkable%20performance%20using%20learning-based%20feature%20comparison.%20However%2C%20the%0Apervasive%20presence%20of%20matching%20redundancy%20between%20images%20gives%20rise%20to%0Aunnecessary%20and%20error-prone%20computations%20in%20these%20methods%2C%20imposing%20limitations%0Aon%20their%20accuracy.%20To%20address%20this%20issue%2C%20we%20propose%20MESA%2C%20a%20novel%20approach%20to%0Aestablish%20precise%20area%20%28or%20region%29%20matches%20for%20efficient%20matching%20redundancy%0Areduction.%20MESA%20first%20leverages%20the%20advanced%20image%20understanding%20capability%20of%0ASAM%2C%20a%20state-of-the-art%20foundation%20model%20for%20image%20segmentation%2C%20to%20obtain%0Aimage%20areas%20with%20implicit%20semantic.%20Then%2C%20a%20multi-relational%20graph%20is%20proposed%0Ato%20model%20the%20spatial%20structure%20of%20these%20areas%20and%20construct%20their%20scale%0Ahierarchy.%20Based%20on%20graphical%20models%20derived%20from%20the%20graph%2C%20the%20area%20matching%0Ais%20reformulated%20as%20an%20energy%20minimization%20task%20and%20effectively%20resolved.%0AExtensive%20experiments%20demonstrate%20that%20MESA%20yields%20substantial%20precision%0Aimprovement%20for%20multiple%20point%20matchers%20in%20indoor%20and%20outdoor%20downstream%20tasks%2C%0Ae.g.%20%2B13.61%25%20for%20DKM%20in%20indoor%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16741v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MESA%3A%20Matching%20Everything%20by%20Segmenting%20Anything&entry.906535625=Yesheng%20Zhang%20and%20Xu%20Zhao&entry.1292438233=%20%20Feature%20matching%20is%20a%20crucial%20task%20in%20the%20field%20of%20computer%20vision%2C%20which%0Ainvolves%20finding%20correspondences%20between%20images.%20Previous%20studies%20achieve%0Aremarkable%20performance%20using%20learning-based%20feature%20comparison.%20However%2C%20the%0Apervasive%20presence%20of%20matching%20redundancy%20between%20images%20gives%20rise%20to%0Aunnecessary%20and%20error-prone%20computations%20in%20these%20methods%2C%20imposing%20limitations%0Aon%20their%20accuracy.%20To%20address%20this%20issue%2C%20we%20propose%20MESA%2C%20a%20novel%20approach%20to%0Aestablish%20precise%20area%20%28or%20region%29%20matches%20for%20efficient%20matching%20redundancy%0Areduction.%20MESA%20first%20leverages%20the%20advanced%20image%20understanding%20capability%20of%0ASAM%2C%20a%20state-of-the-art%20foundation%20model%20for%20image%20segmentation%2C%20to%20obtain%0Aimage%20areas%20with%20implicit%20semantic.%20Then%2C%20a%20multi-relational%20graph%20is%20proposed%0Ato%20model%20the%20spatial%20structure%20of%20these%20areas%20and%20construct%20their%20scale%0Ahierarchy.%20Based%20on%20graphical%20models%20derived%20from%20the%20graph%2C%20the%20area%20matching%0Ais%20reformulated%20as%20an%20energy%20minimization%20task%20and%20effectively%20resolved.%0AExtensive%20experiments%20demonstrate%20that%20MESA%20yields%20substantial%20precision%0Aimprovement%20for%20multiple%20point%20matchers%20in%20indoor%20and%20outdoor%20downstream%20tasks%2C%0Ae.g.%20%2B13.61%25%20for%20DKM%20in%20indoor%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16741v2&entry.124074799=Read"},
{"title": "Learning 3D-Aware GANs from Unposed Images with Template Feature Field", "author": "Xinya Chen and Hanlei Guo and Yanrui Bin and Shangzhan Zhang and Yuanbo Yang and Yue Wang and Yujun Shen and Yiyi Liao", "abstract": "  Collecting accurate camera poses of training images has been shown to well\nserve the learning of 3D-aware generative adversarial networks (GANs) yet can\nbe quite expensive in practice. This work targets learning 3D-aware GANs from\nunposed images, for which we propose to perform on-the-fly pose estimation of\ntraining images with a learned template feature field (TeFF). Concretely, in\naddition to a generative radiance field as in previous approaches, we ask the\ngenerator to also learn a field from 2D semantic features while sharing the\ndensity from the radiance field. Such a framework allows us to acquire a\ncanonical 3D feature template leveraging the dataset mean discovered by the\ngenerative model, and further efficiently estimate the pose parameters on real\ndata. Experimental results on various challenging datasets demonstrate the\nsuperiority of our approach over state-of-the-art alternatives from both the\nqualitative and the quantitative perspectives.\n", "link": "http://arxiv.org/abs/2404.05705v1", "date": "2024-04-08", "relevancy": 2.6932, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5464}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5301}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%203D-Aware%20GANs%20from%20Unposed%20Images%20with%20Template%20Feature%20Field&body=Title%3A%20Learning%203D-Aware%20GANs%20from%20Unposed%20Images%20with%20Template%20Feature%20Field%0AAuthor%3A%20Xinya%20Chen%20and%20Hanlei%20Guo%20and%20Yanrui%20Bin%20and%20Shangzhan%20Zhang%20and%20Yuanbo%20Yang%20and%20Yue%20Wang%20and%20Yujun%20Shen%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20Collecting%20accurate%20camera%20poses%20of%20training%20images%20has%20been%20shown%20to%20well%0Aserve%20the%20learning%20of%203D-aware%20generative%20adversarial%20networks%20%28GANs%29%20yet%20can%0Abe%20quite%20expensive%20in%20practice.%20This%20work%20targets%20learning%203D-aware%20GANs%20from%0Aunposed%20images%2C%20for%20which%20we%20propose%20to%20perform%20on-the-fly%20pose%20estimation%20of%0Atraining%20images%20with%20a%20learned%20template%20feature%20field%20%28TeFF%29.%20Concretely%2C%20in%0Aaddition%20to%20a%20generative%20radiance%20field%20as%20in%20previous%20approaches%2C%20we%20ask%20the%0Agenerator%20to%20also%20learn%20a%20field%20from%202D%20semantic%20features%20while%20sharing%20the%0Adensity%20from%20the%20radiance%20field.%20Such%20a%20framework%20allows%20us%20to%20acquire%20a%0Acanonical%203D%20feature%20template%20leveraging%20the%20dataset%20mean%20discovered%20by%20the%0Agenerative%20model%2C%20and%20further%20efficiently%20estimate%20the%20pose%20parameters%20on%20real%0Adata.%20Experimental%20results%20on%20various%20challenging%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20over%20state-of-the-art%20alternatives%20from%20both%20the%0Aqualitative%20and%20the%20quantitative%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05705v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%203D-Aware%20GANs%20from%20Unposed%20Images%20with%20Template%20Feature%20Field&entry.906535625=Xinya%20Chen%20and%20Hanlei%20Guo%20and%20Yanrui%20Bin%20and%20Shangzhan%20Zhang%20and%20Yuanbo%20Yang%20and%20Yue%20Wang%20and%20Yujun%20Shen%20and%20Yiyi%20Liao&entry.1292438233=%20%20Collecting%20accurate%20camera%20poses%20of%20training%20images%20has%20been%20shown%20to%20well%0Aserve%20the%20learning%20of%203D-aware%20generative%20adversarial%20networks%20%28GANs%29%20yet%20can%0Abe%20quite%20expensive%20in%20practice.%20This%20work%20targets%20learning%203D-aware%20GANs%20from%0Aunposed%20images%2C%20for%20which%20we%20propose%20to%20perform%20on-the-fly%20pose%20estimation%20of%0Atraining%20images%20with%20a%20learned%20template%20feature%20field%20%28TeFF%29.%20Concretely%2C%20in%0Aaddition%20to%20a%20generative%20radiance%20field%20as%20in%20previous%20approaches%2C%20we%20ask%20the%0Agenerator%20to%20also%20learn%20a%20field%20from%202D%20semantic%20features%20while%20sharing%20the%0Adensity%20from%20the%20radiance%20field.%20Such%20a%20framework%20allows%20us%20to%20acquire%20a%0Acanonical%203D%20feature%20template%20leveraging%20the%20dataset%20mean%20discovered%20by%20the%0Agenerative%20model%2C%20and%20further%20efficiently%20estimate%20the%20pose%20parameters%20on%20real%0Adata.%20Experimental%20results%20on%20various%20challenging%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20over%20state-of-the-art%20alternatives%20from%20both%20the%0Aqualitative%20and%20the%20quantitative%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05705v1&entry.124074799=Read"},
{"title": "Understanding normalization in contrastive representation learning and\n  out-of-distribution detection", "author": "Tai Le-Gia and Jaehyun Ahn", "abstract": "  Contrastive representation learning has emerged as an outstanding approach\nfor anomaly detection. In this work, we explore the $\\ell_2$-norm of\ncontrastive features and its applications in out-of-distribution detection. We\npropose a simple method based on contrastive learning, which incorporates\nout-of-distribution data by discriminating against normal samples in the\ncontrastive layer space. Our approach can be applied flexibly as an outlier\nexposure (OE) approach, where the out-of-distribution data is a huge collective\nof random images, or as a fully self-supervised learning approach, where the\nout-of-distribution data is self-generated by applying distribution-shifting\ntransformations. The ability to incorporate additional out-of-distribution\nsamples enables a feasible solution for datasets where AD methods based on\ncontrastive learning generally underperform, such as aerial images or\nmicroscopy images. Furthermore, the high-quality features learned through\ncontrastive learning consistently enhance performance in OE scenarios, even\nwhen the available out-of-distribution dataset is not diverse enough. Our\nextensive experiments demonstrate the superiority of our proposed method under\nvarious scenarios, including unimodal and multimodal settings, with various\nimage datasets.\n", "link": "http://arxiv.org/abs/2312.15288v2", "date": "2024-04-08", "relevancy": 2.6422, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5283}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5207}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20normalization%20in%20contrastive%20representation%20learning%20and%0A%20%20out-of-distribution%20detection&body=Title%3A%20Understanding%20normalization%20in%20contrastive%20representation%20learning%20and%0A%20%20out-of-distribution%20detection%0AAuthor%3A%20Tai%20Le-Gia%20and%20Jaehyun%20Ahn%0AAbstract%3A%20%20%20Contrastive%20representation%20learning%20has%20emerged%20as%20an%20outstanding%20approach%0Afor%20anomaly%20detection.%20In%20this%20work%2C%20we%20explore%20the%20%24%5Cell_2%24-norm%20of%0Acontrastive%20features%20and%20its%20applications%20in%20out-of-distribution%20detection.%20We%0Apropose%20a%20simple%20method%20based%20on%20contrastive%20learning%2C%20which%20incorporates%0Aout-of-distribution%20data%20by%20discriminating%20against%20normal%20samples%20in%20the%0Acontrastive%20layer%20space.%20Our%20approach%20can%20be%20applied%20flexibly%20as%20an%20outlier%0Aexposure%20%28OE%29%20approach%2C%20where%20the%20out-of-distribution%20data%20is%20a%20huge%20collective%0Aof%20random%20images%2C%20or%20as%20a%20fully%20self-supervised%20learning%20approach%2C%20where%20the%0Aout-of-distribution%20data%20is%20self-generated%20by%20applying%20distribution-shifting%0Atransformations.%20The%20ability%20to%20incorporate%20additional%20out-of-distribution%0Asamples%20enables%20a%20feasible%20solution%20for%20datasets%20where%20AD%20methods%20based%20on%0Acontrastive%20learning%20generally%20underperform%2C%20such%20as%20aerial%20images%20or%0Amicroscopy%20images.%20Furthermore%2C%20the%20high-quality%20features%20learned%20through%0Acontrastive%20learning%20consistently%20enhance%20performance%20in%20OE%20scenarios%2C%20even%0Awhen%20the%20available%20out-of-distribution%20dataset%20is%20not%20diverse%20enough.%20Our%0Aextensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20under%0Avarious%20scenarios%2C%20including%20unimodal%20and%20multimodal%20settings%2C%20with%20various%0Aimage%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15288v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20normalization%20in%20contrastive%20representation%20learning%20and%0A%20%20out-of-distribution%20detection&entry.906535625=Tai%20Le-Gia%20and%20Jaehyun%20Ahn&entry.1292438233=%20%20Contrastive%20representation%20learning%20has%20emerged%20as%20an%20outstanding%20approach%0Afor%20anomaly%20detection.%20In%20this%20work%2C%20we%20explore%20the%20%24%5Cell_2%24-norm%20of%0Acontrastive%20features%20and%20its%20applications%20in%20out-of-distribution%20detection.%20We%0Apropose%20a%20simple%20method%20based%20on%20contrastive%20learning%2C%20which%20incorporates%0Aout-of-distribution%20data%20by%20discriminating%20against%20normal%20samples%20in%20the%0Acontrastive%20layer%20space.%20Our%20approach%20can%20be%20applied%20flexibly%20as%20an%20outlier%0Aexposure%20%28OE%29%20approach%2C%20where%20the%20out-of-distribution%20data%20is%20a%20huge%20collective%0Aof%20random%20images%2C%20or%20as%20a%20fully%20self-supervised%20learning%20approach%2C%20where%20the%0Aout-of-distribution%20data%20is%20self-generated%20by%20applying%20distribution-shifting%0Atransformations.%20The%20ability%20to%20incorporate%20additional%20out-of-distribution%0Asamples%20enables%20a%20feasible%20solution%20for%20datasets%20where%20AD%20methods%20based%20on%0Acontrastive%20learning%20generally%20underperform%2C%20such%20as%20aerial%20images%20or%0Amicroscopy%20images.%20Furthermore%2C%20the%20high-quality%20features%20learned%20through%0Acontrastive%20learning%20consistently%20enhance%20performance%20in%20OE%20scenarios%2C%20even%0Awhen%20the%20available%20out-of-distribution%20dataset%20is%20not%20diverse%20enough.%20Our%0Aextensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20under%0Avarious%20scenarios%2C%20including%20unimodal%20and%20multimodal%20settings%2C%20with%20various%0Aimage%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15288v2&entry.124074799=Read"},
{"title": "Tangling-Untangling Cycle for Efficient Learning", "author": "Xin Li", "abstract": "  The conventional wisdom of manifold learning is based on nonlinear\ndimensionality reduction techniques such as IsoMAP and locally linear embedding\n(LLE). We challenge this paradigm by exploiting the blessing of dimensionality.\nOur intuition is simple: it is easier to untangle a low-dimensional manifold in\na higher-dimensional space due to its vastness, as guaranteed by Whitney\nembedding theorem. A new insight brought by this work is to introduce class\nlabels as the context variables in the lifted higher-dimensional space (so\nsupervised learning becomes unsupervised learning). We rigorously show that\nmanifold untangling leads to linearly separable classifiers in the lifted\nspace. To correct the inevitable overfitting, we consider the dual process of\nmanifold untangling -- tangling or aliasing -- which is important for\ngeneralization. Using context as the bonding element, we construct a pair of\nmanifold untangling and tangling operators, known as tangling-untangling cycle\n(TUC). Untangling operator maps context-independent representations (CIR) in\nlow-dimensional space to context-dependent representations (CDR) in\nhigh-dimensional space by inducing context as hidden variables. The tangling\noperator maps CDR back to CIR by a simple integral transformation for\ninvariance and generalization. We also present the hierarchical extensions of\nTUC based on the Cartesian product and the fractal geometry. Despite the\nconceptual simplicity, TUC admits a biologically plausible and energy-efficient\nimplementation based on the time-locking behavior of polychronization neural\ngroups (PNG) and sleep-wake cycle (SWC). The TUC-based theory applies to the\ncomputational modeling of various cognitive functions by\nhippocampal-neocortical systems.\n", "link": "http://arxiv.org/abs/2404.05484v1", "date": "2024-04-08", "relevancy": 2.6333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5322}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tangling-Untangling%20Cycle%20for%20Efficient%20Learning&body=Title%3A%20Tangling-Untangling%20Cycle%20for%20Efficient%20Learning%0AAuthor%3A%20Xin%20Li%0AAbstract%3A%20%20%20The%20conventional%20wisdom%20of%20manifold%20learning%20is%20based%20on%20nonlinear%0Adimensionality%20reduction%20techniques%20such%20as%20IsoMAP%20and%20locally%20linear%20embedding%0A%28LLE%29.%20We%20challenge%20this%20paradigm%20by%20exploiting%20the%20blessing%20of%20dimensionality.%0AOur%20intuition%20is%20simple%3A%20it%20is%20easier%20to%20untangle%20a%20low-dimensional%20manifold%20in%0Aa%20higher-dimensional%20space%20due%20to%20its%20vastness%2C%20as%20guaranteed%20by%20Whitney%0Aembedding%20theorem.%20A%20new%20insight%20brought%20by%20this%20work%20is%20to%20introduce%20class%0Alabels%20as%20the%20context%20variables%20in%20the%20lifted%20higher-dimensional%20space%20%28so%0Asupervised%20learning%20becomes%20unsupervised%20learning%29.%20We%20rigorously%20show%20that%0Amanifold%20untangling%20leads%20to%20linearly%20separable%20classifiers%20in%20the%20lifted%0Aspace.%20To%20correct%20the%20inevitable%20overfitting%2C%20we%20consider%20the%20dual%20process%20of%0Amanifold%20untangling%20--%20tangling%20or%20aliasing%20--%20which%20is%20important%20for%0Ageneralization.%20Using%20context%20as%20the%20bonding%20element%2C%20we%20construct%20a%20pair%20of%0Amanifold%20untangling%20and%20tangling%20operators%2C%20known%20as%20tangling-untangling%20cycle%0A%28TUC%29.%20Untangling%20operator%20maps%20context-independent%20representations%20%28CIR%29%20in%0Alow-dimensional%20space%20to%20context-dependent%20representations%20%28CDR%29%20in%0Ahigh-dimensional%20space%20by%20inducing%20context%20as%20hidden%20variables.%20The%20tangling%0Aoperator%20maps%20CDR%20back%20to%20CIR%20by%20a%20simple%20integral%20transformation%20for%0Ainvariance%20and%20generalization.%20We%20also%20present%20the%20hierarchical%20extensions%20of%0ATUC%20based%20on%20the%20Cartesian%20product%20and%20the%20fractal%20geometry.%20Despite%20the%0Aconceptual%20simplicity%2C%20TUC%20admits%20a%20biologically%20plausible%20and%20energy-efficient%0Aimplementation%20based%20on%20the%20time-locking%20behavior%20of%20polychronization%20neural%0Agroups%20%28PNG%29%20and%20sleep-wake%20cycle%20%28SWC%29.%20The%20TUC-based%20theory%20applies%20to%20the%0Acomputational%20modeling%20of%20various%20cognitive%20functions%20by%0Ahippocampal-neocortical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05484v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tangling-Untangling%20Cycle%20for%20Efficient%20Learning&entry.906535625=Xin%20Li&entry.1292438233=%20%20The%20conventional%20wisdom%20of%20manifold%20learning%20is%20based%20on%20nonlinear%0Adimensionality%20reduction%20techniques%20such%20as%20IsoMAP%20and%20locally%20linear%20embedding%0A%28LLE%29.%20We%20challenge%20this%20paradigm%20by%20exploiting%20the%20blessing%20of%20dimensionality.%0AOur%20intuition%20is%20simple%3A%20it%20is%20easier%20to%20untangle%20a%20low-dimensional%20manifold%20in%0Aa%20higher-dimensional%20space%20due%20to%20its%20vastness%2C%20as%20guaranteed%20by%20Whitney%0Aembedding%20theorem.%20A%20new%20insight%20brought%20by%20this%20work%20is%20to%20introduce%20class%0Alabels%20as%20the%20context%20variables%20in%20the%20lifted%20higher-dimensional%20space%20%28so%0Asupervised%20learning%20becomes%20unsupervised%20learning%29.%20We%20rigorously%20show%20that%0Amanifold%20untangling%20leads%20to%20linearly%20separable%20classifiers%20in%20the%20lifted%0Aspace.%20To%20correct%20the%20inevitable%20overfitting%2C%20we%20consider%20the%20dual%20process%20of%0Amanifold%20untangling%20--%20tangling%20or%20aliasing%20--%20which%20is%20important%20for%0Ageneralization.%20Using%20context%20as%20the%20bonding%20element%2C%20we%20construct%20a%20pair%20of%0Amanifold%20untangling%20and%20tangling%20operators%2C%20known%20as%20tangling-untangling%20cycle%0A%28TUC%29.%20Untangling%20operator%20maps%20context-independent%20representations%20%28CIR%29%20in%0Alow-dimensional%20space%20to%20context-dependent%20representations%20%28CDR%29%20in%0Ahigh-dimensional%20space%20by%20inducing%20context%20as%20hidden%20variables.%20The%20tangling%0Aoperator%20maps%20CDR%20back%20to%20CIR%20by%20a%20simple%20integral%20transformation%20for%0Ainvariance%20and%20generalization.%20We%20also%20present%20the%20hierarchical%20extensions%20of%0ATUC%20based%20on%20the%20Cartesian%20product%20and%20the%20fractal%20geometry.%20Despite%20the%0Aconceptual%20simplicity%2C%20TUC%20admits%20a%20biologically%20plausible%20and%20energy-efficient%0Aimplementation%20based%20on%20the%20time-locking%20behavior%20of%20polychronization%20neural%0Agroups%20%28PNG%29%20and%20sleep-wake%20cycle%20%28SWC%29.%20The%20TUC-based%20theory%20applies%20to%20the%0Acomputational%20modeling%20of%20various%20cognitive%20functions%20by%0Ahippocampal-neocortical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05484v1&entry.124074799=Read"},
{"title": "Energy-Calibrated VAE with Test Time Free Lunch", "author": "Yihong Luo and Siya Qiu and Xingjian Tao and Yujun Cai and Jing Tang", "abstract": "  In this paper, we propose a novel generative model that utilizes a\nconditional Energy-Based Model (EBM) for enhancing Variational Autoencoder\n(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer\nfrom blurry generated samples due to the lack of a tailored training on the\nsamples generated in the generative direction. On the other hand, EBMs can\ngenerate high-quality samples but require expensive Markov Chain Monte Carlo\n(MCMC) sampling. To address these issues, we introduce a conditional EBM for\ncalibrating the generative direction of VAE during training, without requiring\nit for the generation at test time. In particular, we train EC-VAE upon both\nthe input data and the calibrated samples with adaptive weight to enhance\nefficacy while avoiding MCMC sampling at test time. Furthermore, we extend the\ncalibration idea of EC-VAE to variational learning and normalizing flows, and\napply EC-VAE to an additional application of zero-shot image restoration via\nneural transport prior and range-null theory. We evaluate the proposed method\nwith two applications, including image generation and zero-shot image\nrestoration, and the experimental results show that our method achieves\ncompetitive performance over single-step non-adversarial generation. Our code\nis available at https://github.com/DJ-LYH/EC-VAE.\n", "link": "http://arxiv.org/abs/2311.04071v4", "date": "2024-04-08", "relevancy": 2.6134, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5337}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5216}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5127}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Energy-Calibrated%20VAE%20with%20Test%20Time%20Free%20Lunch&body=Title%3A%20Energy-Calibrated%20VAE%20with%20Test%20Time%20Free%20Lunch%0AAuthor%3A%20Yihong%20Luo%20and%20Siya%20Qiu%20and%20Xingjian%20Tao%20and%20Yujun%20Cai%20and%20Jing%20Tang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20generative%20model%20that%20utilizes%20a%0Aconditional%20Energy-Based%20Model%20%28EBM%29%20for%20enhancing%20Variational%20Autoencoder%0A%28VAE%29%2C%20termed%20Energy-Calibrated%20VAE%20%28EC-VAE%29.%20Specifically%2C%20VAEs%20often%20suffer%0Afrom%20blurry%20generated%20samples%20due%20to%20the%20lack%20of%20a%20tailored%20training%20on%20the%0Asamples%20generated%20in%20the%20generative%20direction.%20On%20the%20other%20hand%2C%20EBMs%20can%0Agenerate%20high-quality%20samples%20but%20require%20expensive%20Markov%20Chain%20Monte%20Carlo%0A%28MCMC%29%20sampling.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20conditional%20EBM%20for%0Acalibrating%20the%20generative%20direction%20of%20VAE%20during%20training%2C%20without%20requiring%0Ait%20for%20the%20generation%20at%20test%20time.%20In%20particular%2C%20we%20train%20EC-VAE%20upon%20both%0Athe%20input%20data%20and%20the%20calibrated%20samples%20with%20adaptive%20weight%20to%20enhance%0Aefficacy%20while%20avoiding%20MCMC%20sampling%20at%20test%20time.%20Furthermore%2C%20we%20extend%20the%0Acalibration%20idea%20of%20EC-VAE%20to%20variational%20learning%20and%20normalizing%20flows%2C%20and%0Aapply%20EC-VAE%20to%20an%20additional%20application%20of%20zero-shot%20image%20restoration%20via%0Aneural%20transport%20prior%20and%20range-null%20theory.%20We%20evaluate%20the%20proposed%20method%0Awith%20two%20applications%2C%20including%20image%20generation%20and%20zero-shot%20image%0Arestoration%2C%20and%20the%20experimental%20results%20show%20that%20our%20method%20achieves%0Acompetitive%20performance%20over%20single-step%20non-adversarial%20generation.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/DJ-LYH/EC-VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04071v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Calibrated%20VAE%20with%20Test%20Time%20Free%20Lunch&entry.906535625=Yihong%20Luo%20and%20Siya%20Qiu%20and%20Xingjian%20Tao%20and%20Yujun%20Cai%20and%20Jing%20Tang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20generative%20model%20that%20utilizes%20a%0Aconditional%20Energy-Based%20Model%20%28EBM%29%20for%20enhancing%20Variational%20Autoencoder%0A%28VAE%29%2C%20termed%20Energy-Calibrated%20VAE%20%28EC-VAE%29.%20Specifically%2C%20VAEs%20often%20suffer%0Afrom%20blurry%20generated%20samples%20due%20to%20the%20lack%20of%20a%20tailored%20training%20on%20the%0Asamples%20generated%20in%20the%20generative%20direction.%20On%20the%20other%20hand%2C%20EBMs%20can%0Agenerate%20high-quality%20samples%20but%20require%20expensive%20Markov%20Chain%20Monte%20Carlo%0A%28MCMC%29%20sampling.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20conditional%20EBM%20for%0Acalibrating%20the%20generative%20direction%20of%20VAE%20during%20training%2C%20without%20requiring%0Ait%20for%20the%20generation%20at%20test%20time.%20In%20particular%2C%20we%20train%20EC-VAE%20upon%20both%0Athe%20input%20data%20and%20the%20calibrated%20samples%20with%20adaptive%20weight%20to%20enhance%0Aefficacy%20while%20avoiding%20MCMC%20sampling%20at%20test%20time.%20Furthermore%2C%20we%20extend%20the%0Acalibration%20idea%20of%20EC-VAE%20to%20variational%20learning%20and%20normalizing%20flows%2C%20and%0Aapply%20EC-VAE%20to%20an%20additional%20application%20of%20zero-shot%20image%20restoration%20via%0Aneural%20transport%20prior%20and%20range-null%20theory.%20We%20evaluate%20the%20proposed%20method%0Awith%20two%20applications%2C%20including%20image%20generation%20and%20zero-shot%20image%0Arestoration%2C%20and%20the%20experimental%20results%20show%20that%20our%20method%20achieves%0Acompetitive%20performance%20over%20single-step%20non-adversarial%20generation.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/DJ-LYH/EC-VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04071v4&entry.124074799=Read"},
{"title": "CDAD-Net: Bridging Domain Gaps in Generalized Category Discovery", "author": "Sai Bhargav Rongali and Sarthak Mehrotra and Ankit Jha and Mohamad Hassan N C and Shirsha Bose and Tanisha Gupta and Mainak Singha and Biplab Banerjee", "abstract": "  In Generalized Category Discovery (GCD), we cluster unlabeled samples of\nknown and novel classes, leveraging a training dataset of known classes. A\nsalient challenge arises due to domain shifts between these datasets. To\naddress this, we present a novel setting: Across Domain Generalized Category\nDiscovery (AD-GCD) and bring forth CDAD-NET (Class Discoverer Across Domains)\nas a remedy. CDAD-NET is architected to synchronize potential known class\nsamples across both the labeled (source) and unlabeled (target) datasets, while\nemphasizing the distinct categorization of the target data. To facilitate this,\nwe propose an entropy-driven adversarial learning strategy that accounts for\nthe distance distributions of target samples relative to source-domain class\nprototypes. Parallelly, the discriminative nature of the shared space is upheld\nthrough a fusion of three metric learning objectives. In the source domain, our\nfocus is on refining the proximity between samples and their affiliated class\nprototypes, while in the target domain, we integrate a neighborhood-centric\ncontrastive learning mechanism, enriched with an adept neighborsmining\napproach. To further accentuate the nuanced feature interrelation among\nsemantically aligned images, we champion the concept of conditional image\ninpainting, underscoring the premise that semantically analogous images prove\nmore efficacious to the task than their disjointed counterparts.\nExperimentally, CDAD-NET eclipses existing literature with a performance\nincrement of 8-15% on three AD-GCD benchmarks we present.\n", "link": "http://arxiv.org/abs/2404.05366v1", "date": "2024-04-08", "relevancy": 2.6103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5093}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CDAD-Net%3A%20Bridging%20Domain%20Gaps%20in%20Generalized%20Category%20Discovery&body=Title%3A%20CDAD-Net%3A%20Bridging%20Domain%20Gaps%20in%20Generalized%20Category%20Discovery%0AAuthor%3A%20Sai%20Bhargav%20Rongali%20and%20Sarthak%20Mehrotra%20and%20Ankit%20Jha%20and%20Mohamad%20Hassan%20N%20C%20and%20Shirsha%20Bose%20and%20Tanisha%20Gupta%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee%0AAbstract%3A%20%20%20In%20Generalized%20Category%20Discovery%20%28GCD%29%2C%20we%20cluster%20unlabeled%20samples%20of%0Aknown%20and%20novel%20classes%2C%20leveraging%20a%20training%20dataset%20of%20known%20classes.%20A%0Asalient%20challenge%20arises%20due%20to%20domain%20shifts%20between%20these%20datasets.%20To%0Aaddress%20this%2C%20we%20present%20a%20novel%20setting%3A%20Across%20Domain%20Generalized%20Category%0ADiscovery%20%28AD-GCD%29%20and%20bring%20forth%20CDAD-NET%20%28Class%20Discoverer%20Across%20Domains%29%0Aas%20a%20remedy.%20CDAD-NET%20is%20architected%20to%20synchronize%20potential%20known%20class%0Asamples%20across%20both%20the%20labeled%20%28source%29%20and%20unlabeled%20%28target%29%20datasets%2C%20while%0Aemphasizing%20the%20distinct%20categorization%20of%20the%20target%20data.%20To%20facilitate%20this%2C%0Awe%20propose%20an%20entropy-driven%20adversarial%20learning%20strategy%20that%20accounts%20for%0Athe%20distance%20distributions%20of%20target%20samples%20relative%20to%20source-domain%20class%0Aprototypes.%20Parallelly%2C%20the%20discriminative%20nature%20of%20the%20shared%20space%20is%20upheld%0Athrough%20a%20fusion%20of%20three%20metric%20learning%20objectives.%20In%20the%20source%20domain%2C%20our%0Afocus%20is%20on%20refining%20the%20proximity%20between%20samples%20and%20their%20affiliated%20class%0Aprototypes%2C%20while%20in%20the%20target%20domain%2C%20we%20integrate%20a%20neighborhood-centric%0Acontrastive%20learning%20mechanism%2C%20enriched%20with%20an%20adept%20neighborsmining%0Aapproach.%20To%20further%20accentuate%20the%20nuanced%20feature%20interrelation%20among%0Asemantically%20aligned%20images%2C%20we%20champion%20the%20concept%20of%20conditional%20image%0Ainpainting%2C%20underscoring%20the%20premise%20that%20semantically%20analogous%20images%20prove%0Amore%20efficacious%20to%20the%20task%20than%20their%20disjointed%20counterparts.%0AExperimentally%2C%20CDAD-NET%20eclipses%20existing%20literature%20with%20a%20performance%0Aincrement%20of%208-15%25%20on%20three%20AD-GCD%20benchmarks%20we%20present.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05366v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDAD-Net%3A%20Bridging%20Domain%20Gaps%20in%20Generalized%20Category%20Discovery&entry.906535625=Sai%20Bhargav%20Rongali%20and%20Sarthak%20Mehrotra%20and%20Ankit%20Jha%20and%20Mohamad%20Hassan%20N%20C%20and%20Shirsha%20Bose%20and%20Tanisha%20Gupta%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee&entry.1292438233=%20%20In%20Generalized%20Category%20Discovery%20%28GCD%29%2C%20we%20cluster%20unlabeled%20samples%20of%0Aknown%20and%20novel%20classes%2C%20leveraging%20a%20training%20dataset%20of%20known%20classes.%20A%0Asalient%20challenge%20arises%20due%20to%20domain%20shifts%20between%20these%20datasets.%20To%0Aaddress%20this%2C%20we%20present%20a%20novel%20setting%3A%20Across%20Domain%20Generalized%20Category%0ADiscovery%20%28AD-GCD%29%20and%20bring%20forth%20CDAD-NET%20%28Class%20Discoverer%20Across%20Domains%29%0Aas%20a%20remedy.%20CDAD-NET%20is%20architected%20to%20synchronize%20potential%20known%20class%0Asamples%20across%20both%20the%20labeled%20%28source%29%20and%20unlabeled%20%28target%29%20datasets%2C%20while%0Aemphasizing%20the%20distinct%20categorization%20of%20the%20target%20data.%20To%20facilitate%20this%2C%0Awe%20propose%20an%20entropy-driven%20adversarial%20learning%20strategy%20that%20accounts%20for%0Athe%20distance%20distributions%20of%20target%20samples%20relative%20to%20source-domain%20class%0Aprototypes.%20Parallelly%2C%20the%20discriminative%20nature%20of%20the%20shared%20space%20is%20upheld%0Athrough%20a%20fusion%20of%20three%20metric%20learning%20objectives.%20In%20the%20source%20domain%2C%20our%0Afocus%20is%20on%20refining%20the%20proximity%20between%20samples%20and%20their%20affiliated%20class%0Aprototypes%2C%20while%20in%20the%20target%20domain%2C%20we%20integrate%20a%20neighborhood-centric%0Acontrastive%20learning%20mechanism%2C%20enriched%20with%20an%20adept%20neighborsmining%0Aapproach.%20To%20further%20accentuate%20the%20nuanced%20feature%20interrelation%20among%0Asemantically%20aligned%20images%2C%20we%20champion%20the%20concept%20of%20conditional%20image%0Ainpainting%2C%20underscoring%20the%20premise%20that%20semantically%20analogous%20images%20prove%0Amore%20efficacious%20to%20the%20task%20than%20their%20disjointed%20counterparts.%0AExperimentally%2C%20CDAD-NET%20eclipses%20existing%20literature%20with%20a%20performance%0Aincrement%20of%208-15%25%20on%20three%20AD-GCD%20benchmarks%20we%20present.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05366v1&entry.124074799=Read"},
{"title": "Towards Domain-agnostic Depth Completion", "author": "Guangkai Xu and Wei Yin and Jianming Zhang and Oliver Wang and Simon Niklaus and Simon Chen and Jia-Wang Bian", "abstract": "  Existing depth completion methods are often targeted at a specific sparse\ndepth type and generalize poorly across task domains. We present a method to\ncomplete sparse/semi-dense, noisy, and potentially low-resolution depth maps\nobtained by various range sensors, including those in modern mobile phones, or\nby multi-view reconstruction algorithms. Our method leverages a data-driven\nprior in the form of a single image depth prediction network trained on\nlarge-scale datasets, the output of which is used as an input to our model. We\npropose an effective training scheme where we simulate various sparsity\npatterns in typical task domains. In addition, we design two new benchmarks to\nevaluate the generalizability and the robustness of depth completion methods.\nOur simple method shows superior cross-domain generalization ability against\nstate-of-the-art depth completion methods, introducing a practical solution to\nhigh-quality depth capture on a mobile device. The code is available at:\nhttps://github.com/YvanYin/FillDepth.\n", "link": "http://arxiv.org/abs/2207.14466v2", "date": "2024-04-08", "relevancy": 2.5818, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5304}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5052}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Domain-agnostic%20Depth%20Completion&body=Title%3A%20Towards%20Domain-agnostic%20Depth%20Completion%0AAuthor%3A%20Guangkai%20Xu%20and%20Wei%20Yin%20and%20Jianming%20Zhang%20and%20Oliver%20Wang%20and%20Simon%20Niklaus%20and%20Simon%20Chen%20and%20Jia-Wang%20Bian%0AAbstract%3A%20%20%20Existing%20depth%20completion%20methods%20are%20often%20targeted%20at%20a%20specific%20sparse%0Adepth%20type%20and%20generalize%20poorly%20across%20task%20domains.%20We%20present%20a%20method%20to%0Acomplete%20sparse/semi-dense%2C%20noisy%2C%20and%20potentially%20low-resolution%20depth%20maps%0Aobtained%20by%20various%20range%20sensors%2C%20including%20those%20in%20modern%20mobile%20phones%2C%20or%0Aby%20multi-view%20reconstruction%20algorithms.%20Our%20method%20leverages%20a%20data-driven%0Aprior%20in%20the%20form%20of%20a%20single%20image%20depth%20prediction%20network%20trained%20on%0Alarge-scale%20datasets%2C%20the%20output%20of%20which%20is%20used%20as%20an%20input%20to%20our%20model.%20We%0Apropose%20an%20effective%20training%20scheme%20where%20we%20simulate%20various%20sparsity%0Apatterns%20in%20typical%20task%20domains.%20In%20addition%2C%20we%20design%20two%20new%20benchmarks%20to%0Aevaluate%20the%20generalizability%20and%20the%20robustness%20of%20depth%20completion%20methods.%0AOur%20simple%20method%20shows%20superior%20cross-domain%20generalization%20ability%20against%0Astate-of-the-art%20depth%20completion%20methods%2C%20introducing%20a%20practical%20solution%20to%0Ahigh-quality%20depth%20capture%20on%20a%20mobile%20device.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/YvanYin/FillDepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14466v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Domain-agnostic%20Depth%20Completion&entry.906535625=Guangkai%20Xu%20and%20Wei%20Yin%20and%20Jianming%20Zhang%20and%20Oliver%20Wang%20and%20Simon%20Niklaus%20and%20Simon%20Chen%20and%20Jia-Wang%20Bian&entry.1292438233=%20%20Existing%20depth%20completion%20methods%20are%20often%20targeted%20at%20a%20specific%20sparse%0Adepth%20type%20and%20generalize%20poorly%20across%20task%20domains.%20We%20present%20a%20method%20to%0Acomplete%20sparse/semi-dense%2C%20noisy%2C%20and%20potentially%20low-resolution%20depth%20maps%0Aobtained%20by%20various%20range%20sensors%2C%20including%20those%20in%20modern%20mobile%20phones%2C%20or%0Aby%20multi-view%20reconstruction%20algorithms.%20Our%20method%20leverages%20a%20data-driven%0Aprior%20in%20the%20form%20of%20a%20single%20image%20depth%20prediction%20network%20trained%20on%0Alarge-scale%20datasets%2C%20the%20output%20of%20which%20is%20used%20as%20an%20input%20to%20our%20model.%20We%0Apropose%20an%20effective%20training%20scheme%20where%20we%20simulate%20various%20sparsity%0Apatterns%20in%20typical%20task%20domains.%20In%20addition%2C%20we%20design%20two%20new%20benchmarks%20to%0Aevaluate%20the%20generalizability%20and%20the%20robustness%20of%20depth%20completion%20methods.%0AOur%20simple%20method%20shows%20superior%20cross-domain%20generalization%20ability%20against%0Astate-of-the-art%20depth%20completion%20methods%2C%20introducing%20a%20practical%20solution%20to%0Ahigh-quality%20depth%20capture%20on%20a%20mobile%20device.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/YvanYin/FillDepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14466v2&entry.124074799=Read"},
{"title": "SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for\n  Skeleton-based Action Recognition", "author": "Shaojie Zhang and Jianqin Yin and Yonghao Dang and Jiajun Fu", "abstract": "  Graph convolution networks (GCNs) have achieved remarkable performance in\nskeleton-based action recognition. However, previous GCN-based methods rely on\nelaborate human priors excessively and construct complex feature aggregation\nmechanisms, which limits the generalizability and effectiveness of networks. To\nsolve these problems, we propose a novel Spatial Topology Gating Unit (STGU),\nan MLP-based variant without extra priors, to capture the co-occurrence\ntopology features that encode the spatial dependency across all joints. In\nSTGU, to learn the point-wise topology features, a new gate-based feature\ninteraction mechanism is introduced to activate the features point-to-point by\nthe attention map generated from the input sample. Based on the STGU, we\npropose the first MLP-based model, SiT-MLP, for skeleton-based action\nrecognition in this work. Compared with previous methods on three large-scale\ndatasets, SiT-MLP achieves competitive performance. In addition, SiT-MLP\nreduces the parameters significantly with favorable results. The code will be\navailable at https://github.com/BUPTSJZhang/SiT?MLP.\n", "link": "http://arxiv.org/abs/2308.16018v4", "date": "2024-04-08", "relevancy": 2.4427, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.497}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4864}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4822}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SiT-MLP%3A%20A%20Simple%20MLP%20with%20Point-wise%20Topology%20Feature%20Learning%20for%0A%20%20Skeleton-based%20Action%20Recognition&body=Title%3A%20SiT-MLP%3A%20A%20Simple%20MLP%20with%20Point-wise%20Topology%20Feature%20Learning%20for%0A%20%20Skeleton-based%20Action%20Recognition%0AAuthor%3A%20Shaojie%20Zhang%20and%20Jianqin%20Yin%20and%20Yonghao%20Dang%20and%20Jiajun%20Fu%0AAbstract%3A%20%20%20Graph%20convolution%20networks%20%28GCNs%29%20have%20achieved%20remarkable%20performance%20in%0Askeleton-based%20action%20recognition.%20However%2C%20previous%20GCN-based%20methods%20rely%20on%0Aelaborate%20human%20priors%20excessively%20and%20construct%20complex%20feature%20aggregation%0Amechanisms%2C%20which%20limits%20the%20generalizability%20and%20effectiveness%20of%20networks.%20To%0Asolve%20these%20problems%2C%20we%20propose%20a%20novel%20Spatial%20Topology%20Gating%20Unit%20%28STGU%29%2C%0Aan%20MLP-based%20variant%20without%20extra%20priors%2C%20to%20capture%20the%20co-occurrence%0Atopology%20features%20that%20encode%20the%20spatial%20dependency%20across%20all%20joints.%20In%0ASTGU%2C%20to%20learn%20the%20point-wise%20topology%20features%2C%20a%20new%20gate-based%20feature%0Ainteraction%20mechanism%20is%20introduced%20to%20activate%20the%20features%20point-to-point%20by%0Athe%20attention%20map%20generated%20from%20the%20input%20sample.%20Based%20on%20the%20STGU%2C%20we%0Apropose%20the%20first%20MLP-based%20model%2C%20SiT-MLP%2C%20for%20skeleton-based%20action%0Arecognition%20in%20this%20work.%20Compared%20with%20previous%20methods%20on%20three%20large-scale%0Adatasets%2C%20SiT-MLP%20achieves%20competitive%20performance.%20In%20addition%2C%20SiT-MLP%0Areduces%20the%20parameters%20significantly%20with%20favorable%20results.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/BUPTSJZhang/SiT%3FMLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.16018v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiT-MLP%3A%20A%20Simple%20MLP%20with%20Point-wise%20Topology%20Feature%20Learning%20for%0A%20%20Skeleton-based%20Action%20Recognition&entry.906535625=Shaojie%20Zhang%20and%20Jianqin%20Yin%20and%20Yonghao%20Dang%20and%20Jiajun%20Fu&entry.1292438233=%20%20Graph%20convolution%20networks%20%28GCNs%29%20have%20achieved%20remarkable%20performance%20in%0Askeleton-based%20action%20recognition.%20However%2C%20previous%20GCN-based%20methods%20rely%20on%0Aelaborate%20human%20priors%20excessively%20and%20construct%20complex%20feature%20aggregation%0Amechanisms%2C%20which%20limits%20the%20generalizability%20and%20effectiveness%20of%20networks.%20To%0Asolve%20these%20problems%2C%20we%20propose%20a%20novel%20Spatial%20Topology%20Gating%20Unit%20%28STGU%29%2C%0Aan%20MLP-based%20variant%20without%20extra%20priors%2C%20to%20capture%20the%20co-occurrence%0Atopology%20features%20that%20encode%20the%20spatial%20dependency%20across%20all%20joints.%20In%0ASTGU%2C%20to%20learn%20the%20point-wise%20topology%20features%2C%20a%20new%20gate-based%20feature%0Ainteraction%20mechanism%20is%20introduced%20to%20activate%20the%20features%20point-to-point%20by%0Athe%20attention%20map%20generated%20from%20the%20input%20sample.%20Based%20on%20the%20STGU%2C%20we%0Apropose%20the%20first%20MLP-based%20model%2C%20SiT-MLP%2C%20for%20skeleton-based%20action%0Arecognition%20in%20this%20work.%20Compared%20with%20previous%20methods%20on%20three%20large-scale%0Adatasets%2C%20SiT-MLP%20achieves%20competitive%20performance.%20In%20addition%2C%20SiT-MLP%0Areduces%20the%20parameters%20significantly%20with%20favorable%20results.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/BUPTSJZhang/SiT%3FMLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.16018v4&entry.124074799=Read"},
{"title": "CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification", "author": "Yiyu Chen and Zheyi Fan and Zhaoru Chen and Yixuan Zhu", "abstract": "  Person re-identification (re-ID) is a challenging task that aims to learn\ndiscriminative features for person retrieval. In person re-ID, Jaccard distance\nis a widely used distance metric, especially in re-ranking and clustering\nscenarios. However, we discover that camera variation has a significant\nnegative impact on the reliability of Jaccard distance. In particular, Jaccard\ndistance calculates the distance based on the overlap of relevant neighbors.\nDue to camera variation, intra-camera samples dominate the relevant neighbors,\nwhich reduces the reliability of the neighbors by introducing intra-camera\nnegative samples and excluding inter-camera positive samples. To overcome this\nproblem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that\nleverages camera information to enhance the reliability of Jaccard distance.\nSpecifically, we design camera-aware k-reciprocal nearest neighbors (CKRNNs) to\nfind k-reciprocal nearest neighbors on the intra-camera and inter-camera\nranking lists, which improves the reliability of relevant neighbors and\nguarantees the contribution of inter-camera samples in the overlap. Moreover,\nwe propose a camera-aware local query expansion (CLQE) to mine reliable samples\nin relevant neighbors by exploiting camera variation as a strong constraint and\nassign these samples higher weights in overlap, further improving the\nreliability. Our CA-Jaccard distance is simple yet effective and can serve as a\ngeneral distance metric for person re-ID methods with high reliability and low\ncomputational cost. Extensive experiments demonstrate the effectiveness of our\nmethod.\n", "link": "http://arxiv.org/abs/2311.10605v2", "date": "2024-04-08", "relevancy": 2.4421, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5023}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4897}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4733}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CA-Jaccard%3A%20Camera-aware%20Jaccard%20Distance%20for%20Person%20Re-identification&body=Title%3A%20CA-Jaccard%3A%20Camera-aware%20Jaccard%20Distance%20for%20Person%20Re-identification%0AAuthor%3A%20Yiyu%20Chen%20and%20Zheyi%20Fan%20and%20Zhaoru%20Chen%20and%20Yixuan%20Zhu%0AAbstract%3A%20%20%20Person%20re-identification%20%28re-ID%29%20is%20a%20challenging%20task%20that%20aims%20to%20learn%0Adiscriminative%20features%20for%20person%20retrieval.%20In%20person%20re-ID%2C%20Jaccard%20distance%0Ais%20a%20widely%20used%20distance%20metric%2C%20especially%20in%20re-ranking%20and%20clustering%0Ascenarios.%20However%2C%20we%20discover%20that%20camera%20variation%20has%20a%20significant%0Anegative%20impact%20on%20the%20reliability%20of%20Jaccard%20distance.%20In%20particular%2C%20Jaccard%0Adistance%20calculates%20the%20distance%20based%20on%20the%20overlap%20of%20relevant%20neighbors.%0ADue%20to%20camera%20variation%2C%20intra-camera%20samples%20dominate%20the%20relevant%20neighbors%2C%0Awhich%20reduces%20the%20reliability%20of%20the%20neighbors%20by%20introducing%20intra-camera%0Anegative%20samples%20and%20excluding%20inter-camera%20positive%20samples.%20To%20overcome%20this%0Aproblem%2C%20we%20propose%20a%20novel%20camera-aware%20Jaccard%20%28CA-Jaccard%29%20distance%20that%0Aleverages%20camera%20information%20to%20enhance%20the%20reliability%20of%20Jaccard%20distance.%0ASpecifically%2C%20we%20design%20camera-aware%20k-reciprocal%20nearest%20neighbors%20%28CKRNNs%29%20to%0Afind%20k-reciprocal%20nearest%20neighbors%20on%20the%20intra-camera%20and%20inter-camera%0Aranking%20lists%2C%20which%20improves%20the%20reliability%20of%20relevant%20neighbors%20and%0Aguarantees%20the%20contribution%20of%20inter-camera%20samples%20in%20the%20overlap.%20Moreover%2C%0Awe%20propose%20a%20camera-aware%20local%20query%20expansion%20%28CLQE%29%20to%20mine%20reliable%20samples%0Ain%20relevant%20neighbors%20by%20exploiting%20camera%20variation%20as%20a%20strong%20constraint%20and%0Aassign%20these%20samples%20higher%20weights%20in%20overlap%2C%20further%20improving%20the%0Areliability.%20Our%20CA-Jaccard%20distance%20is%20simple%20yet%20effective%20and%20can%20serve%20as%20a%0Ageneral%20distance%20metric%20for%20person%20re-ID%20methods%20with%20high%20reliability%20and%20low%0Acomputational%20cost.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10605v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CA-Jaccard%3A%20Camera-aware%20Jaccard%20Distance%20for%20Person%20Re-identification&entry.906535625=Yiyu%20Chen%20and%20Zheyi%20Fan%20and%20Zhaoru%20Chen%20and%20Yixuan%20Zhu&entry.1292438233=%20%20Person%20re-identification%20%28re-ID%29%20is%20a%20challenging%20task%20that%20aims%20to%20learn%0Adiscriminative%20features%20for%20person%20retrieval.%20In%20person%20re-ID%2C%20Jaccard%20distance%0Ais%20a%20widely%20used%20distance%20metric%2C%20especially%20in%20re-ranking%20and%20clustering%0Ascenarios.%20However%2C%20we%20discover%20that%20camera%20variation%20has%20a%20significant%0Anegative%20impact%20on%20the%20reliability%20of%20Jaccard%20distance.%20In%20particular%2C%20Jaccard%0Adistance%20calculates%20the%20distance%20based%20on%20the%20overlap%20of%20relevant%20neighbors.%0ADue%20to%20camera%20variation%2C%20intra-camera%20samples%20dominate%20the%20relevant%20neighbors%2C%0Awhich%20reduces%20the%20reliability%20of%20the%20neighbors%20by%20introducing%20intra-camera%0Anegative%20samples%20and%20excluding%20inter-camera%20positive%20samples.%20To%20overcome%20this%0Aproblem%2C%20we%20propose%20a%20novel%20camera-aware%20Jaccard%20%28CA-Jaccard%29%20distance%20that%0Aleverages%20camera%20information%20to%20enhance%20the%20reliability%20of%20Jaccard%20distance.%0ASpecifically%2C%20we%20design%20camera-aware%20k-reciprocal%20nearest%20neighbors%20%28CKRNNs%29%20to%0Afind%20k-reciprocal%20nearest%20neighbors%20on%20the%20intra-camera%20and%20inter-camera%0Aranking%20lists%2C%20which%20improves%20the%20reliability%20of%20relevant%20neighbors%20and%0Aguarantees%20the%20contribution%20of%20inter-camera%20samples%20in%20the%20overlap.%20Moreover%2C%0Awe%20propose%20a%20camera-aware%20local%20query%20expansion%20%28CLQE%29%20to%20mine%20reliable%20samples%0Ain%20relevant%20neighbors%20by%20exploiting%20camera%20variation%20as%20a%20strong%20constraint%20and%0Aassign%20these%20samples%20higher%20weights%20in%20overlap%2C%20further%20improving%20the%0Areliability.%20Our%20CA-Jaccard%20distance%20is%20simple%20yet%20effective%20and%20can%20serve%20as%20a%0Ageneral%20distance%20metric%20for%20person%20re-ID%20methods%20with%20high%20reliability%20and%20low%0Acomputational%20cost.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10605v2&entry.124074799=Read"},
{"title": "AnchorAL: Computationally Efficient Active Learning for Large and\n  Imbalanced Datasets", "author": "Pietro Lesci and Andreas Vlachos", "abstract": "  Active learning for imbalanced classification tasks is challenging as the\nminority classes naturally occur rarely. Gathering a large pool of unlabelled\ndata is thus essential to capture minority instances. Standard pool-based\nactive learning is computationally expensive on large pools and often reaches\nlow accuracy by overfitting the initial decision boundary, thus failing to\nexplore the input space and find minority instances. To address these issues we\npropose AnchorAL. At each iteration, AnchorAL chooses class-specific instances\nfrom the labelled set, or anchors, and retrieves the most similar unlabelled\ninstances from the pool. This resulting subpool is then used for active\nlearning. Using a small, fixed-sized subpool AnchorAL allows scaling any active\nlearning strategy to large pools. By dynamically selecting different anchors at\neach iteration it promotes class balance and prevents overfitting the initial\ndecision boundary, thus promoting the discovery of new clusters of minority\ninstances. Experiments across different classification tasks, active learning\nstrategies, and model architectures AnchorAL is (i) faster, often reducing\nruntime from hours to minutes, (ii) trains more performant models, (iii) and\nreturns more balanced datasets than competing methods.\n", "link": "http://arxiv.org/abs/2404.05623v1", "date": "2024-04-08", "relevancy": 2.4404, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4643}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnchorAL%3A%20Computationally%20Efficient%20Active%20Learning%20for%20Large%20and%0A%20%20Imbalanced%20Datasets&body=Title%3A%20AnchorAL%3A%20Computationally%20Efficient%20Active%20Learning%20for%20Large%20and%0A%20%20Imbalanced%20Datasets%0AAuthor%3A%20Pietro%20Lesci%20and%20Andreas%20Vlachos%0AAbstract%3A%20%20%20Active%20learning%20for%20imbalanced%20classification%20tasks%20is%20challenging%20as%20the%0Aminority%20classes%20naturally%20occur%20rarely.%20Gathering%20a%20large%20pool%20of%20unlabelled%0Adata%20is%20thus%20essential%20to%20capture%20minority%20instances.%20Standard%20pool-based%0Aactive%20learning%20is%20computationally%20expensive%20on%20large%20pools%20and%20often%20reaches%0Alow%20accuracy%20by%20overfitting%20the%20initial%20decision%20boundary%2C%20thus%20failing%20to%0Aexplore%20the%20input%20space%20and%20find%20minority%20instances.%20To%20address%20these%20issues%20we%0Apropose%20AnchorAL.%20At%20each%20iteration%2C%20AnchorAL%20chooses%20class-specific%20instances%0Afrom%20the%20labelled%20set%2C%20or%20anchors%2C%20and%20retrieves%20the%20most%20similar%20unlabelled%0Ainstances%20from%20the%20pool.%20This%20resulting%20subpool%20is%20then%20used%20for%20active%0Alearning.%20Using%20a%20small%2C%20fixed-sized%20subpool%20AnchorAL%20allows%20scaling%20any%20active%0Alearning%20strategy%20to%20large%20pools.%20By%20dynamically%20selecting%20different%20anchors%20at%0Aeach%20iteration%20it%20promotes%20class%20balance%20and%20prevents%20overfitting%20the%20initial%0Adecision%20boundary%2C%20thus%20promoting%20the%20discovery%20of%20new%20clusters%20of%20minority%0Ainstances.%20Experiments%20across%20different%20classification%20tasks%2C%20active%20learning%0Astrategies%2C%20and%20model%20architectures%20AnchorAL%20is%20%28i%29%20faster%2C%20often%20reducing%0Aruntime%20from%20hours%20to%20minutes%2C%20%28ii%29%20trains%20more%20performant%20models%2C%20%28iii%29%20and%0Areturns%20more%20balanced%20datasets%20than%20competing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorAL%3A%20Computationally%20Efficient%20Active%20Learning%20for%20Large%20and%0A%20%20Imbalanced%20Datasets&entry.906535625=Pietro%20Lesci%20and%20Andreas%20Vlachos&entry.1292438233=%20%20Active%20learning%20for%20imbalanced%20classification%20tasks%20is%20challenging%20as%20the%0Aminority%20classes%20naturally%20occur%20rarely.%20Gathering%20a%20large%20pool%20of%20unlabelled%0Adata%20is%20thus%20essential%20to%20capture%20minority%20instances.%20Standard%20pool-based%0Aactive%20learning%20is%20computationally%20expensive%20on%20large%20pools%20and%20often%20reaches%0Alow%20accuracy%20by%20overfitting%20the%20initial%20decision%20boundary%2C%20thus%20failing%20to%0Aexplore%20the%20input%20space%20and%20find%20minority%20instances.%20To%20address%20these%20issues%20we%0Apropose%20AnchorAL.%20At%20each%20iteration%2C%20AnchorAL%20chooses%20class-specific%20instances%0Afrom%20the%20labelled%20set%2C%20or%20anchors%2C%20and%20retrieves%20the%20most%20similar%20unlabelled%0Ainstances%20from%20the%20pool.%20This%20resulting%20subpool%20is%20then%20used%20for%20active%0Alearning.%20Using%20a%20small%2C%20fixed-sized%20subpool%20AnchorAL%20allows%20scaling%20any%20active%0Alearning%20strategy%20to%20large%20pools.%20By%20dynamically%20selecting%20different%20anchors%20at%0Aeach%20iteration%20it%20promotes%20class%20balance%20and%20prevents%20overfitting%20the%20initial%0Adecision%20boundary%2C%20thus%20promoting%20the%20discovery%20of%20new%20clusters%20of%20minority%0Ainstances.%20Experiments%20across%20different%20classification%20tasks%2C%20active%20learning%0Astrategies%2C%20and%20model%20architectures%20AnchorAL%20is%20%28i%29%20faster%2C%20often%20reducing%0Aruntime%20from%20hours%20to%20minutes%2C%20%28ii%29%20trains%20more%20performant%20models%2C%20%28iii%29%20and%0Areturns%20more%20balanced%20datasets%20than%20competing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05623v1&entry.124074799=Read"},
{"title": "Photo-SLAM: Real-time Simultaneous Localization and Photorealistic\n  Mapping for Monocular, Stereo, and RGB-D Cameras", "author": "Huajian Huang and Longwei Li and Hui Cheng and Sai-Kit Yeung", "abstract": "  The integration of neural rendering and the SLAM system recently showed\npromising results in joint localization and photorealistic view reconstruction.\nHowever, existing methods, fully relying on implicit representations, are so\nresource-hungry that they cannot run on portable devices, which deviates from\nthe original intention of SLAM. In this paper, we present Photo-SLAM, a novel\nSLAM framework with a hyper primitives map. Specifically, we simultaneously\nexploit explicit geometric features for localization and learn implicit\nphotometric features to represent the texture information of the observed\nenvironment. In addition to actively densifying hyper primitives based on\ngeometric features, we further introduce a Gaussian-Pyramid-based training\nmethod to progressively learn multi-level features, enhancing photorealistic\nmapping performance. The extensive experiments with monocular, stereo, and\nRGB-D datasets prove that our proposed system Photo-SLAM significantly\noutperforms current state-of-the-art SLAM systems for online photorealistic\nmapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times\nfaster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time\nspeed using an embedded platform such as Jetson AGX Orin, showing the potential\nof robotics applications.\n", "link": "http://arxiv.org/abs/2311.16728v2", "date": "2024-04-08", "relevancy": 2.4389, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5996}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5671}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Photo-SLAM%3A%20Real-time%20Simultaneous%20Localization%20and%20Photorealistic%0A%20%20Mapping%20for%20Monocular%2C%20Stereo%2C%20and%20RGB-D%20Cameras&body=Title%3A%20Photo-SLAM%3A%20Real-time%20Simultaneous%20Localization%20and%20Photorealistic%0A%20%20Mapping%20for%20Monocular%2C%20Stereo%2C%20and%20RGB-D%20Cameras%0AAuthor%3A%20Huajian%20Huang%20and%20Longwei%20Li%20and%20Hui%20Cheng%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20The%20integration%20of%20neural%20rendering%20and%20the%20SLAM%20system%20recently%20showed%0Apromising%20results%20in%20joint%20localization%20and%20photorealistic%20view%20reconstruction.%0AHowever%2C%20existing%20methods%2C%20fully%20relying%20on%20implicit%20representations%2C%20are%20so%0Aresource-hungry%20that%20they%20cannot%20run%20on%20portable%20devices%2C%20which%20deviates%20from%0Athe%20original%20intention%20of%20SLAM.%20In%20this%20paper%2C%20we%20present%20Photo-SLAM%2C%20a%20novel%0ASLAM%20framework%20with%20a%20hyper%20primitives%20map.%20Specifically%2C%20we%20simultaneously%0Aexploit%20explicit%20geometric%20features%20for%20localization%20and%20learn%20implicit%0Aphotometric%20features%20to%20represent%20the%20texture%20information%20of%20the%20observed%0Aenvironment.%20In%20addition%20to%20actively%20densifying%20hyper%20primitives%20based%20on%0Ageometric%20features%2C%20we%20further%20introduce%20a%20Gaussian-Pyramid-based%20training%0Amethod%20to%20progressively%20learn%20multi-level%20features%2C%20enhancing%20photorealistic%0Amapping%20performance.%20The%20extensive%20experiments%20with%20monocular%2C%20stereo%2C%20and%0ARGB-D%20datasets%20prove%20that%20our%20proposed%20system%20Photo-SLAM%20significantly%0Aoutperforms%20current%20state-of-the-art%20SLAM%20systems%20for%20online%20photorealistic%0Amapping%2C%20e.g.%2C%20PSNR%20is%2030%25%20higher%20and%20rendering%20speed%20is%20hundreds%20of%20times%0Afaster%20in%20the%20Replica%20dataset.%20Moreover%2C%20the%20Photo-SLAM%20can%20run%20at%20real-time%0Aspeed%20using%20an%20embedded%20platform%20such%20as%20Jetson%20AGX%20Orin%2C%20showing%20the%20potential%0Aof%20robotics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16728v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photo-SLAM%3A%20Real-time%20Simultaneous%20Localization%20and%20Photorealistic%0A%20%20Mapping%20for%20Monocular%2C%20Stereo%2C%20and%20RGB-D%20Cameras&entry.906535625=Huajian%20Huang%20and%20Longwei%20Li%20and%20Hui%20Cheng%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20The%20integration%20of%20neural%20rendering%20and%20the%20SLAM%20system%20recently%20showed%0Apromising%20results%20in%20joint%20localization%20and%20photorealistic%20view%20reconstruction.%0AHowever%2C%20existing%20methods%2C%20fully%20relying%20on%20implicit%20representations%2C%20are%20so%0Aresource-hungry%20that%20they%20cannot%20run%20on%20portable%20devices%2C%20which%20deviates%20from%0Athe%20original%20intention%20of%20SLAM.%20In%20this%20paper%2C%20we%20present%20Photo-SLAM%2C%20a%20novel%0ASLAM%20framework%20with%20a%20hyper%20primitives%20map.%20Specifically%2C%20we%20simultaneously%0Aexploit%20explicit%20geometric%20features%20for%20localization%20and%20learn%20implicit%0Aphotometric%20features%20to%20represent%20the%20texture%20information%20of%20the%20observed%0Aenvironment.%20In%20addition%20to%20actively%20densifying%20hyper%20primitives%20based%20on%0Ageometric%20features%2C%20we%20further%20introduce%20a%20Gaussian-Pyramid-based%20training%0Amethod%20to%20progressively%20learn%20multi-level%20features%2C%20enhancing%20photorealistic%0Amapping%20performance.%20The%20extensive%20experiments%20with%20monocular%2C%20stereo%2C%20and%0ARGB-D%20datasets%20prove%20that%20our%20proposed%20system%20Photo-SLAM%20significantly%0Aoutperforms%20current%20state-of-the-art%20SLAM%20systems%20for%20online%20photorealistic%0Amapping%2C%20e.g.%2C%20PSNR%20is%2030%25%20higher%20and%20rendering%20speed%20is%20hundreds%20of%20times%0Afaster%20in%20the%20Replica%20dataset.%20Moreover%2C%20the%20Photo-SLAM%20can%20run%20at%20real-time%0Aspeed%20using%20an%20embedded%20platform%20such%20as%20Jetson%20AGX%20Orin%2C%20showing%20the%20potential%0Aof%20robotics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16728v2&entry.124074799=Read"},
{"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "author": "Xinrun Du and Zhouliang Yu and Songyang Gao and Ding Pan and Yuyang Cheng and Ziyang Ma and Ruibin Yuan and Xingwei Qu and Jiaheng Liu and Tianyu Zheng and Xinchen Luo and Guorui Zhou and Binhang Yuan and Wenhu Chen and Jie Fu and Ge Zhang", "abstract": "  In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.\n", "link": "http://arxiv.org/abs/2404.04167v2", "date": "2024-04-08", "relevancy": 2.4341, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&body=Title%3A%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model%0AAuthor%3A%20Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04167v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&entry.906535625=Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04167v2&entry.124074799=Read"},
{"title": "Action-conditioned video data improves predictability", "author": "Meenakshi Sarkar and Debasish Ghose", "abstract": "  Long-term video generation and prediction remain challenging tasks in\ncomputer vision, particularly in partially observable scenarios where cameras\nare mounted on moving platforms. The interaction between observed image frames\nand the motion of the recording agent introduces additional complexities. To\naddress these issues, we introduce the Action-Conditioned Video Generation\n(ACVG) framework, a novel approach that investigates the relationship between\nactions and generated image frames through a deep dual Generator-Actor\narchitecture. ACVG generates video sequences conditioned on the actions of\nrobots, enabling exploration and analysis of how vision and action mutually\ninfluence one another in dynamic environments. We evaluate the framework's\neffectiveness on an indoor robot motion dataset which consists of sequences of\nimage frames along with the sequences of actions taken by the robotic agent,\nconducting a comprehensive empirical study comparing ACVG to other\nstate-of-the-art frameworks along with a detailed ablation study.\n", "link": "http://arxiv.org/abs/2404.05439v1", "date": "2024-04-08", "relevancy": 2.3993, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6219}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.61}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5808}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Action-conditioned%20video%20data%20improves%20predictability&body=Title%3A%20Action-conditioned%20video%20data%20improves%20predictability%0AAuthor%3A%20Meenakshi%20Sarkar%20and%20Debasish%20Ghose%0AAbstract%3A%20%20%20Long-term%20video%20generation%20and%20prediction%20remain%20challenging%20tasks%20in%0Acomputer%20vision%2C%20particularly%20in%20partially%20observable%20scenarios%20where%20cameras%0Aare%20mounted%20on%20moving%20platforms.%20The%20interaction%20between%20observed%20image%20frames%0Aand%20the%20motion%20of%20the%20recording%20agent%20introduces%20additional%20complexities.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Action-Conditioned%20Video%20Generation%0A%28ACVG%29%20framework%2C%20a%20novel%20approach%20that%20investigates%20the%20relationship%20between%0Aactions%20and%20generated%20image%20frames%20through%20a%20deep%20dual%20Generator-Actor%0Aarchitecture.%20ACVG%20generates%20video%20sequences%20conditioned%20on%20the%20actions%20of%0Arobots%2C%20enabling%20exploration%20and%20analysis%20of%20how%20vision%20and%20action%20mutually%0Ainfluence%20one%20another%20in%20dynamic%20environments.%20We%20evaluate%20the%20framework%27s%0Aeffectiveness%20on%20an%20indoor%20robot%20motion%20dataset%20which%20consists%20of%20sequences%20of%0Aimage%20frames%20along%20with%20the%20sequences%20of%20actions%20taken%20by%20the%20robotic%20agent%2C%0Aconducting%20a%20comprehensive%20empirical%20study%20comparing%20ACVG%20to%20other%0Astate-of-the-art%20frameworks%20along%20with%20a%20detailed%20ablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05439v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action-conditioned%20video%20data%20improves%20predictability&entry.906535625=Meenakshi%20Sarkar%20and%20Debasish%20Ghose&entry.1292438233=%20%20Long-term%20video%20generation%20and%20prediction%20remain%20challenging%20tasks%20in%0Acomputer%20vision%2C%20particularly%20in%20partially%20observable%20scenarios%20where%20cameras%0Aare%20mounted%20on%20moving%20platforms.%20The%20interaction%20between%20observed%20image%20frames%0Aand%20the%20motion%20of%20the%20recording%20agent%20introduces%20additional%20complexities.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Action-Conditioned%20Video%20Generation%0A%28ACVG%29%20framework%2C%20a%20novel%20approach%20that%20investigates%20the%20relationship%20between%0Aactions%20and%20generated%20image%20frames%20through%20a%20deep%20dual%20Generator-Actor%0Aarchitecture.%20ACVG%20generates%20video%20sequences%20conditioned%20on%20the%20actions%20of%0Arobots%2C%20enabling%20exploration%20and%20analysis%20of%20how%20vision%20and%20action%20mutually%0Ainfluence%20one%20another%20in%20dynamic%20environments.%20We%20evaluate%20the%20framework%27s%0Aeffectiveness%20on%20an%20indoor%20robot%20motion%20dataset%20which%20consists%20of%20sequences%20of%0Aimage%20frames%20along%20with%20the%20sequences%20of%20actions%20taken%20by%20the%20robotic%20agent%2C%0Aconducting%20a%20comprehensive%20empirical%20study%20comparing%20ACVG%20to%20other%0Astate-of-the-art%20frameworks%20along%20with%20a%20detailed%20ablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05439v1&entry.124074799=Read"},
{"title": "360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization\n  with Cross-device Queries", "author": "Huajian Huang and Changkun Liu and Yipeng Zhu and Hui Cheng and Tristan Braud and Sai-Kit Yeung", "abstract": "  Portable 360$^\\circ$ cameras are becoming a cheap and efficient tool to\nestablish large visual databases. By capturing omnidirectional views of a\nscene, these cameras could expedite building environment models that are\nessential for visual localization. However, such an advantage is often\noverlooked due to the lack of valuable datasets. This paper introduces a new\nbenchmark dataset, 360Loc, composed of 360$^\\circ$ images with ground truth\nposes for visual localization. We present a practical implementation of\n360$^\\circ$ mapping combining 360$^\\circ$ images with lidar data to generate\nthe ground truth 6DoF poses. 360Loc is the first dataset and benchmark that\nexplores the challenge of cross-device visual positioning, involving\n360$^\\circ$ reference frames, and query frames from pinhole, ultra-wide FoV\nfisheye, and 360$^\\circ$ cameras. We propose a virtual camera approach to\ngenerate lower-FoV query frames from 360$^\\circ$ images, which ensures a fair\ncomparison of performance among different query types in visual localization\ntasks. We also extend this virtual camera approach to feature matching-based\nand pose regression-based methods to alleviate the performance loss caused by\nthe cross-device domain gap, and evaluate its effectiveness against\nstate-of-the-art baselines. We demonstrate that omnidirectional visual\nlocalization is more robust in challenging large-scale scenes with symmetries\nand repetitive structures. These results provide new insights into 360-camera\nmapping and omnidirectional visual localization with cross-device queries.\n", "link": "http://arxiv.org/abs/2311.17389v2", "date": "2024-04-08", "relevancy": 2.3977, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries&body=Title%3A%20360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries%0AAuthor%3A%20Huajian%20Huang%20and%20Changkun%20Liu%20and%20Yipeng%20Zhu%20and%20Hui%20Cheng%20and%20Tristan%20Braud%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20Portable%20360%24%5E%5Ccirc%24%20cameras%20are%20becoming%20a%20cheap%20and%20efficient%20tool%20to%0Aestablish%20large%20visual%20databases.%20By%20capturing%20omnidirectional%20views%20of%20a%0Ascene%2C%20these%20cameras%20could%20expedite%20building%20environment%20models%20that%20are%0Aessential%20for%20visual%20localization.%20However%2C%20such%20an%20advantage%20is%20often%0Aoverlooked%20due%20to%20the%20lack%20of%20valuable%20datasets.%20This%20paper%20introduces%20a%20new%0Abenchmark%20dataset%2C%20360Loc%2C%20composed%20of%20360%24%5E%5Ccirc%24%20images%20with%20ground%20truth%0Aposes%20for%20visual%20localization.%20We%20present%20a%20practical%20implementation%20of%0A360%24%5E%5Ccirc%24%20mapping%20combining%20360%24%5E%5Ccirc%24%20images%20with%20lidar%20data%20to%20generate%0Athe%20ground%20truth%206DoF%20poses.%20360Loc%20is%20the%20first%20dataset%20and%20benchmark%20that%0Aexplores%20the%20challenge%20of%20cross-device%20visual%20positioning%2C%20involving%0A360%24%5E%5Ccirc%24%20reference%20frames%2C%20and%20query%20frames%20from%20pinhole%2C%20ultra-wide%20FoV%0Afisheye%2C%20and%20360%24%5E%5Ccirc%24%20cameras.%20We%20propose%20a%20virtual%20camera%20approach%20to%0Agenerate%20lower-FoV%20query%20frames%20from%20360%24%5E%5Ccirc%24%20images%2C%20which%20ensures%20a%20fair%0Acomparison%20of%20performance%20among%20different%20query%20types%20in%20visual%20localization%0Atasks.%20We%20also%20extend%20this%20virtual%20camera%20approach%20to%20feature%20matching-based%0Aand%20pose%20regression-based%20methods%20to%20alleviate%20the%20performance%20loss%20caused%20by%0Athe%20cross-device%20domain%20gap%2C%20and%20evaluate%20its%20effectiveness%20against%0Astate-of-the-art%20baselines.%20We%20demonstrate%20that%20omnidirectional%20visual%0Alocalization%20is%20more%20robust%20in%20challenging%20large-scale%20scenes%20with%20symmetries%0Aand%20repetitive%20structures.%20These%20results%20provide%20new%20insights%20into%20360-camera%0Amapping%20and%20omnidirectional%20visual%20localization%20with%20cross-device%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17389v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries&entry.906535625=Huajian%20Huang%20and%20Changkun%20Liu%20and%20Yipeng%20Zhu%20and%20Hui%20Cheng%20and%20Tristan%20Braud%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20Portable%20360%24%5E%5Ccirc%24%20cameras%20are%20becoming%20a%20cheap%20and%20efficient%20tool%20to%0Aestablish%20large%20visual%20databases.%20By%20capturing%20omnidirectional%20views%20of%20a%0Ascene%2C%20these%20cameras%20could%20expedite%20building%20environment%20models%20that%20are%0Aessential%20for%20visual%20localization.%20However%2C%20such%20an%20advantage%20is%20often%0Aoverlooked%20due%20to%20the%20lack%20of%20valuable%20datasets.%20This%20paper%20introduces%20a%20new%0Abenchmark%20dataset%2C%20360Loc%2C%20composed%20of%20360%24%5E%5Ccirc%24%20images%20with%20ground%20truth%0Aposes%20for%20visual%20localization.%20We%20present%20a%20practical%20implementation%20of%0A360%24%5E%5Ccirc%24%20mapping%20combining%20360%24%5E%5Ccirc%24%20images%20with%20lidar%20data%20to%20generate%0Athe%20ground%20truth%206DoF%20poses.%20360Loc%20is%20the%20first%20dataset%20and%20benchmark%20that%0Aexplores%20the%20challenge%20of%20cross-device%20visual%20positioning%2C%20involving%0A360%24%5E%5Ccirc%24%20reference%20frames%2C%20and%20query%20frames%20from%20pinhole%2C%20ultra-wide%20FoV%0Afisheye%2C%20and%20360%24%5E%5Ccirc%24%20cameras.%20We%20propose%20a%20virtual%20camera%20approach%20to%0Agenerate%20lower-FoV%20query%20frames%20from%20360%24%5E%5Ccirc%24%20images%2C%20which%20ensures%20a%20fair%0Acomparison%20of%20performance%20among%20different%20query%20types%20in%20visual%20localization%0Atasks.%20We%20also%20extend%20this%20virtual%20camera%20approach%20to%20feature%20matching-based%0Aand%20pose%20regression-based%20methods%20to%20alleviate%20the%20performance%20loss%20caused%20by%0Athe%20cross-device%20domain%20gap%2C%20and%20evaluate%20its%20effectiveness%20against%0Astate-of-the-art%20baselines.%20We%20demonstrate%20that%20omnidirectional%20visual%0Alocalization%20is%20more%20robust%20in%20challenging%20large-scale%20scenes%20with%20symmetries%0Aand%20repetitive%20structures.%20These%20results%20provide%20new%20insights%20into%20360-camera%0Amapping%20and%20omnidirectional%20visual%20localization%20with%20cross-device%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17389v2&entry.124074799=Read"},
{"title": "A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion", "author": "Guokai Zhang and Lanjun Wang and Yuting Su and An-An Liu", "abstract": "  Nowadays, the family of Stable Diffusion (SD) models has gained prominence\nfor its high quality outputs and scalability. This has also raised security\nconcerns on social media, as malicious users can create and disseminate harmful\ncontent. Existing approaches involve training components or entire SDs to embed\na watermark in generated images for traceability and responsibility\nattribution. However, in the era of AI-generated content (AIGC), the rapid\niteration of SDs renders retraining with watermark models costly. To address\nthis, we propose a training-free plug-and-play watermark framework for SDs.\nWithout modifying any components of SDs, we embed diverse watermarks in the\nlatent space, adapting to the denoising process. Our experimental findings\nreveal that our method effectively harmonizes image quality and watermark\ninvisibility. Furthermore, it performs robustly under various attacks. We also\nhave validated that our method is generalized to multiple versions of SDs, even\nwithout retraining the watermark model.\n", "link": "http://arxiv.org/abs/2404.05607v1", "date": "2024-04-08", "relevancy": 2.3797, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6399}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5855}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Training-Free%20Plug-and-Play%20Watermark%20Framework%20for%20Stable%20Diffusion&body=Title%3A%20A%20Training-Free%20Plug-and-Play%20Watermark%20Framework%20for%20Stable%20Diffusion%0AAuthor%3A%20Guokai%20Zhang%20and%20Lanjun%20Wang%20and%20Yuting%20Su%20and%20An-An%20Liu%0AAbstract%3A%20%20%20Nowadays%2C%20the%20family%20of%20Stable%20Diffusion%20%28SD%29%20models%20has%20gained%20prominence%0Afor%20its%20high%20quality%20outputs%20and%20scalability.%20This%20has%20also%20raised%20security%0Aconcerns%20on%20social%20media%2C%20as%20malicious%20users%20can%20create%20and%20disseminate%20harmful%0Acontent.%20Existing%20approaches%20involve%20training%20components%20or%20entire%20SDs%20to%20embed%0Aa%20watermark%20in%20generated%20images%20for%20traceability%20and%20responsibility%0Aattribution.%20However%2C%20in%20the%20era%20of%20AI-generated%20content%20%28AIGC%29%2C%20the%20rapid%0Aiteration%20of%20SDs%20renders%20retraining%20with%20watermark%20models%20costly.%20To%20address%0Athis%2C%20we%20propose%20a%20training-free%20plug-and-play%20watermark%20framework%20for%20SDs.%0AWithout%20modifying%20any%20components%20of%20SDs%2C%20we%20embed%20diverse%20watermarks%20in%20the%0Alatent%20space%2C%20adapting%20to%20the%20denoising%20process.%20Our%20experimental%20findings%0Areveal%20that%20our%20method%20effectively%20harmonizes%20image%20quality%20and%20watermark%0Ainvisibility.%20Furthermore%2C%20it%20performs%20robustly%20under%20various%20attacks.%20We%20also%0Ahave%20validated%20that%20our%20method%20is%20generalized%20to%20multiple%20versions%20of%20SDs%2C%20even%0Awithout%20retraining%20the%20watermark%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05607v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Training-Free%20Plug-and-Play%20Watermark%20Framework%20for%20Stable%20Diffusion&entry.906535625=Guokai%20Zhang%20and%20Lanjun%20Wang%20and%20Yuting%20Su%20and%20An-An%20Liu&entry.1292438233=%20%20Nowadays%2C%20the%20family%20of%20Stable%20Diffusion%20%28SD%29%20models%20has%20gained%20prominence%0Afor%20its%20high%20quality%20outputs%20and%20scalability.%20This%20has%20also%20raised%20security%0Aconcerns%20on%20social%20media%2C%20as%20malicious%20users%20can%20create%20and%20disseminate%20harmful%0Acontent.%20Existing%20approaches%20involve%20training%20components%20or%20entire%20SDs%20to%20embed%0Aa%20watermark%20in%20generated%20images%20for%20traceability%20and%20responsibility%0Aattribution.%20However%2C%20in%20the%20era%20of%20AI-generated%20content%20%28AIGC%29%2C%20the%20rapid%0Aiteration%20of%20SDs%20renders%20retraining%20with%20watermark%20models%20costly.%20To%20address%0Athis%2C%20we%20propose%20a%20training-free%20plug-and-play%20watermark%20framework%20for%20SDs.%0AWithout%20modifying%20any%20components%20of%20SDs%2C%20we%20embed%20diverse%20watermarks%20in%20the%0Alatent%20space%2C%20adapting%20to%20the%20denoising%20process.%20Our%20experimental%20findings%0Areveal%20that%20our%20method%20effectively%20harmonizes%20image%20quality%20and%20watermark%0Ainvisibility.%20Furthermore%2C%20it%20performs%20robustly%20under%20various%20attacks.%20We%20also%0Ahave%20validated%20that%20our%20method%20is%20generalized%20to%20multiple%20versions%20of%20SDs%2C%20even%0Awithout%20retraining%20the%20watermark%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05607v1&entry.124074799=Read"},
{"title": "Language-Independent Representations Improve Zero-Shot Summarization", "author": "Vladimir Solovyev and Danni Liu and Jan Niehues", "abstract": "  Finetuning pretrained models on downstream generation tasks often leads to\ncatastrophic forgetting in zero-shot conditions. In this work, we focus on\nsummarization and tackle the problem through the lens of language-independent\nrepresentations. After training on monolingual summarization, we perform\nzero-shot transfer to new languages or language pairs. We first show naively\nfinetuned models are highly language-specific in both output behavior and\ninternal representations, resulting in poor zero-shot performance. Next, we\npropose query-key (QK) finetuning to decouple task-specific knowledge from the\npretrained language generation abilities. Then, after showing downsides of the\nstandard adversarial language classifier, we propose a balanced variant that\nmore directly enforces language-agnostic representations. Moreover, our\nqualitative analyses show removing source language identity correlates to\nzero-shot summarization performance. Our code is openly available.\n", "link": "http://arxiv.org/abs/2404.05720v1", "date": "2024-04-08", "relevancy": 2.3741, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4786}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Language-Independent%20Representations%20Improve%20Zero-Shot%20Summarization&body=Title%3A%20Language-Independent%20Representations%20Improve%20Zero-Shot%20Summarization%0AAuthor%3A%20Vladimir%20Solovyev%20and%20Danni%20Liu%20and%20Jan%20Niehues%0AAbstract%3A%20%20%20Finetuning%20pretrained%20models%20on%20downstream%20generation%20tasks%20often%20leads%20to%0Acatastrophic%20forgetting%20in%20zero-shot%20conditions.%20In%20this%20work%2C%20we%20focus%20on%0Asummarization%20and%20tackle%20the%20problem%20through%20the%20lens%20of%20language-independent%0Arepresentations.%20After%20training%20on%20monolingual%20summarization%2C%20we%20perform%0Azero-shot%20transfer%20to%20new%20languages%20or%20language%20pairs.%20We%20first%20show%20naively%0Afinetuned%20models%20are%20highly%20language-specific%20in%20both%20output%20behavior%20and%0Ainternal%20representations%2C%20resulting%20in%20poor%20zero-shot%20performance.%20Next%2C%20we%0Apropose%20query-key%20%28QK%29%20finetuning%20to%20decouple%20task-specific%20knowledge%20from%20the%0Apretrained%20language%20generation%20abilities.%20Then%2C%20after%20showing%20downsides%20of%20the%0Astandard%20adversarial%20language%20classifier%2C%20we%20propose%20a%20balanced%20variant%20that%0Amore%20directly%20enforces%20language-agnostic%20representations.%20Moreover%2C%20our%0Aqualitative%20analyses%20show%20removing%20source%20language%20identity%20correlates%20to%0Azero-shot%20summarization%20performance.%20Our%20code%20is%20openly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05720v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Independent%20Representations%20Improve%20Zero-Shot%20Summarization&entry.906535625=Vladimir%20Solovyev%20and%20Danni%20Liu%20and%20Jan%20Niehues&entry.1292438233=%20%20Finetuning%20pretrained%20models%20on%20downstream%20generation%20tasks%20often%20leads%20to%0Acatastrophic%20forgetting%20in%20zero-shot%20conditions.%20In%20this%20work%2C%20we%20focus%20on%0Asummarization%20and%20tackle%20the%20problem%20through%20the%20lens%20of%20language-independent%0Arepresentations.%20After%20training%20on%20monolingual%20summarization%2C%20we%20perform%0Azero-shot%20transfer%20to%20new%20languages%20or%20language%20pairs.%20We%20first%20show%20naively%0Afinetuned%20models%20are%20highly%20language-specific%20in%20both%20output%20behavior%20and%0Ainternal%20representations%2C%20resulting%20in%20poor%20zero-shot%20performance.%20Next%2C%20we%0Apropose%20query-key%20%28QK%29%20finetuning%20to%20decouple%20task-specific%20knowledge%20from%20the%0Apretrained%20language%20generation%20abilities.%20Then%2C%20after%20showing%20downsides%20of%20the%0Astandard%20adversarial%20language%20classifier%2C%20we%20propose%20a%20balanced%20variant%20that%0Amore%20directly%20enforces%20language-agnostic%20representations.%20Moreover%2C%20our%0Aqualitative%20analyses%20show%20removing%20source%20language%20identity%20correlates%20to%0Azero-shot%20summarization%20performance.%20Our%20code%20is%20openly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05720v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress,\n  and Prospects", "author": "Kexin Zhang and Qingsong Wen and Chaoli Zhang and Rongyao Cai and Ming Jin and Yong Liu and James Zhang and Yuxuan Liang and Guansong Pang and Dongjin Song and Shirui Pan", "abstract": "  Self-supervised learning (SSL) has recently achieved impressive performance\non various time series tasks. The most prominent advantage of SSL is that it\nreduces the dependence on labeled data. Based on the pre-training and\nfine-tuning strategy, even a small amount of labeled data can achieve high\nperformance. Compared with many published self-supervised surveys on computer\nvision and natural language processing, a comprehensive survey for time series\nSSL is still missing. To fill this gap, we review current state-of-the-art SSL\nmethods for time series data in this article. To this end, we first\ncomprehensively review existing surveys related to SSL and time series, and\nthen provide a new taxonomy of existing time series SSL methods by summarizing\nthem from three perspectives: generative-based, contrastive-based, and\nadversarial-based. These methods are further divided into ten subcategories\nwith detailed reviews and discussions about their key intuitions, main\nframeworks, advantages and disadvantages. To facilitate the experiments and\nvalidation of time series SSL methods, we also summarize datasets commonly used\nin time series forecasting, classification, anomaly detection, and clustering\ntasks. Finally, we present the future directions of SSL for time series\nanalysis.\n", "link": "http://arxiv.org/abs/2306.10125v4", "date": "2024-04-08", "relevancy": 2.3599, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4902}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4287}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Time%20Series%20Analysis%3A%20Taxonomy%2C%20Progress%2C%0A%20%20and%20Prospects&body=Title%3A%20Self-Supervised%20Learning%20for%20Time%20Series%20Analysis%3A%20Taxonomy%2C%20Progress%2C%0A%20%20and%20Prospects%0AAuthor%3A%20Kexin%20Zhang%20and%20Qingsong%20Wen%20and%20Chaoli%20Zhang%20and%20Rongyao%20Cai%20and%20Ming%20Jin%20and%20Yong%20Liu%20and%20James%20Zhang%20and%20Yuxuan%20Liang%20and%20Guansong%20Pang%20and%20Dongjin%20Song%20and%20Shirui%20Pan%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20achieved%20impressive%20performance%0Aon%20various%20time%20series%20tasks.%20The%20most%20prominent%20advantage%20of%20SSL%20is%20that%20it%0Areduces%20the%20dependence%20on%20labeled%20data.%20Based%20on%20the%20pre-training%20and%0Afine-tuning%20strategy%2C%20even%20a%20small%20amount%20of%20labeled%20data%20can%20achieve%20high%0Aperformance.%20Compared%20with%20many%20published%20self-supervised%20surveys%20on%20computer%0Avision%20and%20natural%20language%20processing%2C%20a%20comprehensive%20survey%20for%20time%20series%0ASSL%20is%20still%20missing.%20To%20fill%20this%20gap%2C%20we%20review%20current%20state-of-the-art%20SSL%0Amethods%20for%20time%20series%20data%20in%20this%20article.%20To%20this%20end%2C%20we%20first%0Acomprehensively%20review%20existing%20surveys%20related%20to%20SSL%20and%20time%20series%2C%20and%0Athen%20provide%20a%20new%20taxonomy%20of%20existing%20time%20series%20SSL%20methods%20by%20summarizing%0Athem%20from%20three%20perspectives%3A%20generative-based%2C%20contrastive-based%2C%20and%0Aadversarial-based.%20These%20methods%20are%20further%20divided%20into%20ten%20subcategories%0Awith%20detailed%20reviews%20and%20discussions%20about%20their%20key%20intuitions%2C%20main%0Aframeworks%2C%20advantages%20and%20disadvantages.%20To%20facilitate%20the%20experiments%20and%0Avalidation%20of%20time%20series%20SSL%20methods%2C%20we%20also%20summarize%20datasets%20commonly%20used%0Ain%20time%20series%20forecasting%2C%20classification%2C%20anomaly%20detection%2C%20and%20clustering%0Atasks.%20Finally%2C%20we%20present%20the%20future%20directions%20of%20SSL%20for%20time%20series%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10125v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Time%20Series%20Analysis%3A%20Taxonomy%2C%20Progress%2C%0A%20%20and%20Prospects&entry.906535625=Kexin%20Zhang%20and%20Qingsong%20Wen%20and%20Chaoli%20Zhang%20and%20Rongyao%20Cai%20and%20Ming%20Jin%20and%20Yong%20Liu%20and%20James%20Zhang%20and%20Yuxuan%20Liang%20and%20Guansong%20Pang%20and%20Dongjin%20Song%20and%20Shirui%20Pan&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20achieved%20impressive%20performance%0Aon%20various%20time%20series%20tasks.%20The%20most%20prominent%20advantage%20of%20SSL%20is%20that%20it%0Areduces%20the%20dependence%20on%20labeled%20data.%20Based%20on%20the%20pre-training%20and%0Afine-tuning%20strategy%2C%20even%20a%20small%20amount%20of%20labeled%20data%20can%20achieve%20high%0Aperformance.%20Compared%20with%20many%20published%20self-supervised%20surveys%20on%20computer%0Avision%20and%20natural%20language%20processing%2C%20a%20comprehensive%20survey%20for%20time%20series%0ASSL%20is%20still%20missing.%20To%20fill%20this%20gap%2C%20we%20review%20current%20state-of-the-art%20SSL%0Amethods%20for%20time%20series%20data%20in%20this%20article.%20To%20this%20end%2C%20we%20first%0Acomprehensively%20review%20existing%20surveys%20related%20to%20SSL%20and%20time%20series%2C%20and%0Athen%20provide%20a%20new%20taxonomy%20of%20existing%20time%20series%20SSL%20methods%20by%20summarizing%0Athem%20from%20three%20perspectives%3A%20generative-based%2C%20contrastive-based%2C%20and%0Aadversarial-based.%20These%20methods%20are%20further%20divided%20into%20ten%20subcategories%0Awith%20detailed%20reviews%20and%20discussions%20about%20their%20key%20intuitions%2C%20main%0Aframeworks%2C%20advantages%20and%20disadvantages.%20To%20facilitate%20the%20experiments%20and%0Avalidation%20of%20time%20series%20SSL%20methods%2C%20we%20also%20summarize%20datasets%20commonly%20used%0Ain%20time%20series%20forecasting%2C%20classification%2C%20anomaly%20detection%2C%20and%20clustering%0Atasks.%20Finally%2C%20we%20present%20the%20future%20directions%20of%20SSL%20for%20time%20series%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10125v4&entry.124074799=Read"},
{"title": "Test-Time Zero-Shot Temporal Action Localization", "author": "Benedetta Liberatori and Alessandro Conti and Paolo Rota and Yiming Wang and Elisa Ricci", "abstract": "  Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate\nactions in untrimmed videos unseen during training. Existing ZS-TAL methods\ninvolve fine-tuning a model on a large amount of annotated training data. While\neffective, training-based ZS-TAL approaches assume the availability of labeled\ndata for supervised learning, which can be impractical in some applications.\nFurthermore, the training process naturally induces a domain bias into the\nlearned model, which may adversely affect the model's generalization ability to\narbitrary videos. These considerations prompt us to approach the ZS-TAL problem\nfrom a radically novel perspective, relaxing the requirement for training data.\nTo this aim, we introduce a novel method that performs Test-Time adaptation for\nTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained\nVision and Language Model (VLM). T3AL operates in three steps. First, a\nvideo-level pseudo-label of the action category is computed by aggregating\ninformation from the entire video. Then, action localization is performed\nadopting a novel procedure inspired by self-supervised learning. Finally,\nframe-level textual descriptions extracted with a state-of-the-art captioning\nmodel are employed for refining the action region proposals. We validate the\neffectiveness of T3AL by conducting experiments on the THUMOS14 and the\nActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly\noutperforms zero-shot baselines based on state-of-the-art VLMs, confirming the\nbenefit of a test-time adaptation approach.\n", "link": "http://arxiv.org/abs/2404.05426v1", "date": "2024-04-08", "relevancy": 2.336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5919}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5828}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5674}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Zero-Shot%20Temporal%20Action%20Localization&body=Title%3A%20Test-Time%20Zero-Shot%20Temporal%20Action%20Localization%0AAuthor%3A%20Benedetta%20Liberatori%20and%20Alessandro%20Conti%20and%20Paolo%20Rota%20and%20Yiming%20Wang%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Zero-Shot%20Temporal%20Action%20Localization%20%28ZS-TAL%29%20seeks%20to%20identify%20and%20locate%0Aactions%20in%20untrimmed%20videos%20unseen%20during%20training.%20Existing%20ZS-TAL%20methods%0Ainvolve%20fine-tuning%20a%20model%20on%20a%20large%20amount%20of%20annotated%20training%20data.%20While%0Aeffective%2C%20training-based%20ZS-TAL%20approaches%20assume%20the%20availability%20of%20labeled%0Adata%20for%20supervised%20learning%2C%20which%20can%20be%20impractical%20in%20some%20applications.%0AFurthermore%2C%20the%20training%20process%20naturally%20induces%20a%20domain%20bias%20into%20the%0Alearned%20model%2C%20which%20may%20adversely%20affect%20the%20model%27s%20generalization%20ability%20to%0Aarbitrary%20videos.%20These%20considerations%20prompt%20us%20to%20approach%20the%20ZS-TAL%20problem%0Afrom%20a%20radically%20novel%20perspective%2C%20relaxing%20the%20requirement%20for%20training%20data.%0ATo%20this%20aim%2C%20we%20introduce%20a%20novel%20method%20that%20performs%20Test-Time%20adaptation%20for%0ATemporal%20Action%20Localization%20%28T3AL%29.%20In%20a%20nutshell%2C%20T3AL%20adapts%20a%20pre-trained%0AVision%20and%20Language%20Model%20%28VLM%29.%20T3AL%20operates%20in%20three%20steps.%20First%2C%20a%0Avideo-level%20pseudo-label%20of%20the%20action%20category%20is%20computed%20by%20aggregating%0Ainformation%20from%20the%20entire%20video.%20Then%2C%20action%20localization%20is%20performed%0Aadopting%20a%20novel%20procedure%20inspired%20by%20self-supervised%20learning.%20Finally%2C%0Aframe-level%20textual%20descriptions%20extracted%20with%20a%20state-of-the-art%20captioning%0Amodel%20are%20employed%20for%20refining%20the%20action%20region%20proposals.%20We%20validate%20the%0Aeffectiveness%20of%20T3AL%20by%20conducting%20experiments%20on%20the%20THUMOS14%20and%20the%0AActivityNet-v1.3%20datasets.%20Our%20results%20demonstrate%20that%20T3AL%20significantly%0Aoutperforms%20zero-shot%20baselines%20based%20on%20state-of-the-art%20VLMs%2C%20confirming%20the%0Abenefit%20of%20a%20test-time%20adaptation%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05426v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Zero-Shot%20Temporal%20Action%20Localization&entry.906535625=Benedetta%20Liberatori%20and%20Alessandro%20Conti%20and%20Paolo%20Rota%20and%20Yiming%20Wang%20and%20Elisa%20Ricci&entry.1292438233=%20%20Zero-Shot%20Temporal%20Action%20Localization%20%28ZS-TAL%29%20seeks%20to%20identify%20and%20locate%0Aactions%20in%20untrimmed%20videos%20unseen%20during%20training.%20Existing%20ZS-TAL%20methods%0Ainvolve%20fine-tuning%20a%20model%20on%20a%20large%20amount%20of%20annotated%20training%20data.%20While%0Aeffective%2C%20training-based%20ZS-TAL%20approaches%20assume%20the%20availability%20of%20labeled%0Adata%20for%20supervised%20learning%2C%20which%20can%20be%20impractical%20in%20some%20applications.%0AFurthermore%2C%20the%20training%20process%20naturally%20induces%20a%20domain%20bias%20into%20the%0Alearned%20model%2C%20which%20may%20adversely%20affect%20the%20model%27s%20generalization%20ability%20to%0Aarbitrary%20videos.%20These%20considerations%20prompt%20us%20to%20approach%20the%20ZS-TAL%20problem%0Afrom%20a%20radically%20novel%20perspective%2C%20relaxing%20the%20requirement%20for%20training%20data.%0ATo%20this%20aim%2C%20we%20introduce%20a%20novel%20method%20that%20performs%20Test-Time%20adaptation%20for%0ATemporal%20Action%20Localization%20%28T3AL%29.%20In%20a%20nutshell%2C%20T3AL%20adapts%20a%20pre-trained%0AVision%20and%20Language%20Model%20%28VLM%29.%20T3AL%20operates%20in%20three%20steps.%20First%2C%20a%0Avideo-level%20pseudo-label%20of%20the%20action%20category%20is%20computed%20by%20aggregating%0Ainformation%20from%20the%20entire%20video.%20Then%2C%20action%20localization%20is%20performed%0Aadopting%20a%20novel%20procedure%20inspired%20by%20self-supervised%20learning.%20Finally%2C%0Aframe-level%20textual%20descriptions%20extracted%20with%20a%20state-of-the-art%20captioning%0Amodel%20are%20employed%20for%20refining%20the%20action%20region%20proposals.%20We%20validate%20the%0Aeffectiveness%20of%20T3AL%20by%20conducting%20experiments%20on%20the%20THUMOS14%20and%20the%0AActivityNet-v1.3%20datasets.%20Our%20results%20demonstrate%20that%20T3AL%20significantly%0Aoutperforms%20zero-shot%20baselines%20based%20on%20state-of-the-art%20VLMs%2C%20confirming%20the%0Abenefit%20of%20a%20test-time%20adaptation%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05426v1&entry.124074799=Read"},
{"title": "Unifying Foundation Models with Quadrotor Control for Visual Tracking\n  Beyond Object Categories", "author": "Alessandro Saviolo and Pratyaksh Rao and Vivek Radhakrishnan and Jiuhong Xiao and Giuseppe Loianno", "abstract": "  Visual control enables quadrotors to adaptively navigate using real-time\nsensory data, bridging perception with action. Yet, challenges persist,\nincluding generalization across scenarios, maintaining reliability, and\nensuring real-time responsiveness. This paper introduces a perception framework\ngrounded in foundation models for universal object detection and tracking,\nmoving beyond specific training categories. Integral to our approach is a\nmulti-layered tracker integrated with the foundation detector, ensuring\ncontinuous target visibility, even when faced with motion blur, abrupt light\nshifts, and occlusions. Complementing this, we introduce a model-free\ncontroller tailored for resilient quadrotor visual tracking. Our system\noperates efficiently on limited hardware, relying solely on an onboard camera\nand an inertial measurement unit. Through extensive validation in diverse\nchallenging indoor and outdoor environments, we demonstrate our system's\neffectiveness and adaptability. In conclusion, our research represents a step\nforward in quadrotor visual tracking, moving from task-specific methods to more\nversatile and adaptable operations.\n", "link": "http://arxiv.org/abs/2310.04781v3", "date": "2024-04-08", "relevancy": 2.3069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5877}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5797}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5694}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unifying%20Foundation%20Models%20with%20Quadrotor%20Control%20for%20Visual%20Tracking%0A%20%20Beyond%20Object%20Categories&body=Title%3A%20Unifying%20Foundation%20Models%20with%20Quadrotor%20Control%20for%20Visual%20Tracking%0A%20%20Beyond%20Object%20Categories%0AAuthor%3A%20Alessandro%20Saviolo%20and%20Pratyaksh%20Rao%20and%20Vivek%20Radhakrishnan%20and%20Jiuhong%20Xiao%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Visual%20control%20enables%20quadrotors%20to%20adaptively%20navigate%20using%20real-time%0Asensory%20data%2C%20bridging%20perception%20with%20action.%20Yet%2C%20challenges%20persist%2C%0Aincluding%20generalization%20across%20scenarios%2C%20maintaining%20reliability%2C%20and%0Aensuring%20real-time%20responsiveness.%20This%20paper%20introduces%20a%20perception%20framework%0Agrounded%20in%20foundation%20models%20for%20universal%20object%20detection%20and%20tracking%2C%0Amoving%20beyond%20specific%20training%20categories.%20Integral%20to%20our%20approach%20is%20a%0Amulti-layered%20tracker%20integrated%20with%20the%20foundation%20detector%2C%20ensuring%0Acontinuous%20target%20visibility%2C%20even%20when%20faced%20with%20motion%20blur%2C%20abrupt%20light%0Ashifts%2C%20and%20occlusions.%20Complementing%20this%2C%20we%20introduce%20a%20model-free%0Acontroller%20tailored%20for%20resilient%20quadrotor%20visual%20tracking.%20Our%20system%0Aoperates%20efficiently%20on%20limited%20hardware%2C%20relying%20solely%20on%20an%20onboard%20camera%0Aand%20an%20inertial%20measurement%20unit.%20Through%20extensive%20validation%20in%20diverse%0Achallenging%20indoor%20and%20outdoor%20environments%2C%20we%20demonstrate%20our%20system%27s%0Aeffectiveness%20and%20adaptability.%20In%20conclusion%2C%20our%20research%20represents%20a%20step%0Aforward%20in%20quadrotor%20visual%20tracking%2C%20moving%20from%20task-specific%20methods%20to%20more%0Aversatile%20and%20adaptable%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04781v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Foundation%20Models%20with%20Quadrotor%20Control%20for%20Visual%20Tracking%0A%20%20Beyond%20Object%20Categories&entry.906535625=Alessandro%20Saviolo%20and%20Pratyaksh%20Rao%20and%20Vivek%20Radhakrishnan%20and%20Jiuhong%20Xiao%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Visual%20control%20enables%20quadrotors%20to%20adaptively%20navigate%20using%20real-time%0Asensory%20data%2C%20bridging%20perception%20with%20action.%20Yet%2C%20challenges%20persist%2C%0Aincluding%20generalization%20across%20scenarios%2C%20maintaining%20reliability%2C%20and%0Aensuring%20real-time%20responsiveness.%20This%20paper%20introduces%20a%20perception%20framework%0Agrounded%20in%20foundation%20models%20for%20universal%20object%20detection%20and%20tracking%2C%0Amoving%20beyond%20specific%20training%20categories.%20Integral%20to%20our%20approach%20is%20a%0Amulti-layered%20tracker%20integrated%20with%20the%20foundation%20detector%2C%20ensuring%0Acontinuous%20target%20visibility%2C%20even%20when%20faced%20with%20motion%20blur%2C%20abrupt%20light%0Ashifts%2C%20and%20occlusions.%20Complementing%20this%2C%20we%20introduce%20a%20model-free%0Acontroller%20tailored%20for%20resilient%20quadrotor%20visual%20tracking.%20Our%20system%0Aoperates%20efficiently%20on%20limited%20hardware%2C%20relying%20solely%20on%20an%20onboard%20camera%0Aand%20an%20inertial%20measurement%20unit.%20Through%20extensive%20validation%20in%20diverse%0Achallenging%20indoor%20and%20outdoor%20environments%2C%20we%20demonstrate%20our%20system%27s%0Aeffectiveness%20and%20adaptability.%20In%20conclusion%2C%20our%20research%20represents%20a%20step%0Aforward%20in%20quadrotor%20visual%20tracking%2C%20moving%20from%20task-specific%20methods%20to%20more%0Aversatile%20and%20adaptable%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04781v3&entry.124074799=Read"},
{"title": "TIM: A Time Interval Machine for Audio-Visual Action Recognition", "author": "Jacob Chalk and Jaesung Huh and Evangelos Kazakos and Andrew Zisserman and Dima Damen", "abstract": "  Diverse actions give rise to rich audio-visual signals in long videos. Recent\nworks showcase that the two modalities of audio and video exhibit different\ntemporal extents of events and distinct labels. We address the interplay\nbetween the two modalities in long videos by explicitly modelling the temporal\nextents of audio and visual events. We propose the Time Interval Machine (TIM)\nwhere a modality-specific time interval poses as a query to a transformer\nencoder that ingests a long video input. The encoder then attends to the\nspecified interval, as well as the surrounding context in both modalities, in\norder to recognise the ongoing action.\n  We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,\nPerception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. On\nEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly\nlarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, we\nshow that TIM can be adapted for action detection, using dense multi-scale\ninterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and\nshowing strong performance on the Perception Test. Our ablations show the\ncritical role of integrating the two modalities and modelling their time\nintervals in achieving this performance. Code and models at:\nhttps://github.com/JacobChalk/TIM\n", "link": "http://arxiv.org/abs/2404.05559v1", "date": "2024-04-08", "relevancy": 2.2942, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5871}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5709}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5462}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TIM%3A%20A%20Time%20Interval%20Machine%20for%20Audio-Visual%20Action%20Recognition&body=Title%3A%20TIM%3A%20A%20Time%20Interval%20Machine%20for%20Audio-Visual%20Action%20Recognition%0AAuthor%3A%20Jacob%20Chalk%20and%20Jaesung%20Huh%20and%20Evangelos%20Kazakos%20and%20Andrew%20Zisserman%20and%20Dima%20Damen%0AAbstract%3A%20%20%20Diverse%20actions%20give%20rise%20to%20rich%20audio-visual%20signals%20in%20long%20videos.%20Recent%0Aworks%20showcase%20that%20the%20two%20modalities%20of%20audio%20and%20video%20exhibit%20different%0Atemporal%20extents%20of%20events%20and%20distinct%20labels.%20We%20address%20the%20interplay%0Abetween%20the%20two%20modalities%20in%20long%20videos%20by%20explicitly%20modelling%20the%20temporal%0Aextents%20of%20audio%20and%20visual%20events.%20We%20propose%20the%20Time%20Interval%20Machine%20%28TIM%29%0Awhere%20a%20modality-specific%20time%20interval%20poses%20as%20a%20query%20to%20a%20transformer%0Aencoder%20that%20ingests%20a%20long%20video%20input.%20The%20encoder%20then%20attends%20to%20the%0Aspecified%20interval%2C%20as%20well%20as%20the%20surrounding%20context%20in%20both%20modalities%2C%20in%0Aorder%20to%20recognise%20the%20ongoing%20action.%0A%20%20We%20test%20TIM%20on%20three%20long%20audio-visual%20video%20datasets%3A%20EPIC-KITCHENS%2C%0APerception%20Test%2C%20and%20AVE%2C%20reporting%20state-of-the-art%20%28SOTA%29%20for%20recognition.%20On%0AEPIC-KITCHENS%2C%20we%20beat%20previous%20SOTA%20that%20utilises%20LLMs%20and%20significantly%0Alarger%20pre-training%20by%202.9%25%20top-1%20action%20recognition%20accuracy.%20Additionally%2C%20we%0Ashow%20that%20TIM%20can%20be%20adapted%20for%20action%20detection%2C%20using%20dense%20multi-scale%0Ainterval%20queries%2C%20outperforming%20SOTA%20on%20EPIC-KITCHENS-100%20for%20most%20metrics%2C%20and%0Ashowing%20strong%20performance%20on%20the%20Perception%20Test.%20Our%20ablations%20show%20the%0Acritical%20role%20of%20integrating%20the%20two%20modalities%20and%20modelling%20their%20time%0Aintervals%20in%20achieving%20this%20performance.%20Code%20and%20models%20at%3A%0Ahttps%3A//github.com/JacobChalk/TIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05559v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIM%3A%20A%20Time%20Interval%20Machine%20for%20Audio-Visual%20Action%20Recognition&entry.906535625=Jacob%20Chalk%20and%20Jaesung%20Huh%20and%20Evangelos%20Kazakos%20and%20Andrew%20Zisserman%20and%20Dima%20Damen&entry.1292438233=%20%20Diverse%20actions%20give%20rise%20to%20rich%20audio-visual%20signals%20in%20long%20videos.%20Recent%0Aworks%20showcase%20that%20the%20two%20modalities%20of%20audio%20and%20video%20exhibit%20different%0Atemporal%20extents%20of%20events%20and%20distinct%20labels.%20We%20address%20the%20interplay%0Abetween%20the%20two%20modalities%20in%20long%20videos%20by%20explicitly%20modelling%20the%20temporal%0Aextents%20of%20audio%20and%20visual%20events.%20We%20propose%20the%20Time%20Interval%20Machine%20%28TIM%29%0Awhere%20a%20modality-specific%20time%20interval%20poses%20as%20a%20query%20to%20a%20transformer%0Aencoder%20that%20ingests%20a%20long%20video%20input.%20The%20encoder%20then%20attends%20to%20the%0Aspecified%20interval%2C%20as%20well%20as%20the%20surrounding%20context%20in%20both%20modalities%2C%20in%0Aorder%20to%20recognise%20the%20ongoing%20action.%0A%20%20We%20test%20TIM%20on%20three%20long%20audio-visual%20video%20datasets%3A%20EPIC-KITCHENS%2C%0APerception%20Test%2C%20and%20AVE%2C%20reporting%20state-of-the-art%20%28SOTA%29%20for%20recognition.%20On%0AEPIC-KITCHENS%2C%20we%20beat%20previous%20SOTA%20that%20utilises%20LLMs%20and%20significantly%0Alarger%20pre-training%20by%202.9%25%20top-1%20action%20recognition%20accuracy.%20Additionally%2C%20we%0Ashow%20that%20TIM%20can%20be%20adapted%20for%20action%20detection%2C%20using%20dense%20multi-scale%0Ainterval%20queries%2C%20outperforming%20SOTA%20on%20EPIC-KITCHENS-100%20for%20most%20metrics%2C%20and%0Ashowing%20strong%20performance%20on%20the%20Perception%20Test.%20Our%20ablations%20show%20the%0Acritical%20role%20of%20integrating%20the%20two%20modalities%20and%20modelling%20their%20time%0Aintervals%20in%20achieving%20this%20performance.%20Code%20and%20models%20at%3A%0Ahttps%3A//github.com/JacobChalk/TIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05559v1&entry.124074799=Read"},
{"title": "Taming Transformers for Realistic Lidar Point Cloud Generation", "author": "Hamed Haghighi and Amir Samadi and Mehrdad Dianati and Valentina Donzella and Kurt Debattista", "abstract": "  Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the\nLidar point cloud generation task, benefiting from their stable training and\niterative refinement during sampling. However, DMs often fail to realistically\nmodel Lidar raydrop noise due to their inherent denoising process. To retain\nthe strength of iterative sampling while enhancing the generation of raydrop\nnoise, we introduce LidarGRIT, a generative model that uses auto-regressive\ntransformers to iteratively sample the range images in the latent space rather\nthan image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode\nrange images and raydrop masks. Our results show that LidarGRIT achieves\nsuperior performance compared to SOTA models on KITTI-360 and KITTI odometry\ndatasets. Code available at:https://github.com/hamedhaghighi/LidarGRIT.\n", "link": "http://arxiv.org/abs/2404.05505v1", "date": "2024-04-08", "relevancy": 2.2937, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6245}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5694}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.557}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Taming%20Transformers%20for%20Realistic%20Lidar%20Point%20Cloud%20Generation&body=Title%3A%20Taming%20Transformers%20for%20Realistic%20Lidar%20Point%20Cloud%20Generation%0AAuthor%3A%20Hamed%20Haghighi%20and%20Amir%20Samadi%20and%20Mehrdad%20Dianati%20and%20Valentina%20Donzella%20and%20Kurt%20Debattista%0AAbstract%3A%20%20%20Diffusion%20Models%20%28DMs%29%20have%20achieved%20State-Of-The-Art%20%28SOTA%29%20results%20in%20the%0ALidar%20point%20cloud%20generation%20task%2C%20benefiting%20from%20their%20stable%20training%20and%0Aiterative%20refinement%20during%20sampling.%20However%2C%20DMs%20often%20fail%20to%20realistically%0Amodel%20Lidar%20raydrop%20noise%20due%20to%20their%20inherent%20denoising%20process.%20To%20retain%0Athe%20strength%20of%20iterative%20sampling%20while%20enhancing%20the%20generation%20of%20raydrop%0Anoise%2C%20we%20introduce%20LidarGRIT%2C%20a%20generative%20model%20that%20uses%20auto-regressive%0Atransformers%20to%20iteratively%20sample%20the%20range%20images%20in%20the%20latent%20space%20rather%0Athan%20image%20space.%20Furthermore%2C%20LidarGRIT%20utilises%20VQ-VAE%20to%20separately%20decode%0Arange%20images%20and%20raydrop%20masks.%20Our%20results%20show%20that%20LidarGRIT%20achieves%0Asuperior%20performance%20compared%20to%20SOTA%20models%20on%20KITTI-360%20and%20KITTI%20odometry%0Adatasets.%20Code%20available%20at%3Ahttps%3A//github.com/hamedhaghighi/LidarGRIT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05505v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Transformers%20for%20Realistic%20Lidar%20Point%20Cloud%20Generation&entry.906535625=Hamed%20Haghighi%20and%20Amir%20Samadi%20and%20Mehrdad%20Dianati%20and%20Valentina%20Donzella%20and%20Kurt%20Debattista&entry.1292438233=%20%20Diffusion%20Models%20%28DMs%29%20have%20achieved%20State-Of-The-Art%20%28SOTA%29%20results%20in%20the%0ALidar%20point%20cloud%20generation%20task%2C%20benefiting%20from%20their%20stable%20training%20and%0Aiterative%20refinement%20during%20sampling.%20However%2C%20DMs%20often%20fail%20to%20realistically%0Amodel%20Lidar%20raydrop%20noise%20due%20to%20their%20inherent%20denoising%20process.%20To%20retain%0Athe%20strength%20of%20iterative%20sampling%20while%20enhancing%20the%20generation%20of%20raydrop%0Anoise%2C%20we%20introduce%20LidarGRIT%2C%20a%20generative%20model%20that%20uses%20auto-regressive%0Atransformers%20to%20iteratively%20sample%20the%20range%20images%20in%20the%20latent%20space%20rather%0Athan%20image%20space.%20Furthermore%2C%20LidarGRIT%20utilises%20VQ-VAE%20to%20separately%20decode%0Arange%20images%20and%20raydrop%20masks.%20Our%20results%20show%20that%20LidarGRIT%20achieves%0Asuperior%20performance%20compared%20to%20SOTA%20models%20on%20KITTI-360%20and%20KITTI%20odometry%0Adatasets.%20Code%20available%20at%3Ahttps%3A//github.com/hamedhaghighi/LidarGRIT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05505v1&entry.124074799=Read"},
{"title": "NAF-DPM: A Nonlinear Activation-Free Diffusion Probabilistic Model for\n  Document Enhancement", "author": "Giordano Cicchetti and Danilo Comminiello", "abstract": "  Real-world documents may suffer various forms of degradation, often resulting\nin lower accuracy in optical character recognition (OCR) systems. Therefore, a\ncrucial preprocessing step is essential to eliminate noise while preserving\ntext and key features of documents. In this paper, we propose NAF-DPM, a novel\ngenerative framework based on a diffusion probabilistic model (DPM) designed to\nrestore the original quality of degraded documents. While DPMs are recognized\nfor their high-quality generated images, they are also known for their large\ninference time. To mitigate this problem we provide the DPM with an efficient\nnonlinear activation-free (NAF) network and we employ as a sampler a fast\nsolver of ordinary differential equations, which can converge in a few\niterations. To better preserve text characters, we introduce an additional\ndifferentiable module based on convolutional recurrent neural networks,\nsimulating the behavior of an OCR system during training. Experiments conducted\non various datasets showcase the superiority of our approach, achieving\nstate-of-the-art performance in terms of pixel-level and perceptual similarity\nmetrics. Furthermore, the results demonstrate a notable character error\nreduction made by OCR systems when transcribing real-world document images\nenhanced by our framework. Code and pre-trained models are available at\nhttps://github.com/ispamm/NAF-DPM.\n", "link": "http://arxiv.org/abs/2404.05669v1", "date": "2024-04-08", "relevancy": 2.2875, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5828}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NAF-DPM%3A%20A%20Nonlinear%20Activation-Free%20Diffusion%20Probabilistic%20Model%20for%0A%20%20Document%20Enhancement&body=Title%3A%20NAF-DPM%3A%20A%20Nonlinear%20Activation-Free%20Diffusion%20Probabilistic%20Model%20for%0A%20%20Document%20Enhancement%0AAuthor%3A%20Giordano%20Cicchetti%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Real-world%20documents%20may%20suffer%20various%20forms%20of%20degradation%2C%20often%20resulting%0Ain%20lower%20accuracy%20in%20optical%20character%20recognition%20%28OCR%29%20systems.%20Therefore%2C%20a%0Acrucial%20preprocessing%20step%20is%20essential%20to%20eliminate%20noise%20while%20preserving%0Atext%20and%20key%20features%20of%20documents.%20In%20this%20paper%2C%20we%20propose%20NAF-DPM%2C%20a%20novel%0Agenerative%20framework%20based%20on%20a%20diffusion%20probabilistic%20model%20%28DPM%29%20designed%20to%0Arestore%20the%20original%20quality%20of%20degraded%20documents.%20While%20DPMs%20are%20recognized%0Afor%20their%20high-quality%20generated%20images%2C%20they%20are%20also%20known%20for%20their%20large%0Ainference%20time.%20To%20mitigate%20this%20problem%20we%20provide%20the%20DPM%20with%20an%20efficient%0Anonlinear%20activation-free%20%28NAF%29%20network%20and%20we%20employ%20as%20a%20sampler%20a%20fast%0Asolver%20of%20ordinary%20differential%20equations%2C%20which%20can%20converge%20in%20a%20few%0Aiterations.%20To%20better%20preserve%20text%20characters%2C%20we%20introduce%20an%20additional%0Adifferentiable%20module%20based%20on%20convolutional%20recurrent%20neural%20networks%2C%0Asimulating%20the%20behavior%20of%20an%20OCR%20system%20during%20training.%20Experiments%20conducted%0Aon%20various%20datasets%20showcase%20the%20superiority%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20in%20terms%20of%20pixel-level%20and%20perceptual%20similarity%0Ametrics.%20Furthermore%2C%20the%20results%20demonstrate%20a%20notable%20character%20error%0Areduction%20made%20by%20OCR%20systems%20when%20transcribing%20real-world%20document%20images%0Aenhanced%20by%20our%20framework.%20Code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/ispamm/NAF-DPM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05669v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAF-DPM%3A%20A%20Nonlinear%20Activation-Free%20Diffusion%20Probabilistic%20Model%20for%0A%20%20Document%20Enhancement&entry.906535625=Giordano%20Cicchetti%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Real-world%20documents%20may%20suffer%20various%20forms%20of%20degradation%2C%20often%20resulting%0Ain%20lower%20accuracy%20in%20optical%20character%20recognition%20%28OCR%29%20systems.%20Therefore%2C%20a%0Acrucial%20preprocessing%20step%20is%20essential%20to%20eliminate%20noise%20while%20preserving%0Atext%20and%20key%20features%20of%20documents.%20In%20this%20paper%2C%20we%20propose%20NAF-DPM%2C%20a%20novel%0Agenerative%20framework%20based%20on%20a%20diffusion%20probabilistic%20model%20%28DPM%29%20designed%20to%0Arestore%20the%20original%20quality%20of%20degraded%20documents.%20While%20DPMs%20are%20recognized%0Afor%20their%20high-quality%20generated%20images%2C%20they%20are%20also%20known%20for%20their%20large%0Ainference%20time.%20To%20mitigate%20this%20problem%20we%20provide%20the%20DPM%20with%20an%20efficient%0Anonlinear%20activation-free%20%28NAF%29%20network%20and%20we%20employ%20as%20a%20sampler%20a%20fast%0Asolver%20of%20ordinary%20differential%20equations%2C%20which%20can%20converge%20in%20a%20few%0Aiterations.%20To%20better%20preserve%20text%20characters%2C%20we%20introduce%20an%20additional%0Adifferentiable%20module%20based%20on%20convolutional%20recurrent%20neural%20networks%2C%0Asimulating%20the%20behavior%20of%20an%20OCR%20system%20during%20training.%20Experiments%20conducted%0Aon%20various%20datasets%20showcase%20the%20superiority%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20in%20terms%20of%20pixel-level%20and%20perceptual%20similarity%0Ametrics.%20Furthermore%2C%20the%20results%20demonstrate%20a%20notable%20character%20error%0Areduction%20made%20by%20OCR%20systems%20when%20transcribing%20real-world%20document%20images%0Aenhanced%20by%20our%20framework.%20Code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/ispamm/NAF-DPM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05669v1&entry.124074799=Read"},
{"title": "Towards More General Video-based Deepfake Detection through Facial\n  Feature Guided Adaptation for Foundation Model", "author": "Yue-Hua Han and Tai-Ming Huang and Shu-Tzu Lo and Po-Han Huang and Kai-Lung Hua and Jun-Cheng Chen", "abstract": "  With the rise of deep learning, generative models have enabled the creation\nof highly realistic synthetic images, presenting challenges due to their\npotential misuse. While research in Deepfake detection has grown rapidly in\nresponse, many detection methods struggle with unseen Deepfakes generated by\nnew synthesis techniques. To address this generalisation challenge, we propose\na novel Deepfake detection approach by adapting rich information encoded inside\nthe Foundation Models with rich information encoded inside, specifically using\nthe image encoder from CLIP which has demonstrated strong zero-shot capability\nfor downstream tasks. Inspired by the recent advances of parameter efficient\nfine-tuning, we propose a novel side-network-based decoder to extract spatial\nand temporal cues from the given video clip, with the promotion of the Facial\nComponent Guidance (FCG) to guidencourage the spatial feature to include\nfeatures of key facial parts for more robust and general Deepfake detection.\nThrough extensive cross-dataset evaluations, our approach exhibits superior\neffectiveness in identifying unseen Deepfake samples, achieving notable\nperformance improvementsuccess even with limited training samples and\nmanipulation types. Our model secures an average performance enhancement of\n0.9% AUROC in cross-dataset assessments comparing with state-of-the-art\nmethods, especiallytablishing a significant lead of achieving 4.4% improvement\non the challenging DFDC dataset.\n", "link": "http://arxiv.org/abs/2404.05583v1", "date": "2024-04-08", "relevancy": 2.2787, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5867}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5679}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5533}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20More%20General%20Video-based%20Deepfake%20Detection%20through%20Facial%0A%20%20Feature%20Guided%20Adaptation%20for%20Foundation%20Model&body=Title%3A%20Towards%20More%20General%20Video-based%20Deepfake%20Detection%20through%20Facial%0A%20%20Feature%20Guided%20Adaptation%20for%20Foundation%20Model%0AAuthor%3A%20Yue-Hua%20Han%20and%20Tai-Ming%20Huang%20and%20Shu-Tzu%20Lo%20and%20Po-Han%20Huang%20and%20Kai-Lung%20Hua%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20With%20the%20rise%20of%20deep%20learning%2C%20generative%20models%20have%20enabled%20the%20creation%0Aof%20highly%20realistic%20synthetic%20images%2C%20presenting%20challenges%20due%20to%20their%0Apotential%20misuse.%20While%20research%20in%20Deepfake%20detection%20has%20grown%20rapidly%20in%0Aresponse%2C%20many%20detection%20methods%20struggle%20with%20unseen%20Deepfakes%20generated%20by%0Anew%20synthesis%20techniques.%20To%20address%20this%20generalisation%20challenge%2C%20we%20propose%0Aa%20novel%20Deepfake%20detection%20approach%20by%20adapting%20rich%20information%20encoded%20inside%0Athe%20Foundation%20Models%20with%20rich%20information%20encoded%20inside%2C%20specifically%20using%0Athe%20image%20encoder%20from%20CLIP%20which%20has%20demonstrated%20strong%20zero-shot%20capability%0Afor%20downstream%20tasks.%20Inspired%20by%20the%20recent%20advances%20of%20parameter%20efficient%0Afine-tuning%2C%20we%20propose%20a%20novel%20side-network-based%20decoder%20to%20extract%20spatial%0Aand%20temporal%20cues%20from%20the%20given%20video%20clip%2C%20with%20the%20promotion%20of%20the%20Facial%0AComponent%20Guidance%20%28FCG%29%20to%20guidencourage%20the%20spatial%20feature%20to%20include%0Afeatures%20of%20key%20facial%20parts%20for%20more%20robust%20and%20general%20Deepfake%20detection.%0AThrough%20extensive%20cross-dataset%20evaluations%2C%20our%20approach%20exhibits%20superior%0Aeffectiveness%20in%20identifying%20unseen%20Deepfake%20samples%2C%20achieving%20notable%0Aperformance%20improvementsuccess%20even%20with%20limited%20training%20samples%20and%0Amanipulation%20types.%20Our%20model%20secures%20an%20average%20performance%20enhancement%20of%0A0.9%25%20AUROC%20in%20cross-dataset%20assessments%20comparing%20with%20state-of-the-art%0Amethods%2C%20especiallytablishing%20a%20significant%20lead%20of%20achieving%204.4%25%20improvement%0Aon%20the%20challenging%20DFDC%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05583v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20More%20General%20Video-based%20Deepfake%20Detection%20through%20Facial%0A%20%20Feature%20Guided%20Adaptation%20for%20Foundation%20Model&entry.906535625=Yue-Hua%20Han%20and%20Tai-Ming%20Huang%20and%20Shu-Tzu%20Lo%20and%20Po-Han%20Huang%20and%20Kai-Lung%20Hua%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20With%20the%20rise%20of%20deep%20learning%2C%20generative%20models%20have%20enabled%20the%20creation%0Aof%20highly%20realistic%20synthetic%20images%2C%20presenting%20challenges%20due%20to%20their%0Apotential%20misuse.%20While%20research%20in%20Deepfake%20detection%20has%20grown%20rapidly%20in%0Aresponse%2C%20many%20detection%20methods%20struggle%20with%20unseen%20Deepfakes%20generated%20by%0Anew%20synthesis%20techniques.%20To%20address%20this%20generalisation%20challenge%2C%20we%20propose%0Aa%20novel%20Deepfake%20detection%20approach%20by%20adapting%20rich%20information%20encoded%20inside%0Athe%20Foundation%20Models%20with%20rich%20information%20encoded%20inside%2C%20specifically%20using%0Athe%20image%20encoder%20from%20CLIP%20which%20has%20demonstrated%20strong%20zero-shot%20capability%0Afor%20downstream%20tasks.%20Inspired%20by%20the%20recent%20advances%20of%20parameter%20efficient%0Afine-tuning%2C%20we%20propose%20a%20novel%20side-network-based%20decoder%20to%20extract%20spatial%0Aand%20temporal%20cues%20from%20the%20given%20video%20clip%2C%20with%20the%20promotion%20of%20the%20Facial%0AComponent%20Guidance%20%28FCG%29%20to%20guidencourage%20the%20spatial%20feature%20to%20include%0Afeatures%20of%20key%20facial%20parts%20for%20more%20robust%20and%20general%20Deepfake%20detection.%0AThrough%20extensive%20cross-dataset%20evaluations%2C%20our%20approach%20exhibits%20superior%0Aeffectiveness%20in%20identifying%20unseen%20Deepfake%20samples%2C%20achieving%20notable%0Aperformance%20improvementsuccess%20even%20with%20limited%20training%20samples%20and%0Amanipulation%20types.%20Our%20model%20secures%20an%20average%20performance%20enhancement%20of%0A0.9%25%20AUROC%20in%20cross-dataset%20assessments%20comparing%20with%20state-of-the-art%0Amethods%2C%20especiallytablishing%20a%20significant%20lead%20of%20achieving%204.4%25%20improvement%0Aon%20the%20challenging%20DFDC%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05583v1&entry.124074799=Read"},
{"title": "Social-MAE: Social Masked Autoencoder for Multi-person Motion\n  Representation Learning", "author": "Mahsa Ehsanpour and Ian Reid and Hamid Rezatofighi", "abstract": "  For a complete comprehension of multi-person scenes, it is essential to go\nbeyond basic tasks like detection and tracking. Higher-level tasks, such as\nunderstanding the interactions and social activities among individuals, are\nalso crucial. Progress towards models that can fully understand scenes\ninvolving multiple people is hindered by a lack of sufficient annotated data\nfor such high-level tasks. To address this challenge, we introduce Social-MAE,\na simple yet effective transformer-based masked autoencoder framework for\nmulti-person human motion data. The framework uses masked modeling to pre-train\nthe encoder to reconstruct masked human joint trajectories, enabling it to\nlearn generalizable and data efficient representations of motion in human\ncrowded scenes. Social-MAE comprises a transformer as the MAE encoder and a\nlighter-weight transformer as the MAE decoder which operates on multi-person\njoints' trajectory in the frequency domain. After the reconstruction task, the\nMAE decoder is replaced with a task-specific decoder and the model is\nfine-tuned end-to-end for a variety of high-level social tasks. Our proposed\nmodel combined with our pre-training approach achieves the state-of-the-art\nresults on various high-level social tasks, including multi-person pose\nforecasting, social grouping, and social action understanding. These\nimprovements are demonstrated across four popular multi-person datasets\nencompassing both human 2D and 3D body pose.\n", "link": "http://arxiv.org/abs/2404.05578v1", "date": "2024-04-08", "relevancy": 2.2704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Social-MAE%3A%20Social%20Masked%20Autoencoder%20for%20Multi-person%20Motion%0A%20%20Representation%20Learning&body=Title%3A%20Social-MAE%3A%20Social%20Masked%20Autoencoder%20for%20Multi-person%20Motion%0A%20%20Representation%20Learning%0AAuthor%3A%20Mahsa%20Ehsanpour%20and%20Ian%20Reid%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20%20%20For%20a%20complete%20comprehension%20of%20multi-person%20scenes%2C%20it%20is%20essential%20to%20go%0Abeyond%20basic%20tasks%20like%20detection%20and%20tracking.%20Higher-level%20tasks%2C%20such%20as%0Aunderstanding%20the%20interactions%20and%20social%20activities%20among%20individuals%2C%20are%0Aalso%20crucial.%20Progress%20towards%20models%20that%20can%20fully%20understand%20scenes%0Ainvolving%20multiple%20people%20is%20hindered%20by%20a%20lack%20of%20sufficient%20annotated%20data%0Afor%20such%20high-level%20tasks.%20To%20address%20this%20challenge%2C%20we%20introduce%20Social-MAE%2C%0Aa%20simple%20yet%20effective%20transformer-based%20masked%20autoencoder%20framework%20for%0Amulti-person%20human%20motion%20data.%20The%20framework%20uses%20masked%20modeling%20to%20pre-train%0Athe%20encoder%20to%20reconstruct%20masked%20human%20joint%20trajectories%2C%20enabling%20it%20to%0Alearn%20generalizable%20and%20data%20efficient%20representations%20of%20motion%20in%20human%0Acrowded%20scenes.%20Social-MAE%20comprises%20a%20transformer%20as%20the%20MAE%20encoder%20and%20a%0Alighter-weight%20transformer%20as%20the%20MAE%20decoder%20which%20operates%20on%20multi-person%0Ajoints%27%20trajectory%20in%20the%20frequency%20domain.%20After%20the%20reconstruction%20task%2C%20the%0AMAE%20decoder%20is%20replaced%20with%20a%20task-specific%20decoder%20and%20the%20model%20is%0Afine-tuned%20end-to-end%20for%20a%20variety%20of%20high-level%20social%20tasks.%20Our%20proposed%0Amodel%20combined%20with%20our%20pre-training%20approach%20achieves%20the%20state-of-the-art%0Aresults%20on%20various%20high-level%20social%20tasks%2C%20including%20multi-person%20pose%0Aforecasting%2C%20social%20grouping%2C%20and%20social%20action%20understanding.%20These%0Aimprovements%20are%20demonstrated%20across%20four%20popular%20multi-person%20datasets%0Aencompassing%20both%20human%202D%20and%203D%20body%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05578v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social-MAE%3A%20Social%20Masked%20Autoencoder%20for%20Multi-person%20Motion%0A%20%20Representation%20Learning&entry.906535625=Mahsa%20Ehsanpour%20and%20Ian%20Reid%20and%20Hamid%20Rezatofighi&entry.1292438233=%20%20For%20a%20complete%20comprehension%20of%20multi-person%20scenes%2C%20it%20is%20essential%20to%20go%0Abeyond%20basic%20tasks%20like%20detection%20and%20tracking.%20Higher-level%20tasks%2C%20such%20as%0Aunderstanding%20the%20interactions%20and%20social%20activities%20among%20individuals%2C%20are%0Aalso%20crucial.%20Progress%20towards%20models%20that%20can%20fully%20understand%20scenes%0Ainvolving%20multiple%20people%20is%20hindered%20by%20a%20lack%20of%20sufficient%20annotated%20data%0Afor%20such%20high-level%20tasks.%20To%20address%20this%20challenge%2C%20we%20introduce%20Social-MAE%2C%0Aa%20simple%20yet%20effective%20transformer-based%20masked%20autoencoder%20framework%20for%0Amulti-person%20human%20motion%20data.%20The%20framework%20uses%20masked%20modeling%20to%20pre-train%0Athe%20encoder%20to%20reconstruct%20masked%20human%20joint%20trajectories%2C%20enabling%20it%20to%0Alearn%20generalizable%20and%20data%20efficient%20representations%20of%20motion%20in%20human%0Acrowded%20scenes.%20Social-MAE%20comprises%20a%20transformer%20as%20the%20MAE%20encoder%20and%20a%0Alighter-weight%20transformer%20as%20the%20MAE%20decoder%20which%20operates%20on%20multi-person%0Ajoints%27%20trajectory%20in%20the%20frequency%20domain.%20After%20the%20reconstruction%20task%2C%20the%0AMAE%20decoder%20is%20replaced%20with%20a%20task-specific%20decoder%20and%20the%20model%20is%0Afine-tuned%20end-to-end%20for%20a%20variety%20of%20high-level%20social%20tasks.%20Our%20proposed%0Amodel%20combined%20with%20our%20pre-training%20approach%20achieves%20the%20state-of-the-art%0Aresults%20on%20various%20high-level%20social%20tasks%2C%20including%20multi-person%20pose%0Aforecasting%2C%20social%20grouping%2C%20and%20social%20action%20understanding.%20These%0Aimprovements%20are%20demonstrated%20across%20four%20popular%20multi-person%20datasets%0Aencompassing%20both%20human%202D%20and%203D%20body%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05578v1&entry.124074799=Read"},
{"title": "Representing Noisy Image Without Denoising", "author": "Shuren Qi and Yushu Zhang and Chao Wang and Tao Xiang and Xiaochun Cao and Yong Xiang", "abstract": "  A long-standing topic in artificial intelligence is the effective recognition\nof patterns from noisy images. In this regard, the recent data-driven paradigm\nconsiders 1) improving the representation robustness by adding noisy samples in\ntraining phase (i.e., data augmentation) or 2) pre-processing the noisy image\nby learning to solve the inverse problem (i.e., image denoising). However, such\nmethods generally exhibit inefficient process and unstable result, limiting\ntheir practical applications. In this paper, we explore a non-learning paradigm\nthat aims to derive robust representation directly from noisy images, without\nthe denoising as pre-processing. Here, the noise-robust representation is\ndesigned as Fractional-order Moments in Radon space (FMR), with also beneficial\nproperties of orthogonality and rotation invariance. Unlike earlier\ninteger-order methods, our work is a more generic design taking such classical\nmethods as special cases, and the introduced fractional-order parameter offers\ntime-frequency analysis capability that is not available in classical methods.\nFormally, both implicit and explicit paths for constructing the FMR are\ndiscussed in detail. Extensive simulation experiments and an image security\napplication are provided to demonstrate the uniqueness and usefulness of our\nFMR, especially for noise robustness, rotation invariance, and time-frequency\ndiscriminability.\n", "link": "http://arxiv.org/abs/2301.07409v2", "date": "2024-04-08", "relevancy": 2.2582, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6237}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5212}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Representing%20Noisy%20Image%20Without%20Denoising&body=Title%3A%20Representing%20Noisy%20Image%20Without%20Denoising%0AAuthor%3A%20Shuren%20Qi%20and%20Yushu%20Zhang%20and%20Chao%20Wang%20and%20Tao%20Xiang%20and%20Xiaochun%20Cao%20and%20Yong%20Xiang%0AAbstract%3A%20%20%20A%20long-standing%20topic%20in%20artificial%20intelligence%20is%20the%20effective%20recognition%0Aof%20patterns%20from%20noisy%20images.%20In%20this%20regard%2C%20the%20recent%20data-driven%20paradigm%0Aconsiders%201%29%20improving%20the%20representation%20robustness%20by%20adding%20noisy%20samples%20in%0Atraining%20phase%20%28i.e.%2C%20data%20augmentation%29%20or%202%29%20pre-processing%20the%20noisy%20image%0Aby%20learning%20to%20solve%20the%20inverse%20problem%20%28i.e.%2C%20image%20denoising%29.%20However%2C%20such%0Amethods%20generally%20exhibit%20inefficient%20process%20and%20unstable%20result%2C%20limiting%0Atheir%20practical%20applications.%20In%20this%20paper%2C%20we%20explore%20a%20non-learning%20paradigm%0Athat%20aims%20to%20derive%20robust%20representation%20directly%20from%20noisy%20images%2C%20without%0Athe%20denoising%20as%20pre-processing.%20Here%2C%20the%20noise-robust%20representation%20is%0Adesigned%20as%20Fractional-order%20Moments%20in%20Radon%20space%20%28FMR%29%2C%20with%20also%20beneficial%0Aproperties%20of%20orthogonality%20and%20rotation%20invariance.%20Unlike%20earlier%0Ainteger-order%20methods%2C%20our%20work%20is%20a%20more%20generic%20design%20taking%20such%20classical%0Amethods%20as%20special%20cases%2C%20and%20the%20introduced%20fractional-order%20parameter%20offers%0Atime-frequency%20analysis%20capability%20that%20is%20not%20available%20in%20classical%20methods.%0AFormally%2C%20both%20implicit%20and%20explicit%20paths%20for%20constructing%20the%20FMR%20are%0Adiscussed%20in%20detail.%20Extensive%20simulation%20experiments%20and%20an%20image%20security%0Aapplication%20are%20provided%20to%20demonstrate%20the%20uniqueness%20and%20usefulness%20of%20our%0AFMR%2C%20especially%20for%20noise%20robustness%2C%20rotation%20invariance%2C%20and%20time-frequency%0Adiscriminability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.07409v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Noisy%20Image%20Without%20Denoising&entry.906535625=Shuren%20Qi%20and%20Yushu%20Zhang%20and%20Chao%20Wang%20and%20Tao%20Xiang%20and%20Xiaochun%20Cao%20and%20Yong%20Xiang&entry.1292438233=%20%20A%20long-standing%20topic%20in%20artificial%20intelligence%20is%20the%20effective%20recognition%0Aof%20patterns%20from%20noisy%20images.%20In%20this%20regard%2C%20the%20recent%20data-driven%20paradigm%0Aconsiders%201%29%20improving%20the%20representation%20robustness%20by%20adding%20noisy%20samples%20in%0Atraining%20phase%20%28i.e.%2C%20data%20augmentation%29%20or%202%29%20pre-processing%20the%20noisy%20image%0Aby%20learning%20to%20solve%20the%20inverse%20problem%20%28i.e.%2C%20image%20denoising%29.%20However%2C%20such%0Amethods%20generally%20exhibit%20inefficient%20process%20and%20unstable%20result%2C%20limiting%0Atheir%20practical%20applications.%20In%20this%20paper%2C%20we%20explore%20a%20non-learning%20paradigm%0Athat%20aims%20to%20derive%20robust%20representation%20directly%20from%20noisy%20images%2C%20without%0Athe%20denoising%20as%20pre-processing.%20Here%2C%20the%20noise-robust%20representation%20is%0Adesigned%20as%20Fractional-order%20Moments%20in%20Radon%20space%20%28FMR%29%2C%20with%20also%20beneficial%0Aproperties%20of%20orthogonality%20and%20rotation%20invariance.%20Unlike%20earlier%0Ainteger-order%20methods%2C%20our%20work%20is%20a%20more%20generic%20design%20taking%20such%20classical%0Amethods%20as%20special%20cases%2C%20and%20the%20introduced%20fractional-order%20parameter%20offers%0Atime-frequency%20analysis%20capability%20that%20is%20not%20available%20in%20classical%20methods.%0AFormally%2C%20both%20implicit%20and%20explicit%20paths%20for%20constructing%20the%20FMR%20are%0Adiscussed%20in%20detail.%20Extensive%20simulation%20experiments%20and%20an%20image%20security%0Aapplication%20are%20provided%20to%20demonstrate%20the%20uniqueness%20and%20usefulness%20of%20our%0AFMR%2C%20especially%20for%20noise%20robustness%2C%20rotation%20invariance%2C%20and%20time-frequency%0Adiscriminability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.07409v2&entry.124074799=Read"},
{"title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video\n  Understanding", "author": "Bo He and Hengduo Li and Young Kyun Jang and Menglin Jia and Xuefei Cao and Ashish Shah and Abhinav Shrivastava and Ser-Nam Lim", "abstract": "  With the success of large language models (LLMs), integrating the vision\nmodel into LLMs to build vision-language foundation models has gained much more\ninterest recently. However, existing LLM-based large multimodal models (e.g.,\nVideo-LLaMA, VideoChat) can only take in a limited number of frames for short\nvideo understanding. In this study, we mainly focus on designing an efficient\nand effective model for long-term video understanding. Instead of trying to\nprocess more frames simultaneously like most existing work, we propose to\nprocess videos in an online manner and store past video information in a memory\nbank. This allows our model to reference historical video content for long-term\nanalysis without exceeding LLMs' context length constraints or GPU memory\nlimits. Our memory bank can be seamlessly integrated into current multimodal\nLLMs in an off-the-shelf manner. We conduct extensive experiments on various\nvideo understanding tasks, such as long-video understanding, video question\nanswering, and video captioning, and our model can achieve state-of-the-art\nperformances across multiple datasets. Code available at\nhttps://boheumd.github.io/MA-LMM/.\n", "link": "http://arxiv.org/abs/2404.05726v1", "date": "2024-04-08", "relevancy": 2.2581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5655}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MA-LMM%3A%20Memory-Augmented%20Large%20Multimodal%20Model%20for%20Long-Term%20Video%0A%20%20Understanding&body=Title%3A%20MA-LMM%3A%20Memory-Augmented%20Large%20Multimodal%20Model%20for%20Long-Term%20Video%0A%20%20Understanding%0AAuthor%3A%20Bo%20He%20and%20Hengduo%20Li%20and%20Young%20Kyun%20Jang%20and%20Menglin%20Jia%20and%20Xuefei%20Cao%20and%20Ashish%20Shah%20and%20Abhinav%20Shrivastava%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20With%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20integrating%20the%20vision%0Amodel%20into%20LLMs%20to%20build%20vision-language%20foundation%20models%20has%20gained%20much%20more%0Ainterest%20recently.%20However%2C%20existing%20LLM-based%20large%20multimodal%20models%20%28e.g.%2C%0AVideo-LLaMA%2C%20VideoChat%29%20can%20only%20take%20in%20a%20limited%20number%20of%20frames%20for%20short%0Avideo%20understanding.%20In%20this%20study%2C%20we%20mainly%20focus%20on%20designing%20an%20efficient%0Aand%20effective%20model%20for%20long-term%20video%20understanding.%20Instead%20of%20trying%20to%0Aprocess%20more%20frames%20simultaneously%20like%20most%20existing%20work%2C%20we%20propose%20to%0Aprocess%20videos%20in%20an%20online%20manner%20and%20store%20past%20video%20information%20in%20a%20memory%0Abank.%20This%20allows%20our%20model%20to%20reference%20historical%20video%20content%20for%20long-term%0Aanalysis%20without%20exceeding%20LLMs%27%20context%20length%20constraints%20or%20GPU%20memory%0Alimits.%20Our%20memory%20bank%20can%20be%20seamlessly%20integrated%20into%20current%20multimodal%0ALLMs%20in%20an%20off-the-shelf%20manner.%20We%20conduct%20extensive%20experiments%20on%20various%0Avideo%20understanding%20tasks%2C%20such%20as%20long-video%20understanding%2C%20video%20question%0Aanswering%2C%20and%20video%20captioning%2C%20and%20our%20model%20can%20achieve%20state-of-the-art%0Aperformances%20across%20multiple%20datasets.%20Code%20available%20at%0Ahttps%3A//boheumd.github.io/MA-LMM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05726v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MA-LMM%3A%20Memory-Augmented%20Large%20Multimodal%20Model%20for%20Long-Term%20Video%0A%20%20Understanding&entry.906535625=Bo%20He%20and%20Hengduo%20Li%20and%20Young%20Kyun%20Jang%20and%20Menglin%20Jia%20and%20Xuefei%20Cao%20and%20Ashish%20Shah%20and%20Abhinav%20Shrivastava%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20With%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20integrating%20the%20vision%0Amodel%20into%20LLMs%20to%20build%20vision-language%20foundation%20models%20has%20gained%20much%20more%0Ainterest%20recently.%20However%2C%20existing%20LLM-based%20large%20multimodal%20models%20%28e.g.%2C%0AVideo-LLaMA%2C%20VideoChat%29%20can%20only%20take%20in%20a%20limited%20number%20of%20frames%20for%20short%0Avideo%20understanding.%20In%20this%20study%2C%20we%20mainly%20focus%20on%20designing%20an%20efficient%0Aand%20effective%20model%20for%20long-term%20video%20understanding.%20Instead%20of%20trying%20to%0Aprocess%20more%20frames%20simultaneously%20like%20most%20existing%20work%2C%20we%20propose%20to%0Aprocess%20videos%20in%20an%20online%20manner%20and%20store%20past%20video%20information%20in%20a%20memory%0Abank.%20This%20allows%20our%20model%20to%20reference%20historical%20video%20content%20for%20long-term%0Aanalysis%20without%20exceeding%20LLMs%27%20context%20length%20constraints%20or%20GPU%20memory%0Alimits.%20Our%20memory%20bank%20can%20be%20seamlessly%20integrated%20into%20current%20multimodal%0ALLMs%20in%20an%20off-the-shelf%20manner.%20We%20conduct%20extensive%20experiments%20on%20various%0Avideo%20understanding%20tasks%2C%20such%20as%20long-video%20understanding%2C%20video%20question%0Aanswering%2C%20and%20video%20captioning%2C%20and%20our%20model%20can%20achieve%20state-of-the-art%0Aperformances%20across%20multiple%20datasets.%20Code%20available%20at%0Ahttps%3A//boheumd.github.io/MA-LMM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05726v1&entry.124074799=Read"},
{"title": "Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view\n  Reconstruction", "author": "Yating Wang and Ran Yi and Ke Fan and Jinkun Hao and Jiangbo Lu and Lizhuang Ma", "abstract": "  Face meshes in consistent topology serve as the foundation for many\nface-related applications, such as 3DMM constrained face reconstruction and\nexpression retargeting. Traditional methods commonly acquire topology uniformed\nface meshes by two separate steps: multi-view stereo (MVS) to reconstruct\nshapes followed by non-rigid registration to align topology, but struggles with\nhandling noise and non-lambertian surfaces. Recently neural volume rendering\ntechniques have been rapidly evolved and shown great advantages in 3D\nreconstruction or novel view synthesis. Our goal is to leverage the superiority\nof neural volume rendering into multi-view reconstruction of face mesh with\nconsistent topology. We propose a mesh volume rendering method that enables\ndirectly optimizing mesh geometry while preserving topology, and learning\nimplicit features to model complex facial appearance from multi-view images.\nThe key innovation lies in spreading sparse mesh features into the surrounding\nspace to simulate radiance field required for volume rendering, which\nfacilitates backpropagation of gradients from images to mesh geometry and\nimplicit appearance features. Our proposed feature spreading module exhibits\ndeformation invariance, enabling photorealistic rendering seamlessly after mesh\nediting. We conduct experiments on multi-view face image dataset to evaluate\nthe reconstruction and implement an application for photorealistic rendering of\nanimated face mesh.\n", "link": "http://arxiv.org/abs/2404.05606v1", "date": "2024-04-08", "relevancy": 2.2574, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.576}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5585}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Topology%20Uniformed%20Face%20Mesh%20by%20Volume%20Rendering%20for%20Multi-view%0A%20%20Reconstruction&body=Title%3A%20Learning%20Topology%20Uniformed%20Face%20Mesh%20by%20Volume%20Rendering%20for%20Multi-view%0A%20%20Reconstruction%0AAuthor%3A%20Yating%20Wang%20and%20Ran%20Yi%20and%20Ke%20Fan%20and%20Jinkun%20Hao%20and%20Jiangbo%20Lu%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Face%20meshes%20in%20consistent%20topology%20serve%20as%20the%20foundation%20for%20many%0Aface-related%20applications%2C%20such%20as%203DMM%20constrained%20face%20reconstruction%20and%0Aexpression%20retargeting.%20Traditional%20methods%20commonly%20acquire%20topology%20uniformed%0Aface%20meshes%20by%20two%20separate%20steps%3A%20multi-view%20stereo%20%28MVS%29%20to%20reconstruct%0Ashapes%20followed%20by%20non-rigid%20registration%20to%20align%20topology%2C%20but%20struggles%20with%0Ahandling%20noise%20and%20non-lambertian%20surfaces.%20Recently%20neural%20volume%20rendering%0Atechniques%20have%20been%20rapidly%20evolved%20and%20shown%20great%20advantages%20in%203D%0Areconstruction%20or%20novel%20view%20synthesis.%20Our%20goal%20is%20to%20leverage%20the%20superiority%0Aof%20neural%20volume%20rendering%20into%20multi-view%20reconstruction%20of%20face%20mesh%20with%0Aconsistent%20topology.%20We%20propose%20a%20mesh%20volume%20rendering%20method%20that%20enables%0Adirectly%20optimizing%20mesh%20geometry%20while%20preserving%20topology%2C%20and%20learning%0Aimplicit%20features%20to%20model%20complex%20facial%20appearance%20from%20multi-view%20images.%0AThe%20key%20innovation%20lies%20in%20spreading%20sparse%20mesh%20features%20into%20the%20surrounding%0Aspace%20to%20simulate%20radiance%20field%20required%20for%20volume%20rendering%2C%20which%0Afacilitates%20backpropagation%20of%20gradients%20from%20images%20to%20mesh%20geometry%20and%0Aimplicit%20appearance%20features.%20Our%20proposed%20feature%20spreading%20module%20exhibits%0Adeformation%20invariance%2C%20enabling%20photorealistic%20rendering%20seamlessly%20after%20mesh%0Aediting.%20We%20conduct%20experiments%20on%20multi-view%20face%20image%20dataset%20to%20evaluate%0Athe%20reconstruction%20and%20implement%20an%20application%20for%20photorealistic%20rendering%20of%0Aanimated%20face%20mesh.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Topology%20Uniformed%20Face%20Mesh%20by%20Volume%20Rendering%20for%20Multi-view%0A%20%20Reconstruction&entry.906535625=Yating%20Wang%20and%20Ran%20Yi%20and%20Ke%20Fan%20and%20Jinkun%20Hao%20and%20Jiangbo%20Lu%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Face%20meshes%20in%20consistent%20topology%20serve%20as%20the%20foundation%20for%20many%0Aface-related%20applications%2C%20such%20as%203DMM%20constrained%20face%20reconstruction%20and%0Aexpression%20retargeting.%20Traditional%20methods%20commonly%20acquire%20topology%20uniformed%0Aface%20meshes%20by%20two%20separate%20steps%3A%20multi-view%20stereo%20%28MVS%29%20to%20reconstruct%0Ashapes%20followed%20by%20non-rigid%20registration%20to%20align%20topology%2C%20but%20struggles%20with%0Ahandling%20noise%20and%20non-lambertian%20surfaces.%20Recently%20neural%20volume%20rendering%0Atechniques%20have%20been%20rapidly%20evolved%20and%20shown%20great%20advantages%20in%203D%0Areconstruction%20or%20novel%20view%20synthesis.%20Our%20goal%20is%20to%20leverage%20the%20superiority%0Aof%20neural%20volume%20rendering%20into%20multi-view%20reconstruction%20of%20face%20mesh%20with%0Aconsistent%20topology.%20We%20propose%20a%20mesh%20volume%20rendering%20method%20that%20enables%0Adirectly%20optimizing%20mesh%20geometry%20while%20preserving%20topology%2C%20and%20learning%0Aimplicit%20features%20to%20model%20complex%20facial%20appearance%20from%20multi-view%20images.%0AThe%20key%20innovation%20lies%20in%20spreading%20sparse%20mesh%20features%20into%20the%20surrounding%0Aspace%20to%20simulate%20radiance%20field%20required%20for%20volume%20rendering%2C%20which%0Afacilitates%20backpropagation%20of%20gradients%20from%20images%20to%20mesh%20geometry%20and%0Aimplicit%20appearance%20features.%20Our%20proposed%20feature%20spreading%20module%20exhibits%0Adeformation%20invariance%2C%20enabling%20photorealistic%20rendering%20seamlessly%20after%20mesh%0Aediting.%20We%20conduct%20experiments%20on%20multi-view%20face%20image%20dataset%20to%20evaluate%0Athe%20reconstruction%20and%20implement%20an%20application%20for%20photorealistic%20rendering%20of%0Aanimated%20face%20mesh.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05606v1&entry.124074799=Read"},
{"title": "Automatic Controllable Colorization via Imagination", "author": "Xiaoyan Cong and Yue Wu and Qifeng Chen and Chenyang Lei", "abstract": "  We propose a framework for automatic colorization that allows for iterative\nediting and modifications. The core of our framework lies in an imagination\nmodule: by understanding the content within a grayscale image, we utilize a\npre-trained image generation model to generate multiple images that contain the\nsame content. These images serve as references for coloring, mimicking the\nprocess of human experts. As the synthesized images can be imperfect or\ndifferent from the original grayscale image, we propose a Reference Refinement\nModule to select the optimal reference composition. Unlike most previous\nend-to-end automatic colorization algorithms, our framework allows for\niterative and localized modifications of the colorization results because we\nexplicitly model the coloring samples. Extensive experiments demonstrate the\nsuperiority of our framework over existing automatic colorization algorithms in\neditability and flexibility. Project page:\nhttps://xy-cong.github.io/imagine-colorization.\n", "link": "http://arxiv.org/abs/2404.05661v1", "date": "2024-04-08", "relevancy": 2.215, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5704}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5476}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5274}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Controllable%20Colorization%20via%20Imagination&body=Title%3A%20Automatic%20Controllable%20Colorization%20via%20Imagination%0AAuthor%3A%20Xiaoyan%20Cong%20and%20Yue%20Wu%20and%20Qifeng%20Chen%20and%20Chenyang%20Lei%0AAbstract%3A%20%20%20We%20propose%20a%20framework%20for%20automatic%20colorization%20that%20allows%20for%20iterative%0Aediting%20and%20modifications.%20The%20core%20of%20our%20framework%20lies%20in%20an%20imagination%0Amodule%3A%20by%20understanding%20the%20content%20within%20a%20grayscale%20image%2C%20we%20utilize%20a%0Apre-trained%20image%20generation%20model%20to%20generate%20multiple%20images%20that%20contain%20the%0Asame%20content.%20These%20images%20serve%20as%20references%20for%20coloring%2C%20mimicking%20the%0Aprocess%20of%20human%20experts.%20As%20the%20synthesized%20images%20can%20be%20imperfect%20or%0Adifferent%20from%20the%20original%20grayscale%20image%2C%20we%20propose%20a%20Reference%20Refinement%0AModule%20to%20select%20the%20optimal%20reference%20composition.%20Unlike%20most%20previous%0Aend-to-end%20automatic%20colorization%20algorithms%2C%20our%20framework%20allows%20for%0Aiterative%20and%20localized%20modifications%20of%20the%20colorization%20results%20because%20we%0Aexplicitly%20model%20the%20coloring%20samples.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20framework%20over%20existing%20automatic%20colorization%20algorithms%20in%0Aeditability%20and%20flexibility.%20Project%20page%3A%0Ahttps%3A//xy-cong.github.io/imagine-colorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05661v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Controllable%20Colorization%20via%20Imagination&entry.906535625=Xiaoyan%20Cong%20and%20Yue%20Wu%20and%20Qifeng%20Chen%20and%20Chenyang%20Lei&entry.1292438233=%20%20We%20propose%20a%20framework%20for%20automatic%20colorization%20that%20allows%20for%20iterative%0Aediting%20and%20modifications.%20The%20core%20of%20our%20framework%20lies%20in%20an%20imagination%0Amodule%3A%20by%20understanding%20the%20content%20within%20a%20grayscale%20image%2C%20we%20utilize%20a%0Apre-trained%20image%20generation%20model%20to%20generate%20multiple%20images%20that%20contain%20the%0Asame%20content.%20These%20images%20serve%20as%20references%20for%20coloring%2C%20mimicking%20the%0Aprocess%20of%20human%20experts.%20As%20the%20synthesized%20images%20can%20be%20imperfect%20or%0Adifferent%20from%20the%20original%20grayscale%20image%2C%20we%20propose%20a%20Reference%20Refinement%0AModule%20to%20select%20the%20optimal%20reference%20composition.%20Unlike%20most%20previous%0Aend-to-end%20automatic%20colorization%20algorithms%2C%20our%20framework%20allows%20for%0Aiterative%20and%20localized%20modifications%20of%20the%20colorization%20results%20because%20we%0Aexplicitly%20model%20the%20coloring%20samples.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20framework%20over%20existing%20automatic%20colorization%20algorithms%20in%0Aeditability%20and%20flexibility.%20Project%20page%3A%0Ahttps%3A//xy-cong.github.io/imagine-colorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05661v1&entry.124074799=Read"},
{"title": "Embedded light-weight approach for safe landing in populated areas", "author": "Tilemahos Mitroudas and Vasiliki Balaska and Athanasios Psomoulis and Antonios Gasteratos", "abstract": "  Landing safety is a challenge heavily engaging the research community\nrecently, due to the increasing interest in applications availed by aerial\nvehicles. In this paper, we propose a landing safety pipeline based on state of\nthe art object detectors and OctoMap. First, a point cloud of surface obstacles\nis generated, which is then inserted in an OctoMap. The unoccupied areas are\nidentified, thus resulting to a list of safe landing points. Due to the low\ninference time achieved by state of the art object detectors and the efficient\npoint cloud manipulation using OctoMap, it is feasible for our approach to\ndeploy on low-weight embedded systems. The proposed pipeline has been evaluated\nin many simulation scenarios, varying in people density, number, and movement.\nSimulations were executed with an Nvidia Jetson Nano in the loop to confirm the\npipeline's performance and robustness in a low computing power hardware. The\nexperiments yielded promising results with a 95% success rate.\n", "link": "http://arxiv.org/abs/2302.14445v2", "date": "2024-04-08", "relevancy": 2.186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5856}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5048}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embedded%20light-weight%20approach%20for%20safe%20landing%20in%20populated%20areas&body=Title%3A%20Embedded%20light-weight%20approach%20for%20safe%20landing%20in%20populated%20areas%0AAuthor%3A%20Tilemahos%20Mitroudas%20and%20Vasiliki%20Balaska%20and%20Athanasios%20Psomoulis%20and%20Antonios%20Gasteratos%0AAbstract%3A%20%20%20Landing%20safety%20is%20a%20challenge%20heavily%20engaging%20the%20research%20community%0Arecently%2C%20due%20to%20the%20increasing%20interest%20in%20applications%20availed%20by%20aerial%0Avehicles.%20In%20this%20paper%2C%20we%20propose%20a%20landing%20safety%20pipeline%20based%20on%20state%20of%0Athe%20art%20object%20detectors%20and%20OctoMap.%20First%2C%20a%20point%20cloud%20of%20surface%20obstacles%0Ais%20generated%2C%20which%20is%20then%20inserted%20in%20an%20OctoMap.%20The%20unoccupied%20areas%20are%0Aidentified%2C%20thus%20resulting%20to%20a%20list%20of%20safe%20landing%20points.%20Due%20to%20the%20low%0Ainference%20time%20achieved%20by%20state%20of%20the%20art%20object%20detectors%20and%20the%20efficient%0Apoint%20cloud%20manipulation%20using%20OctoMap%2C%20it%20is%20feasible%20for%20our%20approach%20to%0Adeploy%20on%20low-weight%20embedded%20systems.%20The%20proposed%20pipeline%20has%20been%20evaluated%0Ain%20many%20simulation%20scenarios%2C%20varying%20in%20people%20density%2C%20number%2C%20and%20movement.%0ASimulations%20were%20executed%20with%20an%20Nvidia%20Jetson%20Nano%20in%20the%20loop%20to%20confirm%20the%0Apipeline%27s%20performance%20and%20robustness%20in%20a%20low%20computing%20power%20hardware.%20The%0Aexperiments%20yielded%20promising%20results%20with%20a%2095%25%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.14445v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20light-weight%20approach%20for%20safe%20landing%20in%20populated%20areas&entry.906535625=Tilemahos%20Mitroudas%20and%20Vasiliki%20Balaska%20and%20Athanasios%20Psomoulis%20and%20Antonios%20Gasteratos&entry.1292438233=%20%20Landing%20safety%20is%20a%20challenge%20heavily%20engaging%20the%20research%20community%0Arecently%2C%20due%20to%20the%20increasing%20interest%20in%20applications%20availed%20by%20aerial%0Avehicles.%20In%20this%20paper%2C%20we%20propose%20a%20landing%20safety%20pipeline%20based%20on%20state%20of%0Athe%20art%20object%20detectors%20and%20OctoMap.%20First%2C%20a%20point%20cloud%20of%20surface%20obstacles%0Ais%20generated%2C%20which%20is%20then%20inserted%20in%20an%20OctoMap.%20The%20unoccupied%20areas%20are%0Aidentified%2C%20thus%20resulting%20to%20a%20list%20of%20safe%20landing%20points.%20Due%20to%20the%20low%0Ainference%20time%20achieved%20by%20state%20of%20the%20art%20object%20detectors%20and%20the%20efficient%0Apoint%20cloud%20manipulation%20using%20OctoMap%2C%20it%20is%20feasible%20for%20our%20approach%20to%0Adeploy%20on%20low-weight%20embedded%20systems.%20The%20proposed%20pipeline%20has%20been%20evaluated%0Ain%20many%20simulation%20scenarios%2C%20varying%20in%20people%20density%2C%20number%2C%20and%20movement.%0ASimulations%20were%20executed%20with%20an%20Nvidia%20Jetson%20Nano%20in%20the%20loop%20to%20confirm%20the%0Apipeline%27s%20performance%20and%20robustness%20in%20a%20low%20computing%20power%20hardware.%20The%0Aexperiments%20yielded%20promising%20results%20with%20a%2095%25%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.14445v2&entry.124074799=Read"},
{"title": "T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise\n  Event Spotting in Sports Videos", "author": "Artur Xarles and Sergio Escalera and Thomas B. Moeslund and Albert Clap\u00e9s", "abstract": "  In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer\nEncoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses\nmultiple challenges in the task, including the need for discriminability among\nframe representations, high output temporal resolution to maintain prediction\nprecision, and the necessity to capture information at different temporal\nscales to handle events with varying dynamics. It tackles these challenges\nthrough its specifically designed architecture, featuring an encoder-decoder\nfor leveraging multiple temporal scales and achieving high output temporal\nresolution, along with temporal modules designed to increase token\ndiscriminability. Leveraging these characteristics, T-DEED achieves SOTA\nperformance on the FigureSkating and FineDiving datasets.\n", "link": "http://arxiv.org/abs/2404.05392v1", "date": "2024-04-08", "relevancy": 2.1812, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5511}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5434}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5356}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos&body=Title%3A%20T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos%0AAuthor%3A%20Artur%20Xarles%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund%20and%20Albert%20Clap%C3%A9s%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20T-DEED%2C%20a%20Temporal-Discriminability%20Enhancer%0AEncoder-Decoder%20for%20Precise%20Event%20Spotting%20in%20sports%20videos.%20T-DEED%20addresses%0Amultiple%20challenges%20in%20the%20task%2C%20including%20the%20need%20for%20discriminability%20among%0Aframe%20representations%2C%20high%20output%20temporal%20resolution%20to%20maintain%20prediction%0Aprecision%2C%20and%20the%20necessity%20to%20capture%20information%20at%20different%20temporal%0Ascales%20to%20handle%20events%20with%20varying%20dynamics.%20It%20tackles%20these%20challenges%0Athrough%20its%20specifically%20designed%20architecture%2C%20featuring%20an%20encoder-decoder%0Afor%20leveraging%20multiple%20temporal%20scales%20and%20achieving%20high%20output%20temporal%0Aresolution%2C%20along%20with%20temporal%20modules%20designed%20to%20increase%20token%0Adiscriminability.%20Leveraging%20these%20characteristics%2C%20T-DEED%20achieves%20SOTA%0Aperformance%20on%20the%20FigureSkating%20and%20FineDiving%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05392v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-DEED%3A%20Temporal-Discriminability%20Enhancer%20Encoder-Decoder%20for%20Precise%0A%20%20Event%20Spotting%20in%20Sports%20Videos&entry.906535625=Artur%20Xarles%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund%20and%20Albert%20Clap%C3%A9s&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20T-DEED%2C%20a%20Temporal-Discriminability%20Enhancer%0AEncoder-Decoder%20for%20Precise%20Event%20Spotting%20in%20sports%20videos.%20T-DEED%20addresses%0Amultiple%20challenges%20in%20the%20task%2C%20including%20the%20need%20for%20discriminability%20among%0Aframe%20representations%2C%20high%20output%20temporal%20resolution%20to%20maintain%20prediction%0Aprecision%2C%20and%20the%20necessity%20to%20capture%20information%20at%20different%20temporal%0Ascales%20to%20handle%20events%20with%20varying%20dynamics.%20It%20tackles%20these%20challenges%0Athrough%20its%20specifically%20designed%20architecture%2C%20featuring%20an%20encoder-decoder%0Afor%20leveraging%20multiple%20temporal%20scales%20and%20achieving%20high%20output%20temporal%0Aresolution%2C%20along%20with%20temporal%20modules%20designed%20to%20increase%20token%0Adiscriminability.%20Leveraging%20these%20characteristics%2C%20T-DEED%20achieves%20SOTA%0Aperformance%20on%20the%20FigureSkating%20and%20FineDiving%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05392v1&entry.124074799=Read"},
{"title": "DPHMs: Diffusion Parametric Head Models for Depth-based Tracking", "author": "Jiapeng Tang and Angela Dai and Yinyu Nie and Lev Markhasin and Justus Thies and Matthias Niessner", "abstract": "  We introduce Diffusion Parametric Head Models (DPHMs), a generative model\nthat enables robust volumetric head reconstruction and tracking from monocular\ndepth sequences. While recent volumetric head models, such as NPHMs, can now\nexcel in representing high-fidelity head geometries, tracking and\nreconstructing heads from real-world single-view depth sequences remains very\nchallenging, as the fitting to partial and noisy observations is\nunderconstrained. To tackle these challenges, we propose a latent\ndiffusion-based prior to regularize volumetric head reconstruction and\ntracking. This prior-based regularizer effectively constrains the identity and\nexpression codes to lie on the underlying latent manifold which represents\nplausible head shapes. To evaluate the effectiveness of the diffusion-based\nprior, we collect a dataset of monocular Kinect sequences consisting of various\ncomplex facial expression motions and rapid transitions. We compare our method\nto state-of-the-art tracking methods and demonstrate improved head identity\nreconstruction as well as robust expression tracking.\n", "link": "http://arxiv.org/abs/2312.01068v2", "date": "2024-04-08", "relevancy": 2.1803, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5597}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5588}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5249}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DPHMs%3A%20Diffusion%20Parametric%20Head%20Models%20for%20Depth-based%20Tracking&body=Title%3A%20DPHMs%3A%20Diffusion%20Parametric%20Head%20Models%20for%20Depth-based%20Tracking%0AAuthor%3A%20Jiapeng%20Tang%20and%20Angela%20Dai%20and%20Yinyu%20Nie%20and%20Lev%20Markhasin%20and%20Justus%20Thies%20and%20Matthias%20Niessner%0AAbstract%3A%20%20%20We%20introduce%20Diffusion%20Parametric%20Head%20Models%20%28DPHMs%29%2C%20a%20generative%20model%0Athat%20enables%20robust%20volumetric%20head%20reconstruction%20and%20tracking%20from%20monocular%0Adepth%20sequences.%20While%20recent%20volumetric%20head%20models%2C%20such%20as%20NPHMs%2C%20can%20now%0Aexcel%20in%20representing%20high-fidelity%20head%20geometries%2C%20tracking%20and%0Areconstructing%20heads%20from%20real-world%20single-view%20depth%20sequences%20remains%20very%0Achallenging%2C%20as%20the%20fitting%20to%20partial%20and%20noisy%20observations%20is%0Aunderconstrained.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20latent%0Adiffusion-based%20prior%20to%20regularize%20volumetric%20head%20reconstruction%20and%0Atracking.%20This%20prior-based%20regularizer%20effectively%20constrains%20the%20identity%20and%0Aexpression%20codes%20to%20lie%20on%20the%20underlying%20latent%20manifold%20which%20represents%0Aplausible%20head%20shapes.%20To%20evaluate%20the%20effectiveness%20of%20the%20diffusion-based%0Aprior%2C%20we%20collect%20a%20dataset%20of%20monocular%20Kinect%20sequences%20consisting%20of%20various%0Acomplex%20facial%20expression%20motions%20and%20rapid%20transitions.%20We%20compare%20our%20method%0Ato%20state-of-the-art%20tracking%20methods%20and%20demonstrate%20improved%20head%20identity%0Areconstruction%20as%20well%20as%20robust%20expression%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01068v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPHMs%3A%20Diffusion%20Parametric%20Head%20Models%20for%20Depth-based%20Tracking&entry.906535625=Jiapeng%20Tang%20and%20Angela%20Dai%20and%20Yinyu%20Nie%20and%20Lev%20Markhasin%20and%20Justus%20Thies%20and%20Matthias%20Niessner&entry.1292438233=%20%20We%20introduce%20Diffusion%20Parametric%20Head%20Models%20%28DPHMs%29%2C%20a%20generative%20model%0Athat%20enables%20robust%20volumetric%20head%20reconstruction%20and%20tracking%20from%20monocular%0Adepth%20sequences.%20While%20recent%20volumetric%20head%20models%2C%20such%20as%20NPHMs%2C%20can%20now%0Aexcel%20in%20representing%20high-fidelity%20head%20geometries%2C%20tracking%20and%0Areconstructing%20heads%20from%20real-world%20single-view%20depth%20sequences%20remains%20very%0Achallenging%2C%20as%20the%20fitting%20to%20partial%20and%20noisy%20observations%20is%0Aunderconstrained.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20latent%0Adiffusion-based%20prior%20to%20regularize%20volumetric%20head%20reconstruction%20and%0Atracking.%20This%20prior-based%20regularizer%20effectively%20constrains%20the%20identity%20and%0Aexpression%20codes%20to%20lie%20on%20the%20underlying%20latent%20manifold%20which%20represents%0Aplausible%20head%20shapes.%20To%20evaluate%20the%20effectiveness%20of%20the%20diffusion-based%0Aprior%2C%20we%20collect%20a%20dataset%20of%20monocular%20Kinect%20sequences%20consisting%20of%20various%0Acomplex%20facial%20expression%20motions%20and%20rapid%20transitions.%20We%20compare%20our%20method%0Ato%20state-of-the-art%20tracking%20methods%20and%20demonstrate%20improved%20head%20identity%0Areconstruction%20as%20well%20as%20robust%20expression%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01068v2&entry.124074799=Read"},
{"title": "New Online Communities: Graph Deep Learning on Anonymous Voting Networks\n  to Identify Sybils in Polycentric Governance", "author": "Quinn DuPont", "abstract": "  This research examines the polycentric governance of digital assets in\nblockchain-based Decentralized Autonomous Organizations (DAOs). It offers a\ntheoretical framework and addresses a critical challenge facing decentralized\ngovernance by developing a method to identify Sybils, or spurious identities.\nSybils pose significant organizational sustainability threats to DAOs and\nother, commons-based online communities, and threat models are identified. The\nexperimental method uses an autoencoder architecture and graph deep learning\ntechniques to identify Sybil activity in a DAO governance dataset\n(snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN)\nlearned voting behaviours and a fast vector clustering algorithm used\nhigh-dimensional embeddings to identify similar nodes in a graph. The results\nreveal that deep learning can effectively identify Sybils, reducing the voting\ngraph by 2-5%. This research underscores the importance of Sybil resistance in\nDAOs, identifies challenges and opportunities for forensics and analysis of\nanonymous networks, and offers a novel perspective on decentralized governance,\ninforming future policy, regulation, and governance practices.\n", "link": "http://arxiv.org/abs/2311.17929v7", "date": "2024-04-08", "relevancy": 2.1729, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4435}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4319}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4284}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20New%20Online%20Communities%3A%20Graph%20Deep%20Learning%20on%20Anonymous%20Voting%20Networks%0A%20%20to%20Identify%20Sybils%20in%20Polycentric%20Governance&body=Title%3A%20New%20Online%20Communities%3A%20Graph%20Deep%20Learning%20on%20Anonymous%20Voting%20Networks%0A%20%20to%20Identify%20Sybils%20in%20Polycentric%20Governance%0AAuthor%3A%20Quinn%20DuPont%0AAbstract%3A%20%20%20This%20research%20examines%20the%20polycentric%20governance%20of%20digital%20assets%20in%0Ablockchain-based%20Decentralized%20Autonomous%20Organizations%20%28DAOs%29.%20It%20offers%20a%0Atheoretical%20framework%20and%20addresses%20a%20critical%20challenge%20facing%20decentralized%0Agovernance%20by%20developing%20a%20method%20to%20identify%20Sybils%2C%20or%20spurious%20identities.%0ASybils%20pose%20significant%20organizational%20sustainability%20threats%20to%20DAOs%20and%0Aother%2C%20commons-based%20online%20communities%2C%20and%20threat%20models%20are%20identified.%20The%0Aexperimental%20method%20uses%20an%20autoencoder%20architecture%20and%20graph%20deep%20learning%0Atechniques%20to%20identify%20Sybil%20activity%20in%20a%20DAO%20governance%20dataset%0A%28snapshot.org%29.%20Specifically%2C%20a%20Graph%20Convolutional%20Neural%20Network%20%28GCNN%29%0Alearned%20voting%20behaviours%20and%20a%20fast%20vector%20clustering%20algorithm%20used%0Ahigh-dimensional%20embeddings%20to%20identify%20similar%20nodes%20in%20a%20graph.%20The%20results%0Areveal%20that%20deep%20learning%20can%20effectively%20identify%20Sybils%2C%20reducing%20the%20voting%0Agraph%20by%202-5%25.%20This%20research%20underscores%20the%20importance%20of%20Sybil%20resistance%20in%0ADAOs%2C%20identifies%20challenges%20and%20opportunities%20for%20forensics%20and%20analysis%20of%0Aanonymous%20networks%2C%20and%20offers%20a%20novel%20perspective%20on%20decentralized%20governance%2C%0Ainforming%20future%20policy%2C%20regulation%2C%20and%20governance%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17929v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Online%20Communities%3A%20Graph%20Deep%20Learning%20on%20Anonymous%20Voting%20Networks%0A%20%20to%20Identify%20Sybils%20in%20Polycentric%20Governance&entry.906535625=Quinn%20DuPont&entry.1292438233=%20%20This%20research%20examines%20the%20polycentric%20governance%20of%20digital%20assets%20in%0Ablockchain-based%20Decentralized%20Autonomous%20Organizations%20%28DAOs%29.%20It%20offers%20a%0Atheoretical%20framework%20and%20addresses%20a%20critical%20challenge%20facing%20decentralized%0Agovernance%20by%20developing%20a%20method%20to%20identify%20Sybils%2C%20or%20spurious%20identities.%0ASybils%20pose%20significant%20organizational%20sustainability%20threats%20to%20DAOs%20and%0Aother%2C%20commons-based%20online%20communities%2C%20and%20threat%20models%20are%20identified.%20The%0Aexperimental%20method%20uses%20an%20autoencoder%20architecture%20and%20graph%20deep%20learning%0Atechniques%20to%20identify%20Sybil%20activity%20in%20a%20DAO%20governance%20dataset%0A%28snapshot.org%29.%20Specifically%2C%20a%20Graph%20Convolutional%20Neural%20Network%20%28GCNN%29%0Alearned%20voting%20behaviours%20and%20a%20fast%20vector%20clustering%20algorithm%20used%0Ahigh-dimensional%20embeddings%20to%20identify%20similar%20nodes%20in%20a%20graph.%20The%20results%0Areveal%20that%20deep%20learning%20can%20effectively%20identify%20Sybils%2C%20reducing%20the%20voting%0Agraph%20by%202-5%25.%20This%20research%20underscores%20the%20importance%20of%20Sybil%20resistance%20in%0ADAOs%2C%20identifies%20challenges%20and%20opportunities%20for%20forensics%20and%20analysis%20of%0Aanonymous%20networks%2C%20and%20offers%20a%20novel%20perspective%20on%20decentralized%20governance%2C%0Ainforming%20future%20policy%2C%20regulation%2C%20and%20governance%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17929v7&entry.124074799=Read"},
{"title": "LASIL: Learner-Aware Supervised Imitation Learning For Long-term\n  Microscopic Traffic Simulation", "author": "Ke Guo and Zhenwei Miao and Wei Jing and Weiwei Liu and Weizi Li and Dayang Hao and Jia Pan", "abstract": "  Microscopic traffic simulation plays a crucial role in transportation\nengineering by providing insights into individual vehicle behavior and overall\ntraffic flow. However, creating a realistic simulator that accurately\nreplicates human driving behaviors in various traffic conditions presents\nsignificant challenges. Traditional simulators relying on heuristic models\noften fail to deliver accurate simulations due to the complexity of real-world\ntraffic environments. Due to the covariate shift issue, existing imitation\nlearning-based simulators often fail to generate stable long-term simulations.\nIn this paper, we propose a novel approach called learner-aware supervised\nimitation learning to address the covariate shift problem in multi-agent\nimitation learning. By leveraging a variational autoencoder simultaneously\nmodeling the expert and learner state distribution, our approach augments\nexpert states such that the augmented state is aware of learner state\ndistribution. Our method, applied to urban traffic simulation, demonstrates\nsignificant improvements over existing state-of-the-art baselines in both\nshort-term microscopic and long-term macroscopic realism when evaluated on the\nreal-world dataset pNEUMA.\n", "link": "http://arxiv.org/abs/2403.17601v2", "date": "2024-04-08", "relevancy": 2.1629, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5496}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LASIL%3A%20Learner-Aware%20Supervised%20Imitation%20Learning%20For%20Long-term%0A%20%20Microscopic%20Traffic%20Simulation&body=Title%3A%20LASIL%3A%20Learner-Aware%20Supervised%20Imitation%20Learning%20For%20Long-term%0A%20%20Microscopic%20Traffic%20Simulation%0AAuthor%3A%20Ke%20Guo%20and%20Zhenwei%20Miao%20and%20Wei%20Jing%20and%20Weiwei%20Liu%20and%20Weizi%20Li%20and%20Dayang%20Hao%20and%20Jia%20Pan%0AAbstract%3A%20%20%20Microscopic%20traffic%20simulation%20plays%20a%20crucial%20role%20in%20transportation%0Aengineering%20by%20providing%20insights%20into%20individual%20vehicle%20behavior%20and%20overall%0Atraffic%20flow.%20However%2C%20creating%20a%20realistic%20simulator%20that%20accurately%0Areplicates%20human%20driving%20behaviors%20in%20various%20traffic%20conditions%20presents%0Asignificant%20challenges.%20Traditional%20simulators%20relying%20on%20heuristic%20models%0Aoften%20fail%20to%20deliver%20accurate%20simulations%20due%20to%20the%20complexity%20of%20real-world%0Atraffic%20environments.%20Due%20to%20the%20covariate%20shift%20issue%2C%20existing%20imitation%0Alearning-based%20simulators%20often%20fail%20to%20generate%20stable%20long-term%20simulations.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20learner-aware%20supervised%0Aimitation%20learning%20to%20address%20the%20covariate%20shift%20problem%20in%20multi-agent%0Aimitation%20learning.%20By%20leveraging%20a%20variational%20autoencoder%20simultaneously%0Amodeling%20the%20expert%20and%20learner%20state%20distribution%2C%20our%20approach%20augments%0Aexpert%20states%20such%20that%20the%20augmented%20state%20is%20aware%20of%20learner%20state%0Adistribution.%20Our%20method%2C%20applied%20to%20urban%20traffic%20simulation%2C%20demonstrates%0Asignificant%20improvements%20over%20existing%20state-of-the-art%20baselines%20in%20both%0Ashort-term%20microscopic%20and%20long-term%20macroscopic%20realism%20when%20evaluated%20on%20the%0Areal-world%20dataset%20pNEUMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17601v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASIL%3A%20Learner-Aware%20Supervised%20Imitation%20Learning%20For%20Long-term%0A%20%20Microscopic%20Traffic%20Simulation&entry.906535625=Ke%20Guo%20and%20Zhenwei%20Miao%20and%20Wei%20Jing%20and%20Weiwei%20Liu%20and%20Weizi%20Li%20and%20Dayang%20Hao%20and%20Jia%20Pan&entry.1292438233=%20%20Microscopic%20traffic%20simulation%20plays%20a%20crucial%20role%20in%20transportation%0Aengineering%20by%20providing%20insights%20into%20individual%20vehicle%20behavior%20and%20overall%0Atraffic%20flow.%20However%2C%20creating%20a%20realistic%20simulator%20that%20accurately%0Areplicates%20human%20driving%20behaviors%20in%20various%20traffic%20conditions%20presents%0Asignificant%20challenges.%20Traditional%20simulators%20relying%20on%20heuristic%20models%0Aoften%20fail%20to%20deliver%20accurate%20simulations%20due%20to%20the%20complexity%20of%20real-world%0Atraffic%20environments.%20Due%20to%20the%20covariate%20shift%20issue%2C%20existing%20imitation%0Alearning-based%20simulators%20often%20fail%20to%20generate%20stable%20long-term%20simulations.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20learner-aware%20supervised%0Aimitation%20learning%20to%20address%20the%20covariate%20shift%20problem%20in%20multi-agent%0Aimitation%20learning.%20By%20leveraging%20a%20variational%20autoencoder%20simultaneously%0Amodeling%20the%20expert%20and%20learner%20state%20distribution%2C%20our%20approach%20augments%0Aexpert%20states%20such%20that%20the%20augmented%20state%20is%20aware%20of%20learner%20state%0Adistribution.%20Our%20method%2C%20applied%20to%20urban%20traffic%20simulation%2C%20demonstrates%0Asignificant%20improvements%20over%20existing%20state-of-the-art%20baselines%20in%20both%0Ashort-term%20microscopic%20and%20long-term%20macroscopic%20realism%20when%20evaluated%20on%20the%0Areal-world%20dataset%20pNEUMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17601v2&entry.124074799=Read"},
{"title": "MLP Can Be A Good Transformer Learner", "author": "Sihao Lin and Pumeng Lyu and Dongrui Liu and Tao Tang and Xiaodan Liang and Andy Song and Xiaojun Chang", "abstract": "  Self-attention mechanism is the key of the Transformer but often criticized\nfor its computation demands. Previous token pruning works motivate their\nmethods from the view of computation redundancy but still need to load the full\nnetwork and require same memory costs. This paper introduces a novel strategy\nthat simplifies vision transformers and reduces computational load through the\nselective removal of non-essential attention layers, guided by entropy\nconsiderations. We identify that regarding the attention layer in bottom\nblocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit\nthe same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited\nsince they exhibit smaller feature entropy compared to those MLPs in the top\nblocks. Therefore, we propose to integrate the uninformative attention layers\ninto their subsequent counterparts by degenerating them into identical mapping,\nyielding only MLP in certain transformer blocks. Experimental results on\nImageNet-1k show that the proposed method can remove 40% attention layer of\nDeiT-B, improving throughput and memory bound without performance compromise.\nCode is available at https://github.com/sihaoevery/lambda_vit.\n", "link": "http://arxiv.org/abs/2404.05657v1", "date": "2024-04-08", "relevancy": 2.1613, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5862}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5381}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MLP%20Can%20Be%20A%20Good%20Transformer%20Learner&body=Title%3A%20MLP%20Can%20Be%20A%20Good%20Transformer%20Learner%0AAuthor%3A%20Sihao%20Lin%20and%20Pumeng%20Lyu%20and%20Dongrui%20Liu%20and%20Tao%20Tang%20and%20Xiaodan%20Liang%20and%20Andy%20Song%20and%20Xiaojun%20Chang%0AAbstract%3A%20%20%20Self-attention%20mechanism%20is%20the%20key%20of%20the%20Transformer%20but%20often%20criticized%0Afor%20its%20computation%20demands.%20Previous%20token%20pruning%20works%20motivate%20their%0Amethods%20from%20the%20view%20of%20computation%20redundancy%20but%20still%20need%20to%20load%20the%20full%0Anetwork%20and%20require%20same%20memory%20costs.%20This%20paper%20introduces%20a%20novel%20strategy%0Athat%20simplifies%20vision%20transformers%20and%20reduces%20computational%20load%20through%20the%0Aselective%20removal%20of%20non-essential%20attention%20layers%2C%20guided%20by%20entropy%0Aconsiderations.%20We%20identify%20that%20regarding%20the%20attention%20layer%20in%20bottom%0Ablocks%2C%20their%20subsequent%20MLP%20layers%2C%20i.e.%20two%20feed-forward%20layers%2C%20can%20elicit%0Athe%20same%20entropy%20quantity.%20Meanwhile%2C%20the%20accompanied%20MLPs%20are%20under-exploited%0Asince%20they%20exhibit%20smaller%20feature%20entropy%20compared%20to%20those%20MLPs%20in%20the%20top%0Ablocks.%20Therefore%2C%20we%20propose%20to%20integrate%20the%20uninformative%20attention%20layers%0Ainto%20their%20subsequent%20counterparts%20by%20degenerating%20them%20into%20identical%20mapping%2C%0Ayielding%20only%20MLP%20in%20certain%20transformer%20blocks.%20Experimental%20results%20on%0AImageNet-1k%20show%20that%20the%20proposed%20method%20can%20remove%2040%25%20attention%20layer%20of%0ADeiT-B%2C%20improving%20throughput%20and%20memory%20bound%20without%20performance%20compromise.%0ACode%20is%20available%20at%20https%3A//github.com/sihaoevery/lambda_vit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05657v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLP%20Can%20Be%20A%20Good%20Transformer%20Learner&entry.906535625=Sihao%20Lin%20and%20Pumeng%20Lyu%20and%20Dongrui%20Liu%20and%20Tao%20Tang%20and%20Xiaodan%20Liang%20and%20Andy%20Song%20and%20Xiaojun%20Chang&entry.1292438233=%20%20Self-attention%20mechanism%20is%20the%20key%20of%20the%20Transformer%20but%20often%20criticized%0Afor%20its%20computation%20demands.%20Previous%20token%20pruning%20works%20motivate%20their%0Amethods%20from%20the%20view%20of%20computation%20redundancy%20but%20still%20need%20to%20load%20the%20full%0Anetwork%20and%20require%20same%20memory%20costs.%20This%20paper%20introduces%20a%20novel%20strategy%0Athat%20simplifies%20vision%20transformers%20and%20reduces%20computational%20load%20through%20the%0Aselective%20removal%20of%20non-essential%20attention%20layers%2C%20guided%20by%20entropy%0Aconsiderations.%20We%20identify%20that%20regarding%20the%20attention%20layer%20in%20bottom%0Ablocks%2C%20their%20subsequent%20MLP%20layers%2C%20i.e.%20two%20feed-forward%20layers%2C%20can%20elicit%0Athe%20same%20entropy%20quantity.%20Meanwhile%2C%20the%20accompanied%20MLPs%20are%20under-exploited%0Asince%20they%20exhibit%20smaller%20feature%20entropy%20compared%20to%20those%20MLPs%20in%20the%20top%0Ablocks.%20Therefore%2C%20we%20propose%20to%20integrate%20the%20uninformative%20attention%20layers%0Ainto%20their%20subsequent%20counterparts%20by%20degenerating%20them%20into%20identical%20mapping%2C%0Ayielding%20only%20MLP%20in%20certain%20transformer%20blocks.%20Experimental%20results%20on%0AImageNet-1k%20show%20that%20the%20proposed%20method%20can%20remove%2040%25%20attention%20layer%20of%0ADeiT-B%2C%20improving%20throughput%20and%20memory%20bound%20without%20performance%20compromise.%0ACode%20is%20available%20at%20https%3A//github.com/sihaoevery/lambda_vit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05657v1&entry.124074799=Read"},
{"title": "Residual Chain Prediction for Autonomous Driving Path Planning", "author": "Liguo Zhou and Yirui Zhou and Huaming Liu and Alois Knoll", "abstract": "  In the rapidly evolving field of autonomous driving systems, the refinement\nof path planning algorithms is paramount for navigating vehicles through\ndynamic environments, particularly in complex urban scenarios. Traditional path\nplanning algorithms, which are heavily reliant on static rules and manually\ndefined parameters, often fall short in such contexts, highlighting the need\nfor more adaptive, learning-based approaches. Among these, behavior cloning\nemerges as a noteworthy strategy for its simplicity and efficiency, especially\nwithin the realm of end-to-end path planning. However, behavior cloning faces\nchallenges, such as covariate shift when employing traditional Manhattan\ndistance as the metric. Addressing this, our study introduces the novel concept\nof Residual Chain Loss. Residual Chain Loss dynamically adjusts the loss\ncalculation process to enhance the temporal dependency and accuracy of\npredicted path points, significantly improving the model's performance without\nadditional computational overhead. Through testing on the nuScenes dataset, we\nunderscore the method's substantial advancements in addressing covariate shift,\nfacilitating dynamic loss adjustments, and ensuring seamless integration with\nend-to-end path planning frameworks. Our findings highlight the potential of\nResidual Chain Loss to revolutionize planning component of autonomous driving\nsystems, marking a significant step forward in the quest for level 5 autonomous\ndriving system.\n", "link": "http://arxiv.org/abs/2404.05423v1", "date": "2024-04-08", "relevancy": 2.1611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5858}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5226}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Residual%20Chain%20Prediction%20for%20Autonomous%20Driving%20Path%20Planning&body=Title%3A%20Residual%20Chain%20Prediction%20for%20Autonomous%20Driving%20Path%20Planning%0AAuthor%3A%20Liguo%20Zhou%20and%20Yirui%20Zhou%20and%20Huaming%20Liu%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%20systems%2C%20the%20refinement%0Aof%20path%20planning%20algorithms%20is%20paramount%20for%20navigating%20vehicles%20through%0Adynamic%20environments%2C%20particularly%20in%20complex%20urban%20scenarios.%20Traditional%20path%0Aplanning%20algorithms%2C%20which%20are%20heavily%20reliant%20on%20static%20rules%20and%20manually%0Adefined%20parameters%2C%20often%20fall%20short%20in%20such%20contexts%2C%20highlighting%20the%20need%0Afor%20more%20adaptive%2C%20learning-based%20approaches.%20Among%20these%2C%20behavior%20cloning%0Aemerges%20as%20a%20noteworthy%20strategy%20for%20its%20simplicity%20and%20efficiency%2C%20especially%0Awithin%20the%20realm%20of%20end-to-end%20path%20planning.%20However%2C%20behavior%20cloning%20faces%0Achallenges%2C%20such%20as%20covariate%20shift%20when%20employing%20traditional%20Manhattan%0Adistance%20as%20the%20metric.%20Addressing%20this%2C%20our%20study%20introduces%20the%20novel%20concept%0Aof%20Residual%20Chain%20Loss.%20Residual%20Chain%20Loss%20dynamically%20adjusts%20the%20loss%0Acalculation%20process%20to%20enhance%20the%20temporal%20dependency%20and%20accuracy%20of%0Apredicted%20path%20points%2C%20significantly%20improving%20the%20model%27s%20performance%20without%0Aadditional%20computational%20overhead.%20Through%20testing%20on%20the%20nuScenes%20dataset%2C%20we%0Aunderscore%20the%20method%27s%20substantial%20advancements%20in%20addressing%20covariate%20shift%2C%0Afacilitating%20dynamic%20loss%20adjustments%2C%20and%20ensuring%20seamless%20integration%20with%0Aend-to-end%20path%20planning%20frameworks.%20Our%20findings%20highlight%20the%20potential%20of%0AResidual%20Chain%20Loss%20to%20revolutionize%20planning%20component%20of%20autonomous%20driving%0Asystems%2C%20marking%20a%20significant%20step%20forward%20in%20the%20quest%20for%20level%205%20autonomous%0Adriving%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05423v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Chain%20Prediction%20for%20Autonomous%20Driving%20Path%20Planning&entry.906535625=Liguo%20Zhou%20and%20Yirui%20Zhou%20and%20Huaming%20Liu%20and%20Alois%20Knoll&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%20systems%2C%20the%20refinement%0Aof%20path%20planning%20algorithms%20is%20paramount%20for%20navigating%20vehicles%20through%0Adynamic%20environments%2C%20particularly%20in%20complex%20urban%20scenarios.%20Traditional%20path%0Aplanning%20algorithms%2C%20which%20are%20heavily%20reliant%20on%20static%20rules%20and%20manually%0Adefined%20parameters%2C%20often%20fall%20short%20in%20such%20contexts%2C%20highlighting%20the%20need%0Afor%20more%20adaptive%2C%20learning-based%20approaches.%20Among%20these%2C%20behavior%20cloning%0Aemerges%20as%20a%20noteworthy%20strategy%20for%20its%20simplicity%20and%20efficiency%2C%20especially%0Awithin%20the%20realm%20of%20end-to-end%20path%20planning.%20However%2C%20behavior%20cloning%20faces%0Achallenges%2C%20such%20as%20covariate%20shift%20when%20employing%20traditional%20Manhattan%0Adistance%20as%20the%20metric.%20Addressing%20this%2C%20our%20study%20introduces%20the%20novel%20concept%0Aof%20Residual%20Chain%20Loss.%20Residual%20Chain%20Loss%20dynamically%20adjusts%20the%20loss%0Acalculation%20process%20to%20enhance%20the%20temporal%20dependency%20and%20accuracy%20of%0Apredicted%20path%20points%2C%20significantly%20improving%20the%20model%27s%20performance%20without%0Aadditional%20computational%20overhead.%20Through%20testing%20on%20the%20nuScenes%20dataset%2C%20we%0Aunderscore%20the%20method%27s%20substantial%20advancements%20in%20addressing%20covariate%20shift%2C%0Afacilitating%20dynamic%20loss%20adjustments%2C%20and%20ensuring%20seamless%20integration%20with%0Aend-to-end%20path%20planning%20frameworks.%20Our%20findings%20highlight%20the%20potential%20of%0AResidual%20Chain%20Loss%20to%20revolutionize%20planning%20component%20of%20autonomous%20driving%0Asystems%2C%20marking%20a%20significant%20step%20forward%20in%20the%20quest%20for%20level%205%20autonomous%0Adriving%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05423v1&entry.124074799=Read"},
{"title": "RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose\n  Estimation", "author": "Peng Lu and Tao Jiang and Yining Li and Xiangtai Li and Kai Chen and Wenming Yang", "abstract": "  Real-time multi-person pose estimation presents significant challenges in\nbalancing speed and precision. While two-stage top-down methods slow down as\nthe number of people in the image increases, existing one-stage methods often\nfail to simultaneously deliver high accuracy and real-time performance. This\npaper introduces RTMO, a one-stage pose estimation framework that seamlessly\nintegrates coordinate classification by representing keypoints using dual 1-D\nheatmaps within the YOLO architecture, achieving accuracy comparable to\ntop-down methods while maintaining high speed. We propose a dynamic coordinate\nclassifier and a tailored loss function for heatmap learning, specifically\ndesigned to address the incompatibilities between coordinate classification and\ndense prediction models. RTMO outperforms state-of-the-art one-stage pose\nestimators, achieving 1.1% higher AP on COCO while operating about 9 times\nfaster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP on\nCOCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and\naccuracy. The code and models are available at\nhttps://github.com/open-mmlab/mmpose/tree/main/projects/rtmo.\n", "link": "http://arxiv.org/abs/2312.07526v2", "date": "2024-04-08", "relevancy": 2.1441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5581}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RTMO%3A%20Towards%20High-Performance%20One-Stage%20Real-Time%20Multi-Person%20Pose%0A%20%20Estimation&body=Title%3A%20RTMO%3A%20Towards%20High-Performance%20One-Stage%20Real-Time%20Multi-Person%20Pose%0A%20%20Estimation%0AAuthor%3A%20Peng%20Lu%20and%20Tao%20Jiang%20and%20Yining%20Li%20and%20Xiangtai%20Li%20and%20Kai%20Chen%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20Real-time%20multi-person%20pose%20estimation%20presents%20significant%20challenges%20in%0Abalancing%20speed%20and%20precision.%20While%20two-stage%20top-down%20methods%20slow%20down%20as%0Athe%20number%20of%20people%20in%20the%20image%20increases%2C%20existing%20one-stage%20methods%20often%0Afail%20to%20simultaneously%20deliver%20high%20accuracy%20and%20real-time%20performance.%20This%0Apaper%20introduces%20RTMO%2C%20a%20one-stage%20pose%20estimation%20framework%20that%20seamlessly%0Aintegrates%20coordinate%20classification%20by%20representing%20keypoints%20using%20dual%201-D%0Aheatmaps%20within%20the%20YOLO%20architecture%2C%20achieving%20accuracy%20comparable%20to%0Atop-down%20methods%20while%20maintaining%20high%20speed.%20We%20propose%20a%20dynamic%20coordinate%0Aclassifier%20and%20a%20tailored%20loss%20function%20for%20heatmap%20learning%2C%20specifically%0Adesigned%20to%20address%20the%20incompatibilities%20between%20coordinate%20classification%20and%0Adense%20prediction%20models.%20RTMO%20outperforms%20state-of-the-art%20one-stage%20pose%0Aestimators%2C%20achieving%201.1%25%20higher%20AP%20on%20COCO%20while%20operating%20about%209%20times%0Afaster%20with%20the%20same%20backbone.%20Our%20largest%20model%2C%20RTMO-l%2C%20attains%2074.8%25%20AP%20on%0ACOCO%20val2017%20and%20141%20FPS%20on%20a%20single%20V100%20GPU%2C%20demonstrating%20its%20efficiency%20and%0Aaccuracy.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/open-mmlab/mmpose/tree/main/projects/rtmo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07526v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTMO%3A%20Towards%20High-Performance%20One-Stage%20Real-Time%20Multi-Person%20Pose%0A%20%20Estimation&entry.906535625=Peng%20Lu%20and%20Tao%20Jiang%20and%20Yining%20Li%20and%20Xiangtai%20Li%20and%20Kai%20Chen%20and%20Wenming%20Yang&entry.1292438233=%20%20Real-time%20multi-person%20pose%20estimation%20presents%20significant%20challenges%20in%0Abalancing%20speed%20and%20precision.%20While%20two-stage%20top-down%20methods%20slow%20down%20as%0Athe%20number%20of%20people%20in%20the%20image%20increases%2C%20existing%20one-stage%20methods%20often%0Afail%20to%20simultaneously%20deliver%20high%20accuracy%20and%20real-time%20performance.%20This%0Apaper%20introduces%20RTMO%2C%20a%20one-stage%20pose%20estimation%20framework%20that%20seamlessly%0Aintegrates%20coordinate%20classification%20by%20representing%20keypoints%20using%20dual%201-D%0Aheatmaps%20within%20the%20YOLO%20architecture%2C%20achieving%20accuracy%20comparable%20to%0Atop-down%20methods%20while%20maintaining%20high%20speed.%20We%20propose%20a%20dynamic%20coordinate%0Aclassifier%20and%20a%20tailored%20loss%20function%20for%20heatmap%20learning%2C%20specifically%0Adesigned%20to%20address%20the%20incompatibilities%20between%20coordinate%20classification%20and%0Adense%20prediction%20models.%20RTMO%20outperforms%20state-of-the-art%20one-stage%20pose%0Aestimators%2C%20achieving%201.1%25%20higher%20AP%20on%20COCO%20while%20operating%20about%209%20times%0Afaster%20with%20the%20same%20backbone.%20Our%20largest%20model%2C%20RTMO-l%2C%20attains%2074.8%25%20AP%20on%0ACOCO%20val2017%20and%20141%20FPS%20on%20a%20single%20V100%20GPU%2C%20demonstrating%20its%20efficiency%20and%0Aaccuracy.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/open-mmlab/mmpose/tree/main/projects/rtmo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07526v2&entry.124074799=Read"},
{"title": "AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic\n  Segmentation", "author": "Jiannan Ge and Lingxi Xie and Hongtao Xie and Pandeng Li and Xiaopeng Zhang and Yongdong Zhang and Qi Tian", "abstract": "  A serious issue that harms the performance of zero-shot visual recognition is\nnamed objective misalignment, i.e., the learning objective prioritizes\nimproving the recognition accuracy of seen classes rather than unseen classes,\nwhile the latter is the true target to pursue. This issue becomes more\nsignificant in zero-shot image segmentation because the stronger (i.e.,\npixel-level) supervision brings a larger gap between seen and unseen classes.\nTo mitigate it, we propose a novel architecture named AlignZeg, which embodies\na comprehensive improvement of the segmentation pipeline, including proposal\nextraction, classification, and correction, to better fit the goal of zero-shot\nsegmentation. (1) Mutually-Refined Proposal Extraction. AlignZeg harnesses a\nmutual interaction between mask queries and visual features, facilitating\ndetailed class-agnostic mask proposal extraction. (2) Generalization-Enhanced\nProposal Classification. AlignZeg introduces synthetic data and incorporates\nmultiple background prototypes to allocate a more generalizable feature space.\n(3) Predictive Bias Correction. During the inference stage, AlignZeg uses a\nclass indicator to find potential unseen class proposals followed by a\nprediction postprocess to correct the prediction bias. Experiments demonstrate\nthat AlignZeg markedly enhances zero-shot semantic segmentation, as shown by an\naverage 3.8% increase in hIoU, primarily attributed to a 7.1% improvement in\nidentifying unseen classes, and we further validate that the improvement comes\nfrom alleviating the objective misalignment issue.\n", "link": "http://arxiv.org/abs/2404.05667v1", "date": "2024-04-08", "relevancy": 2.1402, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5342}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AlignZeg%3A%20Mitigating%20Objective%20Misalignment%20for%20Zero-shot%20Semantic%0A%20%20Segmentation&body=Title%3A%20AlignZeg%3A%20Mitigating%20Objective%20Misalignment%20for%20Zero-shot%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jiannan%20Ge%20and%20Lingxi%20Xie%20and%20Hongtao%20Xie%20and%20Pandeng%20Li%20and%20Xiaopeng%20Zhang%20and%20Yongdong%20Zhang%20and%20Qi%20Tian%0AAbstract%3A%20%20%20A%20serious%20issue%20that%20harms%20the%20performance%20of%20zero-shot%20visual%20recognition%20is%0Anamed%20objective%20misalignment%2C%20i.e.%2C%20the%20learning%20objective%20prioritizes%0Aimproving%20the%20recognition%20accuracy%20of%20seen%20classes%20rather%20than%20unseen%20classes%2C%0Awhile%20the%20latter%20is%20the%20true%20target%20to%20pursue.%20This%20issue%20becomes%20more%0Asignificant%20in%20zero-shot%20image%20segmentation%20because%20the%20stronger%20%28i.e.%2C%0Apixel-level%29%20supervision%20brings%20a%20larger%20gap%20between%20seen%20and%20unseen%20classes.%0ATo%20mitigate%20it%2C%20we%20propose%20a%20novel%20architecture%20named%20AlignZeg%2C%20which%20embodies%0Aa%20comprehensive%20improvement%20of%20the%20segmentation%20pipeline%2C%20including%20proposal%0Aextraction%2C%20classification%2C%20and%20correction%2C%20to%20better%20fit%20the%20goal%20of%20zero-shot%0Asegmentation.%20%281%29%20Mutually-Refined%20Proposal%20Extraction.%20AlignZeg%20harnesses%20a%0Amutual%20interaction%20between%20mask%20queries%20and%20visual%20features%2C%20facilitating%0Adetailed%20class-agnostic%20mask%20proposal%20extraction.%20%282%29%20Generalization-Enhanced%0AProposal%20Classification.%20AlignZeg%20introduces%20synthetic%20data%20and%20incorporates%0Amultiple%20background%20prototypes%20to%20allocate%20a%20more%20generalizable%20feature%20space.%0A%283%29%20Predictive%20Bias%20Correction.%20During%20the%20inference%20stage%2C%20AlignZeg%20uses%20a%0Aclass%20indicator%20to%20find%20potential%20unseen%20class%20proposals%20followed%20by%20a%0Aprediction%20postprocess%20to%20correct%20the%20prediction%20bias.%20Experiments%20demonstrate%0Athat%20AlignZeg%20markedly%20enhances%20zero-shot%20semantic%20segmentation%2C%20as%20shown%20by%20an%0Aaverage%203.8%25%20increase%20in%20hIoU%2C%20primarily%20attributed%20to%20a%207.1%25%20improvement%20in%0Aidentifying%20unseen%20classes%2C%20and%20we%20further%20validate%20that%20the%20improvement%20comes%0Afrom%20alleviating%20the%20objective%20misalignment%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignZeg%3A%20Mitigating%20Objective%20Misalignment%20for%20Zero-shot%20Semantic%0A%20%20Segmentation&entry.906535625=Jiannan%20Ge%20and%20Lingxi%20Xie%20and%20Hongtao%20Xie%20and%20Pandeng%20Li%20and%20Xiaopeng%20Zhang%20and%20Yongdong%20Zhang%20and%20Qi%20Tian&entry.1292438233=%20%20A%20serious%20issue%20that%20harms%20the%20performance%20of%20zero-shot%20visual%20recognition%20is%0Anamed%20objective%20misalignment%2C%20i.e.%2C%20the%20learning%20objective%20prioritizes%0Aimproving%20the%20recognition%20accuracy%20of%20seen%20classes%20rather%20than%20unseen%20classes%2C%0Awhile%20the%20latter%20is%20the%20true%20target%20to%20pursue.%20This%20issue%20becomes%20more%0Asignificant%20in%20zero-shot%20image%20segmentation%20because%20the%20stronger%20%28i.e.%2C%0Apixel-level%29%20supervision%20brings%20a%20larger%20gap%20between%20seen%20and%20unseen%20classes.%0ATo%20mitigate%20it%2C%20we%20propose%20a%20novel%20architecture%20named%20AlignZeg%2C%20which%20embodies%0Aa%20comprehensive%20improvement%20of%20the%20segmentation%20pipeline%2C%20including%20proposal%0Aextraction%2C%20classification%2C%20and%20correction%2C%20to%20better%20fit%20the%20goal%20of%20zero-shot%0Asegmentation.%20%281%29%20Mutually-Refined%20Proposal%20Extraction.%20AlignZeg%20harnesses%20a%0Amutual%20interaction%20between%20mask%20queries%20and%20visual%20features%2C%20facilitating%0Adetailed%20class-agnostic%20mask%20proposal%20extraction.%20%282%29%20Generalization-Enhanced%0AProposal%20Classification.%20AlignZeg%20introduces%20synthetic%20data%20and%20incorporates%0Amultiple%20background%20prototypes%20to%20allocate%20a%20more%20generalizable%20feature%20space.%0A%283%29%20Predictive%20Bias%20Correction.%20During%20the%20inference%20stage%2C%20AlignZeg%20uses%20a%0Aclass%20indicator%20to%20find%20potential%20unseen%20class%20proposals%20followed%20by%20a%0Aprediction%20postprocess%20to%20correct%20the%20prediction%20bias.%20Experiments%20demonstrate%0Athat%20AlignZeg%20markedly%20enhances%20zero-shot%20semantic%20segmentation%2C%20as%20shown%20by%20an%0Aaverage%203.8%25%20increase%20in%20hIoU%2C%20primarily%20attributed%20to%20a%207.1%25%20improvement%20in%0Aidentifying%20unseen%20classes%2C%20and%20we%20further%20validate%20that%20the%20improvement%20comes%0Afrom%20alleviating%20the%20objective%20misalignment%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05667v1&entry.124074799=Read"},
{"title": "Two Hands Are Better Than One: Resolving Hand to Hand Intersections via\n  Occupancy Networks", "author": "Maksym Ivashechkin and Oscar Mendez and Richard Bowden", "abstract": "  3D hand pose estimation from images has seen considerable interest from the\nliterature, with new methods improving overall 3D accuracy. One current\nchallenge is to address hand-to-hand interaction where self-occlusions and\nfinger articulation pose a significant problem to estimation. Little work has\napplied physical constraints that minimize the hand intersections that occur as\na result of noisy estimation. This work addresses the intersection of hands by\nexploiting an occupancy network that represents the hand's volume as a\ncontinuous manifold. This allows us to model the probability distribution of\npoints being inside a hand. We designed an intersection loss function to\nminimize the likelihood of hand-to-point intersections. Moreover, we propose a\nnew hand mesh parameterization that is superior to the commonly used MANO model\nin many respects including lower mesh complexity, underlying 3D skeleton\nextraction, watertightness, etc. On the benchmark InterHand2.6M dataset, the\nmodels trained using our intersection loss achieve better results than the\nstate-of-the-art by significantly decreasing the number of hand intersections\nwhile lowering the mean per-joint positional error. Additionally, we\ndemonstrate superior performance for 3D hand uplift on Re:InterHand and SMILE\ndatasets and show reduced hand-to-hand intersections for complex domains such\nas sign-language pose estimation.\n", "link": "http://arxiv.org/abs/2404.05414v1", "date": "2024-04-08", "relevancy": 2.1385, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two%20Hands%20Are%20Better%20Than%20One%3A%20Resolving%20Hand%20to%20Hand%20Intersections%20via%0A%20%20Occupancy%20Networks&body=Title%3A%20Two%20Hands%20Are%20Better%20Than%20One%3A%20Resolving%20Hand%20to%20Hand%20Intersections%20via%0A%20%20Occupancy%20Networks%0AAuthor%3A%20Maksym%20Ivashechkin%20and%20Oscar%20Mendez%20and%20Richard%20Bowden%0AAbstract%3A%20%20%203D%20hand%20pose%20estimation%20from%20images%20has%20seen%20considerable%20interest%20from%20the%0Aliterature%2C%20with%20new%20methods%20improving%20overall%203D%20accuracy.%20One%20current%0Achallenge%20is%20to%20address%20hand-to-hand%20interaction%20where%20self-occlusions%20and%0Afinger%20articulation%20pose%20a%20significant%20problem%20to%20estimation.%20Little%20work%20has%0Aapplied%20physical%20constraints%20that%20minimize%20the%20hand%20intersections%20that%20occur%20as%0Aa%20result%20of%20noisy%20estimation.%20This%20work%20addresses%20the%20intersection%20of%20hands%20by%0Aexploiting%20an%20occupancy%20network%20that%20represents%20the%20hand%27s%20volume%20as%20a%0Acontinuous%20manifold.%20This%20allows%20us%20to%20model%20the%20probability%20distribution%20of%0Apoints%20being%20inside%20a%20hand.%20We%20designed%20an%20intersection%20loss%20function%20to%0Aminimize%20the%20likelihood%20of%20hand-to-point%20intersections.%20Moreover%2C%20we%20propose%20a%0Anew%20hand%20mesh%20parameterization%20that%20is%20superior%20to%20the%20commonly%20used%20MANO%20model%0Ain%20many%20respects%20including%20lower%20mesh%20complexity%2C%20underlying%203D%20skeleton%0Aextraction%2C%20watertightness%2C%20etc.%20On%20the%20benchmark%20InterHand2.6M%20dataset%2C%20the%0Amodels%20trained%20using%20our%20intersection%20loss%20achieve%20better%20results%20than%20the%0Astate-of-the-art%20by%20significantly%20decreasing%20the%20number%20of%20hand%20intersections%0Awhile%20lowering%20the%20mean%20per-joint%20positional%20error.%20Additionally%2C%20we%0Ademonstrate%20superior%20performance%20for%203D%20hand%20uplift%20on%20Re%3AInterHand%20and%20SMILE%0Adatasets%20and%20show%20reduced%20hand-to-hand%20intersections%20for%20complex%20domains%20such%0Aas%20sign-language%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05414v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Hands%20Are%20Better%20Than%20One%3A%20Resolving%20Hand%20to%20Hand%20Intersections%20via%0A%20%20Occupancy%20Networks&entry.906535625=Maksym%20Ivashechkin%20and%20Oscar%20Mendez%20and%20Richard%20Bowden&entry.1292438233=%20%203D%20hand%20pose%20estimation%20from%20images%20has%20seen%20considerable%20interest%20from%20the%0Aliterature%2C%20with%20new%20methods%20improving%20overall%203D%20accuracy.%20One%20current%0Achallenge%20is%20to%20address%20hand-to-hand%20interaction%20where%20self-occlusions%20and%0Afinger%20articulation%20pose%20a%20significant%20problem%20to%20estimation.%20Little%20work%20has%0Aapplied%20physical%20constraints%20that%20minimize%20the%20hand%20intersections%20that%20occur%20as%0Aa%20result%20of%20noisy%20estimation.%20This%20work%20addresses%20the%20intersection%20of%20hands%20by%0Aexploiting%20an%20occupancy%20network%20that%20represents%20the%20hand%27s%20volume%20as%20a%0Acontinuous%20manifold.%20This%20allows%20us%20to%20model%20the%20probability%20distribution%20of%0Apoints%20being%20inside%20a%20hand.%20We%20designed%20an%20intersection%20loss%20function%20to%0Aminimize%20the%20likelihood%20of%20hand-to-point%20intersections.%20Moreover%2C%20we%20propose%20a%0Anew%20hand%20mesh%20parameterization%20that%20is%20superior%20to%20the%20commonly%20used%20MANO%20model%0Ain%20many%20respects%20including%20lower%20mesh%20complexity%2C%20underlying%203D%20skeleton%0Aextraction%2C%20watertightness%2C%20etc.%20On%20the%20benchmark%20InterHand2.6M%20dataset%2C%20the%0Amodels%20trained%20using%20our%20intersection%20loss%20achieve%20better%20results%20than%20the%0Astate-of-the-art%20by%20significantly%20decreasing%20the%20number%20of%20hand%20intersections%0Awhile%20lowering%20the%20mean%20per-joint%20positional%20error.%20Additionally%2C%20we%0Ademonstrate%20superior%20performance%20for%203D%20hand%20uplift%20on%20Re%3AInterHand%20and%20SMILE%0Adatasets%20and%20show%20reduced%20hand-to-hand%20intersections%20for%20complex%20domains%20such%0Aas%20sign-language%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05414v1&entry.124074799=Read"},
{"title": "Retrieval-Augmented Open-Vocabulary Object Detection", "author": "Jooyeon Kim and Eulrang Cho and Sehyung Kim and Hyunwoo J. Kim", "abstract": "  Open-vocabulary object detection (OVD) has been studied with Vision-Language\nModels (VLMs) to detect novel objects beyond the pre-trained categories.\nPrevious approaches improve the generalization ability to expand the knowledge\nof the detector, using 'positive' pseudo-labels with additional 'class' names,\ne.g., sock, iPod, and alligator. To extend the previous methods in two aspects,\nwe propose Retrieval-Augmented Losses and visual Features (RALF). Our method\nretrieves related 'negative' classes and augments loss functions. Also, visual\nfeatures are augmented with 'verbalized concepts' of classes, e.g., worn on the\nfeet, handheld music player, and sharp teeth. Specifically, RALF consists of\ntwo modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual\nFeatures (RAF). RAL constitutes two losses reflecting the semantic similarity\nwith negative vocabularies. In addition, RAF augments visual features with the\nverbalized concepts from a large language model (LLM). Our experiments\ndemonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We\nachieve improvement up to 3.4 box AP$_{50}^{\\text{N}}$ on novel categories of\nthe COCO dataset and 3.6 mask AP$_{\\text{r}}$ gains on the LVIS dataset. Code\nis available at https://github.com/mlvlab/RALF .\n", "link": "http://arxiv.org/abs/2404.05687v1", "date": "2024-04-08", "relevancy": 2.1017, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Open-Vocabulary%20Object%20Detection&body=Title%3A%20Retrieval-Augmented%20Open-Vocabulary%20Object%20Detection%0AAuthor%3A%20Jooyeon%20Kim%20and%20Eulrang%20Cho%20and%20Sehyung%20Kim%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20has%20been%20studied%20with%20Vision-Language%0AModels%20%28VLMs%29%20to%20detect%20novel%20objects%20beyond%20the%20pre-trained%20categories.%0APrevious%20approaches%20improve%20the%20generalization%20ability%20to%20expand%20the%20knowledge%0Aof%20the%20detector%2C%20using%20%27positive%27%20pseudo-labels%20with%20additional%20%27class%27%20names%2C%0Ae.g.%2C%20sock%2C%20iPod%2C%20and%20alligator.%20To%20extend%20the%20previous%20methods%20in%20two%20aspects%2C%0Awe%20propose%20Retrieval-Augmented%20Losses%20and%20visual%20Features%20%28RALF%29.%20Our%20method%0Aretrieves%20related%20%27negative%27%20classes%20and%20augments%20loss%20functions.%20Also%2C%20visual%0Afeatures%20are%20augmented%20with%20%27verbalized%20concepts%27%20of%20classes%2C%20e.g.%2C%20worn%20on%20the%0Afeet%2C%20handheld%20music%20player%2C%20and%20sharp%20teeth.%20Specifically%2C%20RALF%20consists%20of%0Atwo%20modules%3A%20Retrieval%20Augmented%20Losses%20%28RAL%29%20and%20Retrieval-Augmented%20visual%0AFeatures%20%28RAF%29.%20RAL%20constitutes%20two%20losses%20reflecting%20the%20semantic%20similarity%0Awith%20negative%20vocabularies.%20In%20addition%2C%20RAF%20augments%20visual%20features%20with%20the%0Averbalized%20concepts%20from%20a%20large%20language%20model%20%28LLM%29.%20Our%20experiments%0Ademonstrate%20the%20effectiveness%20of%20RALF%20on%20COCO%20and%20LVIS%20benchmark%20datasets.%20We%0Aachieve%20improvement%20up%20to%203.4%20box%20AP%24_%7B50%7D%5E%7B%5Ctext%7BN%7D%7D%24%20on%20novel%20categories%20of%0Athe%20COCO%20dataset%20and%203.6%20mask%20AP%24_%7B%5Ctext%7Br%7D%7D%24%20gains%20on%20the%20LVIS%20dataset.%20Code%0Ais%20available%20at%20https%3A//github.com/mlvlab/RALF%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Open-Vocabulary%20Object%20Detection&entry.906535625=Jooyeon%20Kim%20and%20Eulrang%20Cho%20and%20Sehyung%20Kim%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20has%20been%20studied%20with%20Vision-Language%0AModels%20%28VLMs%29%20to%20detect%20novel%20objects%20beyond%20the%20pre-trained%20categories.%0APrevious%20approaches%20improve%20the%20generalization%20ability%20to%20expand%20the%20knowledge%0Aof%20the%20detector%2C%20using%20%27positive%27%20pseudo-labels%20with%20additional%20%27class%27%20names%2C%0Ae.g.%2C%20sock%2C%20iPod%2C%20and%20alligator.%20To%20extend%20the%20previous%20methods%20in%20two%20aspects%2C%0Awe%20propose%20Retrieval-Augmented%20Losses%20and%20visual%20Features%20%28RALF%29.%20Our%20method%0Aretrieves%20related%20%27negative%27%20classes%20and%20augments%20loss%20functions.%20Also%2C%20visual%0Afeatures%20are%20augmented%20with%20%27verbalized%20concepts%27%20of%20classes%2C%20e.g.%2C%20worn%20on%20the%0Afeet%2C%20handheld%20music%20player%2C%20and%20sharp%20teeth.%20Specifically%2C%20RALF%20consists%20of%0Atwo%20modules%3A%20Retrieval%20Augmented%20Losses%20%28RAL%29%20and%20Retrieval-Augmented%20visual%0AFeatures%20%28RAF%29.%20RAL%20constitutes%20two%20losses%20reflecting%20the%20semantic%20similarity%0Awith%20negative%20vocabularies.%20In%20addition%2C%20RAF%20augments%20visual%20features%20with%20the%0Averbalized%20concepts%20from%20a%20large%20language%20model%20%28LLM%29.%20Our%20experiments%0Ademonstrate%20the%20effectiveness%20of%20RALF%20on%20COCO%20and%20LVIS%20benchmark%20datasets.%20We%0Aachieve%20improvement%20up%20to%203.4%20box%20AP%24_%7B50%7D%5E%7B%5Ctext%7BN%7D%7D%24%20on%20novel%20categories%20of%0Athe%20COCO%20dataset%20and%203.6%20mask%20AP%24_%7B%5Ctext%7Br%7D%7D%24%20gains%20on%20the%20LVIS%20dataset.%20Code%0Ais%20available%20at%20https%3A//github.com/mlvlab/RALF%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05687v1&entry.124074799=Read"},
{"title": "Anatomical Conditioning for Contrastive Unpaired Image-to-Image\n  Translation of Optical Coherence Tomography Images", "author": "Marc S. Seibel and Hristina Uzunova and Timo Kepp and Heinz Handels", "abstract": "  For a unified analysis of medical images from different modalities, data\nharmonization using image-to-image (I2I) translation is desired. We study this\nproblem employing an optical coherence tomography (OCT) data set of\nSpectralis-OCT and Home-OCT images. I2I translation is challenging because the\nimages are unpaired, and a bijective mapping does not exist due to the\ninformation discrepancy between both domains. This problem has been addressed\nby the Contrastive Learning for Unpaired I2I Translation (CUT) approach, but it\nreduces semantic consistency. To restore the semantic consistency, we support\nthe style decoder using an additional segmentation decoder. Our approach\nincreases the similarity between the style-translated images and the target\ndistribution. Importantly, we improve the segmentation of biomarkers in\nHome-OCT images in an unsupervised domain adaptation scenario. Our data\nharmonization approach provides potential for the monitoring of diseases, e.g.,\nage related macular disease, using different OCT devices.\n", "link": "http://arxiv.org/abs/2404.05409v1", "date": "2024-04-08", "relevancy": 2.0992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5285}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5022}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Conditioning%20for%20Contrastive%20Unpaired%20Image-to-Image%0A%20%20Translation%20of%20Optical%20Coherence%20Tomography%20Images&body=Title%3A%20Anatomical%20Conditioning%20for%20Contrastive%20Unpaired%20Image-to-Image%0A%20%20Translation%20of%20Optical%20Coherence%20Tomography%20Images%0AAuthor%3A%20Marc%20S.%20Seibel%20and%20Hristina%20Uzunova%20and%20Timo%20Kepp%20and%20Heinz%20Handels%0AAbstract%3A%20%20%20For%20a%20unified%20analysis%20of%20medical%20images%20from%20different%20modalities%2C%20data%0Aharmonization%20using%20image-to-image%20%28I2I%29%20translation%20is%20desired.%20We%20study%20this%0Aproblem%20employing%20an%20optical%20coherence%20tomography%20%28OCT%29%20data%20set%20of%0ASpectralis-OCT%20and%20Home-OCT%20images.%20I2I%20translation%20is%20challenging%20because%20the%0Aimages%20are%20unpaired%2C%20and%20a%20bijective%20mapping%20does%20not%20exist%20due%20to%20the%0Ainformation%20discrepancy%20between%20both%20domains.%20This%20problem%20has%20been%20addressed%0Aby%20the%20Contrastive%20Learning%20for%20Unpaired%20I2I%20Translation%20%28CUT%29%20approach%2C%20but%20it%0Areduces%20semantic%20consistency.%20To%20restore%20the%20semantic%20consistency%2C%20we%20support%0Athe%20style%20decoder%20using%20an%20additional%20segmentation%20decoder.%20Our%20approach%0Aincreases%20the%20similarity%20between%20the%20style-translated%20images%20and%20the%20target%0Adistribution.%20Importantly%2C%20we%20improve%20the%20segmentation%20of%20biomarkers%20in%0AHome-OCT%20images%20in%20an%20unsupervised%20domain%20adaptation%20scenario.%20Our%20data%0Aharmonization%20approach%20provides%20potential%20for%20the%20monitoring%20of%20diseases%2C%20e.g.%2C%0Aage%20related%20macular%20disease%2C%20using%20different%20OCT%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05409v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Conditioning%20for%20Contrastive%20Unpaired%20Image-to-Image%0A%20%20Translation%20of%20Optical%20Coherence%20Tomography%20Images&entry.906535625=Marc%20S.%20Seibel%20and%20Hristina%20Uzunova%20and%20Timo%20Kepp%20and%20Heinz%20Handels&entry.1292438233=%20%20For%20a%20unified%20analysis%20of%20medical%20images%20from%20different%20modalities%2C%20data%0Aharmonization%20using%20image-to-image%20%28I2I%29%20translation%20is%20desired.%20We%20study%20this%0Aproblem%20employing%20an%20optical%20coherence%20tomography%20%28OCT%29%20data%20set%20of%0ASpectralis-OCT%20and%20Home-OCT%20images.%20I2I%20translation%20is%20challenging%20because%20the%0Aimages%20are%20unpaired%2C%20and%20a%20bijective%20mapping%20does%20not%20exist%20due%20to%20the%0Ainformation%20discrepancy%20between%20both%20domains.%20This%20problem%20has%20been%20addressed%0Aby%20the%20Contrastive%20Learning%20for%20Unpaired%20I2I%20Translation%20%28CUT%29%20approach%2C%20but%20it%0Areduces%20semantic%20consistency.%20To%20restore%20the%20semantic%20consistency%2C%20we%20support%0Athe%20style%20decoder%20using%20an%20additional%20segmentation%20decoder.%20Our%20approach%0Aincreases%20the%20similarity%20between%20the%20style-translated%20images%20and%20the%20target%0Adistribution.%20Importantly%2C%20we%20improve%20the%20segmentation%20of%20biomarkers%20in%0AHome-OCT%20images%20in%20an%20unsupervised%20domain%20adaptation%20scenario.%20Our%20data%0Aharmonization%20approach%20provides%20potential%20for%20the%20monitoring%20of%20diseases%2C%20e.g.%2C%0Aage%20related%20macular%20disease%2C%20using%20different%20OCT%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05409v1&entry.124074799=Read"},
{"title": "SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane\n  Representation", "author": "Heyuan Li and Ce Chen and Tianhao Shi and Yuda Qiu and Sizhe An and Guanying Chen and Xiaoguang Han", "abstract": "  While recent advances in 3D-aware Generative Adversarial Networks (GANs) have\naided the development of near-frontal view human face synthesis, the challenge\nof comprehensively synthesizing a full 3D head viewable from all angles still\npersists. Although PanoHead proves the possibilities of using a large-scale\ndataset with images of both frontal and back views for full-head synthesis, it\noften causes artifacts for back views. Based on our in-depth analysis, we found\nthe reasons are mainly twofold. First, from network architecture perspective,\nwe found each plane in the utilized tri-plane/tri-grid representation space\ntends to confuse the features from both sides, causing \"mirroring\" artifacts\n(e.g., the glasses appear in the back). Second, from data supervision aspect,\nwe found that existing discriminator training in 3D GANs mainly focuses on the\nquality of the rendered image itself, and does not care much about its\nplausibility with the perspective from which it was rendered. This makes it\npossible to generate \"face\" in non-frontal views, due to its easiness to fool\nthe discriminator. In response, we propose SphereHead, a novel tri-plane\nrepresentation in the spherical coordinate system that fits the human head's\ngeometric characteristics and efficiently mitigates many of the generated\nartifacts. We further introduce a view-image consistency loss for the\ndiscriminator to emphasize the correspondence of the camera parameters and the\nimages. The combination of these efforts results in visually superior outcomes\nwith significantly fewer artifacts. Our code and dataset are publicly available\nat https://lhyfst.github.io/spherehead.\n", "link": "http://arxiv.org/abs/2404.05680v1", "date": "2024-04-08", "relevancy": 2.0855, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation&body=Title%3A%20SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation%0AAuthor%3A%20Heyuan%20Li%20and%20Ce%20Chen%20and%20Tianhao%20Shi%20and%20Yuda%20Qiu%20and%20Sizhe%20An%20and%20Guanying%20Chen%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20While%20recent%20advances%20in%203D-aware%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%0Aaided%20the%20development%20of%20near-frontal%20view%20human%20face%20synthesis%2C%20the%20challenge%0Aof%20comprehensively%20synthesizing%20a%20full%203D%20head%20viewable%20from%20all%20angles%20still%0Apersists.%20Although%20PanoHead%20proves%20the%20possibilities%20of%20using%20a%20large-scale%0Adataset%20with%20images%20of%20both%20frontal%20and%20back%20views%20for%20full-head%20synthesis%2C%20it%0Aoften%20causes%20artifacts%20for%20back%20views.%20Based%20on%20our%20in-depth%20analysis%2C%20we%20found%0Athe%20reasons%20are%20mainly%20twofold.%20First%2C%20from%20network%20architecture%20perspective%2C%0Awe%20found%20each%20plane%20in%20the%20utilized%20tri-plane/tri-grid%20representation%20space%0Atends%20to%20confuse%20the%20features%20from%20both%20sides%2C%20causing%20%22mirroring%22%20artifacts%0A%28e.g.%2C%20the%20glasses%20appear%20in%20the%20back%29.%20Second%2C%20from%20data%20supervision%20aspect%2C%0Awe%20found%20that%20existing%20discriminator%20training%20in%203D%20GANs%20mainly%20focuses%20on%20the%0Aquality%20of%20the%20rendered%20image%20itself%2C%20and%20does%20not%20care%20much%20about%20its%0Aplausibility%20with%20the%20perspective%20from%20which%20it%20was%20rendered.%20This%20makes%20it%0Apossible%20to%20generate%20%22face%22%20in%20non-frontal%20views%2C%20due%20to%20its%20easiness%20to%20fool%0Athe%20discriminator.%20In%20response%2C%20we%20propose%20SphereHead%2C%20a%20novel%20tri-plane%0Arepresentation%20in%20the%20spherical%20coordinate%20system%20that%20fits%20the%20human%20head%27s%0Ageometric%20characteristics%20and%20efficiently%20mitigates%20many%20of%20the%20generated%0Aartifacts.%20We%20further%20introduce%20a%20view-image%20consistency%20loss%20for%20the%0Adiscriminator%20to%20emphasize%20the%20correspondence%20of%20the%20camera%20parameters%20and%20the%0Aimages.%20The%20combination%20of%20these%20efforts%20results%20in%20visually%20superior%20outcomes%0Awith%20significantly%20fewer%20artifacts.%20Our%20code%20and%20dataset%20are%20publicly%20available%0Aat%20https%3A//lhyfst.github.io/spherehead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05680v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation&entry.906535625=Heyuan%20Li%20and%20Ce%20Chen%20and%20Tianhao%20Shi%20and%20Yuda%20Qiu%20and%20Sizhe%20An%20and%20Guanying%20Chen%20and%20Xiaoguang%20Han&entry.1292438233=%20%20While%20recent%20advances%20in%203D-aware%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%0Aaided%20the%20development%20of%20near-frontal%20view%20human%20face%20synthesis%2C%20the%20challenge%0Aof%20comprehensively%20synthesizing%20a%20full%203D%20head%20viewable%20from%20all%20angles%20still%0Apersists.%20Although%20PanoHead%20proves%20the%20possibilities%20of%20using%20a%20large-scale%0Adataset%20with%20images%20of%20both%20frontal%20and%20back%20views%20for%20full-head%20synthesis%2C%20it%0Aoften%20causes%20artifacts%20for%20back%20views.%20Based%20on%20our%20in-depth%20analysis%2C%20we%20found%0Athe%20reasons%20are%20mainly%20twofold.%20First%2C%20from%20network%20architecture%20perspective%2C%0Awe%20found%20each%20plane%20in%20the%20utilized%20tri-plane/tri-grid%20representation%20space%0Atends%20to%20confuse%20the%20features%20from%20both%20sides%2C%20causing%20%22mirroring%22%20artifacts%0A%28e.g.%2C%20the%20glasses%20appear%20in%20the%20back%29.%20Second%2C%20from%20data%20supervision%20aspect%2C%0Awe%20found%20that%20existing%20discriminator%20training%20in%203D%20GANs%20mainly%20focuses%20on%20the%0Aquality%20of%20the%20rendered%20image%20itself%2C%20and%20does%20not%20care%20much%20about%20its%0Aplausibility%20with%20the%20perspective%20from%20which%20it%20was%20rendered.%20This%20makes%20it%0Apossible%20to%20generate%20%22face%22%20in%20non-frontal%20views%2C%20due%20to%20its%20easiness%20to%20fool%0Athe%20discriminator.%20In%20response%2C%20we%20propose%20SphereHead%2C%20a%20novel%20tri-plane%0Arepresentation%20in%20the%20spherical%20coordinate%20system%20that%20fits%20the%20human%20head%27s%0Ageometric%20characteristics%20and%20efficiently%20mitigates%20many%20of%20the%20generated%0Aartifacts.%20We%20further%20introduce%20a%20view-image%20consistency%20loss%20for%20the%0Adiscriminator%20to%20emphasize%20the%20correspondence%20of%20the%20camera%20parameters%20and%20the%0Aimages.%20The%20combination%20of%20these%20efforts%20results%20in%20visually%20superior%20outcomes%0Awith%20significantly%20fewer%20artifacts.%20Our%20code%20and%20dataset%20are%20publicly%20available%0Aat%20https%3A//lhyfst.github.io/spherehead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05680v1&entry.124074799=Read"},
{"title": "SegForestNet: Spatial-Partitioning-Based Aerial Image Segmentation", "author": "Daniel Gritzner and J\u00f6rn Ostermann", "abstract": "  Aerial image segmentation is the basis for applications such as automatically\ncreating maps or tracking deforestation. In true orthophotos, which are often\nused in these applications, many objects and regions can be approximated well\nby polygons. However, this fact is rarely exploited by state-of-the-art\nsemantic segmentation models. Instead, most models allow unnecessary degrees of\nfreedom in their predictions by allowing arbitrary region shapes. We therefore\npresent a refinement of our deep learning model which predicts binary space\npartitioning trees, an efficient polygon representation. The refinements\ninclude a new feature decoder architecture and a new differentiable BSP tree\nrenderer which both avoid vanishing gradients. Additionally, we designed a\nnovel loss function specifically designed to improve the spatial partitioning\ndefined by the predicted trees. Furthermore, our expanded model can predict\nmultiple trees at once and thus can predict class-specific segmentations. As an\nadditional contribution, we investigate the impact of a non-optimal training\nprocess in comparison to an optimized training process. While model\narchitectures optimized for aerial images, such as PFNet or our own model, show\nan advantage under non-optimal conditions, this advantage disappears under\noptimal training conditions. Despite this observation, our model still makes\nbetter predictions for small rectangular objects, e.g., cars.\n", "link": "http://arxiv.org/abs/2302.01585v3", "date": "2024-04-08", "relevancy": 2.081, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5299}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SegForestNet%3A%20Spatial-Partitioning-Based%20Aerial%20Image%20Segmentation&body=Title%3A%20SegForestNet%3A%20Spatial-Partitioning-Based%20Aerial%20Image%20Segmentation%0AAuthor%3A%20Daniel%20Gritzner%20and%20J%C3%B6rn%20Ostermann%0AAbstract%3A%20%20%20Aerial%20image%20segmentation%20is%20the%20basis%20for%20applications%20such%20as%20automatically%0Acreating%20maps%20or%20tracking%20deforestation.%20In%20true%20orthophotos%2C%20which%20are%20often%0Aused%20in%20these%20applications%2C%20many%20objects%20and%20regions%20can%20be%20approximated%20well%0Aby%20polygons.%20However%2C%20this%20fact%20is%20rarely%20exploited%20by%20state-of-the-art%0Asemantic%20segmentation%20models.%20Instead%2C%20most%20models%20allow%20unnecessary%20degrees%20of%0Afreedom%20in%20their%20predictions%20by%20allowing%20arbitrary%20region%20shapes.%20We%20therefore%0Apresent%20a%20refinement%20of%20our%20deep%20learning%20model%20which%20predicts%20binary%20space%0Apartitioning%20trees%2C%20an%20efficient%20polygon%20representation.%20The%20refinements%0Ainclude%20a%20new%20feature%20decoder%20architecture%20and%20a%20new%20differentiable%20BSP%20tree%0Arenderer%20which%20both%20avoid%20vanishing%20gradients.%20Additionally%2C%20we%20designed%20a%0Anovel%20loss%20function%20specifically%20designed%20to%20improve%20the%20spatial%20partitioning%0Adefined%20by%20the%20predicted%20trees.%20Furthermore%2C%20our%20expanded%20model%20can%20predict%0Amultiple%20trees%20at%20once%20and%20thus%20can%20predict%20class-specific%20segmentations.%20As%20an%0Aadditional%20contribution%2C%20we%20investigate%20the%20impact%20of%20a%20non-optimal%20training%0Aprocess%20in%20comparison%20to%20an%20optimized%20training%20process.%20While%20model%0Aarchitectures%20optimized%20for%20aerial%20images%2C%20such%20as%20PFNet%20or%20our%20own%20model%2C%20show%0Aan%20advantage%20under%20non-optimal%20conditions%2C%20this%20advantage%20disappears%20under%0Aoptimal%20training%20conditions.%20Despite%20this%20observation%2C%20our%20model%20still%20makes%0Abetter%20predictions%20for%20small%20rectangular%20objects%2C%20e.g.%2C%20cars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.01585v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegForestNet%3A%20Spatial-Partitioning-Based%20Aerial%20Image%20Segmentation&entry.906535625=Daniel%20Gritzner%20and%20J%C3%B6rn%20Ostermann&entry.1292438233=%20%20Aerial%20image%20segmentation%20is%20the%20basis%20for%20applications%20such%20as%20automatically%0Acreating%20maps%20or%20tracking%20deforestation.%20In%20true%20orthophotos%2C%20which%20are%20often%0Aused%20in%20these%20applications%2C%20many%20objects%20and%20regions%20can%20be%20approximated%20well%0Aby%20polygons.%20However%2C%20this%20fact%20is%20rarely%20exploited%20by%20state-of-the-art%0Asemantic%20segmentation%20models.%20Instead%2C%20most%20models%20allow%20unnecessary%20degrees%20of%0Afreedom%20in%20their%20predictions%20by%20allowing%20arbitrary%20region%20shapes.%20We%20therefore%0Apresent%20a%20refinement%20of%20our%20deep%20learning%20model%20which%20predicts%20binary%20space%0Apartitioning%20trees%2C%20an%20efficient%20polygon%20representation.%20The%20refinements%0Ainclude%20a%20new%20feature%20decoder%20architecture%20and%20a%20new%20differentiable%20BSP%20tree%0Arenderer%20which%20both%20avoid%20vanishing%20gradients.%20Additionally%2C%20we%20designed%20a%0Anovel%20loss%20function%20specifically%20designed%20to%20improve%20the%20spatial%20partitioning%0Adefined%20by%20the%20predicted%20trees.%20Furthermore%2C%20our%20expanded%20model%20can%20predict%0Amultiple%20trees%20at%20once%20and%20thus%20can%20predict%20class-specific%20segmentations.%20As%20an%0Aadditional%20contribution%2C%20we%20investigate%20the%20impact%20of%20a%20non-optimal%20training%0Aprocess%20in%20comparison%20to%20an%20optimized%20training%20process.%20While%20model%0Aarchitectures%20optimized%20for%20aerial%20images%2C%20such%20as%20PFNet%20or%20our%20own%20model%2C%20show%0Aan%20advantage%20under%20non-optimal%20conditions%2C%20this%20advantage%20disappears%20under%0Aoptimal%20training%20conditions.%20Despite%20this%20observation%2C%20our%20model%20still%20makes%0Abetter%20predictions%20for%20small%20rectangular%20objects%2C%20e.g.%2C%20cars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01585v3&entry.124074799=Read"},
{"title": "Enhancing Lip Reading with Multi-Scale Video and Multi-Encoder", "author": "He Wang and Pengcheng Guo and Xucheng Wan and Huan Zhou and Lei Xie", "abstract": "  Automatic lip-reading (ALR) aims to automatically transcribe spoken content\nfrom a speaker's silent lip motion captured in video. Current mainstream\nlip-reading approaches only use a single visual encoder to model input videos\nof a single scale. In this paper, we propose to enhance lipreading by\nincorporating multi-scale video data and multi-encoder. Specifically, we first\npropose a novel multi-scale lip extraction algorithm based on the size of the\nspeaker's face and an enhanced ResNet3D visual front-end (VFE) to extract lip\nfeatures at different scales. For the multi-encoder, in addition to the\nmainstream Transformer and Conformer, we also incorporate the recently proposed\nBranchformer and EBranchformer as visual encoders. In the experiments, we\nexplore the influence of different video data scales and encoders on ALR system\nperformance and fuse the texts transcribed by all ALR systems using recognizer\noutput voting error reduction (ROVER). Finally, our proposed approach placed\nsecond in the ICME 2024 ChatCLR Challenge Task 2, with a 21.52% reduction in\ncharacter error rate (CER) compared to the official baseline on the evaluation\nset.\n", "link": "http://arxiv.org/abs/2404.05466v1", "date": "2024-04-08", "relevancy": 2.0794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4913}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder&body=Title%3A%20Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder%0AAuthor%3A%20He%20Wang%20and%20Pengcheng%20Guo%20and%20Xucheng%20Wan%20and%20Huan%20Zhou%20and%20Lei%20Xie%0AAbstract%3A%20%20%20Automatic%20lip-reading%20%28ALR%29%20aims%20to%20automatically%20transcribe%20spoken%20content%0Afrom%20a%20speaker%27s%20silent%20lip%20motion%20captured%20in%20video.%20Current%20mainstream%0Alip-reading%20approaches%20only%20use%20a%20single%20visual%20encoder%20to%20model%20input%20videos%0Aof%20a%20single%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20enhance%20lipreading%20by%0Aincorporating%20multi-scale%20video%20data%20and%20multi-encoder.%20Specifically%2C%20we%20first%0Apropose%20a%20novel%20multi-scale%20lip%20extraction%20algorithm%20based%20on%20the%20size%20of%20the%0Aspeaker%27s%20face%20and%20an%20enhanced%20ResNet3D%20visual%20front-end%20%28VFE%29%20to%20extract%20lip%0Afeatures%20at%20different%20scales.%20For%20the%20multi-encoder%2C%20in%20addition%20to%20the%0Amainstream%20Transformer%20and%20Conformer%2C%20we%20also%20incorporate%20the%20recently%20proposed%0ABranchformer%20and%20EBranchformer%20as%20visual%20encoders.%20In%20the%20experiments%2C%20we%0Aexplore%20the%20influence%20of%20different%20video%20data%20scales%20and%20encoders%20on%20ALR%20system%0Aperformance%20and%20fuse%20the%20texts%20transcribed%20by%20all%20ALR%20systems%20using%20recognizer%0Aoutput%20voting%20error%20reduction%20%28ROVER%29.%20Finally%2C%20our%20proposed%20approach%20placed%0Asecond%20in%20the%20ICME%202024%20ChatCLR%20Challenge%20Task%202%2C%20with%20a%2021.52%25%20reduction%20in%0Acharacter%20error%20rate%20%28CER%29%20compared%20to%20the%20official%20baseline%20on%20the%20evaluation%0Aset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05466v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder&entry.906535625=He%20Wang%20and%20Pengcheng%20Guo%20and%20Xucheng%20Wan%20and%20Huan%20Zhou%20and%20Lei%20Xie&entry.1292438233=%20%20Automatic%20lip-reading%20%28ALR%29%20aims%20to%20automatically%20transcribe%20spoken%20content%0Afrom%20a%20speaker%27s%20silent%20lip%20motion%20captured%20in%20video.%20Current%20mainstream%0Alip-reading%20approaches%20only%20use%20a%20single%20visual%20encoder%20to%20model%20input%20videos%0Aof%20a%20single%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20enhance%20lipreading%20by%0Aincorporating%20multi-scale%20video%20data%20and%20multi-encoder.%20Specifically%2C%20we%20first%0Apropose%20a%20novel%20multi-scale%20lip%20extraction%20algorithm%20based%20on%20the%20size%20of%20the%0Aspeaker%27s%20face%20and%20an%20enhanced%20ResNet3D%20visual%20front-end%20%28VFE%29%20to%20extract%20lip%0Afeatures%20at%20different%20scales.%20For%20the%20multi-encoder%2C%20in%20addition%20to%20the%0Amainstream%20Transformer%20and%20Conformer%2C%20we%20also%20incorporate%20the%20recently%20proposed%0ABranchformer%20and%20EBranchformer%20as%20visual%20encoders.%20In%20the%20experiments%2C%20we%0Aexplore%20the%20influence%20of%20different%20video%20data%20scales%20and%20encoders%20on%20ALR%20system%0Aperformance%20and%20fuse%20the%20texts%20transcribed%20by%20all%20ALR%20systems%20using%20recognizer%0Aoutput%20voting%20error%20reduction%20%28ROVER%29.%20Finally%2C%20our%20proposed%20approach%20placed%0Asecond%20in%20the%20ICME%202024%20ChatCLR%20Challenge%20Task%202%2C%20with%20a%2021.52%25%20reduction%20in%0Acharacter%20error%20rate%20%28CER%29%20compared%20to%20the%20official%20baseline%20on%20the%20evaluation%0Aset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05466v1&entry.124074799=Read"},
{"title": "Improving Algorithm-Selection and Performance-Prediction via Learning\n  Discriminating Training Samples", "author": "Quentin Renau and Emma Hart", "abstract": "  The choice of input-data used to train algorithm-selection models is\nrecognised as being a critical part of the model success. Recently,\nfeature-free methods for algorithm-selection that use short trajectories\nobtained from running a solver as input have shown promise. However, it is\nunclear to what extent these trajectories reliably discriminate between\nsolvers. We propose a meta approach to generating discriminatory trajectories\nwith respect to a portfolio of solvers. The algorithm-configuration tool irace\nis used to tune the parameters of a simple Simulated Annealing algorithm (SA)\nto produce trajectories that maximise the performance metrics of ML models\ntrained on this data. We show that when the trajectories obtained from the\ntuned SA algorithm are used in ML models for algorithm-selection and\nperformance prediction, we obtain significantly improved performance metrics\ncompared to models trained both on raw trajectory data and on exploratory\nlandscape features.\n", "link": "http://arxiv.org/abs/2404.05359v1", "date": "2024-04-08", "relevancy": 2.076, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5048}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4956}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Algorithm-Selection%20and%20Performance-Prediction%20via%20Learning%0A%20%20Discriminating%20Training%20Samples&body=Title%3A%20Improving%20Algorithm-Selection%20and%20Performance-Prediction%20via%20Learning%0A%20%20Discriminating%20Training%20Samples%0AAuthor%3A%20Quentin%20Renau%20and%20Emma%20Hart%0AAbstract%3A%20%20%20The%20choice%20of%20input-data%20used%20to%20train%20algorithm-selection%20models%20is%0Arecognised%20as%20being%20a%20critical%20part%20of%20the%20model%20success.%20Recently%2C%0Afeature-free%20methods%20for%20algorithm-selection%20that%20use%20short%20trajectories%0Aobtained%20from%20running%20a%20solver%20as%20input%20have%20shown%20promise.%20However%2C%20it%20is%0Aunclear%20to%20what%20extent%20these%20trajectories%20reliably%20discriminate%20between%0Asolvers.%20We%20propose%20a%20meta%20approach%20to%20generating%20discriminatory%20trajectories%0Awith%20respect%20to%20a%20portfolio%20of%20solvers.%20The%20algorithm-configuration%20tool%20irace%0Ais%20used%20to%20tune%20the%20parameters%20of%20a%20simple%20Simulated%20Annealing%20algorithm%20%28SA%29%0Ato%20produce%20trajectories%20that%20maximise%20the%20performance%20metrics%20of%20ML%20models%0Atrained%20on%20this%20data.%20We%20show%20that%20when%20the%20trajectories%20obtained%20from%20the%0Atuned%20SA%20algorithm%20are%20used%20in%20ML%20models%20for%20algorithm-selection%20and%0Aperformance%20prediction%2C%20we%20obtain%20significantly%20improved%20performance%20metrics%0Acompared%20to%20models%20trained%20both%20on%20raw%20trajectory%20data%20and%20on%20exploratory%0Alandscape%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05359v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Algorithm-Selection%20and%20Performance-Prediction%20via%20Learning%0A%20%20Discriminating%20Training%20Samples&entry.906535625=Quentin%20Renau%20and%20Emma%20Hart&entry.1292438233=%20%20The%20choice%20of%20input-data%20used%20to%20train%20algorithm-selection%20models%20is%0Arecognised%20as%20being%20a%20critical%20part%20of%20the%20model%20success.%20Recently%2C%0Afeature-free%20methods%20for%20algorithm-selection%20that%20use%20short%20trajectories%0Aobtained%20from%20running%20a%20solver%20as%20input%20have%20shown%20promise.%20However%2C%20it%20is%0Aunclear%20to%20what%20extent%20these%20trajectories%20reliably%20discriminate%20between%0Asolvers.%20We%20propose%20a%20meta%20approach%20to%20generating%20discriminatory%20trajectories%0Awith%20respect%20to%20a%20portfolio%20of%20solvers.%20The%20algorithm-configuration%20tool%20irace%0Ais%20used%20to%20tune%20the%20parameters%20of%20a%20simple%20Simulated%20Annealing%20algorithm%20%28SA%29%0Ato%20produce%20trajectories%20that%20maximise%20the%20performance%20metrics%20of%20ML%20models%0Atrained%20on%20this%20data.%20We%20show%20that%20when%20the%20trajectories%20obtained%20from%20the%0Atuned%20SA%20algorithm%20are%20used%20in%20ML%20models%20for%20algorithm-selection%20and%0Aperformance%20prediction%2C%20we%20obtain%20significantly%20improved%20performance%20metrics%0Acompared%20to%20models%20trained%20both%20on%20raw%20trajectory%20data%20and%20on%20exploratory%0Alandscape%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05359v1&entry.124074799=Read"},
{"title": "Dynamic Backtracking in GFlowNet: Enhancing Decision Steps with\n  Reward-Dependent Adjustment Mechanisms", "author": "Shuai Guo and Jielei Chu and Lei Zhu and Tianrui Li", "abstract": "  Generative Flow Networks (GFlowNets) are probabilistic models predicated on\nMarkov flows, employing specific amortization algorithms to learn stochastic\npolicies that generate compositional substances including biomolecules,\nchemical materials, and more. Demonstrating formidable prowess in generating\nhigh-performance biochemical molecules, GFlowNets accelerate the discovery of\nscientific substances, effectively circumventing the time-consuming,\nlabor-intensive, and costly shortcomings intrinsic to conventional material\ndiscovery. However, previous work often struggles to accumulate exploratory\nexperience and is prone to becoming disoriented within expansive sampling\nspaces. Attempts to address this issue, such as LS-GFN, are limited to local\ngreedy searches and lack broader global adjustments. This paper introduces a\nnovel GFlowNet variant, the Dynamic Backtracking GFN (DB-GFN), which enhances\nthe adaptability of decision-making steps through a reward-based dynamic\nbacktracking mechanism. DB-GFN permits backtracking during the network\nconstruction process according to the current state's reward value, thus\ncorrecting disadvantageous decisions and exploring alternative pathways during\nthe exploration process. Applied to generative tasks of biochemical molecules\nand genetic material sequences, DB-GFN surpasses existing GFlowNet models and\ntraditional reinforcement learning methods in terms of sample quality,\nexploration sample quantity, and training convergence speed. Furthermore, the\northogonal nature of DB-GFN suggests its potential as a powerful tool for\nfuture improvements in GFN networks, with the promise of integrating with other\nstrategies to achieve more efficient search performance.\n", "link": "http://arxiv.org/abs/2404.05576v1", "date": "2024-04-08", "relevancy": 2.0715, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5489}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5145}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Backtracking%20in%20GFlowNet%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms&body=Title%3A%20Dynamic%20Backtracking%20in%20GFlowNet%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms%0AAuthor%3A%20Shuai%20Guo%20and%20Jielei%20Chu%20and%20Lei%20Zhu%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20probabilistic%20models%20predicated%20on%0AMarkov%20flows%2C%20employing%20specific%20amortization%20algorithms%20to%20learn%20stochastic%0Apolicies%20that%20generate%20compositional%20substances%20including%20biomolecules%2C%0Achemical%20materials%2C%20and%20more.%20Demonstrating%20formidable%20prowess%20in%20generating%0Ahigh-performance%20biochemical%20molecules%2C%20GFlowNets%20accelerate%20the%20discovery%20of%0Ascientific%20substances%2C%20effectively%20circumventing%20the%20time-consuming%2C%0Alabor-intensive%2C%20and%20costly%20shortcomings%20intrinsic%20to%20conventional%20material%0Adiscovery.%20However%2C%20previous%20work%20often%20struggles%20to%20accumulate%20exploratory%0Aexperience%20and%20is%20prone%20to%20becoming%20disoriented%20within%20expansive%20sampling%0Aspaces.%20Attempts%20to%20address%20this%20issue%2C%20such%20as%20LS-GFN%2C%20are%20limited%20to%20local%0Agreedy%20searches%20and%20lack%20broader%20global%20adjustments.%20This%20paper%20introduces%20a%0Anovel%20GFlowNet%20variant%2C%20the%20Dynamic%20Backtracking%20GFN%20%28DB-GFN%29%2C%20which%20enhances%0Athe%20adaptability%20of%20decision-making%20steps%20through%20a%20reward-based%20dynamic%0Abacktracking%20mechanism.%20DB-GFN%20permits%20backtracking%20during%20the%20network%0Aconstruction%20process%20according%20to%20the%20current%20state%27s%20reward%20value%2C%20thus%0Acorrecting%20disadvantageous%20decisions%20and%20exploring%20alternative%20pathways%20during%0Athe%20exploration%20process.%20Applied%20to%20generative%20tasks%20of%20biochemical%20molecules%0Aand%20genetic%20material%20sequences%2C%20DB-GFN%20surpasses%20existing%20GFlowNet%20models%20and%0Atraditional%20reinforcement%20learning%20methods%20in%20terms%20of%20sample%20quality%2C%0Aexploration%20sample%20quantity%2C%20and%20training%20convergence%20speed.%20Furthermore%2C%20the%0Aorthogonal%20nature%20of%20DB-GFN%20suggests%20its%20potential%20as%20a%20powerful%20tool%20for%0Afuture%20improvements%20in%20GFN%20networks%2C%20with%20the%20promise%20of%20integrating%20with%20other%0Astrategies%20to%20achieve%20more%20efficient%20search%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05576v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Backtracking%20in%20GFlowNet%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms&entry.906535625=Shuai%20Guo%20and%20Jielei%20Chu%20and%20Lei%20Zhu%20and%20Tianrui%20Li&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20probabilistic%20models%20predicated%20on%0AMarkov%20flows%2C%20employing%20specific%20amortization%20algorithms%20to%20learn%20stochastic%0Apolicies%20that%20generate%20compositional%20substances%20including%20biomolecules%2C%0Achemical%20materials%2C%20and%20more.%20Demonstrating%20formidable%20prowess%20in%20generating%0Ahigh-performance%20biochemical%20molecules%2C%20GFlowNets%20accelerate%20the%20discovery%20of%0Ascientific%20substances%2C%20effectively%20circumventing%20the%20time-consuming%2C%0Alabor-intensive%2C%20and%20costly%20shortcomings%20intrinsic%20to%20conventional%20material%0Adiscovery.%20However%2C%20previous%20work%20often%20struggles%20to%20accumulate%20exploratory%0Aexperience%20and%20is%20prone%20to%20becoming%20disoriented%20within%20expansive%20sampling%0Aspaces.%20Attempts%20to%20address%20this%20issue%2C%20such%20as%20LS-GFN%2C%20are%20limited%20to%20local%0Agreedy%20searches%20and%20lack%20broader%20global%20adjustments.%20This%20paper%20introduces%20a%0Anovel%20GFlowNet%20variant%2C%20the%20Dynamic%20Backtracking%20GFN%20%28DB-GFN%29%2C%20which%20enhances%0Athe%20adaptability%20of%20decision-making%20steps%20through%20a%20reward-based%20dynamic%0Abacktracking%20mechanism.%20DB-GFN%20permits%20backtracking%20during%20the%20network%0Aconstruction%20process%20according%20to%20the%20current%20state%27s%20reward%20value%2C%20thus%0Acorrecting%20disadvantageous%20decisions%20and%20exploring%20alternative%20pathways%20during%0Athe%20exploration%20process.%20Applied%20to%20generative%20tasks%20of%20biochemical%20molecules%0Aand%20genetic%20material%20sequences%2C%20DB-GFN%20surpasses%20existing%20GFlowNet%20models%20and%0Atraditional%20reinforcement%20learning%20methods%20in%20terms%20of%20sample%20quality%2C%0Aexploration%20sample%20quantity%2C%20and%20training%20convergence%20speed.%20Furthermore%2C%20the%0Aorthogonal%20nature%20of%20DB-GFN%20suggests%20its%20potential%20as%20a%20powerful%20tool%20for%0Afuture%20improvements%20in%20GFN%20networks%2C%20with%20the%20promise%20of%20integrating%20with%20other%0Astrategies%20to%20achieve%20more%20efficient%20search%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05576v1&entry.124074799=Read"},
{"title": "Rethinking Dimensional Rationale in Graph Contrastive Learning from\n  Causal Perspective", "author": "Qirui Ji and Jiangmeng Li and Jie Hu and Rui Wang and Changwen Zheng and Fanjiang Xu", "abstract": "  Graph contrastive learning is a general learning paradigm excelling at\ncapturing invariant information from diverse perturbations in graphs. Recent\nworks focus on exploring the structural rationale from graphs, thereby\nincreasing the discriminability of the invariant information. However, such\nmethods may incur in the mis-learning of graph models towards the\ninterpretability of graphs, and thus the learned noisy and task-agnostic\ninformation interferes with the prediction of graphs. To this end, with the\npurpose of exploring the intrinsic rationale of graphs, we accordingly propose\nto capture the dimensional rationale from graphs, which has not received\nsufficient attention in the literature. The conducted exploratory experiments\nattest to the feasibility of the aforementioned roadmap. To elucidate the\ninnate mechanism behind the performance improvement arising from the\ndimensional rationale, we rethink the dimensional rationale in graph\ncontrastive learning from a causal perspective and further formalize the\ncausality among the variables in the pre-training stage to build the\ncorresponding structural causal model. On the basis of the understanding of the\nstructural causal model, we propose the dimensional rationale-aware graph\ncontrastive learning approach, which introduces a learnable dimensional\nrationale acquiring network and a redundancy reduction constraint. The\nlearnable dimensional rationale acquiring network is updated by leveraging a\nbi-level meta-learning technique, and the redundancy reduction constraint\ndisentangles the redundant features through a decorrelation process during\nlearning. Empirically, compared with state-of-the-art methods, our method can\nyield significant performance boosts on various benchmarks with respect to\ndiscriminability and transferability. The code implementation of our method is\navailable at https://github.com/ByronJi/DRGCL.\n", "link": "http://arxiv.org/abs/2312.10401v3", "date": "2024-04-08", "relevancy": 2.0707, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Dimensional%20Rationale%20in%20Graph%20Contrastive%20Learning%20from%0A%20%20Causal%20Perspective&body=Title%3A%20Rethinking%20Dimensional%20Rationale%20in%20Graph%20Contrastive%20Learning%20from%0A%20%20Causal%20Perspective%0AAuthor%3A%20Qirui%20Ji%20and%20Jiangmeng%20Li%20and%20Jie%20Hu%20and%20Rui%20Wang%20and%20Changwen%20Zheng%20and%20Fanjiang%20Xu%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20is%20a%20general%20learning%20paradigm%20excelling%20at%0Acapturing%20invariant%20information%20from%20diverse%20perturbations%20in%20graphs.%20Recent%0Aworks%20focus%20on%20exploring%20the%20structural%20rationale%20from%20graphs%2C%20thereby%0Aincreasing%20the%20discriminability%20of%20the%20invariant%20information.%20However%2C%20such%0Amethods%20may%20incur%20in%20the%20mis-learning%20of%20graph%20models%20towards%20the%0Ainterpretability%20of%20graphs%2C%20and%20thus%20the%20learned%20noisy%20and%20task-agnostic%0Ainformation%20interferes%20with%20the%20prediction%20of%20graphs.%20To%20this%20end%2C%20with%20the%0Apurpose%20of%20exploring%20the%20intrinsic%20rationale%20of%20graphs%2C%20we%20accordingly%20propose%0Ato%20capture%20the%20dimensional%20rationale%20from%20graphs%2C%20which%20has%20not%20received%0Asufficient%20attention%20in%20the%20literature.%20The%20conducted%20exploratory%20experiments%0Aattest%20to%20the%20feasibility%20of%20the%20aforementioned%20roadmap.%20To%20elucidate%20the%0Ainnate%20mechanism%20behind%20the%20performance%20improvement%20arising%20from%20the%0Adimensional%20rationale%2C%20we%20rethink%20the%20dimensional%20rationale%20in%20graph%0Acontrastive%20learning%20from%20a%20causal%20perspective%20and%20further%20formalize%20the%0Acausality%20among%20the%20variables%20in%20the%20pre-training%20stage%20to%20build%20the%0Acorresponding%20structural%20causal%20model.%20On%20the%20basis%20of%20the%20understanding%20of%20the%0Astructural%20causal%20model%2C%20we%20propose%20the%20dimensional%20rationale-aware%20graph%0Acontrastive%20learning%20approach%2C%20which%20introduces%20a%20learnable%20dimensional%0Arationale%20acquiring%20network%20and%20a%20redundancy%20reduction%20constraint.%20The%0Alearnable%20dimensional%20rationale%20acquiring%20network%20is%20updated%20by%20leveraging%20a%0Abi-level%20meta-learning%20technique%2C%20and%20the%20redundancy%20reduction%20constraint%0Adisentangles%20the%20redundant%20features%20through%20a%20decorrelation%20process%20during%0Alearning.%20Empirically%2C%20compared%20with%20state-of-the-art%20methods%2C%20our%20method%20can%0Ayield%20significant%20performance%20boosts%20on%20various%20benchmarks%20with%20respect%20to%0Adiscriminability%20and%20transferability.%20The%20code%20implementation%20of%20our%20method%20is%0Aavailable%20at%20https%3A//github.com/ByronJi/DRGCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10401v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Dimensional%20Rationale%20in%20Graph%20Contrastive%20Learning%20from%0A%20%20Causal%20Perspective&entry.906535625=Qirui%20Ji%20and%20Jiangmeng%20Li%20and%20Jie%20Hu%20and%20Rui%20Wang%20and%20Changwen%20Zheng%20and%20Fanjiang%20Xu&entry.1292438233=%20%20Graph%20contrastive%20learning%20is%20a%20general%20learning%20paradigm%20excelling%20at%0Acapturing%20invariant%20information%20from%20diverse%20perturbations%20in%20graphs.%20Recent%0Aworks%20focus%20on%20exploring%20the%20structural%20rationale%20from%20graphs%2C%20thereby%0Aincreasing%20the%20discriminability%20of%20the%20invariant%20information.%20However%2C%20such%0Amethods%20may%20incur%20in%20the%20mis-learning%20of%20graph%20models%20towards%20the%0Ainterpretability%20of%20graphs%2C%20and%20thus%20the%20learned%20noisy%20and%20task-agnostic%0Ainformation%20interferes%20with%20the%20prediction%20of%20graphs.%20To%20this%20end%2C%20with%20the%0Apurpose%20of%20exploring%20the%20intrinsic%20rationale%20of%20graphs%2C%20we%20accordingly%20propose%0Ato%20capture%20the%20dimensional%20rationale%20from%20graphs%2C%20which%20has%20not%20received%0Asufficient%20attention%20in%20the%20literature.%20The%20conducted%20exploratory%20experiments%0Aattest%20to%20the%20feasibility%20of%20the%20aforementioned%20roadmap.%20To%20elucidate%20the%0Ainnate%20mechanism%20behind%20the%20performance%20improvement%20arising%20from%20the%0Adimensional%20rationale%2C%20we%20rethink%20the%20dimensional%20rationale%20in%20graph%0Acontrastive%20learning%20from%20a%20causal%20perspective%20and%20further%20formalize%20the%0Acausality%20among%20the%20variables%20in%20the%20pre-training%20stage%20to%20build%20the%0Acorresponding%20structural%20causal%20model.%20On%20the%20basis%20of%20the%20understanding%20of%20the%0Astructural%20causal%20model%2C%20we%20propose%20the%20dimensional%20rationale-aware%20graph%0Acontrastive%20learning%20approach%2C%20which%20introduces%20a%20learnable%20dimensional%0Arationale%20acquiring%20network%20and%20a%20redundancy%20reduction%20constraint.%20The%0Alearnable%20dimensional%20rationale%20acquiring%20network%20is%20updated%20by%20leveraging%20a%0Abi-level%20meta-learning%20technique%2C%20and%20the%20redundancy%20reduction%20constraint%0Adisentangles%20the%20redundant%20features%20through%20a%20decorrelation%20process%20during%0Alearning.%20Empirically%2C%20compared%20with%20state-of-the-art%20methods%2C%20our%20method%20can%0Ayield%20significant%20performance%20boosts%20on%20various%20benchmarks%20with%20respect%20to%0Adiscriminability%20and%20transferability.%20The%20code%20implementation%20of%20our%20method%20is%0Aavailable%20at%20https%3A//github.com/ByronJi/DRGCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10401v3&entry.124074799=Read"},
{"title": "On the Convergence of Continual Learning with Adaptive Methods", "author": "Seungyub Han and Yeongmo Kim and Taehyun Cho and Jungwoo Lee", "abstract": "  One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.\n", "link": "http://arxiv.org/abs/2404.05555v1", "date": "2024-04-08", "relevancy": 2.0662, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5165}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5041}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Continual%20Learning%20with%20Adaptive%20Methods&body=Title%3A%20On%20the%20Convergence%20of%20Continual%20Learning%20with%20Adaptive%20Methods%0AAuthor%3A%20Seungyub%20Han%20and%20Yeongmo%20Kim%20and%20Taehyun%20Cho%20and%20Jungwoo%20Lee%0AAbstract%3A%20%20%20One%20of%20the%20objectives%20of%20continual%20learning%20is%20to%20prevent%20catastrophic%0Aforgetting%20in%20learning%20multiple%20tasks%20sequentially%2C%20and%20the%20existing%20solutions%0Ahave%20been%20driven%20by%20the%20conceptualization%20of%20the%20plasticity-stability%20dilemma.%0AHowever%2C%20the%20convergence%20of%20continual%20learning%20for%20each%20sequential%20task%20is%20less%0Astudied%20so%20far.%20In%20this%20paper%2C%20we%20provide%20a%20convergence%20analysis%20of%0Amemory-based%20continual%20learning%20with%20stochastic%20gradient%20descent%20and%20empirical%0Aevidence%20that%20training%20current%20tasks%20causes%20the%20cumulative%20degradation%20of%0Aprevious%20tasks.%20We%20propose%20an%20adaptive%20method%20for%20nonconvex%20continual%20learning%0A%28NCCL%29%2C%20which%20adjusts%20step%20sizes%20of%20both%20previous%20and%20current%20tasks%20with%20the%0Agradients.%20The%20proposed%20method%20can%20achieve%20the%20same%20convergence%20rate%20as%20the%20SGD%0Amethod%20when%20the%20catastrophic%20forgetting%20term%20which%20we%20define%20in%20the%20paper%20is%0Asuppressed%20at%20each%20iteration.%20Further%2C%20we%20demonstrate%20that%20the%20proposed%0Aalgorithm%20improves%20the%20performance%20of%20continual%20learning%20over%20existing%20methods%0Afor%20several%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05555v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Continual%20Learning%20with%20Adaptive%20Methods&entry.906535625=Seungyub%20Han%20and%20Yeongmo%20Kim%20and%20Taehyun%20Cho%20and%20Jungwoo%20Lee&entry.1292438233=%20%20One%20of%20the%20objectives%20of%20continual%20learning%20is%20to%20prevent%20catastrophic%0Aforgetting%20in%20learning%20multiple%20tasks%20sequentially%2C%20and%20the%20existing%20solutions%0Ahave%20been%20driven%20by%20the%20conceptualization%20of%20the%20plasticity-stability%20dilemma.%0AHowever%2C%20the%20convergence%20of%20continual%20learning%20for%20each%20sequential%20task%20is%20less%0Astudied%20so%20far.%20In%20this%20paper%2C%20we%20provide%20a%20convergence%20analysis%20of%0Amemory-based%20continual%20learning%20with%20stochastic%20gradient%20descent%20and%20empirical%0Aevidence%20that%20training%20current%20tasks%20causes%20the%20cumulative%20degradation%20of%0Aprevious%20tasks.%20We%20propose%20an%20adaptive%20method%20for%20nonconvex%20continual%20learning%0A%28NCCL%29%2C%20which%20adjusts%20step%20sizes%20of%20both%20previous%20and%20current%20tasks%20with%20the%0Agradients.%20The%20proposed%20method%20can%20achieve%20the%20same%20convergence%20rate%20as%20the%20SGD%0Amethod%20when%20the%20catastrophic%20forgetting%20term%20which%20we%20define%20in%20the%20paper%20is%0Asuppressed%20at%20each%20iteration.%20Further%2C%20we%20demonstrate%20that%20the%20proposed%0Aalgorithm%20improves%20the%20performance%20of%20continual%20learning%20over%20existing%20methods%0Afor%20several%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05555v1&entry.124074799=Read"},
{"title": "PEEB: Part-based Image Classifiers with an Explainable and Editable\n  Language Bottleneck", "author": "Thang M. Pham and Peijie Chen and Tin Nguyen and Seunghyun Yoon and Trung Bui and Anh Nguyen", "abstract": "  CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. Therefore, they perform poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of text\ndescriptors that describe the visual parts of that class; and (2) match the\nembeddings of the detected parts to their textual descriptors in each class to\ncompute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a huge margin (~10x in top-1\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20%\naccuracy on CUB-200 and Dogs-120, respectively) but also the first to enable\nusers to edit the text descriptors to form a new classifier without any\nre-training. Compared to concept bottleneck models, PEEB is also the SOTA in\nboth zero-shot and supervised-learning settings.\n", "link": "http://arxiv.org/abs/2403.05297v2", "date": "2024-04-08", "relevancy": 2.0627, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PEEB%3A%20Part-based%20Image%20Classifiers%20with%20an%20Explainable%20and%20Editable%0A%20%20Language%20Bottleneck&body=Title%3A%20PEEB%3A%20Part-based%20Image%20Classifiers%20with%20an%20Explainable%20and%20Editable%0A%20%20Language%20Bottleneck%0AAuthor%3A%20Thang%20M.%20Pham%20and%20Peijie%20Chen%20and%20Tin%20Nguyen%20and%20Seunghyun%20Yoon%20and%20Trung%20Bui%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20CLIP-based%20classifiers%20rely%20on%20the%20prompt%20containing%20a%20%7Bclass%20name%7D%20that%20is%0Aknown%20to%20the%20text%20encoder.%20Therefore%2C%20they%20perform%20poorly%20on%20new%20classes%20or%20the%0Aclasses%20whose%20names%20rarely%20appear%20on%20the%20Internet%20%28e.g.%2C%20scientific%20names%20of%0Abirds%29.%20For%20fine-grained%20classification%2C%20we%20propose%20PEEB%20-%20an%20explainable%20and%0Aeditable%20classifier%20to%20%281%29%20express%20the%20class%20name%20into%20a%20set%20of%20text%0Adescriptors%20that%20describe%20the%20visual%20parts%20of%20that%20class%3B%20and%20%282%29%20match%20the%0Aembeddings%20of%20the%20detected%20parts%20to%20their%20textual%20descriptors%20in%20each%20class%20to%0Acompute%20a%20logit%20score%20for%20classification.%20In%20a%20zero-shot%20setting%20where%20the%0Aclass%20names%20are%20unknown%2C%20PEEB%20outperforms%20CLIP%20by%20a%20huge%20margin%20%28~10x%20in%20top-1%0Aaccuracy%29.%20Compared%20to%20part-based%20classifiers%2C%20PEEB%20is%20not%20only%20the%0Astate-of-the-art%20%28SOTA%29%20on%20the%20supervised-learning%20setting%20%2888.80%25%20and%2092.20%25%0Aaccuracy%20on%20CUB-200%20and%20Dogs-120%2C%20respectively%29%20but%20also%20the%20first%20to%20enable%0Ausers%20to%20edit%20the%20text%20descriptors%20to%20form%20a%20new%20classifier%20without%20any%0Are-training.%20Compared%20to%20concept%20bottleneck%20models%2C%20PEEB%20is%20also%20the%20SOTA%20in%0Aboth%20zero-shot%20and%20supervised-learning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05297v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEEB%3A%20Part-based%20Image%20Classifiers%20with%20an%20Explainable%20and%20Editable%0A%20%20Language%20Bottleneck&entry.906535625=Thang%20M.%20Pham%20and%20Peijie%20Chen%20and%20Tin%20Nguyen%20and%20Seunghyun%20Yoon%20and%20Trung%20Bui%20and%20Anh%20Nguyen&entry.1292438233=%20%20CLIP-based%20classifiers%20rely%20on%20the%20prompt%20containing%20a%20%7Bclass%20name%7D%20that%20is%0Aknown%20to%20the%20text%20encoder.%20Therefore%2C%20they%20perform%20poorly%20on%20new%20classes%20or%20the%0Aclasses%20whose%20names%20rarely%20appear%20on%20the%20Internet%20%28e.g.%2C%20scientific%20names%20of%0Abirds%29.%20For%20fine-grained%20classification%2C%20we%20propose%20PEEB%20-%20an%20explainable%20and%0Aeditable%20classifier%20to%20%281%29%20express%20the%20class%20name%20into%20a%20set%20of%20text%0Adescriptors%20that%20describe%20the%20visual%20parts%20of%20that%20class%3B%20and%20%282%29%20match%20the%0Aembeddings%20of%20the%20detected%20parts%20to%20their%20textual%20descriptors%20in%20each%20class%20to%0Acompute%20a%20logit%20score%20for%20classification.%20In%20a%20zero-shot%20setting%20where%20the%0Aclass%20names%20are%20unknown%2C%20PEEB%20outperforms%20CLIP%20by%20a%20huge%20margin%20%28~10x%20in%20top-1%0Aaccuracy%29.%20Compared%20to%20part-based%20classifiers%2C%20PEEB%20is%20not%20only%20the%0Astate-of-the-art%20%28SOTA%29%20on%20the%20supervised-learning%20setting%20%2888.80%25%20and%2092.20%25%0Aaccuracy%20on%20CUB-200%20and%20Dogs-120%2C%20respectively%29%20but%20also%20the%20first%20to%20enable%0Ausers%20to%20edit%20the%20text%20descriptors%20to%20form%20a%20new%20classifier%20without%20any%0Are-training.%20Compared%20to%20concept%20bottleneck%20models%2C%20PEEB%20is%20also%20the%20SOTA%20in%0Aboth%20zero-shot%20and%20supervised-learning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05297v2&entry.124074799=Read"},
{"title": "MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning", "author": "Matteo Farina and Massimiliano Mancini and Elia Cunegatti and Gaowen Liu and Giovanni Iacca and Elisa Ricci", "abstract": "  While excellent in transfer learning, Vision-Language models (VLMs) come with\nhigh computational costs due to their large number of parameters. To address\nthis issue, removing parameters via model pruning is a viable solution.\nHowever, existing techniques for VLMs are task-specific, and thus require\npruning the network from scratch for each new task of interest. In this work,\nwe explore a new direction: Task-Agnostic Vision-Language Pruning (TA-VLP).\nGiven a pretrained VLM, the goal is to find a unique pruned counterpart\ntransferable to multiple unknown downstream tasks. In this challenging setting,\nthe transferable representations already encoded in the pretrained model are a\nkey aspect to preserve. Thus, we propose Multimodal Flow Pruning (MULTIFLOW), a\nfirst, gradient-free, pruning framework for TA-VLP where: (i) the importance of\na parameter is expressed in terms of its magnitude and its information flow, by\nincorporating the saliency of the neurons it connects; and (ii) pruning is\ndriven by the emergent (multimodal) distribution of the VLM parameters after\npretraining. We benchmark eight state-of-the-art pruning algorithms in the\ncontext of TA-VLP, experimenting with two VLMs, three vision-language tasks,\nand three pruning ratios. Our experimental results show that MULTIFLOW\noutperforms recent sophisticated, combinatorial competitors in the vast\nmajority of the cases, paving the way towards addressing TA-VLP. The code is\npublicly available at https://github.com/FarinaMatteo/multiflow.\n", "link": "http://arxiv.org/abs/2404.05621v1", "date": "2024-04-08", "relevancy": 2.0609, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5417}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5275}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MULTIFLOW%3A%20Shifting%20Towards%20Task-Agnostic%20Vision-Language%20Pruning&body=Title%3A%20MULTIFLOW%3A%20Shifting%20Towards%20Task-Agnostic%20Vision-Language%20Pruning%0AAuthor%3A%20Matteo%20Farina%20and%20Massimiliano%20Mancini%20and%20Elia%20Cunegatti%20and%20Gaowen%20Liu%20and%20Giovanni%20Iacca%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20While%20excellent%20in%20transfer%20learning%2C%20Vision-Language%20models%20%28VLMs%29%20come%20with%0Ahigh%20computational%20costs%20due%20to%20their%20large%20number%20of%20parameters.%20To%20address%0Athis%20issue%2C%20removing%20parameters%20via%20model%20pruning%20is%20a%20viable%20solution.%0AHowever%2C%20existing%20techniques%20for%20VLMs%20are%20task-specific%2C%20and%20thus%20require%0Apruning%20the%20network%20from%20scratch%20for%20each%20new%20task%20of%20interest.%20In%20this%20work%2C%0Awe%20explore%20a%20new%20direction%3A%20Task-Agnostic%20Vision-Language%20Pruning%20%28TA-VLP%29.%0AGiven%20a%20pretrained%20VLM%2C%20the%20goal%20is%20to%20find%20a%20unique%20pruned%20counterpart%0Atransferable%20to%20multiple%20unknown%20downstream%20tasks.%20In%20this%20challenging%20setting%2C%0Athe%20transferable%20representations%20already%20encoded%20in%20the%20pretrained%20model%20are%20a%0Akey%20aspect%20to%20preserve.%20Thus%2C%20we%20propose%20Multimodal%20Flow%20Pruning%20%28MULTIFLOW%29%2C%20a%0Afirst%2C%20gradient-free%2C%20pruning%20framework%20for%20TA-VLP%20where%3A%20%28i%29%20the%20importance%20of%0Aa%20parameter%20is%20expressed%20in%20terms%20of%20its%20magnitude%20and%20its%20information%20flow%2C%20by%0Aincorporating%20the%20saliency%20of%20the%20neurons%20it%20connects%3B%20and%20%28ii%29%20pruning%20is%0Adriven%20by%20the%20emergent%20%28multimodal%29%20distribution%20of%20the%20VLM%20parameters%20after%0Apretraining.%20We%20benchmark%20eight%20state-of-the-art%20pruning%20algorithms%20in%20the%0Acontext%20of%20TA-VLP%2C%20experimenting%20with%20two%20VLMs%2C%20three%20vision-language%20tasks%2C%0Aand%20three%20pruning%20ratios.%20Our%20experimental%20results%20show%20that%20MULTIFLOW%0Aoutperforms%20recent%20sophisticated%2C%20combinatorial%20competitors%20in%20the%20vast%0Amajority%20of%20the%20cases%2C%20paving%20the%20way%20towards%20addressing%20TA-VLP.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/FarinaMatteo/multiflow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05621v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULTIFLOW%3A%20Shifting%20Towards%20Task-Agnostic%20Vision-Language%20Pruning&entry.906535625=Matteo%20Farina%20and%20Massimiliano%20Mancini%20and%20Elia%20Cunegatti%20and%20Gaowen%20Liu%20and%20Giovanni%20Iacca%20and%20Elisa%20Ricci&entry.1292438233=%20%20While%20excellent%20in%20transfer%20learning%2C%20Vision-Language%20models%20%28VLMs%29%20come%20with%0Ahigh%20computational%20costs%20due%20to%20their%20large%20number%20of%20parameters.%20To%20address%0Athis%20issue%2C%20removing%20parameters%20via%20model%20pruning%20is%20a%20viable%20solution.%0AHowever%2C%20existing%20techniques%20for%20VLMs%20are%20task-specific%2C%20and%20thus%20require%0Apruning%20the%20network%20from%20scratch%20for%20each%20new%20task%20of%20interest.%20In%20this%20work%2C%0Awe%20explore%20a%20new%20direction%3A%20Task-Agnostic%20Vision-Language%20Pruning%20%28TA-VLP%29.%0AGiven%20a%20pretrained%20VLM%2C%20the%20goal%20is%20to%20find%20a%20unique%20pruned%20counterpart%0Atransferable%20to%20multiple%20unknown%20downstream%20tasks.%20In%20this%20challenging%20setting%2C%0Athe%20transferable%20representations%20already%20encoded%20in%20the%20pretrained%20model%20are%20a%0Akey%20aspect%20to%20preserve.%20Thus%2C%20we%20propose%20Multimodal%20Flow%20Pruning%20%28MULTIFLOW%29%2C%20a%0Afirst%2C%20gradient-free%2C%20pruning%20framework%20for%20TA-VLP%20where%3A%20%28i%29%20the%20importance%20of%0Aa%20parameter%20is%20expressed%20in%20terms%20of%20its%20magnitude%20and%20its%20information%20flow%2C%20by%0Aincorporating%20the%20saliency%20of%20the%20neurons%20it%20connects%3B%20and%20%28ii%29%20pruning%20is%0Adriven%20by%20the%20emergent%20%28multimodal%29%20distribution%20of%20the%20VLM%20parameters%20after%0Apretraining.%20We%20benchmark%20eight%20state-of-the-art%20pruning%20algorithms%20in%20the%0Acontext%20of%20TA-VLP%2C%20experimenting%20with%20two%20VLMs%2C%20three%20vision-language%20tasks%2C%0Aand%20three%20pruning%20ratios.%20Our%20experimental%20results%20show%20that%20MULTIFLOW%0Aoutperforms%20recent%20sophisticated%2C%20combinatorial%20competitors%20in%20the%20vast%0Amajority%20of%20the%20cases%2C%20paving%20the%20way%20towards%20addressing%20TA-VLP.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/FarinaMatteo/multiflow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05621v1&entry.124074799=Read"},
{"title": "3D-COCO: extension of MS-COCO dataset for image detection and 3D\n  reconstruction modules", "author": "Maxence Bideaux and Alice Phe and Mohamed Chaouch and Bertrand Luvison and Quoc-Cuong Pham", "abstract": "  We introduce 3D-COCO, an extension of the original MS-COCO dataset providing\n3D models and 2D-3D alignment annotations. 3D-COCO was designed to achieve\ncomputer vision tasks such as 3D reconstruction or image detection configurable\nwith textual, 2D image, and 3D CAD model queries. We complete the existing\nMS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. By\nusing an IoU-based method, we match each MS-COCO annotation with the best 3D\nmodels to provide a 2D-3D alignment. The open-source nature of 3D-COCO is a\npremiere that should pave the way for new research on 3D-related topics. The\ndataset and its source codes is available at\nhttps://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/\n", "link": "http://arxiv.org/abs/2404.05641v1", "date": "2024-04-08", "relevancy": 2.053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5004}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4891}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules&body=Title%3A%203D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules%0AAuthor%3A%20Maxence%20Bideaux%20and%20Alice%20Phe%20and%20Mohamed%20Chaouch%20and%20Bertrand%20Luvison%20and%20Quoc-Cuong%20Pham%0AAbstract%3A%20%20%20We%20introduce%203D-COCO%2C%20an%20extension%20of%20the%20original%20MS-COCO%20dataset%20providing%0A3D%20models%20and%202D-3D%20alignment%20annotations.%203D-COCO%20was%20designed%20to%20achieve%0Acomputer%20vision%20tasks%20such%20as%203D%20reconstruction%20or%20image%20detection%20configurable%0Awith%20textual%2C%202D%20image%2C%20and%203D%20CAD%20model%20queries.%20We%20complete%20the%20existing%0AMS-COCO%20dataset%20with%2028K%203D%20models%20collected%20on%20ShapeNet%20and%20Objaverse.%20By%0Ausing%20an%20IoU-based%20method%2C%20we%20match%20each%20MS-COCO%20annotation%20with%20the%20best%203D%0Amodels%20to%20provide%20a%202D-3D%20alignment.%20The%20open-source%20nature%20of%203D-COCO%20is%20a%0Apremiere%20that%20should%20pave%20the%20way%20for%20new%20research%20on%203D-related%20topics.%20The%0Adataset%20and%20its%20source%20codes%20is%20available%20at%0Ahttps%3A//kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05641v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules&entry.906535625=Maxence%20Bideaux%20and%20Alice%20Phe%20and%20Mohamed%20Chaouch%20and%20Bertrand%20Luvison%20and%20Quoc-Cuong%20Pham&entry.1292438233=%20%20We%20introduce%203D-COCO%2C%20an%20extension%20of%20the%20original%20MS-COCO%20dataset%20providing%0A3D%20models%20and%202D-3D%20alignment%20annotations.%203D-COCO%20was%20designed%20to%20achieve%0Acomputer%20vision%20tasks%20such%20as%203D%20reconstruction%20or%20image%20detection%20configurable%0Awith%20textual%2C%202D%20image%2C%20and%203D%20CAD%20model%20queries.%20We%20complete%20the%20existing%0AMS-COCO%20dataset%20with%2028K%203D%20models%20collected%20on%20ShapeNet%20and%20Objaverse.%20By%0Ausing%20an%20IoU-based%20method%2C%20we%20match%20each%20MS-COCO%20annotation%20with%20the%20best%203D%0Amodels%20to%20provide%20a%202D-3D%20alignment.%20The%20open-source%20nature%20of%203D-COCO%20is%20a%0Apremiere%20that%20should%20pave%20the%20way%20for%20new%20research%20on%203D-related%20topics.%20The%0Adataset%20and%20its%20source%20codes%20is%20available%20at%0Ahttps%3A//kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05641v1&entry.124074799=Read"},
{"title": "Deep Internal Learning: Deep Learning from a Single Input", "author": "Tom Tirer and Raja Giryes and Se Young Chun and Yonina C. Eldar", "abstract": "  Deep learning, in general, focuses on training a neural network from large\nlabeled datasets. Yet, in many cases there is value in training a network just\nfrom the input at hand. This is particularly relevant in many signal and image\nprocessing problems where training data is scarce and diversity is large on the\none hand, and on the other, there is a lot of structure in the data that can be\nexploited. Using this information is the key to deep internal-learning\nstrategies, which may involve training a network from scratch using a single\ninput or adapting an already trained network to a provided input example at\ninference time. This survey paper aims at covering deep internal-learning\ntechniques that have been proposed in the past few years for these two\nimportant directions. While our main focus will be on image processing\nproblems, most of the approaches that we survey are derived for general signals\n(vectors with recurring patterns that can be distinguished from noise) and are\ntherefore applicable to other modalities.\n", "link": "http://arxiv.org/abs/2312.07425v2", "date": "2024-04-08", "relevancy": 2.0328, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5221}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Internal%20Learning%3A%20Deep%20Learning%20from%20a%20Single%20Input&body=Title%3A%20Deep%20Internal%20Learning%3A%20Deep%20Learning%20from%20a%20Single%20Input%0AAuthor%3A%20Tom%20Tirer%20and%20Raja%20Giryes%20and%20Se%20Young%20Chun%20and%20Yonina%20C.%20Eldar%0AAbstract%3A%20%20%20Deep%20learning%2C%20in%20general%2C%20focuses%20on%20training%20a%20neural%20network%20from%20large%0Alabeled%20datasets.%20Yet%2C%20in%20many%20cases%20there%20is%20value%20in%20training%20a%20network%20just%0Afrom%20the%20input%20at%20hand.%20This%20is%20particularly%20relevant%20in%20many%20signal%20and%20image%0Aprocessing%20problems%20where%20training%20data%20is%20scarce%20and%20diversity%20is%20large%20on%20the%0Aone%20hand%2C%20and%20on%20the%20other%2C%20there%20is%20a%20lot%20of%20structure%20in%20the%20data%20that%20can%20be%0Aexploited.%20Using%20this%20information%20is%20the%20key%20to%20deep%20internal-learning%0Astrategies%2C%20which%20may%20involve%20training%20a%20network%20from%20scratch%20using%20a%20single%0Ainput%20or%20adapting%20an%20already%20trained%20network%20to%20a%20provided%20input%20example%20at%0Ainference%20time.%20This%20survey%20paper%20aims%20at%20covering%20deep%20internal-learning%0Atechniques%20that%20have%20been%20proposed%20in%20the%20past%20few%20years%20for%20these%20two%0Aimportant%20directions.%20While%20our%20main%20focus%20will%20be%20on%20image%20processing%0Aproblems%2C%20most%20of%20the%20approaches%20that%20we%20survey%20are%20derived%20for%20general%20signals%0A%28vectors%20with%20recurring%20patterns%20that%20can%20be%20distinguished%20from%20noise%29%20and%20are%0Atherefore%20applicable%20to%20other%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07425v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Internal%20Learning%3A%20Deep%20Learning%20from%20a%20Single%20Input&entry.906535625=Tom%20Tirer%20and%20Raja%20Giryes%20and%20Se%20Young%20Chun%20and%20Yonina%20C.%20Eldar&entry.1292438233=%20%20Deep%20learning%2C%20in%20general%2C%20focuses%20on%20training%20a%20neural%20network%20from%20large%0Alabeled%20datasets.%20Yet%2C%20in%20many%20cases%20there%20is%20value%20in%20training%20a%20network%20just%0Afrom%20the%20input%20at%20hand.%20This%20is%20particularly%20relevant%20in%20many%20signal%20and%20image%0Aprocessing%20problems%20where%20training%20data%20is%20scarce%20and%20diversity%20is%20large%20on%20the%0Aone%20hand%2C%20and%20on%20the%20other%2C%20there%20is%20a%20lot%20of%20structure%20in%20the%20data%20that%20can%20be%0Aexploited.%20Using%20this%20information%20is%20the%20key%20to%20deep%20internal-learning%0Astrategies%2C%20which%20may%20involve%20training%20a%20network%20from%20scratch%20using%20a%20single%0Ainput%20or%20adapting%20an%20already%20trained%20network%20to%20a%20provided%20input%20example%20at%0Ainference%20time.%20This%20survey%20paper%20aims%20at%20covering%20deep%20internal-learning%0Atechniques%20that%20have%20been%20proposed%20in%20the%20past%20few%20years%20for%20these%20two%0Aimportant%20directions.%20While%20our%20main%20focus%20will%20be%20on%20image%20processing%0Aproblems%2C%20most%20of%20the%20approaches%20that%20we%20survey%20are%20derived%20for%20general%20signals%0A%28vectors%20with%20recurring%20patterns%20that%20can%20be%20distinguished%20from%20noise%29%20and%20are%0Atherefore%20applicable%20to%20other%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07425v2&entry.124074799=Read"},
{"title": "Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A\n  Multi-Leader Multi-Follower Stackelberg Game Approach", "author": "Jiawen Kang and Yue Zhong and Minrui Xu and Jiangtian Nie and Jinbo Wen and Hongyang Du and Dongdong Ye and Xumin Huang and Dusit Niyato and Shengli Xie", "abstract": "  The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving\nrise to an emerging paradigm named UAV metaverses, which create a unified\necosystem that blends physical and virtual spaces, transforming drone\ninteraction and virtual exploration. UAV Twins (UTs), as the digital twins of\nUAVs that revolutionize UAV applications by making them more immersive,\nrealistic, and informative, are deployed and updated on ground base stations,\ne.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse\nUsers (UMUs). Due to the dynamic mobility of UAVs and limited communication\ncoverages of RSUs, it is essential to perform real-time UT migration to ensure\nseamless immersive experiences for UMUs. However, selecting appropriate RSUs\nand optimizing the required bandwidth is challenging for achieving reliable and\nefficient UT migration. To address the challenges, we propose a tiny machine\nlearning-based Stackelberg game framework based on pruning techniques for\nefficient UT migration in UAV metaverses. Specifically, we formulate a\nmulti-leader multi-follower Stackelberg model considering a new immersion\nmetric of UMUs in the utilities of UAVs. Then, we design a Tiny Multi-Agent\nDeep Reinforcement Learning (Tiny MADRL) algorithm to obtain the tiny networks\nrepresenting the optimal game solution. Specifically, the actor-critic network\nleverages the pruning techniques to reduce the number of network parameters and\nachieve model size and computation reduction, allowing for efficient\nimplementation of Tiny MADRL. Numerical results demonstrate that our proposed\nschemes have better performance than traditional schemes.\n", "link": "http://arxiv.org/abs/2401.09680v2", "date": "2024-04-08", "relevancy": 2.0204, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4928}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tiny%20Multi-Agent%20DRL%20for%20Twins%20Migration%20in%20UAV%20Metaverses%3A%20A%0A%20%20Multi-Leader%20Multi-Follower%20Stackelberg%20Game%20Approach&body=Title%3A%20Tiny%20Multi-Agent%20DRL%20for%20Twins%20Migration%20in%20UAV%20Metaverses%3A%20A%0A%20%20Multi-Leader%20Multi-Follower%20Stackelberg%20Game%20Approach%0AAuthor%3A%20Jiawen%20Kang%20and%20Yue%20Zhong%20and%20Minrui%20Xu%20and%20Jiangtian%20Nie%20and%20Jinbo%20Wen%20and%20Hongyang%20Du%20and%20Dongdong%20Ye%20and%20Xumin%20Huang%20and%20Dusit%20Niyato%20and%20Shengli%20Xie%0AAbstract%3A%20%20%20The%20synergy%20between%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20metaverses%20is%20giving%0Arise%20to%20an%20emerging%20paradigm%20named%20UAV%20metaverses%2C%20which%20create%20a%20unified%0Aecosystem%20that%20blends%20physical%20and%20virtual%20spaces%2C%20transforming%20drone%0Ainteraction%20and%20virtual%20exploration.%20UAV%20Twins%20%28UTs%29%2C%20as%20the%20digital%20twins%20of%0AUAVs%20that%20revolutionize%20UAV%20applications%20by%20making%20them%20more%20immersive%2C%0Arealistic%2C%20and%20informative%2C%20are%20deployed%20and%20updated%20on%20ground%20base%20stations%2C%0Ae.g.%2C%20RoadSide%20Units%20%28RSUs%29%2C%20to%20offer%20metaverse%20services%20for%20UAV%20Metaverse%0AUsers%20%28UMUs%29.%20Due%20to%20the%20dynamic%20mobility%20of%20UAVs%20and%20limited%20communication%0Acoverages%20of%20RSUs%2C%20it%20is%20essential%20to%20perform%20real-time%20UT%20migration%20to%20ensure%0Aseamless%20immersive%20experiences%20for%20UMUs.%20However%2C%20selecting%20appropriate%20RSUs%0Aand%20optimizing%20the%20required%20bandwidth%20is%20challenging%20for%20achieving%20reliable%20and%0Aefficient%20UT%20migration.%20To%20address%20the%20challenges%2C%20we%20propose%20a%20tiny%20machine%0Alearning-based%20Stackelberg%20game%20framework%20based%20on%20pruning%20techniques%20for%0Aefficient%20UT%20migration%20in%20UAV%20metaverses.%20Specifically%2C%20we%20formulate%20a%0Amulti-leader%20multi-follower%20Stackelberg%20model%20considering%20a%20new%20immersion%0Ametric%20of%20UMUs%20in%20the%20utilities%20of%20UAVs.%20Then%2C%20we%20design%20a%20Tiny%20Multi-Agent%0ADeep%20Reinforcement%20Learning%20%28Tiny%20MADRL%29%20algorithm%20to%20obtain%20the%20tiny%20networks%0Arepresenting%20the%20optimal%20game%20solution.%20Specifically%2C%20the%20actor-critic%20network%0Aleverages%20the%20pruning%20techniques%20to%20reduce%20the%20number%20of%20network%20parameters%20and%0Aachieve%20model%20size%20and%20computation%20reduction%2C%20allowing%20for%20efficient%0Aimplementation%20of%20Tiny%20MADRL.%20Numerical%20results%20demonstrate%20that%20our%20proposed%0Aschemes%20have%20better%20performance%20than%20traditional%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09680v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20Multi-Agent%20DRL%20for%20Twins%20Migration%20in%20UAV%20Metaverses%3A%20A%0A%20%20Multi-Leader%20Multi-Follower%20Stackelberg%20Game%20Approach&entry.906535625=Jiawen%20Kang%20and%20Yue%20Zhong%20and%20Minrui%20Xu%20and%20Jiangtian%20Nie%20and%20Jinbo%20Wen%20and%20Hongyang%20Du%20and%20Dongdong%20Ye%20and%20Xumin%20Huang%20and%20Dusit%20Niyato%20and%20Shengli%20Xie&entry.1292438233=%20%20The%20synergy%20between%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20metaverses%20is%20giving%0Arise%20to%20an%20emerging%20paradigm%20named%20UAV%20metaverses%2C%20which%20create%20a%20unified%0Aecosystem%20that%20blends%20physical%20and%20virtual%20spaces%2C%20transforming%20drone%0Ainteraction%20and%20virtual%20exploration.%20UAV%20Twins%20%28UTs%29%2C%20as%20the%20digital%20twins%20of%0AUAVs%20that%20revolutionize%20UAV%20applications%20by%20making%20them%20more%20immersive%2C%0Arealistic%2C%20and%20informative%2C%20are%20deployed%20and%20updated%20on%20ground%20base%20stations%2C%0Ae.g.%2C%20RoadSide%20Units%20%28RSUs%29%2C%20to%20offer%20metaverse%20services%20for%20UAV%20Metaverse%0AUsers%20%28UMUs%29.%20Due%20to%20the%20dynamic%20mobility%20of%20UAVs%20and%20limited%20communication%0Acoverages%20of%20RSUs%2C%20it%20is%20essential%20to%20perform%20real-time%20UT%20migration%20to%20ensure%0Aseamless%20immersive%20experiences%20for%20UMUs.%20However%2C%20selecting%20appropriate%20RSUs%0Aand%20optimizing%20the%20required%20bandwidth%20is%20challenging%20for%20achieving%20reliable%20and%0Aefficient%20UT%20migration.%20To%20address%20the%20challenges%2C%20we%20propose%20a%20tiny%20machine%0Alearning-based%20Stackelberg%20game%20framework%20based%20on%20pruning%20techniques%20for%0Aefficient%20UT%20migration%20in%20UAV%20metaverses.%20Specifically%2C%20we%20formulate%20a%0Amulti-leader%20multi-follower%20Stackelberg%20model%20considering%20a%20new%20immersion%0Ametric%20of%20UMUs%20in%20the%20utilities%20of%20UAVs.%20Then%2C%20we%20design%20a%20Tiny%20Multi-Agent%0ADeep%20Reinforcement%20Learning%20%28Tiny%20MADRL%29%20algorithm%20to%20obtain%20the%20tiny%20networks%0Arepresenting%20the%20optimal%20game%20solution.%20Specifically%2C%20the%20actor-critic%20network%0Aleverages%20the%20pruning%20techniques%20to%20reduce%20the%20number%20of%20network%20parameters%20and%0Aachieve%20model%20size%20and%20computation%20reduction%2C%20allowing%20for%20efficient%0Aimplementation%20of%20Tiny%20MADRL.%20Numerical%20results%20demonstrate%20that%20our%20proposed%0Aschemes%20have%20better%20performance%20than%20traditional%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09680v2&entry.124074799=Read"},
{"title": "Deep Feature Statistics Mapping for Generalized Screen Content Image\n  Quality Assessment", "author": "Baoliang Chen and Hanwei Zhu and Lingyu Zhu and Shiqi Wang and Sam Kwong", "abstract": "  The statistical regularities of natural images, referred to as natural scene\nstatistics, play an important role in no-reference image quality assessment.\nHowever, it has been widely acknowledged that screen content images (SCIs),\nwhich are typically computer generated, do not hold such statistics. Here we\nmake the first attempt to learn the statistics of SCIs, based upon which the\nquality of SCIs can be effectively determined. The underlying mechanism of the\nproposed approach is based upon the mild assumption that the SCIs, which are\nnot physically acquired, still obey certain statistics that could be understood\nin a learning fashion. We empirically show that the statistics deviation could\nbe effectively leveraged in quality assessment, and the proposed method is\nsuperior when evaluated in different settings. Extensive experimental results\ndemonstrate the Deep Feature Statistics based SCI Quality Assessment (DFSS-IQA)\nmodel delivers promising performance compared with existing NR-IQA models and\nshows a high generalization capability in the cross-dataset settings. The\nimplementation of our method is publicly available at\nhttps://github.com/Baoliang93/DFSS-IQA.\n", "link": "http://arxiv.org/abs/2209.05321v3", "date": "2024-04-08", "relevancy": 2.0159, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.515}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4988}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.495}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Feature%20Statistics%20Mapping%20for%20Generalized%20Screen%20Content%20Image%0A%20%20Quality%20Assessment&body=Title%3A%20Deep%20Feature%20Statistics%20Mapping%20for%20Generalized%20Screen%20Content%20Image%0A%20%20Quality%20Assessment%0AAuthor%3A%20Baoliang%20Chen%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Shiqi%20Wang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20The%20statistical%20regularities%20of%20natural%20images%2C%20referred%20to%20as%20natural%20scene%0Astatistics%2C%20play%20an%20important%20role%20in%20no-reference%20image%20quality%20assessment.%0AHowever%2C%20it%20has%20been%20widely%20acknowledged%20that%20screen%20content%20images%20%28SCIs%29%2C%0Awhich%20are%20typically%20computer%20generated%2C%20do%20not%20hold%20such%20statistics.%20Here%20we%0Amake%20the%20first%20attempt%20to%20learn%20the%20statistics%20of%20SCIs%2C%20based%20upon%20which%20the%0Aquality%20of%20SCIs%20can%20be%20effectively%20determined.%20The%20underlying%20mechanism%20of%20the%0Aproposed%20approach%20is%20based%20upon%20the%20mild%20assumption%20that%20the%20SCIs%2C%20which%20are%0Anot%20physically%20acquired%2C%20still%20obey%20certain%20statistics%20that%20could%20be%20understood%0Ain%20a%20learning%20fashion.%20We%20empirically%20show%20that%20the%20statistics%20deviation%20could%0Abe%20effectively%20leveraged%20in%20quality%20assessment%2C%20and%20the%20proposed%20method%20is%0Asuperior%20when%20evaluated%20in%20different%20settings.%20Extensive%20experimental%20results%0Ademonstrate%20the%20Deep%20Feature%20Statistics%20based%20SCI%20Quality%20Assessment%20%28DFSS-IQA%29%0Amodel%20delivers%20promising%20performance%20compared%20with%20existing%20NR-IQA%20models%20and%0Ashows%20a%20high%20generalization%20capability%20in%20the%20cross-dataset%20settings.%20The%0Aimplementation%20of%20our%20method%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Baoliang93/DFSS-IQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.05321v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Feature%20Statistics%20Mapping%20for%20Generalized%20Screen%20Content%20Image%0A%20%20Quality%20Assessment&entry.906535625=Baoliang%20Chen%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Shiqi%20Wang%20and%20Sam%20Kwong&entry.1292438233=%20%20The%20statistical%20regularities%20of%20natural%20images%2C%20referred%20to%20as%20natural%20scene%0Astatistics%2C%20play%20an%20important%20role%20in%20no-reference%20image%20quality%20assessment.%0AHowever%2C%20it%20has%20been%20widely%20acknowledged%20that%20screen%20content%20images%20%28SCIs%29%2C%0Awhich%20are%20typically%20computer%20generated%2C%20do%20not%20hold%20such%20statistics.%20Here%20we%0Amake%20the%20first%20attempt%20to%20learn%20the%20statistics%20of%20SCIs%2C%20based%20upon%20which%20the%0Aquality%20of%20SCIs%20can%20be%20effectively%20determined.%20The%20underlying%20mechanism%20of%20the%0Aproposed%20approach%20is%20based%20upon%20the%20mild%20assumption%20that%20the%20SCIs%2C%20which%20are%0Anot%20physically%20acquired%2C%20still%20obey%20certain%20statistics%20that%20could%20be%20understood%0Ain%20a%20learning%20fashion.%20We%20empirically%20show%20that%20the%20statistics%20deviation%20could%0Abe%20effectively%20leveraged%20in%20quality%20assessment%2C%20and%20the%20proposed%20method%20is%0Asuperior%20when%20evaluated%20in%20different%20settings.%20Extensive%20experimental%20results%0Ademonstrate%20the%20Deep%20Feature%20Statistics%20based%20SCI%20Quality%20Assessment%20%28DFSS-IQA%29%0Amodel%20delivers%20promising%20performance%20compared%20with%20existing%20NR-IQA%20models%20and%0Ashows%20a%20high%20generalization%20capability%20in%20the%20cross-dataset%20settings.%20The%0Aimplementation%20of%20our%20method%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Baoliang93/DFSS-IQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.05321v3&entry.124074799=Read"},
{"title": "Understanding the Learning Dynamics of Alignment with Human Feedback", "author": "Shawn Im and Yixuan Li", "abstract": "  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n", "link": "http://arxiv.org/abs/2403.18742v3", "date": "2024-04-08", "relevancy": 2.0107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&body=Title%3A%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback%0AAuthor%3A%20Shawn%20Im%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18742v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&entry.906535625=Shawn%20Im%20and%20Yixuan%20Li&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18742v3&entry.124074799=Read"},
{"title": "MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech\n  Recognition", "author": "He Wang and Pengcheng Guo and Pan Zhou and Lei Xie", "abstract": "  While automatic speech recognition (ASR) systems degrade significantly in\nnoisy environments, audio-visual speech recognition (AVSR) systems aim to\ncomplement the audio stream with noise-invariant visual cues and improve the\nsystem's robustness. However, current studies mainly focus on fusing the\nwell-learned modality features, like the output of modality-specific encoders,\nwithout considering the contextual relationship during the modality feature\nlearning. In this study, we propose a multi-layer cross-attention fusion based\nAVSR (MLCA-AVSR) approach that promotes representation learning of each\nmodality by fusing them at different levels of audio/visual encoders.\nExperimental results on the MISP2022-AVSR Challenge dataset show the efficacy\nof our proposed system, achieving a concatenated minimum permutation character\nerror rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative\nimprovement compared with our previous system which ranked the second place in\nthe challenge. Following the fusion of multiple systems, our proposed approach\nsurpasses the first-place system, establishing a new SOTA cpCER of 29.13% on\nthis dataset.\n", "link": "http://arxiv.org/abs/2401.03424v3", "date": "2024-04-08", "relevancy": 2.0075, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4865}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MLCA-AVSR%3A%20Multi-Layer%20Cross%20Attention%20Fusion%20based%20Audio-Visual%20Speech%0A%20%20Recognition&body=Title%3A%20MLCA-AVSR%3A%20Multi-Layer%20Cross%20Attention%20Fusion%20based%20Audio-Visual%20Speech%0A%20%20Recognition%0AAuthor%3A%20He%20Wang%20and%20Pengcheng%20Guo%20and%20Pan%20Zhou%20and%20Lei%20Xie%0AAbstract%3A%20%20%20While%20automatic%20speech%20recognition%20%28ASR%29%20systems%20degrade%20significantly%20in%0Anoisy%20environments%2C%20audio-visual%20speech%20recognition%20%28AVSR%29%20systems%20aim%20to%0Acomplement%20the%20audio%20stream%20with%20noise-invariant%20visual%20cues%20and%20improve%20the%0Asystem%27s%20robustness.%20However%2C%20current%20studies%20mainly%20focus%20on%20fusing%20the%0Awell-learned%20modality%20features%2C%20like%20the%20output%20of%20modality-specific%20encoders%2C%0Awithout%20considering%20the%20contextual%20relationship%20during%20the%20modality%20feature%0Alearning.%20In%20this%20study%2C%20we%20propose%20a%20multi-layer%20cross-attention%20fusion%20based%0AAVSR%20%28MLCA-AVSR%29%20approach%20that%20promotes%20representation%20learning%20of%20each%0Amodality%20by%20fusing%20them%20at%20different%20levels%20of%20audio/visual%20encoders.%0AExperimental%20results%20on%20the%20MISP2022-AVSR%20Challenge%20dataset%20show%20the%20efficacy%0Aof%20our%20proposed%20system%2C%20achieving%20a%20concatenated%20minimum%20permutation%20character%0Aerror%20rate%20%28cpCER%29%20of%2030.57%25%20on%20the%20Eval%20set%20and%20yielding%20up%20to%203.17%25%20relative%0Aimprovement%20compared%20with%20our%20previous%20system%20which%20ranked%20the%20second%20place%20in%0Athe%20challenge.%20Following%20the%20fusion%20of%20multiple%20systems%2C%20our%20proposed%20approach%0Asurpasses%20the%20first-place%20system%2C%20establishing%20a%20new%20SOTA%20cpCER%20of%2029.13%25%20on%0Athis%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03424v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLCA-AVSR%3A%20Multi-Layer%20Cross%20Attention%20Fusion%20based%20Audio-Visual%20Speech%0A%20%20Recognition&entry.906535625=He%20Wang%20and%20Pengcheng%20Guo%20and%20Pan%20Zhou%20and%20Lei%20Xie&entry.1292438233=%20%20While%20automatic%20speech%20recognition%20%28ASR%29%20systems%20degrade%20significantly%20in%0Anoisy%20environments%2C%20audio-visual%20speech%20recognition%20%28AVSR%29%20systems%20aim%20to%0Acomplement%20the%20audio%20stream%20with%20noise-invariant%20visual%20cues%20and%20improve%20the%0Asystem%27s%20robustness.%20However%2C%20current%20studies%20mainly%20focus%20on%20fusing%20the%0Awell-learned%20modality%20features%2C%20like%20the%20output%20of%20modality-specific%20encoders%2C%0Awithout%20considering%20the%20contextual%20relationship%20during%20the%20modality%20feature%0Alearning.%20In%20this%20study%2C%20we%20propose%20a%20multi-layer%20cross-attention%20fusion%20based%0AAVSR%20%28MLCA-AVSR%29%20approach%20that%20promotes%20representation%20learning%20of%20each%0Amodality%20by%20fusing%20them%20at%20different%20levels%20of%20audio/visual%20encoders.%0AExperimental%20results%20on%20the%20MISP2022-AVSR%20Challenge%20dataset%20show%20the%20efficacy%0Aof%20our%20proposed%20system%2C%20achieving%20a%20concatenated%20minimum%20permutation%20character%0Aerror%20rate%20%28cpCER%29%20of%2030.57%25%20on%20the%20Eval%20set%20and%20yielding%20up%20to%203.17%25%20relative%0Aimprovement%20compared%20with%20our%20previous%20system%20which%20ranked%20the%20second%20place%20in%0Athe%20challenge.%20Following%20the%20fusion%20of%20multiple%20systems%2C%20our%20proposed%20approach%0Asurpasses%20the%20first-place%20system%2C%20establishing%20a%20new%20SOTA%20cpCER%20of%2029.13%25%20on%0Athis%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03424v3&entry.124074799=Read"},
{"title": "Restricted Bayesian Neural Network", "author": "Sourav Ganguly and Saprativa Bhattacharjee", "abstract": "  Modern deep learning tools are remarkably effective in addressing intricate\nproblems. However, their operation as black-box models introduces increased\nuncertainty in predictions. Additionally, they contend with various challenges,\nincluding the need for substantial storage space in large networks, issues of\noverfitting, underfitting, vanishing gradients, and more. This study explores\nthe concept of Bayesian Neural Networks, presenting a novel architecture\ndesigned to significantly alleviate the storage space complexity of a network.\nFurthermore, we introduce an algorithm adept at efficiently handling\nuncertainties, ensuring robust convergence values without becoming trapped in\nlocal optima, particularly when the objective function lacks perfect convexity.\n", "link": "http://arxiv.org/abs/2403.04810v3", "date": "2024-04-08", "relevancy": 1.9951, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4725}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Restricted%20Bayesian%20Neural%20Network&body=Title%3A%20Restricted%20Bayesian%20Neural%20Network%0AAuthor%3A%20Sourav%20Ganguly%20and%20Saprativa%20Bhattacharjee%0AAbstract%3A%20%20%20Modern%20deep%20learning%20tools%20are%20remarkably%20effective%20in%20addressing%20intricate%0Aproblems.%20However%2C%20their%20operation%20as%20black-box%20models%20introduces%20increased%0Auncertainty%20in%20predictions.%20Additionally%2C%20they%20contend%20with%20various%20challenges%2C%0Aincluding%20the%20need%20for%20substantial%20storage%20space%20in%20large%20networks%2C%20issues%20of%0Aoverfitting%2C%20underfitting%2C%20vanishing%20gradients%2C%20and%20more.%20This%20study%20explores%0Athe%20concept%20of%20Bayesian%20Neural%20Networks%2C%20presenting%20a%20novel%20architecture%0Adesigned%20to%20significantly%20alleviate%20the%20storage%20space%20complexity%20of%20a%20network.%0AFurthermore%2C%20we%20introduce%20an%20algorithm%20adept%20at%20efficiently%20handling%0Auncertainties%2C%20ensuring%20robust%20convergence%20values%20without%20becoming%20trapped%20in%0Alocal%20optima%2C%20particularly%20when%20the%20objective%20function%20lacks%20perfect%20convexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04810v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restricted%20Bayesian%20Neural%20Network&entry.906535625=Sourav%20Ganguly%20and%20Saprativa%20Bhattacharjee&entry.1292438233=%20%20Modern%20deep%20learning%20tools%20are%20remarkably%20effective%20in%20addressing%20intricate%0Aproblems.%20However%2C%20their%20operation%20as%20black-box%20models%20introduces%20increased%0Auncertainty%20in%20predictions.%20Additionally%2C%20they%20contend%20with%20various%20challenges%2C%0Aincluding%20the%20need%20for%20substantial%20storage%20space%20in%20large%20networks%2C%20issues%20of%0Aoverfitting%2C%20underfitting%2C%20vanishing%20gradients%2C%20and%20more.%20This%20study%20explores%0Athe%20concept%20of%20Bayesian%20Neural%20Networks%2C%20presenting%20a%20novel%20architecture%0Adesigned%20to%20significantly%20alleviate%20the%20storage%20space%20complexity%20of%20a%20network.%0AFurthermore%2C%20we%20introduce%20an%20algorithm%20adept%20at%20efficiently%20handling%0Auncertainties%2C%20ensuring%20robust%20convergence%20values%20without%20becoming%20trapped%20in%0Alocal%20optima%2C%20particularly%20when%20the%20objective%20function%20lacks%20perfect%20convexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04810v3&entry.124074799=Read"},
{"title": "Robust Control using Control Lyapunov Function and Hamilton-Jacobi\n  Reachability", "author": "Chun-Ming Yang and Pranav A. Bhounsule", "abstract": "  The paper presents a robust control technique that combines the Control\nLyapunov function and Hamilton-Jacobi Reachability to compute a controller and\nits Region of Attraction (ROA). The Control Lyapunov function uses a linear\nsystem model with an assumed additive uncertainty to calculate a control gain\nand the level sets of the ROA as a function of the uncertainty. Next,\nHamilton-Jacobi reachability uses the nonlinear model with the modeled\nuncertainty, which need not be additive, to compute the backward reachable set\n(BRS). Finally, by juxtaposing the level sets of the ROA with BRS, we can\ncalculate the worst-case additive disturbance and the ROA of the nonlinear\nmodel. We illustrate our approach on a 2D quadcopter tracking trajectory and a\n2D quadcopter with height and velocity regulation in simulation.\n", "link": "http://arxiv.org/abs/2404.05625v1", "date": "2024-04-08", "relevancy": 1.985, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Control%20using%20Control%20Lyapunov%20Function%20and%20Hamilton-Jacobi%0A%20%20Reachability&body=Title%3A%20Robust%20Control%20using%20Control%20Lyapunov%20Function%20and%20Hamilton-Jacobi%0A%20%20Reachability%0AAuthor%3A%20Chun-Ming%20Yang%20and%20Pranav%20A.%20Bhounsule%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20robust%20control%20technique%20that%20combines%20the%20Control%0ALyapunov%20function%20and%20Hamilton-Jacobi%20Reachability%20to%20compute%20a%20controller%20and%0Aits%20Region%20of%20Attraction%20%28ROA%29.%20The%20Control%20Lyapunov%20function%20uses%20a%20linear%0Asystem%20model%20with%20an%20assumed%20additive%20uncertainty%20to%20calculate%20a%20control%20gain%0Aand%20the%20level%20sets%20of%20the%20ROA%20as%20a%20function%20of%20the%20uncertainty.%20Next%2C%0AHamilton-Jacobi%20reachability%20uses%20the%20nonlinear%20model%20with%20the%20modeled%0Auncertainty%2C%20which%20need%20not%20be%20additive%2C%20to%20compute%20the%20backward%20reachable%20set%0A%28BRS%29.%20Finally%2C%20by%20juxtaposing%20the%20level%20sets%20of%20the%20ROA%20with%20BRS%2C%20we%20can%0Acalculate%20the%20worst-case%20additive%20disturbance%20and%20the%20ROA%20of%20the%20nonlinear%0Amodel.%20We%20illustrate%20our%20approach%20on%20a%202D%20quadcopter%20tracking%20trajectory%20and%20a%0A2D%20quadcopter%20with%20height%20and%20velocity%20regulation%20in%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Control%20using%20Control%20Lyapunov%20Function%20and%20Hamilton-Jacobi%0A%20%20Reachability&entry.906535625=Chun-Ming%20Yang%20and%20Pranav%20A.%20Bhounsule&entry.1292438233=%20%20The%20paper%20presents%20a%20robust%20control%20technique%20that%20combines%20the%20Control%0ALyapunov%20function%20and%20Hamilton-Jacobi%20Reachability%20to%20compute%20a%20controller%20and%0Aits%20Region%20of%20Attraction%20%28ROA%29.%20The%20Control%20Lyapunov%20function%20uses%20a%20linear%0Asystem%20model%20with%20an%20assumed%20additive%20uncertainty%20to%20calculate%20a%20control%20gain%0Aand%20the%20level%20sets%20of%20the%20ROA%20as%20a%20function%20of%20the%20uncertainty.%20Next%2C%0AHamilton-Jacobi%20reachability%20uses%20the%20nonlinear%20model%20with%20the%20modeled%0Auncertainty%2C%20which%20need%20not%20be%20additive%2C%20to%20compute%20the%20backward%20reachable%20set%0A%28BRS%29.%20Finally%2C%20by%20juxtaposing%20the%20level%20sets%20of%20the%20ROA%20with%20BRS%2C%20we%20can%0Acalculate%20the%20worst-case%20additive%20disturbance%20and%20the%20ROA%20of%20the%20nonlinear%0Amodel.%20We%20illustrate%20our%20approach%20on%20a%202D%20quadcopter%20tracking%20trajectory%20and%20a%0A2D%20quadcopter%20with%20height%20and%20velocity%20regulation%20in%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05625v1&entry.124074799=Read"},
{"title": "Learning Mutually Informed Representations for Characters and Subwords", "author": "Yilin Wang and Xinyi Hu and Matthew R. Gormley", "abstract": "  Most pretrained language models rely on subword tokenization, which processes\ntext as a sequence of subword tokens. However, different granularities of text,\nsuch as characters, subwords, and words, can contain different kinds of\ninformation. Previous studies have shown that incorporating multiple input\ngranularities improves model generalization, yet very few of them outputs\nuseful representations for each granularity. In this paper, we introduce the\nentanglement model, aiming to combine character and subword language models.\nInspired by vision-language models, our model treats characters and subwords as\nseparate modalities, and it generates mutually informed representations for\nboth granularities as output. We evaluate our model on text classification,\nnamed entity recognition, POS-tagging, and character-level sequence labeling\n(intraword code-switching). Notably, the entanglement model outperforms its\nbackbone language models, particularly in the presence of noisy texts and\nlow-resource languages. Furthermore, the entanglement model even outperforms\nlarger pre-trained models on all English sequence labeling tasks and\nclassification tasks. We make our code publically available.\n", "link": "http://arxiv.org/abs/2311.07853v2", "date": "2024-04-08", "relevancy": 1.9827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Mutually%20Informed%20Representations%20for%20Characters%20and%20Subwords&body=Title%3A%20Learning%20Mutually%20Informed%20Representations%20for%20Characters%20and%20Subwords%0AAuthor%3A%20Yilin%20Wang%20and%20Xinyi%20Hu%20and%20Matthew%20R.%20Gormley%0AAbstract%3A%20%20%20Most%20pretrained%20language%20models%20rely%20on%20subword%20tokenization%2C%20which%20processes%0Atext%20as%20a%20sequence%20of%20subword%20tokens.%20However%2C%20different%20granularities%20of%20text%2C%0Asuch%20as%20characters%2C%20subwords%2C%20and%20words%2C%20can%20contain%20different%20kinds%20of%0Ainformation.%20Previous%20studies%20have%20shown%20that%20incorporating%20multiple%20input%0Agranularities%20improves%20model%20generalization%2C%20yet%20very%20few%20of%20them%20outputs%0Auseful%20representations%20for%20each%20granularity.%20In%20this%20paper%2C%20we%20introduce%20the%0Aentanglement%20model%2C%20aiming%20to%20combine%20character%20and%20subword%20language%20models.%0AInspired%20by%20vision-language%20models%2C%20our%20model%20treats%20characters%20and%20subwords%20as%0Aseparate%20modalities%2C%20and%20it%20generates%20mutually%20informed%20representations%20for%0Aboth%20granularities%20as%20output.%20We%20evaluate%20our%20model%20on%20text%20classification%2C%0Anamed%20entity%20recognition%2C%20POS-tagging%2C%20and%20character-level%20sequence%20labeling%0A%28intraword%20code-switching%29.%20Notably%2C%20the%20entanglement%20model%20outperforms%20its%0Abackbone%20language%20models%2C%20particularly%20in%20the%20presence%20of%20noisy%20texts%20and%0Alow-resource%20languages.%20Furthermore%2C%20the%20entanglement%20model%20even%20outperforms%0Alarger%20pre-trained%20models%20on%20all%20English%20sequence%20labeling%20tasks%20and%0Aclassification%20tasks.%20We%20make%20our%20code%20publically%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07853v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Mutually%20Informed%20Representations%20for%20Characters%20and%20Subwords&entry.906535625=Yilin%20Wang%20and%20Xinyi%20Hu%20and%20Matthew%20R.%20Gormley&entry.1292438233=%20%20Most%20pretrained%20language%20models%20rely%20on%20subword%20tokenization%2C%20which%20processes%0Atext%20as%20a%20sequence%20of%20subword%20tokens.%20However%2C%20different%20granularities%20of%20text%2C%0Asuch%20as%20characters%2C%20subwords%2C%20and%20words%2C%20can%20contain%20different%20kinds%20of%0Ainformation.%20Previous%20studies%20have%20shown%20that%20incorporating%20multiple%20input%0Agranularities%20improves%20model%20generalization%2C%20yet%20very%20few%20of%20them%20outputs%0Auseful%20representations%20for%20each%20granularity.%20In%20this%20paper%2C%20we%20introduce%20the%0Aentanglement%20model%2C%20aiming%20to%20combine%20character%20and%20subword%20language%20models.%0AInspired%20by%20vision-language%20models%2C%20our%20model%20treats%20characters%20and%20subwords%20as%0Aseparate%20modalities%2C%20and%20it%20generates%20mutually%20informed%20representations%20for%0Aboth%20granularities%20as%20output.%20We%20evaluate%20our%20model%20on%20text%20classification%2C%0Anamed%20entity%20recognition%2C%20POS-tagging%2C%20and%20character-level%20sequence%20labeling%0A%28intraword%20code-switching%29.%20Notably%2C%20the%20entanglement%20model%20outperforms%20its%0Abackbone%20language%20models%2C%20particularly%20in%20the%20presence%20of%20noisy%20texts%20and%0Alow-resource%20languages.%20Furthermore%2C%20the%20entanglement%20model%20even%20outperforms%0Alarger%20pre-trained%20models%20on%20all%20English%20sequence%20labeling%20tasks%20and%0Aclassification%20tasks.%20We%20make%20our%20code%20publically%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07853v2&entry.124074799=Read"},
{"title": "Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot\n  Editing of Text-to-Video Diffusion Models", "author": "Saman Motamed and Wouter Van Gansbeke and Luc Van Gool", "abstract": "  With recent advances in image and video diffusion models for content\ncreation, a plethora of techniques have been proposed for customizing their\ngenerated content. In particular, manipulating the cross-attention layers of\nText-to-Image (T2I) diffusion models has shown great promise in controlling the\nshape and location of objects in the scene. Transferring image-editing\ntechniques to the video domain, however, is extremely challenging as object\nmotion and temporal consistency are difficult to capture accurately. In this\nwork, we take a first look at the role of cross-attention in Text-to-Video\n(T2V) diffusion models for zero-shot video editing. While one-shot models have\nshown potential in controlling motion and camera movement, we demonstrate\nzero-shot control over object shape, position and movement in T2V models. We\nshow that despite the limitations of current T2V models, cross-attention\nguidance can be a promising approach for editing videos.\n", "link": "http://arxiv.org/abs/2404.05519v1", "date": "2024-04-08", "relevancy": 1.9779, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7413}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6545}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5893}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Effectiveness%20of%20Cross-Attention%20to%20Unlock%20Zero-Shot%0A%20%20Editing%20of%20Text-to-Video%20Diffusion%20Models&body=Title%3A%20Investigating%20the%20Effectiveness%20of%20Cross-Attention%20to%20Unlock%20Zero-Shot%0A%20%20Editing%20of%20Text-to-Video%20Diffusion%20Models%0AAuthor%3A%20Saman%20Motamed%20and%20Wouter%20Van%20Gansbeke%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20With%20recent%20advances%20in%20image%20and%20video%20diffusion%20models%20for%20content%0Acreation%2C%20a%20plethora%20of%20techniques%20have%20been%20proposed%20for%20customizing%20their%0Agenerated%20content.%20In%20particular%2C%20manipulating%20the%20cross-attention%20layers%20of%0AText-to-Image%20%28T2I%29%20diffusion%20models%20has%20shown%20great%20promise%20in%20controlling%20the%0Ashape%20and%20location%20of%20objects%20in%20the%20scene.%20Transferring%20image-editing%0Atechniques%20to%20the%20video%20domain%2C%20however%2C%20is%20extremely%20challenging%20as%20object%0Amotion%20and%20temporal%20consistency%20are%20difficult%20to%20capture%20accurately.%20In%20this%0Awork%2C%20we%20take%20a%20first%20look%20at%20the%20role%20of%20cross-attention%20in%20Text-to-Video%0A%28T2V%29%20diffusion%20models%20for%20zero-shot%20video%20editing.%20While%20one-shot%20models%20have%0Ashown%20potential%20in%20controlling%20motion%20and%20camera%20movement%2C%20we%20demonstrate%0Azero-shot%20control%20over%20object%20shape%2C%20position%20and%20movement%20in%20T2V%20models.%20We%0Ashow%20that%20despite%20the%20limitations%20of%20current%20T2V%20models%2C%20cross-attention%0Aguidance%20can%20be%20a%20promising%20approach%20for%20editing%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05519v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Effectiveness%20of%20Cross-Attention%20to%20Unlock%20Zero-Shot%0A%20%20Editing%20of%20Text-to-Video%20Diffusion%20Models&entry.906535625=Saman%20Motamed%20and%20Wouter%20Van%20Gansbeke%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20With%20recent%20advances%20in%20image%20and%20video%20diffusion%20models%20for%20content%0Acreation%2C%20a%20plethora%20of%20techniques%20have%20been%20proposed%20for%20customizing%20their%0Agenerated%20content.%20In%20particular%2C%20manipulating%20the%20cross-attention%20layers%20of%0AText-to-Image%20%28T2I%29%20diffusion%20models%20has%20shown%20great%20promise%20in%20controlling%20the%0Ashape%20and%20location%20of%20objects%20in%20the%20scene.%20Transferring%20image-editing%0Atechniques%20to%20the%20video%20domain%2C%20however%2C%20is%20extremely%20challenging%20as%20object%0Amotion%20and%20temporal%20consistency%20are%20difficult%20to%20capture%20accurately.%20In%20this%0Awork%2C%20we%20take%20a%20first%20look%20at%20the%20role%20of%20cross-attention%20in%20Text-to-Video%0A%28T2V%29%20diffusion%20models%20for%20zero-shot%20video%20editing.%20While%20one-shot%20models%20have%0Ashown%20potential%20in%20controlling%20motion%20and%20camera%20movement%2C%20we%20demonstrate%0Azero-shot%20control%20over%20object%20shape%2C%20position%20and%20movement%20in%20T2V%20models.%20We%0Ashow%20that%20despite%20the%20limitations%20of%20current%20T2V%20models%2C%20cross-attention%0Aguidance%20can%20be%20a%20promising%20approach%20for%20editing%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05519v1&entry.124074799=Read"},
{"title": "Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO\n  Satellite Learning", "author": "Mohamed Elmahallawy and Tie Luo", "abstract": "  In the ambitious realm of space AI, the integration of federated learning\n(FL) with low Earth orbit (LEO) satellite constellations holds immense promise.\nHowever, many challenges persist in terms of feasibility, learning efficiency,\nand convergence. These hurdles stem from the bottleneck in communication,\ncharacterized by sporadic and irregular connectivity between LEO satellites and\nground stations, coupled with the limited computation capability of satellite\nedge computing (SEC). This paper proposes a novel FL-SEC framework that\nempowers LEO satellites to execute large-scale machine learning (ML) tasks\nonboard efficiently. Its key components include i) personalized learning via\ndivide-and-conquer, which identifies and eliminates redundant satellite images\nand converts complex multi-class classification problems to simple binary\nclassification, enabling rapid and energy-efficient training of lightweight ML\nmodels suitable for IoT/edge devices on satellites; ii) orbital model\nretraining, which generates an aggregated \"orbital model\" per orbit and\nretrains it before sending to the ground station, significantly reducing the\nrequired communication rounds. We conducted experiments using Jetson Nano, an\nedge device closely mimicking the limited compute on LEO satellites, and a real\nsatellite dataset. The results underscore the effectiveness of our approach,\nhighlighting SEC's ability to run lightweight ML models on real and\nhigh-resolution satellite imagery. Our approach dramatically reduces FL\nconvergence time by nearly 30 times, and satellite energy consumption down to\nas low as 1.38 watts, all while maintaining an exceptional accuracy of up to\n96%.\n", "link": "http://arxiv.org/abs/2401.15541v2", "date": "2024-04-08", "relevancy": 1.9744, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5185}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4711}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stitching%20Satellites%20to%20the%20Edge%3A%20Pervasive%20and%20Efficient%20Federated%20LEO%0A%20%20Satellite%20Learning&body=Title%3A%20Stitching%20Satellites%20to%20the%20Edge%3A%20Pervasive%20and%20Efficient%20Federated%20LEO%0A%20%20Satellite%20Learning%0AAuthor%3A%20Mohamed%20Elmahallawy%20and%20Tie%20Luo%0AAbstract%3A%20%20%20In%20the%20ambitious%20realm%20of%20space%20AI%2C%20the%20integration%20of%20federated%20learning%0A%28FL%29%20with%20low%20Earth%20orbit%20%28LEO%29%20satellite%20constellations%20holds%20immense%20promise.%0AHowever%2C%20many%20challenges%20persist%20in%20terms%20of%20feasibility%2C%20learning%20efficiency%2C%0Aand%20convergence.%20These%20hurdles%20stem%20from%20the%20bottleneck%20in%20communication%2C%0Acharacterized%20by%20sporadic%20and%20irregular%20connectivity%20between%20LEO%20satellites%20and%0Aground%20stations%2C%20coupled%20with%20the%20limited%20computation%20capability%20of%20satellite%0Aedge%20computing%20%28SEC%29.%20This%20paper%20proposes%20a%20novel%20FL-SEC%20framework%20that%0Aempowers%20LEO%20satellites%20to%20execute%20large-scale%20machine%20learning%20%28ML%29%20tasks%0Aonboard%20efficiently.%20Its%20key%20components%20include%20i%29%20personalized%20learning%20via%0Adivide-and-conquer%2C%20which%20identifies%20and%20eliminates%20redundant%20satellite%20images%0Aand%20converts%20complex%20multi-class%20classification%20problems%20to%20simple%20binary%0Aclassification%2C%20enabling%20rapid%20and%20energy-efficient%20training%20of%20lightweight%20ML%0Amodels%20suitable%20for%20IoT/edge%20devices%20on%20satellites%3B%20ii%29%20orbital%20model%0Aretraining%2C%20which%20generates%20an%20aggregated%20%22orbital%20model%22%20per%20orbit%20and%0Aretrains%20it%20before%20sending%20to%20the%20ground%20station%2C%20significantly%20reducing%20the%0Arequired%20communication%20rounds.%20We%20conducted%20experiments%20using%20Jetson%20Nano%2C%20an%0Aedge%20device%20closely%20mimicking%20the%20limited%20compute%20on%20LEO%20satellites%2C%20and%20a%20real%0Asatellite%20dataset.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20approach%2C%0Ahighlighting%20SEC%27s%20ability%20to%20run%20lightweight%20ML%20models%20on%20real%20and%0Ahigh-resolution%20satellite%20imagery.%20Our%20approach%20dramatically%20reduces%20FL%0Aconvergence%20time%20by%20nearly%2030%20times%2C%20and%20satellite%20energy%20consumption%20down%20to%0Aas%20low%20as%201.38%20watts%2C%20all%20while%20maintaining%20an%20exceptional%20accuracy%20of%20up%20to%0A96%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15541v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stitching%20Satellites%20to%20the%20Edge%3A%20Pervasive%20and%20Efficient%20Federated%20LEO%0A%20%20Satellite%20Learning&entry.906535625=Mohamed%20Elmahallawy%20and%20Tie%20Luo&entry.1292438233=%20%20In%20the%20ambitious%20realm%20of%20space%20AI%2C%20the%20integration%20of%20federated%20learning%0A%28FL%29%20with%20low%20Earth%20orbit%20%28LEO%29%20satellite%20constellations%20holds%20immense%20promise.%0AHowever%2C%20many%20challenges%20persist%20in%20terms%20of%20feasibility%2C%20learning%20efficiency%2C%0Aand%20convergence.%20These%20hurdles%20stem%20from%20the%20bottleneck%20in%20communication%2C%0Acharacterized%20by%20sporadic%20and%20irregular%20connectivity%20between%20LEO%20satellites%20and%0Aground%20stations%2C%20coupled%20with%20the%20limited%20computation%20capability%20of%20satellite%0Aedge%20computing%20%28SEC%29.%20This%20paper%20proposes%20a%20novel%20FL-SEC%20framework%20that%0Aempowers%20LEO%20satellites%20to%20execute%20large-scale%20machine%20learning%20%28ML%29%20tasks%0Aonboard%20efficiently.%20Its%20key%20components%20include%20i%29%20personalized%20learning%20via%0Adivide-and-conquer%2C%20which%20identifies%20and%20eliminates%20redundant%20satellite%20images%0Aand%20converts%20complex%20multi-class%20classification%20problems%20to%20simple%20binary%0Aclassification%2C%20enabling%20rapid%20and%20energy-efficient%20training%20of%20lightweight%20ML%0Amodels%20suitable%20for%20IoT/edge%20devices%20on%20satellites%3B%20ii%29%20orbital%20model%0Aretraining%2C%20which%20generates%20an%20aggregated%20%22orbital%20model%22%20per%20orbit%20and%0Aretrains%20it%20before%20sending%20to%20the%20ground%20station%2C%20significantly%20reducing%20the%0Arequired%20communication%20rounds.%20We%20conducted%20experiments%20using%20Jetson%20Nano%2C%20an%0Aedge%20device%20closely%20mimicking%20the%20limited%20compute%20on%20LEO%20satellites%2C%20and%20a%20real%0Asatellite%20dataset.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20approach%2C%0Ahighlighting%20SEC%27s%20ability%20to%20run%20lightweight%20ML%20models%20on%20real%20and%0Ahigh-resolution%20satellite%20imagery.%20Our%20approach%20dramatically%20reduces%20FL%0Aconvergence%20time%20by%20nearly%2030%20times%2C%20and%20satellite%20energy%20consumption%20down%20to%0Aas%20low%20as%201.38%20watts%2C%20all%20while%20maintaining%20an%20exceptional%20accuracy%20of%20up%20to%0A96%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15541v2&entry.124074799=Read"},
{"title": "WEEP: A method for spatial interpretation of weakly supervised CNN\n  models in computational pathology", "author": "Abhinav Sharma and Bojing Liu and Mattias Rantalainen", "abstract": "  Deep learning enables the modelling of high-resolution histopathology\nwhole-slide images (WSI). Weakly supervised learning of tile-level data is\ntypically applied for tasks where labels only exist on the patient or WSI level\n(e.g. patient outcomes or histological grading). In this context, there is a\nneed for improved spatial interpretability of predictions from such models. We\npropose a novel method, Wsi rEgion sElection aPproach (WEEP), for model\ninterpretation. It provides a principled yet straightforward way to establish\nthe spatial area of WSI required for assigning a particular prediction label.\nWe demonstrate WEEP on a binary classification task in the area of breast\ncancer computational pathology. WEEP is easy to implement, is directly\nconnected to the model-based decision process, and offers information relevant\nto both research and diagnostic applications.\n", "link": "http://arxiv.org/abs/2403.15238v2", "date": "2024-04-08", "relevancy": 1.9728, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4815}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology&body=Title%3A%20WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology%0AAuthor%3A%20Abhinav%20Sharma%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen%0AAbstract%3A%20%20%20Deep%20learning%20enables%20the%20modelling%20of%20high-resolution%20histopathology%0Awhole-slide%20images%20%28WSI%29.%20Weakly%20supervised%20learning%20of%20tile-level%20data%20is%0Atypically%20applied%20for%20tasks%20where%20labels%20only%20exist%20on%20the%20patient%20or%20WSI%20level%0A%28e.g.%20patient%20outcomes%20or%20histological%20grading%29.%20In%20this%20context%2C%20there%20is%20a%0Aneed%20for%20improved%20spatial%20interpretability%20of%20predictions%20from%20such%20models.%20We%0Apropose%20a%20novel%20method%2C%20Wsi%20rEgion%20sElection%20aPproach%20%28WEEP%29%2C%20for%20model%0Ainterpretation.%20It%20provides%20a%20principled%20yet%20straightforward%20way%20to%20establish%0Athe%20spatial%20area%20of%20WSI%20required%20for%20assigning%20a%20particular%20prediction%20label.%0AWe%20demonstrate%20WEEP%20on%20a%20binary%20classification%20task%20in%20the%20area%20of%20breast%0Acancer%20computational%20pathology.%20WEEP%20is%20easy%20to%20implement%2C%20is%20directly%0Aconnected%20to%20the%20model-based%20decision%20process%2C%20and%20offers%20information%20relevant%0Ato%20both%20research%20and%20diagnostic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15238v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology&entry.906535625=Abhinav%20Sharma%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen&entry.1292438233=%20%20Deep%20learning%20enables%20the%20modelling%20of%20high-resolution%20histopathology%0Awhole-slide%20images%20%28WSI%29.%20Weakly%20supervised%20learning%20of%20tile-level%20data%20is%0Atypically%20applied%20for%20tasks%20where%20labels%20only%20exist%20on%20the%20patient%20or%20WSI%20level%0A%28e.g.%20patient%20outcomes%20or%20histological%20grading%29.%20In%20this%20context%2C%20there%20is%20a%0Aneed%20for%20improved%20spatial%20interpretability%20of%20predictions%20from%20such%20models.%20We%0Apropose%20a%20novel%20method%2C%20Wsi%20rEgion%20sElection%20aPproach%20%28WEEP%29%2C%20for%20model%0Ainterpretation.%20It%20provides%20a%20principled%20yet%20straightforward%20way%20to%20establish%0Athe%20spatial%20area%20of%20WSI%20required%20for%20assigning%20a%20particular%20prediction%20label.%0AWe%20demonstrate%20WEEP%20on%20a%20binary%20classification%20task%20in%20the%20area%20of%20breast%0Acancer%20computational%20pathology.%20WEEP%20is%20easy%20to%20implement%2C%20is%20directly%0Aconnected%20to%20the%20model-based%20decision%20process%2C%20and%20offers%20information%20relevant%0Ato%20both%20research%20and%20diagnostic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15238v2&entry.124074799=Read"},
{"title": "LTNER: Large Language Model Tagging for Named Entity Recognition with\n  Contextualized Entity Marking", "author": "Faren Yan and Peng Yu and Xin Chen", "abstract": "  The use of LLMs for natural language processing has become a popular trend in\nthe past two years, driven by their formidable capacity for context\ncomprehension and learning, which has inspired a wave of research from\nacademics and industry professionals. However, for certain NLP tasks, such as\nNER, the performance of LLMs still falls short when compared to supervised\nlearning methods. In our research, we developed a NER processing framework\ncalled LTNER that incorporates a revolutionary Contextualized Entity Marking\nGen Method. By leveraging the cost-effective GPT-3.5 coupled with context\nlearning that does not require additional training, we significantly improved\nthe accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset\nincreased from the initial 85.9% to 91.9%, approaching the performance of\nsupervised fine-tuning. This outcome has led to a deeper understanding of the\npotential of LLMs.\n", "link": "http://arxiv.org/abs/2404.05624v1", "date": "2024-04-08", "relevancy": 1.9685, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5284}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4747}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LTNER%3A%20Large%20Language%20Model%20Tagging%20for%20Named%20Entity%20Recognition%20with%0A%20%20Contextualized%20Entity%20Marking&body=Title%3A%20LTNER%3A%20Large%20Language%20Model%20Tagging%20for%20Named%20Entity%20Recognition%20with%0A%20%20Contextualized%20Entity%20Marking%0AAuthor%3A%20Faren%20Yan%20and%20Peng%20Yu%20and%20Xin%20Chen%0AAbstract%3A%20%20%20The%20use%20of%20LLMs%20for%20natural%20language%20processing%20has%20become%20a%20popular%20trend%20in%0Athe%20past%20two%20years%2C%20driven%20by%20their%20formidable%20capacity%20for%20context%0Acomprehension%20and%20learning%2C%20which%20has%20inspired%20a%20wave%20of%20research%20from%0Aacademics%20and%20industry%20professionals.%20However%2C%20for%20certain%20NLP%20tasks%2C%20such%20as%0ANER%2C%20the%20performance%20of%20LLMs%20still%20falls%20short%20when%20compared%20to%20supervised%0Alearning%20methods.%20In%20our%20research%2C%20we%20developed%20a%20NER%20processing%20framework%0Acalled%20LTNER%20that%20incorporates%20a%20revolutionary%20Contextualized%20Entity%20Marking%0AGen%20Method.%20By%20leveraging%20the%20cost-effective%20GPT-3.5%20coupled%20with%20context%0Alearning%20that%20does%20not%20require%20additional%20training%2C%20we%20significantly%20improved%0Athe%20accuracy%20of%20LLMs%20in%20handling%20NER%20tasks.%20The%20F1%20score%20on%20the%20CoNLL03%20dataset%0Aincreased%20from%20the%20initial%2085.9%25%20to%2091.9%25%2C%20approaching%20the%20performance%20of%0Asupervised%20fine-tuning.%20This%20outcome%20has%20led%20to%20a%20deeper%20understanding%20of%20the%0Apotential%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTNER%3A%20Large%20Language%20Model%20Tagging%20for%20Named%20Entity%20Recognition%20with%0A%20%20Contextualized%20Entity%20Marking&entry.906535625=Faren%20Yan%20and%20Peng%20Yu%20and%20Xin%20Chen&entry.1292438233=%20%20The%20use%20of%20LLMs%20for%20natural%20language%20processing%20has%20become%20a%20popular%20trend%20in%0Athe%20past%20two%20years%2C%20driven%20by%20their%20formidable%20capacity%20for%20context%0Acomprehension%20and%20learning%2C%20which%20has%20inspired%20a%20wave%20of%20research%20from%0Aacademics%20and%20industry%20professionals.%20However%2C%20for%20certain%20NLP%20tasks%2C%20such%20as%0ANER%2C%20the%20performance%20of%20LLMs%20still%20falls%20short%20when%20compared%20to%20supervised%0Alearning%20methods.%20In%20our%20research%2C%20we%20developed%20a%20NER%20processing%20framework%0Acalled%20LTNER%20that%20incorporates%20a%20revolutionary%20Contextualized%20Entity%20Marking%0AGen%20Method.%20By%20leveraging%20the%20cost-effective%20GPT-3.5%20coupled%20with%20context%0Alearning%20that%20does%20not%20require%20additional%20training%2C%20we%20significantly%20improved%0Athe%20accuracy%20of%20LLMs%20in%20handling%20NER%20tasks.%20The%20F1%20score%20on%20the%20CoNLL03%20dataset%0Aincreased%20from%20the%20initial%2085.9%25%20to%2091.9%25%2C%20approaching%20the%20performance%20of%0Asupervised%20fine-tuning.%20This%20outcome%20has%20led%20to%20a%20deeper%20understanding%20of%20the%0Apotential%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05624v1&entry.124074799=Read"},
{"title": "IRCoder: Intermediate Representations Make Language Models Robust\n  Multilingual Code Generators", "author": "Indraneil Paul and Goran Glava\u0161 and Iryna Gurevych", "abstract": "  Code understanding and generation have fast become some of the most popular\napplications of language models (LMs). Nonetheless, research on multilingual\naspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual\ntransfer between different programming languages, language-specific data\naugmentation, and post-hoc LM adaptation, alongside exploitation of data\nsources other than the original textual content, has been much sparser than for\ntheir natural language counterparts. In particular, most mainstream Code-LMs\nhave been pre-trained on source code files alone. In this work, we investigate\nthe prospect of leveraging readily available compiler intermediate\nrepresentations (IR) - shared across programming languages - to improve the\nmultilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n  To this end, we first compile SLTrans, a parallel dataset consisting of\nnearly 4M self-contained source code files coupled with respective intermediate\nrepresentations. Next, starting from various base Code-LMs (ranging in size\nfrom 1.1B to 7.3B parameters), we carry out continued causal language modelling\ntraining on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)\nalign the IR constructs with respective constructs of various programming\nlanguages. Our resulting models, dubbed IRCoder, display sizeable and\nconsistent gains across a wide variety of code generation tasks and metrics,\nincluding prompt robustness, multilingual code completion, code understanding,\nand instruction following.\n", "link": "http://arxiv.org/abs/2403.03894v2", "date": "2024-04-08", "relevancy": 1.9678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5043}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4841}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4809}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IRCoder%3A%20Intermediate%20Representations%20Make%20Language%20Models%20Robust%0A%20%20Multilingual%20Code%20Generators&body=Title%3A%20IRCoder%3A%20Intermediate%20Representations%20Make%20Language%20Models%20Robust%0A%20%20Multilingual%20Code%20Generators%0AAuthor%3A%20Indraneil%20Paul%20and%20Goran%20Glava%C5%A1%20and%20Iryna%20Gurevych%0AAbstract%3A%20%20%20Code%20understanding%20and%20generation%20have%20fast%20become%20some%20of%20the%20most%20popular%0Aapplications%20of%20language%20models%20%28LMs%29.%20Nonetheless%2C%20research%20on%20multilingual%0Aaspects%20of%20Code-LMs%20%28i.e.%2C%20LMs%20for%20code%20generation%29%20such%20as%20cross-lingual%0Atransfer%20between%20different%20programming%20languages%2C%20language-specific%20data%0Aaugmentation%2C%20and%20post-hoc%20LM%20adaptation%2C%20alongside%20exploitation%20of%20data%0Asources%20other%20than%20the%20original%20textual%20content%2C%20has%20been%20much%20sparser%20than%20for%0Atheir%20natural%20language%20counterparts.%20In%20particular%2C%20most%20mainstream%20Code-LMs%0Ahave%20been%20pre-trained%20on%20source%20code%20files%20alone.%20In%20this%20work%2C%20we%20investigate%0Athe%20prospect%20of%20leveraging%20readily%20available%20compiler%20intermediate%0Arepresentations%20%28IR%29%20-%20shared%20across%20programming%20languages%20-%20to%20improve%20the%0Amultilingual%20capabilities%20of%20Code-LMs%20and%20facilitate%20cross-lingual%20transfer.%0A%20%20To%20this%20end%2C%20we%20first%20compile%20SLTrans%2C%20a%20parallel%20dataset%20consisting%20of%0Anearly%204M%20self-contained%20source%20code%20files%20coupled%20with%20respective%20intermediate%0Arepresentations.%20Next%2C%20starting%20from%20various%20base%20Code-LMs%20%28ranging%20in%20size%0Afrom%201.1B%20to%207.3B%20parameters%29%2C%20we%20carry%20out%20continued%20causal%20language%20modelling%0Atraining%20on%20SLTrans%2C%20forcing%20the%20Code-LMs%20to%20%281%29%20learn%20the%20IR%20language%20and%20%282%29%0Aalign%20the%20IR%20constructs%20with%20respective%20constructs%20of%20various%20programming%0Alanguages.%20Our%20resulting%20models%2C%20dubbed%20IRCoder%2C%20display%20sizeable%20and%0Aconsistent%20gains%20across%20a%20wide%20variety%20of%20code%20generation%20tasks%20and%20metrics%2C%0Aincluding%20prompt%20robustness%2C%20multilingual%20code%20completion%2C%20code%20understanding%2C%0Aand%20instruction%20following.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03894v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRCoder%3A%20Intermediate%20Representations%20Make%20Language%20Models%20Robust%0A%20%20Multilingual%20Code%20Generators&entry.906535625=Indraneil%20Paul%20and%20Goran%20Glava%C5%A1%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Code%20understanding%20and%20generation%20have%20fast%20become%20some%20of%20the%20most%20popular%0Aapplications%20of%20language%20models%20%28LMs%29.%20Nonetheless%2C%20research%20on%20multilingual%0Aaspects%20of%20Code-LMs%20%28i.e.%2C%20LMs%20for%20code%20generation%29%20such%20as%20cross-lingual%0Atransfer%20between%20different%20programming%20languages%2C%20language-specific%20data%0Aaugmentation%2C%20and%20post-hoc%20LM%20adaptation%2C%20alongside%20exploitation%20of%20data%0Asources%20other%20than%20the%20original%20textual%20content%2C%20has%20been%20much%20sparser%20than%20for%0Atheir%20natural%20language%20counterparts.%20In%20particular%2C%20most%20mainstream%20Code-LMs%0Ahave%20been%20pre-trained%20on%20source%20code%20files%20alone.%20In%20this%20work%2C%20we%20investigate%0Athe%20prospect%20of%20leveraging%20readily%20available%20compiler%20intermediate%0Arepresentations%20%28IR%29%20-%20shared%20across%20programming%20languages%20-%20to%20improve%20the%0Amultilingual%20capabilities%20of%20Code-LMs%20and%20facilitate%20cross-lingual%20transfer.%0A%20%20To%20this%20end%2C%20we%20first%20compile%20SLTrans%2C%20a%20parallel%20dataset%20consisting%20of%0Anearly%204M%20self-contained%20source%20code%20files%20coupled%20with%20respective%20intermediate%0Arepresentations.%20Next%2C%20starting%20from%20various%20base%20Code-LMs%20%28ranging%20in%20size%0Afrom%201.1B%20to%207.3B%20parameters%29%2C%20we%20carry%20out%20continued%20causal%20language%20modelling%0Atraining%20on%20SLTrans%2C%20forcing%20the%20Code-LMs%20to%20%281%29%20learn%20the%20IR%20language%20and%20%282%29%0Aalign%20the%20IR%20constructs%20with%20respective%20constructs%20of%20various%20programming%0Alanguages.%20Our%20resulting%20models%2C%20dubbed%20IRCoder%2C%20display%20sizeable%20and%0Aconsistent%20gains%20across%20a%20wide%20variety%20of%20code%20generation%20tasks%20and%20metrics%2C%0Aincluding%20prompt%20robustness%2C%20multilingual%20code%20completion%2C%20code%20understanding%2C%0Aand%20instruction%20following.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03894v2&entry.124074799=Read"},
{"title": "Graph Neural Networks Automated Design and Deployment on Device-Edge\n  Co-Inference Systems", "author": "Ao Zhou and Jianlei Yang and Tong Qiao and Yingjie Qi and Zhi Yang and Weisheng Zhao and Chunming Hu", "abstract": "  The key to device-edge co-inference paradigm is to partition models into\ncomputation-friendly and computation-intensive parts across the device and the\nedge, respectively. However, for Graph Neural Networks (GNNs), we find that\nsimply partitioning without altering their structures can hardly achieve the\nfull potential of the co-inference paradigm due to various\ncomputational-communication overheads of GNN operations over heterogeneous\ndevices. We present GCoDE, the first automatic framework for GNN that\ninnovatively Co-designs the architecture search and the mapping of each\noperation on Device-Edge hierarchies. GCoDE abstracts the device communication\nprocess into an explicit operation and fuses the search of architecture and the\noperations mapping in a unified space for joint-optimization. Also, the\nperformance-awareness approach, utilized in the constraint-based search process\nof GCoDE, enables effective evaluation of architecture efficiency in diverse\nheterogeneous systems. We implement the co-inference engine and runtime\ndispatcher in GCoDE to enhance the deployment efficiency. Experimental results\nshow that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy\nreduction compared to existing approaches across various applications and\nsystem configurations.\n", "link": "http://arxiv.org/abs/2404.05605v1", "date": "2024-04-08", "relevancy": 1.9598, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4863}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Automated%20Design%20and%20Deployment%20on%20Device-Edge%0A%20%20Co-Inference%20Systems&body=Title%3A%20Graph%20Neural%20Networks%20Automated%20Design%20and%20Deployment%20on%20Device-Edge%0A%20%20Co-Inference%20Systems%0AAuthor%3A%20Ao%20Zhou%20and%20Jianlei%20Yang%20and%20Tong%20Qiao%20and%20Yingjie%20Qi%20and%20Zhi%20Yang%20and%20Weisheng%20Zhao%20and%20Chunming%20Hu%0AAbstract%3A%20%20%20The%20key%20to%20device-edge%20co-inference%20paradigm%20is%20to%20partition%20models%20into%0Acomputation-friendly%20and%20computation-intensive%20parts%20across%20the%20device%20and%20the%0Aedge%2C%20respectively.%20However%2C%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20find%20that%0Asimply%20partitioning%20without%20altering%20their%20structures%20can%20hardly%20achieve%20the%0Afull%20potential%20of%20the%20co-inference%20paradigm%20due%20to%20various%0Acomputational-communication%20overheads%20of%20GNN%20operations%20over%20heterogeneous%0Adevices.%20We%20present%20GCoDE%2C%20the%20first%20automatic%20framework%20for%20GNN%20that%0Ainnovatively%20Co-designs%20the%20architecture%20search%20and%20the%20mapping%20of%20each%0Aoperation%20on%20Device-Edge%20hierarchies.%20GCoDE%20abstracts%20the%20device%20communication%0Aprocess%20into%20an%20explicit%20operation%20and%20fuses%20the%20search%20of%20architecture%20and%20the%0Aoperations%20mapping%20in%20a%20unified%20space%20for%20joint-optimization.%20Also%2C%20the%0Aperformance-awareness%20approach%2C%20utilized%20in%20the%20constraint-based%20search%20process%0Aof%20GCoDE%2C%20enables%20effective%20evaluation%20of%20architecture%20efficiency%20in%20diverse%0Aheterogeneous%20systems.%20We%20implement%20the%20co-inference%20engine%20and%20runtime%0Adispatcher%20in%20GCoDE%20to%20enhance%20the%20deployment%20efficiency.%20Experimental%20results%0Ashow%20that%20GCoDE%20can%20achieve%20up%20to%20%2444.9%5Ctimes%24%20speedup%20and%20%2498.2%5C%25%24%20energy%0Areduction%20compared%20to%20existing%20approaches%20across%20various%20applications%20and%0Asystem%20configurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Automated%20Design%20and%20Deployment%20on%20Device-Edge%0A%20%20Co-Inference%20Systems&entry.906535625=Ao%20Zhou%20and%20Jianlei%20Yang%20and%20Tong%20Qiao%20and%20Yingjie%20Qi%20and%20Zhi%20Yang%20and%20Weisheng%20Zhao%20and%20Chunming%20Hu&entry.1292438233=%20%20The%20key%20to%20device-edge%20co-inference%20paradigm%20is%20to%20partition%20models%20into%0Acomputation-friendly%20and%20computation-intensive%20parts%20across%20the%20device%20and%20the%0Aedge%2C%20respectively.%20However%2C%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20find%20that%0Asimply%20partitioning%20without%20altering%20their%20structures%20can%20hardly%20achieve%20the%0Afull%20potential%20of%20the%20co-inference%20paradigm%20due%20to%20various%0Acomputational-communication%20overheads%20of%20GNN%20operations%20over%20heterogeneous%0Adevices.%20We%20present%20GCoDE%2C%20the%20first%20automatic%20framework%20for%20GNN%20that%0Ainnovatively%20Co-designs%20the%20architecture%20search%20and%20the%20mapping%20of%20each%0Aoperation%20on%20Device-Edge%20hierarchies.%20GCoDE%20abstracts%20the%20device%20communication%0Aprocess%20into%20an%20explicit%20operation%20and%20fuses%20the%20search%20of%20architecture%20and%20the%0Aoperations%20mapping%20in%20a%20unified%20space%20for%20joint-optimization.%20Also%2C%20the%0Aperformance-awareness%20approach%2C%20utilized%20in%20the%20constraint-based%20search%20process%0Aof%20GCoDE%2C%20enables%20effective%20evaluation%20of%20architecture%20efficiency%20in%20diverse%0Aheterogeneous%20systems.%20We%20implement%20the%20co-inference%20engine%20and%20runtime%0Adispatcher%20in%20GCoDE%20to%20enhance%20the%20deployment%20efficiency.%20Experimental%20results%0Ashow%20that%20GCoDE%20can%20achieve%20up%20to%20%2444.9%5Ctimes%24%20speedup%20and%20%2498.2%5C%25%24%20energy%0Areduction%20compared%20to%20existing%20approaches%20across%20various%20applications%20and%0Asystem%20configurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05605v1&entry.124074799=Read"},
{"title": "Technical Report: The Graph Spectral Token -- Enhancing Graph\n  Transformers with Spectral Information", "author": "Zihan Pengmei and Zimu Li", "abstract": "  Graph Transformers have emerged as a powerful alternative to Message-Passing\nGraph Neural Networks (MP-GNNs) to address limitations such as over-squashing\nof information exchange. However, incorporating graph inductive bias into\ntransformer architectures remains a significant challenge. In this report, we\npropose the Graph Spectral Token, a novel approach to directly encode graph\nspectral information, which captures the global structure of the graph, into\nthe transformer architecture. By parameterizing the auxiliary [CLS] token and\nleaving other tokens representing graph nodes, our method seamlessly integrates\nspectral information into the learning process. We benchmark the effectiveness\nof our approach by enhancing two existing graph transformers, GraphTrans and\nSubFormer. The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10%\nimprovements on large graph benchmark datasets while maintaining efficiency\ncomparable to MP-GNNs. SubFormer-Spec demonstrates strong performance across\nvarious datasets.\n", "link": "http://arxiv.org/abs/2404.05604v1", "date": "2024-04-08", "relevancy": 1.9506, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5474}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5123}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%3A%20The%20Graph%20Spectral%20Token%20--%20Enhancing%20Graph%0A%20%20Transformers%20with%20Spectral%20Information&body=Title%3A%20Technical%20Report%3A%20The%20Graph%20Spectral%20Token%20--%20Enhancing%20Graph%0A%20%20Transformers%20with%20Spectral%20Information%0AAuthor%3A%20Zihan%20Pengmei%20and%20Zimu%20Li%0AAbstract%3A%20%20%20Graph%20Transformers%20have%20emerged%20as%20a%20powerful%20alternative%20to%20Message-Passing%0AGraph%20Neural%20Networks%20%28MP-GNNs%29%20to%20address%20limitations%20such%20as%20over-squashing%0Aof%20information%20exchange.%20However%2C%20incorporating%20graph%20inductive%20bias%20into%0Atransformer%20architectures%20remains%20a%20significant%20challenge.%20In%20this%20report%2C%20we%0Apropose%20the%20Graph%20Spectral%20Token%2C%20a%20novel%20approach%20to%20directly%20encode%20graph%0Aspectral%20information%2C%20which%20captures%20the%20global%20structure%20of%20the%20graph%2C%20into%0Athe%20transformer%20architecture.%20By%20parameterizing%20the%20auxiliary%20%5BCLS%5D%20token%20and%0Aleaving%20other%20tokens%20representing%20graph%20nodes%2C%20our%20method%20seamlessly%20integrates%0Aspectral%20information%20into%20the%20learning%20process.%20We%20benchmark%20the%20effectiveness%0Aof%20our%20approach%20by%20enhancing%20two%20existing%20graph%20transformers%2C%20GraphTrans%20and%0ASubFormer.%20The%20improved%20GraphTrans%2C%20dubbed%20GraphTrans-Spec%2C%20achieves%20over%2010%25%0Aimprovements%20on%20large%20graph%20benchmark%20datasets%20while%20maintaining%20efficiency%0Acomparable%20to%20MP-GNNs.%20SubFormer-Spec%20demonstrates%20strong%20performance%20across%0Avarious%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05604v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%3A%20The%20Graph%20Spectral%20Token%20--%20Enhancing%20Graph%0A%20%20Transformers%20with%20Spectral%20Information&entry.906535625=Zihan%20Pengmei%20and%20Zimu%20Li&entry.1292438233=%20%20Graph%20Transformers%20have%20emerged%20as%20a%20powerful%20alternative%20to%20Message-Passing%0AGraph%20Neural%20Networks%20%28MP-GNNs%29%20to%20address%20limitations%20such%20as%20over-squashing%0Aof%20information%20exchange.%20However%2C%20incorporating%20graph%20inductive%20bias%20into%0Atransformer%20architectures%20remains%20a%20significant%20challenge.%20In%20this%20report%2C%20we%0Apropose%20the%20Graph%20Spectral%20Token%2C%20a%20novel%20approach%20to%20directly%20encode%20graph%0Aspectral%20information%2C%20which%20captures%20the%20global%20structure%20of%20the%20graph%2C%20into%0Athe%20transformer%20architecture.%20By%20parameterizing%20the%20auxiliary%20%5BCLS%5D%20token%20and%0Aleaving%20other%20tokens%20representing%20graph%20nodes%2C%20our%20method%20seamlessly%20integrates%0Aspectral%20information%20into%20the%20learning%20process.%20We%20benchmark%20the%20effectiveness%0Aof%20our%20approach%20by%20enhancing%20two%20existing%20graph%20transformers%2C%20GraphTrans%20and%0ASubFormer.%20The%20improved%20GraphTrans%2C%20dubbed%20GraphTrans-Spec%2C%20achieves%20over%2010%25%0Aimprovements%20on%20large%20graph%20benchmark%20datasets%20while%20maintaining%20efficiency%0Acomparable%20to%20MP-GNNs.%20SubFormer-Spec%20demonstrates%20strong%20performance%20across%0Avarious%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05604v1&entry.124074799=Read"},
{"title": "Unsupervised Training of Convex Regularizers using Maximum Likelihood\n  Estimation", "author": "Hong Ye Tan and Ziruo Cai and Marcelo Pereyra and Subhadip Mukherjee and Junqi Tang and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Unsupervised learning is a training approach in the situation where ground\ntruth data is unavailable, such as inverse imaging problems. We present an\nunsupervised Bayesian training approach to learning convex neural network\nregularizers using a fixed noisy dataset, based on a dual Markov chain\nestimation method. Compared to classical supervised adversarial regularization\nmethods, where there is access to both clean images as well as unlimited to\nnoisy copies, we demonstrate close performance on natural image Gaussian\ndeconvolution and Poisson denoising tasks.\n", "link": "http://arxiv.org/abs/2404.05445v1", "date": "2024-04-08", "relevancy": 1.9454, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5067}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4723}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4716}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&body=Title%3A%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation%0AAuthor%3A%20Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Unsupervised%20learning%20is%20a%20training%20approach%20in%20the%20situation%20where%20ground%0Atruth%20data%20is%20unavailable%2C%20such%20as%20inverse%20imaging%20problems.%20We%20present%20an%0Aunsupervised%20Bayesian%20training%20approach%20to%20learning%20convex%20neural%20network%0Aregularizers%20using%20a%20fixed%20noisy%20dataset%2C%20based%20on%20a%20dual%20Markov%20chain%0Aestimation%20method.%20Compared%20to%20classical%20supervised%20adversarial%20regularization%0Amethods%2C%20where%20there%20is%20access%20to%20both%20clean%20images%20as%20well%20as%20unlimited%20to%0Anoisy%20copies%2C%20we%20demonstrate%20close%20performance%20on%20natural%20image%20Gaussian%0Adeconvolution%20and%20Poisson%20denoising%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05445v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&entry.906535625=Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Unsupervised%20learning%20is%20a%20training%20approach%20in%20the%20situation%20where%20ground%0Atruth%20data%20is%20unavailable%2C%20such%20as%20inverse%20imaging%20problems.%20We%20present%20an%0Aunsupervised%20Bayesian%20training%20approach%20to%20learning%20convex%20neural%20network%0Aregularizers%20using%20a%20fixed%20noisy%20dataset%2C%20based%20on%20a%20dual%20Markov%20chain%0Aestimation%20method.%20Compared%20to%20classical%20supervised%20adversarial%20regularization%0Amethods%2C%20where%20there%20is%20access%20to%20both%20clean%20images%20as%20well%20as%20unlimited%20to%0Anoisy%20copies%2C%20we%20demonstrate%20close%20performance%20on%20natural%20image%20Gaussian%0Adeconvolution%20and%20Poisson%20denoising%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05445v1&entry.124074799=Read"},
{"title": "Impact of LiDAR visualisations on semantic segmentation of\n  archaeological objects", "author": "Raveerat Jaturapitpornchai and Giulio Poggi and Gregory Sech and Ziga Kokalj and Marco Fiorucci and Arianna Traviglia", "abstract": "  Deep learning methods in LiDAR-based archaeological research often leverage\nvisualisation techniques derived from Digital Elevation Models to enhance\ncharacteristics of archaeological objects present in the images. This paper\ninvestigates the impact of visualisations on deep learning performance through\na comprehensive testing framework. The study involves the use of eight semantic\nsegmentation models to evaluate seven diverse visualisations across two study\nareas, encompassing five archaeological classes. Experimental results reveal\nthat the choice of appropriate visualisations can influence performance by up\nto 8%. Yet, pinpointing one visualisation that outperforms the others in\nsegmenting all archaeological classes proves challenging. The observed\nperformance variation, reaching up to 25% across different model\nconfigurations, underscores the importance of thoughtfully selecting model\nconfigurations and LiDAR visualisations for successfully segmenting\narchaeological objects.\n", "link": "http://arxiv.org/abs/2404.05512v1", "date": "2024-04-08", "relevancy": 1.9405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4755}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20LiDAR%20visualisations%20on%20semantic%20segmentation%20of%0A%20%20archaeological%20objects&body=Title%3A%20Impact%20of%20LiDAR%20visualisations%20on%20semantic%20segmentation%20of%0A%20%20archaeological%20objects%0AAuthor%3A%20Raveerat%20Jaturapitpornchai%20and%20Giulio%20Poggi%20and%20Gregory%20Sech%20and%20Ziga%20Kokalj%20and%20Marco%20Fiorucci%20and%20Arianna%20Traviglia%0AAbstract%3A%20%20%20Deep%20learning%20methods%20in%20LiDAR-based%20archaeological%20research%20often%20leverage%0Avisualisation%20techniques%20derived%20from%20Digital%20Elevation%20Models%20to%20enhance%0Acharacteristics%20of%20archaeological%20objects%20present%20in%20the%20images.%20This%20paper%0Ainvestigates%20the%20impact%20of%20visualisations%20on%20deep%20learning%20performance%20through%0Aa%20comprehensive%20testing%20framework.%20The%20study%20involves%20the%20use%20of%20eight%20semantic%0Asegmentation%20models%20to%20evaluate%20seven%20diverse%20visualisations%20across%20two%20study%0Aareas%2C%20encompassing%20five%20archaeological%20classes.%20Experimental%20results%20reveal%0Athat%20the%20choice%20of%20appropriate%20visualisations%20can%20influence%20performance%20by%20up%0Ato%208%25.%20Yet%2C%20pinpointing%20one%20visualisation%20that%20outperforms%20the%20others%20in%0Asegmenting%20all%20archaeological%20classes%20proves%20challenging.%20The%20observed%0Aperformance%20variation%2C%20reaching%20up%20to%2025%25%20across%20different%20model%0Aconfigurations%2C%20underscores%20the%20importance%20of%20thoughtfully%20selecting%20model%0Aconfigurations%20and%20LiDAR%20visualisations%20for%20successfully%20segmenting%0Aarchaeological%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05512v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20LiDAR%20visualisations%20on%20semantic%20segmentation%20of%0A%20%20archaeological%20objects&entry.906535625=Raveerat%20Jaturapitpornchai%20and%20Giulio%20Poggi%20and%20Gregory%20Sech%20and%20Ziga%20Kokalj%20and%20Marco%20Fiorucci%20and%20Arianna%20Traviglia&entry.1292438233=%20%20Deep%20learning%20methods%20in%20LiDAR-based%20archaeological%20research%20often%20leverage%0Avisualisation%20techniques%20derived%20from%20Digital%20Elevation%20Models%20to%20enhance%0Acharacteristics%20of%20archaeological%20objects%20present%20in%20the%20images.%20This%20paper%0Ainvestigates%20the%20impact%20of%20visualisations%20on%20deep%20learning%20performance%20through%0Aa%20comprehensive%20testing%20framework.%20The%20study%20involves%20the%20use%20of%20eight%20semantic%0Asegmentation%20models%20to%20evaluate%20seven%20diverse%20visualisations%20across%20two%20study%0Aareas%2C%20encompassing%20five%20archaeological%20classes.%20Experimental%20results%20reveal%0Athat%20the%20choice%20of%20appropriate%20visualisations%20can%20influence%20performance%20by%20up%0Ato%208%25.%20Yet%2C%20pinpointing%20one%20visualisation%20that%20outperforms%20the%20others%20in%0Asegmenting%20all%20archaeological%20classes%20proves%20challenging.%20The%20observed%0Aperformance%20variation%2C%20reaching%20up%20to%2025%25%20across%20different%20model%0Aconfigurations%2C%20underscores%20the%20importance%20of%20thoughtfully%20selecting%20model%0Aconfigurations%20and%20LiDAR%20visualisations%20for%20successfully%20segmenting%0Aarchaeological%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05512v1&entry.124074799=Read"},
{"title": "Robust Data Pruning: Uncovering and Overcoming Implicit Bias", "author": "Artem Vysogorets and Kartik Ahuja and Julia Kempe", "abstract": "  In the era of exceptionally data-hungry models, careful selection of the\ntraining data is essential to mitigate the extensive costs of deep learning.\nData pruning offers a solution by removing redundant or uninformative samples\nfrom the dataset, which yields faster convergence and improved neural scaling\nlaws. However, little is known about its impact on classification bias of the\ntrained models. We conduct the first systematic study of this effect and reveal\nthat existing data pruning algorithms can produce highly biased classifiers. At\nthe same time, we argue that random data pruning with appropriate class ratios\nhas potential to improve the worst-class performance. We propose a\n\"fairness-aware\" approach to pruning and empirically demonstrate its\nperformance on standard computer vision benchmarks. In sharp contrast to\nexisting algorithms, our proposed method continues improving robustness at a\ntolerable drop of average performance as we prune more from the datasets. We\npresent theoretical analysis of the classification risk in a mixture of\nGaussians to further motivate our algorithm and support our findings.\n", "link": "http://arxiv.org/abs/2404.05579v1", "date": "2024-04-08", "relevancy": 1.9394, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4721}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Data%20Pruning%3A%20Uncovering%20and%20Overcoming%20Implicit%20Bias&body=Title%3A%20Robust%20Data%20Pruning%3A%20Uncovering%20and%20Overcoming%20Implicit%20Bias%0AAuthor%3A%20Artem%20Vysogorets%20and%20Kartik%20Ahuja%20and%20Julia%20Kempe%0AAbstract%3A%20%20%20In%20the%20era%20of%20exceptionally%20data-hungry%20models%2C%20careful%20selection%20of%20the%0Atraining%20data%20is%20essential%20to%20mitigate%20the%20extensive%20costs%20of%20deep%20learning.%0AData%20pruning%20offers%20a%20solution%20by%20removing%20redundant%20or%20uninformative%20samples%0Afrom%20the%20dataset%2C%20which%20yields%20faster%20convergence%20and%20improved%20neural%20scaling%0Alaws.%20However%2C%20little%20is%20known%20about%20its%20impact%20on%20classification%20bias%20of%20the%0Atrained%20models.%20We%20conduct%20the%20first%20systematic%20study%20of%20this%20effect%20and%20reveal%0Athat%20existing%20data%20pruning%20algorithms%20can%20produce%20highly%20biased%20classifiers.%20At%0Athe%20same%20time%2C%20we%20argue%20that%20random%20data%20pruning%20with%20appropriate%20class%20ratios%0Ahas%20potential%20to%20improve%20the%20worst-class%20performance.%20We%20propose%20a%0A%22fairness-aware%22%20approach%20to%20pruning%20and%20empirically%20demonstrate%20its%0Aperformance%20on%20standard%20computer%20vision%20benchmarks.%20In%20sharp%20contrast%20to%0Aexisting%20algorithms%2C%20our%20proposed%20method%20continues%20improving%20robustness%20at%20a%0Atolerable%20drop%20of%20average%20performance%20as%20we%20prune%20more%20from%20the%20datasets.%20We%0Apresent%20theoretical%20analysis%20of%20the%20classification%20risk%20in%20a%20mixture%20of%0AGaussians%20to%20further%20motivate%20our%20algorithm%20and%20support%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Data%20Pruning%3A%20Uncovering%20and%20Overcoming%20Implicit%20Bias&entry.906535625=Artem%20Vysogorets%20and%20Kartik%20Ahuja%20and%20Julia%20Kempe&entry.1292438233=%20%20In%20the%20era%20of%20exceptionally%20data-hungry%20models%2C%20careful%20selection%20of%20the%0Atraining%20data%20is%20essential%20to%20mitigate%20the%20extensive%20costs%20of%20deep%20learning.%0AData%20pruning%20offers%20a%20solution%20by%20removing%20redundant%20or%20uninformative%20samples%0Afrom%20the%20dataset%2C%20which%20yields%20faster%20convergence%20and%20improved%20neural%20scaling%0Alaws.%20However%2C%20little%20is%20known%20about%20its%20impact%20on%20classification%20bias%20of%20the%0Atrained%20models.%20We%20conduct%20the%20first%20systematic%20study%20of%20this%20effect%20and%20reveal%0Athat%20existing%20data%20pruning%20algorithms%20can%20produce%20highly%20biased%20classifiers.%20At%0Athe%20same%20time%2C%20we%20argue%20that%20random%20data%20pruning%20with%20appropriate%20class%20ratios%0Ahas%20potential%20to%20improve%20the%20worst-class%20performance.%20We%20propose%20a%0A%22fairness-aware%22%20approach%20to%20pruning%20and%20empirically%20demonstrate%20its%0Aperformance%20on%20standard%20computer%20vision%20benchmarks.%20In%20sharp%20contrast%20to%0Aexisting%20algorithms%2C%20our%20proposed%20method%20continues%20improving%20robustness%20at%20a%0Atolerable%20drop%20of%20average%20performance%20as%20we%20prune%20more%20from%20the%20datasets.%20We%0Apresent%20theoretical%20analysis%20of%20the%20classification%20risk%20in%20a%20mixture%20of%0AGaussians%20to%20further%20motivate%20our%20algorithm%20and%20support%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05579v1&entry.124074799=Read"},
{"title": "Pansharpening of PRISMA products for archaeological prospection", "author": "Gregory Sech and Giulio Poggi and Marina Ljubenovic and Marco Fiorucci and Arianna Traviglia", "abstract": "  Hyperspectral data recorded from satellite platforms are often ill-suited for\ngeo-archaeological prospection due to low spatial resolution. The established\npotential of hyperspectral data from airborne sensors in identifying\narchaeological features has, on the other side, generated increased interest in\nenhancing hyperspectral data to achieve higher spatial resolution. This\nimprovement is crucial for detecting traces linked to sub-surface\ngeo-archaeological features and can make satellite hyperspectral acquisitions\nmore suitable for archaeological research. This research assesses the usability\nof pansharpened PRISMA satellite products in geo-archaeological prospections.\nThree pan-sharpening methods (GSA, MTF-GLP and HySure) are compared\nquantitatively and qualitatively and tested over the archaeological landscape\nof Aquileia (Italy). The results suggest that the application of pansharpening\ntechniques makes hyperspectral satellite imagery highly suitable, under certain\nconditions, to the identification of sub-surface archaeological features of\nsmall and large size.\n", "link": "http://arxiv.org/abs/2404.05447v1", "date": "2024-04-08", "relevancy": 1.9345, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3615}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3501}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pansharpening%20of%20PRISMA%20products%20for%20archaeological%20prospection&body=Title%3A%20Pansharpening%20of%20PRISMA%20products%20for%20archaeological%20prospection%0AAuthor%3A%20Gregory%20Sech%20and%20Giulio%20Poggi%20and%20Marina%20Ljubenovic%20and%20Marco%20Fiorucci%20and%20Arianna%20Traviglia%0AAbstract%3A%20%20%20Hyperspectral%20data%20recorded%20from%20satellite%20platforms%20are%20often%20ill-suited%20for%0Ageo-archaeological%20prospection%20due%20to%20low%20spatial%20resolution.%20The%20established%0Apotential%20of%20hyperspectral%20data%20from%20airborne%20sensors%20in%20identifying%0Aarchaeological%20features%20has%2C%20on%20the%20other%20side%2C%20generated%20increased%20interest%20in%0Aenhancing%20hyperspectral%20data%20to%20achieve%20higher%20spatial%20resolution.%20This%0Aimprovement%20is%20crucial%20for%20detecting%20traces%20linked%20to%20sub-surface%0Ageo-archaeological%20features%20and%20can%20make%20satellite%20hyperspectral%20acquisitions%0Amore%20suitable%20for%20archaeological%20research.%20This%20research%20assesses%20the%20usability%0Aof%20pansharpened%20PRISMA%20satellite%20products%20in%20geo-archaeological%20prospections.%0AThree%20pan-sharpening%20methods%20%28GSA%2C%20MTF-GLP%20and%20HySure%29%20are%20compared%0Aquantitatively%20and%20qualitatively%20and%20tested%20over%20the%20archaeological%20landscape%0Aof%20Aquileia%20%28Italy%29.%20The%20results%20suggest%20that%20the%20application%20of%20pansharpening%0Atechniques%20makes%20hyperspectral%20satellite%20imagery%20highly%20suitable%2C%20under%20certain%0Aconditions%2C%20to%20the%20identification%20of%20sub-surface%20archaeological%20features%20of%0Asmall%20and%20large%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05447v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pansharpening%20of%20PRISMA%20products%20for%20archaeological%20prospection&entry.906535625=Gregory%20Sech%20and%20Giulio%20Poggi%20and%20Marina%20Ljubenovic%20and%20Marco%20Fiorucci%20and%20Arianna%20Traviglia&entry.1292438233=%20%20Hyperspectral%20data%20recorded%20from%20satellite%20platforms%20are%20often%20ill-suited%20for%0Ageo-archaeological%20prospection%20due%20to%20low%20spatial%20resolution.%20The%20established%0Apotential%20of%20hyperspectral%20data%20from%20airborne%20sensors%20in%20identifying%0Aarchaeological%20features%20has%2C%20on%20the%20other%20side%2C%20generated%20increased%20interest%20in%0Aenhancing%20hyperspectral%20data%20to%20achieve%20higher%20spatial%20resolution.%20This%0Aimprovement%20is%20crucial%20for%20detecting%20traces%20linked%20to%20sub-surface%0Ageo-archaeological%20features%20and%20can%20make%20satellite%20hyperspectral%20acquisitions%0Amore%20suitable%20for%20archaeological%20research.%20This%20research%20assesses%20the%20usability%0Aof%20pansharpened%20PRISMA%20satellite%20products%20in%20geo-archaeological%20prospections.%0AThree%20pan-sharpening%20methods%20%28GSA%2C%20MTF-GLP%20and%20HySure%29%20are%20compared%0Aquantitatively%20and%20qualitatively%20and%20tested%20over%20the%20archaeological%20landscape%0Aof%20Aquileia%20%28Italy%29.%20The%20results%20suggest%20that%20the%20application%20of%20pansharpening%0Atechniques%20makes%20hyperspectral%20satellite%20imagery%20highly%20suitable%2C%20under%20certain%0Aconditions%2C%20to%20the%20identification%20of%20sub-surface%20archaeological%20features%20of%0Asmall%20and%20large%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05447v1&entry.124074799=Read"},
{"title": "Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "author": "Wenjing Wang and Huan Yang and Zixi Tuo and Huiguo He and Junchen Zhu and Jianlong Fu and Jiaying Liu", "abstract": "  With the explosive popularity of AI-generated content (AIGC), video\ngeneration has recently received a lot of attention. Generating videos guided\nby text instructions poses significant challenges, such as modeling the complex\nrelationship between space and time, and the lack of large-scale text-video\npaired data. Existing text-video datasets suffer from limitations in both\ncontent quality and scale, or they are not open-source, rendering them\ninaccessible for study and use. For model design, previous approaches extend\npretrained text-to-image generation models by adding temporal 1D\nconvolution/attention modules for video generation. However, these approaches\noverlook the importance of jointly modeling space and time, inevitably leading\nto temporal distortions and misalignment between texts and videos. In this\npaper, we propose a novel approach that strengthens the interaction between\nspatial and temporal perceptions. In particular, we utilize a swapped\ncross-attention mechanism in 3D windows that alternates the ``query'' role\nbetween spatial and temporal blocks, enabling mutual reinforcement for each\nother. Moreover, to fully unlock model capabilities for high-quality video\ngeneration and promote the development of the field, we curate a large-scale\nand open-source video dataset called HD-VG-130M. This dataset comprises 130\nmillion text-video pairs from the open-domain, ensuring high-definition,\nwidescreen and watermark-free characters. A smaller-scale yet more meticulously\ncleaned subset further enhances the data quality, aiding models in achieving\nsuperior performance. Experimental quantitative and qualitative results\ndemonstrate the superiority of our approach in terms of per-frame quality,\ntemporal correlation, and text-video alignment, with clear margins.\n", "link": "http://arxiv.org/abs/2305.10874v3", "date": "2024-04-08", "relevancy": 1.9302, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6709}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5929}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Swap%20Attention%20in%20Spatiotemporal%20Diffusions%20for%20Text-to-Video%20Generation&body=Title%3A%20Swap%20Attention%20in%20Spatiotemporal%20Diffusions%20for%20Text-to-Video%20Generation%0AAuthor%3A%20Wenjing%20Wang%20and%20Huan%20Yang%20and%20Zixi%20Tuo%20and%20Huiguo%20He%20and%20Junchen%20Zhu%20and%20Jianlong%20Fu%20and%20Jiaying%20Liu%0AAbstract%3A%20%20%20With%20the%20explosive%20popularity%20of%20AI-generated%20content%20%28AIGC%29%2C%20video%0Ageneration%20has%20recently%20received%20a%20lot%20of%20attention.%20Generating%20videos%20guided%0Aby%20text%20instructions%20poses%20significant%20challenges%2C%20such%20as%20modeling%20the%20complex%0Arelationship%20between%20space%20and%20time%2C%20and%20the%20lack%20of%20large-scale%20text-video%0Apaired%20data.%20Existing%20text-video%20datasets%20suffer%20from%20limitations%20in%20both%0Acontent%20quality%20and%20scale%2C%20or%20they%20are%20not%20open-source%2C%20rendering%20them%0Ainaccessible%20for%20study%20and%20use.%20For%20model%20design%2C%20previous%20approaches%20extend%0Apretrained%20text-to-image%20generation%20models%20by%20adding%20temporal%201D%0Aconvolution/attention%20modules%20for%20video%20generation.%20However%2C%20these%20approaches%0Aoverlook%20the%20importance%20of%20jointly%20modeling%20space%20and%20time%2C%20inevitably%20leading%0Ato%20temporal%20distortions%20and%20misalignment%20between%20texts%20and%20videos.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20that%20strengthens%20the%20interaction%20between%0Aspatial%20and%20temporal%20perceptions.%20In%20particular%2C%20we%20utilize%20a%20swapped%0Across-attention%20mechanism%20in%203D%20windows%20that%20alternates%20the%20%60%60query%27%27%20role%0Abetween%20spatial%20and%20temporal%20blocks%2C%20enabling%20mutual%20reinforcement%20for%20each%0Aother.%20Moreover%2C%20to%20fully%20unlock%20model%20capabilities%20for%20high-quality%20video%0Ageneration%20and%20promote%20the%20development%20of%20the%20field%2C%20we%20curate%20a%20large-scale%0Aand%20open-source%20video%20dataset%20called%20HD-VG-130M.%20This%20dataset%20comprises%20130%0Amillion%20text-video%20pairs%20from%20the%20open-domain%2C%20ensuring%20high-definition%2C%0Awidescreen%20and%20watermark-free%20characters.%20A%20smaller-scale%20yet%20more%20meticulously%0Acleaned%20subset%20further%20enhances%20the%20data%20quality%2C%20aiding%20models%20in%20achieving%0Asuperior%20performance.%20Experimental%20quantitative%20and%20qualitative%20results%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%20per-frame%20quality%2C%0Atemporal%20correlation%2C%20and%20text-video%20alignment%2C%20with%20clear%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10874v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swap%20Attention%20in%20Spatiotemporal%20Diffusions%20for%20Text-to-Video%20Generation&entry.906535625=Wenjing%20Wang%20and%20Huan%20Yang%20and%20Zixi%20Tuo%20and%20Huiguo%20He%20and%20Junchen%20Zhu%20and%20Jianlong%20Fu%20and%20Jiaying%20Liu&entry.1292438233=%20%20With%20the%20explosive%20popularity%20of%20AI-generated%20content%20%28AIGC%29%2C%20video%0Ageneration%20has%20recently%20received%20a%20lot%20of%20attention.%20Generating%20videos%20guided%0Aby%20text%20instructions%20poses%20significant%20challenges%2C%20such%20as%20modeling%20the%20complex%0Arelationship%20between%20space%20and%20time%2C%20and%20the%20lack%20of%20large-scale%20text-video%0Apaired%20data.%20Existing%20text-video%20datasets%20suffer%20from%20limitations%20in%20both%0Acontent%20quality%20and%20scale%2C%20or%20they%20are%20not%20open-source%2C%20rendering%20them%0Ainaccessible%20for%20study%20and%20use.%20For%20model%20design%2C%20previous%20approaches%20extend%0Apretrained%20text-to-image%20generation%20models%20by%20adding%20temporal%201D%0Aconvolution/attention%20modules%20for%20video%20generation.%20However%2C%20these%20approaches%0Aoverlook%20the%20importance%20of%20jointly%20modeling%20space%20and%20time%2C%20inevitably%20leading%0Ato%20temporal%20distortions%20and%20misalignment%20between%20texts%20and%20videos.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20that%20strengthens%20the%20interaction%20between%0Aspatial%20and%20temporal%20perceptions.%20In%20particular%2C%20we%20utilize%20a%20swapped%0Across-attention%20mechanism%20in%203D%20windows%20that%20alternates%20the%20%60%60query%27%27%20role%0Abetween%20spatial%20and%20temporal%20blocks%2C%20enabling%20mutual%20reinforcement%20for%20each%0Aother.%20Moreover%2C%20to%20fully%20unlock%20model%20capabilities%20for%20high-quality%20video%0Ageneration%20and%20promote%20the%20development%20of%20the%20field%2C%20we%20curate%20a%20large-scale%0Aand%20open-source%20video%20dataset%20called%20HD-VG-130M.%20This%20dataset%20comprises%20130%0Amillion%20text-video%20pairs%20from%20the%20open-domain%2C%20ensuring%20high-definition%2C%0Awidescreen%20and%20watermark-free%20characters.%20A%20smaller-scale%20yet%20more%20meticulously%0Acleaned%20subset%20further%20enhances%20the%20data%20quality%2C%20aiding%20models%20in%20achieving%0Asuperior%20performance.%20Experimental%20quantitative%20and%20qualitative%20results%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%20per-frame%20quality%2C%0Atemporal%20correlation%2C%20and%20text-video%20alignment%2C%20with%20clear%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10874v3&entry.124074799=Read"},
{"title": "UniFL: Improve Stable Diffusion via Unified Feedback Learning", "author": "Jiacheng Zhang and Jie Wu and Yuxi Ren and Xin Xia and Huafeng Kuang and Pan Xie and Jiashi Li and Xuefeng Xiao and Weilin Huang and Min Zheng and Lean Fu and Guanbin Li", "abstract": "  Diffusion models have revolutionized the field of image generation, leading\nto the proliferation of high-quality models and diverse downstream\napplications. However, despite these significant advancements, the current\ncompetitive solutions still suffer from several limitations, including inferior\nvisual quality, a lack of aesthetic appeal, and inefficient inference, without\na comprehensive solution in sight. To address these challenges, we present\nUniFL, a unified framework that leverages feedback learning to enhance\ndiffusion models comprehensively. UniFL stands out as a universal, effective,\nand generalizable solution applicable to various diffusion models, such as\nSD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual\nfeedback learning, which enhances visual quality; decoupled feedback learning,\nwhich improves aesthetic appeal; and adversarial feedback learning, which\noptimizes inference speed. In-depth experiments and extensive user studies\nvalidate the superior performance of our proposed method in enhancing both the\nquality of generated models and their acceleration. For instance, UniFL\nsurpasses ImageReward by 17% user preference in terms of generation quality and\noutperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we\nhave verified the efficacy of our approach in downstream tasks, including Lora,\nControlNet, and AnimateDiff.\n", "link": "http://arxiv.org/abs/2404.05595v1", "date": "2024-04-08", "relevancy": 1.9087, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6925}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6365}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6137}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UniFL%3A%20Improve%20Stable%20Diffusion%20via%20Unified%20Feedback%20Learning&body=Title%3A%20UniFL%3A%20Improve%20Stable%20Diffusion%20via%20Unified%20Feedback%20Learning%0AAuthor%3A%20Jiacheng%20Zhang%20and%20Jie%20Wu%20and%20Yuxi%20Ren%20and%20Xin%20Xia%20and%20Huafeng%20Kuang%20and%20Pan%20Xie%20and%20Jiashi%20Li%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Min%20Zheng%20and%20Lean%20Fu%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20have%20revolutionized%20the%20field%20of%20image%20generation%2C%20leading%0Ato%20the%20proliferation%20of%20high-quality%20models%20and%20diverse%20downstream%0Aapplications.%20However%2C%20despite%20these%20significant%20advancements%2C%20the%20current%0Acompetitive%20solutions%20still%20suffer%20from%20several%20limitations%2C%20including%20inferior%0Avisual%20quality%2C%20a%20lack%20of%20aesthetic%20appeal%2C%20and%20inefficient%20inference%2C%20without%0Aa%20comprehensive%20solution%20in%20sight.%20To%20address%20these%20challenges%2C%20we%20present%0AUniFL%2C%20a%20unified%20framework%20that%20leverages%20feedback%20learning%20to%20enhance%0Adiffusion%20models%20comprehensively.%20UniFL%20stands%20out%20as%20a%20universal%2C%20effective%2C%0Aand%20generalizable%20solution%20applicable%20to%20various%20diffusion%20models%2C%20such%20as%0ASD1.5%20and%20SDXL.%20Notably%2C%20UniFL%20incorporates%20three%20key%20components%3A%20perceptual%0Afeedback%20learning%2C%20which%20enhances%20visual%20quality%3B%20decoupled%20feedback%20learning%2C%0Awhich%20improves%20aesthetic%20appeal%3B%20and%20adversarial%20feedback%20learning%2C%20which%0Aoptimizes%20inference%20speed.%20In-depth%20experiments%20and%20extensive%20user%20studies%0Avalidate%20the%20superior%20performance%20of%20our%20proposed%20method%20in%20enhancing%20both%20the%0Aquality%20of%20generated%20models%20and%20their%20acceleration.%20For%20instance%2C%20UniFL%0Asurpasses%20ImageReward%20by%2017%25%20user%20preference%20in%20terms%20of%20generation%20quality%20and%0Aoutperforms%20LCM%20and%20SDXL%20Turbo%20by%2057%25%20and%2020%25%20in%204-step%20inference.%20Moreover%2C%20we%0Ahave%20verified%20the%20efficacy%20of%20our%20approach%20in%20downstream%20tasks%2C%20including%20Lora%2C%0AControlNet%2C%20and%20AnimateDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05595v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniFL%3A%20Improve%20Stable%20Diffusion%20via%20Unified%20Feedback%20Learning&entry.906535625=Jiacheng%20Zhang%20and%20Jie%20Wu%20and%20Yuxi%20Ren%20and%20Xin%20Xia%20and%20Huafeng%20Kuang%20and%20Pan%20Xie%20and%20Jiashi%20Li%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Min%20Zheng%20and%20Lean%20Fu%20and%20Guanbin%20Li&entry.1292438233=%20%20Diffusion%20models%20have%20revolutionized%20the%20field%20of%20image%20generation%2C%20leading%0Ato%20the%20proliferation%20of%20high-quality%20models%20and%20diverse%20downstream%0Aapplications.%20However%2C%20despite%20these%20significant%20advancements%2C%20the%20current%0Acompetitive%20solutions%20still%20suffer%20from%20several%20limitations%2C%20including%20inferior%0Avisual%20quality%2C%20a%20lack%20of%20aesthetic%20appeal%2C%20and%20inefficient%20inference%2C%20without%0Aa%20comprehensive%20solution%20in%20sight.%20To%20address%20these%20challenges%2C%20we%20present%0AUniFL%2C%20a%20unified%20framework%20that%20leverages%20feedback%20learning%20to%20enhance%0Adiffusion%20models%20comprehensively.%20UniFL%20stands%20out%20as%20a%20universal%2C%20effective%2C%0Aand%20generalizable%20solution%20applicable%20to%20various%20diffusion%20models%2C%20such%20as%0ASD1.5%20and%20SDXL.%20Notably%2C%20UniFL%20incorporates%20three%20key%20components%3A%20perceptual%0Afeedback%20learning%2C%20which%20enhances%20visual%20quality%3B%20decoupled%20feedback%20learning%2C%0Awhich%20improves%20aesthetic%20appeal%3B%20and%20adversarial%20feedback%20learning%2C%20which%0Aoptimizes%20inference%20speed.%20In-depth%20experiments%20and%20extensive%20user%20studies%0Avalidate%20the%20superior%20performance%20of%20our%20proposed%20method%20in%20enhancing%20both%20the%0Aquality%20of%20generated%20models%20and%20their%20acceleration.%20For%20instance%2C%20UniFL%0Asurpasses%20ImageReward%20by%2017%25%20user%20preference%20in%20terms%20of%20generation%20quality%20and%0Aoutperforms%20LCM%20and%20SDXL%20Turbo%20by%2057%25%20and%2020%25%20in%204-step%20inference.%20Moreover%2C%20we%0Ahave%20verified%20the%20efficacy%20of%20our%20approach%20in%20downstream%20tasks%2C%20including%20Lora%2C%0AControlNet%2C%20and%20AnimateDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05595v1&entry.124074799=Read"},
{"title": "SepVAE: a contrastive VAE to separate pathological patterns from healthy\n  ones", "author": "Robin Louiset and Edouard Duchesnay and Antoine Grigis and Benoit Dufumier and Pietro Gori", "abstract": "  Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders\n(VAEs) that aims at separating the common factors of variation between a\nbackground dataset (BG) (i.e., healthy subjects) and a target dataset (TG)\n(i.e., patients) from the ones that only exist in the target dataset. To do so,\nthese methods separate the latent space into a set of salient features (i.e.,\nproper to the target dataset) and a set of common features (i.e., exist in both\ndatasets). Currently, all models fail to prevent the sharing of information\nbetween latent spaces effectively and to capture all salient factors of\nvariation. To this end, we introduce two crucial regularization losses: a\ndisentangling term between common and salient representations and a\nclassification term between background and target samples in the salient space.\nWe show a better performance than previous CA-VAEs methods on three medical\napplications and a natural images dataset (CelebA). Code and datasets are\navailable on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.\n", "link": "http://arxiv.org/abs/2307.06206v2", "date": "2024-04-08", "relevancy": 1.9004, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4855}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4626}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SepVAE%3A%20a%20contrastive%20VAE%20to%20separate%20pathological%20patterns%20from%20healthy%0A%20%20ones&body=Title%3A%20SepVAE%3A%20a%20contrastive%20VAE%20to%20separate%20pathological%20patterns%20from%20healthy%0A%20%20ones%0AAuthor%3A%20Robin%20Louiset%20and%20Edouard%20Duchesnay%20and%20Antoine%20Grigis%20and%20Benoit%20Dufumier%20and%20Pietro%20Gori%0AAbstract%3A%20%20%20Contrastive%20Analysis%20VAE%20%28CA-VAEs%29%20is%20a%20family%20of%20Variational%20auto-encoders%0A%28VAEs%29%20that%20aims%20at%20separating%20the%20common%20factors%20of%20variation%20between%20a%0Abackground%20dataset%20%28BG%29%20%28i.e.%2C%20healthy%20subjects%29%20and%20a%20target%20dataset%20%28TG%29%0A%28i.e.%2C%20patients%29%20from%20the%20ones%20that%20only%20exist%20in%20the%20target%20dataset.%20To%20do%20so%2C%0Athese%20methods%20separate%20the%20latent%20space%20into%20a%20set%20of%20salient%20features%20%28i.e.%2C%0Aproper%20to%20the%20target%20dataset%29%20and%20a%20set%20of%20common%20features%20%28i.e.%2C%20exist%20in%20both%0Adatasets%29.%20Currently%2C%20all%20models%20fail%20to%20prevent%20the%20sharing%20of%20information%0Abetween%20latent%20spaces%20effectively%20and%20to%20capture%20all%20salient%20factors%20of%0Avariation.%20To%20this%20end%2C%20we%20introduce%20two%20crucial%20regularization%20losses%3A%20a%0Adisentangling%20term%20between%20common%20and%20salient%20representations%20and%20a%0Aclassification%20term%20between%20background%20and%20target%20samples%20in%20the%20salient%20space.%0AWe%20show%20a%20better%20performance%20than%20previous%20CA-VAEs%20methods%20on%20three%20medical%0Aapplications%20and%20a%20natural%20images%20dataset%20%28CelebA%29.%20Code%20and%20datasets%20are%0Aavailable%20on%20GitHub%20https%3A//github.com/neurospin-projects/2023_rlouiset_sepvae.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.06206v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SepVAE%3A%20a%20contrastive%20VAE%20to%20separate%20pathological%20patterns%20from%20healthy%0A%20%20ones&entry.906535625=Robin%20Louiset%20and%20Edouard%20Duchesnay%20and%20Antoine%20Grigis%20and%20Benoit%20Dufumier%20and%20Pietro%20Gori&entry.1292438233=%20%20Contrastive%20Analysis%20VAE%20%28CA-VAEs%29%20is%20a%20family%20of%20Variational%20auto-encoders%0A%28VAEs%29%20that%20aims%20at%20separating%20the%20common%20factors%20of%20variation%20between%20a%0Abackground%20dataset%20%28BG%29%20%28i.e.%2C%20healthy%20subjects%29%20and%20a%20target%20dataset%20%28TG%29%0A%28i.e.%2C%20patients%29%20from%20the%20ones%20that%20only%20exist%20in%20the%20target%20dataset.%20To%20do%20so%2C%0Athese%20methods%20separate%20the%20latent%20space%20into%20a%20set%20of%20salient%20features%20%28i.e.%2C%0Aproper%20to%20the%20target%20dataset%29%20and%20a%20set%20of%20common%20features%20%28i.e.%2C%20exist%20in%20both%0Adatasets%29.%20Currently%2C%20all%20models%20fail%20to%20prevent%20the%20sharing%20of%20information%0Abetween%20latent%20spaces%20effectively%20and%20to%20capture%20all%20salient%20factors%20of%0Avariation.%20To%20this%20end%2C%20we%20introduce%20two%20crucial%20regularization%20losses%3A%20a%0Adisentangling%20term%20between%20common%20and%20salient%20representations%20and%20a%0Aclassification%20term%20between%20background%20and%20target%20samples%20in%20the%20salient%20space.%0AWe%20show%20a%20better%20performance%20than%20previous%20CA-VAEs%20methods%20on%20three%20medical%0Aapplications%20and%20a%20natural%20images%20dataset%20%28CelebA%29.%20Code%20and%20datasets%20are%0Aavailable%20on%20GitHub%20https%3A//github.com/neurospin-projects/2023_rlouiset_sepvae.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.06206v2&entry.124074799=Read"},
{"title": "Relation Extraction Using Large Language Models: A Case Study on\n  Acupuncture Point Locations", "author": "Yiming Li and Xueqing Peng and Jianfu Li and Xu Zuo and Suyuan Peng and Donghong Pei and Cui Tao and Hua Xu and Na Hong", "abstract": "  In acupuncture therapy, the accurate location of acupoints is essential for\nits effectiveness. The advanced language understanding capabilities of large\nlanguage models (LLMs) like Generative Pre-trained Transformers (GPT) present a\nsignificant opportunity for extracting relations related to acupoint locations\nfrom textual knowledge sources. This study aims to compare the performance of\nGPT with traditional deep learning models (Long Short-Term Memory (LSTM) and\nBidirectional Encoder Representations from Transformers for Biomedical Text\nMining (BioBERT)) in extracting acupoint-related location relations and assess\nthe impact of pretraining and fine-tuning on GPT's performance. We utilized the\nWorld Health Organization Standard Acupuncture Point Locations in the Western\nPacific Region (WHO Standard) as our corpus, which consists of descriptions of\n361 acupoints. Five types of relations ('direction_of,' 'distance_of,'\n'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints\nwere annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5,\nand fine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics\nincluded micro-average exact match precision, recall, and F1 scores. Our\nresults demonstrate that fine-tuned GPT-3.5 consistently outperformed other\nmodels in F1 scores across all relation types. Overall, it achieved the highest\nmicro-average F1 score of 0.92. This study underscores the effectiveness of\nLLMs like GPT in extracting relations related to acupoint locations, with\nimplications for accurately modeling acupuncture knowledge and promoting\nstandard implementation in acupuncture training and practice. The findings also\ncontribute to advancing informatics applications in traditional and\ncomplementary medicine, showcasing the potential of LLMs in natural language\nprocessing.\n", "link": "http://arxiv.org/abs/2404.05415v1", "date": "2024-04-08", "relevancy": 1.8742, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4659}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Relation%20Extraction%20Using%20Large%20Language%20Models%3A%20A%20Case%20Study%20on%0A%20%20Acupuncture%20Point%20Locations&body=Title%3A%20Relation%20Extraction%20Using%20Large%20Language%20Models%3A%20A%20Case%20Study%20on%0A%20%20Acupuncture%20Point%20Locations%0AAuthor%3A%20Yiming%20Li%20and%20Xueqing%20Peng%20and%20Jianfu%20Li%20and%20Xu%20Zuo%20and%20Suyuan%20Peng%20and%20Donghong%20Pei%20and%20Cui%20Tao%20and%20Hua%20Xu%20and%20Na%20Hong%0AAbstract%3A%20%20%20In%20acupuncture%20therapy%2C%20the%20accurate%20location%20of%20acupoints%20is%20essential%20for%0Aits%20effectiveness.%20The%20advanced%20language%20understanding%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%20like%20Generative%20Pre-trained%20Transformers%20%28GPT%29%20present%20a%0Asignificant%20opportunity%20for%20extracting%20relations%20related%20to%20acupoint%20locations%0Afrom%20textual%20knowledge%20sources.%20This%20study%20aims%20to%20compare%20the%20performance%20of%0AGPT%20with%20traditional%20deep%20learning%20models%20%28Long%20Short-Term%20Memory%20%28LSTM%29%20and%0ABidirectional%20Encoder%20Representations%20from%20Transformers%20for%20Biomedical%20Text%0AMining%20%28BioBERT%29%29%20in%20extracting%20acupoint-related%20location%20relations%20and%20assess%0Athe%20impact%20of%20pretraining%20and%20fine-tuning%20on%20GPT%27s%20performance.%20We%20utilized%20the%0AWorld%20Health%20Organization%20Standard%20Acupuncture%20Point%20Locations%20in%20the%20Western%0APacific%20Region%20%28WHO%20Standard%29%20as%20our%20corpus%2C%20which%20consists%20of%20descriptions%20of%0A361%20acupoints.%20Five%20types%20of%20relations%20%28%27direction_of%2C%27%20%27distance_of%2C%27%0A%27part_of%2C%27%20%27near_acupoint%2C%27%20and%20%27located_near%27%29%20%28n%3D%203%2C174%29%20between%20acupoints%0Awere%20annotated.%20Five%20models%20were%20compared%3A%20BioBERT%2C%20LSTM%2C%20pre-trained%20GPT-3.5%2C%0Aand%20fine-tuned%20GPT-3.5%2C%20as%20well%20as%20pre-trained%20GPT-4.%20Performance%20metrics%0Aincluded%20micro-average%20exact%20match%20precision%2C%20recall%2C%20and%20F1%20scores.%20Our%0Aresults%20demonstrate%20that%20fine-tuned%20GPT-3.5%20consistently%20outperformed%20other%0Amodels%20in%20F1%20scores%20across%20all%20relation%20types.%20Overall%2C%20it%20achieved%20the%20highest%0Amicro-average%20F1%20score%20of%200.92.%20This%20study%20underscores%20the%20effectiveness%20of%0ALLMs%20like%20GPT%20in%20extracting%20relations%20related%20to%20acupoint%20locations%2C%20with%0Aimplications%20for%20accurately%20modeling%20acupuncture%20knowledge%20and%20promoting%0Astandard%20implementation%20in%20acupuncture%20training%20and%20practice.%20The%20findings%20also%0Acontribute%20to%20advancing%20informatics%20applications%20in%20traditional%20and%0Acomplementary%20medicine%2C%20showcasing%20the%20potential%20of%20LLMs%20in%20natural%20language%0Aprocessing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05415v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relation%20Extraction%20Using%20Large%20Language%20Models%3A%20A%20Case%20Study%20on%0A%20%20Acupuncture%20Point%20Locations&entry.906535625=Yiming%20Li%20and%20Xueqing%20Peng%20and%20Jianfu%20Li%20and%20Xu%20Zuo%20and%20Suyuan%20Peng%20and%20Donghong%20Pei%20and%20Cui%20Tao%20and%20Hua%20Xu%20and%20Na%20Hong&entry.1292438233=%20%20In%20acupuncture%20therapy%2C%20the%20accurate%20location%20of%20acupoints%20is%20essential%20for%0Aits%20effectiveness.%20The%20advanced%20language%20understanding%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%20like%20Generative%20Pre-trained%20Transformers%20%28GPT%29%20present%20a%0Asignificant%20opportunity%20for%20extracting%20relations%20related%20to%20acupoint%20locations%0Afrom%20textual%20knowledge%20sources.%20This%20study%20aims%20to%20compare%20the%20performance%20of%0AGPT%20with%20traditional%20deep%20learning%20models%20%28Long%20Short-Term%20Memory%20%28LSTM%29%20and%0ABidirectional%20Encoder%20Representations%20from%20Transformers%20for%20Biomedical%20Text%0AMining%20%28BioBERT%29%29%20in%20extracting%20acupoint-related%20location%20relations%20and%20assess%0Athe%20impact%20of%20pretraining%20and%20fine-tuning%20on%20GPT%27s%20performance.%20We%20utilized%20the%0AWorld%20Health%20Organization%20Standard%20Acupuncture%20Point%20Locations%20in%20the%20Western%0APacific%20Region%20%28WHO%20Standard%29%20as%20our%20corpus%2C%20which%20consists%20of%20descriptions%20of%0A361%20acupoints.%20Five%20types%20of%20relations%20%28%27direction_of%2C%27%20%27distance_of%2C%27%0A%27part_of%2C%27%20%27near_acupoint%2C%27%20and%20%27located_near%27%29%20%28n%3D%203%2C174%29%20between%20acupoints%0Awere%20annotated.%20Five%20models%20were%20compared%3A%20BioBERT%2C%20LSTM%2C%20pre-trained%20GPT-3.5%2C%0Aand%20fine-tuned%20GPT-3.5%2C%20as%20well%20as%20pre-trained%20GPT-4.%20Performance%20metrics%0Aincluded%20micro-average%20exact%20match%20precision%2C%20recall%2C%20and%20F1%20scores.%20Our%0Aresults%20demonstrate%20that%20fine-tuned%20GPT-3.5%20consistently%20outperformed%20other%0Amodels%20in%20F1%20scores%20across%20all%20relation%20types.%20Overall%2C%20it%20achieved%20the%20highest%0Amicro-average%20F1%20score%20of%200.92.%20This%20study%20underscores%20the%20effectiveness%20of%0ALLMs%20like%20GPT%20in%20extracting%20relations%20related%20to%20acupoint%20locations%2C%20with%0Aimplications%20for%20accurately%20modeling%20acupuncture%20knowledge%20and%20promoting%0Astandard%20implementation%20in%20acupuncture%20training%20and%20practice.%20The%20findings%20also%0Acontribute%20to%20advancing%20informatics%20applications%20in%20traditional%20and%0Acomplementary%20medicine%2C%20showcasing%20the%20potential%20of%20LLMs%20in%20natural%20language%0Aprocessing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05415v1&entry.124074799=Read"},
{"title": "Investigating the Impact of Quantization on Adversarial Robustness", "author": "Qun Li and Yuan Meng and Chen Tang and Jiacheng Jiang and Zhi Wang", "abstract": "  Quantization is a promising technique for reducing the bit-width of deep\nmodels to improve their runtime performance and storage efficiency, and thus\nbecomes a fundamental step for deployment. In real-world scenarios, quantized\nmodels are often faced with adversarial attacks which cause the model to make\nincorrect inferences by introducing slight perturbations. However, recent\nstudies have paid less attention to the impact of quantization on the model\nrobustness. More surprisingly, existing studies on this topic even present\ninconsistent conclusions, which prompted our in-depth investigation. In this\npaper, we conduct a first-time analysis of the impact of the quantization\npipeline components that can incorporate robust optimization under the settings\nof Post-Training Quantization and Quantization-Aware Training. Through our\ndetailed analysis, we discovered that this inconsistency arises from the use of\ndifferent pipelines in different studies, specifically regarding whether robust\noptimization is performed and at which quantization stage it occurs. Our\nresearch findings contribute insights into deploying more secure and robust\nquantized networks, assisting practitioners in reference for scenarios with\nhigh-security requirements and limited resources.\n", "link": "http://arxiv.org/abs/2404.05639v1", "date": "2024-04-08", "relevancy": 1.8728, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.473}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4708}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4623}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Impact%20of%20Quantization%20on%20Adversarial%20Robustness&body=Title%3A%20Investigating%20the%20Impact%20of%20Quantization%20on%20Adversarial%20Robustness%0AAuthor%3A%20Qun%20Li%20and%20Yuan%20Meng%20and%20Chen%20Tang%20and%20Jiacheng%20Jiang%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Quantization%20is%20a%20promising%20technique%20for%20reducing%20the%20bit-width%20of%20deep%0Amodels%20to%20improve%20their%20runtime%20performance%20and%20storage%20efficiency%2C%20and%20thus%0Abecomes%20a%20fundamental%20step%20for%20deployment.%20In%20real-world%20scenarios%2C%20quantized%0Amodels%20are%20often%20faced%20with%20adversarial%20attacks%20which%20cause%20the%20model%20to%20make%0Aincorrect%20inferences%20by%20introducing%20slight%20perturbations.%20However%2C%20recent%0Astudies%20have%20paid%20less%20attention%20to%20the%20impact%20of%20quantization%20on%20the%20model%0Arobustness.%20More%20surprisingly%2C%20existing%20studies%20on%20this%20topic%20even%20present%0Ainconsistent%20conclusions%2C%20which%20prompted%20our%20in-depth%20investigation.%20In%20this%0Apaper%2C%20we%20conduct%20a%20first-time%20analysis%20of%20the%20impact%20of%20the%20quantization%0Apipeline%20components%20that%20can%20incorporate%20robust%20optimization%20under%20the%20settings%0Aof%20Post-Training%20Quantization%20and%20Quantization-Aware%20Training.%20Through%20our%0Adetailed%20analysis%2C%20we%20discovered%20that%20this%20inconsistency%20arises%20from%20the%20use%20of%0Adifferent%20pipelines%20in%20different%20studies%2C%20specifically%20regarding%20whether%20robust%0Aoptimization%20is%20performed%20and%20at%20which%20quantization%20stage%20it%20occurs.%20Our%0Aresearch%20findings%20contribute%20insights%20into%20deploying%20more%20secure%20and%20robust%0Aquantized%20networks%2C%20assisting%20practitioners%20in%20reference%20for%20scenarios%20with%0Ahigh-security%20requirements%20and%20limited%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Impact%20of%20Quantization%20on%20Adversarial%20Robustness&entry.906535625=Qun%20Li%20and%20Yuan%20Meng%20and%20Chen%20Tang%20and%20Jiacheng%20Jiang%20and%20Zhi%20Wang&entry.1292438233=%20%20Quantization%20is%20a%20promising%20technique%20for%20reducing%20the%20bit-width%20of%20deep%0Amodels%20to%20improve%20their%20runtime%20performance%20and%20storage%20efficiency%2C%20and%20thus%0Abecomes%20a%20fundamental%20step%20for%20deployment.%20In%20real-world%20scenarios%2C%20quantized%0Amodels%20are%20often%20faced%20with%20adversarial%20attacks%20which%20cause%20the%20model%20to%20make%0Aincorrect%20inferences%20by%20introducing%20slight%20perturbations.%20However%2C%20recent%0Astudies%20have%20paid%20less%20attention%20to%20the%20impact%20of%20quantization%20on%20the%20model%0Arobustness.%20More%20surprisingly%2C%20existing%20studies%20on%20this%20topic%20even%20present%0Ainconsistent%20conclusions%2C%20which%20prompted%20our%20in-depth%20investigation.%20In%20this%0Apaper%2C%20we%20conduct%20a%20first-time%20analysis%20of%20the%20impact%20of%20the%20quantization%0Apipeline%20components%20that%20can%20incorporate%20robust%20optimization%20under%20the%20settings%0Aof%20Post-Training%20Quantization%20and%20Quantization-Aware%20Training.%20Through%20our%0Adetailed%20analysis%2C%20we%20discovered%20that%20this%20inconsistency%20arises%20from%20the%20use%20of%0Adifferent%20pipelines%20in%20different%20studies%2C%20specifically%20regarding%20whether%20robust%0Aoptimization%20is%20performed%20and%20at%20which%20quantization%20stage%20it%20occurs.%20Our%0Aresearch%20findings%20contribute%20insights%20into%20deploying%20more%20secure%20and%20robust%0Aquantized%20networks%2C%20assisting%20practitioners%20in%20reference%20for%20scenarios%20with%0Ahigh-security%20requirements%20and%20limited%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05639v1&entry.124074799=Read"},
{"title": "WaveCatBoost for Probabilistic Forecasting of Regional Air Quality Data", "author": "Jintu Borah and Tanujit Chakraborty and Md. Shahrul Md. Nadzir and Mylene G. Cayetano and Shubhankar Majumdar", "abstract": "  Accurate and reliable air quality forecasting is essential for protecting\npublic health, sustainable development, pollution control, and enhanced urban\nplanning. This letter presents a novel WaveCatBoost architecture designed to\nforecast the real-time concentrations of air pollutants by combining the\nmaximal overlapping discrete wavelet transform (MODWT) with the CatBoost model.\nThis hybrid approach efficiently transforms time series into high-frequency and\nlow-frequency components, thereby extracting signal from noise and improving\nprediction accuracy and robustness. Evaluation of two distinct regional\ndatasets, from the Central Air Pollution Control Board (CPCB) sensor network\nand a low-cost air quality sensor system (LAQS), underscores the superior\nperformance of our proposed methodology in real-time forecasting compared to\nthe state-of-the-art statistical and deep learning architectures. Moreover, we\nemploy a conformal prediction strategy to provide probabilistic bands with our\nforecasts.\n", "link": "http://arxiv.org/abs/2404.05482v1", "date": "2024-04-08", "relevancy": 1.8713, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4812}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WaveCatBoost%20for%20Probabilistic%20Forecasting%20of%20Regional%20Air%20Quality%20Data&body=Title%3A%20WaveCatBoost%20for%20Probabilistic%20Forecasting%20of%20Regional%20Air%20Quality%20Data%0AAuthor%3A%20Jintu%20Borah%20and%20Tanujit%20Chakraborty%20and%20Md.%20Shahrul%20Md.%20Nadzir%20and%20Mylene%20G.%20Cayetano%20and%20Shubhankar%20Majumdar%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20air%20quality%20forecasting%20is%20essential%20for%20protecting%0Apublic%20health%2C%20sustainable%20development%2C%20pollution%20control%2C%20and%20enhanced%20urban%0Aplanning.%20This%20letter%20presents%20a%20novel%20WaveCatBoost%20architecture%20designed%20to%0Aforecast%20the%20real-time%20concentrations%20of%20air%20pollutants%20by%20combining%20the%0Amaximal%20overlapping%20discrete%20wavelet%20transform%20%28MODWT%29%20with%20the%20CatBoost%20model.%0AThis%20hybrid%20approach%20efficiently%20transforms%20time%20series%20into%20high-frequency%20and%0Alow-frequency%20components%2C%20thereby%20extracting%20signal%20from%20noise%20and%20improving%0Aprediction%20accuracy%20and%20robustness.%20Evaluation%20of%20two%20distinct%20regional%0Adatasets%2C%20from%20the%20Central%20Air%20Pollution%20Control%20Board%20%28CPCB%29%20sensor%20network%0Aand%20a%20low-cost%20air%20quality%20sensor%20system%20%28LAQS%29%2C%20underscores%20the%20superior%0Aperformance%20of%20our%20proposed%20methodology%20in%20real-time%20forecasting%20compared%20to%0Athe%20state-of-the-art%20statistical%20and%20deep%20learning%20architectures.%20Moreover%2C%20we%0Aemploy%20a%20conformal%20prediction%20strategy%20to%20provide%20probabilistic%20bands%20with%20our%0Aforecasts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05482v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveCatBoost%20for%20Probabilistic%20Forecasting%20of%20Regional%20Air%20Quality%20Data&entry.906535625=Jintu%20Borah%20and%20Tanujit%20Chakraborty%20and%20Md.%20Shahrul%20Md.%20Nadzir%20and%20Mylene%20G.%20Cayetano%20and%20Shubhankar%20Majumdar&entry.1292438233=%20%20Accurate%20and%20reliable%20air%20quality%20forecasting%20is%20essential%20for%20protecting%0Apublic%20health%2C%20sustainable%20development%2C%20pollution%20control%2C%20and%20enhanced%20urban%0Aplanning.%20This%20letter%20presents%20a%20novel%20WaveCatBoost%20architecture%20designed%20to%0Aforecast%20the%20real-time%20concentrations%20of%20air%20pollutants%20by%20combining%20the%0Amaximal%20overlapping%20discrete%20wavelet%20transform%20%28MODWT%29%20with%20the%20CatBoost%20model.%0AThis%20hybrid%20approach%20efficiently%20transforms%20time%20series%20into%20high-frequency%20and%0Alow-frequency%20components%2C%20thereby%20extracting%20signal%20from%20noise%20and%20improving%0Aprediction%20accuracy%20and%20robustness.%20Evaluation%20of%20two%20distinct%20regional%0Adatasets%2C%20from%20the%20Central%20Air%20Pollution%20Control%20Board%20%28CPCB%29%20sensor%20network%0Aand%20a%20low-cost%20air%20quality%20sensor%20system%20%28LAQS%29%2C%20underscores%20the%20superior%0Aperformance%20of%20our%20proposed%20methodology%20in%20real-time%20forecasting%20compared%20to%0Athe%20state-of-the-art%20statistical%20and%20deep%20learning%20architectures.%20Moreover%2C%20we%0Aemploy%20a%20conformal%20prediction%20strategy%20to%20provide%20probabilistic%20bands%20with%20our%0Aforecasts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05482v1&entry.124074799=Read"},
{"title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation", "author": "Kunpeng Song and Yizhe Zhu and Bingchen Liu and Qing Yan and Ahmed Elgammal and Xiao Yang", "abstract": "  In this paper, we present MoMA: an open-vocabulary, training-free\npersonalized image model that boasts flexible zero-shot capabilities. As\nfoundational text-to-image models rapidly evolve, the demand for robust\nimage-to-image translation grows. Addressing this need, MoMA specializes in\nsubject-driven personalized image generation. Utilizing an open-source,\nMultimodal Large Language Model (MLLM), we train MoMA to serve a dual role as\nboth a feature extractor and a generator. This approach effectively synergizes\nreference image and text prompt information to produce valuable image features,\nfacilitating an image diffusion model. To better leverage the generated\nfeatures, we further introduce a novel self-attention shortcut method that\nefficiently transfers image features to an image diffusion model, improving the\nresemblance of the target object in generated images. Remarkably, as a\ntuning-free plug-and-play module, our model requires only a single reference\nimage and outperforms existing methods in generating images with high detail\nfidelity, enhanced identity-preservation and prompt faithfulness. Our work is\nopen-source, thereby providing universal access to these advancements.\n", "link": "http://arxiv.org/abs/2404.05674v1", "date": "2024-04-08", "relevancy": 1.8693, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6274}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6255}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.61}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoMA%3A%20Multimodal%20LLM%20Adapter%20for%20Fast%20Personalized%20Image%20Generation&body=Title%3A%20MoMA%3A%20Multimodal%20LLM%20Adapter%20for%20Fast%20Personalized%20Image%20Generation%0AAuthor%3A%20Kunpeng%20Song%20and%20Yizhe%20Zhu%20and%20Bingchen%20Liu%20and%20Qing%20Yan%20and%20Ahmed%20Elgammal%20and%20Xiao%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20MoMA%3A%20an%20open-vocabulary%2C%20training-free%0Apersonalized%20image%20model%20that%20boasts%20flexible%20zero-shot%20capabilities.%20As%0Afoundational%20text-to-image%20models%20rapidly%20evolve%2C%20the%20demand%20for%20robust%0Aimage-to-image%20translation%20grows.%20Addressing%20this%20need%2C%20MoMA%20specializes%20in%0Asubject-driven%20personalized%20image%20generation.%20Utilizing%20an%20open-source%2C%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%2C%20we%20train%20MoMA%20to%20serve%20a%20dual%20role%20as%0Aboth%20a%20feature%20extractor%20and%20a%20generator.%20This%20approach%20effectively%20synergizes%0Areference%20image%20and%20text%20prompt%20information%20to%20produce%20valuable%20image%20features%2C%0Afacilitating%20an%20image%20diffusion%20model.%20To%20better%20leverage%20the%20generated%0Afeatures%2C%20we%20further%20introduce%20a%20novel%20self-attention%20shortcut%20method%20that%0Aefficiently%20transfers%20image%20features%20to%20an%20image%20diffusion%20model%2C%20improving%20the%0Aresemblance%20of%20the%20target%20object%20in%20generated%20images.%20Remarkably%2C%20as%20a%0Atuning-free%20plug-and-play%20module%2C%20our%20model%20requires%20only%20a%20single%20reference%0Aimage%20and%20outperforms%20existing%20methods%20in%20generating%20images%20with%20high%20detail%0Afidelity%2C%20enhanced%20identity-preservation%20and%20prompt%20faithfulness.%20Our%20work%20is%0Aopen-source%2C%20thereby%20providing%20universal%20access%20to%20these%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoMA%3A%20Multimodal%20LLM%20Adapter%20for%20Fast%20Personalized%20Image%20Generation&entry.906535625=Kunpeng%20Song%20and%20Yizhe%20Zhu%20and%20Bingchen%20Liu%20and%20Qing%20Yan%20and%20Ahmed%20Elgammal%20and%20Xiao%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20MoMA%3A%20an%20open-vocabulary%2C%20training-free%0Apersonalized%20image%20model%20that%20boasts%20flexible%20zero-shot%20capabilities.%20As%0Afoundational%20text-to-image%20models%20rapidly%20evolve%2C%20the%20demand%20for%20robust%0Aimage-to-image%20translation%20grows.%20Addressing%20this%20need%2C%20MoMA%20specializes%20in%0Asubject-driven%20personalized%20image%20generation.%20Utilizing%20an%20open-source%2C%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%2C%20we%20train%20MoMA%20to%20serve%20a%20dual%20role%20as%0Aboth%20a%20feature%20extractor%20and%20a%20generator.%20This%20approach%20effectively%20synergizes%0Areference%20image%20and%20text%20prompt%20information%20to%20produce%20valuable%20image%20features%2C%0Afacilitating%20an%20image%20diffusion%20model.%20To%20better%20leverage%20the%20generated%0Afeatures%2C%20we%20further%20introduce%20a%20novel%20self-attention%20shortcut%20method%20that%0Aefficiently%20transfers%20image%20features%20to%20an%20image%20diffusion%20model%2C%20improving%20the%0Aresemblance%20of%20the%20target%20object%20in%20generated%20images.%20Remarkably%2C%20as%20a%0Atuning-free%20plug-and-play%20module%2C%20our%20model%20requires%20only%20a%20single%20reference%0Aimage%20and%20outperforms%20existing%20methods%20in%20generating%20images%20with%20high%20detail%0Afidelity%2C%20enhanced%20identity-preservation%20and%20prompt%20faithfulness.%20Our%20work%20is%0Aopen-source%2C%20thereby%20providing%20universal%20access%20to%20these%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05674v1&entry.124074799=Read"},
{"title": "Cell-Free Multi-User MIMO Equalization via In-Context Learning", "author": "Matteo Zecchin and Kai Zu and Osvaldo Simeone", "abstract": "  Large pre-trained sequence models, such as transformers, excel as few-shot\nlearners capable of in-context learning (ICL). In ICL, a model is trained to\nadapt its operation to a new task based on limited contextual information,\ntypically in the form of a few training examples for the given task. Previous\nwork has explored the use of ICL for channel equalization in single-user\nmulti-input and multiple-output (MIMO) systems. In this work, we demonstrate\nthat ICL can be also used to tackle the problem of multi-user equalization in\ncell-free MIMO systems with limited fronthaul capacity. In this scenario, a\ntask is defined by channel statistics, signal-to-noise ratio, and modulation\nschemes. The context encompasses the users' pilot sequences, the corresponding\nquantized received signals, and the current received data signal. Different\nprompt design strategies are proposed and evaluated that encompass also\nlarge-scale fading and modulation information. Experiments demonstrate that\nICL-based equalization provides estimates with lower mean squared error as\ncompared to the linear minimum mean squared error equalizer, especially in the\npresence of limited fronthaul capacity and pilot contamination.\n", "link": "http://arxiv.org/abs/2404.05538v1", "date": "2024-04-08", "relevancy": 1.8598, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4514}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.448}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cell-Free%20Multi-User%20MIMO%20Equalization%20via%20In-Context%20Learning&body=Title%3A%20Cell-Free%20Multi-User%20MIMO%20Equalization%20via%20In-Context%20Learning%0AAuthor%3A%20Matteo%20Zecchin%20and%20Kai%20Zu%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20Large%20pre-trained%20sequence%20models%2C%20such%20as%20transformers%2C%20excel%20as%20few-shot%0Alearners%20capable%20of%20in-context%20learning%20%28ICL%29.%20In%20ICL%2C%20a%20model%20is%20trained%20to%0Aadapt%20its%20operation%20to%20a%20new%20task%20based%20on%20limited%20contextual%20information%2C%0Atypically%20in%20the%20form%20of%20a%20few%20training%20examples%20for%20the%20given%20task.%20Previous%0Awork%20has%20explored%20the%20use%20of%20ICL%20for%20channel%20equalization%20in%20single-user%0Amulti-input%20and%20multiple-output%20%28MIMO%29%20systems.%20In%20this%20work%2C%20we%20demonstrate%0Athat%20ICL%20can%20be%20also%20used%20to%20tackle%20the%20problem%20of%20multi-user%20equalization%20in%0Acell-free%20MIMO%20systems%20with%20limited%20fronthaul%20capacity.%20In%20this%20scenario%2C%20a%0Atask%20is%20defined%20by%20channel%20statistics%2C%20signal-to-noise%20ratio%2C%20and%20modulation%0Aschemes.%20The%20context%20encompasses%20the%20users%27%20pilot%20sequences%2C%20the%20corresponding%0Aquantized%20received%20signals%2C%20and%20the%20current%20received%20data%20signal.%20Different%0Aprompt%20design%20strategies%20are%20proposed%20and%20evaluated%20that%20encompass%20also%0Alarge-scale%20fading%20and%20modulation%20information.%20Experiments%20demonstrate%20that%0AICL-based%20equalization%20provides%20estimates%20with%20lower%20mean%20squared%20error%20as%0Acompared%20to%20the%20linear%20minimum%20mean%20squared%20error%20equalizer%2C%20especially%20in%20the%0Apresence%20of%20limited%20fronthaul%20capacity%20and%20pilot%20contamination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05538v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell-Free%20Multi-User%20MIMO%20Equalization%20via%20In-Context%20Learning&entry.906535625=Matteo%20Zecchin%20and%20Kai%20Zu%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20Large%20pre-trained%20sequence%20models%2C%20such%20as%20transformers%2C%20excel%20as%20few-shot%0Alearners%20capable%20of%20in-context%20learning%20%28ICL%29.%20In%20ICL%2C%20a%20model%20is%20trained%20to%0Aadapt%20its%20operation%20to%20a%20new%20task%20based%20on%20limited%20contextual%20information%2C%0Atypically%20in%20the%20form%20of%20a%20few%20training%20examples%20for%20the%20given%20task.%20Previous%0Awork%20has%20explored%20the%20use%20of%20ICL%20for%20channel%20equalization%20in%20single-user%0Amulti-input%20and%20multiple-output%20%28MIMO%29%20systems.%20In%20this%20work%2C%20we%20demonstrate%0Athat%20ICL%20can%20be%20also%20used%20to%20tackle%20the%20problem%20of%20multi-user%20equalization%20in%0Acell-free%20MIMO%20systems%20with%20limited%20fronthaul%20capacity.%20In%20this%20scenario%2C%20a%0Atask%20is%20defined%20by%20channel%20statistics%2C%20signal-to-noise%20ratio%2C%20and%20modulation%0Aschemes.%20The%20context%20encompasses%20the%20users%27%20pilot%20sequences%2C%20the%20corresponding%0Aquantized%20received%20signals%2C%20and%20the%20current%20received%20data%20signal.%20Different%0Aprompt%20design%20strategies%20are%20proposed%20and%20evaluated%20that%20encompass%20also%0Alarge-scale%20fading%20and%20modulation%20information.%20Experiments%20demonstrate%20that%0AICL-based%20equalization%20provides%20estimates%20with%20lower%20mean%20squared%20error%20as%0Acompared%20to%20the%20linear%20minimum%20mean%20squared%20error%20equalizer%2C%20especially%20in%20the%0Apresence%20of%20limited%20fronthaul%20capacity%20and%20pilot%20contamination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05538v1&entry.124074799=Read"},
{"title": "Neural Cellular Automata for Lightweight, Robust and Explainable\n  Classification of White Blood Cell Images", "author": "Michael Deutges and Ario Sadafi and Nassir Navab and Carsten Marr", "abstract": "  Diagnosis of hematological malignancies depends on accurate identification of\nwhite blood cells in peripheral blood smears. Deep learning techniques are\nemerging as a viable solution to scale and optimize this process by automatic\nidentification of cells in laboratories. However, these techniques face several\nchallenges such as limited generalizability, sensitivity to domain shifts and\nlack of explainability. Here, we are introducing a novel approach based on\nneural cellular automata (NCA) for white blood cell classification. We test our\napproach on three datasets of white blood cell images and show that we achieve\ncompetitive performance compared to conventional methods. Our NCA-based method\nis significantly smaller in terms of parameters and exhibits robustness to\ndomain shifts. Furthermore, the architecture is inherently explainable,\nproviding insights into the decision process for each classification, helping\nexperts understand and validate model predictions. Results demonstrate that NCA\nnot only can be used for image classification, but also address key challenges\nof conventional methods, indicating a high potential for applicability in\nclinical practice.\n", "link": "http://arxiv.org/abs/2404.05584v1", "date": "2024-04-08", "relevancy": 1.8554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4695}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4594}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images&body=Title%3A%20Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images%0AAuthor%3A%20Michael%20Deutges%20and%20Ario%20Sadafi%20and%20Nassir%20Navab%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20Diagnosis%20of%20hematological%20malignancies%20depends%20on%20accurate%20identification%20of%0Awhite%20blood%20cells%20in%20peripheral%20blood%20smears.%20Deep%20learning%20techniques%20are%0Aemerging%20as%20a%20viable%20solution%20to%20scale%20and%20optimize%20this%20process%20by%20automatic%0Aidentification%20of%20cells%20in%20laboratories.%20However%2C%20these%20techniques%20face%20several%0Achallenges%20such%20as%20limited%20generalizability%2C%20sensitivity%20to%20domain%20shifts%20and%0Alack%20of%20explainability.%20Here%2C%20we%20are%20introducing%20a%20novel%20approach%20based%20on%0Aneural%20cellular%20automata%20%28NCA%29%20for%20white%20blood%20cell%20classification.%20We%20test%20our%0Aapproach%20on%20three%20datasets%20of%20white%20blood%20cell%20images%20and%20show%20that%20we%20achieve%0Acompetitive%20performance%20compared%20to%20conventional%20methods.%20Our%20NCA-based%20method%0Ais%20significantly%20smaller%20in%20terms%20of%20parameters%20and%20exhibits%20robustness%20to%0Adomain%20shifts.%20Furthermore%2C%20the%20architecture%20is%20inherently%20explainable%2C%0Aproviding%20insights%20into%20the%20decision%20process%20for%20each%20classification%2C%20helping%0Aexperts%20understand%20and%20validate%20model%20predictions.%20Results%20demonstrate%20that%20NCA%0Anot%20only%20can%20be%20used%20for%20image%20classification%2C%20but%20also%20address%20key%20challenges%0Aof%20conventional%20methods%2C%20indicating%20a%20high%20potential%20for%20applicability%20in%0Aclinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images&entry.906535625=Michael%20Deutges%20and%20Ario%20Sadafi%20and%20Nassir%20Navab%20and%20Carsten%20Marr&entry.1292438233=%20%20Diagnosis%20of%20hematological%20malignancies%20depends%20on%20accurate%20identification%20of%0Awhite%20blood%20cells%20in%20peripheral%20blood%20smears.%20Deep%20learning%20techniques%20are%0Aemerging%20as%20a%20viable%20solution%20to%20scale%20and%20optimize%20this%20process%20by%20automatic%0Aidentification%20of%20cells%20in%20laboratories.%20However%2C%20these%20techniques%20face%20several%0Achallenges%20such%20as%20limited%20generalizability%2C%20sensitivity%20to%20domain%20shifts%20and%0Alack%20of%20explainability.%20Here%2C%20we%20are%20introducing%20a%20novel%20approach%20based%20on%0Aneural%20cellular%20automata%20%28NCA%29%20for%20white%20blood%20cell%20classification.%20We%20test%20our%0Aapproach%20on%20three%20datasets%20of%20white%20blood%20cell%20images%20and%20show%20that%20we%20achieve%0Acompetitive%20performance%20compared%20to%20conventional%20methods.%20Our%20NCA-based%20method%0Ais%20significantly%20smaller%20in%20terms%20of%20parameters%20and%20exhibits%20robustness%20to%0Adomain%20shifts.%20Furthermore%2C%20the%20architecture%20is%20inherently%20explainable%2C%0Aproviding%20insights%20into%20the%20decision%20process%20for%20each%20classification%2C%20helping%0Aexperts%20understand%20and%20validate%20model%20predictions.%20Results%20demonstrate%20that%20NCA%0Anot%20only%20can%20be%20used%20for%20image%20classification%2C%20but%20also%20address%20key%20challenges%0Aof%20conventional%20methods%2C%20indicating%20a%20high%20potential%20for%20applicability%20in%0Aclinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05584v1&entry.124074799=Read"},
{"title": "Analysis of Off-Policy Multi-Step TD-Learning with Linear Function\n  Approximation", "author": "Donghwan Lee", "abstract": "  This paper analyzes multi-step TD-learning algorithms within the `deadly\ntriad' scenario, characterized by linear function approximation, off-policy\nlearning, and bootstrapping. In particular, we prove that n-step TD-learning\nalgorithms converge to a solution as the sampling horizon n increases\nsufficiently. The paper is divided into two parts. In the first part, we\ncomprehensively examine the fundamental properties of their model-based\ndeterministic counterparts, including projected value iteration, gradient\ndescent algorithms, and the control theoretic approach, which can be viewed as\nprototype deterministic algorithms whose analysis plays a pivotal role in\nunderstanding and developing their model-free reinforcement learning\ncounterparts. In particular, we prove that these algorithms converge to\nmeaningful solutions when n is sufficiently large. Based on these findings, two\nn-step TD-learning algorithms are proposed and analyzed, which can be seen as\nthe model-free reinforcement learning counterparts of the gradient and control\ntheoretic algorithms.\n", "link": "http://arxiv.org/abs/2402.15781v2", "date": "2024-04-08", "relevancy": 1.8447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4585}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Off-Policy%20Multi-Step%20TD-Learning%20with%20Linear%20Function%0A%20%20Approximation&body=Title%3A%20Analysis%20of%20Off-Policy%20Multi-Step%20TD-Learning%20with%20Linear%20Function%0A%20%20Approximation%0AAuthor%3A%20Donghwan%20Lee%0AAbstract%3A%20%20%20This%20paper%20analyzes%20multi-step%20TD-learning%20algorithms%20within%20the%20%60deadly%0Atriad%27%20scenario%2C%20characterized%20by%20linear%20function%20approximation%2C%20off-policy%0Alearning%2C%20and%20bootstrapping.%20In%20particular%2C%20we%20prove%20that%20n-step%20TD-learning%0Aalgorithms%20converge%20to%20a%20solution%20as%20the%20sampling%20horizon%20n%20increases%0Asufficiently.%20The%20paper%20is%20divided%20into%20two%20parts.%20In%20the%20first%20part%2C%20we%0Acomprehensively%20examine%20the%20fundamental%20properties%20of%20their%20model-based%0Adeterministic%20counterparts%2C%20including%20projected%20value%20iteration%2C%20gradient%0Adescent%20algorithms%2C%20and%20the%20control%20theoretic%20approach%2C%20which%20can%20be%20viewed%20as%0Aprototype%20deterministic%20algorithms%20whose%20analysis%20plays%20a%20pivotal%20role%20in%0Aunderstanding%20and%20developing%20their%20model-free%20reinforcement%20learning%0Acounterparts.%20In%20particular%2C%20we%20prove%20that%20these%20algorithms%20converge%20to%0Ameaningful%20solutions%20when%20n%20is%20sufficiently%20large.%20Based%20on%20these%20findings%2C%20two%0An-step%20TD-learning%20algorithms%20are%20proposed%20and%20analyzed%2C%20which%20can%20be%20seen%20as%0Athe%20model-free%20reinforcement%20learning%20counterparts%20of%20the%20gradient%20and%20control%0Atheoretic%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15781v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Off-Policy%20Multi-Step%20TD-Learning%20with%20Linear%20Function%0A%20%20Approximation&entry.906535625=Donghwan%20Lee&entry.1292438233=%20%20This%20paper%20analyzes%20multi-step%20TD-learning%20algorithms%20within%20the%20%60deadly%0Atriad%27%20scenario%2C%20characterized%20by%20linear%20function%20approximation%2C%20off-policy%0Alearning%2C%20and%20bootstrapping.%20In%20particular%2C%20we%20prove%20that%20n-step%20TD-learning%0Aalgorithms%20converge%20to%20a%20solution%20as%20the%20sampling%20horizon%20n%20increases%0Asufficiently.%20The%20paper%20is%20divided%20into%20two%20parts.%20In%20the%20first%20part%2C%20we%0Acomprehensively%20examine%20the%20fundamental%20properties%20of%20their%20model-based%0Adeterministic%20counterparts%2C%20including%20projected%20value%20iteration%2C%20gradient%0Adescent%20algorithms%2C%20and%20the%20control%20theoretic%20approach%2C%20which%20can%20be%20viewed%20as%0Aprototype%20deterministic%20algorithms%20whose%20analysis%20plays%20a%20pivotal%20role%20in%0Aunderstanding%20and%20developing%20their%20model-free%20reinforcement%20learning%0Acounterparts.%20In%20particular%2C%20we%20prove%20that%20these%20algorithms%20converge%20to%0Ameaningful%20solutions%20when%20n%20is%20sufficiently%20large.%20Based%20on%20these%20findings%2C%20two%0An-step%20TD-learning%20algorithms%20are%20proposed%20and%20analyzed%2C%20which%20can%20be%20seen%20as%0Athe%20model-free%20reinforcement%20learning%20counterparts%20of%20the%20gradient%20and%20control%0Atheoretic%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15781v2&entry.124074799=Read"},
{"title": "A Hessian for Gaussian Mixture Likelihoods in Nonlinear Least Squares", "author": "Vassili Korotkine and Mitchell Cohen and James Richard Forbes", "abstract": "  This paper proposes a novel Hessian approximation for Maximum a Posteriori\nestimation problems in robotics involving Gaussian mixture likelihoods. The\nproposed Hessian leads to better convergence properties. Previous approaches\nmanipulate the Gaussian mixture likelihood into a form that allows the problem\nto be represented as a nonlinear least squares (NLS) problem. However, they\nresult in an inaccurate Hessian approximation due to additional nonlinearities\nthat are not accounted for in NLS solvers. The proposed Hessian approximation\nis derived by setting the Hessians of the Gaussian mixture component errors to\nzero, which is the same starting point as for the Gauss-Newton Hessian\napproximation for NLS, and using the chain rule to account for additional\nnonlinearities. The proposed Hessian approximation is more accurate, resulting\nin improved convergence properties that are demonstrated on simulated and\nreal-world experiments. A method to maintain compatibility with existing\nsolvers, such as ceres, is also presented. Accompanying software and\nsupplementary material can be found at\nhttps://github.com/decargroup/hessian_sum_mixtures.\n", "link": "http://arxiv.org/abs/2404.05452v1", "date": "2024-04-08", "relevancy": 1.8442, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4614}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4247}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Hessian%20for%20Gaussian%20Mixture%20Likelihoods%20in%20Nonlinear%20Least%20Squares&body=Title%3A%20A%20Hessian%20for%20Gaussian%20Mixture%20Likelihoods%20in%20Nonlinear%20Least%20Squares%0AAuthor%3A%20Vassili%20Korotkine%20and%20Mitchell%20Cohen%20and%20James%20Richard%20Forbes%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20Hessian%20approximation%20for%20Maximum%20a%20Posteriori%0Aestimation%20problems%20in%20robotics%20involving%20Gaussian%20mixture%20likelihoods.%20The%0Aproposed%20Hessian%20leads%20to%20better%20convergence%20properties.%20Previous%20approaches%0Amanipulate%20the%20Gaussian%20mixture%20likelihood%20into%20a%20form%20that%20allows%20the%20problem%0Ato%20be%20represented%20as%20a%20nonlinear%20least%20squares%20%28NLS%29%20problem.%20However%2C%20they%0Aresult%20in%20an%20inaccurate%20Hessian%20approximation%20due%20to%20additional%20nonlinearities%0Athat%20are%20not%20accounted%20for%20in%20NLS%20solvers.%20The%20proposed%20Hessian%20approximation%0Ais%20derived%20by%20setting%20the%20Hessians%20of%20the%20Gaussian%20mixture%20component%20errors%20to%0Azero%2C%20which%20is%20the%20same%20starting%20point%20as%20for%20the%20Gauss-Newton%20Hessian%0Aapproximation%20for%20NLS%2C%20and%20using%20the%20chain%20rule%20to%20account%20for%20additional%0Anonlinearities.%20The%20proposed%20Hessian%20approximation%20is%20more%20accurate%2C%20resulting%0Ain%20improved%20convergence%20properties%20that%20are%20demonstrated%20on%20simulated%20and%0Areal-world%20experiments.%20A%20method%20to%20maintain%20compatibility%20with%20existing%0Asolvers%2C%20such%20as%20ceres%2C%20is%20also%20presented.%20Accompanying%20software%20and%0Asupplementary%20material%20can%20be%20found%20at%0Ahttps%3A//github.com/decargroup/hessian_sum_mixtures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05452v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hessian%20for%20Gaussian%20Mixture%20Likelihoods%20in%20Nonlinear%20Least%20Squares&entry.906535625=Vassili%20Korotkine%20and%20Mitchell%20Cohen%20and%20James%20Richard%20Forbes&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20Hessian%20approximation%20for%20Maximum%20a%20Posteriori%0Aestimation%20problems%20in%20robotics%20involving%20Gaussian%20mixture%20likelihoods.%20The%0Aproposed%20Hessian%20leads%20to%20better%20convergence%20properties.%20Previous%20approaches%0Amanipulate%20the%20Gaussian%20mixture%20likelihood%20into%20a%20form%20that%20allows%20the%20problem%0Ato%20be%20represented%20as%20a%20nonlinear%20least%20squares%20%28NLS%29%20problem.%20However%2C%20they%0Aresult%20in%20an%20inaccurate%20Hessian%20approximation%20due%20to%20additional%20nonlinearities%0Athat%20are%20not%20accounted%20for%20in%20NLS%20solvers.%20The%20proposed%20Hessian%20approximation%0Ais%20derived%20by%20setting%20the%20Hessians%20of%20the%20Gaussian%20mixture%20component%20errors%20to%0Azero%2C%20which%20is%20the%20same%20starting%20point%20as%20for%20the%20Gauss-Newton%20Hessian%0Aapproximation%20for%20NLS%2C%20and%20using%20the%20chain%20rule%20to%20account%20for%20additional%0Anonlinearities.%20The%20proposed%20Hessian%20approximation%20is%20more%20accurate%2C%20resulting%0Ain%20improved%20convergence%20properties%20that%20are%20demonstrated%20on%20simulated%20and%0Areal-world%20experiments.%20A%20method%20to%20maintain%20compatibility%20with%20existing%0Asolvers%2C%20such%20as%20ceres%2C%20is%20also%20presented.%20Accompanying%20software%20and%0Asupplementary%20material%20can%20be%20found%20at%0Ahttps%3A//github.com/decargroup/hessian_sum_mixtures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05452v1&entry.124074799=Read"},
{"title": "Finding Visual Task Vectors", "author": "Alberto Hojel and Yutong Bai and Trevor Darrell and Amir Globerson and Amir Bar", "abstract": "  Visual Prompting is a technique for teaching models to perform a visual task\nvia in-context examples, without any additional training. In this work, we\nanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find\ntask vectors, activations that encode task-specific information. Equipped with\nthis insight, we demonstrate that it is possible to identify the task vectors\nand use them to guide the network towards performing different tasks without\nproviding any input-output examples. To find task vectors, we compute the\naverage intermediate activations per task and use the REINFORCE algorithm to\nsearch for the subset of task vectors. The resulting task vectors guide the\nmodel towards performing a task better than the original model without the need\nfor input-output examples.\n", "link": "http://arxiv.org/abs/2404.05729v1", "date": "2024-04-08", "relevancy": 1.8334, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4694}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4539}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Finding%20Visual%20Task%20Vectors&body=Title%3A%20Finding%20Visual%20Task%20Vectors%0AAuthor%3A%20Alberto%20Hojel%20and%20Yutong%20Bai%20and%20Trevor%20Darrell%20and%20Amir%20Globerson%20and%20Amir%20Bar%0AAbstract%3A%20%20%20Visual%20Prompting%20is%20a%20technique%20for%20teaching%20models%20to%20perform%20a%20visual%20task%0Avia%20in-context%20examples%2C%20without%20any%20additional%20training.%20In%20this%20work%2C%20we%0Aanalyze%20the%20activations%20of%20MAE-VQGAN%2C%20a%20recent%20Visual%20Prompting%20model%2C%20and%20find%0Atask%20vectors%2C%20activations%20that%20encode%20task-specific%20information.%20Equipped%20with%0Athis%20insight%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20identify%20the%20task%20vectors%0Aand%20use%20them%20to%20guide%20the%20network%20towards%20performing%20different%20tasks%20without%0Aproviding%20any%20input-output%20examples.%20To%20find%20task%20vectors%2C%20we%20compute%20the%0Aaverage%20intermediate%20activations%20per%20task%20and%20use%20the%20REINFORCE%20algorithm%20to%0Asearch%20for%20the%20subset%20of%20task%20vectors.%20The%20resulting%20task%20vectors%20guide%20the%0Amodel%20towards%20performing%20a%20task%20better%20than%20the%20original%20model%20without%20the%20need%0Afor%20input-output%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Visual%20Task%20Vectors&entry.906535625=Alberto%20Hojel%20and%20Yutong%20Bai%20and%20Trevor%20Darrell%20and%20Amir%20Globerson%20and%20Amir%20Bar&entry.1292438233=%20%20Visual%20Prompting%20is%20a%20technique%20for%20teaching%20models%20to%20perform%20a%20visual%20task%0Avia%20in-context%20examples%2C%20without%20any%20additional%20training.%20In%20this%20work%2C%20we%0Aanalyze%20the%20activations%20of%20MAE-VQGAN%2C%20a%20recent%20Visual%20Prompting%20model%2C%20and%20find%0Atask%20vectors%2C%20activations%20that%20encode%20task-specific%20information.%20Equipped%20with%0Athis%20insight%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20identify%20the%20task%20vectors%0Aand%20use%20them%20to%20guide%20the%20network%20towards%20performing%20different%20tasks%20without%0Aproviding%20any%20input-output%20examples.%20To%20find%20task%20vectors%2C%20we%20compute%20the%0Aaverage%20intermediate%20activations%20per%20task%20and%20use%20the%20REINFORCE%20algorithm%20to%0Asearch%20for%20the%20subset%20of%20task%20vectors.%20The%20resulting%20task%20vectors%20guide%20the%0Amodel%20towards%20performing%20a%20task%20better%20than%20the%20original%20model%20without%20the%20need%0Afor%20input-output%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05729v1&entry.124074799=Read"},
{"title": "Fact-Checking Generative AI: Ontology-Driven Biological Graphs for\n  Disease-Gene Link Verification", "author": "Ahmed Abdeen Hamed and Byung Suk Lee and Alessandro Crimi and Magdalena M. Misiak", "abstract": "  Since the launch of various generative AI tools, scientists have been\nstriving to evaluate their capabilities and contents, in the hope of\nestablishing trust in their generative abilities. Regulations and guidelines\nare emerging to verify generated contents and identify novel uses. we aspire to\ndemonstrate how ChatGPT claims are checked computationally using the rigor of\nnetwork models. We aim to achieve fact-checking of the knowledge embedded in\nbiological graphs that were contrived from ChatGPT contents at the aggregate\nlevel. We adopted a biological networks approach that enables the systematic\ninterrogation of ChatGPT's linked entities. We designed an ontology-driven\nfact-checking algorithm that compares biological graphs constructed from\napproximately 200,000 PubMed abstracts with counterparts constructed from a\ndataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250\nrandomly selected records a ChatGPT dataset of 1000 \"simulated\" articles , the\nfact-checking link accuracy ranged from 70% to 86%. This study demonstrated\nhigh accuracy of aggregate disease-gene links relationships found in\nChatGPT-generated texts.\n", "link": "http://arxiv.org/abs/2308.03929v4", "date": "2024-04-08", "relevancy": 1.8218, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4734}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4341}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fact-Checking%20Generative%20AI%3A%20Ontology-Driven%20Biological%20Graphs%20for%0A%20%20Disease-Gene%20Link%20Verification&body=Title%3A%20Fact-Checking%20Generative%20AI%3A%20Ontology-Driven%20Biological%20Graphs%20for%0A%20%20Disease-Gene%20Link%20Verification%0AAuthor%3A%20Ahmed%20Abdeen%20Hamed%20and%20Byung%20Suk%20Lee%20and%20Alessandro%20Crimi%20and%20Magdalena%20M.%20Misiak%0AAbstract%3A%20%20%20Since%20the%20launch%20of%20various%20generative%20AI%20tools%2C%20scientists%20have%20been%0Astriving%20to%20evaluate%20their%20capabilities%20and%20contents%2C%20in%20the%20hope%20of%0Aestablishing%20trust%20in%20their%20generative%20abilities.%20Regulations%20and%20guidelines%0Aare%20emerging%20to%20verify%20generated%20contents%20and%20identify%20novel%20uses.%20we%20aspire%20to%0Ademonstrate%20how%20ChatGPT%20claims%20are%20checked%20computationally%20using%20the%20rigor%20of%0Anetwork%20models.%20We%20aim%20to%20achieve%20fact-checking%20of%20the%20knowledge%20embedded%20in%0Abiological%20graphs%20that%20were%20contrived%20from%20ChatGPT%20contents%20at%20the%20aggregate%0Alevel.%20We%20adopted%20a%20biological%20networks%20approach%20that%20enables%20the%20systematic%0Ainterrogation%20of%20ChatGPT%27s%20linked%20entities.%20We%20designed%20an%20ontology-driven%0Afact-checking%20algorithm%20that%20compares%20biological%20graphs%20constructed%20from%0Aapproximately%20200%2C000%20PubMed%20abstracts%20with%20counterparts%20constructed%20from%20a%0Adataset%20generated%20using%20the%20ChatGPT-3.5%20Turbo%20model.%20In%2010-samples%20of%20250%0Arandomly%20selected%20records%20a%20ChatGPT%20dataset%20of%201000%20%22simulated%22%20articles%20%2C%20the%0Afact-checking%20link%20accuracy%20ranged%20from%2070%25%20to%2086%25.%20This%20study%20demonstrated%0Ahigh%20accuracy%20of%20aggregate%20disease-gene%20links%20relationships%20found%20in%0AChatGPT-generated%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03929v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fact-Checking%20Generative%20AI%3A%20Ontology-Driven%20Biological%20Graphs%20for%0A%20%20Disease-Gene%20Link%20Verification&entry.906535625=Ahmed%20Abdeen%20Hamed%20and%20Byung%20Suk%20Lee%20and%20Alessandro%20Crimi%20and%20Magdalena%20M.%20Misiak&entry.1292438233=%20%20Since%20the%20launch%20of%20various%20generative%20AI%20tools%2C%20scientists%20have%20been%0Astriving%20to%20evaluate%20their%20capabilities%20and%20contents%2C%20in%20the%20hope%20of%0Aestablishing%20trust%20in%20their%20generative%20abilities.%20Regulations%20and%20guidelines%0Aare%20emerging%20to%20verify%20generated%20contents%20and%20identify%20novel%20uses.%20we%20aspire%20to%0Ademonstrate%20how%20ChatGPT%20claims%20are%20checked%20computationally%20using%20the%20rigor%20of%0Anetwork%20models.%20We%20aim%20to%20achieve%20fact-checking%20of%20the%20knowledge%20embedded%20in%0Abiological%20graphs%20that%20were%20contrived%20from%20ChatGPT%20contents%20at%20the%20aggregate%0Alevel.%20We%20adopted%20a%20biological%20networks%20approach%20that%20enables%20the%20systematic%0Ainterrogation%20of%20ChatGPT%27s%20linked%20entities.%20We%20designed%20an%20ontology-driven%0Afact-checking%20algorithm%20that%20compares%20biological%20graphs%20constructed%20from%0Aapproximately%20200%2C000%20PubMed%20abstracts%20with%20counterparts%20constructed%20from%20a%0Adataset%20generated%20using%20the%20ChatGPT-3.5%20Turbo%20model.%20In%2010-samples%20of%20250%0Arandomly%20selected%20records%20a%20ChatGPT%20dataset%20of%201000%20%22simulated%22%20articles%20%2C%20the%0Afact-checking%20link%20accuracy%20ranged%20from%2070%25%20to%2086%25.%20This%20study%20demonstrated%0Ahigh%20accuracy%20of%20aggregate%20disease-gene%20links%20relationships%20found%20in%0AChatGPT-generated%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03929v4&entry.124074799=Read"},
{"title": "SoK: Gradient Leakage in Federated Learning", "author": "Jiacheng Du and Jiahui Hu and Zhibo Wang and Peng Sun and Neil Zhenqiang Gong and Kui Ren", "abstract": "  Federated learning (FL) enables collaborative model training among multiple\nclients without raw data exposure. However, recent studies have shown that\nclients' private training data can be reconstructed from the gradients they\nshare in FL, known as gradient inversion attacks (GIAs). While GIAs have\ndemonstrated effectiveness under \\emph{ideal settings and auxiliary\nassumptions}, their actual efficacy against \\emph{practical FL systems} remains\nunder-explored. To address this gap, we conduct a comprehensive study on GIAs\nin this work. We start with a survey of GIAs that establishes a milestone to\ntrace their evolution and develops a systematization to uncover their inherent\nthreats. Specifically, we categorize the auxiliary assumptions used by existing\nGIAs based on their practical accessibility to potential adversaries. To\nfacilitate deeper analysis, we highlight the challenges that GIAs face in\npractical FL systems from three perspectives: \\textit{local training},\n\\textit{model}, and \\textit{post-processing}. We then perform extensive\ntheoretical and empirical evaluations of state-of-the-art GIAs across diverse\nsettings, utilizing eight datasets and thirteen models. Our findings indicate\nthat GIAs have inherent limitations when reconstructing data under practical\nlocal training settings. Furthermore, their efficacy is sensitive to the\ntrained model, and even simple post-processing measures applied to gradients\ncan be effective defenses. Overall, our work provides crucial insights into the\nlimited effectiveness of GIAs in practical FL systems. By rectifying prior\nmisconceptions, we hope to inspire more accurate and realistic investigations\non this topic.\n", "link": "http://arxiv.org/abs/2404.05403v1", "date": "2024-04-08", "relevancy": 1.8201, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4712}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4605}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4431}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Gradient%20Leakage%20in%20Federated%20Learning&body=Title%3A%20SoK%3A%20Gradient%20Leakage%20in%20Federated%20Learning%0AAuthor%3A%20Jiacheng%20Du%20and%20Jiahui%20Hu%20and%20Zhibo%20Wang%20and%20Peng%20Sun%20and%20Neil%20Zhenqiang%20Gong%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20among%20multiple%0Aclients%20without%20raw%20data%20exposure.%20However%2C%20recent%20studies%20have%20shown%20that%0Aclients%27%20private%20training%20data%20can%20be%20reconstructed%20from%20the%20gradients%20they%0Ashare%20in%20FL%2C%20known%20as%20gradient%20inversion%20attacks%20%28GIAs%29.%20While%20GIAs%20have%0Ademonstrated%20effectiveness%20under%20%5Cemph%7Bideal%20settings%20and%20auxiliary%0Aassumptions%7D%2C%20their%20actual%20efficacy%20against%20%5Cemph%7Bpractical%20FL%20systems%7D%20remains%0Aunder-explored.%20To%20address%20this%20gap%2C%20we%20conduct%20a%20comprehensive%20study%20on%20GIAs%0Ain%20this%20work.%20We%20start%20with%20a%20survey%20of%20GIAs%20that%20establishes%20a%20milestone%20to%0Atrace%20their%20evolution%20and%20develops%20a%20systematization%20to%20uncover%20their%20inherent%0Athreats.%20Specifically%2C%20we%20categorize%20the%20auxiliary%20assumptions%20used%20by%20existing%0AGIAs%20based%20on%20their%20practical%20accessibility%20to%20potential%20adversaries.%20To%0Afacilitate%20deeper%20analysis%2C%20we%20highlight%20the%20challenges%20that%20GIAs%20face%20in%0Apractical%20FL%20systems%20from%20three%20perspectives%3A%20%5Ctextit%7Blocal%20training%7D%2C%0A%5Ctextit%7Bmodel%7D%2C%20and%20%5Ctextit%7Bpost-processing%7D.%20We%20then%20perform%20extensive%0Atheoretical%20and%20empirical%20evaluations%20of%20state-of-the-art%20GIAs%20across%20diverse%0Asettings%2C%20utilizing%20eight%20datasets%20and%20thirteen%20models.%20Our%20findings%20indicate%0Athat%20GIAs%20have%20inherent%20limitations%20when%20reconstructing%20data%20under%20practical%0Alocal%20training%20settings.%20Furthermore%2C%20their%20efficacy%20is%20sensitive%20to%20the%0Atrained%20model%2C%20and%20even%20simple%20post-processing%20measures%20applied%20to%20gradients%0Acan%20be%20effective%20defenses.%20Overall%2C%20our%20work%20provides%20crucial%20insights%20into%20the%0Alimited%20effectiveness%20of%20GIAs%20in%20practical%20FL%20systems.%20By%20rectifying%20prior%0Amisconceptions%2C%20we%20hope%20to%20inspire%20more%20accurate%20and%20realistic%20investigations%0Aon%20this%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05403v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Gradient%20Leakage%20in%20Federated%20Learning&entry.906535625=Jiacheng%20Du%20and%20Jiahui%20Hu%20and%20Zhibo%20Wang%20and%20Peng%20Sun%20and%20Neil%20Zhenqiang%20Gong%20and%20Kui%20Ren&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20among%20multiple%0Aclients%20without%20raw%20data%20exposure.%20However%2C%20recent%20studies%20have%20shown%20that%0Aclients%27%20private%20training%20data%20can%20be%20reconstructed%20from%20the%20gradients%20they%0Ashare%20in%20FL%2C%20known%20as%20gradient%20inversion%20attacks%20%28GIAs%29.%20While%20GIAs%20have%0Ademonstrated%20effectiveness%20under%20%5Cemph%7Bideal%20settings%20and%20auxiliary%0Aassumptions%7D%2C%20their%20actual%20efficacy%20against%20%5Cemph%7Bpractical%20FL%20systems%7D%20remains%0Aunder-explored.%20To%20address%20this%20gap%2C%20we%20conduct%20a%20comprehensive%20study%20on%20GIAs%0Ain%20this%20work.%20We%20start%20with%20a%20survey%20of%20GIAs%20that%20establishes%20a%20milestone%20to%0Atrace%20their%20evolution%20and%20develops%20a%20systematization%20to%20uncover%20their%20inherent%0Athreats.%20Specifically%2C%20we%20categorize%20the%20auxiliary%20assumptions%20used%20by%20existing%0AGIAs%20based%20on%20their%20practical%20accessibility%20to%20potential%20adversaries.%20To%0Afacilitate%20deeper%20analysis%2C%20we%20highlight%20the%20challenges%20that%20GIAs%20face%20in%0Apractical%20FL%20systems%20from%20three%20perspectives%3A%20%5Ctextit%7Blocal%20training%7D%2C%0A%5Ctextit%7Bmodel%7D%2C%20and%20%5Ctextit%7Bpost-processing%7D.%20We%20then%20perform%20extensive%0Atheoretical%20and%20empirical%20evaluations%20of%20state-of-the-art%20GIAs%20across%20diverse%0Asettings%2C%20utilizing%20eight%20datasets%20and%20thirteen%20models.%20Our%20findings%20indicate%0Athat%20GIAs%20have%20inherent%20limitations%20when%20reconstructing%20data%20under%20practical%0Alocal%20training%20settings.%20Furthermore%2C%20their%20efficacy%20is%20sensitive%20to%20the%0Atrained%20model%2C%20and%20even%20simple%20post-processing%20measures%20applied%20to%20gradients%0Acan%20be%20effective%20defenses.%20Overall%2C%20our%20work%20provides%20crucial%20insights%20into%20the%0Alimited%20effectiveness%20of%20GIAs%20in%20practical%20FL%20systems.%20By%20rectifying%20prior%0Amisconceptions%2C%20we%20hope%20to%20inspire%20more%20accurate%20and%20realistic%20investigations%0Aon%20this%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05403v1&entry.124074799=Read"},
{"title": "PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of\n  LLM-generated Text?", "author": "Kseniia Petukhova and Roman Kazakov and Ekaterina Kochmar", "abstract": "  In this paper, we present our submission to the SemEval-2024 Task 8\n\"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection\", focusing on the detection of machine-generated texts (MGTs) in\nEnglish. Specifically, our approach relies on combining embeddings from the\nRoBERTa-base with diversity features and uses a resampled training set. We\nscore 12th from 124 in the ranking for Subtask A (monolingual track), and our\nresults show that our approach is generalizable across unseen models and\ndomains, achieving an accuracy of 0.91.\n", "link": "http://arxiv.org/abs/2404.05483v1", "date": "2024-04-08", "relevancy": 1.815, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PetKaz%20at%20SemEval-2024%20Task%208%3A%20Can%20Linguistics%20Capture%20the%20Specifics%20of%0A%20%20LLM-generated%20Text%3F&body=Title%3A%20PetKaz%20at%20SemEval-2024%20Task%208%3A%20Can%20Linguistics%20Capture%20the%20Specifics%20of%0A%20%20LLM-generated%20Text%3F%0AAuthor%3A%20Kseniia%20Petukhova%20and%20Roman%20Kazakov%20and%20Ekaterina%20Kochmar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20submission%20to%20the%20SemEval-2024%20Task%208%0A%22Multigenerator%2C%20Multidomain%2C%20and%20Multilingual%20Black-Box%20Machine-Generated%20Text%0ADetection%22%2C%20focusing%20on%20the%20detection%20of%20machine-generated%20texts%20%28MGTs%29%20in%0AEnglish.%20Specifically%2C%20our%20approach%20relies%20on%20combining%20embeddings%20from%20the%0ARoBERTa-base%20with%20diversity%20features%20and%20uses%20a%20resampled%20training%20set.%20We%0Ascore%2012th%20from%20124%20in%20the%20ranking%20for%20Subtask%20A%20%28monolingual%20track%29%2C%20and%20our%0Aresults%20show%20that%20our%20approach%20is%20generalizable%20across%20unseen%20models%20and%0Adomains%2C%20achieving%20an%20accuracy%20of%200.91.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PetKaz%20at%20SemEval-2024%20Task%208%3A%20Can%20Linguistics%20Capture%20the%20Specifics%20of%0A%20%20LLM-generated%20Text%3F&entry.906535625=Kseniia%20Petukhova%20and%20Roman%20Kazakov%20and%20Ekaterina%20Kochmar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20submission%20to%20the%20SemEval-2024%20Task%208%0A%22Multigenerator%2C%20Multidomain%2C%20and%20Multilingual%20Black-Box%20Machine-Generated%20Text%0ADetection%22%2C%20focusing%20on%20the%20detection%20of%20machine-generated%20texts%20%28MGTs%29%20in%0AEnglish.%20Specifically%2C%20our%20approach%20relies%20on%20combining%20embeddings%20from%20the%0ARoBERTa-base%20with%20diversity%20features%20and%20uses%20a%20resampled%20training%20set.%20We%0Ascore%2012th%20from%20124%20in%20the%20ranking%20for%20Subtask%20A%20%28monolingual%20track%29%2C%20and%20our%0Aresults%20show%20that%20our%20approach%20is%20generalizable%20across%20unseen%20models%20and%0Adomains%2C%20achieving%20an%20accuracy%20of%200.91.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05483v1&entry.124074799=Read"},
{"title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of\n  Vietnamese in the Medical Domain", "author": "Khai Le-Duc", "abstract": "  Due to privacy restrictions, there's a shortage of publicly available speech\nrecognition datasets in the medical domain. In this work, we present VietMed -\na Vietnamese speech recognition dataset in the medical domain comprising 16h of\nlabeled medical speech, 1000h of unlabeled medical speech and 1200h of\nunlabeled general-domain speech. To our best knowledge, VietMed is by far the\nworld's largest public medical speech recognition dataset in 7 aspects: total\nduration, number of speakers, diseases, recording conditions, speaker roles,\nunique medical terms and accents. VietMed is also by far the largest public\nVietnamese speech dataset in terms of total duration. Additionally, we are the\nfirst to present a medical ASR dataset covering all ICD-10 disease groups and\nall accents within a country. Moreover, we release the first public large-scale\npre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with\nthe first public large-scale fine-tuned models for medical ASR. Even without\nany medical data in unsupervised pre-training, our best pre-trained model\nXLSR-53-Viet generalizes very well to the medical domain by outperforming\nstate-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative\nreduction of more than 40%). All code, data and models are made publicly\navailable here: https://github.com/leduckhai/MultiMed.\n", "link": "http://arxiv.org/abs/2404.05659v1", "date": "2024-04-08", "relevancy": 1.8034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.442}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4364}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VietMed%3A%20A%20Dataset%20and%20Benchmark%20for%20Automatic%20Speech%20Recognition%20of%0A%20%20Vietnamese%20in%20the%20Medical%20Domain&body=Title%3A%20VietMed%3A%20A%20Dataset%20and%20Benchmark%20for%20Automatic%20Speech%20Recognition%20of%0A%20%20Vietnamese%20in%20the%20Medical%20Domain%0AAuthor%3A%20Khai%20Le-Duc%0AAbstract%3A%20%20%20Due%20to%20privacy%20restrictions%2C%20there%27s%20a%20shortage%20of%20publicly%20available%20speech%0Arecognition%20datasets%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20present%20VietMed%20-%0Aa%20Vietnamese%20speech%20recognition%20dataset%20in%20the%20medical%20domain%20comprising%2016h%20of%0Alabeled%20medical%20speech%2C%201000h%20of%20unlabeled%20medical%20speech%20and%201200h%20of%0Aunlabeled%20general-domain%20speech.%20To%20our%20best%20knowledge%2C%20VietMed%20is%20by%20far%20the%0Aworld%27s%20largest%20public%20medical%20speech%20recognition%20dataset%20in%207%20aspects%3A%20total%0Aduration%2C%20number%20of%20speakers%2C%20diseases%2C%20recording%20conditions%2C%20speaker%20roles%2C%0Aunique%20medical%20terms%20and%20accents.%20VietMed%20is%20also%20by%20far%20the%20largest%20public%0AVietnamese%20speech%20dataset%20in%20terms%20of%20total%20duration.%20Additionally%2C%20we%20are%20the%0Afirst%20to%20present%20a%20medical%20ASR%20dataset%20covering%20all%20ICD-10%20disease%20groups%20and%0Aall%20accents%20within%20a%20country.%20Moreover%2C%20we%20release%20the%20first%20public%20large-scale%0Apre-trained%20models%20for%20Vietnamese%20ASR%2C%20w2v2-Viet%20and%20XLSR-53-Viet%2C%20along%20with%0Athe%20first%20public%20large-scale%20fine-tuned%20models%20for%20medical%20ASR.%20Even%20without%0Aany%20medical%20data%20in%20unsupervised%20pre-training%2C%20our%20best%20pre-trained%20model%0AXLSR-53-Viet%20generalizes%20very%20well%20to%20the%20medical%20domain%20by%20outperforming%0Astate-of-the-art%20XLSR-53%2C%20from%2051.8%25%20to%2029.6%25%20WER%20on%20test%20set%20%28a%20relative%0Areduction%20of%20more%20than%2040%25%29.%20All%20code%2C%20data%20and%20models%20are%20made%20publicly%0Aavailable%20here%3A%20https%3A//github.com/leduckhai/MultiMed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05659v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VietMed%3A%20A%20Dataset%20and%20Benchmark%20for%20Automatic%20Speech%20Recognition%20of%0A%20%20Vietnamese%20in%20the%20Medical%20Domain&entry.906535625=Khai%20Le-Duc&entry.1292438233=%20%20Due%20to%20privacy%20restrictions%2C%20there%27s%20a%20shortage%20of%20publicly%20available%20speech%0Arecognition%20datasets%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20present%20VietMed%20-%0Aa%20Vietnamese%20speech%20recognition%20dataset%20in%20the%20medical%20domain%20comprising%2016h%20of%0Alabeled%20medical%20speech%2C%201000h%20of%20unlabeled%20medical%20speech%20and%201200h%20of%0Aunlabeled%20general-domain%20speech.%20To%20our%20best%20knowledge%2C%20VietMed%20is%20by%20far%20the%0Aworld%27s%20largest%20public%20medical%20speech%20recognition%20dataset%20in%207%20aspects%3A%20total%0Aduration%2C%20number%20of%20speakers%2C%20diseases%2C%20recording%20conditions%2C%20speaker%20roles%2C%0Aunique%20medical%20terms%20and%20accents.%20VietMed%20is%20also%20by%20far%20the%20largest%20public%0AVietnamese%20speech%20dataset%20in%20terms%20of%20total%20duration.%20Additionally%2C%20we%20are%20the%0Afirst%20to%20present%20a%20medical%20ASR%20dataset%20covering%20all%20ICD-10%20disease%20groups%20and%0Aall%20accents%20within%20a%20country.%20Moreover%2C%20we%20release%20the%20first%20public%20large-scale%0Apre-trained%20models%20for%20Vietnamese%20ASR%2C%20w2v2-Viet%20and%20XLSR-53-Viet%2C%20along%20with%0Athe%20first%20public%20large-scale%20fine-tuned%20models%20for%20medical%20ASR.%20Even%20without%0Aany%20medical%20data%20in%20unsupervised%20pre-training%2C%20our%20best%20pre-trained%20model%0AXLSR-53-Viet%20generalizes%20very%20well%20to%20the%20medical%20domain%20by%20outperforming%0Astate-of-the-art%20XLSR-53%2C%20from%2051.8%25%20to%2029.6%25%20WER%20on%20test%20set%20%28a%20relative%0Areduction%20of%20more%20than%2040%25%29.%20All%20code%2C%20data%20and%20models%20are%20made%20publicly%0Aavailable%20here%3A%20https%3A//github.com/leduckhai/MultiMed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05659v1&entry.124074799=Read"},
{"title": "Constraining Large Language Model for Generating Computer-Parsable\n  Content", "author": "Jiaye Wang", "abstract": "  We propose a method to guide Large Language Models (LLMs) in generating\nstructured content adhering to specific conventions without fine-tuning. By\nutilizing coroutine-based content generation constraints through a pre-agreed\ncontext-free grammar (CFG), LLMs are directed during decoding to produce formal\nlanguage compliant outputs. This enhances stability and consistency in\ngenerating target data structures, types, or instructions, reducing application\ndevelopment complexities. Experimentally, error rates of GPT-2 and Gemma exceed\n95% for DSLs longer than 36 and 282 tokens, respectively. We introduce\nYieldLang, a coroutine-based DSL generation framework, and evaluate it with\nLLMs on various tasks including JSON and Mermaid flowchart generation. Compared\nto benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs\nrequiring only about 16.5% of the samples to generate JSON effectively. This\nenhances usability of LLM-generated content for computer programs.\n", "link": "http://arxiv.org/abs/2404.05499v1", "date": "2024-04-08", "relevancy": 1.7967, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4564}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4535}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Constraining%20Large%20Language%20Model%20for%20Generating%20Computer-Parsable%0A%20%20Content&body=Title%3A%20Constraining%20Large%20Language%20Model%20for%20Generating%20Computer-Parsable%0A%20%20Content%0AAuthor%3A%20Jiaye%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20method%20to%20guide%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%0Astructured%20content%20adhering%20to%20specific%20conventions%20without%20fine-tuning.%20By%0Autilizing%20coroutine-based%20content%20generation%20constraints%20through%20a%20pre-agreed%0Acontext-free%20grammar%20%28CFG%29%2C%20LLMs%20are%20directed%20during%20decoding%20to%20produce%20formal%0Alanguage%20compliant%20outputs.%20This%20enhances%20stability%20and%20consistency%20in%0Agenerating%20target%20data%20structures%2C%20types%2C%20or%20instructions%2C%20reducing%20application%0Adevelopment%20complexities.%20Experimentally%2C%20error%20rates%20of%20GPT-2%20and%20Gemma%20exceed%0A95%25%20for%20DSLs%20longer%20than%2036%20and%20282%20tokens%2C%20respectively.%20We%20introduce%0AYieldLang%2C%20a%20coroutine-based%20DSL%20generation%20framework%2C%20and%20evaluate%20it%20with%0ALLMs%20on%20various%20tasks%20including%20JSON%20and%20Mermaid%20flowchart%20generation.%20Compared%0Ato%20benchmarks%2C%20our%20approach%20improves%20accuracy%20by%201.09%20to%2011.6%20times%2C%20with%20LLMs%0Arequiring%20only%20about%2016.5%25%20of%20the%20samples%20to%20generate%20JSON%20effectively.%20This%0Aenhances%20usability%20of%20LLM-generated%20content%20for%20computer%20programs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraining%20Large%20Language%20Model%20for%20Generating%20Computer-Parsable%0A%20%20Content&entry.906535625=Jiaye%20Wang&entry.1292438233=%20%20We%20propose%20a%20method%20to%20guide%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%0Astructured%20content%20adhering%20to%20specific%20conventions%20without%20fine-tuning.%20By%0Autilizing%20coroutine-based%20content%20generation%20constraints%20through%20a%20pre-agreed%0Acontext-free%20grammar%20%28CFG%29%2C%20LLMs%20are%20directed%20during%20decoding%20to%20produce%20formal%0Alanguage%20compliant%20outputs.%20This%20enhances%20stability%20and%20consistency%20in%0Agenerating%20target%20data%20structures%2C%20types%2C%20or%20instructions%2C%20reducing%20application%0Adevelopment%20complexities.%20Experimentally%2C%20error%20rates%20of%20GPT-2%20and%20Gemma%20exceed%0A95%25%20for%20DSLs%20longer%20than%2036%20and%20282%20tokens%2C%20respectively.%20We%20introduce%0AYieldLang%2C%20a%20coroutine-based%20DSL%20generation%20framework%2C%20and%20evaluate%20it%20with%0ALLMs%20on%20various%20tasks%20including%20JSON%20and%20Mermaid%20flowchart%20generation.%20Compared%0Ato%20benchmarks%2C%20our%20approach%20improves%20accuracy%20by%201.09%20to%2011.6%20times%2C%20with%20LLMs%0Arequiring%20only%20about%2016.5%25%20of%20the%20samples%20to%20generate%20JSON%20effectively.%20This%0Aenhances%20usability%20of%20LLM-generated%20content%20for%20computer%20programs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05499v1&entry.124074799=Read"},
{"title": "Alljoined -- A dataset for EEG-to-Image decoding", "author": "Jonathan Xu and Bruno Aristimunha and Max Emanuel Feucht and Emma Qian and Charles Liu and Tazik Shahjahan and Martyna Spyra and Steven Zifan Zhang and Nicholas Short and Jioh Kim and Paula Perdomo and Ricky Renfeng Mao and Yashvir Sabharwal and Michael Ahedor Moaz Shoura and Adrian Nestor", "abstract": "  We present Alljoined, a dataset built specifically for EEG-to-Image decoding.\nRecognizing that an extensive and unbiased sampling of neural responses to\nvisual stimuli is crucial for image reconstruction efforts, we collected data\nfrom 8 participants looking at 10,000 natural images each. We have currently\ngathered 46,080 epochs of brain responses recorded with a 64-channel EEG\nheadset. The dataset combines response-based stimulus timing, repetition\nbetween blocks and sessions, and diverse image classes with the goal of\nimproving signal quality. For transparency, we also provide data quality\nscores. We publicly release the dataset and all code at\nhttps://linktr.ee/alljoined1.\n", "link": "http://arxiv.org/abs/2404.05553v1", "date": "2024-04-08", "relevancy": 1.7929, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4546}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Alljoined%20--%20A%20dataset%20for%20EEG-to-Image%20decoding&body=Title%3A%20Alljoined%20--%20A%20dataset%20for%20EEG-to-Image%20decoding%0AAuthor%3A%20Jonathan%20Xu%20and%20Bruno%20Aristimunha%20and%20Max%20Emanuel%20Feucht%20and%20Emma%20Qian%20and%20Charles%20Liu%20and%20Tazik%20Shahjahan%20and%20Martyna%20Spyra%20and%20Steven%20Zifan%20Zhang%20and%20Nicholas%20Short%20and%20Jioh%20Kim%20and%20Paula%20Perdomo%20and%20Ricky%20Renfeng%20Mao%20and%20Yashvir%20Sabharwal%20and%20Michael%20Ahedor%20Moaz%20Shoura%20and%20Adrian%20Nestor%0AAbstract%3A%20%20%20We%20present%20Alljoined%2C%20a%20dataset%20built%20specifically%20for%20EEG-to-Image%20decoding.%0ARecognizing%20that%20an%20extensive%20and%20unbiased%20sampling%20of%20neural%20responses%20to%0Avisual%20stimuli%20is%20crucial%20for%20image%20reconstruction%20efforts%2C%20we%20collected%20data%0Afrom%208%20participants%20looking%20at%2010%2C000%20natural%20images%20each.%20We%20have%20currently%0Agathered%2046%2C080%20epochs%20of%20brain%20responses%20recorded%20with%20a%2064-channel%20EEG%0Aheadset.%20The%20dataset%20combines%20response-based%20stimulus%20timing%2C%20repetition%0Abetween%20blocks%20and%20sessions%2C%20and%20diverse%20image%20classes%20with%20the%20goal%20of%0Aimproving%20signal%20quality.%20For%20transparency%2C%20we%20also%20provide%20data%20quality%0Ascores.%20We%20publicly%20release%20the%20dataset%20and%20all%20code%20at%0Ahttps%3A//linktr.ee/alljoined1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05553v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alljoined%20--%20A%20dataset%20for%20EEG-to-Image%20decoding&entry.906535625=Jonathan%20Xu%20and%20Bruno%20Aristimunha%20and%20Max%20Emanuel%20Feucht%20and%20Emma%20Qian%20and%20Charles%20Liu%20and%20Tazik%20Shahjahan%20and%20Martyna%20Spyra%20and%20Steven%20Zifan%20Zhang%20and%20Nicholas%20Short%20and%20Jioh%20Kim%20and%20Paula%20Perdomo%20and%20Ricky%20Renfeng%20Mao%20and%20Yashvir%20Sabharwal%20and%20Michael%20Ahedor%20Moaz%20Shoura%20and%20Adrian%20Nestor&entry.1292438233=%20%20We%20present%20Alljoined%2C%20a%20dataset%20built%20specifically%20for%20EEG-to-Image%20decoding.%0ARecognizing%20that%20an%20extensive%20and%20unbiased%20sampling%20of%20neural%20responses%20to%0Avisual%20stimuli%20is%20crucial%20for%20image%20reconstruction%20efforts%2C%20we%20collected%20data%0Afrom%208%20participants%20looking%20at%2010%2C000%20natural%20images%20each.%20We%20have%20currently%0Agathered%2046%2C080%20epochs%20of%20brain%20responses%20recorded%20with%20a%2064-channel%20EEG%0Aheadset.%20The%20dataset%20combines%20response-based%20stimulus%20timing%2C%20repetition%0Abetween%20blocks%20and%20sessions%2C%20and%20diverse%20image%20classes%20with%20the%20goal%20of%0Aimproving%20signal%20quality.%20For%20transparency%2C%20we%20also%20provide%20data%20quality%0Ascores.%20We%20publicly%20release%20the%20dataset%20and%20all%20code%20at%0Ahttps%3A//linktr.ee/alljoined1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05553v1&entry.124074799=Read"},
{"title": "Design and Simulation of Time-energy Optimal Anti-swing Trajectory\n  Planner for Autonomous Tower Cranes", "author": "Souravik Dutta and Yiyu Cai", "abstract": "  For autonomous crane lifting, optimal trajectories of the crane are required\nas reference inputs to the crane controller to facilitate feedforward control.\nReducing the unactuated payload motion is a crucial issue for under-actuated\ntower cranes with spherical pendulum dynamics. The planned trajectory should be\noptimal in terms of both operating time and energy consumption, to facilitate\noptimum output spending optimum effort. This article proposes an anti-swing\ntower crane trajectory planner that can provide time-energy optimal solutions\nfor the Computer-Aided Lift Planning (CALP) system developed at Nanyang\nTechnological University, which facilitates collision-free lifting path\nplanning of robotized tower cranes in autonomous construction sites. The\ncurrent work introduces a trajectory planning module to the system that\nutilizes the geometric outputs from the path planning module and optimally\nscales them with time information. Firstly, analyzing the non-linear dynamics\nof the crane operations, the tower crane is established as differentially flat.\nSubsequently, the multi-objective trajectory optimization problems for all the\ncrane operations are formulated in the flat output space through consideration\nof the mechanical and safety constraints. Two multi-objective evolutionary\nalgorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II) and\nGeneralized Differential Evolution 3 (GDE3), are extensively compared via\nstatistical measures based on the closeness of solutions to the Pareto front,\ndistribution of solutions in the solution space and the runtime, to select the\noptimization engine of the planner. Finally, the crane operation trajectories\nare obtained via the corresponding planned flat output trajectories. Studies\nsimulating real-world lifting scenarios are conducted to verify the\neffectiveness and reliability of the proposed module of the lift planning\nsystem.\n", "link": "http://arxiv.org/abs/2404.05581v1", "date": "2024-04-08", "relevancy": 1.7925, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4548}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4427}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Simulation%20of%20Time-energy%20Optimal%20Anti-swing%20Trajectory%0A%20%20Planner%20for%20Autonomous%20Tower%20Cranes&body=Title%3A%20Design%20and%20Simulation%20of%20Time-energy%20Optimal%20Anti-swing%20Trajectory%0A%20%20Planner%20for%20Autonomous%20Tower%20Cranes%0AAuthor%3A%20Souravik%20Dutta%20and%20Yiyu%20Cai%0AAbstract%3A%20%20%20For%20autonomous%20crane%20lifting%2C%20optimal%20trajectories%20of%20the%20crane%20are%20required%0Aas%20reference%20inputs%20to%20the%20crane%20controller%20to%20facilitate%20feedforward%20control.%0AReducing%20the%20unactuated%20payload%20motion%20is%20a%20crucial%20issue%20for%20under-actuated%0Atower%20cranes%20with%20spherical%20pendulum%20dynamics.%20The%20planned%20trajectory%20should%20be%0Aoptimal%20in%20terms%20of%20both%20operating%20time%20and%20energy%20consumption%2C%20to%20facilitate%0Aoptimum%20output%20spending%20optimum%20effort.%20This%20article%20proposes%20an%20anti-swing%0Atower%20crane%20trajectory%20planner%20that%20can%20provide%20time-energy%20optimal%20solutions%0Afor%20the%20Computer-Aided%20Lift%20Planning%20%28CALP%29%20system%20developed%20at%20Nanyang%0ATechnological%20University%2C%20which%20facilitates%20collision-free%20lifting%20path%0Aplanning%20of%20robotized%20tower%20cranes%20in%20autonomous%20construction%20sites.%20The%0Acurrent%20work%20introduces%20a%20trajectory%20planning%20module%20to%20the%20system%20that%0Autilizes%20the%20geometric%20outputs%20from%20the%20path%20planning%20module%20and%20optimally%0Ascales%20them%20with%20time%20information.%20Firstly%2C%20analyzing%20the%20non-linear%20dynamics%0Aof%20the%20crane%20operations%2C%20the%20tower%20crane%20is%20established%20as%20differentially%20flat.%0ASubsequently%2C%20the%20multi-objective%20trajectory%20optimization%20problems%20for%20all%20the%0Acrane%20operations%20are%20formulated%20in%20the%20flat%20output%20space%20through%20consideration%0Aof%20the%20mechanical%20and%20safety%20constraints.%20Two%20multi-objective%20evolutionary%0Aalgorithms%2C%20namely%20Non-dominated%20Sorting%20Genetic%20Algorithm%20%28NSGA-II%29%20and%0AGeneralized%20Differential%20Evolution%203%20%28GDE3%29%2C%20are%20extensively%20compared%20via%0Astatistical%20measures%20based%20on%20the%20closeness%20of%20solutions%20to%20the%20Pareto%20front%2C%0Adistribution%20of%20solutions%20in%20the%20solution%20space%20and%20the%20runtime%2C%20to%20select%20the%0Aoptimization%20engine%20of%20the%20planner.%20Finally%2C%20the%20crane%20operation%20trajectories%0Aare%20obtained%20via%20the%20corresponding%20planned%20flat%20output%20trajectories.%20Studies%0Asimulating%20real-world%20lifting%20scenarios%20are%20conducted%20to%20verify%20the%0Aeffectiveness%20and%20reliability%20of%20the%20proposed%20module%20of%20the%20lift%20planning%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05581v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Simulation%20of%20Time-energy%20Optimal%20Anti-swing%20Trajectory%0A%20%20Planner%20for%20Autonomous%20Tower%20Cranes&entry.906535625=Souravik%20Dutta%20and%20Yiyu%20Cai&entry.1292438233=%20%20For%20autonomous%20crane%20lifting%2C%20optimal%20trajectories%20of%20the%20crane%20are%20required%0Aas%20reference%20inputs%20to%20the%20crane%20controller%20to%20facilitate%20feedforward%20control.%0AReducing%20the%20unactuated%20payload%20motion%20is%20a%20crucial%20issue%20for%20under-actuated%0Atower%20cranes%20with%20spherical%20pendulum%20dynamics.%20The%20planned%20trajectory%20should%20be%0Aoptimal%20in%20terms%20of%20both%20operating%20time%20and%20energy%20consumption%2C%20to%20facilitate%0Aoptimum%20output%20spending%20optimum%20effort.%20This%20article%20proposes%20an%20anti-swing%0Atower%20crane%20trajectory%20planner%20that%20can%20provide%20time-energy%20optimal%20solutions%0Afor%20the%20Computer-Aided%20Lift%20Planning%20%28CALP%29%20system%20developed%20at%20Nanyang%0ATechnological%20University%2C%20which%20facilitates%20collision-free%20lifting%20path%0Aplanning%20of%20robotized%20tower%20cranes%20in%20autonomous%20construction%20sites.%20The%0Acurrent%20work%20introduces%20a%20trajectory%20planning%20module%20to%20the%20system%20that%0Autilizes%20the%20geometric%20outputs%20from%20the%20path%20planning%20module%20and%20optimally%0Ascales%20them%20with%20time%20information.%20Firstly%2C%20analyzing%20the%20non-linear%20dynamics%0Aof%20the%20crane%20operations%2C%20the%20tower%20crane%20is%20established%20as%20differentially%20flat.%0ASubsequently%2C%20the%20multi-objective%20trajectory%20optimization%20problems%20for%20all%20the%0Acrane%20operations%20are%20formulated%20in%20the%20flat%20output%20space%20through%20consideration%0Aof%20the%20mechanical%20and%20safety%20constraints.%20Two%20multi-objective%20evolutionary%0Aalgorithms%2C%20namely%20Non-dominated%20Sorting%20Genetic%20Algorithm%20%28NSGA-II%29%20and%0AGeneralized%20Differential%20Evolution%203%20%28GDE3%29%2C%20are%20extensively%20compared%20via%0Astatistical%20measures%20based%20on%20the%20closeness%20of%20solutions%20to%20the%20Pareto%20front%2C%0Adistribution%20of%20solutions%20in%20the%20solution%20space%20and%20the%20runtime%2C%20to%20select%20the%0Aoptimization%20engine%20of%20the%20planner.%20Finally%2C%20the%20crane%20operation%20trajectories%0Aare%20obtained%20via%20the%20corresponding%20planned%20flat%20output%20trajectories.%20Studies%0Asimulating%20real-world%20lifting%20scenarios%20are%20conducted%20to%20verify%20the%0Aeffectiveness%20and%20reliability%20of%20the%20proposed%20module%20of%20the%20lift%20planning%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05581v1&entry.124074799=Read"},
{"title": "Synergy of Large Language Model and Model Driven Engineering for\n  Automated Development of Centralized Vehicular Systems", "author": "Nenad Petrovic and Fengjunjie Pan and Krzysztof Lebioda and Vahid Zolfaghari and Sven Kirchner and Nils Purschke and Muhammad Aqib Khan and Viktor Vorobev and Alois Knoll", "abstract": "  We present a prototype of a tool leveraging the synergy of model driven\nengineering (MDE) and Large Language Models (LLM) for the purpose of software\ndevelopment process automation in the automotive industry. In this approach,\nthe user-provided input is free form textual requirements, which are first\ntranslated to Ecore model instance representation using an LLM, which is\nafterwards checked for consistency using Object Constraint Language (OCL)\nrules. After successful consistency check, the model instance is fed as input\nto another LLM for the purpose of code generation. The generated code is\nevaluated in a simulated environment using CARLA simulator connected to an\nexample centralized vehicle architecture, in an emergency brake scenario.\n", "link": "http://arxiv.org/abs/2404.05508v1", "date": "2024-04-08", "relevancy": 0.9257, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4551}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Synergy%20of%20Large%20Language%20Model%20and%20Model%20Driven%20Engineering%20for%0A%20%20Automated%20Development%20of%20Centralized%20Vehicular%20Systems&body=Title%3A%20Synergy%20of%20Large%20Language%20Model%20and%20Model%20Driven%20Engineering%20for%0A%20%20Automated%20Development%20of%20Centralized%20Vehicular%20Systems%0AAuthor%3A%20Nenad%20Petrovic%20and%20Fengjunjie%20Pan%20and%20Krzysztof%20Lebioda%20and%20Vahid%20Zolfaghari%20and%20Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Muhammad%20Aqib%20Khan%20and%20Viktor%20Vorobev%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20We%20present%20a%20prototype%20of%20a%20tool%20leveraging%20the%20synergy%20of%20model%20driven%0Aengineering%20%28MDE%29%20and%20Large%20Language%20Models%20%28LLM%29%20for%20the%20purpose%20of%20software%0Adevelopment%20process%20automation%20in%20the%20automotive%20industry.%20In%20this%20approach%2C%0Athe%20user-provided%20input%20is%20free%20form%20textual%20requirements%2C%20which%20are%20first%0Atranslated%20to%20Ecore%20model%20instance%20representation%20using%20an%20LLM%2C%20which%20is%0Aafterwards%20checked%20for%20consistency%20using%20Object%20Constraint%20Language%20%28OCL%29%0Arules.%20After%20successful%20consistency%20check%2C%20the%20model%20instance%20is%20fed%20as%20input%0Ato%20another%20LLM%20for%20the%20purpose%20of%20code%20generation.%20The%20generated%20code%20is%0Aevaluated%20in%20a%20simulated%20environment%20using%20CARLA%20simulator%20connected%20to%20an%0Aexample%20centralized%20vehicle%20architecture%2C%20in%20an%20emergency%20brake%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05508v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergy%20of%20Large%20Language%20Model%20and%20Model%20Driven%20Engineering%20for%0A%20%20Automated%20Development%20of%20Centralized%20Vehicular%20Systems&entry.906535625=Nenad%20Petrovic%20and%20Fengjunjie%20Pan%20and%20Krzysztof%20Lebioda%20and%20Vahid%20Zolfaghari%20and%20Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Muhammad%20Aqib%20Khan%20and%20Viktor%20Vorobev%20and%20Alois%20Knoll&entry.1292438233=%20%20We%20present%20a%20prototype%20of%20a%20tool%20leveraging%20the%20synergy%20of%20model%20driven%0Aengineering%20%28MDE%29%20and%20Large%20Language%20Models%20%28LLM%29%20for%20the%20purpose%20of%20software%0Adevelopment%20process%20automation%20in%20the%20automotive%20industry.%20In%20this%20approach%2C%0Athe%20user-provided%20input%20is%20free%20form%20textual%20requirements%2C%20which%20are%20first%0Atranslated%20to%20Ecore%20model%20instance%20representation%20using%20an%20LLM%2C%20which%20is%0Aafterwards%20checked%20for%20consistency%20using%20Object%20Constraint%20Language%20%28OCL%29%0Arules.%20After%20successful%20consistency%20check%2C%20the%20model%20instance%20is%20fed%20as%20input%0Ato%20another%20LLM%20for%20the%20purpose%20of%20code%20generation.%20The%20generated%20code%20is%0Aevaluated%20in%20a%20simulated%20environment%20using%20CARLA%20simulator%20connected%20to%20an%0Aexample%20centralized%20vehicle%20architecture%2C%20in%20an%20emergency%20brake%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05508v1&entry.124074799=Read"},
{"title": "A Symbolic Framework for Evaluating Mathematical Reasoning and\n  Generalisation with Transformers", "author": "Jordan Meadows and Marco Valentino and Damien Teney and Andre Freitas", "abstract": "  This paper proposes a methodology for generating and perturbing detailed\nderivations of equations at scale, aided by a symbolic engine, to evaluate the\ngeneralisability of Transformers to out-of-distribution mathematical reasoning\nproblems. Instantiating the framework in the context of sequence classification\ntasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned\nBERT models, exploring the relationship between specific operators and\ngeneralisation failure via the perturbation of reasoning aspects such as\nsymmetry and variable surface forms. Surprisingly, our empirical evaluation\nreveals that the average in-distribution performance of fine-tuned models\nsurpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning\ncan reduce their performance by up to 80 F1 points. Overall, the results\nsuggest that the in-distribution performance of smaller open-source models may\npotentially rival GPT by incorporating appropriately structured derivation\ndependencies during training, and highlight a shared weakness between BERT and\nGPT involving a relative inability to decode indirect references to\nmathematical entities. We release the full codebase, constructed datasets, and\nfine-tuned models to encourage future progress in the field.\n", "link": "http://arxiv.org/abs/2305.12563v2", "date": "2024-04-08", "relevancy": 1.5398, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5697}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Symbolic%20Framework%20for%20Evaluating%20Mathematical%20Reasoning%20and%0A%20%20Generalisation%20with%20Transformers&body=Title%3A%20A%20Symbolic%20Framework%20for%20Evaluating%20Mathematical%20Reasoning%20and%0A%20%20Generalisation%20with%20Transformers%0AAuthor%3A%20Jordan%20Meadows%20and%20Marco%20Valentino%20and%20Damien%20Teney%20and%20Andre%20Freitas%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20methodology%20for%20generating%20and%20perturbing%20detailed%0Aderivations%20of%20equations%20at%20scale%2C%20aided%20by%20a%20symbolic%20engine%2C%20to%20evaluate%20the%0Ageneralisability%20of%20Transformers%20to%20out-of-distribution%20mathematical%20reasoning%0Aproblems.%20Instantiating%20the%20framework%20in%20the%20context%20of%20sequence%20classification%0Atasks%2C%20we%20compare%20the%20capabilities%20of%20GPT-4%2C%20GPT-3.5%2C%20and%20a%20canon%20of%20fine-tuned%0ABERT%20models%2C%20exploring%20the%20relationship%20between%20specific%20operators%20and%0Ageneralisation%20failure%20via%20the%20perturbation%20of%20reasoning%20aspects%20such%20as%0Asymmetry%20and%20variable%20surface%20forms.%20Surprisingly%2C%20our%20empirical%20evaluation%0Areveals%20that%20the%20average%20in-distribution%20performance%20of%20fine-tuned%20models%0Asurpasses%20GPT-3.5%2C%20and%20rivals%20GPT-4.%20However%2C%20perturbations%20to%20input%20reasoning%0Acan%20reduce%20their%20performance%20by%20up%20to%2080%20F1%20points.%20Overall%2C%20the%20results%0Asuggest%20that%20the%20in-distribution%20performance%20of%20smaller%20open-source%20models%20may%0Apotentially%20rival%20GPT%20by%20incorporating%20appropriately%20structured%20derivation%0Adependencies%20during%20training%2C%20and%20highlight%20a%20shared%20weakness%20between%20BERT%20and%0AGPT%20involving%20a%20relative%20inability%20to%20decode%20indirect%20references%20to%0Amathematical%20entities.%20We%20release%20the%20full%20codebase%2C%20constructed%20datasets%2C%20and%0Afine-tuned%20models%20to%20encourage%20future%20progress%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12563v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Symbolic%20Framework%20for%20Evaluating%20Mathematical%20Reasoning%20and%0A%20%20Generalisation%20with%20Transformers&entry.906535625=Jordan%20Meadows%20and%20Marco%20Valentino%20and%20Damien%20Teney%20and%20Andre%20Freitas&entry.1292438233=%20%20This%20paper%20proposes%20a%20methodology%20for%20generating%20and%20perturbing%20detailed%0Aderivations%20of%20equations%20at%20scale%2C%20aided%20by%20a%20symbolic%20engine%2C%20to%20evaluate%20the%0Ageneralisability%20of%20Transformers%20to%20out-of-distribution%20mathematical%20reasoning%0Aproblems.%20Instantiating%20the%20framework%20in%20the%20context%20of%20sequence%20classification%0Atasks%2C%20we%20compare%20the%20capabilities%20of%20GPT-4%2C%20GPT-3.5%2C%20and%20a%20canon%20of%20fine-tuned%0ABERT%20models%2C%20exploring%20the%20relationship%20between%20specific%20operators%20and%0Ageneralisation%20failure%20via%20the%20perturbation%20of%20reasoning%20aspects%20such%20as%0Asymmetry%20and%20variable%20surface%20forms.%20Surprisingly%2C%20our%20empirical%20evaluation%0Areveals%20that%20the%20average%20in-distribution%20performance%20of%20fine-tuned%20models%0Asurpasses%20GPT-3.5%2C%20and%20rivals%20GPT-4.%20However%2C%20perturbations%20to%20input%20reasoning%0Acan%20reduce%20their%20performance%20by%20up%20to%2080%20F1%20points.%20Overall%2C%20the%20results%0Asuggest%20that%20the%20in-distribution%20performance%20of%20smaller%20open-source%20models%20may%0Apotentially%20rival%20GPT%20by%20incorporating%20appropriately%20structured%20derivation%0Adependencies%20during%20training%2C%20and%20highlight%20a%20shared%20weakness%20between%20BERT%20and%0AGPT%20involving%20a%20relative%20inability%20to%20decode%20indirect%20references%20to%0Amathematical%20entities.%20We%20release%20the%20full%20codebase%2C%20constructed%20datasets%2C%20and%0Afine-tuned%20models%20to%20encourage%20future%20progress%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12563v2&entry.124074799=Read"},
{"title": "Stretchable Pneumatic Sleeve for Adaptable, Low-Displacement Anchoring\n  in Exosuits", "author": "Katalin Schaffer and Ultan Fallon and Margaret M. Coad", "abstract": "  Despite recent advances in wearable technology, interfacing movement\nassistance devices with the human body remains challenging. We present a\nstretchable pneumatic sleeve that can anchor an exosuit actuator to the human\narm with a low displacement of the actuator's mounting point relative to the\nbody during operation. Our sleeve has the potential to serve as an adaptable\nattachment mechanism for exosuits, since it can adjust its pressure to only\ncompress the arm as much as needed to transmit the applied exosuit forces\nwithout a large displacement. We discuss the design of our sleeve, which is\nmade of fabric pneumatic artificial muscle (fPAM) actuators formed into bands.\nWe quantify the performance of nine fPAM bands of various lengths and widths,\nas well as three sleeves (an fPAM sleeve, a series pouch motor (SPM) sleeve as\nin previous literature, and an off the shelf hook and loop sleeve), through the\nmeasurement of the compressing force as a function of pressure and the\nlocalized pulling force that can be resisted as a function of both pressure and\nmounting point displacement. Our experimental results show that fPAM bands with\nsmaller resting length and/or larger resting width produce higher forces. Also,\nwhen inflated, an fPAM sleeve that has equivalent dimensions to the SPM sleeve\nwhile fully stretched has similar performance to the SPM sleeve. While\ninflated, both pneumatic sleeves decrease the mounting point displacement\ncompared to the hook and loop sleeve. Compared to the SPM sleeve, the fPAM\nsleeve is able to hold larger internal pressure before bursting, increasing its\npossible force range. Also, when not inflated, the fPAM sleeve resists the\npulling force well, indicating its ability to provide anchoring when not\nactuated.\n", "link": "http://arxiv.org/abs/2403.04729v2", "date": "2024-04-08", "relevancy": 1.1108, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3792}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3622}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stretchable%20Pneumatic%20Sleeve%20for%20Adaptable%2C%20Low-Displacement%20Anchoring%0A%20%20in%20Exosuits&body=Title%3A%20Stretchable%20Pneumatic%20Sleeve%20for%20Adaptable%2C%20Low-Displacement%20Anchoring%0A%20%20in%20Exosuits%0AAuthor%3A%20Katalin%20Schaffer%20and%20Ultan%20Fallon%20and%20Margaret%20M.%20Coad%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20wearable%20technology%2C%20interfacing%20movement%0Aassistance%20devices%20with%20the%20human%20body%20remains%20challenging.%20We%20present%20a%0Astretchable%20pneumatic%20sleeve%20that%20can%20anchor%20an%20exosuit%20actuator%20to%20the%20human%0Aarm%20with%20a%20low%20displacement%20of%20the%20actuator%27s%20mounting%20point%20relative%20to%20the%0Abody%20during%20operation.%20Our%20sleeve%20has%20the%20potential%20to%20serve%20as%20an%20adaptable%0Aattachment%20mechanism%20for%20exosuits%2C%20since%20it%20can%20adjust%20its%20pressure%20to%20only%0Acompress%20the%20arm%20as%20much%20as%20needed%20to%20transmit%20the%20applied%20exosuit%20forces%0Awithout%20a%20large%20displacement.%20We%20discuss%20the%20design%20of%20our%20sleeve%2C%20which%20is%0Amade%20of%20fabric%20pneumatic%20artificial%20muscle%20%28fPAM%29%20actuators%20formed%20into%20bands.%0AWe%20quantify%20the%20performance%20of%20nine%20fPAM%20bands%20of%20various%20lengths%20and%20widths%2C%0Aas%20well%20as%20three%20sleeves%20%28an%20fPAM%20sleeve%2C%20a%20series%20pouch%20motor%20%28SPM%29%20sleeve%20as%0Ain%20previous%20literature%2C%20and%20an%20off%20the%20shelf%20hook%20and%20loop%20sleeve%29%2C%20through%20the%0Ameasurement%20of%20the%20compressing%20force%20as%20a%20function%20of%20pressure%20and%20the%0Alocalized%20pulling%20force%20that%20can%20be%20resisted%20as%20a%20function%20of%20both%20pressure%20and%0Amounting%20point%20displacement.%20Our%20experimental%20results%20show%20that%20fPAM%20bands%20with%0Asmaller%20resting%20length%20and/or%20larger%20resting%20width%20produce%20higher%20forces.%20Also%2C%0Awhen%20inflated%2C%20an%20fPAM%20sleeve%20that%20has%20equivalent%20dimensions%20to%20the%20SPM%20sleeve%0Awhile%20fully%20stretched%20has%20similar%20performance%20to%20the%20SPM%20sleeve.%20While%0Ainflated%2C%20both%20pneumatic%20sleeves%20decrease%20the%20mounting%20point%20displacement%0Acompared%20to%20the%20hook%20and%20loop%20sleeve.%20Compared%20to%20the%20SPM%20sleeve%2C%20the%20fPAM%0Asleeve%20is%20able%20to%20hold%20larger%20internal%20pressure%20before%20bursting%2C%20increasing%20its%0Apossible%20force%20range.%20Also%2C%20when%20not%20inflated%2C%20the%20fPAM%20sleeve%20resists%20the%0Apulling%20force%20well%2C%20indicating%20its%20ability%20to%20provide%20anchoring%20when%20not%0Aactuated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04729v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stretchable%20Pneumatic%20Sleeve%20for%20Adaptable%2C%20Low-Displacement%20Anchoring%0A%20%20in%20Exosuits&entry.906535625=Katalin%20Schaffer%20and%20Ultan%20Fallon%20and%20Margaret%20M.%20Coad&entry.1292438233=%20%20Despite%20recent%20advances%20in%20wearable%20technology%2C%20interfacing%20movement%0Aassistance%20devices%20with%20the%20human%20body%20remains%20challenging.%20We%20present%20a%0Astretchable%20pneumatic%20sleeve%20that%20can%20anchor%20an%20exosuit%20actuator%20to%20the%20human%0Aarm%20with%20a%20low%20displacement%20of%20the%20actuator%27s%20mounting%20point%20relative%20to%20the%0Abody%20during%20operation.%20Our%20sleeve%20has%20the%20potential%20to%20serve%20as%20an%20adaptable%0Aattachment%20mechanism%20for%20exosuits%2C%20since%20it%20can%20adjust%20its%20pressure%20to%20only%0Acompress%20the%20arm%20as%20much%20as%20needed%20to%20transmit%20the%20applied%20exosuit%20forces%0Awithout%20a%20large%20displacement.%20We%20discuss%20the%20design%20of%20our%20sleeve%2C%20which%20is%0Amade%20of%20fabric%20pneumatic%20artificial%20muscle%20%28fPAM%29%20actuators%20formed%20into%20bands.%0AWe%20quantify%20the%20performance%20of%20nine%20fPAM%20bands%20of%20various%20lengths%20and%20widths%2C%0Aas%20well%20as%20three%20sleeves%20%28an%20fPAM%20sleeve%2C%20a%20series%20pouch%20motor%20%28SPM%29%20sleeve%20as%0Ain%20previous%20literature%2C%20and%20an%20off%20the%20shelf%20hook%20and%20loop%20sleeve%29%2C%20through%20the%0Ameasurement%20of%20the%20compressing%20force%20as%20a%20function%20of%20pressure%20and%20the%0Alocalized%20pulling%20force%20that%20can%20be%20resisted%20as%20a%20function%20of%20both%20pressure%20and%0Amounting%20point%20displacement.%20Our%20experimental%20results%20show%20that%20fPAM%20bands%20with%0Asmaller%20resting%20length%20and/or%20larger%20resting%20width%20produce%20higher%20forces.%20Also%2C%0Awhen%20inflated%2C%20an%20fPAM%20sleeve%20that%20has%20equivalent%20dimensions%20to%20the%20SPM%20sleeve%0Awhile%20fully%20stretched%20has%20similar%20performance%20to%20the%20SPM%20sleeve.%20While%0Ainflated%2C%20both%20pneumatic%20sleeves%20decrease%20the%20mounting%20point%20displacement%0Acompared%20to%20the%20hook%20and%20loop%20sleeve.%20Compared%20to%20the%20SPM%20sleeve%2C%20the%20fPAM%0Asleeve%20is%20able%20to%20hold%20larger%20internal%20pressure%20before%20bursting%2C%20increasing%20its%0Apossible%20force%20range.%20Also%2C%20when%20not%20inflated%2C%20the%20fPAM%20sleeve%20resists%20the%0Apulling%20force%20well%2C%20indicating%20its%20ability%20to%20provide%20anchoring%20when%20not%0Aactuated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04729v2&entry.124074799=Read"},
{"title": "Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic\n  Segmentation", "author": "Qitian Ma and Shyam Nanda Rai and Carlo Masone and Tatiana Tommasi", "abstract": "  In the domain of computer vision, semantic segmentation emerges as a\nfundamental application within machine learning, wherein individual pixels of\nan image are classified into distinct semantic categories. This task transcends\ntraditional accuracy metrics by incorporating uncertainty quantification, a\ncritical measure for assessing the reliability of each segmentation prediction.\nSuch quantification is instrumental in facilitating informed decision-making,\nparticularly in applications where precision is paramount. Within this nuanced\nframework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty)\nhas been developed as a specialized tool for evaluating entropy-based\nuncertainty in image segmentation tasks. However, our investigation identifies\nthree core deficiencies within the PAvPU framework and proposes robust\nsolutions aimed at refining the metric. By addressing these issues, we aim to\nenhance the reliability and applicability of uncertainty quantification,\nespecially in scenarios that demand high levels of safety and accuracy, thus\ncontributing to the advancement of semantic segmentation methodologies in\ncritical applications.\n", "link": "http://arxiv.org/abs/2403.19826v2", "date": "2024-04-08", "relevancy": 1.7466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6014}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5789}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Segmentation%20Re-thinking%20Uncertainty%20Estimation%20Metrics%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Segmentation%20Re-thinking%20Uncertainty%20Estimation%20Metrics%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Qitian%20Ma%20and%20Shyam%20Nanda%20Rai%20and%20Carlo%20Masone%20and%20Tatiana%20Tommasi%0AAbstract%3A%20%20%20In%20the%20domain%20of%20computer%20vision%2C%20semantic%20segmentation%20emerges%20as%20a%0Afundamental%20application%20within%20machine%20learning%2C%20wherein%20individual%20pixels%20of%0Aan%20image%20are%20classified%20into%20distinct%20semantic%20categories.%20This%20task%20transcends%0Atraditional%20accuracy%20metrics%20by%20incorporating%20uncertainty%20quantification%2C%20a%0Acritical%20measure%20for%20assessing%20the%20reliability%20of%20each%20segmentation%20prediction.%0ASuch%20quantification%20is%20instrumental%20in%20facilitating%20informed%20decision-making%2C%0Aparticularly%20in%20applications%20where%20precision%20is%20paramount.%20Within%20this%20nuanced%0Aframework%2C%20the%20metric%20known%20as%20PAvPU%20%28Patch%20Accuracy%20versus%20Patch%20Uncertainty%29%0Ahas%20been%20developed%20as%20a%20specialized%20tool%20for%20evaluating%20entropy-based%0Auncertainty%20in%20image%20segmentation%20tasks.%20However%2C%20our%20investigation%20identifies%0Athree%20core%20deficiencies%20within%20the%20PAvPU%20framework%20and%20proposes%20robust%0Asolutions%20aimed%20at%20refining%20the%20metric.%20By%20addressing%20these%20issues%2C%20we%20aim%20to%0Aenhance%20the%20reliability%20and%20applicability%20of%20uncertainty%20quantification%2C%0Aespecially%20in%20scenarios%20that%20demand%20high%20levels%20of%20safety%20and%20accuracy%2C%20thus%0Acontributing%20to%20the%20advancement%20of%20semantic%20segmentation%20methodologies%20in%0Acritical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19826v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20Re-thinking%20Uncertainty%20Estimation%20Metrics%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Qitian%20Ma%20and%20Shyam%20Nanda%20Rai%20and%20Carlo%20Masone%20and%20Tatiana%20Tommasi&entry.1292438233=%20%20In%20the%20domain%20of%20computer%20vision%2C%20semantic%20segmentation%20emerges%20as%20a%0Afundamental%20application%20within%20machine%20learning%2C%20wherein%20individual%20pixels%20of%0Aan%20image%20are%20classified%20into%20distinct%20semantic%20categories.%20This%20task%20transcends%0Atraditional%20accuracy%20metrics%20by%20incorporating%20uncertainty%20quantification%2C%20a%0Acritical%20measure%20for%20assessing%20the%20reliability%20of%20each%20segmentation%20prediction.%0ASuch%20quantification%20is%20instrumental%20in%20facilitating%20informed%20decision-making%2C%0Aparticularly%20in%20applications%20where%20precision%20is%20paramount.%20Within%20this%20nuanced%0Aframework%2C%20the%20metric%20known%20as%20PAvPU%20%28Patch%20Accuracy%20versus%20Patch%20Uncertainty%29%0Ahas%20been%20developed%20as%20a%20specialized%20tool%20for%20evaluating%20entropy-based%0Auncertainty%20in%20image%20segmentation%20tasks.%20However%2C%20our%20investigation%20identifies%0Athree%20core%20deficiencies%20within%20the%20PAvPU%20framework%20and%20proposes%20robust%0Asolutions%20aimed%20at%20refining%20the%20metric.%20By%20addressing%20these%20issues%2C%20we%20aim%20to%0Aenhance%20the%20reliability%20and%20applicability%20of%20uncertainty%20quantification%2C%0Aespecially%20in%20scenarios%20that%20demand%20high%20levels%20of%20safety%20and%20accuracy%2C%20thus%0Acontributing%20to%20the%20advancement%20of%20semantic%20segmentation%20methodologies%20in%0Acritical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19826v2&entry.124074799=Read"},
{"title": "How to Evaluate Entity Resolution Systems: An Entity-Centric Framework\n  with Application to Inventor Name Disambiguation", "author": "Olivier Binette and Youngsoo Baek and Siddharth Engineer and Christina Jones and Abel Dasylva and Jerome P. Reiter", "abstract": "  Entity resolution (record linkage, microclustering) systems are notoriously\ndifficult to evaluate. Looking for a needle in a haystack, traditional\nevaluation methods use sophisticated, application-specific sampling schemes to\nfind matching pairs of records among an immense number of non-matches. We\npropose an alternative that facilitates the creation of representative,\nreusable benchmark data sets without necessitating complex sampling schemes.\nThese benchmark data sets can then be used for model training and a variety of\nevaluation tasks. Specifically, we propose an entity-centric data labeling\nmethodology that integrates with a unified framework for monitoring summary\nstatistics, estimating key performance metrics such as cluster and pairwise\nprecision and recall, and analyzing root causes for errors. We validate the\nframework in an application to inventor name disambiguation and through\nsimulation studies. Software: https://github.com/OlivierBinette/er-evaluation/\n", "link": "http://arxiv.org/abs/2404.05622v1", "date": "2024-04-08", "relevancy": 1.4053, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4782}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4695}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4641}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20to%20Evaluate%20Entity%20Resolution%20Systems%3A%20An%20Entity-Centric%20Framework%0A%20%20with%20Application%20to%20Inventor%20Name%20Disambiguation&body=Title%3A%20How%20to%20Evaluate%20Entity%20Resolution%20Systems%3A%20An%20Entity-Centric%20Framework%0A%20%20with%20Application%20to%20Inventor%20Name%20Disambiguation%0AAuthor%3A%20Olivier%20Binette%20and%20Youngsoo%20Baek%20and%20Siddharth%20Engineer%20and%20Christina%20Jones%20and%20Abel%20Dasylva%20and%20Jerome%20P.%20Reiter%0AAbstract%3A%20%20%20Entity%20resolution%20%28record%20linkage%2C%20microclustering%29%20systems%20are%20notoriously%0Adifficult%20to%20evaluate.%20Looking%20for%20a%20needle%20in%20a%20haystack%2C%20traditional%0Aevaluation%20methods%20use%20sophisticated%2C%20application-specific%20sampling%20schemes%20to%0Afind%20matching%20pairs%20of%20records%20among%20an%20immense%20number%20of%20non-matches.%20We%0Apropose%20an%20alternative%20that%20facilitates%20the%20creation%20of%20representative%2C%0Areusable%20benchmark%20data%20sets%20without%20necessitating%20complex%20sampling%20schemes.%0AThese%20benchmark%20data%20sets%20can%20then%20be%20used%20for%20model%20training%20and%20a%20variety%20of%0Aevaluation%20tasks.%20Specifically%2C%20we%20propose%20an%20entity-centric%20data%20labeling%0Amethodology%20that%20integrates%20with%20a%20unified%20framework%20for%20monitoring%20summary%0Astatistics%2C%20estimating%20key%20performance%20metrics%20such%20as%20cluster%20and%20pairwise%0Aprecision%20and%20recall%2C%20and%20analyzing%20root%20causes%20for%20errors.%20We%20validate%20the%0Aframework%20in%20an%20application%20to%20inventor%20name%20disambiguation%20and%20through%0Asimulation%20studies.%20Software%3A%20https%3A//github.com/OlivierBinette/er-evaluation/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05622v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Evaluate%20Entity%20Resolution%20Systems%3A%20An%20Entity-Centric%20Framework%0A%20%20with%20Application%20to%20Inventor%20Name%20Disambiguation&entry.906535625=Olivier%20Binette%20and%20Youngsoo%20Baek%20and%20Siddharth%20Engineer%20and%20Christina%20Jones%20and%20Abel%20Dasylva%20and%20Jerome%20P.%20Reiter&entry.1292438233=%20%20Entity%20resolution%20%28record%20linkage%2C%20microclustering%29%20systems%20are%20notoriously%0Adifficult%20to%20evaluate.%20Looking%20for%20a%20needle%20in%20a%20haystack%2C%20traditional%0Aevaluation%20methods%20use%20sophisticated%2C%20application-specific%20sampling%20schemes%20to%0Afind%20matching%20pairs%20of%20records%20among%20an%20immense%20number%20of%20non-matches.%20We%0Apropose%20an%20alternative%20that%20facilitates%20the%20creation%20of%20representative%2C%0Areusable%20benchmark%20data%20sets%20without%20necessitating%20complex%20sampling%20schemes.%0AThese%20benchmark%20data%20sets%20can%20then%20be%20used%20for%20model%20training%20and%20a%20variety%20of%0Aevaluation%20tasks.%20Specifically%2C%20we%20propose%20an%20entity-centric%20data%20labeling%0Amethodology%20that%20integrates%20with%20a%20unified%20framework%20for%20monitoring%20summary%0Astatistics%2C%20estimating%20key%20performance%20metrics%20such%20as%20cluster%20and%20pairwise%0Aprecision%20and%20recall%2C%20and%20analyzing%20root%20causes%20for%20errors.%20We%20validate%20the%0Aframework%20in%20an%20application%20to%20inventor%20name%20disambiguation%20and%20through%0Asimulation%20studies.%20Software%3A%20https%3A//github.com/OlivierBinette/er-evaluation/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05622v1&entry.124074799=Read"},
{"title": "Robot Interaction Behavior Generation based on Social Motion Forecasting\n  for Human-Robot Interaction", "author": "Esteve Valls Mascaro and Yashuai Yan and Dongheui Lee", "abstract": "  Integrating robots into populated environments is a complex challenge that\nrequires an understanding of human social dynamics. In this work, we propose to\nmodel social motion forecasting in a shared human-robot representation space,\nwhich facilitates us to synthesize robot motions that interact with humans in\nsocial scenarios despite not observing any robot in the motion training. We\ndevelop a transformer-based architecture called ECHO, which operates in the\naforementioned shared space to predict the future motions of the agents\nencountered in social scenarios. Contrary to prior works, we reformulate the\nsocial motion problem as the refinement of the predicted individual motions\nbased on the surrounding agents, which facilitates the training while allowing\nfor single-motion forecasting when only one human is in the scene. We evaluate\nour model in multi-person and human-robot motion forecasting tasks and obtain\nstate-of-the-art performance by a large margin while being efficient and\nperforming in real-time. Additionally, our qualitative results showcase the\neffectiveness of our approach in generating human-robot interaction behaviors\nthat can be controlled via text commands. Webpage: https://evm7.github.io/ECHO/\n", "link": "http://arxiv.org/abs/2402.04768v2", "date": "2024-04-08", "relevancy": 1.6847, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5942}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.54}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robot%20Interaction%20Behavior%20Generation%20based%20on%20Social%20Motion%20Forecasting%0A%20%20for%20Human-Robot%20Interaction&body=Title%3A%20Robot%20Interaction%20Behavior%20Generation%20based%20on%20Social%20Motion%20Forecasting%0A%20%20for%20Human-Robot%20Interaction%0AAuthor%3A%20Esteve%20Valls%20Mascaro%20and%20Yashuai%20Yan%20and%20Dongheui%20Lee%0AAbstract%3A%20%20%20Integrating%20robots%20into%20populated%20environments%20is%20a%20complex%20challenge%20that%0Arequires%20an%20understanding%20of%20human%20social%20dynamics.%20In%20this%20work%2C%20we%20propose%20to%0Amodel%20social%20motion%20forecasting%20in%20a%20shared%20human-robot%20representation%20space%2C%0Awhich%20facilitates%20us%20to%20synthesize%20robot%20motions%20that%20interact%20with%20humans%20in%0Asocial%20scenarios%20despite%20not%20observing%20any%20robot%20in%20the%20motion%20training.%20We%0Adevelop%20a%20transformer-based%20architecture%20called%20ECHO%2C%20which%20operates%20in%20the%0Aaforementioned%20shared%20space%20to%20predict%20the%20future%20motions%20of%20the%20agents%0Aencountered%20in%20social%20scenarios.%20Contrary%20to%20prior%20works%2C%20we%20reformulate%20the%0Asocial%20motion%20problem%20as%20the%20refinement%20of%20the%20predicted%20individual%20motions%0Abased%20on%20the%20surrounding%20agents%2C%20which%20facilitates%20the%20training%20while%20allowing%0Afor%20single-motion%20forecasting%20when%20only%20one%20human%20is%20in%20the%20scene.%20We%20evaluate%0Aour%20model%20in%20multi-person%20and%20human-robot%20motion%20forecasting%20tasks%20and%20obtain%0Astate-of-the-art%20performance%20by%20a%20large%20margin%20while%20being%20efficient%20and%0Aperforming%20in%20real-time.%20Additionally%2C%20our%20qualitative%20results%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20generating%20human-robot%20interaction%20behaviors%0Athat%20can%20be%20controlled%20via%20text%20commands.%20Webpage%3A%20https%3A//evm7.github.io/ECHO/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04768v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Interaction%20Behavior%20Generation%20based%20on%20Social%20Motion%20Forecasting%0A%20%20for%20Human-Robot%20Interaction&entry.906535625=Esteve%20Valls%20Mascaro%20and%20Yashuai%20Yan%20and%20Dongheui%20Lee&entry.1292438233=%20%20Integrating%20robots%20into%20populated%20environments%20is%20a%20complex%20challenge%20that%0Arequires%20an%20understanding%20of%20human%20social%20dynamics.%20In%20this%20work%2C%20we%20propose%20to%0Amodel%20social%20motion%20forecasting%20in%20a%20shared%20human-robot%20representation%20space%2C%0Awhich%20facilitates%20us%20to%20synthesize%20robot%20motions%20that%20interact%20with%20humans%20in%0Asocial%20scenarios%20despite%20not%20observing%20any%20robot%20in%20the%20motion%20training.%20We%0Adevelop%20a%20transformer-based%20architecture%20called%20ECHO%2C%20which%20operates%20in%20the%0Aaforementioned%20shared%20space%20to%20predict%20the%20future%20motions%20of%20the%20agents%0Aencountered%20in%20social%20scenarios.%20Contrary%20to%20prior%20works%2C%20we%20reformulate%20the%0Asocial%20motion%20problem%20as%20the%20refinement%20of%20the%20predicted%20individual%20motions%0Abased%20on%20the%20surrounding%20agents%2C%20which%20facilitates%20the%20training%20while%20allowing%0Afor%20single-motion%20forecasting%20when%20only%20one%20human%20is%20in%20the%20scene.%20We%20evaluate%0Aour%20model%20in%20multi-person%20and%20human-robot%20motion%20forecasting%20tasks%20and%20obtain%0Astate-of-the-art%20performance%20by%20a%20large%20margin%20while%20being%20efficient%20and%0Aperforming%20in%20real-time.%20Additionally%2C%20our%20qualitative%20results%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20generating%20human-robot%20interaction%20behaviors%0Athat%20can%20be%20controlled%20via%20text%20commands.%20Webpage%3A%20https%3A//evm7.github.io/ECHO/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04768v2&entry.124074799=Read"},
{"title": "DRCT: Saving Image Super-resolution away from Information Bottleneck", "author": "Chih-Chung Hsu and Chia-Ming Lee and Yi-Shiuan Chou", "abstract": "  In recent years, Vision Transformer-based applications to low-level vision\ntasks have achieved widespread success. Unlike CNN-based models, Transformers\nare more adept at capturing long-range dependencies, enabling the\nreconstruction of images utilizing information from non-local areas. In the\ndomain of super-resolution, Swin-transformer-based approaches have become\nmainstream due to their capacity to capture global spatial information and\ntheir shifting-window attention mechanism that facilitates the interchange of\ninformation between different windows. Many researchers have enhanced image\nquality and network efficiency by expanding the receptive field or designing\ncomplex networks, yielding commendable results. However, we observed that\nspatial information tends to diminish during the forward propagation process\ndue to increased depth, leading to a loss of spatial information and,\nconsequently, limiting the model's potential. To address this, we propose the\nDense-residual-connected Transformer (DRCT), aimed at mitigating the loss of\nspatial information through dense-residual connections between layers, thereby\nunleashing the model's potential and enhancing performance. Experiment results\nindicate that our approach is not only straightforward but also achieves\nremarkable efficiency, surpassing state-of-the-art methods and performing\ncommendably at NTIRE2024.\n", "link": "http://arxiv.org/abs/2404.00722v3", "date": "2024-04-08", "relevancy": 1.7541, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6154}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5785}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5695}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DRCT%3A%20Saving%20Image%20Super-resolution%20away%20from%20Information%20Bottleneck&body=Title%3A%20DRCT%3A%20Saving%20Image%20Super-resolution%20away%20from%20Information%20Bottleneck%0AAuthor%3A%20Chih-Chung%20Hsu%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Vision%20Transformer-based%20applications%20to%20low-level%20vision%0Atasks%20have%20achieved%20widespread%20success.%20Unlike%20CNN-based%20models%2C%20Transformers%0Aare%20more%20adept%20at%20capturing%20long-range%20dependencies%2C%20enabling%20the%0Areconstruction%20of%20images%20utilizing%20information%20from%20non-local%20areas.%20In%20the%0Adomain%20of%20super-resolution%2C%20Swin-transformer-based%20approaches%20have%20become%0Amainstream%20due%20to%20their%20capacity%20to%20capture%20global%20spatial%20information%20and%0Atheir%20shifting-window%20attention%20mechanism%20that%20facilitates%20the%20interchange%20of%0Ainformation%20between%20different%20windows.%20Many%20researchers%20have%20enhanced%20image%0Aquality%20and%20network%20efficiency%20by%20expanding%20the%20receptive%20field%20or%20designing%0Acomplex%20networks%2C%20yielding%20commendable%20results.%20However%2C%20we%20observed%20that%0Aspatial%20information%20tends%20to%20diminish%20during%20the%20forward%20propagation%20process%0Adue%20to%20increased%20depth%2C%20leading%20to%20a%20loss%20of%20spatial%20information%20and%2C%0Aconsequently%2C%20limiting%20the%20model%27s%20potential.%20To%20address%20this%2C%20we%20propose%20the%0ADense-residual-connected%20Transformer%20%28DRCT%29%2C%20aimed%20at%20mitigating%20the%20loss%20of%0Aspatial%20information%20through%20dense-residual%20connections%20between%20layers%2C%20thereby%0Aunleashing%20the%20model%27s%20potential%20and%20enhancing%20performance.%20Experiment%20results%0Aindicate%20that%20our%20approach%20is%20not%20only%20straightforward%20but%20also%20achieves%0Aremarkable%20efficiency%2C%20surpassing%20state-of-the-art%20methods%20and%20performing%0Acommendably%20at%20NTIRE2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00722v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRCT%3A%20Saving%20Image%20Super-resolution%20away%20from%20Information%20Bottleneck&entry.906535625=Chih-Chung%20Hsu%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou&entry.1292438233=%20%20In%20recent%20years%2C%20Vision%20Transformer-based%20applications%20to%20low-level%20vision%0Atasks%20have%20achieved%20widespread%20success.%20Unlike%20CNN-based%20models%2C%20Transformers%0Aare%20more%20adept%20at%20capturing%20long-range%20dependencies%2C%20enabling%20the%0Areconstruction%20of%20images%20utilizing%20information%20from%20non-local%20areas.%20In%20the%0Adomain%20of%20super-resolution%2C%20Swin-transformer-based%20approaches%20have%20become%0Amainstream%20due%20to%20their%20capacity%20to%20capture%20global%20spatial%20information%20and%0Atheir%20shifting-window%20attention%20mechanism%20that%20facilitates%20the%20interchange%20of%0Ainformation%20between%20different%20windows.%20Many%20researchers%20have%20enhanced%20image%0Aquality%20and%20network%20efficiency%20by%20expanding%20the%20receptive%20field%20or%20designing%0Acomplex%20networks%2C%20yielding%20commendable%20results.%20However%2C%20we%20observed%20that%0Aspatial%20information%20tends%20to%20diminish%20during%20the%20forward%20propagation%20process%0Adue%20to%20increased%20depth%2C%20leading%20to%20a%20loss%20of%20spatial%20information%20and%2C%0Aconsequently%2C%20limiting%20the%20model%27s%20potential.%20To%20address%20this%2C%20we%20propose%20the%0ADense-residual-connected%20Transformer%20%28DRCT%29%2C%20aimed%20at%20mitigating%20the%20loss%20of%0Aspatial%20information%20through%20dense-residual%20connections%20between%20layers%2C%20thereby%0Aunleashing%20the%20model%27s%20potential%20and%20enhancing%20performance.%20Experiment%20results%0Aindicate%20that%20our%20approach%20is%20not%20only%20straightforward%20but%20also%20achieves%0Aremarkable%20efficiency%2C%20surpassing%20state-of-the-art%20methods%20and%20performing%0Acommendably%20at%20NTIRE2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00722v3&entry.124074799=Read"},
{"title": "Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic\n  Segmentation for Satellite Imagery", "author": "Ionut M. Motoi and Leonardo Saraceni and Daniele Nardi and Thomas A. Ciarfuglia", "abstract": "  Satellite imagery is crucial for tasks like environmental monitoring and\nurban planning. Typically, it relies on semantic segmentation or Land Use Land\nCover (LULC) classification to categorize each pixel. Despite the advancements\nbrought about by Deep Neural Networks (DNNs), their performance in segmentation\ntasks is hindered by challenges such as limited availability of labeled data,\nclass imbalance and the inherent variability and complexity of satellite\nimages. In order to mitigate those issues, our study explores the effectiveness\nof a Cut-and-Paste augmentation technique for semantic segmentation in\nsatellite images. We adapt this augmentation, which usually requires labeled\ninstances, to the case of semantic segmentation. By leveraging the connected\ncomponents in the semantic segmentation labels, we extract instances that are\nthen randomly pasted during training. Using the DynamicEarthNet dataset and a\nU-Net model for evaluation, we found that this augmentation significantly\nenhances the mIoU score on the test set from 37.9 to 44.1. This finding\nhighlights the potential of the Cut-and-Paste augmentation to improve the\ngeneralization capabilities of semantic segmentation models in satellite\nimagery.\n", "link": "http://arxiv.org/abs/2404.05693v1", "date": "2024-04-08", "relevancy": 1.5437, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5239}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5198}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Efficacy%20of%20Cut-and-Paste%20Data%20Augmentation%20in%20Semantic%0A%20%20Segmentation%20for%20Satellite%20Imagery&body=Title%3A%20Evaluating%20the%20Efficacy%20of%20Cut-and-Paste%20Data%20Augmentation%20in%20Semantic%0A%20%20Segmentation%20for%20Satellite%20Imagery%0AAuthor%3A%20Ionut%20M.%20Motoi%20and%20Leonardo%20Saraceni%20and%20Daniele%20Nardi%20and%20Thomas%20A.%20Ciarfuglia%0AAbstract%3A%20%20%20Satellite%20imagery%20is%20crucial%20for%20tasks%20like%20environmental%20monitoring%20and%0Aurban%20planning.%20Typically%2C%20it%20relies%20on%20semantic%20segmentation%20or%20Land%20Use%20Land%0ACover%20%28LULC%29%20classification%20to%20categorize%20each%20pixel.%20Despite%20the%20advancements%0Abrought%20about%20by%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20their%20performance%20in%20segmentation%0Atasks%20is%20hindered%20by%20challenges%20such%20as%20limited%20availability%20of%20labeled%20data%2C%0Aclass%20imbalance%20and%20the%20inherent%20variability%20and%20complexity%20of%20satellite%0Aimages.%20In%20order%20to%20mitigate%20those%20issues%2C%20our%20study%20explores%20the%20effectiveness%0Aof%20a%20Cut-and-Paste%20augmentation%20technique%20for%20semantic%20segmentation%20in%0Asatellite%20images.%20We%20adapt%20this%20augmentation%2C%20which%20usually%20requires%20labeled%0Ainstances%2C%20to%20the%20case%20of%20semantic%20segmentation.%20By%20leveraging%20the%20connected%0Acomponents%20in%20the%20semantic%20segmentation%20labels%2C%20we%20extract%20instances%20that%20are%0Athen%20randomly%20pasted%20during%20training.%20Using%20the%20DynamicEarthNet%20dataset%20and%20a%0AU-Net%20model%20for%20evaluation%2C%20we%20found%20that%20this%20augmentation%20significantly%0Aenhances%20the%20mIoU%20score%20on%20the%20test%20set%20from%2037.9%20to%2044.1.%20This%20finding%0Ahighlights%20the%20potential%20of%20the%20Cut-and-Paste%20augmentation%20to%20improve%20the%0Ageneralization%20capabilities%20of%20semantic%20segmentation%20models%20in%20satellite%0Aimagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05693v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Efficacy%20of%20Cut-and-Paste%20Data%20Augmentation%20in%20Semantic%0A%20%20Segmentation%20for%20Satellite%20Imagery&entry.906535625=Ionut%20M.%20Motoi%20and%20Leonardo%20Saraceni%20and%20Daniele%20Nardi%20and%20Thomas%20A.%20Ciarfuglia&entry.1292438233=%20%20Satellite%20imagery%20is%20crucial%20for%20tasks%20like%20environmental%20monitoring%20and%0Aurban%20planning.%20Typically%2C%20it%20relies%20on%20semantic%20segmentation%20or%20Land%20Use%20Land%0ACover%20%28LULC%29%20classification%20to%20categorize%20each%20pixel.%20Despite%20the%20advancements%0Abrought%20about%20by%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20their%20performance%20in%20segmentation%0Atasks%20is%20hindered%20by%20challenges%20such%20as%20limited%20availability%20of%20labeled%20data%2C%0Aclass%20imbalance%20and%20the%20inherent%20variability%20and%20complexity%20of%20satellite%0Aimages.%20In%20order%20to%20mitigate%20those%20issues%2C%20our%20study%20explores%20the%20effectiveness%0Aof%20a%20Cut-and-Paste%20augmentation%20technique%20for%20semantic%20segmentation%20in%0Asatellite%20images.%20We%20adapt%20this%20augmentation%2C%20which%20usually%20requires%20labeled%0Ainstances%2C%20to%20the%20case%20of%20semantic%20segmentation.%20By%20leveraging%20the%20connected%0Acomponents%20in%20the%20semantic%20segmentation%20labels%2C%20we%20extract%20instances%20that%20are%0Athen%20randomly%20pasted%20during%20training.%20Using%20the%20DynamicEarthNet%20dataset%20and%20a%0AU-Net%20model%20for%20evaluation%2C%20we%20found%20that%20this%20augmentation%20significantly%0Aenhances%20the%20mIoU%20score%20on%20the%20test%20set%20from%2037.9%20to%2044.1.%20This%20finding%0Ahighlights%20the%20potential%20of%20the%20Cut-and-Paste%20augmentation%20to%20improve%20the%0Ageneralization%20capabilities%20of%20semantic%20segmentation%20models%20in%20satellite%0Aimagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05693v1&entry.124074799=Read"},
{"title": "Faithful and Robust Local Interpretability for Textual Predictions", "author": "Gianluigi Lopardo and Frederic Precioso and Damien Garreau", "abstract": "  Interpretability is essential for machine learning models to be trusted and\ndeployed in critical domains. However, existing methods for interpreting text\nmodels are often complex, lack mathematical foundations, and their performance\nis not guaranteed. In this paper, we propose FRED (Faithful and Robust\nExplainer for textual Documents), a novel method for interpreting predictions\nover text. FRED offers three key insights to explain a model prediction: (1) it\nidentifies the minimal set of words in a document whose removal has the\nstrongest influence on the prediction, (2) it assigns an importance score to\neach token, reflecting its influence on the model's output, and (3) it provides\ncounterfactual explanations by generating examples similar to the original\ndocument, but leading to a different prediction. We establish the reliability\nof FRED through formal definitions and theoretical analyses on interpretable\nclassifiers. Additionally, our empirical evaluation against state-of-the-art\nmethods demonstrates the effectiveness of FRED in providing insights into text\nmodels.\n", "link": "http://arxiv.org/abs/2311.01605v2", "date": "2024-04-08", "relevancy": 1.5448, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5154}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5099}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions&body=Title%3A%20Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions%0AAuthor%3A%20Gianluigi%20Lopardo%20and%20Frederic%20Precioso%20and%20Damien%20Garreau%0AAbstract%3A%20%20%20Interpretability%20is%20essential%20for%20machine%20learning%20models%20to%20be%20trusted%20and%0Adeployed%20in%20critical%20domains.%20However%2C%20existing%20methods%20for%20interpreting%20text%0Amodels%20are%20often%20complex%2C%20lack%20mathematical%20foundations%2C%20and%20their%20performance%0Ais%20not%20guaranteed.%20In%20this%20paper%2C%20we%20propose%20FRED%20%28Faithful%20and%20Robust%0AExplainer%20for%20textual%20Documents%29%2C%20a%20novel%20method%20for%20interpreting%20predictions%0Aover%20text.%20FRED%20offers%20three%20key%20insights%20to%20explain%20a%20model%20prediction%3A%20%281%29%20it%0Aidentifies%20the%20minimal%20set%20of%20words%20in%20a%20document%20whose%20removal%20has%20the%0Astrongest%20influence%20on%20the%20prediction%2C%20%282%29%20it%20assigns%20an%20importance%20score%20to%0Aeach%20token%2C%20reflecting%20its%20influence%20on%20the%20model%27s%20output%2C%20and%20%283%29%20it%20provides%0Acounterfactual%20explanations%20by%20generating%20examples%20similar%20to%20the%20original%0Adocument%2C%20but%20leading%20to%20a%20different%20prediction.%20We%20establish%20the%20reliability%0Aof%20FRED%20through%20formal%20definitions%20and%20theoretical%20analyses%20on%20interpretable%0Aclassifiers.%20Additionally%2C%20our%20empirical%20evaluation%20against%20state-of-the-art%0Amethods%20demonstrates%20the%20effectiveness%20of%20FRED%20in%20providing%20insights%20into%20text%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01605v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions&entry.906535625=Gianluigi%20Lopardo%20and%20Frederic%20Precioso%20and%20Damien%20Garreau&entry.1292438233=%20%20Interpretability%20is%20essential%20for%20machine%20learning%20models%20to%20be%20trusted%20and%0Adeployed%20in%20critical%20domains.%20However%2C%20existing%20methods%20for%20interpreting%20text%0Amodels%20are%20often%20complex%2C%20lack%20mathematical%20foundations%2C%20and%20their%20performance%0Ais%20not%20guaranteed.%20In%20this%20paper%2C%20we%20propose%20FRED%20%28Faithful%20and%20Robust%0AExplainer%20for%20textual%20Documents%29%2C%20a%20novel%20method%20for%20interpreting%20predictions%0Aover%20text.%20FRED%20offers%20three%20key%20insights%20to%20explain%20a%20model%20prediction%3A%20%281%29%20it%0Aidentifies%20the%20minimal%20set%20of%20words%20in%20a%20document%20whose%20removal%20has%20the%0Astrongest%20influence%20on%20the%20prediction%2C%20%282%29%20it%20assigns%20an%20importance%20score%20to%0Aeach%20token%2C%20reflecting%20its%20influence%20on%20the%20model%27s%20output%2C%20and%20%283%29%20it%20provides%0Acounterfactual%20explanations%20by%20generating%20examples%20similar%20to%20the%20original%0Adocument%2C%20but%20leading%20to%20a%20different%20prediction.%20We%20establish%20the%20reliability%0Aof%20FRED%20through%20formal%20definitions%20and%20theoretical%20analyses%20on%20interpretable%0Aclassifiers.%20Additionally%2C%20our%20empirical%20evaluation%20against%20state-of-the-art%0Amethods%20demonstrates%20the%20effectiveness%20of%20FRED%20in%20providing%20insights%20into%20text%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01605v2&entry.124074799=Read"},
{"title": "The Open Autonomy Safety Case Framework", "author": "Michael Wagner and Carmen Carlan", "abstract": "  A system safety case is a compelling, comprehensible, and valid argument\nabout the satisfaction of the safety goals of a given system operating in a\ngiven environment supported by convincing evidence. Since the publication of UL\n4600 in 2020, safety cases have become a best practice for measuring, managing,\nand communicating the safety of autonomous vehicles (AVs). Although UL 4600\nprovides guidance on how to build the safety case for an AV, the complexity of\nAVs and their operating environments, the novelty of the used technology, the\nneed for complying with various regulations and technical standards, and for\naddressing cybersecurity concerns and ethical considerations make the\ndevelopment of safety cases for AVs challenging. To this end, safety case\nframeworks have been proposed that bring strategies, argument templates, and\nother guidance together to support the development of a safety case. This paper\nintroduces the Open Autonomy Safety Case Framework, developed over years of\nwork with the autonomous vehicle industry, as a roadmap for how AVs can be\ndeployed safely and responsibly.\n", "link": "http://arxiv.org/abs/2404.05444v1", "date": "2024-04-08", "relevancy": 1.6936, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4069}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4037}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Open%20Autonomy%20Safety%20Case%20Framework&body=Title%3A%20The%20Open%20Autonomy%20Safety%20Case%20Framework%0AAuthor%3A%20Michael%20Wagner%20and%20Carmen%20Carlan%0AAbstract%3A%20%20%20A%20system%20safety%20case%20is%20a%20compelling%2C%20comprehensible%2C%20and%20valid%20argument%0Aabout%20the%20satisfaction%20of%20the%20safety%20goals%20of%20a%20given%20system%20operating%20in%20a%0Agiven%20environment%20supported%20by%20convincing%20evidence.%20Since%20the%20publication%20of%20UL%0A4600%20in%202020%2C%20safety%20cases%20have%20become%20a%20best%20practice%20for%20measuring%2C%20managing%2C%0Aand%20communicating%20the%20safety%20of%20autonomous%20vehicles%20%28AVs%29.%20Although%20UL%204600%0Aprovides%20guidance%20on%20how%20to%20build%20the%20safety%20case%20for%20an%20AV%2C%20the%20complexity%20of%0AAVs%20and%20their%20operating%20environments%2C%20the%20novelty%20of%20the%20used%20technology%2C%20the%0Aneed%20for%20complying%20with%20various%20regulations%20and%20technical%20standards%2C%20and%20for%0Aaddressing%20cybersecurity%20concerns%20and%20ethical%20considerations%20make%20the%0Adevelopment%20of%20safety%20cases%20for%20AVs%20challenging.%20To%20this%20end%2C%20safety%20case%0Aframeworks%20have%20been%20proposed%20that%20bring%20strategies%2C%20argument%20templates%2C%20and%0Aother%20guidance%20together%20to%20support%20the%20development%20of%20a%20safety%20case.%20This%20paper%0Aintroduces%20the%20Open%20Autonomy%20Safety%20Case%20Framework%2C%20developed%20over%20years%20of%0Awork%20with%20the%20autonomous%20vehicle%20industry%2C%20as%20a%20roadmap%20for%20how%20AVs%20can%20be%0Adeployed%20safely%20and%20responsibly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05444v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Open%20Autonomy%20Safety%20Case%20Framework&entry.906535625=Michael%20Wagner%20and%20Carmen%20Carlan&entry.1292438233=%20%20A%20system%20safety%20case%20is%20a%20compelling%2C%20comprehensible%2C%20and%20valid%20argument%0Aabout%20the%20satisfaction%20of%20the%20safety%20goals%20of%20a%20given%20system%20operating%20in%20a%0Agiven%20environment%20supported%20by%20convincing%20evidence.%20Since%20the%20publication%20of%20UL%0A4600%20in%202020%2C%20safety%20cases%20have%20become%20a%20best%20practice%20for%20measuring%2C%20managing%2C%0Aand%20communicating%20the%20safety%20of%20autonomous%20vehicles%20%28AVs%29.%20Although%20UL%204600%0Aprovides%20guidance%20on%20how%20to%20build%20the%20safety%20case%20for%20an%20AV%2C%20the%20complexity%20of%0AAVs%20and%20their%20operating%20environments%2C%20the%20novelty%20of%20the%20used%20technology%2C%20the%0Aneed%20for%20complying%20with%20various%20regulations%20and%20technical%20standards%2C%20and%20for%0Aaddressing%20cybersecurity%20concerns%20and%20ethical%20considerations%20make%20the%0Adevelopment%20of%20safety%20cases%20for%20AVs%20challenging.%20To%20this%20end%2C%20safety%20case%0Aframeworks%20have%20been%20proposed%20that%20bring%20strategies%2C%20argument%20templates%2C%20and%0Aother%20guidance%20together%20to%20support%20the%20development%20of%20a%20safety%20case.%20This%20paper%0Aintroduces%20the%20Open%20Autonomy%20Safety%20Case%20Framework%2C%20developed%20over%20years%20of%0Awork%20with%20the%20autonomous%20vehicle%20industry%2C%20as%20a%20roadmap%20for%20how%20AVs%20can%20be%0Adeployed%20safely%20and%20responsibly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05444v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


